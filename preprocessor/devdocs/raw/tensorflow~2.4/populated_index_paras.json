[{"name": "tf.AggregationMethod", "path": "aggregationmethod", "type": "tf", "text": ["A class listing aggregation methods used to combine gradients.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.AggregationMethod", "Computing partial derivatives can require aggregating gradient contributions. This class lists the various methods that can be used to combine gradients in the graph.", "The following aggregation methods are part of the stable API for aggregating gradients:", "The following aggregation methods are experimental and may not be supported in future releases:"]}, {"name": "tf.argsort", "path": "argsort", "type": "tf", "text": ["Returns the indices of a tensor that give its sorted order along an axis.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.argsort", "For a 1D tensor, tf.gather(values, tf.argsort(values)) is equivalent to tf.sort(values). For higher dimensions, the output has the same shape as values, but along the given axis, values represent the index of the sorted element in that slice of the tensor at the given position."]}, {"name": "tf.audio", "path": "audio", "type": "tf.audio", "text": ["Public API for tf.audio namespace.", "decode_wav(...): Decode a 16-bit PCM WAV file to a float tensor.", "encode_wav(...): Encode audio data using the WAV file format."]}, {"name": "tf.audio.decode_wav", "path": "audio/decode_wav", "type": "tf.audio", "text": ["Decode a 16-bit PCM WAV file to a float tensor.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.audio.decode_wav", "The -32768 to 32767 signed 16-bit values will be scaled to -1.0 to 1.0 in float.", "When desired_channels is set, if the input contains fewer channels than this then the last channel will be duplicated to give the requested number, else if the input has more channels than requested then the additional channels will be ignored.", "If desired_samples is set, then the audio will be cropped or padded with zeroes to the requested length.", "The first output contains a Tensor with the content of the audio samples. The lowest dimension will be the number of channels, and the second will be the number of samples. For example, a ten-sample-long stereo WAV file should give an output shape of [10, 2]."]}, {"name": "tf.audio.encode_wav", "path": "audio/encode_wav", "type": "tf.audio", "text": ["Encode audio data using the WAV file format.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.audio.encode_wav", "This operation will generate a string suitable to be saved out to create a .wav audio file. It will be encoded in the 16-bit PCM format. It takes in float values in the range -1.0f to 1.0f, and any outside that value will be clamped to that range.", "audio is a 2-D float Tensor of shape [length, channels]. sample_rate is a scalar Tensor holding the rate to use (e.g. 44100)."]}, {"name": "tf.autodiff", "path": "autodiff", "type": "tf.autodiff", "text": ["Public API for tf.autodiff namespace.", "class ForwardAccumulator: Computes Jacobian-vector products (\"JVP\"s) using forward-mode autodiff.", "class GradientTape: Record operations for automatic differentiation."]}, {"name": "tf.autodiff.ForwardAccumulator", "path": "autodiff/forwardaccumulator", "type": "tf.autodiff", "text": ["Computes Jacobian-vector products (\"JVP\"s) using forward-mode autodiff.", "Compare to tf.GradientTape which computes vector-Jacobian products (\"VJP\"s) using reverse-mode autodiff (backprop). Reverse mode is more attractive when computing gradients of a scalar-valued function with respect to many inputs (e.g. a neural network with many parameters and a scalar loss). Forward mode works best on functions with many outputs and few inputs. Since it does not hold on to intermediate activations, it is much more memory efficient than backprop where it is applicable.", "Consider a simple linear regression:", "The example has two variables containing parameters, dense.kernel (2 parameters) and dense.bias (1 parameter). Considering the training data x as a constant, this means the Jacobian matrix for the function mapping from parameters to loss has one row and three columns.", "With forwardprop, we specify a length-three vector in advance which multiplies the Jacobian. The primals constructor argument is the parameter (a tf.Tensor or tf.Variable) we're specifying a vector for, and the tangents argument is the \"vector\" in Jacobian-vector product. If our goal is to compute the entire Jacobian matrix, forwardprop computes one column at a time while backprop computes one row at a time. Since the Jacobian in the linear regression example has only one row, backprop requires fewer invocations:", "Implicit in the tape.gradient call is a length-one vector which left-multiplies the Jacobian, a vector-Jacobian product.", "ForwardAccumulator maintains JVPs corresponding primal tensors it is watching, derived from the original primals specified in the constructor. As soon as a primal tensor is deleted, ForwardAccumulator deletes the corresponding JVP.", "acc.jvp(x) retrieves acc's JVP corresponding to the primal tensor x. It does not perform any computation. acc.jvp calls can be repeated as long as acc is accessible, whether the context manager is active or not. New JVPs are only computed while the context manager is active.", "Note that ForwardAccumulators are always applied in the order their context managers were entered, so inner accumulators will not see JVP computation from outer accumulators. Take higher-order JVPs from outer accumulators:", "Reversing the collection in the last line to instead retrieve inner.jvp(outer.jvp(primal_out)) will not work.", "Strict nesting also applies to combinations of ForwardAccumulator and tf.GradientTape. More deeply nested GradientTape objects will ignore the products of outer ForwardAccumulator objects. This allows (for example) memory-efficient forward-over-backward computation of Hessian-vector products, where the inner GradientTape would otherwise hold on to all intermediate JVPs:", "View source", "Fetches the Jacobian-vector product computed for primals.", "Note that this method performs no computation, and simply looks up a JVP that was already computed (unlike backprop using a tf.GradientTape, where the computation happens on the call to tape.gradient).", "View source", "View source"]}, {"name": "tf.autograph", "path": "autograph", "type": "tf.autograph", "text": ["Conversion of plain Python into TensorFlow graph code.", "For more information, see the AutoGraph guide.", "By equivalent graph code we mean code that generates a TensorFlow graph when run. The generated graph has the same effects as the original code when executed (for example with tf.function or tf.compat.v1.Session.run). In other words, using AutoGraph can be thought of as running Python in TensorFlow.", "experimental module: Public API for tf.autograph.experimental namespace.", "set_verbosity(...): Sets the AutoGraph verbosity level.", "to_code(...): Returns the source code generated by AutoGraph, as a string.", "to_graph(...): Converts a Python entity into a TensorFlow graph.", "trace(...): Traces argument information at compilation time."]}, {"name": "tf.autograph.experimental", "path": "autograph/experimental", "type": "tf.autograph", "text": ["Public API for tf.autograph.experimental namespace.", "class Feature: This enumeration represents optional conversion options.", "do_not_convert(...): Decorator that suppresses the conversion of a function.", "set_loop_options(...): Specifies additional arguments to be passed to the enclosing while_loop."]}, {"name": "tf.autograph.experimental.do_not_convert", "path": "autograph/experimental/do_not_convert", "type": "tf.autograph", "text": ["Decorator that suppresses the conversion of a function.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.autograph.experimental.do_not_convert"]}, {"name": "tf.autograph.experimental.Feature", "path": "autograph/experimental/feature", "type": "tf.autograph", "text": ["This enumeration represents optional conversion options.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.autograph.experimental.Feature", "These conversion options are experimental. They are subject to change without notice and offer no guarantees.", "Example Usage"]}, {"name": "tf.autograph.experimental.set_loop_options", "path": "autograph/experimental/set_loop_options", "type": "tf.autograph", "text": ["Specifies additional arguments to be passed to the enclosing while_loop.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.autograph.experimental.set_loop_options", "The parameters apply to and only to the immediately enclosing loop. It only has effect if the loop is staged as a TF while_loop; otherwise the parameters have no effect.", "Also see tf.while_loop."]}, {"name": "tf.autograph.set_verbosity", "path": "autograph/set_verbosity", "type": "tf.autograph", "text": ["Sets the AutoGraph verbosity level.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.autograph.set_verbosity", "Debug logging in AutoGraph", "More verbose logging is useful to enable when filing bug reports or doing more in-depth debugging.", "There are two means to control the logging verbosity:", "The set_verbosity function", "The AUTOGRAPH_VERBOSITY environment variable", "set_verbosity takes precedence over the environment variable.", "Logs entries are output to absl's default output, with INFO level. Logs can be mirrored to stdout by using the alsologtostdout argument. Mirroring is enabled by default when Python runs in interactive mode."]}, {"name": "tf.autograph.to_code", "path": "autograph/to_code", "type": "tf.autograph", "text": ["Returns the source code generated by AutoGraph, as a string.", "Also see: tf.autograph.to_graph."]}, {"name": "tf.autograph.to_graph", "path": "autograph/to_graph", "type": "tf.autograph", "text": ["Converts a Python entity into a TensorFlow graph.", "Also see: tf.autograph.to_code, tf.function.", "Unlike tf.function, to_graph is a low-level transpiler that converts Python code to TensorFlow graph code. It does not implement any caching, variable management or create any actual ops, and is best used where greater control over the generated TensorFlow graph is desired. Another difference from tf.function is that to_graph will not wrap the graph into a TensorFlow function or a Python callable. Internally, tf.function uses to_graph.", "Supported Python entities include:", "Functions are converted into new functions with converted code.", "Classes are converted by generating a new class whose methods use converted code.", "Methods are converted into unbound function that have an additional first argument called self.", "For a tutorial, see the tf.function and AutoGraph guide. For more detailed information, see the AutoGraph reference documentation."]}, {"name": "tf.autograph.trace", "path": "autograph/trace", "type": "tf.autograph", "text": ["Traces argument information at compilation time.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.autograph.trace", "trace is useful when debugging, and it always executes during the tracing phase, that is, when the TF graph is constructed.", "Example usage"]}, {"name": "tf.batch_to_space", "path": "batch_to_space", "type": "tf", "text": ["BatchToSpace for N-D tensors of type T.", "This operation reshapes the \"batch\" dimension 0 into M + 1 dimensions of shape block_shape + [batch], interleaves these blocks back into the grid defined by the spatial dimensions [1, ..., M], to obtain a result with the same rank as the input. The spatial dimensions of this intermediate result are then optionally cropped according to crops to produce the output. This is the reverse of SpaceToBatch (see tf.space_to_batch).", "(1) For the following input of shape [4, 1, 1, 1], block_shape = [2, 2], and crops = [[0, 0], [0, 0]]:", "The output tensor has shape [1, 2, 2, 1] and value:", "(2) For the following input of shape [4, 1, 1, 3], block_shape = [2, 2], and crops = [[0, 0], [0, 0]]:", "The output tensor has shape [1, 2, 2, 3] and value:"]}, {"name": "tf.bitcast", "path": "bitcast", "type": "tf", "text": ["Bitcasts a tensor from one type to another without copying data.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.bitcast", "Given a tensor input, this operation returns a tensor that has the same buffer data as input with datatype type.", "If the input datatype T is larger than the output datatype type then the shape changes from [...] to [..., sizeof(T)/sizeof(type)].", "If T is smaller than type, the operator requires that the rightmost dimension be equal to sizeof(type)/sizeof(T). The shape then goes from [..., sizeof(type)/sizeof(T)] to [...].", "tf.bitcast() and tf.cast() work differently when real dtype is casted as a complex dtype (e.g. tf.complex64 or tf.complex128) as tf.cast() make imaginary part 0 while tf.bitcast() gives module error. For example,"]}, {"name": "tf.bitwise", "path": "bitwise", "type": "tf.bitwise", "text": ["Operations for manipulating the binary representations of integers.", "bitwise_and(...): Elementwise computes the bitwise AND of x and y.", "bitwise_or(...): Elementwise computes the bitwise OR of x and y.", "bitwise_xor(...): Elementwise computes the bitwise XOR of x and y.", "invert(...): Invert (flip) each bit of supported types; for example, type uint8 value 01010101 becomes 10101010.", "left_shift(...): Elementwise computes the bitwise left-shift of x and y.", "right_shift(...): Elementwise computes the bitwise right-shift of x and y."]}, {"name": "tf.bitwise.bitwise_and", "path": "bitwise/bitwise_and", "type": "tf.bitwise", "text": ["Elementwise computes the bitwise AND of x and y.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.bitwise.bitwise_and", "The result will have those bits set, that are set in both x and y. The computation is performed on the underlying representations of x and y."]}, {"name": "tf.bitwise.bitwise_or", "path": "bitwise/bitwise_or", "type": "tf.bitwise", "text": ["Elementwise computes the bitwise OR of x and y.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.bitwise.bitwise_or", "The result will have those bits set, that are set in x, y or both. The computation is performed on the underlying representations of x and y."]}, {"name": "tf.bitwise.bitwise_xor", "path": "bitwise/bitwise_xor", "type": "tf.bitwise", "text": ["Elementwise computes the bitwise XOR of x and y.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.bitwise.bitwise_xor", "The result will have those bits set, that are different in x and y. The computation is performed on the underlying representations of x and y."]}, {"name": "tf.bitwise.invert", "path": "bitwise/invert", "type": "tf.bitwise", "text": ["Invert (flip) each bit of supported types; for example, type uint8 value 01010101 becomes 10101010.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.bitwise.invert", "Flip each bit of supported types. For example, type int8 (decimal 2) binary 00000010 becomes (decimal -3) binary 11111101. This operation is performed on each element of the tensor argument x."]}, {"name": "tf.bitwise.left_shift", "path": "bitwise/left_shift", "type": "tf.bitwise", "text": ["Elementwise computes the bitwise left-shift of x and y.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.bitwise.left_shift", "If y is negative, or greater than or equal to the width of x in bits the result is implementation defined."]}, {"name": "tf.bitwise.right_shift", "path": "bitwise/right_shift", "type": "tf.bitwise", "text": ["Elementwise computes the bitwise right-shift of x and y.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.bitwise.right_shift", "Performs a logical shift for unsigned integer types, and an arithmetic shift for signed integer types.", "If y is negative, or greater than or equal to than the width of x in bits the result is implementation defined."]}, {"name": "tf.boolean_mask", "path": "boolean_mask", "type": "tf", "text": ["Apply boolean mask to tensor.", "Numpy equivalent is tensor[mask].", "In general, 0 < dim(mask) = K <= dim(tensor), and mask's shape must match the first K dimensions of tensor's shape. We then have: boolean_mask(tensor, mask)[i, j1,...,jd] = tensor[i1,...,iK,j1,...,jd] where (i1,...,iK) is the ith True entry of mask (row-major order). The axis could be used with mask to indicate the axis to mask from. In that case, axis + dim(mask) <= dim(tensor) and mask's shape must match the first axis + dim(mask) dimensions of tensor's shape.", "See also: tf.ragged.boolean_mask, which can be applied to both dense and ragged tensors, and can be used if you need to preserve the masked dimensions of tensor (rather than flattening them, as tf.boolean_mask does)."]}, {"name": "tf.broadcast_dynamic_shape", "path": "broadcast_dynamic_shape", "type": "tf", "text": ["Computes the shape of a broadcast given symbolic shapes.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.broadcast_dynamic_shape", "When shape_x and shape_y are Tensors representing shapes (i.e. the result of calling tf.shape on another Tensor) this computes a Tensor which is the shape of the result of a broadcasting op applied in tensors of shapes shape_x and shape_y.", "This is useful when validating the result of a broadcasting operation when the tensors do not have statically known shapes."]}, {"name": "tf.broadcast_static_shape", "path": "broadcast_static_shape", "type": "tf", "text": ["Computes the shape of a broadcast given known shapes.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.broadcast_static_shape", "When shape_x and shape_y are fully known TensorShapes this computes a TensorShape which is the shape of the result of a broadcasting op applied in tensors of shapes shape_x and shape_y.", "For example, if shape_x is TensorShape([1, 2, 3]) and shape_y is TensorShape([5, 1, 3]), the result is a TensorShape whose value is TensorShape([5, 2, 3]).", "This is useful when validating the result of a broadcasting operation when the tensors have statically known shapes."]}, {"name": "tf.broadcast_to", "path": "broadcast_to", "type": "tf", "text": ["Broadcast an array for a compatible shape.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.broadcast_to", "Broadcasting is the process of making arrays to have compatible shapes for arithmetic operations. Two shapes are compatible if for each dimension pair they are either equal or one of them is one. When trying to broadcast a Tensor to a shape, it starts with the trailing dimensions, and works its way forward.", "For example,", "In the above example, the input Tensor with the shape of [1, 3] is broadcasted to output Tensor with shape of [3, 3].", "When doing broadcasted operations such as multiplying a tensor by a scalar, broadcasting (usually) confers some time or space benefit, as the broadcasted tensor is never materialized.", "However, broadcast_to does not carry with it any such benefits. The newly-created tensor takes the full memory of the broadcasted shape. (In a graph context, broadcast_to might be fused to subsequent operation and then be optimized away, however.)"]}, {"name": "tf.case", "path": "case", "type": "tf", "text": ["Create a case operation.", "See also tf.switch_case.", "The pred_fn_pairs parameter is a list of pairs of size N. Each pair contains a boolean scalar tensor and a python callable that creates the tensors to be returned if the boolean evaluates to True. default is a callable generating a list of tensors. All the callables in pred_fn_pairs as well as default (if provided) should return the same number and types of tensors.", "If exclusive==True, all predicates are evaluated, and an exception is thrown if more than one of the predicates evaluates to True. If exclusive==False, execution stops at the first predicate which evaluates to True, and the tensors generated by the corresponding function are returned immediately. If none of the predicates evaluate to True, this operation returns the tensors generated by default.", "tf.case supports nested structures as implemented in tf.contrib.framework.nest. All of the callables must return the same (possibly nested) value structure of lists, tuples, and/or named tuples. Singleton lists and tuples form the only exceptions to this: when returned by a callable, they are implicitly unpacked to single values. This behavior is disabled by passing strict=True.", "Example 1:", "Example 2:", "pred_fn_pairs could be a dictionary in v1. However, tf.Tensor and tf.Variable are no longer hashable in v2, so cannot be used as a key for a dictionary. Please use a list or a tuple instead."]}, {"name": "tf.cast", "path": "cast", "type": "tf", "text": ["Casts a tensor to a new type.", " Main aliases ", "tf.dtypes.cast", "See Migration guide for more details.", "tf.compat.v1.cast, tf.compat.v1.dtypes.cast", "The operation casts x (in case of Tensor) or x.values (in case of SparseTensor or IndexedSlices) to dtype.", "The operation supports data types (for x and dtype) of uint8, uint16, uint32, uint64, int8, int16, int32, int64, float16, float32, float64, complex64, complex128, bfloat16. In case of casting from complex types (complex64, complex128) to real types, only the real part of x is returned. In case of casting from real types to complex types (complex64, complex128), the imaginary part of the returned value is set to 0. The handling of complex types here matches the behavior of numpy.", "Note casting nan and inf values to integral types has undefined behavior."]}, {"name": "tf.clip_by_global_norm", "path": "clip_by_global_norm", "type": "tf", "text": ["Clips values of multiple tensors by the ratio of the sum of their norms.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.clip_by_global_norm", "Given a tuple or list of tensors t_list, and a clipping ratio clip_norm, this operation returns a list of clipped tensors list_clipped and the global norm (global_norm) of all tensors in t_list. Optionally, if you've already computed the global norm for t_list, you can specify the global norm with use_norm.", "To perform the clipping, the values t_list[i] are set to:", "where:", "If clip_norm > global_norm then the entries in t_list remain as they are, otherwise they're all shrunk by the global ratio.", "If global_norm == infinity then the entries in t_list are all set to NaN to signal that an error occurred.", "Any of the entries of t_list that are of type None are ignored.", "This is the correct way to perform gradient clipping (Pascanu et al., 2012).", "However, it is slower than clip_by_norm() because all the parameters must be ready before the clipping operation can be performed.", "On the difficulty of training Recurrent Neural Networks: Pascanu et al., 2012 (pdf)"]}, {"name": "tf.clip_by_norm", "path": "clip_by_norm", "type": "tf", "text": ["Clips tensor values to a maximum L2-norm.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.clip_by_norm", "Given a tensor t, and a maximum clip value clip_norm, this operation normalizes t so that its L2-norm is less than or equal to clip_norm, along the dimensions given in axes. Specifically, in the default case where all dimensions are used for calculation, if the L2-norm of t is already less than or equal to clip_norm, then t is not modified. If the L2-norm is greater than clip_norm, then this operation returns a tensor of the same type and shape as t with its values set to:", "t * clip_norm / l2norm(t)", "In this case, the L2-norm of the output tensor is clip_norm.", "As another example, if t is a matrix and axes == [1], then each row of the output will have L2-norm less than or equal to clip_norm. If axes == [0] instead, each column of the output will be clipped.", "This operation is typically used to clip gradients before applying them with an optimizer. Most gradient data is a collection of different shaped tensors for different parts of the model. Thus, this is a common usage:"]}, {"name": "tf.clip_by_value", "path": "clip_by_value", "type": "tf", "text": ["Clips tensor values to a specified min and max.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.clip_by_value", "Given a tensor t, this operation returns a tensor of the same type and shape as t with its values clipped to clip_value_min and clip_value_max. Any values less than clip_value_min are set to clip_value_min. Any values greater than clip_value_max are set to clip_value_max.", "Basic usage passes a scalar as the min and max value.", "The min and max can be the same size as t, or broadcastable to that size.", "Broadcasting fails, intentionally, if you would expand the dimensions of t", "It throws a TypeError if you try to clip an int to a float value (tf.cast the input to float first)."]}, {"name": "tf.compat", "path": "compat", "type": "tf.compat", "text": ["Compatibility functions.", "The tf.compat module contains two sets of compatibility functions.", "The compat.v1 and compat.v2 submodules provide a complete copy of both the v1 and v2 APIs for backwards and forwards compatibility across TensorFlow versions 1.x and 2.x. See the migration guide for details.", "Aside from the compat.v1 and compat.v2 submodules, tf.compat also contains a set of helper functions for writing code that works in both:", "The compatibility module also provides the following aliases for common sets of python types:", "v1 module: Bring in all of the public TensorFlow interface into this module.", "as_bytes(...): Converts bytearray, bytes, or unicode python input types to bytes.", "as_str(...)", "as_str_any(...): Converts input to str type.", "as_text(...): Converts any string-like python input types to unicode.", "dimension_at_index(...): Compatibility utility required to allow for both V1 and V2 behavior in TF.", "dimension_value(...): Compatibility utility required to allow for both V1 and V2 behavior in TF.", "forward_compatibility_horizon(...): Context manager for testing forward compatibility of generated graphs.", "forward_compatible(...): Return true if the forward compatibility window has expired.", "path_to_str(...): Converts input which is a PathLike object to str type."]}, {"name": "tf.compat.as_bytes", "path": "compat/as_bytes", "type": "tf.compat", "text": ["Converts bytearray, bytes, or unicode python input types to bytes.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.compat.as_bytes", "Uses utf-8 encoding for text by default."]}, {"name": "tf.compat.as_str", "path": "compat/as_str", "type": "tf.compat", "text": [" Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.compat.as_str"]}, {"name": "tf.compat.as_str_any", "path": "compat/as_str_any", "type": "tf.compat", "text": ["Converts input to str type.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.compat.as_str_any", "Uses str(value), except for bytes typed inputs, which are converted using as_str."]}, {"name": "tf.compat.as_text", "path": "compat/as_text", "type": "tf.compat", "text": ["Converts any string-like python input types to unicode.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.compat.as_text", "Returns the input as a unicode string. Uses utf-8 encoding for text by default."]}, {"name": "tf.compat.dimension_at_index", "path": "compat/dimension_at_index", "type": "tf.compat", "text": ["Compatibility utility required to allow for both V1 and V2 behavior in TF.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.compat.dimension_at_index, tf.compat.v1.dimension_at_index", "Until the release of TF 2.0, we need the legacy behavior of TensorShape to coexist with the new behavior. This utility is a bridge between the two.", "If you want to retrieve the Dimension instance corresponding to a certain index in a TensorShape instance, use this utility, like this:"]}, {"name": "tf.compat.dimension_value", "path": "compat/dimension_value", "type": "tf.compat", "text": ["Compatibility utility required to allow for both V1 and V2 behavior in TF.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.compat.dimension_value, tf.compat.v1.dimension_value", "Until the release of TF 2.0, we need the legacy behavior of TensorShape to coexist with the new behavior. This utility is a bridge between the two.", "When accessing the value of a TensorShape dimension, use this utility, like this:"]}, {"name": "tf.compat.forward_compatibility_horizon", "path": "compat/forward_compatibility_horizon", "type": "tf.compat", "text": ["Context manager for testing forward compatibility of generated graphs.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.compat.forward_compatibility_horizon", "See Version compatibility.", "To ensure forward compatibility of generated graphs (see forward_compatible) with older binaries, new features can be gated with:", "However, when adding new features, one may want to unittest it before the forward compatibility window expires. This context manager enables such tests. For example:"]}, {"name": "tf.compat.forward_compatible", "path": "compat/forward_compatible", "type": "tf.compat", "text": ["Return true if the forward compatibility window has expired.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.compat.forward_compatible", "See Version compatibility.", "Forward-compatibility refers to scenarios where the producer of a TensorFlow model (a GraphDef or SavedModel) is compiled against a version of the TensorFlow library newer than what the consumer was compiled against. The \"producer\" is typically a Python program that constructs and trains a model while the \"consumer\" is typically another program that loads and serves the model.", "TensorFlow has been supporting a 3 week forward-compatibility window for programs compiled from source at HEAD.", "For example, consider the case where a new operation MyNewAwesomeAdd is created with the intent of replacing the implementation of an existing Python wrapper - tf.add. The Python wrapper implementation should change from something like:", "to:", "Where year, month, and day specify the date beyond which binaries that consume a model are expected to have been updated to include the new operations. This date is typically at least 3 weeks beyond the date the code that adds the new operation is committed."]}, {"name": "tf.compat.path_to_str", "path": "compat/path_to_str", "type": "tf.compat", "text": [" ", "Converts input which is a PathLike object to str type.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.compat.path_to_str", "Converts from any python constant representation of a PathLike object to a string. If the input is not a PathLike object, simply returns the input.", "In case a simplified str version of the path is needed from an os.PathLike object"]}, {"name": "tf.compat.v1", "path": "compat/v1", "type": "tf.compat", "text": [" ", "Bring in all of the public TensorFlow interface into this module.", "app module: Generic entry point script.", "audio module: Public API for tf.audio namespace.", "autograph module: Conversion of plain Python into TensorFlow graph code.", "bitwise module: Operations for manipulating the binary representations of integers.", "compat module: Compatibility functions.", "config module: Public API for tf.config namespace.", "data module: tf.data.Dataset API for input pipelines.", "debugging module: Public API for tf.debugging namespace.", "distribute module: Library for running a computation across multiple devices.", "distributions module: Core module for TensorFlow distribution objects and helpers.", "dtypes module: Public API for tf.dtypes namespace.", "errors module: Exception types for TensorFlow errors.", "estimator module: Estimator: High level tools for working with models.", "experimental module: Public API for tf.experimental namespace.", "feature_column module: Public API for tf.feature_column namespace.", "flags module: Import router for absl.flags. See https://github.com/abseil/abseil-py", "gfile module: Import router for file_io.", "graph_util module: Helpers to manipulate a tensor graph in python.", "image module: Image ops.", "initializers module: Public API for tf.initializers namespace.", "io module: Public API for tf.io namespace.", "keras module: Implementation of the Keras API meant to be a high-level API for TensorFlow.", "layers module: Public API for tf.layers namespace.", "linalg module: Operations for linear algebra.", "lite module: Public API for tf.lite namespace.", "logging module: Logging and Summary Operations.", "lookup module: Public API for tf.lookup namespace.", "losses module: Loss operations for use in neural networks.", "manip module: Operators for manipulating tensors.", "math module: Math Operations.", "metrics module: Evaluation-related metrics.", "mixed_precision module: Public API for tf.mixed_precision namespace.", "mlir module: Public API for tf.mlir namespace.", "nest module: Public API for tf.nest namespace.", "nn module: Wrappers for primitive Neural Net (NN) Operations.", "profiler module: Public API for tf.profiler namespace.", "python_io module: Python functions for directly manipulating TFRecord-formatted files.", "quantization module: Public API for tf.quantization namespace.", "queue module: Public API for tf.queue namespace.", "ragged module: Ragged Tensors.", "random module: Public API for tf.random namespace.", "raw_ops module: Public API for tf.raw_ops namespace.", "resource_loader module: Resource management library.", "saved_model module: Public API for tf.saved_model namespace.", "sets module: Tensorflow set operations.", "signal module: Signal processing operations.", "sparse module: Sparse Tensor Representation.", "spectral module: Public API for tf.spectral namespace.", "strings module: Operations for working with string Tensors.", "summary module: Operations for writing summary data, for use in analysis and visualization.", "sysconfig module: System configuration library.", "test module: Testing.", "tpu module: Ops related to Tensor Processing Units.", "train module: Support for training models.", "types module: Public TensorFlow type definitions.", "user_ops module: Public API for tf.user_ops namespace.", "version module: Public API for tf.version namespace.", "xla module: Public API for tf.xla namespace.", "class AggregationMethod: A class listing aggregation methods used to combine gradients.", "class AttrValue: A ProtocolMessage", "class ConditionalAccumulator: A conditional accumulator for aggregating gradients.", "class ConditionalAccumulatorBase: A conditional accumulator for aggregating gradients.", "class ConfigProto: A ProtocolMessage", "class CriticalSection: Critical section.", "class DType: Represents the type of the elements in a Tensor.", "class DeviceSpec: Represents a (possibly partial) specification for a TensorFlow device.", "class Dimension: Represents the value of one dimension in a TensorShape.", "class Event: A ProtocolMessage", "class FIFOQueue: A queue implementation that dequeues elements in first-in first-out order.", "class FixedLenFeature: Configuration for parsing a fixed-length input feature.", "class FixedLenSequenceFeature: Configuration for parsing a variable-length input feature into a Tensor.", "class FixedLengthRecordReader: A Reader that outputs fixed-length records from a file.", "class GPUOptions: A ProtocolMessage", "class GradientTape: Record operations for automatic differentiation.", "class Graph: A TensorFlow computation, represented as a dataflow graph.", "class GraphDef: A ProtocolMessage", "class GraphKeys: Standard names to use for graph collections.", "class GraphOptions: A ProtocolMessage", "class HistogramProto: A ProtocolMessage", "class IdentityReader: A Reader that outputs the queued work as both the key and value.", "class IndexedSlices: A sparse representation of a set of tensor slices at given indices.", "class IndexedSlicesSpec: Type specification for a tf.IndexedSlices.", "class InteractiveSession: A TensorFlow Session for use in interactive contexts, such as a shell.", "class LMDBReader: A Reader that outputs the records from a LMDB file.", "class LogMessage: A ProtocolMessage", "class MetaGraphDef: A ProtocolMessage", "class Module: Base neural network module class.", "class NameAttrList: A ProtocolMessage", "class NodeDef: A ProtocolMessage", "class OpError: A generic error that is raised when TensorFlow execution fails.", "class Operation: Represents a graph node that performs computation on tensors.", "class OptimizerOptions: A ProtocolMessage", "class OptionalSpec: Type specification for tf.experimental.Optional.", "class PaddingFIFOQueue: A FIFOQueue that supports batching variable-sized tensors by padding.", "class PriorityQueue: A queue implementation that dequeues elements in prioritized order.", "class QueueBase: Base class for queue implementations.", "class RaggedTensor: Represents a ragged tensor.", "class RaggedTensorSpec: Type specification for a tf.RaggedTensor.", "class RandomShuffleQueue: A queue implementation that dequeues elements in a random order.", "class ReaderBase: Base class for different Reader types, that produce a record every step.", "class RegisterGradient: A decorator for registering the gradient function for an op type.", "class RunMetadata: A ProtocolMessage", "class RunOptions: A ProtocolMessage", "class Session: A class for running TensorFlow operations.", "class SessionLog: A ProtocolMessage", "class SparseConditionalAccumulator: A conditional accumulator for aggregating sparse gradients.", "class SparseFeature: Configuration for parsing a sparse input feature from an Example.", "class SparseTensor: Represents a sparse tensor.", "class SparseTensorSpec: Type specification for a tf.sparse.SparseTensor.", "class SparseTensorValue: SparseTensorValue(indices, values, dense_shape)", "class Summary: A ProtocolMessage", "class SummaryMetadata: A ProtocolMessage", "class TFRecordReader: A Reader that outputs the records from a TFRecords file.", "class Tensor: A tensor is a multidimensional array of elements represented by a", "class TensorArray: Class wrapping dynamic-sized, per-time-step, write-once Tensor arrays.", "class TensorArraySpec: Type specification for a tf.TensorArray.", "class TensorInfo: A ProtocolMessage", "class TensorShape: Represents the shape of a Tensor.", "class TensorSpec: Describes a tf.Tensor.", "class TextLineReader: A Reader that outputs the lines of a file delimited by newlines.", "class TypeSpec: Specifies a TensorFlow value type.", "class UnconnectedGradients: Controls how gradient computation behaves when y does not depend on x.", "class VarLenFeature: Configuration for parsing a variable-length input feature.", "class Variable: See the Variables Guide.", "class VariableAggregation: Indicates how a distributed variable will be aggregated.", "class VariableScope: Variable scope object to carry defaults to provide to get_variable.", "class VariableSynchronization: Indicates when a distributed variable will be synced.", "class WholeFileReader: A Reader that outputs the entire contents of a file as a value.", "class constant_initializer: Initializer that generates tensors with constant values.", "class glorot_normal_initializer: The Glorot normal initializer, also called Xavier normal initializer.", "class glorot_uniform_initializer: The Glorot uniform initializer, also called Xavier uniform initializer.", "class name_scope: A context manager for use when defining a Python op.", "class ones_initializer: Initializer that generates tensors initialized to 1.", "class orthogonal_initializer: Initializer that generates an orthogonal matrix.", "class random_normal_initializer: Initializer that generates tensors with a normal distribution.", "class random_uniform_initializer: Initializer that generates tensors with a uniform distribution.", "class truncated_normal_initializer: Initializer that generates a truncated normal distribution.", "class uniform_unit_scaling_initializer: Initializer that generates tensors without scaling variance.", "class variable_scope: A context manager for defining ops that creates variables (layers).", "class variance_scaling_initializer: Initializer capable of adapting its scale to the shape of weights tensors.", "class zeros_initializer: Initializer that generates tensors initialized to 0.", "Assert(...): Asserts that the given condition is true.", "NoGradient(...): Specifies that ops of type op_type is not differentiable.", "NotDifferentiable(...): Specifies that ops of type op_type is not differentiable.", "Print(...): Prints a list of tensors. (deprecated)", "abs(...): Computes the absolute value of a tensor.", "accumulate_n(...): Returns the element-wise sum of a list of tensors.", "acos(...): Computes acos of x element-wise.", "acosh(...): Computes inverse hyperbolic cosine of x element-wise.", "add(...): Returns x + y element-wise.", "add_check_numerics_ops(...): Connect a tf.debugging.check_numerics to every floating point tensor.", "add_n(...): Adds all input tensors element-wise.", "add_to_collection(...): Wrapper for Graph.add_to_collection() using the default graph.", "add_to_collections(...): Wrapper for Graph.add_to_collections() using the default graph.", "all_variables(...): Use tf.compat.v1.global_variables instead. (deprecated)", "angle(...): Returns the element-wise argument of a complex (or real) tensor.", "arg_max(...): Returns the index with the largest value across dimensions of a tensor.", "arg_min(...): Returns the index with the smallest value across dimensions of a tensor.", "argmax(...): Returns the index with the largest value across axes of a tensor. (deprecated arguments)", "argmin(...): Returns the index with the smallest value across axes of a tensor. (deprecated arguments)", "argsort(...): Returns the indices of a tensor that give its sorted order along an axis.", "as_dtype(...): Converts the given type_value to a DType.", "as_string(...): Converts each entry in the given tensor to strings.", "asin(...): Computes the trignometric inverse sine of x element-wise.", "asinh(...): Computes inverse hyperbolic sine of x element-wise.", "assert_equal(...): Assert the condition x == y holds element-wise.", "assert_greater(...): Assert the condition x > y holds element-wise.", "assert_greater_equal(...): Assert the condition x >= y holds element-wise.", "assert_integer(...): Assert that x is of integer dtype.", "assert_less(...): Assert the condition x < y holds element-wise.", "assert_less_equal(...): Assert the condition x <= y holds element-wise.", "assert_near(...): Assert the condition x and y are close element-wise.", "assert_negative(...): Assert the condition x < 0 holds element-wise.", "assert_non_negative(...): Assert the condition x >= 0 holds element-wise.", "assert_non_positive(...): Assert the condition x <= 0 holds element-wise.", "assert_none_equal(...): Assert the condition x != y holds element-wise.", "assert_positive(...): Assert the condition x > 0 holds element-wise.", "assert_proper_iterable(...): Static assert that values is a \"proper\" iterable.", "assert_rank(...): Assert x has rank equal to rank.", "assert_rank_at_least(...): Assert x has rank equal to rank or higher.", "assert_rank_in(...): Assert x has rank in ranks.", "assert_same_float_dtype(...): Validate and return float type based on tensors and dtype.", "assert_scalar(...): Asserts that the given tensor is a scalar (i.e. zero-dimensional).", "assert_type(...): Statically asserts that the given Tensor is of the specified type.", "assert_variables_initialized(...): Returns an Op to check if variables are initialized.", "assign(...): Update ref by assigning value to it.", "assign_add(...): Update ref by adding value to it.", "assign_sub(...): Update ref by subtracting value from it.", "atan(...): Computes the trignometric inverse tangent of x element-wise.", "atan2(...): Computes arctangent of y/x element-wise, respecting signs of the arguments.", "atanh(...): Computes inverse hyperbolic tangent of x element-wise.", "batch_gather(...): Gather slices from params according to indices with leading batch dims. (deprecated)", "batch_scatter_update(...): Generalization of tf.compat.v1.scatter_update to axis different than 0. (deprecated)", "batch_to_space(...): BatchToSpace for 4-D tensors of type T.", "batch_to_space_nd(...): BatchToSpace for N-D tensors of type T.", "betainc(...): Compute the regularized incomplete beta integral \\(I_x(a, b)\\).", "bincount(...): Counts the number of occurrences of each value in an integer array.", "bitcast(...): Bitcasts a tensor from one type to another without copying data.", "boolean_mask(...): Apply boolean mask to tensor.", "broadcast_dynamic_shape(...): Computes the shape of a broadcast given symbolic shapes.", "broadcast_static_shape(...): Computes the shape of a broadcast given known shapes.", "broadcast_to(...): Broadcast an array for a compatible shape.", "case(...): Create a case operation.", "cast(...): Casts a tensor to a new type.", "ceil(...): Return the ceiling of the input, element-wise.", "check_numerics(...): Checks a tensor for NaN and Inf values.", "cholesky(...): Computes the Cholesky decomposition of one or more square matrices.", "cholesky_solve(...): Solves systems of linear eqns A X = RHS, given Cholesky factorizations.", "clip_by_average_norm(...): Clips tensor values to a maximum average L2-norm. (deprecated)", "clip_by_global_norm(...): Clips values of multiple tensors by the ratio of the sum of their norms.", "clip_by_norm(...): Clips tensor values to a maximum L2-norm.", "clip_by_value(...): Clips tensor values to a specified min and max.", "colocate_with(...): DEPRECATED FUNCTION", "complex(...): Converts two real numbers to a complex number.", "concat(...): Concatenates tensors along one dimension.", "cond(...): Return true_fn() if the predicate pred is true else false_fn(). (deprecated arguments)", "confusion_matrix(...): Computes the confusion matrix from predictions and labels.", "conj(...): Returns the complex conjugate of a complex number.", "constant(...): Creates a constant tensor.", "container(...): Wrapper for Graph.container() using the default graph.", "control_dependencies(...): Wrapper for Graph.control_dependencies() using the default graph.", "control_flow_v2_enabled(...): Returns True if v2 control flow is enabled.", "convert_to_tensor(...): Converts the given value to a Tensor.", "convert_to_tensor_or_indexed_slices(...): Converts the given object to a Tensor or an IndexedSlices.", "convert_to_tensor_or_sparse_tensor(...): Converts value to a SparseTensor or Tensor.", "cos(...): Computes cos of x element-wise.", "cosh(...): Computes hyperbolic cosine of x element-wise.", "count_nonzero(...): Computes number of nonzero elements across dimensions of a tensor. (deprecated arguments) (deprecated arguments)", "count_up_to(...): Increments 'ref' until it reaches 'limit'. (deprecated)", "create_partitioned_variables(...): Create a list of partitioned variables according to the given slicing. (deprecated)", "cross(...): Compute the pairwise cross product.", "cumprod(...): Compute the cumulative product of the tensor x along axis.", "cumsum(...): Compute the cumulative sum of the tensor x along axis.", "custom_gradient(...): Decorator to define a function with a custom gradient.", "decode_base64(...): Decode web-safe base64-encoded strings.", "decode_compressed(...): Decompress strings.", "decode_csv(...): Convert CSV records to tensors. Each column maps to one tensor.", "decode_json_example(...): Convert JSON-encoded Example records to binary protocol buffer strings.", "decode_raw(...): Convert raw byte strings into tensors. (deprecated arguments)", "delete_session_tensor(...): Delete the tensor for the given tensor handle.", "depth_to_space(...): DepthToSpace for tensors of type T.", "dequantize(...): Dequantize the 'input' tensor into a float or bfloat16 Tensor.", "deserialize_many_sparse(...): Deserialize and concatenate SparseTensors from a serialized minibatch.", "device(...): Wrapper for Graph.device() using the default graph.", "diag(...): Returns a diagonal tensor with a given diagonal values.", "diag_part(...): Returns the diagonal part of the tensor.", "digamma(...): Computes Psi, the derivative of Lgamma (the log of the absolute value of", "dimension_at_index(...): Compatibility utility required to allow for both V1 and V2 behavior in TF.", "dimension_value(...): Compatibility utility required to allow for both V1 and V2 behavior in TF.", "disable_control_flow_v2(...): Opts out of control flow v2.", "disable_eager_execution(...): Disables eager execution.", "disable_resource_variables(...): Opts out of resource variables. (deprecated)", "disable_tensor_equality(...): Compare Tensors by their id and be hashable.", "disable_v2_behavior(...): Disables TensorFlow 2.x behaviors.", "disable_v2_tensorshape(...): Disables the V2 TensorShape behavior and reverts to V1 behavior.", "div(...): Divides x / y elementwise (using Python 2 division operator semantics). (deprecated)", "div_no_nan(...): Computes a safe divide which returns 0 if the y is zero.", "divide(...): Computes Python style division of x by y.", "dynamic_partition(...): Partitions data into num_partitions tensors using indices from partitions.", "dynamic_stitch(...): Interleave the values from the data tensors into a single tensor.", "edit_distance(...): Computes the Levenshtein distance between sequences.", "einsum(...): Tensor contraction over specified indices and outer product.", "enable_control_flow_v2(...): Use control flow v2.", "enable_eager_execution(...): Enables eager execution for the lifetime of this program.", "enable_resource_variables(...): Creates resource variables by default.", "enable_tensor_equality(...): Compare Tensors with element-wise comparison and thus be unhashable.", "enable_v2_behavior(...): Enables TensorFlow 2.x behaviors.", "enable_v2_tensorshape(...): In TensorFlow 2.0, iterating over a TensorShape instance returns values.", "encode_base64(...): Encode strings into web-safe base64 format.", "ensure_shape(...): Updates the shape of a tensor and checks at runtime that the shape holds.", "equal(...): Returns the truth value of (x == y) element-wise.", "erf(...): Computes the Gauss error function of x element-wise.", "erfc(...): Computes the complementary error function of x element-wise.", "executing_eagerly(...): Checks whether the current thread has eager execution enabled.", "executing_eagerly_outside_functions(...): Returns True if executing eagerly, even if inside a graph function.", "exp(...): Computes exponential of x element-wise. \\(y = e^x\\).", "expand_dims(...): Returns a tensor with a length 1 axis inserted at index axis. (deprecated arguments)", "expm1(...): Computes exp(x) - 1 element-wise.", "extract_image_patches(...): Extract patches from images and put them in the \"depth\" output dimension.", "extract_volume_patches(...): Extract patches from input and put them in the \"depth\" output dimension. 3D extension of extract_image_patches.", "eye(...): Construct an identity matrix, or a batch of matrices.", "fake_quant_with_min_max_args(...): Fake-quantize the 'inputs' tensor, type float to 'outputs' tensor of same type.", "fake_quant_with_min_max_args_gradient(...): Compute gradients for a FakeQuantWithMinMaxArgs operation.", "fake_quant_with_min_max_vars(...): Fake-quantize the 'inputs' tensor of type float via global float scalars", "fake_quant_with_min_max_vars_gradient(...): Compute gradients for a FakeQuantWithMinMaxVars operation.", "fake_quant_with_min_max_vars_per_channel(...): Fake-quantize the 'inputs' tensor of type float via per-channel floats", "fake_quant_with_min_max_vars_per_channel_gradient(...): Compute gradients for a FakeQuantWithMinMaxVarsPerChannel operation.", "fft(...): Fast Fourier transform.", "fft2d(...): 2D fast Fourier transform.", "fft3d(...): 3D fast Fourier transform.", "fill(...): Creates a tensor filled with a scalar value.", "fingerprint(...): Generates fingerprint values.", "fixed_size_partitioner(...): Partitioner to specify a fixed number of shards along given axis.", "floor(...): Returns element-wise largest integer not greater than x.", "floor_div(...): Returns x // y element-wise.", "floordiv(...): Divides x / y elementwise, rounding toward the most negative integer.", "floormod(...): Returns element-wise remainder of division. When x < 0 xor y < 0 is", "foldl(...): foldl on the list of tensors unpacked from elems on dimension 0.", "foldr(...): foldr on the list of tensors unpacked from elems on dimension 0.", "function(...): Compiles a function into a callable TensorFlow graph.", "gather(...): Gather slices from params axis axis according to indices.", "gather_nd(...): Gather slices from params into a Tensor with shape specified by indices.", "get_collection(...): Wrapper for Graph.get_collection() using the default graph.", "get_collection_ref(...): Wrapper for Graph.get_collection_ref() using the default graph.", "get_default_graph(...): Returns the default graph for the current thread.", "get_default_session(...): Returns the default session for the current thread.", "get_local_variable(...): Gets an existing local variable or creates a new one.", "get_logger(...): Return TF logger instance.", "get_seed(...): Returns the local seeds an operation should use given an op-specific seed.", "get_session_handle(...): Return the handle of data.", "get_session_tensor(...): Get the tensor of type dtype by feeding a tensor handle.", "get_static_value(...): Returns the constant value of the given tensor, if efficiently calculable.", "get_variable(...): Gets an existing variable with these parameters or create a new one.", "get_variable_scope(...): Returns the current variable scope.", "global_norm(...): Computes the global norm of multiple tensors.", "global_variables(...): Returns global variables.", "global_variables_initializer(...): Returns an Op that initializes global variables.", "grad_pass_through(...): Creates a grad-pass-through op with the forward behavior provided in f.", "gradients(...): Constructs symbolic derivatives of sum of ys w.r.t. x in xs.", "greater(...): Returns the truth value of (x > y) element-wise.", "greater_equal(...): Returns the truth value of (x >= y) element-wise.", "group(...): Create an op that groups multiple operations.", "guarantee_const(...): Gives a guarantee to the TF runtime that the input tensor is a constant.", "hessians(...): Constructs the Hessian of sum of ys with respect to x in xs.", "histogram_fixed_width(...): Return histogram of values.", "histogram_fixed_width_bins(...): Bins the given values for use in a histogram.", "identity(...): Return a Tensor with the same shape and contents as input.", "identity_n(...): Returns a list of tensors with the same shapes and contents as the input", "ifft(...): Inverse fast Fourier transform.", "ifft2d(...): Inverse 2D fast Fourier transform.", "ifft3d(...): Inverse 3D fast Fourier transform.", "igamma(...): Compute the lower regularized incomplete Gamma function P(a, x).", "igammac(...): Compute the upper regularized incomplete Gamma function Q(a, x).", "imag(...): Returns the imaginary part of a complex (or real) tensor.", "import_graph_def(...): Imports the graph from graph_def into the current default Graph. (deprecated arguments)", "init_scope(...): A context manager that lifts ops out of control-flow scopes and function-building graphs.", "initialize_all_tables(...): Returns an Op that initializes all tables of the default graph. (deprecated)", "initialize_all_variables(...): See tf.compat.v1.global_variables_initializer. (deprecated)", "initialize_local_variables(...): See tf.compat.v1.local_variables_initializer. (deprecated)", "initialize_variables(...): See tf.compat.v1.variables_initializer. (deprecated)", "invert_permutation(...): Computes the inverse permutation of a tensor.", "is_finite(...): Returns which elements of x are finite.", "is_inf(...): Returns which elements of x are Inf.", "is_nan(...): Returns which elements of x are NaN.", "is_non_decreasing(...): Returns True if x is non-decreasing.", "is_numeric_tensor(...): Returns True if the elements of tensor are numbers.", "is_strictly_increasing(...): Returns True if x is strictly increasing.", "is_tensor(...): Checks whether x is a TF-native type that can be passed to many TF ops.", "is_variable_initialized(...): Tests if a variable has been initialized.", "lbeta(...): Computes \\(ln(|Beta(x)|)\\), reducing along the last dimension.", "less(...): Returns the truth value of (x < y) element-wise.", "less_equal(...): Returns the truth value of (x <= y) element-wise.", "lgamma(...): Computes the log of the absolute value of Gamma(x) element-wise.", "lin_space(...): Generates evenly-spaced values in an interval along a given axis.", "linspace(...): Generates evenly-spaced values in an interval along a given axis.", "load_file_system_library(...): Loads a TensorFlow plugin, containing file system implementation. (deprecated)", "load_library(...): Loads a TensorFlow plugin.", "load_op_library(...): Loads a TensorFlow plugin, containing custom ops and kernels.", "local_variables(...): Returns local variables.", "local_variables_initializer(...): Returns an Op that initializes all local variables.", "log(...): Computes natural logarithm of x element-wise.", "log1p(...): Computes natural logarithm of (1 + x) element-wise.", "log_sigmoid(...): Computes log sigmoid of x element-wise.", "logical_and(...): Logical AND function.", "logical_not(...): Returns the truth value of NOT x element-wise.", "logical_or(...): Returns the truth value of x OR y element-wise.", "logical_xor(...): Logical XOR function.", "make_ndarray(...): Create a numpy ndarray from a tensor.", "make_template(...): Given an arbitrary function, wrap it so that it does variable sharing.", "make_tensor_proto(...): Create a TensorProto.", "map_fn(...): Transforms elems by applying fn to each element unstacked on axis 0. (deprecated arguments)", "matching_files(...): Returns the set of files matching one or more glob patterns.", "matmul(...): Multiplies matrix a by matrix b, producing a * b.", "matrix_band_part(...): Copy a tensor setting everything outside a central band in each innermost matrix to zero.", "matrix_determinant(...): Computes the determinant of one or more square matrices.", "matrix_diag(...): Returns a batched diagonal tensor with given batched diagonal values.", "matrix_diag_part(...): Returns the batched diagonal part of a batched tensor.", "matrix_inverse(...): Computes the inverse of one or more square invertible matrices or their adjoints (conjugate transposes).", "matrix_set_diag(...): Returns a batched matrix tensor with new batched diagonal values.", "matrix_solve(...): Solves systems of linear equations.", "matrix_solve_ls(...): Solves one or more linear least-squares problems.", "matrix_square_root(...): Computes the matrix square root of one or more square matrices:", "matrix_transpose(...): Transposes last two dimensions of tensor a.", "matrix_triangular_solve(...): Solve systems of linear equations with upper or lower triangular matrices.", "maximum(...): Returns the max of x and y (i.e. x > y ? x : y) element-wise.", "meshgrid(...): Broadcasts parameters for evaluation on an N-D grid.", "min_max_variable_partitioner(...): Partitioner to allocate minimum size per slice.", "minimum(...): Returns the min of x and y (i.e. x < y ? x : y) element-wise.", "mod(...): Returns element-wise remainder of division. When x < 0 xor y < 0 is", "model_variables(...): Returns all variables in the MODEL_VARIABLES collection.", "moving_average_variables(...): Returns all variables that maintain their moving averages.", "multinomial(...): Draws samples from a multinomial distribution. (deprecated)", "multiply(...): Returns an element-wise x * y.", "negative(...): Computes numerical negative value element-wise.", "no_gradient(...): Specifies that ops of type op_type is not differentiable.", "no_op(...): Does nothing. Only useful as a placeholder for control edges.", "no_regularizer(...): Use this function to prevent regularization of variables.", "nondifferentiable_batch_function(...): Batches the computation done by the decorated function.", "norm(...): Computes the norm of vectors, matrices, and tensors. (deprecated arguments)", "not_equal(...): Returns the truth value of (x != y) element-wise.", "numpy_function(...): Wraps a python function and uses it as a TensorFlow op.", "one_hot(...): Returns a one-hot tensor.", "ones(...): Creates a tensor with all elements set to one (1).", "ones_like(...): Creates a tensor with all elements set to 1.", "op_scope(...): DEPRECATED. Same as name_scope above, just different argument order.", "pad(...): Pads a tensor.", "parallel_stack(...): Stacks a list of rank-R tensors into one rank-(R+1) tensor in parallel.", "parse_example(...): Parses Example protos into a dict of tensors.", "parse_single_example(...): Parses a single Example proto.", "parse_single_sequence_example(...): Parses a single SequenceExample proto.", "parse_tensor(...): Transforms a serialized tensorflow.TensorProto proto into a Tensor.", "placeholder(...): Inserts a placeholder for a tensor that will be always fed.", "placeholder_with_default(...): A placeholder op that passes through input when its output is not fed.", "polygamma(...): Compute the polygamma function \\(\\psi^{(n)}(x)\\).", "pow(...): Computes the power of one value to another.", "print(...): Print the specified inputs.", "py_func(...): Wraps a python function and uses it as a TensorFlow op.", "py_function(...): Wraps a python function into a TensorFlow op that executes it eagerly.", "qr(...): Computes the QR decompositions of one or more matrices.", "quantize(...): Quantize the 'input' tensor of type float to 'output' tensor of type 'T'.", "quantize_and_dequantize_v4(...): Returns the gradient of QuantizeAndDequantizeV4.", "quantize_v2(...): Please use tf.quantization.quantize instead.", "quantized_concat(...): Concatenates quantized tensors along one dimension.", "random_crop(...): Randomly crops a tensor to a given size.", "random_gamma(...): Draws shape samples from each of the given Gamma distribution(s).", "random_normal(...): Outputs random values from a normal distribution.", "random_poisson(...): Draws shape samples from each of the given Poisson distribution(s).", "random_shuffle(...): Randomly shuffles a tensor along its first dimension.", "random_uniform(...): Outputs random values from a uniform distribution.", "range(...): Creates a sequence of numbers.", "rank(...): Returns the rank of a tensor.", "read_file(...): Reads and outputs the entire contents of the input filename.", "real(...): Returns the real part of a complex (or real) tensor.", "realdiv(...): Returns x / y element-wise for real types.", "reciprocal(...): Computes the reciprocal of x element-wise.", "recompute_grad(...): An eager-compatible version of recompute_grad.", "reduce_all(...): Computes the \"logical and\" of elements across dimensions of a tensor. (deprecated arguments)", "reduce_any(...): Computes the \"logical or\" of elements across dimensions of a tensor. (deprecated arguments)", "reduce_join(...): Joins all strings into a single string, or joins along an axis.", "reduce_logsumexp(...): Computes log(sum(exp(elements across dimensions of a tensor))). (deprecated arguments)", "reduce_max(...): Computes the maximum of elements across dimensions of a tensor. (deprecated arguments)", "reduce_mean(...): Computes the mean of elements across dimensions of a tensor.", "reduce_min(...): Computes the minimum of elements across dimensions of a tensor. (deprecated arguments)", "reduce_prod(...): Computes the product of elements across dimensions of a tensor. (deprecated arguments)", "reduce_sum(...): Computes the sum of elements across dimensions of a tensor. (deprecated arguments)", "regex_replace(...): Replace elements of input matching regex pattern with rewrite.", "register_tensor_conversion_function(...): Registers a function for converting objects of base_type to Tensor.", "repeat(...): Repeat elements of input.", "report_uninitialized_variables(...): Adds ops to list the names of uninitialized variables.", "required_space_to_batch_paddings(...): Calculate padding required to make block_shape divide input_shape.", "reset_default_graph(...): Clears the default graph stack and resets the global default graph.", "reshape(...): Reshapes a tensor.", "resource_variables_enabled(...): Returns True if resource variables are enabled.", "reverse(...): Reverses specific dimensions of a tensor.", "reverse_sequence(...): Reverses variable length slices. (deprecated arguments) (deprecated arguments)", "reverse_v2(...): Reverses specific dimensions of a tensor.", "rint(...): Returns element-wise integer closest to x.", "roll(...): Rolls the elements of a tensor along an axis.", "round(...): Rounds the values of a tensor to the nearest integer, element-wise.", "rsqrt(...): Computes reciprocal of square root of x element-wise.", "saturate_cast(...): Performs a safe saturating cast of value to dtype.", "scalar_mul(...): Multiplies a scalar times a Tensor or IndexedSlices object.", "scan(...): scan on the list of tensors unpacked from elems on dimension 0.", "scatter_add(...): Adds sparse updates to the variable referenced by resource.", "scatter_div(...): Divides a variable reference by sparse updates.", "scatter_max(...): Reduces sparse updates into a variable reference using the max operation.", "scatter_min(...): Reduces sparse updates into a variable reference using the min operation.", "scatter_mul(...): Multiplies sparse updates into a variable reference.", "scatter_nd(...): Scatter updates into a new tensor according to indices.", "scatter_nd_add(...): Applies sparse addition to individual values or slices in a Variable.", "scatter_nd_sub(...): Applies sparse subtraction to individual values or slices in a Variable.", "scatter_nd_update(...): Applies sparse updates to individual values or slices in a Variable.", "scatter_sub(...): Subtracts sparse updates to a variable reference.", "scatter_update(...): Applies sparse updates to a variable reference.", "searchsorted(...): Searches input tensor for values on the innermost dimension.", "segment_max(...): Computes the maximum along segments of a tensor.", "segment_mean(...): Computes the mean along segments of a tensor.", "segment_min(...): Computes the minimum along segments of a tensor.", "segment_prod(...): Computes the product along segments of a tensor.", "segment_sum(...): Computes the sum along segments of a tensor.", "self_adjoint_eig(...): Computes the eigen decomposition of a batch of self-adjoint matrices.", "self_adjoint_eigvals(...): Computes the eigenvalues of one or more self-adjoint matrices.", "sequence_mask(...): Returns a mask tensor representing the first N positions of each cell.", "serialize_many_sparse(...): Serialize N-minibatch SparseTensor into an [N, 3] Tensor.", "serialize_sparse(...): Serialize a SparseTensor into a 3-vector (1-D Tensor) object.", "serialize_tensor(...): Transforms a Tensor into a serialized TensorProto proto.", "set_random_seed(...): Sets the graph-level random seed for the default graph.", "setdiff1d(...): Computes the difference between two lists of numbers or strings.", "shape(...): Returns the shape of a tensor.", "shape_n(...): Returns shape of tensors.", "sigmoid(...): Computes sigmoid of x element-wise.", "sign(...): Returns an element-wise indication of the sign of a number.", "sin(...): Computes sine of x element-wise.", "sinh(...): Computes hyperbolic sine of x element-wise.", "size(...): Returns the size of a tensor.", "slice(...): Extracts a slice from a tensor.", "sort(...): Sorts a tensor.", "space_to_batch(...): SpaceToBatch for 4-D tensors of type T.", "space_to_batch_nd(...): SpaceToBatch for N-D tensors of type T.", "space_to_depth(...): SpaceToDepth for tensors of type T.", "sparse_add(...): Adds two tensors, at least one of each is a SparseTensor. (deprecated arguments)", "sparse_concat(...): Concatenates a list of SparseTensor along the specified dimension. (deprecated arguments)", "sparse_fill_empty_rows(...): Fills empty rows in the input 2-D SparseTensor with a default value.", "sparse_mask(...): Masks elements of IndexedSlices.", "sparse_matmul(...): Multiply matrix \"a\" by matrix \"b\".", "sparse_maximum(...): Returns the element-wise max of two SparseTensors.", "sparse_merge(...): Combines a batch of feature ids and values into a single SparseTensor. (deprecated)", "sparse_minimum(...): Returns the element-wise min of two SparseTensors.", "sparse_placeholder(...): Inserts a placeholder for a sparse tensor that will be always fed.", "sparse_reduce_max(...): Computes the max of elements across dimensions of a SparseTensor. (deprecated arguments) (deprecated arguments)", "sparse_reduce_max_sparse(...): Computes the max of elements across dimensions of a SparseTensor. (deprecated arguments)", "sparse_reduce_sum(...): Computes the sum of elements across dimensions of a SparseTensor. (deprecated arguments) (deprecated arguments)", "sparse_reduce_sum_sparse(...): Computes the sum of elements across dimensions of a SparseTensor. (deprecated arguments)", "sparse_reorder(...): Reorders a SparseTensor into the canonical, row-major ordering.", "sparse_reset_shape(...): Resets the shape of a SparseTensor with indices and values unchanged.", "sparse_reshape(...): Reshapes a SparseTensor to represent values in a new dense shape.", "sparse_retain(...): Retains specified non-empty values within a SparseTensor.", "sparse_segment_mean(...): Computes the mean along sparse segments of a tensor.", "sparse_segment_sqrt_n(...): Computes the sum along sparse segments of a tensor divided by the sqrt(N).", "sparse_segment_sum(...): Computes the sum along sparse segments of a tensor.", "sparse_slice(...): Slice a SparseTensor based on the start and `size.", "sparse_softmax(...): Applies softmax to a batched N-D SparseTensor.", "sparse_split(...): Split a SparseTensor into num_split tensors along axis. (deprecated arguments)", "sparse_tensor_dense_matmul(...): Multiply SparseTensor (or dense Matrix) (of rank 2) \"A\" by dense matrix", "sparse_tensor_to_dense(...): Converts a SparseTensor into a dense tensor.", "sparse_to_dense(...): Converts a sparse representation into a dense tensor. (deprecated)", "sparse_to_indicator(...): Converts a SparseTensor of ids into a dense bool indicator tensor.", "sparse_transpose(...): Transposes a SparseTensor", "split(...): Splits a tensor value into a list of sub tensors.", "sqrt(...): Computes element-wise square root of the input tensor.", "square(...): Computes square of x element-wise.", "squared_difference(...): Returns conj(x - y)(x - y) element-wise.", "squeeze(...): Removes dimensions of size 1 from the shape of a tensor. (deprecated arguments)", "stack(...): Stacks a list of rank-R tensors into one rank-(R+1) tensor.", "stop_gradient(...): Stops gradient computation.", "strided_slice(...): Extracts a strided slice of a tensor (generalized Python array indexing).", "string_join(...): Perform element-wise concatenation of a list of string tensors.", "string_split(...): Split elements of source based on delimiter. (deprecated arguments)", "string_strip(...): Strip leading and trailing whitespaces from the Tensor.", "string_to_hash_bucket(...): Converts each string in the input Tensor to its hash mod by a number of buckets.", "string_to_hash_bucket_fast(...): Converts each string in the input Tensor to its hash mod by a number of buckets.", "string_to_hash_bucket_strong(...): Converts each string in the input Tensor to its hash mod by a number of buckets.", "string_to_number(...): Converts each string in the input Tensor to the specified numeric type.", "substr(...): Return substrings from Tensor of strings.", "subtract(...): Returns x - y element-wise.", "svd(...): Computes the singular value decompositions of one or more matrices.", "switch_case(...): Create a switch/case operation, i.e. an integer-indexed conditional.", "tables_initializer(...): Returns an Op that initializes all tables of the default graph.", "tan(...): Computes tan of x element-wise.", "tanh(...): Computes hyperbolic tangent of x element-wise.", "tensor_scatter_add(...): Adds sparse updates to an existing tensor according to indices.", "tensor_scatter_nd_add(...): Adds sparse updates to an existing tensor according to indices.", "tensor_scatter_nd_max(...)", "tensor_scatter_nd_min(...)", "tensor_scatter_nd_sub(...): Subtracts sparse updates from an existing tensor according to indices.", "tensor_scatter_nd_update(...): \"Scatter updates into an existing tensor according to indices.", "tensor_scatter_sub(...): Subtracts sparse updates from an existing tensor according to indices.", "tensor_scatter_update(...): \"Scatter updates into an existing tensor according to indices.", "tensordot(...): Tensor contraction of a and b along specified axes and outer product.", "tile(...): Constructs a tensor by tiling a given tensor.", "timestamp(...): Provides the time since epoch in seconds.", "to_bfloat16(...): Casts a tensor to type bfloat16. (deprecated)", "to_complex128(...): Casts a tensor to type complex128. (deprecated)", "to_complex64(...): Casts a tensor to type complex64. (deprecated)", "to_double(...): Casts a tensor to type float64. (deprecated)", "to_float(...): Casts a tensor to type float32. (deprecated)", "to_int32(...): Casts a tensor to type int32. (deprecated)", "to_int64(...): Casts a tensor to type int64. (deprecated)", "trace(...): Compute the trace of a tensor x.", "trainable_variables(...): Returns all variables created with trainable=True.", "transpose(...): Transposes a.", "truediv(...): Divides x / y elementwise (using Python 3 division operator semantics).", "truncated_normal(...): Outputs random values from a truncated normal distribution.", "truncatediv(...): Returns x / y element-wise for integer types.", "truncatemod(...): Returns element-wise remainder of division. This emulates C semantics in that", "tuple(...): Group tensors together.", "type_spec_from_value(...): Returns a tf.TypeSpec that represents the given value.", "unique(...): Finds unique elements in a 1-D tensor.", "unique_with_counts(...): Finds unique elements in a 1-D tensor.", "unravel_index(...): Converts an array of flat indices into a tuple of coordinate arrays.", "unsorted_segment_max(...): Computes the maximum along segments of a tensor.", "unsorted_segment_mean(...): Computes the mean along segments of a tensor.", "unsorted_segment_min(...): Computes the minimum along segments of a tensor.", "unsorted_segment_prod(...): Computes the product along segments of a tensor.", "unsorted_segment_sqrt_n(...): Computes the sum along segments of a tensor divided by the sqrt(N).", "unsorted_segment_sum(...): Computes the sum along segments of a tensor.", "unstack(...): Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors.", "variable_axis_size_partitioner(...): Get a partitioner for VariableScope to keep shards below max_shard_bytes.", "variable_creator_scope(...): Scope which defines a variable creation function to be used by variable().", "variable_op_scope(...): Deprecated: context manager for defining an op that creates variables.", "variables_initializer(...): Returns an Op that initializes a list of variables.", "vectorized_map(...): Parallel map on the list of tensors unpacked from elems on dimension 0.", "verify_tensor_all_finite(...): Assert that the tensor does not contain any NaN's or Inf's.", "where(...): Return the elements, either from x or y, depending on the condition.", "where_v2(...): Return the elements where condition is True (multiplexing x and y).", "while_loop(...): Repeat body while the condition cond is true.", "wrap_function(...): Wraps the TF 1.x function fn into a graph function.", "write_file(...): Writes contents to the file at input filename. Creates file and recursively", "zeros(...): Creates a tensor with all elements set to zero.", "zeros_like(...): Creates a tensor with all elements set to zero.", "zeta(...): Compute the Hurwitz zeta function \\(\\zeta(x, q)\\)."]}, {"name": "tf.compat.v1.add_check_numerics_ops", "path": "compat/v1/add_check_numerics_ops", "type": "tf.compat", "text": ["Connect a tf.debugging.check_numerics to every floating point tensor.", "check_numerics operations themselves are added for each half, float, or double tensor in the current default graph. For all ops in the graph, the check_numerics op for all of its (half, float, or double) inputs is guaranteed to run before the check_numerics op on any of its outputs.", "Not compatible with eager execution. To check for Infs and NaNs under eager execution, call tf.debugging.enable_check_numerics() once before executing the checked operations."]}, {"name": "tf.compat.v1.add_to_collection", "path": "compat/v1/add_to_collection", "type": "tf.compat", "text": ["Wrapper for Graph.add_to_collection() using the default graph.", "See tf.Graph.add_to_collection for more details.", "Collections are only supported in eager when variables are created inside an EagerVariableStore (e.g. as part of a layer or template)."]}, {"name": "tf.compat.v1.add_to_collections", "path": "compat/v1/add_to_collections", "type": "tf.compat", "text": ["Wrapper for Graph.add_to_collections() using the default graph.", "See tf.Graph.add_to_collections for more details.", "Collections are only supported in eager when variables are created inside an EagerVariableStore (e.g. as part of a layer or template)."]}, {"name": "tf.compat.v1.all_variables", "path": "compat/v1/all_variables", "type": "tf.compat", "text": ["Use tf.compat.v1.global_variables instead. (deprecated)"]}, {"name": "tf.compat.v1.app", "path": "compat/v1/app", "type": "tf.compat", "text": ["Generic entry point script.", "flags module: Import router for absl.flags. See https://github.com/abseil/abseil-py", "run(...): Runs the program with an optional 'main' function and 'argv' list."]}, {"name": "tf.compat.v1.app.run", "path": "compat/v1/app/run", "type": "tf.compat", "text": ["Runs the program with an optional 'main' function and 'argv' list."]}, {"name": "tf.compat.v1.argmax", "path": "compat/v1/argmax", "type": "tf.compat", "text": ["Returns the index with the largest value across axes of a tensor. (deprecated arguments)", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.math.argmax", "Note that in case of ties the identity of the return value is not guaranteed."]}, {"name": "tf.compat.v1.argmin", "path": "compat/v1/argmin", "type": "tf.compat", "text": ["Returns the index with the smallest value across axes of a tensor. (deprecated arguments)", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.math.argmin", "Note that in case of ties the identity of the return value is not guaranteed."]}, {"name": "tf.compat.v1.arg_max", "path": "compat/v1/arg_max", "type": "tf.compat", "text": ["Returns the index with the largest value across dimensions of a tensor.", "Note that in case of ties the identity of the return value is not guaranteed."]}, {"name": "tf.compat.v1.arg_min", "path": "compat/v1/arg_min", "type": "tf.compat", "text": ["Returns the index with the smallest value across dimensions of a tensor.", "Note that in case of ties the identity of the return value is not guaranteed."]}, {"name": "tf.compat.v1.assert_equal", "path": "compat/v1/assert_equal", "type": "tf.compat", "text": ["Assert the condition x == y holds element-wise.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.debugging.assert_equal", "This condition holds if for every pair of (possibly broadcast) elements x[i], y[i], we have x[i] == y[i]. If both x and y are empty, this is trivially satisfied.", "When running in graph mode, you should add a dependency on this operation to ensure that it runs. Example of adding a dependency to an operation:", "returns None"]}, {"name": "tf.compat.v1.assert_greater", "path": "compat/v1/assert_greater", "type": "tf.compat", "text": ["Assert the condition x > y holds element-wise.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.debugging.assert_greater", "This condition holds if for every pair of (possibly broadcast) elements x[i], y[i], we have x[i] > y[i]. If both x and y are empty, this is trivially satisfied.", "When running in graph mode, you should add a dependency on this operation to ensure that it runs. Example of adding a dependency to an operation:", "returns None"]}, {"name": "tf.compat.v1.assert_greater_equal", "path": "compat/v1/assert_greater_equal", "type": "tf.compat", "text": ["Assert the condition x >= y holds element-wise.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.debugging.assert_greater_equal", "This condition holds if for every pair of (possibly broadcast) elements x[i], y[i], we have x[i] >= y[i]. If both x and y are empty, this is trivially satisfied.", "When running in graph mode, you should add a dependency on this operation to ensure that it runs. Example of adding a dependency to an operation:", "returns None"]}, {"name": "tf.compat.v1.assert_integer", "path": "compat/v1/assert_integer", "type": "tf.compat", "text": ["Assert that x is of integer dtype.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.debugging.assert_integer", "Example of adding a dependency to an operation:"]}, {"name": "tf.compat.v1.assert_less", "path": "compat/v1/assert_less", "type": "tf.compat", "text": ["Assert the condition x < y holds element-wise.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.debugging.assert_less", "This condition holds if for every pair of (possibly broadcast) elements x[i], y[i], we have x[i] < y[i]. If both x and y are empty, this is trivially satisfied.", "When running in graph mode, you should add a dependency on this operation to ensure that it runs. Example of adding a dependency to an operation:", "returns None"]}, {"name": "tf.compat.v1.assert_less_equal", "path": "compat/v1/assert_less_equal", "type": "tf.compat", "text": ["Assert the condition x <= y holds element-wise.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.debugging.assert_less_equal", "This condition holds if for every pair of (possibly broadcast) elements x[i], y[i], we have x[i] <= y[i]. If both x and y are empty, this is trivially satisfied.", "When running in graph mode, you should add a dependency on this operation to ensure that it runs. Example of adding a dependency to an operation:", "returns None"]}, {"name": "tf.compat.v1.assert_near", "path": "compat/v1/assert_near", "type": "tf.compat", "text": ["Assert the condition x and y are close element-wise.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.debugging.assert_near", "Example of adding a dependency to an operation:", "This condition holds if for every pair of (possibly broadcast) elements x[i], y[i], we have", "tf.abs(x[i] - y[i]) <= atol + rtol * tf.abs(y[i]).", "If both x and y are empty, this is trivially satisfied.", "The default atol and rtol is 10 * eps, where eps is the smallest representable positive number such that 1 + eps != 1. This is about 1.2e-6 in 32bit, 2.22e-15 in 64bit, and 0.00977 in 16bit. See numpy.finfo.", "Similar to numpy.testing.assert_allclose, except tolerance depends on data type. This is due to the fact that TensorFlow is often used with 32bit, 64bit, and even 16bit data."]}, {"name": "tf.compat.v1.assert_negative", "path": "compat/v1/assert_negative", "type": "tf.compat", "text": ["Assert the condition x < 0 holds element-wise.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.debugging.assert_negative", "When running in graph mode, you should add a dependency on this operation to ensure that it runs. Example of adding a dependency to an operation:", "Negative means, for every element x[i] of x, we have x[i] < 0. If x is empty this is trivially satisfied.", "returns None"]}, {"name": "tf.compat.v1.assert_none_equal", "path": "compat/v1/assert_none_equal", "type": "tf.compat", "text": ["Assert the condition x != y holds element-wise.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.debugging.assert_none_equal", "This condition holds if for every pair of (possibly broadcast) elements x[i], y[i], we have x[i] != y[i]. If both x and y are empty, this is trivially satisfied.", "When running in graph mode, you should add a dependency on this operation to ensure that it runs. Example of adding a dependency to an operation:", "returns None"]}, {"name": "tf.compat.v1.assert_non_negative", "path": "compat/v1/assert_non_negative", "type": "tf.compat", "text": ["Assert the condition x >= 0 holds element-wise.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.debugging.assert_non_negative", "When running in graph mode, you should add a dependency on this operation to ensure that it runs. Example of adding a dependency to an operation:", "Non-negative means, for every element x[i] of x, we have x[i] >= 0. If x is empty this is trivially satisfied.", "returns None"]}, {"name": "tf.compat.v1.assert_non_positive", "path": "compat/v1/assert_non_positive", "type": "tf.compat", "text": ["Assert the condition x <= 0 holds element-wise.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.debugging.assert_non_positive", "When running in graph mode, you should add a dependency on this operation to ensure that it runs. Example of adding a dependency to an operation:", "Non-positive means, for every element x[i] of x, we have x[i] <= 0. If x is empty this is trivially satisfied.", "returns None"]}, {"name": "tf.compat.v1.assert_positive", "path": "compat/v1/assert_positive", "type": "tf.compat", "text": ["Assert the condition x > 0 holds element-wise.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.debugging.assert_positive", "When running in graph mode, you should add a dependency on this operation to ensure that it runs. Example of adding a dependency to an operation:", "Positive means, for every element x[i] of x, we have x[i] > 0. If x is empty this is trivially satisfied.", "returns None"]}, {"name": "tf.compat.v1.assert_rank", "path": "compat/v1/assert_rank", "type": "tf.compat", "text": ["Assert x has rank equal to rank.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.debugging.assert_rank", "Example of adding a dependency to an operation:"]}, {"name": "tf.compat.v1.assert_rank_at_least", "path": "compat/v1/assert_rank_at_least", "type": "tf.compat", "text": ["Assert x has rank equal to rank or higher.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.debugging.assert_rank_at_least", "Example of adding a dependency to an operation:"]}, {"name": "tf.compat.v1.assert_rank_in", "path": "compat/v1/assert_rank_in", "type": "tf.compat", "text": ["Assert x has rank in ranks.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.debugging.assert_rank_in", "Example of adding a dependency to an operation:"]}, {"name": "tf.compat.v1.assert_scalar", "path": "compat/v1/assert_scalar", "type": "tf.compat", "text": ["Asserts that the given tensor is a scalar (i.e. zero-dimensional).", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.debugging.assert_scalar", "This function raises ValueError unless it can be certain that the given tensor is a scalar. ValueError is also raised if the shape of tensor is unknown."]}, {"name": "tf.compat.v1.assert_type", "path": "compat/v1/assert_type", "type": "tf.compat", "text": ["Statically asserts that the given Tensor is of the specified type.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.debugging.assert_type"]}, {"name": "tf.compat.v1.assert_variables_initialized", "path": "compat/v1/assert_variables_initialized", "type": "tf.compat", "text": ["Returns an Op to check if variables are initialized.", "When run, the returned Op will raise the exception FailedPreconditionError if any of the variables has not yet been initialized."]}, {"name": "tf.compat.v1.assign", "path": "compat/v1/assign", "type": "tf.compat", "text": ["Update ref by assigning value to it.", "This operation outputs a Tensor that holds the new value of ref after the value has been assigned. This makes it easier to chain operations that need to use the reset value."]}, {"name": "tf.compat.v1.assign_add", "path": "compat/v1/assign_add", "type": "tf.compat", "text": ["Update ref by adding value to it.", "This operation outputs \"ref\" after the update is done. This makes it easier to chain operations that need to use the reset value. Unlike tf.math.add, this op does not broadcast. ref and value must have the same shape."]}, {"name": "tf.compat.v1.assign_sub", "path": "compat/v1/assign_sub", "type": "tf.compat", "text": ["Update ref by subtracting value from it.", "This operation outputs ref after the update is done. This makes it easier to chain operations that need to use the reset value. Unlike tf.math.subtract, this op does not broadcast. ref and value must have the same shape."]}, {"name": "tf.compat.v1.AttrValue", "path": "compat/v1/attrvalue", "type": "tf.compat", "text": ["A ProtocolMessage", "class ListValue"]}, {"name": "tf.compat.v1.AttrValue.ListValue", "path": "compat/v1/attrvalue/listvalue", "type": "tf.compat", "text": ["A ProtocolMessage"]}, {"name": "tf.compat.v1.audio", "path": "compat/v1/audio", "type": "tf.compat", "text": ["Public API for tf.audio namespace.", "decode_wav(...): Decode a 16-bit PCM WAV file to a float tensor.", "encode_wav(...): Encode audio data using the WAV file format."]}, {"name": "tf.compat.v1.autograph", "path": "compat/v1/autograph", "type": "tf.compat", "text": ["Conversion of plain Python into TensorFlow graph code.", "For more information, see the AutoGraph guide.", "By equivalent graph code we mean code that generates a TensorFlow graph when run. The generated graph has the same effects as the original code when executed (for example with tf.function or tf.compat.v1.Session.run). In other words, using AutoGraph can be thought of as running Python in TensorFlow.", "experimental module: Public API for tf.autograph.experimental namespace.", "set_verbosity(...): Sets the AutoGraph verbosity level.", "to_code(...): Returns the source code generated by AutoGraph, as a string.", "to_graph(...): Converts a Python entity into a TensorFlow graph.", "trace(...): Traces argument information at compilation time."]}, {"name": "tf.compat.v1.autograph.experimental", "path": "compat/v1/autograph/experimental", "type": "tf.compat", "text": ["Public API for tf.autograph.experimental namespace.", "class Feature: This enumeration represents optional conversion options.", "do_not_convert(...): Decorator that suppresses the conversion of a function.", "set_loop_options(...): Specifies additional arguments to be passed to the enclosing while_loop."]}, {"name": "tf.compat.v1.autograph.to_code", "path": "compat/v1/autograph/to_code", "type": "tf.compat", "text": ["Returns the source code generated by AutoGraph, as a string.", "Also see: tf.autograph.to_graph."]}, {"name": "tf.compat.v1.autograph.to_graph", "path": "compat/v1/autograph/to_graph", "type": "tf.compat", "text": ["Converts a Python entity into a TensorFlow graph.", "Also see: tf.autograph.to_code, tf.function.", "Unlike tf.function, to_graph is a low-level transpiler that converts Python code to TensorFlow graph code. It does not implement any caching, variable management or create any actual ops, and is best used where greater control over the generated TensorFlow graph is desired. Another difference from tf.function is that to_graph will not wrap the graph into a TensorFlow function or a Python callable. Internally, tf.function uses to_graph.", "Example Usage", "Supported Python entities include:", "Functions are converted into new functions with converted code.", "Classes are converted by generating a new class whose methods use converted code.", "Methods are converted into unbound function that have an additional first argument called self."]}, {"name": "tf.compat.v1.batch_gather", "path": "compat/v1/batch_gather", "type": "tf.compat", "text": ["Gather slices from params according to indices with leading batch dims. (deprecated)"]}, {"name": "tf.compat.v1.batch_scatter_update", "path": "compat/v1/batch_scatter_update", "type": "tf.compat", "text": ["Generalization of tf.compat.v1.scatter_update to axis different than 0. (deprecated)", "Analogous to batch_gather. This assumes that ref, indices and updates have a series of leading dimensions that are the same for all of them, and the updates are performed on the last dimension of indices. In other words, the dimensions should be the following:", "num_prefix_dims = indices.ndims - 1 batch_dim = num_prefix_dims + 1 updates.shape = indices.shape + var.shape[batch_dim:]", "where", "updates.shape[:num_prefix_dims] == indices.shape[:num_prefix_dims] == var.shape[:num_prefix_dims]", "And the operation performed can be expressed as:", "var[i_1, ..., i_n, indices[i_1, ..., i_n, j]] = updates[i_1, ..., i_n, j]", "When indices is a 1D tensor, this operation is equivalent to tf.compat.v1.scatter_update.", "To avoid this operation there would be 2 alternatives:", "1) Reshaping the variable by merging the first ndims dimensions. However, this is not possible because tf.reshape returns a Tensor, which we cannot use tf.compat.v1.scatter_update on. 2) Looping over the first ndims of the variable and using tf.compat.v1.scatter_update on the subtensors that result of slicing the first dimension. This is a valid option for ndims = 1, but less efficient than this implementation.", "See also tf.compat.v1.scatter_update and tf.compat.v1.scatter_nd_update."]}, {"name": "tf.compat.v1.batch_to_space", "path": "compat/v1/batch_to_space", "type": "tf.compat", "text": ["BatchToSpace for 4-D tensors of type T.", "This is a legacy version of the more general BatchToSpaceND.", "Rearranges (permutes) data from batch into blocks of spatial data, followed by cropping. This is the reverse transformation of SpaceToBatch. More specifically, this op outputs a copy of the input tensor where values from the batch dimension are moved in spatial blocks to the height and width dimensions, followed by cropping along the height and width dimensions.", "crops = [[crop_top, crop_bottom], [crop_left, crop_right]] "]}, {"name": "tf.compat.v1.batch_to_space_nd", "path": "compat/v1/batch_to_space_nd", "type": "tf.compat", "text": ["BatchToSpace for N-D tensors of type T.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.manip.batch_to_space_nd", "This operation reshapes the \"batch\" dimension 0 into M + 1 dimensions of shape block_shape + [batch], interleaves these blocks back into the grid defined by the spatial dimensions [1, ..., M], to obtain a result with the same rank as the input. The spatial dimensions of this intermediate result are then optionally cropped according to crops to produce the output. This is the reverse of SpaceToBatch. See below for a precise description.", "This operation is equivalent to the following steps:", "Reshape input to reshaped of shape: [block_shape[0], ..., block_shape[M-1], batch / prod(block_shape), input_shape[1], ..., input_shape[N-1]]", "Permute dimensions of reshaped to produce permuted of shape [batch / prod(block_shape),", "input_shape[1], block_shape[0], ..., input_shape[M], block_shape[M-1],", "input_shape[M+1], ..., input_shape[N-1]]", "input_shape[1] * block_shape[0], ..., input_shape[M] * block_shape[M-1],", "input_shape[M+1], ..., input_shape[N-1]]", "input_shape[1] * block_shape[0] - crops[0,0] - crops[0,1], ..., input_shape[M] * block_shape[M-1] - crops[M-1,0] - crops[M-1,1],", "input_shape[M+1], ..., input_shape[N-1]]", "Some examples:", "(1) For the following input of shape [4, 1, 1, 1], block_shape = [2, 2], and crops = [[0, 0], [0, 0]]:", "The output tensor has shape [1, 2, 2, 1] and value:", "(2) For the following input of shape [4, 1, 1, 3], block_shape = [2, 2], and crops = [[0, 0], [0, 0]]:", "The output tensor has shape [1, 2, 2, 3] and value:", "(3) For the following input of shape [4, 2, 2, 1], block_shape = [2, 2], and crops = [[0, 0], [0, 0]]:", "The output tensor has shape [1, 4, 4, 1] and value:", "(4) For the following input of shape [8, 1, 3, 1], block_shape = [2, 2], and crops = [[0, 0], [2, 0]]:", "The output tensor has shape [2, 2, 4, 1] and value:"]}, {"name": "tf.compat.v1.bincount", "path": "compat/v1/bincount", "type": "tf.compat", "text": ["Counts the number of occurrences of each value in an integer array.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.math.bincount", "If minlength and maxlength are not given, returns a vector with length tf.reduce_max(arr) + 1 if arr is non-empty, and length 0 otherwise. If weights are non-None, then index i of the output stores the sum of the value in weights at each index where the corresponding value in arr is i."]}, {"name": "tf.compat.v1.bitwise", "path": "compat/v1/bitwise", "type": "tf.compat", "text": ["Operations for manipulating the binary representations of integers.", "bitwise_and(...): Elementwise computes the bitwise AND of x and y.", "bitwise_or(...): Elementwise computes the bitwise OR of x and y.", "bitwise_xor(...): Elementwise computes the bitwise XOR of x and y.", "invert(...): Invert (flip) each bit of supported types; for example, type uint8 value 01010101 becomes 10101010.", "left_shift(...): Elementwise computes the bitwise left-shift of x and y.", "right_shift(...): Elementwise computes the bitwise right-shift of x and y."]}, {"name": "tf.compat.v1.boolean_mask", "path": "compat/v1/boolean_mask", "type": "tf.compat", "text": ["Apply boolean mask to tensor.", "Numpy equivalent is tensor[mask].", "In general, 0 < dim(mask) = K <= dim(tensor), and mask's shape must match the first K dimensions of tensor's shape. We then have: boolean_mask(tensor, mask)[i, j1,...,jd] = tensor[i1,...,iK,j1,...,jd] where (i1,...,iK) is the ith True entry of mask (row-major order). The axis could be used with mask to indicate the axis to mask from. In that case, axis + dim(mask) <= dim(tensor) and mask's shape must match the first axis + dim(mask) dimensions of tensor's shape.", "See also: tf.ragged.boolean_mask, which can be applied to both dense and ragged tensors, and can be used if you need to preserve the masked dimensions of tensor (rather than flattening them, as tf.boolean_mask does)."]}, {"name": "tf.compat.v1.case", "path": "compat/v1/case", "type": "tf.compat", "text": ["Create a case operation.", "See also tf.switch_case.", "The pred_fn_pairs parameter is a dict or list of pairs of size N. Each pair contains a boolean scalar tensor and a python callable that creates the tensors to be returned if the boolean evaluates to True. default is a callable generating a list of tensors. All the callables in pred_fn_pairs as well as default (if provided) should return the same number and types of tensors.", "If exclusive==True, all predicates are evaluated, and an exception is thrown if more than one of the predicates evaluates to True. If exclusive==False, execution stops at the first predicate which evaluates to True, and the tensors generated by the corresponding function are returned immediately. If none of the predicates evaluate to True, this operation returns the tensors generated by default.", "tf.case supports nested structures as implemented in tf.contrib.framework.nest. All of the callables must return the same (possibly nested) value structure of lists, tuples, and/or named tuples. Singleton lists and tuples form the only exceptions to this: when returned by a callable, they are implicitly unpacked to single values. This behavior is disabled by passing strict=True.", "If an unordered dictionary is used for pred_fn_pairs, the order of the conditional tests is not guaranteed. However, the order is guaranteed to be deterministic, so that variables created in conditional branches are created in fixed order across runs.", "Example 1:", "Example 2:", "Unordered dictionaries are not supported in eager mode when exclusive=False. Use a list of tuples instead."]}, {"name": "tf.compat.v1.clip_by_average_norm", "path": "compat/v1/clip_by_average_norm", "type": "tf.compat", "text": ["Clips tensor values to a maximum average L2-norm. (deprecated)", "Given a tensor t, and a maximum clip value clip_norm, this operation normalizes t so that its average L2-norm is less than or equal to clip_norm. Specifically, if the average L2-norm is already less than or equal to clip_norm, then t is not modified. If the average L2-norm is greater than clip_norm, then this operation returns a tensor of the same type and shape as t with its values set to:", "t * clip_norm / l2norm_avg(t)", "In this case, the average L2-norm of the output tensor is clip_norm.", "This operation is typically used to clip gradients before applying them with an optimizer."]}, {"name": "tf.compat.v1.colocate_with", "path": "compat/v1/colocate_with", "type": "tf.compat", "text": ["DEPRECATED FUNCTION"]}, {"name": "tf.compat.v1.compat", "path": "compat/v1/compat", "type": "tf.compat", "text": ["Compatibility functions.", "The tf.compat module contains two sets of compatibility functions.", "The compat.v1 and compat.v2 submodules provide a complete copy of both the v1 and v2 APIs for backwards and forwards compatibility across TensorFlow versions 1.x and 2.x. See the migration guide for details.", "Aside from the compat.v1 and compat.v2 submodules, tf.compat also contains a set of helper functions for writing code that works in both:", "The compatibility module also provides the following aliases for common sets of python types:", "as_bytes(...): Converts bytearray, bytes, or unicode python input types to bytes.", "as_str(...)", "as_str_any(...): Converts input to str type.", "as_text(...): Converts any string-like python input types to unicode.", "dimension_at_index(...): Compatibility utility required to allow for both V1 and V2 behavior in TF.", "dimension_value(...): Compatibility utility required to allow for both V1 and V2 behavior in TF.", "forward_compatibility_horizon(...): Context manager for testing forward compatibility of generated graphs.", "forward_compatible(...): Return true if the forward compatibility window has expired.", "path_to_str(...): Converts input which is a PathLike object to str type."]}, {"name": "tf.compat.v1.cond", "path": "compat/v1/cond", "type": "tf.compat", "text": ["Return true_fn() if the predicate pred is true else false_fn(). (deprecated arguments)", "true_fn and false_fn both return lists of output tensors. true_fn and false_fn must have the same non-zero number and type of outputs.", "Although this behavior is consistent with the dataflow model of TensorFlow, it has frequently surprised users who expected a lazier semantics. Consider the following simple program:", "If x < y, the tf.add operation will be executed and tf.square operation will not be executed. Since z is needed for at least one branch of the cond, the tf.multiply operation is always executed, unconditionally.", "Note that cond calls true_fn and false_fn exactly once (inside the call to cond, and not at all during Session.run()). cond stitches together the graph fragments created during the true_fn and false_fn calls with some additional graph nodes to ensure that the right branch gets executed depending on the value of pred.", "tf.cond supports nested structures as implemented in tensorflow.python.util.nest. Both true_fn and false_fn must return the same (possibly nested) value structure of lists, tuples, and/or named tuples. Singleton lists and tuples form the only exceptions to this: when returned by true_fn and/or false_fn, they are implicitly unpacked to single values. This behavior is disabled by passing strict=True."]}, {"name": "tf.compat.v1.ConditionalAccumulator", "path": "compat/v1/conditionalaccumulator", "type": "tf.compat", "text": ["A conditional accumulator for aggregating gradients.", "Inherits From: ConditionalAccumulatorBase", "Up-to-date gradients (i.e., time step at which gradient was computed is equal to the accumulator's time step) are added to the accumulator.", "Extraction of the average gradient is blocked until the required number of gradients has been accumulated.", "View source", "Attempts to apply a gradient to the accumulator.", "The attempt is silently dropped if the gradient is stale, i.e., local_step is less than the accumulator's global time step.", "View source", "Number of gradients that have currently been aggregated in accumulator.", "View source", "Sets the global time step of the accumulator.", "The operation logs a warning if we attempt to set to a time step that is lower than the accumulator's own time step.", "View source", "Attempts to extract the average gradient from the accumulator.", "The operation blocks until sufficient number of gradients have been successfully applied to the accumulator.", "Once successful, the following actions are also triggered:"]}, {"name": "tf.compat.v1.ConditionalAccumulatorBase", "path": "compat/v1/conditionalaccumulatorbase", "type": "tf.compat", "text": ["A conditional accumulator for aggregating gradients.", "Up-to-date gradients (i.e., time step at which gradient was computed is equal to the accumulator's time step) are added to the accumulator.", "Extraction of the average gradient is blocked until the required number of gradients has been accumulated.", "View source", "Number of gradients that have currently been aggregated in accumulator.", "View source", "Sets the global time step of the accumulator.", "The operation logs a warning if we attempt to set to a time step that is lower than the accumulator's own time step."]}, {"name": "tf.compat.v1.config", "path": "compat/v1/config", "type": "tf.compat", "text": ["Public API for tf.config namespace.", "experimental module: Public API for tf.config.experimental namespace.", "optimizer module: Public API for tf.config.optimizer namespace.", "threading module: Public API for tf.config.threading namespace.", "class LogicalDevice: Abstraction for a logical device initialized by the runtime.", "class LogicalDeviceConfiguration: Configuration class for a logical devices.", "class PhysicalDevice: Abstraction for a locally visible physical device.", "experimental_connect_to_cluster(...): Connects to the given cluster.", "experimental_connect_to_host(...): Connects to a single machine to enable remote execution on it.", "experimental_functions_run_eagerly(...): Returns the value of the experimental_run_functions_eagerly setting. (deprecated)", "experimental_run_functions_eagerly(...): Enables / disables eager execution of tf.functions. (deprecated)", "functions_run_eagerly(...): Returns the value of the run_functions_eagerly setting.", "get_logical_device_configuration(...): Get the virtual device configuration for a tf.config.PhysicalDevice.", "get_soft_device_placement(...): Get if soft device placement is enabled.", "get_visible_devices(...): Get the list of visible physical devices.", "list_logical_devices(...): Return a list of logical devices created by runtime.", "list_physical_devices(...): Return a list of physical devices visible to the host runtime.", "run_functions_eagerly(...): Enables / disables eager execution of tf.functions.", "set_logical_device_configuration(...): Set the logical device configuration for a tf.config.PhysicalDevice.", "set_soft_device_placement(...): Set if soft device placement is enabled.", "set_visible_devices(...): Set the list of visible devices."]}, {"name": "tf.compat.v1.config.experimental", "path": "compat/v1/config/experimental", "type": "tf.compat", "text": ["Public API for tf.config.experimental namespace.", "class ClusterDeviceFilters: Represent a collection of device filters for the remote workers in cluster.", "class VirtualDeviceConfiguration: Configuration class for a logical devices.", "disable_mlir_bridge(...): Disables experimental MLIR-Based TensorFlow Compiler Bridge.", "disable_mlir_graph_optimization(...): Disables experimental MLIR-Based TensorFlow Compiler Optimizations.", "enable_mlir_bridge(...): Enables experimental MLIR-Based TensorFlow Compiler Bridge.", "enable_mlir_graph_optimization(...): Enables experimental MLIR-Based TensorFlow Compiler Optimizations.", "enable_tensor_float_32_execution(...): Enable or disable the use of TensorFloat-32 on supported hardware.", "get_device_details(...): Returns details about a physical devices.", "get_device_policy(...): Gets the current device policy.", "get_memory_growth(...): Get if memory growth is enabled for a PhysicalDevice.", "get_memory_usage(...): Get the memory usage, in bytes, for the chosen device.", "get_synchronous_execution(...): Gets whether operations are executed synchronously or asynchronously.", "get_virtual_device_configuration(...): Get the virtual device configuration for a tf.config.PhysicalDevice.", "get_visible_devices(...): Get the list of visible physical devices.", "list_logical_devices(...): Return a list of logical devices created by runtime.", "list_physical_devices(...): Return a list of physical devices visible to the host runtime.", "set_device_policy(...): Sets the current thread device policy.", "set_memory_growth(...): Set if memory growth should be enabled for a PhysicalDevice.", "set_synchronous_execution(...): Specifies whether operations are executed synchronously or asynchronously.", "set_virtual_device_configuration(...): Set the logical device configuration for a tf.config.PhysicalDevice.", "set_visible_devices(...): Set the list of visible devices.", "tensor_float_32_execution_enabled(...): Returns whether TensorFloat-32 is enabled."]}, {"name": "tf.compat.v1.config.optimizer", "path": "compat/v1/config/optimizer", "type": "tf.compat", "text": ["Public API for tf.config.optimizer namespace.", "get_experimental_options(...): Get experimental optimizer options.", "get_jit(...): Get if JIT compilation is enabled.", "set_experimental_options(...): Set experimental optimizer options.", "set_jit(...): Set if JIT compilation is enabled."]}, {"name": "tf.compat.v1.config.threading", "path": "compat/v1/config/threading", "type": "tf.compat", "text": ["Public API for tf.config.threading namespace.", "get_inter_op_parallelism_threads(...): Get number of threads used for parallelism between independent operations.", "get_intra_op_parallelism_threads(...): Get number of threads used within an individual op for parallelism.", "set_inter_op_parallelism_threads(...): Set number of threads used for parallelism between independent operations.", "set_intra_op_parallelism_threads(...): Set number of threads used within an individual op for parallelism."]}, {"name": "tf.compat.v1.ConfigProto", "path": "compat/v1/configproto", "type": "tf.compat", "text": ["A ProtocolMessage", "class DeviceCountEntry", "class Experimental"]}, {"name": "tf.compat.v1.ConfigProto.DeviceCountEntry", "path": "compat/v1/configproto/devicecountentry", "type": "tf.compat", "text": ["A ProtocolMessage"]}, {"name": "tf.compat.v1.ConfigProto.Experimental", "path": "compat/v1/configproto/experimental", "type": "tf.compat", "text": ["A ProtocolMessage"]}, {"name": "tf.compat.v1.confusion_matrix", "path": "compat/v1/confusion_matrix", "type": "tf.compat", "text": ["Computes the confusion matrix from predictions and labels.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.math.confusion_matrix", "The matrix columns represent the prediction labels and the rows represent the real labels. The confusion matrix is always a 2-D array of shape [n, n], where n is the number of valid labels for a given classification task. Both prediction and labels must be 1-D arrays of the same shape in order for this function to work.", "If num_classes is None, then num_classes will be set to one plus the maximum value in either predictions or labels. Class labels are expected to start at 0. For example, if num_classes is 3, then the possible labels would be [0, 1, 2].", "If weights is not None, then each prediction contributes its corresponding weight to the total value of the confusion matrix cell.", "Note that the possible labels are assumed to be [0, 1, 2, 3, 4], resulting in a 5x5 confusion matrix."]}, {"name": "tf.compat.v1.constant", "path": "compat/v1/constant", "type": "tf.compat", "text": ["Creates a constant tensor.", "The resulting tensor is populated with values of type dtype, as specified by arguments value and (optionally) shape (see examples below).", "The argument value can be a constant value, or a list of values of type dtype. If value is a list, then the length of the list must be less than or equal to the number of elements implied by the shape argument (if specified). In the case where the list length is less than the number of elements specified by shape, the last element in the list will be used to fill the remaining entries.", "The argument shape is optional. If present, it specifies the dimensions of the resulting tensor. If not present, the shape of value is used.", "If the argument dtype is not specified, then the type is inferred from the type of value.", "tf.constant differs from tf.fill in a few ways:"]}, {"name": "tf.compat.v1.container", "path": "compat/v1/container", "type": "tf.compat", "text": ["Wrapper for Graph.container() using the default graph."]}, {"name": "tf.compat.v1.control_flow_v2_enabled", "path": "compat/v1/control_flow_v2_enabled", "type": "tf.compat", "text": ["Returns True if v2 control flow is enabled."]}, {"name": "tf.compat.v1.convert_to_tensor", "path": "compat/v1/convert_to_tensor", "type": "tf.compat", "text": ["Converts the given value to a Tensor.", "This function converts Python objects of various types to Tensor objects. It accepts Tensor objects, numpy arrays, Python lists, and Python scalars. For example:", "This function can be useful when composing a new operation in Python (such as my_func in the example above). All standard Python op constructors apply this function to each of their Tensor-valued inputs, which allows those ops to accept numpy arrays, Python lists, and scalars in addition to Tensor objects."]}, {"name": "tf.compat.v1.convert_to_tensor_or_indexed_slices", "path": "compat/v1/convert_to_tensor_or_indexed_slices", "type": "tf.compat", "text": ["Converts the given object to a Tensor or an IndexedSlices.", "If value is an IndexedSlices or SparseTensor it is returned unmodified. Otherwise, it is converted to a Tensor using convert_to_tensor()."]}, {"name": "tf.compat.v1.convert_to_tensor_or_sparse_tensor", "path": "compat/v1/convert_to_tensor_or_sparse_tensor", "type": "tf.compat", "text": ["Converts value to a SparseTensor or Tensor."]}, {"name": "tf.compat.v1.count_nonzero", "path": "compat/v1/count_nonzero", "type": "tf.compat", "text": ["Computes number of nonzero elements across dimensions of a tensor. (deprecated arguments) (deprecated arguments)", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.math.count_nonzero", "Reduces input_tensor along the dimensions given in axis. Unless keepdims is true, the rank of the tensor is reduced by 1 for each entry in axis. If keepdims is true, the reduced dimensions are retained with length 1.", "If axis has no entries, all dimensions are reduced, and a tensor with a single element is returned."]}, {"name": "tf.compat.v1.count_up_to", "path": "compat/v1/count_up_to", "type": "tf.compat", "text": ["Increments 'ref' until it reaches 'limit'. (deprecated)"]}, {"name": "tf.compat.v1.create_partitioned_variables", "path": "compat/v1/create_partitioned_variables", "type": "tf.compat", "text": ["Create a list of partitioned variables according to the given slicing. (deprecated)", "Currently only one dimension of the full variable can be sliced, and the full variable can be reconstructed by the concatenation of the returned list along that dimension.", "For convenience, The requested number of partitions does not have to divide the corresponding dimension evenly. If it does not, the shapes of the partitions are incremented by 1 starting from partition 0 until all slack is absorbed. The adjustment rules may change in the future, but as you can save/restore these variables with different slicing specifications this should not be a problem. "]}, {"name": "tf.compat.v1.data", "path": "compat/v1/data", "type": "tf.compat", "text": ["tf.data.Dataset API for input pipelines.", "See Importing Data for an overview.", "experimental module: Experimental API for building input pipelines.", "class Dataset: Represents a potentially large set of elements.", "class DatasetSpec: Type specification for tf.data.Dataset.", "class FixedLengthRecordDataset: A Dataset of fixed-length records from one or more binary files.", "class Iterator: Represents the state of iterating through a Dataset.", "class Options: Represents options for tf.data.Dataset.", "class TFRecordDataset: A Dataset comprising records from one or more TFRecord files.", "class TextLineDataset: A Dataset comprising lines from one or more text files.", "get_output_classes(...): Returns the output classes for elements of the input dataset / iterator.", "get_output_shapes(...): Returns the output shapes for elements of the input dataset / iterator.", "get_output_types(...): Returns the output shapes for elements of the input dataset / iterator.", "make_initializable_iterator(...): Creates an iterator for elements of dataset.", "make_one_shot_iterator(...): Creates an iterator for elements of dataset."]}, {"name": "tf.compat.v1.data.Dataset", "path": "compat/v1/data/dataset", "type": "tf.compat", "text": ["Represents a potentially large set of elements.", "Inherits From: Dataset", "A Dataset can be used to represent an input pipeline as a collection of elements and a \"logical plan\" of transformations that act on those elements.", "View source", "Applies a transformation function to this dataset.", "apply enables chaining of custom Dataset transformations, which are represented as functions that take one Dataset argument and return a transformed Dataset.", "View source", "Returns an iterator which converts all elements of the dataset to numpy.", "Use as_numpy_iterator to inspect the content of your dataset. To see element shapes and types, print dataset elements directly instead of using as_numpy_iterator.", "This method requires that you are running in eager mode and the dataset's element_spec contains only TensorSpec components.", "as_numpy_iterator() will preserve the nested structure of dataset elements.", "View source", "Combines consecutive elements of this dataset into batches.", "The components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced.", "View source", "Caches the elements in this dataset.", "The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.", "When caching to a file, the cached data will persist across runs. Even the first iteration through the data will read from the cache file. Changing the input pipeline before the call to .cache() will have no effect until the cache file is removed or the filename is changed.", "View source", "Returns the cardinality of the dataset, if known.", "cardinality may return tf.data.INFINITE_CARDINALITY if the dataset contains an infinite number of elements or tf.data.UNKNOWN_CARDINALITY if the analysis fails to determine the number of elements in the dataset (e.g. when the dataset source is a file).", "View source", "Creates a Dataset by concatenating the given dataset with this dataset.", "View source", "Enumerates the elements of this dataset.", "It is similar to python's enumerate.", "View source", "Filters this dataset according to predicate.", "View source", "Filters this dataset according to predicate. (deprecated)", "View source", "Maps map_func across this dataset and flattens the result.", "Use flat_map if you want to make sure that the order of your dataset stays the same. For example, to flatten a dataset of batches into a dataset of their elements:", "tf.data.Dataset.interleave() is a generalization of flat_map, since flat_map produces the same output as tf.data.Dataset.interleave(cycle_length=1)", "View source", "Creates a Dataset whose elements are generated by generator. (deprecated arguments)", "The generator argument must be a callable object that returns an object that supports the iter() protocol (e.g. a generator function).", "The elements generated by generator must be compatible with either the given output_signature argument or with the given output_types and (optionally) output_shapes arguments, whichiver was specified.", "The recommended way to call from_generator is to use the output_signature argument. In this case the output will be assumed to consist of objects with the classes, shapes and types defined by tf.TypeSpec objects from output_signature argument:", "There is also a deprecated way to call from_generator by either with output_types argument alone or together with output_shapes argument. In this case the output of the function will be assumed to consist of tf.Tensor objects with with the types defined by output_types and with the shapes which are either unknown or defined by output_shapes.", "View source", "Splits each rank-N tf.sparse.SparseTensor in this dataset row-wise. (deprecated)", "View source", "Creates a Dataset whose elements are slices of the given tensors.", "The given tensors are sliced along their first dimension. This operation preserves the structure of the input tensors, removing the first dimension of each tensor and using it as the dataset dimension. All input tensors must have the same size in their first dimensions.", "Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.", "View source", "Creates a Dataset with a single element, comprising the given tensors.", "from_tensors produces a dataset containing only a single element. To slice the input tensor into multiple elements, use from_tensor_slices instead.", "Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.", "View source", "Maps map_func across this dataset, and interleaves the results.", "For example, you can use Dataset.interleave() to process many input files concurrently:", "The cycle_length and block_length arguments control the order in which elements are produced. cycle_length controls the number of input elements that are processed concurrently. If you set cycle_length to 1, this transformation will handle one input element at a time, and will produce identical results to tf.data.Dataset.flat_map. In general, this transformation will apply map_func to cycle_length input elements, open iterators on the returned Dataset objects, and cycle through them producing block_length consecutive elements from each iterator, and consuming the next input element each time it reaches the end of an iterator.", "Performance can often be improved by setting num_parallel_calls so that interleave will use multiple threads to fetch elements. If determinism isn't required, it can also improve performance to set deterministic=False.", "View source", "A dataset of all files matching one or more glob patterns.", "The file_pattern argument should be a small number of glob patterns. If your filenames have already been globbed, use Dataset.from_tensor_slices(filenames) instead, as re-globbing every filename with list_files may result in poor performance with remote storage systems.", "If we had the following files on our filesystem:", "If we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:", "View source", "Creates an iterator for elements of this dataset. (deprecated)", "View source", "Creates an iterator for elements of this dataset. (deprecated)", "View source", "Maps map_func across the elements of this dataset.", "This transformation applies map_func to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. map_func can be used to change both the values and the structure of a dataset's elements. For example, adding 1 to each element, or projecting a subset of element components.", "The input signature of map_func is determined by the structure of each element in this dataset.", "The value or values returned by map_func determine the structure of each element in the returned dataset.", "map_func can accept as arguments and return any type of dataset element.", "Note that irrespective of the context in which map_func is defined (eager vs. graph), tf.data traces the function and executes it as a graph. To use Python code inside of the function you have a few options:", "1) Rely on AutoGraph to convert Python code into an equivalent graph computation. The downside of this approach is that AutoGraph can convert some but not all Python code.", "2) Use tf.py_function, which allows you to write arbitrary Python code but will generally result in worse performance than 1). For example:", "3) Use tf.numpy_function, which also allows you to write arbitrary Python code. Note that tf.py_function accepts tf.Tensor whereas tf.numpy_function accepts numpy arrays and returns only numpy arrays. For example:", "Note that the use of tf.numpy_function and tf.py_function in general precludes the possibility of executing user-defined transformations in parallel (because of Python GIL).", "Performance can often be improved by setting num_parallel_calls so that map will use multiple threads to process elements. If deterministic order isn't required, it can also improve performance to set deterministic=False.", "View source", "Maps map_func across the elements of this dataset. (deprecated)", "View source", "Returns the options for this dataset and its inputs.", "View source", "Combines consecutive elements of this dataset into padded batches.", "This transformation combines multiple consecutive elements of the input dataset into a single element.", "Like tf.data.Dataset.batch, the components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced.", "Unlike tf.data.Dataset.batch, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in padded_shapes. The padded_shapes argument determines the resulting shape for each dimension of each component in an output element:", "See also tf.data.experimental.dense_to_sparse_batch, which combines elements that may have different shapes into a tf.sparse.SparseTensor.", "View source", "Creates a Dataset that prefetches elements from this dataset.", "Most dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.", "View source", "Creates a Dataset of a step-separated range of values.", "View source", "Reduces the input dataset to a single element.", "The transformation calls reduce_func successively on every element of the input dataset until the dataset is exhausted, aggregating information in its internal state. The initial_state argument is used for the initial state and the final state is returned as the result.", "View source", "Repeats this dataset so each original value is seen count times.", "View source", "Creates a Dataset that includes only 1/num_shards of this dataset.", "shard is deterministic. The Dataset produced by A.shard(n, i) will contain all elements of A whose index mod n = i.", "This dataset operator is very useful when running distributed training, as it allows each worker to read a unique subset.", "When reading a single input file, you can shard elements as follows:", "View source", "Randomly shuffles the elements of this dataset.", "This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.", "For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then shuffle will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer.", "reshuffle_each_iteration controls whether the shuffle order should be different for each epoch. In TF 1.X, the idiomatic way to create epochs was through the repeat transformation:", "In TF 2.0, tf.data.Dataset objects are Python iterables which makes it possible to also create epochs through Python iteration:", "View source", "Creates a Dataset that skips count elements from this dataset.", "View source", "Creates a Dataset with at most count elements from this dataset.", "View source", "Splits elements of a dataset into multiple elements.", "For example, if elements of the dataset are shaped [B, a0, a1, ...], where B may vary for each input element, then for each element in the dataset, the unbatched dataset will contain B consecutive elements of shape [a0, a1, ...].", "View source", "Combines (nests of) input elements into a dataset of (nests of) windows.", "A \"window\" is a finite dataset of flat elements of size size (or possibly fewer if there are not enough input elements to fill the window and drop_remainder evaluates to False).", "The shift argument determines the number of input elements by which the window moves on each iteration. If windows and elements are both numbered starting at 0, the first element in window k will be element k * shift of the input dataset. In particular, the first element of the first window will always be the first element of the input dataset.", "The stride argument determines the stride of the input elements, and the shift argument determines the shift of the window.", "Note that when the window transformation is applied to a dataset of nested elements, it produces a dataset of nested windows.", "View source", "Returns a new tf.data.Dataset with the given options set.", "The options are \"global\" in the sense they apply to the entire dataset. If options are set multiple times, they are merged as long as different options do not use different non-default values.", "View source", "Creates a Dataset by zipping together the given datasets.", "This method has similar semantics to the built-in zip() function in Python, with the main difference being that the datasets argument can be an arbitrary nested structure of Dataset objects.", "View source", "View source", "Creates an iterator for elements of this dataset.", "The returned iterator implements the Python Iterator protocol.", "View source", "Returns the length of the dataset if it is known and finite.", "This method requires that you are running in eager mode, and that the length of the dataset is known and non-infinite. When the length may be unknown or infinite, or if you are running in graph mode, use tf.data.Dataset.cardinality instead.", "View source"]}, {"name": "tf.compat.v1.data.experimental", "path": "compat/v1/data/experimental", "type": "tf.compat", "text": ["Experimental API for building input pipelines.", "This module contains experimental Dataset sources and transformations that can be used in conjunction with the tf.data.Dataset API. Note that the tf.data.experimental API is not subject to the same backwards compatibility guarantees as tf.data, but we will provide deprecation advice in advance of removing existing functionality.", "See Importing Data for an overview.", "service module: API for using the tf.data service.", "class AutoShardPolicy: Represents the type of auto-sharding we enable.", "class CheckpointInputPipelineHook: Checkpoints input pipeline state every N steps or seconds.", "class CsvDataset: A Dataset comprising lines from one or more CSV files.", "class DatasetStructure: Type specification for tf.data.Dataset.", "class DistributeOptions: Represents options for distributed data processing.", "class MapVectorizationOptions: Represents options for the MapVectorization optimization.", "class OptimizationOptions: Represents options for dataset optimizations.", "class Optional: Represents a value that may or may not be present.", "class OptionalStructure: Type specification for tf.experimental.Optional.", "class RandomDataset: A Dataset of pseudorandom values.", "class Reducer: A reducer is used for reducing a set of elements.", "class SqlDataset: A Dataset consisting of the results from a SQL query.", "class StatsAggregator: A stateful resource that aggregates statistics from one or more iterators.", "class StatsOptions: Represents options for collecting dataset stats using StatsAggregator.", "class Structure: Specifies a TensorFlow value type.", "class TFRecordWriter: Writes a dataset to a TFRecord file.", "class ThreadingOptions: Represents options for dataset threading.", "Counter(...): Creates a Dataset that counts from start in steps of size step.", "RaggedTensorStructure(...): DEPRECATED FUNCTION", "SparseTensorStructure(...): DEPRECATED FUNCTION", "TensorArrayStructure(...): DEPRECATED FUNCTION", "TensorStructure(...): DEPRECATED FUNCTION", "assert_cardinality(...): Asserts the cardinality of the input dataset.", "bucket_by_sequence_length(...): A transformation that buckets elements in a Dataset by length.", "bytes_produced_stats(...): Records the number of bytes produced by each element of the input dataset.", "cardinality(...): Returns the cardinality of dataset, if known.", "choose_from_datasets(...): Creates a dataset that deterministically chooses elements from datasets.", "copy_to_device(...): A transformation that copies dataset elements to the given target_device.", "dense_to_ragged_batch(...): A transformation that batches ragged elements into tf.RaggedTensors.", "dense_to_sparse_batch(...): A transformation that batches ragged elements into tf.sparse.SparseTensors.", "enumerate_dataset(...): A transformation that enumerates the elements of a dataset. (deprecated)", "from_variant(...): Constructs a dataset from the given variant and structure.", "get_next_as_optional(...): Returns a tf.experimental.Optional with the next element of the iterator. (deprecated)", "get_single_element(...): Returns the single element in dataset as a nested structure of tensors.", "get_structure(...): Returns the type signature for elements of the input dataset / iterator.", "group_by_reducer(...): A transformation that groups elements and performs a reduction.", "group_by_window(...): A transformation that groups windows of elements by key and reduces them.", "ignore_errors(...): Creates a Dataset from another Dataset and silently ignores any errors.", "latency_stats(...): Records the latency of producing each element of the input dataset.", "make_batched_features_dataset(...): Returns a Dataset of feature dictionaries from Example protos.", "make_csv_dataset(...): Reads CSV files into a dataset.", "make_saveable_from_iterator(...): Returns a SaveableObject for saving/restoring iterator state using Saver. (deprecated)", "map_and_batch(...): Fused implementation of map and batch. (deprecated)", "map_and_batch_with_legacy_function(...): Fused implementation of map and batch. (deprecated)", "parallel_interleave(...): A parallel version of the Dataset.interleave() transformation. (deprecated)", "parse_example_dataset(...): A transformation that parses Example protos into a dict of tensors.", "prefetch_to_device(...): A transformation that prefetches dataset values to the given device.", "rejection_resample(...): A transformation that resamples a dataset to achieve a target distribution.", "sample_from_datasets(...): Samples elements at random from the datasets in datasets.", "scan(...): A transformation that scans a function across an input dataset.", "shuffle_and_repeat(...): Shuffles and repeats a Dataset, reshuffling with each repetition. (deprecated)", "snapshot(...): API to persist the output of the input dataset.", "take_while(...): A transformation that stops dataset iteration based on a predicate.", "to_variant(...): Returns a variant representing the given dataset.", "unbatch(...): Splits elements of a dataset into multiple elements on the batch dimension. (deprecated)", "unique(...): Creates a Dataset from another Dataset, discarding duplicates."]}, {"name": "tf.compat.v1.data.experimental.choose_from_datasets", "path": "compat/v1/data/experimental/choose_from_datasets", "type": "tf.compat", "text": ["Creates a dataset that deterministically chooses elements from datasets.", "For example, given the following datasets:", "The elements of result will be:"]}, {"name": "tf.compat.v1.data.experimental.Counter", "path": "compat/v1/data/experimental/counter", "type": "tf.compat", "text": ["Creates a Dataset that counts from start in steps of size step."]}, {"name": "tf.compat.v1.data.experimental.CsvDataset", "path": "compat/v1/data/experimental/csvdataset", "type": "tf.compat", "text": ["A Dataset comprising lines from one or more CSV files.", "Inherits From: Dataset, Dataset", "View source", "Applies a transformation function to this dataset.", "apply enables chaining of custom Dataset transformations, which are represented as functions that take one Dataset argument and return a transformed Dataset.", "View source", "Returns an iterator which converts all elements of the dataset to numpy.", "Use as_numpy_iterator to inspect the content of your dataset. To see element shapes and types, print dataset elements directly instead of using as_numpy_iterator.", "This method requires that you are running in eager mode and the dataset's element_spec contains only TensorSpec components.", "as_numpy_iterator() will preserve the nested structure of dataset elements.", "View source", "Combines consecutive elements of this dataset into batches.", "The components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced.", "View source", "Caches the elements in this dataset.", "The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.", "When caching to a file, the cached data will persist across runs. Even the first iteration through the data will read from the cache file. Changing the input pipeline before the call to .cache() will have no effect until the cache file is removed or the filename is changed.", "View source", "Returns the cardinality of the dataset, if known.", "cardinality may return tf.data.INFINITE_CARDINALITY if the dataset contains an infinite number of elements or tf.data.UNKNOWN_CARDINALITY if the analysis fails to determine the number of elements in the dataset (e.g. when the dataset source is a file).", "View source", "Creates a Dataset by concatenating the given dataset with this dataset.", "View source", "Enumerates the elements of this dataset.", "It is similar to python's enumerate.", "View source", "Filters this dataset according to predicate.", "View source", "Filters this dataset according to predicate. (deprecated)", "View source", "Maps map_func across this dataset and flattens the result.", "Use flat_map if you want to make sure that the order of your dataset stays the same. For example, to flatten a dataset of batches into a dataset of their elements:", "tf.data.Dataset.interleave() is a generalization of flat_map, since flat_map produces the same output as tf.data.Dataset.interleave(cycle_length=1)", "View source", "Creates a Dataset whose elements are generated by generator. (deprecated arguments)", "The generator argument must be a callable object that returns an object that supports the iter() protocol (e.g. a generator function).", "The elements generated by generator must be compatible with either the given output_signature argument or with the given output_types and (optionally) output_shapes arguments, whichiver was specified.", "The recommended way to call from_generator is to use the output_signature argument. In this case the output will be assumed to consist of objects with the classes, shapes and types defined by tf.TypeSpec objects from output_signature argument:", "There is also a deprecated way to call from_generator by either with output_types argument alone or together with output_shapes argument. In this case the output of the function will be assumed to consist of tf.Tensor objects with with the types defined by output_types and with the shapes which are either unknown or defined by output_shapes.", "View source", "Splits each rank-N tf.sparse.SparseTensor in this dataset row-wise. (deprecated)", "View source", "Creates a Dataset whose elements are slices of the given tensors.", "The given tensors are sliced along their first dimension. This operation preserves the structure of the input tensors, removing the first dimension of each tensor and using it as the dataset dimension. All input tensors must have the same size in their first dimensions.", "Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.", "View source", "Creates a Dataset with a single element, comprising the given tensors.", "from_tensors produces a dataset containing only a single element. To slice the input tensor into multiple elements, use from_tensor_slices instead.", "Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.", "View source", "Maps map_func across this dataset, and interleaves the results.", "For example, you can use Dataset.interleave() to process many input files concurrently:", "The cycle_length and block_length arguments control the order in which elements are produced. cycle_length controls the number of input elements that are processed concurrently. If you set cycle_length to 1, this transformation will handle one input element at a time, and will produce identical results to tf.data.Dataset.flat_map. In general, this transformation will apply map_func to cycle_length input elements, open iterators on the returned Dataset objects, and cycle through them producing block_length consecutive elements from each iterator, and consuming the next input element each time it reaches the end of an iterator.", "Performance can often be improved by setting num_parallel_calls so that interleave will use multiple threads to fetch elements. If determinism isn't required, it can also improve performance to set deterministic=False.", "View source", "A dataset of all files matching one or more glob patterns.", "The file_pattern argument should be a small number of glob patterns. If your filenames have already been globbed, use Dataset.from_tensor_slices(filenames) instead, as re-globbing every filename with list_files may result in poor performance with remote storage systems.", "If we had the following files on our filesystem:", "If we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:", "View source", "Creates an iterator for elements of this dataset. (deprecated)", "View source", "Creates an iterator for elements of this dataset. (deprecated)", "View source", "Maps map_func across the elements of this dataset.", "This transformation applies map_func to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. map_func can be used to change both the values and the structure of a dataset's elements. For example, adding 1 to each element, or projecting a subset of element components.", "The input signature of map_func is determined by the structure of each element in this dataset.", "The value or values returned by map_func determine the structure of each element in the returned dataset.", "map_func can accept as arguments and return any type of dataset element.", "Note that irrespective of the context in which map_func is defined (eager vs. graph), tf.data traces the function and executes it as a graph. To use Python code inside of the function you have a few options:", "1) Rely on AutoGraph to convert Python code into an equivalent graph computation. The downside of this approach is that AutoGraph can convert some but not all Python code.", "2) Use tf.py_function, which allows you to write arbitrary Python code but will generally result in worse performance than 1). For example:", "3) Use tf.numpy_function, which also allows you to write arbitrary Python code. Note that tf.py_function accepts tf.Tensor whereas tf.numpy_function accepts numpy arrays and returns only numpy arrays. For example:", "Note that the use of tf.numpy_function and tf.py_function in general precludes the possibility of executing user-defined transformations in parallel (because of Python GIL).", "Performance can often be improved by setting num_parallel_calls so that map will use multiple threads to process elements. If deterministic order isn't required, it can also improve performance to set deterministic=False.", "View source", "Maps map_func across the elements of this dataset. (deprecated)", "View source", "Returns the options for this dataset and its inputs.", "View source", "Combines consecutive elements of this dataset into padded batches.", "This transformation combines multiple consecutive elements of the input dataset into a single element.", "Like tf.data.Dataset.batch, the components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced.", "Unlike tf.data.Dataset.batch, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in padded_shapes. The padded_shapes argument determines the resulting shape for each dimension of each component in an output element:", "See also tf.data.experimental.dense_to_sparse_batch, which combines elements that may have different shapes into a tf.sparse.SparseTensor.", "View source", "Creates a Dataset that prefetches elements from this dataset.", "Most dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.", "View source", "Creates a Dataset of a step-separated range of values.", "View source", "Reduces the input dataset to a single element.", "The transformation calls reduce_func successively on every element of the input dataset until the dataset is exhausted, aggregating information in its internal state. The initial_state argument is used for the initial state and the final state is returned as the result.", "View source", "Repeats this dataset so each original value is seen count times.", "View source", "Creates a Dataset that includes only 1/num_shards of this dataset.", "shard is deterministic. The Dataset produced by A.shard(n, i) will contain all elements of A whose index mod n = i.", "This dataset operator is very useful when running distributed training, as it allows each worker to read a unique subset.", "When reading a single input file, you can shard elements as follows:", "View source", "Randomly shuffles the elements of this dataset.", "This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.", "For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then shuffle will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer.", "reshuffle_each_iteration controls whether the shuffle order should be different for each epoch. In TF 1.X, the idiomatic way to create epochs was through the repeat transformation:", "In TF 2.0, tf.data.Dataset objects are Python iterables which makes it possible to also create epochs through Python iteration:", "View source", "Creates a Dataset that skips count elements from this dataset.", "View source", "Creates a Dataset with at most count elements from this dataset.", "View source", "Splits elements of a dataset into multiple elements.", "For example, if elements of the dataset are shaped [B, a0, a1, ...], where B may vary for each input element, then for each element in the dataset, the unbatched dataset will contain B consecutive elements of shape [a0, a1, ...].", "View source", "Combines (nests of) input elements into a dataset of (nests of) windows.", "A \"window\" is a finite dataset of flat elements of size size (or possibly fewer if there are not enough input elements to fill the window and drop_remainder evaluates to False).", "The shift argument determines the number of input elements by which the window moves on each iteration. If windows and elements are both numbered starting at 0, the first element in window k will be element k * shift of the input dataset. In particular, the first element of the first window will always be the first element of the input dataset.", "The stride argument determines the stride of the input elements, and the shift argument determines the shift of the window.", "Note that when the window transformation is applied to a dataset of nested elements, it produces a dataset of nested windows.", "View source", "Returns a new tf.data.Dataset with the given options set.", "The options are \"global\" in the sense they apply to the entire dataset. If options are set multiple times, they are merged as long as different options do not use different non-default values.", "View source", "Creates a Dataset by zipping together the given datasets.", "This method has similar semantics to the built-in zip() function in Python, with the main difference being that the datasets argument can be an arbitrary nested structure of Dataset objects.", "View source", "View source", "Creates an iterator for elements of this dataset.", "The returned iterator implements the Python Iterator protocol.", "View source", "Returns the length of the dataset if it is known and finite.", "This method requires that you are running in eager mode, and that the length of the dataset is known and non-infinite. When the length may be unknown or infinite, or if you are running in graph mode, use tf.data.Dataset.cardinality instead.", "View source"]}, {"name": "tf.compat.v1.data.experimental.make_batched_features_dataset", "path": "compat/v1/data/experimental/make_batched_features_dataset", "type": "tf.compat", "text": ["Returns a Dataset of feature dictionaries from Example protos.", "If label_key argument is provided, returns a Dataset of tuple comprising of feature dictionaries and label.", "And the expected output is:"]}, {"name": "tf.compat.v1.data.experimental.make_csv_dataset", "path": "compat/v1/data/experimental/make_csv_dataset", "type": "tf.compat", "text": ["Reads CSV files into a dataset.", "Reads CSV files into a dataset, where each element is a (features, labels) tuple that corresponds to a batch of CSV rows. The features dictionary maps feature column names to Tensors containing the corresponding feature data, and labels is a Tensor containing the batch's label data."]}, {"name": "tf.compat.v1.data.experimental.map_and_batch_with_legacy_function", "path": "compat/v1/data/experimental/map_and_batch_with_legacy_function", "type": "tf.compat", "text": ["Fused implementation of map and batch. (deprecated)"]}, {"name": "tf.compat.v1.data.experimental.RaggedTensorStructure", "path": "compat/v1/data/experimental/raggedtensorstructure", "type": "tf.compat", "text": ["DEPRECATED FUNCTION"]}, {"name": "tf.compat.v1.data.experimental.RandomDataset", "path": "compat/v1/data/experimental/randomdataset", "type": "tf.compat", "text": ["A Dataset of pseudorandom values.", "Inherits From: Dataset, Dataset", "View source", "Applies a transformation function to this dataset.", "apply enables chaining of custom Dataset transformations, which are represented as functions that take one Dataset argument and return a transformed Dataset.", "View source", "Returns an iterator which converts all elements of the dataset to numpy.", "Use as_numpy_iterator to inspect the content of your dataset. To see element shapes and types, print dataset elements directly instead of using as_numpy_iterator.", "This method requires that you are running in eager mode and the dataset's element_spec contains only TensorSpec components.", "as_numpy_iterator() will preserve the nested structure of dataset elements.", "View source", "Combines consecutive elements of this dataset into batches.", "The components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced.", "View source", "Caches the elements in this dataset.", "The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.", "When caching to a file, the cached data will persist across runs. Even the first iteration through the data will read from the cache file. Changing the input pipeline before the call to .cache() will have no effect until the cache file is removed or the filename is changed.", "View source", "Returns the cardinality of the dataset, if known.", "cardinality may return tf.data.INFINITE_CARDINALITY if the dataset contains an infinite number of elements or tf.data.UNKNOWN_CARDINALITY if the analysis fails to determine the number of elements in the dataset (e.g. when the dataset source is a file).", "View source", "Creates a Dataset by concatenating the given dataset with this dataset.", "View source", "Enumerates the elements of this dataset.", "It is similar to python's enumerate.", "View source", "Filters this dataset according to predicate.", "View source", "Filters this dataset according to predicate. (deprecated)", "View source", "Maps map_func across this dataset and flattens the result.", "Use flat_map if you want to make sure that the order of your dataset stays the same. For example, to flatten a dataset of batches into a dataset of their elements:", "tf.data.Dataset.interleave() is a generalization of flat_map, since flat_map produces the same output as tf.data.Dataset.interleave(cycle_length=1)", "View source", "Creates a Dataset whose elements are generated by generator. (deprecated arguments)", "The generator argument must be a callable object that returns an object that supports the iter() protocol (e.g. a generator function).", "The elements generated by generator must be compatible with either the given output_signature argument or with the given output_types and (optionally) output_shapes arguments, whichiver was specified.", "The recommended way to call from_generator is to use the output_signature argument. In this case the output will be assumed to consist of objects with the classes, shapes and types defined by tf.TypeSpec objects from output_signature argument:", "There is also a deprecated way to call from_generator by either with output_types argument alone or together with output_shapes argument. In this case the output of the function will be assumed to consist of tf.Tensor objects with with the types defined by output_types and with the shapes which are either unknown or defined by output_shapes.", "View source", "Splits each rank-N tf.sparse.SparseTensor in this dataset row-wise. (deprecated)", "View source", "Creates a Dataset whose elements are slices of the given tensors.", "The given tensors are sliced along their first dimension. This operation preserves the structure of the input tensors, removing the first dimension of each tensor and using it as the dataset dimension. All input tensors must have the same size in their first dimensions.", "Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.", "View source", "Creates a Dataset with a single element, comprising the given tensors.", "from_tensors produces a dataset containing only a single element. To slice the input tensor into multiple elements, use from_tensor_slices instead.", "Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.", "View source", "Maps map_func across this dataset, and interleaves the results.", "For example, you can use Dataset.interleave() to process many input files concurrently:", "The cycle_length and block_length arguments control the order in which elements are produced. cycle_length controls the number of input elements that are processed concurrently. If you set cycle_length to 1, this transformation will handle one input element at a time, and will produce identical results to tf.data.Dataset.flat_map. In general, this transformation will apply map_func to cycle_length input elements, open iterators on the returned Dataset objects, and cycle through them producing block_length consecutive elements from each iterator, and consuming the next input element each time it reaches the end of an iterator.", "Performance can often be improved by setting num_parallel_calls so that interleave will use multiple threads to fetch elements. If determinism isn't required, it can also improve performance to set deterministic=False.", "View source", "A dataset of all files matching one or more glob patterns.", "The file_pattern argument should be a small number of glob patterns. If your filenames have already been globbed, use Dataset.from_tensor_slices(filenames) instead, as re-globbing every filename with list_files may result in poor performance with remote storage systems.", "If we had the following files on our filesystem:", "If we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:", "View source", "Creates an iterator for elements of this dataset. (deprecated)", "View source", "Creates an iterator for elements of this dataset. (deprecated)", "View source", "Maps map_func across the elements of this dataset.", "This transformation applies map_func to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. map_func can be used to change both the values and the structure of a dataset's elements. For example, adding 1 to each element, or projecting a subset of element components.", "The input signature of map_func is determined by the structure of each element in this dataset.", "The value or values returned by map_func determine the structure of each element in the returned dataset.", "map_func can accept as arguments and return any type of dataset element.", "Note that irrespective of the context in which map_func is defined (eager vs. graph), tf.data traces the function and executes it as a graph. To use Python code inside of the function you have a few options:", "1) Rely on AutoGraph to convert Python code into an equivalent graph computation. The downside of this approach is that AutoGraph can convert some but not all Python code.", "2) Use tf.py_function, which allows you to write arbitrary Python code but will generally result in worse performance than 1). For example:", "3) Use tf.numpy_function, which also allows you to write arbitrary Python code. Note that tf.py_function accepts tf.Tensor whereas tf.numpy_function accepts numpy arrays and returns only numpy arrays. For example:", "Note that the use of tf.numpy_function and tf.py_function in general precludes the possibility of executing user-defined transformations in parallel (because of Python GIL).", "Performance can often be improved by setting num_parallel_calls so that map will use multiple threads to process elements. If deterministic order isn't required, it can also improve performance to set deterministic=False.", "View source", "Maps map_func across the elements of this dataset. (deprecated)", "View source", "Returns the options for this dataset and its inputs.", "View source", "Combines consecutive elements of this dataset into padded batches.", "This transformation combines multiple consecutive elements of the input dataset into a single element.", "Like tf.data.Dataset.batch, the components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced.", "Unlike tf.data.Dataset.batch, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in padded_shapes. The padded_shapes argument determines the resulting shape for each dimension of each component in an output element:", "See also tf.data.experimental.dense_to_sparse_batch, which combines elements that may have different shapes into a tf.sparse.SparseTensor.", "View source", "Creates a Dataset that prefetches elements from this dataset.", "Most dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.", "View source", "Creates a Dataset of a step-separated range of values.", "View source", "Reduces the input dataset to a single element.", "The transformation calls reduce_func successively on every element of the input dataset until the dataset is exhausted, aggregating information in its internal state. The initial_state argument is used for the initial state and the final state is returned as the result.", "View source", "Repeats this dataset so each original value is seen count times.", "View source", "Creates a Dataset that includes only 1/num_shards of this dataset.", "shard is deterministic. The Dataset produced by A.shard(n, i) will contain all elements of A whose index mod n = i.", "This dataset operator is very useful when running distributed training, as it allows each worker to read a unique subset.", "When reading a single input file, you can shard elements as follows:", "View source", "Randomly shuffles the elements of this dataset.", "This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.", "For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then shuffle will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer.", "reshuffle_each_iteration controls whether the shuffle order should be different for each epoch. In TF 1.X, the idiomatic way to create epochs was through the repeat transformation:", "In TF 2.0, tf.data.Dataset objects are Python iterables which makes it possible to also create epochs through Python iteration:", "View source", "Creates a Dataset that skips count elements from this dataset.", "View source", "Creates a Dataset with at most count elements from this dataset.", "View source", "Splits elements of a dataset into multiple elements.", "For example, if elements of the dataset are shaped [B, a0, a1, ...], where B may vary for each input element, then for each element in the dataset, the unbatched dataset will contain B consecutive elements of shape [a0, a1, ...].", "View source", "Combines (nests of) input elements into a dataset of (nests of) windows.", "A \"window\" is a finite dataset of flat elements of size size (or possibly fewer if there are not enough input elements to fill the window and drop_remainder evaluates to False).", "The shift argument determines the number of input elements by which the window moves on each iteration. If windows and elements are both numbered starting at 0, the first element in window k will be element k * shift of the input dataset. In particular, the first element of the first window will always be the first element of the input dataset.", "The stride argument determines the stride of the input elements, and the shift argument determines the shift of the window.", "Note that when the window transformation is applied to a dataset of nested elements, it produces a dataset of nested windows.", "View source", "Returns a new tf.data.Dataset with the given options set.", "The options are \"global\" in the sense they apply to the entire dataset. If options are set multiple times, they are merged as long as different options do not use different non-default values.", "View source", "Creates a Dataset by zipping together the given datasets.", "This method has similar semantics to the built-in zip() function in Python, with the main difference being that the datasets argument can be an arbitrary nested structure of Dataset objects.", "View source", "View source", "Creates an iterator for elements of this dataset.", "The returned iterator implements the Python Iterator protocol.", "View source", "Returns the length of the dataset if it is known and finite.", "This method requires that you are running in eager mode, and that the length of the dataset is known and non-infinite. When the length may be unknown or infinite, or if you are running in graph mode, use tf.data.Dataset.cardinality instead.", "View source"]}, {"name": "tf.compat.v1.data.experimental.sample_from_datasets", "path": "compat/v1/data/experimental/sample_from_datasets", "type": "tf.compat", "text": ["Samples elements at random from the datasets in datasets."]}, {"name": "tf.compat.v1.data.experimental.service", "path": "compat/v1/data/experimental/service", "type": "tf.compat", "text": ["API for using the tf.data service.", "The tf.data service offers a way to improve training speed when the host attached to a training device can't keep up with the data consumption of the model. For example, suppose a host can generate 100 examples/second, but the model can process 200 examples/second. Training speed could be doubled by using the tf.data service to generate 200 examples/second.", "There are a few things to do before using the tf.data service to speed up training.", "The tf.data service uses a cluster of workers to prepare data for training your model. The processing_mode argument to tf.data.experimental.service.distribute describes how to leverage multiple workers to process the input dataset. Currently, there are two processing modes to choose from: \"distributed_epoch\" and \"parallel_epochs\".", "\"distributed_epoch\" means that the dataset will be split across all tf.data service workers. The dispatcher produces \"splits\" for the dataset and sends them to workers for further processing. For example, if a dataset begins with a list of filenames, the dispatcher will iterate through the filenames and send the filenames to tf.data workers, which will perform the rest of the dataset transformations on those files. \"distributed_epoch\" is useful when your model needs to see each element of the dataset exactly once, or if it needs to see the data in a generally-sequential order. \"distributed_epoch\" only works for datasets with splittable sources, such as Dataset.from_tensor_slices, Dataset.list_files, or Dataset.range.", "\"parallel_epochs\" means that the entire input dataset will be processed independently by each of the tf.data service workers. For this reason, it is important to shuffle data (e.g. filenames) non-deterministically, so that each worker will process the elements of the dataset in a different order. \"parallel_epochs\" can be used to distribute datasets that aren't splittable.", "Before using the tf.data service, it is useful to first measure the potential performance improvement. To do this, add", "at the end of your dataset, and see how it affects your model's step time. take(1).cache().repeat() will cache the first element of your dataset and produce it repeatedly. This should make the dataset very fast, so that the model becomes the bottleneck and you can identify the ideal model speed. With enough workers, the tf.data service should be able to achieve similar speed.", "tf.data servers should be brought up alongside your training jobs, and brought down when the jobs are finished. The tf.data service uses one DispatchServer and any number of WorkerServers. See https://github.com/tensorflow/ecosystem/tree/master/data_service for an example of using Google Kubernetes Engine (GKE) to manage the tf.data service. The server implementation in tf_std_data_server.py is not GKE-specific, and can be used to run the tf.data service in other contexts.", "By default, the tf.data dispatch server stores its state in-memory, making it a single point of failure during training. To avoid this, pass fault_tolerant_mode=True when creating your DispatchServer. Dispatcher fault tolerance requires work_dir to be configured and accessible from the dispatcher both before and after restart (e.g. a GCS path). With fault tolerant mode enabled, the dispatcher will journal its state to the work directory so that no state is lost when the dispatcher is restarted.", "WorkerServers may be freely restarted, added, or removed during training. At startup, workers will register with the dispatcher and begin processing all outstanding jobs from the beginning.", "Once you have a tf.data service cluster running, take note of the dispatcher IP address and port. To connect to the service, you will use a string in the format \"grpc://:\".", "Below is a toy example that you can run yourself.", "See the documentation of tf.data.experimental.service.distribute for more details about using the distribute transformation.", "class DispatcherConfig: Configuration class for tf.data service dispatchers.", "class WorkerConfig: Configuration class for tf.data service dispatchers.", "distribute(...): A transformation that moves dataset processing to the tf.data service.", "from_dataset_id(...): Creates a dataset which reads data from the tf.data service.", "register_dataset(...): Registers a dataset with the tf.data service."]}, {"name": "tf.compat.v1.data.experimental.SparseTensorStructure", "path": "compat/v1/data/experimental/sparsetensorstructure", "type": "tf.compat", "text": ["DEPRECATED FUNCTION"]}, {"name": "tf.compat.v1.data.experimental.SqlDataset", "path": "compat/v1/data/experimental/sqldataset", "type": "tf.compat", "text": ["A Dataset consisting of the results from a SQL query.", "Inherits From: Dataset, Dataset", "View source", "Applies a transformation function to this dataset.", "apply enables chaining of custom Dataset transformations, which are represented as functions that take one Dataset argument and return a transformed Dataset.", "View source", "Returns an iterator which converts all elements of the dataset to numpy.", "Use as_numpy_iterator to inspect the content of your dataset. To see element shapes and types, print dataset elements directly instead of using as_numpy_iterator.", "This method requires that you are running in eager mode and the dataset's element_spec contains only TensorSpec components.", "as_numpy_iterator() will preserve the nested structure of dataset elements.", "View source", "Combines consecutive elements of this dataset into batches.", "The components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced.", "View source", "Caches the elements in this dataset.", "The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.", "When caching to a file, the cached data will persist across runs. Even the first iteration through the data will read from the cache file. Changing the input pipeline before the call to .cache() will have no effect until the cache file is removed or the filename is changed.", "View source", "Returns the cardinality of the dataset, if known.", "cardinality may return tf.data.INFINITE_CARDINALITY if the dataset contains an infinite number of elements or tf.data.UNKNOWN_CARDINALITY if the analysis fails to determine the number of elements in the dataset (e.g. when the dataset source is a file).", "View source", "Creates a Dataset by concatenating the given dataset with this dataset.", "View source", "Enumerates the elements of this dataset.", "It is similar to python's enumerate.", "View source", "Filters this dataset according to predicate.", "View source", "Filters this dataset according to predicate. (deprecated)", "View source", "Maps map_func across this dataset and flattens the result.", "Use flat_map if you want to make sure that the order of your dataset stays the same. For example, to flatten a dataset of batches into a dataset of their elements:", "tf.data.Dataset.interleave() is a generalization of flat_map, since flat_map produces the same output as tf.data.Dataset.interleave(cycle_length=1)", "View source", "Creates a Dataset whose elements are generated by generator. (deprecated arguments)", "The generator argument must be a callable object that returns an object that supports the iter() protocol (e.g. a generator function).", "The elements generated by generator must be compatible with either the given output_signature argument or with the given output_types and (optionally) output_shapes arguments, whichiver was specified.", "The recommended way to call from_generator is to use the output_signature argument. In this case the output will be assumed to consist of objects with the classes, shapes and types defined by tf.TypeSpec objects from output_signature argument:", "There is also a deprecated way to call from_generator by either with output_types argument alone or together with output_shapes argument. In this case the output of the function will be assumed to consist of tf.Tensor objects with with the types defined by output_types and with the shapes which are either unknown or defined by output_shapes.", "View source", "Splits each rank-N tf.sparse.SparseTensor in this dataset row-wise. (deprecated)", "View source", "Creates a Dataset whose elements are slices of the given tensors.", "The given tensors are sliced along their first dimension. This operation preserves the structure of the input tensors, removing the first dimension of each tensor and using it as the dataset dimension. All input tensors must have the same size in their first dimensions.", "Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.", "View source", "Creates a Dataset with a single element, comprising the given tensors.", "from_tensors produces a dataset containing only a single element. To slice the input tensor into multiple elements, use from_tensor_slices instead.", "Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.", "View source", "Maps map_func across this dataset, and interleaves the results.", "For example, you can use Dataset.interleave() to process many input files concurrently:", "The cycle_length and block_length arguments control the order in which elements are produced. cycle_length controls the number of input elements that are processed concurrently. If you set cycle_length to 1, this transformation will handle one input element at a time, and will produce identical results to tf.data.Dataset.flat_map. In general, this transformation will apply map_func to cycle_length input elements, open iterators on the returned Dataset objects, and cycle through them producing block_length consecutive elements from each iterator, and consuming the next input element each time it reaches the end of an iterator.", "Performance can often be improved by setting num_parallel_calls so that interleave will use multiple threads to fetch elements. If determinism isn't required, it can also improve performance to set deterministic=False.", "View source", "A dataset of all files matching one or more glob patterns.", "The file_pattern argument should be a small number of glob patterns. If your filenames have already been globbed, use Dataset.from_tensor_slices(filenames) instead, as re-globbing every filename with list_files may result in poor performance with remote storage systems.", "If we had the following files on our filesystem:", "If we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:", "View source", "Creates an iterator for elements of this dataset. (deprecated)", "View source", "Creates an iterator for elements of this dataset. (deprecated)", "View source", "Maps map_func across the elements of this dataset.", "This transformation applies map_func to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. map_func can be used to change both the values and the structure of a dataset's elements. For example, adding 1 to each element, or projecting a subset of element components.", "The input signature of map_func is determined by the structure of each element in this dataset.", "The value or values returned by map_func determine the structure of each element in the returned dataset.", "map_func can accept as arguments and return any type of dataset element.", "Note that irrespective of the context in which map_func is defined (eager vs. graph), tf.data traces the function and executes it as a graph. To use Python code inside of the function you have a few options:", "1) Rely on AutoGraph to convert Python code into an equivalent graph computation. The downside of this approach is that AutoGraph can convert some but not all Python code.", "2) Use tf.py_function, which allows you to write arbitrary Python code but will generally result in worse performance than 1). For example:", "3) Use tf.numpy_function, which also allows you to write arbitrary Python code. Note that tf.py_function accepts tf.Tensor whereas tf.numpy_function accepts numpy arrays and returns only numpy arrays. For example:", "Note that the use of tf.numpy_function and tf.py_function in general precludes the possibility of executing user-defined transformations in parallel (because of Python GIL).", "Performance can often be improved by setting num_parallel_calls so that map will use multiple threads to process elements. If deterministic order isn't required, it can also improve performance to set deterministic=False.", "View source", "Maps map_func across the elements of this dataset. (deprecated)", "View source", "Returns the options for this dataset and its inputs.", "View source", "Combines consecutive elements of this dataset into padded batches.", "This transformation combines multiple consecutive elements of the input dataset into a single element.", "Like tf.data.Dataset.batch, the components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced.", "Unlike tf.data.Dataset.batch, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in padded_shapes. The padded_shapes argument determines the resulting shape for each dimension of each component in an output element:", "See also tf.data.experimental.dense_to_sparse_batch, which combines elements that may have different shapes into a tf.sparse.SparseTensor.", "View source", "Creates a Dataset that prefetches elements from this dataset.", "Most dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.", "View source", "Creates a Dataset of a step-separated range of values.", "View source", "Reduces the input dataset to a single element.", "The transformation calls reduce_func successively on every element of the input dataset until the dataset is exhausted, aggregating information in its internal state. The initial_state argument is used for the initial state and the final state is returned as the result.", "View source", "Repeats this dataset so each original value is seen count times.", "View source", "Creates a Dataset that includes only 1/num_shards of this dataset.", "shard is deterministic. The Dataset produced by A.shard(n, i) will contain all elements of A whose index mod n = i.", "This dataset operator is very useful when running distributed training, as it allows each worker to read a unique subset.", "When reading a single input file, you can shard elements as follows:", "View source", "Randomly shuffles the elements of this dataset.", "This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.", "For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then shuffle will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer.", "reshuffle_each_iteration controls whether the shuffle order should be different for each epoch. In TF 1.X, the idiomatic way to create epochs was through the repeat transformation:", "In TF 2.0, tf.data.Dataset objects are Python iterables which makes it possible to also create epochs through Python iteration:", "View source", "Creates a Dataset that skips count elements from this dataset.", "View source", "Creates a Dataset with at most count elements from this dataset.", "View source", "Splits elements of a dataset into multiple elements.", "For example, if elements of the dataset are shaped [B, a0, a1, ...], where B may vary for each input element, then for each element in the dataset, the unbatched dataset will contain B consecutive elements of shape [a0, a1, ...].", "View source", "Combines (nests of) input elements into a dataset of (nests of) windows.", "A \"window\" is a finite dataset of flat elements of size size (or possibly fewer if there are not enough input elements to fill the window and drop_remainder evaluates to False).", "The shift argument determines the number of input elements by which the window moves on each iteration. If windows and elements are both numbered starting at 0, the first element in window k will be element k * shift of the input dataset. In particular, the first element of the first window will always be the first element of the input dataset.", "The stride argument determines the stride of the input elements, and the shift argument determines the shift of the window.", "Note that when the window transformation is applied to a dataset of nested elements, it produces a dataset of nested windows.", "View source", "Returns a new tf.data.Dataset with the given options set.", "The options are \"global\" in the sense they apply to the entire dataset. If options are set multiple times, they are merged as long as different options do not use different non-default values.", "View source", "Creates a Dataset by zipping together the given datasets.", "This method has similar semantics to the built-in zip() function in Python, with the main difference being that the datasets argument can be an arbitrary nested structure of Dataset objects.", "View source", "View source", "Creates an iterator for elements of this dataset.", "The returned iterator implements the Python Iterator protocol.", "View source", "Returns the length of the dataset if it is known and finite.", "This method requires that you are running in eager mode, and that the length of the dataset is known and non-infinite. When the length may be unknown or infinite, or if you are running in graph mode, use tf.data.Dataset.cardinality instead.", "View source"]}, {"name": "tf.compat.v1.data.experimental.StatsAggregator", "path": "compat/v1/data/experimental/statsaggregator", "type": "tf.compat", "text": ["A stateful resource that aggregates statistics from one or more iterators.", "To record statistics, use one of the custom transformation functions defined in this module when defining your tf.data.Dataset. All statistics will be aggregated by the StatsAggregator that is associated with a particular iterator (see below). For example, to record the latency of producing each element by iterating over a dataset:", "To associate a StatsAggregator with a tf.data.Dataset object, use the following pattern:", "To get a protocol buffer summary of the currently aggregated statistics, use the StatsAggregator.get_summary() tensor. The easiest way to do this is to add the returned tensor to the tf.GraphKeys.SUMMARIES collection, so that the summaries will be included with any existing summaries.", "View source", "Returns a string tf.Tensor that summarizes the aggregated statistics.", "The returned tensor will contain a serialized tf.compat.v1.summary.Summary protocol buffer, which can be used with the standard TensorBoard logging facilities."]}, {"name": "tf.compat.v1.data.experimental.TensorArrayStructure", "path": "compat/v1/data/experimental/tensorarraystructure", "type": "tf.compat", "text": ["DEPRECATED FUNCTION"]}, {"name": "tf.compat.v1.data.experimental.TensorStructure", "path": "compat/v1/data/experimental/tensorstructure", "type": "tf.compat", "text": ["DEPRECATED FUNCTION"]}, {"name": "tf.compat.v1.data.FixedLengthRecordDataset", "path": "compat/v1/data/fixedlengthrecorddataset", "type": "tf.compat", "text": ["A Dataset of fixed-length records from one or more binary files.", "Inherits From: Dataset, Dataset", "View source", "Applies a transformation function to this dataset.", "apply enables chaining of custom Dataset transformations, which are represented as functions that take one Dataset argument and return a transformed Dataset.", "View source", "Returns an iterator which converts all elements of the dataset to numpy.", "Use as_numpy_iterator to inspect the content of your dataset. To see element shapes and types, print dataset elements directly instead of using as_numpy_iterator.", "This method requires that you are running in eager mode and the dataset's element_spec contains only TensorSpec components.", "as_numpy_iterator() will preserve the nested structure of dataset elements.", "View source", "Combines consecutive elements of this dataset into batches.", "The components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced.", "View source", "Caches the elements in this dataset.", "The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.", "When caching to a file, the cached data will persist across runs. Even the first iteration through the data will read from the cache file. Changing the input pipeline before the call to .cache() will have no effect until the cache file is removed or the filename is changed.", "View source", "Returns the cardinality of the dataset, if known.", "cardinality may return tf.data.INFINITE_CARDINALITY if the dataset contains an infinite number of elements or tf.data.UNKNOWN_CARDINALITY if the analysis fails to determine the number of elements in the dataset (e.g. when the dataset source is a file).", "View source", "Creates a Dataset by concatenating the given dataset with this dataset.", "View source", "Enumerates the elements of this dataset.", "It is similar to python's enumerate.", "View source", "Filters this dataset according to predicate.", "View source", "Filters this dataset according to predicate. (deprecated)", "View source", "Maps map_func across this dataset and flattens the result.", "Use flat_map if you want to make sure that the order of your dataset stays the same. For example, to flatten a dataset of batches into a dataset of their elements:", "tf.data.Dataset.interleave() is a generalization of flat_map, since flat_map produces the same output as tf.data.Dataset.interleave(cycle_length=1)", "View source", "Creates a Dataset whose elements are generated by generator. (deprecated arguments)", "The generator argument must be a callable object that returns an object that supports the iter() protocol (e.g. a generator function).", "The elements generated by generator must be compatible with either the given output_signature argument or with the given output_types and (optionally) output_shapes arguments, whichiver was specified.", "The recommended way to call from_generator is to use the output_signature argument. In this case the output will be assumed to consist of objects with the classes, shapes and types defined by tf.TypeSpec objects from output_signature argument:", "There is also a deprecated way to call from_generator by either with output_types argument alone or together with output_shapes argument. In this case the output of the function will be assumed to consist of tf.Tensor objects with with the types defined by output_types and with the shapes which are either unknown or defined by output_shapes.", "View source", "Splits each rank-N tf.sparse.SparseTensor in this dataset row-wise. (deprecated)", "View source", "Creates a Dataset whose elements are slices of the given tensors.", "The given tensors are sliced along their first dimension. This operation preserves the structure of the input tensors, removing the first dimension of each tensor and using it as the dataset dimension. All input tensors must have the same size in their first dimensions.", "Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.", "View source", "Creates a Dataset with a single element, comprising the given tensors.", "from_tensors produces a dataset containing only a single element. To slice the input tensor into multiple elements, use from_tensor_slices instead.", "Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.", "View source", "Maps map_func across this dataset, and interleaves the results.", "For example, you can use Dataset.interleave() to process many input files concurrently:", "The cycle_length and block_length arguments control the order in which elements are produced. cycle_length controls the number of input elements that are processed concurrently. If you set cycle_length to 1, this transformation will handle one input element at a time, and will produce identical results to tf.data.Dataset.flat_map. In general, this transformation will apply map_func to cycle_length input elements, open iterators on the returned Dataset objects, and cycle through them producing block_length consecutive elements from each iterator, and consuming the next input element each time it reaches the end of an iterator.", "Performance can often be improved by setting num_parallel_calls so that interleave will use multiple threads to fetch elements. If determinism isn't required, it can also improve performance to set deterministic=False.", "View source", "A dataset of all files matching one or more glob patterns.", "The file_pattern argument should be a small number of glob patterns. If your filenames have already been globbed, use Dataset.from_tensor_slices(filenames) instead, as re-globbing every filename with list_files may result in poor performance with remote storage systems.", "If we had the following files on our filesystem:", "If we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:", "View source", "Creates an iterator for elements of this dataset. (deprecated)", "View source", "Creates an iterator for elements of this dataset. (deprecated)", "View source", "Maps map_func across the elements of this dataset.", "This transformation applies map_func to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. map_func can be used to change both the values and the structure of a dataset's elements. For example, adding 1 to each element, or projecting a subset of element components.", "The input signature of map_func is determined by the structure of each element in this dataset.", "The value or values returned by map_func determine the structure of each element in the returned dataset.", "map_func can accept as arguments and return any type of dataset element.", "Note that irrespective of the context in which map_func is defined (eager vs. graph), tf.data traces the function and executes it as a graph. To use Python code inside of the function you have a few options:", "1) Rely on AutoGraph to convert Python code into an equivalent graph computation. The downside of this approach is that AutoGraph can convert some but not all Python code.", "2) Use tf.py_function, which allows you to write arbitrary Python code but will generally result in worse performance than 1). For example:", "3) Use tf.numpy_function, which also allows you to write arbitrary Python code. Note that tf.py_function accepts tf.Tensor whereas tf.numpy_function accepts numpy arrays and returns only numpy arrays. For example:", "Note that the use of tf.numpy_function and tf.py_function in general precludes the possibility of executing user-defined transformations in parallel (because of Python GIL).", "Performance can often be improved by setting num_parallel_calls so that map will use multiple threads to process elements. If deterministic order isn't required, it can also improve performance to set deterministic=False.", "View source", "Maps map_func across the elements of this dataset. (deprecated)", "View source", "Returns the options for this dataset and its inputs.", "View source", "Combines consecutive elements of this dataset into padded batches.", "This transformation combines multiple consecutive elements of the input dataset into a single element.", "Like tf.data.Dataset.batch, the components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced.", "Unlike tf.data.Dataset.batch, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in padded_shapes. The padded_shapes argument determines the resulting shape for each dimension of each component in an output element:", "See also tf.data.experimental.dense_to_sparse_batch, which combines elements that may have different shapes into a tf.sparse.SparseTensor.", "View source", "Creates a Dataset that prefetches elements from this dataset.", "Most dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.", "View source", "Creates a Dataset of a step-separated range of values.", "View source", "Reduces the input dataset to a single element.", "The transformation calls reduce_func successively on every element of the input dataset until the dataset is exhausted, aggregating information in its internal state. The initial_state argument is used for the initial state and the final state is returned as the result.", "View source", "Repeats this dataset so each original value is seen count times.", "View source", "Creates a Dataset that includes only 1/num_shards of this dataset.", "shard is deterministic. The Dataset produced by A.shard(n, i) will contain all elements of A whose index mod n = i.", "This dataset operator is very useful when running distributed training, as it allows each worker to read a unique subset.", "When reading a single input file, you can shard elements as follows:", "View source", "Randomly shuffles the elements of this dataset.", "This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.", "For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then shuffle will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer.", "reshuffle_each_iteration controls whether the shuffle order should be different for each epoch. In TF 1.X, the idiomatic way to create epochs was through the repeat transformation:", "In TF 2.0, tf.data.Dataset objects are Python iterables which makes it possible to also create epochs through Python iteration:", "View source", "Creates a Dataset that skips count elements from this dataset.", "View source", "Creates a Dataset with at most count elements from this dataset.", "View source", "Splits elements of a dataset into multiple elements.", "For example, if elements of the dataset are shaped [B, a0, a1, ...], where B may vary for each input element, then for each element in the dataset, the unbatched dataset will contain B consecutive elements of shape [a0, a1, ...].", "View source", "Combines (nests of) input elements into a dataset of (nests of) windows.", "A \"window\" is a finite dataset of flat elements of size size (or possibly fewer if there are not enough input elements to fill the window and drop_remainder evaluates to False).", "The shift argument determines the number of input elements by which the window moves on each iteration. If windows and elements are both numbered starting at 0, the first element in window k will be element k * shift of the input dataset. In particular, the first element of the first window will always be the first element of the input dataset.", "The stride argument determines the stride of the input elements, and the shift argument determines the shift of the window.", "Note that when the window transformation is applied to a dataset of nested elements, it produces a dataset of nested windows.", "View source", "Returns a new tf.data.Dataset with the given options set.", "The options are \"global\" in the sense they apply to the entire dataset. If options are set multiple times, they are merged as long as different options do not use different non-default values.", "View source", "Creates a Dataset by zipping together the given datasets.", "This method has similar semantics to the built-in zip() function in Python, with the main difference being that the datasets argument can be an arbitrary nested structure of Dataset objects.", "View source", "View source", "Creates an iterator for elements of this dataset.", "The returned iterator implements the Python Iterator protocol.", "View source", "Returns the length of the dataset if it is known and finite.", "This method requires that you are running in eager mode, and that the length of the dataset is known and non-infinite. When the length may be unknown or infinite, or if you are running in graph mode, use tf.data.Dataset.cardinality instead.", "View source"]}, {"name": "tf.compat.v1.data.get_output_classes", "path": "compat/v1/data/get_output_classes", "type": "tf.compat", "text": ["Returns the output classes for elements of the input dataset / iterator."]}, {"name": "tf.compat.v1.data.get_output_shapes", "path": "compat/v1/data/get_output_shapes", "type": "tf.compat", "text": ["Returns the output shapes for elements of the input dataset / iterator."]}, {"name": "tf.compat.v1.data.get_output_types", "path": "compat/v1/data/get_output_types", "type": "tf.compat", "text": ["Returns the output shapes for elements of the input dataset / iterator."]}, {"name": "tf.compat.v1.data.Iterator", "path": "compat/v1/data/iterator", "type": "tf.compat", "text": ["Represents the state of iterating through a Dataset.", "The expected values are tf.Tensor and tf.sparse.SparseTensor. ", "View source", "Creates a new, uninitialized Iterator based on the given handle.", "This method allows you to define a \"feedable\" iterator where you can choose between concrete iterators by feeding a value in a tf.Session.run call. In that case, string_handle would be a tf.compat.v1.placeholder, and you would feed it with the value of tf.data.Iterator.string_handle in each step.", "For example, if you had two iterators that marked the current position in a training dataset and a test dataset, you could choose which to use in each step as follows:", "View source", "Creates a new, uninitialized Iterator with the given structure.", "This iterator-constructing method can be used to create an iterator that is reusable with many different datasets.", "The returned iterator is not bound to a particular dataset, and it has no initializer. To initialize the iterator, run the operation returned by Iterator.make_initializer(dataset).", "The following is an example", "View source", "Returns a nested structure of tf.Tensors representing the next element.", "In graph mode, you should typically call this method once and use its result as the input to another computation. A typical loop will then call tf.Session.run on the result of that computation. The loop will terminate when the Iterator.get_next() operation raises tf.errors.OutOfRangeError. The following skeleton shows how to use this method when building a training loop:", "View source", "View source", "Returns a tf.Operation that initializes this iterator on dataset.", "View source", "Returns a string-valued tf.Tensor that represents this iterator."]}, {"name": "tf.compat.v1.data.make_initializable_iterator", "path": "compat/v1/data/make_initializable_iterator", "type": "tf.compat", "text": ["Creates an iterator for elements of dataset."]}, {"name": "tf.compat.v1.data.make_one_shot_iterator", "path": "compat/v1/data/make_one_shot_iterator", "type": "tf.compat", "text": ["Creates an iterator for elements of dataset."]}, {"name": "tf.compat.v1.data.TextLineDataset", "path": "compat/v1/data/textlinedataset", "type": "tf.compat", "text": ["A Dataset comprising lines from one or more text files.", "Inherits From: Dataset, Dataset", "View source", "Applies a transformation function to this dataset.", "apply enables chaining of custom Dataset transformations, which are represented as functions that take one Dataset argument and return a transformed Dataset.", "View source", "Returns an iterator which converts all elements of the dataset to numpy.", "Use as_numpy_iterator to inspect the content of your dataset. To see element shapes and types, print dataset elements directly instead of using as_numpy_iterator.", "This method requires that you are running in eager mode and the dataset's element_spec contains only TensorSpec components.", "as_numpy_iterator() will preserve the nested structure of dataset elements.", "View source", "Combines consecutive elements of this dataset into batches.", "The components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced.", "View source", "Caches the elements in this dataset.", "The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.", "When caching to a file, the cached data will persist across runs. Even the first iteration through the data will read from the cache file. Changing the input pipeline before the call to .cache() will have no effect until the cache file is removed or the filename is changed.", "View source", "Returns the cardinality of the dataset, if known.", "cardinality may return tf.data.INFINITE_CARDINALITY if the dataset contains an infinite number of elements or tf.data.UNKNOWN_CARDINALITY if the analysis fails to determine the number of elements in the dataset (e.g. when the dataset source is a file).", "View source", "Creates a Dataset by concatenating the given dataset with this dataset.", "View source", "Enumerates the elements of this dataset.", "It is similar to python's enumerate.", "View source", "Filters this dataset according to predicate.", "View source", "Filters this dataset according to predicate. (deprecated)", "View source", "Maps map_func across this dataset and flattens the result.", "Use flat_map if you want to make sure that the order of your dataset stays the same. For example, to flatten a dataset of batches into a dataset of their elements:", "tf.data.Dataset.interleave() is a generalization of flat_map, since flat_map produces the same output as tf.data.Dataset.interleave(cycle_length=1)", "View source", "Creates a Dataset whose elements are generated by generator. (deprecated arguments)", "The generator argument must be a callable object that returns an object that supports the iter() protocol (e.g. a generator function).", "The elements generated by generator must be compatible with either the given output_signature argument or with the given output_types and (optionally) output_shapes arguments, whichiver was specified.", "The recommended way to call from_generator is to use the output_signature argument. In this case the output will be assumed to consist of objects with the classes, shapes and types defined by tf.TypeSpec objects from output_signature argument:", "There is also a deprecated way to call from_generator by either with output_types argument alone or together with output_shapes argument. In this case the output of the function will be assumed to consist of tf.Tensor objects with with the types defined by output_types and with the shapes which are either unknown or defined by output_shapes.", "View source", "Splits each rank-N tf.sparse.SparseTensor in this dataset row-wise. (deprecated)", "View source", "Creates a Dataset whose elements are slices of the given tensors.", "The given tensors are sliced along their first dimension. This operation preserves the structure of the input tensors, removing the first dimension of each tensor and using it as the dataset dimension. All input tensors must have the same size in their first dimensions.", "Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.", "View source", "Creates a Dataset with a single element, comprising the given tensors.", "from_tensors produces a dataset containing only a single element. To slice the input tensor into multiple elements, use from_tensor_slices instead.", "Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.", "View source", "Maps map_func across this dataset, and interleaves the results.", "For example, you can use Dataset.interleave() to process many input files concurrently:", "The cycle_length and block_length arguments control the order in which elements are produced. cycle_length controls the number of input elements that are processed concurrently. If you set cycle_length to 1, this transformation will handle one input element at a time, and will produce identical results to tf.data.Dataset.flat_map. In general, this transformation will apply map_func to cycle_length input elements, open iterators on the returned Dataset objects, and cycle through them producing block_length consecutive elements from each iterator, and consuming the next input element each time it reaches the end of an iterator.", "Performance can often be improved by setting num_parallel_calls so that interleave will use multiple threads to fetch elements. If determinism isn't required, it can also improve performance to set deterministic=False.", "View source", "A dataset of all files matching one or more glob patterns.", "The file_pattern argument should be a small number of glob patterns. If your filenames have already been globbed, use Dataset.from_tensor_slices(filenames) instead, as re-globbing every filename with list_files may result in poor performance with remote storage systems.", "If we had the following files on our filesystem:", "If we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:", "View source", "Creates an iterator for elements of this dataset. (deprecated)", "View source", "Creates an iterator for elements of this dataset. (deprecated)", "View source", "Maps map_func across the elements of this dataset.", "This transformation applies map_func to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. map_func can be used to change both the values and the structure of a dataset's elements. For example, adding 1 to each element, or projecting a subset of element components.", "The input signature of map_func is determined by the structure of each element in this dataset.", "The value or values returned by map_func determine the structure of each element in the returned dataset.", "map_func can accept as arguments and return any type of dataset element.", "Note that irrespective of the context in which map_func is defined (eager vs. graph), tf.data traces the function and executes it as a graph. To use Python code inside of the function you have a few options:", "1) Rely on AutoGraph to convert Python code into an equivalent graph computation. The downside of this approach is that AutoGraph can convert some but not all Python code.", "2) Use tf.py_function, which allows you to write arbitrary Python code but will generally result in worse performance than 1). For example:", "3) Use tf.numpy_function, which also allows you to write arbitrary Python code. Note that tf.py_function accepts tf.Tensor whereas tf.numpy_function accepts numpy arrays and returns only numpy arrays. For example:", "Note that the use of tf.numpy_function and tf.py_function in general precludes the possibility of executing user-defined transformations in parallel (because of Python GIL).", "Performance can often be improved by setting num_parallel_calls so that map will use multiple threads to process elements. If deterministic order isn't required, it can also improve performance to set deterministic=False.", "View source", "Maps map_func across the elements of this dataset. (deprecated)", "View source", "Returns the options for this dataset and its inputs.", "View source", "Combines consecutive elements of this dataset into padded batches.", "This transformation combines multiple consecutive elements of the input dataset into a single element.", "Like tf.data.Dataset.batch, the components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced.", "Unlike tf.data.Dataset.batch, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in padded_shapes. The padded_shapes argument determines the resulting shape for each dimension of each component in an output element:", "See also tf.data.experimental.dense_to_sparse_batch, which combines elements that may have different shapes into a tf.sparse.SparseTensor.", "View source", "Creates a Dataset that prefetches elements from this dataset.", "Most dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.", "View source", "Creates a Dataset of a step-separated range of values.", "View source", "Reduces the input dataset to a single element.", "The transformation calls reduce_func successively on every element of the input dataset until the dataset is exhausted, aggregating information in its internal state. The initial_state argument is used for the initial state and the final state is returned as the result.", "View source", "Repeats this dataset so each original value is seen count times.", "View source", "Creates a Dataset that includes only 1/num_shards of this dataset.", "shard is deterministic. The Dataset produced by A.shard(n, i) will contain all elements of A whose index mod n = i.", "This dataset operator is very useful when running distributed training, as it allows each worker to read a unique subset.", "When reading a single input file, you can shard elements as follows:", "View source", "Randomly shuffles the elements of this dataset.", "This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.", "For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then shuffle will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer.", "reshuffle_each_iteration controls whether the shuffle order should be different for each epoch. In TF 1.X, the idiomatic way to create epochs was through the repeat transformation:", "In TF 2.0, tf.data.Dataset objects are Python iterables which makes it possible to also create epochs through Python iteration:", "View source", "Creates a Dataset that skips count elements from this dataset.", "View source", "Creates a Dataset with at most count elements from this dataset.", "View source", "Splits elements of a dataset into multiple elements.", "For example, if elements of the dataset are shaped [B, a0, a1, ...], where B may vary for each input element, then for each element in the dataset, the unbatched dataset will contain B consecutive elements of shape [a0, a1, ...].", "View source", "Combines (nests of) input elements into a dataset of (nests of) windows.", "A \"window\" is a finite dataset of flat elements of size size (or possibly fewer if there are not enough input elements to fill the window and drop_remainder evaluates to False).", "The shift argument determines the number of input elements by which the window moves on each iteration. If windows and elements are both numbered starting at 0, the first element in window k will be element k * shift of the input dataset. In particular, the first element of the first window will always be the first element of the input dataset.", "The stride argument determines the stride of the input elements, and the shift argument determines the shift of the window.", "Note that when the window transformation is applied to a dataset of nested elements, it produces a dataset of nested windows.", "View source", "Returns a new tf.data.Dataset with the given options set.", "The options are \"global\" in the sense they apply to the entire dataset. If options are set multiple times, they are merged as long as different options do not use different non-default values.", "View source", "Creates a Dataset by zipping together the given datasets.", "This method has similar semantics to the built-in zip() function in Python, with the main difference being that the datasets argument can be an arbitrary nested structure of Dataset objects.", "View source", "View source", "Creates an iterator for elements of this dataset.", "The returned iterator implements the Python Iterator protocol.", "View source", "Returns the length of the dataset if it is known and finite.", "This method requires that you are running in eager mode, and that the length of the dataset is known and non-infinite. When the length may be unknown or infinite, or if you are running in graph mode, use tf.data.Dataset.cardinality instead.", "View source"]}, {"name": "tf.compat.v1.data.TFRecordDataset", "path": "compat/v1/data/tfrecorddataset", "type": "tf.compat", "text": ["A Dataset comprising records from one or more TFRecord files.", "Inherits From: Dataset, Dataset", "View source", "Applies a transformation function to this dataset.", "apply enables chaining of custom Dataset transformations, which are represented as functions that take one Dataset argument and return a transformed Dataset.", "View source", "Returns an iterator which converts all elements of the dataset to numpy.", "Use as_numpy_iterator to inspect the content of your dataset. To see element shapes and types, print dataset elements directly instead of using as_numpy_iterator.", "This method requires that you are running in eager mode and the dataset's element_spec contains only TensorSpec components.", "as_numpy_iterator() will preserve the nested structure of dataset elements.", "View source", "Combines consecutive elements of this dataset into batches.", "The components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced.", "View source", "Caches the elements in this dataset.", "The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.", "When caching to a file, the cached data will persist across runs. Even the first iteration through the data will read from the cache file. Changing the input pipeline before the call to .cache() will have no effect until the cache file is removed or the filename is changed.", "View source", "Returns the cardinality of the dataset, if known.", "cardinality may return tf.data.INFINITE_CARDINALITY if the dataset contains an infinite number of elements or tf.data.UNKNOWN_CARDINALITY if the analysis fails to determine the number of elements in the dataset (e.g. when the dataset source is a file).", "View source", "Creates a Dataset by concatenating the given dataset with this dataset.", "View source", "Enumerates the elements of this dataset.", "It is similar to python's enumerate.", "View source", "Filters this dataset according to predicate.", "View source", "Filters this dataset according to predicate. (deprecated)", "View source", "Maps map_func across this dataset and flattens the result.", "Use flat_map if you want to make sure that the order of your dataset stays the same. For example, to flatten a dataset of batches into a dataset of their elements:", "tf.data.Dataset.interleave() is a generalization of flat_map, since flat_map produces the same output as tf.data.Dataset.interleave(cycle_length=1)", "View source", "Creates a Dataset whose elements are generated by generator. (deprecated arguments)", "The generator argument must be a callable object that returns an object that supports the iter() protocol (e.g. a generator function).", "The elements generated by generator must be compatible with either the given output_signature argument or with the given output_types and (optionally) output_shapes arguments, whichiver was specified.", "The recommended way to call from_generator is to use the output_signature argument. In this case the output will be assumed to consist of objects with the classes, shapes and types defined by tf.TypeSpec objects from output_signature argument:", "There is also a deprecated way to call from_generator by either with output_types argument alone or together with output_shapes argument. In this case the output of the function will be assumed to consist of tf.Tensor objects with with the types defined by output_types and with the shapes which are either unknown or defined by output_shapes.", "View source", "Splits each rank-N tf.sparse.SparseTensor in this dataset row-wise. (deprecated)", "View source", "Creates a Dataset whose elements are slices of the given tensors.", "The given tensors are sliced along their first dimension. This operation preserves the structure of the input tensors, removing the first dimension of each tensor and using it as the dataset dimension. All input tensors must have the same size in their first dimensions.", "Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.", "View source", "Creates a Dataset with a single element, comprising the given tensors.", "from_tensors produces a dataset containing only a single element. To slice the input tensor into multiple elements, use from_tensor_slices instead.", "Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.", "View source", "Maps map_func across this dataset, and interleaves the results.", "For example, you can use Dataset.interleave() to process many input files concurrently:", "The cycle_length and block_length arguments control the order in which elements are produced. cycle_length controls the number of input elements that are processed concurrently. If you set cycle_length to 1, this transformation will handle one input element at a time, and will produce identical results to tf.data.Dataset.flat_map. In general, this transformation will apply map_func to cycle_length input elements, open iterators on the returned Dataset objects, and cycle through them producing block_length consecutive elements from each iterator, and consuming the next input element each time it reaches the end of an iterator.", "Performance can often be improved by setting num_parallel_calls so that interleave will use multiple threads to fetch elements. If determinism isn't required, it can also improve performance to set deterministic=False.", "View source", "A dataset of all files matching one or more glob patterns.", "The file_pattern argument should be a small number of glob patterns. If your filenames have already been globbed, use Dataset.from_tensor_slices(filenames) instead, as re-globbing every filename with list_files may result in poor performance with remote storage systems.", "If we had the following files on our filesystem:", "If we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:", "View source", "Creates an iterator for elements of this dataset. (deprecated)", "View source", "Creates an iterator for elements of this dataset. (deprecated)", "View source", "Maps map_func across the elements of this dataset.", "This transformation applies map_func to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. map_func can be used to change both the values and the structure of a dataset's elements. For example, adding 1 to each element, or projecting a subset of element components.", "The input signature of map_func is determined by the structure of each element in this dataset.", "The value or values returned by map_func determine the structure of each element in the returned dataset.", "map_func can accept as arguments and return any type of dataset element.", "Note that irrespective of the context in which map_func is defined (eager vs. graph), tf.data traces the function and executes it as a graph. To use Python code inside of the function you have a few options:", "1) Rely on AutoGraph to convert Python code into an equivalent graph computation. The downside of this approach is that AutoGraph can convert some but not all Python code.", "2) Use tf.py_function, which allows you to write arbitrary Python code but will generally result in worse performance than 1). For example:", "3) Use tf.numpy_function, which also allows you to write arbitrary Python code. Note that tf.py_function accepts tf.Tensor whereas tf.numpy_function accepts numpy arrays and returns only numpy arrays. For example:", "Note that the use of tf.numpy_function and tf.py_function in general precludes the possibility of executing user-defined transformations in parallel (because of Python GIL).", "Performance can often be improved by setting num_parallel_calls so that map will use multiple threads to process elements. If deterministic order isn't required, it can also improve performance to set deterministic=False.", "View source", "Maps map_func across the elements of this dataset. (deprecated)", "View source", "Returns the options for this dataset and its inputs.", "View source", "Combines consecutive elements of this dataset into padded batches.", "This transformation combines multiple consecutive elements of the input dataset into a single element.", "Like tf.data.Dataset.batch, the components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced.", "Unlike tf.data.Dataset.batch, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in padded_shapes. The padded_shapes argument determines the resulting shape for each dimension of each component in an output element:", "See also tf.data.experimental.dense_to_sparse_batch, which combines elements that may have different shapes into a tf.sparse.SparseTensor.", "View source", "Creates a Dataset that prefetches elements from this dataset.", "Most dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.", "View source", "Creates a Dataset of a step-separated range of values.", "View source", "Reduces the input dataset to a single element.", "The transformation calls reduce_func successively on every element of the input dataset until the dataset is exhausted, aggregating information in its internal state. The initial_state argument is used for the initial state and the final state is returned as the result.", "View source", "Repeats this dataset so each original value is seen count times.", "View source", "Creates a Dataset that includes only 1/num_shards of this dataset.", "shard is deterministic. The Dataset produced by A.shard(n, i) will contain all elements of A whose index mod n = i.", "This dataset operator is very useful when running distributed training, as it allows each worker to read a unique subset.", "When reading a single input file, you can shard elements as follows:", "View source", "Randomly shuffles the elements of this dataset.", "This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.", "For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then shuffle will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer.", "reshuffle_each_iteration controls whether the shuffle order should be different for each epoch. In TF 1.X, the idiomatic way to create epochs was through the repeat transformation:", "In TF 2.0, tf.data.Dataset objects are Python iterables which makes it possible to also create epochs through Python iteration:", "View source", "Creates a Dataset that skips count elements from this dataset.", "View source", "Creates a Dataset with at most count elements from this dataset.", "View source", "Splits elements of a dataset into multiple elements.", "For example, if elements of the dataset are shaped [B, a0, a1, ...], where B may vary for each input element, then for each element in the dataset, the unbatched dataset will contain B consecutive elements of shape [a0, a1, ...].", "View source", "Combines (nests of) input elements into a dataset of (nests of) windows.", "A \"window\" is a finite dataset of flat elements of size size (or possibly fewer if there are not enough input elements to fill the window and drop_remainder evaluates to False).", "The shift argument determines the number of input elements by which the window moves on each iteration. If windows and elements are both numbered starting at 0, the first element in window k will be element k * shift of the input dataset. In particular, the first element of the first window will always be the first element of the input dataset.", "The stride argument determines the stride of the input elements, and the shift argument determines the shift of the window.", "Note that when the window transformation is applied to a dataset of nested elements, it produces a dataset of nested windows.", "View source", "Returns a new tf.data.Dataset with the given options set.", "The options are \"global\" in the sense they apply to the entire dataset. If options are set multiple times, they are merged as long as different options do not use different non-default values.", "View source", "Creates a Dataset by zipping together the given datasets.", "This method has similar semantics to the built-in zip() function in Python, with the main difference being that the datasets argument can be an arbitrary nested structure of Dataset objects.", "View source", "View source", "Creates an iterator for elements of this dataset.", "The returned iterator implements the Python Iterator protocol.", "View source", "Returns the length of the dataset if it is known and finite.", "This method requires that you are running in eager mode, and that the length of the dataset is known and non-infinite. When the length may be unknown or infinite, or if you are running in graph mode, use tf.data.Dataset.cardinality instead.", "View source"]}, {"name": "tf.compat.v1.debugging", "path": "compat/v1/debugging", "type": "tf.compat", "text": ["Public API for tf.debugging namespace.", "experimental module: Public API for tf.debugging.experimental namespace.", "Assert(...): Asserts that the given condition is true.", "assert_all_finite(...): Assert that the tensor does not contain any NaN's or Inf's.", "assert_equal(...): Assert the condition x == y holds element-wise.", "assert_greater(...): Assert the condition x > y holds element-wise.", "assert_greater_equal(...): Assert the condition x >= y holds element-wise.", "assert_integer(...): Assert that x is of integer dtype.", "assert_less(...): Assert the condition x < y holds element-wise.", "assert_less_equal(...): Assert the condition x <= y holds element-wise.", "assert_near(...): Assert the condition x and y are close element-wise.", "assert_negative(...): Assert the condition x < 0 holds element-wise.", "assert_non_negative(...): Assert the condition x >= 0 holds element-wise.", "assert_non_positive(...): Assert the condition x <= 0 holds element-wise.", "assert_none_equal(...): Assert the condition x != y holds element-wise.", "assert_positive(...): Assert the condition x > 0 holds element-wise.", "assert_proper_iterable(...): Static assert that values is a \"proper\" iterable.", "assert_rank(...): Assert x has rank equal to rank.", "assert_rank_at_least(...): Assert x has rank equal to rank or higher.", "assert_rank_in(...): Assert x has rank in ranks.", "assert_same_float_dtype(...): Validate and return float type based on tensors and dtype.", "assert_scalar(...): Asserts that the given tensor is a scalar (i.e. zero-dimensional).", "assert_shapes(...): Assert tensor shapes and dimension size relationships between tensors.", "assert_type(...): Statically asserts that the given Tensor is of the specified type.", "check_numerics(...): Checks a tensor for NaN and Inf values.", "disable_check_numerics(...): Disable the eager/graph unified numerics checking mechanism.", "enable_check_numerics(...): Enable tensor numerics checking in an eager/graph unified fashion.", "get_log_device_placement(...): Get if device placements are logged.", "is_finite(...): Returns which elements of x are finite.", "is_inf(...): Returns which elements of x are Inf.", "is_nan(...): Returns which elements of x are NaN.", "is_non_decreasing(...): Returns True if x is non-decreasing.", "is_numeric_tensor(...): Returns True if the elements of tensor are numbers.", "is_strictly_increasing(...): Returns True if x is strictly increasing.", "set_log_device_placement(...): Set if device placements should be logged."]}, {"name": "tf.compat.v1.debugging.assert_shapes", "path": "compat/v1/debugging/assert_shapes", "type": "tf.compat", "text": ["Assert tensor shapes and dimension size relationships between tensors.", "This Op checks that a collection of tensors shape relationships satisfies given constraints.", "Example of adding a dependency to an operation:", "If x, y, param or scalar does not have a shape that satisfies all specified constraints, message, as well as the first summarize entries of the first encountered violating tensor are printed, and InvalidArgumentError is raised.", "Size entries in the specified shapes are checked against other entries by their hash, except:", "If the first entry of a shape is ... (type Ellipsis) or '*' that indicates a variable number of outer dimensions of unspecified size, i.e. the constraint applies to the inner-most dimensions only.", "Scalar tensors and specified shapes of length zero (excluding the 'inner-most' prefix) are both treated as having a single dimension of size one."]}, {"name": "tf.compat.v1.debugging.experimental", "path": "compat/v1/debugging/experimental", "type": "tf.compat", "text": ["Public API for tf.debugging.experimental namespace.", "disable_dump_debug_info(...): Disable the currently-enabled debugging dumping.", "enable_dump_debug_info(...): Enable dumping debugging information from a TensorFlow program."]}, {"name": "tf.compat.v1.decode_csv", "path": "compat/v1/decode_csv", "type": "tf.compat", "text": ["Convert CSV records to tensors. Each column maps to one tensor.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.io.decode_csv", "RFC 4180 format is expected for the CSV records. (https://tools.ietf.org/html/rfc4180) Note that we allow leading and trailing spaces with int or float field."]}, {"name": "tf.compat.v1.decode_raw", "path": "compat/v1/decode_raw", "type": "tf.compat", "text": ["Convert raw byte strings into tensors. (deprecated arguments)", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.io.decode_raw"]}, {"name": "tf.compat.v1.delete_session_tensor", "path": "compat/v1/delete_session_tensor", "type": "tf.compat", "text": ["Delete the tensor for the given tensor handle.", "This is EXPERIMENTAL and subject to change.", "Delete the tensor of a given tensor handle. The tensor is produced in a previous run() and stored in the state of the session."]}, {"name": "tf.compat.v1.depth_to_space", "path": "compat/v1/depth_to_space", "type": "tf.compat", "text": ["DepthToSpace for tensors of type T.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.nn.depth_to_space", "Rearranges data from depth into blocks of spatial data. This is the reverse transformation of SpaceToDepth. More specifically, this op outputs a copy of the input tensor where values from the depth dimension are moved in spatial blocks to the height and width dimensions. The attr block_size indicates the input block size and how the data is moved.", "The data_format attr specifies the layout of the input and output tensors with the following options: \"NHWC\": [ batch, height, width, channels ] \"NCHW\": [ batch, channels, height, width ] \"NCHW_VECT_C\": qint8 [ batch, channels / 4, height, width, 4 ]", "It is useful to consider the operation as transforming a 6-D Tensor. e.g. for data_format = NHWC, Each element in the input tensor can be specified via 6 coordinates, ordered by decreasing memory layout significance as: n,iY,iX,bY,bX,oC (where n=batch index, iX, iY means X or Y coordinates within the input image, bX, bY means coordinates within the output block, oC means output channels). The output would be the input transposed to the following layout: n,iY,bY,iX,bX,oC", "This operation is useful for resizing the activations between convolutions (but keeping all data), e.g. instead of pooling. It is also useful for training purely convolutional models.", "For example, given an input of shape [1, 1, 1, 4], data_format = \"NHWC\" and block_size = 2:", "This operation will output a tensor of shape [1, 2, 2, 1]:", "Here, the input has a batch of 1 and each batch element has shape [1, 1, 4], the corresponding output will have 2x2 elements and will have a depth of 1 channel (1 = 4 / (block_size * block_size)). The output element shape is [2, 2, 1].", "For an input tensor with larger depth, here of shape [1, 1, 1, 12], e.g.", "This operation, for block size of 2, will return the following tensor of shape [1, 2, 2, 3]", "Similarly, for the following input of shape [1 2 2 4], and a block size of 2:", "the operator will return the following tensor of shape [1 4 4 1]:"]}, {"name": "tf.compat.v1.device", "path": "compat/v1/device", "type": "tf.compat", "text": ["Wrapper for Graph.device() using the default graph.", "See tf.Graph.device for more details."]}, {"name": "tf.compat.v1.DeviceSpec", "path": "compat/v1/devicespec", "type": "tf.compat", "text": ["Represents a (possibly partial) specification for a TensorFlow device.", "Inherits From: DeviceSpec", "DeviceSpecs are used throughout TensorFlow to describe where state is stored and computations occur. Using DeviceSpec allows you to parse device spec strings to verify their validity, merge them or compose them programmatically.", "With eager execution disabled (by default in TensorFlow 1.x and by calling disable_eager_execution() in TensorFlow 2.x), the following syntax can be used:", "A DeviceSpec consists of 5 components -- each of which is optionally specified:", "View source", "Construct a DeviceSpec from a string.", "View source", "Returns a new DeviceSpec which incorporates dev.", "When combining specs, dev will take precedence over the current spec. So for instance:", "is equivalent to:", "View source", "Merge the properties of \"dev\" into this DeviceSpec.", "View source", "Parse a DeviceSpec name into its components.", "2.x behavior change: In TensorFlow 1.x, this function mutates its own state and returns itself. In 2.x, DeviceSpecs are immutable, and this function will return a DeviceSpec which contains the spec.", "Recommended:", "Will work in 1.x and 2.x (though deprecated in 2.x):", "Will NOT work in 2.x:", "In general, DeviceSpec.from_string should completely replace DeviceSpec.parse_from_string, and DeviceSpec.replace should completely replace setting attributes directly.", "View source", "Convenience method for making a new DeviceSpec by overriding fields.", "View source", "Return a string representation of this DeviceSpec.", "View source", "Checks if the other DeviceSpec is same as the current instance, eg have", "same value for all the internal fields."]}, {"name": "tf.compat.v1.Dimension", "path": "compat/v1/dimension", "type": "tf.compat", "text": ["Represents the value of one dimension in a TensorShape.", "View source", "Raises an exception if other is not compatible with this Dimension.", "View source", "Returns true if other is compatible with this Dimension.", "Two known Dimensions are compatible if they have the same value. An unknown Dimension is compatible with all other Dimensions.", "View source", "Returns a Dimension that combines the information in self and other.", "Dimensions are combined as follows:", "View source", "Returns the sum of self and other.", "Dimensions are summed as follows:", "View source", "DEPRECATED: Use __floordiv__ via x // y instead.", "This function exists only for backwards compatibility purposes; new code should use __floordiv__ via the syntax x // y. Using x // y communicates clearly that the result rounds down, and is forward compatible to Python 3.", "View source", "Returns true if other has the same known value as this Dimension.", "View source", "Returns the quotient of self and other rounded down.", "Dimensions are divided as follows:", "View source", "Returns True if self is known to be greater than or equal to other.", "Dimensions are compared as follows:", "View source", "Returns True if self is known to be greater than other.", "Dimensions are compared as follows:", "View source", "Returns True if self is known to be less than or equal to other.", "Dimensions are compared as follows:", "View source", "Returns True if self is known to be less than other.", "Dimensions are compared as follows:", "View source", "Returns self modulo other.", "Dimension modulo are computed as follows:", "View source", "Returns the product of self and other.", "Dimensions are summed as follows:", "View source", "Returns true if other has a different known value from self.", "View source", "Returns the sum of other and self.", "View source", "Use __floordiv__ via x // y instead.", "This function exists only to have a better error message. Instead of: TypeError: unsupported operand type(s) for /: 'int' and 'Dimension', this function will explicitly call for usage of // instead.", "View source", "Returns the quotient of other and self rounded down.", "View source", "Returns other modulo self.", "View source", "Returns the product of self and other.", "View source", "Returns the subtraction of self from other.", "View source", "Use __floordiv__ via x // y instead.", "This function exists only to have a better error message. Instead of: TypeError: unsupported operand type(s) for /: 'int' and 'Dimension', this function will explicitly call for usage of // instead.", "View source", "Returns the subtraction of other from self.", "Dimensions are subtracted as follows:", "View source", "Use __floordiv__ via x // y instead.", "This function exists only to have a better error message. Instead of: TypeError: unsupported operand type(s) for /: 'Dimension' and 'int', this function will explicitly call for usage of // instead."]}, {"name": "tf.compat.v1.disable_control_flow_v2", "path": "compat/v1/disable_control_flow_v2", "type": "tf.compat", "text": ["Opts out of control flow v2.", "If your code needs tf.disable_control_flow_v2() to be called to work properly please file a bug."]}, {"name": "tf.compat.v1.disable_eager_execution", "path": "compat/v1/disable_eager_execution", "type": "tf.compat", "text": ["Disables eager execution.", "This function can only be called before any Graphs, Ops, or Tensors have been created. It can be used at the beginning of the program for complex migration projects from TensorFlow 1.x to 2.x."]}, {"name": "tf.compat.v1.disable_resource_variables", "path": "compat/v1/disable_resource_variables", "type": "tf.compat", "text": ["Opts out of resource variables. (deprecated)", "If your code needs tf.disable_resource_variables() to be called to work properly please file a bug."]}, {"name": "tf.compat.v1.disable_tensor_equality", "path": "compat/v1/disable_tensor_equality", "type": "tf.compat", "text": ["Compare Tensors by their id and be hashable.", "This is a legacy behaviour of TensorFlow and is highly discouraged."]}, {"name": "tf.compat.v1.disable_v2_behavior", "path": "compat/v1/disable_v2_behavior", "type": "tf.compat", "text": ["Disables TensorFlow 2.x behaviors.", "This function can be called at the beginning of the program (before Tensors, Graphs or other structures have been created, and before devices have been initialized. It switches all global behaviors that are different between TensorFlow 1.x and 2.x to behave as intended for 1.x.", "User can call this function to disable 2.x behavior during complex migrations."]}, {"name": "tf.compat.v1.disable_v2_tensorshape", "path": "compat/v1/disable_v2_tensorshape", "type": "tf.compat", "text": ["Disables the V2 TensorShape behavior and reverts to V1 behavior.", "See docstring for enable_v2_tensorshape for details about the new behavior."]}, {"name": "tf.compat.v1.distribute", "path": "compat/v1/distribute", "type": "tf.compat", "text": ["Library for running a computation across multiple devices.", "The intent of this library is that you can write an algorithm in a stylized way and it will be usable with a variety of different tf.distribute.Strategy implementations. Each descendant will implement a different strategy for distributing the algorithm across multiple devices/machines. Furthermore, these changes can be hidden inside the specific layers and other library classes that need special treatment to run in a distributed setting, so that most users' model definition code can run unchanged. The tf.distribute.Strategy API works the same way with eager and graph execution.", "Guides", "Tutorials", "Distributed Training Tutorials", "The tutorials cover how to use tf.distribute.Strategy to do distributed training with native Keras APIs, custom training loops, and Esitmator APIs. They also cover how to save/load model when using tf.distribute.Strategy.", "Glossary", "Parameter servers: These are machines that hold a single copy of parameters/variables, used by some strategies (right now just tf.distribute.experimental.ParameterServerStrategy). All replicas that want to operate on a variable retrieve it at the beginning of a step and send an update to be applied at the end of the step. These can in priniciple support either sync or async training, but right now we only have support for async training with parameter servers. Compare to tf.distribute.experimental.CentralStorageStrategy, which puts all variables on a single device on the same machine (and does sync training), and tf.distribute.MirroredStrategy, which mirrors variables to multiple devices (see below).", "Replica context vs. Cross-replica context vs Update context", "A replica context applies when you execute the computation function that was called with strategy.run. Conceptually, you're in replica context when executing the computation function that is being replicated.", "An update context is entered in a tf.distribute.StrategyExtended.update call.", "An cross-replica context is entered when you enter a strategy.scope. This is useful for calling tf.distribute.Strategy methods which operate across the replicas (like reduce_to()). By default you start in a replica context (the \"default single replica context\") and then some methods can switch you back and forth.", "Distributed value: Distributed value is represented by the base class tf.distribute.DistributedValues. tf.distribute.DistributedValues is useful to represent values on multiple devices, and it contains a map from replica id to values. Two representative kinds of tf.distribute.DistributedValues are \"PerReplica\" and \"Mirrored\" values.", "\"PerReplica\" values exist on the worker devices, with a different value for each replica. They are produced by iterating through a distributed dataset returned by tf.distribute.Strategy.experimental_distribute_dataset and tf.distribute.Strategy.distribute_datasets_from_function. They are also the typical result returned by tf.distribute.Strategy.run.", "\"Mirrored\" values are like \"PerReplica\" values, except we know that the value on all replicas are the same. We can safely read a \"Mirrored\" value in a cross-replica context by using the value on any replica.", "Unwrapping and merging: Consider calling a function fn on multiple replicas, like strategy.run(fn, args=[w]) with an argument w that is a tf.distribute.DistributedValues. This means w will have a map taking replica id 0 to w0, replica id 1 to w1, etc. strategy.run() unwraps w before calling fn, so it calls fn(w0) on device d0, fn(w1) on device d1, etc. It then merges the return values from fn(), which leads to one common object if the returned values are the same object from every replica, or a DistributedValues object otherwise.", "Reductions and all-reduce: A reduction is a method of aggregating multiple values into one value, like \"sum\" or \"mean\". If a strategy is doing sync training, we will perform a reduction on the gradients to a parameter from all replicas before applying the update. All-reduce is an algorithm for performing a reduction on values from multiple devices and making the result available on all of those devices.", "Mirrored variables: These are variables that are created on multiple devices, where we keep the variables in sync by applying the same updates to every copy. Mirrored variables are created with tf.Variable(...synchronization=tf.VariableSynchronization.ON_WRITE...). Normally they are only used in synchronous training.", "SyncOnRead variables", "SyncOnRead variables are created by tf.Variable(...synchronization=tf.VariableSynchronization.ON_READ...), and they are created on multiple devices. In replica context, each component variable on the local replica can perform reads and writes without synchronization with each other. When the SyncOnRead variable is read in cross-replica context, the values from component variables are aggregated and returned.", "SyncOnRead variables bring a lot of custom configuration difficulty to the underlying logic, so we do not encourage users to instantiate and use SyncOnRead variable on their own. We have mainly used SyncOnRead variables for use cases such as batch norm and metrics. For performance reasons, we often don't need to keep these statistics in sync every step and they can be accumulated on each replica independently. The only time we want to sync them is reporting or checkpointing, which typically happens in cross-replica context. SyncOnRead variables are also often used by advanced users who want to control when variable values are aggregated. For example, users sometimes want to maintain gradients independently on each replica for a couple of steps without aggregation.", "Distribute-aware layers", "Layers are generally called in a replica context, except when defining a Keras functional model. tf.distribute.in_cross_replica_context will let you determine which case you are in. If in a replica context, the tf.distribute.get_replica_context function will return the default replica context outside a strategy scope, None within a strategy scope, and a tf.distribute.ReplicaContext object inside a strategy scope and within a tf.distribute.Strategy.run function. The ReplicaContext object has an all_reduce method for aggregating across all replicas.", "Note that we provide a default version of tf.distribute.Strategy that is used when no other strategy is in scope, that provides the same API with reasonable default behavior.", "cluster_resolver module: Library imports for ClusterResolvers.", "experimental module: Public API for tf.distribute.experimental namespace.", "class CrossDeviceOps: Base class for cross-device reduction and broadcasting algorithms.", "class HierarchicalCopyAllReduce: Hierarchical copy all-reduce implementation of CrossDeviceOps.", "class InputContext: A class wrapping information needed by an input function.", "class InputReplicationMode: Replication mode for input function.", "class MirroredStrategy: Synchronous training across multiple replicas on one machine.", "class NcclAllReduce: NCCL all-reduce implementation of CrossDeviceOps.", "class OneDeviceStrategy: A distribution strategy for running on a single device.", "class ReduceOp: Indicates how a set of values should be reduced.", "class ReductionToOneDevice: A CrossDeviceOps implementation that copies values to one device to reduce.", "class ReplicaContext: A class with a collection of APIs that can be called in a replica context.", "class RunOptions: Run options for strategy.run.", "class Server: An in-process TensorFlow server, for use in distributed training.", "class Strategy: A list of devices with a state & compute distribution policy.", "class StrategyExtended: Additional APIs for algorithms that need to be distribution-aware.", "experimental_set_strategy(...): Set a tf.distribute.Strategy as current without with strategy.scope().", "get_loss_reduction(...): tf.distribute.ReduceOp corresponding to the last loss reduction.", "get_replica_context(...): Returns the current tf.distribute.ReplicaContext or None.", "get_strategy(...): Returns the current tf.distribute.Strategy object.", "has_strategy(...): Return if there is a current non-default tf.distribute.Strategy.", "in_cross_replica_context(...): Returns True if in a cross-replica context."]}, {"name": "tf.compat.v1.distribute.cluster_resolver", "path": "compat/v1/distribute/cluster_resolver", "type": "tf.compat", "text": ["Library imports for ClusterResolvers.", "This library contains all implementations of ClusterResolvers. ClusterResolvers are a way of specifying cluster information for distributed execution. Built on top of existing ClusterSpec framework, ClusterResolvers are a way for TensorFlow to communicate with various cluster management systems (e.g. GCE, AWS, etc...).", "class ClusterResolver: Abstract class for all implementations of ClusterResolvers.", "class GCEClusterResolver: ClusterResolver for Google Compute Engine.", "class KubernetesClusterResolver: ClusterResolver for Kubernetes.", "class SimpleClusterResolver: Simple implementation of ClusterResolver that accepts all attributes.", "class SlurmClusterResolver: ClusterResolver for system with Slurm workload manager.", "class TFConfigClusterResolver: Implementation of a ClusterResolver which reads the TF_CONFIG EnvVar.", "class TPUClusterResolver: Cluster Resolver for Google Cloud TPUs.", "class UnionResolver: Performs a union on underlying ClusterResolvers."]}, {"name": "tf.compat.v1.distribute.experimental", "path": "compat/v1/distribute/experimental", "type": "tf.compat", "text": ["Public API for tf.distribute.experimental namespace.", "class CentralStorageStrategy: A one-machine strategy that puts all variables on a single device.", "class CollectiveCommunication: Cross device communication implementation.", "class CollectiveHints: Hints for collective operations like AllReduce.", "class CommunicationImplementation: Cross device communication implementation.", "class CommunicationOptions: Options for cross device communications like All-reduce.", "class MultiWorkerMirroredStrategy: A distribution strategy for synchronous training on multiple workers.", "class ParameterServerStrategy: An asynchronous multi-worker parameter server tf.distribute strategy.", "class TPUStrategy: TPU distribution strategy implementation."]}, {"name": "tf.compat.v1.distribute.experimental.CentralStorageStrategy", "path": "compat/v1/distribute/experimental/centralstoragestrategy", "type": "tf.compat", "text": ["A one-machine strategy that puts all variables on a single device.", "Inherits From: Strategy", "Variables are assigned to local CPU or the only GPU. If there is more than one GPU, compute operations (other than variable update operations) will be replicated across all GPUs.", "In general, when using a multi-worker tf.distribute strategy such as tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy(), there is a tf.distribute.cluster_resolver.ClusterResolver associated with the strategy used, and such an instance is returned by this property.", "Strategies that intend to have an associated tf.distribute.cluster_resolver.ClusterResolver must set the relevant attribute, or override this property; otherwise, None is returned by default. Those strategies should also provide information regarding what is returned by this property.", "Single-worker strategies usually do not have a tf.distribute.cluster_resolver.ClusterResolver, and in those cases this property will return None.", "The tf.distribute.cluster_resolver.ClusterResolver may be useful when the user needs to access information such as the cluster spec, task type or task id. For example,", "For more information, please see tf.distribute.cluster_resolver.ClusterResolver's API docstring. ", "View source", "Distributes tf.data.Dataset instances created by calls to dataset_fn.", "The argument dataset_fn that users pass in is an input function that has a tf.distribute.InputContext argument and returns a tf.data.Dataset instance. It is expected that the returned dataset from dataset_fn is already batched by per-replica batch size (i.e. global batch size divided by the number of replicas in sync) and sharded. tf.distribute.Strategy.distribute_datasets_from_function does not batch or shard the tf.data.Dataset instance returned from the input function. dataset_fn will be called on the CPU device of each of the workers and each generates a dataset where every replica on that worker will dequeue one batch of inputs (i.e. if a worker has two replicas, two batches will be dequeued from the Dataset every step).", "This method can be used for several purposes. First, it allows you to specify your own batching and sharding logic. (In contrast, tf.distribute.experimental_distribute_dataset does batching and sharding for you.) For example, where experimental_distribute_dataset is unable to shard the input files, this method might be used to manually shard the dataset (avoiding the slow fallback behavior in experimental_distribute_dataset). In cases where the dataset is infinite, this sharding can be done by creating dataset replicas that differ only in their random seed.", "The dataset_fn should take an tf.distribute.InputContext instance where information about batching and input replication can be accessed.", "You can use element_spec property of the tf.distribute.DistributedDataset returned by this API to query the tf.TypeSpec of the elements returned by the iterator. This can be used to set the input_signature property of a tf.function. Follow tf.distribute.DistributedDataset.element_spec to see an example.", "For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input). If you are interested in last partial batch handling, read this section.", "View source", "Creates tf.distribute.DistributedDataset from tf.data.Dataset.", "The returned tf.distribute.DistributedDataset can be iterated over similar to regular datasets. NOTE: The user cannot add any more transformations to a tf.distribute.DistributedDataset. You can only create an iterator or examine the tf.TypeSpec of the data generated by it. See API docs of tf.distribute.DistributedDataset to learn more.", "The following is an example:", "Three key actions happending under the hood of this method are batching, sharding, and prefetching.", "In the code snippet above, dataset is batched by global_batch_size, and calling experimental_distribute_dataset on it rebatches dataset to a new batch size that is equal to the global batch size divided by the number of replicas in sync. We iterate through it using a Pythonic for loop. x is a tf.distribute.DistributedValues containing data for all replicas, and each replica gets data of the new batch size. tf.distribute.Strategy.run will take care of feeding the right per-replica data in x to the right replica_fn executed on each replica.", "Sharding contains autosharding across multiple workers and within every worker. First, in multi-worker distributed training (i.e. when you use tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy), autosharding a dataset over a set of workers means that each worker is assigned a subset of the entire dataset (if the right tf.data.experimental.AutoShardPolicy is set). This is to ensure that at each step, a global batch size of non-overlapping dataset elements will be processed by each worker. Autosharding has a couple of different options that can be specified using tf.data.experimental.DistributeOptions. Then, sharding within each worker means the method will split the data among all the worker devices (if more than one a present). This will happen regardless of multi-worker autosharding.", "By default, this method adds a prefetch transformation at the end of the user provided tf.data.Dataset instance. The argument to the prefetch transformation which is buffer_size is equal to the number of replicas in sync.", "If the above batch splitting and dataset sharding logic is undesirable, please use tf.distribute.Strategy.distribute_datasets_from_function instead, which does not do any automatic batching or sharding for you.", "For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input. If you are interested in last partial batch handling, read this section.", "View source", "Returns the list of all local per-replica values contained in value.", "View source", "Makes a tf.data.Dataset for input provided via a numpy array.", "This avoids adding numpy_input as a large constant in the graph, and copies the data to the machine or machines that will be processing the input.", "Note that you will likely need to use tf.distribute.Strategy.experimental_distribute_dataset with the returned dataset to further distribute it with the strategy.", "View source", "Runs ops in fn on each replica, with inputs from input_iterator.", "DEPRECATED: This method is not available in TF 2.x. Please switch to using run instead.", "When eager execution is enabled, executes ops specified by fn on each replica. Otherwise, builds a graph to execute the ops on each replica.", "Each replica will take a single, different input from the inputs provided by one get_next call on the input iterator.", "fn may call tf.distribute.get_replica_context() to access members such as replica_id_in_sync_group.", "View source", "Makes an iterator for input provided via dataset.", "DEPRECATED: This method is not available in TF 2.x.", "Data from the given dataset will be distributed evenly across all the compute replicas. We will assume that the input dataset is batched by the global batch size. With this assumption, we will make a best effort to divide each batch across all the replicas (one or more workers). If this effort fails, an error will be thrown, and the user should instead use make_input_fn_iterator which provides more control to the user, and does not try to divide a batch across replicas.", "The user could also use make_input_fn_iterator if they want to customize which input is fed to which replica/worker etc.", "View source", "Returns an iterator split across replicas created from an input function.", "DEPRECATED: This method is not available in TF 2.x.", "The input_fn should take an tf.distribute.InputContext object where information about batching and input sharding can be accessed:", "The tf.data.Dataset returned by input_fn should have a per-replica batch size, which may be computed using input_context.get_per_replica_batch_size.", "View source", "Reduce value across replicas and return result on current device.", "To see how this would look with multiple replicas, consider the same example with MirroredStrategy with 2 GPUs:", "This API is typically used for aggregating the results returned from different replicas, for reporting etc. For example, loss computed from different replicas can be averaged using this API before printing.", "There are a number of different tf.distribute APIs for reducing values across replicas:", "What should axis be?", "Given a per-replica value returned by run, say a per-example loss, the batch will be divided across all the replicas. This function allows you to aggregate across replicas and optionally also across batch elements by specifying the axis parameter accordingly.", "For example, if you have a global batch size of 8 and 2 replicas, values for examples [0, 1, 2, 3] will be on replica 0 and [4, 5, 6, 7] will be on replica 1. With axis=None, reduce will aggregate only across replicas, returning [0+4, 1+5, 2+6, 3+7]. This is useful when each replica is computing a scalar or some other value that doesn't have a \"batch\" dimension (like a gradient or loss).", "Sometimes, you will want to aggregate across both the global batch and all replicas. You can get this behavior by specifying the batch dimension as the axis, typically axis=0. In this case it would return a scalar 0+1+2+3+4+5+6+7.", "If there is a last partial batch, you will need to specify an axis so that the resulting shape is consistent across replicas. So if the last batch has size 6 and it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch unless you specify axis=0. If you specify tf.distribute.ReduceOp.MEAN, using axis=0 will use the correct denominator of 6. Contrast this with computing reduce_mean to get a scalar value on each replica and this function to average those means, which will weigh some values 1/8 and others 1/4.", "View source", "Invokes fn on each replica, with the given arguments.", "This method is the primary way to distribute your computation with a tf.distribute object. It invokes fn on each replica. If args or kwargs have tf.distribute.DistributedValues, such as those produced by a tf.distribute.DistributedDataset from tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function, when fn is executed on a particular replica, it will be executed with the component of tf.distribute.DistributedValues that correspond to that replica.", "fn is invoked under a replica context. fn may call tf.distribute.get_replica_context() to access members such as all_reduce. Please see the module-level docstring of tf.distribute for the concept of replica context.", "All arguments in args or kwargs should either be Python values of a nested structure of tensors, e.g. a list of tensors, in which case args and kwargs will be passed to the fn invoked on each replica. Or args or kwargs can be tf.distribute.DistributedValues containing tensors or composite tensors, i.e. tf.compat.v1.TensorInfo.CompositeTensor, in which case each fn call will get the component of a tf.distribute.DistributedValues corresponding to its replica.", "View source", "Context manager to make the strategy current and distribute variables.", "This method returns a context manager, and is used as follows:", "What happens when Strategy.scope is entered?", "What should be in scope and what should be outside?", "There are a number of requirements on what needs to happen inside the scope. However, in places where we have information about which strategy is in use, we often enter the scope for the user, so they don't have to do it explicitly (i.e. calling those either inside or outside the scope is OK).", "View source", "Returns a copy of config_proto modified for use with this strategy.", "DEPRECATED: This method is not available in TF 2.x.", "The updated config has something needed to run a strategy, e.g. configuration to run collective ops, or device filters to improve distributed training performance."]}, {"name": "tf.compat.v1.distribute.experimental.MultiWorkerMirroredStrategy", "path": "compat/v1/distribute/experimental/multiworkermirroredstrategy", "type": "tf.compat", "text": ["A distribution strategy for synchronous training on multiple workers.", "Inherits From: Strategy", "This strategy implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to tf.distribute.MirroredStrategy, it replicates all variables and computations to each local device. The difference is that it uses a distributed collective implementation (e.g. all-reduce), so that multiple workers can work together.", "You need to launch your program on each worker and configure cluster_resolver correctly. For example, if you are using tf.distribute.cluster_resolver.TFConfigClusterResolver, each worker needs to have its corresponding task_type and task_id set in the TF_CONFIG environment variable. An example TF_CONFIG on worker-0 of a two worker cluster is:", "Your program runs on each worker as-is. Note that collectives require each worker to participate. All tf.distribute and non tf.distribute API may use collectives internally, e.g. checkpointing and saving since reading a tf.Variable with tf.VariableSynchronization.ON_READ all-reduces the value. Therefore it's recommended to run exactly the same program on each worker. Dispatching based on task_type or task_id of the worker is error-prone.", "cluster_resolver.num_accelerators() determines the number of GPUs the strategy uses. If it's zero, the strategy uses the CPU. All workers need to use the same number of devices, otherwise the behavior is undefined.", "This strategy is not intended for TPU. Use tf.distribute.TPUStrategy instead.", "After setting up TF_CONFIG, using this strategy is similar to using tf.distribute.MirroredStrategy and tf.distribute.TPUStrategy.", "You can also write your own training loop:", "See Multi-worker training with Keras for a detailed tutorial.", "Saving", "You need to save and checkpoint on all workers instead of just one. This is because variables whose synchronization=ON_READ triggers aggregation during saving. It's recommended to save to a different path on each worker to avoid race conditions. Each worker saves the same thing. See Multi-worker training with Keras tutorial for examples.", "Known Issues", "In general, when using a multi-worker tf.distribute strategy such as tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy(), there is a tf.distribute.cluster_resolver.ClusterResolver associated with the strategy used, and such an instance is returned by this property.", "Strategies that intend to have an associated tf.distribute.cluster_resolver.ClusterResolver must set the relevant attribute, or override this property; otherwise, None is returned by default. Those strategies should also provide information regarding what is returned by this property.", "Single-worker strategies usually do not have a tf.distribute.cluster_resolver.ClusterResolver, and in those cases this property will return None.", "The tf.distribute.cluster_resolver.ClusterResolver may be useful when the user needs to access information such as the cluster spec, task type or task id. For example,", "For more information, please see tf.distribute.cluster_resolver.ClusterResolver's API docstring. ", "View source", "Distributes tf.data.Dataset instances created by calls to dataset_fn.", "The argument dataset_fn that users pass in is an input function that has a tf.distribute.InputContext argument and returns a tf.data.Dataset instance. It is expected that the returned dataset from dataset_fn is already batched by per-replica batch size (i.e. global batch size divided by the number of replicas in sync) and sharded. tf.distribute.Strategy.distribute_datasets_from_function does not batch or shard the tf.data.Dataset instance returned from the input function. dataset_fn will be called on the CPU device of each of the workers and each generates a dataset where every replica on that worker will dequeue one batch of inputs (i.e. if a worker has two replicas, two batches will be dequeued from the Dataset every step).", "This method can be used for several purposes. First, it allows you to specify your own batching and sharding logic. (In contrast, tf.distribute.experimental_distribute_dataset does batching and sharding for you.) For example, where experimental_distribute_dataset is unable to shard the input files, this method might be used to manually shard the dataset (avoiding the slow fallback behavior in experimental_distribute_dataset). In cases where the dataset is infinite, this sharding can be done by creating dataset replicas that differ only in their random seed.", "The dataset_fn should take an tf.distribute.InputContext instance where information about batching and input replication can be accessed.", "You can use element_spec property of the tf.distribute.DistributedDataset returned by this API to query the tf.TypeSpec of the elements returned by the iterator. This can be used to set the input_signature property of a tf.function. Follow tf.distribute.DistributedDataset.element_spec to see an example.", "For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input). If you are interested in last partial batch handling, read this section.", "View source", "Creates tf.distribute.DistributedDataset from tf.data.Dataset.", "The returned tf.distribute.DistributedDataset can be iterated over similar to regular datasets. NOTE: The user cannot add any more transformations to a tf.distribute.DistributedDataset. You can only create an iterator or examine the tf.TypeSpec of the data generated by it. See API docs of tf.distribute.DistributedDataset to learn more.", "The following is an example:", "Three key actions happending under the hood of this method are batching, sharding, and prefetching.", "In the code snippet above, dataset is batched by global_batch_size, and calling experimental_distribute_dataset on it rebatches dataset to a new batch size that is equal to the global batch size divided by the number of replicas in sync. We iterate through it using a Pythonic for loop. x is a tf.distribute.DistributedValues containing data for all replicas, and each replica gets data of the new batch size. tf.distribute.Strategy.run will take care of feeding the right per-replica data in x to the right replica_fn executed on each replica.", "Sharding contains autosharding across multiple workers and within every worker. First, in multi-worker distributed training (i.e. when you use tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy), autosharding a dataset over a set of workers means that each worker is assigned a subset of the entire dataset (if the right tf.data.experimental.AutoShardPolicy is set). This is to ensure that at each step, a global batch size of non-overlapping dataset elements will be processed by each worker. Autosharding has a couple of different options that can be specified using tf.data.experimental.DistributeOptions. Then, sharding within each worker means the method will split the data among all the worker devices (if more than one a present). This will happen regardless of multi-worker autosharding.", "By default, this method adds a prefetch transformation at the end of the user provided tf.data.Dataset instance. The argument to the prefetch transformation which is buffer_size is equal to the number of replicas in sync.", "If the above batch splitting and dataset sharding logic is undesirable, please use tf.distribute.Strategy.distribute_datasets_from_function instead, which does not do any automatic batching or sharding for you.", "For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input. If you are interested in last partial batch handling, read this section.", "View source", "Returns the list of all local per-replica values contained in value.", "View source", "Makes a tf.data.Dataset for input provided via a numpy array.", "This avoids adding numpy_input as a large constant in the graph, and copies the data to the machine or machines that will be processing the input.", "Note that you will likely need to use tf.distribute.Strategy.experimental_distribute_dataset with the returned dataset to further distribute it with the strategy.", "View source", "Runs ops in fn on each replica, with inputs from input_iterator.", "DEPRECATED: This method is not available in TF 2.x. Please switch to using run instead.", "When eager execution is enabled, executes ops specified by fn on each replica. Otherwise, builds a graph to execute the ops on each replica.", "Each replica will take a single, different input from the inputs provided by one get_next call on the input iterator.", "fn may call tf.distribute.get_replica_context() to access members such as replica_id_in_sync_group.", "View source", "Makes an iterator for input provided via dataset.", "DEPRECATED: This method is not available in TF 2.x.", "Data from the given dataset will be distributed evenly across all the compute replicas. We will assume that the input dataset is batched by the global batch size. With this assumption, we will make a best effort to divide each batch across all the replicas (one or more workers). If this effort fails, an error will be thrown, and the user should instead use make_input_fn_iterator which provides more control to the user, and does not try to divide a batch across replicas.", "The user could also use make_input_fn_iterator if they want to customize which input is fed to which replica/worker etc.", "View source", "Returns an iterator split across replicas created from an input function.", "DEPRECATED: This method is not available in TF 2.x.", "The input_fn should take an tf.distribute.InputContext object where information about batching and input sharding can be accessed:", "The tf.data.Dataset returned by input_fn should have a per-replica batch size, which may be computed using input_context.get_per_replica_batch_size.", "View source", "Reduce value across replicas and return result on current device.", "To see how this would look with multiple replicas, consider the same example with MirroredStrategy with 2 GPUs:", "This API is typically used for aggregating the results returned from different replicas, for reporting etc. For example, loss computed from different replicas can be averaged using this API before printing.", "There are a number of different tf.distribute APIs for reducing values across replicas:", "What should axis be?", "Given a per-replica value returned by run, say a per-example loss, the batch will be divided across all the replicas. This function allows you to aggregate across replicas and optionally also across batch elements by specifying the axis parameter accordingly.", "For example, if you have a global batch size of 8 and 2 replicas, values for examples [0, 1, 2, 3] will be on replica 0 and [4, 5, 6, 7] will be on replica 1. With axis=None, reduce will aggregate only across replicas, returning [0+4, 1+5, 2+6, 3+7]. This is useful when each replica is computing a scalar or some other value that doesn't have a \"batch\" dimension (like a gradient or loss).", "Sometimes, you will want to aggregate across both the global batch and all replicas. You can get this behavior by specifying the batch dimension as the axis, typically axis=0. In this case it would return a scalar 0+1+2+3+4+5+6+7.", "If there is a last partial batch, you will need to specify an axis so that the resulting shape is consistent across replicas. So if the last batch has size 6 and it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch unless you specify axis=0. If you specify tf.distribute.ReduceOp.MEAN, using axis=0 will use the correct denominator of 6. Contrast this with computing reduce_mean to get a scalar value on each replica and this function to average those means, which will weigh some values 1/8 and others 1/4.", "View source", "Invokes fn on each replica, with the given arguments.", "This method is the primary way to distribute your computation with a tf.distribute object. It invokes fn on each replica. If args or kwargs have tf.distribute.DistributedValues, such as those produced by a tf.distribute.DistributedDataset from tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function, when fn is executed on a particular replica, it will be executed with the component of tf.distribute.DistributedValues that correspond to that replica.", "fn is invoked under a replica context. fn may call tf.distribute.get_replica_context() to access members such as all_reduce. Please see the module-level docstring of tf.distribute for the concept of replica context.", "All arguments in args or kwargs should either be Python values of a nested structure of tensors, e.g. a list of tensors, in which case args and kwargs will be passed to the fn invoked on each replica. Or args or kwargs can be tf.distribute.DistributedValues containing tensors or composite tensors, i.e. tf.compat.v1.TensorInfo.CompositeTensor, in which case each fn call will get the component of a tf.distribute.DistributedValues corresponding to its replica.", "View source", "Context manager to make the strategy current and distribute variables.", "This method returns a context manager, and is used as follows:", "What happens when Strategy.scope is entered?", "What should be in scope and what should be outside?", "There are a number of requirements on what needs to happen inside the scope. However, in places where we have information about which strategy is in use, we often enter the scope for the user, so they don't have to do it explicitly (i.e. calling those either inside or outside the scope is OK).", "View source", "Returns a copy of config_proto modified for use with this strategy.", "DEPRECATED: This method is not available in TF 2.x.", "The updated config has something needed to run a strategy, e.g. configuration to run collective ops, or device filters to improve distributed training performance."]}, {"name": "tf.compat.v1.distribute.experimental.ParameterServerStrategy", "path": "compat/v1/distribute/experimental/parameterserverstrategy", "type": "tf.compat", "text": ["An asynchronous multi-worker parameter server tf.distribute strategy.", "Inherits From: Strategy", "This strategy requires two roles: workers and parameter servers. Variables and updates to those variables will be assigned to parameter servers and other operations are assigned to workers.", "When each worker has more than one GPU, operations will be replicated on all GPUs. Even though operations may be replicated, variables are not and each worker shares a common view for which parameter server a variable is assigned to.", "By default it uses TFConfigClusterResolver to detect configurations for multi-worker training. This requires a 'TF_CONFIG' environment variable and the 'TF_CONFIG' must have a cluster spec.", "This class assumes each worker is running the same code independently, but parameter servers are running a standard server. This means that while each worker will synchronously compute a single gradient update across all GPUs, updates between workers proceed asynchronously. Operations that occur only on the first replica (such as incrementing the global step), will occur on the first replica of every worker.", "It is expected to call call_for_each_replica(fn, ...) for any operations which potentially can be replicated across replicas (i.e. multiple GPUs) even if there is only CPU or one GPU. When defining the fn, extra caution needs to be taken:", "1) It is generally not recommended to open a device scope under the strategy's scope. A device scope (i.e. calling tf.device) will be merged with or override the device for operations but will not change the device for variables.", "2) It is also not recommended to open a colocation scope (i.e. calling tf.compat.v1.colocate_with) under the strategy's scope. For colocating variables, use strategy.extended.colocate_vars_with instead. Colocation of ops will possibly create device assignment conflicts.", "In general, when using a multi-worker tf.distribute strategy such as tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy(), there is a tf.distribute.cluster_resolver.ClusterResolver associated with the strategy used, and such an instance is returned by this property.", "Strategies that intend to have an associated tf.distribute.cluster_resolver.ClusterResolver must set the relevant attribute, or override this property; otherwise, None is returned by default. Those strategies should also provide information regarding what is returned by this property.", "Single-worker strategies usually do not have a tf.distribute.cluster_resolver.ClusterResolver, and in those cases this property will return None.", "The tf.distribute.cluster_resolver.ClusterResolver may be useful when the user needs to access information such as the cluster spec, task type or task id. For example,", "For more information, please see tf.distribute.cluster_resolver.ClusterResolver's API docstring. ", "View source", "Distributes tf.data.Dataset instances created by calls to dataset_fn.", "The argument dataset_fn that users pass in is an input function that has a tf.distribute.InputContext argument and returns a tf.data.Dataset instance. It is expected that the returned dataset from dataset_fn is already batched by per-replica batch size (i.e. global batch size divided by the number of replicas in sync) and sharded. tf.distribute.Strategy.distribute_datasets_from_function does not batch or shard the tf.data.Dataset instance returned from the input function. dataset_fn will be called on the CPU device of each of the workers and each generates a dataset where every replica on that worker will dequeue one batch of inputs (i.e. if a worker has two replicas, two batches will be dequeued from the Dataset every step).", "This method can be used for several purposes. First, it allows you to specify your own batching and sharding logic. (In contrast, tf.distribute.experimental_distribute_dataset does batching and sharding for you.) For example, where experimental_distribute_dataset is unable to shard the input files, this method might be used to manually shard the dataset (avoiding the slow fallback behavior in experimental_distribute_dataset). In cases where the dataset is infinite, this sharding can be done by creating dataset replicas that differ only in their random seed.", "The dataset_fn should take an tf.distribute.InputContext instance where information about batching and input replication can be accessed.", "You can use element_spec property of the tf.distribute.DistributedDataset returned by this API to query the tf.TypeSpec of the elements returned by the iterator. This can be used to set the input_signature property of a tf.function. Follow tf.distribute.DistributedDataset.element_spec to see an example.", "For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input). If you are interested in last partial batch handling, read this section.", "View source", "Creates tf.distribute.DistributedDataset from tf.data.Dataset.", "The returned tf.distribute.DistributedDataset can be iterated over similar to regular datasets. NOTE: The user cannot add any more transformations to a tf.distribute.DistributedDataset. You can only create an iterator or examine the tf.TypeSpec of the data generated by it. See API docs of tf.distribute.DistributedDataset to learn more.", "The following is an example:", "Three key actions happending under the hood of this method are batching, sharding, and prefetching.", "In the code snippet above, dataset is batched by global_batch_size, and calling experimental_distribute_dataset on it rebatches dataset to a new batch size that is equal to the global batch size divided by the number of replicas in sync. We iterate through it using a Pythonic for loop. x is a tf.distribute.DistributedValues containing data for all replicas, and each replica gets data of the new batch size. tf.distribute.Strategy.run will take care of feeding the right per-replica data in x to the right replica_fn executed on each replica.", "Sharding contains autosharding across multiple workers and within every worker. First, in multi-worker distributed training (i.e. when you use tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy), autosharding a dataset over a set of workers means that each worker is assigned a subset of the entire dataset (if the right tf.data.experimental.AutoShardPolicy is set). This is to ensure that at each step, a global batch size of non-overlapping dataset elements will be processed by each worker. Autosharding has a couple of different options that can be specified using tf.data.experimental.DistributeOptions. Then, sharding within each worker means the method will split the data among all the worker devices (if more than one a present). This will happen regardless of multi-worker autosharding.", "By default, this method adds a prefetch transformation at the end of the user provided tf.data.Dataset instance. The argument to the prefetch transformation which is buffer_size is equal to the number of replicas in sync.", "If the above batch splitting and dataset sharding logic is undesirable, please use tf.distribute.Strategy.distribute_datasets_from_function instead, which does not do any automatic batching or sharding for you.", "For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input. If you are interested in last partial batch handling, read this section.", "View source", "Returns the list of all local per-replica values contained in value.", "View source", "Makes a tf.data.Dataset for input provided via a numpy array.", "This avoids adding numpy_input as a large constant in the graph, and copies the data to the machine or machines that will be processing the input.", "Note that you will likely need to use tf.distribute.Strategy.experimental_distribute_dataset with the returned dataset to further distribute it with the strategy.", "View source", "Runs ops in fn on each replica, with inputs from input_iterator.", "DEPRECATED: This method is not available in TF 2.x. Please switch to using run instead.", "When eager execution is enabled, executes ops specified by fn on each replica. Otherwise, builds a graph to execute the ops on each replica.", "Each replica will take a single, different input from the inputs provided by one get_next call on the input iterator.", "fn may call tf.distribute.get_replica_context() to access members such as replica_id_in_sync_group.", "View source", "Makes an iterator for input provided via dataset.", "DEPRECATED: This method is not available in TF 2.x.", "Data from the given dataset will be distributed evenly across all the compute replicas. We will assume that the input dataset is batched by the global batch size. With this assumption, we will make a best effort to divide each batch across all the replicas (one or more workers). If this effort fails, an error will be thrown, and the user should instead use make_input_fn_iterator which provides more control to the user, and does not try to divide a batch across replicas.", "The user could also use make_input_fn_iterator if they want to customize which input is fed to which replica/worker etc.", "View source", "Returns an iterator split across replicas created from an input function.", "DEPRECATED: This method is not available in TF 2.x.", "The input_fn should take an tf.distribute.InputContext object where information about batching and input sharding can be accessed:", "The tf.data.Dataset returned by input_fn should have a per-replica batch size, which may be computed using input_context.get_per_replica_batch_size.", "View source", "Reduce value across replicas and return result on current device.", "To see how this would look with multiple replicas, consider the same example with MirroredStrategy with 2 GPUs:", "This API is typically used for aggregating the results returned from different replicas, for reporting etc. For example, loss computed from different replicas can be averaged using this API before printing.", "There are a number of different tf.distribute APIs for reducing values across replicas:", "What should axis be?", "Given a per-replica value returned by run, say a per-example loss, the batch will be divided across all the replicas. This function allows you to aggregate across replicas and optionally also across batch elements by specifying the axis parameter accordingly.", "For example, if you have a global batch size of 8 and 2 replicas, values for examples [0, 1, 2, 3] will be on replica 0 and [4, 5, 6, 7] will be on replica 1. With axis=None, reduce will aggregate only across replicas, returning [0+4, 1+5, 2+6, 3+7]. This is useful when each replica is computing a scalar or some other value that doesn't have a \"batch\" dimension (like a gradient or loss).", "Sometimes, you will want to aggregate across both the global batch and all replicas. You can get this behavior by specifying the batch dimension as the axis, typically axis=0. In this case it would return a scalar 0+1+2+3+4+5+6+7.", "If there is a last partial batch, you will need to specify an axis so that the resulting shape is consistent across replicas. So if the last batch has size 6 and it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch unless you specify axis=0. If you specify tf.distribute.ReduceOp.MEAN, using axis=0 will use the correct denominator of 6. Contrast this with computing reduce_mean to get a scalar value on each replica and this function to average those means, which will weigh some values 1/8 and others 1/4.", "View source", "Invokes fn on each replica, with the given arguments.", "This method is the primary way to distribute your computation with a tf.distribute object. It invokes fn on each replica. If args or kwargs have tf.distribute.DistributedValues, such as those produced by a tf.distribute.DistributedDataset from tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function, when fn is executed on a particular replica, it will be executed with the component of tf.distribute.DistributedValues that correspond to that replica.", "fn is invoked under a replica context. fn may call tf.distribute.get_replica_context() to access members such as all_reduce. Please see the module-level docstring of tf.distribute for the concept of replica context.", "All arguments in args or kwargs should either be Python values of a nested structure of tensors, e.g. a list of tensors, in which case args and kwargs will be passed to the fn invoked on each replica. Or args or kwargs can be tf.distribute.DistributedValues containing tensors or composite tensors, i.e. tf.compat.v1.TensorInfo.CompositeTensor, in which case each fn call will get the component of a tf.distribute.DistributedValues corresponding to its replica.", "View source", "Context manager to make the strategy current and distribute variables.", "This method returns a context manager, and is used as follows:", "What happens when Strategy.scope is entered?", "What should be in scope and what should be outside?", "There are a number of requirements on what needs to happen inside the scope. However, in places where we have information about which strategy is in use, we often enter the scope for the user, so they don't have to do it explicitly (i.e. calling those either inside or outside the scope is OK).", "View source", "Returns a copy of config_proto modified for use with this strategy.", "DEPRECATED: This method is not available in TF 2.x.", "The updated config has something needed to run a strategy, e.g. configuration to run collective ops, or device filters to improve distributed training performance."]}, {"name": "tf.compat.v1.distribute.experimental.TPUStrategy", "path": "compat/v1/distribute/experimental/tpustrategy", "type": "tf.compat", "text": ["TPU distribution strategy implementation.", "Inherits From: Strategy", "In general, when using a multi-worker tf.distribute strategy such as tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy(), there is a tf.distribute.cluster_resolver.ClusterResolver associated with the strategy used, and such an instance is returned by this property.", "Strategies that intend to have an associated tf.distribute.cluster_resolver.ClusterResolver must set the relevant attribute, or override this property; otherwise, None is returned by default. Those strategies should also provide information regarding what is returned by this property.", "Single-worker strategies usually do not have a tf.distribute.cluster_resolver.ClusterResolver, and in those cases this property will return None.", "The tf.distribute.cluster_resolver.ClusterResolver may be useful when the user needs to access information such as the cluster spec, task type or task id. For example,", "For more information, please see tf.distribute.cluster_resolver.ClusterResolver's API docstring. ", "View source", "Distributes tf.data.Dataset instances created by calls to dataset_fn.", "The argument dataset_fn that users pass in is an input function that has a tf.distribute.InputContext argument and returns a tf.data.Dataset instance. It is expected that the returned dataset from dataset_fn is already batched by per-replica batch size (i.e. global batch size divided by the number of replicas in sync) and sharded. tf.distribute.Strategy.distribute_datasets_from_function does not batch or shard the tf.data.Dataset instance returned from the input function. dataset_fn will be called on the CPU device of each of the workers and each generates a dataset where every replica on that worker will dequeue one batch of inputs (i.e. if a worker has two replicas, two batches will be dequeued from the Dataset every step).", "This method can be used for several purposes. First, it allows you to specify your own batching and sharding logic. (In contrast, tf.distribute.experimental_distribute_dataset does batching and sharding for you.) For example, where experimental_distribute_dataset is unable to shard the input files, this method might be used to manually shard the dataset (avoiding the slow fallback behavior in experimental_distribute_dataset). In cases where the dataset is infinite, this sharding can be done by creating dataset replicas that differ only in their random seed.", "The dataset_fn should take an tf.distribute.InputContext instance where information about batching and input replication can be accessed.", "You can use element_spec property of the tf.distribute.DistributedDataset returned by this API to query the tf.TypeSpec of the elements returned by the iterator. This can be used to set the input_signature property of a tf.function. Follow tf.distribute.DistributedDataset.element_spec to see an example.", "For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input). If you are interested in last partial batch handling, read this section.", "View source", "Creates tf.distribute.DistributedDataset from tf.data.Dataset.", "The returned tf.distribute.DistributedDataset can be iterated over similar to regular datasets. NOTE: The user cannot add any more transformations to a tf.distribute.DistributedDataset. You can only create an iterator or examine the tf.TypeSpec of the data generated by it. See API docs of tf.distribute.DistributedDataset to learn more.", "The following is an example:", "Three key actions happending under the hood of this method are batching, sharding, and prefetching.", "In the code snippet above, dataset is batched by global_batch_size, and calling experimental_distribute_dataset on it rebatches dataset to a new batch size that is equal to the global batch size divided by the number of replicas in sync. We iterate through it using a Pythonic for loop. x is a tf.distribute.DistributedValues containing data for all replicas, and each replica gets data of the new batch size. tf.distribute.Strategy.run will take care of feeding the right per-replica data in x to the right replica_fn executed on each replica.", "Sharding contains autosharding across multiple workers and within every worker. First, in multi-worker distributed training (i.e. when you use tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy), autosharding a dataset over a set of workers means that each worker is assigned a subset of the entire dataset (if the right tf.data.experimental.AutoShardPolicy is set). This is to ensure that at each step, a global batch size of non-overlapping dataset elements will be processed by each worker. Autosharding has a couple of different options that can be specified using tf.data.experimental.DistributeOptions. Then, sharding within each worker means the method will split the data among all the worker devices (if more than one a present). This will happen regardless of multi-worker autosharding.", "By default, this method adds a prefetch transformation at the end of the user provided tf.data.Dataset instance. The argument to the prefetch transformation which is buffer_size is equal to the number of replicas in sync.", "If the above batch splitting and dataset sharding logic is undesirable, please use tf.distribute.Strategy.distribute_datasets_from_function instead, which does not do any automatic batching or sharding for you.", "For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input. If you are interested in last partial batch handling, read this section.", "View source", "Returns the list of all local per-replica values contained in value.", "View source", "Makes a tf.data.Dataset for input provided via a numpy array.", "This avoids adding numpy_input as a large constant in the graph, and copies the data to the machine or machines that will be processing the input.", "Note that you will likely need to use tf.distribute.Strategy.experimental_distribute_dataset with the returned dataset to further distribute it with the strategy.", "View source", "Runs ops in fn on each replica, with inputs from input_iterator.", "DEPRECATED: This method is not available in TF 2.x. Please switch to using run instead.", "When eager execution is enabled, executes ops specified by fn on each replica. Otherwise, builds a graph to execute the ops on each replica.", "Each replica will take a single, different input from the inputs provided by one get_next call on the input iterator.", "fn may call tf.distribute.get_replica_context() to access members such as replica_id_in_sync_group.", "View source", "Makes an iterator for input provided via dataset.", "DEPRECATED: This method is not available in TF 2.x.", "Data from the given dataset will be distributed evenly across all the compute replicas. We will assume that the input dataset is batched by the global batch size. With this assumption, we will make a best effort to divide each batch across all the replicas (one or more workers). If this effort fails, an error will be thrown, and the user should instead use make_input_fn_iterator which provides more control to the user, and does not try to divide a batch across replicas.", "The user could also use make_input_fn_iterator if they want to customize which input is fed to which replica/worker etc.", "View source", "Returns an iterator split across replicas created from an input function.", "DEPRECATED: This method is not available in TF 2.x.", "The input_fn should take an tf.distribute.InputContext object where information about batching and input sharding can be accessed:", "The tf.data.Dataset returned by input_fn should have a per-replica batch size, which may be computed using input_context.get_per_replica_batch_size.", "View source", "Reduce value across replicas and return result on current device.", "To see how this would look with multiple replicas, consider the same example with MirroredStrategy with 2 GPUs:", "This API is typically used for aggregating the results returned from different replicas, for reporting etc. For example, loss computed from different replicas can be averaged using this API before printing.", "There are a number of different tf.distribute APIs for reducing values across replicas:", "What should axis be?", "Given a per-replica value returned by run, say a per-example loss, the batch will be divided across all the replicas. This function allows you to aggregate across replicas and optionally also across batch elements by specifying the axis parameter accordingly.", "For example, if you have a global batch size of 8 and 2 replicas, values for examples [0, 1, 2, 3] will be on replica 0 and [4, 5, 6, 7] will be on replica 1. With axis=None, reduce will aggregate only across replicas, returning [0+4, 1+5, 2+6, 3+7]. This is useful when each replica is computing a scalar or some other value that doesn't have a \"batch\" dimension (like a gradient or loss).", "Sometimes, you will want to aggregate across both the global batch and all replicas. You can get this behavior by specifying the batch dimension as the axis, typically axis=0. In this case it would return a scalar 0+1+2+3+4+5+6+7.", "If there is a last partial batch, you will need to specify an axis so that the resulting shape is consistent across replicas. So if the last batch has size 6 and it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch unless you specify axis=0. If you specify tf.distribute.ReduceOp.MEAN, using axis=0 will use the correct denominator of 6. Contrast this with computing reduce_mean to get a scalar value on each replica and this function to average those means, which will weigh some values 1/8 and others 1/4.", "View source", "Run fn on each replica, with the given arguments.", "Executes ops specified by fn on each replica. If args or kwargs have \"per-replica\" values, such as those produced by a \"distributed Dataset\", when fn is executed on a particular replica, it will be executed with the component of those \"per-replica\" values that correspond to that replica.", "fn may call tf.distribute.get_replica_context() to access members such as all_reduce.", "All arguments in args or kwargs should either be nest of tensors or per-replica objects containing tensors or composite tensors.", "Users can pass strategy specific options to options argument. An example to enable bucketizing dynamic shapes in TPUStrategy.run is:", "View source", "Context manager to make the strategy current and distribute variables.", "This method returns a context manager, and is used as follows:", "What happens when Strategy.scope is entered?", "What should be in scope and what should be outside?", "There are a number of requirements on what needs to happen inside the scope. However, in places where we have information about which strategy is in use, we often enter the scope for the user, so they don't have to do it explicitly (i.e. calling those either inside or outside the scope is OK).", "View source", "Returns a copy of config_proto modified for use with this strategy.", "DEPRECATED: This method is not available in TF 2.x.", "The updated config has something needed to run a strategy, e.g. configuration to run collective ops, or device filters to improve distributed training performance."]}, {"name": "tf.compat.v1.distribute.get_loss_reduction", "path": "compat/v1/distribute/get_loss_reduction", "type": "tf.compat", "text": ["tf.distribute.ReduceOp corresponding to the last loss reduction.", "This is used to decide whether loss should be scaled in optimizer (used only for estimator + v1 optimizer use case)."]}, {"name": "tf.compat.v1.distribute.MirroredStrategy", "path": "compat/v1/distribute/mirroredstrategy", "type": "tf.compat", "text": ["Synchronous training across multiple replicas on one machine.", "Inherits From: Strategy", "This strategy is typically used for training on one machine with multiple GPUs. For TPUs, use tf.distribute.TPUStrategy. To use MirroredStrategy with multiple workers, please refer to tf.distribute.experimental.MultiWorkerMirroredStrategy.", "For example, a variable created under a MirroredStrategy is a MirroredVariable. If no devices are specified in the constructor argument of the strategy then it will use all the available GPUs. If no GPUs are found, it will use the available CPUs. Note that TensorFlow treats all CPUs on a machine as a single device, and uses threads internally for parallelism.", "While using distribution strategies, all the variable creation should be done within the strategy's scope. This will replicate the variables across all the replicas and keep them in sync using an all-reduce algorithm.", "Variables created inside a MirroredStrategy which is wrapped with a tf.function are still MirroredVariables.", "experimental_distribute_dataset can be used to distribute the dataset across the replicas when writing your own training loop. If you are using .fit and .compile methods available in tf.keras, then tf.keras will handle the distribution for you.", "In general, when using a multi-worker tf.distribute strategy such as tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy(), there is a tf.distribute.cluster_resolver.ClusterResolver associated with the strategy used, and such an instance is returned by this property.", "Strategies that intend to have an associated tf.distribute.cluster_resolver.ClusterResolver must set the relevant attribute, or override this property; otherwise, None is returned by default. Those strategies should also provide information regarding what is returned by this property.", "Single-worker strategies usually do not have a tf.distribute.cluster_resolver.ClusterResolver, and in those cases this property will return None.", "The tf.distribute.cluster_resolver.ClusterResolver may be useful when the user needs to access information such as the cluster spec, task type or task id. For example,", "For more information, please see tf.distribute.cluster_resolver.ClusterResolver's API docstring. ", "View source", "Distributes tf.data.Dataset instances created by calls to dataset_fn.", "The argument dataset_fn that users pass in is an input function that has a tf.distribute.InputContext argument and returns a tf.data.Dataset instance. It is expected that the returned dataset from dataset_fn is already batched by per-replica batch size (i.e. global batch size divided by the number of replicas in sync) and sharded. tf.distribute.Strategy.distribute_datasets_from_function does not batch or shard the tf.data.Dataset instance returned from the input function. dataset_fn will be called on the CPU device of each of the workers and each generates a dataset where every replica on that worker will dequeue one batch of inputs (i.e. if a worker has two replicas, two batches will be dequeued from the Dataset every step).", "This method can be used for several purposes. First, it allows you to specify your own batching and sharding logic. (In contrast, tf.distribute.experimental_distribute_dataset does batching and sharding for you.) For example, where experimental_distribute_dataset is unable to shard the input files, this method might be used to manually shard the dataset (avoiding the slow fallback behavior in experimental_distribute_dataset). In cases where the dataset is infinite, this sharding can be done by creating dataset replicas that differ only in their random seed.", "The dataset_fn should take an tf.distribute.InputContext instance where information about batching and input replication can be accessed.", "You can use element_spec property of the tf.distribute.DistributedDataset returned by this API to query the tf.TypeSpec of the elements returned by the iterator. This can be used to set the input_signature property of a tf.function. Follow tf.distribute.DistributedDataset.element_spec to see an example.", "For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input). If you are interested in last partial batch handling, read this section.", "View source", "Creates tf.distribute.DistributedDataset from tf.data.Dataset.", "The returned tf.distribute.DistributedDataset can be iterated over similar to regular datasets. NOTE: The user cannot add any more transformations to a tf.distribute.DistributedDataset. You can only create an iterator or examine the tf.TypeSpec of the data generated by it. See API docs of tf.distribute.DistributedDataset to learn more.", "The following is an example:", "Three key actions happending under the hood of this method are batching, sharding, and prefetching.", "In the code snippet above, dataset is batched by global_batch_size, and calling experimental_distribute_dataset on it rebatches dataset to a new batch size that is equal to the global batch size divided by the number of replicas in sync. We iterate through it using a Pythonic for loop. x is a tf.distribute.DistributedValues containing data for all replicas, and each replica gets data of the new batch size. tf.distribute.Strategy.run will take care of feeding the right per-replica data in x to the right replica_fn executed on each replica.", "Sharding contains autosharding across multiple workers and within every worker. First, in multi-worker distributed training (i.e. when you use tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy), autosharding a dataset over a set of workers means that each worker is assigned a subset of the entire dataset (if the right tf.data.experimental.AutoShardPolicy is set). This is to ensure that at each step, a global batch size of non-overlapping dataset elements will be processed by each worker. Autosharding has a couple of different options that can be specified using tf.data.experimental.DistributeOptions. Then, sharding within each worker means the method will split the data among all the worker devices (if more than one a present). This will happen regardless of multi-worker autosharding.", "By default, this method adds a prefetch transformation at the end of the user provided tf.data.Dataset instance. The argument to the prefetch transformation which is buffer_size is equal to the number of replicas in sync.", "If the above batch splitting and dataset sharding logic is undesirable, please use tf.distribute.Strategy.distribute_datasets_from_function instead, which does not do any automatic batching or sharding for you.", "For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input. If you are interested in last partial batch handling, read this section.", "View source", "Returns the list of all local per-replica values contained in value.", "View source", "Makes a tf.data.Dataset for input provided via a numpy array.", "This avoids adding numpy_input as a large constant in the graph, and copies the data to the machine or machines that will be processing the input.", "Note that you will likely need to use tf.distribute.Strategy.experimental_distribute_dataset with the returned dataset to further distribute it with the strategy.", "View source", "Runs ops in fn on each replica, with inputs from input_iterator.", "DEPRECATED: This method is not available in TF 2.x. Please switch to using run instead.", "When eager execution is enabled, executes ops specified by fn on each replica. Otherwise, builds a graph to execute the ops on each replica.", "Each replica will take a single, different input from the inputs provided by one get_next call on the input iterator.", "fn may call tf.distribute.get_replica_context() to access members such as replica_id_in_sync_group.", "View source", "Makes an iterator for input provided via dataset.", "DEPRECATED: This method is not available in TF 2.x.", "Data from the given dataset will be distributed evenly across all the compute replicas. We will assume that the input dataset is batched by the global batch size. With this assumption, we will make a best effort to divide each batch across all the replicas (one or more workers). If this effort fails, an error will be thrown, and the user should instead use make_input_fn_iterator which provides more control to the user, and does not try to divide a batch across replicas.", "The user could also use make_input_fn_iterator if they want to customize which input is fed to which replica/worker etc.", "View source", "Returns an iterator split across replicas created from an input function.", "DEPRECATED: This method is not available in TF 2.x.", "The input_fn should take an tf.distribute.InputContext object where information about batching and input sharding can be accessed:", "The tf.data.Dataset returned by input_fn should have a per-replica batch size, which may be computed using input_context.get_per_replica_batch_size.", "View source", "Reduce value across replicas and return result on current device.", "To see how this would look with multiple replicas, consider the same example with MirroredStrategy with 2 GPUs:", "This API is typically used for aggregating the results returned from different replicas, for reporting etc. For example, loss computed from different replicas can be averaged using this API before printing.", "There are a number of different tf.distribute APIs for reducing values across replicas:", "What should axis be?", "Given a per-replica value returned by run, say a per-example loss, the batch will be divided across all the replicas. This function allows you to aggregate across replicas and optionally also across batch elements by specifying the axis parameter accordingly.", "For example, if you have a global batch size of 8 and 2 replicas, values for examples [0, 1, 2, 3] will be on replica 0 and [4, 5, 6, 7] will be on replica 1. With axis=None, reduce will aggregate only across replicas, returning [0+4, 1+5, 2+6, 3+7]. This is useful when each replica is computing a scalar or some other value that doesn't have a \"batch\" dimension (like a gradient or loss).", "Sometimes, you will want to aggregate across both the global batch and all replicas. You can get this behavior by specifying the batch dimension as the axis, typically axis=0. In this case it would return a scalar 0+1+2+3+4+5+6+7.", "If there is a last partial batch, you will need to specify an axis so that the resulting shape is consistent across replicas. So if the last batch has size 6 and it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch unless you specify axis=0. If you specify tf.distribute.ReduceOp.MEAN, using axis=0 will use the correct denominator of 6. Contrast this with computing reduce_mean to get a scalar value on each replica and this function to average those means, which will weigh some values 1/8 and others 1/4.", "View source", "Invokes fn on each replica, with the given arguments.", "This method is the primary way to distribute your computation with a tf.distribute object. It invokes fn on each replica. If args or kwargs have tf.distribute.DistributedValues, such as those produced by a tf.distribute.DistributedDataset from tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function, when fn is executed on a particular replica, it will be executed with the component of tf.distribute.DistributedValues that correspond to that replica.", "fn is invoked under a replica context. fn may call tf.distribute.get_replica_context() to access members such as all_reduce. Please see the module-level docstring of tf.distribute for the concept of replica context.", "All arguments in args or kwargs should either be Python values of a nested structure of tensors, e.g. a list of tensors, in which case args and kwargs will be passed to the fn invoked on each replica. Or args or kwargs can be tf.distribute.DistributedValues containing tensors or composite tensors, i.e. tf.compat.v1.TensorInfo.CompositeTensor, in which case each fn call will get the component of a tf.distribute.DistributedValues corresponding to its replica.", "View source", "Context manager to make the strategy current and distribute variables.", "This method returns a context manager, and is used as follows:", "What happens when Strategy.scope is entered?", "What should be in scope and what should be outside?", "There are a number of requirements on what needs to happen inside the scope. However, in places where we have information about which strategy is in use, we often enter the scope for the user, so they don't have to do it explicitly (i.e. calling those either inside or outside the scope is OK).", "View source", "Returns a copy of config_proto modified for use with this strategy.", "DEPRECATED: This method is not available in TF 2.x.", "The updated config has something needed to run a strategy, e.g. configuration to run collective ops, or device filters to improve distributed training performance."]}, {"name": "tf.compat.v1.distribute.OneDeviceStrategy", "path": "compat/v1/distribute/onedevicestrategy", "type": "tf.compat", "text": ["A distribution strategy for running on a single device.", "Inherits From: Strategy", "Using this strategy will place any variables created in its scope on the specified device. Input distributed through this strategy will be prefetched to the specified device. Moreover, any functions called via strategy.run will also be placed on the specified device as well.", "Typical usage of this strategy could be testing your code with the tf.distribute.Strategy API before switching to other strategies which actually distribute to multiple devices/machines.", "In general, when using a multi-worker tf.distribute strategy such as tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy(), there is a tf.distribute.cluster_resolver.ClusterResolver associated with the strategy used, and such an instance is returned by this property.", "Strategies that intend to have an associated tf.distribute.cluster_resolver.ClusterResolver must set the relevant attribute, or override this property; otherwise, None is returned by default. Those strategies should also provide information regarding what is returned by this property.", "Single-worker strategies usually do not have a tf.distribute.cluster_resolver.ClusterResolver, and in those cases this property will return None.", "The tf.distribute.cluster_resolver.ClusterResolver may be useful when the user needs to access information such as the cluster spec, task type or task id. For example,", "For more information, please see tf.distribute.cluster_resolver.ClusterResolver's API docstring. ", "View source", "Distributes tf.data.Dataset instances created by calls to dataset_fn.", "The argument dataset_fn that users pass in is an input function that has a tf.distribute.InputContext argument and returns a tf.data.Dataset instance. It is expected that the returned dataset from dataset_fn is already batched by per-replica batch size (i.e. global batch size divided by the number of replicas in sync) and sharded. tf.distribute.Strategy.distribute_datasets_from_function does not batch or shard the tf.data.Dataset instance returned from the input function. dataset_fn will be called on the CPU device of each of the workers and each generates a dataset where every replica on that worker will dequeue one batch of inputs (i.e. if a worker has two replicas, two batches will be dequeued from the Dataset every step).", "This method can be used for several purposes. First, it allows you to specify your own batching and sharding logic. (In contrast, tf.distribute.experimental_distribute_dataset does batching and sharding for you.) For example, where experimental_distribute_dataset is unable to shard the input files, this method might be used to manually shard the dataset (avoiding the slow fallback behavior in experimental_distribute_dataset). In cases where the dataset is infinite, this sharding can be done by creating dataset replicas that differ only in their random seed.", "The dataset_fn should take an tf.distribute.InputContext instance where information about batching and input replication can be accessed.", "You can use element_spec property of the tf.distribute.DistributedDataset returned by this API to query the tf.TypeSpec of the elements returned by the iterator. This can be used to set the input_signature property of a tf.function. Follow tf.distribute.DistributedDataset.element_spec to see an example.", "For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input). If you are interested in last partial batch handling, read this section.", "View source", "Creates tf.distribute.DistributedDataset from tf.data.Dataset.", "The returned tf.distribute.DistributedDataset can be iterated over similar to regular datasets. NOTE: The user cannot add any more transformations to a tf.distribute.DistributedDataset. You can only create an iterator or examine the tf.TypeSpec of the data generated by it. See API docs of tf.distribute.DistributedDataset to learn more.", "The following is an example:", "Three key actions happending under the hood of this method are batching, sharding, and prefetching.", "In the code snippet above, dataset is batched by global_batch_size, and calling experimental_distribute_dataset on it rebatches dataset to a new batch size that is equal to the global batch size divided by the number of replicas in sync. We iterate through it using a Pythonic for loop. x is a tf.distribute.DistributedValues containing data for all replicas, and each replica gets data of the new batch size. tf.distribute.Strategy.run will take care of feeding the right per-replica data in x to the right replica_fn executed on each replica.", "Sharding contains autosharding across multiple workers and within every worker. First, in multi-worker distributed training (i.e. when you use tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy), autosharding a dataset over a set of workers means that each worker is assigned a subset of the entire dataset (if the right tf.data.experimental.AutoShardPolicy is set). This is to ensure that at each step, a global batch size of non-overlapping dataset elements will be processed by each worker. Autosharding has a couple of different options that can be specified using tf.data.experimental.DistributeOptions. Then, sharding within each worker means the method will split the data among all the worker devices (if more than one a present). This will happen regardless of multi-worker autosharding.", "By default, this method adds a prefetch transformation at the end of the user provided tf.data.Dataset instance. The argument to the prefetch transformation which is buffer_size is equal to the number of replicas in sync.", "If the above batch splitting and dataset sharding logic is undesirable, please use tf.distribute.Strategy.distribute_datasets_from_function instead, which does not do any automatic batching or sharding for you.", "For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input. If you are interested in last partial batch handling, read this section.", "View source", "Returns the list of all local per-replica values contained in value.", "View source", "Makes a tf.data.Dataset for input provided via a numpy array.", "This avoids adding numpy_input as a large constant in the graph, and copies the data to the machine or machines that will be processing the input.", "Note that you will likely need to use tf.distribute.Strategy.experimental_distribute_dataset with the returned dataset to further distribute it with the strategy.", "View source", "Runs ops in fn on each replica, with inputs from input_iterator.", "DEPRECATED: This method is not available in TF 2.x. Please switch to using run instead.", "When eager execution is enabled, executes ops specified by fn on each replica. Otherwise, builds a graph to execute the ops on each replica.", "Each replica will take a single, different input from the inputs provided by one get_next call on the input iterator.", "fn may call tf.distribute.get_replica_context() to access members such as replica_id_in_sync_group.", "View source", "Makes an iterator for input provided via dataset.", "DEPRECATED: This method is not available in TF 2.x.", "Data from the given dataset will be distributed evenly across all the compute replicas. We will assume that the input dataset is batched by the global batch size. With this assumption, we will make a best effort to divide each batch across all the replicas (one or more workers). If this effort fails, an error will be thrown, and the user should instead use make_input_fn_iterator which provides more control to the user, and does not try to divide a batch across replicas.", "The user could also use make_input_fn_iterator if they want to customize which input is fed to which replica/worker etc.", "View source", "Returns an iterator split across replicas created from an input function.", "DEPRECATED: This method is not available in TF 2.x.", "The input_fn should take an tf.distribute.InputContext object where information about batching and input sharding can be accessed:", "The tf.data.Dataset returned by input_fn should have a per-replica batch size, which may be computed using input_context.get_per_replica_batch_size.", "View source", "Reduce value across replicas and return result on current device.", "To see how this would look with multiple replicas, consider the same example with MirroredStrategy with 2 GPUs:", "This API is typically used for aggregating the results returned from different replicas, for reporting etc. For example, loss computed from different replicas can be averaged using this API before printing.", "There are a number of different tf.distribute APIs for reducing values across replicas:", "What should axis be?", "Given a per-replica value returned by run, say a per-example loss, the batch will be divided across all the replicas. This function allows you to aggregate across replicas and optionally also across batch elements by specifying the axis parameter accordingly.", "For example, if you have a global batch size of 8 and 2 replicas, values for examples [0, 1, 2, 3] will be on replica 0 and [4, 5, 6, 7] will be on replica 1. With axis=None, reduce will aggregate only across replicas, returning [0+4, 1+5, 2+6, 3+7]. This is useful when each replica is computing a scalar or some other value that doesn't have a \"batch\" dimension (like a gradient or loss).", "Sometimes, you will want to aggregate across both the global batch and all replicas. You can get this behavior by specifying the batch dimension as the axis, typically axis=0. In this case it would return a scalar 0+1+2+3+4+5+6+7.", "If there is a last partial batch, you will need to specify an axis so that the resulting shape is consistent across replicas. So if the last batch has size 6 and it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch unless you specify axis=0. If you specify tf.distribute.ReduceOp.MEAN, using axis=0 will use the correct denominator of 6. Contrast this with computing reduce_mean to get a scalar value on each replica and this function to average those means, which will weigh some values 1/8 and others 1/4.", "View source", "Invokes fn on each replica, with the given arguments.", "This method is the primary way to distribute your computation with a tf.distribute object. It invokes fn on each replica. If args or kwargs have tf.distribute.DistributedValues, such as those produced by a tf.distribute.DistributedDataset from tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function, when fn is executed on a particular replica, it will be executed with the component of tf.distribute.DistributedValues that correspond to that replica.", "fn is invoked under a replica context. fn may call tf.distribute.get_replica_context() to access members such as all_reduce. Please see the module-level docstring of tf.distribute for the concept of replica context.", "All arguments in args or kwargs should either be Python values of a nested structure of tensors, e.g. a list of tensors, in which case args and kwargs will be passed to the fn invoked on each replica. Or args or kwargs can be tf.distribute.DistributedValues containing tensors or composite tensors, i.e. tf.compat.v1.TensorInfo.CompositeTensor, in which case each fn call will get the component of a tf.distribute.DistributedValues corresponding to its replica.", "View source", "Context manager to make the strategy current and distribute variables.", "This method returns a context manager, and is used as follows:", "What happens when Strategy.scope is entered?", "What should be in scope and what should be outside?", "There are a number of requirements on what needs to happen inside the scope. However, in places where we have information about which strategy is in use, we often enter the scope for the user, so they don't have to do it explicitly (i.e. calling those either inside or outside the scope is OK).", "View source", "Returns a copy of config_proto modified for use with this strategy.", "DEPRECATED: This method is not available in TF 2.x.", "The updated config has something needed to run a strategy, e.g. configuration to run collective ops, or device filters to improve distributed training performance."]}, {"name": "tf.compat.v1.distribute.ReplicaContext", "path": "compat/v1/distribute/replicacontext", "type": "tf.compat", "text": ["A class with a collection of APIs that can be called in a replica context.", "You can use tf.distribute.get_replica_context to get an instance of ReplicaContext, which can only be called inside the function passed to tf.distribute.Strategy.run.", "This identifies the replica among all replicas that are kept in sync. The value of the replica id can range from 0 to tf.distribute.ReplicaContext.num_replicas_in_sync - 1.", "View source", "All-reduces value across all replicas.", "It supports batched operations. You can pass a list of values and it attempts to batch them when possible. You can also specify options to indicate the desired batching behavior, e.g. batch the values into multiple packs so that they can better overlap with computations.", "Note that all replicas need to participate in the all-reduce, otherwise this operation hangs. Note that if there're multiple all-reduces, they need to execute in the same order on all replicas. Dispatching all-reduce based on conditions is usually error-prone.", "This API currently can only be called in the replica context. Other variants to reduce values across replicas are:", "View source", "Merge args across replicas and run merge_fn in a cross-replica context.", "This allows communication and coordination when there are multiple calls to the step_fn triggered by a call to strategy.run(step_fn, ...).", "See tf.distribute.Strategy.run for an explanation.", "If not inside a distributed scope, this is equivalent to:"]}, {"name": "tf.compat.v1.distribute.Strategy", "path": "compat/v1/distribute/strategy", "type": "tf.compat", "text": ["A list of devices with a state & compute distribution policy.", "See the guide for overview and examples.", "In general, when using a multi-worker tf.distribute strategy such as tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy(), there is a tf.distribute.cluster_resolver.ClusterResolver associated with the strategy used, and such an instance is returned by this property.", "Strategies that intend to have an associated tf.distribute.cluster_resolver.ClusterResolver must set the relevant attribute, or override this property; otherwise, None is returned by default. Those strategies should also provide information regarding what is returned by this property.", "Single-worker strategies usually do not have a tf.distribute.cluster_resolver.ClusterResolver, and in those cases this property will return None.", "The tf.distribute.cluster_resolver.ClusterResolver may be useful when the user needs to access information such as the cluster spec, task type or task id. For example,", "For more information, please see tf.distribute.cluster_resolver.ClusterResolver's API docstring. ", "View source", "Distributes tf.data.Dataset instances created by calls to dataset_fn.", "The argument dataset_fn that users pass in is an input function that has a tf.distribute.InputContext argument and returns a tf.data.Dataset instance. It is expected that the returned dataset from dataset_fn is already batched by per-replica batch size (i.e. global batch size divided by the number of replicas in sync) and sharded. tf.distribute.Strategy.distribute_datasets_from_function does not batch or shard the tf.data.Dataset instance returned from the input function. dataset_fn will be called on the CPU device of each of the workers and each generates a dataset where every replica on that worker will dequeue one batch of inputs (i.e. if a worker has two replicas, two batches will be dequeued from the Dataset every step).", "This method can be used for several purposes. First, it allows you to specify your own batching and sharding logic. (In contrast, tf.distribute.experimental_distribute_dataset does batching and sharding for you.) For example, where experimental_distribute_dataset is unable to shard the input files, this method might be used to manually shard the dataset (avoiding the slow fallback behavior in experimental_distribute_dataset). In cases where the dataset is infinite, this sharding can be done by creating dataset replicas that differ only in their random seed.", "The dataset_fn should take an tf.distribute.InputContext instance where information about batching and input replication can be accessed.", "You can use element_spec property of the tf.distribute.DistributedDataset returned by this API to query the tf.TypeSpec of the elements returned by the iterator. This can be used to set the input_signature property of a tf.function. Follow tf.distribute.DistributedDataset.element_spec to see an example.", "For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input). If you are interested in last partial batch handling, read this section.", "View source", "Creates tf.distribute.DistributedDataset from tf.data.Dataset.", "The returned tf.distribute.DistributedDataset can be iterated over similar to regular datasets. NOTE: The user cannot add any more transformations to a tf.distribute.DistributedDataset. You can only create an iterator or examine the tf.TypeSpec of the data generated by it. See API docs of tf.distribute.DistributedDataset to learn more.", "The following is an example:", "Three key actions happending under the hood of this method are batching, sharding, and prefetching.", "In the code snippet above, dataset is batched by global_batch_size, and calling experimental_distribute_dataset on it rebatches dataset to a new batch size that is equal to the global batch size divided by the number of replicas in sync. We iterate through it using a Pythonic for loop. x is a tf.distribute.DistributedValues containing data for all replicas, and each replica gets data of the new batch size. tf.distribute.Strategy.run will take care of feeding the right per-replica data in x to the right replica_fn executed on each replica.", "Sharding contains autosharding across multiple workers and within every worker. First, in multi-worker distributed training (i.e. when you use tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy), autosharding a dataset over a set of workers means that each worker is assigned a subset of the entire dataset (if the right tf.data.experimental.AutoShardPolicy is set). This is to ensure that at each step, a global batch size of non-overlapping dataset elements will be processed by each worker. Autosharding has a couple of different options that can be specified using tf.data.experimental.DistributeOptions. Then, sharding within each worker means the method will split the data among all the worker devices (if more than one a present). This will happen regardless of multi-worker autosharding.", "By default, this method adds a prefetch transformation at the end of the user provided tf.data.Dataset instance. The argument to the prefetch transformation which is buffer_size is equal to the number of replicas in sync.", "If the above batch splitting and dataset sharding logic is undesirable, please use tf.distribute.Strategy.distribute_datasets_from_function instead, which does not do any automatic batching or sharding for you.", "For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input. If you are interested in last partial batch handling, read this section.", "View source", "Returns the list of all local per-replica values contained in value.", "View source", "Makes a tf.data.Dataset for input provided via a numpy array.", "This avoids adding numpy_input as a large constant in the graph, and copies the data to the machine or machines that will be processing the input.", "Note that you will likely need to use tf.distribute.Strategy.experimental_distribute_dataset with the returned dataset to further distribute it with the strategy.", "View source", "Runs ops in fn on each replica, with inputs from input_iterator.", "DEPRECATED: This method is not available in TF 2.x. Please switch to using run instead.", "When eager execution is enabled, executes ops specified by fn on each replica. Otherwise, builds a graph to execute the ops on each replica.", "Each replica will take a single, different input from the inputs provided by one get_next call on the input iterator.", "fn may call tf.distribute.get_replica_context() to access members such as replica_id_in_sync_group.", "View source", "Makes an iterator for input provided via dataset.", "DEPRECATED: This method is not available in TF 2.x.", "Data from the given dataset will be distributed evenly across all the compute replicas. We will assume that the input dataset is batched by the global batch size. With this assumption, we will make a best effort to divide each batch across all the replicas (one or more workers). If this effort fails, an error will be thrown, and the user should instead use make_input_fn_iterator which provides more control to the user, and does not try to divide a batch across replicas.", "The user could also use make_input_fn_iterator if they want to customize which input is fed to which replica/worker etc.", "View source", "Returns an iterator split across replicas created from an input function.", "DEPRECATED: This method is not available in TF 2.x.", "The input_fn should take an tf.distribute.InputContext object where information about batching and input sharding can be accessed:", "The tf.data.Dataset returned by input_fn should have a per-replica batch size, which may be computed using input_context.get_per_replica_batch_size.", "View source", "Reduce value across replicas and return result on current device.", "To see how this would look with multiple replicas, consider the same example with MirroredStrategy with 2 GPUs:", "This API is typically used for aggregating the results returned from different replicas, for reporting etc. For example, loss computed from different replicas can be averaged using this API before printing.", "There are a number of different tf.distribute APIs for reducing values across replicas:", "What should axis be?", "Given a per-replica value returned by run, say a per-example loss, the batch will be divided across all the replicas. This function allows you to aggregate across replicas and optionally also across batch elements by specifying the axis parameter accordingly.", "For example, if you have a global batch size of 8 and 2 replicas, values for examples [0, 1, 2, 3] will be on replica 0 and [4, 5, 6, 7] will be on replica 1. With axis=None, reduce will aggregate only across replicas, returning [0+4, 1+5, 2+6, 3+7]. This is useful when each replica is computing a scalar or some other value that doesn't have a \"batch\" dimension (like a gradient or loss).", "Sometimes, you will want to aggregate across both the global batch and all replicas. You can get this behavior by specifying the batch dimension as the axis, typically axis=0. In this case it would return a scalar 0+1+2+3+4+5+6+7.", "If there is a last partial batch, you will need to specify an axis so that the resulting shape is consistent across replicas. So if the last batch has size 6 and it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch unless you specify axis=0. If you specify tf.distribute.ReduceOp.MEAN, using axis=0 will use the correct denominator of 6. Contrast this with computing reduce_mean to get a scalar value on each replica and this function to average those means, which will weigh some values 1/8 and others 1/4.", "View source", "Invokes fn on each replica, with the given arguments.", "This method is the primary way to distribute your computation with a tf.distribute object. It invokes fn on each replica. If args or kwargs have tf.distribute.DistributedValues, such as those produced by a tf.distribute.DistributedDataset from tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function, when fn is executed on a particular replica, it will be executed with the component of tf.distribute.DistributedValues that correspond to that replica.", "fn is invoked under a replica context. fn may call tf.distribute.get_replica_context() to access members such as all_reduce. Please see the module-level docstring of tf.distribute for the concept of replica context.", "All arguments in args or kwargs should either be Python values of a nested structure of tensors, e.g. a list of tensors, in which case args and kwargs will be passed to the fn invoked on each replica. Or args or kwargs can be tf.distribute.DistributedValues containing tensors or composite tensors, i.e. tf.compat.v1.TensorInfo.CompositeTensor, in which case each fn call will get the component of a tf.distribute.DistributedValues corresponding to its replica.", "View source", "Context manager to make the strategy current and distribute variables.", "This method returns a context manager, and is used as follows:", "What happens when Strategy.scope is entered?", "What should be in scope and what should be outside?", "There are a number of requirements on what needs to happen inside the scope. However, in places where we have information about which strategy is in use, we often enter the scope for the user, so they don't have to do it explicitly (i.e. calling those either inside or outside the scope is OK).", "View source", "Returns a copy of config_proto modified for use with this strategy.", "DEPRECATED: This method is not available in TF 2.x.", "The updated config has something needed to run a strategy, e.g. configuration to run collective ops, or device filters to improve distributed training performance."]}, {"name": "tf.compat.v1.distribute.StrategyExtended", "path": "compat/v1/distribute/strategyextended", "type": "tf.compat", "text": ["Additional APIs for algorithms that need to be distribution-aware.", "Inherits From: StrategyExtended", "Some common use cases of functions on this page:", "tf.distribute.DistributedValues can have the same locality as a distributed variable, which leads to a mirrored value residing on the same devices as the variable (as opposed to the compute devices). Such values may be passed to a call to tf.distribute.StrategyExtended.update to update the value of a variable. You may use tf.distribute.StrategyExtended.colocate_vars_with to give a variable the same locality as another variable. You may convert a \"PerReplica\" value to a variable's locality by using tf.distribute.StrategyExtended.reduce_to or tf.distribute.StrategyExtended.batch_reduce_to.", "A distributed variable is variables created on multiple devices. As discussed in the glossary, mirrored variable and SyncOnRead variable are two examples. The standard pattern for updating distributed variables is to:", "Steps 2 through 4 are done automatically by class tf.keras.optimizers.Optimizer if you call its tf.keras.optimizers.Optimizer.apply_gradients method in a replica context.", "In fact, a higher-level solution to update a distributed variable is by calling assign on the variable as you would do to a regular tf.Variable. You can call the method in both replica context and cross-replica context. For a mirrored variable, calling assign in replica context requires you to specify the aggregation type in the variable constructor. In that case, the context switching and sync described in steps 2 through 4 are handled for you. If you call assign on mirrored variable in cross-replica context, you can only assign a single value or assign values from another mirrored variable or a mirrored tf.distribute.DistributedValues. For a SyncOnRead variable, in replica context, you can simply call assign on it and no aggregation happens under the hood. In cross-replica context, you can only assign a single value to a SyncOnRead variable. One example case is restoring from a checkpoint: if the aggregation type of the variable is tf.VariableAggregation.SUM, it is assumed that replica values were added before checkpointing, so at the time of restoring, the value is divided by the number of replicas and then assigned to each replica; if the aggregation type is tf.VariableAggregation.MEAN, the value is assigned to each replica directly.", "This is expected to return a constant value that will not be changed throughout its life cycle. ", "View source", "Combine multiple reduce_to calls into one for faster execution.", "Similar to reduce_to, but accepts a list of (value, destinations) pairs. It's more efficient than reduce each value separately.", "This API currently can only be called in cross-replica context. Other variants to reduce values across replicas are:", "See reduce_to for more information.", "View source", "Mirror a tensor on one device to all worker devices.", "View source", "Run fn once per replica.", "fn may call tf.get_replica_context() to access methods such as replica_id_in_sync_group and merge_call().", "merge_call() is used to communicate between the replicas and re-enter the cross-replica context. All replicas pause their execution having encountered a merge_call() call. After that the merge_fn-function is executed. Its results are then unwrapped and given back to each replica call. After that execution resumes until fn is complete or encounters another merge_call(). Example:", "View source", "Scope that controls which devices variables will be created on.", "No operations should be added to the graph inside this scope, it should only be used when creating variables (some implementations work by changing variable creation, others work by using a tf.compat.v1.colocate_with() scope).", "This may only be used inside self.scope().", "View source", "Makes a dataset for input provided via a numpy array.", "This avoids adding numpy_input as a large constant in the graph, and copies the data to the machine or machines that will be processing the input.", "View source", "DEPRECATED: please use run instead.", "Run fn with input from iterator for iterations times.", "This method can be used to run a step function for training a number of times using input from a dataset.", "View source", "Device(s) for non-slot variables.", "DEPRECATED: TF 1.x ONLY.", "This method returns non-slot devices where non-slot variables are placed. Users can create non-slot variables on these devices by using a block:", "View source", "Reads the value of a variable.", "Returns the aggregate value of a replica-local variable, or the (read-only) value of any other variable.", "View source", "Combine (via e.g. sum or mean) values across replicas.", "reduce_to aggregates tf.distribute.DistributedValues and distributed variables. It supports both dense values and tf.IndexedSlices.", "This API currently can only be called in cross-replica context. Other variants to reduce values across replicas are:", "destinations specifies where to reduce the value to, e.g. \"GPU:0\". You can also pass in a Tensor, and the destinations will be the device of that tensor. For all-reduce, pass the same to value and destinations.", "It can be used in tf.distribute.ReplicaContext.merge_call to write code that works for all tf.distribute.Strategy.", "View source", "Run fn to update var using inputs mirrored to the same devices.", "tf.distribute.StrategyExtended.update takes a distributed variable var to be updated, an update function fn, and args and kwargs for fn. It applies fn to each component variable of var and passes corresponding values from args and kwargs. Neither args nor kwargs may contain per-replica values. If they contain mirrored values, they will be unwrapped before calling fn. For example, fn can be assign_add and args can be a mirrored DistributedValues where each component contains the value to be added to this mirrored variable var. Calling update will call assign_add on each component variable of var with the corresponding tensor value on that device.", "If var is mirrored across multiple devices, then this method implements logic as following:", "Otherwise, this method returns fn(var, *args, **kwargs) colocated with var.", "View source", "Runs fn(*args, **kwargs) on colocate_with devices.", "Used to update non-slot variables.", "DEPRECATED: TF 1.x ONLY.", "View source", "Returns the container that this per-replica value belongs to.", "View source", "Tests whether v was created while this strategy scope was active.", "Variables created inside the strategy scope are \"owned\" by it:", "Variables created outside the strategy are not owned by it:"]}, {"name": "tf.compat.v1.distributions", "path": "compat/v1/distributions", "type": "tf.compat", "text": ["Core module for TensorFlow distribution objects and helpers.", "class Bernoulli: Bernoulli distribution.", "class Beta: Beta distribution.", "class Categorical: Categorical distribution.", "class Dirichlet: Dirichlet distribution.", "class DirichletMultinomial: Dirichlet-Multinomial compound distribution.", "class Distribution: A generic probability distribution base class.", "class Exponential: Exponential distribution.", "class Gamma: Gamma distribution.", "class Laplace: The Laplace distribution with location loc and scale parameters.", "class Multinomial: Multinomial distribution.", "class Normal: The Normal distribution with location loc and scale parameters.", "class RegisterKL: Decorator to register a KL divergence implementation function.", "class ReparameterizationType: Instances of this class represent how sampling is reparameterized.", "class StudentT: Student's t-distribution.", "class Uniform: Uniform distribution with low and high parameters.", "kl_divergence(...): Get the KL-divergence KL(distribution_a || distribution_b). (deprecated)"]}, {"name": "tf.compat.v1.distributions.Bernoulli", "path": "compat/v1/distributions/bernoulli", "type": "tf.compat", "text": ["Bernoulli distribution.", "Inherits From: Distribution", "The Bernoulli distribution with probs parameter, i.e., the probability of a 1 outcome (vs a 0 outcome).", "Stats return +/- infinity when it makes sense. E.g., the variance of a Cauchy distribution is infinity. However, sometimes the statistic is undefined, e.g., if a distribution's pdf does not achieve a maximum within the support of the distribution, the mode is undefined. If the mean is undefined, then by definition the variance is undefined. E.g. the mean for Student's T for df = 1 is undefined (no clear way to say it is either + or - infinity), so the variance = E[(X - mean)**2] is also undefined. ", "May be partially defined or unknown.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution. ", "May be partially defined or unknown. ", "Currently this is one of the static instances distributions.FULLY_REPARAMETERIZED or distributions.NOT_REPARAMETERIZED. ", "View source", "Shape of a single sample from a single event index as a 1-D Tensor.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution.", "View source", "Cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "View source", "Creates a deep copy of the distribution.", "View source", "Covariance.", "Covariance is (possibly) defined only for non-scalar-event distributions.", "For example, for a length-k, vector-valued distribution, it is calculated as,", "where Cov is a (batch of) k x k matrix, 0 <= (i, j) < k, and E denotes expectation.", "Alternatively, for non-vector, multivariate distributions (e.g., matrix-valued, Wishart), Covariance shall return a (batch of) matrices under some vectorization of the events, i.e.,", "where Cov is a (batch of) k' x k' matrices, 0 <= (i, j) < k' = reduce_prod(event_shape), and Vec is some function mapping indices of this distribution's event dimensions to indices of a length-k' vector.", "View source", "Computes the (Shannon) cross entropy.", "Denote this distribution (self) by P and the other distribution by Q. Assuming P, Q are absolutely continuous with respect to one another and permit densities p(x) dr(x) and q(x) dr(x), (Shanon) cross entropy is defined as:", "where F denotes the support of the random variable X ~ P.", "View source", "Shannon entropy in nats.", "View source", "Shape of a single sample from a single batch as a 1-D int32 Tensor.", "View source", "Indicates that batch_shape == [].", "View source", "Indicates that event_shape == [].", "View source", "Computes the Kullback--Leibler divergence.", "Denote this distribution (self) by p and the other distribution by q. Assuming p, q are absolutely continuous with respect to reference measure r, the KL divergence is defined as:", "where F denotes the support of the random variable X ~ p, H[., .] denotes (Shanon) cross entropy, and H[.] denotes (Shanon) entropy.", "View source", "Log cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "Often, a numerical approximation can be used for log_cdf(x) that yields a more accurate answer than simply taking the logarithm of the cdf when x << -1.", "View source", "Log probability density/mass function.", "View source", "Log survival function.", "Given random variable X, the survival function is defined:", "Typically, different numerical approximations can be used for the log survival function, which are more accurate than 1 - cdf(x) when x >> 1.", "View source", "Mean.", "View source", "Mode.", "Additional documentation from Bernoulli:", "Returns 1 if prob > 0.5 and 0 otherwise.", "View source", "Shapes of parameters given the desired shape of a call to sample().", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample().", "Subclasses should override class method _param_shapes.", "View source", "param_shapes with static (i.e. TensorShape) shapes.", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample(). Assumes that the sample's shape is known statically.", "Subclasses should override class method _param_shapes to return constant-valued tensors when constant values are fed.", "View source", "Probability density/mass function.", "View source", "Quantile function. Aka \"inverse cdf\" or \"percent point function\".", "Given random variable X and p in [0, 1], the quantile is:", "View source", "Generate samples of the specified shape.", "Note that a call to sample() without arguments will generate a single sample.", "View source", "Standard deviation.", "Standard deviation is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and stddev.shape = batch_shape + event_shape.", "View source", "Survival function.", "Given random variable X, the survival function is defined:", "View source", "Variance.", "Variance is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and Var.shape = batch_shape + event_shape."]}, {"name": "tf.compat.v1.distributions.Beta", "path": "compat/v1/distributions/beta", "type": "tf.compat", "text": ["Beta distribution.", "Inherits From: Distribution", "The Beta distribution is defined over the (0, 1) interval using parameters concentration1 (aka \"alpha\") and concentration0 (aka \"beta\").", "The probability density function (pdf) is,", "where:", "The concentration parameters represent mean total counts of a 1 or a 0, i.e.,", "where mean in (0, 1) and total_concentration is a positive real number representing a mean total_count = concentration1 + concentration0.", "Distribution parameters are automatically broadcast in all functions; see examples for details.", "Samples of this distribution are reparameterized (pathwise differentiable). The derivatives are computed using the approach described in (Figurnov et al., 2018).", "Compute the gradients of samples w.r.t. the parameters:", "Implicit Reparameterization Gradients: Figurnov et al., 2018 (pdf)", "Stats return +/- infinity when it makes sense. E.g., the variance of a Cauchy distribution is infinity. However, sometimes the statistic is undefined, e.g., if a distribution's pdf does not achieve a maximum within the support of the distribution, the mode is undefined. If the mean is undefined, then by definition the variance is undefined. E.g. the mean for Student's T for df = 1 is undefined (no clear way to say it is either + or - infinity), so the variance = E[(X - mean)**2] is also undefined. ", "May be partially defined or unknown.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution. ", "May be partially defined or unknown. ", "Currently this is one of the static instances distributions.FULLY_REPARAMETERIZED or distributions.NOT_REPARAMETERIZED. ", "View source", "Shape of a single sample from a single event index as a 1-D Tensor.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution.", "View source", "Cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "Additional documentation from Beta:", "View source", "Creates a deep copy of the distribution.", "View source", "Covariance.", "Covariance is (possibly) defined only for non-scalar-event distributions.", "For example, for a length-k, vector-valued distribution, it is calculated as,", "where Cov is a (batch of) k x k matrix, 0 <= (i, j) < k, and E denotes expectation.", "Alternatively, for non-vector, multivariate distributions (e.g., matrix-valued, Wishart), Covariance shall return a (batch of) matrices under some vectorization of the events, i.e.,", "where Cov is a (batch of) k' x k' matrices, 0 <= (i, j) < k' = reduce_prod(event_shape), and Vec is some function mapping indices of this distribution's event dimensions to indices of a length-k' vector.", "View source", "Computes the (Shannon) cross entropy.", "Denote this distribution (self) by P and the other distribution by Q. Assuming P, Q are absolutely continuous with respect to one another and permit densities p(x) dr(x) and q(x) dr(x), (Shanon) cross entropy is defined as:", "where F denotes the support of the random variable X ~ P.", "View source", "Shannon entropy in nats.", "View source", "Shape of a single sample from a single batch as a 1-D int32 Tensor.", "View source", "Indicates that batch_shape == [].", "View source", "Indicates that event_shape == [].", "View source", "Computes the Kullback--Leibler divergence.", "Denote this distribution (self) by p and the other distribution by q. Assuming p, q are absolutely continuous with respect to reference measure r, the KL divergence is defined as:", "where F denotes the support of the random variable X ~ p, H[., .] denotes (Shanon) cross entropy, and H[.] denotes (Shanon) entropy.", "View source", "Log cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "Often, a numerical approximation can be used for log_cdf(x) that yields a more accurate answer than simply taking the logarithm of the cdf when x << -1.", "Additional documentation from Beta:", "View source", "Log probability density/mass function.", "Additional documentation from Beta:", "View source", "Log survival function.", "Given random variable X, the survival function is defined:", "Typically, different numerical approximations can be used for the log survival function, which are more accurate than 1 - cdf(x) when x >> 1.", "View source", "Mean.", "View source", "Mode.", "Additional documentation from Beta:", "View source", "Shapes of parameters given the desired shape of a call to sample().", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample().", "Subclasses should override class method _param_shapes.", "View source", "param_shapes with static (i.e. TensorShape) shapes.", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample(). Assumes that the sample's shape is known statically.", "Subclasses should override class method _param_shapes to return constant-valued tensors when constant values are fed.", "View source", "Probability density/mass function.", "Additional documentation from Beta:", "View source", "Quantile function. Aka \"inverse cdf\" or \"percent point function\".", "Given random variable X and p in [0, 1], the quantile is:", "View source", "Generate samples of the specified shape.", "Note that a call to sample() without arguments will generate a single sample.", "View source", "Standard deviation.", "Standard deviation is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and stddev.shape = batch_shape + event_shape.", "View source", "Survival function.", "Given random variable X, the survival function is defined:", "View source", "Variance.", "Variance is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and Var.shape = batch_shape + event_shape."]}, {"name": "tf.compat.v1.distributions.Categorical", "path": "compat/v1/distributions/categorical", "type": "tf.compat", "text": ["Categorical distribution.", "Inherits From: Distribution", "The Categorical distribution is parameterized by either probabilities or log-probabilities of a set of K classes. It is defined over the integers {0, 1, ..., K}.", "The Categorical distribution is closely related to the OneHotCategorical and Multinomial distributions. The Categorical distribution can be intuited as generating samples according to argmax{ OneHotCategorical(probs) } itself being identical to argmax{ Multinomial(probs, total_count=1) }.", "The probability mass function (pmf) is,", "The number of classes, K, must not exceed:", "In other words,", "Creates a 3-class distribution with the 2nd class being most likely.", "Creates a 3-class distribution with the 2nd class being most likely. Parameterized by logits rather than probabilities.", "Creates a 3-class distribution with the 3rd class being most likely. The distribution functions can be evaluated on counts.", "Stats return +/- infinity when it makes sense. E.g., the variance of a Cauchy distribution is infinity. However, sometimes the statistic is undefined, e.g., if a distribution's pdf does not achieve a maximum within the support of the distribution, the mode is undefined. If the mean is undefined, then by definition the variance is undefined. E.g. the mean for Student's T for df = 1 is undefined (no clear way to say it is either + or - infinity), so the variance = E[(X - mean)**2] is also undefined. ", "May be partially defined or unknown.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution. ", "May be partially defined or unknown. ", "Currently this is one of the static instances distributions.FULLY_REPARAMETERIZED or distributions.NOT_REPARAMETERIZED. ", "View source", "Shape of a single sample from a single event index as a 1-D Tensor.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution.", "View source", "Cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "View source", "Creates a deep copy of the distribution.", "View source", "Covariance.", "Covariance is (possibly) defined only for non-scalar-event distributions.", "For example, for a length-k, vector-valued distribution, it is calculated as,", "where Cov is a (batch of) k x k matrix, 0 <= (i, j) < k, and E denotes expectation.", "Alternatively, for non-vector, multivariate distributions (e.g., matrix-valued, Wishart), Covariance shall return a (batch of) matrices under some vectorization of the events, i.e.,", "where Cov is a (batch of) k' x k' matrices, 0 <= (i, j) < k' = reduce_prod(event_shape), and Vec is some function mapping indices of this distribution's event dimensions to indices of a length-k' vector.", "View source", "Computes the (Shannon) cross entropy.", "Denote this distribution (self) by P and the other distribution by Q. Assuming P, Q are absolutely continuous with respect to one another and permit densities p(x) dr(x) and q(x) dr(x), (Shanon) cross entropy is defined as:", "where F denotes the support of the random variable X ~ P.", "View source", "Shannon entropy in nats.", "View source", "Shape of a single sample from a single batch as a 1-D int32 Tensor.", "View source", "Indicates that batch_shape == [].", "View source", "Indicates that event_shape == [].", "View source", "Computes the Kullback--Leibler divergence.", "Denote this distribution (self) by p and the other distribution by q. Assuming p, q are absolutely continuous with respect to reference measure r, the KL divergence is defined as:", "where F denotes the support of the random variable X ~ p, H[., .] denotes (Shanon) cross entropy, and H[.] denotes (Shanon) entropy.", "View source", "Log cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "Often, a numerical approximation can be used for log_cdf(x) that yields a more accurate answer than simply taking the logarithm of the cdf when x << -1.", "View source", "Log probability density/mass function.", "View source", "Log survival function.", "Given random variable X, the survival function is defined:", "Typically, different numerical approximations can be used for the log survival function, which are more accurate than 1 - cdf(x) when x >> 1.", "View source", "Mean.", "View source", "Mode.", "View source", "Shapes of parameters given the desired shape of a call to sample().", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample().", "Subclasses should override class method _param_shapes.", "View source", "param_shapes with static (i.e. TensorShape) shapes.", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample(). Assumes that the sample's shape is known statically.", "Subclasses should override class method _param_shapes to return constant-valued tensors when constant values are fed.", "View source", "Probability density/mass function.", "View source", "Quantile function. Aka \"inverse cdf\" or \"percent point function\".", "Given random variable X and p in [0, 1], the quantile is:", "View source", "Generate samples of the specified shape.", "Note that a call to sample() without arguments will generate a single sample.", "View source", "Standard deviation.", "Standard deviation is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and stddev.shape = batch_shape + event_shape.", "View source", "Survival function.", "Given random variable X, the survival function is defined:", "View source", "Variance.", "Variance is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and Var.shape = batch_shape + event_shape."]}, {"name": "tf.compat.v1.distributions.Dirichlet", "path": "compat/v1/distributions/dirichlet", "type": "tf.compat", "text": ["Dirichlet distribution.", "Inherits From: Distribution", "The Dirichlet distribution is defined over the (k-1)-simplex using a positive, length-k vector concentration (k > 1). The Dirichlet is identically the Beta distribution when k = 2.", "The Dirichlet is a distribution over the open (k-1)-simplex, i.e.,", "The probability density function (pdf) is,", "where:", "The concentration represents mean total counts of class occurrence, i.e.,", "where mean in S^{k-1} and total_concentration is a positive real number representing a mean total count.", "Distribution parameters are automatically broadcast in all functions; see examples for details.", "Samples of this distribution are reparameterized (pathwise differentiable). The derivatives are computed using the approach described in (Figurnov et al., 2018).", "Compute the gradients of samples w.r.t. the parameters:", "Implicit Reparameterization Gradients: Figurnov et al., 2018 (pdf)", "Stats return +/- infinity when it makes sense. E.g., the variance of a Cauchy distribution is infinity. However, sometimes the statistic is undefined, e.g., if a distribution's pdf does not achieve a maximum within the support of the distribution, the mode is undefined. If the mean is undefined, then by definition the variance is undefined. E.g. the mean for Student's T for df = 1 is undefined (no clear way to say it is either + or - infinity), so the variance = E[(X - mean)**2] is also undefined. ", "May be partially defined or unknown.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution. ", "May be partially defined or unknown. ", "Currently this is one of the static instances distributions.FULLY_REPARAMETERIZED or distributions.NOT_REPARAMETERIZED. ", "View source", "Shape of a single sample from a single event index as a 1-D Tensor.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution.", "View source", "Cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "View source", "Creates a deep copy of the distribution.", "View source", "Covariance.", "Covariance is (possibly) defined only for non-scalar-event distributions.", "For example, for a length-k, vector-valued distribution, it is calculated as,", "where Cov is a (batch of) k x k matrix, 0 <= (i, j) < k, and E denotes expectation.", "Alternatively, for non-vector, multivariate distributions (e.g., matrix-valued, Wishart), Covariance shall return a (batch of) matrices under some vectorization of the events, i.e.,", "where Cov is a (batch of) k' x k' matrices, 0 <= (i, j) < k' = reduce_prod(event_shape), and Vec is some function mapping indices of this distribution's event dimensions to indices of a length-k' vector.", "View source", "Computes the (Shannon) cross entropy.", "Denote this distribution (self) by P and the other distribution by Q. Assuming P, Q are absolutely continuous with respect to one another and permit densities p(x) dr(x) and q(x) dr(x), (Shanon) cross entropy is defined as:", "where F denotes the support of the random variable X ~ P.", "View source", "Shannon entropy in nats.", "View source", "Shape of a single sample from a single batch as a 1-D int32 Tensor.", "View source", "Indicates that batch_shape == [].", "View source", "Indicates that event_shape == [].", "View source", "Computes the Kullback--Leibler divergence.", "Denote this distribution (self) by p and the other distribution by q. Assuming p, q are absolutely continuous with respect to reference measure r, the KL divergence is defined as:", "where F denotes the support of the random variable X ~ p, H[., .] denotes (Shanon) cross entropy, and H[.] denotes (Shanon) entropy.", "View source", "Log cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "Often, a numerical approximation can be used for log_cdf(x) that yields a more accurate answer than simply taking the logarithm of the cdf when x << -1.", "View source", "Log probability density/mass function.", "Additional documentation from Dirichlet:", "View source", "Log survival function.", "Given random variable X, the survival function is defined:", "Typically, different numerical approximations can be used for the log survival function, which are more accurate than 1 - cdf(x) when x >> 1.", "View source", "Mean.", "View source", "Mode.", "Additional documentation from Dirichlet:", "View source", "Shapes of parameters given the desired shape of a call to sample().", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample().", "Subclasses should override class method _param_shapes.", "View source", "param_shapes with static (i.e. TensorShape) shapes.", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample(). Assumes that the sample's shape is known statically.", "Subclasses should override class method _param_shapes to return constant-valued tensors when constant values are fed.", "View source", "Probability density/mass function.", "Additional documentation from Dirichlet:", "View source", "Quantile function. Aka \"inverse cdf\" or \"percent point function\".", "Given random variable X and p in [0, 1], the quantile is:", "View source", "Generate samples of the specified shape.", "Note that a call to sample() without arguments will generate a single sample.", "View source", "Standard deviation.", "Standard deviation is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and stddev.shape = batch_shape + event_shape.", "View source", "Survival function.", "Given random variable X, the survival function is defined:", "View source", "Variance.", "Variance is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and Var.shape = batch_shape + event_shape."]}, {"name": "tf.compat.v1.distributions.DirichletMultinomial", "path": "compat/v1/distributions/dirichletmultinomial", "type": "tf.compat", "text": ["Dirichlet-Multinomial compound distribution.", "Inherits From: Distribution", "The Dirichlet-Multinomial distribution is parameterized by a (batch of) length-K concentration vectors (K > 1) and a total_count number of trials, i.e., the number of trials per draw from the DirichletMultinomial. It is defined over a (batch of) length-K vector counts such that tf.reduce_sum(counts, -1) = total_count. The Dirichlet-Multinomial is identically the Beta-Binomial distribution when K = 2.", "The Dirichlet-Multinomial is a distribution over K-class counts, i.e., a length-K vector of non-negative integer counts = n = [n_0, ..., n_{K-1}].", "The probability mass function (pmf) is,", "where:", "Dirichlet-Multinomial is a compound distribution, i.e., its samples are generated as follows.", "The last concentration dimension parametrizes a single Dirichlet-Multinomial distribution. When calling distribution functions (e.g., dist.prob(counts)), concentration, total_count and counts are broadcast to the same shape. The last dimension of counts corresponds single Dirichlet-Multinomial distributions.", "Distribution parameters are automatically broadcast in all functions; see examples for details.", "The number of classes, K, must not exceed:", "In other words,", "Creates a 3-class distribution, with the 3rd class is most likely to be drawn. The distribution functions can be evaluated on counts.", "Creates a 2-batch of 3-class distributions.", "Stats return +/- infinity when it makes sense. E.g., the variance of a Cauchy distribution is infinity. However, sometimes the statistic is undefined, e.g., if a distribution's pdf does not achieve a maximum within the support of the distribution, the mode is undefined. If the mean is undefined, then by definition the variance is undefined. E.g. the mean for Student's T for df = 1 is undefined (no clear way to say it is either + or - infinity), so the variance = E[(X - mean)**2] is also undefined. ", "May be partially defined or unknown.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution. ", "May be partially defined or unknown. ", "Currently this is one of the static instances distributions.FULLY_REPARAMETERIZED or distributions.NOT_REPARAMETERIZED. ", "View source", "Shape of a single sample from a single event index as a 1-D Tensor.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution.", "View source", "Cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "View source", "Creates a deep copy of the distribution.", "View source", "Covariance.", "Covariance is (possibly) defined only for non-scalar-event distributions.", "For example, for a length-k, vector-valued distribution, it is calculated as,", "where Cov is a (batch of) k x k matrix, 0 <= (i, j) < k, and E denotes expectation.", "Alternatively, for non-vector, multivariate distributions (e.g., matrix-valued, Wishart), Covariance shall return a (batch of) matrices under some vectorization of the events, i.e.,", "where Cov is a (batch of) k' x k' matrices, 0 <= (i, j) < k' = reduce_prod(event_shape), and Vec is some function mapping indices of this distribution's event dimensions to indices of a length-k' vector.", "Additional documentation from DirichletMultinomial:", "The covariance for each batch member is defined as the following:", "where concentration = alpha and total_concentration = alpha_0 = sum_j alpha_j.", "The covariance between elements in a batch is defined as:", "View source", "Computes the (Shannon) cross entropy.", "Denote this distribution (self) by P and the other distribution by Q. Assuming P, Q are absolutely continuous with respect to one another and permit densities p(x) dr(x) and q(x) dr(x), (Shanon) cross entropy is defined as:", "where F denotes the support of the random variable X ~ P.", "View source", "Shannon entropy in nats.", "View source", "Shape of a single sample from a single batch as a 1-D int32 Tensor.", "View source", "Indicates that batch_shape == [].", "View source", "Indicates that event_shape == [].", "View source", "Computes the Kullback--Leibler divergence.", "Denote this distribution (self) by p and the other distribution by q. Assuming p, q are absolutely continuous with respect to reference measure r, the KL divergence is defined as:", "where F denotes the support of the random variable X ~ p, H[., .] denotes (Shanon) cross entropy, and H[.] denotes (Shanon) entropy.", "View source", "Log cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "Often, a numerical approximation can be used for log_cdf(x) that yields a more accurate answer than simply taking the logarithm of the cdf when x << -1.", "View source", "Log probability density/mass function.", "Additional documentation from DirichletMultinomial:", "For each batch of counts, value = [n_0, ..., n_{K-1}], P[value] is the probability that after sampling self.total_count draws from this Dirichlet-Multinomial distribution, the number of draws falling in class j is n_j. Since this definition is exchangeable; different sequences have the same counts so the probability includes a combinatorial coefficient.", "View source", "Log survival function.", "Given random variable X, the survival function is defined:", "Typically, different numerical approximations can be used for the log survival function, which are more accurate than 1 - cdf(x) when x >> 1.", "View source", "Mean.", "View source", "Mode.", "View source", "Shapes of parameters given the desired shape of a call to sample().", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample().", "Subclasses should override class method _param_shapes.", "View source", "param_shapes with static (i.e. TensorShape) shapes.", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample(). Assumes that the sample's shape is known statically.", "Subclasses should override class method _param_shapes to return constant-valued tensors when constant values are fed.", "View source", "Probability density/mass function.", "Additional documentation from DirichletMultinomial:", "For each batch of counts, value = [n_0, ..., n_{K-1}], P[value] is the probability that after sampling self.total_count draws from this Dirichlet-Multinomial distribution, the number of draws falling in class j is n_j. Since this definition is exchangeable; different sequences have the same counts so the probability includes a combinatorial coefficient.", "View source", "Quantile function. Aka \"inverse cdf\" or \"percent point function\".", "Given random variable X and p in [0, 1], the quantile is:", "View source", "Generate samples of the specified shape.", "Note that a call to sample() without arguments will generate a single sample.", "View source", "Standard deviation.", "Standard deviation is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and stddev.shape = batch_shape + event_shape.", "View source", "Survival function.", "Given random variable X, the survival function is defined:", "View source", "Variance.", "Variance is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and Var.shape = batch_shape + event_shape."]}, {"name": "tf.compat.v1.distributions.Distribution", "path": "compat/v1/distributions/distribution", "type": "tf.compat", "text": ["A generic probability distribution base class.", "Distribution is a base class for constructing and organizing properties (e.g., mean, variance) of random variables (e.g, Bernoulli, Gaussian).", "Subclasses are expected to implement a leading-underscore version of the same-named function. The argument signature should be identical except for the omission of name=\"...\". For example, to enable log_prob(value, name=\"log_prob\") a subclass should implement _log_prob(value).", "Subclasses can append to public-level docstrings by providing docstrings for their method specializations. For example:", "would add the string \"Some other details.\" to the log_prob function docstring. This is implemented as a simple decorator to avoid python linter complaining about missing Args/Returns/Raises sections in the partial docstrings.", "All distributions support batches of independent distributions of that type. The batch shape is determined by broadcasting together the parameters.", "The shape of arguments to __init__, cdf, log_cdf, prob, and log_prob reflect this broadcasting, as does the return value of sample and sample_n.", "sample_n_shape = [n] + batch_shape + event_shape, where sample_n_shape is the shape of the Tensor returned from sample_n, n is the number of samples, batch_shape defines how many independent distributions there are, and event_shape defines the shape of samples from each of those independent distributions. Samples are independent along the batch_shape dimensions, but not necessarily so along the event_shape dimensions (depending on the particulars of the underlying distribution).", "Using the Uniform distribution as an example:", "There are three important concepts associated with TensorFlow Distributions shapes:", "The event shape and the batch shape are properties of a Distribution object, whereas the sample shape is associated with a specific call to sample or log_prob.", "For detailed usage examples of TensorFlow Distributions shapes, see this tutorial", "Some distributions do not have well-defined statistics for all initialization parameter values. For example, the beta distribution is parameterized by positive real numbers concentration1 and concentration0, and does not have well-defined mode if concentration1 < 1 or concentration0 < 1.", "The user is given the option of raising an exception or returning NaN.", "In all cases, an exception is raised if invalid parameters are passed, e.g.", "Stats return +/- infinity when it makes sense. E.g., the variance of a Cauchy distribution is infinity. However, sometimes the statistic is undefined, e.g., if a distribution's pdf does not achieve a maximum within the support of the distribution, the mode is undefined. If the mean is undefined, then by definition the variance is undefined. E.g. the mean for Student's T for df = 1 is undefined (no clear way to say it is either + or - infinity), so the variance = E[(X - mean)**2] is also undefined. ", "May be partially defined or unknown.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution. ", "May be partially defined or unknown. ", "Currently this is one of the static instances distributions.FULLY_REPARAMETERIZED or distributions.NOT_REPARAMETERIZED. ", "View source", "Shape of a single sample from a single event index as a 1-D Tensor.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution.", "View source", "Cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "View source", "Creates a deep copy of the distribution.", "View source", "Covariance.", "Covariance is (possibly) defined only for non-scalar-event distributions.", "For example, for a length-k, vector-valued distribution, it is calculated as,", "where Cov is a (batch of) k x k matrix, 0 <= (i, j) < k, and E denotes expectation.", "Alternatively, for non-vector, multivariate distributions (e.g., matrix-valued, Wishart), Covariance shall return a (batch of) matrices under some vectorization of the events, i.e.,", "where Cov is a (batch of) k' x k' matrices, 0 <= (i, j) < k' = reduce_prod(event_shape), and Vec is some function mapping indices of this distribution's event dimensions to indices of a length-k' vector.", "View source", "Computes the (Shannon) cross entropy.", "Denote this distribution (self) by P and the other distribution by Q. Assuming P, Q are absolutely continuous with respect to one another and permit densities p(x) dr(x) and q(x) dr(x), (Shanon) cross entropy is defined as:", "where F denotes the support of the random variable X ~ P.", "View source", "Shannon entropy in nats.", "View source", "Shape of a single sample from a single batch as a 1-D int32 Tensor.", "View source", "Indicates that batch_shape == [].", "View source", "Indicates that event_shape == [].", "View source", "Computes the Kullback--Leibler divergence.", "Denote this distribution (self) by p and the other distribution by q. Assuming p, q are absolutely continuous with respect to reference measure r, the KL divergence is defined as:", "where F denotes the support of the random variable X ~ p, H[., .] denotes (Shanon) cross entropy, and H[.] denotes (Shanon) entropy.", "View source", "Log cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "Often, a numerical approximation can be used for log_cdf(x) that yields a more accurate answer than simply taking the logarithm of the cdf when x << -1.", "View source", "Log probability density/mass function.", "View source", "Log survival function.", "Given random variable X, the survival function is defined:", "Typically, different numerical approximations can be used for the log survival function, which are more accurate than 1 - cdf(x) when x >> 1.", "View source", "Mean.", "View source", "Mode.", "View source", "Shapes of parameters given the desired shape of a call to sample().", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample().", "Subclasses should override class method _param_shapes.", "View source", "param_shapes with static (i.e. TensorShape) shapes.", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample(). Assumes that the sample's shape is known statically.", "Subclasses should override class method _param_shapes to return constant-valued tensors when constant values are fed.", "View source", "Probability density/mass function.", "View source", "Quantile function. Aka \"inverse cdf\" or \"percent point function\".", "Given random variable X and p in [0, 1], the quantile is:", "View source", "Generate samples of the specified shape.", "Note that a call to sample() without arguments will generate a single sample.", "View source", "Standard deviation.", "Standard deviation is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and stddev.shape = batch_shape + event_shape.", "View source", "Survival function.", "Given random variable X, the survival function is defined:", "View source", "Variance.", "Variance is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and Var.shape = batch_shape + event_shape."]}, {"name": "tf.compat.v1.distributions.Exponential", "path": "compat/v1/distributions/exponential", "type": "tf.compat", "text": ["Exponential distribution.", "Inherits From: Gamma, Distribution", "The Exponential distribution is parameterized by an event rate parameter.", "The probability density function (pdf) is,", "where rate = lambda and Z is the normalizaing constant.", "The Exponential distribution is a special case of the Gamma distribution, i.e.,", "The Exponential distribution uses a rate parameter, or \"inverse scale\", which can be intuited as,", "Stats return +/- infinity when it makes sense. E.g., the variance of a Cauchy distribution is infinity. However, sometimes the statistic is undefined, e.g., if a distribution's pdf does not achieve a maximum within the support of the distribution, the mode is undefined. If the mean is undefined, then by definition the variance is undefined. E.g. the mean for Student's T for df = 1 is undefined (no clear way to say it is either + or - infinity), so the variance = E[(X - mean)**2] is also undefined. ", "May be partially defined or unknown.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution. ", "May be partially defined or unknown. ", "Currently this is one of the static instances distributions.FULLY_REPARAMETERIZED or distributions.NOT_REPARAMETERIZED. ", "View source", "Shape of a single sample from a single event index as a 1-D Tensor.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution.", "View source", "Cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "View source", "Creates a deep copy of the distribution.", "View source", "Covariance.", "Covariance is (possibly) defined only for non-scalar-event distributions.", "For example, for a length-k, vector-valued distribution, it is calculated as,", "where Cov is a (batch of) k x k matrix, 0 <= (i, j) < k, and E denotes expectation.", "Alternatively, for non-vector, multivariate distributions (e.g., matrix-valued, Wishart), Covariance shall return a (batch of) matrices under some vectorization of the events, i.e.,", "where Cov is a (batch of) k' x k' matrices, 0 <= (i, j) < k' = reduce_prod(event_shape), and Vec is some function mapping indices of this distribution's event dimensions to indices of a length-k' vector.", "View source", "Computes the (Shannon) cross entropy.", "Denote this distribution (self) by P and the other distribution by Q. Assuming P, Q are absolutely continuous with respect to one another and permit densities p(x) dr(x) and q(x) dr(x), (Shanon) cross entropy is defined as:", "where F denotes the support of the random variable X ~ P.", "View source", "Shannon entropy in nats.", "View source", "Shape of a single sample from a single batch as a 1-D int32 Tensor.", "View source", "Indicates that batch_shape == [].", "View source", "Indicates that event_shape == [].", "View source", "Computes the Kullback--Leibler divergence.", "Denote this distribution (self) by p and the other distribution by q. Assuming p, q are absolutely continuous with respect to reference measure r, the KL divergence is defined as:", "where F denotes the support of the random variable X ~ p, H[., .] denotes (Shanon) cross entropy, and H[.] denotes (Shanon) entropy.", "View source", "Log cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "Often, a numerical approximation can be used for log_cdf(x) that yields a more accurate answer than simply taking the logarithm of the cdf when x << -1.", "View source", "Log probability density/mass function.", "View source", "Log survival function.", "Given random variable X, the survival function is defined:", "Typically, different numerical approximations can be used for the log survival function, which are more accurate than 1 - cdf(x) when x >> 1.", "View source", "Mean.", "View source", "Mode.", "Additional documentation from Gamma:", "The mode of a gamma distribution is (shape - 1) / rate when shape > 1, and NaN otherwise. If self.allow_nan_stats is False, an exception will be raised rather than returning NaN.", "View source", "Shapes of parameters given the desired shape of a call to sample().", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample().", "Subclasses should override class method _param_shapes.", "View source", "param_shapes with static (i.e. TensorShape) shapes.", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample(). Assumes that the sample's shape is known statically.", "Subclasses should override class method _param_shapes to return constant-valued tensors when constant values are fed.", "View source", "Probability density/mass function.", "View source", "Quantile function. Aka \"inverse cdf\" or \"percent point function\".", "Given random variable X and p in [0, 1], the quantile is:", "View source", "Generate samples of the specified shape.", "Note that a call to sample() without arguments will generate a single sample.", "View source", "Standard deviation.", "Standard deviation is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and stddev.shape = batch_shape + event_shape.", "View source", "Survival function.", "Given random variable X, the survival function is defined:", "View source", "Variance.", "Variance is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and Var.shape = batch_shape + event_shape."]}, {"name": "tf.compat.v1.distributions.Gamma", "path": "compat/v1/distributions/gamma", "type": "tf.compat", "text": ["Gamma distribution.", "Inherits From: Distribution", "The Gamma distribution is defined over positive real numbers using parameters concentration (aka \"alpha\") and rate (aka \"beta\").", "The probability density function (pdf) is,", "where:", "The cumulative density function (cdf) is,", "where GammaInc is the lower incomplete Gamma function.", "The parameters can be intuited via their relationship to mean and stddev,", "Distribution parameters are automatically broadcast in all functions; see examples for details.", "Samples of this distribution are reparameterized (pathwise differentiable). The derivatives are computed using the approach described in (Figurnov et al., 2018).", "Compute the gradients of samples w.r.t. the parameters:", "Implicit Reparameterization Gradients: Figurnov et al., 2018 (pdf)", "Stats return +/- infinity when it makes sense. E.g., the variance of a Cauchy distribution is infinity. However, sometimes the statistic is undefined, e.g., if a distribution's pdf does not achieve a maximum within the support of the distribution, the mode is undefined. If the mean is undefined, then by definition the variance is undefined. E.g. the mean for Student's T for df = 1 is undefined (no clear way to say it is either + or - infinity), so the variance = E[(X - mean)**2] is also undefined. ", "May be partially defined or unknown.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution. ", "May be partially defined or unknown. ", "Currently this is one of the static instances distributions.FULLY_REPARAMETERIZED or distributions.NOT_REPARAMETERIZED. ", "View source", "Shape of a single sample from a single event index as a 1-D Tensor.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution.", "View source", "Cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "View source", "Creates a deep copy of the distribution.", "View source", "Covariance.", "Covariance is (possibly) defined only for non-scalar-event distributions.", "For example, for a length-k, vector-valued distribution, it is calculated as,", "where Cov is a (batch of) k x k matrix, 0 <= (i, j) < k, and E denotes expectation.", "Alternatively, for non-vector, multivariate distributions (e.g., matrix-valued, Wishart), Covariance shall return a (batch of) matrices under some vectorization of the events, i.e.,", "where Cov is a (batch of) k' x k' matrices, 0 <= (i, j) < k' = reduce_prod(event_shape), and Vec is some function mapping indices of this distribution's event dimensions to indices of a length-k' vector.", "View source", "Computes the (Shannon) cross entropy.", "Denote this distribution (self) by P and the other distribution by Q. Assuming P, Q are absolutely continuous with respect to one another and permit densities p(x) dr(x) and q(x) dr(x), (Shanon) cross entropy is defined as:", "where F denotes the support of the random variable X ~ P.", "View source", "Shannon entropy in nats.", "View source", "Shape of a single sample from a single batch as a 1-D int32 Tensor.", "View source", "Indicates that batch_shape == [].", "View source", "Indicates that event_shape == [].", "View source", "Computes the Kullback--Leibler divergence.", "Denote this distribution (self) by p and the other distribution by q. Assuming p, q are absolutely continuous with respect to reference measure r, the KL divergence is defined as:", "where F denotes the support of the random variable X ~ p, H[., .] denotes (Shanon) cross entropy, and H[.] denotes (Shanon) entropy.", "View source", "Log cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "Often, a numerical approximation can be used for log_cdf(x) that yields a more accurate answer than simply taking the logarithm of the cdf when x << -1.", "View source", "Log probability density/mass function.", "View source", "Log survival function.", "Given random variable X, the survival function is defined:", "Typically, different numerical approximations can be used for the log survival function, which are more accurate than 1 - cdf(x) when x >> 1.", "View source", "Mean.", "View source", "Mode.", "Additional documentation from Gamma:", "The mode of a gamma distribution is (shape - 1) / rate when shape > 1, and NaN otherwise. If self.allow_nan_stats is False, an exception will be raised rather than returning NaN.", "View source", "Shapes of parameters given the desired shape of a call to sample().", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample().", "Subclasses should override class method _param_shapes.", "View source", "param_shapes with static (i.e. TensorShape) shapes.", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample(). Assumes that the sample's shape is known statically.", "Subclasses should override class method _param_shapes to return constant-valued tensors when constant values are fed.", "View source", "Probability density/mass function.", "View source", "Quantile function. Aka \"inverse cdf\" or \"percent point function\".", "Given random variable X and p in [0, 1], the quantile is:", "View source", "Generate samples of the specified shape.", "Note that a call to sample() without arguments will generate a single sample.", "View source", "Standard deviation.", "Standard deviation is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and stddev.shape = batch_shape + event_shape.", "View source", "Survival function.", "Given random variable X, the survival function is defined:", "View source", "Variance.", "Variance is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and Var.shape = batch_shape + event_shape."]}, {"name": "tf.compat.v1.distributions.kl_divergence", "path": "compat/v1/distributions/kl_divergence", "type": "tf.compat", "text": ["Get the KL-divergence KL(distribution_a || distribution_b). (deprecated)", "If there is no KL method registered specifically for type(distribution_a) and type(distribution_b), then the class hierarchies of these types are searched.", "If one KL method is registered between any pairs of classes in these two parent hierarchies, it is used.", "If more than one such registered method exists, the method whose registered classes have the shortest sum MRO paths to the input types is used.", "If more than one such shortest path exists, the first method identified in the search is used (favoring a shorter MRO distance to type(distribution_a))."]}, {"name": "tf.compat.v1.distributions.Laplace", "path": "compat/v1/distributions/laplace", "type": "tf.compat", "text": ["The Laplace distribution with location loc and scale parameters.", "Inherits From: Distribution", "The probability density function (pdf) of this distribution is,", "where loc = mu, scale = sigma, and Z is the normalization constant.", "Note that the Laplace distribution can be thought of two exponential distributions spliced together \"back-to-back.\"", "The Lpalce distribution is a member of the location-scale family, i.e., it can be constructed as,", "Stats return +/- infinity when it makes sense. E.g., the variance of a Cauchy distribution is infinity. However, sometimes the statistic is undefined, e.g., if a distribution's pdf does not achieve a maximum within the support of the distribution, the mode is undefined. If the mean is undefined, then by definition the variance is undefined. E.g. the mean for Student's T for df = 1 is undefined (no clear way to say it is either + or - infinity), so the variance = E[(X - mean)**2] is also undefined. ", "May be partially defined or unknown.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution. ", "May be partially defined or unknown. ", "Currently this is one of the static instances distributions.FULLY_REPARAMETERIZED or distributions.NOT_REPARAMETERIZED. ", "View source", "Shape of a single sample from a single event index as a 1-D Tensor.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution.", "View source", "Cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "View source", "Creates a deep copy of the distribution.", "View source", "Covariance.", "Covariance is (possibly) defined only for non-scalar-event distributions.", "For example, for a length-k, vector-valued distribution, it is calculated as,", "where Cov is a (batch of) k x k matrix, 0 <= (i, j) < k, and E denotes expectation.", "Alternatively, for non-vector, multivariate distributions (e.g., matrix-valued, Wishart), Covariance shall return a (batch of) matrices under some vectorization of the events, i.e.,", "where Cov is a (batch of) k' x k' matrices, 0 <= (i, j) < k' = reduce_prod(event_shape), and Vec is some function mapping indices of this distribution's event dimensions to indices of a length-k' vector.", "View source", "Computes the (Shannon) cross entropy.", "Denote this distribution (self) by P and the other distribution by Q. Assuming P, Q are absolutely continuous with respect to one another and permit densities p(x) dr(x) and q(x) dr(x), (Shanon) cross entropy is defined as:", "where F denotes the support of the random variable X ~ P.", "View source", "Shannon entropy in nats.", "View source", "Shape of a single sample from a single batch as a 1-D int32 Tensor.", "View source", "Indicates that batch_shape == [].", "View source", "Indicates that event_shape == [].", "View source", "Computes the Kullback--Leibler divergence.", "Denote this distribution (self) by p and the other distribution by q. Assuming p, q are absolutely continuous with respect to reference measure r, the KL divergence is defined as:", "where F denotes the support of the random variable X ~ p, H[., .] denotes (Shanon) cross entropy, and H[.] denotes (Shanon) entropy.", "View source", "Log cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "Often, a numerical approximation can be used for log_cdf(x) that yields a more accurate answer than simply taking the logarithm of the cdf when x << -1.", "View source", "Log probability density/mass function.", "View source", "Log survival function.", "Given random variable X, the survival function is defined:", "Typically, different numerical approximations can be used for the log survival function, which are more accurate than 1 - cdf(x) when x >> 1.", "View source", "Mean.", "View source", "Mode.", "View source", "Shapes of parameters given the desired shape of a call to sample().", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample().", "Subclasses should override class method _param_shapes.", "View source", "param_shapes with static (i.e. TensorShape) shapes.", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample(). Assumes that the sample's shape is known statically.", "Subclasses should override class method _param_shapes to return constant-valued tensors when constant values are fed.", "View source", "Probability density/mass function.", "View source", "Quantile function. Aka \"inverse cdf\" or \"percent point function\".", "Given random variable X and p in [0, 1], the quantile is:", "View source", "Generate samples of the specified shape.", "Note that a call to sample() without arguments will generate a single sample.", "View source", "Standard deviation.", "Standard deviation is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and stddev.shape = batch_shape + event_shape.", "View source", "Survival function.", "Given random variable X, the survival function is defined:", "View source", "Variance.", "Variance is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and Var.shape = batch_shape + event_shape."]}, {"name": "tf.compat.v1.distributions.Multinomial", "path": "compat/v1/distributions/multinomial", "type": "tf.compat", "text": ["Multinomial distribution.", "Inherits From: Distribution", "This Multinomial distribution is parameterized by probs, a (batch of) length-K prob (probability) vectors (K > 1) such that tf.reduce_sum(probs, -1) = 1, and a total_count number of trials, i.e., the number of trials per draw from the Multinomial. It is defined over a (batch of) length-K vector counts such that tf.reduce_sum(counts, -1) = total_count. The Multinomial is identically the Binomial distribution when K = 2.", "The Multinomial is a distribution over K-class counts, i.e., a length-K vector of non-negative integer counts = n = [n_0, ..., n_{K-1}].", "The probability mass function (pmf) is,", "where:", "Distribution parameters are automatically broadcast in all functions; see examples for details.", "The number of classes, K, must not exceed:", "In other words,", "Create a 3-class distribution, with the 3rd class is most likely to be drawn, using logits.", "Create a 3-class distribution, with the 3rd class is most likely to be drawn.", "The distribution functions can be evaluated on counts.", "Create a 2-batch of 3-class distributions.", "Stats return +/- infinity when it makes sense. E.g., the variance of a Cauchy distribution is infinity. However, sometimes the statistic is undefined, e.g., if a distribution's pdf does not achieve a maximum within the support of the distribution, the mode is undefined. If the mean is undefined, then by definition the variance is undefined. E.g. the mean for Student's T for df = 1 is undefined (no clear way to say it is either + or - infinity), so the variance = E[(X - mean)**2] is also undefined. ", "May be partially defined or unknown.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution. ", "May be partially defined or unknown. ", "Currently this is one of the static instances distributions.FULLY_REPARAMETERIZED or distributions.NOT_REPARAMETERIZED. ", "View source", "Shape of a single sample from a single event index as a 1-D Tensor.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution.", "View source", "Cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "View source", "Creates a deep copy of the distribution.", "View source", "Covariance.", "Covariance is (possibly) defined only for non-scalar-event distributions.", "For example, for a length-k, vector-valued distribution, it is calculated as,", "where Cov is a (batch of) k x k matrix, 0 <= (i, j) < k, and E denotes expectation.", "Alternatively, for non-vector, multivariate distributions (e.g., matrix-valued, Wishart), Covariance shall return a (batch of) matrices under some vectorization of the events, i.e.,", "where Cov is a (batch of) k' x k' matrices, 0 <= (i, j) < k' = reduce_prod(event_shape), and Vec is some function mapping indices of this distribution's event dimensions to indices of a length-k' vector.", "View source", "Computes the (Shannon) cross entropy.", "Denote this distribution (self) by P and the other distribution by Q. Assuming P, Q are absolutely continuous with respect to one another and permit densities p(x) dr(x) and q(x) dr(x), (Shanon) cross entropy is defined as:", "where F denotes the support of the random variable X ~ P.", "View source", "Shannon entropy in nats.", "View source", "Shape of a single sample from a single batch as a 1-D int32 Tensor.", "View source", "Indicates that batch_shape == [].", "View source", "Indicates that event_shape == [].", "View source", "Computes the Kullback--Leibler divergence.", "Denote this distribution (self) by p and the other distribution by q. Assuming p, q are absolutely continuous with respect to reference measure r, the KL divergence is defined as:", "where F denotes the support of the random variable X ~ p, H[., .] denotes (Shanon) cross entropy, and H[.] denotes (Shanon) entropy.", "View source", "Log cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "Often, a numerical approximation can be used for log_cdf(x) that yields a more accurate answer than simply taking the logarithm of the cdf when x << -1.", "View source", "Log probability density/mass function.", "Additional documentation from Multinomial:", "For each batch of counts, value = [n_0, ... ,n_{k-1}], P[value] is the probability that after sampling self.total_count draws from this Multinomial distribution, the number of draws falling in class j is n_j. Since this definition is exchangeable; different sequences have the same counts so the probability includes a combinatorial coefficient.", "View source", "Log survival function.", "Given random variable X, the survival function is defined:", "Typically, different numerical approximations can be used for the log survival function, which are more accurate than 1 - cdf(x) when x >> 1.", "View source", "Mean.", "View source", "Mode.", "View source", "Shapes of parameters given the desired shape of a call to sample().", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample().", "Subclasses should override class method _param_shapes.", "View source", "param_shapes with static (i.e. TensorShape) shapes.", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample(). Assumes that the sample's shape is known statically.", "Subclasses should override class method _param_shapes to return constant-valued tensors when constant values are fed.", "View source", "Probability density/mass function.", "View source", "Quantile function. Aka \"inverse cdf\" or \"percent point function\".", "Given random variable X and p in [0, 1], the quantile is:", "View source", "Generate samples of the specified shape.", "Note that a call to sample() without arguments will generate a single sample.", "View source", "Standard deviation.", "Standard deviation is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and stddev.shape = batch_shape + event_shape.", "View source", "Survival function.", "Given random variable X, the survival function is defined:", "View source", "Variance.", "Variance is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and Var.shape = batch_shape + event_shape."]}, {"name": "tf.compat.v1.distributions.Normal", "path": "compat/v1/distributions/normal", "type": "tf.compat", "text": ["The Normal distribution with location loc and scale parameters.", "Inherits From: Distribution", "The probability density function (pdf) is,", "where loc = mu is the mean, scale = sigma is the std. deviation, and, Z is the normalization constant.", "The Normal distribution is a member of the location-scale family, i.e., it can be constructed as,", "Examples of initialization of one or a batch of distributions.", "Arguments are broadcast when possible.", "Stats return +/- infinity when it makes sense. E.g., the variance of a Cauchy distribution is infinity. However, sometimes the statistic is undefined, e.g., if a distribution's pdf does not achieve a maximum within the support of the distribution, the mode is undefined. If the mean is undefined, then by definition the variance is undefined. E.g. the mean for Student's T for df = 1 is undefined (no clear way to say it is either + or - infinity), so the variance = E[(X - mean)**2] is also undefined. ", "May be partially defined or unknown.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution. ", "May be partially defined or unknown. ", "Currently this is one of the static instances distributions.FULLY_REPARAMETERIZED or distributions.NOT_REPARAMETERIZED. ", "View source", "Shape of a single sample from a single event index as a 1-D Tensor.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution.", "View source", "Cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "View source", "Creates a deep copy of the distribution.", "View source", "Covariance.", "Covariance is (possibly) defined only for non-scalar-event distributions.", "For example, for a length-k, vector-valued distribution, it is calculated as,", "where Cov is a (batch of) k x k matrix, 0 <= (i, j) < k, and E denotes expectation.", "Alternatively, for non-vector, multivariate distributions (e.g., matrix-valued, Wishart), Covariance shall return a (batch of) matrices under some vectorization of the events, i.e.,", "where Cov is a (batch of) k' x k' matrices, 0 <= (i, j) < k' = reduce_prod(event_shape), and Vec is some function mapping indices of this distribution's event dimensions to indices of a length-k' vector.", "View source", "Computes the (Shannon) cross entropy.", "Denote this distribution (self) by P and the other distribution by Q. Assuming P, Q are absolutely continuous with respect to one another and permit densities p(x) dr(x) and q(x) dr(x), (Shanon) cross entropy is defined as:", "where F denotes the support of the random variable X ~ P.", "View source", "Shannon entropy in nats.", "View source", "Shape of a single sample from a single batch as a 1-D int32 Tensor.", "View source", "Indicates that batch_shape == [].", "View source", "Indicates that event_shape == [].", "View source", "Computes the Kullback--Leibler divergence.", "Denote this distribution (self) by p and the other distribution by q. Assuming p, q are absolutely continuous with respect to reference measure r, the KL divergence is defined as:", "where F denotes the support of the random variable X ~ p, H[., .] denotes (Shanon) cross entropy, and H[.] denotes (Shanon) entropy.", "View source", "Log cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "Often, a numerical approximation can be used for log_cdf(x) that yields a more accurate answer than simply taking the logarithm of the cdf when x << -1.", "View source", "Log probability density/mass function.", "View source", "Log survival function.", "Given random variable X, the survival function is defined:", "Typically, different numerical approximations can be used for the log survival function, which are more accurate than 1 - cdf(x) when x >> 1.", "View source", "Mean.", "View source", "Mode.", "View source", "Shapes of parameters given the desired shape of a call to sample().", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample().", "Subclasses should override class method _param_shapes.", "View source", "param_shapes with static (i.e. TensorShape) shapes.", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample(). Assumes that the sample's shape is known statically.", "Subclasses should override class method _param_shapes to return constant-valued tensors when constant values are fed.", "View source", "Probability density/mass function.", "View source", "Quantile function. Aka \"inverse cdf\" or \"percent point function\".", "Given random variable X and p in [0, 1], the quantile is:", "View source", "Generate samples of the specified shape.", "Note that a call to sample() without arguments will generate a single sample.", "View source", "Standard deviation.", "Standard deviation is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and stddev.shape = batch_shape + event_shape.", "View source", "Survival function.", "Given random variable X, the survival function is defined:", "View source", "Variance.", "Variance is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and Var.shape = batch_shape + event_shape."]}, {"name": "tf.compat.v1.distributions.RegisterKL", "path": "compat/v1/distributions/registerkl", "type": "tf.compat", "text": ["Decorator to register a KL divergence implementation function.", "@distributions.RegisterKL(distributions.Normal, distributions.Normal) def _kl_normal_mvn(norm_a, norm_b): # Return KL(norm_a || norm_b)", "View source", "Perform the KL registration."]}, {"name": "tf.compat.v1.distributions.ReparameterizationType", "path": "compat/v1/distributions/reparameterizationtype", "type": "tf.compat", "text": ["Instances of this class represent how sampling is reparameterized.", "Two static instances exist in the distributions library, signifying one of two possible properties for samples from a distribution:", "FULLY_REPARAMETERIZED: Samples from the distribution are fully reparameterized, and straight-through gradients are supported.", "NOT_REPARAMETERIZED: Samples from the distribution are not fully reparameterized, and straight-through gradients are either partially unsupported or are not supported at all. In this case, for purposes of e.g. RL or variational inference, it is generally safest to wrap the sample results in a stop_gradients call and use policy gradients / surrogate loss instead.", "View source", "Determine if this ReparameterizationType is equal to another.", "Since ReparameterizationType instances are constant static global instances, equality checks if two instances' id() values are equal."]}, {"name": "tf.compat.v1.distributions.StudentT", "path": "compat/v1/distributions/studentt", "type": "tf.compat", "text": ["Student's t-distribution.", "Inherits From: Distribution", "This distribution has parameters: degree of freedom df, location loc, and scale.", "The probability density function (pdf) is,", "where:", "The StudentT distribution is a member of the location-scale family, i.e., it can be constructed as,", "Notice that scale has semantics more similar to standard deviation than variance. However it is not actually the std. deviation; the Student's t-distribution std. dev. is scale sqrt(df / (df - 2)) when df > 2.", "Samples of this distribution are reparameterized (pathwise differentiable). The derivatives are computed using the approach described in (Figurnov et al., 2018).", "Examples of initialization of one or a batch of distributions.", "Arguments are broadcast when possible.", "Compute the gradients of samples w.r.t. the parameters:", "Implicit Reparameterization Gradients: Figurnov et al., 2018 (pdf)", "Stats return +/- infinity when it makes sense. E.g., the variance of a Cauchy distribution is infinity. However, sometimes the statistic is undefined, e.g., if a distribution's pdf does not achieve a maximum within the support of the distribution, the mode is undefined. If the mean is undefined, then by definition the variance is undefined. E.g. the mean for Student's T for df = 1 is undefined (no clear way to say it is either + or - infinity), so the variance = E[(X - mean)**2] is also undefined. ", "May be partially defined or unknown.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution. ", "May be partially defined or unknown. ", "Currently this is one of the static instances distributions.FULLY_REPARAMETERIZED or distributions.NOT_REPARAMETERIZED. ", "View source", "Shape of a single sample from a single event index as a 1-D Tensor.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution.", "View source", "Cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "View source", "Creates a deep copy of the distribution.", "View source", "Covariance.", "Covariance is (possibly) defined only for non-scalar-event distributions.", "For example, for a length-k, vector-valued distribution, it is calculated as,", "where Cov is a (batch of) k x k matrix, 0 <= (i, j) < k, and E denotes expectation.", "Alternatively, for non-vector, multivariate distributions (e.g., matrix-valued, Wishart), Covariance shall return a (batch of) matrices under some vectorization of the events, i.e.,", "where Cov is a (batch of) k' x k' matrices, 0 <= (i, j) < k' = reduce_prod(event_shape), and Vec is some function mapping indices of this distribution's event dimensions to indices of a length-k' vector.", "View source", "Computes the (Shannon) cross entropy.", "Denote this distribution (self) by P and the other distribution by Q. Assuming P, Q are absolutely continuous with respect to one another and permit densities p(x) dr(x) and q(x) dr(x), (Shanon) cross entropy is defined as:", "where F denotes the support of the random variable X ~ P.", "View source", "Shannon entropy in nats.", "View source", "Shape of a single sample from a single batch as a 1-D int32 Tensor.", "View source", "Indicates that batch_shape == [].", "View source", "Indicates that event_shape == [].", "View source", "Computes the Kullback--Leibler divergence.", "Denote this distribution (self) by p and the other distribution by q. Assuming p, q are absolutely continuous with respect to reference measure r, the KL divergence is defined as:", "where F denotes the support of the random variable X ~ p, H[., .] denotes (Shanon) cross entropy, and H[.] denotes (Shanon) entropy.", "View source", "Log cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "Often, a numerical approximation can be used for log_cdf(x) that yields a more accurate answer than simply taking the logarithm of the cdf when x << -1.", "View source", "Log probability density/mass function.", "View source", "Log survival function.", "Given random variable X, the survival function is defined:", "Typically, different numerical approximations can be used for the log survival function, which are more accurate than 1 - cdf(x) when x >> 1.", "View source", "Mean.", "Additional documentation from StudentT:", "The mean of Student's T equals loc if df > 1, otherwise it is NaN. If self.allow_nan_stats=True, then an exception will be raised rather than returning NaN.", "View source", "Mode.", "View source", "Shapes of parameters given the desired shape of a call to sample().", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample().", "Subclasses should override class method _param_shapes.", "View source", "param_shapes with static (i.e. TensorShape) shapes.", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample(). Assumes that the sample's shape is known statically.", "Subclasses should override class method _param_shapes to return constant-valued tensors when constant values are fed.", "View source", "Probability density/mass function.", "View source", "Quantile function. Aka \"inverse cdf\" or \"percent point function\".", "Given random variable X and p in [0, 1], the quantile is:", "View source", "Generate samples of the specified shape.", "Note that a call to sample() without arguments will generate a single sample.", "View source", "Standard deviation.", "Standard deviation is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and stddev.shape = batch_shape + event_shape.", "View source", "Survival function.", "Given random variable X, the survival function is defined:", "View source", "Variance.", "Variance is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and Var.shape = batch_shape + event_shape.", "Additional documentation from StudentT:", "The variance for Student's T equals"]}, {"name": "tf.compat.v1.distributions.Uniform", "path": "compat/v1/distributions/uniform", "type": "tf.compat", "text": ["Uniform distribution with low and high parameters.", "Inherits From: Distribution", "The probability density function (pdf) is,", "where", "The parameters low and high must be shaped in a way that supports broadcasting (e.g., high - low is a valid operation).", "Stats return +/- infinity when it makes sense. E.g., the variance of a Cauchy distribution is infinity. However, sometimes the statistic is undefined, e.g., if a distribution's pdf does not achieve a maximum within the support of the distribution, the mode is undefined. If the mean is undefined, then by definition the variance is undefined. E.g. the mean for Student's T for df = 1 is undefined (no clear way to say it is either + or - infinity), so the variance = E[(X - mean)**2] is also undefined. ", "May be partially defined or unknown.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution. ", "May be partially defined or unknown. ", "Currently this is one of the static instances distributions.FULLY_REPARAMETERIZED or distributions.NOT_REPARAMETERIZED. ", "View source", "Shape of a single sample from a single event index as a 1-D Tensor.", "The batch dimensions are indexes into independent, non-identical parameterizations of this distribution.", "View source", "Cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "View source", "Creates a deep copy of the distribution.", "View source", "Covariance.", "Covariance is (possibly) defined only for non-scalar-event distributions.", "For example, for a length-k, vector-valued distribution, it is calculated as,", "where Cov is a (batch of) k x k matrix, 0 <= (i, j) < k, and E denotes expectation.", "Alternatively, for non-vector, multivariate distributions (e.g., matrix-valued, Wishart), Covariance shall return a (batch of) matrices under some vectorization of the events, i.e.,", "where Cov is a (batch of) k' x k' matrices, 0 <= (i, j) < k' = reduce_prod(event_shape), and Vec is some function mapping indices of this distribution's event dimensions to indices of a length-k' vector.", "View source", "Computes the (Shannon) cross entropy.", "Denote this distribution (self) by P and the other distribution by Q. Assuming P, Q are absolutely continuous with respect to one another and permit densities p(x) dr(x) and q(x) dr(x), (Shanon) cross entropy is defined as:", "where F denotes the support of the random variable X ~ P.", "View source", "Shannon entropy in nats.", "View source", "Shape of a single sample from a single batch as a 1-D int32 Tensor.", "View source", "Indicates that batch_shape == [].", "View source", "Indicates that event_shape == [].", "View source", "Computes the Kullback--Leibler divergence.", "Denote this distribution (self) by p and the other distribution by q. Assuming p, q are absolutely continuous with respect to reference measure r, the KL divergence is defined as:", "where F denotes the support of the random variable X ~ p, H[., .] denotes (Shanon) cross entropy, and H[.] denotes (Shanon) entropy.", "View source", "Log cumulative distribution function.", "Given random variable X, the cumulative distribution function cdf is:", "Often, a numerical approximation can be used for log_cdf(x) that yields a more accurate answer than simply taking the logarithm of the cdf when x << -1.", "View source", "Log probability density/mass function.", "View source", "Log survival function.", "Given random variable X, the survival function is defined:", "Typically, different numerical approximations can be used for the log survival function, which are more accurate than 1 - cdf(x) when x >> 1.", "View source", "Mean.", "View source", "Mode.", "View source", "Shapes of parameters given the desired shape of a call to sample().", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample().", "Subclasses should override class method _param_shapes.", "View source", "param_shapes with static (i.e. TensorShape) shapes.", "This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample(). Assumes that the sample's shape is known statically.", "Subclasses should override class method _param_shapes to return constant-valued tensors when constant values are fed.", "View source", "Probability density/mass function.", "View source", "Quantile function. Aka \"inverse cdf\" or \"percent point function\".", "Given random variable X and p in [0, 1], the quantile is:", "View source", "high - low.", "View source", "Generate samples of the specified shape.", "Note that a call to sample() without arguments will generate a single sample.", "View source", "Standard deviation.", "Standard deviation is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and stddev.shape = batch_shape + event_shape.", "View source", "Survival function.", "Given random variable X, the survival function is defined:", "View source", "Variance.", "Variance is defined as,", "where X is the random variable associated with this distribution, E denotes expectation, and Var.shape = batch_shape + event_shape."]}, {"name": "tf.compat.v1.dtypes", "path": "compat/v1/dtypes", "type": "tf.compat", "text": ["Public API for tf.dtypes namespace.", "class DType: Represents the type of the elements in a Tensor.", "as_dtype(...): Converts the given type_value to a DType.", "as_string(...): Converts each entry in the given tensor to strings.", "cast(...): Casts a tensor to a new type.", "complex(...): Converts two real numbers to a complex number.", "saturate_cast(...): Performs a safe saturating cast of value to dtype."]}, {"name": "tf.compat.v1.enable_control_flow_v2", "path": "compat/v1/enable_control_flow_v2", "type": "tf.compat", "text": ["Use control flow v2.", "control flow v2 (cfv2) is an improved version of control flow in TensorFlow with support for higher order derivatives. Enabling cfv2 will change the graph/function representation of control flow, e.g., tf.while_loop and tf.cond will generate functional While and If ops instead of low-level Switch, Merge etc. ops. Note: Importing and running graphs exported with old control flow will still be supported.", "Calling tf.enable_control_flow_v2() lets you opt-in to this TensorFlow 2.0 feature."]}, {"name": "tf.compat.v1.enable_eager_execution", "path": "compat/v1/enable_eager_execution", "type": "tf.compat", "text": ["Enables eager execution for the lifetime of this program.", "Eager execution provides an imperative interface to TensorFlow. With eager execution enabled, TensorFlow functions execute operations immediately (as opposed to adding to a graph to be executed later in a tf.compat.v1.Session) and return concrete values (as opposed to symbolic references to a node in a computational graph).", "Eager execution cannot be enabled after TensorFlow APIs have been used to create or execute graphs. It is typically recommended to invoke this function at program startup and not in a library (as most libraries should be usable both with and without eager execution)."]}, {"name": "tf.compat.v1.enable_resource_variables", "path": "compat/v1/enable_resource_variables", "type": "tf.compat", "text": ["Creates resource variables by default.", "Resource variables are improved versions of TensorFlow variables with a well-defined memory model. Accessing a resource variable reads its value, and all ops which access a specific read value of the variable are guaranteed to see the same value for that tensor. Writes which happen after a read (by having a control or data dependency on the read) are guaranteed not to affect the value of the read tensor, and similarly writes which happen before a read are guaranteed to affect the value. No guarantees are made about unordered read/write pairs.", "Calling tf.enable_resource_variables() lets you opt-in to this TensorFlow 2.0 feature."]}, {"name": "tf.compat.v1.enable_tensor_equality", "path": "compat/v1/enable_tensor_equality", "type": "tf.compat", "text": ["Compare Tensors with element-wise comparison and thus be unhashable.", "Comparing tensors with element-wise allows comparisons such as tf.Variable(1.0) == 1.0. Element-wise equality implies that tensors are unhashable. Thus tensors can no longer be directly used in sets or as a key in a dictionary."]}, {"name": "tf.compat.v1.enable_v2_behavior", "path": "compat/v1/enable_v2_behavior", "type": "tf.compat", "text": ["Enables TensorFlow 2.x behaviors.", "This function can be called at the beginning of the program (before Tensors, Graphs or other structures have been created, and before devices have been initialized. It switches all global behaviors that are different between TensorFlow 1.x and 2.x to behave as intended for 2.x.", "This function is called in the main TensorFlow __init__.py file, user should not need to call it, except during complex migrations."]}, {"name": "tf.compat.v1.enable_v2_tensorshape", "path": "compat/v1/enable_v2_tensorshape", "type": "tf.compat", "text": ["In TensorFlow 2.0, iterating over a TensorShape instance returns values.", "This enables the new behavior.", "Concretely, tensor_shape[i] returned a Dimension instance in V1, but it V2 it returns either an integer, or None."]}, {"name": "tf.compat.v1.errors", "path": "compat/v1/errors", "type": "tf.compat", "text": ["Exception types for TensorFlow errors.", "class AbortedError: The operation was aborted, typically due to a concurrent action.", "class AlreadyExistsError: Raised when an entity that we attempted to create already exists.", "class CancelledError: Raised when an operation or step is cancelled.", "class DataLossError: Raised when unrecoverable data loss or corruption is encountered.", "class DeadlineExceededError: Raised when a deadline expires before an operation could complete.", "class FailedPreconditionError: Operation was rejected because the system is not in a state to execute it.", "class InternalError: Raised when the system experiences an internal error.", "class InvalidArgumentError: Raised when an operation receives an invalid argument.", "class NotFoundError: Raised when a requested entity (e.g., a file or directory) was not found.", "class OpError: A generic error that is raised when TensorFlow execution fails.", "class OutOfRangeError: Raised when an operation iterates past the valid input range.", "class PermissionDeniedError: Raised when the caller does not have permission to run an operation.", "class ResourceExhaustedError: Some resource has been exhausted.", "class UnauthenticatedError: The request does not have valid authentication credentials.", "class UnavailableError: Raised when the runtime is currently unavailable.", "class UnimplementedError: Raised when an operation has not been implemented.", "class UnknownError: Unknown error.", "class raise_exception_on_not_ok_status: Context manager to check for C API status.", "error_code_from_exception_type(...)", "exception_type_from_error_code(...)"]}, {"name": "tf.compat.v1.errors.error_code_from_exception_type", "path": "compat/v1/errors/error_code_from_exception_type", "type": "tf.compat", "text": []}, {"name": "tf.compat.v1.errors.exception_type_from_error_code", "path": "compat/v1/errors/exception_type_from_error_code", "type": "tf.compat", "text": []}, {"name": "tf.compat.v1.errors.raise_exception_on_not_ok_status", "path": "compat/v1/errors/raise_exception_on_not_ok_status", "type": "tf.compat", "text": ["Context manager to check for C API status.", "View source", "View source"]}, {"name": "tf.compat.v1.estimator", "path": "compat/v1/estimator", "type": "tf.compat", "text": ["Estimator: High level tools for working with models.", "experimental module: Public API for tf.estimator.experimental namespace.", "export module: All public utility methods for exporting Estimator to SavedModel.", "inputs module: Utility methods to create simple input_fns.", "tpu module: Public API for tf.estimator.tpu namespace.", "class BaselineClassifier: A classifier that can establish a simple baseline.", "class BaselineEstimator: An estimator that can establish a simple baseline.", "class BaselineRegressor: A regressor that can establish a simple baseline.", "class BestExporter: This class exports the serving graph and checkpoints of the best models.", "class BinaryClassHead: Creates a Head for single label binary classification.", "class BoostedTreesClassifier: A Classifier for Tensorflow Boosted Trees models.", "class BoostedTreesEstimator: An Estimator for Tensorflow Boosted Trees models.", "class BoostedTreesRegressor: A Regressor for Tensorflow Boosted Trees models.", "class CheckpointSaverHook: Saves checkpoints every N steps or seconds.", "class CheckpointSaverListener: Interface for listeners that take action before or after checkpoint save.", "class DNNClassifier: A classifier for TensorFlow DNN models.", "class DNNEstimator: An estimator for TensorFlow DNN models with user-specified head.", "class DNNLinearCombinedClassifier: An estimator for TensorFlow Linear and DNN joined classification models.", "class DNNLinearCombinedEstimator: An estimator for TensorFlow Linear and DNN joined models with custom head.", "class DNNLinearCombinedRegressor: An estimator for TensorFlow Linear and DNN joined models for regression.", "class DNNRegressor: A regressor for TensorFlow DNN models.", "class Estimator: Estimator class to train and evaluate TensorFlow models.", "class EstimatorSpec: Ops and objects returned from a model_fn and passed to an Estimator.", "class EvalSpec: Configuration for the \"eval\" part for the train_and_evaluate call.", "class Exporter: A class representing a type of model export.", "class FeedFnHook: Runs feed_fn and sets the feed_dict accordingly.", "class FinalExporter: This class exports the serving graph and checkpoints at the end.", "class FinalOpsHook: A hook which evaluates Tensors at the end of a session.", "class GlobalStepWaiterHook: Delays execution until global step reaches wait_until_step.", "class Head: Interface for the head/top of a model.", "class LatestExporter: This class regularly exports the serving graph and checkpoints.", "class LinearClassifier: Linear classifier model.", "class LinearEstimator: An estimator for TensorFlow linear models with user-specified head.", "class LinearRegressor: An estimator for TensorFlow Linear regression problems.", "class LoggingTensorHook: Prints the given tensors every N local steps, every N seconds, or at end.", "class LogisticRegressionHead: Creates a Head for logistic regression.", "class ModeKeys: Standard names for Estimator model modes.", "class MultiClassHead: Creates a Head for multi class classification.", "class MultiHead: Creates a Head for multi-objective learning.", "class MultiLabelHead: Creates a Head for multi-label classification.", "class NanLossDuringTrainingError: Unspecified run-time error.", "class NanTensorHook: Monitors the loss tensor and stops training if loss is NaN.", "class PoissonRegressionHead: Creates a Head for poisson regression using tf.nn.log_poisson_loss.", "class ProfilerHook: Captures CPU/GPU profiling information every N steps or seconds.", "class RegressionHead: Creates a Head for regression using the mean_squared_error loss.", "class RunConfig: This class specifies the configurations for an Estimator run.", "class SecondOrStepTimer: Timer that triggers at most once every N seconds or once every N steps.", "class SessionRunArgs: Represents arguments to be added to a Session.run() call.", "class SessionRunContext: Provides information about the session.run() call being made.", "class SessionRunHook: Hook to extend calls to MonitoredSession.run().", "class SessionRunValues: Contains the results of Session.run().", "class StepCounterHook: Hook that counts steps per second.", "class StopAtStepHook: Hook that requests stop at a specified step.", "class SummarySaverHook: Saves summaries every N steps.", "class TrainSpec: Configuration for the \"train\" part for the train_and_evaluate call.", "class VocabInfo: Vocabulary information for warm-starting.", "class WarmStartSettings: Settings for warm-starting in tf.estimator.Estimators.", "add_metrics(...): Creates a new tf.estimator.Estimator which has given metrics.", "classifier_parse_example_spec(...): Generates parsing spec for tf.parse_example to be used with classifiers.", "regressor_parse_example_spec(...): Generates parsing spec for tf.parse_example to be used with regressors.", "train_and_evaluate(...): Train and evaluate the estimator."]}, {"name": "tf.compat.v1.estimator.BaselineClassifier", "path": "compat/v1/estimator/baselineclassifier", "type": "tf.compat", "text": ["A classifier that can establish a simple baseline.", "Inherits From: Estimator", "This classifier ignores feature values and will learn to predict the average value of each label. For single-label problems, this will predict the probability distribution of the classes as seen in the labels. For multi-label problems, this will predict the fraction of examples that are positive for each class.", "Input of train and evaluate should have following features, otherwise there will be a KeyError:", "Estimators can be used while eager execution is enabled. Note that input_fn and all hooks are executed inside a graph context, so they have to be written to be compatible with graph mode. Note that input_fn code using tf.data generally works in both graph and eager modes.", "View source", "Shows the directory name where evaluation metrics are dumped.", "View source", "Evaluates the model given evaluation data input_fn.", "For each step, calls input_fn, which returns one batch of data. Evaluates until:", "View source", "Exports a SavedModel with tf.MetaGraphDefs for each requested mode.", "For each mode passed in via the input_receiver_fn_map, this method builds a new graph by calling the input_receiver_fn to obtain feature and label Tensors. Next, this method calls the Estimator's model_fn in the passed mode to generate the model graph based on those features and labels, and restores the given checkpoint (or, lacking that, the most recent checkpoint) into the graph. Only one of the modes is used for saving variables to the SavedModel (order of preference: tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL, then tf.estimator.ModeKeys.PREDICT), such that up to three tf.MetaGraphDefs are saved with a single set of variables in a single SavedModel directory.", "For the variables and tf.MetaGraphDefs, a timestamped export directory below export_dir_base, and writes a SavedModel into it containing the tf.MetaGraphDef for the given mode and its associated signatures.", "For prediction, the exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "For training and evaluation, the train_op is stored in an extra collection, and loss, metrics, and predictions are included in a SignatureDef for the mode in question.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Exports inference graph as a SavedModel into the given dir.", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "The experimental_mode parameter can be used to export a single train/eval/predict graph as a SavedModel. See experimental_export_all_saved_models for full docs.", "View source", "Exports inference graph as a SavedModel into the given dir. (deprecated)", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Returns list of all variable names in this model.", "View source", "Returns value of the variable given by name.", "View source", "Finds the filename of the latest saved checkpoint file in model_dir.", "View source", "Yields predictions for given features.", "Please note that interleaving two predict outputs does not work. See: issue/20506", "View source", "Trains a model given training data input_fn."]}, {"name": "tf.compat.v1.estimator.BaselineEstimator", "path": "compat/v1/estimator/baselineestimator", "type": "tf.compat", "text": ["An estimator that can establish a simple baseline.", "Inherits From: Estimator", "The estimator uses a user-specified head.", "This estimator ignores feature values and will learn to predict the average value of each label. E.g. for single-label classification problems, this will predict the probability distribution of the classes as seen in the labels. For multi-label classification problems, it will predict the ratio of examples that contain each class.", "Input of train and evaluate should have following features, otherwise there will be a KeyError:", "View source", "Shows the directory name where evaluation metrics are dumped.", "View source", "Evaluates the model given evaluation data input_fn.", "For each step, calls input_fn, which returns one batch of data. Evaluates until:", "View source", "Exports a SavedModel with tf.MetaGraphDefs for each requested mode.", "For each mode passed in via the input_receiver_fn_map, this method builds a new graph by calling the input_receiver_fn to obtain feature and label Tensors. Next, this method calls the Estimator's model_fn in the passed mode to generate the model graph based on those features and labels, and restores the given checkpoint (or, lacking that, the most recent checkpoint) into the graph. Only one of the modes is used for saving variables to the SavedModel (order of preference: tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL, then tf.estimator.ModeKeys.PREDICT), such that up to three tf.MetaGraphDefs are saved with a single set of variables in a single SavedModel directory.", "For the variables and tf.MetaGraphDefs, a timestamped export directory below export_dir_base, and writes a SavedModel into it containing the tf.MetaGraphDef for the given mode and its associated signatures.", "For prediction, the exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "For training and evaluation, the train_op is stored in an extra collection, and loss, metrics, and predictions are included in a SignatureDef for the mode in question.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Exports inference graph as a SavedModel into the given dir.", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "The experimental_mode parameter can be used to export a single train/eval/predict graph as a SavedModel. See experimental_export_all_saved_models for full docs.", "View source", "Exports inference graph as a SavedModel into the given dir. (deprecated)", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Returns list of all variable names in this model.", "View source", "Returns value of the variable given by name.", "View source", "Finds the filename of the latest saved checkpoint file in model_dir.", "View source", "Yields predictions for given features.", "Please note that interleaving two predict outputs does not work. See: issue/20506", "View source", "Trains a model given training data input_fn."]}, {"name": "tf.compat.v1.estimator.BaselineRegressor", "path": "compat/v1/estimator/baselineregressor", "type": "tf.compat", "text": ["A regressor that can establish a simple baseline.", "Inherits From: Estimator", "This regressor ignores feature values and will learn to predict the average value of each label.", "Input of train and evaluate should have following features, otherwise there will be a KeyError:", "Estimators can be used while eager execution is enabled. Note that input_fn and all hooks are executed inside a graph context, so they have to be written to be compatible with graph mode. Note that input_fn code using tf.data generally works in both graph and eager modes.", "View source", "Shows the directory name where evaluation metrics are dumped.", "View source", "Evaluates the model given evaluation data input_fn.", "For each step, calls input_fn, which returns one batch of data. Evaluates until:", "View source", "Exports a SavedModel with tf.MetaGraphDefs for each requested mode.", "For each mode passed in via the input_receiver_fn_map, this method builds a new graph by calling the input_receiver_fn to obtain feature and label Tensors. Next, this method calls the Estimator's model_fn in the passed mode to generate the model graph based on those features and labels, and restores the given checkpoint (or, lacking that, the most recent checkpoint) into the graph. Only one of the modes is used for saving variables to the SavedModel (order of preference: tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL, then tf.estimator.ModeKeys.PREDICT), such that up to three tf.MetaGraphDefs are saved with a single set of variables in a single SavedModel directory.", "For the variables and tf.MetaGraphDefs, a timestamped export directory below export_dir_base, and writes a SavedModel into it containing the tf.MetaGraphDef for the given mode and its associated signatures.", "For prediction, the exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "For training and evaluation, the train_op is stored in an extra collection, and loss, metrics, and predictions are included in a SignatureDef for the mode in question.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Exports inference graph as a SavedModel into the given dir.", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "The experimental_mode parameter can be used to export a single train/eval/predict graph as a SavedModel. See experimental_export_all_saved_models for full docs.", "View source", "Exports inference graph as a SavedModel into the given dir. (deprecated)", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Returns list of all variable names in this model.", "View source", "Returns value of the variable given by name.", "View source", "Finds the filename of the latest saved checkpoint file in model_dir.", "View source", "Yields predictions for given features.", "Please note that interleaving two predict outputs does not work. See: issue/20506", "View source", "Trains a model given training data input_fn."]}, {"name": "tf.compat.v1.estimator.classifier_parse_example_spec", "path": "compat/v1/estimator/classifier_parse_example_spec", "type": "tf.compat", "text": ["Generates parsing spec for tf.parse_example to be used with classifiers.", "If users keep data in tf.Example format, they need to call tf.parse_example with a proper feature spec. There are two main things that this utility helps:", "Example output of parsing spec:", "Example usage with a classifier:"]}, {"name": "tf.compat.v1.estimator.DNNClassifier", "path": "compat/v1/estimator/dnnclassifier", "type": "tf.compat", "text": ["A classifier for TensorFlow DNN models.", "Inherits From: Estimator", "Input of train and evaluate should have following features, otherwise there will be a KeyError:", "Loss is calculated by using softmax cross entropy.", "Estimators can be used while eager execution is enabled. Note that input_fn and all hooks are executed inside a graph context, so they have to be written to be compatible with graph mode. Note that input_fn code using tf.data generally works in both graph and eager modes.", "View source", "Shows the directory name where evaluation metrics are dumped.", "View source", "Evaluates the model given evaluation data input_fn.", "For each step, calls input_fn, which returns one batch of data. Evaluates until:", "View source", "Exports a SavedModel with tf.MetaGraphDefs for each requested mode.", "For each mode passed in via the input_receiver_fn_map, this method builds a new graph by calling the input_receiver_fn to obtain feature and label Tensors. Next, this method calls the Estimator's model_fn in the passed mode to generate the model graph based on those features and labels, and restores the given checkpoint (or, lacking that, the most recent checkpoint) into the graph. Only one of the modes is used for saving variables to the SavedModel (order of preference: tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL, then tf.estimator.ModeKeys.PREDICT), such that up to three tf.MetaGraphDefs are saved with a single set of variables in a single SavedModel directory.", "For the variables and tf.MetaGraphDefs, a timestamped export directory below export_dir_base, and writes a SavedModel into it containing the tf.MetaGraphDef for the given mode and its associated signatures.", "For prediction, the exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "For training and evaluation, the train_op is stored in an extra collection, and loss, metrics, and predictions are included in a SignatureDef for the mode in question.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Exports inference graph as a SavedModel into the given dir.", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "The experimental_mode parameter can be used to export a single train/eval/predict graph as a SavedModel. See experimental_export_all_saved_models for full docs.", "View source", "Exports inference graph as a SavedModel into the given dir. (deprecated)", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Returns list of all variable names in this model.", "View source", "Returns value of the variable given by name.", "View source", "Finds the filename of the latest saved checkpoint file in model_dir.", "View source", "Yields predictions for given features.", "Please note that interleaving two predict outputs does not work. See: issue/20506", "View source", "Trains a model given training data input_fn."]}, {"name": "tf.compat.v1.estimator.DNNEstimator", "path": "compat/v1/estimator/dnnestimator", "type": "tf.compat", "text": ["An estimator for TensorFlow DNN models with user-specified head.", "Inherits From: Estimator", "Input of train and evaluate should have following features, otherwise there will be a KeyError:", "Loss and predicted output are determined by the specified head.", "Estimators can be used while eager execution is enabled. Note that input_fn and all hooks are executed inside a graph context, so they have to be written to be compatible with graph mode. Note that input_fn code using tf.data generally works in both graph and eager modes.", "View source", "Shows the directory name where evaluation metrics are dumped.", "View source", "Evaluates the model given evaluation data input_fn.", "For each step, calls input_fn, which returns one batch of data. Evaluates until:", "View source", "Exports a SavedModel with tf.MetaGraphDefs for each requested mode.", "For each mode passed in via the input_receiver_fn_map, this method builds a new graph by calling the input_receiver_fn to obtain feature and label Tensors. Next, this method calls the Estimator's model_fn in the passed mode to generate the model graph based on those features and labels, and restores the given checkpoint (or, lacking that, the most recent checkpoint) into the graph. Only one of the modes is used for saving variables to the SavedModel (order of preference: tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL, then tf.estimator.ModeKeys.PREDICT), such that up to three tf.MetaGraphDefs are saved with a single set of variables in a single SavedModel directory.", "For the variables and tf.MetaGraphDefs, a timestamped export directory below export_dir_base, and writes a SavedModel into it containing the tf.MetaGraphDef for the given mode and its associated signatures.", "For prediction, the exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "For training and evaluation, the train_op is stored in an extra collection, and loss, metrics, and predictions are included in a SignatureDef for the mode in question.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Exports inference graph as a SavedModel into the given dir.", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "The experimental_mode parameter can be used to export a single train/eval/predict graph as a SavedModel. See experimental_export_all_saved_models for full docs.", "View source", "Exports inference graph as a SavedModel into the given dir. (deprecated)", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Returns list of all variable names in this model.", "View source", "Returns value of the variable given by name.", "View source", "Finds the filename of the latest saved checkpoint file in model_dir.", "View source", "Yields predictions for given features.", "Please note that interleaving two predict outputs does not work. See: issue/20506", "View source", "Trains a model given training data input_fn."]}, {"name": "tf.compat.v1.estimator.DNNLinearCombinedClassifier", "path": "compat/v1/estimator/dnnlinearcombinedclassifier", "type": "tf.compat", "text": ["An estimator for TensorFlow Linear and DNN joined classification models.", "Inherits From: Estimator", "Input of train and evaluate should have following features, otherwise there will be a KeyError:", "Loss is calculated by using softmax cross entropy.", "Estimators can be used while eager execution is enabled. Note that input_fn and all hooks are executed inside a graph context, so they have to be written to be compatible with graph mode. Note that input_fn code using tf.data generally works in both graph and eager modes.", "View source", "Shows the directory name where evaluation metrics are dumped.", "View source", "Evaluates the model given evaluation data input_fn.", "For each step, calls input_fn, which returns one batch of data. Evaluates until:", "View source", "Exports a SavedModel with tf.MetaGraphDefs for each requested mode.", "For each mode passed in via the input_receiver_fn_map, this method builds a new graph by calling the input_receiver_fn to obtain feature and label Tensors. Next, this method calls the Estimator's model_fn in the passed mode to generate the model graph based on those features and labels, and restores the given checkpoint (or, lacking that, the most recent checkpoint) into the graph. Only one of the modes is used for saving variables to the SavedModel (order of preference: tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL, then tf.estimator.ModeKeys.PREDICT), such that up to three tf.MetaGraphDefs are saved with a single set of variables in a single SavedModel directory.", "For the variables and tf.MetaGraphDefs, a timestamped export directory below export_dir_base, and writes a SavedModel into it containing the tf.MetaGraphDef for the given mode and its associated signatures.", "For prediction, the exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "For training and evaluation, the train_op is stored in an extra collection, and loss, metrics, and predictions are included in a SignatureDef for the mode in question.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Exports inference graph as a SavedModel into the given dir.", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "The experimental_mode parameter can be used to export a single train/eval/predict graph as a SavedModel. See experimental_export_all_saved_models for full docs.", "View source", "Exports inference graph as a SavedModel into the given dir. (deprecated)", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Returns list of all variable names in this model.", "View source", "Returns value of the variable given by name.", "View source", "Finds the filename of the latest saved checkpoint file in model_dir.", "View source", "Yields predictions for given features.", "Please note that interleaving two predict outputs does not work. See: issue/20506", "View source", "Trains a model given training data input_fn."]}, {"name": "tf.compat.v1.estimator.DNNLinearCombinedEstimator", "path": "compat/v1/estimator/dnnlinearcombinedestimator", "type": "tf.compat", "text": ["An estimator for TensorFlow Linear and DNN joined models with custom head.", "Inherits From: Estimator", "Input of train and evaluate should have following features, otherwise there will be a KeyError:", "Loss is calculated by using mean squared error.", "Estimators can be used while eager execution is enabled. Note that input_fn and all hooks are executed inside a graph context, so they have to be written to be compatible with graph mode. Note that input_fn code using tf.data generally works in both graph and eager modes.", "View source", "Shows the directory name where evaluation metrics are dumped.", "View source", "Evaluates the model given evaluation data input_fn.", "For each step, calls input_fn, which returns one batch of data. Evaluates until:", "View source", "Exports a SavedModel with tf.MetaGraphDefs for each requested mode.", "For each mode passed in via the input_receiver_fn_map, this method builds a new graph by calling the input_receiver_fn to obtain feature and label Tensors. Next, this method calls the Estimator's model_fn in the passed mode to generate the model graph based on those features and labels, and restores the given checkpoint (or, lacking that, the most recent checkpoint) into the graph. Only one of the modes is used for saving variables to the SavedModel (order of preference: tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL, then tf.estimator.ModeKeys.PREDICT), such that up to three tf.MetaGraphDefs are saved with a single set of variables in a single SavedModel directory.", "For the variables and tf.MetaGraphDefs, a timestamped export directory below export_dir_base, and writes a SavedModel into it containing the tf.MetaGraphDef for the given mode and its associated signatures.", "For prediction, the exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "For training and evaluation, the train_op is stored in an extra collection, and loss, metrics, and predictions are included in a SignatureDef for the mode in question.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Exports inference graph as a SavedModel into the given dir.", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "The experimental_mode parameter can be used to export a single train/eval/predict graph as a SavedModel. See experimental_export_all_saved_models for full docs.", "View source", "Exports inference graph as a SavedModel into the given dir. (deprecated)", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Returns list of all variable names in this model.", "View source", "Returns value of the variable given by name.", "View source", "Finds the filename of the latest saved checkpoint file in model_dir.", "View source", "Yields predictions for given features.", "Please note that interleaving two predict outputs does not work. See: issue/20506", "View source", "Trains a model given training data input_fn."]}, {"name": "tf.compat.v1.estimator.DNNLinearCombinedRegressor", "path": "compat/v1/estimator/dnnlinearcombinedregressor", "type": "tf.compat", "text": ["An estimator for TensorFlow Linear and DNN joined models for regression.", "Inherits From: Estimator", "Input of train and evaluate should have following features, otherwise there will be a KeyError:", "Loss is calculated by using mean squared error.", "Estimators can be used while eager execution is enabled. Note that input_fn and all hooks are executed inside a graph context, so they have to be written to be compatible with graph mode. Note that input_fn code using tf.data generally works in both graph and eager modes.", "View source", "Shows the directory name where evaluation metrics are dumped.", "View source", "Evaluates the model given evaluation data input_fn.", "For each step, calls input_fn, which returns one batch of data. Evaluates until:", "View source", "Exports a SavedModel with tf.MetaGraphDefs for each requested mode.", "For each mode passed in via the input_receiver_fn_map, this method builds a new graph by calling the input_receiver_fn to obtain feature and label Tensors. Next, this method calls the Estimator's model_fn in the passed mode to generate the model graph based on those features and labels, and restores the given checkpoint (or, lacking that, the most recent checkpoint) into the graph. Only one of the modes is used for saving variables to the SavedModel (order of preference: tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL, then tf.estimator.ModeKeys.PREDICT), such that up to three tf.MetaGraphDefs are saved with a single set of variables in a single SavedModel directory.", "For the variables and tf.MetaGraphDefs, a timestamped export directory below export_dir_base, and writes a SavedModel into it containing the tf.MetaGraphDef for the given mode and its associated signatures.", "For prediction, the exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "For training and evaluation, the train_op is stored in an extra collection, and loss, metrics, and predictions are included in a SignatureDef for the mode in question.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Exports inference graph as a SavedModel into the given dir.", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "The experimental_mode parameter can be used to export a single train/eval/predict graph as a SavedModel. See experimental_export_all_saved_models for full docs.", "View source", "Exports inference graph as a SavedModel into the given dir. (deprecated)", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Returns list of all variable names in this model.", "View source", "Returns value of the variable given by name.", "View source", "Finds the filename of the latest saved checkpoint file in model_dir.", "View source", "Yields predictions for given features.", "Please note that interleaving two predict outputs does not work. See: issue/20506", "View source", "Trains a model given training data input_fn."]}, {"name": "tf.compat.v1.estimator.DNNRegressor", "path": "compat/v1/estimator/dnnregressor", "type": "tf.compat", "text": ["A regressor for TensorFlow DNN models.", "Inherits From: Estimator", "Input of train and evaluate should have following features, otherwise there will be a KeyError:", "Loss is calculated by using mean squared error.", "Estimators can be used while eager execution is enabled. Note that input_fn and all hooks are executed inside a graph context, so they have to be written to be compatible with graph mode. Note that input_fn code using tf.data generally works in both graph and eager modes.", "View source", "Shows the directory name where evaluation metrics are dumped.", "View source", "Evaluates the model given evaluation data input_fn.", "For each step, calls input_fn, which returns one batch of data. Evaluates until:", "View source", "Exports a SavedModel with tf.MetaGraphDefs for each requested mode.", "For each mode passed in via the input_receiver_fn_map, this method builds a new graph by calling the input_receiver_fn to obtain feature and label Tensors. Next, this method calls the Estimator's model_fn in the passed mode to generate the model graph based on those features and labels, and restores the given checkpoint (or, lacking that, the most recent checkpoint) into the graph. Only one of the modes is used for saving variables to the SavedModel (order of preference: tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL, then tf.estimator.ModeKeys.PREDICT), such that up to three tf.MetaGraphDefs are saved with a single set of variables in a single SavedModel directory.", "For the variables and tf.MetaGraphDefs, a timestamped export directory below export_dir_base, and writes a SavedModel into it containing the tf.MetaGraphDef for the given mode and its associated signatures.", "For prediction, the exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "For training and evaluation, the train_op is stored in an extra collection, and loss, metrics, and predictions are included in a SignatureDef for the mode in question.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Exports inference graph as a SavedModel into the given dir.", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "The experimental_mode parameter can be used to export a single train/eval/predict graph as a SavedModel. See experimental_export_all_saved_models for full docs.", "View source", "Exports inference graph as a SavedModel into the given dir. (deprecated)", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Returns list of all variable names in this model.", "View source", "Returns value of the variable given by name.", "View source", "Finds the filename of the latest saved checkpoint file in model_dir.", "View source", "Yields predictions for given features.", "Please note that interleaving two predict outputs does not work. See: issue/20506", "View source", "Trains a model given training data input_fn."]}, {"name": "tf.compat.v1.estimator.Estimator", "path": "compat/v1/estimator/estimator", "type": "tf.compat", "text": ["Estimator class to train and evaluate TensorFlow models.", "The Estimator object wraps a model which is specified by a model_fn, which, given inputs and a number of other parameters, returns the ops necessary to perform training, evaluation, or predictions.", "All outputs (checkpoints, event files, etc.) are written to model_dir, or a subdirectory thereof. If model_dir is not set, a temporary directory is used.", "The config argument can be passed tf.estimator.RunConfig object containing information about the execution environment. It is passed on to the model_fn, if the model_fn has a parameter named \"config\" (and input functions in the same manner). If the config parameter is not passed, it is instantiated by the Estimator. Not passing config means that defaults useful for local execution are used. Estimator makes config available to the model (for instance, to allow specialization based on the number of workers available), and also uses some of its fields to control internals, especially regarding checkpointing.", "The params argument contains hyperparameters. It is passed to the model_fn, if the model_fn has a parameter named \"params\", and to the input functions in the same manner. Estimator only passes params along, it does not inspect it. The structure of params is therefore entirely up to the developer.", "None of Estimator's methods can be overridden in subclasses (its constructor enforces this). Subclasses should use model_fn to configure the base class, and may add methods implementing specialized functionality.", "See estimators for more information.", "To warm-start an Estimator:", "For more details on warm-start configuration, see tf.estimator.WarmStartSettings.", "Calling methods of Estimator will work while eager execution is enabled. However, the model_fn and input_fn is not executed eagerly, Estimator will switch to graph mode before calling all user-provided functions (incl. hooks), so their code has to be compatible with graph mode execution. Note that input_fn code using tf.data generally works in both graph and eager modes.", "View source", "Shows the directory name where evaluation metrics are dumped.", "View source", "Evaluates the model given evaluation data input_fn.", "For each step, calls input_fn, which returns one batch of data. Evaluates until:", "View source", "Exports a SavedModel with tf.MetaGraphDefs for each requested mode.", "For each mode passed in via the input_receiver_fn_map, this method builds a new graph by calling the input_receiver_fn to obtain feature and label Tensors. Next, this method calls the Estimator's model_fn in the passed mode to generate the model graph based on those features and labels, and restores the given checkpoint (or, lacking that, the most recent checkpoint) into the graph. Only one of the modes is used for saving variables to the SavedModel (order of preference: tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL, then tf.estimator.ModeKeys.PREDICT), such that up to three tf.MetaGraphDefs are saved with a single set of variables in a single SavedModel directory.", "For the variables and tf.MetaGraphDefs, a timestamped export directory below export_dir_base, and writes a SavedModel into it containing the tf.MetaGraphDef for the given mode and its associated signatures.", "For prediction, the exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "For training and evaluation, the train_op is stored in an extra collection, and loss, metrics, and predictions are included in a SignatureDef for the mode in question.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Exports inference graph as a SavedModel into the given dir.", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "The experimental_mode parameter can be used to export a single train/eval/predict graph as a SavedModel. See experimental_export_all_saved_models for full docs.", "View source", "Exports inference graph as a SavedModel into the given dir. (deprecated)", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Returns list of all variable names in this model.", "View source", "Returns value of the variable given by name.", "View source", "Finds the filename of the latest saved checkpoint file in model_dir.", "View source", "Yields predictions for given features.", "Please note that interleaving two predict outputs does not work. See: issue/20506", "View source", "Trains a model given training data input_fn."]}, {"name": "tf.compat.v1.estimator.experimental", "path": "compat/v1/estimator/experimental", "type": "tf.compat", "text": ["Public API for tf.estimator.experimental namespace.", "class InMemoryEvaluatorHook: Hook to run evaluation in training without a checkpoint.", "class KMeans: An Estimator for K-Means clustering.", "class LinearSDCA: Stochastic Dual Coordinate Ascent helper for linear estimators.", "build_raw_supervised_input_receiver_fn(...): Build a supervised_input_receiver_fn for raw features and labels.", "call_logit_fn(...): Calls logit_fn (experimental).", "dnn_logit_fn_builder(...): Function builder for a dnn logit_fn.", "linear_logit_fn_builder(...): Function builder for a linear logit_fn.", "make_early_stopping_hook(...): Creates early-stopping hook.", "make_stop_at_checkpoint_step_hook(...): Creates a proper StopAtCheckpointStepHook based on chief status.", "stop_if_higher_hook(...): Creates hook to stop if the given metric is higher than the threshold.", "stop_if_lower_hook(...): Creates hook to stop if the given metric is lower than the threshold.", "stop_if_no_decrease_hook(...): Creates hook to stop if metric does not decrease within given max steps.", "stop_if_no_increase_hook(...): Creates hook to stop if metric does not increase within given max steps."]}, {"name": "tf.compat.v1.estimator.experimental.dnn_logit_fn_builder", "path": "compat/v1/estimator/experimental/dnn_logit_fn_builder", "type": "tf.compat", "text": ["Function builder for a dnn logit_fn."]}, {"name": "tf.compat.v1.estimator.experimental.KMeans", "path": "compat/v1/estimator/experimental/kmeans", "type": "tf.compat", "text": [" ", "An Estimator for K-Means clustering.", "Inherits From: Estimator", "The SavedModel saved by the export_saved_model method does not include the cluster centers. However, the cluster centers may be retrieved by the latest checkpoint saved during training. Specifically,", "is equivalent to", "View source", "Returns the cluster centers.", "View source", "Shows the directory name where evaluation metrics are dumped.", "View source", "Evaluates the model given evaluation data input_fn.", "For each step, calls input_fn, which returns one batch of data. Evaluates until:", "View source", "Exports a SavedModel with tf.MetaGraphDefs for each requested mode.", "For each mode passed in via the input_receiver_fn_map, this method builds a new graph by calling the input_receiver_fn to obtain feature and label Tensors. Next, this method calls the Estimator's model_fn in the passed mode to generate the model graph based on those features and labels, and restores the given checkpoint (or, lacking that, the most recent checkpoint) into the graph. Only one of the modes is used for saving variables to the SavedModel (order of preference: tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL, then tf.estimator.ModeKeys.PREDICT), such that up to three tf.MetaGraphDefs are saved with a single set of variables in a single SavedModel directory.", "For the variables and tf.MetaGraphDefs, a timestamped export directory below export_dir_base, and writes a SavedModel into it containing the tf.MetaGraphDef for the given mode and its associated signatures.", "For prediction, the exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "For training and evaluation, the train_op is stored in an extra collection, and loss, metrics, and predictions are included in a SignatureDef for the mode in question.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Exports inference graph as a SavedModel into the given dir.", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "The experimental_mode parameter can be used to export a single train/eval/predict graph as a SavedModel. See experimental_export_all_saved_models for full docs.", "View source", "Exports inference graph as a SavedModel into the given dir. (deprecated)", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Returns list of all variable names in this model.", "View source", "Returns value of the variable given by name.", "View source", "Finds the filename of the latest saved checkpoint file in model_dir.", "View source", "Yields predictions for given features.", "Please note that interleaving two predict outputs does not work. See: issue/20506", "View source", "Finds the index of the closest cluster center to each input point.", "View source", "Returns the sum of squared distances to nearest clusters.", "Note that this function is different from the corresponding one in sklearn which returns the negative sum.", "View source", "Trains a model given training data input_fn.", "View source", "Transforms each input point to its distances to all cluster centers.", "Note that if distance_metric=KMeansClustering.SQUARED_EUCLIDEAN_DISTANCE, this function returns the squared Euclidean distance while the corresponding sklearn function returns the Euclidean distance."]}, {"name": "tf.compat.v1.estimator.experimental.linear_logit_fn_builder", "path": "compat/v1/estimator/experimental/linear_logit_fn_builder", "type": "tf.compat", "text": ["Function builder for a linear logit_fn."]}, {"name": "tf.compat.v1.estimator.export", "path": "compat/v1/estimator/export", "type": "tf.compat", "text": ["All public utility methods for exporting Estimator to SavedModel.", "This file includes functions and constants from core (model_utils) and export.py", "class ClassificationOutput: Represents the output of a classification head.", "class ExportOutput: Represents an output of a model that can be served.", "class PredictOutput: Represents the output of a generic prediction head.", "class RegressionOutput: Represents the output of a regression head.", "class ServingInputReceiver: A return type for a serving_input_receiver_fn.", "class TensorServingInputReceiver: A return type for a serving_input_receiver_fn.", "build_parsing_serving_input_receiver_fn(...): Build a serving_input_receiver_fn expecting fed tf.Examples.", "build_raw_serving_input_receiver_fn(...): Build a serving_input_receiver_fn expecting feature Tensors."]}, {"name": "tf.compat.v1.estimator.inputs", "path": "compat/v1/estimator/inputs", "type": "tf.compat", "text": ["Utility methods to create simple input_fns.", "numpy_input_fn(...): Returns input function that would feed dict of numpy arrays into the model.", "pandas_input_fn(...): Returns input function that would feed Pandas DataFrame into the model."]}, {"name": "tf.compat.v1.estimator.inputs.numpy_input_fn", "path": "compat/v1/estimator/inputs/numpy_input_fn", "type": "tf.compat", "text": ["Returns input function that would feed dict of numpy arrays into the model.", "This returns a function outputting features and targets based on the dict of numpy arrays. The dict features has the same keys as the x. The dict targets has the same keys as the y if y is a dict."]}, {"name": "tf.compat.v1.estimator.inputs.pandas_input_fn", "path": "compat/v1/estimator/inputs/pandas_input_fn", "type": "tf.compat", "text": ["Returns input function that would feed Pandas DataFrame into the model."]}, {"name": "tf.compat.v1.estimator.LinearClassifier", "path": "compat/v1/estimator/linearclassifier", "type": "tf.compat", "text": ["Linear classifier model.", "Inherits From: Estimator", "Train a linear model to classify instances into one of multiple possible classes. When number of possible classes is 2, this is binary classification.", "Input of train and evaluate should have following features, otherwise there will be a KeyError:", "Loss is calculated by using softmax cross entropy.", "Estimators can be used while eager execution is enabled. Note that input_fn and all hooks are executed inside a graph context, so they have to be written to be compatible with graph mode. Note that input_fn code using tf.data generally works in both graph and eager modes.", "View source", "Shows the directory name where evaluation metrics are dumped.", "View source", "Evaluates the model given evaluation data input_fn.", "For each step, calls input_fn, which returns one batch of data. Evaluates until:", "View source", "Exports a SavedModel with tf.MetaGraphDefs for each requested mode.", "For each mode passed in via the input_receiver_fn_map, this method builds a new graph by calling the input_receiver_fn to obtain feature and label Tensors. Next, this method calls the Estimator's model_fn in the passed mode to generate the model graph based on those features and labels, and restores the given checkpoint (or, lacking that, the most recent checkpoint) into the graph. Only one of the modes is used for saving variables to the SavedModel (order of preference: tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL, then tf.estimator.ModeKeys.PREDICT), such that up to three tf.MetaGraphDefs are saved with a single set of variables in a single SavedModel directory.", "For the variables and tf.MetaGraphDefs, a timestamped export directory below export_dir_base, and writes a SavedModel into it containing the tf.MetaGraphDef for the given mode and its associated signatures.", "For prediction, the exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "For training and evaluation, the train_op is stored in an extra collection, and loss, metrics, and predictions are included in a SignatureDef for the mode in question.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Exports inference graph as a SavedModel into the given dir.", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "The experimental_mode parameter can be used to export a single train/eval/predict graph as a SavedModel. See experimental_export_all_saved_models for full docs.", "View source", "Exports inference graph as a SavedModel into the given dir. (deprecated)", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Returns list of all variable names in this model.", "View source", "Returns value of the variable given by name.", "View source", "Finds the filename of the latest saved checkpoint file in model_dir.", "View source", "Yields predictions for given features.", "Please note that interleaving two predict outputs does not work. See: issue/20506", "View source", "Trains a model given training data input_fn."]}, {"name": "tf.compat.v1.estimator.LinearEstimator", "path": "compat/v1/estimator/linearestimator", "type": "tf.compat", "text": ["An estimator for TensorFlow linear models with user-specified head.", "Inherits From: Estimator", "Input of train and evaluate should have following features, otherwise there will be a KeyError:", "Loss and predicted output are determined by the specified head.", "Estimators can be used while eager execution is enabled. Note that input_fn and all hooks are executed inside a graph context, so they have to be written to be compatible with graph mode. Note that input_fn code using tf.data generally works in both graph and eager modes.", "View source", "Shows the directory name where evaluation metrics are dumped.", "View source", "Evaluates the model given evaluation data input_fn.", "For each step, calls input_fn, which returns one batch of data. Evaluates until:", "View source", "Exports a SavedModel with tf.MetaGraphDefs for each requested mode.", "For each mode passed in via the input_receiver_fn_map, this method builds a new graph by calling the input_receiver_fn to obtain feature and label Tensors. Next, this method calls the Estimator's model_fn in the passed mode to generate the model graph based on those features and labels, and restores the given checkpoint (or, lacking that, the most recent checkpoint) into the graph. Only one of the modes is used for saving variables to the SavedModel (order of preference: tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL, then tf.estimator.ModeKeys.PREDICT), such that up to three tf.MetaGraphDefs are saved with a single set of variables in a single SavedModel directory.", "For the variables and tf.MetaGraphDefs, a timestamped export directory below export_dir_base, and writes a SavedModel into it containing the tf.MetaGraphDef for the given mode and its associated signatures.", "For prediction, the exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "For training and evaluation, the train_op is stored in an extra collection, and loss, metrics, and predictions are included in a SignatureDef for the mode in question.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Exports inference graph as a SavedModel into the given dir.", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "The experimental_mode parameter can be used to export a single train/eval/predict graph as a SavedModel. See experimental_export_all_saved_models for full docs.", "View source", "Exports inference graph as a SavedModel into the given dir. (deprecated)", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Returns list of all variable names in this model.", "View source", "Returns value of the variable given by name.", "View source", "Finds the filename of the latest saved checkpoint file in model_dir.", "View source", "Yields predictions for given features.", "Please note that interleaving two predict outputs does not work. See: issue/20506", "View source", "Trains a model given training data input_fn."]}, {"name": "tf.compat.v1.estimator.LinearRegressor", "path": "compat/v1/estimator/linearregressor", "type": "tf.compat", "text": ["An estimator for TensorFlow Linear regression problems.", "Inherits From: Estimator", "Train a linear regression model to predict label value given observation of feature values.", "Input of train and evaluate should have following features, otherwise there will be a KeyError:", "Loss is calculated by using mean squared error.", "Estimators can be used while eager execution is enabled. Note that input_fn and all hooks are executed inside a graph context, so they have to be written to be compatible with graph mode. Note that input_fn code using tf.data generally works in both graph and eager modes.", "View source", "Shows the directory name where evaluation metrics are dumped.", "View source", "Evaluates the model given evaluation data input_fn.", "For each step, calls input_fn, which returns one batch of data. Evaluates until:", "View source", "Exports a SavedModel with tf.MetaGraphDefs for each requested mode.", "For each mode passed in via the input_receiver_fn_map, this method builds a new graph by calling the input_receiver_fn to obtain feature and label Tensors. Next, this method calls the Estimator's model_fn in the passed mode to generate the model graph based on those features and labels, and restores the given checkpoint (or, lacking that, the most recent checkpoint) into the graph. Only one of the modes is used for saving variables to the SavedModel (order of preference: tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL, then tf.estimator.ModeKeys.PREDICT), such that up to three tf.MetaGraphDefs are saved with a single set of variables in a single SavedModel directory.", "For the variables and tf.MetaGraphDefs, a timestamped export directory below export_dir_base, and writes a SavedModel into it containing the tf.MetaGraphDef for the given mode and its associated signatures.", "For prediction, the exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "For training and evaluation, the train_op is stored in an extra collection, and loss, metrics, and predictions are included in a SignatureDef for the mode in question.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Exports inference graph as a SavedModel into the given dir.", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "The experimental_mode parameter can be used to export a single train/eval/predict graph as a SavedModel. See experimental_export_all_saved_models for full docs.", "View source", "Exports inference graph as a SavedModel into the given dir. (deprecated)", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Returns list of all variable names in this model.", "View source", "Returns value of the variable given by name.", "View source", "Finds the filename of the latest saved checkpoint file in model_dir.", "View source", "Yields predictions for given features.", "Please note that interleaving two predict outputs does not work. See: issue/20506", "View source", "Trains a model given training data input_fn."]}, {"name": "tf.compat.v1.estimator.regressor_parse_example_spec", "path": "compat/v1/estimator/regressor_parse_example_spec", "type": "tf.compat", "text": ["Generates parsing spec for tf.parse_example to be used with regressors.", "If users keep data in tf.Example format, they need to call tf.parse_example with a proper feature spec. There are two main things that this utility helps:", "Example output of parsing spec:", "Example usage with a regressor:"]}, {"name": "tf.compat.v1.estimator.tpu", "path": "compat/v1/estimator/tpu", "type": "tf.compat", "text": ["Public API for tf.estimator.tpu namespace.", "experimental module: Public API for tf.estimator.tpu.experimental namespace.", "class InputPipelineConfig: Please see the definition of these values in TPUConfig.", "class RunConfig: RunConfig with TPU support.", "class TPUConfig: TPU related configuration required by TPUEstimator.", "class TPUEstimator: Estimator with TPU support.", "class TPUEstimatorSpec: Ops and objects returned from a model_fn and passed to TPUEstimator."]}, {"name": "tf.compat.v1.estimator.tpu.experimental", "path": "compat/v1/estimator/tpu/experimental", "type": "tf.compat", "text": ["Public API for tf.estimator.tpu.experimental namespace.", "class EmbeddingConfigSpec: Class to keep track of the specification for TPU embeddings."]}, {"name": "tf.compat.v1.estimator.tpu.experimental.EmbeddingConfigSpec", "path": "compat/v1/estimator/tpu/experimental/embeddingconfigspec", "type": "tf.compat", "text": ["Class to keep track of the specification for TPU embeddings.", "Pass this class to tf.estimator.tpu.TPUEstimator via the embedding_config_spec parameter. At minimum you need to specify feature_columns and optimization_parameters. The feature columns passed should be created with some combination of tf.tpu.experimental.embedding_column and tf.tpu.experimental.shared_embedding_columns.", "TPU embeddings do not support arbitrary Tensorflow optimizers and the main optimizer you use for your model will be ignored for the embedding table variables. Instead TPU embeddigns support a fixed set of predefined optimizers that you can select from and set the parameters of. These include adagrad, adam and stochastic gradient descent. Each supported optimizer has a Parameters class in the tf.tpu.experimental namespace."]}, {"name": "tf.compat.v1.estimator.tpu.InputPipelineConfig", "path": "compat/v1/estimator/tpu/inputpipelineconfig", "type": "tf.compat", "text": ["Please see the definition of these values in TPUConfig."]}, {"name": "tf.compat.v1.estimator.tpu.RunConfig", "path": "compat/v1/estimator/tpu/runconfig", "type": "tf.compat", "text": ["RunConfig with TPU support.", "Inherits From: RunConfig", "If device_fn is not None, it overrides the default device function used in Estimator. Otherwise the default one is used. ", "All global ids in the training cluster are assigned from an increasing sequence of consecutive integers. The first id is 0.", "Nodes with task type worker can have id 0, 1, 2. Nodes with task type ps can have id, 0, 1. So, task_id is not unique, but the pair (task_type, task_id) can uniquely determine a node in the cluster.", "Global id, i.e., this field, is tracking the index of the node among ALL nodes in the cluster. It is uniquely assigned. For example, for the cluster spec given above, the global ids are assigned as:", "View source", "Returns a new instance of RunConfig replacing specified properties.", "Only the properties in the following list are allowed to be replaced:", "In addition, either save_checkpoints_steps or save_checkpoints_secs can be set (should not be both)."]}, {"name": "tf.compat.v1.estimator.tpu.TPUConfig", "path": "compat/v1/estimator/tpu/tpuconfig", "type": "tf.compat", "text": ["TPU related configuration required by TPUEstimator."]}, {"name": "tf.compat.v1.estimator.tpu.TPUEstimator", "path": "compat/v1/estimator/tpu/tpuestimator", "type": "tf.compat", "text": ["Estimator with TPU support.", "Inherits From: Estimator", "TPUEstimator also supports training on CPU and GPU. You don't need to define a separate tf.estimator.Estimator.", "TPUEstimator handles many of the details of running on TPU devices, such as replicating inputs and models for each core, and returning to host periodically to run hooks.", "TPUEstimator transforms a global batch size in params to a per-shard batch size when calling the input_fn and model_fn. Users should specify global batch size in constructor, and then get the batch size for each shard in input_fn and model_fn by params['batch_size'].", "For training, model_fn gets per-core batch size; input_fn may get per-core or per-host batch size depending on per_host_input_for_training in TPUConfig (See docstring for TPUConfig for details).", "For evaluation and prediction, model_fn gets per-core batch size and input_fn get per-host batch size.", "model_fn should return TPUEstimatorSpec, which expects the eval_metrics for TPU evaluation. If eval_on_tpu is False, the evaluation will execute on CPU or GPU; in this case the following discussion on TPU evaluation does not apply.", "TPUEstimatorSpec.eval_metrics is a tuple of metric_fn and tensors, where tensors could be a list of any nested structure of Tensors (See TPUEstimatorSpec for details). metric_fn takes the tensors and returns a dict from metric string name to the result of calling a metric function, namely a (metric_tensor, update_op) tuple.", "One can set use_tpu to False for testing. All training, evaluation, and predict will be executed on CPU. input_fn and model_fn will receive train_batch_size or eval_batch_size unmodified as params['batch_size'].", "TPU evaluation only works on a single host (one TPU worker) except BROADCAST mode.", "input_fn for evaluation should NOT raise an end-of-input exception (OutOfRangeError or StopIteration). And all evaluation steps and all batches should have the same size.", "Prediction on TPU is an experimental feature to support large batch inference. It is not designed for latency-critical system. In addition, due to some usability issues, for prediction with small dataset, CPU .predict, i.e., creating a new TPUEstimator instance with use_tpu=False, might be more convenient.", "TPU prediction only works on a single host (one TPU worker).", "input_fn must return a Dataset instance rather than features. In fact, .train() and .evaluate() also support Dataset as return value.", "export_saved_model exports 2 metagraphs, one with saved_model.SERVING, and another with saved_model.SERVING and saved_model.TPU tags. At serving time, these tags are used to select the appropriate metagraph to load.", "Before running the graph on TPU, the TPU system needs to be initialized. If TensorFlow Serving model-server is used, this is done automatically. If not, please use session.run(tpu.initialize_system()).", "There are two versions of the API: 1 or 2.", "In V1, the exported CPU graph is model_fn as it is. The exported TPU graph wraps tpu.rewrite() and TPUPartitionedCallOp around model_fn so model_fn is on TPU by default. To place ops on CPU, tpu.outside_compilation(host_call, logits) can be used.", "In V2, export_saved_model() sets up params['use_tpu'] flag to let the user know if the code is exporting to TPU (or not). When params['use_tpu'] is True, users need to call tpu.rewrite(), TPUPartitionedCallOp and/or batch_function(). Alternatively use inference_on_tpu() which is a convenience wrapper of the three.", "TIP: V2 is recommended as it is more flexible (eg: batching, etc).", "View source", "Shows the directory name where evaluation metrics are dumped.", "View source", "Evaluates the model given evaluation data input_fn.", "For each step, calls input_fn, which returns one batch of data. Evaluates until:", "View source", "Exports a SavedModel with tf.MetaGraphDefs for each requested mode.", "For each mode passed in via the input_receiver_fn_map, this method builds a new graph by calling the input_receiver_fn to obtain feature and label Tensors. Next, this method calls the Estimator's model_fn in the passed mode to generate the model graph based on those features and labels, and restores the given checkpoint (or, lacking that, the most recent checkpoint) into the graph. Only one of the modes is used for saving variables to the SavedModel (order of preference: tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL, then tf.estimator.ModeKeys.PREDICT), such that up to three tf.MetaGraphDefs are saved with a single set of variables in a single SavedModel directory.", "For the variables and tf.MetaGraphDefs, a timestamped export directory below export_dir_base, and writes a SavedModel into it containing the tf.MetaGraphDef for the given mode and its associated signatures.", "For prediction, the exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "For training and evaluation, the train_op is stored in an extra collection, and loss, metrics, and predictions are included in a SignatureDef for the mode in question.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Exports inference graph as a SavedModel into the given dir.", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "The experimental_mode parameter can be used to export a single train/eval/predict graph as a SavedModel. See experimental_export_all_saved_models for full docs.", "View source", "Exports inference graph as a SavedModel into the given dir. (deprecated)", "For a detailed guide, see SavedModel from Estimators.", "This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single tf.MetaGraphDef saved from this session.", "The exported MetaGraphDef will provide one SignatureDef for each element of the export_outputs dict returned from the model_fn, named using the same keys. One of these keys is always tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which signature will be served when a serving request does not specify one. For each signature, the outputs are provided by the corresponding tf.estimator.export.ExportOutputs, and the inputs are always the input receivers provided by the serving_input_receiver_fn.", "Extra assets may be written into the SavedModel via the assets_extra argument. This should be a dict, where each key gives a destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.", "View source", "Returns list of all variable names in this model.", "View source", "Returns value of the variable given by name.", "View source", "Finds the filename of the latest saved checkpoint file in model_dir.", "View source", "Yields predictions for given features.", "Please note that interleaving two predict outputs does not work. See: issue/20506", "View source", "Trains a model given training data input_fn."]}, {"name": "tf.compat.v1.estimator.tpu.TPUEstimatorSpec", "path": "compat/v1/estimator/tpu/tpuestimatorspec", "type": "tf.compat", "text": ["Ops and objects returned from a model_fn and passed to TPUEstimator.", "See EstimatorSpec for mode, predictions, loss, train_op, and export_outputs.", "For evaluation, eval_metricsis a tuple of metric_fn and tensors, where metric_fn runs on CPU to generate metrics and tensors represents the Tensors transferred from TPU system to CPU host and passed to metric_fn. To be precise, TPU evaluation expects a slightly different signature from the tf.estimator.Estimator. While EstimatorSpec.eval_metric_ops expects a dict, TPUEstimatorSpec.eval_metrics is a tuple of metric_fn and tensors. The tensors could be a list of Tensors or dict of names to Tensors. The tensors usually specify the model logits, which are transferred back from TPU system to CPU host. All tensors must have be batch-major, i.e., the batch size is the first dimension. Once all tensors are available at CPU host from all shards, they are concatenated (on CPU) and passed as positional arguments to the metric_fn if tensors is list or keyword arguments if tensors is a dict. metric_fn takes the tensors and returns a dict from metric string name to the result of calling a metric function, namely a (metric_tensor, update_op) tuple. See TPUEstimator for MNIST example how to specify the eval_metrics.", "scaffold_fn is a function running on CPU to generate the Scaffold. This function should not capture any Tensors in model_fn.", "host_call is a tuple of a function and a list or dictionary of tensors to pass to that function and returns a list of Tensors. host_call currently works for train() and evaluate(). The Tensors returned by the function is executed on the CPU on every step, so there is communication overhead when sending tensors from TPU to CPU. To reduce the overhead, try reducing the size of the tensors. The tensors are concatenated along their major (batch) dimension, and so must be >= rank 1. The host_call is useful for writing summaries with tf.contrib.summary.create_file_writer.", "View source", "Creates an equivalent EstimatorSpec used by CPU train/eval."]}, {"name": "tf.compat.v1.Event", "path": "compat/v1/event", "type": "tf.compat", "text": ["A ProtocolMessage", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.summary.Event"]}, {"name": "tf.compat.v1.executing_eagerly", "path": "compat/v1/executing_eagerly", "type": "tf.compat", "text": ["Checks whether the current thread has eager execution enabled.", "Eager execution is typically enabled via tf.compat.v1.enable_eager_execution, but may also be enabled within the context of a Python function via tf.contrib.eager.py_func.", "When eager execution is enabled, returns True in most cases. However, this API might return False in the following use cases.", "Inside tf.function:", "Inside tf.function after tf.config.run_functions_eagerly(True) is called:", "Inside a transformation function for tf.dataset:"]}, {"name": "tf.compat.v1.executing_eagerly_outside_functions", "path": "compat/v1/executing_eagerly_outside_functions", "type": "tf.compat", "text": ["Returns True if executing eagerly, even if inside a graph function.", "This function will check the outermost context for the program and see if it is in eager mode. It is useful comparing to tf.executing_eagerly(), which checks the current context and will return False within a tf.function body. It can be used to build library that behave differently in eager runtime and v1 session runtime (deprecated)."]}, {"name": "tf.compat.v1.expand_dims", "path": "compat/v1/expand_dims", "type": "tf.compat", "text": ["Returns a tensor with a length 1 axis inserted at index axis. (deprecated arguments)", "Given a tensor input, this operation inserts a dimension of length 1 at the dimension index axis of input's shape. The dimension index follows Python indexing rules: It's zero-based, a negative index it is counted backward from the end.", "This operation is useful to:", "If you have a single image of shape [height, width, channels]:", "You can add an outer batch axis by passing axis=0:", "The new axis location matches Python list.insert(axis, 1):", "Following standard Python indexing rules, a negative axis counts from the end so axis=-1 adds an inner most dimension:", "This operation requires that axis is a valid index for input.shape, following Python indexing rules:", "This operation is related to:"]}, {"name": "tf.compat.v1.experimental", "path": "compat/v1/experimental", "type": "tf.compat", "text": ["Public API for tf.experimental namespace.", "class Optional: Represents a value that may or may not be present.", "async_clear_error(...): Clear pending operations and error statuses in async execution.", "async_scope(...): Context manager for grouping async operations.", "function_executor_type(...): Context manager for setting the executor of eager defined functions.", "output_all_intermediates(...): Whether to output all intermediates from functional control flow ops.", "register_filesystem_plugin(...): Loads a TensorFlow FileSystem plugin."]}, {"name": "tf.compat.v1.experimental.output_all_intermediates", "path": "compat/v1/experimental/output_all_intermediates", "type": "tf.compat", "text": ["Whether to output all intermediates from functional control flow ops.", "The \"default\" behavior to is to output all intermediates when using v2 control flow inside Keras models in graph mode (possibly inside Estimators). This is needed to support taking gradients of v2 control flow. In graph mode, Keras can sometimes freeze the forward graph before the gradient computation which does not work for v2 control flow since it requires updating the forward ops to output the needed intermediates. We work around this by proactively outputting the needed intermediates when building the forward pass itself. Ideally any such extra tensors should be pruned out at runtime. However, if for any reason this doesn't work for you or if you have an inference-only model you can turn this behavior off using tf.compat.v1.experimental.output_all_intermediates(False).", "If with the default behavior you are still seeing errors of the form \"Connecting to invalid output X of source node Y which has Z outputs\" try setting tf.compat.v1.experimental.output_all_intermediates(True) and please file an issue at https://github.com/tensorflow/tensorflow/issues."]}, {"name": "tf.compat.v1.extract_image_patches", "path": "compat/v1/extract_image_patches", "type": "tf.compat", "text": ["Extract patches from images and put them in the \"depth\" output dimension.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.image.extract_image_patches"]}, {"name": "tf.compat.v1.feature_column", "path": "compat/v1/feature_column", "type": "tf.compat", "text": ["Public API for tf.feature_column namespace.", "bucketized_column(...): Represents discretized dense input bucketed by boundaries.", "categorical_column_with_hash_bucket(...): Represents sparse feature where ids are set by hashing.", "categorical_column_with_identity(...): A CategoricalColumn that returns identity values.", "categorical_column_with_vocabulary_file(...): A CategoricalColumn with a vocabulary file.", "categorical_column_with_vocabulary_list(...): A CategoricalColumn with in-memory vocabulary.", "crossed_column(...): Returns a column for performing crosses of categorical features.", "embedding_column(...): DenseColumn that converts from sparse, categorical input.", "indicator_column(...): Represents multi-hot representation of given categorical column.", "input_layer(...): Returns a dense Tensor as input layer based on given feature_columns.", "linear_model(...): Returns a linear prediction Tensor based on given feature_columns.", "make_parse_example_spec(...): Creates parsing spec dictionary from input feature_columns.", "numeric_column(...): Represents real valued or numerical features.", "sequence_categorical_column_with_hash_bucket(...): A sequence of categorical terms where ids are set by hashing.", "sequence_categorical_column_with_identity(...): Returns a feature column that represents sequences of integers.", "sequence_categorical_column_with_vocabulary_file(...): A sequence of categorical terms where ids use a vocabulary file.", "sequence_categorical_column_with_vocabulary_list(...): A sequence of categorical terms where ids use an in-memory list.", "sequence_numeric_column(...): Returns a feature column that represents sequences of numeric data.", "shared_embedding_columns(...): List of dense columns that convert from sparse, categorical input.", "weighted_categorical_column(...): Applies weight values to a CategoricalColumn."]}, {"name": "tf.compat.v1.feature_column.categorical_column_with_vocabulary_file", "path": "compat/v1/feature_column/categorical_column_with_vocabulary_file", "type": "tf.compat", "text": ["A CategoricalColumn with a vocabulary file.", "Use this when your inputs are in string or integer format, and you have a vocabulary file that maps each value to an integer ID. By default, out-of-vocabulary values are ignored. Use either (but not both) of num_oov_buckets and default_value to specify how to include out-of-vocabulary values.", "For input dictionary features, features[key] is either Tensor or SparseTensor. If Tensor, missing values can be represented by -1 for int and '' for string, which will be dropped by this feature column.", "Example with num_oov_buckets: File '/us/states.txt' contains 50 lines, each with a 2-character U.S. state abbreviation. All inputs with values in that file are assigned an ID 0-49, corresponding to its line number. All other values are hashed and assigned an ID 50-54.", "Example with default_value: File '/us/states.txt' contains 51 lines - the first line is 'XX', and the other 50 each have a 2-character U.S. state abbreviation. Both a literal 'XX' in input, and other values missing from the file, will be assigned ID 0. All others are assigned the corresponding line number 1-50.", "And to make an embedding with either:"]}, {"name": "tf.compat.v1.feature_column.input_layer", "path": "compat/v1/feature_column/input_layer", "type": "tf.compat", "text": ["Returns a dense Tensor as input layer based on given feature_columns.", "Generally a single example in training data is described with FeatureColumns. At the first layer of the model, this column oriented data should be converted to a single Tensor."]}, {"name": "tf.compat.v1.feature_column.linear_model", "path": "compat/v1/feature_column/linear_model", "type": "tf.compat", "text": ["Returns a linear prediction Tensor based on given feature_columns.", "This function generates a weighted sum based on output dimension units. Weighted sum refers to logits in classification problems. It refers to the prediction itself for linear regression problems.", "Note on supported columns: linear_model treats categorical columns as indicator_columns. To be specific, assume the input as SparseTensor looks like:", "linear_model assigns weights for the presence of \"a\", \"b\", \"c' implicitly, just like indicator_column, while input_layer explicitly requires wrapping each of categorical columns with an embedding_column or an indicator_column.", "The sparse_combiner argument works as follows For example, for two features represented as the categorical columns:", "with sparse_combiner as \"mean\", the linear model outputs consequently are:", "where y_i is the output, b is the bias, and w_x is the weight assigned to the presence of x in the input features."]}, {"name": "tf.compat.v1.feature_column.make_parse_example_spec", "path": "compat/v1/feature_column/make_parse_example_spec", "type": "tf.compat", "text": ["Creates parsing spec dictionary from input feature_columns.", "The returned dictionary can be used as arg 'features' in tf.io.parse_example.", "For the above example, make_parse_example_spec would return the dict:"]}, {"name": "tf.compat.v1.feature_column.shared_embedding_columns", "path": "compat/v1/feature_column/shared_embedding_columns", "type": "tf.compat", "text": ["List of dense columns that convert from sparse, categorical input.", "This is similar to embedding_column, except that it produces a list of embedding columns that share the same embedding weights.", "Use this when your inputs are sparse and of the same type (e.g. watched and impression video IDs that share the same vocabulary), and you want to convert them to a dense representation (e.g., to feed to a DNN).", "Inputs must be a list of categorical columns created by any of the categorical_column_* function. They must all be of the same type and have the same arguments except key. E.g. they can be categorical_column_with_vocabulary_file with the same vocabulary_file. Some or all columns could also be weighted_categorical_column.", "Here is an example embedding of two features for a DNNClassifier model:", "Here is an example using shared_embedding_columns with model_fn:"]}, {"name": "tf.compat.v1.FixedLengthRecordReader", "path": "compat/v1/fixedlengthrecordreader", "type": "tf.compat", "text": ["A Reader that outputs fixed-length records from a file.", "Inherits From: ReaderBase", "See ReaderBase for supported methods.", "Readers are not compatible with eager execution. Instead, please use tf.data to get data into your model.", "View source", "Returns the number of records this reader has produced.", "This is the same as the number of Read executions that have succeeded.", "View source", "Returns the number of work units this reader has finished processing.", "View source", "Returns the next record (key, value) pair produced by a reader.", "Will dequeue a work unit from queue if necessary (e.g. when the Reader needs to start reading from a new file since it has finished with the previous file).", "View source", "Returns up to num_records (key, value) pairs produced by a reader.", "Will dequeue a work unit from queue if necessary (e.g., when the Reader needs to start reading from a new file since it has finished with the previous file). It may return less than num_records even before the last batch.", "View source", "Restore a reader to its initial clean state.", "View source", "Restore a reader to a previously saved state.", "Not all Readers support being restored, so this can produce an Unimplemented error.", "View source", "Produce a string tensor that encodes the state of a reader.", "Not all Readers support being serialized, so this can produce an Unimplemented error."]}, {"name": "tf.compat.v1.fixed_size_partitioner", "path": "compat/v1/fixed_size_partitioner", "type": "tf.compat", "text": ["Partitioner to specify a fixed number of shards along given axis."]}, {"name": "tf.compat.v1.flags", "path": "compat/v1/flags", "type": "tf.compat", "text": ["Import router for absl.flags. See https://github.com/abseil/abseil-py", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags", "tf_decorator module: Base TFDecorator class and utility functions for working with decorators.", "class ArgumentParser: Base class used to parse and convert arguments.", "class ArgumentSerializer: Base class for generating string representations of a flag value.", "class BaseListParser: Base class for a parser of lists of strings.", "class BooleanFlag: Basic boolean flag.", "class BooleanParser: Parser of boolean values.", "class CantOpenFlagFileError: Raised when flagfile fails to open.", "class CsvListSerializer: Base class for generating string representations of a flag value.", "class DuplicateFlagError: Raised if there is a flag naming conflict.", "class EnumClassFlag: Basic enum flag; its value is an enum class's member.", "class EnumClassParser: Parser of an Enum class member.", "class EnumFlag: Basic enum flag; its value can be any string from list of enum_values.", "class EnumParser: Parser of a string enum value (a string value from a given set).", "class Error: The base class for all flags errors.", "class Flag: Information about a command-line flag.", "class FlagHolder: Holds a defined flag.", "class FlagNameConflictsWithMethodError: Raised when a flag name conflicts with FlagValues methods.", "class FlagValues: Registry of 'Flag' objects.", "class FloatParser: Parser of floating point values.", "class IllegalFlagValueError: Raised when the flag command line argument is illegal.", "class IntegerParser: Parser of an integer value.", "class ListParser: Parser for a comma-separated list of strings.", "class ListSerializer: Base class for generating string representations of a flag value.", "class MultiEnumClassFlag: A multi_enum_class flag.", "class MultiFlag: A flag that can appear multiple time on the command-line.", "class UnparsedFlagAccessError: Raised when accessing the flag value from unparsed FlagValues.", "class UnrecognizedFlagError: Raised when a flag is unrecognized.", "class ValidationError: Raised when flag validator constraint is not satisfied.", "class WhitespaceSeparatedListParser: Parser for a whitespace-separated list of strings.", "DEFINE(...): Registers a generic Flag object.", "DEFINE_alias(...): Defines an alias flag for an existing one.", "DEFINE_bool(...): Registers a boolean flag.", "DEFINE_boolean(...): Registers a boolean flag.", "DEFINE_enum(...): Registers a flag whose value can be any string from enum_values.", "DEFINE_enum_class(...): Registers a flag whose value can be the name of enum members.", "DEFINE_flag(...): Registers a 'Flag' object with a 'FlagValues' object.", "DEFINE_float(...): Registers a flag whose value must be a float.", "DEFINE_integer(...): Registers a flag whose value must be an integer.", "DEFINE_list(...): Registers a flag whose value is a comma-separated list of strings.", "DEFINE_multi(...): Registers a generic MultiFlag that parses its args with a given parser.", "DEFINE_multi_enum(...): Registers a flag whose value can be a list strings from enum_values.", "DEFINE_multi_enum_class(...): Registers a flag whose value can be a list of enum members.", "DEFINE_multi_float(...): Registers a flag whose value can be a list of arbitrary floats.", "DEFINE_multi_integer(...): Registers a flag whose value can be a list of arbitrary integers.", "DEFINE_multi_string(...): Registers a flag whose value can be a list of any strings.", "DEFINE_spaceseplist(...): Registers a flag whose value is a whitespace-separated list of strings.", "DEFINE_string(...): Registers a flag whose value can be any string.", "FLAGS(...): Registry of 'Flag' objects.", "adopt_module_key_flags(...): Declares that all flags key to a module are key to the current module.", "declare_key_flag(...): Declares one flag as key to the current module.", "disclaim_key_flags(...): Declares that the current module will not define any more key flags.", "doc_to_help(...): Takes a doc string and reformats it as help.", "flag_dict_to_args(...): Convert a dict of values into process call parameters.", "get_help_width(...): Returns the integer width of help lines that is used in TextWrap.", "mark_bool_flags_as_mutual_exclusive(...): Ensures that only one flag among flag_names is True.", "mark_flag_as_required(...): Ensures that flag is not None during program execution.", "mark_flags_as_mutual_exclusive(...): Ensures that only one flag among flag_names is not None.", "mark_flags_as_required(...): Ensures that flags are not None during program execution.", "multi_flags_validator(...): A function decorator for defining a multi-flag validator.", "register_multi_flags_validator(...): Adds a constraint to multiple flags.", "register_validator(...): Adds a constraint, which will be enforced during program execution.", "text_wrap(...): Wraps a given text to a maximum line length and returns it.", "validator(...): A function decorator for defining a flag validator."]}, {"name": "tf.compat.v1.flags.adopt_module_key_flags", "path": "compat/v1/flags/adopt_module_key_flags", "type": "tf.compat", "text": ["Declares that all flags key to a module are key to the current module.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.adopt_module_key_flags"]}, {"name": "tf.compat.v1.flags.ArgumentParser", "path": "compat/v1/flags/argumentparser", "type": "tf.compat", "text": ["Base class used to parse and convert arguments.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.ArgumentParser", "The parse() method checks to make sure that the string argument is a legal value and convert it to a native type. If the value cannot be converted, it should throw a 'ValueError' exception with a human readable explanation of why the value is illegal.", "Subclasses should also define a syntactic_help string which may be presented to the user to describe the form of the legal values.", "Argument parser classes must be stateless, since instances are cached and shared between flags. Initializer arguments are allowed, but all member variables must be derived from initializer arguments only.", "Returns a string representing the type of the flag.", "Parses the string argument and returns the native value.", "By default it returns its argument unmodified."]}, {"name": "tf.compat.v1.flags.ArgumentSerializer", "path": "compat/v1/flags/argumentserializer", "type": "tf.compat", "text": ["Base class for generating string representations of a flag value.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.ArgumentSerializer", "Returns a serialized string of the value."]}, {"name": "tf.compat.v1.flags.BaseListParser", "path": "compat/v1/flags/baselistparser", "type": "tf.compat", "text": ["Base class for a parser of lists of strings.", "Inherits From: ArgumentParser", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.BaseListParser", "To extend, inherit from this class; from the subclass init, call", "where token is a character used to tokenize, and name is a description of the separator.", "See base class.", "See base class."]}, {"name": "tf.compat.v1.flags.BooleanFlag", "path": "compat/v1/flags/booleanflag", "type": "tf.compat", "text": ["Basic boolean flag.", "Inherits From: Flag", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.BooleanFlag", "Boolean flags do not take any arguments, and their value is either True (1) or False (0). The false value is specified on the command line by prepending the word 'no' to either the long or the short flag name.", "For example, if a Boolean flag was created whose long name was 'update' and whose short name was 'x', then this flag could be explicitly unset through either --noupdate or --nox.", "Returns a str that describes the type of the flag.", "Parses string and sets flag value.", "Serializes the flag.", "Return self==value.", "Return a >= b. Computed by @total_ordering from (not a < b).", "Return a > b. Computed by @total_ordering from (not a < b) and (a != b).", "Return a <= b. Computed by @total_ordering from (a < b) or (a == b).", "Return self<value."]}, {"name": "tf.compat.v1.flags.BooleanParser", "path": "compat/v1/flags/booleanparser", "type": "tf.compat", "text": ["Parser of boolean values.", "Inherits From: ArgumentParser", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.BooleanParser", "See base class.", "See base class."]}, {"name": "tf.compat.v1.flags.CantOpenFlagFileError", "path": "compat/v1/flags/cantopenflagfileerror", "type": "tf.compat", "text": ["Raised when flagfile fails to open.", "Inherits From: Error", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.CantOpenFlagFileError", "E.g. the file doesn't exist, or has wrong permissions."]}, {"name": "tf.compat.v1.flags.CsvListSerializer", "path": "compat/v1/flags/csvlistserializer", "type": "tf.compat", "text": ["Base class for generating string representations of a flag value.", "Inherits From: ArgumentSerializer", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.CsvListSerializer", "Serializes a list as a CSV string or unicode."]}, {"name": "tf.compat.v1.flags.declare_key_flag", "path": "compat/v1/flags/declare_key_flag", "type": "tf.compat", "text": ["Declares one flag as key to the current module.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.declare_key_flag", "Key flags are flags that are deemed really important for a module. They are important when listing help messages; e.g., if the --helpshort command-line flag is used, then only the key flags of the main module are listed (instead of all flags, as in the case of --helpfull).", "flags.declare_key_flag('flag_1')"]}, {"name": "tf.compat.v1.flags.DEFINE", "path": "compat/v1/flags/define", "type": "tf.compat", "text": ["Registers a generic Flag object.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.DEFINE", "Auxiliary function: clients should use the specialized DEFINE_ function instead."]}, {"name": "tf.compat.v1.flags.DEFINE_alias", "path": "compat/v1/flags/define_alias", "type": "tf.compat", "text": ["Defines an alias flag for an existing one.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.DEFINE_alias"]}, {"name": "tf.compat.v1.flags.DEFINE_bool", "path": "compat/v1/flags/define_bool", "type": "tf.compat", "text": ["Registers a boolean flag.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.DEFINE_bool, tf.compat.v1.app.flags.DEFINE_boolean, tf.compat.v1.flags.DEFINE_boolean", "Such a boolean flag does not take an argument. If a user wants to specify a false value explicitly, the long option beginning with 'no' must be used: i.e. --noflag", "This flag will have a value of None, True or False. None is possible if default=None and the user does not specify the flag on the command line."]}, {"name": "tf.compat.v1.flags.DEFINE_enum", "path": "compat/v1/flags/define_enum", "type": "tf.compat", "text": ["Registers a flag whose value can be any string from enum_values.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.DEFINE_enum", "Instead of a string enum, prefer DEFINE_enum_class, which allows defining enums from an enum.Enum class."]}, {"name": "tf.compat.v1.flags.DEFINE_enum_class", "path": "compat/v1/flags/define_enum_class", "type": "tf.compat", "text": ["Registers a flag whose value can be the name of enum members.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.DEFINE_enum_class"]}, {"name": "tf.compat.v1.flags.DEFINE_flag", "path": "compat/v1/flags/define_flag", "type": "tf.compat", "text": ["Registers a 'Flag' object with a 'FlagValues' object.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.DEFINE_flag", "By default, the global FLAGS 'FlagValue' object is used.", "Typical users will use one of the more specialized DEFINE_xxx functions, such as DEFINE_string or DEFINE_integer. But developers who need to create Flag objects themselves should use this function to register their flags."]}, {"name": "tf.compat.v1.flags.DEFINE_float", "path": "compat/v1/flags/define_float", "type": "tf.compat", "text": ["Registers a flag whose value must be a float.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.DEFINE_float", "If lower_bound or upper_bound are set, then this flag must be within the given range."]}, {"name": "tf.compat.v1.flags.DEFINE_integer", "path": "compat/v1/flags/define_integer", "type": "tf.compat", "text": ["Registers a flag whose value must be an integer.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.DEFINE_integer", "If lower_bound, or upper_bound are set, then this flag must be within the given range."]}, {"name": "tf.compat.v1.flags.DEFINE_list", "path": "compat/v1/flags/define_list", "type": "tf.compat", "text": ["Registers a flag whose value is a comma-separated list of strings.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.DEFINE_list", "The flag value is parsed with a CSV parser."]}, {"name": "tf.compat.v1.flags.DEFINE_multi", "path": "compat/v1/flags/define_multi", "type": "tf.compat", "text": ["Registers a generic MultiFlag that parses its args with a given parser.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.DEFINE_multi", "Auxiliary function. Normal users should NOT use it directly.", "Developers who need to create their own 'Parser' classes for options which can appear multiple times can call this module function to register their flags."]}, {"name": "tf.compat.v1.flags.DEFINE_multi_enum", "path": "compat/v1/flags/define_multi_enum", "type": "tf.compat", "text": ["Registers a flag whose value can be a list strings from enum_values.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.DEFINE_multi_enum", "Use the flag on the command line multiple times to place multiple enum values into the list. The 'default' may be a single string (which will be converted into a single-element list) or a list of strings."]}, {"name": "tf.compat.v1.flags.DEFINE_multi_enum_class", "path": "compat/v1/flags/define_multi_enum_class", "type": "tf.compat", "text": ["Registers a flag whose value can be a list of enum members.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.DEFINE_multi_enum_class", "Use the flag on the command line multiple times to place multiple enum values into the list."]}, {"name": "tf.compat.v1.flags.DEFINE_multi_float", "path": "compat/v1/flags/define_multi_float", "type": "tf.compat", "text": ["Registers a flag whose value can be a list of arbitrary floats.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.DEFINE_multi_float", "Use the flag on the command line multiple times to place multiple float values into the list. The 'default' may be a single float (which will be converted into a single-element list) or a list of floats."]}, {"name": "tf.compat.v1.flags.DEFINE_multi_integer", "path": "compat/v1/flags/define_multi_integer", "type": "tf.compat", "text": ["Registers a flag whose value can be a list of arbitrary integers.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.DEFINE_multi_integer", "Use the flag on the command line multiple times to place multiple integer values into the list. The 'default' may be a single integer (which will be converted into a single-element list) or a list of integers."]}, {"name": "tf.compat.v1.flags.DEFINE_multi_string", "path": "compat/v1/flags/define_multi_string", "type": "tf.compat", "text": ["Registers a flag whose value can be a list of any strings.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.DEFINE_multi_string", "Use the flag on the command line multiple times to place multiple string values into the list. The 'default' may be a single string (which will be converted into a single-element list) or a list of strings."]}, {"name": "tf.compat.v1.flags.DEFINE_spaceseplist", "path": "compat/v1/flags/define_spaceseplist", "type": "tf.compat", "text": ["Registers a flag whose value is a whitespace-separated list of strings.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.DEFINE_spaceseplist", "Any whitespace can be used as a separator."]}, {"name": "tf.compat.v1.flags.DEFINE_string", "path": "compat/v1/flags/define_string", "type": "tf.compat", "text": ["Registers a flag whose value can be any string.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.DEFINE_string"]}, {"name": "tf.compat.v1.flags.disclaim_key_flags", "path": "compat/v1/flags/disclaim_key_flags", "type": "tf.compat", "text": ["Declares that the current module will not define any more key flags.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.disclaim_key_flags", "Normally, the module that calls the DEFINE_xxx functions claims the flag to be its key flag. This is undesirable for modules that define additional DEFINE_yyy functions with its own flag parsers and serializers, since that module will accidentally claim flags defined by DEFINE_yyy as its key flags. After calling this function, the module disclaims flag definitions thereafter, so the key flags will be correctly attributed to the caller of DEFINE_yyy.", "After calling this function, the module will not be able to define any more flags. This function will affect all FlagValues objects."]}, {"name": "tf.compat.v1.flags.doc_to_help", "path": "compat/v1/flags/doc_to_help", "type": "tf.compat", "text": ["Takes a doc string and reformats it as help.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.doc_to_help"]}, {"name": "tf.compat.v1.flags.DuplicateFlagError", "path": "compat/v1/flags/duplicateflagerror", "type": "tf.compat", "text": ["Raised if there is a flag naming conflict.", "Inherits From: Error", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.DuplicateFlagError", "Creates a DuplicateFlagError by providing flag name and values."]}, {"name": "tf.compat.v1.flags.EnumClassFlag", "path": "compat/v1/flags/enumclassflag", "type": "tf.compat", "text": ["Basic enum flag; its value is an enum class's member.", "Inherits From: Flag", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.EnumClassFlag", "Returns a str that describes the type of the flag.", "Parses string and sets flag value.", "Serializes the flag.", "Return self==value.", "Return a >= b. Computed by @total_ordering from (not a < b).", "Return a > b. Computed by @total_ordering from (not a < b) and (a != b).", "Return a <= b. Computed by @total_ordering from (a < b) or (a == b).", "Return self<value."]}, {"name": "tf.compat.v1.flags.EnumClassParser", "path": "compat/v1/flags/enumclassparser", "type": "tf.compat", "text": ["Parser of an Enum class member.", "Inherits From: ArgumentParser", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.EnumClassParser", "See base class.", "Determines validity of argument and returns the correct element of enum."]}, {"name": "tf.compat.v1.flags.EnumFlag", "path": "compat/v1/flags/enumflag", "type": "tf.compat", "text": ["Basic enum flag; its value can be any string from list of enum_values.", "Inherits From: Flag", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.EnumFlag", "Returns a str that describes the type of the flag.", "Parses string and sets flag value.", "Serializes the flag.", "Return self==value.", "Return a >= b. Computed by @total_ordering from (not a < b).", "Return a > b. Computed by @total_ordering from (not a < b) and (a != b).", "Return a <= b. Computed by @total_ordering from (a < b) or (a == b).", "Return self<value."]}, {"name": "tf.compat.v1.flags.EnumParser", "path": "compat/v1/flags/enumparser", "type": "tf.compat", "text": ["Parser of a string enum value (a string value from a given set).", "Inherits From: ArgumentParser", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.EnumParser", "See base class.", "Determines validity of argument and returns the correct element of enum."]}, {"name": "tf.compat.v1.flags.Error", "path": "compat/v1/flags/error", "type": "tf.compat", "text": ["The base class for all flags errors.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.Error"]}, {"name": "tf.compat.v1.flags.Flag", "path": "compat/v1/flags/flag", "type": "tf.compat", "text": ["Information about a command-line flag.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.Flag", "'Flag' objects define the following fields: .name - the name for this flag; .default - the default value for this flag; .default_unparsed - the unparsed default value for this flag. .default_as_str - default value as repr'd string, e.g., \"'true'\" (or None); .value - the most recent parsed value of this flag; set by parse(); .help - a help string or None if no help is available; .short_name - the single letter alias for this flag (or None); .boolean - if 'true', this flag does not accept arguments; .present - true if this flag was parsed from command line flags; .parser - an ArgumentParser object; .serializer - an ArgumentSerializer object; .allow_override - the flag may be redefined without raising an error, and newly defined flag overrides the old one. .allow_override_cpp - use the flag from C++ if available; the flag definition is replaced by the C++ flag after init; .allow_hide_cpp - use the Python flag despite having a C++ flag with the same name (ignore the C++ flag); .using_default_value - the flag value has not been set by user; .allow_overwrite - the flag may be parsed more than once without raising an error, the last set value will be used; .allow_using_method_names - whether this flag can be defined even if it has a name that conflicts with a FlagValues method.", "The only public method of a 'Flag' object is parse(), but it is typically only called by a 'FlagValues' object. The parse() method is a thin wrapper around the 'ArgumentParser' parse() method. The parsed value is saved in .value, and the .present attribute is updated. If this flag was already present, an Error is raised.", "parse() is also called during init to parse the default value and initialize the .value attribute. This enables other python modules to safely use flags even if the main module neglects to parse the command line arguments. The .present attribute is cleared after init parsing. If the default value is set to None, then the init parsing step is skipped and the .value attribute is initialized to None.", "Returns a str that describes the type of the flag.", "Parses string and sets flag value.", "Serializes the flag.", "Return self==value.", "Return a >= b. Computed by @total_ordering from (not a < b).", "Return a > b. Computed by @total_ordering from (not a < b) and (a != b).", "Return a <= b. Computed by @total_ordering from (a < b) or (a == b).", "Return self<value."]}, {"name": "tf.compat.v1.flags.FlagHolder", "path": "compat/v1/flags/flagholder", "type": "tf.compat", "text": ["Holds a defined flag.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.FlagHolder", "This facilitates a cleaner api around global state. Instead of", "it encourages code like", "since the name of the flag appears only once in the source code.", "If _ensure_non_none_value is True, then return value is not None. ", "Return self==value."]}, {"name": "tf.compat.v1.flags.FlagNameConflictsWithMethodError", "path": "compat/v1/flags/flagnameconflictswithmethoderror", "type": "tf.compat", "text": ["Raised when a flag name conflicts with FlagValues methods.", "Inherits From: Error", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.FlagNameConflictsWithMethodError"]}, {"name": "tf.compat.v1.flags.FLAGS", "path": "compat/v1/flags/flags", "type": "tf.compat", "text": ["Registry of 'Flag' objects.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.FLAGS", "A 'FlagValues' can then scan command line arguments, passing flag arguments through to the 'Flag' objects that it owns. It also provides easy access to the flag values. Typically only one 'FlagValues' object is needed by an application: flags.FLAGS", "This class is heavily overloaded:", "'Flag' objects are registered via setitem: FLAGS['longname'] = x # register a new flag", "The .value attribute of the registered 'Flag' objects can be accessed as attributes of this 'FlagValues' object, through getattr. Both the long and short name of the original 'Flag' objects can be used to access its value: FLAGS.longname # parsed flag value FLAGS.x # parsed flag value (short name)", "Command line arguments are scanned and passed to the registered 'Flag' objects through the call method. Unparsed arguments, including argv0 are returned. argv = FLAGS(sys.argv) # scan command line arguments", "The original registered Flag objects can be retrieved through the use of the dictionary-like operator, getitem: x = FLAGS['longname'] # access the registered Flag object", "The str() operator of a 'FlagValues' object provides help for all of the registered 'Flag' objects."]}, {"name": "tf.compat.v1.flags.FlagValues", "path": "compat/v1/flags/flagvalues", "type": "tf.compat", "text": ["Registry of 'Flag' objects.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.FlagValues", "A 'FlagValues' can then scan command line arguments, passing flag arguments through to the 'Flag' objects that it owns. It also provides easy access to the flag values. Typically only one 'FlagValues' object is needed by an application: flags.FLAGS", "This class is heavily overloaded:", "'Flag' objects are registered via setitem: FLAGS['longname'] = x # register a new flag", "The .value attribute of the registered 'Flag' objects can be accessed as attributes of this 'FlagValues' object, through getattr. Both the long and short name of the original 'Flag' objects can be used to access its value: FLAGS.longname # parsed flag value FLAGS.x # parsed flag value (short name)", "Command line arguments are scanned and passed to the registered 'Flag' objects through the call method. Unparsed arguments, including argv0 are returned. argv = FLAGS(sys.argv) # scan command line arguments", "The original registered Flag objects can be retrieved through the use of the dictionary-like operator, getitem: x = FLAGS['longname'] # access the registered Flag object", "The str() operator of a 'FlagValues' object provides help for all of the registered 'Flag' objects.", "Appends flags registered in another FlagValues instance.", "Appends all flags assignments from this FlagInfo object to a file.", "Output will be in the format of a flagfile.", "Return the name of the module defining this flag, or default.", "Return the ID of the module defining this flag, or default.", "Returns a dictionary that maps flag names to flag values.", "Returns the dictionary of module_name -> list of defined flags.", "Returns the dictionary of module_id -> list of defined flags.", "Returns a string with the flags assignments from this FlagValues object.", "This function ignores flags whose value is None. Each flag assignment is separated by a newline.", "Returns the value of a flag (if not None) or a default value.", "Returns the list of flags defined by a module.", "Returns a help string for all known flags.", "Returns the list of key flags for a module.", "Returns whether flags were parsed.", "Returns the dictionary of module_name -> list of key flags.", "Describes the key flags of the main module.", "Explicitly marks flags as parsed.", "Use this when the caller knows that this FlagValues has been parsed as if a call() invocation has happened. This is only a public method for use by things like appcommands which do additional command like parsing.", "Describes the key flags of a module.", "Processes command line args, but also allow args to be read from file.", "This function is called by FLAGS(argv). It scans the input list for a flag that looks like: --flagfile=. Then it opens , reads all valid key and value pairs and inserts them into the input list in exactly the place where the --flagfile arg is found.", "Note that your application's flags are still defined the usual way using absl.flags DEFINE_flag() type functions.", "Notes (assuming we're getting a commandline of some sort as our input): --> For duplicate flags, the last one we hit should \"win\". --> Since flags that appear later win, a flagfile's settings can be \"weak\" if the --flagfile comes at the beginning of the argument sequence, and it can be \"strong\" if the --flagfile comes at the end. --> A further \"--flagfile=\" CAN be nested in a flagfile. It will be expanded in exactly the spot where it is found. --> In a flagfile, a line beginning with # or // is a comment. --> Entirely blank lines should be ignored.", "Records the module that defines a specific flag.", "We keep track of which flag is defined by which module so that we can later sort the flags by module.", "Records the module that defines a specific flag.", "Specifies that a flag is a key flag for a module.", "Remove flags that were previously appended from another FlagValues.", "Changes the default value of the named flag object.", "The flag's current value is also updated if the flag is currently using the default value, i.e. not specified in the command line, and not set by FLAGS.name = value.", "Sets whether or not to use GNU style scanning.", "GNU style allows mixing of flag and non-flag arguments. See http://docs.python.org/library/getopt.html#getopt.gnu_getopt", "Unparses all flags to the point before any FLAGS(argv) was called.", "Verifies whether all flags pass validation.", "Outputs flag documentation in XML format.", "Parses flags from argv; stores parsed flags into this FlagValues object.", "All unparsed arguments are returned.", "Returns True if name is a value (flag) in the dict.", "Returns the Flag object for the flag --name."]}, {"name": "tf.compat.v1.flags.flag_dict_to_args", "path": "compat/v1/flags/flag_dict_to_args", "type": "tf.compat", "text": ["Convert a dict of values into process call parameters.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.flag_dict_to_args", "This method is used to convert a dictionary into a sequence of parameters for a binary that parses arguments using this module."]}, {"name": "tf.compat.v1.flags.FloatParser", "path": "compat/v1/flags/floatparser", "type": "tf.compat", "text": ["Parser of floating point values.", "Inherits From: ArgumentParser", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.FloatParser", "Parsed value may be bounded to a given upper and lower bound.", "Returns the float value of argument.", "See base class.", "Returns whether the value is outside the bounds or not.", "See base class."]}, {"name": "tf.compat.v1.flags.get_help_width", "path": "compat/v1/flags/get_help_width", "type": "tf.compat", "text": ["Returns the integer width of help lines that is used in TextWrap.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.get_help_width"]}, {"name": "tf.compat.v1.flags.IllegalFlagValueError", "path": "compat/v1/flags/illegalflagvalueerror", "type": "tf.compat", "text": ["Raised when the flag command line argument is illegal.", "Inherits From: Error", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.IllegalFlagValueError"]}, {"name": "tf.compat.v1.flags.IntegerParser", "path": "compat/v1/flags/integerparser", "type": "tf.compat", "text": ["Parser of an integer value.", "Inherits From: ArgumentParser", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.IntegerParser", "Parsed value may be bounded to a given upper and lower bound.", "Returns the int value of argument.", "See base class.", "Returns whether the value is outside the bounds or not.", "See base class."]}, {"name": "tf.compat.v1.flags.ListParser", "path": "compat/v1/flags/listparser", "type": "tf.compat", "text": ["Parser for a comma-separated list of strings.", "Inherits From: BaseListParser, ArgumentParser", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.ListParser", "See base class.", "Parses argument as comma-separated list of strings."]}, {"name": "tf.compat.v1.flags.ListSerializer", "path": "compat/v1/flags/listserializer", "type": "tf.compat", "text": ["Base class for generating string representations of a flag value.", "Inherits From: ArgumentSerializer", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.ListSerializer", "See base class."]}, {"name": "tf.compat.v1.flags.mark_bool_flags_as_mutual_exclusive", "path": "compat/v1/flags/mark_bool_flags_as_mutual_exclusive", "type": "tf.compat", "text": ["Ensures that only one flag among flag_names is True.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.mark_bool_flags_as_mutual_exclusive"]}, {"name": "tf.compat.v1.flags.mark_flags_as_mutual_exclusive", "path": "compat/v1/flags/mark_flags_as_mutual_exclusive", "type": "tf.compat", "text": ["Ensures that only one flag among flag_names is not None.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.mark_flags_as_mutual_exclusive", "Important note: This validator checks if flag values are None, and it does not distinguish between default and explicit values. Therefore, this validator does not make sense when applied to flags with default values other than None, including other false values (e.g. False, 0, '', []). That includes multi flags with a default value of [] instead of None."]}, {"name": "tf.compat.v1.flags.mark_flags_as_required", "path": "compat/v1/flags/mark_flags_as_required", "type": "tf.compat", "text": ["Ensures that flags are not None during program execution.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.mark_flags_as_required", "If your module might be imported by others, and you only wish to make the flag required when the module is directly executed, call this method like this:", "if name == 'main': flags.mark_flags_as_required(['flag1', 'flag2', 'flag3']) app.run()"]}, {"name": "tf.compat.v1.flags.mark_flag_as_required", "path": "compat/v1/flags/mark_flag_as_required", "type": "tf.compat", "text": ["Ensures that flag is not None during program execution.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.mark_flag_as_required", "Registers a flag validator, which will follow usual validator rules. Important note: validator will pass for any non-None value, such as False, 0 (zero), '' (empty string) and so on.", "If your module might be imported by others, and you only wish to make the flag required when the module is directly executed, call this method like this:", "if name == 'main': flags.mark_flag_as_required('your_flag_name') app.run()"]}, {"name": "tf.compat.v1.flags.MultiEnumClassFlag", "path": "compat/v1/flags/multienumclassflag", "type": "tf.compat", "text": ["A multi_enum_class flag.", "Inherits From: MultiFlag, Flag", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.MultiEnumClassFlag", "See the doc for MultiFlag for most behaviors of this class. In addition, this class knows how to handle enum.Enum instances as values for this flag type.", "See base class.", "Parses one or more arguments with the installed parser.", "Serializes the flag.", "Return self==value.", "Return a >= b. Computed by @total_ordering from (not a < b).", "Return a > b. Computed by @total_ordering from (not a < b) and (a != b).", "Return a <= b. Computed by @total_ordering from (a < b) or (a == b).", "Return self<value."]}, {"name": "tf.compat.v1.flags.MultiFlag", "path": "compat/v1/flags/multiflag", "type": "tf.compat", "text": ["A flag that can appear multiple time on the command-line.", "Inherits From: Flag", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.MultiFlag", "The value of such a flag is a list that contains the individual values from all the appearances of that flag on the command-line.", "See the doc for Flag for most behavior of this class. Only differences in behavior are described here:", "The default value may be either a single value or an iterable of values. A single value is transformed into a single-item list of that value.", "The value of the flag is always a list, even if the option was only supplied once, and even if the default value is a single value", "See base class.", "Parses one or more arguments with the installed parser.", "Serializes the flag.", "Return self==value.", "Return a >= b. Computed by @total_ordering from (not a < b).", "Return a > b. Computed by @total_ordering from (not a < b) and (a != b).", "Return a <= b. Computed by @total_ordering from (a < b) or (a == b).", "Return self<value."]}, {"name": "tf.compat.v1.flags.multi_flags_validator", "path": "compat/v1/flags/multi_flags_validator", "type": "tf.compat", "text": ["A function decorator for defining a multi-flag validator.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.multi_flags_validator", "Registers the decorated function as a validator for flag_names, e.g.", "@flags.multi_flags_validator(['foo', 'bar']) def _CheckFooBar(flags_dict): ...", "See register_multi_flags_validator() for the specification of checker function."]}, {"name": "tf.compat.v1.flags.register_multi_flags_validator", "path": "compat/v1/flags/register_multi_flags_validator", "type": "tf.compat", "text": ["Adds a constraint to multiple flags.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.register_multi_flags_validator", "The constraint is validated when flags are initially parsed, and after each change of the corresponding flag's value."]}, {"name": "tf.compat.v1.flags.register_validator", "path": "compat/v1/flags/register_validator", "type": "tf.compat", "text": ["Adds a constraint, which will be enforced during program execution.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.register_validator", "The constraint is validated when flags are initially parsed, and after each change of the corresponding flag's value. Args: flag_name: str, name of the flag to be checked. checker: callable, a function to validate the flag. input - A single positional argument: The value of the corresponding flag (string, boolean, etc. This value will be passed to checker by the library). output - bool, True if validator constraint is satisfied. If constraint is not satisfied, it should either return False or raise flags.ValidationError(desired_error_message). message: str, error text to be shown to the user if checker returns False. If checker raises flags.ValidationError, message from the raised error will be shown. flag_values: flags.FlagValues, optional FlagValues instance to validate against. Raises: AttributeError: Raised when flag_name is not registered as a valid flag name."]}, {"name": "tf.compat.v1.flags.text_wrap", "path": "compat/v1/flags/text_wrap", "type": "tf.compat", "text": ["Wraps a given text to a maximum line length and returns it.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.text_wrap", "It turns lines that only contain whitespace into empty lines, keeps new lines, and expands tabs using 4 spaces."]}, {"name": "tf.compat.v1.flags.tf_decorator", "path": "compat/v1/flags/tf_decorator", "type": "tf.compat", "text": ["Base TFDecorator class and utility functions for working with decorators.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.tf_decorator", "There are two ways to create decorators that TensorFlow can introspect into. This is important for documentation generation purposes, so that function signatures aren't obscured by the (*args, **kwds) signature that decorators often provide.", "def print_hello_before_calling(target): def wrapper(*args, *kwargs): print('hello') return target(args, **kwargs) return tf_decorator.make_decorator(target, wrapper)", "class CallCounter(tf_decorator.TFDecorator): def init(self, target): super(CallCounter, self).init('count_calls', target) self.call_count = 0", "def call(self, *args, *kwargs): self.call_count += 1 return super(CallCounter, self).decorated_target(args, **kwargs)", "def count_calls(target): return CallCounter(target)", "tf_stack module: Functions used to extract and analyze stacks. Faster than Python libs.", "class TFDecorator: Base class for all TensorFlow decorators.", "make_decorator(...): Make a decorator from a wrapper and a target.", "rewrap(...): Injects a new target into a function built by make_decorator.", "unwrap(...): Unwraps an object into a list of TFDecorators and a final target."]}, {"name": "tf.compat.v1.flags.tf_decorator.make_decorator", "path": "compat/v1/flags/tf_decorator/make_decorator", "type": "tf.compat", "text": ["Make a decorator from a wrapper and a target.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.tf_decorator.make_decorator"]}, {"name": "tf.compat.v1.flags.tf_decorator.rewrap", "path": "compat/v1/flags/tf_decorator/rewrap", "type": "tf.compat", "text": ["Injects a new target into a function built by make_decorator.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.tf_decorator.rewrap", "This function allows replacing a function wrapped by decorator_func, assuming the decorator that wraps the function is written as described below.", "The decorator function must use <decorator name>.__wrapped__ instead of the wrapped function that is normally used:", "def simple_parametrized_wrapper(*args, *kwds): return wrapped_fn(args, **kwds)", "tf_decorator.make_decorator(simple_parametrized_wrapper, wrapped_fn)", "def simple_parametrized_wrapper(*args, *kwds): return simple_parametrizedwrapper.wrapped_(args, **kwds)", "tf_decorator.make_decorator(simple_parametrized_wrapper, wrapped_fn)", "Note that this process modifies decorator_func."]}, {"name": "tf.compat.v1.flags.tf_decorator.TFDecorator", "path": "compat/v1/flags/tf_decorator/tfdecorator", "type": "tf.compat", "text": ["Base class for all TensorFlow decorators.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.tf_decorator.TFDecorator", "TFDecorator captures and exposes the wrapped target, and provides details about the current decorator.", "View source", "Call self as a function."]}, {"name": "tf.compat.v1.flags.tf_decorator.tf_stack", "path": "compat/v1/flags/tf_decorator/tf_stack", "type": "tf.compat", "text": ["Functions used to extract and analyze stacks. Faster than Python libs.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.tf_decorator.tf_stack", "class CurrentModuleFilter: Filters stack frames from the module where this is used (best effort).", "class FrameSummary", "class StackSummary", "class StackTraceFilter: Allows filtering traceback information by removing superfluous frames.", "class StackTraceMapper: Allows remapping traceback information to different source code.", "class StackTraceTransform: Base class for stack trace transformation functions.", "extract_stack(...): A lightweight, extensible re-implementation of traceback.extract_stack."]}, {"name": "tf.compat.v1.flags.tf_decorator.tf_stack.CurrentModuleFilter", "path": "compat/v1/flags/tf_decorator/tf_stack/currentmodulefilter", "type": "tf.compat", "text": ["Filters stack frames from the module where this is used (best effort).", "Inherits From: StackTraceFilter, StackTraceTransform", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.tf_decorator.tf_stack.CurrentModuleFilter", "View source", "View source", "View source", "View source"]}, {"name": "tf.compat.v1.flags.tf_decorator.tf_stack.extract_stack", "path": "compat/v1/flags/tf_decorator/tf_stack/extract_stack", "type": "tf.compat", "text": ["A lightweight, extensible re-implementation of traceback.extract_stack.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.tf_decorator.tf_stack.extract_stack", "NOTE(mrry): traceback.extract_stack eagerly retrieves the line of code for each stack frame using linecache, which results in an abundance of stat() calls. This implementation does not retrieve the code, and any consumer should apply _convert_stack to the result to obtain a traceback that can be formatted etc. using traceback methods."]}, {"name": "tf.compat.v1.flags.tf_decorator.tf_stack.FrameSummary", "path": "compat/v1/flags/tf_decorator/tf_stack/framesummary", "type": "tf.compat", "text": [" Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.tf_decorator.tf_stack.FrameSummary", "eq(self: tensorflow.python._tf_stack.FrameSummary, arg0: tensorflow.python._tf_stack.FrameSummary) -> bool", "getitem(self: tensorflow.python._tf_stack.FrameSummary, arg0: object) -> object", "iter(self: tensorflow.python._tf_stack.FrameSummary) -> iterator", "len(self: tensorflow.python._tf_stack.FrameSummary) -> int", "ne(self: tensorflow.python._tf_stack.FrameSummary, arg0: tensorflow.python._tf_stack.FrameSummary) -> bool"]}, {"name": "tf.compat.v1.flags.tf_decorator.tf_stack.StackSummary", "path": "compat/v1/flags/tf_decorator/tf_stack/stacksummary", "type": "tf.compat", "text": [" Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.tf_decorator.tf_stack.StackSummary", "append(self: tensorflow.python._tf_stack.StackSummary, x: tensorflow.python._tf_stack.FrameSummary) -> None", "Add an item to the end of the list", "count(self: tensorflow.python._tf_stack.StackSummary, x: tensorflow.python._tf_stack.FrameSummary) -> int", "Return the number of times x appears in the list", "extend(*args, **kwargs) Overloaded function.", "Extend the list by appending all the items in the given list", "Extend the list by appending all the items in the given list", "insert(self: tensorflow.python._tf_stack.StackSummary, i: int, x: tensorflow.python._tf_stack.FrameSummary) -> None", "Insert an item at a given position.", "pop(*args, **kwargs) Overloaded function.", "Remove and return the last item", "Remove and return the item at index i", "remove(self: tensorflow.python._tf_stack.StackSummary, x: tensorflow.python._tf_stack.FrameSummary) -> None", "Remove the first item from the list whose value is x. It is an error if there is no such item.", "bool(self: tensorflow.python._tf_stack.StackSummary) -> bool", "Check whether the list is nonempty", "contains(self: tensorflow.python._tf_stack.StackSummary, x: tensorflow.python._tf_stack.FrameSummary) -> bool", "Return true the container contains x", "eq(self: tensorflow.python._tf_stack.StackSummary, arg0: tensorflow.python._tf_stack.StackSummary) -> bool", "getitem(*args, **kwargs) Overloaded function.", "Retrieve list elements using a slice object", "getitem(self: tensorflow.python._tf_stack.StackSummary, arg0: int) -> tensorflow.python._tf_stack.FrameSummary", "getitem(self: tensorflow.python._tf_stack.StackSummary, arg0: int) -> tensorflow.python._tf_stack.FrameSummary", "iter(self: tensorflow.python._tf_stack.StackSummary) -> iterator", "len(self: tensorflow.python._tf_stack.StackSummary) -> int", "ne(self: tensorflow.python._tf_stack.StackSummary, arg0: tensorflow.python._tf_stack.StackSummary) -> bool"]}, {"name": "tf.compat.v1.flags.tf_decorator.tf_stack.StackTraceFilter", "path": "compat/v1/flags/tf_decorator/tf_stack/stacktracefilter", "type": "tf.compat", "text": ["Allows filtering traceback information by removing superfluous frames.", "Inherits From: StackTraceTransform", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.tf_decorator.tf_stack.StackTraceFilter", "View source", "View source", "View source", "View source"]}, {"name": "tf.compat.v1.flags.tf_decorator.tf_stack.StackTraceMapper", "path": "compat/v1/flags/tf_decorator/tf_stack/stacktracemapper", "type": "tf.compat", "text": ["Allows remapping traceback information to different source code.", "Inherits From: StackTraceTransform", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.tf_decorator.tf_stack.StackTraceMapper", "View source", "Returns a map (filename, lineno) -> (filename, lineno, function_name).", "View source", "View source", "View source"]}, {"name": "tf.compat.v1.flags.tf_decorator.tf_stack.StackTraceTransform", "path": "compat/v1/flags/tf_decorator/tf_stack/stacktracetransform", "type": "tf.compat", "text": ["Base class for stack trace transformation functions.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.tf_decorator.tf_stack.StackTraceTransform", "View source", "View source", "View source"]}, {"name": "tf.compat.v1.flags.tf_decorator.unwrap", "path": "compat/v1/flags/tf_decorator/unwrap", "type": "tf.compat", "text": ["Unwraps an object into a list of TFDecorators and a final target.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.tf_decorator.unwrap"]}, {"name": "tf.compat.v1.flags.UnparsedFlagAccessError", "path": "compat/v1/flags/unparsedflagaccesserror", "type": "tf.compat", "text": ["Raised when accessing the flag value from unparsed FlagValues.", "Inherits From: Error", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.UnparsedFlagAccessError"]}, {"name": "tf.compat.v1.flags.UnrecognizedFlagError", "path": "compat/v1/flags/unrecognizedflagerror", "type": "tf.compat", "text": ["Raised when a flag is unrecognized.", "Inherits From: Error", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.UnrecognizedFlagError"]}, {"name": "tf.compat.v1.flags.ValidationError", "path": "compat/v1/flags/validationerror", "type": "tf.compat", "text": ["Raised when flag validator constraint is not satisfied.", "Inherits From: Error", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.ValidationError"]}, {"name": "tf.compat.v1.flags.validator", "path": "compat/v1/flags/validator", "type": "tf.compat", "text": ["A function decorator for defining a flag validator.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.validator", "Registers the decorated function as a validator for flag_name, e.g.", "@flags.validator('foo') def _CheckFoo(foo): ...", "See register_validator() for the specification of checker function."]}, {"name": "tf.compat.v1.flags.WhitespaceSeparatedListParser", "path": "compat/v1/flags/whitespaceseparatedlistparser", "type": "tf.compat", "text": ["Parser for a whitespace-separated list of strings.", "Inherits From: BaseListParser, ArgumentParser", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.app.flags.WhitespaceSeparatedListParser", "See base class.", "Parses argument as whitespace-separated list of strings.", "It also parses argument as comma-separated list of strings if requested."]}, {"name": "tf.compat.v1.floor_div", "path": "compat/v1/floor_div", "type": "tf.compat", "text": ["Returns x // y element-wise."]}, {"name": "tf.compat.v1.foldl", "path": "compat/v1/foldl", "type": "tf.compat", "text": ["foldl on the list of tensors unpacked from elems on dimension 0.", "This foldl operator repeatedly applies the callable fn to a sequence of elements from first to last. The elements are made of the tensors unpacked from elems on dimension 0. The callable fn takes two tensors as arguments. The first argument is the accumulated value computed from the preceding invocation of fn, and the second is the value at the current position of elems. If initializer is None, elems must contain at least one element, and its first element is used as the initializer.", "Suppose that elems is unpacked into values, a list of tensors. The shape of the result tensor is fn(initializer, values[0]).shape`.", "This method also allows multi-arity elems and output of fn. If elems is a (possibly nested) list or tuple of tensors, then each of these tensors must have a matching first (unpack) dimension. The signature of fn may match the structure of elems. That is, if elems is (t1, [t2, t3, [t4, t5]]), then an appropriate signature for fn is: fn = lambda (t1, [t2, t3, [t4, t5]]):."]}, {"name": "tf.compat.v1.foldr", "path": "compat/v1/foldr", "type": "tf.compat", "text": ["foldr on the list of tensors unpacked from elems on dimension 0.", "This foldr operator repeatedly applies the callable fn to a sequence of elements from last to first. The elements are made of the tensors unpacked from elems. The callable fn takes two tensors as arguments. The first argument is the accumulated value computed from the preceding invocation of fn, and the second is the value at the current position of elems. If initializer is None, elems must contain at least one element, and its first element is used as the initializer.", "Suppose that elems is unpacked into values, a list of tensors. The shape of the result tensor is fn(initializer, values[0]).shape.", "This method also allows multi-arity elems and output of fn. If elems is a (possibly nested) list or tuple of tensors, then each of these tensors must have a matching first (unpack) dimension. The signature of fn may match the structure of elems. That is, if elems is (t1, [t2, t3, [t4, t5]]), then an appropriate signature for fn is: fn = lambda (t1, [t2, t3, [t4, t5]]):."]}, {"name": "tf.compat.v1.gather", "path": "compat/v1/gather", "type": "tf.compat", "text": [" ", "Gather slices from params axis axis according to indices.", "Gather slices from params axis axis according to indices. indices must be an integer tensor of any dimension (usually 0-D or 1-D).", "For 0-D (scalar) indices:", "Where N = ndims(params).", "For 1-D (vector) indices with batch_dims=0:", "In the general case, produces an output tensor where:", "Where N = ndims(params), M = ndims(indices), and B = batch_dims. Note that params.shape[:batch_dims] must be identical to indices.shape[:batch_dims].", "The shape of the output tensor is:", "output.shape = params.shape[:axis] + indices.shape[batch_dims:] + params.shape[axis + 1:].", "Note that on CPU, if an out of bound index is found, an error is returned. On GPU, if an out of bound index is found, a 0 is stored in the corresponding output value.", "See also tf.gather_nd."]}, {"name": "tf.compat.v1.gather_nd", "path": "compat/v1/gather_nd", "type": "tf.compat", "text": [" ", "Gather slices from params into a Tensor with shape specified by indices.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.manip.gather_nd", "indices is an K-dimensional integer tensor, best thought of as a (K-1)-dimensional tensor of indices into params, where each element defines a slice of params:", "Whereas in tf.gather indices defines slices into the first dimension of params, in tf.gather_nd, indices defines slices into the first N dimensions of params, where N = indices.shape[-1].", "The last dimension of indices can be at most the rank of params:", "The last dimension of indices corresponds to elements (if indices.shape[-1] == params.rank) or slices (if indices.shape[-1] < params.rank) along dimension indices.shape[-1] of params. The output tensor has shape", "Additionally both 'params' and 'indices' can have M leading batch dimensions that exactly match. In this case 'batch_dims' must be M.", "Note that on CPU, if an out of bound index is found, an error is returned. On GPU, if an out of bound index is found, a 0 is stored in the corresponding output value.", "Some examples below.", "Simple indexing into a matrix:", "Slice indexing into a matrix:", "Indexing into a 3-tensor:", "The examples below are for the case when only indices have leading extra dimensions. If both 'params' and 'indices' have leading batch dimensions, use the 'batch_dims' parameter to run gather_nd in batch mode.", "Batched indexing into a matrix:", "Batched slice indexing into a matrix:", "Batched indexing into a 3-tensor:", "Examples with batched 'params' and 'indices':", "See also tf.gather."]}, {"name": "tf.compat.v1.get_collection", "path": "compat/v1/get_collection", "type": "tf.compat", "text": ["Wrapper for Graph.get_collection() using the default graph.", "See tf.Graph.get_collection for more details.", "Collections are not supported when eager execution is enabled."]}, {"name": "tf.compat.v1.get_collection_ref", "path": "compat/v1/get_collection_ref", "type": "tf.compat", "text": ["Wrapper for Graph.get_collection_ref() using the default graph.", "See tf.Graph.get_collection_ref for more details.", "Collections are not supported when eager execution is enabled."]}, {"name": "tf.compat.v1.get_default_graph", "path": "compat/v1/get_default_graph", "type": "tf.compat", "text": ["Returns the default graph for the current thread.", "The returned graph will be the innermost graph on which a Graph.as_default() context has been entered, or a global default graph if none has been explicitly created."]}, {"name": "tf.compat.v1.get_default_session", "path": "compat/v1/get_default_session", "type": "tf.compat", "text": ["Returns the default session for the current thread.", "The returned Session will be the innermost session on which a Session or Session.as_default() context has been entered."]}, {"name": "tf.compat.v1.get_local_variable", "path": "compat/v1/get_local_variable", "type": "tf.compat", "text": ["Gets an existing local variable or creates a new one.", "Behavior is the same as in get_variable, except that variables are added to the LOCAL_VARIABLES collection and trainable is set to False. This function prefixes the name with the current variable scope and performs reuse checks. See the Variable Scope How To for an extensive description of how reusing works. Here is a basic example:", "If initializer is None (the default), the default initializer passed in the variable scope will be used. If that one is None too, a glorot_uniform_initializer will be used. The initializer can also be a Tensor, in which case the variable is initialized to this value and shape.", "Similarly, if the regularizer is None (the default), the default regularizer passed in the variable scope will be used (if that is None too, then by default no regularization is performed).", "If a partitioner is provided, a PartitionedVariable is returned. Accessing this object as a Tensor returns the shards concatenated along the partition axis.", "Some useful partitioners are available. See, e.g., variable_axis_size_partitioner and min_max_variable_partitioner."]}, {"name": "tf.compat.v1.get_seed", "path": "compat/v1/get_seed", "type": "tf.compat", "text": ["Returns the local seeds an operation should use given an op-specific seed.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.random.get_seed", "Given operation-specific seed, op_seed, this helper function returns two seeds derived from graph-level and op-level seeds. Many random operations internally use the two seeds to allow user to change the seed globally for a graph, or for only specific operations.", "For details on how the graph-level seed interacts with op seeds, see tf.compat.v1.random.set_random_seed."]}, {"name": "tf.compat.v1.get_session_handle", "path": "compat/v1/get_session_handle", "type": "tf.compat", "text": ["Return the handle of data.", "This is EXPERIMENTAL and subject to change.", "Keep data \"in-place\" in the runtime and create a handle that can be used to retrieve data in a subsequent run().", "Combined with get_session_tensor, we can keep a tensor produced in one run call in place, and use it as the input in a future run call."]}, {"name": "tf.compat.v1.get_session_tensor", "path": "compat/v1/get_session_tensor", "type": "tf.compat", "text": ["Get the tensor of type dtype by feeding a tensor handle.", "This is EXPERIMENTAL and subject to change.", "Get the value of the tensor from a tensor handle. The tensor is produced in a previous run() and stored in the state of the session."]}, {"name": "tf.compat.v1.get_variable", "path": "compat/v1/get_variable", "type": "tf.compat", "text": ["Gets an existing variable with these parameters or create a new one.", "This function prefixes the name with the current variable scope and performs reuse checks. See the Variable Scope How To for an extensive description of how reusing works. Here is a basic example:", "If initializer is None (the default), the default initializer passed in the variable scope will be used. If that one is None too, a glorot_uniform_initializer will be used. The initializer can also be a Tensor, in which case the variable is initialized to this value and shape.", "Similarly, if the regularizer is None (the default), the default regularizer passed in the variable scope will be used (if that is None too, then by default no regularization is performed).", "If a partitioner is provided, a PartitionedVariable is returned. Accessing this object as a Tensor returns the shards concatenated along the partition axis.", "Some useful partitioners are available. See, e.g., variable_axis_size_partitioner and min_max_variable_partitioner."]}, {"name": "tf.compat.v1.get_variable_scope", "path": "compat/v1/get_variable_scope", "type": "tf.compat", "text": ["Returns the current variable scope."]}, {"name": "tf.compat.v1.gfile", "path": "compat/v1/gfile", "type": "tf.compat", "text": ["Import router for file_io.", "class FastGFile: File I/O wrappers without thread locking.", "class GFile: File I/O wrappers without thread locking.", "class Open: File I/O wrappers without thread locking.", "Copy(...): Copies data from oldpath to newpath.", "DeleteRecursively(...): Deletes everything under dirname recursively.", "Exists(...): Determines whether a path exists or not.", "Glob(...): Returns a list of files that match the given pattern(s).", "IsDirectory(...): Returns whether the path is a directory or not.", "ListDirectory(...): Returns a list of entries contained within a directory.", "MakeDirs(...): Creates a directory and all parent/intermediate directories.", "MkDir(...): Creates a directory with the name dirname.", "Remove(...): Deletes the file located at 'filename'.", "Rename(...): Rename or move a file / directory.", "Stat(...): Returns file statistics for a given path.", "Walk(...): Recursive directory tree generator for directories."]}, {"name": "tf.compat.v1.gfile.Copy", "path": "compat/v1/gfile/copy", "type": "tf.compat", "text": ["Copies data from oldpath to newpath."]}, {"name": "tf.compat.v1.gfile.DeleteRecursively", "path": "compat/v1/gfile/deleterecursively", "type": "tf.compat", "text": ["Deletes everything under dirname recursively."]}, {"name": "tf.compat.v1.gfile.Exists", "path": "compat/v1/gfile/exists", "type": "tf.compat", "text": ["Determines whether a path exists or not."]}, {"name": "tf.compat.v1.gfile.FastGFile", "path": "compat/v1/gfile/fastgfile", "type": "tf.compat", "text": ["File I/O wrappers without thread locking.", "Note, that this is somewhat like builtin Python file I/O, but there are semantic differences to make it more efficient for some backing filesystems. For example, a write mode file will not be opened until the first write call (to minimize RPC invocations in network filesystems).", "View source", "Closes FileIO. Should be called for the WritableFile to be flushed.", "View source", "Flushes the Writable file.", "This only ensures that the data has made its way out of the process without any guarantees on whether it's written to disk. This means that the data would survive an application crash but not necessarily an OS crash.", "View source", "View source", "Returns the contents of a file as a string.", "Starts reading from current position in file.", "View source", "Reads the next line, keeping \\n. At EOF, returns ''.", "View source", "Returns all lines from the file in a list.", "View source", "Seeks to the offset in the file. (deprecated arguments)", "View source", "Returns True as FileIO supports random access ops of seek()/tell()", "View source", "Returns the size of the file.", "View source", "Returns the current position in the file.", "View source", "Writes file_content to the file. Appends to the end of the file.", "View source", "Make usable with \"with\" statement.", "View source", "Make usable with \"with\" statement.", "View source"]}, {"name": "tf.compat.v1.gfile.Glob", "path": "compat/v1/gfile/glob", "type": "tf.compat", "text": ["Returns a list of files that match the given pattern(s)."]}, {"name": "tf.compat.v1.gfile.IsDirectory", "path": "compat/v1/gfile/isdirectory", "type": "tf.compat", "text": ["Returns whether the path is a directory or not."]}, {"name": "tf.compat.v1.gfile.ListDirectory", "path": "compat/v1/gfile/listdirectory", "type": "tf.compat", "text": ["Returns a list of entries contained within a directory.", "The list is in arbitrary order. It does not contain the special entries \".\" and \"..\"."]}, {"name": "tf.compat.v1.gfile.MakeDirs", "path": "compat/v1/gfile/makedirs", "type": "tf.compat", "text": ["Creates a directory and all parent/intermediate directories.", "It succeeds if dirname already exists and is writable."]}, {"name": "tf.compat.v1.gfile.MkDir", "path": "compat/v1/gfile/mkdir", "type": "tf.compat", "text": ["Creates a directory with the name dirname.", "Notes: The parent directories need to exist. Use tf.io.gfile.makedirs instead if there is the possibility that the parent dirs don't exist."]}, {"name": "tf.compat.v1.gfile.Remove", "path": "compat/v1/gfile/remove", "type": "tf.compat", "text": ["Deletes the file located at 'filename'."]}, {"name": "tf.compat.v1.gfile.Rename", "path": "compat/v1/gfile/rename", "type": "tf.compat", "text": ["Rename or move a file / directory."]}, {"name": "tf.compat.v1.gfile.Stat", "path": "compat/v1/gfile/stat", "type": "tf.compat", "text": ["Returns file statistics for a given path."]}, {"name": "tf.compat.v1.gfile.Walk", "path": "compat/v1/gfile/walk", "type": "tf.compat", "text": ["Recursive directory tree generator for directories."]}, {"name": "tf.compat.v1.global_variables", "path": "compat/v1/global_variables", "type": "tf.compat", "text": ["Returns global variables.", "Global variables are variables that are shared across machines in a distributed environment. The Variable() constructor or get_variable() automatically adds new variables to the graph collection GraphKeys.GLOBAL_VARIABLES. This convenience function returns the contents of that collection.", "An alternative to global variables are local variables. See tf.compat.v1.local_variables"]}, {"name": "tf.compat.v1.global_variables_initializer", "path": "compat/v1/global_variables_initializer", "type": "tf.compat", "text": ["Returns an Op that initializes global variables.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.initializers.global_variables", "This is just a shortcut for variables_initializer(global_variables())"]}, {"name": "tf.compat.v1.GPUOptions", "path": "compat/v1/gpuoptions", "type": "tf.compat", "text": ["A ProtocolMessage", "class Experimental"]}, {"name": "tf.compat.v1.GPUOptions.Experimental", "path": "compat/v1/gpuoptions/experimental", "type": "tf.compat", "text": ["A ProtocolMessage", "class VirtualDevices"]}, {"name": "tf.compat.v1.GPUOptions.Experimental.VirtualDevices", "path": "compat/v1/gpuoptions/experimental/virtualdevices", "type": "tf.compat", "text": ["A ProtocolMessage"]}, {"name": "tf.compat.v1.gradients", "path": "compat/v1/gradients", "type": "tf.compat", "text": ["Constructs symbolic derivatives of sum of ys w.r.t. x in xs.", "ys and xs are each a Tensor or a list of tensors. grad_ys is a list of Tensor, holding the gradients received by the ys. The list must be the same length as ys.", "gradients() adds ops to the graph to output the derivatives of ys with respect to xs. It returns a list of Tensor of length len(xs) where each tensor is the sum(dy/dx) for y in ys and for x in xs.", "grad_ys is a list of tensors of the same length as ys that holds the initial gradients for each y in ys. When grad_ys is None, we fill in a tensor of '1's of the shape of y for each y in ys. A user can provide their own initial grad_ys to compute the derivatives using a different initial gradient for each y (e.g., if one wanted to weight the gradient differently for each value in each y).", "stop_gradients is a Tensor or a list of tensors to be considered constant with respect to all xs. These tensors will not be backpropagated through, as though they had been explicitly disconnected using stop_gradient. Among other things, this allows computation of partial derivatives as opposed to total derivatives. For example:", "Here the partial derivatives g evaluate to [1.0, 1.0], compared to the total derivatives tf.gradients(a + b, [a, b]), which take into account the influence of a on b and evaluate to [3.0, 1.0]. Note that the above is equivalent to:", "stop_gradients provides a way of stopping gradient after the graph has already been constructed, as compared to tf.stop_gradient which is used during graph construction. When the two approaches are combined, backpropagation stops at both tf.stop_gradient nodes and nodes in stop_gradients, whichever is encountered first.", "All integer tensors are considered constant with respect to all xs, as if they were included in stop_gradients.", "unconnected_gradients determines the value returned for each x in xs if it is unconnected in the graph to ys. By default this is None to safeguard against errors. Mathematically these gradients are zero which can be requested using the 'zero' option. tf.UnconnectedGradients provides the following options and behaviors:", "Let us take one practical example which comes during the back propogation phase. This function is used to evaluate the derivatives of the cost function with respect to Weights Ws and Biases bs. Below sample implementation provides the exaplantion of what it is actually used for :"]}, {"name": "tf.compat.v1.GraphDef", "path": "compat/v1/graphdef", "type": "tf.compat", "text": ["A ProtocolMessage"]}, {"name": "tf.compat.v1.GraphKeys", "path": "compat/v1/graphkeys", "type": "tf.compat", "text": ["Standard names to use for graph collections.", "The standard library uses various well-known names to collect and retrieve values associated with a graph. For example, the tf.Optimizer subclasses default to optimizing the variables collected under tf.GraphKeys.TRAINABLE_VARIABLES if none is specified, but it is also possible to pass an explicit list of variables.", "The following standard keys are defined:", "The following standard keys are defined, but their collections are not automatically populated as many of the others are:"]}, {"name": "tf.compat.v1.GraphOptions", "path": "compat/v1/graphoptions", "type": "tf.compat", "text": ["A ProtocolMessage"]}, {"name": "tf.compat.v1.graph_util", "path": "compat/v1/graph_util", "type": "tf.compat", "text": ["Helpers to manipulate a tensor graph in python.", "convert_variables_to_constants(...): Replaces all the variables in a graph with constants of the same values. (deprecated)", "extract_sub_graph(...): Extract the subgraph that can reach any of the nodes in 'dest_nodes'. (deprecated)", "import_graph_def(...): Imports the graph from graph_def into the current default Graph. (deprecated arguments)", "must_run_on_cpu(...): Returns True if the given node_def must run on CPU, otherwise False. (deprecated)", "remove_training_nodes(...): Prunes out nodes that aren't needed for inference. (deprecated)", "tensor_shape_from_node_def_name(...): Convenience function to get a shape from a NodeDef's input string. (deprecated)"]}, {"name": "tf.compat.v1.graph_util.convert_variables_to_constants", "path": "compat/v1/graph_util/convert_variables_to_constants", "type": "tf.compat", "text": ["Replaces all the variables in a graph with constants of the same values. (deprecated)", "If you have a trained graph containing Variable ops, it can be convenient to convert them all to Const ops holding the same values. This makes it possible to describe the network fully with a single GraphDef file, and allows the removal of a lot of ops related to loading and saving the variables."]}, {"name": "tf.compat.v1.graph_util.extract_sub_graph", "path": "compat/v1/graph_util/extract_sub_graph", "type": "tf.compat", "text": ["Extract the subgraph that can reach any of the nodes in 'dest_nodes'. (deprecated)"]}, {"name": "tf.compat.v1.graph_util.must_run_on_cpu", "path": "compat/v1/graph_util/must_run_on_cpu", "type": "tf.compat", "text": ["Returns True if the given node_def must run on CPU, otherwise False. (deprecated)"]}, {"name": "tf.compat.v1.graph_util.remove_training_nodes", "path": "compat/v1/graph_util/remove_training_nodes", "type": "tf.compat", "text": ["Prunes out nodes that aren't needed for inference. (deprecated)", "There are nodes like Identity and CheckNumerics that are only useful during training, and can be removed in graphs that will be used for nothing but inference. Here we identify and remove them, returning an equivalent graph. To be specific, CheckNumerics nodes are always removed, and Identity nodes that aren't involved in control edges are spliced out so that their input and outputs are directly connected."]}, {"name": "tf.compat.v1.graph_util.tensor_shape_from_node_def_name", "path": "compat/v1/graph_util/tensor_shape_from_node_def_name", "type": "tf.compat", "text": ["Convenience function to get a shape from a NodeDef's input string. (deprecated)"]}, {"name": "tf.compat.v1.hessians", "path": "compat/v1/hessians", "type": "tf.compat", "text": ["Constructs the Hessian of sum of ys with respect to x in xs.", "hessians() adds ops to the graph to output the Hessian matrix of ys with respect to xs. It returns a list of Tensor of length len(xs) where each tensor is the Hessian of sum(ys).", "The Hessian is a matrix of second-order partial derivatives of a scalar tensor (see https://en.wikipedia.org/wiki/Hessian_matrix for more details)."]}, {"name": "tf.compat.v1.HistogramProto", "path": "compat/v1/histogramproto", "type": "tf.compat", "text": ["A ProtocolMessage"]}, {"name": "tf.compat.v1.IdentityReader", "path": "compat/v1/identityreader", "type": "tf.compat", "text": ["A Reader that outputs the queued work as both the key and value.", "Inherits From: ReaderBase", "To use, enqueue strings in a Queue. Read will take the front work string and output (work, work).", "See ReaderBase for supported methods.", "Readers are not compatible with eager execution. Instead, please use tf.data to get data into your model.", "View source", "Returns the number of records this reader has produced.", "This is the same as the number of Read executions that have succeeded.", "View source", "Returns the number of work units this reader has finished processing.", "View source", "Returns the next record (key, value) pair produced by a reader.", "Will dequeue a work unit from queue if necessary (e.g. when the Reader needs to start reading from a new file since it has finished with the previous file).", "View source", "Returns up to num_records (key, value) pairs produced by a reader.", "Will dequeue a work unit from queue if necessary (e.g., when the Reader needs to start reading from a new file since it has finished with the previous file). It may return less than num_records even before the last batch.", "View source", "Restore a reader to its initial clean state.", "View source", "Restore a reader to a previously saved state.", "Not all Readers support being restored, so this can produce an Unimplemented error.", "View source", "Produce a string tensor that encodes the state of a reader.", "Not all Readers support being serialized, so this can produce an Unimplemented error."]}, {"name": "tf.compat.v1.image", "path": "compat/v1/image", "type": "tf.compat", "text": ["Image ops.", "The tf.image module contains various functions for image processing and decoding-encoding Ops.", "Many of the encoding/decoding functions are also available in the core tf.io module.", "The resizing Ops accept input images as tensors of several types. They always output resized images as float32 tensors.", "The convenience function tf.image.resize supports both 4-D and 3-D tensors as input and output. 4-D tensors are for batches of images, 3-D tensors for individual images.", "Resized images will be distorted if their original aspect ratio is not the same as size. To avoid distortions see tf.image.resize_with_pad.", "The Class tf.image.ResizeMethod provides various resize methods like bilinear, nearest_neighbor.", "Image ops work either on individual images or on batches of images, depending on the shape of their input Tensor.", "If 3-D, the shape is [height, width, channels], and the Tensor represents one image. If 4-D, the shape is [batch_size, height, width, channels], and the Tensor represents batch_size images.", "Currently, channels can usefully be 1, 2, 3, or 4. Single-channel images are grayscale, images with 3 channels are encoded as either RGB or HSV. Images with 2 or 4 channels include an alpha channel, which has to be stripped from the image before passing the image to most image processing functions (and can be re-attached later).", "Internally, images are either stored in as one float32 per channel per pixel (implicitly, values are assumed to lie in [0,1)) or one uint8 per channel per pixel (values are assumed to lie in [0,255]).", "TensorFlow can convert between images in RGB or HSV or YIQ.", "TensorFlow provides functions to adjust images in various ways: brightness, contrast, hue, and saturation. Each adjustment can be done with predefined parameters or with random parameters picked from predefined intervals. Random adjustments are often useful to expand a training set and reduce overfitting.", "If several adjustments are chained it is advisable to minimize the number of redundant conversions by first converting the images to the most natural data type and representation.", "TensorFlow provides Ops to decode and encode JPEG and PNG formats. Encoded images are represented by scalar string Tensors, decoded images by 3-D uint8 tensors of shape [height, width, channels]. (PNG also supports uint16.)", "The encode and decode Ops apply to one image at a time. Their input and output are all of variable size. If you need fixed size images, pass the output of the decode Ops to one of the cropping and resizing Ops.", "class ResizeMethod: See v1.image.resize for details.", "adjust_brightness(...): Adjust the brightness of RGB or Grayscale images.", "adjust_contrast(...): Adjust contrast of RGB or grayscale images.", "adjust_gamma(...): Performs Gamma Correction.", "adjust_hue(...): Adjust hue of RGB images.", "adjust_jpeg_quality(...): Adjust jpeg encoding quality of an image.", "adjust_saturation(...): Adjust saturation of RGB images.", "central_crop(...): Crop the central region of the image(s).", "combined_non_max_suppression(...): Greedily selects a subset of bounding boxes in descending order of score.", "convert_image_dtype(...): Convert image to dtype, scaling its values if needed.", "crop_and_resize(...): Extracts crops from the input image tensor and resizes them.", "crop_to_bounding_box(...): Crops an image to a specified bounding box.", "decode_and_crop_jpeg(...): Decode and Crop a JPEG-encoded image to a uint8 tensor.", "decode_bmp(...): Decode the first frame of a BMP-encoded image to a uint8 tensor.", "decode_gif(...): Decode the frame(s) of a GIF-encoded image to a uint8 tensor.", "decode_image(...): Function for decode_bmp, decode_gif, decode_jpeg, and decode_png.", "decode_jpeg(...): Decode a JPEG-encoded image to a uint8 tensor.", "decode_png(...): Decode a PNG-encoded image to a uint8 or uint16 tensor.", "draw_bounding_boxes(...): Draw bounding boxes on a batch of images.", "encode_jpeg(...): JPEG-encode an image.", "encode_png(...): PNG-encode an image.", "extract_glimpse(...): Extracts a glimpse from the input tensor.", "extract_image_patches(...): Extract patches from images and put them in the \"depth\" output dimension.", "extract_jpeg_shape(...): Extract the shape information of a JPEG-encoded image.", "extract_patches(...): Extract patches from images.", "flip_left_right(...): Flip an image horizontally (left to right).", "flip_up_down(...): Flip an image vertically (upside down).", "generate_bounding_box_proposals(...): Generate bounding box proposals from encoded bounding boxes.", "grayscale_to_rgb(...): Converts one or more images from Grayscale to RGB.", "hsv_to_rgb(...): Convert one or more images from HSV to RGB.", "image_gradients(...): Returns image gradients (dy, dx) for each color channel.", "is_jpeg(...): Convenience function to check if the 'contents' encodes a JPEG image.", "non_max_suppression(...): Greedily selects a subset of bounding boxes in descending order of score.", "non_max_suppression_overlaps(...): Greedily selects a subset of bounding boxes in descending order of score.", "non_max_suppression_padded(...): Greedily selects a subset of bounding boxes in descending order of score.", "non_max_suppression_with_scores(...): Greedily selects a subset of bounding boxes in descending order of score.", "pad_to_bounding_box(...): Pad image with zeros to the specified height and width.", "per_image_standardization(...): Linearly scales each image in image to have mean 0 and variance 1.", "psnr(...): Returns the Peak Signal-to-Noise Ratio between a and b.", "random_brightness(...): Adjust the brightness of images by a random factor.", "random_contrast(...): Adjust the contrast of an image or images by a random factor.", "random_crop(...): Randomly crops a tensor to a given size.", "random_flip_left_right(...): Randomly flip an image horizontally (left to right).", "random_flip_up_down(...): Randomly flips an image vertically (upside down).", "random_hue(...): Adjust the hue of RGB images by a random factor.", "random_jpeg_quality(...): Randomly changes jpeg encoding quality for inducing jpeg noise.", "random_saturation(...): Adjust the saturation of RGB images by a random factor.", "resize(...): Resize images to size using the specified method.", "resize_area(...): Resize images to size using area interpolation.", "resize_bicubic(...)", "resize_bilinear(...)", "resize_image_with_crop_or_pad(...): Crops and/or pads an image to a target width and height.", "resize_image_with_pad(...): Resizes and pads an image to a target width and height.", "resize_images(...): Resize images to size using the specified method.", "resize_nearest_neighbor(...)", "resize_with_crop_or_pad(...): Crops and/or pads an image to a target width and height.", "rgb_to_grayscale(...): Converts one or more images from RGB to Grayscale.", "rgb_to_hsv(...): Converts one or more images from RGB to HSV.", "rgb_to_yiq(...): Converts one or more images from RGB to YIQ.", "rgb_to_yuv(...): Converts one or more images from RGB to YUV.", "rot90(...): Rotate image(s) counter-clockwise by 90 degrees.", "sample_distorted_bounding_box(...): Generate a single randomly distorted bounding box for an image. (deprecated)", "sobel_edges(...): Returns a tensor holding Sobel edge maps.", "ssim(...): Computes SSIM index between img1 and img2.", "ssim_multiscale(...): Computes the MS-SSIM between img1 and img2.", "total_variation(...): Calculate and return the total variation for one or more images.", "transpose(...): Transpose image(s) by swapping the height and width dimension.", "transpose_image(...): Transpose image(s) by swapping the height and width dimension.", "yiq_to_rgb(...): Converts one or more images from YIQ to RGB.", "yuv_to_rgb(...): Converts one or more images from YUV to RGB."]}, {"name": "tf.compat.v1.image.crop_and_resize", "path": "compat/v1/image/crop_and_resize", "type": "tf.compat", "text": ["Extracts crops from the input image tensor and resizes them.", "Extracts crops from the input image tensor and resizes them using bilinear sampling or nearest neighbor sampling (possibly with aspect ratio change) to a common output size specified by crop_size. This is more general than the crop_to_bounding_box op which extracts a fixed size slice from the input image and does not allow resizing or aspect ratio change.", "Returns a tensor with crops from the input image at positions defined at the bounding box locations in boxes. The cropped boxes are all resized (with bilinear or nearest neighbor interpolation) to a fixed size = [crop_height, crop_width]. The result is a 4-D tensor [num_boxes, crop_height, crop_width, depth]. The resizing is corner aligned. In particular, if boxes = [[0, 0, 1, 1]], the method will give identical results to using tf.image.resize_bilinear() or tf.image.resize_nearest_neighbor()(depends on the method argument) with align_corners=True."]}, {"name": "tf.compat.v1.image.draw_bounding_boxes", "path": "compat/v1/image/draw_bounding_boxes", "type": "tf.compat", "text": ["Draw bounding boxes on a batch of images.", "Outputs a copy of images but draws on top of the pixels zero or more bounding boxes specified by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates are floats in [0.0, 1.0] relative to the width and the height of the underlying image.", "For example, if an image is 100 x 200 pixels (height x width) and the bounding box is [0.1, 0.2, 0.5, 0.9], the upper-left and bottom-right coordinates of the bounding box will be (40, 10) to (180, 50) (in (x,y) coordinates).", "Parts of the bounding box may fall outside the image."]}, {"name": "tf.compat.v1.image.extract_glimpse", "path": "compat/v1/image/extract_glimpse", "type": "tf.compat", "text": ["Extracts a glimpse from the input tensor.", "Returns a set of windows called glimpses extracted at location offsets from the input tensor. If the windows only partially overlaps the inputs, the non-overlapping areas will be filled with random noise.", "The result is a 4-D tensor of shape [batch_size, glimpse_height, glimpse_width, channels]. The channels and batch dimensions are the same as that of the input tensor. The height and width of the output windows are specified in the size parameter.", "The argument normalized and centered controls how the windows are built:"]}, {"name": "tf.compat.v1.image.resize", "path": "compat/v1/image/resize", "type": "tf.compat", "text": ["Resize images to size using the specified method.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.image.resize_images", "Resized images will be distorted if their original aspect ratio is not the same as size. To avoid distortions see tf.image.resize_with_pad or tf.image.resize_with_crop_or_pad.", "The method can be one of:", "The return value has the same type as images if method is tf.image.ResizeMethod.NEAREST_NEIGHBOR. It will also have the same type as images if the size of images can be statically determined to be the same as size, because images is returned in this case. Otherwise, the return value has type float32."]}, {"name": "tf.compat.v1.image.ResizeMethod", "path": "compat/v1/image/resizemethod", "type": "tf.compat", "text": ["See v1.image.resize for details."]}, {"name": "tf.compat.v1.image.resize_area", "path": "compat/v1/image/resize_area", "type": "tf.compat", "text": ["Resize images to size using area interpolation.", "Input images can be of different types but output images are always float.", "The range of pixel values for the output image might be slightly different from the range for the input image because of limited numerical precision. To guarantee an output range, for example [0.0, 1.0], apply tf.clip_by_value to the output.", "Each output pixel is computed by first transforming the pixel's footprint into the input tensor and then averaging the pixels that intersect the footprint. An input pixel's contribution to the average is weighted by the fraction of its area that intersects the footprint. This is the same as OpenCV's INTER_AREA."]}, {"name": "tf.compat.v1.image.resize_bicubic", "path": "compat/v1/image/resize_bicubic", "type": "tf.compat", "text": []}, {"name": "tf.compat.v1.image.resize_bilinear", "path": "compat/v1/image/resize_bilinear", "type": "tf.compat", "text": []}, {"name": "tf.compat.v1.image.resize_image_with_pad", "path": "compat/v1/image/resize_image_with_pad", "type": "tf.compat", "text": ["Resizes and pads an image to a target width and height.", "Resizes an image to a target width and height by keeping the aspect ratio the same without distortion. If the target dimensions don't match the image dimensions, the image is resized and then padded with zeroes to match requested dimensions."]}, {"name": "tf.compat.v1.image.resize_nearest_neighbor", "path": "compat/v1/image/resize_nearest_neighbor", "type": "tf.compat", "text": []}, {"name": "tf.compat.v1.image.sample_distorted_bounding_box", "path": "compat/v1/image/sample_distorted_bounding_box", "type": "tf.compat", "text": ["Generate a single randomly distorted bounding box for an image. (deprecated)", "Bounding box annotations are often supplied in addition to ground-truth labels in image recognition or object localization tasks. A common technique for training such a system is to randomly distort an image while preserving its content, i.e. data augmentation. This Op outputs a randomly distorted localization of an object, i.e. bounding box, given an image_size, bounding_boxes and a series of constraints.", "The output of this Op is a single bounding box that may be used to crop the original image. The output is returned as 3 tensors: begin, size and bboxes. The first 2 tensors can be fed directly into tf.slice to crop the image. The latter may be supplied to tf.image.draw_bounding_boxes to visualize what the bounding box looks like.", "Bounding boxes are supplied and returned as [y_min, x_min, y_max, x_max]. The bounding box coordinates are floats in [0.0, 1.0] relative to the width and height of the underlying image.", "For example,", "Note that if no bounding box information is available, setting use_image_if_no_bounding_boxes = True will assume there is a single implicit bounding box covering the whole image. If use_image_if_no_bounding_boxes is false and no bounding boxes are supplied, an error is raised."]}, {"name": "tf.compat.v1.initializers", "path": "compat/v1/initializers", "type": "tf.compat", "text": ["Public API for tf.initializers namespace.", "class constant: Initializer that generates tensors with constant values.", "class glorot_normal: The Glorot normal initializer, also called Xavier normal initializer.", "class glorot_uniform: The Glorot uniform initializer, also called Xavier uniform initializer.", "class identity: Initializer that generates the identity matrix.", "class ones: Initializer that generates tensors initialized to 1.", "class orthogonal: Initializer that generates an orthogonal matrix.", "class random_normal: Initializer that generates tensors with a normal distribution.", "class random_uniform: Initializer that generates tensors with a uniform distribution.", "class truncated_normal: Initializer that generates a truncated normal distribution.", "class uniform_unit_scaling: Initializer that generates tensors without scaling variance.", "class variance_scaling: Initializer capable of adapting its scale to the shape of weights tensors.", "class zeros: Initializer that generates tensors initialized to 0.", "global_variables(...): Returns an Op that initializes global variables.", "he_normal(...): He normal initializer.", "he_uniform(...): He uniform variance scaling initializer.", "lecun_normal(...): LeCun normal initializer.", "lecun_uniform(...): LeCun uniform initializer.", "local_variables(...): Returns an Op that initializes all local variables.", "tables_initializer(...): Returns an Op that initializes all tables of the default graph.", "variables(...): Returns an Op that initializes a list of variables."]}, {"name": "tf.compat.v1.initializers.he_normal", "path": "compat/v1/initializers/he_normal", "type": "tf.compat", "text": ["He normal initializer.", "It draws samples from a truncated normal distribution centered on 0 with standard deviation (after truncation) given by stddev = sqrt(2 / fan_in) where fan_in is the number of input units in the weight tensor.", "He et al., 2015", "(pdf)"]}, {"name": "tf.compat.v1.initializers.he_uniform", "path": "compat/v1/initializers/he_uniform", "type": "tf.compat", "text": ["He uniform variance scaling initializer.", "It draws samples from a uniform distribution within [-limit, limit] where limit is sqrt(6 / fan_in) where fan_in is the number of input units in the weight tensor.", "He et al., 2015", "(pdf)"]}, {"name": "tf.compat.v1.initializers.lecun_normal", "path": "compat/v1/initializers/lecun_normal", "type": "tf.compat", "text": ["LeCun normal initializer.", "It draws samples from a truncated normal distribution centered on 0 with standard deviation (after truncation) given by stddev = sqrt(1 / fan_in) where fan_in is the number of input units in the weight tensor.", "(pdf)"]}, {"name": "tf.compat.v1.initializers.lecun_uniform", "path": "compat/v1/initializers/lecun_uniform", "type": "tf.compat", "text": ["LeCun uniform initializer.", "It draws samples from a uniform distribution within [-limit, limit] where limit is sqrt(3 / fan_in) where fan_in is the number of input units in the weight tensor.", "(pdf)"]}, {"name": "tf.compat.v1.initialize_all_tables", "path": "compat/v1/initialize_all_tables", "type": "tf.compat", "text": ["Returns an Op that initializes all tables of the default graph. (deprecated)"]}, {"name": "tf.compat.v1.initialize_all_variables", "path": "compat/v1/initialize_all_variables", "type": "tf.compat", "text": ["See tf.compat.v1.global_variables_initializer. (deprecated)"]}, {"name": "tf.compat.v1.initialize_local_variables", "path": "compat/v1/initialize_local_variables", "type": "tf.compat", "text": ["See tf.compat.v1.local_variables_initializer. (deprecated)"]}, {"name": "tf.compat.v1.initialize_variables", "path": "compat/v1/initialize_variables", "type": "tf.compat", "text": ["See tf.compat.v1.variables_initializer. (deprecated)"]}, {"name": "tf.compat.v1.InteractiveSession", "path": "compat/v1/interactivesession", "type": "tf.compat", "text": ["A TensorFlow Session for use in interactive contexts, such as a shell.", "The only difference with a regular Session is that an InteractiveSession installs itself as the default session on construction. The methods tf.Tensor.eval and tf.Operation.run will use that session to run ops.", "This is convenient in interactive shells and IPython notebooks, as it avoids having to pass an explicit Session object to run ops.", "Note that a regular session installs itself as the default session when it is created in a with statement. The common usage in non-interactive programs is to follow that pattern:", "View source", "Returns a context manager that makes this object the default session.", "Use with the with keyword to specify that calls to tf.Operation.run or tf.Tensor.eval should be executed in this session.", "To get the current default session, use tf.compat.v1.get_default_session.", "Alternatively, you can use with tf.compat.v1.Session(): to create a session that is automatically closed on exiting the context, including when an uncaught exception is raised.", "View source", "Closes an InteractiveSession.", "View source", "Lists available devices in this session.", "Each element in the list has the following properties", "View source", "Returns a Python callable that runs a particular step.", "The returned callable will take len(feed_list) arguments whose types must be compatible feed values for the respective elements of feed_list. For example, if element i of feed_list is a tf.Tensor, the ith argument to the returned callable must be a numpy ndarray (or something convertible to an ndarray) with matching element type and shape. See tf.Session.run for details of the allowable feed key and value types.", "The returned callable will have the same return type as tf.Session.run(fetches, ...). For example, if fetches is a tf.Tensor, the callable will return a numpy ndarray; if fetches is a tf.Operation, it will return None.", "View source", "Continues the execution with more feeds and fetches.", "This is EXPERIMENTAL and subject to change.", "To use partial execution, a user first calls partial_run_setup() and then a sequence of partial_run(). partial_run_setup specifies the list of feeds and fetches that will be used in the subsequent partial_run calls.", "The optional feed_dict argument allows the caller to override the value of tensors in the graph. See run() for more information.", "Below is a simple example:", "View source", "Sets up a graph with feeds and fetches for partial run.", "This is EXPERIMENTAL and subject to change.", "Note that contrary to run, feeds only specifies the graph elements. The tensors will be supplied by the subsequent partial_run calls.", "View source", "Runs operations and evaluates tensors in fetches.", "This method runs one \"step\" of TensorFlow computation, by running the necessary graph fragment to execute every Operation and evaluate every Tensor in fetches, substituting the values in feed_dict for the corresponding input values.", "The fetches argument may be a single graph element, or an arbitrarily nested list, tuple, namedtuple, dict, or OrderedDict containing graph elements at its leaves. A graph element can be one of the following types:", "The value returned by run() has the same shape as the fetches argument, where the leaves are replaced by the corresponding values returned by TensorFlow.", "The optional feed_dict argument allows the caller to override the value of tensors in the graph. Each key in feed_dict can be one of the following types:", "Each value in feed_dict must be convertible to a numpy array of the dtype of the corresponding key.", "The optional options argument expects a [RunOptions] proto. The options allow controlling the behavior of this particular step (e.g. turning tracing on).", "The optional run_metadata argument expects a [RunMetadata] proto. When appropriate, the non-Tensor output of this step will be collected there. For example, when users turn on tracing in options, the profiled info will be collected into this argument and passed back."]}, {"name": "tf.compat.v1.io", "path": "compat/v1/io", "type": "tf.compat", "text": ["Public API for tf.io namespace.", "gfile module: Public API for tf.io.gfile namespace.", "class FixedLenFeature: Configuration for parsing a fixed-length input feature.", "class FixedLenSequenceFeature: Configuration for parsing a variable-length input feature into a Tensor.", "class PaddingFIFOQueue: A FIFOQueue that supports batching variable-sized tensors by padding.", "class PriorityQueue: A queue implementation that dequeues elements in prioritized order.", "class QueueBase: Base class for queue implementations.", "class RaggedFeature: Configuration for passing a RaggedTensor input feature.", "class RandomShuffleQueue: A queue implementation that dequeues elements in a random order.", "class SparseFeature: Configuration for parsing a sparse input feature from an Example.", "class TFRecordCompressionType: The type of compression for the record.", "class TFRecordOptions: Options used for manipulating TFRecord files.", "class TFRecordWriter: A class to write records to a TFRecords file.", "class VarLenFeature: Configuration for parsing a variable-length input feature.", "decode_and_crop_jpeg(...): Decode and Crop a JPEG-encoded image to a uint8 tensor.", "decode_base64(...): Decode web-safe base64-encoded strings.", "decode_bmp(...): Decode the first frame of a BMP-encoded image to a uint8 tensor.", "decode_compressed(...): Decompress strings.", "decode_csv(...): Convert CSV records to tensors. Each column maps to one tensor.", "decode_gif(...): Decode the frame(s) of a GIF-encoded image to a uint8 tensor.", "decode_image(...): Function for decode_bmp, decode_gif, decode_jpeg, and decode_png.", "decode_jpeg(...): Decode a JPEG-encoded image to a uint8 tensor.", "decode_json_example(...): Convert JSON-encoded Example records to binary protocol buffer strings.", "decode_png(...): Decode a PNG-encoded image to a uint8 or uint16 tensor.", "decode_proto(...): The op extracts fields from a serialized protocol buffers message into tensors.", "decode_raw(...): Convert raw byte strings into tensors. (deprecated arguments)", "deserialize_many_sparse(...): Deserialize and concatenate SparseTensors from a serialized minibatch.", "encode_base64(...): Encode strings into web-safe base64 format.", "encode_jpeg(...): JPEG-encode an image.", "encode_png(...): PNG-encode an image.", "encode_proto(...): The op serializes protobuf messages provided in the input tensors.", "extract_jpeg_shape(...): Extract the shape information of a JPEG-encoded image.", "is_jpeg(...): Convenience function to check if the 'contents' encodes a JPEG image.", "match_filenames_once(...): Save the list of files matching pattern, so it is only computed once.", "matching_files(...): Returns the set of files matching one or more glob patterns.", "parse_example(...): Parses Example protos into a dict of tensors.", "parse_sequence_example(...): Parses a batch of SequenceExample protos.", "parse_single_example(...): Parses a single Example proto.", "parse_single_sequence_example(...): Parses a single SequenceExample proto.", "parse_tensor(...): Transforms a serialized tensorflow.TensorProto proto into a Tensor.", "read_file(...): Reads and outputs the entire contents of the input filename.", "serialize_many_sparse(...): Serialize N-minibatch SparseTensor into an [N, 3] Tensor.", "serialize_sparse(...): Serialize a SparseTensor into a 3-vector (1-D Tensor) object.", "serialize_tensor(...): Transforms a Tensor into a serialized TensorProto proto.", "tf_record_iterator(...): An iterator that read the records from a TFRecords file. (deprecated)", "write_file(...): Writes contents to the file at input filename. Creates file and recursively", "write_graph(...): Writes a graph proto to a file."]}, {"name": "tf.compat.v1.io.gfile", "path": "compat/v1/io/gfile", "type": "tf.compat", "text": ["Public API for tf.io.gfile namespace.", "class GFile: File I/O wrappers without thread locking.", "copy(...): Copies data from src to dst.", "exists(...): Determines whether a path exists or not.", "glob(...): Returns a list of files that match the given pattern(s).", "isdir(...): Returns whether the path is a directory or not.", "listdir(...): Returns a list of entries contained within a directory.", "makedirs(...): Creates a directory and all parent/intermediate directories.", "mkdir(...): Creates a directory with the name given by path.", "remove(...): Deletes the path located at 'path'.", "rename(...): Rename or move a file / directory.", "rmtree(...): Deletes everything under path recursively.", "stat(...): Returns file statistics for a given path.", "walk(...): Recursive directory tree generator for directories."]}, {"name": "tf.compat.v1.io.TFRecordCompressionType", "path": "compat/v1/io/tfrecordcompressiontype", "type": "tf.compat", "text": ["The type of compression for the record.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.python_io.TFRecordCompressionType"]}, {"name": "tf.compat.v1.io.tf_record_iterator", "path": "compat/v1/io/tf_record_iterator", "type": "tf.compat", "text": ["An iterator that read the records from a TFRecords file. (deprecated)", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.python_io.tf_record_iterator"]}, {"name": "tf.compat.v1.is_variable_initialized", "path": "compat/v1/is_variable_initialized", "type": "tf.compat", "text": ["Tests if a variable has been initialized."]}, {"name": "tf.compat.v1.keras", "path": "compat/v1/keras", "type": "tf.compat", "text": ["Implementation of the Keras API meant to be a high-level API for TensorFlow.", "Detailed documentation and user guides are available at tensorflow.org.", "activations module: Built-in activation functions.", "applications module: Keras Applications are canned architectures with pre-trained weights.", "backend module: Keras backend API.", "callbacks module: Callbacks: utilities called at certain points during model training.", "constraints module: Constraints: functions that impose constraints on weight values.", "datasets module: Public API for tf.keras.datasets namespace.", "estimator module: Keras estimator API.", "experimental module: Public API for tf.keras.experimental namespace.", "initializers module: Keras initializer serialization / deserialization.", "layers module: Keras layers API.", "losses module: Built-in loss functions.", "metrics module: Built-in metrics.", "mixed_precision module: Keras mixed precision API.", "models module: Code for model cloning, plus model-related API entries.", "optimizers module: Built-in optimizer classes.", "preprocessing module: Keras data preprocessing utils.", "regularizers module: Built-in regularizers.", "utils module: Public API for tf.keras.utils namespace.", "wrappers module: Public API for tf.keras.wrappers namespace.", "class Model: Model groups layers into an object with training and inference features.", "class Sequential: Sequential groups a linear stack of layers into a tf.keras.Model.", "Input(...): Input() is used to instantiate a Keras tensor."]}, {"name": "tf.compat.v1.keras.activations", "path": "compat/v1/keras/activations", "type": "tf.compat", "text": ["Built-in activation functions.", "deserialize(...): Returns activation function given a string identifier.", "elu(...): Exponential Linear Unit.", "exponential(...): Exponential activation function.", "get(...): Returns function.", "hard_sigmoid(...): Hard sigmoid activation function.", "linear(...): Linear activation function (pass-through).", "relu(...): Applies the rectified linear unit activation function.", "selu(...): Scaled Exponential Linear Unit (SELU).", "serialize(...): Returns the string identifier of an activation function.", "sigmoid(...): Sigmoid activation function, sigmoid(x) = 1 / (1 + exp(-x)).", "softmax(...): Softmax converts a real vector to a vector of categorical probabilities.", "softplus(...): Softplus activation function, softplus(x) = log(exp(x) + 1).", "softsign(...): Softsign activation function, softsign(x) = x / (abs(x) + 1).", "swish(...): Swish activation function, swish(x) = x * sigmoid(x).", "tanh(...): Hyperbolic tangent activation function."]}, {"name": "tf.compat.v1.keras.applications", "path": "compat/v1/keras/applications", "type": "tf.compat", "text": ["Keras Applications are canned architectures with pre-trained weights.", "densenet module: DenseNet models for Keras.", "efficientnet module: EfficientNet models for Keras.", "imagenet_utils module: Utilities for ImageNet data preprocessing & prediction decoding.", "inception_resnet_v2 module: Inception-ResNet V2 model for Keras.", "inception_v3 module: Inception V3 model for Keras.", "mobilenet module: MobileNet v1 models for Keras.", "mobilenet_v2 module: MobileNet v2 models for Keras.", "mobilenet_v3 module: MobileNet v3 models for Keras.", "nasnet module: NASNet-A models for Keras.", "resnet module: ResNet models for Keras.", "resnet50 module: Public API for tf.keras.applications.resnet50 namespace.", "resnet_v2 module: ResNet v2 models for Keras.", "vgg16 module: VGG16 model for Keras.", "vgg19 module: VGG19 model for Keras.", "xception module: Xception V1 model for Keras.", "DenseNet121(...): Instantiates the Densenet121 architecture.", "DenseNet169(...): Instantiates the Densenet169 architecture.", "DenseNet201(...): Instantiates the Densenet201 architecture.", "EfficientNetB0(...): Instantiates the EfficientNetB0 architecture.", "EfficientNetB1(...): Instantiates the EfficientNetB1 architecture.", "EfficientNetB2(...): Instantiates the EfficientNetB2 architecture.", "EfficientNetB3(...): Instantiates the EfficientNetB3 architecture.", "EfficientNetB4(...): Instantiates the EfficientNetB4 architecture.", "EfficientNetB5(...): Instantiates the EfficientNetB5 architecture.", "EfficientNetB6(...): Instantiates the EfficientNetB6 architecture.", "EfficientNetB7(...): Instantiates the EfficientNetB7 architecture.", "InceptionResNetV2(...): Instantiates the Inception-ResNet v2 architecture.", "InceptionV3(...): Instantiates the Inception v3 architecture.", "MobileNet(...): Instantiates the MobileNet architecture.", "MobileNetV2(...): Instantiates the MobileNetV2 architecture.", "MobileNetV3Large(...): Instantiates the MobileNetV3Large architecture.", "MobileNetV3Small(...): Instantiates the MobileNetV3Small architecture.", "NASNetLarge(...): Instantiates a NASNet model in ImageNet mode.", "NASNetMobile(...): Instantiates a Mobile NASNet model in ImageNet mode.", "ResNet101(...): Instantiates the ResNet101 architecture.", "ResNet101V2(...): Instantiates the ResNet101V2 architecture.", "ResNet152(...): Instantiates the ResNet152 architecture.", "ResNet152V2(...): Instantiates the ResNet152V2 architecture.", "ResNet50(...): Instantiates the ResNet50 architecture.", "ResNet50V2(...): Instantiates the ResNet50V2 architecture.", "VGG16(...): Instantiates the VGG16 model.", "VGG19(...): Instantiates the VGG19 architecture.", "Xception(...): Instantiates the Xception architecture."]}, {"name": "tf.compat.v1.keras.applications.densenet", "path": "compat/v1/keras/applications/densenet", "type": "tf.compat", "text": ["DenseNet models for Keras.", "DenseNet121(...): Instantiates the Densenet121 architecture.", "DenseNet169(...): Instantiates the Densenet169 architecture.", "DenseNet201(...): Instantiates the Densenet201 architecture.", "decode_predictions(...): Decodes the prediction of an ImageNet model.", "preprocess_input(...): Preprocesses a tensor or Numpy array encoding a batch of images."]}, {"name": "tf.compat.v1.keras.applications.efficientnet", "path": "compat/v1/keras/applications/efficientnet", "type": "tf.compat", "text": ["EfficientNet models for Keras.", "EfficientNetB0(...): Instantiates the EfficientNetB0 architecture.", "EfficientNetB1(...): Instantiates the EfficientNetB1 architecture.", "EfficientNetB2(...): Instantiates the EfficientNetB2 architecture.", "EfficientNetB3(...): Instantiates the EfficientNetB3 architecture.", "EfficientNetB4(...): Instantiates the EfficientNetB4 architecture.", "EfficientNetB5(...): Instantiates the EfficientNetB5 architecture.", "EfficientNetB6(...): Instantiates the EfficientNetB6 architecture.", "EfficientNetB7(...): Instantiates the EfficientNetB7 architecture.", "decode_predictions(...): Decodes the prediction of an ImageNet model.", "preprocess_input(...)"]}, {"name": "tf.compat.v1.keras.applications.imagenet_utils", "path": "compat/v1/keras/applications/imagenet_utils", "type": "tf.compat", "text": ["Utilities for ImageNet data preprocessing & prediction decoding.", "decode_predictions(...): Decodes the prediction of an ImageNet model.", "preprocess_input(...): Preprocesses a tensor or Numpy array encoding a batch of images."]}, {"name": "tf.compat.v1.keras.applications.inception_resnet_v2", "path": "compat/v1/keras/applications/inception_resnet_v2", "type": "tf.compat", "text": ["Inception-ResNet V2 model for Keras.", "InceptionResNetV2(...): Instantiates the Inception-ResNet v2 architecture.", "decode_predictions(...): Decodes the prediction of an ImageNet model.", "preprocess_input(...): Preprocesses a tensor or Numpy array encoding a batch of images."]}, {"name": "tf.compat.v1.keras.applications.inception_v3", "path": "compat/v1/keras/applications/inception_v3", "type": "tf.compat", "text": ["Inception V3 model for Keras.", "InceptionV3(...): Instantiates the Inception v3 architecture.", "decode_predictions(...): Decodes the prediction of an ImageNet model.", "preprocess_input(...): Preprocesses a tensor or Numpy array encoding a batch of images."]}, {"name": "tf.compat.v1.keras.applications.mobilenet", "path": "compat/v1/keras/applications/mobilenet", "type": "tf.compat", "text": ["MobileNet v1 models for Keras.", "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices.", "MobileNets support any input size greater than 32 x 32, with larger image sizes offering better performance. The number of parameters and number of multiply-adds can be modified by using the alpha parameter, which increases/decreases the number of filters in each layer. By altering the image size and alpha parameter, all 16 models from the paper can be built, with ImageNet weights provided.", "The paper demonstrates the performance of MobileNets using alpha values of 1.0 (also called 100 % MobileNet), 0.75, 0.5 and 0.25. For each of these alpha values, weights for 4 different input image sizes are provided (224, 192, 160, 128).", "The following table describes the size and accuracy of the 100% MobileNet", "| 1.0 MobileNet-224 | 70.6 % | 529 | 4.2 | | 0.75 MobileNet-224 | 68.4 % | 325 | 2.6 | | 0.50 MobileNet-224 | 63.7 % | 149 | 1.3 |", "The following table describes the performance of", "| 1.0 MobileNet-224 | 70.6 % | 529 | 4.2 | | 1.0 MobileNet-192 | 69.1 % | 529 | 4.2 | | 1.0 MobileNet-160 | 67.2 % | 529 | 4.2 |", "MobileNet(...): Instantiates the MobileNet architecture.", "decode_predictions(...): Decodes the prediction of an ImageNet model.", "preprocess_input(...): Preprocesses a tensor or Numpy array encoding a batch of images."]}, {"name": "tf.compat.v1.keras.applications.mobilenet_v2", "path": "compat/v1/keras/applications/mobilenet_v2", "type": "tf.compat", "text": ["MobileNet v2 models for Keras.", "MobileNetV2 is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices.", "MobileNetV2 is very similar to the original MobileNet, except that it uses inverted residual blocks with bottlenecking features. It has a drastically lower parameter count than the original MobileNet. MobileNets support any input size greater than 32 x 32, with larger image sizes offering better performance.", "The number of parameters and number of multiply-adds can be modified by using the alpha parameter, which increases/decreases the number of filters in each layer. By altering the image size and alpha parameter, all 22 models from the paper can be built, with ImageNet weights provided.", "The paper demonstrates the performance of MobileNets using alpha values of 1.0 (also called 100 % MobileNet), 0.35, 0.5, 0.75, 1.0, 1.3, and 1.4 For each of these alpha values, weights for 5 different input image sizes are provided (224, 192, 160, 128, and 96).", "The following table describes the performance of", "MACs stands for Multiply Adds Classification Checkpoint|MACs (M)|Parameters (M)|Top 1 Accuracy|Top 5 Accuracy --------------------------|------------|---------------|---------|----|--------- | [mobilenet_v2_1.4_224] | 582 | 6.06 | 75.0 | 92.5 | | [mobilenet_v2_1.3_224] | 509 | 5.34 | 74.4 | 92.1 | | [mobilenet_v2_1.0_224] | 300 | 3.47 | 71.8 | 91.0 | | [mobilenet_v2_1.0_192] | 221 | 3.47 | 70.7 | 90.1 | | [mobilenet_v2_1.0_160] | 154 | 3.47 | 68.8 | 89.0 | | [mobilenet_v2_1.0_128] | 99 | 3.47 | 65.3 | 86.9 | | [mobilenet_v2_1.0_96] | 56 | 3.47 | 60.3 | 83.2 | | [mobilenet_v2_0.75_224] | 209 | 2.61 | 69.8 | 89.6 | | [mobilenet_v2_0.75_192] | 153 | 2.61 | 68.7 | 88.9 | | [mobilenet_v2_0.75_160] | 107 | 2.61 | 66.4 | 87.3 | | [mobilenet_v2_0.75_128] | 69 | 2.61 | 63.2 | 85.3 | | [mobilenet_v2_0.75_96] | 39 | 2.61 | 58.8 | 81.6 | | [mobilenet_v2_0.5_224] | 97 | 1.95 | 65.4 | 86.4 | | [mobilenet_v2_0.5_192] | 71 | 1.95 | 63.9 | 85.4 | | [mobilenet_v2_0.5_160] | 50 | 1.95 | 61.0 | 83.2 | | [mobilenet_v2_0.5_128] | 32 | 1.95 | 57.7 | 80.8 | | [mobilenet_v2_0.5_96] | 18 | 1.95 | 51.2 | 75.8 | | [mobilenet_v2_0.35_224] | 59 | 1.66 | 60.3 | 82.9 | | [mobilenet_v2_0.35_192] | 43 | 1.66 | 58.2 | 81.2 | | [mobilenet_v2_0.35_160] | 30 | 1.66 | 55.7 | 79.1 | | [mobilenet_v2_0.35_128] | 20 | 1.66 | 50.8 | 75.0 | | [mobilenet_v2_0.35_96] | 11 | 1.66 | 45.5 | 70.4 |", "Reference:", "MobileNetV2(...): Instantiates the MobileNetV2 architecture.", "decode_predictions(...): Decodes the prediction of an ImageNet model.", "preprocess_input(...): Preprocesses a tensor or Numpy array encoding a batch of images."]}, {"name": "tf.compat.v1.keras.applications.mobilenet_v3", "path": "compat/v1/keras/applications/mobilenet_v3", "type": "tf.compat", "text": ["MobileNet v3 models for Keras.", "decode_predictions(...): Decodes the prediction of an ImageNet model.", "preprocess_input(...): Preprocesses a tensor or Numpy array encoding a batch of images."]}, {"name": "tf.compat.v1.keras.applications.nasnet", "path": "compat/v1/keras/applications/nasnet", "type": "tf.compat", "text": ["NASNet-A models for Keras.", "NASNet refers to Neural Architecture Search Network, a family of models that were designed automatically by learning the model architectures directly on the dataset of interest.", "Here we consider NASNet-A, the highest performance model that was found for the CIFAR-10 dataset, and then extended to ImageNet 2012 dataset, obtaining state of the art performance on CIFAR-10 and ImageNet 2012. Only the NASNet-A models, and their respective weights, which are suited for ImageNet 2012 are provided.", "| NASNet-A (4 @ 1056) | 74.0 % | 91.6 % | 564 M | 5.3 |", "NASNetLarge(...): Instantiates a NASNet model in ImageNet mode.", "NASNetMobile(...): Instantiates a Mobile NASNet model in ImageNet mode.", "decode_predictions(...): Decodes the prediction of an ImageNet model.", "preprocess_input(...): Preprocesses a tensor or Numpy array encoding a batch of images."]}, {"name": "tf.compat.v1.keras.applications.resnet", "path": "compat/v1/keras/applications/resnet", "type": "tf.compat", "text": ["ResNet models for Keras.", "ResNet101(...): Instantiates the ResNet101 architecture.", "ResNet152(...): Instantiates the ResNet152 architecture.", "ResNet50(...): Instantiates the ResNet50 architecture.", "decode_predictions(...): Decodes the prediction of an ImageNet model.", "preprocess_input(...): Preprocesses a tensor or Numpy array encoding a batch of images."]}, {"name": "tf.compat.v1.keras.applications.resnet50", "path": "compat/v1/keras/applications/resnet50", "type": "tf.compat", "text": ["Public API for tf.keras.applications.resnet50 namespace.", "ResNet50(...): Instantiates the ResNet50 architecture.", "decode_predictions(...): Decodes the prediction of an ImageNet model.", "preprocess_input(...): Preprocesses a tensor or Numpy array encoding a batch of images."]}, {"name": "tf.compat.v1.keras.applications.resnet_v2", "path": "compat/v1/keras/applications/resnet_v2", "type": "tf.compat", "text": ["ResNet v2 models for Keras.", "ResNet101V2(...): Instantiates the ResNet101V2 architecture.", "ResNet152V2(...): Instantiates the ResNet152V2 architecture.", "ResNet50V2(...): Instantiates the ResNet50V2 architecture.", "decode_predictions(...): Decodes the prediction of an ImageNet model.", "preprocess_input(...): Preprocesses a tensor or Numpy array encoding a batch of images."]}, {"name": "tf.compat.v1.keras.applications.vgg16", "path": "compat/v1/keras/applications/vgg16", "type": "tf.compat", "text": ["VGG16 model for Keras.", "VGG16(...): Instantiates the VGG16 model.", "decode_predictions(...): Decodes the prediction of an ImageNet model.", "preprocess_input(...): Preprocesses a tensor or Numpy array encoding a batch of images."]}, {"name": "tf.compat.v1.keras.applications.vgg19", "path": "compat/v1/keras/applications/vgg19", "type": "tf.compat", "text": ["VGG19 model for Keras.", "VGG19(...): Instantiates the VGG19 architecture.", "decode_predictions(...): Decodes the prediction of an ImageNet model.", "preprocess_input(...): Preprocesses a tensor or Numpy array encoding a batch of images."]}, {"name": "tf.compat.v1.keras.applications.xception", "path": "compat/v1/keras/applications/xception", "type": "tf.compat", "text": ["Xception V1 model for Keras.", "On ImageNet, this model gets to a top-1 validation accuracy of 0.790 and a top-5 validation accuracy of 0.945.", "Xception(...): Instantiates the Xception architecture.", "decode_predictions(...): Decodes the prediction of an ImageNet model.", "preprocess_input(...): Preprocesses a tensor or Numpy array encoding a batch of images."]}, {"name": "tf.compat.v1.keras.backend", "path": "compat/v1/keras/backend", "type": "tf.compat", "text": ["Keras backend API.", "class name_scope: A context manager for use when defining a Python op.", "clear_session(...): Resets all state generated by Keras.", "epsilon(...): Returns the value of the fuzz factor used in numeric expressions.", "floatx(...): Returns the default float type, as a string.", "get_session(...): Returns the TF session to be used by the backend.", "get_uid(...): Associates a string prefix with an integer counter in a TensorFlow graph.", "image_data_format(...): Returns the default image data format convention.", "is_keras_tensor(...): Returns whether x is a Keras tensor.", "reset_uids(...): Resets graph identifiers.", "rnn(...): Iterates over the time dimension of a tensor.", "set_epsilon(...): Sets the value of the fuzz factor used in numeric expressions.", "set_floatx(...): Sets the default float type.", "set_image_data_format(...): Sets the value of the image data format convention.", "set_session(...): Sets the global TensorFlow session."]}, {"name": "tf.compat.v1.keras.backend.get_session", "path": "compat/v1/keras/backend/get_session", "type": "tf.compat", "text": ["Returns the TF session to be used by the backend.", "If a default TensorFlow session is available, we will return it.", "Else, we will return the global Keras session assuming it matches the current graph.", "If no global Keras session exists at this point: we will create a new global session.", "Note that you can manually set the global session via K.set_session(sess)."]}, {"name": "tf.compat.v1.keras.backend.name_scope", "path": "compat/v1/keras/backend/name_scope", "type": "tf.compat", "text": ["A context manager for use when defining a Python op.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.name_scope", "This context manager validates that the given values are from the same graph, makes that graph the default graph, and pushes a name scope in that graph (see tf.Graph.name_scope for more details on that).", "For example, to define a new Python op called my_op:", "View source", "View source"]}, {"name": "tf.compat.v1.keras.backend.set_session", "path": "compat/v1/keras/backend/set_session", "type": "tf.compat", "text": ["Sets the global TensorFlow session."]}, {"name": "tf.compat.v1.keras.callbacks", "path": "compat/v1/keras/callbacks", "type": "tf.compat", "text": ["Callbacks: utilities called at certain points during model training.", "class BaseLogger: Callback that accumulates epoch averages of metrics.", "class CSVLogger: Callback that streams epoch results to a CSV file.", "class Callback: Abstract base class used to build new callbacks.", "class CallbackList: Container abstracting a list of callbacks.", "class EarlyStopping: Stop training when a monitored metric has stopped improving.", "class History: Callback that records events into a History object.", "class LambdaCallback: Callback for creating simple, custom callbacks on-the-fly.", "class LearningRateScheduler: Learning rate scheduler.", "class ModelCheckpoint: Callback to save the Keras model or model weights at some frequency.", "class ProgbarLogger: Callback that prints metrics to stdout.", "class ReduceLROnPlateau: Reduce learning rate when a metric has stopped improving.", "class RemoteMonitor: Callback used to stream events to a server.", "class TensorBoard: Enable visualizations for TensorBoard.", "class TerminateOnNaN: Callback that terminates training when a NaN loss is encountered."]}, {"name": "tf.compat.v1.keras.callbacks.TensorBoard", "path": "compat/v1/keras/callbacks/tensorboard", "type": "tf.compat", "text": ["Enable visualizations for TensorBoard.", "Inherits From: TensorBoard, Callback", "TensorBoard is a visualization tool provided with TensorFlow.", "This callback logs events for TensorBoard, including:", "If you have installed TensorFlow with pip, you should be able to launch TensorBoard from the command line:", "You can find more information about TensorBoard here.", "Using the TensorBoard callback will work when eager execution is enabled, with the restriction that outputting histogram summaries of weights and gradients is not supported. Consequently, histogram_freq will be ignored.", "View source", "Sets Keras model and creates summary ops.", "View source"]}, {"name": "tf.compat.v1.keras.constraints", "path": "compat/v1/keras/constraints", "type": "tf.compat", "text": ["Constraints: functions that impose constraints on weight values.", "class Constraint", "class MaxNorm: MaxNorm weight constraint.", "class MinMaxNorm: MinMaxNorm weight constraint.", "class NonNeg: Constrains the weights to be non-negative.", "class RadialConstraint: Constrains Conv2D kernel weights to be the same for each radius.", "class UnitNorm: Constrains the weights incident to each hidden unit to have unit norm.", "class max_norm: MaxNorm weight constraint.", "class min_max_norm: MinMaxNorm weight constraint.", "class non_neg: Constrains the weights to be non-negative.", "class radial_constraint: Constrains Conv2D kernel weights to be the same for each radius.", "class unit_norm: Constrains the weights incident to each hidden unit to have unit norm.", "deserialize(...)", "get(...)", "serialize(...)"]}, {"name": "tf.compat.v1.keras.datasets", "path": "compat/v1/keras/datasets", "type": "tf.compat", "text": ["Public API for tf.keras.datasets namespace.", "boston_housing module: Boston housing price regression dataset.", "cifar10 module: CIFAR10 small images classification dataset.", "cifar100 module: CIFAR100 small images classification dataset.", "fashion_mnist module: Fashion-MNIST dataset.", "imdb module: IMDB sentiment classification dataset.", "mnist module: MNIST handwritten digits dataset.", "reuters module: Reuters topic classification dataset."]}, {"name": "tf.compat.v1.keras.datasets.boston_housing", "path": "compat/v1/keras/datasets/boston_housing", "type": "tf.compat", "text": ["Boston housing price regression dataset.", "load_data(...): Loads the Boston Housing dataset."]}, {"name": "tf.compat.v1.keras.datasets.cifar10", "path": "compat/v1/keras/datasets/cifar10", "type": "tf.compat", "text": ["CIFAR10 small images classification dataset.", "load_data(...): Loads CIFAR10 dataset."]}, {"name": "tf.compat.v1.keras.datasets.cifar100", "path": "compat/v1/keras/datasets/cifar100", "type": "tf.compat", "text": ["CIFAR100 small images classification dataset.", "load_data(...): Loads CIFAR100 dataset."]}, {"name": "tf.compat.v1.keras.datasets.fashion_mnist", "path": "compat/v1/keras/datasets/fashion_mnist", "type": "tf.compat", "text": ["Fashion-MNIST dataset.", "load_data(...): Loads the Fashion-MNIST dataset."]}, {"name": "tf.compat.v1.keras.datasets.imdb", "path": "compat/v1/keras/datasets/imdb", "type": "tf.compat", "text": ["IMDB sentiment classification dataset.", "get_word_index(...): Retrieves a dict mapping words to their index in the IMDB dataset.", "load_data(...): Loads the IMDB dataset."]}, {"name": "tf.compat.v1.keras.datasets.mnist", "path": "compat/v1/keras/datasets/mnist", "type": "tf.compat", "text": ["MNIST handwritten digits dataset.", "load_data(...): Loads the MNIST dataset."]}, {"name": "tf.compat.v1.keras.datasets.reuters", "path": "compat/v1/keras/datasets/reuters", "type": "tf.compat", "text": ["Reuters topic classification dataset.", "get_word_index(...): Retrieves a dict mapping words to their index in the Reuters dataset.", "load_data(...): Loads the Reuters newswire classification dataset."]}, {"name": "tf.compat.v1.keras.estimator", "path": "compat/v1/keras/estimator", "type": "tf.compat", "text": ["Keras estimator API.", "model_to_estimator(...): Constructs an Estimator instance from given keras model."]}, {"name": "tf.compat.v1.keras.estimator.model_to_estimator", "path": "compat/v1/keras/estimator/model_to_estimator", "type": "tf.compat", "text": ["Constructs an Estimator instance from given keras model.", "If you use infrastructure or other tooling that relies on Estimators, you can still build a Keras model and use model_to_estimator to convert the Keras model to an Estimator for use with downstream systems.", "For usage example, please see: Creating estimators from Keras Models.", "Estimators returned by model_to_estimator are configured so that they can handle sample weights (similar to keras_model.fit(x, y, sample_weights)).", "To pass sample weights when training or evaluating the Estimator, the first item returned by the input function should be a dictionary with keys features and sample_weights. Example below:", "Example with customized export signature:"]}, {"name": "tf.compat.v1.keras.experimental", "path": "compat/v1/keras/experimental", "type": "tf.compat", "text": ["Public API for tf.keras.experimental namespace.", "class CosineDecay: A LearningRateSchedule that uses a cosine decay schedule.", "class CosineDecayRestarts: A LearningRateSchedule that uses a cosine decay schedule with restarts.", "class LinearCosineDecay: A LearningRateSchedule that uses a linear cosine decay schedule.", "class LinearModel: Linear Model for regression and classification problems.", "class NoisyLinearCosineDecay: A LearningRateSchedule that uses a noisy linear cosine decay schedule.", "class PeepholeLSTMCell: Equivalent to LSTMCell class but adds peephole connections.", "class SequenceFeatures: A layer for sequence input.", "class WideDeepModel: Wide & Deep Model for regression and classification problems.", "export_saved_model(...): Exports a tf.keras.Model as a Tensorflow SavedModel.", "load_from_saved_model(...): Loads a keras Model from a SavedModel created by export_saved_model()."]}, {"name": "tf.compat.v1.keras.experimental.export_saved_model", "path": "compat/v1/keras/experimental/export_saved_model", "type": "tf.compat", "text": ["Exports a tf.keras.Model as a Tensorflow SavedModel.", "Note that at this time, subclassed models can only be saved using serving_only=True.", "The exported SavedModel is a standalone serialization of Tensorflow objects, and is supported by TF language APIs and the Tensorflow Serving system. To load the model, use the function tf.keras.experimental.load_from_saved_model.", "The SavedModel contains:"]}, {"name": "tf.compat.v1.keras.experimental.load_from_saved_model", "path": "compat/v1/keras/experimental/load_from_saved_model", "type": "tf.compat", "text": ["Loads a keras Model from a SavedModel created by export_saved_model().", "This function reinstantiates model state by:", "1) loading model topology from json (this will eventually come from metagraph). 2) loading model weights from checkpoint."]}, {"name": "tf.compat.v1.keras.initializers", "path": "compat/v1/keras/initializers", "type": "tf.compat", "text": ["Keras initializer serialization / deserialization.", "class Constant: Initializer that generates tensors with constant values.", "class Identity: Initializer that generates the identity matrix.", "class Initializer: Initializer base class: all Keras initializers inherit from this class.", "class Ones: Initializer that generates tensors initialized to 1.", "class Orthogonal: Initializer that generates an orthogonal matrix.", "class RandomNormal: Initializer that generates tensors with a normal distribution.", "class RandomUniform: Initializer that generates tensors with a uniform distribution.", "class TruncatedNormal: Initializer that generates a truncated normal distribution.", "class VarianceScaling: Initializer capable of adapting its scale to the shape of weights tensors.", "class Zeros: Initializer that generates tensors initialized to 0.", "class constant: Initializer that generates tensors with constant values.", "class glorot_normal: The Glorot normal initializer, also called Xavier normal initializer.", "class glorot_uniform: The Glorot uniform initializer, also called Xavier uniform initializer.", "class he_normal: Initializer capable of adapting its scale to the shape of weights tensors.", "class he_uniform: Initializer capable of adapting its scale to the shape of weights tensors.", "class identity: Initializer that generates the identity matrix.", "class lecun_normal: Initializer capable of adapting its scale to the shape of weights tensors.", "class lecun_uniform: Initializer capable of adapting its scale to the shape of weights tensors.", "class normal: Initializer that generates tensors with a normal distribution.", "class ones: Initializer that generates tensors initialized to 1.", "class orthogonal: Initializer that generates an orthogonal matrix.", "class random_normal: Initializer that generates tensors with a normal distribution.", "class random_uniform: Initializer that generates tensors with a uniform distribution.", "class truncated_normal: Initializer that generates a truncated normal distribution.", "class uniform: Initializer that generates tensors with a uniform distribution.", "class zeros: Initializer that generates tensors initialized to 0.", "deserialize(...): Return an Initializer object from its config.", "get(...)", "serialize(...)"]}, {"name": "tf.compat.v1.keras.initializers.Constant", "path": "compat/v1/keras/initializers/constant", "type": "tf.compat", "text": ["Initializer that generates tensors with constant values.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.constant_initializer, tf.compat.v1.initializers.constant, tf.compat.v1.keras.initializers.constant", "The resulting tensor is populated with values of type dtype, as specified by arguments value following the desired shape of the new tensor (see examples below).", "The argument value can be a constant value, or a list of values of type dtype. If value is a list, then the length of the list must be less than or equal to the number of elements implied by the desired shape of the tensor. In the case where the total number of elements in value is less than the number of elements required by the tensor shape, the last element in value will be used to fill the remaining entries. If the total number of elements in value is greater than the number of elements required by the tensor shape, the initializer will raise a ValueError.", "The following example can be rewritten using a numpy.ndarray instead of the value list, even reshaped, as shown in the two commented lines below the value list initialization.", "View source", "Instantiates an initializer from a configuration dictionary.", "View source", "Returns the configuration of the initializer as a JSON-serializable dict.", "View source", "Returns a tensor object initialized as specified by the initializer."]}, {"name": "tf.compat.v1.keras.initializers.glorot_normal", "path": "compat/v1/keras/initializers/glorot_normal", "type": "tf.compat", "text": ["The Glorot normal initializer, also called Xavier normal initializer.", "Inherits From: VarianceScaling", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.glorot_normal_initializer, tf.compat.v1.initializers.glorot_normal", "It draws samples from a truncated normal distribution centered on 0 with standard deviation (after truncation) given by stddev = sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.", "Glorot et al., 2010 (pdf)", "View source", "Instantiates an initializer from a configuration dictionary.", "View source", "Returns the configuration of the initializer as a JSON-serializable dict.", "View source", "Returns a tensor object initialized as specified by the initializer."]}, {"name": "tf.compat.v1.keras.initializers.glorot_uniform", "path": "compat/v1/keras/initializers/glorot_uniform", "type": "tf.compat", "text": ["The Glorot uniform initializer, also called Xavier uniform initializer.", "Inherits From: VarianceScaling", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.glorot_uniform_initializer, tf.compat.v1.initializers.glorot_uniform", "It draws samples from a uniform distribution within [-limit, limit] where limit is sqrt(6 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.", "Glorot et al., 2010 (pdf)", "View source", "Instantiates an initializer from a configuration dictionary.", "View source", "Returns the configuration of the initializer as a JSON-serializable dict.", "View source", "Returns a tensor object initialized as specified by the initializer."]}, {"name": "tf.compat.v1.keras.initializers.he_normal", "path": "compat/v1/keras/initializers/he_normal", "type": "tf.compat", "text": ["Initializer capable of adapting its scale to the shape of weights tensors.", "Inherits From: VarianceScaling", "With distribution=\"truncated_normal\" or \"untruncated_normal\", samples are drawn from a truncated/untruncated normal distribution with a mean of zero and a standard deviation (after truncation, if used) stddev = sqrt(scale / n) where n is:", "With distribution=\"uniform\", samples are drawn from a uniform distribution within [-limit, limit], with limit = sqrt(3 * scale / n).", "View source", "Instantiates an initializer from a configuration dictionary.", "View source", "Returns the configuration of the initializer as a JSON-serializable dict.", "View source", "Returns a tensor object initialized as specified by the initializer."]}, {"name": "tf.compat.v1.keras.initializers.he_uniform", "path": "compat/v1/keras/initializers/he_uniform", "type": "tf.compat", "text": ["Initializer capable of adapting its scale to the shape of weights tensors.", "Inherits From: VarianceScaling", "With distribution=\"truncated_normal\" or \"untruncated_normal\", samples are drawn from a truncated/untruncated normal distribution with a mean of zero and a standard deviation (after truncation, if used) stddev = sqrt(scale / n) where n is:", "With distribution=\"uniform\", samples are drawn from a uniform distribution within [-limit, limit], with limit = sqrt(3 * scale / n).", "View source", "Instantiates an initializer from a configuration dictionary.", "View source", "Returns the configuration of the initializer as a JSON-serializable dict.", "View source", "Returns a tensor object initialized as specified by the initializer."]}, {"name": "tf.compat.v1.keras.initializers.Identity", "path": "compat/v1/keras/initializers/identity", "type": "tf.compat", "text": ["Initializer that generates the identity matrix.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.initializers.identity, tf.compat.v1.keras.initializers.identity", "Only use for 2D matrices.", "View source", "Instantiates an initializer from a configuration dictionary.", "View source", "Returns the configuration of the initializer as a JSON-serializable dict.", "View source", "Returns a tensor object initialized as specified by the initializer."]}, {"name": "tf.compat.v1.keras.initializers.lecun_normal", "path": "compat/v1/keras/initializers/lecun_normal", "type": "tf.compat", "text": ["Initializer capable of adapting its scale to the shape of weights tensors.", "Inherits From: VarianceScaling", "With distribution=\"truncated_normal\" or \"untruncated_normal\", samples are drawn from a truncated/untruncated normal distribution with a mean of zero and a standard deviation (after truncation, if used) stddev = sqrt(scale / n) where n is:", "With distribution=\"uniform\", samples are drawn from a uniform distribution within [-limit, limit], with limit = sqrt(3 * scale / n).", "View source", "Instantiates an initializer from a configuration dictionary.", "View source", "Returns the configuration of the initializer as a JSON-serializable dict.", "View source", "Returns a tensor object initialized as specified by the initializer."]}, {"name": "tf.compat.v1.keras.initializers.lecun_uniform", "path": "compat/v1/keras/initializers/lecun_uniform", "type": "tf.compat", "text": ["Initializer capable of adapting its scale to the shape of weights tensors.", "Inherits From: VarianceScaling", "With distribution=\"truncated_normal\" or \"untruncated_normal\", samples are drawn from a truncated/untruncated normal distribution with a mean of zero and a standard deviation (after truncation, if used) stddev = sqrt(scale / n) where n is:", "With distribution=\"uniform\", samples are drawn from a uniform distribution within [-limit, limit], with limit = sqrt(3 * scale / n).", "View source", "Instantiates an initializer from a configuration dictionary.", "View source", "Returns the configuration of the initializer as a JSON-serializable dict.", "View source", "Returns a tensor object initialized as specified by the initializer."]}, {"name": "tf.compat.v1.keras.initializers.Ones", "path": "compat/v1/keras/initializers/ones", "type": "tf.compat", "text": ["Initializer that generates tensors initialized to 1.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.initializers.ones, tf.compat.v1.keras.initializers.ones, tf.compat.v1.ones_initializer", "View source", "Instantiates an initializer from a configuration dictionary.", "View source", "Returns the configuration of the initializer as a JSON-serializable dict.", "View source", "Returns a tensor object initialized as specified by the initializer."]}, {"name": "tf.compat.v1.keras.initializers.Orthogonal", "path": "compat/v1/keras/initializers/orthogonal", "type": "tf.compat", "text": ["Initializer that generates an orthogonal matrix.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.initializers.orthogonal, tf.compat.v1.keras.initializers.orthogonal, tf.compat.v1.orthogonal_initializer", "If the shape of the tensor to initialize is two-dimensional, it is initialized with an orthogonal matrix obtained from the QR decomposition of a matrix of random numbers drawn from a normal distribution. If the matrix has fewer rows than columns then the output will have orthogonal rows. Otherwise, the output will have orthogonal columns.", "If the shape of the tensor to initialize is more than two-dimensional, a matrix of shape (shape[0] * ... * shape[n - 2], shape[n - 1]) is initialized, where n is the length of the shape vector. The matrix is subsequently reshaped to give a tensor of the desired shape.", "Saxe et al., 2014 (pdf)", "View source", "Instantiates an initializer from a configuration dictionary.", "View source", "Returns the configuration of the initializer as a JSON-serializable dict.", "View source", "Returns a tensor object initialized as specified by the initializer."]}, {"name": "tf.compat.v1.keras.initializers.RandomNormal", "path": "compat/v1/keras/initializers/randomnormal", "type": "tf.compat", "text": ["Initializer that generates tensors with a normal distribution.", "Inherits From: random_normal_initializer", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.keras.initializers.normal, tf.compat.v1.keras.initializers.random_normal", "View source", "Instantiates an initializer from a configuration dictionary.", "View source", "Returns the configuration of the initializer as a JSON-serializable dict.", "View source", "Returns a tensor object initialized as specified by the initializer."]}, {"name": "tf.compat.v1.keras.initializers.RandomUniform", "path": "compat/v1/keras/initializers/randomuniform", "type": "tf.compat", "text": ["Initializer that generates tensors with a uniform distribution.", "Inherits From: random_uniform_initializer", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.keras.initializers.random_uniform, tf.compat.v1.keras.initializers.uniform", "View source", "Instantiates an initializer from a configuration dictionary.", "View source", "Returns the configuration of the initializer as a JSON-serializable dict.", "View source", "Returns a tensor object initialized as specified by the initializer."]}, {"name": "tf.compat.v1.keras.initializers.TruncatedNormal", "path": "compat/v1/keras/initializers/truncatednormal", "type": "tf.compat", "text": ["Initializer that generates a truncated normal distribution.", "Inherits From: truncated_normal_initializer", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.keras.initializers.truncated_normal", "These values are similar to values from a random_normal_initializer except that values more than two standard deviations from the mean are discarded and re-drawn. This is the recommended initializer for neural network weights and filters.", "View source", "Instantiates an initializer from a configuration dictionary.", "View source", "Returns the configuration of the initializer as a JSON-serializable dict.", "View source", "Returns a tensor object initialized as specified by the initializer."]}, {"name": "tf.compat.v1.keras.initializers.VarianceScaling", "path": "compat/v1/keras/initializers/variancescaling", "type": "tf.compat", "text": ["Initializer capable of adapting its scale to the shape of weights tensors.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.initializers.variance_scaling, tf.compat.v1.variance_scaling_initializer", "With distribution=\"truncated_normal\" or \"untruncated_normal\", samples are drawn from a truncated/untruncated normal distribution with a mean of zero and a standard deviation (after truncation, if used) stddev = sqrt(scale / n) where n is:", "With distribution=\"uniform\", samples are drawn from a uniform distribution within [-limit, limit], with limit = sqrt(3 * scale / n).", "View source", "Instantiates an initializer from a configuration dictionary.", "View source", "Returns the configuration of the initializer as a JSON-serializable dict.", "View source", "Returns a tensor object initialized as specified by the initializer."]}, {"name": "tf.compat.v1.keras.initializers.Zeros", "path": "compat/v1/keras/initializers/zeros", "type": "tf.compat", "text": ["Initializer that generates tensors initialized to 0.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.initializers.zeros, tf.compat.v1.keras.initializers.zeros, tf.compat.v1.zeros_initializer", "View source", "Instantiates an initializer from a configuration dictionary.", "View source", "Returns the configuration of the initializer as a JSON-serializable dict.", "View source", "Returns a tensor object initialized as specified by the initializer."]}, {"name": "tf.compat.v1.keras.layers", "path": "compat/v1/keras/layers", "type": "tf.compat", "text": ["Keras layers API.", "experimental module: Public API for tf.keras.layers.experimental namespace.", "class AbstractRNNCell: Abstract object representing an RNN cell.", "class Activation: Applies an activation function to an output.", "class ActivityRegularization: Layer that applies an update to the cost function based input activity.", "class Add: Layer that adds a list of inputs.", "class AdditiveAttention: Additive attention layer, a.k.a. Bahdanau-style attention.", "class AlphaDropout: Applies Alpha Dropout to the input.", "class Attention: Dot-product attention layer, a.k.a. Luong-style attention.", "class Average: Layer that averages a list of inputs element-wise.", "class AveragePooling1D: Average pooling for temporal data.", "class AveragePooling2D: Average pooling operation for spatial data.", "class AveragePooling3D: Average pooling operation for 3D data (spatial or spatio-temporal).", "class AvgPool1D: Average pooling for temporal data.", "class AvgPool2D: Average pooling operation for spatial data.", "class AvgPool3D: Average pooling operation for 3D data (spatial or spatio-temporal).", "class BatchNormalization: Layer that normalizes its inputs.", "class Bidirectional: Bidirectional wrapper for RNNs.", "class Concatenate: Layer that concatenates a list of inputs.", "class Conv1D: 1D convolution layer (e.g. temporal convolution).", "class Conv1DTranspose: Transposed convolution layer (sometimes called Deconvolution).", "class Conv2D: 2D convolution layer (e.g. spatial convolution over images).", "class Conv2DTranspose: Transposed convolution layer (sometimes called Deconvolution).", "class Conv3D: 3D convolution layer (e.g. spatial convolution over volumes).", "class Conv3DTranspose: Transposed convolution layer (sometimes called Deconvolution).", "class ConvLSTM2D: Convolutional LSTM.", "class Convolution1D: 1D convolution layer (e.g. temporal convolution).", "class Convolution1DTranspose: Transposed convolution layer (sometimes called Deconvolution).", "class Convolution2D: 2D convolution layer (e.g. spatial convolution over images).", "class Convolution2DTranspose: Transposed convolution layer (sometimes called Deconvolution).", "class Convolution3D: 3D convolution layer (e.g. spatial convolution over volumes).", "class Convolution3DTranspose: Transposed convolution layer (sometimes called Deconvolution).", "class Cropping1D: Cropping layer for 1D input (e.g. temporal sequence).", "class Cropping2D: Cropping layer for 2D input (e.g. picture).", "class Cropping3D: Cropping layer for 3D data (e.g. spatial or spatio-temporal).", "class CuDNNGRU: Fast GRU implementation backed by cuDNN.", "class CuDNNLSTM: Fast LSTM implementation backed by cuDNN.", "class Dense: Just your regular densely-connected NN layer.", "class DenseFeatures: A layer that produces a dense Tensor based on given feature_columns.", "class DepthwiseConv2D: Depthwise separable 2D convolution.", "class Dot: Layer that computes a dot product between samples in two tensors.", "class Dropout: Applies Dropout to the input.", "class ELU: Exponential Linear Unit.", "class Embedding: Turns positive integers (indexes) into dense vectors of fixed size.", "class Flatten: Flattens the input. Does not affect the batch size.", "class GRU: Gated Recurrent Unit - Cho et al. 2014.", "class GRUCell: Cell class for the GRU layer.", "class GaussianDropout: Apply multiplicative 1-centered Gaussian noise.", "class GaussianNoise: Apply additive zero-centered Gaussian noise.", "class GlobalAveragePooling1D: Global average pooling operation for temporal data.", "class GlobalAveragePooling2D: Global average pooling operation for spatial data.", "class GlobalAveragePooling3D: Global Average pooling operation for 3D data.", "class GlobalAvgPool1D: Global average pooling operation for temporal data.", "class GlobalAvgPool2D: Global average pooling operation for spatial data.", "class GlobalAvgPool3D: Global Average pooling operation for 3D data.", "class GlobalMaxPool1D: Global max pooling operation for 1D temporal data.", "class GlobalMaxPool2D: Global max pooling operation for spatial data.", "class GlobalMaxPool3D: Global Max pooling operation for 3D data.", "class GlobalMaxPooling1D: Global max pooling operation for 1D temporal data.", "class GlobalMaxPooling2D: Global max pooling operation for spatial data.", "class GlobalMaxPooling3D: Global Max pooling operation for 3D data.", "class InputLayer: Layer to be used as an entry point into a Network (a graph of layers).", "class InputSpec: Specifies the rank, dtype and shape of every input to a layer.", "class LSTM: Long Short-Term Memory layer - Hochreiter 1997.", "class LSTMCell: Cell class for the LSTM layer.", "class Lambda: Wraps arbitrary expressions as a Layer object.", "class Layer: This is the class from which all layers inherit.", "class LayerNormalization: Layer normalization layer (Ba et al., 2016).", "class LeakyReLU: Leaky version of a Rectified Linear Unit.", "class LocallyConnected1D: Locally-connected layer for 1D inputs.", "class LocallyConnected2D: Locally-connected layer for 2D inputs.", "class Masking: Masks a sequence by using a mask value to skip timesteps.", "class MaxPool1D: Max pooling operation for 1D temporal data.", "class MaxPool2D: Max pooling operation for 2D spatial data.", "class MaxPool3D: Max pooling operation for 3D data (spatial or spatio-temporal).", "class MaxPooling1D: Max pooling operation for 1D temporal data.", "class MaxPooling2D: Max pooling operation for 2D spatial data.", "class MaxPooling3D: Max pooling operation for 3D data (spatial or spatio-temporal).", "class Maximum: Layer that computes the maximum (element-wise) a list of inputs.", "class Minimum: Layer that computes the minimum (element-wise) a list of inputs.", "class MultiHeadAttention: MultiHeadAttention layer.", "class Multiply: Layer that multiplies (element-wise) a list of inputs.", "class PReLU: Parametric Rectified Linear Unit.", "class Permute: Permutes the dimensions of the input according to a given pattern.", "class RNN: Base class for recurrent layers.", "class ReLU: Rectified Linear Unit activation function.", "class RepeatVector: Repeats the input n times.", "class Reshape: Layer that reshapes inputs into the given shape.", "class SeparableConv1D: Depthwise separable 1D convolution.", "class SeparableConv2D: Depthwise separable 2D convolution.", "class SeparableConvolution1D: Depthwise separable 1D convolution.", "class SeparableConvolution2D: Depthwise separable 2D convolution.", "class SimpleRNN: Fully-connected RNN where the output is to be fed back to input.", "class SimpleRNNCell: Cell class for SimpleRNN.", "class Softmax: Softmax activation function.", "class SpatialDropout1D: Spatial 1D version of Dropout.", "class SpatialDropout2D: Spatial 2D version of Dropout.", "class SpatialDropout3D: Spatial 3D version of Dropout.", "class StackedRNNCells: Wrapper allowing a stack of RNN cells to behave as a single cell.", "class Subtract: Layer that subtracts two inputs.", "class ThresholdedReLU: Thresholded Rectified Linear Unit.", "class TimeDistributed: This wrapper allows to apply a layer to every temporal slice of an input.", "class UpSampling1D: Upsampling layer for 1D inputs.", "class UpSampling2D: Upsampling layer for 2D inputs.", "class UpSampling3D: Upsampling layer for 3D inputs.", "class Wrapper: Abstract wrapper base class.", "class ZeroPadding1D: Zero-padding layer for 1D input (e.g. temporal sequence).", "class ZeroPadding2D: Zero-padding layer for 2D input (e.g. picture).", "class ZeroPadding3D: Zero-padding layer for 3D data (spatial or spatio-temporal).", "Input(...): Input() is used to instantiate a Keras tensor.", "add(...): Functional interface to the tf.keras.layers.Add layer.", "average(...): Functional interface to the tf.keras.layers.Average layer.", "concatenate(...): Functional interface to the Concatenate layer.", "deserialize(...): Instantiates a layer from a config dictionary.", "disable_v2_dtype_behavior(...): Disables the V2 dtype behavior for Keras layers.", "dot(...): Functional interface to the Dot layer.", "enable_v2_dtype_behavior(...): Enable the V2 dtype behavior for Keras layers.", "maximum(...): Functional interface to compute maximum (element-wise) list of inputs.", "minimum(...): Functional interface to the Minimum layer.", "multiply(...): Functional interface to the Multiply layer.", "serialize(...)", "subtract(...): Functional interface to the Subtract layer."]}, {"name": "tf.compat.v1.keras.layers.BatchNormalization", "path": "compat/v1/keras/layers/batchnormalization", "type": "tf.compat", "text": ["Layer that normalizes its inputs.", "Inherits From: Layer, Module", "Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.", "Importantly, batch normalization works differently during training and during inference.", "During training (i.e. when using fit() or when calling the layer/model with the argument training=True), the layer normalizes its output using the mean and standard deviation of the current batch of inputs. That is to say, for each channel being normalized, the layer returns (batch - mean(batch)) / (var(batch) + epsilon) * gamma + beta, where:", "During inference (i.e. when using evaluate() or predict() or when calling the layer/model with the argument training=False (which is the default), the layer normalizes its output using a moving average of the mean and standard deviation of the batches it has seen during training. That is to say, it returns (batch - self.moving_mean) / (self.moving_var + epsilon) * gamma + beta.", "self.moving_mean and self.moving_var are non-trainable variables that are updated each time the layer in called in training mode, as such:", "As such, the layer will only normalize its inputs during inference after having been trained on data that has similar statistics as the inference data.", "Input shape: Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.", "Output shape: Same shape as input. "]}, {"name": "tf.compat.v1.keras.layers.CuDNNGRU", "path": "compat/v1/keras/layers/cudnngru", "type": "tf.compat", "text": ["Fast GRU implementation backed by cuDNN.", "Inherits From: RNN, Layer, Module", "More information about cuDNN can be found on the NVIDIA developer website. Can only be run on GPU.", "View source", "Reset the recorded states for the stateful RNN layer.", "Can only be used when RNN layer is constructed with stateful = True. Args: states: Numpy arrays that contains the value for the initial state, which will be feed to cell at the first time step. When the value is None, zero filled numpy array will be created based on the cell state size."]}, {"name": "tf.compat.v1.keras.layers.CuDNNLSTM", "path": "compat/v1/keras/layers/cudnnlstm", "type": "tf.compat", "text": ["Fast LSTM implementation backed by cuDNN.", "Inherits From: RNN, Layer, Module", "More information about cuDNN can be found on the NVIDIA developer website. Can only be run on GPU.", "View source", "Reset the recorded states for the stateful RNN layer.", "Can only be used when RNN layer is constructed with stateful = True. Args: states: Numpy arrays that contains the value for the initial state, which will be feed to cell at the first time step. When the value is None, zero filled numpy array will be created based on the cell state size."]}, {"name": "tf.compat.v1.keras.layers.DenseFeatures", "path": "compat/v1/keras/layers/densefeatures", "type": "tf.compat", "text": ["A layer that produces a dense Tensor based on given feature_columns.", "Inherits From: Layer, Module", "Generally a single example in training data is described with FeatureColumns. At the first layer of the model, this column-oriented data should be converted to a single Tensor.", "This layer can be called multiple times with different features.", "This is the V1 version of this layer that uses variable_scope's or partitioner to create variables which works well with PartitionedVariables. Variable scopes are deprecated in V2, so the V2 version uses name_scopes instead. But currently that lacks support for partitioned variables. Use this if you need partitioned variables. Use the partitioner argument if you have a Keras model and uses tf.compat.v1.keras.estimator.model_to_estimator for training."]}, {"name": "tf.compat.v1.keras.layers.disable_v2_dtype_behavior", "path": "compat/v1/keras/layers/disable_v2_dtype_behavior", "type": "tf.compat", "text": ["Disables the V2 dtype behavior for Keras layers.", "See tf.compat.v1.keras.layers.enable_v2_dtype_behavior."]}, {"name": "tf.compat.v1.keras.layers.enable_v2_dtype_behavior", "path": "compat/v1/keras/layers/enable_v2_dtype_behavior", "type": "tf.compat", "text": ["Enable the V2 dtype behavior for Keras layers.", "By default, the V2 dtype behavior is enabled in TensorFlow 2, so this function is only useful if tf.compat.v1.disable_v2_behavior has been called. Since mixed precision requires V2 dtype behavior to be enabled, this function allows you to use mixed precision in Keras layers if disable_v2_behavior has been called.", "When enabled, the dtype of Keras layers defaults to floatx (which is typically float32) instead of None. In addition, layers will automatically cast floating-point inputs to the layer's dtype.", "A layer author can opt-out their layer from the automatic input casting by passing autocast=False to the base Layer's constructor. This disables the autocasting part of the V2 behavior for that layer, but not the defaulting to floatx part of the V2 behavior.", "When a global tf.keras.mixed_precision.Policy is set, a Keras layer's dtype will default to the global policy instead of floatx. Layers will automatically cast inputs to the policy's compute_dtype."]}, {"name": "tf.compat.v1.keras.layers.experimental", "path": "compat/v1/keras/layers/experimental", "type": "tf.compat", "text": ["Public API for tf.keras.layers.experimental namespace.", "preprocessing module: Public API for tf.keras.layers.experimental.preprocessing namespace.", "class EinsumDense: A layer that uses tf.einsum as the backing computation.", "class RandomFourierFeatures: Layer that projects its inputs into a random feature space."]}, {"name": "tf.compat.v1.keras.layers.experimental.preprocessing", "path": "compat/v1/keras/layers/experimental/preprocessing", "type": "tf.compat", "text": ["Public API for tf.keras.layers.experimental.preprocessing namespace.", "class CategoryCrossing: Category crossing layer.", "class CategoryEncoding: CategoryEncoding layer.", "class CenterCrop: Crop the central portion of the images to target height and width.", "class Discretization: Buckets data into discrete ranges.", "class Hashing: Implements categorical feature hashing, also known as \"hashing trick\".", "class IntegerLookup: Maps integers from a vocabulary to integer indices.", "class Normalization: Feature-wise normalization of the data.", "class PreprocessingLayer: Base class for PreprocessingLayers.", "class RandomContrast: Adjust the contrast of an image or images by a random factor.", "class RandomCrop: Randomly crop the images to target height and width.", "class RandomFlip: Randomly flip each image horizontally and vertically.", "class RandomHeight: Randomly vary the height of a batch of images during training.", "class RandomRotation: Randomly rotate each image.", "class RandomTranslation: Randomly translate each image during training.", "class RandomWidth: Randomly vary the width of a batch of images during training.", "class RandomZoom: Randomly zoom each image during training.", "class Rescaling: Multiply inputs by scale and adds offset.", "class Resizing: Image resizing layer.", "class StringLookup: Maps strings from a vocabulary to integer indices.", "class TextVectorization: Text vectorization layer."]}, {"name": "tf.compat.v1.keras.layers.experimental.preprocessing.CategoryEncoding", "path": "compat/v1/keras/layers/experimental/preprocessing/categoryencoding", "type": "tf.compat", "text": ["CategoryEncoding layer.", "Inherits From: CategoryEncoding, PreprocessingLayer, Layer, Module", "This layer provides options for condensing input data into denser representations. It accepts either integer values or strings as inputs, allows users to map those inputs into a contiguous integer space, and outputs either those integer values (one sample = 1D tensor of integer token indices) or a dense representation (one sample = 1D tensor of float values representing data about the sample's tokens).", "If desired, the user can call this layer's adapt() method on a dataset. When this layer is adapted, it will analyze the dataset, determine the frequency of individual integer or string values, and create a 'vocabulary' from them. This vocabulary can have unlimited size or be capped, depending on the configuration options for this layer; if there are more unique values in the input than the maximum vocabulary size, the most frequent terms will be used to create the vocabulary.", "View source", "Fits the state of the preprocessing layer to the dataset.", "Overrides the default adapt method to apply relevant preprocessing to the inputs before passing to the combiner.", "View source", "View source"]}, {"name": "tf.compat.v1.keras.layers.experimental.preprocessing.IntegerLookup", "path": "compat/v1/keras/layers/experimental/preprocessing/integerlookup", "type": "tf.compat", "text": ["Maps integers from a vocabulary to integer indices.", "Inherits From: IntegerLookup, PreprocessingLayer, Layer, Module", "View source", "Fits the state of the preprocessing layer to the dataset.", "Overrides the default adapt method to apply relevant preprocessing to the inputs before passing to the combiner.", "View source", "View source", "Sets vocabulary data for this layer with inverse=False.", "This method sets the vocabulary for this layer directly, instead of analyzing a dataset through 'adapt'. It should be used whenever the vocab information is already known. If vocabulary data is already present in the layer, this method will either replace it", "View source"]}, {"name": "tf.compat.v1.keras.layers.experimental.preprocessing.Normalization", "path": "compat/v1/keras/layers/experimental/preprocessing/normalization", "type": "tf.compat", "text": ["Feature-wise normalization of the data.", "Inherits From: Normalization, PreprocessingLayer, Layer, Module", "This layer will coerce its inputs into a distribution centered around 0 with standard deviation 1. It accomplishes this by precomputing the mean and variance of the data, and calling (input-mean)/sqrt(var) at runtime.", "What happens in adapt: Compute mean and variance of the data and store them as the layer's weights. adapt should be called before fit, evaluate, or predict.", "Calculate the mean and variance by analyzing the dataset in adapt.", "Pass the mean and variance directly.", "View source", "Fits the state of the preprocessing layer to the data being passed."]}, {"name": "tf.compat.v1.keras.layers.experimental.preprocessing.StringLookup", "path": "compat/v1/keras/layers/experimental/preprocessing/stringlookup", "type": "tf.compat", "text": ["Maps strings from a vocabulary to integer indices.", "Inherits From: StringLookup, PreprocessingLayer, Layer, Module", "View source", "Fits the state of the preprocessing layer to the dataset.", "Overrides the default adapt method to apply relevant preprocessing to the inputs before passing to the combiner.", "View source", "View source", "Sets vocabulary data for this layer with inverse=False.", "This method sets the vocabulary for this layer directly, instead of analyzing a dataset through 'adapt'. It should be used whenever the vocab information is already known. If vocabulary data is already present in the layer, this method will either replace it", "View source"]}, {"name": "tf.compat.v1.keras.layers.experimental.preprocessing.TextVectorization", "path": "compat/v1/keras/layers/experimental/preprocessing/textvectorization", "type": "tf.compat", "text": ["Text vectorization layer.", "Inherits From: TextVectorization, PreprocessingLayer, Layer, Module", "This layer has basic options for managing text in a Keras model. It transforms a batch of strings (one sample = one string) into either a list of token indices (one sample = 1D tensor of integer token indices) or a dense representation (one sample = 1D tensor of float values representing data about the sample's tokens).", "The processing of each sample contains the following steps:", "1) standardize each sample (usually lowercasing + punctuation stripping) 2) split each sample into substrings (usually words) 3) recombine substrings into tokens (usually ngrams) 4) index tokens (associate a unique int value with each token) 5) transform each sample using this index, either into a vector of ints or a dense float vector.", "View source", "Fits the state of the preprocessing layer to the dataset.", "Overrides the default adapt method to apply relevant preprocessing to the inputs before passing to the combiner.", "View source", "View source", "Sets vocabulary (and optionally document frequency) data for this layer.", "This method sets the vocabulary and DF data for this layer directly, instead of analyzing a dataset through 'adapt'. It should be used whenever the vocab (and optionally document frequency) information is already known. If vocabulary data is already present in the layer, this method will replace it."]}, {"name": "tf.compat.v1.keras.layers.GRU", "path": "compat/v1/keras/layers/gru", "type": "tf.compat", "text": ["Gated Recurrent Unit - Cho et al. 2014.", "Inherits From: RNN, Layer, Module", "There are two variants. The default one is based on 1406.1078v3 and has reset gate applied to hidden state before matrix multiplication. The other one is based on original 1406.1078v1 and has the order reversed.", "The second variant is compatible with CuDNNGRU (GPU-only) and allows inference on CPU. Thus it has separate biases for kernel and recurrent_kernel. Use 'reset_after'=True and recurrent_activation='sigmoid'.", "View source", "Reset the recorded states for the stateful RNN layer.", "Can only be used when RNN layer is constructed with stateful = True. Args: states: Numpy arrays that contains the value for the initial state, which will be feed to cell at the first time step. When the value is None, zero filled numpy array will be created based on the cell state size."]}, {"name": "tf.compat.v1.keras.layers.GRUCell", "path": "compat/v1/keras/layers/grucell", "type": "tf.compat", "text": ["Cell class for the GRU layer.", "Inherits From: Layer, Module", "View source", "Get the dropout mask for RNN cell's input.", "It will create mask based on context if there isn't any existing cached mask. If a new mask is generated, it will update the cache in the cell.", "View source", "View source", "Get the recurrent dropout mask for RNN cell.", "It will create mask based on context if there isn't any existing cached mask. If a new mask is generated, it will update the cache in the cell.", "View source", "Reset the cached dropout masks if any.", "This is important for the RNN layer to invoke this in it call() method so that the cached mask is cleared before calling the cell.call(). The mask should be cached across the timestep within the same batch, but shouldn't be cached between batches. Otherwise it will introduce unreasonable bias against certain index of data within the batch.", "View source", "Reset the cached recurrent dropout masks if any.", "This is important for the RNN layer to invoke this in it call() method so that the cached mask is cleared before calling the cell.call(). The mask should be cached across the timestep within the same batch, but shouldn't be cached between batches. Otherwise it will introduce unreasonable bias against certain index of data within the batch."]}, {"name": "tf.compat.v1.keras.layers.LSTM", "path": "compat/v1/keras/layers/lstm", "type": "tf.compat", "text": ["Long Short-Term Memory layer - Hochreiter 1997.", "Inherits From: RNN, Layer, Module", "Note that this cell is not optimized for performance on GPU. Please use tf.compat.v1.keras.layers.CuDNNLSTM for better performance on GPU.", "View source", "Reset the recorded states for the stateful RNN layer.", "Can only be used when RNN layer is constructed with stateful = True. Args: states: Numpy arrays that contains the value for the initial state, which will be feed to cell at the first time step. When the value is None, zero filled numpy array will be created based on the cell state size."]}, {"name": "tf.compat.v1.keras.layers.LSTMCell", "path": "compat/v1/keras/layers/lstmcell", "type": "tf.compat", "text": ["Cell class for the LSTM layer.", "Inherits From: Layer, Module", "View source", "Get the dropout mask for RNN cell's input.", "It will create mask based on context if there isn't any existing cached mask. If a new mask is generated, it will update the cache in the cell.", "View source", "View source", "Get the recurrent dropout mask for RNN cell.", "It will create mask based on context if there isn't any existing cached mask. If a new mask is generated, it will update the cache in the cell.", "View source", "Reset the cached dropout masks if any.", "This is important for the RNN layer to invoke this in it call() method so that the cached mask is cleared before calling the cell.call(). The mask should be cached across the timestep within the same batch, but shouldn't be cached between batches. Otherwise it will introduce unreasonable bias against certain index of data within the batch.", "View source", "Reset the cached recurrent dropout masks if any.", "This is important for the RNN layer to invoke this in it call() method so that the cached mask is cleared before calling the cell.call(). The mask should be cached across the timestep within the same batch, but shouldn't be cached between batches. Otherwise it will introduce unreasonable bias against certain index of data within the batch."]}, {"name": "tf.compat.v1.keras.losses", "path": "compat/v1/keras/losses", "type": "tf.compat", "text": ["Built-in loss functions.", "class BinaryCrossentropy: Computes the cross-entropy loss between true labels and predicted labels.", "class CategoricalCrossentropy: Computes the crossentropy loss between the labels and predictions.", "class CategoricalHinge: Computes the categorical hinge loss between y_true and y_pred.", "class CosineSimilarity: Computes the cosine similarity between labels and predictions.", "class Hinge: Computes the hinge loss between y_true and y_pred.", "class Huber: Computes the Huber loss between y_true and y_pred.", "class KLDivergence: Computes Kullback-Leibler divergence loss between y_true and y_pred.", "class LogCosh: Computes the logarithm of the hyperbolic cosine of the prediction error.", "class Loss: Loss base class.", "class MeanAbsoluteError: Computes the mean of absolute difference between labels and predictions.", "class MeanAbsolutePercentageError: Computes the mean absolute percentage error between y_true and y_pred.", "class MeanSquaredError: Computes the mean of squares of errors between labels and predictions.", "class MeanSquaredLogarithmicError: Computes the mean squared logarithmic error between y_true and y_pred.", "class Poisson: Computes the Poisson loss between y_true and y_pred.", "class SparseCategoricalCrossentropy: Computes the crossentropy loss between the labels and predictions.", "class SquaredHinge: Computes the squared hinge loss between y_true and y_pred.", "KLD(...): Computes Kullback-Leibler divergence loss between y_true and y_pred.", "MAE(...): Computes the mean absolute error between labels and predictions.", "MAPE(...): Computes the mean absolute percentage error between y_true and y_pred.", "MSE(...): Computes the mean squared error between labels and predictions.", "MSLE(...): Computes the mean squared logarithmic error between y_true and y_pred.", "binary_crossentropy(...): Computes the binary crossentropy loss.", "categorical_crossentropy(...): Computes the categorical crossentropy loss.", "categorical_hinge(...): Computes the categorical hinge loss between y_true and y_pred.", "cosine(...): Computes the cosine similarity between labels and predictions.", "cosine_proximity(...): Computes the cosine similarity between labels and predictions.", "cosine_similarity(...): Computes the cosine similarity between labels and predictions.", "deserialize(...): Deserializes a serialized loss class/function instance.", "get(...): Retrieves a Keras loss as a function/Loss class instance.", "hinge(...): Computes the hinge loss between y_true and y_pred.", "kl_divergence(...): Computes Kullback-Leibler divergence loss between y_true and y_pred.", "kld(...): Computes Kullback-Leibler divergence loss between y_true and y_pred.", "kullback_leibler_divergence(...): Computes Kullback-Leibler divergence loss between y_true and y_pred.", "log_cosh(...): Logarithm of the hyperbolic cosine of the prediction error.", "logcosh(...): Logarithm of the hyperbolic cosine of the prediction error.", "mae(...): Computes the mean absolute error between labels and predictions.", "mape(...): Computes the mean absolute percentage error between y_true and y_pred.", "mean_absolute_error(...): Computes the mean absolute error between labels and predictions.", "mean_absolute_percentage_error(...): Computes the mean absolute percentage error between y_true and y_pred.", "mean_squared_error(...): Computes the mean squared error between labels and predictions.", "mean_squared_logarithmic_error(...): Computes the mean squared logarithmic error between y_true and y_pred.", "mse(...): Computes the mean squared error between labels and predictions.", "msle(...): Computes the mean squared logarithmic error between y_true and y_pred.", "poisson(...): Computes the Poisson loss between y_true and y_pred.", "serialize(...): Serializes loss function or Loss instance.", "sparse_categorical_crossentropy(...): Computes the sparse categorical crossentropy loss.", "squared_hinge(...): Computes the squared hinge loss between y_true and y_pred."]}, {"name": "tf.compat.v1.keras.metrics", "path": "compat/v1/keras/metrics", "type": "tf.compat", "text": ["Built-in metrics.", "class AUC: Computes the approximate AUC (Area under the curve) via a Riemann sum.", "class Accuracy: Calculates how often predictions equal labels.", "class BinaryAccuracy: Calculates how often predictions match binary labels.", "class BinaryCrossentropy: Computes the crossentropy metric between the labels and predictions.", "class CategoricalAccuracy: Calculates how often predictions matches one-hot labels.", "class CategoricalCrossentropy: Computes the crossentropy metric between the labels and predictions.", "class CategoricalHinge: Computes the categorical hinge metric between y_true and y_pred.", "class CosineSimilarity: Computes the cosine similarity between the labels and predictions.", "class FalseNegatives: Calculates the number of false negatives.", "class FalsePositives: Calculates the number of false positives.", "class Hinge: Computes the hinge metric between y_true and y_pred.", "class KLDivergence: Computes Kullback-Leibler divergence metric between y_true and y_pred.", "class LogCoshError: Computes the logarithm of the hyperbolic cosine of the prediction error.", "class Mean: Computes the (weighted) mean of the given values.", "class MeanAbsoluteError: Computes the mean absolute error between the labels and predictions.", "class MeanAbsolutePercentageError: Computes the mean absolute percentage error between y_true and y_pred.", "class MeanIoU: Computes the mean Intersection-Over-Union metric.", "class MeanRelativeError: Computes the mean relative error by normalizing with the given values.", "class MeanSquaredError: Computes the mean squared error between y_true and y_pred.", "class MeanSquaredLogarithmicError: Computes the mean squared logarithmic error between y_true and y_pred.", "class MeanTensor: Computes the element-wise (weighted) mean of the given tensors.", "class Metric: Encapsulates metric logic and state.", "class Poisson: Computes the Poisson metric between y_true and y_pred.", "class Precision: Computes the precision of the predictions with respect to the labels.", "class PrecisionAtRecall: Computes best precision where recall is >= specified value.", "class Recall: Computes the recall of the predictions with respect to the labels.", "class RecallAtPrecision: Computes best recall where precision is >= specified value.", "class RootMeanSquaredError: Computes root mean squared error metric between y_true and y_pred.", "class SensitivityAtSpecificity: Computes best sensitivity where specificity is >= specified value.", "class SparseCategoricalAccuracy: Calculates how often predictions matches integer labels.", "class SparseCategoricalCrossentropy: Computes the crossentropy metric between the labels and predictions.", "class SparseTopKCategoricalAccuracy: Computes how often integer targets are in the top K predictions.", "class SpecificityAtSensitivity: Computes best specificity where sensitivity is >= specified value.", "class SquaredHinge: Computes the squared hinge metric between y_true and y_pred.", "class Sum: Computes the (weighted) sum of the given values.", "class TopKCategoricalAccuracy: Computes how often targets are in the top K predictions.", "class TrueNegatives: Calculates the number of true negatives.", "class TruePositives: Calculates the number of true positives.", "KLD(...): Computes Kullback-Leibler divergence loss between y_true and y_pred.", "MAE(...): Computes the mean absolute error between labels and predictions.", "MAPE(...): Computes the mean absolute percentage error between y_true and y_pred.", "MSE(...): Computes the mean squared error between labels and predictions.", "MSLE(...): Computes the mean squared logarithmic error between y_true and y_pred.", "binary_accuracy(...): Calculates how often predictions matches binary labels.", "binary_crossentropy(...): Computes the binary crossentropy loss.", "categorical_accuracy(...): Calculates how often predictions matches one-hot labels.", "categorical_crossentropy(...): Computes the categorical crossentropy loss.", "cosine(...): Computes the cosine similarity between labels and predictions.", "cosine_proximity(...): Computes the cosine similarity between labels and predictions.", "deserialize(...): Deserializes a serialized metric class/function instance.", "get(...): Retrieves a Keras metric as a function/Metric class instance.", "hinge(...): Computes the hinge loss between y_true and y_pred.", "kl_divergence(...): Computes Kullback-Leibler divergence loss between y_true and y_pred.", "kld(...): Computes Kullback-Leibler divergence loss between y_true and y_pred.", "kullback_leibler_divergence(...): Computes Kullback-Leibler divergence loss between y_true and y_pred.", "log_cosh(...): Logarithm of the hyperbolic cosine of the prediction error.", "logcosh(...): Logarithm of the hyperbolic cosine of the prediction error.", "mae(...): Computes the mean absolute error between labels and predictions.", "mape(...): Computes the mean absolute percentage error between y_true and y_pred.", "mean_absolute_error(...): Computes the mean absolute error between labels and predictions.", "mean_absolute_percentage_error(...): Computes the mean absolute percentage error between y_true and y_pred.", "mean_squared_error(...): Computes the mean squared error between labels and predictions.", "mean_squared_logarithmic_error(...): Computes the mean squared logarithmic error between y_true and y_pred.", "mse(...): Computes the mean squared error between labels and predictions.", "msle(...): Computes the mean squared logarithmic error between y_true and y_pred.", "poisson(...): Computes the Poisson loss between y_true and y_pred.", "serialize(...): Serializes metric function or Metric instance.", "sparse_categorical_accuracy(...): Calculates how often predictions matches integer labels.", "sparse_categorical_crossentropy(...): Computes the sparse categorical crossentropy loss.", "sparse_top_k_categorical_accuracy(...): Computes how often integer targets are in the top K predictions.", "squared_hinge(...): Computes the squared hinge loss between y_true and y_pred.", "top_k_categorical_accuracy(...): Computes how often targets are in the top K predictions."]}, {"name": "tf.compat.v1.keras.mixed_precision", "path": "compat/v1/keras/mixed_precision", "type": "tf.compat", "text": ["Keras mixed precision API.", "See the mixed precision guide to learn how to use the API.", "experimental module: Public API for tf.keras.mixed_precision.experimental namespace.", "class LossScaleOptimizer: An optimizer that applies loss scaling to prevent numeric underflow."]}, {"name": "tf.compat.v1.keras.mixed_precision.experimental", "path": "compat/v1/keras/mixed_precision/experimental", "type": "tf.compat", "text": ["Public API for tf.keras.mixed_precision.experimental namespace.", "class LossScaleOptimizer: An deprecated optimizer that applies loss scaling."]}, {"name": "tf.compat.v1.keras.models", "path": "compat/v1/keras/models", "type": "tf.compat", "text": ["Code for model cloning, plus model-related API entries.", "class Model: Model groups layers into an object with training and inference features.", "class Sequential: Sequential groups a linear stack of layers into a tf.keras.Model.", "clone_model(...): Clone any Model instance.", "load_model(...): Loads a model saved via model.save().", "model_from_config(...): Instantiates a Keras model from its config.", "model_from_json(...): Parses a JSON model configuration string and returns a model instance.", "model_from_yaml(...): Parses a yaml model configuration file and returns a model instance.", "save_model(...): Saves a model as a TensorFlow SavedModel or HDF5 file."]}, {"name": "tf.compat.v1.keras.optimizers", "path": "compat/v1/keras/optimizers", "type": "tf.compat", "text": ["Built-in optimizer classes.", "For more examples see the base class tf.keras.optimizers.Optimizer.", "schedules module: Public API for tf.keras.optimizers.schedules namespace.", "class Adadelta: Optimizer that implements the Adadelta algorithm.", "class Adagrad: Optimizer that implements the Adagrad algorithm.", "class Adam: Optimizer that implements the Adam algorithm.", "class Adamax: Optimizer that implements the Adamax algorithm.", "class Ftrl: Optimizer that implements the FTRL algorithm.", "class Nadam: Optimizer that implements the NAdam algorithm.", "class Optimizer: Base class for Keras optimizers.", "class RMSprop: Optimizer that implements the RMSprop algorithm.", "class SGD: Gradient descent (with momentum) optimizer.", "deserialize(...): Inverse of the serialize function.", "get(...): Retrieves a Keras Optimizer instance.", "serialize(...)"]}, {"name": "tf.compat.v1.keras.optimizers.schedules", "path": "compat/v1/keras/optimizers/schedules", "type": "tf.compat", "text": ["Public API for tf.keras.optimizers.schedules namespace.", "class ExponentialDecay: A LearningRateSchedule that uses an exponential decay schedule.", "class InverseTimeDecay: A LearningRateSchedule that uses an inverse time decay schedule.", "class LearningRateSchedule: A serializable learning rate decay schedule.", "class PiecewiseConstantDecay: A LearningRateSchedule that uses a piecewise constant decay schedule.", "class PolynomialDecay: A LearningRateSchedule that uses a polynomial decay schedule.", "deserialize(...)", "serialize(...)"]}, {"name": "tf.compat.v1.keras.preprocessing", "path": "compat/v1/keras/preprocessing", "type": "tf.compat", "text": ["Keras data preprocessing utils.", "image module: Set of tools for real-time data augmentation on image data.", "sequence module: Utilities for preprocessing sequence data.", "text module: Utilities for text input preprocessing."]}, {"name": "tf.compat.v1.keras.preprocessing.image", "path": "compat/v1/keras/preprocessing/image", "type": "tf.compat", "text": ["Set of tools for real-time data augmentation on image data.", "class DirectoryIterator: Iterator capable of reading images from a directory on disk.", "class ImageDataGenerator: Generate batches of tensor image data with real-time data augmentation.", "class Iterator: Base class for image data iterators.", "class NumpyArrayIterator: Iterator yielding data from a Numpy array.", "apply_affine_transform(...): Applies an affine transformation specified by the parameters given.", "apply_brightness_shift(...): Performs a brightness shift.", "apply_channel_shift(...): Performs a channel shift.", "array_to_img(...): Converts a 3D Numpy array to a PIL Image instance.", "img_to_array(...): Converts a PIL Image instance to a Numpy array.", "load_img(...): Loads an image into PIL format.", "random_brightness(...): Performs a random brightness shift.", "random_channel_shift(...): Performs a random channel shift.", "random_rotation(...): Performs a random rotation of a Numpy image tensor.", "random_shear(...): Performs a random spatial shear of a Numpy image tensor.", "random_shift(...): Performs a random spatial shift of a Numpy image tensor.", "random_zoom(...): Performs a random spatial zoom of a Numpy image tensor.", "save_img(...): Saves an image stored as a Numpy array to a path or file object."]}, {"name": "tf.compat.v1.keras.preprocessing.sequence", "path": "compat/v1/keras/preprocessing/sequence", "type": "tf.compat", "text": ["Utilities for preprocessing sequence data.", "class TimeseriesGenerator: Utility class for generating batches of temporal data.", "make_sampling_table(...): Generates a word rank-based probabilistic sampling table.", "pad_sequences(...): Pads sequences to the same length.", "skipgrams(...): Generates skipgram word pairs."]}, {"name": "tf.compat.v1.keras.preprocessing.text", "path": "compat/v1/keras/preprocessing/text", "type": "tf.compat", "text": ["Utilities for text input preprocessing.", "class Tokenizer: Text tokenization utility class.", "hashing_trick(...): Converts a text to a sequence of indexes in a fixed-size hashing space.", "one_hot(...): One-hot encodes a text into a list of word indexes of size n.", "text_to_word_sequence(...): Converts a text to a sequence of words (or tokens).", "tokenizer_from_json(...): Parses a JSON tokenizer configuration file and returns a"]}, {"name": "tf.compat.v1.keras.regularizers", "path": "compat/v1/keras/regularizers", "type": "tf.compat", "text": ["Built-in regularizers.", "class L1: A regularizer that applies a L1 regularization penalty.", "class L1L2: A regularizer that applies both L1 and L2 regularization penalties.", "class L2: A regularizer that applies a L2 regularization penalty.", "class Regularizer: Regularizer base class.", "class l1: A regularizer that applies a L1 regularization penalty.", "class l2: A regularizer that applies a L2 regularization penalty.", "deserialize(...)", "get(...): Retrieve a regularizer instance from a config or identifier.", "l1_l2(...): Create a regularizer that applies both L1 and L2 penalties.", "serialize(...)"]}, {"name": "tf.compat.v1.keras.utils", "path": "compat/v1/keras/utils", "type": "tf.compat", "text": ["Public API for tf.keras.utils namespace.", "class CustomObjectScope: Exposes custom classes/functions to Keras deserialization internals.", "class GeneratorEnqueuer: Builds a queue out of a data generator.", "class OrderedEnqueuer: Builds a Enqueuer from a Sequence.", "class Progbar: Displays a progress bar.", "class Sequence: Base object for fitting to a sequence of data, such as a dataset.", "class SequenceEnqueuer: Base class to enqueue inputs.", "class custom_object_scope: Exposes custom classes/functions to Keras deserialization internals.", "deserialize_keras_object(...): Turns the serialized form of a Keras object back into an actual object.", "get_custom_objects(...): Retrieves a live reference to the global dictionary of custom objects.", "get_file(...): Downloads a file from a URL if it not already in the cache.", "get_registered_name(...): Returns the name registered to an object within the Keras framework.", "get_registered_object(...): Returns the class associated with name if it is registered with Keras.", "get_source_inputs(...): Returns the list of input tensors necessary to compute tensor.", "model_to_dot(...): Convert a Keras model to dot format.", "normalize(...): Normalizes a Numpy array.", "plot_model(...): Converts a Keras model to dot format and save to a file.", "register_keras_serializable(...): Registers an object with the Keras serialization framework.", "serialize_keras_object(...): Serialize a Keras object into a JSON-compatible representation.", "to_categorical(...): Converts a class vector (integers) to binary class matrix."]}, {"name": "tf.compat.v1.keras.wrappers", "path": "compat/v1/keras/wrappers", "type": "tf.compat", "text": ["Public API for tf.keras.wrappers namespace.", "scikit_learn module: Wrapper for using the Scikit-Learn API with Keras models."]}, {"name": "tf.compat.v1.keras.wrappers.scikit_learn", "path": "compat/v1/keras/wrappers/scikit_learn", "type": "tf.compat", "text": ["Wrapper for using the Scikit-Learn API with Keras models.", "class KerasClassifier: Implementation of the scikit-learn classifier API for Keras.", "class KerasRegressor: Implementation of the scikit-learn regressor API for Keras."]}, {"name": "tf.compat.v1.layers", "path": "compat/v1/layers", "type": "tf.compat", "text": ["Public API for tf.layers namespace.", "experimental module: Public API for tf.layers.experimental namespace.", "class AveragePooling1D: Average Pooling layer for 1D inputs.", "class AveragePooling2D: Average pooling layer for 2D inputs (e.g. images).", "class AveragePooling3D: Average pooling layer for 3D inputs (e.g. volumes).", "class BatchNormalization: Batch Normalization layer from (Ioffe et al., 2015).", "class Conv1D: 1D convolution layer (e.g. temporal convolution).", "class Conv2D: 2D convolution layer (e.g. spatial convolution over images).", "class Conv2DTranspose: Transposed 2D convolution layer (sometimes called 2D Deconvolution).", "class Conv3D: 3D convolution layer (e.g. spatial convolution over volumes).", "class Conv3DTranspose: Transposed 3D convolution layer (sometimes called 3D Deconvolution).", "class Dense: Densely-connected layer class.", "class Dropout: Applies Dropout to the input.", "class Flatten: Flattens an input tensor while preserving the batch axis (axis 0).", "class InputSpec: Specifies the rank, dtype and shape of every input to a layer.", "class Layer: Base layer class.", "class MaxPooling1D: Max Pooling layer for 1D inputs.", "class MaxPooling2D: Max pooling layer for 2D inputs (e.g. images).", "class MaxPooling3D: Max pooling layer for 3D inputs (e.g. volumes).", "class SeparableConv1D: Depthwise separable 1D convolution.", "class SeparableConv2D: Depthwise separable 2D convolution.", "average_pooling1d(...): Average Pooling layer for 1D inputs.", "average_pooling2d(...): Average pooling layer for 2D inputs (e.g. images).", "average_pooling3d(...): Average pooling layer for 3D inputs (e.g. volumes).", "batch_normalization(...): Functional interface for the batch normalization layer from_config(Ioffe et al., 2015).", "conv1d(...): Functional interface for 1D convolution layer (e.g. temporal convolution).", "conv2d(...): Functional interface for the 2D convolution layer.", "conv2d_transpose(...): Functional interface for transposed 2D convolution layer.", "conv3d(...): Functional interface for the 3D convolution layer.", "conv3d_transpose(...): Functional interface for transposed 3D convolution layer.", "dense(...): Functional interface for the densely-connected layer.", "dropout(...): Applies Dropout to the input.", "flatten(...): Flattens an input tensor while preserving the batch axis (axis 0).", "max_pooling1d(...): Max Pooling layer for 1D inputs.", "max_pooling2d(...): Max pooling layer for 2D inputs (e.g. images).", "max_pooling3d(...): Max pooling layer for 3D inputs (e.g.", "separable_conv1d(...): Functional interface for the depthwise separable 1D convolution layer.", "separable_conv2d(...): Functional interface for the depthwise separable 2D convolution layer."]}, {"name": "tf.compat.v1.layers.AveragePooling1D", "path": "compat/v1/layers/averagepooling1d", "type": "tf.compat", "text": ["Average Pooling layer for 1D inputs.", "Inherits From: AveragePooling1D, Layer, Layer, Module"]}, {"name": "tf.compat.v1.layers.AveragePooling2D", "path": "compat/v1/layers/averagepooling2d", "type": "tf.compat", "text": ["Average pooling layer for 2D inputs (e.g. images).", "Inherits From: AveragePooling2D, Layer, Layer, Module"]}, {"name": "tf.compat.v1.layers.AveragePooling3D", "path": "compat/v1/layers/averagepooling3d", "type": "tf.compat", "text": ["Average pooling layer for 3D inputs (e.g. volumes).", "Inherits From: AveragePooling3D, Layer, Layer, Module"]}, {"name": "tf.compat.v1.layers.average_pooling1d", "path": "compat/v1/layers/average_pooling1d", "type": "tf.compat", "text": ["Average Pooling layer for 1D inputs."]}, {"name": "tf.compat.v1.layers.average_pooling2d", "path": "compat/v1/layers/average_pooling2d", "type": "tf.compat", "text": ["Average pooling layer for 2D inputs (e.g. images)."]}, {"name": "tf.compat.v1.layers.average_pooling3d", "path": "compat/v1/layers/average_pooling3d", "type": "tf.compat", "text": ["Average pooling layer for 3D inputs (e.g. volumes)."]}, {"name": "tf.compat.v1.layers.BatchNormalization", "path": "compat/v1/layers/batchnormalization", "type": "tf.compat", "text": ["Batch Normalization layer from (Ioffe et al., 2015).", "Inherits From: BatchNormalization, Layer, Layer, Module", "Keras APIs handle BatchNormalization updates to the moving_mean and moving_variance as part of their fit() and evaluate() loops. However, if a custom training loop is used with an instance of Model, these updates need to be explicitly included. Here's a simple example of how it can be done:", "Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift: Ioffe et al., 2015 (pdf) Batch Renormalization - Towards Reducing Minibatch Dependence in Batch-Normalized Models: Ioffe, 2017 (pdf)"]}, {"name": "tf.compat.v1.layers.batch_normalization", "path": "compat/v1/layers/batch_normalization", "type": "tf.compat", "text": ["Functional interface for the batch normalization layer from_config(Ioffe et al., 2015).", "Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift: Ioffe et al., 2015 (pdf) Batch Renormalization - Towards Reducing Minibatch Dependence in Batch-Normalized Models: Ioffe, 2017 (pdf)"]}, {"name": "tf.compat.v1.layers.Conv1D", "path": "compat/v1/layers/conv1d", "type": "tf.compat", "text": ["1D convolution layer (e.g. temporal convolution).", "Inherits From: Conv1D, Layer, Layer, Module", "This layer creates a convolution kernel that is convolved (actually cross-correlated) with the layer input to produce a tensor of outputs. If use_bias is True (and a bias_initializer is provided), a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well."]}, {"name": "tf.compat.v1.layers.Conv2D", "path": "compat/v1/layers/conv2d", "type": "tf.compat", "text": ["2D convolution layer (e.g. spatial convolution over images).", "Inherits From: Conv2D, Layer, Layer, Module", "This layer creates a convolution kernel that is convolved (actually cross-correlated) with the layer input to produce a tensor of outputs. If use_bias is True (and a bias_initializer is provided), a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well."]}, {"name": "tf.compat.v1.layers.Conv2DTranspose", "path": "compat/v1/layers/conv2dtranspose", "type": "tf.compat", "text": ["Transposed 2D convolution layer (sometimes called 2D Deconvolution).", "Inherits From: Conv2DTranspose, Conv2D, Layer, Layer, Module", "The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution."]}, {"name": "tf.compat.v1.layers.conv2d_transpose", "path": "compat/v1/layers/conv2d_transpose", "type": "tf.compat", "text": ["Functional interface for transposed 2D convolution layer.", "The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution."]}, {"name": "tf.compat.v1.layers.Conv3D", "path": "compat/v1/layers/conv3d", "type": "tf.compat", "text": ["3D convolution layer (e.g. spatial convolution over volumes).", "Inherits From: Conv3D, Layer, Layer, Module", "This layer creates a convolution kernel that is convolved (actually cross-correlated) with the layer input to produce a tensor of outputs. If use_bias is True (and a bias_initializer is provided), a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well."]}, {"name": "tf.compat.v1.layers.Conv3DTranspose", "path": "compat/v1/layers/conv3dtranspose", "type": "tf.compat", "text": ["Transposed 3D convolution layer (sometimes called 3D Deconvolution).", "Inherits From: Conv3DTranspose, Conv3D, Layer, Layer, Module"]}, {"name": "tf.compat.v1.layers.conv3d_transpose", "path": "compat/v1/layers/conv3d_transpose", "type": "tf.compat", "text": ["Functional interface for transposed 3D convolution layer."]}, {"name": "tf.compat.v1.layers.Dense", "path": "compat/v1/layers/dense", "type": "tf.compat", "text": ["Densely-connected layer class.", "Inherits From: Dense, Layer, Layer, Module", "This layer implements the operation: outputs = activation(inputs * kernel + bias) Where activation is the activation function passed as the activation argument (if not None), kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only if use_bias is True)."]}, {"name": "tf.compat.v1.layers.Dropout", "path": "compat/v1/layers/dropout", "type": "tf.compat", "text": ["Applies Dropout to the input.", "Inherits From: Dropout, Layer, Layer, Module", "Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting. The units that are kept are scaled by 1 / (1 - rate), so that their sum is unchanged at training time and inference time."]}, {"name": "tf.compat.v1.layers.experimental", "path": "compat/v1/layers/experimental", "type": "tf.compat", "text": ["Public API for tf.layers.experimental namespace.", "keras_style_scope(...): Use Keras-style variable management.", "set_keras_style(...): Use Keras-style variable management."]}, {"name": "tf.compat.v1.layers.experimental.keras_style_scope", "path": "compat/v1/layers/experimental/keras_style_scope", "type": "tf.compat", "text": ["Use Keras-style variable management.", "All tf.layers and tf RNN cells created in this scope use Keras-style variable management. Creating such layers with a scope= argument is disallowed, and reuse=True is disallowed.", "The purpose of this scope is to allow users of existing layers to slowly transition to a Keras layers API without breaking existing functionality.", "One example of this is when using TensorFlow's RNN classes with Keras Models or Networks. Because Keras models do not properly set variable scopes, users of RNNs may either accidentally share scopes between two different models, or get errors about variables that already exist.", "The solution is to wrap the model construction and execution in a keras-style scope:"]}, {"name": "tf.compat.v1.layers.experimental.set_keras_style", "path": "compat/v1/layers/experimental/set_keras_style", "type": "tf.compat", "text": ["Use Keras-style variable management.", "All tf.layers and tf RNN cells created after keras style ha been enabled use Keras-style variable management. Creating such layers with a scope= argument is disallowed, and reuse=True is disallowed.", "The purpose of this function is to allow users of existing layers to slowly transition to Keras layers API without breaking existing functionality.", "For more details, see the documentation for keras_style_scope.", "Note, once keras style has been set, it is set globally for the entire program and cannot be unset."]}, {"name": "tf.compat.v1.layers.Flatten", "path": "compat/v1/layers/flatten", "type": "tf.compat", "text": ["Flattens an input tensor while preserving the batch axis (axis 0).", "Inherits From: Flatten, Layer, Layer, Module"]}, {"name": "tf.compat.v1.layers.Layer", "path": "compat/v1/layers/layer", "type": "tf.compat", "text": ["Base layer class.", "Inherits From: Layer, Module", "It is considered legacy, and we recommend the use of tf.keras.layers.Layer instead.", "Read-only properties: name: The name of the layer (string). dtype: Default dtype of the layer's weights (default of None means use the type of the first input). trainable_variables: List of trainable variables. non_trainable_variables: List of non-trainable variables. variables: List of all variables of this layer, trainable and non-trainable. updates: List of update ops of this layer. losses: List of losses added by this layer. trainable_weights: List of variables to be included in backprop. non_trainable_weights: List of variables that should not be included in backprop. weights: The concatenation of the lists trainable_weights and non_trainable_weights (in this order)."]}, {"name": "tf.compat.v1.layers.MaxPooling1D", "path": "compat/v1/layers/maxpooling1d", "type": "tf.compat", "text": ["Max Pooling layer for 1D inputs.", "Inherits From: MaxPool1D, Layer, Layer, Module"]}, {"name": "tf.compat.v1.layers.MaxPooling2D", "path": "compat/v1/layers/maxpooling2d", "type": "tf.compat", "text": ["Max pooling layer for 2D inputs (e.g. images).", "Inherits From: MaxPool2D, Layer, Layer, Module"]}, {"name": "tf.compat.v1.layers.MaxPooling3D", "path": "compat/v1/layers/maxpooling3d", "type": "tf.compat", "text": ["Max pooling layer for 3D inputs (e.g. volumes).", "Inherits From: MaxPool3D, Layer, Layer, Module"]}, {"name": "tf.compat.v1.layers.max_pooling1d", "path": "compat/v1/layers/max_pooling1d", "type": "tf.compat", "text": ["Max Pooling layer for 1D inputs."]}, {"name": "tf.compat.v1.layers.max_pooling2d", "path": "compat/v1/layers/max_pooling2d", "type": "tf.compat", "text": ["Max pooling layer for 2D inputs (e.g. images)."]}, {"name": "tf.compat.v1.layers.max_pooling3d", "path": "compat/v1/layers/max_pooling3d", "type": "tf.compat", "text": ["Max pooling layer for 3D inputs (e.g.", "volumes)."]}, {"name": "tf.compat.v1.layers.SeparableConv1D", "path": "compat/v1/layers/separableconv1d", "type": "tf.compat", "text": ["Depthwise separable 1D convolution.", "Inherits From: SeparableConv1D, Layer, Layer, Module", "This layer performs a depthwise convolution that acts separately on channels, followed by a pointwise convolution that mixes channels. If use_bias is True and a bias initializer is provided, it adds a bias vector to the output. It then optionally applies an activation function to produce the final output."]}, {"name": "tf.compat.v1.layers.SeparableConv2D", "path": "compat/v1/layers/separableconv2d", "type": "tf.compat", "text": ["Depthwise separable 2D convolution.", "Inherits From: SeparableConv2D, Layer, Layer, Module", "This layer performs a depthwise convolution that acts separately on channels, followed by a pointwise convolution that mixes channels. If use_bias is True and a bias initializer is provided, it adds a bias vector to the output. It then optionally applies an activation function to produce the final output."]}, {"name": "tf.compat.v1.layers.separable_conv1d", "path": "compat/v1/layers/separable_conv1d", "type": "tf.compat", "text": ["Functional interface for the depthwise separable 1D convolution layer.", "This layer performs a depthwise convolution that acts separately on channels, followed by a pointwise convolution that mixes channels. If use_bias is True and a bias initializer is provided, it adds a bias vector to the output. It then optionally applies an activation function to produce the final output."]}, {"name": "tf.compat.v1.layers.separable_conv2d", "path": "compat/v1/layers/separable_conv2d", "type": "tf.compat", "text": ["Functional interface for the depthwise separable 2D convolution layer.", "This layer performs a depthwise convolution that acts separately on channels, followed by a pointwise convolution that mixes channels. If use_bias is True and a bias initializer is provided, it adds a bias vector to the output. It then optionally applies an activation function to produce the final output."]}, {"name": "tf.compat.v1.linalg", "path": "compat/v1/linalg", "type": "tf.compat", "text": ["Operations for linear algebra.", "experimental module: Public API for tf.linalg.experimental namespace.", "class LinearOperator: Base class defining a [batch of] linear operator[s].", "class LinearOperatorAdjoint: LinearOperator representing the adjoint of another operator.", "class LinearOperatorBlockDiag: Combines one or more LinearOperators in to a Block Diagonal matrix.", "class LinearOperatorBlockLowerTriangular: Combines LinearOperators into a blockwise lower-triangular matrix.", "class LinearOperatorCirculant: LinearOperator acting like a circulant matrix.", "class LinearOperatorCirculant2D: LinearOperator acting like a block circulant matrix.", "class LinearOperatorCirculant3D: LinearOperator acting like a nested block circulant matrix.", "class LinearOperatorComposition: Composes one or more LinearOperators.", "class LinearOperatorDiag: LinearOperator acting like a [batch] square diagonal matrix.", "class LinearOperatorFullMatrix: LinearOperator that wraps a [batch] matrix.", "class LinearOperatorHouseholder: LinearOperator acting like a [batch] of Householder transformations.", "class LinearOperatorIdentity: LinearOperator acting like a [batch] square identity matrix.", "class LinearOperatorInversion: LinearOperator representing the inverse of another operator.", "class LinearOperatorKronecker: Kronecker product between two LinearOperators.", "class LinearOperatorLowRankUpdate: Perturb a LinearOperator with a rank K update.", "class LinearOperatorLowerTriangular: LinearOperator acting like a [batch] square lower triangular matrix.", "class LinearOperatorPermutation: LinearOperator acting like a [batch] of permutation matrices.", "class LinearOperatorScaledIdentity: LinearOperator acting like a scaled [batch] identity matrix A = c I.", "class LinearOperatorToeplitz: LinearOperator acting like a [batch] of toeplitz matrices.", "class LinearOperatorTridiag: LinearOperator acting like a [batch] square tridiagonal matrix.", "class LinearOperatorZeros: LinearOperator acting like a [batch] zero matrix.", "adjoint(...): Transposes the last two dimensions of and conjugates tensor matrix.", "band_part(...): Copy a tensor setting everything outside a central band in each innermost matrix to zero.", "cholesky(...): Computes the Cholesky decomposition of one or more square matrices.", "cholesky_solve(...): Solves systems of linear eqns A X = RHS, given Cholesky factorizations.", "cross(...): Compute the pairwise cross product.", "det(...): Computes the determinant of one or more square matrices.", "diag(...): Returns a batched diagonal tensor with given batched diagonal values.", "diag_part(...): Returns the batched diagonal part of a batched tensor.", "eigh(...): Computes the eigen decomposition of a batch of self-adjoint matrices.", "eigvalsh(...): Computes the eigenvalues of one or more self-adjoint matrices.", "einsum(...): Tensor contraction over specified indices and outer product.", "expm(...): Computes the matrix exponential of one or more square matrices.", "eye(...): Construct an identity matrix, or a batch of matrices.", "global_norm(...): Computes the global norm of multiple tensors.", "inv(...): Computes the inverse of one or more square invertible matrices or their adjoints (conjugate transposes).", "l2_normalize(...): Normalizes along dimension axis using an L2 norm. (deprecated arguments)", "logdet(...): Computes log of the determinant of a hermitian positive definite matrix.", "logm(...): Computes the matrix logarithm of one or more square matrices:", "lstsq(...): Solves one or more linear least-squares problems.", "lu(...): Computes the LU decomposition of one or more square matrices.", "lu_matrix_inverse(...): Computes the inverse given the LU decomposition(s) of one or more matrices.", "lu_reconstruct(...): The reconstruct one or more matrices from their LU decomposition(s).", "lu_solve(...): Solves systems of linear eqns A X = RHS, given LU factorizations.", "matmul(...): Multiplies matrix a by matrix b, producing a * b.", "matrix_rank(...): Compute the matrix rank of one or more matrices.", "matrix_transpose(...): Transposes last two dimensions of tensor a.", "matvec(...): Multiplies matrix a by vector b, producing a * b.", "norm(...): Computes the norm of vectors, matrices, and tensors. (deprecated arguments)", "normalize(...): Normalizes tensor along dimension axis using specified norm.", "pinv(...): Compute the Moore-Penrose pseudo-inverse of one or more matrices.", "qr(...): Computes the QR decompositions of one or more matrices.", "set_diag(...): Returns a batched matrix tensor with new batched diagonal values.", "slogdet(...): Computes the sign and the log of the absolute value of the determinant of", "solve(...): Solves systems of linear equations.", "sqrtm(...): Computes the matrix square root of one or more square matrices:", "svd(...): Computes the singular value decompositions of one or more matrices.", "tensor_diag(...): Returns a diagonal tensor with a given diagonal values.", "tensor_diag_part(...): Returns the diagonal part of the tensor.", "tensordot(...): Tensor contraction of a and b along specified axes and outer product.", "trace(...): Compute the trace of a tensor x.", "transpose(...): Transposes last two dimensions of tensor a.", "triangular_solve(...): Solve systems of linear equations with upper or lower triangular matrices.", "tridiagonal_matmul(...): Multiplies tridiagonal matrix by matrix.", "tridiagonal_solve(...): Solves tridiagonal systems of equations."]}, {"name": "tf.compat.v1.linalg.experimental", "path": "compat/v1/linalg/experimental", "type": "tf.compat", "text": ["Public API for tf.linalg.experimental namespace.", "conjugate_gradient(...): Conjugate gradient solver."]}, {"name": "tf.compat.v1.linalg.l2_normalize", "path": "compat/v1/linalg/l2_normalize", "type": "tf.compat", "text": ["Normalizes along dimension axis using an L2 norm. (deprecated arguments)", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.math.l2_normalize, tf.compat.v1.nn.l2_normalize", "For a 1-D tensor with axis = 0, computes", "For x with more dimensions, independently normalizes each 1-D slice along dimension axis."]}, {"name": "tf.compat.v1.lite", "path": "compat/v1/lite", "type": "tf.compat", "text": ["Public API for tf.lite namespace.", "constants module: Public API for tf.lite.constants namespace.", "experimental module: Public API for tf.lite.experimental namespace.", "class Interpreter: Interpreter interface for TensorFlow Lite Models.", "class OpHint: A class that helps build tflite function invocations.", "class OpsSet: Enum class defining the sets of ops available to generate TFLite models.", "class Optimize: Enum defining the optimizations to apply when generating tflite graphs.", "class RepresentativeDataset: Representative dataset to evaluate optimizations.", "class TFLiteConverter: Convert a TensorFlow model into output_format.", "class TargetSpec: Specification of target device.", "class TocoConverter: Convert a TensorFlow model into output_format using TOCO.", "toco_convert(...): Convert a model using TOCO. (deprecated)"]}, {"name": "tf.compat.v1.lite.constants", "path": "compat/v1/lite/constants", "type": "tf.compat", "text": ["Public API for tf.lite.constants namespace."]}, {"name": "tf.compat.v1.lite.experimental", "path": "compat/v1/lite/experimental", "type": "tf.compat", "text": ["Public API for tf.lite.experimental namespace.", "nn module: Public API for tf.lite.experimental.nn namespace.", "convert_op_hints_to_stubs(...): Converts a graphdef with LiteOp hints into stub operations.", "get_potentially_supported_ops(...): Returns operations potentially supported by TensorFlow Lite.", "load_delegate(...): Returns loaded Delegate object."]}, {"name": "tf.compat.v1.lite.experimental.convert_op_hints_to_stubs", "path": "compat/v1/lite/experimental/convert_op_hints_to_stubs", "type": "tf.compat", "text": ["Converts a graphdef with LiteOp hints into stub operations.", "This is used to prepare for toco conversion of complex intrinsic usages. Note: only one of session or graph_def should be used, not both."]}, {"name": "tf.compat.v1.lite.experimental.get_potentially_supported_ops", "path": "compat/v1/lite/experimental/get_potentially_supported_ops", "type": "tf.compat", "text": ["Returns operations potentially supported by TensorFlow Lite.", "The potentially support list contains a list of ops that are partially or fully supported, which is derived by simply scanning op names to check whether they can be handled without real conversion and specific parameters.", "Given that some ops may be partially supported, the optimal way to determine if a model's operations are supported is by converting using the TensorFlow Lite converter."]}, {"name": "tf.compat.v1.lite.experimental.nn", "path": "compat/v1/lite/experimental/nn", "type": "tf.compat", "text": ["Public API for tf.lite.experimental.nn namespace.", "class TFLiteLSTMCell: Long short-term memory unit (LSTM) recurrent network cell.", "class TfLiteRNNCell: The most basic RNN cell.", "dynamic_rnn(...): Creates a recurrent neural network specified by RNNCell cell."]}, {"name": "tf.compat.v1.lite.experimental.nn.dynamic_rnn", "path": "compat/v1/lite/experimental/nn/dynamic_rnn", "type": "tf.compat", "text": ["Creates a recurrent neural network specified by RNNCell cell.", "Performs fully dynamic unrolling of inputs.", "If time_major == False (default), this will be a Tensor shaped: [batch_size, max_time, cell.output_size].", "If time_major == True, this will be a Tensor shaped: [max_time, batch_size, cell.output_size].", "Note, if cell.output_size is a (possibly nested) tuple of integers or TensorShape objects, then outputs will be a tuple having the same structure as cell.output_size, containing Tensors having shapes corresponding to the shape data in cell.output_size. "]}, {"name": "tf.compat.v1.lite.experimental.nn.TFLiteLSTMCell", "path": "compat/v1/lite/experimental/nn/tflitelstmcell", "type": "tf.compat", "text": ["Long short-term memory unit (LSTM) recurrent network cell.", "Inherits From: RNNCell, Layer, Layer, Module", "This is used only for TfLite, it provides hints and it also makes the variables in the desired for the tflite ops (transposed and separated).", "The default non-peephole implementation is based on:", "https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf", "Felix Gers, Jurgen Schmidhuber, and Fred Cummins. \"Learning to forget: Continual prediction with LSTM.\" IET, 850-855, 1999.", "The peephole implementation is based on:", "https://research.google.com/pubs/archive/43905.pdf", "Hasim Sak, Andrew Senior, and Francoise Beaufays. \"Long short-term memory recurrent neural network architectures for large scale acoustic modeling.\" INTERSPEECH, 2014.", "The class uses optional peep-hole connections, optional cell clipping, and an optional projection layer.", "Note that this cell is not optimized for performance. Please use tf.contrib.cudnn_rnn.CudnnLSTM for better performance on GPU, or tf.contrib.rnn.LSTMBlockCell and tf.contrib.rnn.LSTMBlockFusedCell for better performance on CPU.", "It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. ", "View source", "View source", "Return zero-filled state tensor(s).", "If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size, s] for each s in state_size. "]}, {"name": "tf.compat.v1.lite.experimental.nn.TfLiteRNNCell", "path": "compat/v1/lite/experimental/nn/tfliternncell", "type": "tf.compat", "text": ["The most basic RNN cell.", "Inherits From: RNNCell, Layer, Layer, Module", "This is used only for TfLite, it provides hints and it also makes the variables in the desired for the tflite ops.", "It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. ", "View source", "View source", "Return zero-filled state tensor(s).", "If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size, s] for each s in state_size. "]}, {"name": "tf.compat.v1.lite.OpHint", "path": "compat/v1/lite/ophint", "type": "tf.compat", "text": ["A class that helps build tflite function invocations.", "It allows you to take a bunch of TensorFlow ops and annotate the construction such that toco knows how to convert it to tflite. This embeds a pseudo function in a TensorFlow graph. This allows embedding high-level API usage information in a lower level TensorFlow implementation so that an alternative implementation can be substituted later.", "Essentially, any \"input\" into this pseudo op is fed into an identity, and attributes are added to that input before being used by the constituent ops that make up the pseudo op. A similar process is done to any output that is to be exported from the current op.", "class OpHintArgumentTracker", "View source", "Add a wrapped input argument to the hint.", "View source", "Add a sequence of inputs to the function invocation.", "View source", "Add a wrapped output argument to the hint.", "View source", "Add a sequence of outputs to the function invocation."]}, {"name": "tf.compat.v1.lite.OpHint.OpHintArgumentTracker", "path": "compat/v1/lite/ophint/ophintargumenttracker", "type": "tf.compat", "text": ["Conceptually tracks indices of arguments of \"OpHint functions\".", "The inputs and arguments of these functions both use an instance of the class so they can have independent numbering.", "View source", "Return a wrapped tensor of an input tensor as an argument."]}, {"name": "tf.compat.v1.lite.TFLiteConverter", "path": "compat/v1/lite/tfliteconverter", "type": "tf.compat", "text": ["Convert a TensorFlow model into output_format.", "This is used to convert from a TensorFlow GraphDef, SavedModel or tf.keras model into either a TFLite FlatBuffer or graph visualization.", "View source", "Converts a TensorFlow GraphDef based on instance variables.", "View source", "Creates a TFLiteConverter class from a file containing a frozen GraphDef.", "View source", "Creates a TFLiteConverter class from a tf.keras model file.", "View source", "Creates a TFLiteConverter class from a SavedModel.", "View source", "Creates a TFLiteConverter class from a TensorFlow Session.", "View source", "Returns a list of the names of the input tensors."]}, {"name": "tf.compat.v1.lite.TocoConverter", "path": "compat/v1/lite/tococonverter", "type": "tf.compat", "text": ["Convert a TensorFlow model into output_format using TOCO.", "This class has been deprecated. Please use lite.TFLiteConverter instead.", "View source", "Creates a TocoConverter class from a file containing a frozen graph. (deprecated)", "View source", "Creates a TocoConverter class from a tf.keras model file. (deprecated)", "View source", "Creates a TocoConverter class from a SavedModel. (deprecated)", "View source", "Creates a TocoConverter class from a TensorFlow Session. (deprecated)"]}, {"name": "tf.compat.v1.lite.toco_convert", "path": "compat/v1/lite/toco_convert", "type": "tf.compat", "text": ["Convert a model using TOCO. (deprecated)", "Typically this function is used to convert from TensorFlow GraphDef to TFLite. Conversion can be customized by providing arguments that are forwarded to build_toco_convert_protos (see documentation for details). This function has been deprecated. Please use lite.TFLiteConverter instead."]}, {"name": "tf.compat.v1.LMDBReader", "path": "compat/v1/lmdbreader", "type": "tf.compat", "text": ["A Reader that outputs the records from a LMDB file.", "Inherits From: ReaderBase", "See ReaderBase for supported methods.", "Readers are not compatible with eager execution. Instead, please use tf.data to get data into your model.", "View source", "Returns the number of records this reader has produced.", "This is the same as the number of Read executions that have succeeded.", "View source", "Returns the number of work units this reader has finished processing.", "View source", "Returns the next record (key, value) pair produced by a reader.", "Will dequeue a work unit from queue if necessary (e.g. when the Reader needs to start reading from a new file since it has finished with the previous file).", "View source", "Returns up to num_records (key, value) pairs produced by a reader.", "Will dequeue a work unit from queue if necessary (e.g., when the Reader needs to start reading from a new file since it has finished with the previous file). It may return less than num_records even before the last batch.", "View source", "Restore a reader to its initial clean state.", "View source", "Restore a reader to a previously saved state.", "Not all Readers support being restored, so this can produce an Unimplemented error.", "View source", "Produce a string tensor that encodes the state of a reader.", "Not all Readers support being serialized, so this can produce an Unimplemented error."]}, {"name": "tf.compat.v1.load_file_system_library", "path": "compat/v1/load_file_system_library", "type": "tf.compat", "text": ["Loads a TensorFlow plugin, containing file system implementation. (deprecated)", "Pass library_filename to a platform-specific mechanism for dynamically loading a library. The rules for determining the exact location of the library are platform-specific and are not documented here."]}, {"name": "tf.compat.v1.local_variables", "path": "compat/v1/local_variables", "type": "tf.compat", "text": ["Returns local variables.", "Local variables - per process variables, usually not saved/restored to checkpoint and used for temporary or intermediate values. For example, they can be used as counters for metrics computation or number of epochs this machine has read data. The tf.contrib.framework.local_variable() function automatically adds the new variable to GraphKeys.LOCAL_VARIABLES. This convenience function returns the contents of that collection.", "An alternative to local variables are global variables. See tf.compat.v1.global_variables"]}, {"name": "tf.compat.v1.local_variables_initializer", "path": "compat/v1/local_variables_initializer", "type": "tf.compat", "text": ["Returns an Op that initializes all local variables.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.initializers.local_variables", "This is just a shortcut for variables_initializer(local_variables())"]}, {"name": "tf.compat.v1.logging", "path": "compat/v1/logging", "type": "tf.compat", "text": ["Logging and Summary Operations.", "TaskLevelStatusMessage(...)", "debug(...)", "error(...)", "fatal(...)", "flush(...)", "get_verbosity(...): Return how much logging output will be produced.", "info(...)", "log(...)", "log_every_n(...): Log 'msg % args' at level 'level' once per 'n' times.", "log_first_n(...): Log 'msg % args' at level 'level' only first 'n' times.", "log_if(...): Log 'msg % args' at level 'level' only if condition is fulfilled.", "set_verbosity(...): Sets the threshold for what messages will be logged.", "vlog(...)", "warn(...)", "warning(...)"]}, {"name": "tf.compat.v1.logging.debug", "path": "compat/v1/logging/debug", "type": "tf.compat", "text": []}, {"name": "tf.compat.v1.logging.error", "path": "compat/v1/logging/error", "type": "tf.compat", "text": []}, {"name": "tf.compat.v1.logging.fatal", "path": "compat/v1/logging/fatal", "type": "tf.compat", "text": []}, {"name": "tf.compat.v1.logging.flush", "path": "compat/v1/logging/flush", "type": "tf.compat", "text": []}, {"name": "tf.compat.v1.logging.get_verbosity", "path": "compat/v1/logging/get_verbosity", "type": "tf.compat", "text": ["Return how much logging output will be produced."]}, {"name": "tf.compat.v1.logging.info", "path": "compat/v1/logging/info", "type": "tf.compat", "text": []}, {"name": "tf.compat.v1.logging.log", "path": "compat/v1/logging/log", "type": "tf.compat", "text": []}, {"name": "tf.compat.v1.logging.log_every_n", "path": "compat/v1/logging/log_every_n", "type": "tf.compat", "text": ["Log 'msg % args' at level 'level' once per 'n' times.", "Logs the 1st call, (N+1)st call, (2N+1)st call, etc. Not threadsafe."]}, {"name": "tf.compat.v1.logging.log_first_n", "path": "compat/v1/logging/log_first_n", "type": "tf.compat", "text": ["Log 'msg % args' at level 'level' only first 'n' times.", "Not threadsafe."]}, {"name": "tf.compat.v1.logging.log_if", "path": "compat/v1/logging/log_if", "type": "tf.compat", "text": ["Log 'msg % args' at level 'level' only if condition is fulfilled."]}, {"name": "tf.compat.v1.logging.set_verbosity", "path": "compat/v1/logging/set_verbosity", "type": "tf.compat", "text": ["Sets the threshold for what messages will be logged."]}, {"name": "tf.compat.v1.logging.TaskLevelStatusMessage", "path": "compat/v1/logging/tasklevelstatusmessage", "type": "tf.compat", "text": []}, {"name": "tf.compat.v1.logging.vlog", "path": "compat/v1/logging/vlog", "type": "tf.compat", "text": []}, {"name": "tf.compat.v1.logging.warn", "path": "compat/v1/logging/warn", "type": "tf.compat", "text": []}, {"name": "tf.compat.v1.logging.warning", "path": "compat/v1/logging/warning", "type": "tf.compat", "text": []}, {"name": "tf.compat.v1.LogMessage", "path": "compat/v1/logmessage", "type": "tf.compat", "text": ["A ProtocolMessage"]}, {"name": "tf.compat.v1.lookup", "path": "compat/v1/lookup", "type": "tf.compat", "text": ["Public API for tf.lookup namespace.", "experimental module: Public API for tf.lookup.experimental namespace.", "class KeyValueTensorInitializer: Table initializers given keys and values tensors.", "class StaticHashTable: A generic hash table that is immutable once initialized.", "class StaticVocabularyTable: String to Id table that assigns out-of-vocabulary keys to hash buckets.", "class TextFileIndex: The key and value content to get from each line.", "class TextFileInitializer: Table initializers from a text file."]}, {"name": "tf.compat.v1.lookup.experimental", "path": "compat/v1/lookup/experimental", "type": "tf.compat", "text": ["Public API for tf.lookup.experimental namespace.", "class DatasetInitializer: Creates a table initializer from a tf.data.Dataset.", "class DenseHashTable: A generic mutable hash table implementation using tensors as backing store."]}, {"name": "tf.compat.v1.lookup.StaticHashTable", "path": "compat/v1/lookup/statichashtable", "type": "tf.compat", "text": ["A generic hash table that is immutable once initialized.", "Inherits From: StaticHashTable", "When running in graph mode, you must evaluate the tensor returned by tf.tables_initializer() before evaluating the tensor returned by this class's lookup() method. Example usage in graph mode:", "In eager mode, no special code is needed to initialize the table. Example usage in eager mode:", "View source", "Returns tensors of all keys and values in the table.", "View source", "Looks up keys in a table, outputs the corresponding values.", "The default_value is used for keys not present in the table.", "View source", "Compute the number of elements in this table.", "View source", "Looks up keys in a table, outputs the corresponding values."]}, {"name": "tf.compat.v1.lookup.StaticVocabularyTable", "path": "compat/v1/lookup/staticvocabularytable", "type": "tf.compat", "text": ["String to Id table that assigns out-of-vocabulary keys to hash buckets.", "Inherits From: StaticVocabularyTable", "For example, if an instance of StaticVocabularyTable is initialized with a string-to-id initializer that maps:", "The Vocabulary object will performs the following mapping:", "If initializer is None, only out-of-vocabulary buckets are used.", "The hash function used for generating out-of-vocabulary buckets ID is Fingerprint64.", "View source", "Looks up keys in the table, outputs the corresponding values.", "It assigns out-of-vocabulary keys to buckets based in their hashes.", "View source", "Compute the number of elements in this table.", "View source", "Looks up keys in a table, outputs the corresponding values."]}, {"name": "tf.compat.v1.losses", "path": "compat/v1/losses", "type": "tf.compat", "text": ["Loss operations for use in neural networks.", "class Reduction: Types of loss reduction.", "absolute_difference(...): Adds an Absolute Difference loss to the training procedure.", "add_loss(...): Adds a externally defined loss to the collection of losses.", "compute_weighted_loss(...): Computes the weighted loss.", "cosine_distance(...): Adds a cosine-distance loss to the training procedure. (deprecated arguments)", "get_losses(...): Gets the list of losses from the loss_collection.", "get_regularization_loss(...): Gets the total regularization loss.", "get_regularization_losses(...): Gets the list of regularization losses.", "get_total_loss(...): Returns a tensor whose value represents the total loss.", "hinge_loss(...): Adds a hinge loss to the training procedure.", "huber_loss(...): Adds a Huber Loss term to the training procedure.", "log_loss(...): Adds a Log Loss term to the training procedure.", "mean_pairwise_squared_error(...): Adds a pairwise-errors-squared loss to the training procedure.", "mean_squared_error(...): Adds a Sum-of-Squares loss to the training procedure.", "sigmoid_cross_entropy(...): Creates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits.", "softmax_cross_entropy(...): Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2.", "sparse_softmax_cross_entropy(...): Cross-entropy loss using tf.nn.sparse_softmax_cross_entropy_with_logits."]}, {"name": "tf.compat.v1.losses.absolute_difference", "path": "compat/v1/losses/absolute_difference", "type": "tf.compat", "text": ["Adds an Absolute Difference loss to the training procedure.", "weights acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If weights is a Tensor of shape [batch_size], then the total loss for each sample of the batch is rescaled by the corresponding element in the weights vector. If the shape of weights matches the shape of predictions, then the loss of each measurable element of predictions is scaled by the corresponding value of weights.", "The loss_collection argument is ignored when executing eagerly. Consider holding on to the return value or collecting losses via a tf.keras.Model."]}, {"name": "tf.compat.v1.losses.add_loss", "path": "compat/v1/losses/add_loss", "type": "tf.compat", "text": ["Adds a externally defined loss to the collection of losses."]}, {"name": "tf.compat.v1.losses.compute_weighted_loss", "path": "compat/v1/losses/compute_weighted_loss", "type": "tf.compat", "text": ["Computes the weighted loss.", "When calculating the gradient of a weighted loss contributions from both losses and weights are considered. If your weights depend on some model parameters but you do not want this to affect the loss gradient, you need to apply tf.stop_gradient to weights before passing them to compute_weighted_loss.", "The loss_collection argument is ignored when executing eagerly. Consider holding on to the return value or collecting losses via a tf.keras.Model."]}, {"name": "tf.compat.v1.losses.cosine_distance", "path": "compat/v1/losses/cosine_distance", "type": "tf.compat", "text": ["Adds a cosine-distance loss to the training procedure. (deprecated arguments)", "Note that the function assumes that predictions and labels are already unit-normalized.", "The loss_collection argument is ignored when executing eagerly. Consider holding on to the return value or collecting losses via a tf.keras.Model."]}, {"name": "tf.compat.v1.losses.get_losses", "path": "compat/v1/losses/get_losses", "type": "tf.compat", "text": ["Gets the list of losses from the loss_collection."]}, {"name": "tf.compat.v1.losses.get_regularization_loss", "path": "compat/v1/losses/get_regularization_loss", "type": "tf.compat", "text": ["Gets the total regularization loss."]}, {"name": "tf.compat.v1.losses.get_regularization_losses", "path": "compat/v1/losses/get_regularization_losses", "type": "tf.compat", "text": ["Gets the list of regularization losses."]}, {"name": "tf.compat.v1.losses.get_total_loss", "path": "compat/v1/losses/get_total_loss", "type": "tf.compat", "text": ["Returns a tensor whose value represents the total loss.", "In particular, this adds any losses you have added with tf.add_loss() to any regularization losses that have been added by regularization parameters on layers constructors e.g. tf.layers. Be very sure to use this if you are constructing a loss_op manually. Otherwise regularization arguments on tf.layers methods will not function."]}, {"name": "tf.compat.v1.losses.hinge_loss", "path": "compat/v1/losses/hinge_loss", "type": "tf.compat", "text": ["Adds a hinge loss to the training procedure.", "The loss_collection argument is ignored when executing eagerly. Consider holding on to the return value or collecting losses via a tf.keras.Model."]}, {"name": "tf.compat.v1.losses.huber_loss", "path": "compat/v1/losses/huber_loss", "type": "tf.compat", "text": ["Adds a Huber Loss term to the training procedure.", "For each value x in error=labels-predictions, the following is calculated:", "where d is delta.", "weights acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If weights is a tensor of size [batch_size], then the total loss for each sample of the batch is rescaled by the corresponding element in the weights vector. If the shape of weights matches the shape of predictions, then the loss of each measurable element of predictions is scaled by the corresponding value of weights.", "The loss_collection argument is ignored when executing eagerly. Consider holding on to the return value or collecting losses via a tf.keras.Model."]}, {"name": "tf.compat.v1.losses.log_loss", "path": "compat/v1/losses/log_loss", "type": "tf.compat", "text": ["Adds a Log Loss term to the training procedure.", "weights acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If weights is a tensor of size [batch_size], then the total loss for each sample of the batch is rescaled by the corresponding element in the weights vector. If the shape of weights matches the shape of predictions, then the loss of each measurable element of predictions is scaled by the corresponding value of weights.", "The loss_collection argument is ignored when executing eagerly. Consider holding on to the return value or collecting losses via a tf.keras.Model."]}, {"name": "tf.compat.v1.losses.mean_pairwise_squared_error", "path": "compat/v1/losses/mean_pairwise_squared_error", "type": "tf.compat", "text": ["Adds a pairwise-errors-squared loss to the training procedure.", "Unlike mean_squared_error, which is a measure of the differences between corresponding elements of predictions and labels, mean_pairwise_squared_error is a measure of the differences between pairs of corresponding elements of predictions and labels.", "For example, if labels=[a, b, c] and predictions=[x, y, z], there are three pairs of differences are summed to compute the loss: loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3", "Note that since the inputs are of shape [batch_size, d0, ... dN], the corresponding pairs are computed within each batch sample but not across samples within a batch. For example, if predictions represents a batch of 16 grayscale images of dimension [batch_size, 100, 200], then the set of pairs is drawn from each image, but not across images.", "weights acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If weights is a tensor of size [batch_size], then the total loss for each sample of the batch is rescaled by the corresponding element in the weights vector.", "The loss_collection argument is ignored when executing eagerly. Consider holding on to the return value or collecting losses via a tf.keras.Model."]}, {"name": "tf.compat.v1.losses.mean_squared_error", "path": "compat/v1/losses/mean_squared_error", "type": "tf.compat", "text": ["Adds a Sum-of-Squares loss to the training procedure.", "weights acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If weights is a tensor of size [batch_size], then the total loss for each sample of the batch is rescaled by the corresponding element in the weights vector. If the shape of weights matches the shape of predictions, then the loss of each measurable element of predictions is scaled by the corresponding value of weights.", "The loss_collection argument is ignored when executing eagerly. Consider holding on to the return value or collecting losses via a tf.keras.Model."]}, {"name": "tf.compat.v1.losses.Reduction", "path": "compat/v1/losses/reduction", "type": "tf.compat", "text": ["Types of loss reduction.", "Contains the following values:", "View source", "View source"]}, {"name": "tf.compat.v1.losses.sigmoid_cross_entropy", "path": "compat/v1/losses/sigmoid_cross_entropy", "type": "tf.compat", "text": ["Creates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits.", "weights acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If weights is a tensor of shape [batch_size], then the loss weights apply to each corresponding sample.", "If label_smoothing is nonzero, smooth the labels towards 1/2:", "The loss_collection argument is ignored when executing eagerly. Consider holding on to the return value or collecting losses via a tf.keras.Model."]}, {"name": "tf.compat.v1.losses.softmax_cross_entropy", "path": "compat/v1/losses/softmax_cross_entropy", "type": "tf.compat", "text": ["Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2.", "weights acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If weights is a tensor of shape [batch_size], then the loss weights apply to each corresponding sample.", "If label_smoothing is nonzero, smooth the labels towards 1/num_classes: new_onehot_labels = onehot_labels * (1 - label_smoothing)", "Note that onehot_labels and logits must have the same shape, e.g. [batch_size, num_classes]. The shape of weights must be broadcastable to loss, whose shape is decided by the shape of logits. In case the shape of logits is [batch_size, num_classes], loss is a Tensor of shape [batch_size].", "The loss_collection argument is ignored when executing eagerly. Consider holding on to the return value or collecting losses via a tf.keras.Model."]}, {"name": "tf.compat.v1.losses.sparse_softmax_cross_entropy", "path": "compat/v1/losses/sparse_softmax_cross_entropy", "type": "tf.compat", "text": ["Cross-entropy loss using tf.nn.sparse_softmax_cross_entropy_with_logits.", "weights acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If weights is a tensor of shape [batch_size], then the loss weights apply to each corresponding sample.", "The loss_collection argument is ignored when executing eagerly. Consider holding on to the return value or collecting losses via a tf.keras.Model."]}, {"name": "tf.compat.v1.make_template", "path": "compat/v1/make_template", "type": "tf.compat", "text": ["Given an arbitrary function, wrap it so that it does variable sharing.", "This wraps func_ in a Template and partially evaluates it. Templates are functions that create variables the first time they are called and reuse them thereafter. In order for func_ to be compatible with a Template it must have the following properties:", "In the following example, both z and w will be scaled by the same y. It is important to note that if we didn't assign scalar_name and used a different name for z and w that a ValueError would be thrown because it couldn't reuse the variable.", "As a safe-guard, the returned function will raise a ValueError after the first call if trainable variables are created by calling tf.Variable.", "If all of these are true, then 2 properties are enforced by the template:", "Depending on the value of create_scope_now_, the full variable scope may be captured either at the time of first call or at the time of construction. If this option is set to True, then all Tensors created by repeated calls to the template will have an extra trailing _N+1 to their name, as the first time the scope is entered in the Template constructor no Tensors are created."]}, {"name": "tf.compat.v1.manip", "path": "compat/v1/manip", "type": "tf.compat", "text": ["Operators for manipulating tensors.", "batch_to_space_nd(...): BatchToSpace for N-D tensors of type T.", "gather_nd(...): Gather slices from params into a Tensor with shape specified by indices.", "reshape(...): Reshapes a tensor.", "reverse(...): Reverses specific dimensions of a tensor.", "roll(...): Rolls the elements of a tensor along an axis.", "scatter_nd(...): Scatter updates into a new tensor according to indices.", "space_to_batch_nd(...): SpaceToBatch for N-D tensors of type T.", "tile(...): Constructs a tensor by tiling a given tensor."]}, {"name": "tf.compat.v1.map_fn", "path": "compat/v1/map_fn", "type": "tf.compat", "text": ["Transforms elems by applying fn to each element unstacked on axis 0. (deprecated arguments)", "See also tf.scan.", "map_fn unstacks elems on axis 0 to obtain a sequence of elements; calls fn to transform each element; and then stacks the transformed values back together.", "If elems is a single tensor and fn's signature is tf.Tensor->tf.Tensor, then map_fn(fn, elems) is equivalent to tf.stack([fn(elem) for elem in tf.unstack(elems)]). E.g.:", "map_fn(fn, elems).shape = [elems.shape[0]] + fn(elems[0]).shape.", "map_fn also supports functions with multi-arity inputs and outputs:", "If elems is a tuple (or nested structure) of tensors, then those tensors must all have the same outer-dimension size (num_elems); and fn is used to transform each tuple (or structure) of corresponding slices from elems. E.g., if elems is a tuple (t1, t2, t3), then fn is used to transform each tuple of slices (t1[i], t2[i], t3[i]) (where 0 <= i < num_elems).", "If fn returns a tuple (or nested structure) of tensors, then the result is formed by stacking corresponding elements from those structures.", "If fn's input and output signatures are different, then the output signature must be specified using fn_output_signature. (The input and output signatures are differ if their structures, dtypes, or tensor types do not match). E.g.:", "fn_output_signature can be specified using any of the following:", "map_fn supports tf.RaggedTensor inputs and outputs. In particular:", "If elems is a RaggedTensor, then fn will be called with each row of that ragged tensor.", "If the result of map_fn should be a RaggedTensor, then use a tf.RaggedTensorSpec to specify fn_output_signature.", "E.g.:", "map_fn supports tf.sparse.SparseTensor inputs and outputs. In particular:", "If elems is a SparseTensor, then fn will be called with each row of that sparse tensor. In particular, the value passed to fn will be a tf.sparse.SparseTensor with one fewer dimension than elems.", "If the result of map_fn should be a SparseTensor, then use a tf.SparseTensorSpec to specify fn_output_signature. The individual SparseTensors returned by fn will be stacked into a single SparseTensor with one more dimension.", "If the function is expressible as TensorFlow ops, use:", "Otherwise, use:", "map_fn will apply the operations used by fn to each element of elems, resulting in O(elems.shape[0]) total operations. This is somewhat mitigated by the fact that map_fn can process elements in parallel. However, a transform expressed using map_fn is still typically less efficient than an equivalent transform expressed using vectorized operations.", "map_fn should typically only be used if one of the following is true:", "E.g., the example given above that maps fn=lambda t: tf.range(t, t + 3) across elems could be rewritten more efficiently using vectorized ops:", "In some cases, tf.vectorized_map can be used to automatically convert a function to a vectorized eqivalent.", "When executing eagerly, map_fn does not execute in parallel even if parallel_iterations is set to a value > 1. You can still get the performance benefits of running a function in parallel by using the tf.function decorator:"]}, {"name": "tf.compat.v1.math", "path": "compat/v1/math", "type": "tf.compat", "text": [" ", "Math Operations.", "TensorFlow provides a variety of math functions including:", "See: tf.linalg for matrix and tensor functions.", "TensorFlow provides several operations that you can use to perform common math computations on tensor segments. Here a segmentation is a partitioning of a tensor along the first dimension, i.e. it defines a mapping from the first dimension onto segment_ids. The segment_ids tensor should be the size of the first dimension, d0, with consecutive IDs in the range 0 to k, where k<d0. In particular, a segmentation of a matrix tensor is a mapping of rows to segments.", "The standard segment_* functions assert that the segment indices are sorted. If you have unsorted indices use the equivalent unsorted_segment_ function. These functions take an additional argument num_segments so that the output tensor can be efficiently allocated.", "special module: Public API for tf.math.special namespace.", "abs(...): Computes the absolute value of a tensor.", "accumulate_n(...): Returns the element-wise sum of a list of tensors.", "acos(...): Computes acos of x element-wise.", "acosh(...): Computes inverse hyperbolic cosine of x element-wise.", "add(...): Returns x + y element-wise.", "add_n(...): Adds all input tensors element-wise.", "angle(...): Returns the element-wise argument of a complex (or real) tensor.", "argmax(...): Returns the index with the largest value across axes of a tensor. (deprecated arguments)", "argmin(...): Returns the index with the smallest value across axes of a tensor. (deprecated arguments)", "asin(...): Computes the trignometric inverse sine of x element-wise.", "asinh(...): Computes inverse hyperbolic sine of x element-wise.", "atan(...): Computes the trignometric inverse tangent of x element-wise.", "atan2(...): Computes arctangent of y/x element-wise, respecting signs of the arguments.", "atanh(...): Computes inverse hyperbolic tangent of x element-wise.", "bessel_i0(...): Computes the Bessel i0 function of x element-wise.", "bessel_i0e(...): Computes the Bessel i0e function of x element-wise.", "bessel_i1(...): Computes the Bessel i1 function of x element-wise.", "bessel_i1e(...): Computes the Bessel i1e function of x element-wise.", "betainc(...): Compute the regularized incomplete beta integral \\(I_x(a, b)\\).", "bincount(...): Counts the number of occurrences of each value in an integer array.", "ceil(...): Return the ceiling of the input, element-wise.", "confusion_matrix(...): Computes the confusion matrix from predictions and labels.", "conj(...): Returns the complex conjugate of a complex number.", "cos(...): Computes cos of x element-wise.", "cosh(...): Computes hyperbolic cosine of x element-wise.", "count_nonzero(...): Computes number of nonzero elements across dimensions of a tensor. (deprecated arguments) (deprecated arguments)", "cumprod(...): Compute the cumulative product of the tensor x along axis.", "cumsum(...): Compute the cumulative sum of the tensor x along axis.", "cumulative_logsumexp(...): Compute the cumulative log-sum-exp of the tensor x along axis.", "digamma(...): Computes Psi, the derivative of Lgamma (the log of the absolute value of", "divide(...): Computes Python style division of x by y.", "divide_no_nan(...): Computes a safe divide which returns 0 if the y is zero.", "equal(...): Returns the truth value of (x == y) element-wise.", "erf(...): Computes the Gauss error function of x element-wise.", "erfc(...): Computes the complementary error function of x element-wise.", "erfcinv(...): Computes the inverse of complementary error function.", "erfinv(...): Compute inverse error function.", "exp(...): Computes exponential of x element-wise. \\(y = e^x\\).", "expm1(...): Computes exp(x) - 1 element-wise.", "floor(...): Returns element-wise largest integer not greater than x.", "floordiv(...): Divides x / y elementwise, rounding toward the most negative integer.", "floormod(...): Returns element-wise remainder of division. When x < 0 xor y < 0 is", "greater(...): Returns the truth value of (x > y) element-wise.", "greater_equal(...): Returns the truth value of (x >= y) element-wise.", "igamma(...): Compute the lower regularized incomplete Gamma function P(a, x).", "igammac(...): Compute the upper regularized incomplete Gamma function Q(a, x).", "imag(...): Returns the imaginary part of a complex (or real) tensor.", "in_top_k(...): Says whether the targets are in the top K predictions.", "invert_permutation(...): Computes the inverse permutation of a tensor.", "is_finite(...): Returns which elements of x are finite.", "is_inf(...): Returns which elements of x are Inf.", "is_nan(...): Returns which elements of x are NaN.", "is_non_decreasing(...): Returns True if x is non-decreasing.", "is_strictly_increasing(...): Returns True if x is strictly increasing.", "l2_normalize(...): Normalizes along dimension axis using an L2 norm. (deprecated arguments)", "lbeta(...): Computes \\(ln(|Beta(x)|)\\), reducing along the last dimension.", "less(...): Returns the truth value of (x < y) element-wise.", "less_equal(...): Returns the truth value of (x <= y) element-wise.", "lgamma(...): Computes the log of the absolute value of Gamma(x) element-wise.", "log(...): Computes natural logarithm of x element-wise.", "log1p(...): Computes natural logarithm of (1 + x) element-wise.", "log_sigmoid(...): Computes log sigmoid of x element-wise.", "log_softmax(...): Computes log softmax activations. (deprecated arguments)", "logical_and(...): Logical AND function.", "logical_not(...): Returns the truth value of NOT x element-wise.", "logical_or(...): Returns the truth value of x OR y element-wise.", "logical_xor(...): Logical XOR function.", "maximum(...): Returns the max of x and y (i.e. x > y ? x : y) element-wise.", "minimum(...): Returns the min of x and y (i.e. x < y ? x : y) element-wise.", "mod(...): Returns element-wise remainder of division. When x < 0 xor y < 0 is", "multiply(...): Returns an element-wise x * y.", "multiply_no_nan(...): Computes the product of x and y and returns 0 if the y is zero, even if x is NaN or infinite.", "ndtri(...): Compute quantile of Standard Normal.", "negative(...): Computes numerical negative value element-wise.", "nextafter(...): Returns the next representable value of x1 in the direction of x2, element-wise.", "not_equal(...): Returns the truth value of (x != y) element-wise.", "polygamma(...): Compute the polygamma function \\(\\psi^{(n)}(x)\\).", "polyval(...): Computes the elementwise value of a polynomial.", "pow(...): Computes the power of one value to another.", "real(...): Returns the real part of a complex (or real) tensor.", "reciprocal(...): Computes the reciprocal of x element-wise.", "reciprocal_no_nan(...): Performs a safe reciprocal operation, element wise.", "reduce_all(...): Computes the \"logical and\" of elements across dimensions of a tensor. (deprecated arguments)", "reduce_any(...): Computes the \"logical or\" of elements across dimensions of a tensor. (deprecated arguments)", "reduce_euclidean_norm(...): Computes the Euclidean norm of elements across dimensions of a tensor.", "reduce_logsumexp(...): Computes log(sum(exp(elements across dimensions of a tensor))). (deprecated arguments)", "reduce_max(...): Computes the maximum of elements across dimensions of a tensor. (deprecated arguments)", "reduce_mean(...): Computes the mean of elements across dimensions of a tensor.", "reduce_min(...): Computes the minimum of elements across dimensions of a tensor. (deprecated arguments)", "reduce_prod(...): Computes the product of elements across dimensions of a tensor. (deprecated arguments)", "reduce_std(...): Computes the standard deviation of elements across dimensions of a tensor.", "reduce_sum(...): Computes the sum of elements across dimensions of a tensor. (deprecated arguments)", "reduce_variance(...): Computes the variance of elements across dimensions of a tensor.", "rint(...): Returns element-wise integer closest to x.", "round(...): Rounds the values of a tensor to the nearest integer, element-wise.", "rsqrt(...): Computes reciprocal of square root of x element-wise.", "scalar_mul(...): Multiplies a scalar times a Tensor or IndexedSlices object.", "segment_max(...): Computes the maximum along segments of a tensor.", "segment_mean(...): Computes the mean along segments of a tensor.", "segment_min(...): Computes the minimum along segments of a tensor.", "segment_prod(...): Computes the product along segments of a tensor.", "segment_sum(...): Computes the sum along segments of a tensor.", "sigmoid(...): Computes sigmoid of x element-wise.", "sign(...): Returns an element-wise indication of the sign of a number.", "sin(...): Computes sine of x element-wise.", "sinh(...): Computes hyperbolic sine of x element-wise.", "sobol_sample(...): Generates points from the Sobol sequence.", "softmax(...): Computes softmax activations. (deprecated arguments)", "softplus(...): Computes softplus: log(exp(features) + 1).", "softsign(...): Computes softsign: features / (abs(features) + 1).", "sqrt(...): Computes element-wise square root of the input tensor.", "square(...): Computes square of x element-wise.", "squared_difference(...): Returns conj(x - y)(x - y) element-wise.", "subtract(...): Returns x - y element-wise.", "tan(...): Computes tan of x element-wise.", "tanh(...): Computes hyperbolic tangent of x element-wise.", "top_k(...): Finds values and indices of the k largest entries for the last dimension.", "truediv(...): Divides x / y elementwise (using Python 3 division operator semantics).", "unsorted_segment_max(...): Computes the maximum along segments of a tensor.", "unsorted_segment_mean(...): Computes the mean along segments of a tensor.", "unsorted_segment_min(...): Computes the minimum along segments of a tensor.", "unsorted_segment_prod(...): Computes the product along segments of a tensor.", "unsorted_segment_sqrt_n(...): Computes the sum along segments of a tensor divided by the sqrt(N).", "unsorted_segment_sum(...): Computes the sum along segments of a tensor.", "xdivy(...): Returns 0 if x == 0, and x / y otherwise, elementwise.", "xlog1py(...): Compute x * log1p(y).", "xlogy(...): Returns 0 if x == 0, and x * log(y) otherwise, elementwise.", "zero_fraction(...): Returns the fraction of zeros in value.", "zeta(...): Compute the Hurwitz zeta function \\(\\zeta(x, q)\\)."]}, {"name": "tf.compat.v1.math.in_top_k", "path": "compat/v1/math/in_top_k", "type": "tf.compat", "text": [" ", "Says whether the targets are in the top K predictions.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.nn.in_top_k", "This outputs a batch_size bool array, an entry out[i] is true if the prediction for the target class is finite (not inf, -inf, or nan) and among the top k predictions among all predictions for example i. Note that the behavior of InTopK differs from the TopK op in its handling of ties; if multiple classes have the same prediction value and straddle the top-k boundary, all of those classes are considered to be in the top k.", "More formally, let", "\\(predictions_i\\) be the predictions for all classes for example i, \\(targets_i\\) be the target class for example i, \\(out_i\\) be the output for example i,"]}, {"name": "tf.compat.v1.math.log_softmax", "path": "compat/v1/math/log_softmax", "type": "tf.compat", "text": ["Computes log softmax activations. (deprecated arguments)", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.nn.log_softmax", "For each batch i and class j we have"]}, {"name": "tf.compat.v1.math.softmax", "path": "compat/v1/math/softmax", "type": "tf.compat", "text": ["Computes softmax activations. (deprecated arguments)", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.nn.softmax", "This function performs the equivalent of", "See: https://en.wikipedia.org/wiki/Softmax_function"]}, {"name": "tf.compat.v1.math.special", "path": "compat/v1/math/special", "type": "tf.compat", "text": ["Public API for tf.math.special namespace.", "bessel_i0(...): Computes the Bessel i0 function of x element-wise.", "bessel_i0e(...): Computes the Bessel i0e function of x element-wise.", "bessel_i1(...): Computes the Bessel i1 function of x element-wise.", "bessel_i1e(...): Computes the Bessel i1e function of x element-wise.", "bessel_j0(...): Computes the Bessel j0 function of x element-wise.", "bessel_j1(...): Computes the Bessel j1 function of x element-wise.", "bessel_k0(...): Computes the Bessel k0 function of x element-wise.", "bessel_k0e(...): Computes the Bessel k0e function of x element-wise.", "bessel_k1(...): Computes the Bessel k1 function of x element-wise.", "bessel_k1e(...): Computes the Bessel k1e function of x element-wise.", "bessel_y0(...): Computes the Bessel y0 function of x element-wise.", "bessel_y1(...): Computes the Bessel y1 function of x element-wise.", "dawsn(...): Computes Dawson's integral of x element-wise.", "expint(...): Computes the Exponential integral of x element-wise.", "fresnel_cos(...): Computes Fresnel's cosine integral of x element-wise.", "fresnel_sin(...): Computes Fresnel's sine integral of x element-wise.", "spence(...): Computes Spence's integral of x element-wise."]}, {"name": "tf.compat.v1.MetaGraphDef", "path": "compat/v1/metagraphdef", "type": "tf.compat", "text": ["A ProtocolMessage", "class CollectionDefEntry", "class MetaInfoDef", "class SignatureDefEntry"]}, {"name": "tf.compat.v1.MetaGraphDef.CollectionDefEntry", "path": "compat/v1/metagraphdef/collectiondefentry", "type": "tf.compat", "text": ["A ProtocolMessage"]}, {"name": "tf.compat.v1.MetaGraphDef.MetaInfoDef", "path": "compat/v1/metagraphdef/metainfodef", "type": "tf.compat", "text": ["A ProtocolMessage", "class FunctionAliasesEntry"]}, {"name": "tf.compat.v1.MetaGraphDef.MetaInfoDef.FunctionAliasesEntry", "path": "compat/v1/metagraphdef/metainfodef/functionaliasesentry", "type": "tf.compat", "text": ["A ProtocolMessage"]}, {"name": "tf.compat.v1.MetaGraphDef.SignatureDefEntry", "path": "compat/v1/metagraphdef/signaturedefentry", "type": "tf.compat", "text": ["A ProtocolMessage"]}, {"name": "tf.compat.v1.metrics", "path": "compat/v1/metrics", "type": "tf.compat", "text": ["Evaluation-related metrics.", "accuracy(...): Calculates how often predictions matches labels.", "auc(...): Computes the approximate AUC via a Riemann sum. (deprecated)", "average_precision_at_k(...): Computes average precision@k of predictions with respect to sparse labels.", "false_negatives(...): Computes the total number of false negatives.", "false_negatives_at_thresholds(...): Computes false negatives at provided threshold values.", "false_positives(...): Sum the weights of false positives.", "false_positives_at_thresholds(...): Computes false positives at provided threshold values.", "mean(...): Computes the (weighted) mean of the given values.", "mean_absolute_error(...): Computes the mean absolute error between the labels and predictions.", "mean_cosine_distance(...): Computes the cosine distance between the labels and predictions.", "mean_iou(...): Calculate per-step mean Intersection-Over-Union (mIOU).", "mean_per_class_accuracy(...): Calculates the mean of the per-class accuracies.", "mean_relative_error(...): Computes the mean relative error by normalizing with the given values.", "mean_squared_error(...): Computes the mean squared error between the labels and predictions.", "mean_tensor(...): Computes the element-wise (weighted) mean of the given tensors.", "percentage_below(...): Computes the percentage of values less than the given threshold.", "precision(...): Computes the precision of the predictions with respect to the labels.", "precision_at_k(...): Computes precision@k of the predictions with respect to sparse labels.", "precision_at_thresholds(...): Computes precision values for different thresholds on predictions.", "precision_at_top_k(...): Computes precision@k of the predictions with respect to sparse labels.", "recall(...): Computes the recall of the predictions with respect to the labels.", "recall_at_k(...): Computes recall@k of the predictions with respect to sparse labels.", "recall_at_thresholds(...): Computes various recall values for different thresholds on predictions.", "recall_at_top_k(...): Computes recall@k of top-k predictions with respect to sparse labels.", "root_mean_squared_error(...): Computes the root mean squared error between the labels and predictions.", "sensitivity_at_specificity(...): Computes the specificity at a given sensitivity.", "sparse_average_precision_at_k(...): Renamed to average_precision_at_k, please use that method instead. (deprecated)", "sparse_precision_at_k(...): Renamed to precision_at_k, please use that method instead. (deprecated)", "specificity_at_sensitivity(...): Computes the specificity at a given sensitivity.", "true_negatives(...): Sum the weights of true_negatives.", "true_negatives_at_thresholds(...): Computes true negatives at provided threshold values.", "true_positives(...): Sum the weights of true_positives.", "true_positives_at_thresholds(...): Computes true positives at provided threshold values."]}, {"name": "tf.compat.v1.metrics.accuracy", "path": "compat/v1/metrics/accuracy", "type": "tf.compat", "text": ["Calculates how often predictions matches labels.", "The accuracy function creates two local variables, total and count that are used to compute the frequency with which predictions matches labels. This frequency is ultimately returned as accuracy: an idempotent operation that simply divides total by count.", "For estimation of the metric over a stream of data, the function creates an update_op operation that updates these variables and returns the accuracy. Internally, an is_correct operation computes a Tensor with elements 1.0 where the corresponding elements of predictions and labels match and 0.0 otherwise. Then update_op increments total with the reduced sum of the product of weights and is_correct, and it increments count with the reduced sum of weights.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.auc", "path": "compat/v1/metrics/auc", "type": "tf.compat", "text": ["Computes the approximate AUC via a Riemann sum. (deprecated)", "The auc function creates four local variables, true_positives, true_negatives, false_positives and false_negatives that are used to compute the AUC. To discretize the AUC curve, a linearly spaced set of thresholds is used to compute pairs of recall and precision values. The area under the ROC-curve is therefore computed using the height of the recall values by the false positive rate, while the area under the PR-curve is the computed using the height of the precision values by the recall.", "This value is ultimately returned as auc, an idempotent operation that computes the area under a discretized curve of precision versus recall values (computed using the aforementioned variables). The num_thresholds variable controls the degree of discretization with larger numbers of thresholds more closely approximating the true AUC. The quality of the approximation may vary dramatically depending on num_thresholds.", "For best results, predictions should be distributed approximately uniformly in the range [0, 1] and not peaked around 0 or 1. The quality of the AUC approximation may be poor if this is not the case. Setting summation_method to 'minoring' or 'majoring' can help quantify the error in the approximation by providing lower or upper bound estimate of the AUC. The thresholds parameter can be used to manually specify thresholds which split the predictions more evenly.", "For estimation of the metric over a stream of data, the function creates an update_op operation that updates these variables and returns the auc.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.average_precision_at_k", "path": "compat/v1/metrics/average_precision_at_k", "type": "tf.compat", "text": ["Computes average precision@k of predictions with respect to sparse labels.", "average_precision_at_k creates two local variables, average_precision_at_<k>/total and average_precision_at_<k>/max, that are used to compute the frequency. This frequency is ultimately returned as average_precision_at_<k>: an idempotent operation that simply divides average_precision_at_<k>/total by average_precision_at_<k>/max.", "For estimation of the metric over a stream of data, the function creates an update_op operation that updates these variables and returns the precision_at_<k>. Internally, a top_k operation computes a Tensor indicating the top k predictions. Set operations applied to top_k and labels calculate the true positives and false positives weighted by weights. Then update_op increments true_positive_at_<k> and false_positive_at_<k> using these values.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.false_negatives", "path": "compat/v1/metrics/false_negatives", "type": "tf.compat", "text": ["Computes the total number of false negatives.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.false_negatives_at_thresholds", "path": "compat/v1/metrics/false_negatives_at_thresholds", "type": "tf.compat", "text": ["Computes false negatives at provided threshold values.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.false_positives", "path": "compat/v1/metrics/false_positives", "type": "tf.compat", "text": ["Sum the weights of false positives.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.false_positives_at_thresholds", "path": "compat/v1/metrics/false_positives_at_thresholds", "type": "tf.compat", "text": ["Computes false positives at provided threshold values.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.mean", "path": "compat/v1/metrics/mean", "type": "tf.compat", "text": ["Computes the (weighted) mean of the given values.", "The mean function creates two local variables, total and count that are used to compute the average of values. This average is ultimately returned as mean which is an idempotent operation that simply divides total by count.", "For estimation of the metric over a stream of data, the function creates an update_op operation that updates these variables and returns the mean. update_op increments total with the reduced sum of the product of values and weights, and it increments count with the reduced sum of weights.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.mean_absolute_error", "path": "compat/v1/metrics/mean_absolute_error", "type": "tf.compat", "text": ["Computes the mean absolute error between the labels and predictions.", "The mean_absolute_error function creates two local variables, total and count that are used to compute the mean absolute error. This average is weighted by weights, and it is ultimately returned as mean_absolute_error: an idempotent operation that simply divides total by count.", "For estimation of the metric over a stream of data, the function creates an update_op operation that updates these variables and returns the mean_absolute_error. Internally, an absolute_errors operation computes the absolute value of the differences between predictions and labels. Then update_op increments total with the reduced sum of the product of weights and absolute_errors, and it increments count with the reduced sum of weights", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.mean_cosine_distance", "path": "compat/v1/metrics/mean_cosine_distance", "type": "tf.compat", "text": ["Computes the cosine distance between the labels and predictions.", "The mean_cosine_distance function creates two local variables, total and count that are used to compute the average cosine distance between predictions and labels. This average is weighted by weights, and it is ultimately returned as mean_distance, which is an idempotent operation that simply divides total by count.", "For estimation of the metric over a stream of data, the function creates an update_op operation that updates these variables and returns the mean_distance.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.mean_iou", "path": "compat/v1/metrics/mean_iou", "type": "tf.compat", "text": ["Calculate per-step mean Intersection-Over-Union (mIOU).", "Mean Intersection-Over-Union is a common evaluation metric for semantic image segmentation, which first computes the IOU for each semantic class and then computes the average over classes. IOU is defined as follows: IOU = true_positive / (true_positive + false_positive + false_negative). The predictions are accumulated in a confusion matrix, weighted by weights, and mIOU is then calculated from it.", "For estimation of the metric over a stream of data, the function creates an update_op operation that updates these variables and returns the mean_iou.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.mean_per_class_accuracy", "path": "compat/v1/metrics/mean_per_class_accuracy", "type": "tf.compat", "text": ["Calculates the mean of the per-class accuracies.", "Calculates the accuracy for each class, then takes the mean of that.", "For estimation of the metric over a stream of data, the function creates an update_op operation that updates the accuracy of each class and returns them.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.mean_relative_error", "path": "compat/v1/metrics/mean_relative_error", "type": "tf.compat", "text": ["Computes the mean relative error by normalizing with the given values.", "The mean_relative_error function creates two local variables, total and count that are used to compute the mean relative absolute error. This average is weighted by weights, and it is ultimately returned as mean_relative_error: an idempotent operation that simply divides total by count.", "For estimation of the metric over a stream of data, the function creates an update_op operation that updates these variables and returns the mean_reative_error. Internally, a relative_errors operation divides the absolute value of the differences between predictions and labels by the normalizer. Then update_op increments total with the reduced sum of the product of weights and relative_errors, and it increments count with the reduced sum of weights.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.mean_squared_error", "path": "compat/v1/metrics/mean_squared_error", "type": "tf.compat", "text": ["Computes the mean squared error between the labels and predictions.", "The mean_squared_error function creates two local variables, total and count that are used to compute the mean squared error. This average is weighted by weights, and it is ultimately returned as mean_squared_error: an idempotent operation that simply divides total by count.", "For estimation of the metric over a stream of data, the function creates an update_op operation that updates these variables and returns the mean_squared_error. Internally, a squared_error operation computes the element-wise square of the difference between predictions and labels. Then update_op increments total with the reduced sum of the product of weights and squared_error, and it increments count with the reduced sum of weights.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.mean_tensor", "path": "compat/v1/metrics/mean_tensor", "type": "tf.compat", "text": ["Computes the element-wise (weighted) mean of the given tensors.", "In contrast to the mean function which returns a scalar with the mean, this function returns an average tensor with the same shape as the input tensors.", "The mean_tensor function creates two local variables, total_tensor and count_tensor that are used to compute the average of values. This average is ultimately returned as mean which is an idempotent operation that simply divides total by count.", "For estimation of the metric over a stream of data, the function creates an update_op operation that updates these variables and returns the mean. update_op increments total with the reduced sum of the product of values and weights, and it increments count with the reduced sum of weights.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.percentage_below", "path": "compat/v1/metrics/percentage_below", "type": "tf.compat", "text": ["Computes the percentage of values less than the given threshold.", "The percentage_below function creates two local variables, total and count that are used to compute the percentage of values that fall below threshold. This rate is weighted by weights, and it is ultimately returned as percentage which is an idempotent operation that simply divides total by count.", "For estimation of the metric over a stream of data, the function creates an update_op operation that updates these variables and returns the percentage.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.precision", "path": "compat/v1/metrics/precision", "type": "tf.compat", "text": ["Computes the precision of the predictions with respect to the labels.", "The precision function creates two local variables, true_positives and false_positives, that are used to compute the precision. This value is ultimately returned as precision, an idempotent operation that simply divides true_positives by the sum of true_positives and false_positives.", "For estimation of the metric over a stream of data, the function creates an update_op operation that updates these variables and returns the precision. update_op weights each prediction by the corresponding value in weights.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.precision_at_k", "path": "compat/v1/metrics/precision_at_k", "type": "tf.compat", "text": ["Computes precision@k of the predictions with respect to sparse labels.", "If class_id is specified, we calculate precision by considering only the entries in the batch for which class_id is in the top-k highest predictions, and computing the fraction of them for which class_id is indeed a correct label. If class_id is not specified, we'll calculate precision as how often on average a class among the top-k classes with the highest predicted values of a batch entry is correct and can be found in the label for that entry.", "precision_at_k creates two local variables, true_positive_at_<k> and false_positive_at_<k>, that are used to compute the precision@k frequency. This frequency is ultimately returned as precision_at_<k>: an idempotent operation that simply divides true_positive_at_<k> by total (true_positive_at_<k> + false_positive_at_<k>).", "For estimation of the metric over a stream of data, the function creates an update_op operation that updates these variables and returns the precision_at_<k>. Internally, a top_k operation computes a Tensor indicating the top k predictions. Set operations applied to top_k and labels calculate the true positives and false positives weighted by weights. Then update_op increments true_positive_at_<k> and false_positive_at_<k> using these values.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.precision_at_thresholds", "path": "compat/v1/metrics/precision_at_thresholds", "type": "tf.compat", "text": ["Computes precision values for different thresholds on predictions.", "The precision_at_thresholds function creates four local variables, true_positives, true_negatives, false_positives and false_negatives for various values of thresholds. precision[i] is defined as the total weight of values in predictions above thresholds[i] whose corresponding entry in labels is True, divided by the total weight of values in predictions above thresholds[i] (true_positives[i] / (true_positives[i] + false_positives[i])).", "For estimation of the metric over a stream of data, the function creates an update_op operation that updates these variables and returns the precision.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.precision_at_top_k", "path": "compat/v1/metrics/precision_at_top_k", "type": "tf.compat", "text": ["Computes precision@k of the predictions with respect to sparse labels.", "Differs from sparse_precision_at_k in that predictions must be in the form of top k class indices, whereas sparse_precision_at_k expects logits. Refer to sparse_precision_at_k for more details."]}, {"name": "tf.compat.v1.metrics.recall", "path": "compat/v1/metrics/recall", "type": "tf.compat", "text": ["Computes the recall of the predictions with respect to the labels.", "The recall function creates two local variables, true_positives and false_negatives, that are used to compute the recall. This value is ultimately returned as recall, an idempotent operation that simply divides true_positives by the sum of true_positives and false_negatives.", "For estimation of the metric over a stream of data, the function creates an update_op that updates these variables and returns the recall. update_op weights each prediction by the corresponding value in weights.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.recall_at_k", "path": "compat/v1/metrics/recall_at_k", "type": "tf.compat", "text": ["Computes recall@k of the predictions with respect to sparse labels.", "If class_id is specified, we calculate recall by considering only the entries in the batch for which class_id is in the label, and computing the fraction of them for which class_id is in the top-k predictions. If class_id is not specified, we'll calculate recall as how often on average a class among the labels of a batch entry is in the top-k predictions.", "sparse_recall_at_k creates two local variables, true_positive_at_<k> and false_negative_at_<k>, that are used to compute the recall_at_k frequency. This frequency is ultimately returned as recall_at_<k>: an idempotent operation that simply divides true_positive_at_<k> by total (true_positive_at_<k> + false_negative_at_<k>).", "For estimation of the metric over a stream of data, the function creates an update_op operation that updates these variables and returns the recall_at_<k>. Internally, a top_k operation computes a Tensor indicating the top k predictions. Set operations applied to top_k and labels calculate the true positives and false negatives weighted by weights. Then update_op increments true_positive_at_<k> and false_negative_at_<k> using these values.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.recall_at_thresholds", "path": "compat/v1/metrics/recall_at_thresholds", "type": "tf.compat", "text": ["Computes various recall values for different thresholds on predictions.", "The recall_at_thresholds function creates four local variables, true_positives, true_negatives, false_positives and false_negatives for various values of thresholds. recall[i] is defined as the total weight of values in predictions above thresholds[i] whose corresponding entry in labels is True, divided by the total weight of True values in labels (true_positives[i] / (true_positives[i] + false_negatives[i])).", "For estimation of the metric over a stream of data, the function creates an update_op operation that updates these variables and returns the recall.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.recall_at_top_k", "path": "compat/v1/metrics/recall_at_top_k", "type": "tf.compat", "text": ["Computes recall@k of top-k predictions with respect to sparse labels.", "Differs from recall_at_k in that predictions must be in the form of top k class indices, whereas recall_at_k expects logits. Refer to recall_at_k for more details."]}, {"name": "tf.compat.v1.metrics.root_mean_squared_error", "path": "compat/v1/metrics/root_mean_squared_error", "type": "tf.compat", "text": ["Computes the root mean squared error between the labels and predictions.", "The root_mean_squared_error function creates two local variables, total and count that are used to compute the root mean squared error. This average is weighted by weights, and it is ultimately returned as root_mean_squared_error: an idempotent operation that takes the square root of the division of total by count.", "For estimation of the metric over a stream of data, the function creates an update_op operation that updates these variables and returns the root_mean_squared_error. Internally, a squared_error operation computes the element-wise square of the difference between predictions and labels. Then update_op increments total with the reduced sum of the product of weights and squared_error, and it increments count with the reduced sum of weights.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.sensitivity_at_specificity", "path": "compat/v1/metrics/sensitivity_at_specificity", "type": "tf.compat", "text": ["Computes the specificity at a given sensitivity.", "The sensitivity_at_specificity function creates four local variables, true_positives, true_negatives, false_positives and false_negatives that are used to compute the sensitivity at the given specificity value. The threshold for the given specificity value is computed and used to evaluate the corresponding sensitivity.", "For estimation of the metric over a stream of data, the function creates an update_op operation that updates these variables and returns the sensitivity. update_op increments the true_positives, true_negatives, false_positives and false_negatives counts with the weight of each case found in the predictions and labels.", "If weights is None, weights default to 1. Use weights of 0 to mask values.", "For additional information about specificity and sensitivity, see the following: https://en.wikipedia.org/wiki/Sensitivity_and_specificity"]}, {"name": "tf.compat.v1.metrics.sparse_average_precision_at_k", "path": "compat/v1/metrics/sparse_average_precision_at_k", "type": "tf.compat", "text": ["Renamed to average_precision_at_k, please use that method instead. (deprecated)"]}, {"name": "tf.compat.v1.metrics.sparse_precision_at_k", "path": "compat/v1/metrics/sparse_precision_at_k", "type": "tf.compat", "text": ["Renamed to precision_at_k, please use that method instead. (deprecated)"]}, {"name": "tf.compat.v1.metrics.specificity_at_sensitivity", "path": "compat/v1/metrics/specificity_at_sensitivity", "type": "tf.compat", "text": ["Computes the specificity at a given sensitivity.", "The specificity_at_sensitivity function creates four local variables, true_positives, true_negatives, false_positives and false_negatives that are used to compute the specificity at the given sensitivity value. The threshold for the given sensitivity value is computed and used to evaluate the corresponding specificity.", "For estimation of the metric over a stream of data, the function creates an update_op operation that updates these variables and returns the specificity. update_op increments the true_positives, true_negatives, false_positives and false_negatives counts with the weight of each case found in the predictions and labels.", "If weights is None, weights default to 1. Use weights of 0 to mask values.", "For additional information about specificity and sensitivity, see the following: https://en.wikipedia.org/wiki/Sensitivity_and_specificity"]}, {"name": "tf.compat.v1.metrics.true_negatives", "path": "compat/v1/metrics/true_negatives", "type": "tf.compat", "text": ["Sum the weights of true_negatives.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.true_negatives_at_thresholds", "path": "compat/v1/metrics/true_negatives_at_thresholds", "type": "tf.compat", "text": ["Computes true negatives at provided threshold values.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.true_positives", "path": "compat/v1/metrics/true_positives", "type": "tf.compat", "text": ["Sum the weights of true_positives.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.metrics.true_positives_at_thresholds", "path": "compat/v1/metrics/true_positives_at_thresholds", "type": "tf.compat", "text": ["Computes true positives at provided threshold values.", "If weights is None, weights default to 1. Use weights of 0 to mask values."]}, {"name": "tf.compat.v1.min_max_variable_partitioner", "path": "compat/v1/min_max_variable_partitioner", "type": "tf.compat", "text": ["Partitioner to allocate minimum size per slice.", "Returns a partitioner that partitions the variable of given shape and dtype such that each partition has a minimum of min_slice_size slice of the variable. The maximum number of such partitions (upper bound) is given by max_partitions."]}, {"name": "tf.compat.v1.mixed_precision", "path": "compat/v1/mixed_precision", "type": "tf.compat", "text": ["Public API for tf.mixed_precision namespace.", "experimental module: Public API for tf.mixed_precision.experimental namespace.", "class DynamicLossScale: Loss scale that dynamically adjusts itself.", "class FixedLossScale: Loss scale with a fixed value.", "class LossScale: Base class for all TF1 loss scales.", "class MixedPrecisionLossScaleOptimizer: An optimizer that applies loss scaling.", "disable_mixed_precision_graph_rewrite(...): Disables the mixed precision graph rewrite.", "enable_mixed_precision_graph_rewrite(...): Enable mixed precision via a graph rewrite."]}, {"name": "tf.compat.v1.mixed_precision.disable_mixed_precision_graph_rewrite", "path": "compat/v1/mixed_precision/disable_mixed_precision_graph_rewrite", "type": "tf.compat", "text": ["Disables the mixed precision graph rewrite.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.train.experimental.disable_mixed_precision_graph_rewrite", "After this is called, the mixed precision graph rewrite will no longer run for new Sessions, and so float32 operations will no longer be converted to float16 in such Sessions. However, any existing Sessions will continue to have the graph rewrite enabled if they were created after enable_mixed_precision_graph_rewrite was called but before disable_mixed_precision_graph_rewrite was called.", "This does not undo the effects of loss scaling. Any optimizers wrapped with a LossScaleOptimizer will continue to do loss scaling, although this loss scaling will no longer be useful if the optimizer is used in new Sessions, as the graph rewrite no longer converts the graph to use float16.", "This function is useful for unit testing. A unit tests can test using the mixed precision graph rewrite, then disable it so future unit tests continue using float32. If this is done, unit tests should not share a single session, as enable_mixed_precision_graph_rewrite and disable_mixed_precision_graph_rewrite have no effect on existing sessions."]}, {"name": "tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite", "path": "compat/v1/mixed_precision/enable_mixed_precision_graph_rewrite", "type": "tf.compat", "text": ["Enable mixed precision via a graph rewrite.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.train.experimental.enable_mixed_precision_graph_rewrite", "Mixed precision is the use of both float32 and float16 data types when training a model to improve performance. This is achieved via a graph rewrite operation and a loss-scale optimizer.", "Performing arithmetic operations in float16 takes advantage of specialized processing units, such as NVIDIA Tensor Cores, for much higher arithmetic throughput. However, due to the smaller representable range, performing the entire training with float16 can result in gradient underflow, that is, small gradient values becoming zeroes. Instead, performing only select arithmetic operations in float16 results in higher throughput and decreased training time when using compatible hardware accelerators while also reducing memory usage, typically without sacrificing model accuracy.", "Calling enable_mixed_precision_graph_rewrite(opt) enables the graph rewrite operation before computing gradients. The function additionally returns an Optimizer (opt) wrapped with a LossScaleOptimizer. This prevents underflow in the float16 tensors during the backward pass. An optimizer of type tf.train.Optimizer or tf.keras.optimizers.Optimizer must be passed to this function, which will then be wrapped to use loss scaling.", "The graph rewrite operation changes the dtype of certain operations in the graph from float32 to float16. There are several categories of operations that are either included or excluded by this rewrite operation. The following categories of Ops are defined inside corresponding functions under the class AutoMixedPrecisionLists in  auto_mixed_precision_lists.h:", "When this function is used, gradients should only be computed and applied with the returned optimizer, either by calling opt.minimize() or opt.compute_gradients() followed by opt.apply_gradients(). Gradients should not be computed with tf.gradients or tf.GradientTape. This is because the returned optimizer will apply loss scaling, and tf.gradients or tf.GradientTape will not. If you do directly use tf.gradients or tf.GradientTape, your model may not converge due to float16 underflow problems.", "When eager execution is enabled, the mixed precision graph rewrite is only enabled within tf.functions, as outside tf.functions, there is no graph.", "For NVIDIA GPUs with Tensor cores, as a general performance guide, dimensions (such as batch size, input size, output size, and channel counts) should be powers of two if under 256, or otherwise divisible by 8 if above 256. For more information, check out the NVIDIA Deep Learning Performance Guide.", "Currently, mixed precision is only enabled on NVIDIA Tensor Core GPUs with Compute Capability 7.0 and above (Volta, Turing, or newer architectures). The parts of the graph on CPUs and TPUs are untouched by the graph rewrite."]}, {"name": "tf.compat.v1.mixed_precision.experimental", "path": "compat/v1/mixed_precision/experimental", "type": "tf.compat", "text": ["Public API for tf.mixed_precision.experimental namespace.", "class DynamicLossScale: Loss scale that dynamically adjusts itself.", "class FixedLossScale: Loss scale with a fixed value.", "class LossScale: Base class for all TF1 loss scales."]}, {"name": "tf.compat.v1.mixed_precision.MixedPrecisionLossScaleOptimizer", "path": "compat/v1/mixed_precision/mixedprecisionlossscaleoptimizer", "type": "tf.compat", "text": ["An optimizer that applies loss scaling.", "Inherits From: Optimizer", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.train.experimental.MixedPrecisionLossScaleOptimizer", "Loss scaling is a process that multiplies the loss by a multiplier called the loss scale, and divides each gradient by the same multiplier. The pseudocode for this process is:", "Mathematically, loss scaling has no effect, but can help avoid numerical underflow in intermediate gradients when float16 tensors are used for mixed precision training. By multiplying the loss, each intermediate gradient will have the same multiplier applied.", "The loss scale can either be a fixed constant, chosen by the user, or be dynamically determined. Dynamically determining the loss scale is convenient as a loss scale does not have to be explicitly chosen. However it reduces performance.", "This optimizer wraps another optimizer and applies loss scaling to it via a LossScale. Loss scaling is applied whenever gradients are computed, such as through minimize().", "View source", "Apply gradients to variables.", "This is the second part of minimize(). It returns an Operation that conditionally applies gradients if all gradient values are finite. Otherwise no update is performed (nor is global_step incremented).", "View source", "Compute gradients of loss for the variables in var_list.", "This adjusts the dynamic range of the gradient evaluation by scaling up the loss value. The gradient values are then scaled back down by the reciprocal of the loss scale. This is useful in reduced precision training where small gradient values would otherwise underflow the representable range.", "View source", "View source", "Return a slot named name created for var by the Optimizer.", "Some Optimizer subclasses use additional variables. For example Momentum and Adagrad use variables to accumulate updates. This method gives access to these Variable objects if for some reason you need them.", "Use get_slot_names() to get the list of slot names created by the Optimizer.", "View source", "Return a list of the names of slots created by the Optimizer.", "See get_slot().", "View source", "Add operations to minimize loss by updating var_list.", "This method simply combines calls compute_gradients() and apply_gradients(). If you want to process the gradient before applying them call compute_gradients() and apply_gradients() explicitly instead of using this function.", "When eager execution is enabled, loss should be a Python function that takes no arguments and computes the value to be minimized. Minimization (and gradient computation) is done with respect to the elements of var_list if not None, else with respect to any trainable variables created during the execution of the loss function. gate_gradients, aggregation_method, colocate_gradients_with_ops and grad_loss are ignored when eager execution is enabled.", "View source", "Returns the variables of the Optimizer."]}, {"name": "tf.compat.v1.mlir", "path": "compat/v1/mlir", "type": "tf.compat", "text": ["Public API for tf.mlir namespace.", "experimental module: Public API for tf.mlir.experimental namespace."]}, {"name": "tf.compat.v1.mlir.experimental", "path": "compat/v1/mlir/experimental", "type": "tf.compat", "text": ["Public API for tf.mlir.experimental namespace.", "convert_function(...): Import a ConcreteFunction and convert it to a textual MLIR module.", "convert_graph_def(...): Import a GraphDef and convert it to a textual MLIR module."]}, {"name": "tf.compat.v1.model_variables", "path": "compat/v1/model_variables", "type": "tf.compat", "text": ["Returns all variables in the MODEL_VARIABLES collection."]}, {"name": "tf.compat.v1.moving_average_variables", "path": "compat/v1/moving_average_variables", "type": "tf.compat", "text": ["Returns all variables that maintain their moving averages.", "If an ExponentialMovingAverage object is created and the apply() method is called on a list of variables, these variables will be added to the GraphKeys.MOVING_AVERAGE_VARIABLES collection. This convenience function returns the contents of that collection."]}, {"name": "tf.compat.v1.multinomial", "path": "compat/v1/multinomial", "type": "tf.compat", "text": ["Draws samples from a multinomial distribution. (deprecated)", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.random.multinomial"]}, {"name": "tf.compat.v1.NameAttrList", "path": "compat/v1/nameattrlist", "type": "tf.compat", "text": ["A ProtocolMessage", "class AttrEntry"]}, {"name": "tf.compat.v1.NameAttrList.AttrEntry", "path": "compat/v1/nameattrlist/attrentry", "type": "tf.compat", "text": ["A ProtocolMessage"]}, {"name": "tf.compat.v1.nest", "path": "compat/v1/nest", "type": "tf.compat", "text": ["Public API for tf.nest namespace.", "assert_same_structure(...): Asserts that two structures are nested in the same way.", "flatten(...): Returns a flat list from a given nested structure.", "is_nested(...): Returns true if its input is a collections.abc.Sequence (except strings).", "map_structure(...): Applies func to each entry in structure and returns a new structure.", "pack_sequence_as(...): Returns a given flattened sequence packed into a given structure."]}, {"name": "tf.compat.v1.nn", "path": "compat/v1/nn", "type": "tf.compat", "text": ["Wrappers for primitive Neural Net (NN) Operations.", "rnn_cell module: Module for constructing RNN Cells.", "all_candidate_sampler(...): Generate the set of all classes.", "atrous_conv2d(...): Atrous convolution (a.k.a. convolution with holes or dilated convolution).", "atrous_conv2d_transpose(...): The transpose of atrous_conv2d.", "avg_pool(...): Performs the average pooling on the input.", "avg_pool1d(...): Performs the average pooling on the input.", "avg_pool2d(...): Performs the average pooling on the input.", "avg_pool3d(...): Performs the average pooling on the input.", "avg_pool_v2(...): Performs the avg pooling on the input.", "batch_norm_with_global_normalization(...): Batch normalization.", "batch_normalization(...): Batch normalization.", "bias_add(...): Adds bias to value.", "bidirectional_dynamic_rnn(...): Creates a dynamic version of bidirectional recurrent neural network. (deprecated)", "collapse_repeated(...): Merge repeated labels into single labels.", "compute_accidental_hits(...): Compute the position ids in sampled_candidates matching true_classes.", "compute_average_loss(...): Scales per-example losses with sample_weights and computes their average.", "conv1d(...): Computes a 1-D convolution of input with rank >=3 and a 3-D filter. (deprecated argument values) (deprecated argument values)", "conv1d_transpose(...): The transpose of conv1d.", "conv2d(...): Computes a 2-D convolution given 4-D input and filter tensors.", "conv2d_backprop_filter(...): Computes the gradients of convolution with respect to the filter.", "conv2d_backprop_input(...): Computes the gradients of convolution with respect to the input.", "conv2d_transpose(...): The transpose of conv2d.", "conv3d(...): Computes a 3-D convolution given 5-D input and filter tensors.", "conv3d_backprop_filter(...): Computes the gradients of 3-D convolution with respect to the filter.", "conv3d_backprop_filter_v2(...): Computes the gradients of 3-D convolution with respect to the filter.", "conv3d_transpose(...): The transpose of conv3d.", "conv_transpose(...): The transpose of convolution.", "convolution(...): Computes sums of N-D convolutions (actually cross-correlation).", "crelu(...): Computes Concatenated ReLU.", "ctc_beam_search_decoder(...): Performs beam search decoding on the logits given in input.", "ctc_beam_search_decoder_v2(...): Performs beam search decoding on the logits given in input.", "ctc_greedy_decoder(...): Performs greedy decoding on the logits given in input (best path).", "ctc_loss(...): Computes the CTC (Connectionist Temporal Classification) Loss.", "ctc_loss_v2(...): Computes CTC (Connectionist Temporal Classification) loss.", "ctc_unique_labels(...): Get unique labels and indices for batched labels for tf.nn.ctc_loss.", "depth_to_space(...): DepthToSpace for tensors of type T.", "depthwise_conv2d(...): Depthwise 2-D convolution.", "depthwise_conv2d_backprop_filter(...): Computes the gradients of depthwise convolution with respect to the filter.", "depthwise_conv2d_backprop_input(...): Computes the gradients of depthwise convolution with respect to the input.", "depthwise_conv2d_native(...): Computes a 2-D depthwise convolution.", "depthwise_conv2d_native_backprop_filter(...): Computes the gradients of depthwise convolution with respect to the filter.", "depthwise_conv2d_native_backprop_input(...): Computes the gradients of depthwise convolution with respect to the input.", "dilation2d(...): Computes the grayscale dilation of 4-D input and 3-D filter tensors.", "dropout(...): Computes dropout. (deprecated arguments)", "dynamic_rnn(...): Creates a recurrent neural network specified by RNNCell cell. (deprecated)", "elu(...): Computes exponential linear: exp(features) - 1 if < 0, features otherwise.", "embedding_lookup(...): Looks up embeddings for the given ids from a list of tensors.", "embedding_lookup_sparse(...): Looks up embeddings for the given ids and weights from a list of tensors.", "erosion2d(...): Computes the grayscale erosion of 4-D value and 3-D kernel tensors.", "fixed_unigram_candidate_sampler(...): Samples a set of classes using the provided (fixed) base distribution.", "fractional_avg_pool(...): Performs fractional average pooling on the input. (deprecated)", "fractional_max_pool(...): Performs fractional max pooling on the input. (deprecated)", "fused_batch_norm(...): Batch normalization.", "in_top_k(...): Says whether the targets are in the top K predictions.", "l2_loss(...): L2 Loss.", "l2_normalize(...): Normalizes along dimension axis using an L2 norm. (deprecated arguments)", "leaky_relu(...): Compute the Leaky ReLU activation function.", "learned_unigram_candidate_sampler(...): Samples a set of classes from a distribution learned during training.", "local_response_normalization(...): Local Response Normalization.", "log_poisson_loss(...): Computes log Poisson loss given log_input.", "log_softmax(...): Computes log softmax activations. (deprecated arguments)", "log_uniform_candidate_sampler(...): Samples a set of classes using a log-uniform (Zipfian) base distribution.", "lrn(...): Local Response Normalization.", "max_pool(...): Performs the max pooling on the input.", "max_pool1d(...): Performs the max pooling on the input.", "max_pool2d(...): Performs the max pooling on the input.", "max_pool3d(...): Performs the max pooling on the input.", "max_pool_v2(...): Performs the max pooling on the input.", "max_pool_with_argmax(...): Performs max pooling on the input and outputs both max values and indices.", "moments(...): Calculate the mean and variance of x.", "nce_loss(...): Computes and returns the noise-contrastive estimation training loss.", "normalize_moments(...): Calculate the mean and variance of based on the sufficient statistics.", "pool(...): Performs an N-D pooling operation.", "quantized_avg_pool(...): Produces the average pool of the input tensor for quantized types.", "quantized_conv2d(...): Computes a 2D convolution given quantized 4D input and filter tensors.", "quantized_max_pool(...): Produces the max pool of the input tensor for quantized types.", "quantized_relu_x(...): Computes Quantized Rectified Linear X: min(max(features, 0), max_value)", "raw_rnn(...): Creates an RNN specified by RNNCell cell and loop function loop_fn.", "relu(...): Computes rectified linear: max(features, 0).", "relu6(...): Computes Rectified Linear 6: min(max(features, 0), 6).", "relu_layer(...): Computes Relu(x * weight + biases).", "safe_embedding_lookup_sparse(...): Lookup embedding results, accounting for invalid IDs and empty features.", "sampled_softmax_loss(...): Computes and returns the sampled softmax training loss.", "scale_regularization_loss(...): Scales the sum of the given regularization losses by number of replicas.", "selu(...): Computes scaled exponential linear: scale * alpha * (exp(features) - 1)", "separable_conv2d(...): 2-D convolution with separable filters.", "sigmoid(...): Computes sigmoid of x element-wise.", "sigmoid_cross_entropy_with_logits(...): Computes sigmoid cross entropy given logits.", "silu(...): Computes the SiLU or Swish activation function: x * sigmoid(x).", "softmax(...): Computes softmax activations. (deprecated arguments)", "softmax_cross_entropy_with_logits(...): Computes softmax cross entropy between logits and labels. (deprecated)", "softmax_cross_entropy_with_logits_v2(...): Computes softmax cross entropy between logits and labels. (deprecated arguments)", "softplus(...): Computes softplus: log(exp(features) + 1).", "softsign(...): Computes softsign: features / (abs(features) + 1).", "space_to_batch(...): SpaceToBatch for 4-D tensors of type T.", "space_to_depth(...): SpaceToDepth for tensors of type T.", "sparse_softmax_cross_entropy_with_logits(...): Computes sparse softmax cross entropy between logits and labels.", "static_bidirectional_rnn(...): Creates a bidirectional recurrent neural network. (deprecated)", "static_rnn(...): Creates a recurrent neural network specified by RNNCell cell. (deprecated)", "static_state_saving_rnn(...): RNN that accepts a state saver for time-truncated RNN calculation. (deprecated)", "sufficient_statistics(...): Calculate the sufficient statistics for the mean and variance of x.", "swish(...): Computes the SiLU or Swish activation function: x * sigmoid(x).", "tanh(...): Computes hyperbolic tangent of x element-wise.", "top_k(...): Finds values and indices of the k largest entries for the last dimension.", "uniform_candidate_sampler(...): Samples a set of classes using a uniform base distribution.", "weighted_cross_entropy_with_logits(...): Computes a weighted cross entropy. (deprecated arguments)", "weighted_moments(...): Returns the frequency-weighted mean and variance of x.", "with_space_to_batch(...): Performs op on the space-to-batch representation of input.", "xw_plus_b(...): Computes matmul(x, weights) + biases.", "zero_fraction(...): Returns the fraction of zeros in value."]}, {"name": "tf.compat.v1.nn.avg_pool", "path": "compat/v1/nn/avg_pool", "type": "tf.compat", "text": ["Performs the average pooling on the input.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.nn.avg_pool2d", "Each entry in output is the mean of the corresponding size ksize window in value."]}, {"name": "tf.compat.v1.nn.batch_norm_with_global_normalization", "path": "compat/v1/nn/batch_norm_with_global_normalization", "type": "tf.compat", "text": ["Batch normalization.", "This op is deprecated. See tf.nn.batch_normalization.", "Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift: Ioffe et al., 2015 (pdf)"]}, {"name": "tf.compat.v1.nn.bidirectional_dynamic_rnn", "path": "compat/v1/nn/bidirectional_dynamic_rnn", "type": "tf.compat", "text": ["Creates a dynamic version of bidirectional recurrent neural network. (deprecated)", "Takes input and builds independent forward and backward RNNs. The input_size of forward and backward cell must match. The initial state for both directions is zero by default (but can be set optionally) and no intermediate states are ever returned -- the network is fully unrolled for the given (passed in) length(s) of the sequence(s) or completely unrolled if length(s) is not given."]}, {"name": "tf.compat.v1.nn.conv1d", "path": "compat/v1/nn/conv1d", "type": "tf.compat", "text": [" ", "Computes a 1-D convolution of input with rank >=3 and a 3-D filter. (deprecated argument values) (deprecated argument values)", "Given an input tensor of shape batch_shape + [in_width, in_channels] if data_format is \"NWC\", or batch_shape + [in_channels, in_width] if data_format is \"NCW\", and a filter / kernel tensor of shape [filter_width, in_channels, out_channels], this op reshapes the arguments to pass them to conv2d to perform the equivalent convolution operation.", "Internally, this op reshapes the input tensors and invokes tf.nn.conv2d. For example, if data_format does not start with \"NC\", a tensor of shape batch_shape + [in_width, in_channels] is reshaped to batch_shape + [1, in_width, in_channels], and the filter is reshaped to [1, filter_width, in_channels, out_channels]. The result is then reshaped back to batch_shape + [out_width, out_channels] (where out_width is a function of the stride and padding as in conv2d) and returned to the caller."]}, {"name": "tf.compat.v1.nn.conv2d", "path": "compat/v1/nn/conv2d", "type": "tf.compat", "text": ["Computes a 2-D convolution given 4-D input and filter tensors.", "Given an input tensor of shape [batch, in_height, in_width, in_channels] and a filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels], this op performs the following:", "In detail, with the default NHWC format,", "Must have strides[0] = strides[3] = 1. For the most common case of the same horizontal and vertical strides, strides = [1, stride, stride, 1]."]}, {"name": "tf.compat.v1.nn.conv2d_backprop_filter", "path": "compat/v1/nn/conv2d_backprop_filter", "type": "tf.compat", "text": ["Computes the gradients of convolution with respect to the filter."]}, {"name": "tf.compat.v1.nn.conv2d_backprop_input", "path": "compat/v1/nn/conv2d_backprop_input", "type": "tf.compat", "text": ["Computes the gradients of convolution with respect to the input."]}, {"name": "tf.compat.v1.nn.conv2d_transpose", "path": "compat/v1/nn/conv2d_transpose", "type": "tf.compat", "text": ["The transpose of conv2d.", "This operation is sometimes called \"deconvolution\" after (Zeiler et al., 2010), but is really the transpose (gradient) of conv2d rather than an actual deconvolution.", "Deconvolutional Networks: Zeiler et al., 2010 (pdf)"]}, {"name": "tf.compat.v1.nn.conv3d", "path": "compat/v1/nn/conv3d", "type": "tf.compat", "text": ["Computes a 3-D convolution given 5-D input and filter tensors.", "In signal processing, cross-correlation is a measure of similarity of two waveforms as a function of a time-lag applied to one of them. This is also known as a sliding dot product or sliding inner-product.", "Our Conv3D implements a form of cross-correlation."]}, {"name": "tf.compat.v1.nn.conv3d_backprop_filter", "path": "compat/v1/nn/conv3d_backprop_filter", "type": "tf.compat", "text": ["Computes the gradients of 3-D convolution with respect to the filter.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.nn.conv3d_backprop_filter_v2"]}, {"name": "tf.compat.v1.nn.conv3d_transpose", "path": "compat/v1/nn/conv3d_transpose", "type": "tf.compat", "text": ["The transpose of conv3d.", "This operation is sometimes called \"deconvolution\" after (Zeiler et al., 2010), but is really the transpose (gradient) of conv3d rather than an actual deconvolution.", "Deconvolutional Networks: Zeiler et al., 2010 (pdf)"]}, {"name": "tf.compat.v1.nn.convolution", "path": "compat/v1/nn/convolution", "type": "tf.compat", "text": ["Computes sums of N-D convolutions (actually cross-correlation).", "This also supports either output striding via the optional strides parameter or atrous convolution (also known as convolution with holes or dilated convolution, based on the French word \"trous\" meaning holes in English) via the optional dilation_rate parameter. Currently, however, output striding is not supported for atrous convolutions.", "Specifically, in the case that data_format does not start with \"NC\", given a rank (N+2) input Tensor of shape", "[num_batches, input_spatial_shape[0], ..., input_spatial_shape[N-1], num_input_channels],", "a rank (N+2) filter Tensor of shape", "[spatial_filter_shape[0], ..., spatial_filter_shape[N-1], num_input_channels, num_output_channels],", "an optional dilation_rate tensor of shape N specifying the filter upsampling/input downsampling rate, and an optional list of N strides (defaulting [1]*N), this computes for each N-D spatial output position (x[0], ..., x[N-1]):", "where b is the index into the batch, k is the output channel number, q is the input channel number, and z is the N-D spatial offset within the filter. Here, padded_input is obtained by zero padding the input using an effective spatial filter shape of (spatial_filter_shape-1) * dilation_rate + 1 and output striding strides as described in the comment here.", "In the case that data_format does start with \"NC\", the input and output (but not the filter) are simply transposed as follows:", "convolution(input, data_format, **kwargs) = tf.transpose(convolution(tf.transpose(input, [0] + range(2,N+2) + [1]), **kwargs), [0, N+1] + range(1, N+1))", "It is required that 1 <= N <= 3.", "[batch_size] + output_spatial_shape + [out_channels]", "if data_format is None or does not start with \"NC\", or", "[batch_size, out_channels] + output_spatial_shape", "if data_format starts with \"NC\", where output_spatial_shape depends on the value of padding.", "If padding == \"SAME\": output_spatial_shape[i] = ceil(input_spatial_shape[i] / strides[i])", "If padding == \"VALID\": output_spatial_shape[i] = ceil((input_spatial_shape[i] - (spatial_filter_shape[i]-1) * dilation_rate[i]) / strides[i]). "]}, {"name": "tf.compat.v1.nn.crelu", "path": "compat/v1/nn/crelu", "type": "tf.compat", "text": ["Computes Concatenated ReLU.", "Concatenates a ReLU which selects only the positive part of the activation with a ReLU which selects only the negative part of the activation. Note that as a result this non-linearity doubles the depth of the activations. Source: Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units. W. Shang, et al.", "Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units: Shang et al., 2016 (pdf)"]}, {"name": "tf.compat.v1.nn.ctc_beam_search_decoder", "path": "compat/v1/nn/ctc_beam_search_decoder", "type": "tf.compat", "text": ["Performs beam search decoding on the logits given in input.", "If merge_repeated is True, merge repeated classes in the output beams. This means that if consecutive entries in a beam are the same, only the first of these is emitted. That is, when the sequence is A B B * B * B (where '*' is the blank label), the return value is:", "decoded[j].indices: Indices matrix (total_decoded_outputs[j] x 2) The rows store: [batch, time].", "decoded[j].values: Values vector, size (total_decoded_outputs[j]). The vector stores the decoded classes for beam j.", "decoded[j].dense_shape: Shape vector, size (2). The shape values are: [batch_size, max_decoded_length[j]]. "]}, {"name": "tf.compat.v1.nn.ctc_loss", "path": "compat/v1/nn/ctc_loss", "type": "tf.compat", "text": ["Computes the CTC (Connectionist Temporal Classification) Loss.", "This op implements the CTC loss as presented in (Graves et al., 2006).", "This class performs the softmax operation for you, so inputs should be e.g. linear projections of outputs by an LSTM.", "The inputs Tensor's innermost dimension size, num_classes, represents num_labels + 1 classes, where num_labels is the number of true labels, and the largest value (num_classes - 1) is reserved for the blank label.", "For example, for a vocabulary containing 3 labels [a, b, c], num_classes = 4 and the labels indexing is {a: 0, b: 1, c: 2, blank: 3}.", "Regarding the arguments preprocess_collapse_repeated and ctc_merge_repeated:", "If preprocess_collapse_repeated is True, then a preprocessing step runs before loss calculation, wherein repeated labels passed to the loss are merged into single labels. This is useful if the training labels come from, e.g., forced alignments and therefore have unnecessary repetitions.", "If ctc_merge_repeated is set False, then deep within the CTC calculation, repeated non-blank labels will not be merged and are interpreted as individual labels. This is a simplified (non-standard) version of CTC.", "Here is a table of the (roughly) expected first order behavior:", "preprocess_collapse_repeated=False, ctc_merge_repeated=True", "Classical CTC behavior: Outputs true repeated classes with blanks in between, and can also output repeated classes with no blanks in between that need to be collapsed by the decoder.", "preprocess_collapse_repeated=True, ctc_merge_repeated=False", "Never learns to output repeated classes, as they are collapsed in the input labels before training.", "preprocess_collapse_repeated=False, ctc_merge_repeated=False", "Outputs repeated classes with blanks in between, but generally does not require the decoder to collapse/merge repeated classes.", "preprocess_collapse_repeated=True, ctc_merge_repeated=True", "Untested. Very likely will not learn to output repeated classes.", "The ignore_longer_outputs_than_inputs option allows to specify the behavior of the CTCLoss when dealing with sequences that have longer outputs than inputs. If true, the CTCLoss will simply return zero gradient for those items, otherwise an InvalidArgument error is returned, stopping training.", "Connectionist Temporal Classification - Labeling Unsegmented Sequence Data with Recurrent Neural Networks: Graves et al., 2006 (pdf)"]}, {"name": "tf.compat.v1.nn.ctc_loss_v2", "path": "compat/v1/nn/ctc_loss_v2", "type": "tf.compat", "text": ["Computes CTC (Connectionist Temporal Classification) loss.", "This op implements the CTC loss as presented in (Graves et al., 2006).", "Connectionist Temporal Classification - Labeling Unsegmented Sequence Data with Recurrent Neural Networks: Graves et al., 2006 (pdf)"]}, {"name": "tf.compat.v1.nn.depthwise_conv2d", "path": "compat/v1/nn/depthwise_conv2d", "type": "tf.compat", "text": ["Depthwise 2-D convolution.", "Given a 4D input tensor ('NHWC' or 'NCHW' data formats) and a filter tensor of shape [filter_height, filter_width, in_channels, channel_multiplier] containing in_channels convolutional filters of depth 1, depthwise_conv2d applies a different filter to each input channel (expanding from 1 channel to channel_multiplier channels for each), then concatenates the results together. The output has in_channels * channel_multiplier channels.", "In detail, with the default NHWC format,", "Must have strides[0] = strides[3] = 1. For the most common case of the same horizontal and vertical strides, strides = [1, stride, stride, 1]. If any value in rate is greater than 1, we perform atrous depthwise convolution, in which case all values in the strides tensor must be equal to 1."]}, {"name": "tf.compat.v1.nn.depthwise_conv2d_native", "path": "compat/v1/nn/depthwise_conv2d_native", "type": "tf.compat", "text": ["Computes a 2-D depthwise convolution.", "Given an input tensor of shape [batch, in_height, in_width, in_channels] and a filter / kernel tensor of shape [filter_height, filter_width, in_channels, channel_multiplier], containing in_channels convolutional filters of depth 1, depthwise_conv2d applies a different filter to each input channel (expanding from 1 channel to channel_multiplier channels for each), then concatenates the results together. Thus, the output has in_channels * channel_multiplier channels.", "Must have strides[0] = strides[3] = 1. For the most common case of the same horizontal and vertices strides, strides = [1, stride, stride, 1]."]}, {"name": "tf.compat.v1.nn.dilation2d", "path": "compat/v1/nn/dilation2d", "type": "tf.compat", "text": ["Computes the grayscale dilation of 4-D input and 3-D filter tensors.", "The input tensor has shape [batch, in_height, in_width, depth] and the filter tensor has shape [filter_height, filter_width, depth], i.e., each input channel is processed independently of the others with its own structuring function. The output tensor has shape [batch, out_height, out_width, depth]. The spatial dimensions of the output tensor depend on the padding algorithm. We currently only support the default \"NHWC\" data_format.", "In detail, the grayscale morphological 2-D dilation is the max-sum correlation (for consistency with conv2d, we use unmirrored filters):", "Max-pooling is a special case when the filter has size equal to the pooling kernel size and contains all zeros.", "Note on duality: The dilation of input by the filter is equal to the negation of the erosion of -input by the reflected filter."]}, {"name": "tf.compat.v1.nn.dropout", "path": "compat/v1/nn/dropout", "type": "tf.compat", "text": ["Computes dropout. (deprecated arguments)", "For each element of x, with probability rate, outputs 0, and otherwise scales up the input by 1 / (1-rate). The scaling is such that the expected sum is unchanged.", "By default, each element is kept or dropped independently. If noise_shape is specified, it must be broadcastable to the shape of x, and only dimensions with noise_shape[i] == shape(x)[i] will make independent decisions. For example, if shape(x) = [k, l, m, n] and noise_shape = [k, 1, 1, n], each batch and channel component will be kept independently and each row and column will be kept or not kept together."]}, {"name": "tf.compat.v1.nn.dynamic_rnn", "path": "compat/v1/nn/dynamic_rnn", "type": "tf.compat", "text": ["Creates a recurrent neural network specified by RNNCell cell. (deprecated)", "Performs fully dynamic unrolling of inputs.", "If time_major == False (default), this will be a Tensor shaped: [batch_size, max_time, cell.output_size].", "If time_major == True, this will be a Tensor shaped: [max_time, batch_size, cell.output_size].", "Note, if cell.output_size is a (possibly nested) tuple of integers or TensorShape objects, then outputs will be a tuple having the same structure as cell.output_size, containing Tensors having shapes corresponding to the shape data in cell.output_size. "]}, {"name": "tf.compat.v1.nn.embedding_lookup", "path": "compat/v1/nn/embedding_lookup", "type": "tf.compat", "text": ["Looks up embeddings for the given ids from a list of tensors.", "This function is used to perform parallel lookups on the list of tensors in params. It is a generalization of tf.gather, where params is interpreted as a partitioning of a large embedding tensor. params may be a PartitionedVariable as returned by using tf.compat.v1.get_variable() with a partitioner.", "If len(params) > 1, each element id of ids is partitioned between the elements of params according to the partition_strategy. In all strategies, if the id space does not evenly divide the number of partitions, each of the first (max_id + 1) % len(params) partitions will be assigned one more id.", "If partition_strategy is \"mod\", we assign each id to partition p = id % len(params). For instance, 13 ids are split across 5 partitions as: [[0, 5, 10], [1, 6, 11], [2, 7, 12], [3, 8], [4, 9]]", "If partition_strategy is \"div\", we assign ids to partitions in a contiguous manner. In this case, 13 ids are split across 5 partitions as: [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12]]", "If the input ids are ragged tensors, partition variables are not supported and the partition strategy and the max_norm are ignored. The results of the lookup are concatenated into a dense tensor. The returned tensor has shape shape(ids) + shape(params)[1:]."]}, {"name": "tf.compat.v1.nn.embedding_lookup_sparse", "path": "compat/v1/nn/embedding_lookup_sparse", "type": "tf.compat", "text": ["Looks up embeddings for the given ids and weights from a list of tensors.", "This op assumes that there is at least one id for each row in the dense tensor represented by sp_ids (i.e. there are no rows with empty features), and that all the indices of sp_ids are in canonical row-major order.", "sp_ids and sp_weights (if not None) are SparseTensors with rank of 2. Embeddings are always aggregated along the last dimension.", "It also assumes that all id values lie in the range [0, p0), where p0 is the sum of the size of params along dimension 0.", "In other words, if", "shape(combined params) = [p0, p1, ..., pm]", "and", "shape(sp_ids) = shape(sp_weights) = [d0, d1]", "then", "shape(output) = [d0, p1, ..., pm].", "For instance, if params is a 10x20 matrix, and sp_ids / sp_weights are", "with combiner=\"mean\", then the output will be a 3x20 matrix where"]}, {"name": "tf.compat.v1.nn.erosion2d", "path": "compat/v1/nn/erosion2d", "type": "tf.compat", "text": ["Computes the grayscale erosion of 4-D value and 3-D kernel tensors.", "The value tensor has shape [batch, in_height, in_width, depth] and the kernel tensor has shape [kernel_height, kernel_width, depth], i.e., each input channel is processed independently of the others with its own structuring function. The output tensor has shape [batch, out_height, out_width, depth]. The spatial dimensions of the output tensor depend on the padding algorithm. We currently only support the default \"NHWC\" data_format.", "In detail, the grayscale morphological 2-D erosion is given by:", "Duality: The erosion of value by the kernel is equal to the negation of the dilation of -value by the reflected kernel."]}, {"name": "tf.compat.v1.nn.fractional_avg_pool", "path": "compat/v1/nn/fractional_avg_pool", "type": "tf.compat", "text": ["Performs fractional average pooling on the input. (deprecated)", "This is a deprecated version of fractional_avg_pool.", "Fractional average pooling is similar to Fractional max pooling in the pooling region generation step. The only difference is that after pooling regions are generated, a mean operation is performed instead of a max operation in each pooling region.", "A tuple of Tensor objects (output, row_pooling_sequence, col_pooling_sequence). output: Output Tensor after fractional avg pooling. Has the same type as value. row_pooling_sequence: A Tensor of type int64. col_pooling_sequence: A Tensor of type int64.", "Fractional Max-Pooling: Graham, 2015 (pdf)"]}, {"name": "tf.compat.v1.nn.fractional_max_pool", "path": "compat/v1/nn/fractional_max_pool", "type": "tf.compat", "text": ["Performs fractional max pooling on the input. (deprecated)", "This is a deprecated version of fractional_max_pool.", "Fractional max pooling is slightly different than regular max pooling. In regular max pooling, you downsize an input set by taking the maximum value of smaller N x N subsections of the set (often 2x2), and try to reduce the set by a factor of N, where N is an integer. Fractional max pooling, as you might expect from the word \"fractional\", means that the overall reduction ratio N does not have to be an integer.", "The sizes of the pooling regions are generated randomly but are fairly uniform. For example, let's look at the height dimension, and the constraints on the list of rows that will be pool boundaries.", "First we define the following:", "Then, row_pooling_sequence should satisfy:", "A tuple of Tensor objects (output, row_pooling_sequence, col_pooling_sequence). output: Output Tensor after fractional max pooling. Has the same type as value. row_pooling_sequence: A Tensor of type int64. col_pooling_sequence: A Tensor of type int64.", "Fractional Max-Pooling: Graham, 2015 (pdf)"]}, {"name": "tf.compat.v1.nn.fused_batch_norm", "path": "compat/v1/nn/fused_batch_norm", "type": "tf.compat", "text": ["Batch normalization.", "See Source: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift; S. Ioffe, C. Szegedy.", "Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift: Ioffe et al., 2015 (pdf)"]}, {"name": "tf.compat.v1.nn.max_pool", "path": "compat/v1/nn/max_pool", "type": "tf.compat", "text": ["Performs the max pooling on the input."]}, {"name": "tf.compat.v1.nn.max_pool_with_argmax", "path": "compat/v1/nn/max_pool_with_argmax", "type": "tf.compat", "text": ["Performs max pooling on the input and outputs both max values and indices.", "The indices in argmax are flattened, so that a maximum value at position [b, y, x, c] becomes flattened index: (y * width + x) * channels + c if include_batch_in_index is False; ((b * height + y) * width + x) * channels + c if include_batch_in_index is True.", "The indices returned are always in [0, height) x [0, width) before flattening, even if padding is involved and the mathematically correct answer is outside (either negative or too large). This is a bug, but fixing it is difficult to do in a safe backwards compatible way, especially due to flattening."]}, {"name": "tf.compat.v1.nn.moments", "path": "compat/v1/nn/moments", "type": "tf.compat", "text": ["Calculate the mean and variance of x.", "The mean and variance are calculated by aggregating the contents of x across axes. If x is 1-D and axes = [0] this is just the mean and variance of a vector.", "When using these moments for batch normalization (see tf.nn.batch_normalization):"]}, {"name": "tf.compat.v1.nn.nce_loss", "path": "compat/v1/nn/nce_loss", "type": "tf.compat", "text": ["Computes and returns the noise-contrastive estimation training loss.", "A common use case is to use this method for training, and calculate the full sigmoid loss for evaluation or inference. In this case, you must set partition_strategy=\"div\" for the two losses to be consistent, as in the following example:", "Noise-contrastive estimation - A new estimation principle for unnormalized statistical models: Gutmann et al., 2010 (pdf)"]}, {"name": "tf.compat.v1.nn.pool", "path": "compat/v1/nn/pool", "type": "tf.compat", "text": ["Performs an N-D pooling operation.", "In the case that data_format does not start with \"NC\", computes for 0 <= b < batch_size, 0 <= x[i] < output_spatial_shape[i], 0 <= c < num_channels:", "where the reduction function REDUCE depends on the value of pooling_type, and pad_before is defined based on the value of padding as described in the \"returns\" section of tf.nn.convolution for details. The reduction never includes out-of-bounds positions.", "In the case that data_format starts with \"NC\", the input and output are simply transposed as follows:", "if data_format is None or does not start with \"NC\", or", "[batch_size, num_channels] + output_spatial_shape", "if data_format starts with \"NC\", where output_spatial_shape depends on the value of padding:", "If padding = \"SAME\": output_spatial_shape[i] = ceil(input_spatial_shape[i] / strides[i])", "If padding = \"VALID\": output_spatial_shape[i] = ceil((input_spatial_shape[i] - (window_shape[i] - 1) * dilation_rate[i]) / strides[i]). "]}, {"name": "tf.compat.v1.nn.quantized_avg_pool", "path": "compat/v1/nn/quantized_avg_pool", "type": "tf.compat", "text": ["Produces the average pool of the input tensor for quantized types."]}, {"name": "tf.compat.v1.nn.quantized_conv2d", "path": "compat/v1/nn/quantized_conv2d", "type": "tf.compat", "text": ["Computes a 2D convolution given quantized 4D input and filter tensors.", "The inputs are quantized tensors where the lowest value represents the real number of the associated minimum, and the highest represents the maximum. This means that you can only interpret the quantized output in the same way, by taking the returned minimum and maximum values into account."]}, {"name": "tf.compat.v1.nn.quantized_max_pool", "path": "compat/v1/nn/quantized_max_pool", "type": "tf.compat", "text": ["Produces the max pool of the input tensor for quantized types."]}, {"name": "tf.compat.v1.nn.quantized_relu_x", "path": "compat/v1/nn/quantized_relu_x", "type": "tf.compat", "text": ["Computes Quantized Rectified Linear X: min(max(features, 0), max_value)"]}, {"name": "tf.compat.v1.nn.raw_rnn", "path": "compat/v1/nn/raw_rnn", "type": "tf.compat", "text": ["Creates an RNN specified by RNNCell cell and loop function loop_fn.", "This function is a more primitive version of dynamic_rnn that provides more direct access to the inputs each iteration. It also provides more control over when to start and finish reading the sequence, and what to emit for the output.", "For example, it can be used to implement the dynamic decoder of a seq2seq model.", "Instead of working with Tensor objects, most operations work with TensorArray objects directly.", "The operation of raw_rnn, in pseudo-code, is basically the following:", "with the additional properties that output and state may be (possibly nested) tuples, as determined by cell.output_size and cell.state_size, and as a result the final state and emit_ta may themselves be tuples.", "A simple implementation of dynamic_rnn via raw_rnn looks like this:", "emit_ta: The RNN output TensorArray. If loop_fn returns a (possibly nested) set of Tensors for emit_output during initialization, (inputs time = 0, cell_output = None, and loop_state = None), then emit_ta will have the same structure, dtypes, and shapes as emit_output instead. If loop_fn returns emit_output = None during this call, the structure of cell.output_size is used: If cell.output_size is a (possibly nested) tuple of integers or TensorShape objects, then emit_ta will be a tuple having the same structure as cell.output_size, containing TensorArrays whose elements' shapes correspond to the shape data in cell.output_size.", "final_state: The final cell state. If cell.state_size is an int, this will be shaped [batch_size, cell.state_size]. If it is a TensorShape, this will be shaped [batch_size] + cell.state_size. If it is a (possibly nested) tuple of ints or TensorShape, this will be a tuple having the corresponding shapes.", "final_loop_state: The final loop state as returned by loop_fn. "]}, {"name": "tf.compat.v1.nn.relu_layer", "path": "compat/v1/nn/relu_layer", "type": "tf.compat", "text": ["Computes Relu(x * weight + biases)."]}, {"name": "tf.compat.v1.nn.rnn_cell", "path": "compat/v1/nn/rnn_cell", "type": "tf.compat", "text": ["Module for constructing RNN Cells.", "class BasicLSTMCell: DEPRECATED: Please use tf.compat.v1.nn.rnn_cell.LSTMCell instead.", "class BasicRNNCell: The most basic RNN cell.", "class DeviceWrapper: Operator that ensures an RNNCell runs on a particular device.", "class DropoutWrapper: Operator adding dropout to inputs and outputs of the given cell.", "class GRUCell: Gated Recurrent Unit cell.", "class LSTMCell: Long short-term memory unit (LSTM) recurrent network cell.", "class LSTMStateTuple: Tuple used by LSTM Cells for state_size, zero_state, and output state.", "class MultiRNNCell: RNN cell composed sequentially of multiple simple cells.", "class RNNCell: Abstract object representing an RNN cell.", "class ResidualWrapper: RNNCell wrapper that ensures cell inputs are added to the outputs."]}, {"name": "tf.compat.v1.nn.rnn_cell.BasicLSTMCell", "path": "compat/v1/nn/rnn_cell/basiclstmcell", "type": "tf.compat", "text": ["DEPRECATED: Please use tf.compat.v1.nn.rnn_cell.LSTMCell instead.", "Inherits From: RNNCell, Layer, Layer, Module", "Basic LSTM recurrent network cell.", "The implementation is based on", "We add forget_bias (default: 1) to the biases of the forget gate in order to reduce the scale of forgetting in the beginning of the training.", "It does not allow cell clipping, a projection layer, and does not use peep-hole connections: it is the basic baseline.", "For advanced models, please use the full tf.compat.v1.nn.rnn_cell.LSTMCell that follows.", "Note that this cell is not optimized for performance. Please use tf.contrib.cudnn_rnn.CudnnLSTM for better performance on GPU, or tf.contrib.rnn.LSTMBlockCell and tf.contrib.rnn.LSTMBlockFusedCell for better performance on CPU.", "It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. ", "View source", "View source", "Return zero-filled state tensor(s).", "If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size, s] for each s in state_size. "]}, {"name": "tf.compat.v1.nn.rnn_cell.BasicRNNCell", "path": "compat/v1/nn/rnn_cell/basicrnncell", "type": "tf.compat", "text": ["The most basic RNN cell.", "Inherits From: RNNCell, Layer, Layer, Module", "Note that this cell is not optimized for performance. Please use tf.contrib.cudnn_rnn.CudnnRNNTanh for better performance on GPU.", "It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. ", "View source", "View source", "Return zero-filled state tensor(s).", "If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size, s] for each s in state_size. "]}, {"name": "tf.compat.v1.nn.rnn_cell.DeviceWrapper", "path": "compat/v1/nn/rnn_cell/devicewrapper", "type": "tf.compat", "text": ["Operator that ensures an RNNCell runs on a particular device.", "Inherits From: RNNCell, Layer, Layer, Module", "View source", "View source"]}, {"name": "tf.compat.v1.nn.rnn_cell.DropoutWrapper", "path": "compat/v1/nn/rnn_cell/dropoutwrapper", "type": "tf.compat", "text": ["Operator adding dropout to inputs and outputs of the given cell.", "Inherits From: RNNCell, Layer, Layer, Module", "View source", "View source"]}, {"name": "tf.compat.v1.nn.rnn_cell.GRUCell", "path": "compat/v1/nn/rnn_cell/grucell", "type": "tf.compat", "text": ["Gated Recurrent Unit cell.", "Inherits From: RNNCell, Layer, Layer, Module", "Note that this cell is not optimized for performance. Please use tf.contrib.cudnn_rnn.CudnnGRU for better performance on GPU, or tf.contrib.rnn.GRUBlockCellV2 for better performance on CPU.", "References: Learning Phrase Representations using RNN Encoder Decoder for Statistical Machine Translation: Cho et al., 2014 (pdf) ", "It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. ", "View source", "View source", "Return zero-filled state tensor(s).", "If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size, s] for each s in state_size. "]}, {"name": "tf.compat.v1.nn.rnn_cell.LSTMCell", "path": "compat/v1/nn/rnn_cell/lstmcell", "type": "tf.compat", "text": ["Long short-term memory unit (LSTM) recurrent network cell.", "Inherits From: RNNCell, Layer, Layer, Module", "The default non-peephole implementation is based on (Gers et al., 1999). The peephole implementation is based on (Sak et al., 2014).", "The class uses optional peep-hole connections, optional cell clipping, and an optional projection layer.", "Note that this cell is not optimized for performance. Please use tf.contrib.cudnn_rnn.CudnnLSTM for better performance on GPU, or tf.contrib.rnn.LSTMBlockCell and tf.contrib.rnn.LSTMBlockFusedCell for better performance on CPU. References: Long short-term memory recurrent neural network architectures for large scale acoustic modeling: Sak et al., 2014 (pdf) Learning to forget: Gers et al., 1999 (pdf) Long Short-Term Memory: Hochreiter et al., 1997 (pdf)", "It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. ", "View source", "View source", "Return zero-filled state tensor(s).", "If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size, s] for each s in state_size. "]}, {"name": "tf.compat.v1.nn.rnn_cell.LSTMStateTuple", "path": "compat/v1/nn/rnn_cell/lstmstatetuple", "type": "tf.compat", "text": ["Tuple used by LSTM Cells for state_size, zero_state, and output state.", "Stores two elements: (c, h), in that order. Where c is the hidden state and h is the output.", "Only used when state_is_tuple=True."]}, {"name": "tf.compat.v1.nn.rnn_cell.MultiRNNCell", "path": "compat/v1/nn/rnn_cell/multirnncell", "type": "tf.compat", "text": ["RNN cell composed sequentially of multiple simple cells.", "Inherits From: RNNCell, Layer, Layer, Module", "It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. ", "View source", "View source", "Return zero-filled state tensor(s).", "If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size, s] for each s in state_size. "]}, {"name": "tf.compat.v1.nn.rnn_cell.ResidualWrapper", "path": "compat/v1/nn/rnn_cell/residualwrapper", "type": "tf.compat", "text": ["RNNCell wrapper that ensures cell inputs are added to the outputs.", "Inherits From: RNNCell, Layer, Layer, Module", "View source", "View source"]}, {"name": "tf.compat.v1.nn.rnn_cell.RNNCell", "path": "compat/v1/nn/rnn_cell/rnncell", "type": "tf.compat", "text": ["Abstract object representing an RNN cell.", "Inherits From: Layer, Layer, Module", "Every RNNCell must have the properties below and implement call with the signature (output, next_state) = call(input, state). The optional third input argument, scope, is allowed for backwards compatibility purposes; but should be left off for new subclasses.", "This definition of cell differs from the definition used in the literature. In the literature, 'cell' refers to an object with a single scalar output. This definition refers to a horizontal array of such units.", "An RNN cell, in the most abstract setting, is anything that has a state and performs some operation that takes a matrix of inputs. This operation results in an output matrix with self.output_size columns. If self.state_size is an integer, this operation also results in a new state matrix with self.state_size columns. If self.state_size is a (possibly nested tuple of) TensorShape object(s), then it should return a matching structure of Tensors having shape [batch_size].concatenate(s) for each s in self.batch_size.", "It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. ", "View source", "View source", "Return zero-filled state tensor(s).", "If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size, s] for each s in state_size. "]}, {"name": "tf.compat.v1.nn.safe_embedding_lookup_sparse", "path": "compat/v1/nn/safe_embedding_lookup_sparse", "type": "tf.compat", "text": ["Lookup embedding results, accounting for invalid IDs and empty features.", "The partitioned embedding in embedding_weights must all be the same shape except for the first dimension. The first dimension is allowed to vary as the vocabulary size is not necessarily a multiple of P. embedding_weights may be a PartitionedVariable as returned by using tf.compat.v1.get_variable() with a partitioner.", "Invalid IDs (< 0) are pruned from input IDs and weights, as well as any IDs with non-positive weight. For an entry with no features, the embedding vector for default_id is returned, or the 0-vector if default_id is not supplied.", "The ids and weights may be multi-dimensional. Embeddings are always aggregated along the last dimension.", "In other words, if", "shape(combined embedding_weights) = [p0, p1, ..., pm]", "and", "shape(sparse_ids) = shape(sparse_weights) = [d0, d1, ..., dn]", "then", "shape(output) = [d0, d1, ... dn-1, p1, ..., pm].", "For instance, if params is a 10x20 matrix, and sp_ids / sp_weights are", "default_id is 0.", "with combiner=\"mean\", then the output will be a 3x20 matrix where"]}, {"name": "tf.compat.v1.nn.sampled_softmax_loss", "path": "compat/v1/nn/sampled_softmax_loss", "type": "tf.compat", "text": ["Computes and returns the sampled softmax training loss.", "This is a faster way to train a softmax classifier over a huge number of classes.", "This operation is for training only. It is generally an underestimate of the full softmax loss.", "A common use case is to use this method for training, and calculate the full softmax loss for evaluation or inference. In this case, you must set partition_strategy=\"div\" for the two losses to be consistent, as in the following example:", "See our Candidate Sampling Algorithms Reference (pdf). Also see Section 3 of (Jean et al., 2014) for the math.", "On Using Very Large Target Vocabulary for Neural Machine Translation: Jean et al., 2014 (pdf)"]}, {"name": "tf.compat.v1.nn.separable_conv2d", "path": "compat/v1/nn/separable_conv2d", "type": "tf.compat", "text": ["2-D convolution with separable filters.", "Performs a depthwise convolution that acts separately on channels followed by a pointwise convolution that mixes channels. Note that this is separability between dimensions [1, 2] and 3, not spatial separability between dimensions 1 and 2.", "In detail, with the default NHWC format,", "strides controls the strides for the depthwise convolution only, since the pointwise convolution has implicit strides of [1, 1, 1, 1]. Must have strides[0] = strides[3] = 1. For the most common case of the same horizontal and vertical strides, strides = [1, stride, stride, 1]. If any value in rate is greater than 1, we perform atrous depthwise convolution, in which case all values in the strides tensor must be equal to 1."]}, {"name": "tf.compat.v1.nn.sigmoid_cross_entropy_with_logits", "path": "compat/v1/nn/sigmoid_cross_entropy_with_logits", "type": "tf.compat", "text": ["Computes sigmoid cross entropy given logits.", "Measures the probability error in discrete classification tasks in which each class is independent and not mutually exclusive. For instance, one could perform multilabel classification where a picture can contain both an elephant and a dog at the same time.", "For brevity, let x = logits, z = labels. The logistic loss is", "For x < 0, to avoid overflow in exp(-x), we reformulate the above", "Hence, to ensure stability and avoid overflow, the implementation uses this equivalent formulation", "logits and labels must have the same type and shape."]}, {"name": "tf.compat.v1.nn.softmax_cross_entropy_with_logits", "path": "compat/v1/nn/softmax_cross_entropy_with_logits", "type": "tf.compat", "text": ["Computes softmax cross entropy between logits and labels. (deprecated)", "Future major versions of TensorFlow will allow gradients to flow into the labels input on backprop by default.", "See tf.nn.softmax_cross_entropy_with_logits_v2.", "Measures the probability error in discrete classification tasks in which the classes are mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is labeled with one and only one label: an image can be a dog or a truck, but not both.", "If using exclusive labels (wherein one and only one class is true at a time), see sparse_softmax_cross_entropy_with_logits.", "A common use case is to have logits and labels of shape [batch_size, num_classes], but higher dimensions are supported, with the dim argument specifying the class dimension.", "Backpropagation will happen only into logits. To calculate a cross entropy loss that allows backpropagation into both logits and labels, see tf.nn.softmax_cross_entropy_with_logits_v2.", "Note that to avoid confusion, it is required to pass only named arguments to this function."]}, {"name": "tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2", "path": "compat/v1/nn/softmax_cross_entropy_with_logits_v2", "type": "tf.compat", "text": ["Computes softmax cross entropy between logits and labels. (deprecated arguments)", "Measures the probability error in discrete classification tasks in which the classes are mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is labeled with one and only one label: an image can be a dog or a truck, but not both.", "If using exclusive labels (wherein one and only one class is true at a time), see sparse_softmax_cross_entropy_with_logits.", "A common use case is to have logits and labels of shape [batch_size, num_classes], but higher dimensions are supported, with the axis argument specifying the class dimension.", "logits and labels must have the same dtype (either float16, float32, or float64).", "Backpropagation will happen into both logits and labels. To disallow backpropagation into labels, pass label tensors through tf.stop_gradient before feeding it to this function.", "Note that to avoid confusion, it is required to pass only named arguments to this function."]}, {"name": "tf.compat.v1.nn.sparse_softmax_cross_entropy_with_logits", "path": "compat/v1/nn/sparse_softmax_cross_entropy_with_logits", "type": "tf.compat", "text": ["Computes sparse softmax cross entropy between logits and labels.", "Measures the probability error in discrete classification tasks in which the classes are mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is labeled with one and only one label: an image can be a dog or a truck, but not both.", "A common use case is to have logits of shape [batch_size, num_classes] and have labels of shape [batch_size], but higher dimensions are supported, in which case the dim-th dimension is assumed to be of size num_classes. logits must have the dtype of float16, float32, or float64, and labels must have the dtype of int32 or int64.", "Note that to avoid confusion, it is required to pass only named arguments to this function."]}, {"name": "tf.compat.v1.nn.static_bidirectional_rnn", "path": "compat/v1/nn/static_bidirectional_rnn", "type": "tf.compat", "text": ["Creates a bidirectional recurrent neural network. (deprecated)", "Similar to the unidirectional case above (rnn) but takes input and builds independent forward and backward RNNs with the final forward and backward outputs depth-concatenated, such that the output will have the format [time][batch][cell_fw.output_size + cell_bw.output_size]. The input_size of forward and backward cell must match. The initial state for both directions is zero by default (but can be set optionally) and no intermediate states are ever returned -- the network is fully unrolled for the given (passed in) length(s) of the sequence(s) or completely unrolled if length(s) is not given."]}, {"name": "tf.compat.v1.nn.static_rnn", "path": "compat/v1/nn/static_rnn", "type": "tf.compat", "text": ["Creates a recurrent neural network specified by RNNCell cell. (deprecated)", "The simplest form of RNN network generated is:", "However, a few other options are available:", "An initial state can be provided. If the sequence_length vector is provided, dynamic calculation is performed. This method of calculation does not compute the RNN steps past the maximum sequence length of the minibatch (thus saving computational time), and properly propagates the state at an example's sequence length to the final state output.", "The dynamic calculation performed is, at time t for batch row b,"]}, {"name": "tf.compat.v1.nn.static_state_saving_rnn", "path": "compat/v1/nn/static_state_saving_rnn", "type": "tf.compat", "text": ["RNN that accepts a state saver for time-truncated RNN calculation. (deprecated)"]}, {"name": "tf.compat.v1.nn.sufficient_statistics", "path": "compat/v1/nn/sufficient_statistics", "type": "tf.compat", "text": ["Calculate the sufficient statistics for the mean and variance of x.", "These sufficient statistics are computed using the one pass algorithm on an input that's optionally shifted. See: https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Computing_shifted_data"]}, {"name": "tf.compat.v1.nn.weighted_cross_entropy_with_logits", "path": "compat/v1/nn/weighted_cross_entropy_with_logits", "type": "tf.compat", "text": ["Computes a weighted cross entropy. (deprecated arguments)", "This is like sigmoid_cross_entropy_with_logits() except that pos_weight, allows one to trade off recall and precision by up- or down-weighting the cost of a positive error relative to a negative error.", "The usual cross-entropy cost is defined as:", "A value pos_weight > 1 decreases the false negative count, hence increasing the recall. Conversely setting pos_weight < 1 decreases the false positive count and increases the precision. This can be seen from the fact that pos_weight is introduced as a multiplicative coefficient for the positive labels term in the loss expression:", "For brevity, let x = logits, z = labels, q = pos_weight. The loss is:", "Setting l = (1 + (q - 1) * z), to ensure stability and avoid overflow, the implementation uses", "logits and labels must have the same type and shape."]}, {"name": "tf.compat.v1.nn.weighted_moments", "path": "compat/v1/nn/weighted_moments", "type": "tf.compat", "text": ["Returns the frequency-weighted mean and variance of x."]}, {"name": "tf.compat.v1.nn.xw_plus_b", "path": "compat/v1/nn/xw_plus_b", "type": "tf.compat", "text": ["Computes matmul(x, weights) + biases."]}, {"name": "tf.compat.v1.NodeDef", "path": "compat/v1/nodedef", "type": "tf.compat", "text": ["A ProtocolMessage", "class AttrEntry", "class ExperimentalDebugInfo"]}, {"name": "tf.compat.v1.NodeDef.AttrEntry", "path": "compat/v1/nodedef/attrentry", "type": "tf.compat", "text": ["A ProtocolMessage"]}, {"name": "tf.compat.v1.NodeDef.ExperimentalDebugInfo", "path": "compat/v1/nodedef/experimentaldebuginfo", "type": "tf.compat", "text": ["A ProtocolMessage"]}, {"name": "tf.compat.v1.norm", "path": "compat/v1/norm", "type": "tf.compat", "text": ["Computes the norm of vectors, matrices, and tensors. (deprecated arguments)", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.linalg.norm", "This function can compute several different vector norms (the 1-norm, the Euclidean or 2-norm, the inf-norm, and in general the p-norm for p > 0) and matrix norms (Frobenius, 1-norm, 2-norm and inf-norm).", "Mostly equivalent to numpy.linalg.norm. Not supported: ord <= 0, 2-norm for matrices, nuclear norm. Other differences: a) If axis is None, treats the flattened tensor as a vector regardless of rank. b) Explicitly supports 'euclidean' norm as the default, including for higher order tensors."]}, {"name": "tf.compat.v1.no_regularizer", "path": "compat/v1/no_regularizer", "type": "tf.compat", "text": ["Use this function to prevent regularization of variables."]}, {"name": "tf.compat.v1.ones_like", "path": "compat/v1/ones_like", "type": "tf.compat", "text": ["Creates a tensor with all elements set to 1.", "See also tf.ones.", "Given a single tensor (tensor), this operation returns a tensor of the same type and shape as tensor with all elements set to 1. Optionally, you can specify a new type (dtype) for the returned tensor."]}, {"name": "tf.compat.v1.OptimizerOptions", "path": "compat/v1/optimizeroptions", "type": "tf.compat", "text": ["A ProtocolMessage"]}, {"name": "tf.compat.v1.op_scope", "path": "compat/v1/op_scope", "type": "tf.compat", "text": ["DEPRECATED. Same as name_scope above, just different argument order."]}, {"name": "tf.compat.v1.pad", "path": "compat/v1/pad", "type": "tf.compat", "text": ["Pads a tensor.", "This operation pads a tensor according to the paddings you specify. paddings is an integer tensor with shape [n, 2], where n is the rank of tensor. For each dimension D of input, paddings[D, 0] indicates how many values to add before the contents of tensor in that dimension, and paddings[D, 1] indicates how many values to add after the contents of tensor in that dimension. If mode is \"REFLECT\" then both paddings[D, 0] and paddings[D, 1] must be no greater than tensor.dim_size(D) - 1. If mode is \"SYMMETRIC\" then both paddings[D, 0] and paddings[D, 1] must be no greater than tensor.dim_size(D).", "The padded size of each dimension D of the output is:", "paddings[D, 0] + tensor.dim_size(D) + paddings[D, 1]"]}, {"name": "tf.compat.v1.parse_example", "path": "compat/v1/parse_example", "type": "tf.compat", "text": ["Parses Example protos into a dict of tensors.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.io.parse_example", "Parses a number of serialized Example protos given in serialized. We refer to serialized as a batch with batch_size many entries of individual Example protos.", "example_names may contain descriptive names for the corresponding serialized protos. These may be useful for debugging purposes, but they have no effect on the output. If not None, example_names must be the same length as serialized.", "This op parses serialized examples into a dictionary mapping keys to Tensor SparseTensor, and RaggedTensor objects. features is a dict from keys to VarLenFeature, SparseFeature, RaggedFeature, and FixedLenFeature objects. Each VarLenFeature and SparseFeature is mapped to a SparseTensor; each FixedLenFeature is mapped to a Tensor; and each RaggedFeature is mapped to a RaggedTensor.", "Each VarLenFeature maps to a SparseTensor of the specified type representing a ragged matrix. Its indices are [batch, index] where batch identifies the example in serialized, and index is the value's index in the list of values associated with that feature and example.", "Each SparseFeature maps to a SparseTensor of the specified type representing a Tensor of dense_shape [batch_size] + SparseFeature.size. Its values come from the feature in the examples with key value_key. A values[i] comes from a position k in the feature of an example at batch entry batch. This positional information is recorded in indices[i] as [batch, index_0, index_1, ...] where index_j is the k-th value of the feature in the example at with key SparseFeature.index_key[j]. In other words, we split the indices (except the first index indicating the batch entry) of a SparseTensor by dimension into different features of the Example. Due to its complexity a VarLenFeature should be preferred over a SparseFeature whenever possible.", "Each FixedLenFeature df maps to a Tensor of the specified type (or tf.float32 if not specified) and shape (serialized.size(),) + df.shape.", "FixedLenFeature entries with a default_value are optional. With no default value, we will fail if that Feature is missing from any example in serialized.", "Each FixedLenSequenceFeature df maps to a Tensor of the specified type (or tf.float32 if not specified) and shape (serialized.size(), None) + df.shape. All examples in serialized will be padded with default_value along the second dimension.", "Each RaggedFeature maps to a RaggedTensor of the specified type. It is formed by stacking the RaggedTensor for each example, where the RaggedTensor for each individual example is constructed using the tensors specified by RaggedTensor.values_key and RaggedTensor.partition. See the tf.io.RaggedFeature documentation for details and examples.", "For example, if one expects a tf.float32 VarLenFeature ft and three serialized Examples are provided:", "then the output will look like:", "If instead a FixedLenSequenceFeature with default_value = -1.0 and shape=[] is used then the output will look like:", "Given two Example input protos in serialized:", "And arguments", "Then the output is a dictionary:", "For dense results in two serialized Examples:", "And the expected output is:", "An alternative to VarLenFeature to obtain a SparseTensor is SparseFeature. For example, given two Example input protos in serialized:", "And arguments", "Then the output is a dictionary:", "See the tf.io.RaggedFeature documentation for examples showing how RaggedFeature can be used to obtain RaggedTensors."]}, {"name": "tf.compat.v1.parse_single_example", "path": "compat/v1/parse_single_example", "type": "tf.compat", "text": ["Parses a single Example proto.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.io.parse_single_example", "Similar to parse_example, except:", "For dense tensors, the returned Tensor is identical to the output of parse_example, except there is no batch dimension, the output shape is the same as the shape given in dense_shape.", "For SparseTensors, the first (batch) column of the indices matrix is removed (the indices matrix is a column vector), the values vector is unchanged, and the first (batch_size) entry of the shape vector is removed (it is now a single element vector).", "One might see performance advantages by batching Example protos with parse_example instead of using this function directly."]}, {"name": "tf.compat.v1.placeholder", "path": "compat/v1/placeholder", "type": "tf.compat", "text": ["Inserts a placeholder for a tensor that will be always fed.", "Placeholders are not compatible with eager execution."]}, {"name": "tf.compat.v1.placeholder_with_default", "path": "compat/v1/placeholder_with_default", "type": "tf.compat", "text": ["A placeholder op that passes through input when its output is not fed."]}, {"name": "tf.compat.v1.Print", "path": "compat/v1/print", "type": "tf.compat", "text": ["Prints a list of tensors. (deprecated)", "This is an identity op (behaves like tf.identity) with the side effect of printing data when evaluating."]}, {"name": "tf.compat.v1.profiler", "path": "compat/v1/profiler", "type": "tf.compat", "text": ["Public API for tf.profiler namespace.", "class AdviceProto: A ProtocolMessage", "class GraphNodeProto: A ProtocolMessage", "class MultiGraphNodeProto: A ProtocolMessage", "class OpLogProto: A ProtocolMessage", "class ProfileOptionBuilder: Option Builder for Profiling API.", "class Profiler: TensorFlow multi-step profiler.", "advise(...): Auto profile and advise.", "profile(...): Profile model.", "write_op_log(...): Log provided 'op_log', and add additional model information below."]}, {"name": "tf.compat.v1.profiler.AdviceProto", "path": "compat/v1/profiler/adviceproto", "type": "tf.compat", "text": ["A ProtocolMessage", "class Checker", "class CheckersEntry"]}, {"name": "tf.compat.v1.profiler.AdviceProto.Checker", "path": "compat/v1/profiler/adviceproto/checker", "type": "tf.compat", "text": ["A ProtocolMessage"]}, {"name": "tf.compat.v1.profiler.AdviceProto.CheckersEntry", "path": "compat/v1/profiler/adviceproto/checkersentry", "type": "tf.compat", "text": ["A ProtocolMessage"]}, {"name": "tf.compat.v1.profiler.advise", "path": "compat/v1/profiler/advise", "type": "tf.compat", "text": ["Auto profile and advise.", "Builds profiles and automatically check anomalies of various aspects. For more details: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/README.md"]}, {"name": "tf.compat.v1.profiler.GraphNodeProto", "path": "compat/v1/profiler/graphnodeproto", "type": "tf.compat", "text": ["A ProtocolMessage", "class InputShapesEntry"]}, {"name": "tf.compat.v1.profiler.GraphNodeProto.InputShapesEntry", "path": "compat/v1/profiler/graphnodeproto/inputshapesentry", "type": "tf.compat", "text": ["A ProtocolMessage"]}, {"name": "tf.compat.v1.profiler.MultiGraphNodeProto", "path": "compat/v1/profiler/multigraphnodeproto", "type": "tf.compat", "text": ["A ProtocolMessage"]}, {"name": "tf.compat.v1.profiler.OpLogProto", "path": "compat/v1/profiler/oplogproto", "type": "tf.compat", "text": ["A ProtocolMessage", "class IdToStringEntry"]}, {"name": "tf.compat.v1.profiler.OpLogProto.IdToStringEntry", "path": "compat/v1/profiler/oplogproto/idtostringentry", "type": "tf.compat", "text": ["A ProtocolMessage"]}, {"name": "tf.compat.v1.profiler.profile", "path": "compat/v1/profiler/profile", "type": "tf.compat", "text": ["Profile model.", "Tutorials and examples can be found in: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/python_api.md"]}, {"name": "tf.compat.v1.profiler.ProfileOptionBuilder", "path": "compat/v1/profiler/profileoptionbuilder", "type": "tf.compat", "text": ["Option Builder for Profiling API.", "For tutorial on the options, see https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/g3doc/options.md", "View source", "Whether only account the statistics of displayed profiler nodes.", "View source", "Build a profiling option.", "View source", "Options used to profile float operations.", "Please see https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/g3doc/profile_model_architecture.md on the caveats of calculating float operations.", "View source", "Order the displayed profiler nodes based on a attribute.", "Supported attribute includes micros, bytes, occurrence, params, etc. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/g3doc/options.md", "View source", "Select the attributes to display.", "See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/g3doc/options.md for supported attributes.", "View source", "Show operation time and memory consumptions.", "View source", "Options used to profile trainable variable parameters.", "Normally used together with 'scope' view.", "View source", "Selectively counting statistics based on node types.", "Here, 'types' means the profiler nodes' properties. Profiler by default consider device name (e.g. /job:xx/.../device:GPU:0) and operation type (e.g. MatMul) as profiler nodes' properties. User can also associate customized 'types' to profiler nodes through OpLogProto proto.", "For example, user can select profiler nodes placed on gpu:0 with: account_type_regexes=['.*gpu:0.*']", "If none of a node's properties match the specified regexes, the node is not displayed nor accounted.", "View source", "Do not generate side-effect outputs.", "View source", "Print the result to a file.", "View source", "Set the maximum depth of display.", "The depth depends on profiling view. For 'scope' view, it's the depth of name scope hierarchy (tree), for 'op' view, it's the number of operation types (list), etc.", "View source", "Only show profiler nodes consuming no less than 'min_micros'.", "View source", "Only show profiler nodes consuming no less than 'min_float_ops'.", "Please see https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/g3doc/profile_model_architecture.md on the caveats of calculating float operations.", "View source", "Only show profiler nodes consuming no less than 'min_bytes'.", "View source", "Only show profiler nodes including no less than 'min_occurrence' graph nodes.", "A \"node\" means a profiler output node, which can be a python line (code view), an operation type (op view), or a graph node (graph/scope view). A python line includes all graph nodes created by that line, while an operation type includes all graph nodes of that type.", "View source", "Only show profiler nodes holding no less than 'min_params' parameters.", "'Parameters' normally refers the weights of in TensorFlow variables. It reflects the 'capacity' of models.", "View source", "Regular expressions used to select profiler nodes to display.", "After 'with_accounted_types' is evaluated, 'with_node_names' are evaluated as follows:", "For a profile data structure, profiler first finds the profiler nodes matching 'start_name_regexes', and starts displaying profiler nodes from there. Then, if a node matches 'show_name_regexes' and doesn't match 'hide_name_regexes', it's displayed. If a node matches 'trim_name_regexes', profiler stops further searching that branch.", "View source", "Generate a pprof profile gzip file.", "pprof -png --nodecount=100 --sample_index=1 ", "View source", "Print the result to stdout.", "View source", "Which profile step to use for profiling.", "The 'step' here refers to the step defined by Profiler.add_step() API.", "View source", "Generate a timeline json file."]}, {"name": "tf.compat.v1.profiler.Profiler", "path": "compat/v1/profiler/profiler", "type": "tf.compat", "text": ["TensorFlow multi-step profiler.", "https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/README.md", "View source", "Add statistics of a step.", "View source", "Automatically detect problems and generate reports.", "View source", "Profile the statistics of graph nodes, organized by dataflow graph.", "View source", "Profile the statistics of graph nodes, organized by name scope.", "View source", "Profile the statistics of the Operation types (e.g. MatMul, Conv2D).", "View source", "Profile the statistics of the Python codes.", "By default, it shows the call stack from root. To avoid redundant output, you may use options to filter as below options['show_name_regexes'] = ['.my_code.py.']", "View source", "Serialize the ProfileProto to a binary string.", "Users can write it to file for offline analysis by tfprof commandline or graphical interface."]}, {"name": "tf.compat.v1.profiler.write_op_log", "path": "compat/v1/profiler/write_op_log", "type": "tf.compat", "text": ["Log provided 'op_log', and add additional model information below.", "The API also assigns ops in tf.compat.v1.trainable_variables() an op type called '_trainable_variables'. The API also logs 'flops' statistics for ops with op.RegisterStatistics() defined. flops calculation depends on Tensor shapes defined in 'graph', which might not be complete. 'run_meta', if provided, completes the shape information with best effort."]}, {"name": "tf.compat.v1.python_io", "path": "compat/v1/python_io", "type": "tf.compat", "text": ["Python functions for directly manipulating TFRecord-formatted files.", "class TFRecordCompressionType: The type of compression for the record.", "class TFRecordOptions: Options used for manipulating TFRecord files.", "class TFRecordWriter: A class to write records to a TFRecords file.", "tf_record_iterator(...): An iterator that read the records from a TFRecords file. (deprecated)"]}, {"name": "tf.compat.v1.py_func", "path": "compat/v1/py_func", "type": "tf.compat", "text": ["Wraps a python function and uses it as a TensorFlow op.", "Given a python function func, which takes numpy arrays as its arguments and returns numpy arrays as its outputs, wrap this function as an operation in a TensorFlow graph. The following snippet constructs a simple TensorFlow graph that invokes the np.sinh() NumPy function as a operation in the graph:", "The body of the function (i.e. func) will not be serialized in a GraphDef. Therefore, you should not use this function if you need to serialize your model and restore it in a different environment.", "The operation must run in the same address space as the Python program that calls tf.compat.v1.py_func(). If you are using distributed TensorFlow, you must run a tf.distribute.Server in the same process as the program that calls tf.compat.v1.py_func() and you must pin the created operation to a device in that server (e.g. using with tf.device():)."]}, {"name": "tf.compat.v1.quantization", "path": "compat/v1/quantization", "type": "tf.compat", "text": ["Public API for tf.quantization namespace.", "dequantize(...): Dequantize the 'input' tensor into a float or bfloat16 Tensor.", "fake_quant_with_min_max_args(...): Fake-quantize the 'inputs' tensor, type float to 'outputs' tensor of same type.", "fake_quant_with_min_max_args_gradient(...): Compute gradients for a FakeQuantWithMinMaxArgs operation.", "fake_quant_with_min_max_vars(...): Fake-quantize the 'inputs' tensor of type float via global float scalars", "fake_quant_with_min_max_vars_gradient(...): Compute gradients for a FakeQuantWithMinMaxVars operation.", "fake_quant_with_min_max_vars_per_channel(...): Fake-quantize the 'inputs' tensor of type float via per-channel floats", "fake_quant_with_min_max_vars_per_channel_gradient(...): Compute gradients for a FakeQuantWithMinMaxVarsPerChannel operation.", "quantize(...): Quantize the 'input' tensor of type float to 'output' tensor of type 'T'.", "quantize_and_dequantize(...): Quantizes then dequantizes a tensor. (deprecated)", "quantize_and_dequantize_v2(...): Quantizes then dequantizes a tensor.", "quantized_concat(...): Concatenates quantized tensors along one dimension."]}, {"name": "tf.compat.v1.quantize_v2", "path": "compat/v1/quantize_v2", "type": "tf.compat", "text": ["Please use tf.quantization.quantize instead."]}, {"name": "tf.compat.v1.queue", "path": "compat/v1/queue", "type": "tf.compat", "text": ["Public API for tf.queue namespace.", "class FIFOQueue: A queue implementation that dequeues elements in first-in first-out order.", "class PaddingFIFOQueue: A FIFOQueue that supports batching variable-sized tensors by padding.", "class PriorityQueue: A queue implementation that dequeues elements in prioritized order.", "class QueueBase: Base class for queue implementations.", "class RandomShuffleQueue: A queue implementation that dequeues elements in a random order."]}, {"name": "tf.compat.v1.ragged", "path": "compat/v1/ragged", "type": "tf.compat", "text": ["Ragged Tensors.", "This package defines ops for manipulating ragged tensors (tf.RaggedTensor), which are tensors with non-uniform shapes. In particular, each RaggedTensor has one or more ragged dimensions, which are dimensions whose slices may have different lengths. For example, the inner (column) dimension of rt=[[3, 1, 4, 1], [], [5, 9, 2], [6], []] is ragged, since the column slices (rt[0, :], ..., rt[4, :]) have different lengths. For a more detailed description of ragged tensors, see the tf.RaggedTensor class documentation and the Ragged Tensor Guide.", "Arguments that accept RaggedTensors are marked in bold.", "class RaggedTensorValue: Represents the value of a RaggedTensor.", "boolean_mask(...): Applies a boolean mask to data without flattening the mask dimensions.", "constant(...): Constructs a constant RaggedTensor from a nested Python list.", "constant_value(...): Constructs a RaggedTensorValue from a nested Python list.", "cross(...): Generates feature cross from a list of tensors.", "cross_hashed(...): Generates hashed feature cross from a list of tensors.", "map_flat_values(...): Applies op to the values of one or more RaggedTensors.", "placeholder(...): Creates a placeholder for a tf.RaggedTensor that will always be fed.", "range(...): Returns a RaggedTensor containing the specified sequences of numbers.", "row_splits_to_segment_ids(...): Generates the segmentation corresponding to a RaggedTensor row_splits.", "segment_ids_to_row_splits(...): Generates the RaggedTensor row_splits corresponding to a segmentation.", "stack(...): Stacks a list of rank-R tensors into one rank-(R+1) RaggedTensor.", "stack_dynamic_partitions(...): Stacks dynamic partitions of a Tensor or RaggedTensor."]}, {"name": "tf.compat.v1.ragged.constant_value", "path": "compat/v1/ragged/constant_value", "type": "tf.compat", "text": ["Constructs a RaggedTensorValue from a nested Python list.", "All scalar values in pylist must have the same nesting depth K, and the returned RaggedTensorValue will have rank K. If pylist contains no scalar values, then K is one greater than the maximum depth of empty lists in pylist. All scalar values in pylist must be compatible with dtype."]}, {"name": "tf.compat.v1.ragged.placeholder", "path": "compat/v1/ragged/placeholder", "type": "tf.compat", "text": ["Creates a placeholder for a tf.RaggedTensor that will always be fed.", "@compatibility{eager} Placeholders are not compatible with eager execution."]}, {"name": "tf.compat.v1.ragged.RaggedTensorValue", "path": "compat/v1/ragged/raggedtensorvalue", "type": "tf.compat", "text": ["Represents the value of a RaggedTensor.", "See tf.RaggedTensor for a description of ragged tensors.", "View source", "Returns this ragged tensor value as a nested Python list."]}, {"name": "tf.compat.v1.random", "path": "compat/v1/random", "type": "tf.compat", "text": ["Public API for tf.random namespace.", "experimental module: Public API for tf.random.experimental namespace.", "class Algorithm: An enumeration.", "class Generator: Random-number generator.", "all_candidate_sampler(...): Generate the set of all classes.", "categorical(...): Draws samples from a categorical distribution.", "create_rng_state(...): Creates a RNG state from an integer or a vector.", "fixed_unigram_candidate_sampler(...): Samples a set of classes using the provided (fixed) base distribution.", "gamma(...): Draws shape samples from each of the given Gamma distribution(s).", "get_global_generator(...): Retrieves the global generator.", "get_seed(...): Returns the local seeds an operation should use given an op-specific seed.", "learned_unigram_candidate_sampler(...): Samples a set of classes from a distribution learned during training.", "log_uniform_candidate_sampler(...): Samples a set of classes using a log-uniform (Zipfian) base distribution.", "multinomial(...): Draws samples from a multinomial distribution. (deprecated)", "normal(...): Outputs random values from a normal distribution.", "poisson(...): Draws shape samples from each of the given Poisson distribution(s).", "set_global_generator(...): Replaces the global generator with another Generator object.", "set_random_seed(...): Sets the graph-level random seed for the default graph.", "shuffle(...): Randomly shuffles a tensor along its first dimension.", "stateless_binomial(...): Outputs deterministic pseudorandom values from a binomial distribution.", "stateless_categorical(...): Draws deterministic pseudorandom samples from a categorical distribution.", "stateless_gamma(...): Outputs deterministic pseudorandom values from a gamma distribution.", "stateless_multinomial(...): Draws deterministic pseudorandom samples from a multinomial distribution. (deprecated)", "stateless_normal(...): Outputs deterministic pseudorandom values from a normal distribution.", "stateless_parameterized_truncated_normal(...): Outputs random values from a truncated normal distribution.", "stateless_poisson(...): Outputs deterministic pseudorandom values from a Poisson distribution.", "stateless_truncated_normal(...): Outputs deterministic pseudorandom values, truncated normally distributed.", "stateless_uniform(...): Outputs deterministic pseudorandom values from a uniform distribution.", "truncated_normal(...): Outputs random values from a truncated normal distribution.", "uniform(...): Outputs random values from a uniform distribution.", "uniform_candidate_sampler(...): Samples a set of classes using a uniform base distribution."]}, {"name": "tf.compat.v1.random.experimental", "path": "compat/v1/random/experimental", "type": "tf.compat", "text": ["Public API for tf.random.experimental namespace.", "class Algorithm: An enumeration.", "class Generator: Random-number generator.", "create_rng_state(...): Creates a RNG state from an integer or a vector.", "get_global_generator(...): Retrieves the global generator.", "set_global_generator(...): Replaces the global generator with another Generator object.", "stateless_fold_in(...): Folds in data to an RNG seed to form a new RNG seed.", "stateless_split(...): Splits an RNG seed into num new seeds by adding a leading axis."]}, {"name": "tf.compat.v1.random.stateless_multinomial", "path": "compat/v1/random/stateless_multinomial", "type": "tf.compat", "text": ["Draws deterministic pseudorandom samples from a multinomial distribution. (deprecated)", "This is a stateless version of tf.random.categorical: if run twice with the same seeds and shapes, it will produce the same pseudorandom numbers. The output is consistent across multiple runs on the same hardware (and between CPU and GPU), but may change between versions of TensorFlow or on non-CPU/GPU hardware."]}, {"name": "tf.compat.v1.random_normal_initializer", "path": "compat/v1/random_normal_initializer", "type": "tf.compat", "text": ["Initializer that generates tensors with a normal distribution.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.initializers.random_normal", "View source", "Instantiates an initializer from a configuration dictionary.", "View source", "Returns the configuration of the initializer as a JSON-serializable dict.", "View source", "Returns a tensor object initialized as specified by the initializer."]}, {"name": "tf.compat.v1.random_poisson", "path": "compat/v1/random_poisson", "type": "tf.compat", "text": ["Draws shape samples from each of the given Poisson distribution(s).", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.random.poisson", "lam is the rate parameter describing the distribution(s)."]}, {"name": "tf.compat.v1.random_uniform_initializer", "path": "compat/v1/random_uniform_initializer", "type": "tf.compat", "text": ["Initializer that generates tensors with a uniform distribution.", " Compat aliases for migration ", "See Migration guide for more details.", "tf.compat.v1.initializers.random_uniform", "View source", "Instantiates an initializer from a configuration dictionary.", "View source", "Returns the configuration of the initializer as a JSON-serializable dict.", "View source", "Returns a tensor object initialized as specified by the initializer."]}, {"name": "tf.compat.v1.raw_ops", "path": "compat/v1/raw_ops", "type": "tf.compat", "text": [" ", "Public API for tf.raw_ops namespace.", "Abort(...): Raise a exception to abort the process when called.", "Abs(...): Computes the absolute value of a tensor.", "AccumulateNV2(...): Returns the element-wise sum of a list of tensors.", "AccumulatorApplyGradient(...): Applies a gradient to a given accumulator.", "AccumulatorNumAccumulated(...): Returns the number of gradients aggregated in the given accumulators.", "AccumulatorSetGlobalStep(...): Updates the accumulator with a new value for global_step.", "AccumulatorTakeGradient(...): Extracts the average gradient in the given ConditionalAccumulator.", "Acos(...): Computes acos of x element-wise.", "Acosh(...): Computes inverse hyperbolic cosine of x element-wise.", "Add(...): Returns x + y element-wise.", "AddManySparseToTensorsMap(...): Add an N-minibatch SparseTensor to a SparseTensorsMap, return N handles.", "AddN(...): Add all input tensors element wise.", "AddSparseToTensorsMap(...): Add a SparseTensor to a SparseTensorsMap return its handle.", "AddV2(...): Returns x + y element-wise.", "AdjustContrast(...): Deprecated. Disallowed in GraphDef version >= 2.", "AdjustContrastv2(...): Adjust the contrast of one or more images.", "AdjustHue(...): Adjust the hue of one or more images.", "AdjustSaturation(...): Adjust the saturation of one or more images.", "All(...): Computes the \"logical and\" of elements across dimensions of a tensor.", "AllCandidateSampler(...): Generates labels for candidate sampling with a learned unigram distribution.", "AllToAll(...): An Op to exchange data across TPU replicas.", "Angle(...): Returns the argument of a complex number.", "AnonymousIterator(...): A container for an iterator resource.", "AnonymousIteratorV2(...): A container for an iterator resource.", "AnonymousMemoryCache(...)", "AnonymousMultiDeviceIterator(...): A container for a multi device iterator resource.", "AnonymousRandomSeedGenerator(...)", "AnonymousSeedGenerator(...)", "Any(...): Computes the \"logical or\" of elements across dimensions of a tensor.", "ApplyAdaMax(...): Update '*var' according to the AdaMax algorithm.", "ApplyAdadelta(...): Update '*var' according to the adadelta scheme.", "ApplyAdagrad(...): Update '*var' according to the adagrad scheme.", "ApplyAdagradDA(...): Update '*var' according to the proximal adagrad scheme.", "ApplyAdagradV2(...): Update '*var' according to the adagrad scheme.", "ApplyAdam(...): Update '*var' according to the Adam algorithm.", "ApplyAddSign(...): Update '*var' according to the AddSign update.", "ApplyCenteredRMSProp(...): Update '*var' according to the centered RMSProp algorithm.", "ApplyFtrl(...): Update '*var' according to the Ftrl-proximal scheme.", "ApplyFtrlV2(...): Update '*var' according to the Ftrl-proximal scheme.", "ApplyGradientDescent(...): Update '*var' by subtracting 'alpha' * 'delta' from it.", "ApplyMomentum(...): Update '*var' according to the momentum scheme.", "ApplyPowerSign(...): Update '*var' according to the AddSign update.", "ApplyProximalAdagrad(...): Update 'var' and 'accum' according to FOBOS with Adagrad learning rate.", "ApplyProximalGradientDescent(...): Update '*var' as FOBOS algorithm with fixed learning rate.", "ApplyRMSProp(...): Update '*var' according to the RMSProp algorithm.", "ApproximateEqual(...): Returns the truth value of abs(x-y) < tolerance element-wise.", "ArgMax(...): Returns the index with the largest value across dimensions of a tensor.", "ArgMin(...): Returns the index with the smallest value across dimensions of a tensor.", "AsString(...): Converts each entry in the given tensor to strings.", "Asin(...): Computes the trignometric inverse sine of x element-wise.", "Asinh(...): Computes inverse hyperbolic sine of x element-wise.", "Assert(...): Asserts that the given condition is true.", "AssertCardinalityDataset(...)", "AssertNextDataset(...): A transformation that asserts which transformations happen next.", "Assign(...): Update 'ref' by assigning 'value' to it.", "AssignAdd(...): Update 'ref' by adding 'value' to it.", "AssignAddVariableOp(...): Adds a value to the current value of a variable.", "AssignSub(...): Update 'ref' by subtracting 'value' from it.", "AssignSubVariableOp(...): Subtracts a value from the current value of a variable.", "AssignVariableOp(...): Assigns a new value to a variable.", "Atan(...): Computes the trignometric inverse tangent of x element-wise.", "Atan2(...): Computes arctangent of y/x element-wise, respecting signs of the arguments.", "Atanh(...): Computes inverse hyperbolic tangent of x element-wise.", "AudioSpectrogram(...): Produces a visualization of audio data over time.", "AudioSummary(...): Outputs a Summary protocol buffer with audio.", "AudioSummaryV2(...): Outputs a Summary protocol buffer with audio.", "AutoShardDataset(...): Creates a dataset that shards the input dataset.", "AvgPool(...): Performs average pooling on the input.", "AvgPool3D(...): Performs 3D average pooling on the input.", "AvgPool3DGrad(...): Computes gradients of average pooling function.", "AvgPoolGrad(...): Computes gradients of the average pooling function.", "BandedTriangularSolve(...)", "Barrier(...): Defines a barrier that persists across different graph executions.", "BarrierClose(...): Closes the given barrier.", "BarrierIncompleteSize(...): Computes the number of incomplete elements in the given barrier.", "BarrierInsertMany(...): For each key, assigns the respective value to the specified component.", "BarrierReadySize(...): Computes the number of complete elements in the given barrier.", "BarrierTakeMany(...): Takes the given number of completed elements from a barrier.", "Batch(...): Batches all input tensors nondeterministically.", "BatchCholesky(...)", "BatchCholeskyGrad(...)", "BatchDataset(...): Creates a dataset that batches batch_size elements from input_dataset.", "BatchDatasetV2(...): Creates a dataset that batches batch_size elements from input_dataset.", "BatchFFT(...)", "BatchFFT2D(...)", "BatchFFT3D(...)", "BatchFunction(...): Batches all the inputs tensors to the computation done by the function.", "BatchIFFT(...)", "BatchIFFT2D(...)", "BatchIFFT3D(...)", "BatchMatMul(...): Multiplies slices of two tensors in batches.", "BatchMatMulV2(...): Multiplies slices of two tensors in batches.", "BatchMatrixBandPart(...)", "BatchMatrixDeterminant(...)", "BatchMatrixDiag(...)", "BatchMatrixDiagPart(...)", "BatchMatrixInverse(...)", "BatchMatrixSetDiag(...)", "BatchMatrixSolve(...)", "BatchMatrixSolveLs(...)", "BatchMatrixTriangularSolve(...)", "BatchNormWithGlobalNormalization(...): Batch normalization.", "BatchNormWithGlobalNormalizationGrad(...): Gradients for batch normalization.", "BatchSelfAdjointEig(...)", "BatchSelfAdjointEigV2(...)", "BatchSvd(...)", "BatchToSpace(...): BatchToSpace for 4-D tensors of type T.", "BatchToSpaceND(...): BatchToSpace for N-D tensors of type T.", "BesselI0(...)", "BesselI0e(...)", "BesselI1(...)", "BesselI1e(...)", "BesselJ0(...)", "BesselJ1(...)", "BesselK0(...)", "BesselK0e(...)", "BesselK1(...)", "BesselK1e(...)", "BesselY0(...)", "BesselY1(...)", "Betainc(...): Compute the regularized incomplete beta integral \\(I_x(a, b)\\).", "BiasAdd(...): Adds bias to value.", "BiasAddGrad(...): The backward operation for \"BiasAdd\" on the \"bias\" tensor.", "BiasAddV1(...): Adds bias to value.", "Bincount(...): Counts the number of occurrences of each value in an integer array.", "Bitcast(...): Bitcasts a tensor from one type to another without copying data.", "BitwiseAnd(...): Elementwise computes the bitwise AND of x and y.", "BitwiseOr(...): Elementwise computes the bitwise OR of x and y.", "BitwiseXor(...): Elementwise computes the bitwise XOR of x and y.", "BlockLSTM(...): Computes the LSTM cell forward propagation for all the time steps.", "BlockLSTMGrad(...): Computes the LSTM cell backward propagation for the entire time sequence.", "BlockLSTMGradV2(...): Computes the LSTM cell backward propagation for the entire time sequence.", "BlockLSTMV2(...): Computes the LSTM cell forward propagation for all the time steps.", "BoostedTreesAggregateStats(...): Aggregates the summary of accumulated stats for the batch.", "BoostedTreesBucketize(...): Bucketize each feature based on bucket boundaries.", "BoostedTreesCalculateBestFeatureSplit(...): Calculates gains for each feature and returns the best possible split information for the feature.", "BoostedTreesCalculateBestFeatureSplitV2(...): Calculates gains for each feature and returns the best possible split information for each node. However, if no split is found, then no split information is returned for that node.", "BoostedTreesCalculateBestGainsPerFeature(...): Calculates gains for each feature and returns the best possible split information for the feature.", "BoostedTreesCenterBias(...): Calculates the prior from the training data (the bias) and fills in the first node with the logits' prior. Returns a boolean indicating whether to continue centering.", "BoostedTreesCreateEnsemble(...): Creates a tree ensemble model and returns a handle to it.", "BoostedTreesCreateQuantileStreamResource(...): Create the Resource for Quantile Streams.", "BoostedTreesDeserializeEnsemble(...): Deserializes a serialized tree ensemble config and replaces current tree", "BoostedTreesEnsembleResourceHandleOp(...): Creates a handle to a BoostedTreesEnsembleResource", "BoostedTreesExampleDebugOutputs(...): Debugging/model interpretability outputs for each example.", "BoostedTreesFlushQuantileSummaries(...): Flush the quantile summaries from each quantile stream resource.", "BoostedTreesGetEnsembleStates(...): Retrieves the tree ensemble resource stamp token, number of trees and growing statistics.", "BoostedTreesMakeQuantileSummaries(...): Makes the summary of quantiles for the batch.", "BoostedTreesMakeStatsSummary(...): Makes the summary of accumulated stats for the batch.", "BoostedTreesPredict(...): Runs multiple additive regression ensemble predictors on input instances and", "BoostedTreesQuantileStreamResourceAddSummaries(...): Add the quantile summaries to each quantile stream resource.", "BoostedTreesQuantileStreamResourceDeserialize(...): Deserialize bucket boundaries and ready flag into current QuantileAccumulator.", "BoostedTreesQuantileStreamResourceFlush(...): Flush the summaries for a quantile stream resource.", "BoostedTreesQuantileStreamResourceGetBucketBoundaries(...): Generate the bucket boundaries for each feature based on accumulated summaries.", "BoostedTreesQuantileStreamResourceHandleOp(...): Creates a handle to a BoostedTreesQuantileStreamResource.", "BoostedTreesSerializeEnsemble(...): Serializes the tree ensemble to a proto.", "BoostedTreesSparseAggregateStats(...): Aggregates the summary of accumulated stats for the batch.", "BoostedTreesSparseCalculateBestFeatureSplit(...): Calculates gains for each feature and returns the best possible split information for the feature.", "BoostedTreesTrainingPredict(...): Runs multiple additive regression ensemble predictors on input instances and", "BoostedTreesUpdateEnsemble(...): Updates the tree ensemble by either adding a layer to the last tree being grown", "BoostedTreesUpdateEnsembleV2(...): Updates the tree ensemble by adding a layer to the last tree being grown", "BroadcastArgs(...): Return the shape of s0 op s1 with broadcast.", "BroadcastGradientArgs(...): Return the reduction indices for computing gradients of s0 op s1 with broadcast.", "BroadcastTo(...): Broadcast an array for a compatible shape.", "Bucketize(...): Bucketizes 'input' based on 'boundaries'.", "BytesProducedStatsDataset(...): Records the bytes size of each element of input_dataset in a StatsAggregator.", "CSRSparseMatrixComponents(...): Reads out the CSR components at batch index.", "CSRSparseMatrixToDense(...): Convert a (possibly batched) CSRSparseMatrix to dense.", "CSRSparseMatrixToSparseTensor(...): Converts a (possibly batched) CSRSparesMatrix to a SparseTensor.", "CSVDataset(...)", "CSVDatasetV2(...)", "CTCBeamSearchDecoder(...): Performs beam search decoding on the logits given in input.", "CTCGreedyDecoder(...): Performs greedy decoding on the logits given in inputs.", "CTCLoss(...): Calculates the CTC Loss (log probability) for each batch entry. Also calculates", "CTCLossV2(...): Calculates the CTC Loss (log probability) for each batch entry. Also calculates", "CacheDataset(...): Creates a dataset that caches elements from input_dataset.", "CacheDatasetV2(...)", "Case(...): An n-way switch statement which calls a single branch function.", "Cast(...): Cast x of type SrcT to y of DstT.", "Ceil(...): Returns element-wise smallest integer not less than x.", "CheckNumerics(...): Checks a tensor for NaN and Inf values.", "CheckNumericsV2(...): Checks a tensor for NaN, -Inf and +Inf values.", "Cholesky(...): Computes the Cholesky decomposition of one or more square matrices.", "CholeskyGrad(...): Computes the reverse mode backpropagated gradient of the Cholesky algorithm.", "ChooseFastestBranchDataset(...)", "ChooseFastestDataset(...)", "ClipByValue(...): Clips tensor values to a specified min and max.", "CloseSummaryWriter(...)", "CollectiveBcastRecv(...): Receives a tensor value broadcast from another device.", "CollectiveBcastSend(...): Broadcasts a tensor value to one or more other devices.", "CollectiveGather(...): Mutually accumulates multiple tensors of identical type and shape.", "CollectiveGatherV2(...): Mutually accumulates multiple tensors of identical type and shape.", "CollectivePermute(...): An Op to permute tensors across replicated TPU instances.", "CollectiveReduce(...): Mutually reduces multiple tensors of identical type and shape.", "CollectiveReduceV2(...): Mutually reduces multiple tensors of identical type and shape.", "CombinedNonMaxSuppression(...): Greedily selects a subset of bounding boxes in descending order of score,", "CompareAndBitpack(...): Compare values of input to threshold and pack resulting bits into a uint8.", "Complex(...): Converts two real numbers to a complex number.", "ComplexAbs(...): Computes the complex absolute value of a tensor.", "CompressElement(...): Compresses a dataset element.", "ComputeAccidentalHits(...): Computes the ids of the positions in sampled_candidates that match true_labels.", "ComputeBatchSize(...): Computes the static batch size of a dataset sans partial batches.", "Concat(...): Concatenates tensors along one dimension.", "ConcatOffset(...): Computes offsets of concat inputs within its output.", "ConcatV2(...): Concatenates tensors along one dimension.", "ConcatenateDataset(...): Creates a dataset that concatenates input_dataset with another_dataset.", "ConditionalAccumulator(...): A conditional accumulator for aggregating gradients.", "ConfigureDistributedTPU(...): Sets up the centralized structures for a distributed TPU system.", "ConfigureTPUEmbedding(...): Sets up TPUEmbedding in a distributed TPU system.", "Conj(...): Returns the complex conjugate of a complex number.", "ConjugateTranspose(...): Shuffle dimensions of x according to a permutation and conjugate the result.", "Const(...): Returns a constant tensor.", "ConsumeMutexLock(...): This op consumes a lock created by MutexLock.", "ControlTrigger(...): Does nothing. Serves as a control trigger for scheduling.", "Conv2D(...): Computes a 2-D convolution given 4-D input and filter tensors.", "Conv2DBackpropFilter(...): Computes the gradients of convolution with respect to the filter.", "Conv2DBackpropInput(...): Computes the gradients of convolution with respect to the input.", "Conv3D(...): Computes a 3-D convolution given 5-D input and filter tensors.", "Conv3DBackpropFilter(...): Computes the gradients of 3-D convolution with respect to the filter.", "Conv3DBackpropFilterV2(...): Computes the gradients of 3-D convolution with respect to the filter.", "Conv3DBackpropInput(...): Computes the gradients of 3-D convolution with respect to the input.", "Conv3DBackpropInputV2(...): Computes the gradients of 3-D convolution with respect to the input.", "Copy(...): Copy a tensor from CPU-to-CPU or GPU-to-GPU.", "CopyHost(...): Copy a tensor to host.", "Cos(...): Computes cos of x element-wise.", "Cosh(...): Computes hyperbolic cosine of x element-wise.", "CountUpTo(...): Increments 'ref' until it reaches 'limit'.", "CreateSummaryDbWriter(...)", "CreateSummaryFileWriter(...)", "CropAndResize(...): Extracts crops from the input image tensor and resizes them.", "CropAndResizeGradBoxes(...): Computes the gradient of the crop_and_resize op wrt the input boxes tensor.", "CropAndResizeGradImage(...): Computes the gradient of the crop_and_resize op wrt the input image tensor.", "Cross(...): Compute the pairwise cross product.", "CrossReplicaSum(...): An Op to sum inputs across replicated TPU instances.", "CudnnRNN(...): A RNN backed by cuDNN.", "CudnnRNNBackprop(...): Backprop step of CudnnRNN.", "CudnnRNNBackpropV2(...): Backprop step of CudnnRNN.", "CudnnRNNBackpropV3(...): Backprop step of CudnnRNNV3.", "CudnnRNNCanonicalToParams(...): Converts CudnnRNN params from canonical form to usable form.", "CudnnRNNCanonicalToParamsV2(...): Converts CudnnRNN params from canonical form to usable form. It supports the projection in LSTM.", "CudnnRNNParamsSize(...): Computes size of weights that can be used by a Cudnn RNN model.", "CudnnRNNParamsToCanonical(...): Retrieves CudnnRNN params in canonical form.", "CudnnRNNParamsToCanonicalV2(...): Retrieves CudnnRNN params in canonical form. It supports the projection in LSTM.", "CudnnRNNV2(...): A RNN backed by cuDNN.", "CudnnRNNV3(...): A RNN backed by cuDNN.", "Cumprod(...): Compute the cumulative product of the tensor x along axis.", "Cumsum(...): Compute the cumulative sum of the tensor x along axis.", "CumulativeLogsumexp(...): Compute the cumulative product of the tensor x along axis.", "DataFormatDimMap(...): Returns the dimension index in the destination data format given the one in", "DataFormatVecPermute(...): Permute input tensor from src_format to dst_format.", "DataServiceDataset(...): Creates a dataset that reads data from the tf.data service.", "DatasetCardinality(...): Returns the cardinality of input_dataset.", "DatasetFromGraph(...): Creates a dataset from the given graph_def.", "DatasetToGraph(...): Returns a serialized GraphDef representing input_dataset.", "DatasetToGraphV2(...): Returns a serialized GraphDef representing input_dataset.", "DatasetToSingleElement(...): Outputs the single element from the given dataset.", "DatasetToTFRecord(...): Writes the given dataset to the given file using the TFRecord format.", "Dawsn(...)", "DebugGradientIdentity(...): Identity op for gradient debugging.", "DebugGradientRefIdentity(...): Identity op for gradient debugging.", "DebugIdentity(...): Provides an identity mapping of the non-Ref type input tensor for debugging.", "DebugIdentityV2(...): Debug Identity V2 Op.", "DebugNanCount(...): Debug NaN Value Counter Op.", "DebugNumericSummary(...): Debug Numeric Summary Op.", "DebugNumericSummaryV2(...): Debug Numeric Summary V2 Op.", "DecodeAndCropJpeg(...): Decode and Crop a JPEG-encoded image to a uint8 tensor.", "DecodeBase64(...): Decode web-safe base64-encoded strings.", "DecodeBmp(...): Decode the first frame of a BMP-encoded image to a uint8 tensor.", "DecodeCSV(...): Convert CSV records to tensors. Each column maps to one tensor.", "DecodeCompressed(...): Decompress strings.", "DecodeGif(...): Decode the frame(s) of a GIF-encoded image to a uint8 tensor.", "DecodeImage(...): Function for decode_bmp, decode_gif, decode_jpeg, and decode_png.", "DecodeJSONExample(...): Convert JSON-encoded Example records to binary protocol buffer strings.", "DecodeJpeg(...): Decode a JPEG-encoded image to a uint8 tensor.", "DecodePaddedRaw(...): Reinterpret the bytes of a string as a vector of numbers.", "DecodePng(...): Decode a PNG-encoded image to a uint8 or uint16 tensor.", "DecodeProtoV2(...): The op extracts fields from a serialized protocol buffers message into tensors.", "DecodeRaw(...): Reinterpret the bytes of a string as a vector of numbers.", "DecodeWav(...): Decode a 16-bit PCM WAV file to a float tensor.", "DeepCopy(...): Makes a copy of x.", "DeleteIterator(...): A container for an iterator resource.", "DeleteMemoryCache(...)", "DeleteMultiDeviceIterator(...): A container for an iterator resource.", "DeleteRandomSeedGenerator(...)", "DeleteSeedGenerator(...)", "DeleteSessionTensor(...): Delete the tensor specified by its handle in the session.", "DenseBincount(...): Counts the number of occurrences of each value in an integer array.", "DenseCountSparseOutput(...): Performs sparse-output bin counting for a tf.tensor input.", "DenseToCSRSparseMatrix(...): Converts a dense tensor to a (possibly batched) CSRSparseMatrix.", "DenseToDenseSetOperation(...): Applies set operation along last dimension of 2 Tensor inputs.", "DenseToSparseBatchDataset(...): Creates a dataset that batches input elements into a SparseTensor.", "DenseToSparseSetOperation(...): Applies set operation along last dimension of Tensor and SparseTensor.", "DepthToSpace(...): DepthToSpace for tensors of type T.", "DepthwiseConv2dNative(...): Computes a 2-D depthwise convolution given 4-D input and filter tensors.", "DepthwiseConv2dNativeBackpropFilter(...): Computes the gradients of depthwise convolution with respect to the filter.", "DepthwiseConv2dNativeBackpropInput(...): Computes the gradients of depthwise convolution with respect to the input.", "Dequantize(...): Dequantize the 'input' tensor into a float or bfloat16 Tensor.", "DeserializeIterator(...): Converts the given variant tensor to an iterator and stores it in the given resource.", "DeserializeManySparse(...): Deserialize and concatenate SparseTensors from a serialized minibatch.", "DeserializeSparse(...): Deserialize SparseTensor objects.", "DestroyResourceOp(...): Deletes the resource specified by the handle.", "DestroyTemporaryVariable(...): Destroys the temporary variable and returns its final value.", "DeviceIndex(...): Return the index of device the op runs.", "Diag(...): Returns a diagonal tensor with a given diagonal values.", "DiagPart(...): Returns the diagonal part of the tensor.", "Digamma(...): Computes Psi, the derivative of Lgamma (the log of the absolute value of", "Dilation2D(...): Computes the grayscale dilation of 4-D input and 3-D filter tensors.", "Dilation2DBackpropFilter(...): Computes the gradient of morphological 2-D dilation with respect to the filter.", "Dilation2DBackpropInput(...): Computes the gradient of morphological 2-D dilation with respect to the input.", "DirectedInterleaveDataset(...): A substitute for InterleaveDataset on a fixed list of N datasets.", "Div(...): Returns x / y element-wise.", "DivNoNan(...): Returns 0 if the denominator is zero.", "DrawBoundingBoxes(...): Draw bounding boxes on a batch of images.", "DrawBoundingBoxesV2(...): Draw bounding boxes on a batch of images.", "DummyIterationCounter(...)", "DummyMemoryCache(...)", "DummySeedGenerator(...)", "DynamicPartition(...): Partitions data into num_partitions tensors using indices from partitions.", "DynamicStitch(...): Interleave the values from the data tensors into a single tensor.", "EagerPyFunc(...): Eagerly executes a python function to compute func(input)->output. The", "EditDistance(...): Computes the (possibly normalized) Levenshtein Edit Distance.", "Eig(...): Computes the eigen decomposition of one or more square matrices.", "Einsum(...): Tensor contraction according to Einstein summation convention.", "Elu(...): Computes exponential linear: exp(features) - 1 if < 0, features otherwise.", "EluGrad(...): Computes gradients for the exponential linear (Elu) operation.", "Empty(...): Creates a tensor with the given shape.", "EmptyTensorList(...): Creates and returns an empty tensor list.", "EncodeBase64(...): Encode strings into web-safe base64 format.", "EncodeJpeg(...): JPEG-encode an image.", "EncodeJpegVariableQuality(...): JPEG encode input image with provided compression quality.", "EncodePng(...): PNG-encode an image.", "EncodeProto(...): The op serializes protobuf messages provided in the input tensors.", "EncodeWav(...): Encode audio data using the WAV file format.", "EnqueueTPUEmbeddingIntegerBatch(...): An op that enqueues a list of input batch tensors to TPUEmbedding.", "EnqueueTPUEmbeddingRaggedTensorBatch(...): Eases the porting of code that uses tf.nn.embedding_lookup().", "EnqueueTPUEmbeddingSparseBatch(...): An op that enqueues TPUEmbedding input indices from a SparseTensor.", "EnqueueTPUEmbeddingSparseTensorBatch(...): Eases the porting of code that uses tf.nn.embedding_lookup_sparse().", "EnsureShape(...): Ensures that the tensor's shape matches the expected shape.", "Enter(...): Creates or finds a child frame, and makes data available to the child frame.", "Equal(...): Returns the truth value of (x == y) element-wise.", "Erf(...): Computes the Gauss error function of x element-wise.", "Erfc(...): Computes the complementary error function of x element-wise.", "Erfinv(...)", "EuclideanNorm(...): Computes the euclidean norm of elements across dimensions of a tensor.", "Exit(...): Exits the current frame to its parent frame.", "Exp(...): Computes exponential of x element-wise. \\(y = e^x\\).", "ExpandDims(...): Inserts a dimension of 1 into a tensor's shape.", "ExperimentalAssertNextDataset(...)", "ExperimentalAutoShardDataset(...): Creates a dataset that shards the input dataset.", "ExperimentalBytesProducedStatsDataset(...): Records the bytes size of each element of input_dataset in a StatsAggregator.", "ExperimentalCSVDataset(...)", "ExperimentalChooseFastestDataset(...)", "ExperimentalDatasetCardinality(...): Returns the cardinality of input_dataset.", "ExperimentalDatasetToTFRecord(...): Writes the given dataset to the given file using the TFRecord format.", "ExperimentalDenseToSparseBatchDataset(...): Creates a dataset that batches input elements into a SparseTensor.", "ExperimentalDirectedInterleaveDataset(...): A substitute for InterleaveDataset on a fixed list of N datasets.", "ExperimentalGroupByReducerDataset(...): Creates a dataset that computes a group-by on input_dataset.", "ExperimentalGroupByWindowDataset(...): Creates a dataset that computes a windowed group-by on input_dataset.", "ExperimentalIgnoreErrorsDataset(...): Creates a dataset that contains the elements of input_dataset ignoring errors.", "ExperimentalIteratorGetDevice(...): Returns the name of the device on which resource has been placed.", "ExperimentalLMDBDataset(...)", "ExperimentalLatencyStatsDataset(...): Records the latency of producing input_dataset elements in a StatsAggregator.", "ExperimentalMapAndBatchDataset(...): Creates a dataset that fuses mapping with batching.", "ExperimentalMapDataset(...): Creates a dataset that applies f to the outputs of input_dataset.", "ExperimentalMatchingFilesDataset(...)", "ExperimentalMaxIntraOpParallelismDataset(...): Creates a dataset that overrides the maximum intra-op parallelism.", "ExperimentalNonSerializableDataset(...)", "ExperimentalParallelInterleaveDataset(...): Creates a dataset that applies f to the outputs of input_dataset.", "ExperimentalParseExampleDataset(...): Transforms input_dataset containing Example protos as vectors of DT_STRING into a dataset of Tensor or SparseTensor objects representing the parsed features.", "ExperimentalPrivateThreadPoolDataset(...): Creates a dataset that uses a custom thread pool to compute input_dataset.", "ExperimentalRandomDataset(...): Creates a Dataset that returns pseudorandom numbers.", "ExperimentalRebatchDataset(...): Creates a dataset that changes the batch size.", "ExperimentalScanDataset(...): Creates a dataset successively reduces f over the elements of input_dataset.", "ExperimentalSetStatsAggregatorDataset(...)", "ExperimentalSleepDataset(...)", "ExperimentalSlidingWindowDataset(...): Creates a dataset that passes a sliding window over input_dataset.", "ExperimentalSqlDataset(...): Creates a dataset that executes a SQL query and emits rows of the result set.", "ExperimentalStatsAggregatorHandle(...): Creates a statistics manager resource.", "ExperimentalStatsAggregatorSummary(...): Produces a summary of any statistics recorded by the given statistics manager.", "ExperimentalTakeWhileDataset(...): Creates a dataset that stops iteration when predicate` is false.", "ExperimentalThreadPoolDataset(...): Creates a dataset that uses a custom thread pool to compute input_dataset.", "ExperimentalThreadPoolHandle(...): Creates a dataset that uses a custom thread pool to compute input_dataset.", "ExperimentalUnbatchDataset(...): A dataset that splits the elements of its input into multiple elements.", "ExperimentalUniqueDataset(...): Creates a dataset that contains the unique elements of input_dataset.", "Expint(...)", "Expm1(...): Computes exp(x) - 1 element-wise.", "ExtractGlimpse(...): Extracts a glimpse from the input tensor.", "ExtractGlimpseV2(...): Extracts a glimpse from the input tensor.", "ExtractImagePatches(...): Extract patches from images and put them in the \"depth\" output dimension.", "ExtractJpegShape(...): Extract the shape information of a JPEG-encoded image.", "ExtractVolumePatches(...): Extract patches from input and put them in the \"depth\" output dimension. 3D extension of extract_image_patches.", "FFT(...): Fast Fourier transform.", "FFT2D(...): 2D fast Fourier transform.", "FFT3D(...): 3D fast Fourier transform.", "FIFOQueue(...): A queue that produces elements in first-in first-out order.", "FIFOQueueV2(...): A queue that produces elements in first-in first-out order.", "Fact(...): Output a fact about factorials.", "FakeParam(...): This op is used as a placeholder in If branch functions. It doesn't provide a", "FakeQuantWithMinMaxArgs(...): Fake-quantize the 'inputs' tensor, type float to 'outputs' tensor of same type.", "FakeQuantWithMinMaxArgsGradient(...): Compute gradients for a FakeQuantWithMinMaxArgs operation.", "FakeQuantWithMinMaxVars(...): Fake-quantize the 'inputs' tensor of type float via global float scalars", "FakeQuantWithMinMaxVarsGradient(...): Compute gradients for a FakeQuantWithMinMaxVars operation.", "FakeQuantWithMinMaxVarsPerChannel(...): Fake-quantize the 'inputs' tensor of type float via per-channel floats", "FakeQuantWithMinMaxVarsPerChannelGradient(...): Compute gradients for a FakeQuantWithMinMaxVarsPerChannel operation.", "FakeQueue(...): Deprecated. Do not use.", "Fill(...): Creates a tensor filled with a scalar value.", "FilterByLastComponentDataset(...): Creates a dataset containing elements of first component of input_dataset having true in the last component.", "FilterDataset(...): Creates a dataset containing elements of input_dataset matching predicate.", "Fingerprint(...): Generates fingerprint values.", "FixedLengthRecordDataset(...): Creates a dataset that emits the records from one or more binary files.", "FixedLengthRecordDatasetV2(...)", "FixedLengthRecordReader(...): A Reader that outputs fixed-length records from a file.", "FixedLengthRecordReaderV2(...): A Reader that outputs fixed-length records from a file.", "FixedUnigramCandidateSampler(...): Generates labels for candidate sampling with a learned unigram distribution.", "FlatMapDataset(...): Creates a dataset that applies f to the outputs of input_dataset.", "Floor(...): Returns element-wise largest integer not greater than x.", "FloorDiv(...): Returns x // y element-wise.", "FloorMod(...): Returns element-wise remainder of division. When x < 0 xor y < 0 is", "FlushSummaryWriter(...)", "For(...): ```python", "FractionalAvgPool(...): Performs fractional average pooling on the input.", "FractionalAvgPoolGrad(...): Computes gradient of the FractionalAvgPool function.", "FractionalMaxPool(...): Performs fractional max pooling on the input.", "FractionalMaxPoolGrad(...): Computes gradient of the FractionalMaxPool function.", "FresnelCos(...)", "FresnelSin(...)", "FusedBatchNorm(...): Batch normalization.", "FusedBatchNormGrad(...): Gradient for batch normalization.", "FusedBatchNormGradV2(...): Gradient for batch normalization.", "FusedBatchNormGradV3(...): Gradient for batch normalization.", "FusedBatchNormV2(...): Batch normalization.", "FusedBatchNormV3(...): Batch normalization.", "FusedPadConv2D(...): Performs a padding as a preprocess during a convolution.", "FusedResizeAndPadConv2D(...): Performs a resize and padding as a preprocess during a convolution.", "GRUBlockCell(...): Computes the GRU cell forward propagation for 1 time step.", "GRUBlockCellGrad(...): Computes the GRU cell back-propagation for 1 time step.", "Gather(...): Gather slices from params according to indices.", "GatherNd(...): Gather slices from params into a Tensor with shape specified by indices.", "GatherV2(...): Gather slices from params axis axis according to indices.", "GenerateBoundingBoxProposals(...): This op produces Region of Interests from given bounding boxes(bbox_deltas) encoded wrt anchors according to eq.2 in arXiv:1506.01497", "GenerateVocabRemapping(...): Given a path to new and old vocabulary files, returns a remapping Tensor of", "GeneratorDataset(...): Creates a dataset that invokes a function to generate elements.", "GetSessionHandle(...): Store the input tensor in the state of the current session.", "GetSessionHandleV2(...): Store the input tensor in the state of the current session.", "GetSessionTensor(...): Get the value of the tensor specified by its handle.", "Greater(...): Returns the truth value of (x > y) element-wise.", "GreaterEqual(...): Returns the truth value of (x >= y) element-wise.", "GroupByReducerDataset(...): Creates a dataset that computes a group-by on input_dataset.", "GroupByWindowDataset(...): Creates a dataset that computes a windowed group-by on input_dataset.", "GuaranteeConst(...): Gives a guarantee to the TF runtime that the input tensor is a constant.", "HSVToRGB(...): Convert one or more images from HSV to RGB.", "HashTable(...): Creates a non-initialized hash table.", "HashTableV2(...): Creates a non-initialized hash table.", "HistogramFixedWidth(...): Return histogram of values.", "HistogramSummary(...): Outputs a Summary protocol buffer with a histogram.", "IFFT(...): Inverse fast Fourier transform.", "IFFT2D(...): Inverse 2D fast Fourier transform.", "IFFT3D(...): Inverse 3D fast Fourier transform.", "IRFFT(...): Inverse real-valued fast Fourier transform.", "IRFFT2D(...): Inverse 2D real-valued fast Fourier transform.", "IRFFT3D(...): Inverse 3D real-valued fast Fourier transform.", "Identity(...): Return a tensor with the same shape and contents as the input tensor or value.", "IdentityN(...): Returns a list of tensors with the same shapes and contents as the input", "IdentityReader(...): A Reader that outputs the queued work as both the key and value.", "IdentityReaderV2(...): A Reader that outputs the queued work as both the key and value.", "If(...): output = cond ? then_branch(input) : else_branch(input)", "Igamma(...): Compute the lower regularized incomplete Gamma function P(a, x).", "IgammaGradA(...): Computes the gradient of igamma(a, x) wrt a.", "Igammac(...): Compute the upper regularized incomplete Gamma function Q(a, x).", "IgnoreErrorsDataset(...): Creates a dataset that contains the elements of input_dataset ignoring errors.", "Imag(...): Returns the imaginary part of a complex number.", "ImageProjectiveTransformV2(...): Applies the given transform to each of the images.", "ImageProjectiveTransformV3(...): Applies the given transform to each of the images.", "ImageSummary(...): Outputs a Summary protocol buffer with images.", "ImmutableConst(...): Returns immutable tensor from memory region.", "ImportEvent(...)", "InTopK(...): Says whether the targets are in the top K predictions.", "InTopKV2(...): Says whether the targets are in the top K predictions.", "InfeedDequeue(...): A placeholder op for a value that will be fed into the computation.", "InfeedDequeueTuple(...): Fetches multiple values from infeed as an XLA tuple.", "InfeedEnqueue(...): An op which feeds a single Tensor value into the computation.", "InfeedEnqueuePrelinearizedBuffer(...): An op which enqueues prelinearized buffer into TPU infeed.", "InfeedEnqueueTuple(...): Feeds multiple Tensor values into the computation as an XLA tuple.", "InitializeTable(...): Table initializer that takes two tensors for keys and values respectively.", "InitializeTableFromDataset(...)", "InitializeTableFromTextFile(...): Initializes a table from a text file.", "InitializeTableFromTextFileV2(...): Initializes a table from a text file.", "InitializeTableV2(...): Table initializer that takes two tensors for keys and values respectively.", "InplaceAdd(...): Adds v into specified rows of x.", "InplaceSub(...): Subtracts v into specified rows of x.", "InplaceUpdate(...): Updates specified rows 'i' with values 'v'.", "InterleaveDataset(...): Creates a dataset that applies f to the outputs of input_dataset.", "Inv(...): Computes the reciprocal of x element-wise.", "InvGrad(...): Computes the gradient for the inverse of x wrt its input.", "Invert(...): Invert (flip) each bit of supported types; for example, type uint8 value 01010101 becomes 10101010.", "InvertPermutation(...): Computes the inverse permutation of a tensor.", "IsBoostedTreesEnsembleInitialized(...): Checks whether a tree ensemble has been initialized.", "IsBoostedTreesQuantileStreamResourceInitialized(...): Checks whether a quantile stream has been initialized.", "IsFinite(...): Returns which elements of x are finite.", "IsInf(...): Returns which elements of x are Inf.", "IsNan(...): Returns which elements of x are NaN.", "IsVariableInitialized(...): Checks whether a tensor has been initialized.", "IsotonicRegression(...): Solves a batch of isotonic regression problems.", "Iterator(...): A container for an iterator resource.", "IteratorFromStringHandle(...): Converts the given string representing a handle to an iterator to a resource.", "IteratorFromStringHandleV2(...)", "IteratorGetDevice(...): Returns the name of the device on which resource has been placed.", "IteratorGetNext(...): Gets the next output from the given iterator .", "IteratorGetNextAsOptional(...): Gets the next output from the given iterator as an Optional variant.", "IteratorGetNextSync(...): Gets the next output from the given iterator.", "IteratorToStringHandle(...): Converts the given resource_handle representing an iterator to a string.", "IteratorV2(...)", "L2Loss(...): L2 Loss.", "LMDBDataset(...): Creates a dataset that emits the key-value pairs in one or more LMDB files.", "LMDBReader(...): A Reader that outputs the records from a LMDB file.", "LRN(...): Local Response Normalization.", "LRNGrad(...): Gradients for Local Response Normalization.", "LSTMBlockCell(...): Computes the LSTM cell forward propagation for 1 time step.", "LSTMBlockCellGrad(...): Computes the LSTM cell backward propagation for 1 timestep.", "LatencyStatsDataset(...): Records the latency of producing input_dataset elements in a StatsAggregator.", "LeakyRelu(...): Computes rectified linear: max(features, features * alpha).", "LeakyReluGrad(...): Computes rectified linear gradients for a LeakyRelu operation.", "LearnedUnigramCandidateSampler(...): Generates labels for candidate sampling with a learned unigram distribution.", "LeftShift(...): Elementwise computes the bitwise left-shift of x and y.", "LegacyParallelInterleaveDatasetV2(...): Creates a dataset that applies f to the outputs of input_dataset.", "Less(...): Returns the truth value of (x < y) element-wise.", "LessEqual(...): Returns the truth value of (x <= y) element-wise.", "Lgamma(...): Computes the log of the absolute value of Gamma(x) element-wise.", "LinSpace(...): Generates values in an interval.", "ListDiff(...): Computes the difference between two lists of numbers or strings.", "LoadAndRemapMatrix(...): Loads a 2-D (matrix) Tensor with name old_tensor_name from the checkpoint", "LoadDataset(...)", "LoadTPUEmbeddingADAMParameters(...): Load ADAM embedding parameters.", "LoadTPUEmbeddingADAMParametersGradAccumDebug(...): Load ADAM embedding parameters with debug support.", "LoadTPUEmbeddingAdadeltaParameters(...): Load Adadelta embedding parameters.", "LoadTPUEmbeddingAdadeltaParametersGradAccumDebug(...): Load Adadelta parameters with debug support.", "LoadTPUEmbeddingAdagradParameters(...): Load Adagrad embedding parameters.", "LoadTPUEmbeddingAdagradParametersGradAccumDebug(...): Load Adagrad embedding parameters with debug support.", "LoadTPUEmbeddingCenteredRMSPropParameters(...): Load centered RMSProp embedding parameters.", "LoadTPUEmbeddingFTRLParameters(...): Load FTRL embedding parameters.", "LoadTPUEmbeddingFTRLParametersGradAccumDebug(...): Load FTRL embedding parameters with debug support.", "LoadTPUEmbeddingMDLAdagradLightParameters(...): Load MDL Adagrad Light embedding parameters.", "LoadTPUEmbeddingMomentumParameters(...): Load Momentum embedding parameters.", "LoadTPUEmbeddingMomentumParametersGradAccumDebug(...): Load Momentum embedding parameters with debug support.", "LoadTPUEmbeddingProximalAdagradParameters(...): Load proximal Adagrad embedding parameters.", "LoadTPUEmbeddingProximalAdagradParametersGradAccumDebug(...): Load proximal Adagrad embedding parameters with debug support.", "LoadTPUEmbeddingProximalYogiParameters(...)", "LoadTPUEmbeddingProximalYogiParametersGradAccumDebug(...)", "LoadTPUEmbeddingRMSPropParameters(...): Load RMSProp embedding parameters.", "LoadTPUEmbeddingRMSPropParametersGradAccumDebug(...): Load RMSProp embedding parameters with debug support.", "LoadTPUEmbeddingStochasticGradientDescentParameters(...): Load SGD embedding parameters.", "LoadTPUEmbeddingStochasticGradientDescentParametersGradAccumDebug(...): Load SGD embedding parameters.", "Log(...): Computes natural logarithm of x element-wise.", "Log1p(...): Computes natural logarithm of (1 + x) element-wise.", "LogMatrixDeterminant(...): Computes the sign and the log of the absolute value of the determinant of", "LogSoftmax(...): Computes log softmax activations.", "LogUniformCandidateSampler(...): Generates labels for candidate sampling with a log-uniform distribution.", "LogicalAnd(...): Returns the truth value of x AND y element-wise.", "LogicalNot(...): Returns the truth value of NOT x element-wise.", "LogicalOr(...): Returns the truth value of x OR y element-wise.", "LookupTableExport(...): Outputs all keys and values in the table.", "LookupTableExportV2(...): Outputs all keys and values in the table.", "LookupTableFind(...): Looks up keys in a table, outputs the corresponding values.", "LookupTableFindV2(...): Looks up keys in a table, outputs the corresponding values.", "LookupTableImport(...): Replaces the contents of the table with the specified keys and values.", "LookupTableImportV2(...): Replaces the contents of the table with the specified keys and values.", "LookupTableInsert(...): Updates the table to associates keys with values.", "LookupTableInsertV2(...): Updates the table to associates keys with values.", "LookupTableRemoveV2(...): Removes keys and its associated values from a table.", "LookupTableSize(...): Computes the number of elements in the given table.", "LookupTableSizeV2(...): Computes the number of elements in the given table.", "LoopCond(...): Forwards the input to the output.", "LowerBound(...): Applies lower_bound(sorted_search_values, values) along each row.", "Lu(...): Computes the LU decomposition of one or more square matrices.", "MakeIterator(...): Makes a new iterator from the given dataset and stores it in iterator.", "MapAndBatchDataset(...): Creates a dataset that fuses mapping with batching.", "MapClear(...): Op removes all elements in the underlying container.", "MapDataset(...): Creates a dataset that applies f to the outputs of input_dataset.", "MapDefun(...): Maps a function on the list of tensors unpacked from arguments on dimension 0.", "MapIncompleteSize(...): Op returns the number of incomplete elements in the underlying container.", "MapPeek(...): Op peeks at the values at the specified key. If the", "MapSize(...): Op returns the number of elements in the underlying container.", "MapStage(...): Stage (key, values) in the underlying container which behaves like a hashtable.", "MapUnstage(...): Op removes and returns the values associated with the key", "MapUnstageNoKey(...): Op removes and returns a random (key, value)", "MatMul(...): Multiply the matrix \"a\" by the matrix \"b\".", "MatchingFiles(...): Returns the set of files matching one or more glob patterns.", "MatchingFilesDataset(...)", "MatrixBandPart(...): Copy a tensor setting everything outside a central band in each innermost matrix to zero.", "MatrixDeterminant(...): Computes the determinant of one or more square matrices.", "MatrixDiag(...): Returns a batched diagonal tensor with a given batched diagonal values.", "MatrixDiagPart(...): Returns the batched diagonal part of a batched tensor.", "MatrixDiagPartV2(...): Returns the batched diagonal part of a batched tensor.", "MatrixDiagPartV3(...): Returns the batched diagonal part of a batched tensor.", "MatrixDiagV2(...): Returns a batched diagonal tensor with given batched diagonal values.", "MatrixDiagV3(...): Returns a batched diagonal tensor with given batched diagonal values.", "MatrixExponential(...): Deprecated, use python implementation tf.linalg.matrix_exponential.", "MatrixInverse(...): Computes the inverse of one or more square invertible matrices or their adjoints (conjugate transposes).", "MatrixLogarithm(...): Computes the matrix logarithm of one or more square matrices:", "MatrixSetDiag(...): Returns a batched matrix tensor with new batched diagonal values.", "MatrixSetDiagV2(...): Returns a batched matrix tensor with new batched diagonal values.", "MatrixSetDiagV3(...): Returns a batched matrix tensor with new batched diagonal values.", "MatrixSolve(...): Solves systems of linear equations.", "MatrixSolveLs(...): Solves one or more linear least-squares problems.", "MatrixSquareRoot(...): Computes the matrix square root of one or more square matrices:", "MatrixTriangularSolve(...): Solves systems of linear equations with upper or lower triangular matrices by backsubstitution.", "Max(...): Computes the maximum of elements across dimensions of a tensor.", "MaxIntraOpParallelismDataset(...): Creates a dataset that overrides the maximum intra-op parallelism.", "MaxPool(...): Performs max pooling on the input.", "MaxPool3D(...): Performs 3D max pooling on the input.", "MaxPool3DGrad(...): Computes gradients of 3D max pooling function.", "MaxPool3DGradGrad(...): Computes second-order gradients of the maxpooling function.", "MaxPoolGrad(...): Computes gradients of the maxpooling function.", "MaxPoolGradGrad(...): Computes second-order gradients of the maxpooling function.", "MaxPoolGradGradV2(...): Computes second-order gradients of the maxpooling function.", "MaxPoolGradGradWithArgmax(...): Computes second-order gradients of the maxpooling function.", "MaxPoolGradV2(...): Computes gradients of the maxpooling function.", "MaxPoolGradWithArgmax(...): Computes gradients of the maxpooling function.", "MaxPoolV2(...): Performs max pooling on the input.", "MaxPoolWithArgmax(...): Performs max pooling on the input and outputs both max values and indices.", "Maximum(...): Returns the max of x and y (i.e. x > y ? x : y) element-wise.", "Mean(...): Computes the mean of elements across dimensions of a tensor.", "Merge(...): Forwards the value of an available tensor from inputs to output.", "MergeSummary(...): Merges summaries.", "MergeV2Checkpoints(...): V2 format specific: merges the metadata files of sharded checkpoints. The", "Mfcc(...): Transforms a spectrogram into a form that's useful for speech recognition.", "Min(...): Computes the minimum of elements across dimensions of a tensor.", "Minimum(...): Returns the min of x and y (i.e. x < y ? x : y) element-wise.", "MirrorPad(...): Pads a tensor with mirrored values.", "MirrorPadGrad(...): Gradient op for MirrorPad op. This op folds a mirror-padded tensor.", "Mod(...): Returns element-wise remainder of division. This emulates C semantics in that", "ModelDataset(...): Identity transformation that models performance.", "Mul(...): Returns x * y element-wise.", "MulNoNan(...): Returns x * y element-wise. Returns zero if y is zero, even if x if infinite or NaN.", "MultiDeviceIterator(...): Creates a MultiDeviceIterator resource.", "MultiDeviceIteratorFromStringHandle(...): Generates a MultiDeviceIterator resource from its provided string handle.", "MultiDeviceIteratorGetNextFromShard(...): Gets next element for the provided shard number.", "MultiDeviceIteratorInit(...): Initializes the multi device iterator with the given dataset.", "MultiDeviceIteratorToStringHandle(...): Produces a string handle for the given MultiDeviceIterator.", "Multinomial(...): Draws samples from a multinomial distribution.", "MutableDenseHashTable(...): Creates an empty hash table that uses tensors as the backing store.", "MutableDenseHashTableV2(...): Creates an empty hash table that uses tensors as the backing store.", "MutableHashTable(...): Creates an empty hash table.", "MutableHashTableOfTensors(...): Creates an empty hash table.", "MutableHashTableOfTensorsV2(...): Creates an empty hash table.", "MutableHashTableV2(...): Creates an empty hash table.", "MutexLock(...): Locks a mutex resource. The output is the lock. So long as the lock tensor", "MutexV2(...): Creates a Mutex resource that can be locked by MutexLock.", "NcclAllReduce(...): Outputs a tensor containing the reduction across all input tensors.", "NcclBroadcast(...): Sends input to all devices that are connected to the output.", "NcclReduce(...): Reduces input from num_devices using reduction to a single device.", "Ndtri(...)", "Neg(...): Computes numerical negative value element-wise.", "NextAfter(...): Returns the next representable value of x1 in the direction of x2, element-wise.", "NextIteration(...): Makes its input available to the next iteration.", "NoOp(...): Does nothing. Only useful as a placeholder for control edges.", "NonDeterministicInts(...): Non-deterministically generates some integers.", "NonMaxSuppression(...): Greedily selects a subset of bounding boxes in descending order of score,", "NonMaxSuppressionV2(...): Greedily selects a subset of bounding boxes in descending order of score,", "NonMaxSuppressionV3(...): Greedily selects a subset of bounding boxes in descending order of score,", "NonMaxSuppressionV4(...): Greedily selects a subset of bounding boxes in descending order of score,", "NonMaxSuppressionV5(...): Greedily selects a subset of bounding boxes in descending order of score,", "NonMaxSuppressionWithOverlaps(...): Greedily selects a subset of bounding boxes in descending order of score,", "NonSerializableDataset(...)", "NotEqual(...): Returns the truth value of (x != y) element-wise.", "NthElement(...): Finds values of the n-th order statistic for the last dimension.", "OneHot(...): Returns a one-hot tensor.", "OneShotIterator(...): Makes a \"one-shot\" iterator that can be iterated only once.", "OnesLike(...): Returns a tensor of ones with the same shape and type as x.", "OptimizeDataset(...): Creates a dataset by applying optimizations to input_dataset.", "OptimizeDatasetV2(...): Creates a dataset by applying related optimizations to input_dataset.", "OptionalFromValue(...): Constructs an Optional variant from a tuple of tensors.", "OptionalGetValue(...): Returns the value stored in an Optional variant or raises an error if none exists.", "OptionalHasValue(...): Returns true if and only if the given Optional variant has a value.", "OptionalNone(...): Creates an Optional variant with no value.", "OrderedMapClear(...): Op removes all elements in the underlying container.", "OrderedMapIncompleteSize(...): Op returns the number of incomplete elements in the underlying container.", "OrderedMapPeek(...): Op peeks at the values at the specified key. If the", "OrderedMapSize(...): Op returns the number of elements in the underlying container.", "OrderedMapStage(...): Stage (key, values) in the underlying container which behaves like a ordered", "OrderedMapUnstage(...): Op removes and returns the values associated with the key", "OrderedMapUnstageNoKey(...): Op removes and returns the (key, value) element with the smallest", "OutfeedDequeue(...): Retrieves a single tensor from the computation outfeed.", "OutfeedDequeueTuple(...): Retrieve multiple values from the computation outfeed.", "OutfeedDequeueTupleV2(...): Retrieve multiple values from the computation outfeed. Device ordinal is a", "OutfeedDequeueV2(...): Retrieves a single tensor from the computation outfeed. Device ordinal is a", "OutfeedEnqueue(...): Enqueue a Tensor on the computation outfeed.", "OutfeedEnqueueTuple(...): Enqueue multiple Tensor values on the computation outfeed.", "Pack(...): Packs a list of N rank-R tensors into one rank-(R+1) tensor.", "Pad(...): Pads a tensor with zeros.", "PadV2(...): Pads a tensor.", "PaddedBatchDataset(...): Creates a dataset that batches and pads batch_size elements from the input.", "PaddedBatchDatasetV2(...): Creates a dataset that batches and pads batch_size elements from the input.", "PaddingFIFOQueue(...): A queue that produces elements in first-in first-out order.", "PaddingFIFOQueueV2(...): A queue that produces elements in first-in first-out order.", "ParallelConcat(...): Concatenates a list of N tensors along the first dimension.", "ParallelDynamicStitch(...): Interleave the values from the data tensors into a single tensor.", "ParallelInterleaveDataset(...): Creates a dataset that applies f to the outputs of input_dataset.", "ParallelInterleaveDatasetV2(...): Creates a dataset that applies f to the outputs of input_dataset.", "ParallelInterleaveDatasetV3(...): Creates a dataset that applies f to the outputs of input_dataset.", "ParallelInterleaveDatasetV4(...): Creates a dataset that applies f to the outputs of input_dataset.", "ParallelMapDataset(...): Creates a dataset that applies f to the outputs of input_dataset.", "ParallelMapDatasetV2(...): Creates a dataset that applies f to the outputs of input_dataset.", "ParameterizedTruncatedNormal(...): Outputs random values from a normal distribution. The parameters may each be a", "ParseExample(...): Transforms a vector of brain.Example protos (as strings) into typed tensors.", "ParseExampleDataset(...): Transforms input_dataset containing Example protos as vectors of DT_STRING into a dataset of Tensor or SparseTensor objects representing the parsed features.", "ParseExampleDatasetV2(...): Transforms input_dataset containing Example protos as vectors of DT_STRING into a dataset of Tensor or SparseTensor objects representing the parsed features.", "ParseExampleV2(...): Transforms a vector of tf.Example protos (as strings) into typed tensors.", "ParseSequenceExample(...): Transforms a vector of brain.SequenceExample protos (as strings) into typed tensors.", "ParseSequenceExampleV2(...): Transforms a vector of tf.io.SequenceExample protos (as strings) into", "ParseSingleExample(...): Transforms a tf.Example proto (as a string) into typed tensors.", "ParseSingleSequenceExample(...): Transforms a scalar brain.SequenceExample proto (as strings) into typed tensors.", "ParseTensor(...): Transforms a serialized tensorflow.TensorProto proto into a Tensor.", "PartitionedCall(...): returns f(inputs), where f's body is placed and partitioned.", "Placeholder(...): A placeholder op for a value that will be fed into the computation.", "PlaceholderV2(...): A placeholder op for a value that will be fed into the computation.", "PlaceholderWithDefault(...): A placeholder op that passes through input when its output is not fed.", "Polygamma(...): Compute the polygamma function \\(\\psi^{(n)}(x)\\).", "PopulationCount(...): Computes element-wise population count (a.k.a. popcount, bitsum, bitcount).", "Pow(...): Computes the power of one value to another.", "PrefetchDataset(...): Creates a dataset that asynchronously prefetches elements from input_dataset.", "Prelinearize(...): An op which linearizes one Tensor value to an opaque variant tensor.", "PrelinearizeTuple(...): An op which linearizes multiple Tensor values to an opaque variant tensor.", "PreventGradient(...): An identity op that triggers an error if a gradient is requested.", "Print(...): Prints a list of tensors.", "PrintV2(...): Prints a string scalar.", "PriorityQueue(...): A queue that produces elements sorted by the first component value.", "PriorityQueueV2(...): A queue that produces elements sorted by the first component value.", "PrivateThreadPoolDataset(...): Creates a dataset that uses a custom thread pool to compute input_dataset.", "Prod(...): Computes the product of elements across dimensions of a tensor.", "PyFunc(...): Invokes a python function to compute func(input)->output.", "PyFuncStateless(...): A stateless version of PyFunc.", "Qr(...): Computes the QR decompositions of one or more matrices.", "QuantizeAndDequantize(...): Use QuantizeAndDequantizeV2 instead.", "QuantizeAndDequantizeV2(...): Quantizes then dequantizes a tensor.", "QuantizeAndDequantizeV3(...): Quantizes then dequantizes a tensor.", "QuantizeAndDequantizeV4(...): Returns the gradient of QuantizeAndDequantizeV4.", "QuantizeAndDequantizeV4Grad(...): Returns the gradient of QuantizeAndDequantizeV4.", "QuantizeDownAndShrinkRange(...): Convert the quantized 'input' tensor into a lower-precision 'output', using the", "QuantizeV2(...): Quantize the 'input' tensor of type float to 'output' tensor of type 'T'.", "QuantizedAdd(...): Returns x + y element-wise, working on quantized buffers.", "QuantizedAvgPool(...): Produces the average pool of the input tensor for quantized types.", "QuantizedBatchNormWithGlobalNormalization(...): Quantized Batch normalization.", "QuantizedBiasAdd(...): Adds Tensor 'bias' to Tensor 'input' for Quantized types.", "QuantizedConcat(...): Concatenates quantized tensors along one dimension.", "QuantizedConv2D(...): Computes a 2D convolution given quantized 4D input and filter tensors.", "QuantizedConv2DAndRelu(...)", "QuantizedConv2DAndReluAndRequantize(...)", "QuantizedConv2DAndRequantize(...)", "QuantizedConv2DPerChannel(...): Computes QuantizedConv2D per channel.", "QuantizedConv2DWithBias(...)", "QuantizedConv2DWithBiasAndRelu(...)", "QuantizedConv2DWithBiasAndReluAndRequantize(...)", "QuantizedConv2DWithBiasAndRequantize(...)", "QuantizedConv2DWithBiasSignedSumAndReluAndRequantize(...)", "QuantizedConv2DWithBiasSumAndRelu(...)", "QuantizedConv2DWithBiasSumAndReluAndRequantize(...)", "QuantizedDepthwiseConv2D(...): Computes quantized depthwise Conv2D.", "QuantizedDepthwiseConv2DWithBias(...): Computes quantized depthwise Conv2D with Bias.", "QuantizedDepthwiseConv2DWithBiasAndRelu(...): Computes quantized depthwise Conv2D with Bias and Relu.", "QuantizedDepthwiseConv2DWithBiasAndReluAndRequantize(...): Computes quantized depthwise Conv2D with Bias, Relu and Requantize.", "QuantizedInstanceNorm(...): Quantized Instance normalization.", "QuantizedMatMul(...): Perform a quantized matrix multiplication of a by the matrix b.", "QuantizedMatMulWithBias(...): Performs a quantized matrix multiplication of a by the matrix b with bias", "QuantizedMatMulWithBiasAndDequantize(...)", "QuantizedMatMulWithBiasAndRelu(...): Perform a quantized matrix multiplication of a by the matrix b with bias", "QuantizedMatMulWithBiasAndReluAndRequantize(...): Perform a quantized matrix multiplication of a by the matrix b with bias", "QuantizedMatMulWithBiasAndRequantize(...)", "QuantizedMaxPool(...): Produces the max pool of the input tensor for quantized types.", "QuantizedMul(...): Returns x * y element-wise, working on quantized buffers.", "QuantizedRelu(...): Computes Quantized Rectified Linear: max(features, 0)", "QuantizedRelu6(...): Computes Quantized Rectified Linear 6: min(max(features, 0), 6)", "QuantizedReluX(...): Computes Quantized Rectified Linear X: min(max(features, 0), max_value)", "QuantizedReshape(...): Reshapes a quantized tensor as per the Reshape op.", "QuantizedResizeBilinear(...): Resize quantized images to size using quantized bilinear interpolation.", "QueueClose(...): Closes the given queue.", "QueueCloseV2(...): Closes the given queue.", "QueueDequeue(...): Dequeues a tuple of one or more tensors from the given queue.", "QueueDequeueMany(...): Dequeues n tuples of one or more tensors from the given queue.", "QueueDequeueManyV2(...): Dequeues n tuples of one or more tensors from the given queue.", "QueueDequeueUpTo(...): Dequeues n tuples of one or more tensors from the given queue.", "QueueDequeueUpToV2(...): Dequeues n tuples of one or more tensors from the given queue.", "QueueDequeueV2(...): Dequeues a tuple of one or more tensors from the given queue.", "QueueEnqueue(...): Enqueues a tuple of one or more tensors in the given queue.", "QueueEnqueueMany(...): Enqueues zero or more tuples of one or more tensors in the given queue.", "QueueEnqueueManyV2(...): Enqueues zero or more tuples of one or more tensors in the given queue.", "QueueEnqueueV2(...): Enqueues a tuple of one or more tensors in the given queue.", "QueueIsClosed(...): Returns true if queue is closed.", "QueueIsClosedV2(...): Returns true if queue is closed.", "QueueSize(...): Computes the number of elements in the given queue.", "QueueSizeV2(...): Computes the number of elements in the given queue.", "RFFT(...): Real-valued fast Fourier transform.", "RFFT2D(...): 2D real-valued fast Fourier transform.", "RFFT3D(...): 3D real-valued fast Fourier transform.", "RGBToHSV(...): Converts one or more images from RGB to HSV.", "RaggedBincount(...): Counts the number of occurrences of each value in an integer array.", "RaggedCountSparseOutput(...): Performs sparse-output bin counting for a ragged tensor input.", "RaggedCross(...): Generates a feature cross from a list of tensors, and returns it as a", "RaggedGather(...): Gather ragged slices from params axis 0 according to indices.", "RaggedRange(...): Returns a RaggedTensor containing the specified sequences of numbers.", "RaggedTensorFromVariant(...): Decodes a variant Tensor into a RaggedTensor.", "RaggedTensorToSparse(...): Converts a RaggedTensor into a SparseTensor with the same values.", "RaggedTensorToTensor(...): Create a dense tensor from a ragged tensor, possibly altering its shape.", "RaggedTensorToVariant(...): Encodes a RaggedTensor into a variant Tensor.", "RaggedTensorToVariantGradient(...): Helper used to compute the gradient for RaggedTensorToVariant.", "RandomCrop(...): Randomly crop image.", "RandomDataset(...): Creates a Dataset that returns pseudorandom numbers.", "RandomGamma(...): Outputs random values from the Gamma distribution(s) described by alpha.", "RandomGammaGrad(...): Computes the derivative of a Gamma random sample w.r.t. alpha.", "RandomPoisson(...): Use RandomPoissonV2 instead.", "RandomPoissonV2(...): Outputs random values from the Poisson distribution(s) described by rate.", "RandomShuffle(...): Randomly shuffles a tensor along its first dimension.", "RandomShuffleQueue(...): A queue that randomizes the order of elements.", "RandomShuffleQueueV2(...): A queue that randomizes the order of elements.", "RandomStandardNormal(...): Outputs random values from a normal distribution.", "RandomUniform(...): Outputs random values from a uniform distribution.", "RandomUniformInt(...): Outputs random integers from a uniform distribution.", "Range(...): Creates a sequence of numbers.", "RangeDataset(...): Creates a dataset with a range of values. Corresponds to python's xrange.", "Rank(...): Returns the rank of a tensor.", "ReadFile(...): Reads and outputs the entire contents of the input filename.", "ReadVariableOp(...): Reads the value of a variable.", "ReaderNumRecordsProduced(...): Returns the number of records this Reader has produced.", "ReaderNumRecordsProducedV2(...): Returns the number of records this Reader has produced.", "ReaderNumWorkUnitsCompleted(...): Returns the number of work units this Reader has finished processing.", "ReaderNumWorkUnitsCompletedV2(...): Returns the number of work units this Reader has finished processing.", "ReaderRead(...): Returns the next record (key, value pair) produced by a Reader.", "ReaderReadUpTo(...): Returns up to num_records (key, value) pairs produced by a Reader.", "ReaderReadUpToV2(...): Returns up to num_records (key, value) pairs produced by a Reader.", "ReaderReadV2(...): Returns the next record (key, value pair) produced by a Reader.", "ReaderReset(...): Restore a Reader to its initial clean state.", "ReaderResetV2(...): Restore a Reader to its initial clean state.", "ReaderRestoreState(...): Restore a reader to a previously saved state.", "ReaderRestoreStateV2(...): Restore a reader to a previously saved state.", "ReaderSerializeState(...): Produce a string tensor that encodes the state of a Reader.", "ReaderSerializeStateV2(...): Produce a string tensor that encodes the state of a Reader.", "Real(...): Returns the real part of a complex number.", "RealDiv(...): Returns x / y element-wise for real types.", "RebatchDataset(...): Creates a dataset that changes the batch size.", "RebatchDatasetV2(...): Creates a dataset that changes the batch size.", "Reciprocal(...): Computes the reciprocal of x element-wise.", "ReciprocalGrad(...): Computes the gradient for the inverse of x wrt its input.", "RecordInput(...): Emits randomized records.", "Recv(...): Receives the named tensor from send_device on recv_device.", "RecvTPUEmbeddingActivations(...): An op that receives embedding activations on the TPU.", "ReduceDataset(...): Reduces the input dataset to a singleton using a reduce function.", "ReduceJoin(...): Joins a string Tensor across the given dimensions.", "RefEnter(...): Creates or finds a child frame, and makes data available to the child frame.", "RefExit(...): Exits the current frame to its parent frame.", "RefIdentity(...): Return the same ref tensor as the input ref tensor.", "RefMerge(...): Forwards the value of an available tensor from inputs to output.", "RefNextIteration(...): Makes its input available to the next iteration.", "RefSelect(...): Forwards the indexth element of inputs to output.", "RefSwitch(...): Forwards the ref tensor data to the output port determined by pred.", "RegexFullMatch(...): Check if the input matches the regex pattern.", "RegexReplace(...): Replaces matches of the pattern regular expression in input with the", "RegisterDataset(...): Registers a dataset with the tf.data service.", "Relu(...): Computes rectified linear: max(features, 0).", "Relu6(...): Computes rectified linear 6: min(max(features, 0), 6).", "Relu6Grad(...): Computes rectified linear 6 gradients for a Relu6 operation.", "ReluGrad(...): Computes rectified linear gradients for a Relu operation.", "RemoteCall(...): Runs function f on a remote device indicated by target.", "RepeatDataset(...): Creates a dataset that emits the outputs of input_dataset count times.", "RequantizationRange(...): Computes a range that covers the actual values present in a quantized tensor.", "RequantizationRangePerChannel(...): Computes requantization range per channel.", "Requantize(...): Converts the quantized input tensor into a lower-precision output.", "RequantizePerChannel(...): Requantizes input with min and max values known per channel.", "Reshape(...): Reshapes a tensor.", "ResizeArea(...): Resize images to size using area interpolation.", "ResizeBicubic(...): Resize images to size using bicubic interpolation.", "ResizeBicubicGrad(...): Computes the gradient of bicubic interpolation.", "ResizeBilinear(...): Resize images to size using bilinear interpolation.", "ResizeBilinearGrad(...): Computes the gradient of bilinear interpolation.", "ResizeNearestNeighbor(...): Resize images to size using nearest neighbor interpolation.", "ResizeNearestNeighborGrad(...): Computes the gradient of nearest neighbor interpolation.", "ResourceAccumulatorApplyGradient(...): Applies a gradient to a given accumulator.", "ResourceAccumulatorNumAccumulated(...): Returns the number of gradients aggregated in the given accumulators.", "ResourceAccumulatorSetGlobalStep(...): Updates the accumulator with a new value for global_step.", "ResourceAccumulatorTakeGradient(...): Extracts the average gradient in the given ConditionalAccumulator.", "ResourceApplyAdaMax(...): Update '*var' according to the AdaMax algorithm.", "ResourceApplyAdadelta(...): Update '*var' according to the adadelta scheme.", "ResourceApplyAdagrad(...): Update '*var' according to the adagrad scheme.", "ResourceApplyAdagradDA(...): Update '*var' according to the proximal adagrad scheme.", "ResourceApplyAdagradV2(...): Update '*var' according to the adagrad scheme.", "ResourceApplyAdam(...): Update '*var' according to the Adam algorithm.", "ResourceApplyAdamWithAmsgrad(...): Update '*var' according to the Adam algorithm.", "ResourceApplyAddSign(...): Update '*var' according to the AddSign update.", "ResourceApplyCenteredRMSProp(...): Update '*var' according to the centered RMSProp algorithm.", "ResourceApplyFtrl(...): Update '*var' according to the Ftrl-proximal scheme.", "ResourceApplyFtrlV2(...): Update '*var' according to the Ftrl-proximal scheme.", "ResourceApplyGradientDescent(...): Update '*var' by subtracting 'alpha' * 'delta' from it.", "ResourceApplyKerasMomentum(...): Update '*var' according to the momentum scheme.", "ResourceApplyMomentum(...): Update '*var' according to the momentum scheme.", "ResourceApplyPowerSign(...): Update '*var' according to the AddSign update.", "ResourceApplyProximalAdagrad(...): Update 'var' and 'accum' according to FOBOS with Adagrad learning rate.", "ResourceApplyProximalGradientDescent(...): Update '*var' as FOBOS algorithm with fixed learning rate.", "ResourceApplyRMSProp(...): Update '*var' according to the RMSProp algorithm.", "ResourceConditionalAccumulator(...): A conditional accumulator for aggregating gradients.", "ResourceCountUpTo(...): Increments variable pointed to by 'resource' until it reaches 'limit'.", "ResourceGather(...): Gather slices from the variable pointed to by resource according to indices.", "ResourceGatherNd(...)", "ResourceScatterAdd(...): Adds sparse updates to the variable referenced by resource.", "ResourceScatterDiv(...): Divides sparse updates into the variable referenced by resource.", "ResourceScatterMax(...): Reduces sparse updates into the variable referenced by resource using the max operation.", "ResourceScatterMin(...): Reduces sparse updates into the variable referenced by resource using the min operation.", "ResourceScatterMul(...): Multiplies sparse updates into the variable referenced by resource.", "ResourceScatterNdAdd(...): Applies sparse addition to individual values or sli