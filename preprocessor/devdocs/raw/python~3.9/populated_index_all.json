[{"name": "abc", "path": "library/abc", "type": "Runtime", "text": "abc \u2014 Abstract Base Classes Source code: Lib/abc.py This module provides the infrastructure for defining abstract base classes (ABCs) in Python, as outlined in PEP 3119; see the PEP for why this was added to Python. (See also PEP 3141 and the numbers module regarding a type hierarchy for numbers based on ABCs.) The collections module has some concrete classes that derive from ABCs; these can, of course, be further derived. In addition, the collections.abc submodule has some ABCs that can be used to test whether a class or instance provides a particular interface, for example, if it is hashable or if it is a mapping. This module provides the metaclass ABCMeta for defining ABCs and a helper class ABC to alternatively define ABCs through inheritance:  \nclass abc.ABC  \nA helper class that has ABCMeta as its metaclass. With this class, an abstract base class can be created by simply deriving from ABC avoiding sometimes confusing metaclass usage, for example: from abc import ABC\n\nclass MyABC(ABC):\n    pass\n Note that the type of ABC is still ABCMeta, therefore inheriting from ABC requires the usual precautions regarding metaclass usage, as multiple inheritance may lead to metaclass conflicts. One may also define an abstract base class by passing the metaclass keyword and using ABCMeta directly, for example: from abc import ABCMeta\n\nclass MyABC(metaclass=ABCMeta):\n    pass\n  New in version 3.4.  \n  \nclass abc.ABCMeta  \nMetaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as \u201cvirtual subclasses\u201d \u2013 these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won\u2019t show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). 1 Classes created with a metaclass of ABCMeta have the following method:  \nregister(subclass)  \nRegister subclass as a \u201cvirtual subclass\u201d of this ABC. For example: from abc import ABC\n\nclass MyABC(ABC):\n    pass\n\nMyABC.register(tuple)\n\nassert issubclass(tuple, MyABC)\nassert isinstance((), MyABC)\n  Changed in version 3.3: Returns the registered subclass, to allow usage as a class decorator.   Changed in version 3.4: To detect calls to register(), you can use the get_cache_token() function.  \n You can also override this method in an abstract base class:  \n__subclasshook__(subclass)  \n(Must be defined as a class method.) Check whether subclass is considered a subclass of this ABC. This means that you can customize the behavior of issubclass further without the need to call register() on every class you want to consider a subclass of the ABC. (This class method is called from the __subclasscheck__() method of the ABC.) This method should return True, False or NotImplemented. If it returns True, the subclass is considered a subclass of this ABC. If it returns False, the subclass is not considered a subclass of this ABC, even if it would normally be one. If it returns NotImplemented, the subclass check is continued with the usual mechanism. \n For a demonstration of these concepts, look at this example ABC definition: class Foo:\n    def __getitem__(self, index):\n        ...\n    def __len__(self):\n        ...\n    def get_iterator(self):\n        return iter(self)\n\nclass MyIterable(ABC):\n\n    @abstractmethod\n    def __iter__(self):\n        while False:\n            yield None\n\n    def get_iterator(self):\n        return self.__iter__()\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is MyIterable:\n            if any(\"__iter__\" in B.__dict__ for B in C.__mro__):\n                return True\n        return NotImplemented\n\nMyIterable.register(Foo)\n The ABC MyIterable defines the standard iterable method, __iter__(), as an abstract method. The implementation given here can still be called from subclasses. The get_iterator() method is also part of the MyIterable abstract base class, but it does not have to be overridden in non-abstract derived classes. The __subclasshook__() class method defined here says that any class that has an __iter__() method in its __dict__ (or in that of one of its base classes, accessed via the __mro__ list) is considered a MyIterable too. Finally, the last line makes Foo a virtual subclass of MyIterable, even though it does not define an __iter__() method (it uses the old-style iterable protocol, defined in terms of __len__() and __getitem__()). Note that this will not make get_iterator available as a method of Foo, so it is provided separately. \n The abc module also provides the following decorator:  \n@abc.abstractmethod  \nA decorator indicating abstract methods. Using this decorator requires that the class\u2019s metaclass is ABCMeta or is derived from it. A class that has a metaclass derived from ABCMeta cannot be instantiated unless all of its abstract methods and properties are overridden. The abstract methods can be called using any of the normal \u2018super\u2019 call mechanisms. abstractmethod() may be used to declare abstract methods for properties and descriptors. Dynamically adding abstract methods to a class, or attempting to modify the abstraction status of a method or class once it is created, are not supported. The abstractmethod() only affects subclasses derived using regular inheritance; \u201cvirtual subclasses\u201d registered with the ABC\u2019s register() method are not affected. When abstractmethod() is applied in combination with other method descriptors, it should be applied as the innermost decorator, as shown in the following usage examples: class C(ABC):\n    @abstractmethod\n    def my_abstract_method(self, ...):\n        ...\n    @classmethod\n    @abstractmethod\n    def my_abstract_classmethod(cls, ...):\n        ...\n    @staticmethod\n    @abstractmethod\n    def my_abstract_staticmethod(...):\n        ...\n\n    @property\n    @abstractmethod\n    def my_abstract_property(self):\n        ...\n    @my_abstract_property.setter\n    @abstractmethod\n    def my_abstract_property(self, val):\n        ...\n\n    @abstractmethod\n    def _get_x(self):\n        ...\n    @abstractmethod\n    def _set_x(self, val):\n        ...\n    x = property(_get_x, _set_x)\n In order to correctly interoperate with the abstract base class machinery, the descriptor must identify itself as abstract using __isabstractmethod__. In general, this attribute should be True if any of the methods used to compose the descriptor are abstract. For example, Python\u2019s built-in property does the equivalent of: class Descriptor:\n    ...\n    @property\n    def __isabstractmethod__(self):\n        return any(getattr(f, '__isabstractmethod__', False) for\n                   f in (self._fget, self._fset, self._fdel))\n  Note Unlike Java abstract methods, these abstract methods may have an implementation. This implementation can be called via the super() mechanism from the class that overrides it. This could be useful as an end-point for a super-call in a framework that uses cooperative multiple-inheritance.  \n The abc module also supports the following legacy decorators:  \n@abc.abstractclassmethod  \n New in version 3.2.   Deprecated since version 3.3: It is now possible to use classmethod with abstractmethod(), making this decorator redundant.  A subclass of the built-in classmethod(), indicating an abstract classmethod. Otherwise it is similar to abstractmethod(). This special case is deprecated, as the classmethod() decorator is now correctly identified as abstract when applied to an abstract method: class C(ABC):\n    @classmethod\n    @abstractmethod\n    def my_abstract_classmethod(cls, ...):\n        ...\n \n  \n@abc.abstractstaticmethod  \n New in version 3.2.   Deprecated since version 3.3: It is now possible to use staticmethod with abstractmethod(), making this decorator redundant.  A subclass of the built-in staticmethod(), indicating an abstract staticmethod. Otherwise it is similar to abstractmethod(). This special case is deprecated, as the staticmethod() decorator is now correctly identified as abstract when applied to an abstract method: class C(ABC):\n    @staticmethod\n    @abstractmethod\n    def my_abstract_staticmethod(...):\n        ...\n \n  \n@abc.abstractproperty  \n Deprecated since version 3.3: It is now possible to use property, property.getter(), property.setter() and property.deleter() with abstractmethod(), making this decorator redundant.  A subclass of the built-in property(), indicating an abstract property. This special case is deprecated, as the property() decorator is now correctly identified as abstract when applied to an abstract method: class C(ABC):\n    @property\n    @abstractmethod\n    def my_abstract_property(self):\n        ...\n The above example defines a read-only property; you can also define a read-write abstract property by appropriately marking one or more of the underlying methods as abstract: class C(ABC):\n    @property\n    def x(self):\n        ...\n\n    @x.setter\n    @abstractmethod\n    def x(self, val):\n        ...\n If only some components are abstract, only those components need to be updated to create a concrete property in a subclass: class D(C):\n    @C.x.setter\n    def x(self, val):\n        ...\n \n The abc module also provides the following functions:  \nabc.get_cache_token()  \nReturns the current abstract base class cache token. The token is an opaque object (that supports equality testing) identifying the current version of the abstract base class cache for virtual subclasses. The token changes with every call to ABCMeta.register() on any ABC.  New in version 3.4.  \n Footnotes  \n1  \nC++ programmers should note that Python\u2019s virtual base class concept is not the same as C++\u2019s.  \n"}, {"name": "abc.ABC", "path": "library/abc#abc.ABC", "type": "Runtime", "text": " \nclass abc.ABC  \nA helper class that has ABCMeta as its metaclass. With this class, an abstract base class can be created by simply deriving from ABC avoiding sometimes confusing metaclass usage, for example: from abc import ABC\n\nclass MyABC(ABC):\n    pass\n Note that the type of ABC is still ABCMeta, therefore inheriting from ABC requires the usual precautions regarding metaclass usage, as multiple inheritance may lead to metaclass conflicts. One may also define an abstract base class by passing the metaclass keyword and using ABCMeta directly, for example: from abc import ABCMeta\n\nclass MyABC(metaclass=ABCMeta):\n    pass\n  New in version 3.4.  \n"}, {"name": "abc.ABCMeta", "path": "library/abc#abc.ABCMeta", "type": "Runtime", "text": " \nclass abc.ABCMeta  \nMetaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as \u201cvirtual subclasses\u201d \u2013 these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won\u2019t show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). 1 Classes created with a metaclass of ABCMeta have the following method:  \nregister(subclass)  \nRegister subclass as a \u201cvirtual subclass\u201d of this ABC. For example: from abc import ABC\n\nclass MyABC(ABC):\n    pass\n\nMyABC.register(tuple)\n\nassert issubclass(tuple, MyABC)\nassert isinstance((), MyABC)\n  Changed in version 3.3: Returns the registered subclass, to allow usage as a class decorator.   Changed in version 3.4: To detect calls to register(), you can use the get_cache_token() function.  \n You can also override this method in an abstract base class:  \n__subclasshook__(subclass)  \n(Must be defined as a class method.) Check whether subclass is considered a subclass of this ABC. This means that you can customize the behavior of issubclass further without the need to call register() on every class you want to consider a subclass of the ABC. (This class method is called from the __subclasscheck__() method of the ABC.) This method should return True, False or NotImplemented. If it returns True, the subclass is considered a subclass of this ABC. If it returns False, the subclass is not considered a subclass of this ABC, even if it would normally be one. If it returns NotImplemented, the subclass check is continued with the usual mechanism. \n For a demonstration of these concepts, look at this example ABC definition: class Foo:\n    def __getitem__(self, index):\n        ...\n    def __len__(self):\n        ...\n    def get_iterator(self):\n        return iter(self)\n\nclass MyIterable(ABC):\n\n    @abstractmethod\n    def __iter__(self):\n        while False:\n            yield None\n\n    def get_iterator(self):\n        return self.__iter__()\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is MyIterable:\n            if any(\"__iter__\" in B.__dict__ for B in C.__mro__):\n                return True\n        return NotImplemented\n\nMyIterable.register(Foo)\n The ABC MyIterable defines the standard iterable method, __iter__(), as an abstract method. The implementation given here can still be called from subclasses. The get_iterator() method is also part of the MyIterable abstract base class, but it does not have to be overridden in non-abstract derived classes. The __subclasshook__() class method defined here says that any class that has an __iter__() method in its __dict__ (or in that of one of its base classes, accessed via the __mro__ list) is considered a MyIterable too. Finally, the last line makes Foo a virtual subclass of MyIterable, even though it does not define an __iter__() method (it uses the old-style iterable protocol, defined in terms of __len__() and __getitem__()). Note that this will not make get_iterator available as a method of Foo, so it is provided separately. \n"}, {"name": "abc.ABCMeta.register()", "path": "library/abc#abc.ABCMeta.register", "type": "Runtime", "text": " \nregister(subclass)  \nRegister subclass as a \u201cvirtual subclass\u201d of this ABC. For example: from abc import ABC\n\nclass MyABC(ABC):\n    pass\n\nMyABC.register(tuple)\n\nassert issubclass(tuple, MyABC)\nassert isinstance((), MyABC)\n  Changed in version 3.3: Returns the registered subclass, to allow usage as a class decorator.   Changed in version 3.4: To detect calls to register(), you can use the get_cache_token() function.  \n"}, {"name": "abc.ABCMeta.__subclasshook__()", "path": "library/abc#abc.ABCMeta.__subclasshook__", "type": "Runtime", "text": " \n__subclasshook__(subclass)  \n(Must be defined as a class method.) Check whether subclass is considered a subclass of this ABC. This means that you can customize the behavior of issubclass further without the need to call register() on every class you want to consider a subclass of the ABC. (This class method is called from the __subclasscheck__() method of the ABC.) This method should return True, False or NotImplemented. If it returns True, the subclass is considered a subclass of this ABC. If it returns False, the subclass is not considered a subclass of this ABC, even if it would normally be one. If it returns NotImplemented, the subclass check is continued with the usual mechanism. \n"}, {"name": "abc.abstractclassmethod()", "path": "library/abc#abc.abstractclassmethod", "type": "Runtime", "text": " \n@abc.abstractclassmethod  \n New in version 3.2.   Deprecated since version 3.3: It is now possible to use classmethod with abstractmethod(), making this decorator redundant.  A subclass of the built-in classmethod(), indicating an abstract classmethod. Otherwise it is similar to abstractmethod(). This special case is deprecated, as the classmethod() decorator is now correctly identified as abstract when applied to an abstract method: class C(ABC):\n    @classmethod\n    @abstractmethod\n    def my_abstract_classmethod(cls, ...):\n        ...\n \n"}, {"name": "abc.abstractmethod()", "path": "library/abc#abc.abstractmethod", "type": "Runtime", "text": " \n@abc.abstractmethod  \nA decorator indicating abstract methods. Using this decorator requires that the class\u2019s metaclass is ABCMeta or is derived from it. A class that has a metaclass derived from ABCMeta cannot be instantiated unless all of its abstract methods and properties are overridden. The abstract methods can be called using any of the normal \u2018super\u2019 call mechanisms. abstractmethod() may be used to declare abstract methods for properties and descriptors. Dynamically adding abstract methods to a class, or attempting to modify the abstraction status of a method or class once it is created, are not supported. The abstractmethod() only affects subclasses derived using regular inheritance; \u201cvirtual subclasses\u201d registered with the ABC\u2019s register() method are not affected. When abstractmethod() is applied in combination with other method descriptors, it should be applied as the innermost decorator, as shown in the following usage examples: class C(ABC):\n    @abstractmethod\n    def my_abstract_method(self, ...):\n        ...\n    @classmethod\n    @abstractmethod\n    def my_abstract_classmethod(cls, ...):\n        ...\n    @staticmethod\n    @abstractmethod\n    def my_abstract_staticmethod(...):\n        ...\n\n    @property\n    @abstractmethod\n    def my_abstract_property(self):\n        ...\n    @my_abstract_property.setter\n    @abstractmethod\n    def my_abstract_property(self, val):\n        ...\n\n    @abstractmethod\n    def _get_x(self):\n        ...\n    @abstractmethod\n    def _set_x(self, val):\n        ...\n    x = property(_get_x, _set_x)\n In order to correctly interoperate with the abstract base class machinery, the descriptor must identify itself as abstract using __isabstractmethod__. In general, this attribute should be True if any of the methods used to compose the descriptor are abstract. For example, Python\u2019s built-in property does the equivalent of: class Descriptor:\n    ...\n    @property\n    def __isabstractmethod__(self):\n        return any(getattr(f, '__isabstractmethod__', False) for\n                   f in (self._fget, self._fset, self._fdel))\n  Note Unlike Java abstract methods, these abstract methods may have an implementation. This implementation can be called via the super() mechanism from the class that overrides it. This could be useful as an end-point for a super-call in a framework that uses cooperative multiple-inheritance.  \n"}, {"name": "abc.abstractproperty()", "path": "library/abc#abc.abstractproperty", "type": "Runtime", "text": " \n@abc.abstractproperty  \n Deprecated since version 3.3: It is now possible to use property, property.getter(), property.setter() and property.deleter() with abstractmethod(), making this decorator redundant.  A subclass of the built-in property(), indicating an abstract property. This special case is deprecated, as the property() decorator is now correctly identified as abstract when applied to an abstract method: class C(ABC):\n    @property\n    @abstractmethod\n    def my_abstract_property(self):\n        ...\n The above example defines a read-only property; you can also define a read-write abstract property by appropriately marking one or more of the underlying methods as abstract: class C(ABC):\n    @property\n    def x(self):\n        ...\n\n    @x.setter\n    @abstractmethod\n    def x(self, val):\n        ...\n If only some components are abstract, only those components need to be updated to create a concrete property in a subclass: class D(C):\n    @C.x.setter\n    def x(self, val):\n        ...\n \n"}, {"name": "abc.abstractstaticmethod()", "path": "library/abc#abc.abstractstaticmethod", "type": "Runtime", "text": " \n@abc.abstractstaticmethod  \n New in version 3.2.   Deprecated since version 3.3: It is now possible to use staticmethod with abstractmethod(), making this decorator redundant.  A subclass of the built-in staticmethod(), indicating an abstract staticmethod. Otherwise it is similar to abstractmethod(). This special case is deprecated, as the staticmethod() decorator is now correctly identified as abstract when applied to an abstract method: class C(ABC):\n    @staticmethod\n    @abstractmethod\n    def my_abstract_staticmethod(...):\n        ...\n \n"}, {"name": "abc.get_cache_token()", "path": "library/abc#abc.get_cache_token", "type": "Runtime", "text": " \nabc.get_cache_token()  \nReturns the current abstract base class cache token. The token is an opaque object (that supports equality testing) identifying the current version of the abstract base class cache for virtual subclasses. The token changes with every call to ABCMeta.register() on any ABC.  New in version 3.4.  \n"}, {"name": "abs()", "path": "library/functions#abs", "type": "Built-in Functions", "text": " \nabs(x)  \nReturn the absolute value of a number. The argument may be an integer, a floating point number, or an object implementing __abs__(). If the argument is a complex number, its magnitude is returned. \n"}, {"name": "aifc", "path": "library/aifc", "type": "Multimedia", "text": "aifc \u2014 Read and write AIFF and AIFC files Source code: Lib/aifc.py This module provides support for reading and writing AIFF and AIFF-C files. AIFF is Audio Interchange File Format, a format for storing digital audio samples in a file. AIFF-C is a newer version of the format that includes the ability to compress the audio data. Audio files have a number of parameters that describe the audio data. The sampling rate or frame rate is the number of times per second the sound is sampled. The number of channels indicate if the audio is mono, stereo, or quadro. Each frame consists of one sample per channel. The sample size is the size in bytes of each sample. Thus a frame consists of nchannels * samplesize bytes, and a second\u2019s worth of audio consists of nchannels * samplesize * framerate bytes. For example, CD quality audio has a sample size of two bytes (16 bits), uses two channels (stereo) and has a frame rate of 44,100 frames/second. This gives a frame size of 4 bytes (2*2), and a second\u2019s worth occupies 2*2*44100 bytes (176,400 bytes). Module aifc defines the following function:  \naifc.open(file, mode=None)  \nOpen an AIFF or AIFF-C file and return an object instance with methods that are described below. The argument file is either a string naming a file or a file object. mode must be 'r' or 'rb' when the file must be opened for reading, or 'w' or 'wb' when the file must be opened for writing. If omitted, file.mode is used if it exists, otherwise 'rb' is used. When used for writing, the file object should be seekable, unless you know ahead of time how many samples you are going to write in total and use writeframesraw() and setnframes(). The open() function may be used in a with statement. When the with block completes, the close() method is called.  Changed in version 3.4: Support for the with statement was added.  \n Objects returned by open() when a file is opened for reading have the following methods:  \naifc.getnchannels()  \nReturn the number of audio channels (1 for mono, 2 for stereo). \n  \naifc.getsampwidth()  \nReturn the size in bytes of individual samples. \n  \naifc.getframerate()  \nReturn the sampling rate (number of audio frames per second). \n  \naifc.getnframes()  \nReturn the number of audio frames in the file. \n  \naifc.getcomptype()  \nReturn a bytes array of length 4 describing the type of compression used in the audio file. For AIFF files, the returned value is b'NONE'. \n  \naifc.getcompname()  \nReturn a bytes array convertible to a human-readable description of the type of compression used in the audio file. For AIFF files, the returned value is b'not compressed'. \n  \naifc.getparams()  \nReturns a namedtuple() (nchannels, sampwidth,\nframerate, nframes, comptype, compname), equivalent to output of the get*() methods. \n  \naifc.getmarkers()  \nReturn a list of markers in the audio file. A marker consists of a tuple of three elements. The first is the mark ID (an integer), the second is the mark position in frames from the beginning of the data (an integer), the third is the name of the mark (a string). \n  \naifc.getmark(id)  \nReturn the tuple as described in getmarkers() for the mark with the given id. \n  \naifc.readframes(nframes)  \nRead and return the next nframes frames from the audio file. The returned data is a string containing for each frame the uncompressed samples of all channels. \n  \naifc.rewind()  \nRewind the read pointer. The next readframes() will start from the beginning. \n  \naifc.setpos(pos)  \nSeek to the specified frame number. \n  \naifc.tell()  \nReturn the current frame number. \n  \naifc.close()  \nClose the AIFF file. After calling this method, the object can no longer be used. \n Objects returned by open() when a file is opened for writing have all the above methods, except for readframes() and setpos(). In addition the following methods exist. The get*() methods can only be called after the corresponding set*() methods have been called. Before the first writeframes() or writeframesraw(), all parameters except for the number of frames must be filled in.  \naifc.aiff()  \nCreate an AIFF file. The default is that an AIFF-C file is created, unless the name of the file ends in '.aiff' in which case the default is an AIFF file. \n  \naifc.aifc()  \nCreate an AIFF-C file. The default is that an AIFF-C file is created, unless the name of the file ends in '.aiff' in which case the default is an AIFF file. \n  \naifc.setnchannels(nchannels)  \nSpecify the number of channels in the audio file. \n  \naifc.setsampwidth(width)  \nSpecify the size in bytes of audio samples. \n  \naifc.setframerate(rate)  \nSpecify the sampling frequency in frames per second. \n  \naifc.setnframes(nframes)  \nSpecify the number of frames that are to be written to the audio file. If this parameter is not set, or not set correctly, the file needs to support seeking. \n  \naifc.setcomptype(type, name)  \nSpecify the compression type. If not specified, the audio data will not be compressed. In AIFF files, compression is not possible. The name parameter should be a human-readable description of the compression type as a bytes array, the type parameter should be a bytes array of length 4. Currently the following compression types are supported: b'NONE', b'ULAW', b'ALAW', b'G722'. \n  \naifc.setparams(nchannels, sampwidth, framerate, comptype, compname)  \nSet all the above parameters at once. The argument is a tuple consisting of the various parameters. This means that it is possible to use the result of a getparams() call as argument to setparams(). \n  \naifc.setmark(id, pos, name)  \nAdd a mark with the given id (larger than 0), and the given name at the given position. This method can be called at any time before close(). \n  \naifc.tell()  \nReturn the current write position in the output file. Useful in combination with setmark(). \n  \naifc.writeframes(data)  \nWrite data to the output file. This method can only be called after the audio file parameters have been set.  Changed in version 3.4: Any bytes-like object is now accepted.  \n  \naifc.writeframesraw(data)  \nLike writeframes(), except that the header of the audio file is not updated.  Changed in version 3.4: Any bytes-like object is now accepted.  \n  \naifc.close()  \nClose the AIFF file. The header of the file is updated to reflect the actual size of the audio data. After calling this method, the object can no longer be used. \n\n"}, {"name": "aifc.aifc.aifc()", "path": "library/aifc#aifc.aifc.aifc", "type": "Multimedia", "text": " \naifc.aifc()  \nCreate an AIFF-C file. The default is that an AIFF-C file is created, unless the name of the file ends in '.aiff' in which case the default is an AIFF file. \n"}, {"name": "aifc.aifc.aiff()", "path": "library/aifc#aifc.aifc.aiff", "type": "Multimedia", "text": " \naifc.aiff()  \nCreate an AIFF file. The default is that an AIFF-C file is created, unless the name of the file ends in '.aiff' in which case the default is an AIFF file. \n"}, {"name": "aifc.aifc.close()", "path": "library/aifc#aifc.aifc.close", "type": "Multimedia", "text": " \naifc.close()  \nClose the AIFF file. After calling this method, the object can no longer be used. \n"}, {"name": "aifc.aifc.getcompname()", "path": "library/aifc#aifc.aifc.getcompname", "type": "Multimedia", "text": " \naifc.getcompname()  \nReturn a bytes array convertible to a human-readable description of the type of compression used in the audio file. For AIFF files, the returned value is b'not compressed'. \n"}, {"name": "aifc.aifc.getcomptype()", "path": "library/aifc#aifc.aifc.getcomptype", "type": "Multimedia", "text": " \naifc.getcomptype()  \nReturn a bytes array of length 4 describing the type of compression used in the audio file. For AIFF files, the returned value is b'NONE'. \n"}, {"name": "aifc.aifc.getframerate()", "path": "library/aifc#aifc.aifc.getframerate", "type": "Multimedia", "text": " \naifc.getframerate()  \nReturn the sampling rate (number of audio frames per second). \n"}, {"name": "aifc.aifc.getmark()", "path": "library/aifc#aifc.aifc.getmark", "type": "Multimedia", "text": " \naifc.getmark(id)  \nReturn the tuple as described in getmarkers() for the mark with the given id. \n"}, {"name": "aifc.aifc.getmarkers()", "path": "library/aifc#aifc.aifc.getmarkers", "type": "Multimedia", "text": " \naifc.getmarkers()  \nReturn a list of markers in the audio file. A marker consists of a tuple of three elements. The first is the mark ID (an integer), the second is the mark position in frames from the beginning of the data (an integer), the third is the name of the mark (a string). \n"}, {"name": "aifc.aifc.getnchannels()", "path": "library/aifc#aifc.aifc.getnchannels", "type": "Multimedia", "text": " \naifc.getnchannels()  \nReturn the number of audio channels (1 for mono, 2 for stereo). \n"}, {"name": "aifc.aifc.getnframes()", "path": "library/aifc#aifc.aifc.getnframes", "type": "Multimedia", "text": " \naifc.getnframes()  \nReturn the number of audio frames in the file. \n"}, {"name": "aifc.aifc.getparams()", "path": "library/aifc#aifc.aifc.getparams", "type": "Multimedia", "text": " \naifc.getparams()  \nReturns a namedtuple() (nchannels, sampwidth,\nframerate, nframes, comptype, compname), equivalent to output of the get*() methods. \n"}, {"name": "aifc.aifc.getsampwidth()", "path": "library/aifc#aifc.aifc.getsampwidth", "type": "Multimedia", "text": " \naifc.getsampwidth()  \nReturn the size in bytes of individual samples. \n"}, {"name": "aifc.aifc.readframes()", "path": "library/aifc#aifc.aifc.readframes", "type": "Multimedia", "text": " \naifc.readframes(nframes)  \nRead and return the next nframes frames from the audio file. The returned data is a string containing for each frame the uncompressed samples of all channels. \n"}, {"name": "aifc.aifc.rewind()", "path": "library/aifc#aifc.aifc.rewind", "type": "Multimedia", "text": " \naifc.rewind()  \nRewind the read pointer. The next readframes() will start from the beginning. \n"}, {"name": "aifc.aifc.setcomptype()", "path": "library/aifc#aifc.aifc.setcomptype", "type": "Multimedia", "text": " \naifc.setcomptype(type, name)  \nSpecify the compression type. If not specified, the audio data will not be compressed. In AIFF files, compression is not possible. The name parameter should be a human-readable description of the compression type as a bytes array, the type parameter should be a bytes array of length 4. Currently the following compression types are supported: b'NONE', b'ULAW', b'ALAW', b'G722'. \n"}, {"name": "aifc.aifc.setframerate()", "path": "library/aifc#aifc.aifc.setframerate", "type": "Multimedia", "text": " \naifc.setframerate(rate)  \nSpecify the sampling frequency in frames per second. \n"}, {"name": "aifc.aifc.setmark()", "path": "library/aifc#aifc.aifc.setmark", "type": "Multimedia", "text": " \naifc.setmark(id, pos, name)  \nAdd a mark with the given id (larger than 0), and the given name at the given position. This method can be called at any time before close(). \n"}, {"name": "aifc.aifc.setnchannels()", "path": "library/aifc#aifc.aifc.setnchannels", "type": "Multimedia", "text": " \naifc.setnchannels(nchannels)  \nSpecify the number of channels in the audio file. \n"}, {"name": "aifc.aifc.setnframes()", "path": "library/aifc#aifc.aifc.setnframes", "type": "Multimedia", "text": " \naifc.setnframes(nframes)  \nSpecify the number of frames that are to be written to the audio file. If this parameter is not set, or not set correctly, the file needs to support seeking. \n"}, {"name": "aifc.aifc.setparams()", "path": "library/aifc#aifc.aifc.setparams", "type": "Multimedia", "text": " \naifc.setparams(nchannels, sampwidth, framerate, comptype, compname)  \nSet all the above parameters at once. The argument is a tuple consisting of the various parameters. This means that it is possible to use the result of a getparams() call as argument to setparams(). \n"}, {"name": "aifc.aifc.setpos()", "path": "library/aifc#aifc.aifc.setpos", "type": "Multimedia", "text": " \naifc.setpos(pos)  \nSeek to the specified frame number. \n"}, {"name": "aifc.aifc.setsampwidth()", "path": "library/aifc#aifc.aifc.setsampwidth", "type": "Multimedia", "text": " \naifc.setsampwidth(width)  \nSpecify the size in bytes of audio samples. \n"}, {"name": "aifc.aifc.tell()", "path": "library/aifc#aifc.aifc.tell", "type": "Multimedia", "text": " \naifc.tell()  \nReturn the current frame number. \n"}, {"name": "aifc.aifc.writeframes()", "path": "library/aifc#aifc.aifc.writeframes", "type": "Multimedia", "text": " \naifc.writeframes(data)  \nWrite data to the output file. This method can only be called after the audio file parameters have been set.  Changed in version 3.4: Any bytes-like object is now accepted.  \n"}, {"name": "aifc.aifc.writeframesraw()", "path": "library/aifc#aifc.aifc.writeframesraw", "type": "Multimedia", "text": " \naifc.writeframesraw(data)  \nLike writeframes(), except that the header of the audio file is not updated.  Changed in version 3.4: Any bytes-like object is now accepted.  \n"}, {"name": "aifc.open()", "path": "library/aifc#aifc.open", "type": "Multimedia", "text": " \naifc.open(file, mode=None)  \nOpen an AIFF or AIFF-C file and return an object instance with methods that are described below. The argument file is either a string naming a file or a file object. mode must be 'r' or 'rb' when the file must be opened for reading, or 'w' or 'wb' when the file must be opened for writing. If omitted, file.mode is used if it exists, otherwise 'rb' is used. When used for writing, the file object should be seekable, unless you know ahead of time how many samples you are going to write in total and use writeframesraw() and setnframes(). The open() function may be used in a with statement. When the with block completes, the close() method is called.  Changed in version 3.4: Support for the with statement was added.  \n"}, {"name": "all()", "path": "library/functions#all", "type": "Built-in Functions", "text": " \nall(iterable)  \nReturn True if all elements of the iterable are true (or if the iterable is empty). Equivalent to: def all(iterable):\n    for element in iterable:\n        if not element:\n            return False\n    return True\n \n"}, {"name": "any()", "path": "library/functions#any", "type": "Built-in Functions", "text": " \nany(iterable)  \nReturn True if any element of the iterable is true. If the iterable is empty, return False. Equivalent to: def any(iterable):\n    for element in iterable:\n        if element:\n            return True\n    return False\n \n"}, {"name": "argparse", "path": "library/argparse", "type": "Operating System", "text": "argparse \u2014 Parser for command-line options, arguments and sub-commands  New in version 3.2.  Source code: Lib/argparse.py  Tutorial This page contains the API reference information. For a more gentle introduction to Python command-line parsing, have a look at the argparse tutorial.  The argparse module makes it easy to write user-friendly command-line interfaces. The program defines what arguments it requires, and argparse will figure out how to parse those out of sys.argv. The argparse module also automatically generates help and usage messages and issues errors when users give the program invalid arguments. Example The following code is a Python program that takes a list of integers and produces either the sum or the max: import argparse\n\nparser = argparse.ArgumentParser(description='Process some integers.')\nparser.add_argument('integers', metavar='N', type=int, nargs='+',\n                    help='an integer for the accumulator')\nparser.add_argument('--sum', dest='accumulate', action='store_const',\n                    const=sum, default=max,\n                    help='sum the integers (default: find the max)')\n\nargs = parser.parse_args()\nprint(args.accumulate(args.integers))\n Assuming the Python code above is saved into a file called prog.py, it can be run at the command line and provides useful help messages: $ python prog.py -h\nusage: prog.py [-h] [--sum] N [N ...]\n\nProcess some integers.\n\npositional arguments:\n N           an integer for the accumulator\n\noptional arguments:\n -h, --help  show this help message and exit\n --sum       sum the integers (default: find the max)\n When run with the appropriate arguments, it prints either the sum or the max of the command-line integers: $ python prog.py 1 2 3 4\n4\n\n$ python prog.py 1 2 3 4 --sum\n10\n If invalid arguments are passed in, it will issue an error: $ python prog.py a b c\nusage: prog.py [-h] [--sum] N [N ...]\nprog.py: error: argument N: invalid int value: 'a'\n The following sections walk you through this example. Creating a parser The first step in using the argparse is creating an ArgumentParser object: >>> parser = argparse.ArgumentParser(description='Process some integers.')\n The ArgumentParser object will hold all the information necessary to parse the command line into Python data types. Adding arguments Filling an ArgumentParser with information about program arguments is done by making calls to the add_argument() method. Generally, these calls tell the ArgumentParser how to take the strings on the command line and turn them into objects. This information is stored and used when parse_args() is called. For example: >>> parser.add_argument('integers', metavar='N', type=int, nargs='+',\n...                     help='an integer for the accumulator')\n>>> parser.add_argument('--sum', dest='accumulate', action='store_const',\n...                     const=sum, default=max,\n...                     help='sum the integers (default: find the max)')\n Later, calling parse_args() will return an object with two attributes, integers and accumulate. The integers attribute will be a list of one or more ints, and the accumulate attribute will be either the sum() function, if --sum was specified at the command line, or the max() function if it was not. Parsing arguments ArgumentParser parses arguments through the parse_args() method. This will inspect the command line, convert each argument to the appropriate type and then invoke the appropriate action. In most cases, this means a simple Namespace object will be built up from attributes parsed out of the command line: >>> parser.parse_args(['--sum', '7', '-1', '42'])\nNamespace(accumulate=<built-in function sum>, integers=[7, -1, 42])\n In a script, parse_args() will typically be called with no arguments, and the ArgumentParser will automatically determine the command-line arguments from sys.argv. ArgumentParser objects  \nclass argparse.ArgumentParser(prog=None, usage=None, description=None, epilog=None, parents=[], formatter_class=argparse.HelpFormatter, prefix_chars='-', fromfile_prefix_chars=None, argument_default=None, conflict_handler='error', add_help=True, allow_abbrev=True, exit_on_error=True)  \nCreate a new ArgumentParser object. All parameters should be passed as keyword arguments. Each parameter has its own more detailed description below, but in short they are:  \nprog - The name of the program (default: sys.argv[0]) \nusage - The string describing the program usage (default: generated from arguments added to parser) \ndescription - Text to display before the argument help (default: none) \nepilog - Text to display after the argument help (default: none) \nparents - A list of ArgumentParser objects whose arguments should also be included \nformatter_class - A class for customizing the help output \nprefix_chars - The set of characters that prefix optional arguments (default: \u2018-\u2018) \nfromfile_prefix_chars - The set of characters that prefix files from which additional arguments should be read (default: None) \nargument_default - The global default value for arguments (default: None) \nconflict_handler - The strategy for resolving conflicting optionals (usually unnecessary) \nadd_help - Add a -h/--help option to the parser (default: True) \nallow_abbrev - Allows long options to be abbreviated if the abbreviation is unambiguous. (default: True) \nexit_on_error - Determines whether or not ArgumentParser exits with error info when an error occurs. (default: True)   Changed in version 3.5: allow_abbrev parameter was added.   Changed in version 3.8: In previous versions, allow_abbrev also disabled grouping of short flags such as -vv to mean -v -v.   Changed in version 3.9: exit_on_error parameter was added.  \n The following sections describe how each of these are used. prog By default, ArgumentParser objects use sys.argv[0] to determine how to display the name of the program in help messages. This default is almost always desirable because it will make the help messages match how the program was invoked on the command line. For example, consider a file named myprogram.py with the following code: import argparse\nparser = argparse.ArgumentParser()\nparser.add_argument('--foo', help='foo help')\nargs = parser.parse_args()\n The help for this program will display myprogram.py as the program name (regardless of where the program was invoked from): $ python myprogram.py --help\nusage: myprogram.py [-h] [--foo FOO]\n\noptional arguments:\n -h, --help  show this help message and exit\n --foo FOO   foo help\n$ cd ..\n$ python subdir/myprogram.py --help\nusage: myprogram.py [-h] [--foo FOO]\n\noptional arguments:\n -h, --help  show this help message and exit\n --foo FOO   foo help\n To change this default behavior, another value can be supplied using the prog= argument to ArgumentParser: >>> parser = argparse.ArgumentParser(prog='myprogram')\n>>> parser.print_help()\nusage: myprogram [-h]\n\noptional arguments:\n -h, --help  show this help message and exit\n Note that the program name, whether determined from sys.argv[0] or from the prog= argument, is available to help messages using the %(prog)s format specifier. >>> parser = argparse.ArgumentParser(prog='myprogram')\n>>> parser.add_argument('--foo', help='foo of the %(prog)s program')\n>>> parser.print_help()\nusage: myprogram [-h] [--foo FOO]\n\noptional arguments:\n -h, --help  show this help message and exit\n --foo FOO   foo of the myprogram program\n usage By default, ArgumentParser calculates the usage message from the arguments it contains: >>> parser = argparse.ArgumentParser(prog='PROG')\n>>> parser.add_argument('--foo', nargs='?', help='foo help')\n>>> parser.add_argument('bar', nargs='+', help='bar help')\n>>> parser.print_help()\nusage: PROG [-h] [--foo [FOO]] bar [bar ...]\n\npositional arguments:\n bar          bar help\n\noptional arguments:\n -h, --help   show this help message and exit\n --foo [FOO]  foo help\n The default message can be overridden with the usage= keyword argument: >>> parser = argparse.ArgumentParser(prog='PROG', usage='%(prog)s [options]')\n>>> parser.add_argument('--foo', nargs='?', help='foo help')\n>>> parser.add_argument('bar', nargs='+', help='bar help')\n>>> parser.print_help()\nusage: PROG [options]\n\npositional arguments:\n bar          bar help\n\noptional arguments:\n -h, --help   show this help message and exit\n --foo [FOO]  foo help\n The %(prog)s format specifier is available to fill in the program name in your usage messages. description Most calls to the ArgumentParser constructor will use the description= keyword argument. This argument gives a brief description of what the program does and how it works. In help messages, the description is displayed between the command-line usage string and the help messages for the various arguments: >>> parser = argparse.ArgumentParser(description='A foo that bars')\n>>> parser.print_help()\nusage: argparse.py [-h]\n\nA foo that bars\n\noptional arguments:\n -h, --help  show this help message and exit\n By default, the description will be line-wrapped so that it fits within the given space. To change this behavior, see the formatter_class argument. epilog Some programs like to display additional description of the program after the description of the arguments. Such text can be specified using the epilog= argument to ArgumentParser: >>> parser = argparse.ArgumentParser(\n...     description='A foo that bars',\n...     epilog=\"And that's how you'd foo a bar\")\n>>> parser.print_help()\nusage: argparse.py [-h]\n\nA foo that bars\n\noptional arguments:\n -h, --help  show this help message and exit\n\nAnd that's how you'd foo a bar\n As with the description argument, the epilog= text is by default line-wrapped, but this behavior can be adjusted with the formatter_class argument to ArgumentParser. parents Sometimes, several parsers share a common set of arguments. Rather than repeating the definitions of these arguments, a single parser with all the shared arguments and passed to parents= argument to ArgumentParser can be used. The parents= argument takes a list of ArgumentParser objects, collects all the positional and optional actions from them, and adds these actions to the ArgumentParser object being constructed: >>> parent_parser = argparse.ArgumentParser(add_help=False)\n>>> parent_parser.add_argument('--parent', type=int)\n\n>>> foo_parser = argparse.ArgumentParser(parents=[parent_parser])\n>>> foo_parser.add_argument('foo')\n>>> foo_parser.parse_args(['--parent', '2', 'XXX'])\nNamespace(foo='XXX', parent=2)\n\n>>> bar_parser = argparse.ArgumentParser(parents=[parent_parser])\n>>> bar_parser.add_argument('--bar')\n>>> bar_parser.parse_args(['--bar', 'YYY'])\nNamespace(bar='YYY', parent=None)\n Note that most parent parsers will specify add_help=False. Otherwise, the ArgumentParser will see two -h/--help options (one in the parent and one in the child) and raise an error.  Note You must fully initialize the parsers before passing them via parents=. If you change the parent parsers after the child parser, those changes will not be reflected in the child.  formatter_class ArgumentParser objects allow the help formatting to be customized by specifying an alternate formatting class. Currently, there are four such classes:  \nclass argparse.RawDescriptionHelpFormatter  \nclass argparse.RawTextHelpFormatter  \nclass argparse.ArgumentDefaultsHelpFormatter  \nclass argparse.MetavarTypeHelpFormatter \n RawDescriptionHelpFormatter and RawTextHelpFormatter give more control over how textual descriptions are displayed. By default, ArgumentParser objects line-wrap the description and epilog texts in command-line help messages: >>> parser = argparse.ArgumentParser(\n...     prog='PROG',\n...     description='''this description\n...         was indented weird\n...             but that is okay''',\n...     epilog='''\n...             likewise for this epilog whose whitespace will\n...         be cleaned up and whose words will be wrapped\n...         across a couple lines''')\n>>> parser.print_help()\nusage: PROG [-h]\n\nthis description was indented weird but that is okay\n\noptional arguments:\n -h, --help  show this help message and exit\n\nlikewise for this epilog whose whitespace will be cleaned up and whose words\nwill be wrapped across a couple lines\n Passing RawDescriptionHelpFormatter as formatter_class= indicates that description and epilog are already correctly formatted and should not be line-wrapped: >>> parser = argparse.ArgumentParser(\n...     prog='PROG',\n...     formatter_class=argparse.RawDescriptionHelpFormatter,\n...     description=textwrap.dedent('''\\\n...         Please do not mess up this text!\n...         --------------------------------\n...             I have indented it\n...             exactly the way\n...             I want it\n...         '''))\n>>> parser.print_help()\nusage: PROG [-h]\n\nPlease do not mess up this text!\n--------------------------------\n   I have indented it\n   exactly the way\n   I want it\n\noptional arguments:\n -h, --help  show this help message and exit\n RawTextHelpFormatter maintains whitespace for all sorts of help text, including argument descriptions. However, multiple new lines are replaced with one. If you wish to preserve multiple blank lines, add spaces between the newlines. ArgumentDefaultsHelpFormatter automatically adds information about default values to each of the argument help messages: >>> parser = argparse.ArgumentParser(\n...     prog='PROG',\n...     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n>>> parser.add_argument('--foo', type=int, default=42, help='FOO!')\n>>> parser.add_argument('bar', nargs='*', default=[1, 2, 3], help='BAR!')\n>>> parser.print_help()\nusage: PROG [-h] [--foo FOO] [bar ...]\n\npositional arguments:\n bar         BAR! (default: [1, 2, 3])\n\noptional arguments:\n -h, --help  show this help message and exit\n --foo FOO   FOO! (default: 42)\n MetavarTypeHelpFormatter uses the name of the type argument for each argument as the display name for its values (rather than using the dest as the regular formatter does): >>> parser = argparse.ArgumentParser(\n...     prog='PROG',\n...     formatter_class=argparse.MetavarTypeHelpFormatter)\n>>> parser.add_argument('--foo', type=int)\n>>> parser.add_argument('bar', type=float)\n>>> parser.print_help()\nusage: PROG [-h] [--foo int] float\n\npositional arguments:\n  float\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --foo int\n prefix_chars Most command-line options will use - as the prefix, e.g. -f/--foo. Parsers that need to support different or additional prefix characters, e.g. for options like +f or /foo, may specify them using the prefix_chars= argument to the ArgumentParser constructor: >>> parser = argparse.ArgumentParser(prog='PROG', prefix_chars='-+')\n>>> parser.add_argument('+f')\n>>> parser.add_argument('++bar')\n>>> parser.parse_args('+f X ++bar Y'.split())\nNamespace(bar='Y', f='X')\n The prefix_chars= argument defaults to '-'. Supplying a set of characters that does not include - will cause -f/--foo options to be disallowed. fromfile_prefix_chars Sometimes, for example when dealing with a particularly long argument lists, it may make sense to keep the list of arguments in a file rather than typing it out at the command line. If the fromfile_prefix_chars= argument is given to the ArgumentParser constructor, then arguments that start with any of the specified characters will be treated as files, and will be replaced by the arguments they contain. For example: >>> with open('args.txt', 'w') as fp:\n...     fp.write('-f\\nbar')\n>>> parser = argparse.ArgumentParser(fromfile_prefix_chars='@')\n>>> parser.add_argument('-f')\n>>> parser.parse_args(['-f', 'foo', '@args.txt'])\nNamespace(f='bar')\n Arguments read from a file must by default be one per line (but see also convert_arg_line_to_args()) and are treated as if they were in the same place as the original file referencing argument on the command line. So in the example above, the expression ['-f', 'foo', '@args.txt'] is considered equivalent to the expression ['-f', 'foo', '-f', 'bar']. The fromfile_prefix_chars= argument defaults to None, meaning that arguments will never be treated as file references. argument_default Generally, argument defaults are specified either by passing a default to add_argument() or by calling the set_defaults() methods with a specific set of name-value pairs. Sometimes however, it may be useful to specify a single parser-wide default for arguments. This can be accomplished by passing the argument_default= keyword argument to ArgumentParser. For example, to globally suppress attribute creation on parse_args() calls, we supply argument_default=SUPPRESS: >>> parser = argparse.ArgumentParser(argument_default=argparse.SUPPRESS)\n>>> parser.add_argument('--foo')\n>>> parser.add_argument('bar', nargs='?')\n>>> parser.parse_args(['--foo', '1', 'BAR'])\nNamespace(bar='BAR', foo='1')\n>>> parser.parse_args([])\nNamespace()\n allow_abbrev Normally, when you pass an argument list to the parse_args() method of an ArgumentParser, it recognizes abbreviations of long options. This feature can be disabled by setting allow_abbrev to False: >>> parser = argparse.ArgumentParser(prog='PROG', allow_abbrev=False)\n>>> parser.add_argument('--foobar', action='store_true')\n>>> parser.add_argument('--foonley', action='store_false')\n>>> parser.parse_args(['--foon'])\nusage: PROG [-h] [--foobar] [--foonley]\nPROG: error: unrecognized arguments: --foon\n  New in version 3.5.  conflict_handler ArgumentParser objects do not allow two actions with the same option string. By default, ArgumentParser objects raise an exception if an attempt is made to create an argument with an option string that is already in use: >>> parser = argparse.ArgumentParser(prog='PROG')\n>>> parser.add_argument('-f', '--foo', help='old foo help')\n>>> parser.add_argument('--foo', help='new foo help')\nTraceback (most recent call last):\n ..\nArgumentError: argument --foo: conflicting option string(s): --foo\n Sometimes (e.g. when using parents) it may be useful to simply override any older arguments with the same option string. To get this behavior, the value 'resolve' can be supplied to the conflict_handler= argument of ArgumentParser: >>> parser = argparse.ArgumentParser(prog='PROG', conflict_handler='resolve')\n>>> parser.add_argument('-f', '--foo', help='old foo help')\n>>> parser.add_argument('--foo', help='new foo help')\n>>> parser.print_help()\nusage: PROG [-h] [-f FOO] [--foo FOO]\n\noptional arguments:\n -h, --help  show this help message and exit\n -f FOO      old foo help\n --foo FOO   new foo help\n Note that ArgumentParser objects only remove an action if all of its option strings are overridden. So, in the example above, the old -f/--foo action is retained as the -f action, because only the --foo option string was overridden. add_help By default, ArgumentParser objects add an option which simply displays the parser\u2019s help message. For example, consider a file named myprogram.py containing the following code: import argparse\nparser = argparse.ArgumentParser()\nparser.add_argument('--foo', help='foo help')\nargs = parser.parse_args()\n If -h or --help is supplied at the command line, the ArgumentParser help will be printed: $ python myprogram.py --help\nusage: myprogram.py [-h] [--foo FOO]\n\noptional arguments:\n -h, --help  show this help message and exit\n --foo FOO   foo help\n Occasionally, it may be useful to disable the addition of this help option. This can be achieved by passing False as the add_help= argument to ArgumentParser: >>> parser = argparse.ArgumentParser(prog='PROG', add_help=False)\n>>> parser.add_argument('--foo', help='foo help')\n>>> parser.print_help()\nusage: PROG [--foo FOO]\n\noptional arguments:\n --foo FOO  foo help\n The help option is typically -h/--help. The exception to this is if the prefix_chars= is specified and does not include -, in which case -h and --help are not valid options. In this case, the first character in prefix_chars is used to prefix the help options: >>> parser = argparse.ArgumentParser(prog='PROG', prefix_chars='+/')\n>>> parser.print_help()\nusage: PROG [+h]\n\noptional arguments:\n  +h, ++help  show this help message and exit\n exit_on_error Normally, when you pass an invalid argument list to the parse_args() method of an ArgumentParser, it will exit with error info. If the user would like to catch errors manually, the feature can be enabled by setting exit_on_error to False: >>> parser = argparse.ArgumentParser(exit_on_error=False)\n>>> parser.add_argument('--integers', type=int)\n_StoreAction(option_strings=['--integers'], dest='integers', nargs=None, const=None, default=None, type=<class 'int'>, choices=None, help=None, metavar=None)\n>>> try:\n...     parser.parse_args('--integers a'.split())\n... except argparse.ArgumentError:\n...     print('Catching an argumentError')\n...\nCatching an argumentError\n  New in version 3.9.  The add_argument() method  \nArgumentParser.add_argument(name or flags...[, action][, nargs][, const][, default][, type][, choices][, required][, help][, metavar][, dest])  \nDefine how a single command-line argument should be parsed. Each parameter has its own more detailed description below, but in short they are:  \nname or flags - Either a name or a list of option strings, e.g. foo or -f, --foo. \naction - The basic type of action to be taken when this argument is encountered at the command line. \nnargs - The number of command-line arguments that should be consumed. \nconst - A constant value required by some action and nargs selections. \ndefault - The value produced if the argument is absent from the command line and if it is absent from the namespace object. \ntype - The type to which the command-line argument should be converted. \nchoices - A container of the allowable values for the argument. \nrequired - Whether or not the command-line option may be omitted (optionals only). \nhelp - A brief description of what the argument does. \nmetavar - A name for the argument in usage messages. \ndest - The name of the attribute to be added to the object returned by parse_args().  \n The following sections describe how each of these are used. name or flags The add_argument() method must know whether an optional argument, like -f or --foo, or a positional argument, like a list of filenames, is expected. The first arguments passed to add_argument() must therefore be either a series of flags, or a simple argument name. For example, an optional argument could be created like: >>> parser.add_argument('-f', '--foo')\n while a positional argument could be created like: >>> parser.add_argument('bar')\n When parse_args() is called, optional arguments will be identified by the - prefix, and the remaining arguments will be assumed to be positional: >>> parser = argparse.ArgumentParser(prog='PROG')\n>>> parser.add_argument('-f', '--foo')\n>>> parser.add_argument('bar')\n>>> parser.parse_args(['BAR'])\nNamespace(bar='BAR', foo=None)\n>>> parser.parse_args(['BAR', '--foo', 'FOO'])\nNamespace(bar='BAR', foo='FOO')\n>>> parser.parse_args(['--foo', 'FOO'])\nusage: PROG [-h] [-f FOO] bar\nPROG: error: the following arguments are required: bar\n action ArgumentParser objects associate command-line arguments with actions. These actions can do just about anything with the command-line arguments associated with them, though most actions simply add an attribute to the object returned by parse_args(). The action keyword argument specifies how the command-line arguments should be handled. The supplied actions are:  \n'store' - This just stores the argument\u2019s value. This is the default action. For example: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--foo')\n>>> parser.parse_args('--foo 1'.split())\nNamespace(foo='1')\n  \n'store_const' - This stores the value specified by the const keyword argument. The 'store_const' action is most commonly used with optional arguments that specify some sort of flag. For example: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--foo', action='store_const', const=42)\n>>> parser.parse_args(['--foo'])\nNamespace(foo=42)\n  \n'store_true' and 'store_false' - These are special cases of 'store_const' used for storing the values True and False respectively. In addition, they create default values of False and True respectively. For example: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--foo', action='store_true')\n>>> parser.add_argument('--bar', action='store_false')\n>>> parser.add_argument('--baz', action='store_false')\n>>> parser.parse_args('--foo --bar'.split())\nNamespace(foo=True, bar=False, baz=True)\n  \n'append' - This stores a list, and appends each argument value to the list. This is useful to allow an option to be specified multiple times. Example usage: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--foo', action='append')\n>>> parser.parse_args('--foo 1 --foo 2'.split())\nNamespace(foo=['1', '2'])\n  \n'append_const' - This stores a list, and appends the value specified by the const keyword argument to the list. (Note that the const keyword argument defaults to None.) The 'append_const' action is typically useful when multiple arguments need to store constants to the same list. For example: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--str', dest='types', action='append_const', const=str)\n>>> parser.add_argument('--int', dest='types', action='append_const', const=int)\n>>> parser.parse_args('--str --int'.split())\nNamespace(types=[<class 'str'>, <class 'int'>])\n  \n'count' - This counts the number of times a keyword argument occurs. For example, this is useful for increasing verbosity levels: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--verbose', '-v', action='count', default=0)\n>>> parser.parse_args(['-vvv'])\nNamespace(verbose=3)\n Note, the default will be None unless explicitly set to 0.  \n'help' - This prints a complete help message for all the options in the current parser and then exits. By default a help action is automatically added to the parser. See ArgumentParser for details of how the output is created. \n'version' - This expects a version= keyword argument in the add_argument() call, and prints version information and exits when invoked: >>> import argparse\n>>> parser = argparse.ArgumentParser(prog='PROG')\n>>> parser.add_argument('--version', action='version', version='%(prog)s 2.0')\n>>> parser.parse_args(['--version'])\nPROG 2.0\n  \n'extend' - This stores a list, and extends each argument value to the list. Example usage: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument(\"--foo\", action=\"extend\", nargs=\"+\", type=str)\n>>> parser.parse_args([\"--foo\", \"f1\", \"--foo\", \"f2\", \"f3\", \"f4\"])\nNamespace(foo=['f1', 'f2', 'f3', 'f4'])\n  New in version 3.8.    You may also specify an arbitrary action by passing an Action subclass or other object that implements the same interface. The BooleanOptionalAction is available in argparse and adds support for boolean actions such as --foo and --no-foo: >>> import argparse\n>>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--foo', action=argparse.BooleanOptionalAction)\n>>> parser.parse_args(['--no-foo'])\nNamespace(foo=False)\n The recommended way to create a custom action is to extend Action, overriding the __call__ method and optionally the __init__ and format_usage methods. An example of a custom action: >>> class FooAction(argparse.Action):\n...     def __init__(self, option_strings, dest, nargs=None, **kwargs):\n...         if nargs is not None:\n...             raise ValueError(\"nargs not allowed\")\n...         super().__init__(option_strings, dest, **kwargs)\n...     def __call__(self, parser, namespace, values, option_string=None):\n...         print('%r %r %r' % (namespace, values, option_string))\n...         setattr(namespace, self.dest, values)\n...\n>>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--foo', action=FooAction)\n>>> parser.add_argument('bar', action=FooAction)\n>>> args = parser.parse_args('1 --foo 2'.split())\nNamespace(bar=None, foo=None) '1' None\nNamespace(bar='1', foo=None) '2' '--foo'\n>>> args\nNamespace(bar='1', foo='2')\n For more details, see Action. nargs ArgumentParser objects usually associate a single command-line argument with a single action to be taken. The nargs keyword argument associates a different number of command-line arguments with a single action. The supported values are:  \nN (an integer). N arguments from the command line will be gathered together into a list. For example: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--foo', nargs=2)\n>>> parser.add_argument('bar', nargs=1)\n>>> parser.parse_args('c --foo a b'.split())\nNamespace(bar=['c'], foo=['a', 'b'])\n Note that nargs=1 produces a list of one item. This is different from the default, in which the item is produced by itself.    \n'?'. One argument will be consumed from the command line if possible, and produced as a single item. If no command-line argument is present, the value from default will be produced. Note that for optional arguments, there is an additional case - the option string is present but not followed by a command-line argument. In this case the value from const will be produced. Some examples to illustrate this: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--foo', nargs='?', const='c', default='d')\n>>> parser.add_argument('bar', nargs='?', default='d')\n>>> parser.parse_args(['XX', '--foo', 'YY'])\nNamespace(bar='XX', foo='YY')\n>>> parser.parse_args(['XX', '--foo'])\nNamespace(bar='XX', foo='c')\n>>> parser.parse_args([])\nNamespace(bar='d', foo='d')\n One of the more common uses of nargs='?' is to allow optional input and output files: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('infile', nargs='?', type=argparse.FileType('r'),\n...                     default=sys.stdin)\n>>> parser.add_argument('outfile', nargs='?', type=argparse.FileType('w'),\n...                     default=sys.stdout)\n>>> parser.parse_args(['input.txt', 'output.txt'])\nNamespace(infile=<_io.TextIOWrapper name='input.txt' encoding='UTF-8'>,\n          outfile=<_io.TextIOWrapper name='output.txt' encoding='UTF-8'>)\n>>> parser.parse_args([])\nNamespace(infile=<_io.TextIOWrapper name='<stdin>' encoding='UTF-8'>,\n          outfile=<_io.TextIOWrapper name='<stdout>' encoding='UTF-8'>)\n    \n'*'. All command-line arguments present are gathered into a list. Note that it generally doesn\u2019t make much sense to have more than one positional argument with nargs='*', but multiple optional arguments with nargs='*' is possible. For example: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--foo', nargs='*')\n>>> parser.add_argument('--bar', nargs='*')\n>>> parser.add_argument('baz', nargs='*')\n>>> parser.parse_args('a b --foo x y --bar 1 2'.split())\nNamespace(bar=['1', '2'], baz=['a', 'b'], foo=['x', 'y'])\n    \n'+'. Just like '*', all command-line args present are gathered into a list. Additionally, an error message will be generated if there wasn\u2019t at least one command-line argument present. For example: >>> parser = argparse.ArgumentParser(prog='PROG')\n>>> parser.add_argument('foo', nargs='+')\n>>> parser.parse_args(['a', 'b'])\nNamespace(foo=['a', 'b'])\n>>> parser.parse_args([])\nusage: PROG [-h] foo [foo ...]\nPROG: error: the following arguments are required: foo\n   If the nargs keyword argument is not provided, the number of arguments consumed is determined by the action. Generally this means a single command-line argument will be consumed and a single item (not a list) will be produced. const The const argument of add_argument() is used to hold constant values that are not read from the command line but are required for the various ArgumentParser actions. The two most common uses of it are:  When add_argument() is called with action='store_const' or action='append_const'. These actions add the const value to one of the attributes of the object returned by parse_args(). See the action description for examples. When add_argument() is called with option strings (like -f or --foo) and nargs='?'. This creates an optional argument that can be followed by zero or one command-line arguments. When parsing the command line, if the option string is encountered with no command-line argument following it, the value of const will be assumed instead. See the nargs description for examples.  With the 'store_const' and 'append_const' actions, the const keyword argument must be given. For other actions, it defaults to None. default All optional arguments and some positional arguments may be omitted at the command line. The default keyword argument of add_argument(), whose value defaults to None, specifies what value should be used if the command-line argument is not present. For optional arguments, the default value is used when the option string was not present at the command line: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--foo', default=42)\n>>> parser.parse_args(['--foo', '2'])\nNamespace(foo='2')\n>>> parser.parse_args([])\nNamespace(foo=42)\n If the target namespace already has an attribute set, the action default will not over write it: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--foo', default=42)\n>>> parser.parse_args([], namespace=argparse.Namespace(foo=101))\nNamespace(foo=101)\n If the default value is a string, the parser parses the value as if it were a command-line argument. In particular, the parser applies any type conversion argument, if provided, before setting the attribute on the Namespace return value. Otherwise, the parser uses the value as is: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--length', default='10', type=int)\n>>> parser.add_argument('--width', default=10.5, type=int)\n>>> parser.parse_args()\nNamespace(length=10, width=10.5)\n For positional arguments with nargs equal to ? or *, the default value is used when no command-line argument was present: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('foo', nargs='?', default=42)\n>>> parser.parse_args(['a'])\nNamespace(foo='a')\n>>> parser.parse_args([])\nNamespace(foo=42)\n Providing default=argparse.SUPPRESS causes no attribute to be added if the command-line argument was not present: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--foo', default=argparse.SUPPRESS)\n>>> parser.parse_args([])\nNamespace()\n>>> parser.parse_args(['--foo', '1'])\nNamespace(foo='1')\n type By default, the parser reads command-line arguments in as simple strings. However, quite often the command-line string should instead be interpreted as another type, such as a float or int. The type keyword for add_argument() allows any necessary type-checking and type conversions to be performed. If the type keyword is used with the default keyword, the type converter is only applied if the default is a string. The argument to type can be any callable that accepts a single string. If the function raises ArgumentTypeError, TypeError, or ValueError, the exception is caught and a nicely formatted error message is displayed. No other exception types are handled. Common built-in types and functions can be used as type converters: import argparse\nimport pathlib\n\nparser = argparse.ArgumentParser()\nparser.add_argument('count', type=int)\nparser.add_argument('distance', type=float)\nparser.add_argument('street', type=ascii)\nparser.add_argument('code_point', type=ord)\nparser.add_argument('source_file', type=open)\nparser.add_argument('dest_file', type=argparse.FileType('w', encoding='latin-1'))\nparser.add_argument('datapath', type=pathlib.Path)\n User defined functions can be used as well: >>> def hyphenated(string):\n...     return '-'.join([word[:4] for word in string.casefold().split()])\n...\n>>> parser = argparse.ArgumentParser()\n>>> _ = parser.add_argument('short_title', type=hyphenated)\n>>> parser.parse_args(['\"The Tale of Two Cities\"'])\nNamespace(short_title='\"the-tale-of-two-citi')\n The bool() function is not recommended as a type converter. All it does is convert empty strings to False and non-empty strings to True. This is usually not what is desired. In general, the type keyword is a convenience that should only be used for simple conversions that can only raise one of the three supported exceptions. Anything with more interesting error-handling or resource management should be done downstream after the arguments are parsed. For example, JSON or YAML conversions have complex error cases that require better reporting than can be given by the type keyword. An JSONDecodeError would not be well formatted and a FileNotFound exception would not be handled at all. Even FileType has its limitations for use with the type keyword. If one argument uses FileType and then a subsequent argument fails, an error is reported but the file is not automatically closed. In this case, it would be better to wait until after the parser has run and then use the with-statement to manage the files. For type checkers that simply check against a fixed set of values, consider using the choices keyword instead. choices Some command-line arguments should be selected from a restricted set of values. These can be handled by passing a container object as the choices keyword argument to add_argument(). When the command line is parsed, argument values will be checked, and an error message will be displayed if the argument was not one of the acceptable values: >>> parser = argparse.ArgumentParser(prog='game.py')\n>>> parser.add_argument('move', choices=['rock', 'paper', 'scissors'])\n>>> parser.parse_args(['rock'])\nNamespace(move='rock')\n>>> parser.parse_args(['fire'])\nusage: game.py [-h] {rock,paper,scissors}\ngame.py: error: argument move: invalid choice: 'fire' (choose from 'rock',\n'paper', 'scissors')\n Note that inclusion in the choices container is checked after any type conversions have been performed, so the type of the objects in the choices container should match the type specified: >>> parser = argparse.ArgumentParser(prog='doors.py')\n>>> parser.add_argument('door', type=int, choices=range(1, 4))\n>>> print(parser.parse_args(['3']))\nNamespace(door=3)\n>>> parser.parse_args(['4'])\nusage: doors.py [-h] {1,2,3}\ndoors.py: error: argument door: invalid choice: 4 (choose from 1, 2, 3)\n Any container can be passed as the choices value, so list objects, set objects, and custom containers are all supported. Use of enum.Enum is not recommended because it is difficult to control its appearance in usage, help, and error messages. Formatted choices overrides the default metavar which is normally derived from dest. This is usually what you want because the user never sees the dest parameter. If this display isn\u2019t desirable (perhaps because there are many choices), just specify an explicit metavar. required In general, the argparse module assumes that flags like -f and --bar indicate optional arguments, which can always be omitted at the command line. To make an option required, True can be specified for the required= keyword argument to add_argument(): >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--foo', required=True)\n>>> parser.parse_args(['--foo', 'BAR'])\nNamespace(foo='BAR')\n>>> parser.parse_args([])\nusage: [-h] --foo FOO\n: error: the following arguments are required: --foo\n As the example shows, if an option is marked as required, parse_args() will report an error if that option is not present at the command line.  Note Required options are generally considered bad form because users expect options to be optional, and thus they should be avoided when possible.  help The help value is a string containing a brief description of the argument. When a user requests help (usually by using -h or --help at the command line), these help descriptions will be displayed with each argument: >>> parser = argparse.ArgumentParser(prog='frobble')\n>>> parser.add_argument('--foo', action='store_true',\n...                     help='foo the bars before frobbling')\n>>> parser.add_argument('bar', nargs='+',\n...                     help='one of the bars to be frobbled')\n>>> parser.parse_args(['-h'])\nusage: frobble [-h] [--foo] bar [bar ...]\n\npositional arguments:\n bar     one of the bars to be frobbled\n\noptional arguments:\n -h, --help  show this help message and exit\n --foo   foo the bars before frobbling\n The help strings can include various format specifiers to avoid repetition of things like the program name or the argument default. The available specifiers include the program name, %(prog)s and most keyword arguments to add_argument(), e.g. %(default)s, %(type)s, etc.: >>> parser = argparse.ArgumentParser(prog='frobble')\n>>> parser.add_argument('bar', nargs='?', type=int, default=42,\n...                     help='the bar to %(prog)s (default: %(default)s)')\n>>> parser.print_help()\nusage: frobble [-h] [bar]\n\npositional arguments:\n bar     the bar to frobble (default: 42)\n\noptional arguments:\n -h, --help  show this help message and exit\n As the help string supports %-formatting, if you want a literal % to appear in the help string, you must escape it as %%. argparse supports silencing the help entry for certain options, by setting the help value to argparse.SUPPRESS: >>> parser = argparse.ArgumentParser(prog='frobble')\n>>> parser.add_argument('--foo', help=argparse.SUPPRESS)\n>>> parser.print_help()\nusage: frobble [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n metavar When ArgumentParser generates help messages, it needs some way to refer to each expected argument. By default, ArgumentParser objects use the dest value as the \u201cname\u201d of each object. By default, for positional argument actions, the dest value is used directly, and for optional argument actions, the dest value is uppercased. So, a single positional argument with dest='bar' will be referred to as bar. A single optional argument --foo that should be followed by a single command-line argument will be referred to as FOO. An example: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--foo')\n>>> parser.add_argument('bar')\n>>> parser.parse_args('X --foo Y'.split())\nNamespace(bar='X', foo='Y')\n>>> parser.print_help()\nusage:  [-h] [--foo FOO] bar\n\npositional arguments:\n bar\n\noptional arguments:\n -h, --help  show this help message and exit\n --foo FOO\n An alternative name can be specified with metavar: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--foo', metavar='YYY')\n>>> parser.add_argument('bar', metavar='XXX')\n>>> parser.parse_args('X --foo Y'.split())\nNamespace(bar='X', foo='Y')\n>>> parser.print_help()\nusage:  [-h] [--foo YYY] XXX\n\npositional arguments:\n XXX\n\noptional arguments:\n -h, --help  show this help message and exit\n --foo YYY\n Note that metavar only changes the displayed name - the name of the attribute on the parse_args() object is still determined by the dest value. Different values of nargs may cause the metavar to be used multiple times. Providing a tuple to metavar specifies a different display for each of the arguments: >>> parser = argparse.ArgumentParser(prog='PROG')\n>>> parser.add_argument('-x', nargs=2)\n>>> parser.add_argument('--foo', nargs=2, metavar=('bar', 'baz'))\n>>> parser.print_help()\nusage: PROG [-h] [-x X X] [--foo bar baz]\n\noptional arguments:\n -h, --help     show this help message and exit\n -x X X\n --foo bar baz\n dest Most ArgumentParser actions add some value as an attribute of the object returned by parse_args(). The name of this attribute is determined by the dest keyword argument of add_argument(). For positional argument actions, dest is normally supplied as the first argument to add_argument(): >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('bar')\n>>> parser.parse_args(['XXX'])\nNamespace(bar='XXX')\n For optional argument actions, the value of dest is normally inferred from the option strings. ArgumentParser generates the value of dest by taking the first long option string and stripping away the initial -- string. If no long option strings were supplied, dest will be derived from the first short option string by stripping the initial - character. Any internal - characters will be converted to _ characters to make sure the string is a valid attribute name. The examples below illustrate this behavior: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('-f', '--foo-bar', '--foo')\n>>> parser.add_argument('-x', '-y')\n>>> parser.parse_args('-f 1 -x 2'.split())\nNamespace(foo_bar='1', x='2')\n>>> parser.parse_args('--foo 1 -y 2'.split())\nNamespace(foo_bar='1', x='2')\n dest allows a custom attribute name to be provided: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--foo', dest='bar')\n>>> parser.parse_args('--foo XXX'.split())\nNamespace(bar='XXX')\n Action classes Action classes implement the Action API, a callable which returns a callable which processes arguments from the command-line. Any object which follows this API may be passed as the action parameter to add_argument().  \nclass argparse.Action(option_strings, dest, nargs=None, const=None, default=None, type=None, choices=None, required=False, help=None, metavar=None) \n Action objects are used by an ArgumentParser to represent the information needed to parse a single argument from one or more strings from the command line. The Action class must accept the two positional arguments plus any keyword arguments passed to ArgumentParser.add_argument() except for the action itself. Instances of Action (or return value of any callable to the action parameter) should have attributes \u201cdest\u201d, \u201coption_strings\u201d, \u201cdefault\u201d, \u201ctype\u201d, \u201crequired\u201d, \u201chelp\u201d, etc. defined. The easiest way to ensure these attributes are defined is to call Action.__init__. Action instances should be callable, so subclasses must override the __call__ method, which should accept four parameters:  \nparser - The ArgumentParser object which contains this action. \nnamespace - The Namespace object that will be returned by parse_args(). Most actions add an attribute to this object using setattr(). \nvalues - The associated command-line arguments, with any type conversions applied. Type conversions are specified with the type keyword argument to add_argument(). \noption_string - The option string that was used to invoke this action. The option_string argument is optional, and will be absent if the action is associated with a positional argument.  The __call__ method may perform arbitrary actions, but will typically set attributes on the namespace based on dest and values. Action subclasses can define a format_usage method that takes no argument and return a string which will be used when printing the usage of the program. If such method is not provided, a sensible default will be used. The parse_args() method  \nArgumentParser.parse_args(args=None, namespace=None)  \nConvert argument strings to objects and assign them as attributes of the namespace. Return the populated namespace. Previous calls to add_argument() determine exactly what objects are created and how they are assigned. See the documentation for add_argument() for details.  \nargs - List of strings to parse. The default is taken from sys.argv. \nnamespace - An object to take the attributes. The default is a new empty Namespace object.  \n Option value syntax The parse_args() method supports several ways of specifying the value of an option (if it takes one). In the simplest case, the option and its value are passed as two separate arguments: >>> parser = argparse.ArgumentParser(prog='PROG')\n>>> parser.add_argument('-x')\n>>> parser.add_argument('--foo')\n>>> parser.parse_args(['-x', 'X'])\nNamespace(foo=None, x='X')\n>>> parser.parse_args(['--foo', 'FOO'])\nNamespace(foo='FOO', x=None)\n For long options (options with names longer than a single character), the option and value can also be passed as a single command-line argument, using = to separate them: >>> parser.parse_args(['--foo=FOO'])\nNamespace(foo='FOO', x=None)\n For short options (options only one character long), the option and its value can be concatenated: >>> parser.parse_args(['-xX'])\nNamespace(foo=None, x='X')\n Several short options can be joined together, using only a single - prefix, as long as only the last option (or none of them) requires a value: >>> parser = argparse.ArgumentParser(prog='PROG')\n>>> parser.add_argument('-x', action='store_true')\n>>> parser.add_argument('-y', action='store_true')\n>>> parser.add_argument('-z')\n>>> parser.parse_args(['-xyzZ'])\nNamespace(x=True, y=True, z='Z')\n Invalid arguments While parsing the command line, parse_args() checks for a variety of errors, including ambiguous options, invalid types, invalid options, wrong number of positional arguments, etc. When it encounters such an error, it exits and prints the error along with a usage message: >>> parser = argparse.ArgumentParser(prog='PROG')\n>>> parser.add_argument('--foo', type=int)\n>>> parser.add_argument('bar', nargs='?')\n\n>>> # invalid type\n>>> parser.parse_args(['--foo', 'spam'])\nusage: PROG [-h] [--foo FOO] [bar]\nPROG: error: argument --foo: invalid int value: 'spam'\n\n>>> # invalid option\n>>> parser.parse_args(['--bar'])\nusage: PROG [-h] [--foo FOO] [bar]\nPROG: error: no such option: --bar\n\n>>> # wrong number of arguments\n>>> parser.parse_args(['spam', 'badger'])\nusage: PROG [-h] [--foo FOO] [bar]\nPROG: error: extra arguments found: badger\n Arguments containing -\n The parse_args() method attempts to give errors whenever the user has clearly made a mistake, but some situations are inherently ambiguous. For example, the command-line argument -1 could either be an attempt to specify an option or an attempt to provide a positional argument. The parse_args() method is cautious here: positional arguments may only begin with - if they look like negative numbers and there are no options in the parser that look like negative numbers: >>> parser = argparse.ArgumentParser(prog='PROG')\n>>> parser.add_argument('-x')\n>>> parser.add_argument('foo', nargs='?')\n\n>>> # no negative number options, so -1 is a positional argument\n>>> parser.parse_args(['-x', '-1'])\nNamespace(foo=None, x='-1')\n\n>>> # no negative number options, so -1 and -5 are positional arguments\n>>> parser.parse_args(['-x', '-1', '-5'])\nNamespace(foo='-5', x='-1')\n\n>>> parser = argparse.ArgumentParser(prog='PROG')\n>>> parser.add_argument('-1', dest='one')\n>>> parser.add_argument('foo', nargs='?')\n\n>>> # negative number options present, so -1 is an option\n>>> parser.parse_args(['-1', 'X'])\nNamespace(foo=None, one='X')\n\n>>> # negative number options present, so -2 is an option\n>>> parser.parse_args(['-2'])\nusage: PROG [-h] [-1 ONE] [foo]\nPROG: error: no such option: -2\n\n>>> # negative number options present, so both -1s are options\n>>> parser.parse_args(['-1', '-1'])\nusage: PROG [-h] [-1 ONE] [foo]\nPROG: error: argument -1: expected one argument\n If you have positional arguments that must begin with - and don\u2019t look like negative numbers, you can insert the pseudo-argument '--' which tells parse_args() that everything after that is a positional argument: >>> parser.parse_args(['--', '-f'])\nNamespace(foo='-f', one=None)\n Argument abbreviations (prefix matching) The parse_args() method by default allows long options to be abbreviated to a prefix, if the abbreviation is unambiguous (the prefix matches a unique option): >>> parser = argparse.ArgumentParser(prog='PROG')\n>>> parser.add_argument('-bacon')\n>>> parser.add_argument('-badger')\n>>> parser.parse_args('-bac MMM'.split())\nNamespace(bacon='MMM', badger=None)\n>>> parser.parse_args('-bad WOOD'.split())\nNamespace(bacon=None, badger='WOOD')\n>>> parser.parse_args('-ba BA'.split())\nusage: PROG [-h] [-bacon BACON] [-badger BADGER]\nPROG: error: ambiguous option: -ba could match -badger, -bacon\n An error is produced for arguments that could produce more than one options. This feature can be disabled by setting allow_abbrev to False. Beyond sys.argv\n Sometimes it may be useful to have an ArgumentParser parse arguments other than those of sys.argv. This can be accomplished by passing a list of strings to parse_args(). This is useful for testing at the interactive prompt: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument(\n...     'integers', metavar='int', type=int, choices=range(10),\n...     nargs='+', help='an integer in the range 0..9')\n>>> parser.add_argument(\n...     '--sum', dest='accumulate', action='store_const', const=sum,\n...     default=max, help='sum the integers (default: find the max)')\n>>> parser.parse_args(['1', '2', '3', '4'])\nNamespace(accumulate=<built-in function max>, integers=[1, 2, 3, 4])\n>>> parser.parse_args(['1', '2', '3', '4', '--sum'])\nNamespace(accumulate=<built-in function sum>, integers=[1, 2, 3, 4])\n The Namespace object  \nclass argparse.Namespace  \nSimple class used by default by parse_args() to create an object holding attributes and return it. \n This class is deliberately simple, just an object subclass with a readable string representation. If you prefer to have dict-like view of the attributes, you can use the standard Python idiom, vars(): >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--foo')\n>>> args = parser.parse_args(['--foo', 'BAR'])\n>>> vars(args)\n{'foo': 'BAR'}\n It may also be useful to have an ArgumentParser assign attributes to an already existing object, rather than a new Namespace object. This can be achieved by specifying the namespace= keyword argument: >>> class C:\n...     pass\n...\n>>> c = C()\n>>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--foo')\n>>> parser.parse_args(args=['--foo', 'BAR'], namespace=c)\n>>> c.foo\n'BAR'\n Other utilities Sub-commands  \nArgumentParser.add_subparsers([title][, description][, prog][, parser_class][, action][, option_string][, dest][, required][, help][, metavar])  \nMany programs split up their functionality into a number of sub-commands, for example, the svn program can invoke sub-commands like svn\ncheckout, svn update, and svn commit. Splitting up functionality this way can be a particularly good idea when a program performs several different functions which require different kinds of command-line arguments. ArgumentParser supports the creation of such sub-commands with the add_subparsers() method. The add_subparsers() method is normally called with no arguments and returns a special action object. This object has a single method, add_parser(), which takes a command name and any ArgumentParser constructor arguments, and returns an ArgumentParser object that can be modified as usual. Description of parameters:  title - title for the sub-parser group in help output; by default \u201csubcommands\u201d if description is provided, otherwise uses title for positional arguments description - description for the sub-parser group in help output, by default None\n prog - usage information that will be displayed with sub-command help, by default the name of the program and any positional arguments before the subparser argument parser_class - class which will be used to create sub-parser instances, by default the class of the current parser (e.g. ArgumentParser) \naction - the basic type of action to be taken when this argument is encountered at the command line \ndest - name of the attribute under which sub-command name will be stored; by default None and no value is stored \nrequired - Whether or not a subcommand must be provided, by default False (added in 3.7) \nhelp - help for sub-parser group in help output, by default None\n \nmetavar - string presenting available sub-commands in help; by default it is None and presents sub-commands in form {cmd1, cmd2, ..}  Some example usage: >>> # create the top-level parser\n>>> parser = argparse.ArgumentParser(prog='PROG')\n>>> parser.add_argument('--foo', action='store_true', help='foo help')\n>>> subparsers = parser.add_subparsers(help='sub-command help')\n>>>\n>>> # create the parser for the \"a\" command\n>>> parser_a = subparsers.add_parser('a', help='a help')\n>>> parser_a.add_argument('bar', type=int, help='bar help')\n>>>\n>>> # create the parser for the \"b\" command\n>>> parser_b = subparsers.add_parser('b', help='b help')\n>>> parser_b.add_argument('--baz', choices='XYZ', help='baz help')\n>>>\n>>> # parse some argument lists\n>>> parser.parse_args(['a', '12'])\nNamespace(bar=12, foo=False)\n>>> parser.parse_args(['--foo', 'b', '--baz', 'Z'])\nNamespace(baz='Z', foo=True)\n Note that the object returned by parse_args() will only contain attributes for the main parser and the subparser that was selected by the command line (and not any other subparsers). So in the example above, when the a command is specified, only the foo and bar attributes are present, and when the b command is specified, only the foo and baz attributes are present. Similarly, when a help message is requested from a subparser, only the help for that particular parser will be printed. The help message will not include parent parser or sibling parser messages. (A help message for each subparser command, however, can be given by supplying the help= argument to add_parser() as above.) >>> parser.parse_args(['--help'])\nusage: PROG [-h] [--foo] {a,b} ...\n\npositional arguments:\n  {a,b}   sub-command help\n    a     a help\n    b     b help\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --foo   foo help\n\n>>> parser.parse_args(['a', '--help'])\nusage: PROG a [-h] bar\n\npositional arguments:\n  bar     bar help\n\noptional arguments:\n  -h, --help  show this help message and exit\n\n>>> parser.parse_args(['b', '--help'])\nusage: PROG b [-h] [--baz {X,Y,Z}]\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --baz {X,Y,Z}  baz help\n The add_subparsers() method also supports title and description keyword arguments. When either is present, the subparser\u2019s commands will appear in their own group in the help output. For example: >>> parser = argparse.ArgumentParser()\n>>> subparsers = parser.add_subparsers(title='subcommands',\n...                                    description='valid subcommands',\n...                                    help='additional help')\n>>> subparsers.add_parser('foo')\n>>> subparsers.add_parser('bar')\n>>> parser.parse_args(['-h'])\nusage:  [-h] {foo,bar} ...\n\noptional arguments:\n  -h, --help  show this help message and exit\n\nsubcommands:\n  valid subcommands\n\n  {foo,bar}   additional help\n Furthermore, add_parser supports an additional aliases argument, which allows multiple strings to refer to the same subparser. This example, like svn, aliases co as a shorthand for checkout: >>> parser = argparse.ArgumentParser()\n>>> subparsers = parser.add_subparsers()\n>>> checkout = subparsers.add_parser('checkout', aliases=['co'])\n>>> checkout.add_argument('foo')\n>>> parser.parse_args(['co', 'bar'])\nNamespace(foo='bar')\n One particularly effective way of handling sub-commands is to combine the use of the add_subparsers() method with calls to set_defaults() so that each subparser knows which Python function it should execute. For example: >>> # sub-command functions\n>>> def foo(args):\n...     print(args.x * args.y)\n...\n>>> def bar(args):\n...     print('((%s))' % args.z)\n...\n>>> # create the top-level parser\n>>> parser = argparse.ArgumentParser()\n>>> subparsers = parser.add_subparsers()\n>>>\n>>> # create the parser for the \"foo\" command\n>>> parser_foo = subparsers.add_parser('foo')\n>>> parser_foo.add_argument('-x', type=int, default=1)\n>>> parser_foo.add_argument('y', type=float)\n>>> parser_foo.set_defaults(func=foo)\n>>>\n>>> # create the parser for the \"bar\" command\n>>> parser_bar = subparsers.add_parser('bar')\n>>> parser_bar.add_argument('z')\n>>> parser_bar.set_defaults(func=bar)\n>>>\n>>> # parse the args and call whatever function was selected\n>>> args = parser.parse_args('foo 1 -x 2'.split())\n>>> args.func(args)\n2.0\n>>>\n>>> # parse the args and call whatever function was selected\n>>> args = parser.parse_args('bar XYZYX'.split())\n>>> args.func(args)\n((XYZYX))\n This way, you can let parse_args() do the job of calling the appropriate function after argument parsing is complete. Associating functions with actions like this is typically the easiest way to handle the different actions for each of your subparsers. However, if it is necessary to check the name of the subparser that was invoked, the dest keyword argument to the add_subparsers() call will work: >>> parser = argparse.ArgumentParser()\n>>> subparsers = parser.add_subparsers(dest='subparser_name')\n>>> subparser1 = subparsers.add_parser('1')\n>>> subparser1.add_argument('-x')\n>>> subparser2 = subparsers.add_parser('2')\n>>> subparser2.add_argument('y')\n>>> parser.parse_args(['2', 'frobble'])\nNamespace(subparser_name='2', y='frobble')\n  Changed in version 3.7: New required keyword argument.  \n FileType objects  \nclass argparse.FileType(mode='r', bufsize=-1, encoding=None, errors=None)  \nThe FileType factory creates objects that can be passed to the type argument of ArgumentParser.add_argument(). Arguments that have FileType objects as their type will open command-line arguments as files with the requested modes, buffer sizes, encodings and error handling (see the open() function for more details): >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--raw', type=argparse.FileType('wb', 0))\n>>> parser.add_argument('out', type=argparse.FileType('w', encoding='UTF-8'))\n>>> parser.parse_args(['--raw', 'raw.dat', 'file.txt'])\nNamespace(out=<_io.TextIOWrapper name='file.txt' mode='w' encoding='UTF-8'>, raw=<_io.FileIO name='raw.dat' mode='wb'>)\n FileType objects understand the pseudo-argument '-' and automatically convert this into sys.stdin for readable FileType objects and sys.stdout for writable FileType objects: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('infile', type=argparse.FileType('r'))\n>>> parser.parse_args(['-'])\nNamespace(infile=<_io.TextIOWrapper name='<stdin>' encoding='UTF-8'>)\n  New in version 3.4: The encodings and errors keyword arguments.  \n Argument groups  \nArgumentParser.add_argument_group(title=None, description=None)  \nBy default, ArgumentParser groups command-line arguments into \u201cpositional arguments\u201d and \u201coptional arguments\u201d when displaying help messages. When there is a better conceptual grouping of arguments than this default one, appropriate groups can be created using the add_argument_group() method: >>> parser = argparse.ArgumentParser(prog='PROG', add_help=False)\n>>> group = parser.add_argument_group('group')\n>>> group.add_argument('--foo', help='foo help')\n>>> group.add_argument('bar', help='bar help')\n>>> parser.print_help()\nusage: PROG [--foo FOO] bar\n\ngroup:\n  bar    bar help\n  --foo FOO  foo help\n The add_argument_group() method returns an argument group object which has an add_argument() method just like a regular ArgumentParser. When an argument is added to the group, the parser treats it just like a normal argument, but displays the argument in a separate group for help messages. The add_argument_group() method accepts title and description arguments which can be used to customize this display: >>> parser = argparse.ArgumentParser(prog='PROG', add_help=False)\n>>> group1 = parser.add_argument_group('group1', 'group1 description')\n>>> group1.add_argument('foo', help='foo help')\n>>> group2 = parser.add_argument_group('group2', 'group2 description')\n>>> group2.add_argument('--bar', help='bar help')\n>>> parser.print_help()\nusage: PROG [--bar BAR] foo\n\ngroup1:\n  group1 description\n\n  foo    foo help\n\ngroup2:\n  group2 description\n\n  --bar BAR  bar help\n Note that any arguments not in your user-defined groups will end up back in the usual \u201cpositional arguments\u201d and \u201coptional arguments\u201d sections. \n Mutual exclusion  \nArgumentParser.add_mutually_exclusive_group(required=False)  \nCreate a mutually exclusive group. argparse will make sure that only one of the arguments in the mutually exclusive group was present on the command line: >>> parser = argparse.ArgumentParser(prog='PROG')\n>>> group = parser.add_mutually_exclusive_group()\n>>> group.add_argument('--foo', action='store_true')\n>>> group.add_argument('--bar', action='store_false')\n>>> parser.parse_args(['--foo'])\nNamespace(bar=True, foo=True)\n>>> parser.parse_args(['--bar'])\nNamespace(bar=False, foo=False)\n>>> parser.parse_args(['--foo', '--bar'])\nusage: PROG [-h] [--foo | --bar]\nPROG: error: argument --bar: not allowed with argument --foo\n The add_mutually_exclusive_group() method also accepts a required argument, to indicate that at least one of the mutually exclusive arguments is required: >>> parser = argparse.ArgumentParser(prog='PROG')\n>>> group = parser.add_mutually_exclusive_group(required=True)\n>>> group.add_argument('--foo', action='store_true')\n>>> group.add_argument('--bar', action='store_false')\n>>> parser.parse_args([])\nusage: PROG [-h] (--foo | --bar)\nPROG: error: one of the arguments --foo --bar is required\n Note that currently mutually exclusive argument groups do not support the title and description arguments of add_argument_group(). \n Parser defaults  \nArgumentParser.set_defaults(**kwargs)  \nMost of the time, the attributes of the object returned by parse_args() will be fully determined by inspecting the command-line arguments and the argument actions. set_defaults() allows some additional attributes that are determined without any inspection of the command line to be added: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('foo', type=int)\n>>> parser.set_defaults(bar=42, baz='badger')\n>>> parser.parse_args(['736'])\nNamespace(bar=42, baz='badger', foo=736)\n Note that parser-level defaults always override argument-level defaults: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--foo', default='bar')\n>>> parser.set_defaults(foo='spam')\n>>> parser.parse_args([])\nNamespace(foo='spam')\n Parser-level defaults can be particularly useful when working with multiple parsers. See the add_subparsers() method for an example of this type. \n  \nArgumentParser.get_default(dest)  \nGet the default value for a namespace attribute, as set by either add_argument() or by set_defaults(): >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--foo', default='badger')\n>>> parser.get_default('foo')\n'badger'\n \n Printing help In most typical applications, parse_args() will take care of formatting and printing any usage or error messages. However, several formatting methods are available:  \nArgumentParser.print_usage(file=None)  \nPrint a brief description of how the ArgumentParser should be invoked on the command line. If file is None, sys.stdout is assumed. \n  \nArgumentParser.print_help(file=None)  \nPrint a help message, including the program usage and information about the arguments registered with the ArgumentParser. If file is None, sys.stdout is assumed. \n There are also variants of these methods that simply return a string instead of printing it:  \nArgumentParser.format_usage()  \nReturn a string containing a brief description of how the ArgumentParser should be invoked on the command line. \n  \nArgumentParser.format_help()  \nReturn a string containing a help message, including the program usage and information about the arguments registered with the ArgumentParser. \n Partial parsing  \nArgumentParser.parse_known_args(args=None, namespace=None) \n Sometimes a script may only parse a few of the command-line arguments, passing the remaining arguments on to another script or program. In these cases, the parse_known_args() method can be useful. It works much like parse_args() except that it does not produce an error when extra arguments are present. Instead, it returns a two item tuple containing the populated namespace and the list of remaining argument strings. >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--foo', action='store_true')\n>>> parser.add_argument('bar')\n>>> parser.parse_known_args(['--foo', '--badger', 'BAR', 'spam'])\n(Namespace(bar='BAR', foo=True), ['--badger', 'spam'])\n  Warning Prefix matching rules apply to parse_known_args(). The parser may consume an option even if it\u2019s just a prefix of one of its known options, instead of leaving it in the remaining arguments list.  Customizing file parsing  \nArgumentParser.convert_arg_line_to_args(arg_line)  \nArguments that are read from a file (see the fromfile_prefix_chars keyword argument to the ArgumentParser constructor) are read one argument per line. convert_arg_line_to_args() can be overridden for fancier reading. This method takes a single argument arg_line which is a string read from the argument file. It returns a list of arguments parsed from this string. The method is called once per line read from the argument file, in order. A useful override of this method is one that treats each space-separated word as an argument. The following example demonstrates how to do this: class MyArgumentParser(argparse.ArgumentParser):\n    def convert_arg_line_to_args(self, arg_line):\n        return arg_line.split()\n \n Exiting methods  \nArgumentParser.exit(status=0, message=None)  \nThis method terminates the program, exiting with the specified status and, if given, it prints a message before that. The user can override this method to handle these steps differently: class ErrorCatchingArgumentParser(argparse.ArgumentParser):\n    def exit(self, status=0, message=None):\n        if status:\n            raise Exception(f'Exiting because of an error: {message}')\n        exit(status)\n \n  \nArgumentParser.error(message)  \nThis method prints a usage message including the message to the standard error and terminates the program with a status code of 2. \n Intermixed parsing  \nArgumentParser.parse_intermixed_args(args=None, namespace=None) \n  \nArgumentParser.parse_known_intermixed_args(args=None, namespace=None) \n A number of Unix commands allow the user to intermix optional arguments with positional arguments. The parse_intermixed_args() and parse_known_intermixed_args() methods support this parsing style. These parsers do not support all the argparse features, and will raise exceptions if unsupported features are used. In particular, subparsers, argparse.REMAINDER, and mutually exclusive groups that include both optionals and positionals are not supported. The following example shows the difference between parse_known_args() and parse_intermixed_args(): the former returns ['2',\n'3'] as unparsed arguments, while the latter collects all the positionals into rest. >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--foo')\n>>> parser.add_argument('cmd')\n>>> parser.add_argument('rest', nargs='*', type=int)\n>>> parser.parse_known_args('doit 1 --foo bar 2 3'.split())\n(Namespace(cmd='doit', foo='bar', rest=[1]), ['2', '3'])\n>>> parser.parse_intermixed_args('doit 1 --foo bar 2 3'.split())\nNamespace(cmd='doit', foo='bar', rest=[1, 2, 3])\n parse_known_intermixed_args() returns a two item tuple containing the populated namespace and the list of remaining argument strings. parse_intermixed_args() raises an error if there are any remaining unparsed argument strings.  New in version 3.7.  Upgrading optparse code Originally, the argparse module had attempted to maintain compatibility with optparse. However, optparse was difficult to extend transparently, particularly with the changes required to support the new nargs= specifiers and better usage messages. When most everything in optparse had either been copy-pasted over or monkey-patched, it no longer seemed practical to try to maintain the backwards compatibility. The argparse module improves on the standard library optparse module in a number of ways including:  Handling positional arguments. Supporting sub-commands. Allowing alternative option prefixes like + and /. Handling zero-or-more and one-or-more style arguments. Producing more informative usage messages. Providing a much simpler interface for custom type and action.  A partial upgrade path from optparse to argparse:  Replace all optparse.OptionParser.add_option() calls with ArgumentParser.add_argument() calls. Replace (options, args) = parser.parse_args() with args =\nparser.parse_args() and add additional ArgumentParser.add_argument() calls for the positional arguments. Keep in mind that what was previously called options, now in the argparse context is called args. Replace optparse.OptionParser.disable_interspersed_args() by using parse_intermixed_args() instead of parse_args(). Replace callback actions and the callback_* keyword arguments with type or action arguments. Replace string names for type keyword arguments with the corresponding type objects (e.g. int, float, complex, etc). Replace optparse.Values with Namespace and optparse.OptionError and optparse.OptionValueError with ArgumentError. Replace strings with implicit arguments such as %default or %prog with the standard Python syntax to use dictionaries to format strings, that is, %(default)s and %(prog)s. Replace the OptionParser constructor version argument with a call to parser.add_argument('--version', action='version', version='<the version>'). \n"}, {"name": "argparse.Action", "path": "library/argparse#argparse.Action", "type": "Operating System", "text": " \nclass argparse.Action(option_strings, dest, nargs=None, const=None, default=None, type=None, choices=None, required=False, help=None, metavar=None) \n"}, {"name": "argparse.ArgumentDefaultsHelpFormatter", "path": "library/argparse#argparse.ArgumentDefaultsHelpFormatter", "type": "Operating System", "text": " \nclass argparse.RawDescriptionHelpFormatter  \nclass argparse.RawTextHelpFormatter  \nclass argparse.ArgumentDefaultsHelpFormatter  \nclass argparse.MetavarTypeHelpFormatter \n"}, {"name": "argparse.ArgumentParser", "path": "library/argparse#argparse.ArgumentParser", "type": "Operating System", "text": " \nclass argparse.ArgumentParser(prog=None, usage=None, description=None, epilog=None, parents=[], formatter_class=argparse.HelpFormatter, prefix_chars='-', fromfile_prefix_chars=None, argument_default=None, conflict_handler='error', add_help=True, allow_abbrev=True, exit_on_error=True)  \nCreate a new ArgumentParser object. All parameters should be passed as keyword arguments. Each parameter has its own more detailed description below, but in short they are:  \nprog - The name of the program (default: sys.argv[0]) \nusage - The string describing the program usage (default: generated from arguments added to parser) \ndescription - Text to display before the argument help (default: none) \nepilog - Text to display after the argument help (default: none) \nparents - A list of ArgumentParser objects whose arguments should also be included \nformatter_class - A class for customizing the help output \nprefix_chars - The set of characters that prefix optional arguments (default: \u2018-\u2018) \nfromfile_prefix_chars - The set of characters that prefix files from which additional arguments should be read (default: None) \nargument_default - The global default value for arguments (default: None) \nconflict_handler - The strategy for resolving conflicting optionals (usually unnecessary) \nadd_help - Add a -h/--help option to the parser (default: True) \nallow_abbrev - Allows long options to be abbreviated if the abbreviation is unambiguous. (default: True) \nexit_on_error - Determines whether or not ArgumentParser exits with error info when an error occurs. (default: True)   Changed in version 3.5: allow_abbrev parameter was added.   Changed in version 3.8: In previous versions, allow_abbrev also disabled grouping of short flags such as -vv to mean -v -v.   Changed in version 3.9: exit_on_error parameter was added.  \n"}, {"name": "argparse.ArgumentParser.add_argument()", "path": "library/argparse#argparse.ArgumentParser.add_argument", "type": "Operating System", "text": " \nArgumentParser.add_argument(name or flags...[, action][, nargs][, const][, default][, type][, choices][, required][, help][, metavar][, dest])  \nDefine how a single command-line argument should be parsed. Each parameter has its own more detailed description below, but in short they are:  \nname or flags - Either a name or a list of option strings, e.g. foo or -f, --foo. \naction - The basic type of action to be taken when this argument is encountered at the command line. \nnargs - The number of command-line arguments that should be consumed. \nconst - A constant value required by some action and nargs selections. \ndefault - The value produced if the argument is absent from the command line and if it is absent from the namespace object. \ntype - The type to which the command-line argument should be converted. \nchoices - A container of the allowable values for the argument. \nrequired - Whether or not the command-line option may be omitted (optionals only). \nhelp - A brief description of what the argument does. \nmetavar - A name for the argument in usage messages. \ndest - The name of the attribute to be added to the object returned by parse_args().  \n"}, {"name": "argparse.ArgumentParser.add_argument_group()", "path": "library/argparse#argparse.ArgumentParser.add_argument_group", "type": "Operating System", "text": " \nArgumentParser.add_argument_group(title=None, description=None)  \nBy default, ArgumentParser groups command-line arguments into \u201cpositional arguments\u201d and \u201coptional arguments\u201d when displaying help messages. When there is a better conceptual grouping of arguments than this default one, appropriate groups can be created using the add_argument_group() method: >>> parser = argparse.ArgumentParser(prog='PROG', add_help=False)\n>>> group = parser.add_argument_group('group')\n>>> group.add_argument('--foo', help='foo help')\n>>> group.add_argument('bar', help='bar help')\n>>> parser.print_help()\nusage: PROG [--foo FOO] bar\n\ngroup:\n  bar    bar help\n  --foo FOO  foo help\n The add_argument_group() method returns an argument group object which has an add_argument() method just like a regular ArgumentParser. When an argument is added to the group, the parser treats it just like a normal argument, but displays the argument in a separate group for help messages. The add_argument_group() method accepts title and description arguments which can be used to customize this display: >>> parser = argparse.ArgumentParser(prog='PROG', add_help=False)\n>>> group1 = parser.add_argument_group('group1', 'group1 description')\n>>> group1.add_argument('foo', help='foo help')\n>>> group2 = parser.add_argument_group('group2', 'group2 description')\n>>> group2.add_argument('--bar', help='bar help')\n>>> parser.print_help()\nusage: PROG [--bar BAR] foo\n\ngroup1:\n  group1 description\n\n  foo    foo help\n\ngroup2:\n  group2 description\n\n  --bar BAR  bar help\n Note that any arguments not in your user-defined groups will end up back in the usual \u201cpositional arguments\u201d and \u201coptional arguments\u201d sections. \n"}, {"name": "argparse.ArgumentParser.add_mutually_exclusive_group()", "path": "library/argparse#argparse.ArgumentParser.add_mutually_exclusive_group", "type": "Operating System", "text": " \nArgumentParser.add_mutually_exclusive_group(required=False)  \nCreate a mutually exclusive group. argparse will make sure that only one of the arguments in the mutually exclusive group was present on the command line: >>> parser = argparse.ArgumentParser(prog='PROG')\n>>> group = parser.add_mutually_exclusive_group()\n>>> group.add_argument('--foo', action='store_true')\n>>> group.add_argument('--bar', action='store_false')\n>>> parser.parse_args(['--foo'])\nNamespace(bar=True, foo=True)\n>>> parser.parse_args(['--bar'])\nNamespace(bar=False, foo=False)\n>>> parser.parse_args(['--foo', '--bar'])\nusage: PROG [-h] [--foo | --bar]\nPROG: error: argument --bar: not allowed with argument --foo\n The add_mutually_exclusive_group() method also accepts a required argument, to indicate that at least one of the mutually exclusive arguments is required: >>> parser = argparse.ArgumentParser(prog='PROG')\n>>> group = parser.add_mutually_exclusive_group(required=True)\n>>> group.add_argument('--foo', action='store_true')\n>>> group.add_argument('--bar', action='store_false')\n>>> parser.parse_args([])\nusage: PROG [-h] (--foo | --bar)\nPROG: error: one of the arguments --foo --bar is required\n Note that currently mutually exclusive argument groups do not support the title and description arguments of add_argument_group(). \n"}, {"name": "argparse.ArgumentParser.add_subparsers()", "path": "library/argparse#argparse.ArgumentParser.add_subparsers", "type": "Operating System", "text": " \nArgumentParser.add_subparsers([title][, description][, prog][, parser_class][, action][, option_string][, dest][, required][, help][, metavar])  \nMany programs split up their functionality into a number of sub-commands, for example, the svn program can invoke sub-commands like svn\ncheckout, svn update, and svn commit. Splitting up functionality this way can be a particularly good idea when a program performs several different functions which require different kinds of command-line arguments. ArgumentParser supports the creation of such sub-commands with the add_subparsers() method. The add_subparsers() method is normally called with no arguments and returns a special action object. This object has a single method, add_parser(), which takes a command name and any ArgumentParser constructor arguments, and returns an ArgumentParser object that can be modified as usual. Description of parameters:  title - title for the sub-parser group in help output; by default \u201csubcommands\u201d if description is provided, otherwise uses title for positional arguments description - description for the sub-parser group in help output, by default None\n prog - usage information that will be displayed with sub-command help, by default the name of the program and any positional arguments before the subparser argument parser_class - class which will be used to create sub-parser instances, by default the class of the current parser (e.g. ArgumentParser) \naction - the basic type of action to be taken when this argument is encountered at the command line \ndest - name of the attribute under which sub-command name will be stored; by default None and no value is stored \nrequired - Whether or not a subcommand must be provided, by default False (added in 3.7) \nhelp - help for sub-parser group in help output, by default None\n \nmetavar - string presenting available sub-commands in help; by default it is None and presents sub-commands in form {cmd1, cmd2, ..}  Some example usage: >>> # create the top-level parser\n>>> parser = argparse.ArgumentParser(prog='PROG')\n>>> parser.add_argument('--foo', action='store_true', help='foo help')\n>>> subparsers = parser.add_subparsers(help='sub-command help')\n>>>\n>>> # create the parser for the \"a\" command\n>>> parser_a = subparsers.add_parser('a', help='a help')\n>>> parser_a.add_argument('bar', type=int, help='bar help')\n>>>\n>>> # create the parser for the \"b\" command\n>>> parser_b = subparsers.add_parser('b', help='b help')\n>>> parser_b.add_argument('--baz', choices='XYZ', help='baz help')\n>>>\n>>> # parse some argument lists\n>>> parser.parse_args(['a', '12'])\nNamespace(bar=12, foo=False)\n>>> parser.parse_args(['--foo', 'b', '--baz', 'Z'])\nNamespace(baz='Z', foo=True)\n Note that the object returned by parse_args() will only contain attributes for the main parser and the subparser that was selected by the command line (and not any other subparsers). So in the example above, when the a command is specified, only the foo and bar attributes are present, and when the b command is specified, only the foo and baz attributes are present. Similarly, when a help message is requested from a subparser, only the help for that particular parser will be printed. The help message will not include parent parser or sibling parser messages. (A help message for each subparser command, however, can be given by supplying the help= argument to add_parser() as above.) >>> parser.parse_args(['--help'])\nusage: PROG [-h] [--foo] {a,b} ...\n\npositional arguments:\n  {a,b}   sub-command help\n    a     a help\n    b     b help\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --foo   foo help\n\n>>> parser.parse_args(['a', '--help'])\nusage: PROG a [-h] bar\n\npositional arguments:\n  bar     bar help\n\noptional arguments:\n  -h, --help  show this help message and exit\n\n>>> parser.parse_args(['b', '--help'])\nusage: PROG b [-h] [--baz {X,Y,Z}]\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --baz {X,Y,Z}  baz help\n The add_subparsers() method also supports title and description keyword arguments. When either is present, the subparser\u2019s commands will appear in their own group in the help output. For example: >>> parser = argparse.ArgumentParser()\n>>> subparsers = parser.add_subparsers(title='subcommands',\n...                                    description='valid subcommands',\n...                                    help='additional help')\n>>> subparsers.add_parser('foo')\n>>> subparsers.add_parser('bar')\n>>> parser.parse_args(['-h'])\nusage:  [-h] {foo,bar} ...\n\noptional arguments:\n  -h, --help  show this help message and exit\n\nsubcommands:\n  valid subcommands\n\n  {foo,bar}   additional help\n Furthermore, add_parser supports an additional aliases argument, which allows multiple strings to refer to the same subparser. This example, like svn, aliases co as a shorthand for checkout: >>> parser = argparse.ArgumentParser()\n>>> subparsers = parser.add_subparsers()\n>>> checkout = subparsers.add_parser('checkout', aliases=['co'])\n>>> checkout.add_argument('foo')\n>>> parser.parse_args(['co', 'bar'])\nNamespace(foo='bar')\n One particularly effective way of handling sub-commands is to combine the use of the add_subparsers() method with calls to set_defaults() so that each subparser knows which Python function it should execute. For example: >>> # sub-command functions\n>>> def foo(args):\n...     print(args.x * args.y)\n...\n>>> def bar(args):\n...     print('((%s))' % args.z)\n...\n>>> # create the top-level parser\n>>> parser = argparse.ArgumentParser()\n>>> subparsers = parser.add_subparsers()\n>>>\n>>> # create the parser for the \"foo\" command\n>>> parser_foo = subparsers.add_parser('foo')\n>>> parser_foo.add_argument('-x', type=int, default=1)\n>>> parser_foo.add_argument('y', type=float)\n>>> parser_foo.set_defaults(func=foo)\n>>>\n>>> # create the parser for the \"bar\" command\n>>> parser_bar = subparsers.add_parser('bar')\n>>> parser_bar.add_argument('z')\n>>> parser_bar.set_defaults(func=bar)\n>>>\n>>> # parse the args and call whatever function was selected\n>>> args = parser.parse_args('foo 1 -x 2'.split())\n>>> args.func(args)\n2.0\n>>>\n>>> # parse the args and call whatever function was selected\n>>> args = parser.parse_args('bar XYZYX'.split())\n>>> args.func(args)\n((XYZYX))\n This way, you can let parse_args() do the job of calling the appropriate function after argument parsing is complete. Associating functions with actions like this is typically the easiest way to handle the different actions for each of your subparsers. However, if it is necessary to check the name of the subparser that was invoked, the dest keyword argument to the add_subparsers() call will work: >>> parser = argparse.ArgumentParser()\n>>> subparsers = parser.add_subparsers(dest='subparser_name')\n>>> subparser1 = subparsers.add_parser('1')\n>>> subparser1.add_argument('-x')\n>>> subparser2 = subparsers.add_parser('2')\n>>> subparser2.add_argument('y')\n>>> parser.parse_args(['2', 'frobble'])\nNamespace(subparser_name='2', y='frobble')\n  Changed in version 3.7: New required keyword argument.  \n"}, {"name": "argparse.ArgumentParser.convert_arg_line_to_args()", "path": "library/argparse#argparse.ArgumentParser.convert_arg_line_to_args", "type": "Operating System", "text": " \nArgumentParser.convert_arg_line_to_args(arg_line)  \nArguments that are read from a file (see the fromfile_prefix_chars keyword argument to the ArgumentParser constructor) are read one argument per line. convert_arg_line_to_args() can be overridden for fancier reading. This method takes a single argument arg_line which is a string read from the argument file. It returns a list of arguments parsed from this string. The method is called once per line read from the argument file, in order. A useful override of this method is one that treats each space-separated word as an argument. The following example demonstrates how to do this: class MyArgumentParser(argparse.ArgumentParser):\n    def convert_arg_line_to_args(self, arg_line):\n        return arg_line.split()\n \n"}, {"name": "argparse.ArgumentParser.error()", "path": "library/argparse#argparse.ArgumentParser.error", "type": "Operating System", "text": " \nArgumentParser.error(message)  \nThis method prints a usage message including the message to the standard error and terminates the program with a status code of 2. \n"}, {"name": "argparse.ArgumentParser.exit()", "path": "library/argparse#argparse.ArgumentParser.exit", "type": "Operating System", "text": " \nArgumentParser.exit(status=0, message=None)  \nThis method terminates the program, exiting with the specified status and, if given, it prints a message before that. The user can override this method to handle these steps differently: class ErrorCatchingArgumentParser(argparse.ArgumentParser):\n    def exit(self, status=0, message=None):\n        if status:\n            raise Exception(f'Exiting because of an error: {message}')\n        exit(status)\n \n"}, {"name": "argparse.ArgumentParser.format_help()", "path": "library/argparse#argparse.ArgumentParser.format_help", "type": "Operating System", "text": " \nArgumentParser.format_help()  \nReturn a string containing a help message, including the program usage and information about the arguments registered with the ArgumentParser. \n"}, {"name": "argparse.ArgumentParser.format_usage()", "path": "library/argparse#argparse.ArgumentParser.format_usage", "type": "Operating System", "text": " \nArgumentParser.format_usage()  \nReturn a string containing a brief description of how the ArgumentParser should be invoked on the command line. \n"}, {"name": "argparse.ArgumentParser.get_default()", "path": "library/argparse#argparse.ArgumentParser.get_default", "type": "Operating System", "text": " \nArgumentParser.get_default(dest)  \nGet the default value for a namespace attribute, as set by either add_argument() or by set_defaults(): >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--foo', default='badger')\n>>> parser.get_default('foo')\n'badger'\n \n"}, {"name": "argparse.ArgumentParser.parse_args()", "path": "library/argparse#argparse.ArgumentParser.parse_args", "type": "Operating System", "text": " \nArgumentParser.parse_args(args=None, namespace=None)  \nConvert argument strings to objects and assign them as attributes of the namespace. Return the populated namespace. Previous calls to add_argument() determine exactly what objects are created and how they are assigned. See the documentation for add_argument() for details.  \nargs - List of strings to parse. The default is taken from sys.argv. \nnamespace - An object to take the attributes. The default is a new empty Namespace object.  \n"}, {"name": "argparse.ArgumentParser.parse_intermixed_args()", "path": "library/argparse#argparse.ArgumentParser.parse_intermixed_args", "type": "Operating System", "text": " \nArgumentParser.parse_intermixed_args(args=None, namespace=None) \n"}, {"name": "argparse.ArgumentParser.parse_known_args()", "path": "library/argparse#argparse.ArgumentParser.parse_known_args", "type": "Operating System", "text": " \nArgumentParser.parse_known_args(args=None, namespace=None) \n"}, {"name": "argparse.ArgumentParser.parse_known_intermixed_args()", "path": "library/argparse#argparse.ArgumentParser.parse_known_intermixed_args", "type": "Operating System", "text": " \nArgumentParser.parse_known_intermixed_args(args=None, namespace=None) \n"}, {"name": "argparse.ArgumentParser.print_help()", "path": "library/argparse#argparse.ArgumentParser.print_help", "type": "Operating System", "text": " \nArgumentParser.print_help(file=None)  \nPrint a help message, including the program usage and information about the arguments registered with the ArgumentParser. If file is None, sys.stdout is assumed. \n"}, {"name": "argparse.ArgumentParser.print_usage()", "path": "library/argparse#argparse.ArgumentParser.print_usage", "type": "Operating System", "text": " \nArgumentParser.print_usage(file=None)  \nPrint a brief description of how the ArgumentParser should be invoked on the command line. If file is None, sys.stdout is assumed. \n"}, {"name": "argparse.ArgumentParser.set_defaults()", "path": "library/argparse#argparse.ArgumentParser.set_defaults", "type": "Operating System", "text": " \nArgumentParser.set_defaults(**kwargs)  \nMost of the time, the attributes of the object returned by parse_args() will be fully determined by inspecting the command-line arguments and the argument actions. set_defaults() allows some additional attributes that are determined without any inspection of the command line to be added: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('foo', type=int)\n>>> parser.set_defaults(bar=42, baz='badger')\n>>> parser.parse_args(['736'])\nNamespace(bar=42, baz='badger', foo=736)\n Note that parser-level defaults always override argument-level defaults: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--foo', default='bar')\n>>> parser.set_defaults(foo='spam')\n>>> parser.parse_args([])\nNamespace(foo='spam')\n Parser-level defaults can be particularly useful when working with multiple parsers. See the add_subparsers() method for an example of this type. \n"}, {"name": "argparse.FileType", "path": "library/argparse#argparse.FileType", "type": "Operating System", "text": " \nclass argparse.FileType(mode='r', bufsize=-1, encoding=None, errors=None)  \nThe FileType factory creates objects that can be passed to the type argument of ArgumentParser.add_argument(). Arguments that have FileType objects as their type will open command-line arguments as files with the requested modes, buffer sizes, encodings and error handling (see the open() function for more details): >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('--raw', type=argparse.FileType('wb', 0))\n>>> parser.add_argument('out', type=argparse.FileType('w', encoding='UTF-8'))\n>>> parser.parse_args(['--raw', 'raw.dat', 'file.txt'])\nNamespace(out=<_io.TextIOWrapper name='file.txt' mode='w' encoding='UTF-8'>, raw=<_io.FileIO name='raw.dat' mode='wb'>)\n FileType objects understand the pseudo-argument '-' and automatically convert this into sys.stdin for readable FileType objects and sys.stdout for writable FileType objects: >>> parser = argparse.ArgumentParser()\n>>> parser.add_argument('infile', type=argparse.FileType('r'))\n>>> parser.parse_args(['-'])\nNamespace(infile=<_io.TextIOWrapper name='<stdin>' encoding='UTF-8'>)\n  New in version 3.4: The encodings and errors keyword arguments.  \n"}, {"name": "argparse.MetavarTypeHelpFormatter", "path": "library/argparse#argparse.MetavarTypeHelpFormatter", "type": "Operating System", "text": " \nclass argparse.RawDescriptionHelpFormatter  \nclass argparse.RawTextHelpFormatter  \nclass argparse.ArgumentDefaultsHelpFormatter  \nclass argparse.MetavarTypeHelpFormatter \n"}, {"name": "argparse.Namespace", "path": "library/argparse#argparse.Namespace", "type": "Operating System", "text": " \nclass argparse.Namespace  \nSimple class used by default by parse_args() to create an object holding attributes and return it. \n"}, {"name": "argparse.RawDescriptionHelpFormatter", "path": "library/argparse#argparse.RawDescriptionHelpFormatter", "type": "Operating System", "text": " \nclass argparse.RawDescriptionHelpFormatter  \nclass argparse.RawTextHelpFormatter  \nclass argparse.ArgumentDefaultsHelpFormatter  \nclass argparse.MetavarTypeHelpFormatter \n"}, {"name": "argparse.RawTextHelpFormatter", "path": "library/argparse#argparse.RawTextHelpFormatter", "type": "Operating System", "text": " \nclass argparse.RawDescriptionHelpFormatter  \nclass argparse.RawTextHelpFormatter  \nclass argparse.ArgumentDefaultsHelpFormatter  \nclass argparse.MetavarTypeHelpFormatter \n"}, {"name": "ArithmeticError", "path": "library/exceptions#ArithmeticError", "type": "Built-in Exceptions", "text": " \nexception ArithmeticError  \nThe base class for those built-in exceptions that are raised for various arithmetic errors: OverflowError, ZeroDivisionError, FloatingPointError. \n"}, {"name": "array", "path": "library/array", "type": "Data Types", "text": "array \u2014 Efficient arrays of numeric values This module defines an object type which can compactly represent an array of basic values: characters, integers, floating point numbers. Arrays are sequence types and behave very much like lists, except that the type of objects stored in them is constrained. The type is specified at object creation time by using a type code, which is a single character. The following type codes are defined:   \nType code C Type Python Type Minimum size in bytes Notes   \n'b' signed char int 1   \n'B' unsigned char int 1   \n'u' wchar_t Unicode character 2 (1)  \n'h' signed short int 2   \n'H' unsigned short int 2   \n'i' signed int int 2   \n'I' unsigned int int 2   \n'l' signed long int 4   \n'L' unsigned long int 4   \n'q' signed long long int 8   \n'Q' unsigned long long int 8   \n'f' float float 4   \n'd' double float 8    Notes:  \nIt can be 16 bits or 32 bits depending on the platform.  Changed in version 3.9: array('u') now uses wchar_t as C type instead of deprecated Py_UNICODE. This change doesn\u2019t affect to its behavior because Py_UNICODE is alias of wchar_t since Python 3.3.   Deprecated since version 3.3, will be removed in version 4.0.    The actual representation of values is determined by the machine architecture (strictly speaking, by the C implementation). The actual size can be accessed through the itemsize attribute. The module defines the following type:  \nclass array.array(typecode[, initializer])  \nA new array whose items are restricted by typecode, and initialized from the optional initializer value, which must be a list, a bytes-like object, or iterable over elements of the appropriate type. If given a list or string, the initializer is passed to the new array\u2019s fromlist(), frombytes(), or fromunicode() method (see below) to add initial items to the array. Otherwise, the iterable initializer is passed to the extend() method. Raises an auditing event array.__new__ with arguments typecode, initializer. \n  \narray.typecodes  \nA string with all available type codes. \n Array objects support the ordinary sequence operations of indexing, slicing, concatenation, and multiplication. When using slice assignment, the assigned value must be an array object with the same type code; in all other cases, TypeError is raised. Array objects also implement the buffer interface, and may be used wherever bytes-like objects are supported. The following data items and methods are also supported:  \narray.typecode  \nThe typecode character used to create the array. \n  \narray.itemsize  \nThe length in bytes of one array item in the internal representation. \n  \narray.append(x)  \nAppend a new item with value x to the end of the array. \n  \narray.buffer_info()  \nReturn a tuple (address, length) giving the current memory address and the length in elements of the buffer used to hold array\u2019s contents. The size of the memory buffer in bytes can be computed as array.buffer_info()[1] *\narray.itemsize. This is occasionally useful when working with low-level (and inherently unsafe) I/O interfaces that require memory addresses, such as certain ioctl() operations. The returned numbers are valid as long as the array exists and no length-changing operations are applied to it.  Note When using array objects from code written in C or C++ (the only way to effectively make use of this information), it makes more sense to use the buffer interface supported by array objects. This method is maintained for backward compatibility and should be avoided in new code. The buffer interface is documented in Buffer Protocol.  \n  \narray.byteswap()  \n\u201cByteswap\u201d all items of the array. This is only supported for values which are 1, 2, 4, or 8 bytes in size; for other types of values, RuntimeError is raised. It is useful when reading data from a file written on a machine with a different byte order. \n  \narray.count(x)  \nReturn the number of occurrences of x in the array. \n  \narray.extend(iterable)  \nAppend items from iterable to the end of the array. If iterable is another array, it must have exactly the same type code; if not, TypeError will be raised. If iterable is not an array, it must be iterable and its elements must be the right type to be appended to the array. \n  \narray.frombytes(s)  \nAppends items from the string, interpreting the string as an array of machine values (as if it had been read from a file using the fromfile() method).  New in version 3.2: fromstring() is renamed to frombytes() for clarity.  \n  \narray.fromfile(f, n)  \nRead n items (as machine values) from the file object f and append them to the end of the array. If less than n items are available, EOFError is raised, but the items that were available are still inserted into the array. \n  \narray.fromlist(list)  \nAppend items from the list. This is equivalent to for x in list:\na.append(x) except that if there is a type error, the array is unchanged. \n  \narray.fromunicode(s)  \nExtends this array with data from the given unicode string. The array must be a type 'u' array; otherwise a ValueError is raised. Use array.frombytes(unicodestring.encode(enc)) to append Unicode data to an array of some other type. \n  \narray.index(x)  \nReturn the smallest i such that i is the index of the first occurrence of x in the array. \n  \narray.insert(i, x)  \nInsert a new item with value x in the array before position i. Negative values are treated as being relative to the end of the array. \n  \narray.pop([i])  \nRemoves the item with the index i from the array and returns it. The optional argument defaults to -1, so that by default the last item is removed and returned. \n  \narray.remove(x)  \nRemove the first occurrence of x from the array. \n  \narray.reverse()  \nReverse the order of the items in the array. \n  \narray.tobytes()  \nConvert the array to an array of machine values and return the bytes representation (the same sequence of bytes that would be written to a file by the tofile() method.)  New in version 3.2: tostring() is renamed to tobytes() for clarity.  \n  \narray.tofile(f)  \nWrite all items (as machine values) to the file object f. \n  \narray.tolist()  \nConvert the array to an ordinary list with the same items. \n  \narray.tounicode()  \nConvert the array to a unicode string. The array must be a type 'u' array; otherwise a ValueError is raised. Use array.tobytes().decode(enc) to obtain a unicode string from an array of some other type. \n When an array object is printed or converted to a string, it is represented as array(typecode, initializer). The initializer is omitted if the array is empty, otherwise it is a string if the typecode is 'u', otherwise it is a list of numbers. The string is guaranteed to be able to be converted back to an array with the same type and value using eval(), so long as the array class has been imported using from array import array. Examples: array('l')\narray('u', 'hello \\u2641')\narray('l', [1, 2, 3, 4, 5])\narray('d', [1.0, 2.0, 3.14])\n  See also  \nModule struct\n\n\nPacking and unpacking of heterogeneous binary data.  \nModule xdrlib\n\n\nPacking and unpacking of External Data Representation (XDR) data as used in some remote procedure call systems.  The Numerical Python Documentation\n\nThe Numeric Python extension (NumPy) defines another array type; see http://www.numpy.org/ for further information about Numerical Python.   \n"}, {"name": "array.array", "path": "library/array#array.array", "type": "Data Types", "text": " \nclass array.array(typecode[, initializer])  \nA new array whose items are restricted by typecode, and initialized from the optional initializer value, which must be a list, a bytes-like object, or iterable over elements of the appropriate type. If given a list or string, the initializer is passed to the new array\u2019s fromlist(), frombytes(), or fromunicode() method (see below) to add initial items to the array. Otherwise, the iterable initializer is passed to the extend() method. Raises an auditing event array.__new__ with arguments typecode, initializer. \n"}, {"name": "array.array.append()", "path": "library/array#array.array.append", "type": "Data Types", "text": " \narray.append(x)  \nAppend a new item with value x to the end of the array. \n"}, {"name": "array.array.buffer_info()", "path": "library/array#array.array.buffer_info", "type": "Data Types", "text": " \narray.buffer_info()  \nReturn a tuple (address, length) giving the current memory address and the length in elements of the buffer used to hold array\u2019s contents. The size of the memory buffer in bytes can be computed as array.buffer_info()[1] *\narray.itemsize. This is occasionally useful when working with low-level (and inherently unsafe) I/O interfaces that require memory addresses, such as certain ioctl() operations. The returned numbers are valid as long as the array exists and no length-changing operations are applied to it.  Note When using array objects from code written in C or C++ (the only way to effectively make use of this information), it makes more sense to use the buffer interface supported by array objects. This method is maintained for backward compatibility and should be avoided in new code. The buffer interface is documented in Buffer Protocol.  \n"}, {"name": "array.array.byteswap()", "path": "library/array#array.array.byteswap", "type": "Data Types", "text": " \narray.byteswap()  \n\u201cByteswap\u201d all items of the array. This is only supported for values which are 1, 2, 4, or 8 bytes in size; for other types of values, RuntimeError is raised. It is useful when reading data from a file written on a machine with a different byte order. \n"}, {"name": "array.array.count()", "path": "library/array#array.array.count", "type": "Data Types", "text": " \narray.count(x)  \nReturn the number of occurrences of x in the array. \n"}, {"name": "array.array.extend()", "path": "library/array#array.array.extend", "type": "Data Types", "text": " \narray.extend(iterable)  \nAppend items from iterable to the end of the array. If iterable is another array, it must have exactly the same type code; if not, TypeError will be raised. If iterable is not an array, it must be iterable and its elements must be the right type to be appended to the array. \n"}, {"name": "array.array.frombytes()", "path": "library/array#array.array.frombytes", "type": "Data Types", "text": " \narray.frombytes(s)  \nAppends items from the string, interpreting the string as an array of machine values (as if it had been read from a file using the fromfile() method).  New in version 3.2: fromstring() is renamed to frombytes() for clarity.  \n"}, {"name": "array.array.fromfile()", "path": "library/array#array.array.fromfile", "type": "Data Types", "text": " \narray.fromfile(f, n)  \nRead n items (as machine values) from the file object f and append them to the end of the array. If less than n items are available, EOFError is raised, but the items that were available are still inserted into the array. \n"}, {"name": "array.array.fromlist()", "path": "library/array#array.array.fromlist", "type": "Data Types", "text": " \narray.fromlist(list)  \nAppend items from the list. This is equivalent to for x in list:\na.append(x) except that if there is a type error, the array is unchanged. \n"}, {"name": "array.array.fromunicode()", "path": "library/array#array.array.fromunicode", "type": "Data Types", "text": " \narray.fromunicode(s)  \nExtends this array with data from the given unicode string. The array must be a type 'u' array; otherwise a ValueError is raised. Use array.frombytes(unicodestring.encode(enc)) to append Unicode data to an array of some other type. \n"}, {"name": "array.array.index()", "path": "library/array#array.array.index", "type": "Data Types", "text": " \narray.index(x)  \nReturn the smallest i such that i is the index of the first occurrence of x in the array. \n"}, {"name": "array.array.insert()", "path": "library/array#array.array.insert", "type": "Data Types", "text": " \narray.insert(i, x)  \nInsert a new item with value x in the array before position i. Negative values are treated as being relative to the end of the array. \n"}, {"name": "array.array.itemsize", "path": "library/array#array.array.itemsize", "type": "Data Types", "text": " \narray.itemsize  \nThe length in bytes of one array item in the internal representation. \n"}, {"name": "array.array.pop()", "path": "library/array#array.array.pop", "type": "Data Types", "text": " \narray.pop([i])  \nRemoves the item with the index i from the array and returns it. The optional argument defaults to -1, so that by default the last item is removed and returned. \n"}, {"name": "array.array.remove()", "path": "library/array#array.array.remove", "type": "Data Types", "text": " \narray.remove(x)  \nRemove the first occurrence of x from the array. \n"}, {"name": "array.array.reverse()", "path": "library/array#array.array.reverse", "type": "Data Types", "text": " \narray.reverse()  \nReverse the order of the items in the array. \n"}, {"name": "array.array.tobytes()", "path": "library/array#array.array.tobytes", "type": "Data Types", "text": " \narray.tobytes()  \nConvert the array to an array of machine values and return the bytes representation (the same sequence of bytes that would be written to a file by the tofile() method.)  New in version 3.2: tostring() is renamed to tobytes() for clarity.  \n"}, {"name": "array.array.tofile()", "path": "library/array#array.array.tofile", "type": "Data Types", "text": " \narray.tofile(f)  \nWrite all items (as machine values) to the file object f. \n"}, {"name": "array.array.tolist()", "path": "library/array#array.array.tolist", "type": "Data Types", "text": " \narray.tolist()  \nConvert the array to an ordinary list with the same items. \n"}, {"name": "array.array.tounicode()", "path": "library/array#array.array.tounicode", "type": "Data Types", "text": " \narray.tounicode()  \nConvert the array to a unicode string. The array must be a type 'u' array; otherwise a ValueError is raised. Use array.tobytes().decode(enc) to obtain a unicode string from an array of some other type. \n"}, {"name": "array.array.typecode", "path": "library/array#array.array.typecode", "type": "Data Types", "text": " \narray.typecode  \nThe typecode character used to create the array. \n"}, {"name": "array.typecodes", "path": "library/array#array.typecodes", "type": "Data Types", "text": " \narray.typecodes  \nA string with all available type codes. \n"}, {"name": "ascii()", "path": "library/functions#ascii", "type": "Built-in Functions", "text": " \nascii(object)  \nAs repr(), return a string containing a printable representation of an object, but escape the non-ASCII characters in the string returned by repr() using \\x, \\u or \\U escapes. This generates a string similar to that returned by repr() in Python 2. \n"}, {"name": "AssertionError", "path": "library/exceptions#AssertionError", "type": "Built-in Exceptions", "text": " \nexception AssertionError  \nRaised when an assert statement fails. \n"}, {"name": "ast", "path": "library/ast", "type": "Language", "text": "ast \u2014 Abstract Syntax Trees Source code: Lib/ast.py The ast module helps Python applications to process trees of the Python abstract syntax grammar. The abstract syntax itself might change with each Python release; this module helps to find out programmatically what the current grammar looks like. An abstract syntax tree can be generated by passing ast.PyCF_ONLY_AST as a flag to the compile() built-in function, or using the parse() helper provided in this module. The result will be a tree of objects whose classes all inherit from ast.AST. An abstract syntax tree can be compiled into a Python code object using the built-in compile() function. Abstract Grammar The abstract grammar is currently defined as follows: -- ASDL's 4 builtin types are:\n-- identifier, int, string, constant\n\nmodule Python\n{\n    mod = Module(stmt* body, type_ignore* type_ignores)\n        | Interactive(stmt* body)\n        | Expression(expr body)\n        | FunctionType(expr* argtypes, expr returns)\n\n    stmt = FunctionDef(identifier name, arguments args,\n                       stmt* body, expr* decorator_list, expr? returns,\n                       string? type_comment)\n          | AsyncFunctionDef(identifier name, arguments args,\n                             stmt* body, expr* decorator_list, expr? returns,\n                             string? type_comment)\n\n          | ClassDef(identifier name,\n             expr* bases,\n             keyword* keywords,\n             stmt* body,\n             expr* decorator_list)\n          | Return(expr? value)\n\n          | Delete(expr* targets)\n          | Assign(expr* targets, expr value, string? type_comment)\n          | AugAssign(expr target, operator op, expr value)\n          -- 'simple' indicates that we annotate simple name without parens\n          | AnnAssign(expr target, expr annotation, expr? value, int simple)\n\n          -- use 'orelse' because else is a keyword in target languages\n          | For(expr target, expr iter, stmt* body, stmt* orelse, string? type_comment)\n          | AsyncFor(expr target, expr iter, stmt* body, stmt* orelse, string? type_comment)\n          | While(expr test, stmt* body, stmt* orelse)\n          | If(expr test, stmt* body, stmt* orelse)\n          | With(withitem* items, stmt* body, string? type_comment)\n          | AsyncWith(withitem* items, stmt* body, string? type_comment)\n\n          | Raise(expr? exc, expr? cause)\n          | Try(stmt* body, excepthandler* handlers, stmt* orelse, stmt* finalbody)\n          | Assert(expr test, expr? msg)\n\n          | Import(alias* names)\n          | ImportFrom(identifier? module, alias* names, int? level)\n\n          | Global(identifier* names)\n          | Nonlocal(identifier* names)\n          | Expr(expr value)\n          | Pass | Break | Continue\n\n          -- col_offset is the byte offset in the utf8 string the parser uses\n          attributes (int lineno, int col_offset, int? end_lineno, int? end_col_offset)\n\n          -- BoolOp() can use left & right?\n    expr = BoolOp(boolop op, expr* values)\n         | NamedExpr(expr target, expr value)\n         | BinOp(expr left, operator op, expr right)\n         | UnaryOp(unaryop op, expr operand)\n         | Lambda(arguments args, expr body)\n         | IfExp(expr test, expr body, expr orelse)\n         | Dict(expr* keys, expr* values)\n         | Set(expr* elts)\n         | ListComp(expr elt, comprehension* generators)\n         | SetComp(expr elt, comprehension* generators)\n         | DictComp(expr key, expr value, comprehension* generators)\n         | GeneratorExp(expr elt, comprehension* generators)\n         -- the grammar constrains where yield expressions can occur\n         | Await(expr value)\n         | Yield(expr? value)\n         | YieldFrom(expr value)\n         -- need sequences for compare to distinguish between\n         -- x < 4 < 3 and (x < 4) < 3\n         | Compare(expr left, cmpop* ops, expr* comparators)\n         | Call(expr func, expr* args, keyword* keywords)\n         | FormattedValue(expr value, int? conversion, expr? format_spec)\n         | JoinedStr(expr* values)\n         | Constant(constant value, string? kind)\n\n         -- the following expression can appear in assignment context\n         | Attribute(expr value, identifier attr, expr_context ctx)\n         | Subscript(expr value, expr slice, expr_context ctx)\n         | Starred(expr value, expr_context ctx)\n         | Name(identifier id, expr_context ctx)\n         | List(expr* elts, expr_context ctx)\n         | Tuple(expr* elts, expr_context ctx)\n\n         -- can appear only in Subscript\n         | Slice(expr? lower, expr? upper, expr? step)\n\n          -- col_offset is the byte offset in the utf8 string the parser uses\n          attributes (int lineno, int col_offset, int? end_lineno, int? end_col_offset)\n\n    expr_context = Load | Store | Del\n\n    boolop = And | Or\n\n    operator = Add | Sub | Mult | MatMult | Div | Mod | Pow | LShift\n                 | RShift | BitOr | BitXor | BitAnd | FloorDiv\n\n    unaryop = Invert | Not | UAdd | USub\n\n    cmpop = Eq | NotEq | Lt | LtE | Gt | GtE | Is | IsNot | In | NotIn\n\n    comprehension = (expr target, expr iter, expr* ifs, int is_async)\n\n    excepthandler = ExceptHandler(expr? type, identifier? name, stmt* body)\n                    attributes (int lineno, int col_offset, int? end_lineno, int? end_col_offset)\n\n    arguments = (arg* posonlyargs, arg* args, arg? vararg, arg* kwonlyargs,\n                 expr* kw_defaults, arg? kwarg, expr* defaults)\n\n    arg = (identifier arg, expr? annotation, string? type_comment)\n           attributes (int lineno, int col_offset, int? end_lineno, int? end_col_offset)\n\n    -- keyword arguments supplied to call (NULL identifier for **kwargs)\n    keyword = (identifier? arg, expr value)\n               attributes (int lineno, int col_offset, int? end_lineno, int? end_col_offset)\n\n    -- import name with optional 'as' alias.\n    alias = (identifier name, identifier? asname)\n\n    withitem = (expr context_expr, expr? optional_vars)\n\n    type_ignore = TypeIgnore(int lineno, string tag)\n}\n Node classes  \nclass ast.AST  \nThis is the base of all AST node classes. The actual node classes are derived from the Parser/Python.asdl file, which is reproduced below. They are defined in the _ast C module and re-exported in ast. There is one class defined for each left-hand side symbol in the abstract grammar (for example, ast.stmt or ast.expr). In addition, there is one class defined for each constructor on the right-hand side; these classes inherit from the classes for the left-hand side trees. For example, ast.BinOp inherits from ast.expr. For production rules with alternatives (aka \u201csums\u201d), the left-hand side class is abstract: only instances of specific constructor nodes are ever created.  \n_fields  \nEach concrete class has an attribute _fields which gives the names of all child nodes. Each instance of a concrete class has one attribute for each child node, of the type as defined in the grammar. For example, ast.BinOp instances have an attribute left of type ast.expr. If these attributes are marked as optional in the grammar (using a question mark), the value might be None. If the attributes can have zero-or-more values (marked with an asterisk), the values are represented as Python lists. All possible attributes must be present and have valid values when compiling an AST with compile(). \n  \nlineno  \ncol_offset  \nend_lineno  \nend_col_offset  \nInstances of ast.expr and ast.stmt subclasses have lineno, col_offset, lineno, and col_offset attributes. The lineno and end_lineno are the first and last line numbers of source text span (1-indexed so the first line is line 1) and the col_offset and end_col_offset are the corresponding UTF-8 byte offsets of the first and last tokens that generated the node. The UTF-8 offset is recorded because the parser uses UTF-8 internally. Note that the end positions are not required by the compiler and are therefore optional. The end offset is after the last symbol, for example one can get the source segment of a one-line expression node using source_line[node.col_offset : node.end_col_offset]. \n The constructor of a class ast.T parses its arguments as follows:  If there are positional arguments, there must be as many as there are items in T._fields; they will be assigned as attributes of these names. If there are keyword arguments, they will set the attributes of the same names to the given values.  For example, to create and populate an ast.UnaryOp node, you could use node = ast.UnaryOp()\nnode.op = ast.USub()\nnode.operand = ast.Constant()\nnode.operand.value = 5\nnode.operand.lineno = 0\nnode.operand.col_offset = 0\nnode.lineno = 0\nnode.col_offset = 0\n or the more compact node = ast.UnaryOp(ast.USub(), ast.Constant(5, lineno=0, col_offset=0),\n                   lineno=0, col_offset=0)\n \n  Changed in version 3.8: Class ast.Constant is now used for all constants.   Changed in version 3.9: Simple indices are represented by their value, extended slices are represented as tuples.   Deprecated since version 3.8: Old classes ast.Num, ast.Str, ast.Bytes, ast.NameConstant and ast.Ellipsis are still available, but they will be removed in future Python releases. In the meantime, instantiating them will return an instance of a different class.   Deprecated since version 3.9: Old classes ast.Index and ast.ExtSlice are still available, but they will be removed in future Python releases. In the meantime, instantiating them will return an instance of a different class.   Note The descriptions of the specific node classes displayed here were initially adapted from the fantastic Green Tree Snakes project and all its contributors.  Literals  \nclass ast.Constant(value)  \nA constant value. The value attribute of the Constant literal contains the Python object it represents. The values represented can be simple types such as a number, string or None, but also immutable container types (tuples and frozensets) if all of their elements are constant. >>> print(ast.dump(ast.parse('123', mode='eval'), indent=4))\nExpression(\n    body=Constant(value=123))\n \n  \nclass ast.FormattedValue(value, conversion, format_spec)  \nNode representing a single formatting field in an f-string. If the string contains a single formatting field and nothing else the node can be isolated otherwise it appears in JoinedStr.  \nvalue is any expression node (such as a literal, a variable, or a function call). \nconversion is an integer:  -1: no formatting 115: !s string formatting 114: !r repr formatting 97: !a ascii formatting   \nformat_spec is a JoinedStr node representing the formatting of the value, or None if no format was specified. Both conversion and format_spec can be set at the same time.  \n  \nclass ast.JoinedStr(values)  \nAn f-string, comprising a series of FormattedValue and Constant nodes. >>> print(ast.dump(ast.parse('f\"sin({a}) is {sin(a):.3}\"', mode='eval'), indent=4))\nExpression(\n    body=JoinedStr(\n        values=[\n            Constant(value='sin('),\n            FormattedValue(\n                value=Name(id='a', ctx=Load()),\n                conversion=-1),\n            Constant(value=') is '),\n            FormattedValue(\n                value=Call(\n                    func=Name(id='sin', ctx=Load()),\n                    args=[\n                        Name(id='a', ctx=Load())],\n                    keywords=[]),\n                conversion=-1,\n                format_spec=JoinedStr(\n                    values=[\n                        Constant(value='.3')]))]))\n \n  \nclass ast.List(elts, ctx)  \nclass ast.Tuple(elts, ctx)  \nA list or tuple. elts holds a list of nodes representing the elements. ctx is Store if the container is an assignment target (i.e. (x,y)=something), and Load otherwise. >>> print(ast.dump(ast.parse('[1, 2, 3]', mode='eval'), indent=4))\nExpression(\n    body=List(\n        elts=[\n            Constant(value=1),\n            Constant(value=2),\n            Constant(value=3)],\n        ctx=Load()))\n>>> print(ast.dump(ast.parse('(1, 2, 3)', mode='eval'), indent=4))\nExpression(\n    body=Tuple(\n        elts=[\n            Constant(value=1),\n            Constant(value=2),\n            Constant(value=3)],\n        ctx=Load()))\n \n  \nclass ast.Set(elts)  \nA set. elts holds a list of nodes representing the set\u2019s elements. >>> print(ast.dump(ast.parse('{1, 2, 3}', mode='eval'), indent=4))\nExpression(\n    body=Set(\n        elts=[\n            Constant(value=1),\n            Constant(value=2),\n            Constant(value=3)]))\n \n  \nclass ast.Dict(keys, values)  \nA dictionary. keys and values hold lists of nodes representing the keys and the values respectively, in matching order (what would be returned when calling dictionary.keys() and dictionary.values()). When doing dictionary unpacking using dictionary literals the expression to be expanded goes in the values list, with a None at the corresponding position in keys. >>> print(ast.dump(ast.parse('{\"a\":1, **d}', mode='eval'), indent=4))\nExpression(\n    body=Dict(\n        keys=[\n            Constant(value='a'),\n            None],\n        values=[\n            Constant(value=1),\n            Name(id='d', ctx=Load())]))\n \n Variables  \nclass ast.Name(id, ctx)  \nA variable name. id holds the name as a string, and ctx is one of the following types. \n  \nclass ast.Load  \nclass ast.Store  \nclass ast.Del  \nVariable references can be used to load the value of a variable, to assign a new value to it, or to delete it. Variable references are given a context to distinguish these cases. >>> print(ast.dump(ast.parse('a'), indent=4))\nModule(\n    body=[\n        Expr(\n            value=Name(id='a', ctx=Load()))],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('a = 1'), indent=4))\nModule(\n    body=[\n        Assign(\n            targets=[\n                Name(id='a', ctx=Store())],\n            value=Constant(value=1))],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('del a'), indent=4))\nModule(\n    body=[\n        Delete(\n            targets=[\n                Name(id='a', ctx=Del())])],\n    type_ignores=[])\n \n  \nclass ast.Starred(value, ctx)  \nA *var variable reference. value holds the variable, typically a Name node. This type must be used when building a Call node with *args. >>> print(ast.dump(ast.parse('a, *b = it'), indent=4))\nModule(\n    body=[\n        Assign(\n            targets=[\n                Tuple(\n                    elts=[\n                        Name(id='a', ctx=Store()),\n                        Starred(\n                            value=Name(id='b', ctx=Store()),\n                            ctx=Store())],\n                    ctx=Store())],\n            value=Name(id='it', ctx=Load()))],\n    type_ignores=[])\n \n Expressions  \nclass ast.Expr(value)  \nWhen an expression, such as a function call, appears as a statement by itself with its return value not used or stored, it is wrapped in this container. value holds one of the other nodes in this section, a Constant, a Name, a Lambda, a Yield or YieldFrom node. >>> print(ast.dump(ast.parse('-a'), indent=4))\nModule(\n    body=[\n        Expr(\n            value=UnaryOp(\n                op=USub(),\n                operand=Name(id='a', ctx=Load())))],\n    type_ignores=[])\n \n  \nclass ast.UnaryOp(op, operand)  \nA unary operation. op is the operator, and operand any expression node. \n  \nclass ast.UAdd  \nclass ast.USub  \nclass ast.Not  \nclass ast.Invert  \nUnary operator tokens. Not is the not keyword, Invert is the ~ operator. >>> print(ast.dump(ast.parse('not x', mode='eval'), indent=4))\nExpression(\n    body=UnaryOp(\n        op=Not(),\n        operand=Name(id='x', ctx=Load())))\n \n  \nclass ast.BinOp(left, op, right)  \nA binary operation (like addition or division). op is the operator, and left and right are any expression nodes. >>> print(ast.dump(ast.parse('x + y', mode='eval'), indent=4))\nExpression(\n    body=BinOp(\n        left=Name(id='x', ctx=Load()),\n        op=Add(),\n        right=Name(id='y', ctx=Load())))\n \n  \nclass ast.Add  \nclass ast.Sub  \nclass ast.Mult  \nclass ast.Div  \nclass ast.FloorDiv  \nclass ast.Mod  \nclass ast.Pow  \nclass ast.LShift  \nclass ast.RShift  \nclass ast.BitOr  \nclass ast.BitXor  \nclass ast.BitAnd  \nclass ast.MatMult  \nBinary operator tokens. \n  \nclass ast.BoolOp(op, values)  \nA boolean operation, \u2018or\u2019 or \u2018and\u2019. op is Or or And. values are the values involved. Consecutive operations with the same operator, such as a or b or c, are collapsed into one node with several values. This doesn\u2019t include not, which is a UnaryOp. >>> print(ast.dump(ast.parse('x or y', mode='eval'), indent=4))\nExpression(\n    body=BoolOp(\n        op=Or(),\n        values=[\n            Name(id='x', ctx=Load()),\n            Name(id='y', ctx=Load())]))\n \n  \nclass ast.And  \nclass ast.Or  \nBoolean operator tokens. \n  \nclass ast.Compare(left, ops, comparators)  \nA comparison of two or more values. left is the first value in the comparison, ops the list of operators, and comparators the list of values after the first element in the comparison. >>> print(ast.dump(ast.parse('1 <= a < 10', mode='eval'), indent=4))\nExpression(\n    body=Compare(\n        left=Constant(value=1),\n        ops=[\n            LtE(),\n            Lt()],\n        comparators=[\n            Name(id='a', ctx=Load()),\n            Constant(value=10)]))\n \n  \nclass ast.Eq  \nclass ast.NotEq  \nclass ast.Lt  \nclass ast.LtE  \nclass ast.Gt  \nclass ast.GtE  \nclass ast.Is  \nclass ast.IsNot  \nclass ast.In  \nclass ast.NotIn  \nComparison operator tokens. \n  \nclass ast.Call(func, args, keywords, starargs, kwargs)  \nA function call. func is the function, which will often be a Name or Attribute object. Of the arguments:  \nargs holds a list of the arguments passed by position. \nkeywords holds a list of keyword objects representing arguments passed by keyword.  When creating a Call node, args and keywords are required, but they can be empty lists. starargs and kwargs are optional. >>> print(ast.dump(ast.parse('func(a, b=c, *d, **e)', mode='eval'), indent=4))\nExpression(\n    body=Call(\n        func=Name(id='func', ctx=Load()),\n        args=[\n            Name(id='a', ctx=Load()),\n            Starred(\n                value=Name(id='d', ctx=Load()),\n                ctx=Load())],\n        keywords=[\n            keyword(\n                arg='b',\n                value=Name(id='c', ctx=Load())),\n            keyword(\n                value=Name(id='e', ctx=Load()))]))\n \n  \nclass ast.keyword(arg, value)  \nA keyword argument to a function call or class definition. arg is a raw string of the parameter name, value is a node to pass in. \n  \nclass ast.IfExp(test, body, orelse)  \nAn expression such as a if b else c. Each field holds a single node, so in the following example, all three are Name nodes. >>> print(ast.dump(ast.parse('a if b else c', mode='eval'), indent=4))\nExpression(\n    body=IfExp(\n        test=Name(id='b', ctx=Load()),\n        body=Name(id='a', ctx=Load()),\n        orelse=Name(id='c', ctx=Load())))\n \n  \nclass ast.Attribute(value, attr, ctx)  \nAttribute access, e.g. d.keys. value is a node, typically a Name. attr is a bare string giving the name of the attribute, and ctx is Load, Store or Del according to how the attribute is acted on. >>> print(ast.dump(ast.parse('snake.colour', mode='eval'), indent=4))\nExpression(\n    body=Attribute(\n        value=Name(id='snake', ctx=Load()),\n        attr='colour',\n        ctx=Load()))\n \n  \nclass ast.NamedExpr(target, value)   A named expression. This AST node is produced by the assignment expressions operator (also known as the walrus operator). As opposed to the Assign node in which the first argument can be multiple nodes, in this case both target and value must be single nodes. >>> print(ast.dump(ast.parse('(x := 4)', mode='eval'), indent=4))\nExpression(\n    body=NamedExpr(\n        target=Name(id='x', ctx=Store()),\n        value=Constant(value=4)))\n \n Subscripting  \nclass ast.Subscript(value, slice, ctx)  \nA subscript, such as l[1]. value is the subscripted object (usually sequence or mapping). slice is an index, slice or key. It can be a Tuple and contain a Slice. ctx is Load, Store or Del according to the action performed with the subscript. >>> print(ast.dump(ast.parse('l[1:2, 3]', mode='eval'), indent=4))\nExpression(\n    body=Subscript(\n        value=Name(id='l', ctx=Load()),\n        slice=Tuple(\n            elts=[\n                Slice(\n                    lower=Constant(value=1),\n                    upper=Constant(value=2)),\n                Constant(value=3)],\n            ctx=Load()),\n        ctx=Load()))\n \n  \nclass ast.Slice(lower, upper, step)  \nRegular slicing (on the form lower:upper or lower:upper:step). Can occur only inside the slice field of Subscript, either directly or as an element of Tuple. >>> print(ast.dump(ast.parse('l[1:2]', mode='eval'), indent=4))\nExpression(\n    body=Subscript(\n        value=Name(id='l', ctx=Load()),\n        slice=Slice(\n            lower=Constant(value=1),\n            upper=Constant(value=2)),\n        ctx=Load()))\n \n Comprehensions  \nclass ast.ListComp(elt, generators)  \nclass ast.SetComp(elt, generators)  \nclass ast.GeneratorExp(elt, generators)  \nclass ast.DictComp(key, value, generators)  \nList and set comprehensions, generator expressions, and dictionary comprehensions. elt (or key and value) is a single node representing the part that will be evaluated for each item. generators is a list of comprehension nodes. >>> print(ast.dump(ast.parse('[x for x in numbers]', mode='eval'), indent=4))\nExpression(\n    body=ListComp(\n        elt=Name(id='x', ctx=Load()),\n        generators=[\n            comprehension(\n                target=Name(id='x', ctx=Store()),\n                iter=Name(id='numbers', ctx=Load()),\n                ifs=[],\n                is_async=0)]))\n>>> print(ast.dump(ast.parse('{x: x**2 for x in numbers}', mode='eval'), indent=4))\nExpression(\n    body=DictComp(\n        key=Name(id='x', ctx=Load()),\n        value=BinOp(\n            left=Name(id='x', ctx=Load()),\n            op=Pow(),\n            right=Constant(value=2)),\n        generators=[\n            comprehension(\n                target=Name(id='x', ctx=Store()),\n                iter=Name(id='numbers', ctx=Load()),\n                ifs=[],\n                is_async=0)]))\n>>> print(ast.dump(ast.parse('{x for x in numbers}', mode='eval'), indent=4))\nExpression(\n    body=SetComp(\n        elt=Name(id='x', ctx=Load()),\n        generators=[\n            comprehension(\n                target=Name(id='x', ctx=Store()),\n                iter=Name(id='numbers', ctx=Load()),\n                ifs=[],\n                is_async=0)]))\n \n  \nclass ast.comprehension(target, iter, ifs, is_async)  \nOne for clause in a comprehension. target is the reference to use for each element - typically a Name or Tuple node. iter is the object to iterate over. ifs is a list of test expressions: each for clause can have multiple ifs. is_async indicates a comprehension is asynchronous (using an async for instead of for). The value is an integer (0 or 1). >>> print(ast.dump(ast.parse('[ord(c) for line in file for c in line]', mode='eval'),\n...                indent=4)) # Multiple comprehensions in one.\nExpression(\n    body=ListComp(\n        elt=Call(\n            func=Name(id='ord', ctx=Load()),\n            args=[\n                Name(id='c', ctx=Load())],\n            keywords=[]),\n        generators=[\n            comprehension(\n                target=Name(id='line', ctx=Store()),\n                iter=Name(id='file', ctx=Load()),\n                ifs=[],\n                is_async=0),\n            comprehension(\n                target=Name(id='c', ctx=Store()),\n                iter=Name(id='line', ctx=Load()),\n                ifs=[],\n                is_async=0)]))\n\n>>> print(ast.dump(ast.parse('(n**2 for n in it if n>5 if n<10)', mode='eval'),\n...                indent=4)) # generator comprehension\nExpression(\n    body=GeneratorExp(\n        elt=BinOp(\n            left=Name(id='n', ctx=Load()),\n            op=Pow(),\n            right=Constant(value=2)),\n        generators=[\n            comprehension(\n                target=Name(id='n', ctx=Store()),\n                iter=Name(id='it', ctx=Load()),\n                ifs=[\n                    Compare(\n                        left=Name(id='n', ctx=Load()),\n                        ops=[\n                            Gt()],\n                        comparators=[\n                            Constant(value=5)]),\n                    Compare(\n                        left=Name(id='n', ctx=Load()),\n                        ops=[\n                            Lt()],\n                        comparators=[\n                            Constant(value=10)])],\n                is_async=0)]))\n\n>>> print(ast.dump(ast.parse('[i async for i in soc]', mode='eval'),\n...                indent=4)) # Async comprehension\nExpression(\n    body=ListComp(\n        elt=Name(id='i', ctx=Load()),\n        generators=[\n            comprehension(\n                target=Name(id='i', ctx=Store()),\n                iter=Name(id='soc', ctx=Load()),\n                ifs=[],\n                is_async=1)]))\n \n Statements  \nclass ast.Assign(targets, value, type_comment)  \nAn assignment. targets is a list of nodes, and value is a single node. Multiple nodes in targets represents assigning the same value to each. Unpacking is represented by putting a Tuple or List within targets.  \ntype_comment  \ntype_comment is an optional string with the type annotation as a comment. \n >>> print(ast.dump(ast.parse('a = b = 1'), indent=4)) # Multiple assignment\nModule(\n    body=[\n        Assign(\n            targets=[\n                Name(id='a', ctx=Store()),\n                Name(id='b', ctx=Store())],\n            value=Constant(value=1))],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('a,b = c'), indent=4)) # Unpacking\nModule(\n    body=[\n        Assign(\n            targets=[\n                Tuple(\n                    elts=[\n                        Name(id='a', ctx=Store()),\n                        Name(id='b', ctx=Store())],\n                    ctx=Store())],\n            value=Name(id='c', ctx=Load()))],\n    type_ignores=[])\n \n  \nclass ast.AnnAssign(target, annotation, value, simple)  \nAn assignment with a type annotation. target is a single node and can be a Name, a Attribute or a Subscript. annotation is the annotation, such as a Constant or Name node. value is a single optional node. simple is a boolean integer set to True for a Name node in target that do not appear in between parenthesis and are hence pure names and not expressions. >>> print(ast.dump(ast.parse('c: int'), indent=4))\nModule(\n    body=[\n        AnnAssign(\n            target=Name(id='c', ctx=Store()),\n            annotation=Name(id='int', ctx=Load()),\n            simple=1)],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('(a): int = 1'), indent=4)) # Annotation with parenthesis\nModule(\n    body=[\n        AnnAssign(\n            target=Name(id='a', ctx=Store()),\n            annotation=Name(id='int', ctx=Load()),\n            value=Constant(value=1),\n            simple=0)],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('a.b: int'), indent=4)) # Attribute annotation\nModule(\n    body=[\n        AnnAssign(\n            target=Attribute(\n                value=Name(id='a', ctx=Load()),\n                attr='b',\n                ctx=Store()),\n            annotation=Name(id='int', ctx=Load()),\n            simple=0)],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('a[1]: int'), indent=4)) # Subscript annotation\nModule(\n    body=[\n        AnnAssign(\n            target=Subscript(\n                value=Name(id='a', ctx=Load()),\n                slice=Constant(value=1),\n                ctx=Store()),\n            annotation=Name(id='int', ctx=Load()),\n            simple=0)],\n    type_ignores=[])\n \n  \nclass ast.AugAssign(target, op, value)  \nAugmented assignment, such as a += 1. In the following example, target is a Name node for x (with the Store context), op is Add, and value is a Constant with value for 1. The target attribute connot be of class Tuple or List, unlike the targets of Assign. >>> print(ast.dump(ast.parse('x += 2'), indent=4))\nModule(\n    body=[\n        AugAssign(\n            target=Name(id='x', ctx=Store()),\n            op=Add(),\n            value=Constant(value=2))],\n    type_ignores=[])\n \n  \nclass ast.Raise(exc, cause)  \nA raise statement. exc is the exception object to be raised, normally a Call or Name, or None for a standalone raise. cause is the optional part for y in raise x from y. >>> print(ast.dump(ast.parse('raise x from y'), indent=4))\nModule(\n    body=[\n        Raise(\n            exc=Name(id='x', ctx=Load()),\n            cause=Name(id='y', ctx=Load()))],\n    type_ignores=[])\n \n  \nclass ast.Assert(test, msg)  \nAn assertion. test holds the condition, such as a Compare node. msg holds the failure message. >>> print(ast.dump(ast.parse('assert x,y'), indent=4))\nModule(\n    body=[\n        Assert(\n            test=Name(id='x', ctx=Load()),\n            msg=Name(id='y', ctx=Load()))],\n    type_ignores=[])\n \n  \nclass ast.Delete(targets)  \nRepresents a del statement. targets is a list of nodes, such as Name, Attribute or Subscript nodes. >>> print(ast.dump(ast.parse('del x,y,z'), indent=4))\nModule(\n    body=[\n        Delete(\n            targets=[\n                Name(id='x', ctx=Del()),\n                Name(id='y', ctx=Del()),\n                Name(id='z', ctx=Del())])],\n    type_ignores=[])\n \n  \nclass ast.Pass  \nA pass statement. >>> print(ast.dump(ast.parse('pass'), indent=4))\nModule(\n    body=[\n        Pass()],\n    type_ignores=[])\n \n Other statements which are only applicable inside functions or loops are described in other sections. Imports  \nclass ast.Import(names)  \nAn import statement. names is a list of alias nodes. >>> print(ast.dump(ast.parse('import x,y,z'), indent=4))\nModule(\n    body=[\n        Import(\n            names=[\n                alias(name='x'),\n                alias(name='y'),\n                alias(name='z')])],\n    type_ignores=[])\n \n  \nclass ast.ImportFrom(module, names, level)  \nRepresents from x import y. module is a raw string of the \u2018from\u2019 name, without any leading dots, or None for statements such as from . import foo. level is an integer holding the level of the relative import (0 means absolute import). >>> print(ast.dump(ast.parse('from y import x,y,z'), indent=4))\nModule(\n    body=[\n        ImportFrom(\n            module='y',\n            names=[\n                alias(name='x'),\n                alias(name='y'),\n                alias(name='z')],\n            level=0)],\n    type_ignores=[])\n \n  \nclass ast.alias(name, asname)  \nBoth parameters are raw strings of the names. asname can be None if the regular name is to be used. >>> print(ast.dump(ast.parse('from ..foo.bar import a as b, c'), indent=4))\nModule(\n    body=[\n        ImportFrom(\n            module='foo.bar',\n            names=[\n                alias(name='a', asname='b'),\n                alias(name='c')],\n            level=2)],\n    type_ignores=[])\n \n Control flow  Note Optional clauses such as else are stored as an empty list if they\u2019re not present.   \nclass ast.If(test, body, orelse)  \nAn if statement. test holds a single node, such as a Compare node. body and orelse each hold a list of nodes. elif clauses don\u2019t have a special representation in the AST, but rather appear as extra If nodes within the orelse section of the previous one. >>> print(ast.dump(ast.parse(\"\"\"\n... if x:\n...    ...\n... elif y:\n...    ...\n... else:\n...    ...\n... \"\"\"), indent=4))\nModule(\n    body=[\n        If(\n            test=Name(id='x', ctx=Load()),\n            body=[\n                Expr(\n                    value=Constant(value=Ellipsis))],\n            orelse=[\n                If(\n                    test=Name(id='y', ctx=Load()),\n                    body=[\n                        Expr(\n                            value=Constant(value=Ellipsis))],\n                    orelse=[\n                        Expr(\n                            value=Constant(value=Ellipsis))])])],\n    type_ignores=[])\n \n  \nclass ast.For(target, iter, body, orelse, type_comment)  \nA for loop. target holds the variable(s) the loop assigns to, as a single Name, Tuple or List node. iter holds the item to be looped over, again as a single node. body and orelse contain lists of nodes to execute. Those in orelse are executed if the loop finishes normally, rather than via a break statement.  \ntype_comment  \ntype_comment is an optional string with the type annotation as a comment. \n >>> print(ast.dump(ast.parse(\"\"\"\n... for x in y:\n...     ...\n... else:\n...     ...\n... \"\"\"), indent=4))\nModule(\n    body=[\n        For(\n            target=Name(id='x', ctx=Store()),\n            iter=Name(id='y', ctx=Load()),\n            body=[\n                Expr(\n                    value=Constant(value=Ellipsis))],\n            orelse=[\n                Expr(\n                    value=Constant(value=Ellipsis))])],\n    type_ignores=[])\n \n  \nclass ast.While(test, body, orelse)  \nA while loop. test holds the condition, such as a Compare node. >> print(ast.dump(ast.parse(\"\"\"\n... while x:\n...    ...\n... else:\n...    ...\n... \"\"\"), indent=4))\nModule(\n    body=[\n        While(\n            test=Name(id='x', ctx=Load()),\n            body=[\n                Expr(\n                    value=Constant(value=Ellipsis))],\n            orelse=[\n                Expr(\n                    value=Constant(value=Ellipsis))])],\n    type_ignores=[])\n \n  \nclass ast.Break  \nclass ast.Continue  \nThe break and continue statements. >>> print(ast.dump(ast.parse(\"\"\"\\\n... for a in b:\n...     if a > 5:\n...         break\n...     else:\n...         continue\n...\n... \"\"\"), indent=4))\nModule(\n    body=[\n        For(\n            target=Name(id='a', ctx=Store()),\n            iter=Name(id='b', ctx=Load()),\n            body=[\n                If(\n                    test=Compare(\n                        left=Name(id='a', ctx=Load()),\n                        ops=[\n                            Gt()],\n                        comparators=[\n                            Constant(value=5)]),\n                    body=[\n                        Break()],\n                    orelse=[\n                        Continue()])],\n            orelse=[])],\n    type_ignores=[])\n \n  \nclass ast.Try(body, handlers, orelse, finalbody)  \ntry blocks. All attributes are list of nodes to execute, except for handlers, which is a list of ExceptHandler nodes. >>> print(ast.dump(ast.parse(\"\"\"\n... try:\n...    ...\n... except Exception:\n...    ...\n... except OtherException as e:\n...    ...\n... else:\n...    ...\n... finally:\n...    ...\n... \"\"\"), indent=4))\nModule(\n    body=[\n        Try(\n            body=[\n                Expr(\n                    value=Constant(value=Ellipsis))],\n            handlers=[\n                ExceptHandler(\n                    type=Name(id='Exception', ctx=Load()),\n                    body=[\n                        Expr(\n                            value=Constant(value=Ellipsis))]),\n                ExceptHandler(\n                    type=Name(id='OtherException', ctx=Load()),\n                    name='e',\n                    body=[\n                        Expr(\n                            value=Constant(value=Ellipsis))])],\n            orelse=[\n                Expr(\n                    value=Constant(value=Ellipsis))],\n            finalbody=[\n                Expr(\n                    value=Constant(value=Ellipsis))])],\n    type_ignores=[])\n \n  \nclass ast.ExceptHandler(type, name, body)  \nA single except clause. type is the exception type it will match, typically a Name node (or None for a catch-all except: clause). name is a raw string for the name to hold the exception, or None if the clause doesn\u2019t have as foo. body is a list of nodes. >>> print(ast.dump(ast.parse(\"\"\"\\\n... try:\n...     a + 1\n... except TypeError:\n...     pass\n... \"\"\"), indent=4))\nModule(\n    body=[\n        Try(\n            body=[\n                Expr(\n                    value=BinOp(\n                        left=Name(id='a', ctx=Load()),\n                        op=Add(),\n                        right=Constant(value=1)))],\n            handlers=[\n                ExceptHandler(\n                    type=Name(id='TypeError', ctx=Load()),\n                    body=[\n                        Pass()])],\n            orelse=[],\n            finalbody=[])],\n    type_ignores=[])\n \n  \nclass ast.With(items, body, type_comment)  \nA with block. items is a list of withitem nodes representing the context managers, and body is the indented block inside the context.  \ntype_comment  \ntype_comment is an optional string with the type annotation as a comment. \n \n  \nclass ast.withitem(context_expr, optional_vars)  \nA single context manager in a with block. context_expr is the context manager, often a Call node. optional_vars is a Name, Tuple or List for the as foo part, or None if that isn\u2019t used. >>> print(ast.dump(ast.parse(\"\"\"\\\n... with a as b, c as d:\n...    something(b, d)\n... \"\"\"), indent=4))\nModule(\n    body=[\n        With(\n            items=[\n                withitem(\n                    context_expr=Name(id='a', ctx=Load()),\n                    optional_vars=Name(id='b', ctx=Store())),\n                withitem(\n                    context_expr=Name(id='c', ctx=Load()),\n                    optional_vars=Name(id='d', ctx=Store()))],\n            body=[\n                Expr(\n                    value=Call(\n                        func=Name(id='something', ctx=Load()),\n                        args=[\n                            Name(id='b', ctx=Load()),\n                            Name(id='d', ctx=Load())],\n                        keywords=[]))])],\n    type_ignores=[])\n \n Function and class definitions  \nclass ast.FunctionDef(name, args, body, decorator_list, returns, type_comment)  \nA function definition.  \nname is a raw string of the function name. \nargs is a arguments node. \nbody is the list of nodes inside the function. \ndecorator_list is the list of decorators to be applied, stored outermost first (i.e. the first in the list will be applied last). \nreturns is the return annotation.   \ntype_comment  \ntype_comment is an optional string with the type annotation as a comment. \n \n  \nclass ast.Lambda(args, body)  \nlambda is a minimal function definition that can be used inside an expression. Unlike FunctionDef, body holds a single node. >>> print(ast.dump(ast.parse('lambda x,y: ...'), indent=4))\nModule(\n    body=[\n        Expr(\n            value=Lambda(\n                args=arguments(\n                    posonlyargs=[],\n                    args=[\n                        arg(arg='x'),\n                        arg(arg='y')],\n                    kwonlyargs=[],\n                    kw_defaults=[],\n                    defaults=[]),\n                body=Constant(value=Ellipsis)))],\n    type_ignores=[])\n \n  \nclass ast.arguments(posonlyargs, args, vararg, kwonlyargs, kw_defaults, kwarg, defaults)  \nThe arguments for a function.  \nposonlyargs, args and kwonlyargs are lists of arg nodes. \nvararg and kwarg are single arg nodes, referring to the *args, **kwargs parameters. \nkw_defaults is a list of default values for keyword-only arguments. If one is None, the corresponding argument is required. \ndefaults is a list of default values for arguments that can be passed positionally. If there are fewer defaults, they correspond to the last n arguments.  \n  \nclass ast.arg(arg, annotation, type_comment)  \nA single argument in a list. arg is a raw string of the argument name, annotation is its annotation, such as a Str or Name node.  \ntype_comment  \ntype_comment is an optional string with the type annotation as a comment \n >>> print(ast.dump(ast.parse(\"\"\"\\\n... @decorator1\n... @decorator2\n... def f(a: 'annotation', b=1, c=2, *d, e, f=3, **g) -> 'return annotation':\n...     pass\n... \"\"\"), indent=4))\nModule(\n    body=[\n        FunctionDef(\n            name='f',\n            args=arguments(\n                posonlyargs=[],\n                args=[\n                    arg(\n                        arg='a',\n                        annotation=Constant(value='annotation')),\n                    arg(arg='b'),\n                    arg(arg='c')],\n                vararg=arg(arg='d'),\n                kwonlyargs=[\n                    arg(arg='e'),\n                    arg(arg='f')],\n                kw_defaults=[\n                    None,\n                    Constant(value=3)],\n                kwarg=arg(arg='g'),\n                defaults=[\n                    Constant(value=1),\n                    Constant(value=2)]),\n            body=[\n                Pass()],\n            decorator_list=[\n                Name(id='decorator1', ctx=Load()),\n                Name(id='decorator2', ctx=Load())],\n            returns=Constant(value='return annotation'))],\n    type_ignores=[])\n \n  \nclass ast.Return(value)  \nA return statement. >>> print(ast.dump(ast.parse('return 4'), indent=4))\nModule(\n    body=[\n        Return(\n            value=Constant(value=4))],\n    type_ignores=[])\n \n  \nclass ast.Yield(value)  \nclass ast.YieldFrom(value)  \nA yield or yield from expression. Because these are expressions, they must be wrapped in a Expr node if the value sent back is not used. >>> print(ast.dump(ast.parse('yield x'), indent=4))\nModule(\n    body=[\n        Expr(\n            value=Yield(\n                value=Name(id='x', ctx=Load())))],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('yield from x'), indent=4))\nModule(\n    body=[\n        Expr(\n            value=YieldFrom(\n                value=Name(id='x', ctx=Load())))],\n    type_ignores=[])\n \n  \nclass ast.Global(names)  \nclass ast.Nonlocal(names)  \nglobal and nonlocal statements. names is a list of raw strings. >>> print(ast.dump(ast.parse('global x,y,z'), indent=4))\nModule(\n    body=[\n        Global(\n            names=[\n                'x',\n                'y',\n                'z'])],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('nonlocal x,y,z'), indent=4))\nModule(\n    body=[\n        Nonlocal(\n            names=[\n                'x',\n                'y',\n                'z'])],\n    type_ignores=[])\n \n  \nclass ast.ClassDef(name, bases, keywords, starargs, kwargs, body, decorator_list)  \nA class definition.  \nname is a raw string for the class name \nbases is a list of nodes for explicitly specified base classes. \nkeywords is a list of keyword nodes, principally for \u2018metaclass\u2019. Other keywords will be passed to the metaclass, as per PEP-3115. \nstarargs and kwargs are each a single node, as in a function call. starargs will be expanded to join the list of base classes, and kwargs will be passed to the metaclass. \nbody is a list of nodes representing the code within the class definition. \ndecorator_list is a list of nodes, as in FunctionDef.  >>> print(ast.dump(ast.parse(\"\"\"\\\n... @decorator1\n... @decorator2\n... class Foo(base1, base2, metaclass=meta):\n...     pass\n... \"\"\"), indent=4))\nModule(\n    body=[\n        ClassDef(\n            name='Foo',\n            bases=[\n                Name(id='base1', ctx=Load()),\n                Name(id='base2', ctx=Load())],\n            keywords=[\n                keyword(\n                    arg='metaclass',\n                    value=Name(id='meta', ctx=Load()))],\n            body=[\n                Pass()],\n            decorator_list=[\n                Name(id='decorator1', ctx=Load()),\n                Name(id='decorator2', ctx=Load())])],\n    type_ignores=[])\n \n Async and await  \nclass ast.AsyncFunctionDef(name, args, body, decorator_list, returns, type_comment)  \nAn async def function definition. Has the same fields as FunctionDef. \n  \nclass ast.Await(value)  \nAn await expression. value is what it waits for. Only valid in the body of an AsyncFunctionDef. \n >>> print(ast.dump(ast.parse(\"\"\"\\\n... async def f():\n...     await other_func()\n... \"\"\"), indent=4))\nModule(\n    body=[\n        AsyncFunctionDef(\n            name='f',\n            args=arguments(\n                posonlyargs=[],\n                args=[],\n                kwonlyargs=[],\n                kw_defaults=[],\n                defaults=[]),\n            body=[\n                Expr(\n                    value=Await(\n                        value=Call(\n                            func=Name(id='other_func', ctx=Load()),\n                            args=[],\n                            keywords=[])))],\n            decorator_list=[])],\n    type_ignores=[])\n  \nclass ast.AsyncFor(target, iter, body, orelse, type_comment)  \nclass ast.AsyncWith(items, body, type_comment)  \nasync for loops and async with context managers. They have the same fields as For and With, respectively. Only valid in the body of an AsyncFunctionDef. \n  Note When a string is parsed by ast.parse(), operator nodes (subclasses of ast.operator, ast.unaryop, ast.cmpop, ast.boolop and ast.expr_context) on the returned tree will be singletons. Changes to one will be reflected in all other occurrences of the same value (e.g. ast.Add).  ast Helpers Apart from the node classes, the ast module defines these utility functions and classes for traversing abstract syntax trees:  \nast.parse(source, filename='<unknown>', mode='exec', *, type_comments=False, feature_version=None)  \nParse the source into an AST node. Equivalent to compile(source,\nfilename, mode, ast.PyCF_ONLY_AST). If type_comments=True is given, the parser is modified to check and return type comments as specified by PEP 484 and PEP 526. This is equivalent to adding ast.PyCF_TYPE_COMMENTS to the flags passed to compile(). This will report syntax errors for misplaced type comments. Without this flag, type comments will be ignored, and the type_comment field on selected AST nodes will always be None. In addition, the locations of # type:\nignore comments will be returned as the type_ignores attribute of Module (otherwise it is always an empty list). In addition, if mode is 'func_type', the input syntax is modified to correspond to PEP 484 \u201csignature type comments\u201d, e.g. (str, int) -> List[str]. Also, setting feature_version to a tuple (major, minor) will attempt to parse using that Python version\u2019s grammar. Currently major must equal to 3. For example, setting feature_version=(3, 4) will allow the use of async and await as variable names. The lowest supported version is (3, 4); the highest is sys.version_info[0:2].  Warning It is possible to crash the Python interpreter with a sufficiently large/complex string due to stack depth limitations in Python\u2019s AST compiler.   Changed in version 3.8: Added type_comments, mode='func_type' and feature_version.  \n  \nast.unparse(ast_obj)  \nUnparse an ast.AST object and generate a string with code that would produce an equivalent ast.AST object if parsed back with ast.parse().  Warning The produced code string will not necessarily be equal to the original code that generated the ast.AST object (without any compiler optimizations, such as constant tuples/frozensets).   Warning Trying to unparse a highly complex expression would result with RecursionError.   New in version 3.9.  \n  \nast.literal_eval(node_or_string)  \nSafely evaluate an expression node or a string containing a Python literal or container display. The string or node provided may only consist of the following Python literal structures: strings, bytes, numbers, tuples, lists, dicts, sets, booleans, and None. This can be used for safely evaluating strings containing Python values from untrusted sources without the need to parse the values oneself. It is not capable of evaluating arbitrarily complex expressions, for example involving operators or indexing.  Warning It is possible to crash the Python interpreter with a sufficiently large/complex string due to stack depth limitations in Python\u2019s AST compiler.   Changed in version 3.2: Now allows bytes and set literals.   Changed in version 3.9: Now supports creating empty sets with 'set()'.  \n  \nast.get_docstring(node, clean=True)  \nReturn the docstring of the given node (which must be a FunctionDef, AsyncFunctionDef, ClassDef, or Module node), or None if it has no docstring. If clean is true, clean up the docstring\u2019s indentation with inspect.cleandoc().  Changed in version 3.5: AsyncFunctionDef is now supported.  \n  \nast.get_source_segment(source, node, *, padded=False)  \nGet source code segment of the source that generated node. If some location information (lineno, end_lineno, col_offset, or end_col_offset) is missing, return None. If padded is True, the first line of a multi-line statement will be padded with spaces to match its original position.  New in version 3.8.  \n  \nast.fix_missing_locations(node)  \nWhen you compile a node tree with compile(), the compiler expects lineno and col_offset attributes for every node that supports them. This is rather tedious to fill in for generated nodes, so this helper adds these attributes recursively where not already set, by setting them to the values of the parent node. It works recursively starting at node. \n  \nast.increment_lineno(node, n=1)  \nIncrement the line number and end line number of each node in the tree starting at node by n. This is useful to \u201cmove code\u201d to a different location in a file. \n  \nast.copy_location(new_node, old_node)  \nCopy source location (lineno, col_offset, end_lineno, and end_col_offset) from old_node to new_node if possible, and return new_node. \n  \nast.iter_fields(node)  \nYield a tuple of (fieldname, value) for each field in node._fields that is present on node. \n  \nast.iter_child_nodes(node)  \nYield all direct child nodes of node, that is, all fields that are nodes and all items of fields that are lists of nodes. \n  \nast.walk(node)  \nRecursively yield all descendant nodes in the tree starting at node (including node itself), in no specified order. This is useful if you only want to modify nodes in place and don\u2019t care about the context. \n  \nclass ast.NodeVisitor  \nA node visitor base class that walks the abstract syntax tree and calls a visitor function for every node found. This function may return a value which is forwarded by the visit() method. This class is meant to be subclassed, with the subclass adding visitor methods.  \nvisit(node)  \nVisit a node. The default implementation calls the method called self.visit_classname where classname is the name of the node class, or generic_visit() if that method doesn\u2019t exist. \n  \ngeneric_visit(node)  \nThis visitor calls visit() on all children of the node. Note that child nodes of nodes that have a custom visitor method won\u2019t be visited unless the visitor calls generic_visit() or visits them itself. \n Don\u2019t use the NodeVisitor if you want to apply changes to nodes during traversal. For this a special visitor exists (NodeTransformer) that allows modifications.  Deprecated since version 3.8: Methods visit_Num(), visit_Str(), visit_Bytes(), visit_NameConstant() and visit_Ellipsis() are deprecated now and will not be called in future Python versions. Add the visit_Constant() method to handle all constant nodes.  \n  \nclass ast.NodeTransformer  \nA NodeVisitor subclass that walks the abstract syntax tree and allows modification of nodes. The NodeTransformer will walk the AST and use the return value of the visitor methods to replace or remove the old node. If the return value of the visitor method is None, the node will be removed from its location, otherwise it is replaced with the return value. The return value may be the original node in which case no replacement takes place. Here is an example transformer that rewrites all occurrences of name lookups (foo) to data['foo']: class RewriteName(NodeTransformer):\n\n    def visit_Name(self, node):\n        return Subscript(\n            value=Name(id='data', ctx=Load()),\n            slice=Constant(value=node.id),\n            ctx=node.ctx\n        )\n Keep in mind that if the node you\u2019re operating on has child nodes you must either transform the child nodes yourself or call the generic_visit() method for the node first. For nodes that were part of a collection of statements (that applies to all statement nodes), the visitor may also return a list of nodes rather than just a single node. If NodeTransformer introduces new nodes (that weren\u2019t part of original tree) without giving them location information (such as lineno), fix_missing_locations() should be called with the new sub-tree to recalculate the location information: tree = ast.parse('foo', mode='eval')\nnew_tree = fix_missing_locations(RewriteName().visit(tree))\n Usually you use the transformer like this: node = YourTransformer().visit(node)\n \n  \nast.dump(node, annotate_fields=True, include_attributes=False, *, indent=None)  \nReturn a formatted dump of the tree in node. This is mainly useful for debugging purposes. If annotate_fields is true (by default), the returned string will show the names and the values for fields. If annotate_fields is false, the result string will be more compact by omitting unambiguous field names. Attributes such as line numbers and column offsets are not dumped by default. If this is wanted, include_attributes can be set to true. If indent is a non-negative integer or string, then the tree will be pretty-printed with that indent level. An indent level of 0, negative, or \"\" will only insert newlines. None (the default) selects the single line representation. Using a positive integer indent indents that many spaces per level. If indent is a string (such as \"\\t\"), that string is used to indent each level.  Changed in version 3.9: Added the indent option.  \n Compiler Flags The following flags may be passed to compile() in order to change effects on the compilation of a program:  \nast.PyCF_ALLOW_TOP_LEVEL_AWAIT  \nEnables support for top-level await, async for, async with and async comprehensions.  New in version 3.8.  \n  \nast.PyCF_ONLY_AST  \nGenerates and returns an abstract syntax tree instead of returning a compiled code object. \n  \nast.PyCF_TYPE_COMMENTS  \nEnables support for PEP 484 and PEP 526 style type comments (# type: <type>, # type: ignore <stuff>).  New in version 3.8.  \n Command-Line Usage  New in version 3.9.  The ast module can be executed as a script from the command line. It is as simple as: python -m ast [-m <mode>] [-a] [infile]\n The following options are accepted:  \n-h, --help  \nShow the help message and exit. \n  \n-m <mode>  \n--mode <mode>  \nSpecify what kind of code must be compiled, like the mode argument in parse(). \n  \n--no-type-comments  \nDon\u2019t parse type comments. \n  \n-a, --include-attributes  \nInclude attributes such as line numbers and column offsets. \n  \n-i <indent>  \n--indent <indent>  \nIndentation of nodes in AST (number of spaces). \n If infile is specified its contents are parsed to AST and dumped to stdout. Otherwise, the content is read from stdin.  See also Green Tree Snakes, an external documentation resource, has good details on working with Python ASTs. ASTTokens annotates Python ASTs with the positions of tokens and text in the source code that generated them. This is helpful for tools that make source code transformations. leoAst.py unifies the token-based and parse-tree-based views of python programs by inserting two-way links between tokens and ast nodes. LibCST parses code as a Concrete Syntax Tree that looks like an ast tree and keeps all formatting details. It\u2019s useful for building automated refactoring (codemod) applications and linters. Parso is a Python parser that supports error recovery and round-trip parsing for different Python versions (in multiple Python versions). Parso is also able to list multiple syntax errors in your python file. \n"}, {"name": "ast.Add", "path": "library/ast#ast.Add", "type": "Language", "text": " \nclass ast.Add  \nclass ast.Sub  \nclass ast.Mult  \nclass ast.Div  \nclass ast.FloorDiv  \nclass ast.Mod  \nclass ast.Pow  \nclass ast.LShift  \nclass ast.RShift  \nclass ast.BitOr  \nclass ast.BitXor  \nclass ast.BitAnd  \nclass ast.MatMult  \nBinary operator tokens. \n"}, {"name": "ast.alias", "path": "library/ast#ast.alias", "type": "Language", "text": " \nclass ast.alias(name, asname)  \nBoth parameters are raw strings of the names. asname can be None if the regular name is to be used. >>> print(ast.dump(ast.parse('from ..foo.bar import a as b, c'), indent=4))\nModule(\n    body=[\n        ImportFrom(\n            module='foo.bar',\n            names=[\n                alias(name='a', asname='b'),\n                alias(name='c')],\n            level=2)],\n    type_ignores=[])\n \n"}, {"name": "ast.And", "path": "library/ast#ast.And", "type": "Language", "text": " \nclass ast.And  \nclass ast.Or  \nBoolean operator tokens. \n"}, {"name": "ast.AnnAssign", "path": "library/ast#ast.AnnAssign", "type": "Language", "text": " \nclass ast.AnnAssign(target, annotation, value, simple)  \nAn assignment with a type annotation. target is a single node and can be a Name, a Attribute or a Subscript. annotation is the annotation, such as a Constant or Name node. value is a single optional node. simple is a boolean integer set to True for a Name node in target that do not appear in between parenthesis and are hence pure names and not expressions. >>> print(ast.dump(ast.parse('c: int'), indent=4))\nModule(\n    body=[\n        AnnAssign(\n            target=Name(id='c', ctx=Store()),\n            annotation=Name(id='int', ctx=Load()),\n            simple=1)],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('(a): int = 1'), indent=4)) # Annotation with parenthesis\nModule(\n    body=[\n        AnnAssign(\n            target=Name(id='a', ctx=Store()),\n            annotation=Name(id='int', ctx=Load()),\n            value=Constant(value=1),\n            simple=0)],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('a.b: int'), indent=4)) # Attribute annotation\nModule(\n    body=[\n        AnnAssign(\n            target=Attribute(\n                value=Name(id='a', ctx=Load()),\n                attr='b',\n                ctx=Store()),\n            annotation=Name(id='int', ctx=Load()),\n            simple=0)],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('a[1]: int'), indent=4)) # Subscript annotation\nModule(\n    body=[\n        AnnAssign(\n            target=Subscript(\n                value=Name(id='a', ctx=Load()),\n                slice=Constant(value=1),\n                ctx=Store()),\n            annotation=Name(id='int', ctx=Load()),\n            simple=0)],\n    type_ignores=[])\n \n"}, {"name": "ast.arg", "path": "library/ast#ast.arg", "type": "Language", "text": " \nclass ast.arg(arg, annotation, type_comment)  \nA single argument in a list. arg is a raw string of the argument name, annotation is its annotation, such as a Str or Name node.  \ntype_comment  \ntype_comment is an optional string with the type annotation as a comment \n >>> print(ast.dump(ast.parse(\"\"\"\\\n... @decorator1\n... @decorator2\n... def f(a: 'annotation', b=1, c=2, *d, e, f=3, **g) -> 'return annotation':\n...     pass\n... \"\"\"), indent=4))\nModule(\n    body=[\n        FunctionDef(\n            name='f',\n            args=arguments(\n                posonlyargs=[],\n                args=[\n                    arg(\n                        arg='a',\n                        annotation=Constant(value='annotation')),\n                    arg(arg='b'),\n                    arg(arg='c')],\n                vararg=arg(arg='d'),\n                kwonlyargs=[\n                    arg(arg='e'),\n                    arg(arg='f')],\n                kw_defaults=[\n                    None,\n                    Constant(value=3)],\n                kwarg=arg(arg='g'),\n                defaults=[\n                    Constant(value=1),\n                    Constant(value=2)]),\n            body=[\n                Pass()],\n            decorator_list=[\n                Name(id='decorator1', ctx=Load()),\n                Name(id='decorator2', ctx=Load())],\n            returns=Constant(value='return annotation'))],\n    type_ignores=[])\n \n"}, {"name": "ast.arg.type_comment", "path": "library/ast#ast.arg.type_comment", "type": "Language", "text": " \ntype_comment  \ntype_comment is an optional string with the type annotation as a comment \n"}, {"name": "ast.arguments", "path": "library/ast#ast.arguments", "type": "Language", "text": " \nclass ast.arguments(posonlyargs, args, vararg, kwonlyargs, kw_defaults, kwarg, defaults)  \nThe arguments for a function.  \nposonlyargs, args and kwonlyargs are lists of arg nodes. \nvararg and kwarg are single arg nodes, referring to the *args, **kwargs parameters. \nkw_defaults is a list of default values for keyword-only arguments. If one is None, the corresponding argument is required. \ndefaults is a list of default values for arguments that can be passed positionally. If there are fewer defaults, they correspond to the last n arguments.  \n"}, {"name": "ast.Assert", "path": "library/ast#ast.Assert", "type": "Language", "text": " \nclass ast.Assert(test, msg)  \nAn assertion. test holds the condition, such as a Compare node. msg holds the failure message. >>> print(ast.dump(ast.parse('assert x,y'), indent=4))\nModule(\n    body=[\n        Assert(\n            test=Name(id='x', ctx=Load()),\n            msg=Name(id='y', ctx=Load()))],\n    type_ignores=[])\n \n"}, {"name": "ast.Assign", "path": "library/ast#ast.Assign", "type": "Language", "text": " \nclass ast.Assign(targets, value, type_comment)  \nAn assignment. targets is a list of nodes, and value is a single node. Multiple nodes in targets represents assigning the same value to each. Unpacking is represented by putting a Tuple or List within targets.  \ntype_comment  \ntype_comment is an optional string with the type annotation as a comment. \n >>> print(ast.dump(ast.parse('a = b = 1'), indent=4)) # Multiple assignment\nModule(\n    body=[\n        Assign(\n            targets=[\n                Name(id='a', ctx=Store()),\n                Name(id='b', ctx=Store())],\n            value=Constant(value=1))],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('a,b = c'), indent=4)) # Unpacking\nModule(\n    body=[\n        Assign(\n            targets=[\n                Tuple(\n                    elts=[\n                        Name(id='a', ctx=Store()),\n                        Name(id='b', ctx=Store())],\n                    ctx=Store())],\n            value=Name(id='c', ctx=Load()))],\n    type_ignores=[])\n \n"}, {"name": "ast.Assign.type_comment", "path": "library/ast#ast.Assign.type_comment", "type": "Language", "text": " \ntype_comment  \ntype_comment is an optional string with the type annotation as a comment. \n"}, {"name": "ast.AST", "path": "library/ast#ast.AST", "type": "Language", "text": " \nclass ast.AST  \nThis is the base of all AST node classes. The actual node classes are derived from the Parser/Python.asdl file, which is reproduced below. They are defined in the _ast C module and re-exported in ast. There is one class defined for each left-hand side symbol in the abstract grammar (for example, ast.stmt or ast.expr). In addition, there is one class defined for each constructor on the right-hand side; these classes inherit from the classes for the left-hand side trees. For example, ast.BinOp inherits from ast.expr. For production rules with alternatives (aka \u201csums\u201d), the left-hand side class is abstract: only instances of specific constructor nodes are ever created.  \n_fields  \nEach concrete class has an attribute _fields which gives the names of all child nodes. Each instance of a concrete class has one attribute for each child node, of the type as defined in the grammar. For example, ast.BinOp instances have an attribute left of type ast.expr. If these attributes are marked as optional in the grammar (using a question mark), the value might be None. If the attributes can have zero-or-more values (marked with an asterisk), the values are represented as Python lists. All possible attributes must be present and have valid values when compiling an AST with compile(). \n  \nlineno  \ncol_offset  \nend_lineno  \nend_col_offset  \nInstances of ast.expr and ast.stmt subclasses have lineno, col_offset, lineno, and col_offset attributes. The lineno and end_lineno are the first and last line numbers of source text span (1-indexed so the first line is line 1) and the col_offset and end_col_offset are the corresponding UTF-8 byte offsets of the first and last tokens that generated the node. The UTF-8 offset is recorded because the parser uses UTF-8 internally. Note that the end positions are not required by the compiler and are therefore optional. The end offset is after the last symbol, for example one can get the source segment of a one-line expression node using source_line[node.col_offset : node.end_col_offset]. \n The constructor of a class ast.T parses its arguments as follows:  If there are positional arguments, there must be as many as there are items in T._fields; they will be assigned as attributes of these names. If there are keyword arguments, they will set the attributes of the same names to the given values.  For example, to create and populate an ast.UnaryOp node, you could use node = ast.UnaryOp()\nnode.op = ast.USub()\nnode.operand = ast.Constant()\nnode.operand.value = 5\nnode.operand.lineno = 0\nnode.operand.col_offset = 0\nnode.lineno = 0\nnode.col_offset = 0\n or the more compact node = ast.UnaryOp(ast.USub(), ast.Constant(5, lineno=0, col_offset=0),\n                   lineno=0, col_offset=0)\n \n"}, {"name": "ast.AST.col_offset", "path": "library/ast#ast.AST.col_offset", "type": "Language", "text": " \nlineno  \ncol_offset  \nend_lineno  \nend_col_offset  \nInstances of ast.expr and ast.stmt subclasses have lineno, col_offset, lineno, and col_offset attributes. The lineno and end_lineno are the first and last line numbers of source text span (1-indexed so the first line is line 1) and the col_offset and end_col_offset are the corresponding UTF-8 byte offsets of the first and last tokens that generated the node. The UTF-8 offset is recorded because the parser uses UTF-8 internally. Note that the end positions are not required by the compiler and are therefore optional. The end offset is after the last symbol, for example one can get the source segment of a one-line expression node using source_line[node.col_offset : node.end_col_offset]. \n"}, {"name": "ast.AST.end_col_offset", "path": "library/ast#ast.AST.end_col_offset", "type": "Language", "text": " \nlineno  \ncol_offset  \nend_lineno  \nend_col_offset  \nInstances of ast.expr and ast.stmt subclasses have lineno, col_offset, lineno, and col_offset attributes. The lineno and end_lineno are the first and last line numbers of source text span (1-indexed so the first line is line 1) and the col_offset and end_col_offset are the corresponding UTF-8 byte offsets of the first and last tokens that generated the node. The UTF-8 offset is recorded because the parser uses UTF-8 internally. Note that the end positions are not required by the compiler and are therefore optional. The end offset is after the last symbol, for example one can get the source segment of a one-line expression node using source_line[node.col_offset : node.end_col_offset]. \n"}, {"name": "ast.AST.end_lineno", "path": "library/ast#ast.AST.end_lineno", "type": "Language", "text": " \nlineno  \ncol_offset  \nend_lineno  \nend_col_offset  \nInstances of ast.expr and ast.stmt subclasses have lineno, col_offset, lineno, and col_offset attributes. The lineno and end_lineno are the first and last line numbers of source text span (1-indexed so the first line is line 1) and the col_offset and end_col_offset are the corresponding UTF-8 byte offsets of the first and last tokens that generated the node. The UTF-8 offset is recorded because the parser uses UTF-8 internally. Note that the end positions are not required by the compiler and are therefore optional. The end offset is after the last symbol, for example one can get the source segment of a one-line expression node using source_line[node.col_offset : node.end_col_offset]. \n"}, {"name": "ast.AST.lineno", "path": "library/ast#ast.AST.lineno", "type": "Language", "text": " \nlineno  \ncol_offset  \nend_lineno  \nend_col_offset  \nInstances of ast.expr and ast.stmt subclasses have lineno, col_offset, lineno, and col_offset attributes. The lineno and end_lineno are the first and last line numbers of source text span (1-indexed so the first line is line 1) and the col_offset and end_col_offset are the corresponding UTF-8 byte offsets of the first and last tokens that generated the node. The UTF-8 offset is recorded because the parser uses UTF-8 internally. Note that the end positions are not required by the compiler and are therefore optional. The end offset is after the last symbol, for example one can get the source segment of a one-line expression node using source_line[node.col_offset : node.end_col_offset]. \n"}, {"name": "ast.AST._fields", "path": "library/ast#ast.AST._fields", "type": "Language", "text": " \n_fields  \nEach concrete class has an attribute _fields which gives the names of all child nodes. Each instance of a concrete class has one attribute for each child node, of the type as defined in the grammar. For example, ast.BinOp instances have an attribute left of type ast.expr. If these attributes are marked as optional in the grammar (using a question mark), the value might be None. If the attributes can have zero-or-more values (marked with an asterisk), the values are represented as Python lists. All possible attributes must be present and have valid values when compiling an AST with compile(). \n"}, {"name": "ast.AsyncFor", "path": "library/ast#ast.AsyncFor", "type": "Language", "text": " \nclass ast.AsyncFor(target, iter, body, orelse, type_comment)  \nclass ast.AsyncWith(items, body, type_comment)  \nasync for loops and async with context managers. They have the same fields as For and With, respectively. Only valid in the body of an AsyncFunctionDef. \n"}, {"name": "ast.AsyncFunctionDef", "path": "library/ast#ast.AsyncFunctionDef", "type": "Language", "text": " \nclass ast.AsyncFunctionDef(name, args, body, decorator_list, returns, type_comment)  \nAn async def function definition. Has the same fields as FunctionDef. \n"}, {"name": "ast.AsyncWith", "path": "library/ast#ast.AsyncWith", "type": "Language", "text": " \nclass ast.AsyncFor(target, iter, body, orelse, type_comment)  \nclass ast.AsyncWith(items, body, type_comment)  \nasync for loops and async with context managers. They have the same fields as For and With, respectively. Only valid in the body of an AsyncFunctionDef. \n"}, {"name": "ast.Attribute", "path": "library/ast#ast.Attribute", "type": "Language", "text": " \nclass ast.Attribute(value, attr, ctx)  \nAttribute access, e.g. d.keys. value is a node, typically a Name. attr is a bare string giving the name of the attribute, and ctx is Load, Store or Del according to how the attribute is acted on. >>> print(ast.dump(ast.parse('snake.colour', mode='eval'), indent=4))\nExpression(\n    body=Attribute(\n        value=Name(id='snake', ctx=Load()),\n        attr='colour',\n        ctx=Load()))\n \n"}, {"name": "ast.AugAssign", "path": "library/ast#ast.AugAssign", "type": "Language", "text": " \nclass ast.AugAssign(target, op, value)  \nAugmented assignment, such as a += 1. In the following example, target is a Name node for x (with the Store context), op is Add, and value is a Constant with value for 1. The target attribute connot be of class Tuple or List, unlike the targets of Assign. >>> print(ast.dump(ast.parse('x += 2'), indent=4))\nModule(\n    body=[\n        AugAssign(\n            target=Name(id='x', ctx=Store()),\n            op=Add(),\n            value=Constant(value=2))],\n    type_ignores=[])\n \n"}, {"name": "ast.Await", "path": "library/ast#ast.Await", "type": "Language", "text": " \nclass ast.Await(value)  \nAn await expression. value is what it waits for. Only valid in the body of an AsyncFunctionDef. \n"}, {"name": "ast.BinOp", "path": "library/ast#ast.BinOp", "type": "Language", "text": " \nclass ast.BinOp(left, op, right)  \nA binary operation (like addition or division). op is the operator, and left and right are any expression nodes. >>> print(ast.dump(ast.parse('x + y', mode='eval'), indent=4))\nExpression(\n    body=BinOp(\n        left=Name(id='x', ctx=Load()),\n        op=Add(),\n        right=Name(id='y', ctx=Load())))\n \n"}, {"name": "ast.BitAnd", "path": "library/ast#ast.BitAnd", "type": "Language", "text": " \nclass ast.Add  \nclass ast.Sub  \nclass ast.Mult  \nclass ast.Div  \nclass ast.FloorDiv  \nclass ast.Mod  \nclass ast.Pow  \nclass ast.LShift  \nclass ast.RShift  \nclass ast.BitOr  \nclass ast.BitXor  \nclass ast.BitAnd  \nclass ast.MatMult  \nBinary operator tokens. \n"}, {"name": "ast.BitOr", "path": "library/ast#ast.BitOr", "type": "Language", "text": " \nclass ast.Add  \nclass ast.Sub  \nclass ast.Mult  \nclass ast.Div  \nclass ast.FloorDiv  \nclass ast.Mod  \nclass ast.Pow  \nclass ast.LShift  \nclass ast.RShift  \nclass ast.BitOr  \nclass ast.BitXor  \nclass ast.BitAnd  \nclass ast.MatMult  \nBinary operator tokens. \n"}, {"name": "ast.BitXor", "path": "library/ast#ast.BitXor", "type": "Language", "text": " \nclass ast.Add  \nclass ast.Sub  \nclass ast.Mult  \nclass ast.Div  \nclass ast.FloorDiv  \nclass ast.Mod  \nclass ast.Pow  \nclass ast.LShift  \nclass ast.RShift  \nclass ast.BitOr  \nclass ast.BitXor  \nclass ast.BitAnd  \nclass ast.MatMult  \nBinary operator tokens. \n"}, {"name": "ast.BoolOp", "path": "library/ast#ast.BoolOp", "type": "Language", "text": " \nclass ast.BoolOp(op, values)  \nA boolean operation, \u2018or\u2019 or \u2018and\u2019. op is Or or And. values are the values involved. Consecutive operations with the same operator, such as a or b or c, are collapsed into one node with several values. This doesn\u2019t include not, which is a UnaryOp. >>> print(ast.dump(ast.parse('x or y', mode='eval'), indent=4))\nExpression(\n    body=BoolOp(\n        op=Or(),\n        values=[\n            Name(id='x', ctx=Load()),\n            Name(id='y', ctx=Load())]))\n \n"}, {"name": "ast.Break", "path": "library/ast#ast.Break", "type": "Language", "text": " \nclass ast.Break  \nclass ast.Continue  \nThe break and continue statements. >>> print(ast.dump(ast.parse(\"\"\"\\\n... for a in b:\n...     if a > 5:\n...         break\n...     else:\n...         continue\n...\n... \"\"\"), indent=4))\nModule(\n    body=[\n        For(\n            target=Name(id='a', ctx=Store()),\n            iter=Name(id='b', ctx=Load()),\n            body=[\n                If(\n                    test=Compare(\n                        left=Name(id='a', ctx=Load()),\n                        ops=[\n                            Gt()],\n                        comparators=[\n                            Constant(value=5)]),\n                    body=[\n                        Break()],\n                    orelse=[\n                        Continue()])],\n            orelse=[])],\n    type_ignores=[])\n \n"}, {"name": "ast.Call", "path": "library/ast#ast.Call", "type": "Language", "text": " \nclass ast.Call(func, args, keywords, starargs, kwargs)  \nA function call. func is the function, which will often be a Name or Attribute object. Of the arguments:  \nargs holds a list of the arguments passed by position. \nkeywords holds a list of keyword objects representing arguments passed by keyword.  When creating a Call node, args and keywords are required, but they can be empty lists. starargs and kwargs are optional. >>> print(ast.dump(ast.parse('func(a, b=c, *d, **e)', mode='eval'), indent=4))\nExpression(\n    body=Call(\n        func=Name(id='func', ctx=Load()),\n        args=[\n            Name(id='a', ctx=Load()),\n            Starred(\n                value=Name(id='d', ctx=Load()),\n                ctx=Load())],\n        keywords=[\n            keyword(\n                arg='b',\n                value=Name(id='c', ctx=Load())),\n            keyword(\n                value=Name(id='e', ctx=Load()))]))\n \n"}, {"name": "ast.ClassDef", "path": "library/ast#ast.ClassDef", "type": "Language", "text": " \nclass ast.ClassDef(name, bases, keywords, starargs, kwargs, body, decorator_list)  \nA class definition.  \nname is a raw string for the class name \nbases is a list of nodes for explicitly specified base classes. \nkeywords is a list of keyword nodes, principally for \u2018metaclass\u2019. Other keywords will be passed to the metaclass, as per PEP-3115. \nstarargs and kwargs are each a single node, as in a function call. starargs will be expanded to join the list of base classes, and kwargs will be passed to the metaclass. \nbody is a list of nodes representing the code within the class definition. \ndecorator_list is a list of nodes, as in FunctionDef.  >>> print(ast.dump(ast.parse(\"\"\"\\\n... @decorator1\n... @decorator2\n... class Foo(base1, base2, metaclass=meta):\n...     pass\n... \"\"\"), indent=4))\nModule(\n    body=[\n        ClassDef(\n            name='Foo',\n            bases=[\n                Name(id='base1', ctx=Load()),\n                Name(id='base2', ctx=Load())],\n            keywords=[\n                keyword(\n                    arg='metaclass',\n                    value=Name(id='meta', ctx=Load()))],\n            body=[\n                Pass()],\n            decorator_list=[\n                Name(id='decorator1', ctx=Load()),\n                Name(id='decorator2', ctx=Load())])],\n    type_ignores=[])\n \n"}, {"name": "ast.Compare", "path": "library/ast#ast.Compare", "type": "Language", "text": " \nclass ast.Compare(left, ops, comparators)  \nA comparison of two or more values. left is the first value in the comparison, ops the list of operators, and comparators the list of values after the first element in the comparison. >>> print(ast.dump(ast.parse('1 <= a < 10', mode='eval'), indent=4))\nExpression(\n    body=Compare(\n        left=Constant(value=1),\n        ops=[\n            LtE(),\n            Lt()],\n        comparators=[\n            Name(id='a', ctx=Load()),\n            Constant(value=10)]))\n \n"}, {"name": "ast.comprehension", "path": "library/ast#ast.comprehension", "type": "Language", "text": " \nclass ast.comprehension(target, iter, ifs, is_async)  \nOne for clause in a comprehension. target is the reference to use for each element - typically a Name or Tuple node. iter is the object to iterate over. ifs is a list of test expressions: each for clause can have multiple ifs. is_async indicates a comprehension is asynchronous (using an async for instead of for). The value is an integer (0 or 1). >>> print(ast.dump(ast.parse('[ord(c) for line in file for c in line]', mode='eval'),\n...                indent=4)) # Multiple comprehensions in one.\nExpression(\n    body=ListComp(\n        elt=Call(\n            func=Name(id='ord', ctx=Load()),\n            args=[\n                Name(id='c', ctx=Load())],\n            keywords=[]),\n        generators=[\n            comprehension(\n                target=Name(id='line', ctx=Store()),\n                iter=Name(id='file', ctx=Load()),\n                ifs=[],\n                is_async=0),\n            comprehension(\n                target=Name(id='c', ctx=Store()),\n                iter=Name(id='line', ctx=Load()),\n                ifs=[],\n                is_async=0)]))\n\n>>> print(ast.dump(ast.parse('(n**2 for n in it if n>5 if n<10)', mode='eval'),\n...                indent=4)) # generator comprehension\nExpression(\n    body=GeneratorExp(\n        elt=BinOp(\n            left=Name(id='n', ctx=Load()),\n            op=Pow(),\n            right=Constant(value=2)),\n        generators=[\n            comprehension(\n                target=Name(id='n', ctx=Store()),\n                iter=Name(id='it', ctx=Load()),\n                ifs=[\n                    Compare(\n                        left=Name(id='n', ctx=Load()),\n                        ops=[\n                            Gt()],\n                        comparators=[\n                            Constant(value=5)]),\n                    Compare(\n                        left=Name(id='n', ctx=Load()),\n                        ops=[\n                            Lt()],\n                        comparators=[\n                            Constant(value=10)])],\n                is_async=0)]))\n\n>>> print(ast.dump(ast.parse('[i async for i in soc]', mode='eval'),\n...                indent=4)) # Async comprehension\nExpression(\n    body=ListComp(\n        elt=Name(id='i', ctx=Load()),\n        generators=[\n            comprehension(\n                target=Name(id='i', ctx=Store()),\n                iter=Name(id='soc', ctx=Load()),\n                ifs=[],\n                is_async=1)]))\n \n"}, {"name": "ast.Constant", "path": "library/ast#ast.Constant", "type": "Language", "text": " \nclass ast.Constant(value)  \nA constant value. The value attribute of the Constant literal contains the Python object it represents. The values represented can be simple types such as a number, string or None, but also immutable container types (tuples and frozensets) if all of their elements are constant. >>> print(ast.dump(ast.parse('123', mode='eval'), indent=4))\nExpression(\n    body=Constant(value=123))\n \n"}, {"name": "ast.Continue", "path": "library/ast#ast.Continue", "type": "Language", "text": " \nclass ast.Break  \nclass ast.Continue  \nThe break and continue statements. >>> print(ast.dump(ast.parse(\"\"\"\\\n... for a in b:\n...     if a > 5:\n...         break\n...     else:\n...         continue\n...\n... \"\"\"), indent=4))\nModule(\n    body=[\n        For(\n            target=Name(id='a', ctx=Store()),\n            iter=Name(id='b', ctx=Load()),\n            body=[\n                If(\n                    test=Compare(\n                        left=Name(id='a', ctx=Load()),\n                        ops=[\n                            Gt()],\n                        comparators=[\n                            Constant(value=5)]),\n                    body=[\n                        Break()],\n                    orelse=[\n                        Continue()])],\n            orelse=[])],\n    type_ignores=[])\n \n"}, {"name": "ast.copy_location()", "path": "library/ast#ast.copy_location", "type": "Language", "text": " \nast.copy_location(new_node, old_node)  \nCopy source location (lineno, col_offset, end_lineno, and end_col_offset) from old_node to new_node if possible, and return new_node. \n"}, {"name": "ast.Del", "path": "library/ast#ast.Del", "type": "Language", "text": " \nclass ast.Load  \nclass ast.Store  \nclass ast.Del  \nVariable references can be used to load the value of a variable, to assign a new value to it, or to delete it. Variable references are given a context to distinguish these cases. >>> print(ast.dump(ast.parse('a'), indent=4))\nModule(\n    body=[\n        Expr(\n            value=Name(id='a', ctx=Load()))],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('a = 1'), indent=4))\nModule(\n    body=[\n        Assign(\n            targets=[\n                Name(id='a', ctx=Store())],\n            value=Constant(value=1))],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('del a'), indent=4))\nModule(\n    body=[\n        Delete(\n            targets=[\n                Name(id='a', ctx=Del())])],\n    type_ignores=[])\n \n"}, {"name": "ast.Delete", "path": "library/ast#ast.Delete", "type": "Language", "text": " \nclass ast.Delete(targets)  \nRepresents a del statement. targets is a list of nodes, such as Name, Attribute or Subscript nodes. >>> print(ast.dump(ast.parse('del x,y,z'), indent=4))\nModule(\n    body=[\n        Delete(\n            targets=[\n                Name(id='x', ctx=Del()),\n                Name(id='y', ctx=Del()),\n                Name(id='z', ctx=Del())])],\n    type_ignores=[])\n \n"}, {"name": "ast.Dict", "path": "library/ast#ast.Dict", "type": "Language", "text": " \nclass ast.Dict(keys, values)  \nA dictionary. keys and values hold lists of nodes representing the keys and the values respectively, in matching order (what would be returned when calling dictionary.keys() and dictionary.values()). When doing dictionary unpacking using dictionary literals the expression to be expanded goes in the values list, with a None at the corresponding position in keys. >>> print(ast.dump(ast.parse('{\"a\":1, **d}', mode='eval'), indent=4))\nExpression(\n    body=Dict(\n        keys=[\n            Constant(value='a'),\n            None],\n        values=[\n            Constant(value=1),\n            Name(id='d', ctx=Load())]))\n \n"}, {"name": "ast.DictComp", "path": "library/ast#ast.DictComp", "type": "Language", "text": " \nclass ast.ListComp(elt, generators)  \nclass ast.SetComp(elt, generators)  \nclass ast.GeneratorExp(elt, generators)  \nclass ast.DictComp(key, value, generators)  \nList and set comprehensions, generator expressions, and dictionary comprehensions. elt (or key and value) is a single node representing the part that will be evaluated for each item. generators is a list of comprehension nodes. >>> print(ast.dump(ast.parse('[x for x in numbers]', mode='eval'), indent=4))\nExpression(\n    body=ListComp(\n        elt=Name(id='x', ctx=Load()),\n        generators=[\n            comprehension(\n                target=Name(id='x', ctx=Store()),\n                iter=Name(id='numbers', ctx=Load()),\n                ifs=[],\n                is_async=0)]))\n>>> print(ast.dump(ast.parse('{x: x**2 for x in numbers}', mode='eval'), indent=4))\nExpression(\n    body=DictComp(\n        key=Name(id='x', ctx=Load()),\n        value=BinOp(\n            left=Name(id='x', ctx=Load()),\n            op=Pow(),\n            right=Constant(value=2)),\n        generators=[\n            comprehension(\n                target=Name(id='x', ctx=Store()),\n                iter=Name(id='numbers', ctx=Load()),\n                ifs=[],\n                is_async=0)]))\n>>> print(ast.dump(ast.parse('{x for x in numbers}', mode='eval'), indent=4))\nExpression(\n    body=SetComp(\n        elt=Name(id='x', ctx=Load()),\n        generators=[\n            comprehension(\n                target=Name(id='x', ctx=Store()),\n                iter=Name(id='numbers', ctx=Load()),\n                ifs=[],\n                is_async=0)]))\n \n"}, {"name": "ast.Div", "path": "library/ast#ast.Div", "type": "Language", "text": " \nclass ast.Add  \nclass ast.Sub  \nclass ast.Mult  \nclass ast.Div  \nclass ast.FloorDiv  \nclass ast.Mod  \nclass ast.Pow  \nclass ast.LShift  \nclass ast.RShift  \nclass ast.BitOr  \nclass ast.BitXor  \nclass ast.BitAnd  \nclass ast.MatMult  \nBinary operator tokens. \n"}, {"name": "ast.dump()", "path": "library/ast#ast.dump", "type": "Language", "text": " \nast.dump(node, annotate_fields=True, include_attributes=False, *, indent=None)  \nReturn a formatted dump of the tree in node. This is mainly useful for debugging purposes. If annotate_fields is true (by default), the returned string will show the names and the values for fields. If annotate_fields is false, the result string will be more compact by omitting unambiguous field names. Attributes such as line numbers and column offsets are not dumped by default. If this is wanted, include_attributes can be set to true. If indent is a non-negative integer or string, then the tree will be pretty-printed with that indent level. An indent level of 0, negative, or \"\" will only insert newlines. None (the default) selects the single line representation. Using a positive integer indent indents that many spaces per level. If indent is a string (such as \"\\t\"), that string is used to indent each level.  Changed in version 3.9: Added the indent option.  \n"}, {"name": "ast.Eq", "path": "library/ast#ast.Eq", "type": "Language", "text": " \nclass ast.Eq  \nclass ast.NotEq  \nclass ast.Lt  \nclass ast.LtE  \nclass ast.Gt  \nclass ast.GtE  \nclass ast.Is  \nclass ast.IsNot  \nclass ast.In  \nclass ast.NotIn  \nComparison operator tokens. \n"}, {"name": "ast.ExceptHandler", "path": "library/ast#ast.ExceptHandler", "type": "Language", "text": " \nclass ast.ExceptHandler(type, name, body)  \nA single except clause. type is the exception type it will match, typically a Name node (or None for a catch-all except: clause). name is a raw string for the name to hold the exception, or None if the clause doesn\u2019t have as foo. body is a list of nodes. >>> print(ast.dump(ast.parse(\"\"\"\\\n... try:\n...     a + 1\n... except TypeError:\n...     pass\n... \"\"\"), indent=4))\nModule(\n    body=[\n        Try(\n            body=[\n                Expr(\n                    value=BinOp(\n                        left=Name(id='a', ctx=Load()),\n                        op=Add(),\n                        right=Constant(value=1)))],\n            handlers=[\n                ExceptHandler(\n                    type=Name(id='TypeError', ctx=Load()),\n                    body=[\n                        Pass()])],\n            orelse=[],\n            finalbody=[])],\n    type_ignores=[])\n \n"}, {"name": "ast.Expr", "path": "library/ast#ast.Expr", "type": "Language", "text": " \nclass ast.Expr(value)  \nWhen an expression, such as a function call, appears as a statement by itself with its return value not used or stored, it is wrapped in this container. value holds one of the other nodes in this section, a Constant, a Name, a Lambda, a Yield or YieldFrom node. >>> print(ast.dump(ast.parse('-a'), indent=4))\nModule(\n    body=[\n        Expr(\n            value=UnaryOp(\n                op=USub(),\n                operand=Name(id='a', ctx=Load())))],\n    type_ignores=[])\n \n"}, {"name": "ast.fix_missing_locations()", "path": "library/ast#ast.fix_missing_locations", "type": "Language", "text": " \nast.fix_missing_locations(node)  \nWhen you compile a node tree with compile(), the compiler expects lineno and col_offset attributes for every node that supports them. This is rather tedious to fill in for generated nodes, so this helper adds these attributes recursively where not already set, by setting them to the values of the parent node. It works recursively starting at node. \n"}, {"name": "ast.FloorDiv", "path": "library/ast#ast.FloorDiv", "type": "Language", "text": " \nclass ast.Add  \nclass ast.Sub  \nclass ast.Mult  \nclass ast.Div  \nclass ast.FloorDiv  \nclass ast.Mod  \nclass ast.Pow  \nclass ast.LShift  \nclass ast.RShift  \nclass ast.BitOr  \nclass ast.BitXor  \nclass ast.BitAnd  \nclass ast.MatMult  \nBinary operator tokens. \n"}, {"name": "ast.For", "path": "library/ast#ast.For", "type": "Language", "text": " \nclass ast.For(target, iter, body, orelse, type_comment)  \nA for loop. target holds the variable(s) the loop assigns to, as a single Name, Tuple or List node. iter holds the item to be looped over, again as a single node. body and orelse contain lists of nodes to execute. Those in orelse are executed if the loop finishes normally, rather than via a break statement.  \ntype_comment  \ntype_comment is an optional string with the type annotation as a comment. \n >>> print(ast.dump(ast.parse(\"\"\"\n... for x in y:\n...     ...\n... else:\n...     ...\n... \"\"\"), indent=4))\nModule(\n    body=[\n        For(\n            target=Name(id='x', ctx=Store()),\n            iter=Name(id='y', ctx=Load()),\n            body=[\n                Expr(\n                    value=Constant(value=Ellipsis))],\n            orelse=[\n                Expr(\n                    value=Constant(value=Ellipsis))])],\n    type_ignores=[])\n \n"}, {"name": "ast.For.type_comment", "path": "library/ast#ast.For.type_comment", "type": "Language", "text": " \ntype_comment  \ntype_comment is an optional string with the type annotation as a comment. \n"}, {"name": "ast.FormattedValue", "path": "library/ast#ast.FormattedValue", "type": "Language", "text": " \nclass ast.FormattedValue(value, conversion, format_spec)  \nNode representing a single formatting field in an f-string. If the string contains a single formatting field and nothing else the node can be isolated otherwise it appears in JoinedStr.  \nvalue is any expression node (such as a literal, a variable, or a function call). \nconversion is an integer:  -1: no formatting 115: !s string formatting 114: !r repr formatting 97: !a ascii formatting   \nformat_spec is a JoinedStr node representing the formatting of the value, or None if no format was specified. Both conversion and format_spec can be set at the same time.  \n"}, {"name": "ast.FunctionDef", "path": "library/ast#ast.FunctionDef", "type": "Language", "text": " \nclass ast.FunctionDef(name, args, body, decorator_list, returns, type_comment)  \nA function definition.  \nname is a raw string of the function name. \nargs is a arguments node. \nbody is the list of nodes inside the function. \ndecorator_list is the list of decorators to be applied, stored outermost first (i.e. the first in the list will be applied last). \nreturns is the return annotation.   \ntype_comment  \ntype_comment is an optional string with the type annotation as a comment. \n \n"}, {"name": "ast.FunctionDef.type_comment", "path": "library/ast#ast.FunctionDef.type_comment", "type": "Language", "text": " \ntype_comment  \ntype_comment is an optional string with the type annotation as a comment. \n"}, {"name": "ast.GeneratorExp", "path": "library/ast#ast.GeneratorExp", "type": "Language", "text": " \nclass ast.ListComp(elt, generators)  \nclass ast.SetComp(elt, generators)  \nclass ast.GeneratorExp(elt, generators)  \nclass ast.DictComp(key, value, generators)  \nList and set comprehensions, generator expressions, and dictionary comprehensions. elt (or key and value) is a single node representing the part that will be evaluated for each item. generators is a list of comprehension nodes. >>> print(ast.dump(ast.parse('[x for x in numbers]', mode='eval'), indent=4))\nExpression(\n    body=ListComp(\n        elt=Name(id='x', ctx=Load()),\n        generators=[\n            comprehension(\n                target=Name(id='x', ctx=Store()),\n                iter=Name(id='numbers', ctx=Load()),\n                ifs=[],\n                is_async=0)]))\n>>> print(ast.dump(ast.parse('{x: x**2 for x in numbers}', mode='eval'), indent=4))\nExpression(\n    body=DictComp(\n        key=Name(id='x', ctx=Load()),\n        value=BinOp(\n            left=Name(id='x', ctx=Load()),\n            op=Pow(),\n            right=Constant(value=2)),\n        generators=[\n            comprehension(\n                target=Name(id='x', ctx=Store()),\n                iter=Name(id='numbers', ctx=Load()),\n                ifs=[],\n                is_async=0)]))\n>>> print(ast.dump(ast.parse('{x for x in numbers}', mode='eval'), indent=4))\nExpression(\n    body=SetComp(\n        elt=Name(id='x', ctx=Load()),\n        generators=[\n            comprehension(\n                target=Name(id='x', ctx=Store()),\n                iter=Name(id='numbers', ctx=Load()),\n                ifs=[],\n                is_async=0)]))\n \n"}, {"name": "ast.get_docstring()", "path": "library/ast#ast.get_docstring", "type": "Language", "text": " \nast.get_docstring(node, clean=True)  \nReturn the docstring of the given node (which must be a FunctionDef, AsyncFunctionDef, ClassDef, or Module node), or None if it has no docstring. If clean is true, clean up the docstring\u2019s indentation with inspect.cleandoc().  Changed in version 3.5: AsyncFunctionDef is now supported.  \n"}, {"name": "ast.get_source_segment()", "path": "library/ast#ast.get_source_segment", "type": "Language", "text": " \nast.get_source_segment(source, node, *, padded=False)  \nGet source code segment of the source that generated node. If some location information (lineno, end_lineno, col_offset, or end_col_offset) is missing, return None. If padded is True, the first line of a multi-line statement will be padded with spaces to match its original position.  New in version 3.8.  \n"}, {"name": "ast.Global", "path": "library/ast#ast.Global", "type": "Language", "text": " \nclass ast.Global(names)  \nclass ast.Nonlocal(names)  \nglobal and nonlocal statements. names is a list of raw strings. >>> print(ast.dump(ast.parse('global x,y,z'), indent=4))\nModule(\n    body=[\n        Global(\n            names=[\n                'x',\n                'y',\n                'z'])],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('nonlocal x,y,z'), indent=4))\nModule(\n    body=[\n        Nonlocal(\n            names=[\n                'x',\n                'y',\n                'z'])],\n    type_ignores=[])\n \n"}, {"name": "ast.Gt", "path": "library/ast#ast.Gt", "type": "Language", "text": " \nclass ast.Eq  \nclass ast.NotEq  \nclass ast.Lt  \nclass ast.LtE  \nclass ast.Gt  \nclass ast.GtE  \nclass ast.Is  \nclass ast.IsNot  \nclass ast.In  \nclass ast.NotIn  \nComparison operator tokens. \n"}, {"name": "ast.GtE", "path": "library/ast#ast.GtE", "type": "Language", "text": " \nclass ast.Eq  \nclass ast.NotEq  \nclass ast.Lt  \nclass ast.LtE  \nclass ast.Gt  \nclass ast.GtE  \nclass ast.Is  \nclass ast.IsNot  \nclass ast.In  \nclass ast.NotIn  \nComparison operator tokens. \n"}, {"name": "ast.If", "path": "library/ast#ast.If", "type": "Language", "text": " \nclass ast.If(test, body, orelse)  \nAn if statement. test holds a single node, such as a Compare node. body and orelse each hold a list of nodes. elif clauses don\u2019t have a special representation in the AST, but rather appear as extra If nodes within the orelse section of the previous one. >>> print(ast.dump(ast.parse(\"\"\"\n... if x:\n...    ...\n... elif y:\n...    ...\n... else:\n...    ...\n... \"\"\"), indent=4))\nModule(\n    body=[\n        If(\n            test=Name(id='x', ctx=Load()),\n            body=[\n                Expr(\n                    value=Constant(value=Ellipsis))],\n            orelse=[\n                If(\n                    test=Name(id='y', ctx=Load()),\n                    body=[\n                        Expr(\n                            value=Constant(value=Ellipsis))],\n                    orelse=[\n                        Expr(\n                            value=Constant(value=Ellipsis))])])],\n    type_ignores=[])\n \n"}, {"name": "ast.IfExp", "path": "library/ast#ast.IfExp", "type": "Language", "text": " \nclass ast.IfExp(test, body, orelse)  \nAn expression such as a if b else c. Each field holds a single node, so in the following example, all three are Name nodes. >>> print(ast.dump(ast.parse('a if b else c', mode='eval'), indent=4))\nExpression(\n    body=IfExp(\n        test=Name(id='b', ctx=Load()),\n        body=Name(id='a', ctx=Load()),\n        orelse=Name(id='c', ctx=Load())))\n \n"}, {"name": "ast.Import", "path": "library/ast#ast.Import", "type": "Language", "text": " \nclass ast.Import(names)  \nAn import statement. names is a list of alias nodes. >>> print(ast.dump(ast.parse('import x,y,z'), indent=4))\nModule(\n    body=[\n        Import(\n            names=[\n                alias(name='x'),\n                alias(name='y'),\n                alias(name='z')])],\n    type_ignores=[])\n \n"}, {"name": "ast.ImportFrom", "path": "library/ast#ast.ImportFrom", "type": "Language", "text": " \nclass ast.ImportFrom(module, names, level)  \nRepresents from x import y. module is a raw string of the \u2018from\u2019 name, without any leading dots, or None for statements such as from . import foo. level is an integer holding the level of the relative import (0 means absolute import). >>> print(ast.dump(ast.parse('from y import x,y,z'), indent=4))\nModule(\n    body=[\n        ImportFrom(\n            module='y',\n            names=[\n                alias(name='x'),\n                alias(name='y'),\n                alias(name='z')],\n            level=0)],\n    type_ignores=[])\n \n"}, {"name": "ast.In", "path": "library/ast#ast.In", "type": "Language", "text": " \nclass ast.Eq  \nclass ast.NotEq  \nclass ast.Lt  \nclass ast.LtE  \nclass ast.Gt  \nclass ast.GtE  \nclass ast.Is  \nclass ast.IsNot  \nclass ast.In  \nclass ast.NotIn  \nComparison operator tokens. \n"}, {"name": "ast.increment_lineno()", "path": "library/ast#ast.increment_lineno", "type": "Language", "text": " \nast.increment_lineno(node, n=1)  \nIncrement the line number and end line number of each node in the tree starting at node by n. This is useful to \u201cmove code\u201d to a different location in a file. \n"}, {"name": "ast.Invert", "path": "library/ast#ast.Invert", "type": "Language", "text": " \nclass ast.UAdd  \nclass ast.USub  \nclass ast.Not  \nclass ast.Invert  \nUnary operator tokens. Not is the not keyword, Invert is the ~ operator. >>> print(ast.dump(ast.parse('not x', mode='eval'), indent=4))\nExpression(\n    body=UnaryOp(\n        op=Not(),\n        operand=Name(id='x', ctx=Load())))\n \n"}, {"name": "ast.Is", "path": "library/ast#ast.Is", "type": "Language", "text": " \nclass ast.Eq  \nclass ast.NotEq  \nclass ast.Lt  \nclass ast.LtE  \nclass ast.Gt  \nclass ast.GtE  \nclass ast.Is  \nclass ast.IsNot  \nclass ast.In  \nclass ast.NotIn  \nComparison operator tokens. \n"}, {"name": "ast.IsNot", "path": "library/ast#ast.IsNot", "type": "Language", "text": " \nclass ast.Eq  \nclass ast.NotEq  \nclass ast.Lt  \nclass ast.LtE  \nclass ast.Gt  \nclass ast.GtE  \nclass ast.Is  \nclass ast.IsNot  \nclass ast.In  \nclass ast.NotIn  \nComparison operator tokens. \n"}, {"name": "ast.iter_child_nodes()", "path": "library/ast#ast.iter_child_nodes", "type": "Language", "text": " \nast.iter_child_nodes(node)  \nYield all direct child nodes of node, that is, all fields that are nodes and all items of fields that are lists of nodes. \n"}, {"name": "ast.iter_fields()", "path": "library/ast#ast.iter_fields", "type": "Language", "text": " \nast.iter_fields(node)  \nYield a tuple of (fieldname, value) for each field in node._fields that is present on node. \n"}, {"name": "ast.JoinedStr", "path": "library/ast#ast.JoinedStr", "type": "Language", "text": " \nclass ast.JoinedStr(values)  \nAn f-string, comprising a series of FormattedValue and Constant nodes. >>> print(ast.dump(ast.parse('f\"sin({a}) is {sin(a):.3}\"', mode='eval'), indent=4))\nExpression(\n    body=JoinedStr(\n        values=[\n            Constant(value='sin('),\n            FormattedValue(\n                value=Name(id='a', ctx=Load()),\n                conversion=-1),\n            Constant(value=') is '),\n            FormattedValue(\n                value=Call(\n                    func=Name(id='sin', ctx=Load()),\n                    args=[\n                        Name(id='a', ctx=Load())],\n                    keywords=[]),\n                conversion=-1,\n                format_spec=JoinedStr(\n                    values=[\n                        Constant(value='.3')]))]))\n \n"}, {"name": "ast.keyword", "path": "library/ast#ast.keyword", "type": "Language", "text": " \nclass ast.keyword(arg, value)  \nA keyword argument to a function call or class definition. arg is a raw string of the parameter name, value is a node to pass in. \n"}, {"name": "ast.Lambda", "path": "library/ast#ast.Lambda", "type": "Language", "text": " \nclass ast.Lambda(args, body)  \nlambda is a minimal function definition that can be used inside an expression. Unlike FunctionDef, body holds a single node. >>> print(ast.dump(ast.parse('lambda x,y: ...'), indent=4))\nModule(\n    body=[\n        Expr(\n            value=Lambda(\n                args=arguments(\n                    posonlyargs=[],\n                    args=[\n                        arg(arg='x'),\n                        arg(arg='y')],\n                    kwonlyargs=[],\n                    kw_defaults=[],\n                    defaults=[]),\n                body=Constant(value=Ellipsis)))],\n    type_ignores=[])\n \n"}, {"name": "ast.List", "path": "library/ast#ast.List", "type": "Language", "text": " \nclass ast.List(elts, ctx)  \nclass ast.Tuple(elts, ctx)  \nA list or tuple. elts holds a list of nodes representing the elements. ctx is Store if the container is an assignment target (i.e. (x,y)=something), and Load otherwise. >>> print(ast.dump(ast.parse('[1, 2, 3]', mode='eval'), indent=4))\nExpression(\n    body=List(\n        elts=[\n            Constant(value=1),\n            Constant(value=2),\n            Constant(value=3)],\n        ctx=Load()))\n>>> print(ast.dump(ast.parse('(1, 2, 3)', mode='eval'), indent=4))\nExpression(\n    body=Tuple(\n        elts=[\n            Constant(value=1),\n            Constant(value=2),\n            Constant(value=3)],\n        ctx=Load()))\n \n"}, {"name": "ast.ListComp", "path": "library/ast#ast.ListComp", "type": "Language", "text": " \nclass ast.ListComp(elt, generators)  \nclass ast.SetComp(elt, generators)  \nclass ast.GeneratorExp(elt, generators)  \nclass ast.DictComp(key, value, generators)  \nList and set comprehensions, generator expressions, and dictionary comprehensions. elt (or key and value) is a single node representing the part that will be evaluated for each item. generators is a list of comprehension nodes. >>> print(ast.dump(ast.parse('[x for x in numbers]', mode='eval'), indent=4))\nExpression(\n    body=ListComp(\n        elt=Name(id='x', ctx=Load()),\n        generators=[\n            comprehension(\n                target=Name(id='x', ctx=Store()),\n                iter=Name(id='numbers', ctx=Load()),\n                ifs=[],\n                is_async=0)]))\n>>> print(ast.dump(ast.parse('{x: x**2 for x in numbers}', mode='eval'), indent=4))\nExpression(\n    body=DictComp(\n        key=Name(id='x', ctx=Load()),\n        value=BinOp(\n            left=Name(id='x', ctx=Load()),\n            op=Pow(),\n            right=Constant(value=2)),\n        generators=[\n            comprehension(\n                target=Name(id='x', ctx=Store()),\n                iter=Name(id='numbers', ctx=Load()),\n                ifs=[],\n                is_async=0)]))\n>>> print(ast.dump(ast.parse('{x for x in numbers}', mode='eval'), indent=4))\nExpression(\n    body=SetComp(\n        elt=Name(id='x', ctx=Load()),\n        generators=[\n            comprehension(\n                target=Name(id='x', ctx=Store()),\n                iter=Name(id='numbers', ctx=Load()),\n                ifs=[],\n                is_async=0)]))\n \n"}, {"name": "ast.literal_eval()", "path": "library/ast#ast.literal_eval", "type": "Language", "text": " \nast.literal_eval(node_or_string)  \nSafely evaluate an expression node or a string containing a Python literal or container display. The string or node provided may only consist of the following Python literal structures: strings, bytes, numbers, tuples, lists, dicts, sets, booleans, and None. This can be used for safely evaluating strings containing Python values from untrusted sources without the need to parse the values oneself. It is not capable of evaluating arbitrarily complex expressions, for example involving operators or indexing.  Warning It is possible to crash the Python interpreter with a sufficiently large/complex string due to stack depth limitations in Python\u2019s AST compiler.   Changed in version 3.2: Now allows bytes and set literals.   Changed in version 3.9: Now supports creating empty sets with 'set()'.  \n"}, {"name": "ast.Load", "path": "library/ast#ast.Load", "type": "Language", "text": " \nclass ast.Load  \nclass ast.Store  \nclass ast.Del  \nVariable references can be used to load the value of a variable, to assign a new value to it, or to delete it. Variable references are given a context to distinguish these cases. >>> print(ast.dump(ast.parse('a'), indent=4))\nModule(\n    body=[\n        Expr(\n            value=Name(id='a', ctx=Load()))],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('a = 1'), indent=4))\nModule(\n    body=[\n        Assign(\n            targets=[\n                Name(id='a', ctx=Store())],\n            value=Constant(value=1))],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('del a'), indent=4))\nModule(\n    body=[\n        Delete(\n            targets=[\n                Name(id='a', ctx=Del())])],\n    type_ignores=[])\n \n"}, {"name": "ast.LShift", "path": "library/ast#ast.LShift", "type": "Language", "text": " \nclass ast.Add  \nclass ast.Sub  \nclass ast.Mult  \nclass ast.Div  \nclass ast.FloorDiv  \nclass ast.Mod  \nclass ast.Pow  \nclass ast.LShift  \nclass ast.RShift  \nclass ast.BitOr  \nclass ast.BitXor  \nclass ast.BitAnd  \nclass ast.MatMult  \nBinary operator tokens. \n"}, {"name": "ast.Lt", "path": "library/ast#ast.Lt", "type": "Language", "text": " \nclass ast.Eq  \nclass ast.NotEq  \nclass ast.Lt  \nclass ast.LtE  \nclass ast.Gt  \nclass ast.GtE  \nclass ast.Is  \nclass ast.IsNot  \nclass ast.In  \nclass ast.NotIn  \nComparison operator tokens. \n"}, {"name": "ast.LtE", "path": "library/ast#ast.LtE", "type": "Language", "text": " \nclass ast.Eq  \nclass ast.NotEq  \nclass ast.Lt  \nclass ast.LtE  \nclass ast.Gt  \nclass ast.GtE  \nclass ast.Is  \nclass ast.IsNot  \nclass ast.In  \nclass ast.NotIn  \nComparison operator tokens. \n"}, {"name": "ast.MatMult", "path": "library/ast#ast.MatMult", "type": "Language", "text": " \nclass ast.Add  \nclass ast.Sub  \nclass ast.Mult  \nclass ast.Div  \nclass ast.FloorDiv  \nclass ast.Mod  \nclass ast.Pow  \nclass ast.LShift  \nclass ast.RShift  \nclass ast.BitOr  \nclass ast.BitXor  \nclass ast.BitAnd  \nclass ast.MatMult  \nBinary operator tokens. \n"}, {"name": "ast.Mod", "path": "library/ast#ast.Mod", "type": "Language", "text": " \nclass ast.Add  \nclass ast.Sub  \nclass ast.Mult  \nclass ast.Div  \nclass ast.FloorDiv  \nclass ast.Mod  \nclass ast.Pow  \nclass ast.LShift  \nclass ast.RShift  \nclass ast.BitOr  \nclass ast.BitXor  \nclass ast.BitAnd  \nclass ast.MatMult  \nBinary operator tokens. \n"}, {"name": "ast.Mult", "path": "library/ast#ast.Mult", "type": "Language", "text": " \nclass ast.Add  \nclass ast.Sub  \nclass ast.Mult  \nclass ast.Div  \nclass ast.FloorDiv  \nclass ast.Mod  \nclass ast.Pow  \nclass ast.LShift  \nclass ast.RShift  \nclass ast.BitOr  \nclass ast.BitXor  \nclass ast.BitAnd  \nclass ast.MatMult  \nBinary operator tokens. \n"}, {"name": "ast.Name", "path": "library/ast#ast.Name", "type": "Language", "text": " \nclass ast.Name(id, ctx)  \nA variable name. id holds the name as a string, and ctx is one of the following types. \n"}, {"name": "ast.NamedExpr", "path": "library/ast#ast.NamedExpr", "type": "Language", "text": " \nclass ast.NamedExpr(target, value)   A named expression. This AST node is produced by the assignment expressions operator (also known as the walrus operator). As opposed to the Assign node in which the first argument can be multiple nodes, in this case both target and value must be single nodes. >>> print(ast.dump(ast.parse('(x := 4)', mode='eval'), indent=4))\nExpression(\n    body=NamedExpr(\n        target=Name(id='x', ctx=Store()),\n        value=Constant(value=4)))\n \n"}, {"name": "ast.NodeTransformer", "path": "library/ast#ast.NodeTransformer", "type": "Language", "text": " \nclass ast.NodeTransformer  \nA NodeVisitor subclass that walks the abstract syntax tree and allows modification of nodes. The NodeTransformer will walk the AST and use the return value of the visitor methods to replace or remove the old node. If the return value of the visitor method is None, the node will be removed from its location, otherwise it is replaced with the return value. The return value may be the original node in which case no replacement takes place. Here is an example transformer that rewrites all occurrences of name lookups (foo) to data['foo']: class RewriteName(NodeTransformer):\n\n    def visit_Name(self, node):\n        return Subscript(\n            value=Name(id='data', ctx=Load()),\n            slice=Constant(value=node.id),\n            ctx=node.ctx\n        )\n Keep in mind that if the node you\u2019re operating on has child nodes you must either transform the child nodes yourself or call the generic_visit() method for the node first. For nodes that were part of a collection of statements (that applies to all statement nodes), the visitor may also return a list of nodes rather than just a single node. If NodeTransformer introduces new nodes (that weren\u2019t part of original tree) without giving them location information (such as lineno), fix_missing_locations() should be called with the new sub-tree to recalculate the location information: tree = ast.parse('foo', mode='eval')\nnew_tree = fix_missing_locations(RewriteName().visit(tree))\n Usually you use the transformer like this: node = YourTransformer().visit(node)\n \n"}, {"name": "ast.NodeVisitor", "path": "library/ast#ast.NodeVisitor", "type": "Language", "text": " \nclass ast.NodeVisitor  \nA node visitor base class that walks the abstract syntax tree and calls a visitor function for every node found. This function may return a value which is forwarded by the visit() method. This class is meant to be subclassed, with the subclass adding visitor methods.  \nvisit(node)  \nVisit a node. The default implementation calls the method called self.visit_classname where classname is the name of the node class, or generic_visit() if that method doesn\u2019t exist. \n  \ngeneric_visit(node)  \nThis visitor calls visit() on all children of the node. Note that child nodes of nodes that have a custom visitor method won\u2019t be visited unless the visitor calls generic_visit() or visits them itself. \n Don\u2019t use the NodeVisitor if you want to apply changes to nodes during traversal. For this a special visitor exists (NodeTransformer) that allows modifications.  Deprecated since version 3.8: Methods visit_Num(), visit_Str(), visit_Bytes(), visit_NameConstant() and visit_Ellipsis() are deprecated now and will not be called in future Python versions. Add the visit_Constant() method to handle all constant nodes.  \n"}, {"name": "ast.NodeVisitor.generic_visit()", "path": "library/ast#ast.NodeVisitor.generic_visit", "type": "Language", "text": " \ngeneric_visit(node)  \nThis visitor calls visit() on all children of the node. Note that child nodes of nodes that have a custom visitor method won\u2019t be visited unless the visitor calls generic_visit() or visits them itself. \n"}, {"name": "ast.NodeVisitor.visit()", "path": "library/ast#ast.NodeVisitor.visit", "type": "Language", "text": " \nvisit(node)  \nVisit a node. The default implementation calls the method called self.visit_classname where classname is the name of the node class, or generic_visit() if that method doesn\u2019t exist. \n"}, {"name": "ast.Nonlocal", "path": "library/ast#ast.Nonlocal", "type": "Language", "text": " \nclass ast.Global(names)  \nclass ast.Nonlocal(names)  \nglobal and nonlocal statements. names is a list of raw strings. >>> print(ast.dump(ast.parse('global x,y,z'), indent=4))\nModule(\n    body=[\n        Global(\n            names=[\n                'x',\n                'y',\n                'z'])],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('nonlocal x,y,z'), indent=4))\nModule(\n    body=[\n        Nonlocal(\n            names=[\n                'x',\n                'y',\n                'z'])],\n    type_ignores=[])\n \n"}, {"name": "ast.Not", "path": "library/ast#ast.Not", "type": "Language", "text": " \nclass ast.UAdd  \nclass ast.USub  \nclass ast.Not  \nclass ast.Invert  \nUnary operator tokens. Not is the not keyword, Invert is the ~ operator. >>> print(ast.dump(ast.parse('not x', mode='eval'), indent=4))\nExpression(\n    body=UnaryOp(\n        op=Not(),\n        operand=Name(id='x', ctx=Load())))\n \n"}, {"name": "ast.NotEq", "path": "library/ast#ast.NotEq", "type": "Language", "text": " \nclass ast.Eq  \nclass ast.NotEq  \nclass ast.Lt  \nclass ast.LtE  \nclass ast.Gt  \nclass ast.GtE  \nclass ast.Is  \nclass ast.IsNot  \nclass ast.In  \nclass ast.NotIn  \nComparison operator tokens. \n"}, {"name": "ast.NotIn", "path": "library/ast#ast.NotIn", "type": "Language", "text": " \nclass ast.Eq  \nclass ast.NotEq  \nclass ast.Lt  \nclass ast.LtE  \nclass ast.Gt  \nclass ast.GtE  \nclass ast.Is  \nclass ast.IsNot  \nclass ast.In  \nclass ast.NotIn  \nComparison operator tokens. \n"}, {"name": "ast.Or", "path": "library/ast#ast.Or", "type": "Language", "text": " \nclass ast.And  \nclass ast.Or  \nBoolean operator tokens. \n"}, {"name": "ast.parse()", "path": "library/ast#ast.parse", "type": "Language", "text": " \nast.parse(source, filename='<unknown>', mode='exec', *, type_comments=False, feature_version=None)  \nParse the source into an AST node. Equivalent to compile(source,\nfilename, mode, ast.PyCF_ONLY_AST). If type_comments=True is given, the parser is modified to check and return type comments as specified by PEP 484 and PEP 526. This is equivalent to adding ast.PyCF_TYPE_COMMENTS to the flags passed to compile(). This will report syntax errors for misplaced type comments. Without this flag, type comments will be ignored, and the type_comment field on selected AST nodes will always be None. In addition, the locations of # type:\nignore comments will be returned as the type_ignores attribute of Module (otherwise it is always an empty list). In addition, if mode is 'func_type', the input syntax is modified to correspond to PEP 484 \u201csignature type comments\u201d, e.g. (str, int) -> List[str]. Also, setting feature_version to a tuple (major, minor) will attempt to parse using that Python version\u2019s grammar. Currently major must equal to 3. For example, setting feature_version=(3, 4) will allow the use of async and await as variable names. The lowest supported version is (3, 4); the highest is sys.version_info[0:2].  Warning It is possible to crash the Python interpreter with a sufficiently large/complex string due to stack depth limitations in Python\u2019s AST compiler.   Changed in version 3.8: Added type_comments, mode='func_type' and feature_version.  \n"}, {"name": "ast.Pass", "path": "library/ast#ast.Pass", "type": "Language", "text": " \nclass ast.Pass  \nA pass statement. >>> print(ast.dump(ast.parse('pass'), indent=4))\nModule(\n    body=[\n        Pass()],\n    type_ignores=[])\n \n"}, {"name": "ast.Pow", "path": "library/ast#ast.Pow", "type": "Language", "text": " \nclass ast.Add  \nclass ast.Sub  \nclass ast.Mult  \nclass ast.Div  \nclass ast.FloorDiv  \nclass ast.Mod  \nclass ast.Pow  \nclass ast.LShift  \nclass ast.RShift  \nclass ast.BitOr  \nclass ast.BitXor  \nclass ast.BitAnd  \nclass ast.MatMult  \nBinary operator tokens. \n"}, {"name": "ast.PyCF_ALLOW_TOP_LEVEL_AWAIT", "path": "library/ast#ast.PyCF_ALLOW_TOP_LEVEL_AWAIT", "type": "Language", "text": " \nast.PyCF_ALLOW_TOP_LEVEL_AWAIT  \nEnables support for top-level await, async for, async with and async comprehensions.  New in version 3.8.  \n"}, {"name": "ast.PyCF_ONLY_AST", "path": "library/ast#ast.PyCF_ONLY_AST", "type": "Language", "text": " \nast.PyCF_ONLY_AST  \nGenerates and returns an abstract syntax tree instead of returning a compiled code object. \n"}, {"name": "ast.PyCF_TYPE_COMMENTS", "path": "library/ast#ast.PyCF_TYPE_COMMENTS", "type": "Language", "text": " \nast.PyCF_TYPE_COMMENTS  \nEnables support for PEP 484 and PEP 526 style type comments (# type: <type>, # type: ignore <stuff>).  New in version 3.8.  \n"}, {"name": "ast.Raise", "path": "library/ast#ast.Raise", "type": "Language", "text": " \nclass ast.Raise(exc, cause)  \nA raise statement. exc is the exception object to be raised, normally a Call or Name, or None for a standalone raise. cause is the optional part for y in raise x from y. >>> print(ast.dump(ast.parse('raise x from y'), indent=4))\nModule(\n    body=[\n        Raise(\n            exc=Name(id='x', ctx=Load()),\n            cause=Name(id='y', ctx=Load()))],\n    type_ignores=[])\n \n"}, {"name": "ast.Return", "path": "library/ast#ast.Return", "type": "Language", "text": " \nclass ast.Return(value)  \nA return statement. >>> print(ast.dump(ast.parse('return 4'), indent=4))\nModule(\n    body=[\n        Return(\n            value=Constant(value=4))],\n    type_ignores=[])\n \n"}, {"name": "ast.RShift", "path": "library/ast#ast.RShift", "type": "Language", "text": " \nclass ast.Add  \nclass ast.Sub  \nclass ast.Mult  \nclass ast.Div  \nclass ast.FloorDiv  \nclass ast.Mod  \nclass ast.Pow  \nclass ast.LShift  \nclass ast.RShift  \nclass ast.BitOr  \nclass ast.BitXor  \nclass ast.BitAnd  \nclass ast.MatMult  \nBinary operator tokens. \n"}, {"name": "ast.Set", "path": "library/ast#ast.Set", "type": "Language", "text": " \nclass ast.Set(elts)  \nA set. elts holds a list of nodes representing the set\u2019s elements. >>> print(ast.dump(ast.parse('{1, 2, 3}', mode='eval'), indent=4))\nExpression(\n    body=Set(\n        elts=[\n            Constant(value=1),\n            Constant(value=2),\n            Constant(value=3)]))\n \n"}, {"name": "ast.SetComp", "path": "library/ast#ast.SetComp", "type": "Language", "text": " \nclass ast.ListComp(elt, generators)  \nclass ast.SetComp(elt, generators)  \nclass ast.GeneratorExp(elt, generators)  \nclass ast.DictComp(key, value, generators)  \nList and set comprehensions, generator expressions, and dictionary comprehensions. elt (or key and value) is a single node representing the part that will be evaluated for each item. generators is a list of comprehension nodes. >>> print(ast.dump(ast.parse('[x for x in numbers]', mode='eval'), indent=4))\nExpression(\n    body=ListComp(\n        elt=Name(id='x', ctx=Load()),\n        generators=[\n            comprehension(\n                target=Name(id='x', ctx=Store()),\n                iter=Name(id='numbers', ctx=Load()),\n                ifs=[],\n                is_async=0)]))\n>>> print(ast.dump(ast.parse('{x: x**2 for x in numbers}', mode='eval'), indent=4))\nExpression(\n    body=DictComp(\n        key=Name(id='x', ctx=Load()),\n        value=BinOp(\n            left=Name(id='x', ctx=Load()),\n            op=Pow(),\n            right=Constant(value=2)),\n        generators=[\n            comprehension(\n                target=Name(id='x', ctx=Store()),\n                iter=Name(id='numbers', ctx=Load()),\n                ifs=[],\n                is_async=0)]))\n>>> print(ast.dump(ast.parse('{x for x in numbers}', mode='eval'), indent=4))\nExpression(\n    body=SetComp(\n        elt=Name(id='x', ctx=Load()),\n        generators=[\n            comprehension(\n                target=Name(id='x', ctx=Store()),\n                iter=Name(id='numbers', ctx=Load()),\n                ifs=[],\n                is_async=0)]))\n \n"}, {"name": "ast.Slice", "path": "library/ast#ast.Slice", "type": "Language", "text": " \nclass ast.Slice(lower, upper, step)  \nRegular slicing (on the form lower:upper or lower:upper:step). Can occur only inside the slice field of Subscript, either directly or as an element of Tuple. >>> print(ast.dump(ast.parse('l[1:2]', mode='eval'), indent=4))\nExpression(\n    body=Subscript(\n        value=Name(id='l', ctx=Load()),\n        slice=Slice(\n            lower=Constant(value=1),\n            upper=Constant(value=2)),\n        ctx=Load()))\n \n"}, {"name": "ast.Starred", "path": "library/ast#ast.Starred", "type": "Language", "text": " \nclass ast.Starred(value, ctx)  \nA *var variable reference. value holds the variable, typically a Name node. This type must be used when building a Call node with *args. >>> print(ast.dump(ast.parse('a, *b = it'), indent=4))\nModule(\n    body=[\n        Assign(\n            targets=[\n                Tuple(\n                    elts=[\n                        Name(id='a', ctx=Store()),\n                        Starred(\n                            value=Name(id='b', ctx=Store()),\n                            ctx=Store())],\n                    ctx=Store())],\n            value=Name(id='it', ctx=Load()))],\n    type_ignores=[])\n \n"}, {"name": "ast.Store", "path": "library/ast#ast.Store", "type": "Language", "text": " \nclass ast.Load  \nclass ast.Store  \nclass ast.Del  \nVariable references can be used to load the value of a variable, to assign a new value to it, or to delete it. Variable references are given a context to distinguish these cases. >>> print(ast.dump(ast.parse('a'), indent=4))\nModule(\n    body=[\n        Expr(\n            value=Name(id='a', ctx=Load()))],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('a = 1'), indent=4))\nModule(\n    body=[\n        Assign(\n            targets=[\n                Name(id='a', ctx=Store())],\n            value=Constant(value=1))],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('del a'), indent=4))\nModule(\n    body=[\n        Delete(\n            targets=[\n                Name(id='a', ctx=Del())])],\n    type_ignores=[])\n \n"}, {"name": "ast.Sub", "path": "library/ast#ast.Sub", "type": "Language", "text": " \nclass ast.Add  \nclass ast.Sub  \nclass ast.Mult  \nclass ast.Div  \nclass ast.FloorDiv  \nclass ast.Mod  \nclass ast.Pow  \nclass ast.LShift  \nclass ast.RShift  \nclass ast.BitOr  \nclass ast.BitXor  \nclass ast.BitAnd  \nclass ast.MatMult  \nBinary operator tokens. \n"}, {"name": "ast.Subscript", "path": "library/ast#ast.Subscript", "type": "Language", "text": " \nclass ast.Subscript(value, slice, ctx)  \nA subscript, such as l[1]. value is the subscripted object (usually sequence or mapping). slice is an index, slice or key. It can be a Tuple and contain a Slice. ctx is Load, Store or Del according to the action performed with the subscript. >>> print(ast.dump(ast.parse('l[1:2, 3]', mode='eval'), indent=4))\nExpression(\n    body=Subscript(\n        value=Name(id='l', ctx=Load()),\n        slice=Tuple(\n            elts=[\n                Slice(\n                    lower=Constant(value=1),\n                    upper=Constant(value=2)),\n                Constant(value=3)],\n            ctx=Load()),\n        ctx=Load()))\n \n"}, {"name": "ast.Try", "path": "library/ast#ast.Try", "type": "Language", "text": " \nclass ast.Try(body, handlers, orelse, finalbody)  \ntry blocks. All attributes are list of nodes to execute, except for handlers, which is a list of ExceptHandler nodes. >>> print(ast.dump(ast.parse(\"\"\"\n... try:\n...    ...\n... except Exception:\n...    ...\n... except OtherException as e:\n...    ...\n... else:\n...    ...\n... finally:\n...    ...\n... \"\"\"), indent=4))\nModule(\n    body=[\n        Try(\n            body=[\n                Expr(\n                    value=Constant(value=Ellipsis))],\n            handlers=[\n                ExceptHandler(\n                    type=Name(id='Exception', ctx=Load()),\n                    body=[\n                        Expr(\n                            value=Constant(value=Ellipsis))]),\n                ExceptHandler(\n                    type=Name(id='OtherException', ctx=Load()),\n                    name='e',\n                    body=[\n                        Expr(\n                            value=Constant(value=Ellipsis))])],\n            orelse=[\n                Expr(\n                    value=Constant(value=Ellipsis))],\n            finalbody=[\n                Expr(\n                    value=Constant(value=Ellipsis))])],\n    type_ignores=[])\n \n"}, {"name": "ast.Tuple", "path": "library/ast#ast.Tuple", "type": "Language", "text": " \nclass ast.List(elts, ctx)  \nclass ast.Tuple(elts, ctx)  \nA list or tuple. elts holds a list of nodes representing the elements. ctx is Store if the container is an assignment target (i.e. (x,y)=something), and Load otherwise. >>> print(ast.dump(ast.parse('[1, 2, 3]', mode='eval'), indent=4))\nExpression(\n    body=List(\n        elts=[\n            Constant(value=1),\n            Constant(value=2),\n            Constant(value=3)],\n        ctx=Load()))\n>>> print(ast.dump(ast.parse('(1, 2, 3)', mode='eval'), indent=4))\nExpression(\n    body=Tuple(\n        elts=[\n            Constant(value=1),\n            Constant(value=2),\n            Constant(value=3)],\n        ctx=Load()))\n \n"}, {"name": "ast.UAdd", "path": "library/ast#ast.UAdd", "type": "Language", "text": " \nclass ast.UAdd  \nclass ast.USub  \nclass ast.Not  \nclass ast.Invert  \nUnary operator tokens. Not is the not keyword, Invert is the ~ operator. >>> print(ast.dump(ast.parse('not x', mode='eval'), indent=4))\nExpression(\n    body=UnaryOp(\n        op=Not(),\n        operand=Name(id='x', ctx=Load())))\n \n"}, {"name": "ast.UnaryOp", "path": "library/ast#ast.UnaryOp", "type": "Language", "text": " \nclass ast.UnaryOp(op, operand)  \nA unary operation. op is the operator, and operand any expression node. \n"}, {"name": "ast.unparse()", "path": "library/ast#ast.unparse", "type": "Language", "text": " \nast.unparse(ast_obj)  \nUnparse an ast.AST object and generate a string with code that would produce an equivalent ast.AST object if parsed back with ast.parse().  Warning The produced code string will not necessarily be equal to the original code that generated the ast.AST object (without any compiler optimizations, such as constant tuples/frozensets).   Warning Trying to unparse a highly complex expression would result with RecursionError.   New in version 3.9.  \n"}, {"name": "ast.USub", "path": "library/ast#ast.USub", "type": "Language", "text": " \nclass ast.UAdd  \nclass ast.USub  \nclass ast.Not  \nclass ast.Invert  \nUnary operator tokens. Not is the not keyword, Invert is the ~ operator. >>> print(ast.dump(ast.parse('not x', mode='eval'), indent=4))\nExpression(\n    body=UnaryOp(\n        op=Not(),\n        operand=Name(id='x', ctx=Load())))\n \n"}, {"name": "ast.walk()", "path": "library/ast#ast.walk", "type": "Language", "text": " \nast.walk(node)  \nRecursively yield all descendant nodes in the tree starting at node (including node itself), in no specified order. This is useful if you only want to modify nodes in place and don\u2019t care about the context. \n"}, {"name": "ast.While", "path": "library/ast#ast.While", "type": "Language", "text": " \nclass ast.While(test, body, orelse)  \nA while loop. test holds the condition, such as a Compare node. >> print(ast.dump(ast.parse(\"\"\"\n... while x:\n...    ...\n... else:\n...    ...\n... \"\"\"), indent=4))\nModule(\n    body=[\n        While(\n            test=Name(id='x', ctx=Load()),\n            body=[\n                Expr(\n                    value=Constant(value=Ellipsis))],\n            orelse=[\n                Expr(\n                    value=Constant(value=Ellipsis))])],\n    type_ignores=[])\n \n"}, {"name": "ast.With", "path": "library/ast#ast.With", "type": "Language", "text": " \nclass ast.With(items, body, type_comment)  \nA with block. items is a list of withitem nodes representing the context managers, and body is the indented block inside the context.  \ntype_comment  \ntype_comment is an optional string with the type annotation as a comment. \n \n"}, {"name": "ast.With.type_comment", "path": "library/ast#ast.With.type_comment", "type": "Language", "text": " \ntype_comment  \ntype_comment is an optional string with the type annotation as a comment. \n"}, {"name": "ast.withitem", "path": "library/ast#ast.withitem", "type": "Language", "text": " \nclass ast.withitem(context_expr, optional_vars)  \nA single context manager in a with block. context_expr is the context manager, often a Call node. optional_vars is a Name, Tuple or List for the as foo part, or None if that isn\u2019t used. >>> print(ast.dump(ast.parse(\"\"\"\\\n... with a as b, c as d:\n...    something(b, d)\n... \"\"\"), indent=4))\nModule(\n    body=[\n        With(\n            items=[\n                withitem(\n                    context_expr=Name(id='a', ctx=Load()),\n                    optional_vars=Name(id='b', ctx=Store())),\n                withitem(\n                    context_expr=Name(id='c', ctx=Load()),\n                    optional_vars=Name(id='d', ctx=Store()))],\n            body=[\n                Expr(\n                    value=Call(\n                        func=Name(id='something', ctx=Load()),\n                        args=[\n                            Name(id='b', ctx=Load()),\n                            Name(id='d', ctx=Load())],\n                        keywords=[]))])],\n    type_ignores=[])\n \n"}, {"name": "ast.Yield", "path": "library/ast#ast.Yield", "type": "Language", "text": " \nclass ast.Yield(value)  \nclass ast.YieldFrom(value)  \nA yield or yield from expression. Because these are expressions, they must be wrapped in a Expr node if the value sent back is not used. >>> print(ast.dump(ast.parse('yield x'), indent=4))\nModule(\n    body=[\n        Expr(\n            value=Yield(\n                value=Name(id='x', ctx=Load())))],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('yield from x'), indent=4))\nModule(\n    body=[\n        Expr(\n            value=YieldFrom(\n                value=Name(id='x', ctx=Load())))],\n    type_ignores=[])\n \n"}, {"name": "ast.YieldFrom", "path": "library/ast#ast.YieldFrom", "type": "Language", "text": " \nclass ast.Yield(value)  \nclass ast.YieldFrom(value)  \nA yield or yield from expression. Because these are expressions, they must be wrapped in a Expr node if the value sent back is not used. >>> print(ast.dump(ast.parse('yield x'), indent=4))\nModule(\n    body=[\n        Expr(\n            value=Yield(\n                value=Name(id='x', ctx=Load())))],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('yield from x'), indent=4))\nModule(\n    body=[\n        Expr(\n            value=YieldFrom(\n                value=Name(id='x', ctx=Load())))],\n    type_ignores=[])\n \n"}, {"name": "asynchat", "path": "library/asynchat", "type": "Networking & Interprocess Communication", "text": "asynchat \u2014 Asynchronous socket command/response handler Source code: Lib/asynchat.py  Deprecated since version 3.6: Please use asyncio instead.   Note This module exists for backwards compatibility only. For new code we recommend using asyncio.  This module builds on the asyncore infrastructure, simplifying asynchronous clients and servers and making it easier to handle protocols whose elements are terminated by arbitrary strings, or are of variable length. asynchat defines the abstract class async_chat that you subclass, providing implementations of the collect_incoming_data() and found_terminator() methods. It uses the same asynchronous loop as asyncore, and the two types of channel, asyncore.dispatcher and asynchat.async_chat, can freely be mixed in the channel map. Typically an asyncore.dispatcher server channel generates new asynchat.async_chat channel objects as it receives incoming connection requests.  \nclass asynchat.async_chat  \nThis class is an abstract subclass of asyncore.dispatcher. To make practical use of the code you must subclass async_chat, providing meaningful collect_incoming_data() and found_terminator() methods. The asyncore.dispatcher methods can be used, although not all make sense in a message/response context. Like asyncore.dispatcher, async_chat defines a set of events that are generated by an analysis of socket conditions after a select() call. Once the polling loop has been started the async_chat object\u2019s methods are called by the event-processing framework with no action on the part of the programmer. Two class attributes can be modified, to improve performance, or possibly even to conserve memory.  \nac_in_buffer_size  \nThe asynchronous input buffer size (default 4096). \n  \nac_out_buffer_size  \nThe asynchronous output buffer size (default 4096). \n Unlike asyncore.dispatcher, async_chat allows you to define a FIFO queue of producers. A producer need have only one method, more(), which should return data to be transmitted on the channel. The producer indicates exhaustion (i.e. that it contains no more data) by having its more() method return the empty bytes object. At this point the async_chat object removes the producer from the queue and starts using the next producer, if any. When the producer queue is empty the handle_write() method does nothing. You use the channel object\u2019s set_terminator() method to describe how to recognize the end of, or an important breakpoint in, an incoming transmission from the remote endpoint. To build a functioning async_chat subclass your input methods collect_incoming_data() and found_terminator() must handle the data that the channel receives asynchronously. The methods are described below. \n  \nasync_chat.close_when_done()  \nPushes a None on to the producer queue. When this producer is popped off the queue it causes the channel to be closed. \n  \nasync_chat.collect_incoming_data(data)  \nCalled with data holding an arbitrary amount of received data. The default method, which must be overridden, raises a NotImplementedError exception. \n  \nasync_chat.discard_buffers()  \nIn emergencies this method will discard any data held in the input and/or output buffers and the producer queue. \n  \nasync_chat.found_terminator()  \nCalled when the incoming data stream matches the termination condition set by set_terminator(). The default method, which must be overridden, raises a NotImplementedError exception. The buffered input data should be available via an instance attribute. \n  \nasync_chat.get_terminator()  \nReturns the current terminator for the channel. \n  \nasync_chat.push(data)  \nPushes data on to the channel\u2019s queue to ensure its transmission. This is all you need to do to have the channel write the data out to the network, although it is possible to use your own producers in more complex schemes to implement encryption and chunking, for example. \n  \nasync_chat.push_with_producer(producer)  \nTakes a producer object and adds it to the producer queue associated with the channel. When all currently-pushed producers have been exhausted the channel will consume this producer\u2019s data by calling its more() method and send the data to the remote endpoint. \n  \nasync_chat.set_terminator(term)  \nSets the terminating condition to be recognized on the channel. term may be any of three types of value, corresponding to three different ways to handle incoming protocol data.   \nterm Description   \nstring Will call found_terminator() when the string is found in the input stream  \ninteger Will call found_terminator() when the indicated number of characters have been received  \nNone The channel continues to collect data forever   Note that any data following the terminator will be available for reading by the channel after found_terminator() is called. \n asynchat Example The following partial example shows how HTTP requests can be read with async_chat. A web server might create an http_request_handler object for each incoming client connection. Notice that initially the channel terminator is set to match the blank line at the end of the HTTP headers, and a flag indicates that the headers are being read. Once the headers have been read, if the request is of type POST (indicating that further data are present in the input stream) then the Content-Length: header is used to set a numeric terminator to read the right amount of data from the channel. The handle_request() method is called once all relevant input has been marshalled, after setting the channel terminator to None to ensure that any extraneous data sent by the web client are ignored. import asynchat\n\nclass http_request_handler(asynchat.async_chat):\n\n    def __init__(self, sock, addr, sessions, log):\n        asynchat.async_chat.__init__(self, sock=sock)\n        self.addr = addr\n        self.sessions = sessions\n        self.ibuffer = []\n        self.obuffer = b\"\"\n        self.set_terminator(b\"\\r\\n\\r\\n\")\n        self.reading_headers = True\n        self.handling = False\n        self.cgi_data = None\n        self.log = log\n\n    def collect_incoming_data(self, data):\n        \"\"\"Buffer the data\"\"\"\n        self.ibuffer.append(data)\n\n    def found_terminator(self):\n        if self.reading_headers:\n            self.reading_headers = False\n            self.parse_headers(b\"\".join(self.ibuffer))\n            self.ibuffer = []\n            if self.op.upper() == b\"POST\":\n                clen = self.headers.getheader(\"content-length\")\n                self.set_terminator(int(clen))\n            else:\n                self.handling = True\n                self.set_terminator(None)\n                self.handle_request()\n        elif not self.handling:\n            self.set_terminator(None)  # browsers sometimes over-send\n            self.cgi_data = parse(self.headers, b\"\".join(self.ibuffer))\n            self.handling = True\n            self.ibuffer = []\n            self.handle_request()\n\n"}, {"name": "asynchat.async_chat", "path": "library/asynchat#asynchat.async_chat", "type": "Networking & Interprocess Communication", "text": " \nclass asynchat.async_chat  \nThis class is an abstract subclass of asyncore.dispatcher. To make practical use of the code you must subclass async_chat, providing meaningful collect_incoming_data() and found_terminator() methods. The asyncore.dispatcher methods can be used, although not all make sense in a message/response context. Like asyncore.dispatcher, async_chat defines a set of events that are generated by an analysis of socket conditions after a select() call. Once the polling loop has been started the async_chat object\u2019s methods are called by the event-processing framework with no action on the part of the programmer. Two class attributes can be modified, to improve performance, or possibly even to conserve memory.  \nac_in_buffer_size  \nThe asynchronous input buffer size (default 4096). \n  \nac_out_buffer_size  \nThe asynchronous output buffer size (default 4096). \n Unlike asyncore.dispatcher, async_chat allows you to define a FIFO queue of producers. A producer need have only one method, more(), which should return data to be transmitted on the channel. The producer indicates exhaustion (i.e. that it contains no more data) by having its more() method return the empty bytes object. At this point the async_chat object removes the producer from the queue and starts using the next producer, if any. When the producer queue is empty the handle_write() method does nothing. You use the channel object\u2019s set_terminator() method to describe how to recognize the end of, or an important breakpoint in, an incoming transmission from the remote endpoint. To build a functioning async_chat subclass your input methods collect_incoming_data() and found_terminator() must handle the data that the channel receives asynchronously. The methods are described below. \n"}, {"name": "asynchat.async_chat.ac_in_buffer_size", "path": "library/asynchat#asynchat.async_chat.ac_in_buffer_size", "type": "Networking & Interprocess Communication", "text": " \nac_in_buffer_size  \nThe asynchronous input buffer size (default 4096). \n"}, {"name": "asynchat.async_chat.ac_out_buffer_size", "path": "library/asynchat#asynchat.async_chat.ac_out_buffer_size", "type": "Networking & Interprocess Communication", "text": " \nac_out_buffer_size  \nThe asynchronous output buffer size (default 4096). \n"}, {"name": "asynchat.async_chat.close_when_done()", "path": "library/asynchat#asynchat.async_chat.close_when_done", "type": "Networking & Interprocess Communication", "text": " \nasync_chat.close_when_done()  \nPushes a None on to the producer queue. When this producer is popped off the queue it causes the channel to be closed. \n"}, {"name": "asynchat.async_chat.collect_incoming_data()", "path": "library/asynchat#asynchat.async_chat.collect_incoming_data", "type": "Networking & Interprocess Communication", "text": " \nasync_chat.collect_incoming_data(data)  \nCalled with data holding an arbitrary amount of received data. The default method, which must be overridden, raises a NotImplementedError exception. \n"}, {"name": "asynchat.async_chat.discard_buffers()", "path": "library/asynchat#asynchat.async_chat.discard_buffers", "type": "Networking & Interprocess Communication", "text": " \nasync_chat.discard_buffers()  \nIn emergencies this method will discard any data held in the input and/or output buffers and the producer queue. \n"}, {"name": "asynchat.async_chat.found_terminator()", "path": "library/asynchat#asynchat.async_chat.found_terminator", "type": "Networking & Interprocess Communication", "text": " \nasync_chat.found_terminator()  \nCalled when the incoming data stream matches the termination condition set by set_terminator(). The default method, which must be overridden, raises a NotImplementedError exception. The buffered input data should be available via an instance attribute. \n"}, {"name": "asynchat.async_chat.get_terminator()", "path": "library/asynchat#asynchat.async_chat.get_terminator", "type": "Networking & Interprocess Communication", "text": " \nasync_chat.get_terminator()  \nReturns the current terminator for the channel. \n"}, {"name": "asynchat.async_chat.push()", "path": "library/asynchat#asynchat.async_chat.push", "type": "Networking & Interprocess Communication", "text": " \nasync_chat.push(data)  \nPushes data on to the channel\u2019s queue to ensure its transmission. This is all you need to do to have the channel write the data out to the network, although it is possible to use your own producers in more complex schemes to implement encryption and chunking, for example. \n"}, {"name": "asynchat.async_chat.push_with_producer()", "path": "library/asynchat#asynchat.async_chat.push_with_producer", "type": "Networking & Interprocess Communication", "text": " \nasync_chat.push_with_producer(producer)  \nTakes a producer object and adds it to the producer queue associated with the channel. When all currently-pushed producers have been exhausted the channel will consume this producer\u2019s data by calling its more() method and send the data to the remote endpoint. \n"}, {"name": "asynchat.async_chat.set_terminator()", "path": "library/asynchat#asynchat.async_chat.set_terminator", "type": "Networking & Interprocess Communication", "text": " \nasync_chat.set_terminator(term)  \nSets the terminating condition to be recognized on the channel. term may be any of three types of value, corresponding to three different ways to handle incoming protocol data.   \nterm Description   \nstring Will call found_terminator() when the string is found in the input stream  \ninteger Will call found_terminator() when the indicated number of characters have been received  \nNone The channel continues to collect data forever   Note that any data following the terminator will be available for reading by the channel after found_terminator() is called. \n"}, {"name": "asyncio", "path": "library/asyncio", "type": "Asynchronous I/O", "text": "asyncio \u2014 Asynchronous I/O  Hello World! import asyncio\n\nasync def main():\n    print('Hello ...')\n    await asyncio.sleep(1)\n    print('... World!')\n\n# Python 3.7+\nasyncio.run(main())\n  asyncio is a library to write concurrent code using the async/await syntax. asyncio is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc. asyncio is often a perfect fit for IO-bound and high-level structured network code. asyncio provides a set of high-level APIs to:  \nrun Python coroutines concurrently and have full control over their execution; perform network IO and IPC; control subprocesses; distribute tasks via queues; \nsynchronize concurrent code;  Additionally, there are low-level APIs for library and framework developers to:  create and manage event loops, which provide asynchronous APIs for networking, running subprocesses, handling OS signals, etc; implement efficient protocols using transports; \nbridge callback-based libraries and code with async/await syntax.  Reference High-level APIs  Coroutines and Tasks Streams Synchronization Primitives Subprocesses Queues Exceptions  Low-level APIs  Event Loop Futures Transports and Protocols Policies Platform Support  Guides and Tutorials  High-level API Index Low-level API Index Developing with asyncio   Note The source code for asyncio can be found in Lib/asyncio/. \n"}, {"name": "asyncio.AbstractChildWatcher", "path": "library/asyncio-policy#asyncio.AbstractChildWatcher", "type": "Asynchronous I/O", "text": " \nclass asyncio.AbstractChildWatcher  \n \nadd_child_handler(pid, callback, *args)  \nRegister a new child handler. Arrange for callback(pid, returncode, *args) to be called when a process with PID equal to pid terminates. Specifying another callback for the same process replaces the previous handler. The callback callable must be thread-safe. \n  \nremove_child_handler(pid)  \nRemoves the handler for process with PID equal to pid. The function returns True if the handler was successfully removed, False if there was nothing to remove. \n  \nattach_loop(loop)  \nAttach the watcher to an event loop. If the watcher was previously attached to an event loop, then it is first detached before attaching to the new loop. Note: loop may be None. \n  \nis_active()  \nReturn True if the watcher is ready to use. Spawning a subprocess with inactive current child watcher raises RuntimeError.  New in version 3.8.  \n  \nclose()  \nClose the watcher. This method has to be called to ensure that underlying resources are cleaned-up. \n \n"}, {"name": "asyncio.AbstractChildWatcher.add_child_handler()", "path": "library/asyncio-policy#asyncio.AbstractChildWatcher.add_child_handler", "type": "Asynchronous I/O", "text": " \nadd_child_handler(pid, callback, *args)  \nRegister a new child handler. Arrange for callback(pid, returncode, *args) to be called when a process with PID equal to pid terminates. Specifying another callback for the same process replaces the previous handler. The callback callable must be thread-safe. \n"}, {"name": "asyncio.AbstractChildWatcher.attach_loop()", "path": "library/asyncio-policy#asyncio.AbstractChildWatcher.attach_loop", "type": "Asynchronous I/O", "text": " \nattach_loop(loop)  \nAttach the watcher to an event loop. If the watcher was previously attached to an event loop, then it is first detached before attaching to the new loop. Note: loop may be None. \n"}, {"name": "asyncio.AbstractChildWatcher.close()", "path": "library/asyncio-policy#asyncio.AbstractChildWatcher.close", "type": "Asynchronous I/O", "text": " \nclose()  \nClose the watcher. This method has to be called to ensure that underlying resources are cleaned-up. \n"}, {"name": "asyncio.AbstractChildWatcher.is_active()", "path": "library/asyncio-policy#asyncio.AbstractChildWatcher.is_active", "type": "Asynchronous I/O", "text": " \nis_active()  \nReturn True if the watcher is ready to use. Spawning a subprocess with inactive current child watcher raises RuntimeError.  New in version 3.8.  \n"}, {"name": "asyncio.AbstractChildWatcher.remove_child_handler()", "path": "library/asyncio-policy#asyncio.AbstractChildWatcher.remove_child_handler", "type": "Asynchronous I/O", "text": " \nremove_child_handler(pid)  \nRemoves the handler for process with PID equal to pid. The function returns True if the handler was successfully removed, False if there was nothing to remove. \n"}, {"name": "asyncio.AbstractEventLoop", "path": "library/asyncio-eventloop#asyncio.AbstractEventLoop", "type": "Asynchronous I/O", "text": " \nclass asyncio.AbstractEventLoop  \nAbstract base class for asyncio-compliant event loops. The Event Loop Methods section lists all methods that an alternative implementation of AbstractEventLoop should have defined. \n"}, {"name": "asyncio.AbstractEventLoopPolicy", "path": "library/asyncio-policy#asyncio.AbstractEventLoopPolicy", "type": "Asynchronous I/O", "text": " \nclass asyncio.AbstractEventLoopPolicy  \nAn abstract base class for asyncio policies.  \nget_event_loop()  \nGet the event loop for the current context. Return an event loop object implementing the AbstractEventLoop interface. This method should never return None.  Changed in version 3.6.  \n  \nset_event_loop(loop)  \nSet the event loop for the current context to loop. \n  \nnew_event_loop()  \nCreate and return a new event loop object. This method should never return None. \n  \nget_child_watcher()  \nGet a child process watcher object. Return a watcher object implementing the AbstractChildWatcher interface. This function is Unix specific. \n  \nset_child_watcher(watcher)  \nSet the current child process watcher to watcher. This function is Unix specific. \n \n"}, {"name": "asyncio.AbstractEventLoopPolicy.get_child_watcher()", "path": "library/asyncio-policy#asyncio.AbstractEventLoopPolicy.get_child_watcher", "type": "Asynchronous I/O", "text": " \nget_child_watcher()  \nGet a child process watcher object. Return a watcher object implementing the AbstractChildWatcher interface. This function is Unix specific. \n"}, {"name": "asyncio.AbstractEventLoopPolicy.get_event_loop()", "path": "library/asyncio-policy#asyncio.AbstractEventLoopPolicy.get_event_loop", "type": "Asynchronous I/O", "text": " \nget_event_loop()  \nGet the event loop for the current context. Return an event loop object implementing the AbstractEventLoop interface. This method should never return None.  Changed in version 3.6.  \n"}, {"name": "asyncio.AbstractEventLoopPolicy.new_event_loop()", "path": "library/asyncio-policy#asyncio.AbstractEventLoopPolicy.new_event_loop", "type": "Asynchronous I/O", "text": " \nnew_event_loop()  \nCreate and return a new event loop object. This method should never return None. \n"}, {"name": "asyncio.AbstractEventLoopPolicy.set_child_watcher()", "path": "library/asyncio-policy#asyncio.AbstractEventLoopPolicy.set_child_watcher", "type": "Asynchronous I/O", "text": " \nset_child_watcher(watcher)  \nSet the current child process watcher to watcher. This function is Unix specific. \n"}, {"name": "asyncio.AbstractEventLoopPolicy.set_event_loop()", "path": "library/asyncio-policy#asyncio.AbstractEventLoopPolicy.set_event_loop", "type": "Asynchronous I/O", "text": " \nset_event_loop(loop)  \nSet the event loop for the current context to loop. \n"}, {"name": "asyncio.all_tasks()", "path": "library/asyncio-task#asyncio.all_tasks", "type": "Asynchronous I/O", "text": " \nasyncio.all_tasks(loop=None)  \nReturn a set of not yet finished Task objects run by the loop. If loop is None, get_running_loop() is used for getting current loop.  New in version 3.7.  \n"}, {"name": "asyncio.asyncio.subprocess.DEVNULL", "path": "library/asyncio-subprocess#asyncio.asyncio.subprocess.DEVNULL", "type": "Asynchronous I/O", "text": " \nasyncio.subprocess.DEVNULL  \nSpecial value that can be used as the stdin, stdout or stderr argument to process creation functions. It indicates that the special file os.devnull will be used for the corresponding subprocess stream. \n"}, {"name": "asyncio.asyncio.subprocess.PIPE", "path": "library/asyncio-subprocess#asyncio.asyncio.subprocess.PIPE", "type": "Asynchronous I/O", "text": " \nasyncio.subprocess.PIPE  \nCan be passed to the stdin, stdout or stderr parameters. If PIPE is passed to stdin argument, the Process.stdin attribute will point to a StreamWriter instance. If PIPE is passed to stdout or stderr arguments, the Process.stdout and Process.stderr attributes will point to StreamReader instances. \n"}, {"name": "asyncio.asyncio.subprocess.Process", "path": "library/asyncio-subprocess#asyncio.asyncio.subprocess.Process", "type": "Asynchronous I/O", "text": " \nclass asyncio.subprocess.Process  \nAn object that wraps OS processes created by the create_subprocess_exec() and create_subprocess_shell() functions. This class is designed to have a similar API to the subprocess.Popen class, but there are some notable differences:  unlike Popen, Process instances do not have an equivalent to the poll() method; the communicate() and wait() methods don\u2019t have a timeout parameter: use the wait_for() function; the Process.wait() method is asynchronous, whereas subprocess.Popen.wait() method is implemented as a blocking busy loop; the universal_newlines parameter is not supported.  This class is not thread safe. See also the Subprocess and Threads section.  \ncoroutine wait()  \nWait for the child process to terminate. Set and return the returncode attribute.  Note This method can deadlock when using stdout=PIPE or stderr=PIPE and the child process generates so much output that it blocks waiting for the OS pipe buffer to accept more data. Use the communicate() method when using pipes to avoid this condition.  \n  \ncoroutine communicate(input=None)  \nInteract with process:  send data to stdin (if input is not None); read data from stdout and stderr, until EOF is reached; wait for process to terminate.  The optional input argument is the data (bytes object) that will be sent to the child process. Return a tuple (stdout_data, stderr_data). If either BrokenPipeError or ConnectionResetError exception is raised when writing input into stdin, the exception is ignored. This condition occurs when the process exits before all data are written into stdin. If it is desired to send data to the process\u2019 stdin, the process needs to be created with stdin=PIPE. Similarly, to get anything other than None in the result tuple, the process has to be created with stdout=PIPE and/or stderr=PIPE arguments. Note, that the data read is buffered in memory, so do not use this method if the data size is large or unlimited. \n  \nsend_signal(signal)  \nSends the signal signal to the child process.  Note On Windows, SIGTERM is an alias for terminate(). CTRL_C_EVENT and CTRL_BREAK_EVENT can be sent to processes started with a creationflags parameter which includes CREATE_NEW_PROCESS_GROUP.  \n  \nterminate()  \nStop the child process. On POSIX systems this method sends signal.SIGTERM to the child process. On Windows the Win32 API function TerminateProcess() is called to stop the child process. \n  \nkill()  \nKill the child process. On POSIX systems this method sends SIGKILL to the child process. On Windows this method is an alias for terminate(). \n  \nstdin  \nStandard input stream (StreamWriter) or None if the process was created with stdin=None. \n  \nstdout  \nStandard output stream (StreamReader) or None if the process was created with stdout=None. \n  \nstderr  \nStandard error stream (StreamReader) or None if the process was created with stderr=None. \n  Warning Use the communicate() method rather than process.stdin.write(), await process.stdout.read() or await process.stderr.read. This avoids deadlocks due to streams pausing reading or writing and blocking the child process.   \npid  \nProcess identification number (PID). Note that for processes created by the create_subprocess_shell() function, this attribute is the PID of the spawned shell. \n  \nreturncode  \nReturn code of the process when it exits. A None value indicates that the process has not terminated yet. A negative value -N indicates that the child was terminated by signal N (POSIX only). \n \n"}, {"name": "asyncio.asyncio.subprocess.Process.communicate()", "path": "library/asyncio-subprocess#asyncio.asyncio.subprocess.Process.communicate", "type": "Asynchronous I/O", "text": " \ncoroutine communicate(input=None)  \nInteract with process:  send data to stdin (if input is not None); read data from stdout and stderr, until EOF is reached; wait for process to terminate.  The optional input argument is the data (bytes object) that will be sent to the child process. Return a tuple (stdout_data, stderr_data). If either BrokenPipeError or ConnectionResetError exception is raised when writing input into stdin, the exception is ignored. This condition occurs when the process exits before all data are written into stdin. If it is desired to send data to the process\u2019 stdin, the process needs to be created with stdin=PIPE. Similarly, to get anything other than None in the result tuple, the process has to be created with stdout=PIPE and/or stderr=PIPE arguments. Note, that the data read is buffered in memory, so do not use this method if the data size is large or unlimited. \n"}, {"name": "asyncio.asyncio.subprocess.Process.kill()", "path": "library/asyncio-subprocess#asyncio.asyncio.subprocess.Process.kill", "type": "Asynchronous I/O", "text": " \nkill()  \nKill the child process. On POSIX systems this method sends SIGKILL to the child process. On Windows this method is an alias for terminate(). \n"}, {"name": "asyncio.asyncio.subprocess.Process.pid", "path": "library/asyncio-subprocess#asyncio.asyncio.subprocess.Process.pid", "type": "Asynchronous I/O", "text": " \npid  \nProcess identification number (PID). Note that for processes created by the create_subprocess_shell() function, this attribute is the PID of the spawned shell. \n"}, {"name": "asyncio.asyncio.subprocess.Process.returncode", "path": "library/asyncio-subprocess#asyncio.asyncio.subprocess.Process.returncode", "type": "Asynchronous I/O", "text": " \nreturncode  \nReturn code of the process when it exits. A None value indicates that the process has not terminated yet. A negative value -N indicates that the child was terminated by signal N (POSIX only). \n"}, {"name": "asyncio.asyncio.subprocess.Process.send_signal()", "path": "library/asyncio-subprocess#asyncio.asyncio.subprocess.Process.send_signal", "type": "Asynchronous I/O", "text": " \nsend_signal(signal)  \nSends the signal signal to the child process.  Note On Windows, SIGTERM is an alias for terminate(). CTRL_C_EVENT and CTRL_BREAK_EVENT can be sent to processes started with a creationflags parameter which includes CREATE_NEW_PROCESS_GROUP.  \n"}, {"name": "asyncio.asyncio.subprocess.Process.stderr", "path": "library/asyncio-subprocess#asyncio.asyncio.subprocess.Process.stderr", "type": "Asynchronous I/O", "text": " \nstderr  \nStandard error stream (StreamReader) or None if the process was created with stderr=None. \n"}, {"name": "asyncio.asyncio.subprocess.Process.stdin", "path": "library/asyncio-subprocess#asyncio.asyncio.subprocess.Process.stdin", "type": "Asynchronous I/O", "text": " \nstdin  \nStandard input stream (StreamWriter) or None if the process was created with stdin=None. \n"}, {"name": "asyncio.asyncio.subprocess.Process.stdout", "path": "library/asyncio-subprocess#asyncio.asyncio.subprocess.Process.stdout", "type": "Asynchronous I/O", "text": " \nstdout  \nStandard output stream (StreamReader) or None if the process was created with stdout=None. \n"}, {"name": "asyncio.asyncio.subprocess.Process.terminate()", "path": "library/asyncio-subprocess#asyncio.asyncio.subprocess.Process.terminate", "type": "Asynchronous I/O", "text": " \nterminate()  \nStop the child process. On POSIX systems this method sends signal.SIGTERM to the child process. On Windows the Win32 API function TerminateProcess() is called to stop the child process. \n"}, {"name": "asyncio.asyncio.subprocess.Process.wait()", "path": "library/asyncio-subprocess#asyncio.asyncio.subprocess.Process.wait", "type": "Asynchronous I/O", "text": " \ncoroutine wait()  \nWait for the child process to terminate. Set and return the returncode attribute.  Note This method can deadlock when using stdout=PIPE or stderr=PIPE and the child process generates so much output that it blocks waiting for the OS pipe buffer to accept more data. Use the communicate() method when using pipes to avoid this condition.  \n"}, {"name": "asyncio.asyncio.subprocess.STDOUT", "path": "library/asyncio-subprocess#asyncio.asyncio.subprocess.STDOUT", "type": "Asynchronous I/O", "text": " \nasyncio.subprocess.STDOUT  \nSpecial value that can be used as the stderr argument and indicates that standard error should be redirected into standard output. \n"}, {"name": "asyncio.as_completed()", "path": "library/asyncio-task#asyncio.as_completed", "type": "Asynchronous I/O", "text": " \nasyncio.as_completed(aws, *, loop=None, timeout=None)  \nRun awaitable objects in the aws iterable concurrently. Return an iterator of coroutines. Each coroutine returned can be awaited to get the earliest next result from the iterable of the remaining awaitables. Raises asyncio.TimeoutError if the timeout occurs before all Futures are done.  Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.  Example: for coro in as_completed(aws):\n    earliest_result = await coro\n    # ...\n \n"}, {"name": "asyncio.BaseProtocol", "path": "library/asyncio-protocol#asyncio.BaseProtocol", "type": "Asynchronous I/O", "text": " \nclass asyncio.BaseProtocol  \nBase protocol with methods that all protocols share. \n"}, {"name": "asyncio.BaseProtocol.connection_lost()", "path": "library/asyncio-protocol#asyncio.BaseProtocol.connection_lost", "type": "Asynchronous I/O", "text": " \nBaseProtocol.connection_lost(exc)  \nCalled when the connection is lost or closed. The argument is either an exception object or None. The latter means a regular EOF is received, or the connection was aborted or closed by this side of the connection. \n"}, {"name": "asyncio.BaseProtocol.connection_made()", "path": "library/asyncio-protocol#asyncio.BaseProtocol.connection_made", "type": "Asynchronous I/O", "text": " \nBaseProtocol.connection_made(transport)  \nCalled when a connection is made. The transport argument is the transport representing the connection. The protocol is responsible for storing the reference to its transport. \n"}, {"name": "asyncio.BaseProtocol.pause_writing()", "path": "library/asyncio-protocol#asyncio.BaseProtocol.pause_writing", "type": "Asynchronous I/O", "text": " \nBaseProtocol.pause_writing()  \nCalled when the transport\u2019s buffer goes over the high watermark. \n"}, {"name": "asyncio.BaseProtocol.resume_writing()", "path": "library/asyncio-protocol#asyncio.BaseProtocol.resume_writing", "type": "Asynchronous I/O", "text": " \nBaseProtocol.resume_writing()  \nCalled when the transport\u2019s buffer drains below the low watermark. \n"}, {"name": "asyncio.BaseTransport", "path": "library/asyncio-protocol#asyncio.BaseTransport", "type": "Asynchronous I/O", "text": " \nclass asyncio.BaseTransport  \nBase class for all transports. Contains methods that all asyncio transports share. \n"}, {"name": "asyncio.BaseTransport.close()", "path": "library/asyncio-protocol#asyncio.BaseTransport.close", "type": "Asynchronous I/O", "text": " \nBaseTransport.close()  \nClose the transport. If the transport has a buffer for outgoing data, buffered data will be flushed asynchronously. No more data will be received. After all buffered data is flushed, the protocol\u2019s protocol.connection_lost() method will be called with None as its argument. \n"}, {"name": "asyncio.BaseTransport.get_extra_info()", "path": "library/asyncio-protocol#asyncio.BaseTransport.get_extra_info", "type": "Asynchronous I/O", "text": " \nBaseTransport.get_extra_info(name, default=None)  \nReturn information about the transport or underlying resources it uses. name is a string representing the piece of transport-specific information to get. default is the value to return if the information is not available, or if the transport does not support querying it with the given third-party event loop implementation or on the current platform. For example, the following code attempts to get the underlying socket object of the transport: sock = transport.get_extra_info('socket')\nif sock is not None:\n    print(sock.getsockopt(...))\n Categories of information that can be queried on some transports:  \nsocket:  \n'peername': the remote address to which the socket is connected, result of socket.socket.getpeername() (None on error) \n'socket': socket.socket instance \n'sockname': the socket\u2019s own address, result of socket.socket.getsockname()\n   \nSSL socket:  \n'compression': the compression algorithm being used as a string, or None if the connection isn\u2019t compressed; result of ssl.SSLSocket.compression()\n \n'cipher': a three-value tuple containing the name of the cipher being used, the version of the SSL protocol that defines its use, and the number of secret bits being used; result of ssl.SSLSocket.cipher()\n \n'peercert': peer certificate; result of ssl.SSLSocket.getpeercert()\n \n'sslcontext': ssl.SSLContext instance \n'ssl_object': ssl.SSLObject or ssl.SSLSocket instance   \npipe:  \n'pipe': pipe object   \nsubprocess:  \n'subprocess': subprocess.Popen instance    \n"}, {"name": "asyncio.BaseTransport.get_protocol()", "path": "library/asyncio-protocol#asyncio.BaseTransport.get_protocol", "type": "Asynchronous I/O", "text": " \nBaseTransport.get_protocol()  \nReturn the current protocol. \n"}, {"name": "asyncio.BaseTransport.is_closing()", "path": "library/asyncio-protocol#asyncio.BaseTransport.is_closing", "type": "Asynchronous I/O", "text": " \nBaseTransport.is_closing()  \nReturn True if the transport is closing or is closed. \n"}, {"name": "asyncio.BaseTransport.set_protocol()", "path": "library/asyncio-protocol#asyncio.BaseTransport.set_protocol", "type": "Asynchronous I/O", "text": " \nBaseTransport.set_protocol(protocol)  \nSet a new protocol. Switching protocol should only be done when both protocols are documented to support the switch. \n"}, {"name": "asyncio.BoundedSemaphore", "path": "library/asyncio-sync#asyncio.BoundedSemaphore", "type": "Asynchronous I/O", "text": " \nclass asyncio.BoundedSemaphore(value=1, *, loop=None)  \nA bounded semaphore object. Not thread-safe. Bounded Semaphore is a version of Semaphore that raises a ValueError in release() if it increases the internal counter above the initial value.  Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.  \n"}, {"name": "asyncio.BufferedProtocol", "path": "library/asyncio-protocol#asyncio.BufferedProtocol", "type": "Asynchronous I/O", "text": " \nclass asyncio.BufferedProtocol(BaseProtocol)  \nA base class for implementing streaming protocols with manual control of the receive buffer. \n"}, {"name": "asyncio.BufferedProtocol.buffer_updated()", "path": "library/asyncio-protocol#asyncio.BufferedProtocol.buffer_updated", "type": "Asynchronous I/O", "text": " \nBufferedProtocol.buffer_updated(nbytes)  \nCalled when the buffer was updated with the received data. nbytes is the total number of bytes that were written to the buffer. \n"}, {"name": "asyncio.BufferedProtocol.eof_received()", "path": "library/asyncio-protocol#asyncio.BufferedProtocol.eof_received", "type": "Asynchronous I/O", "text": " \nBufferedProtocol.eof_received()  \nSee the documentation of the protocol.eof_received() method. \n"}, {"name": "asyncio.BufferedProtocol.get_buffer()", "path": "library/asyncio-protocol#asyncio.BufferedProtocol.get_buffer", "type": "Asynchronous I/O", "text": " \nBufferedProtocol.get_buffer(sizehint)  \nCalled to allocate a new receive buffer. sizehint is the recommended minimum size for the returned buffer. It is acceptable to return smaller or larger buffers than what sizehint suggests. When set to -1, the buffer size can be arbitrary. It is an error to return a buffer with a zero size. get_buffer() must return an object implementing the buffer protocol. \n"}, {"name": "asyncio.CancelledError", "path": "library/asyncio-exceptions#asyncio.CancelledError", "type": "Asynchronous I/O", "text": " \nexception asyncio.CancelledError  \nThe operation has been cancelled. This exception can be caught to perform custom operations when asyncio Tasks are cancelled. In almost all situations the exception must be re-raised.  Changed in version 3.8: CancelledError is now a subclass of BaseException.  \n"}, {"name": "asyncio.Condition", "path": "library/asyncio-sync#asyncio.Condition", "type": "Asynchronous I/O", "text": " \nclass asyncio.Condition(lock=None, *, loop=None)  \nA Condition object. Not thread-safe. An asyncio condition primitive can be used by a task to wait for some event to happen and then get exclusive access to a shared resource. In essence, a Condition object combines the functionality of an Event and a Lock. It is possible to have multiple Condition objects share one Lock, which allows coordinating exclusive access to a shared resource between different tasks interested in particular states of that shared resource. The optional lock argument must be a Lock object or None. In the latter case a new Lock object is created automatically.  Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.  The preferred way to use a Condition is an async with statement: cond = asyncio.Condition()\n\n# ... later\nasync with cond:\n    await cond.wait()\n which is equivalent to: cond = asyncio.Condition()\n\n# ... later\nawait cond.acquire()\ntry:\n    await cond.wait()\nfinally:\n    cond.release()\n  \ncoroutine acquire()  \nAcquire the underlying lock. This method waits until the underlying lock is unlocked, sets it to locked and returns True. \n  \nnotify(n=1)  \nWake up at most n tasks (1 by default) waiting on this condition. The method is no-op if no tasks are waiting. The lock must be acquired before this method is called and released shortly after. If called with an unlocked lock a RuntimeError error is raised. \n  \nlocked()  \nReturn True if the underlying lock is acquired. \n  \nnotify_all()  \nWake up all tasks waiting on this condition. This method acts like notify(), but wakes up all waiting tasks. The lock must be acquired before this method is called and released shortly after. If called with an unlocked lock a RuntimeError error is raised. \n  \nrelease()  \nRelease the underlying lock. When invoked on an unlocked lock, a RuntimeError is raised. \n  \ncoroutine wait()  \nWait until notified. If the calling task has not acquired the lock when this method is called, a RuntimeError is raised. This method releases the underlying lock, and then blocks until it is awakened by a notify() or notify_all() call. Once awakened, the Condition re-acquires its lock and this method returns True. \n  \ncoroutine wait_for(predicate)  \nWait until a predicate becomes true. The predicate must be a callable which result will be interpreted as a boolean value. The final value is the return value. \n \n"}, {"name": "asyncio.Condition.acquire()", "path": "library/asyncio-sync#asyncio.Condition.acquire", "type": "Asynchronous I/O", "text": " \ncoroutine acquire()  \nAcquire the underlying lock. This method waits until the underlying lock is unlocked, sets it to locked and returns True. \n"}, {"name": "asyncio.Condition.locked()", "path": "library/asyncio-sync#asyncio.Condition.locked", "type": "Asynchronous I/O", "text": " \nlocked()  \nReturn True if the underlying lock is acquired. \n"}, {"name": "asyncio.Condition.notify()", "path": "library/asyncio-sync#asyncio.Condition.notify", "type": "Asynchronous I/O", "text": " \nnotify(n=1)  \nWake up at most n tasks (1 by default) waiting on this condition. The method is no-op if no tasks are waiting. The lock must be acquired before this method is called and released shortly after. If called with an unlocked lock a RuntimeError error is raised. \n"}, {"name": "asyncio.Condition.notify_all()", "path": "library/asyncio-sync#asyncio.Condition.notify_all", "type": "Asynchronous I/O", "text": " \nnotify_all()  \nWake up all tasks waiting on this condition. This method acts like notify(), but wakes up all waiting tasks. The lock must be acquired before this method is called and released shortly after. If called with an unlocked lock a RuntimeError error is raised. \n"}, {"name": "asyncio.Condition.release()", "path": "library/asyncio-sync#asyncio.Condition.release", "type": "Asynchronous I/O", "text": " \nrelease()  \nRelease the underlying lock. When invoked on an unlocked lock, a RuntimeError is raised. \n"}, {"name": "asyncio.Condition.wait()", "path": "library/asyncio-sync#asyncio.Condition.wait", "type": "Asynchronous I/O", "text": " \ncoroutine wait()  \nWait until notified. If the calling task has not acquired the lock when this method is called, a RuntimeError is raised. This method releases the underlying lock, and then blocks until it is awakened by a notify() or notify_all() call. Once awakened, the Condition re-acquires its lock and this method returns True. \n"}, {"name": "asyncio.Condition.wait_for()", "path": "library/asyncio-sync#asyncio.Condition.wait_for", "type": "Asynchronous I/O", "text": " \ncoroutine wait_for(predicate)  \nWait until a predicate becomes true. The predicate must be a callable which result will be interpreted as a boolean value. The final value is the return value. \n"}, {"name": "asyncio.coroutine()", "path": "library/asyncio-task#asyncio.coroutine", "type": "Asynchronous I/O", "text": " \n@asyncio.coroutine  \nDecorator to mark generator-based coroutines. This decorator enables legacy generator-based coroutines to be compatible with async/await code: @asyncio.coroutine\ndef old_style_coroutine():\n    yield from asyncio.sleep(1)\n\nasync def main():\n    await old_style_coroutine()\n This decorator should not be used for async def coroutines.  Deprecated since version 3.8, will be removed in version 3.10: Use async def instead.  \n"}, {"name": "asyncio.create_subprocess_exec()", "path": "library/asyncio-subprocess#asyncio.create_subprocess_exec", "type": "Asynchronous I/O", "text": " \ncoroutine asyncio.create_subprocess_exec(program, *args, stdin=None, stdout=None, stderr=None, loop=None, limit=None, **kwds)  \nCreate a subprocess. The limit argument sets the buffer limit for StreamReader wrappers for Process.stdout and Process.stderr (if subprocess.PIPE is passed to stdout and stderr arguments). Return a Process instance. See the documentation of loop.subprocess_exec() for other parameters.  Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.  \n"}, {"name": "asyncio.create_subprocess_shell()", "path": "library/asyncio-subprocess#asyncio.create_subprocess_shell", "type": "Asynchronous I/O", "text": " \ncoroutine asyncio.create_subprocess_shell(cmd, stdin=None, stdout=None, stderr=None, loop=None, limit=None, **kwds)  \nRun the cmd shell command. The limit argument sets the buffer limit for StreamReader wrappers for Process.stdout and Process.stderr (if subprocess.PIPE is passed to stdout and stderr arguments). Return a Process instance. See the documentation of loop.subprocess_shell() for other parameters.  Important It is the application\u2019s responsibility to ensure that all whitespace and special characters are quoted appropriately to avoid shell injection vulnerabilities. The shlex.quote() function can be used to properly escape whitespace and special shell characters in strings that are going to be used to construct shell commands.   Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.  \n"}, {"name": "asyncio.create_task()", "path": "library/asyncio-task#asyncio.create_task", "type": "Asynchronous I/O", "text": " \nasyncio.create_task(coro, *, name=None)  \nWrap the coro coroutine into a Task and schedule its execution. Return the Task object. If name is not None, it is set as the name of the task using Task.set_name(). The task is executed in the loop returned by get_running_loop(), RuntimeError is raised if there is no running loop in current thread. This function has been added in Python 3.7. Prior to Python 3.7, the low-level asyncio.ensure_future() function can be used instead: async def coro():\n    ...\n\n# In Python 3.7+\ntask = asyncio.create_task(coro())\n...\n\n# This works in all Python versions but is less readable\ntask = asyncio.ensure_future(coro())\n...\n  New in version 3.7.   Changed in version 3.8: Added the name parameter.  \n"}, {"name": "asyncio.current_task()", "path": "library/asyncio-task#asyncio.current_task", "type": "Asynchronous I/O", "text": " \nasyncio.current_task(loop=None)  \nReturn the currently running Task instance, or None if no task is running. If loop is None get_running_loop() is used to get the current loop.  New in version 3.7.  \n"}, {"name": "asyncio.DatagramProtocol", "path": "library/asyncio-protocol#asyncio.DatagramProtocol", "type": "Asynchronous I/O", "text": " \nclass asyncio.DatagramProtocol(BaseProtocol)  \nThe base class for implementing datagram (UDP) protocols. \n"}, {"name": "asyncio.DatagramProtocol.datagram_received()", "path": "library/asyncio-protocol#asyncio.DatagramProtocol.datagram_received", "type": "Asynchronous I/O", "text": " \nDatagramProtocol.datagram_received(data, addr)  \nCalled when a datagram is received. data is a bytes object containing the incoming data. addr is the address of the peer sending the data; the exact format depends on the transport. \n"}, {"name": "asyncio.DatagramProtocol.error_received()", "path": "library/asyncio-protocol#asyncio.DatagramProtocol.error_received", "type": "Asynchronous I/O", "text": " \nDatagramProtocol.error_received(exc)  \nCalled when a previous send or receive operation raises an OSError. exc is the OSError instance. This method is called in rare conditions, when the transport (e.g. UDP) detects that a datagram could not be delivered to its recipient. In many conditions though, undeliverable datagrams will be silently dropped. \n"}, {"name": "asyncio.DatagramTransport", "path": "library/asyncio-protocol#asyncio.DatagramTransport", "type": "Asynchronous I/O", "text": " \nclass asyncio.DatagramTransport(BaseTransport)  \nA transport for datagram (UDP) connections. Instances of the DatagramTransport class are returned from the loop.create_datagram_endpoint() event loop method. \n"}, {"name": "asyncio.DatagramTransport.abort()", "path": "library/asyncio-protocol#asyncio.DatagramTransport.abort", "type": "Asynchronous I/O", "text": " \nDatagramTransport.abort()  \nClose the transport immediately, without waiting for pending operations to complete. Buffered data will be lost. No more data will be received. The protocol\u2019s protocol.connection_lost() method will eventually be called with None as its argument. \n"}, {"name": "asyncio.DatagramTransport.sendto()", "path": "library/asyncio-protocol#asyncio.DatagramTransport.sendto", "type": "Asynchronous I/O", "text": " \nDatagramTransport.sendto(data, addr=None)  \nSend the data bytes to the remote peer given by addr (a transport-dependent target address). If addr is None, the data is sent to the target address given on transport creation. This method does not block; it buffers the data and arranges for it to be sent out asynchronously. \n"}, {"name": "asyncio.DefaultEventLoopPolicy", "path": "library/asyncio-policy#asyncio.DefaultEventLoopPolicy", "type": "Asynchronous I/O", "text": " \nclass asyncio.DefaultEventLoopPolicy  \nThe default asyncio policy. Uses SelectorEventLoop on Unix and ProactorEventLoop on Windows. There is no need to install the default policy manually. asyncio is configured to use the default policy automatically.  Changed in version 3.8: On Windows, ProactorEventLoop is now used by default.  \n"}, {"name": "asyncio.ensure_future()", "path": "library/asyncio-future#asyncio.ensure_future", "type": "Asynchronous I/O", "text": " \nasyncio.ensure_future(obj, *, loop=None)  \nReturn:  \nobj argument as is, if obj is a Future, a Task, or a Future-like object (isfuture() is used for the test.) a Task object wrapping obj, if obj is a coroutine (iscoroutine() is used for the test); in this case the coroutine will be scheduled by ensure_future(). a Task object that would await on obj, if obj is an awaitable (inspect.isawaitable() is used for the test.)  If obj is neither of the above a TypeError is raised.  Important See also the create_task() function which is the preferred way for creating new Tasks.   Changed in version 3.5.1: The function accepts any awaitable object.  \n"}, {"name": "asyncio.Event", "path": "library/asyncio-sync#asyncio.Event", "type": "Asynchronous I/O", "text": " \nclass asyncio.Event(*, loop=None)  \nAn event object. Not thread-safe. An asyncio event can be used to notify multiple asyncio tasks that some event has happened. An Event object manages an internal flag that can be set to true with the set() method and reset to false with the clear() method. The wait() method blocks until the flag is set to true. The flag is set to false initially.  Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.  Example: async def waiter(event):\n    print('waiting for it ...')\n    await event.wait()\n    print('... got it!')\n\nasync def main():\n    # Create an Event object.\n    event = asyncio.Event()\n\n    # Spawn a Task to wait until 'event' is set.\n    waiter_task = asyncio.create_task(waiter(event))\n\n    # Sleep for 1 second and set the event.\n    await asyncio.sleep(1)\n    event.set()\n\n    # Wait until the waiter task is finished.\n    await waiter_task\n\nasyncio.run(main())\n  \ncoroutine wait()  \nWait until the event is set. If the event is set, return True immediately. Otherwise block until another task calls set(). \n  \nset()  \nSet the event. All tasks waiting for event to be set will be immediately awakened. \n  \nclear()  \nClear (unset) the event. Tasks awaiting on wait() will now block until the set() method is called again. \n  \nis_set()  \nReturn True if the event is set. \n \n"}, {"name": "asyncio.Event.clear()", "path": "library/asyncio-sync#asyncio.Event.clear", "type": "Asynchronous I/O", "text": " \nclear()  \nClear (unset) the event. Tasks awaiting on wait() will now block until the set() method is called again. \n"}, {"name": "asyncio.Event.is_set()", "path": "library/asyncio-sync#asyncio.Event.is_set", "type": "Asynchronous I/O", "text": " \nis_set()  \nReturn True if the event is set. \n"}, {"name": "asyncio.Event.set()", "path": "library/asyncio-sync#asyncio.Event.set", "type": "Asynchronous I/O", "text": " \nset()  \nSet the event. All tasks waiting for event to be set will be immediately awakened. \n"}, {"name": "asyncio.Event.wait()", "path": "library/asyncio-sync#asyncio.Event.wait", "type": "Asynchronous I/O", "text": " \ncoroutine wait()  \nWait until the event is set. If the event is set, return True immediately. Otherwise block until another task calls set(). \n"}, {"name": "asyncio.FastChildWatcher", "path": "library/asyncio-policy#asyncio.FastChildWatcher", "type": "Asynchronous I/O", "text": " \nclass asyncio.FastChildWatcher  \nThis implementation reaps every terminated processes by calling os.waitpid(-1) directly, possibly breaking other code spawning processes and waiting for their termination. There is no noticeable overhead when handling a big number of children (O(1) each time a child terminates). This solution requires a running event loop in the main thread to work, as SafeChildWatcher. \n"}, {"name": "asyncio.Future", "path": "library/asyncio-future#asyncio.Future", "type": "Asynchronous I/O", "text": " \nclass asyncio.Future(*, loop=None)  \nA Future represents an eventual result of an asynchronous operation. Not thread-safe. Future is an awaitable object. Coroutines can await on Future objects until they either have a result or an exception set, or until they are cancelled. Typically Futures are used to enable low-level callback-based code (e.g. in protocols implemented using asyncio transports) to interoperate with high-level async/await code. The rule of thumb is to never expose Future objects in user-facing APIs, and the recommended way to create a Future object is to call loop.create_future(). This way alternative event loop implementations can inject their own optimized implementations of a Future object.  Changed in version 3.7: Added support for the contextvars module.   \nresult()  \nReturn the result of the Future. If the Future is done and has a result set by the set_result() method, the result value is returned. If the Future is done and has an exception set by the set_exception() method, this method raises the exception. If the Future has been cancelled, this method raises a CancelledError exception. If the Future\u2019s result isn\u2019t yet available, this method raises a InvalidStateError exception. \n  \nset_result(result)  \nMark the Future as done and set its result. Raises a InvalidStateError error if the Future is already done. \n  \nset_exception(exception)  \nMark the Future as done and set an exception. Raises a InvalidStateError error if the Future is already done. \n  \ndone()  \nReturn True if the Future is done. A Future is done if it was cancelled or if it has a result or an exception set with set_result() or set_exception() calls. \n  \ncancelled()  \nReturn True if the Future was cancelled. The method is usually used to check if a Future is not cancelled before setting a result or an exception for it: if not fut.cancelled():\n    fut.set_result(42)\n \n  \nadd_done_callback(callback, *, context=None)  \nAdd a callback to be run when the Future is done. The callback is called with the Future object as its only argument. If the Future is already done when this method is called, the callback is scheduled with loop.call_soon(). An optional keyword-only context argument allows specifying a custom contextvars.Context for the callback to run in. The current context is used when no context is provided. functools.partial() can be used to pass parameters to the callback, e.g.: # Call 'print(\"Future:\", fut)' when \"fut\" is done.\nfut.add_done_callback(\n    functools.partial(print, \"Future:\"))\n  Changed in version 3.7: The context keyword-only parameter was added. See PEP 567 for more details.  \n  \nremove_done_callback(callback)  \nRemove callback from the callbacks list. Returns the number of callbacks removed, which is typically 1, unless a callback was added more than once. \n  \ncancel(msg=None)  \nCancel the Future and schedule callbacks. If the Future is already done or cancelled, return False. Otherwise, change the Future\u2019s state to cancelled, schedule the callbacks, and return True.  Changed in version 3.9: Added the msg parameter.  \n  \nexception()  \nReturn the exception that was set on this Future. The exception (or None if no exception was set) is returned only if the Future is done. If the Future has been cancelled, this method raises a CancelledError exception. If the Future isn\u2019t done yet, this method raises an InvalidStateError exception. \n  \nget_loop()  \nReturn the event loop the Future object is bound to.  New in version 3.7.  \n \n"}, {"name": "asyncio.Future.add_done_callback()", "path": "library/asyncio-future#asyncio.Future.add_done_callback", "type": "Asynchronous I/O", "text": " \nadd_done_callback(callback, *, context=None)  \nAdd a callback to be run when the Future is done. The callback is called with the Future object as its only argument. If the Future is already done when this method is called, the callback is scheduled with loop.call_soon(). An optional keyword-only context argument allows specifying a custom contextvars.Context for the callback to run in. The current context is used when no context is provided. functools.partial() can be used to pass parameters to the callback, e.g.: # Call 'print(\"Future:\", fut)' when \"fut\" is done.\nfut.add_done_callback(\n    functools.partial(print, \"Future:\"))\n  Changed in version 3.7: The context keyword-only parameter was added. See PEP 567 for more details.  \n"}, {"name": "asyncio.Future.cancel()", "path": "library/asyncio-future#asyncio.Future.cancel", "type": "Asynchronous I/O", "text": " \ncancel(msg=None)  \nCancel the Future and schedule callbacks. If the Future is already done or cancelled, return False. Otherwise, change the Future\u2019s state to cancelled, schedule the callbacks, and return True.  Changed in version 3.9: Added the msg parameter.  \n"}, {"name": "asyncio.Future.cancelled()", "path": "library/asyncio-future#asyncio.Future.cancelled", "type": "Asynchronous I/O", "text": " \ncancelled()  \nReturn True if the Future was cancelled. The method is usually used to check if a Future is not cancelled before setting a result or an exception for it: if not fut.cancelled():\n    fut.set_result(42)\n \n"}, {"name": "asyncio.Future.done()", "path": "library/asyncio-future#asyncio.Future.done", "type": "Asynchronous I/O", "text": " \ndone()  \nReturn True if the Future is done. A Future is done if it was cancelled or if it has a result or an exception set with set_result() or set_exception() calls. \n"}, {"name": "asyncio.Future.exception()", "path": "library/asyncio-future#asyncio.Future.exception", "type": "Asynchronous I/O", "text": " \nexception()  \nReturn the exception that was set on this Future. The exception (or None if no exception was set) is returned only if the Future is done. If the Future has been cancelled, this method raises a CancelledError exception. If the Future isn\u2019t done yet, this method raises an InvalidStateError exception. \n"}, {"name": "asyncio.Future.get_loop()", "path": "library/asyncio-future#asyncio.Future.get_loop", "type": "Asynchronous I/O", "text": " \nget_loop()  \nReturn the event loop the Future object is bound to.  New in version 3.7.  \n"}, {"name": "asyncio.Future.remove_done_callback()", "path": "library/asyncio-future#asyncio.Future.remove_done_callback", "type": "Asynchronous I/O", "text": " \nremove_done_callback(callback)  \nRemove callback from the callbacks list. Returns the number of callbacks removed, which is typically 1, unless a callback was added more than once. \n"}, {"name": "asyncio.Future.result()", "path": "library/asyncio-future#asyncio.Future.result", "type": "Asynchronous I/O", "text": " \nresult()  \nReturn the result of the Future. If the Future is done and has a result set by the set_result() method, the result value is returned. If the Future is done and has an exception set by the set_exception() method, this method raises the exception. If the Future has been cancelled, this method raises a CancelledError exception. If the Future\u2019s result isn\u2019t yet available, this method raises a InvalidStateError exception. \n"}, {"name": "asyncio.Future.set_exception()", "path": "library/asyncio-future#asyncio.Future.set_exception", "type": "Asynchronous I/O", "text": " \nset_exception(exception)  \nMark the Future as done and set an exception. Raises a InvalidStateError error if the Future is already done. \n"}, {"name": "asyncio.Future.set_result()", "path": "library/asyncio-future#asyncio.Future.set_result", "type": "Asynchronous I/O", "text": " \nset_result(result)  \nMark the Future as done and set its result. Raises a InvalidStateError error if the Future is already done. \n"}, {"name": "asyncio.gather()", "path": "library/asyncio-task#asyncio.gather", "type": "Asynchronous I/O", "text": " \nawaitable asyncio.gather(*aws, loop=None, return_exceptions=False)  \nRun awaitable objects in the aws sequence concurrently. If any awaitable in aws is a coroutine, it is automatically scheduled as a Task. If all awaitables are completed successfully, the result is an aggregate list of returned values. The order of result values corresponds to the order of awaitables in aws. If return_exceptions is False (default), the first raised exception is immediately propagated to the task that awaits on gather(). Other awaitables in the aws sequence won\u2019t be cancelled and will continue to run. If return_exceptions is True, exceptions are treated the same as successful results, and aggregated in the result list. If gather() is cancelled, all submitted awaitables (that have not completed yet) are also cancelled. If any Task or Future from the aws sequence is cancelled, it is treated as if it raised CancelledError \u2013 the gather() call is not cancelled in this case. This is to prevent the cancellation of one submitted Task/Future to cause other Tasks/Futures to be cancelled.  Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.  Example: import asyncio\n\nasync def factorial(name, number):\n    f = 1\n    for i in range(2, number + 1):\n        print(f\"Task {name}: Compute factorial({i})...\")\n        await asyncio.sleep(1)\n        f *= i\n    print(f\"Task {name}: factorial({number}) = {f}\")\n\nasync def main():\n    # Schedule three calls *concurrently*:\n    await asyncio.gather(\n        factorial(\"A\", 2),\n        factorial(\"B\", 3),\n        factorial(\"C\", 4),\n    )\n\nasyncio.run(main())\n\n# Expected output:\n#\n#     Task A: Compute factorial(2)...\n#     Task B: Compute factorial(2)...\n#     Task C: Compute factorial(2)...\n#     Task A: factorial(2) = 2\n#     Task B: Compute factorial(3)...\n#     Task C: Compute factorial(3)...\n#     Task B: factorial(3) = 6\n#     Task C: Compute factorial(4)...\n#     Task C: factorial(4) = 24\n  Note If return_exceptions is False, cancelling gather() after it has been marked done won\u2019t cancel any submitted awaitables. For instance, gather can be marked done after propagating an exception to the caller, therefore, calling gather.cancel() after catching an exception (raised by one of the awaitables) from gather won\u2019t cancel any other awaitables.   Changed in version 3.7: If the gather itself is cancelled, the cancellation is propagated regardless of return_exceptions.  \n"}, {"name": "asyncio.get_child_watcher()", "path": "library/asyncio-policy#asyncio.get_child_watcher", "type": "Asynchronous I/O", "text": " \nasyncio.get_child_watcher()  \nReturn the current child watcher for the current policy. \n"}, {"name": "asyncio.get_event_loop()", "path": "library/asyncio-eventloop#asyncio.get_event_loop", "type": "Asynchronous I/O", "text": " \nasyncio.get_event_loop()  \nGet the current event loop. If there is no current event loop set in the current OS thread, the OS thread is main, and set_event_loop() has not yet been called, asyncio will create a new event loop and set it as the current one. Because this function has rather complex behavior (especially when custom event loop policies are in use), using the get_running_loop() function is preferred to get_event_loop() in coroutines and callbacks. Consider also using the asyncio.run() function instead of using lower level functions to manually create and close an event loop. \n"}, {"name": "asyncio.get_event_loop_policy()", "path": "library/asyncio-policy#asyncio.get_event_loop_policy", "type": "Asynchronous I/O", "text": " \nasyncio.get_event_loop_policy()  \nReturn the current process-wide policy. \n"}, {"name": "asyncio.get_running_loop()", "path": "library/asyncio-eventloop#asyncio.get_running_loop", "type": "Asynchronous I/O", "text": " \nasyncio.get_running_loop()  \nReturn the running event loop in the current OS thread. If there is no running event loop a RuntimeError is raised. This function can only be called from a coroutine or a callback.  New in version 3.7.  \n"}, {"name": "asyncio.Handle", "path": "library/asyncio-eventloop#asyncio.Handle", "type": "Asynchronous I/O", "text": " \nclass asyncio.Handle  \nA callback wrapper object returned by loop.call_soon(), loop.call_soon_threadsafe().  \ncancel()  \nCancel the callback. If the callback has already been canceled or executed, this method has no effect. \n  \ncancelled()  \nReturn True if the callback was cancelled.  New in version 3.7.  \n \n"}, {"name": "asyncio.Handle.cancel()", "path": "library/asyncio-eventloop#asyncio.Handle.cancel", "type": "Asynchronous I/O", "text": " \ncancel()  \nCancel the callback. If the callback has already been canceled or executed, this method has no effect. \n"}, {"name": "asyncio.Handle.cancelled()", "path": "library/asyncio-eventloop#asyncio.Handle.cancelled", "type": "Asynchronous I/O", "text": " \ncancelled()  \nReturn True if the callback was cancelled.  New in version 3.7.  \n"}, {"name": "asyncio.IncompleteReadError", "path": "library/asyncio-exceptions#asyncio.IncompleteReadError", "type": "Asynchronous I/O", "text": " \nexception asyncio.IncompleteReadError  \nThe requested read operation did not complete fully. Raised by the asyncio stream APIs. This exception is a subclass of EOFError.  \nexpected  \nThe total number (int) of expected bytes. \n  \npartial  \nA string of bytes read before the end of stream was reached. \n \n"}, {"name": "asyncio.IncompleteReadError.expected", "path": "library/asyncio-exceptions#asyncio.IncompleteReadError.expected", "type": "Asynchronous I/O", "text": " \nexpected  \nThe total number (int) of expected bytes. \n"}, {"name": "asyncio.IncompleteReadError.partial", "path": "library/asyncio-exceptions#asyncio.IncompleteReadError.partial", "type": "Asynchronous I/O", "text": " \npartial  \nA string of bytes read before the end of stream was reached. \n"}, {"name": "asyncio.InvalidStateError", "path": "library/asyncio-exceptions#asyncio.InvalidStateError", "type": "Asynchronous I/O", "text": " \nexception asyncio.InvalidStateError  \nInvalid internal state of Task or Future. Can be raised in situations like setting a result value for a Future object that already has a result value set. \n"}, {"name": "asyncio.iscoroutine()", "path": "library/asyncio-task#asyncio.iscoroutine", "type": "Asynchronous I/O", "text": " \nasyncio.iscoroutine(obj)  \nReturn True if obj is a coroutine object. This method is different from inspect.iscoroutine() because it returns True for generator-based coroutines. \n"}, {"name": "asyncio.iscoroutinefunction()", "path": "library/asyncio-task#asyncio.iscoroutinefunction", "type": "Asynchronous I/O", "text": " \nasyncio.iscoroutinefunction(func)  \nReturn True if func is a coroutine function. This method is different from inspect.iscoroutinefunction() because it returns True for generator-based coroutine functions decorated with @coroutine. \n"}, {"name": "asyncio.isfuture()", "path": "library/asyncio-future#asyncio.isfuture", "type": "Asynchronous I/O", "text": " \nasyncio.isfuture(obj)  \nReturn True if obj is either of:  an instance of asyncio.Future, an instance of asyncio.Task, a Future-like object with a _asyncio_future_blocking attribute.   New in version 3.5.  \n"}, {"name": "asyncio.LifoQueue", "path": "library/asyncio-queue#asyncio.LifoQueue", "type": "Asynchronous I/O", "text": " \nclass asyncio.LifoQueue  \nA variant of Queue that retrieves most recently added entries first (last in, first out). \n"}, {"name": "asyncio.LimitOverrunError", "path": "library/asyncio-exceptions#asyncio.LimitOverrunError", "type": "Asynchronous I/O", "text": " \nexception asyncio.LimitOverrunError  \nReached the buffer size limit while looking for a separator. Raised by the asyncio stream APIs.  \nconsumed  \nThe total number of to be consumed bytes. \n \n"}, {"name": "asyncio.LimitOverrunError.consumed", "path": "library/asyncio-exceptions#asyncio.LimitOverrunError.consumed", "type": "Asynchronous I/O", "text": " \nconsumed  \nThe total number of to be consumed bytes. \n"}, {"name": "asyncio.Lock", "path": "library/asyncio-sync#asyncio.Lock", "type": "Asynchronous I/O", "text": " \nclass asyncio.Lock(*, loop=None)  \nImplements a mutex lock for asyncio tasks. Not thread-safe. An asyncio lock can be used to guarantee exclusive access to a shared resource. The preferred way to use a Lock is an async with statement: lock = asyncio.Lock()\n\n# ... later\nasync with lock:\n    # access shared state\n which is equivalent to: lock = asyncio.Lock()\n\n# ... later\nawait lock.acquire()\ntry:\n    # access shared state\nfinally:\n    lock.release()\n  Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.   \ncoroutine acquire()  \nAcquire the lock. This method waits until the lock is unlocked, sets it to locked and returns True. When more than one coroutine is blocked in acquire() waiting for the lock to be unlocked, only one coroutine eventually proceeds. Acquiring a lock is fair: the coroutine that proceeds will be the first coroutine that started waiting on the lock. \n  \nrelease()  \nRelease the lock. When the lock is locked, reset it to unlocked and return. If the lock is unlocked, a RuntimeError is raised. \n  \nlocked()  \nReturn True if the lock is locked. \n \n"}, {"name": "asyncio.Lock.acquire()", "path": "library/asyncio-sync#asyncio.Lock.acquire", "type": "Asynchronous I/O", "text": " \ncoroutine acquire()  \nAcquire the lock. This method waits until the lock is unlocked, sets it to locked and returns True. When more than one coroutine is blocked in acquire() waiting for the lock to be unlocked, only one coroutine eventually proceeds. Acquiring a lock is fair: the coroutine that proceeds will be the first coroutine that started waiting on the lock. \n"}, {"name": "asyncio.Lock.locked()", "path": "library/asyncio-sync#asyncio.Lock.locked", "type": "Asynchronous I/O", "text": " \nlocked()  \nReturn True if the lock is locked. \n"}, {"name": "asyncio.Lock.release()", "path": "library/asyncio-sync#asyncio.Lock.release", "type": "Asynchronous I/O", "text": " \nrelease()  \nRelease the lock. When the lock is locked, reset it to unlocked and return. If the lock is unlocked, a RuntimeError is raised. \n"}, {"name": "asyncio.loop.add_reader()", "path": "library/asyncio-eventloop#asyncio.loop.add_reader", "type": "Asynchronous I/O", "text": " \nloop.add_reader(fd, callback, *args)  \nStart monitoring the fd file descriptor for read availability and invoke callback with the specified arguments once fd is available for reading. \n"}, {"name": "asyncio.loop.add_signal_handler()", "path": "library/asyncio-eventloop#asyncio.loop.add_signal_handler", "type": "Asynchronous I/O", "text": " \nloop.add_signal_handler(signum, callback, *args)  \nSet callback as the handler for the signum signal. The callback will be invoked by loop, along with other queued callbacks and runnable coroutines of that event loop. Unlike signal handlers registered using signal.signal(), a callback registered with this function is allowed to interact with the event loop. Raise ValueError if the signal number is invalid or uncatchable. Raise RuntimeError if there is a problem setting up the handler. Use functools.partial() to pass keyword arguments to callback. Like signal.signal(), this function must be invoked in the main thread. \n"}, {"name": "asyncio.loop.add_writer()", "path": "library/asyncio-eventloop#asyncio.loop.add_writer", "type": "Asynchronous I/O", "text": " \nloop.add_writer(fd, callback, *args)  \nStart monitoring the fd file descriptor for write availability and invoke callback with the specified arguments once fd is available for writing. Use functools.partial() to pass keyword arguments to callback. \n"}, {"name": "asyncio.loop.call_at()", "path": "library/asyncio-eventloop#asyncio.loop.call_at", "type": "Asynchronous I/O", "text": " \nloop.call_at(when, callback, *args, context=None)  \nSchedule callback to be called at the given absolute timestamp when (an int or a float), using the same time reference as loop.time(). This method\u2019s behavior is the same as call_later(). An instance of asyncio.TimerHandle is returned which can be used to cancel the callback.  Changed in version 3.7: The context keyword-only parameter was added. See PEP 567 for more details.   Changed in version 3.8: In Python 3.7 and earlier with the default event loop implementation, the difference between when and the current time could not exceed one day. This has been fixed in Python 3.8.  \n"}, {"name": "asyncio.loop.call_exception_handler()", "path": "library/asyncio-eventloop#asyncio.loop.call_exception_handler", "type": "Asynchronous I/O", "text": " \nloop.call_exception_handler(context)  \nCall the current event loop exception handler. context is a dict object containing the following keys (new keys may be introduced in future Python versions):  \u2018message\u2019: Error message; \u2018exception\u2019 (optional): Exception object; \u2018future\u2019 (optional): asyncio.Future instance; \u2018handle\u2019 (optional): asyncio.Handle instance; \u2018protocol\u2019 (optional): Protocol instance; \u2018transport\u2019 (optional): Transport instance; \u2018socket\u2019 (optional): socket.socket instance.   Note This method should not be overloaded in subclassed event loops. For custom exception handling, use the set_exception_handler() method.  \n"}, {"name": "asyncio.loop.call_later()", "path": "library/asyncio-eventloop#asyncio.loop.call_later", "type": "Asynchronous I/O", "text": " \nloop.call_later(delay, callback, *args, context=None)  \nSchedule callback to be called after the given delay number of seconds (can be either an int or a float). An instance of asyncio.TimerHandle is returned which can be used to cancel the callback. callback will be called exactly once. If two callbacks are scheduled for exactly the same time, the order in which they are called is undefined. The optional positional args will be passed to the callback when it is called. If you want the callback to be called with keyword arguments use functools.partial(). An optional keyword-only context argument allows specifying a custom contextvars.Context for the callback to run in. The current context is used when no context is provided.  Changed in version 3.7: The context keyword-only parameter was added. See PEP 567 for more details.   Changed in version 3.8: In Python 3.7 and earlier with the default event loop implementation, the delay could not exceed one day. This has been fixed in Python 3.8.  \n"}, {"name": "asyncio.loop.call_soon()", "path": "library/asyncio-eventloop#asyncio.loop.call_soon", "type": "Asynchronous I/O", "text": " \nloop.call_soon(callback, *args, context=None)  \nSchedule the callback callback to be called with args arguments at the next iteration of the event loop. Callbacks are called in the order in which they are registered. Each callback will be called exactly once. An optional keyword-only context argument allows specifying a custom contextvars.Context for the callback to run in. The current context is used when no context is provided. An instance of asyncio.Handle is returned, which can be used later to cancel the callback. This method is not thread-safe. \n"}, {"name": "asyncio.loop.call_soon_threadsafe()", "path": "library/asyncio-eventloop#asyncio.loop.call_soon_threadsafe", "type": "Asynchronous I/O", "text": " \nloop.call_soon_threadsafe(callback, *args, context=None)  \nA thread-safe variant of call_soon(). Must be used to schedule callbacks from another thread. See the concurrency and multithreading section of the documentation. \n"}, {"name": "asyncio.loop.close()", "path": "library/asyncio-eventloop#asyncio.loop.close", "type": "Asynchronous I/O", "text": " \nloop.close()  \nClose the event loop. The loop must not be running when this function is called. Any pending callbacks will be discarded. This method clears all queues and shuts down the executor, but does not wait for the executor to finish. This method is idempotent and irreversible. No other methods should be called after the event loop is closed. \n"}, {"name": "asyncio.loop.connect_accepted_socket()", "path": "library/asyncio-eventloop#asyncio.loop.connect_accepted_socket", "type": "Asynchronous I/O", "text": " \ncoroutine loop.connect_accepted_socket(protocol_factory, sock, *, ssl=None, ssl_handshake_timeout=None)  \nWrap an already accepted connection into a transport/protocol pair. This method can be used by servers that accept connections outside of asyncio but that use asyncio to handle them. Parameters:  \nprotocol_factory must be a callable returning a protocol implementation. \nsock is a preexisting socket object returned from socket.accept. \nssl can be set to an SSLContext to enable SSL over the accepted connections. \nssl_handshake_timeout is (for an SSL connection) the time in seconds to wait for the SSL handshake to complete before aborting the connection. 60.0 seconds if None (default).  Returns a (transport, protocol) pair.  New in version 3.7: The ssl_handshake_timeout parameter.   New in version 3.5.3.  \n"}, {"name": "asyncio.loop.connect_read_pipe()", "path": "library/asyncio-eventloop#asyncio.loop.connect_read_pipe", "type": "Asynchronous I/O", "text": " \ncoroutine loop.connect_read_pipe(protocol_factory, pipe)  \nRegister the read end of pipe in the event loop. protocol_factory must be a callable returning an asyncio protocol implementation. pipe is a file-like object. Return pair (transport, protocol), where transport supports the ReadTransport interface and protocol is an object instantiated by the protocol_factory. With SelectorEventLoop event loop, the pipe is set to non-blocking mode. \n"}, {"name": "asyncio.loop.connect_write_pipe()", "path": "library/asyncio-eventloop#asyncio.loop.connect_write_pipe", "type": "Asynchronous I/O", "text": " \ncoroutine loop.connect_write_pipe(protocol_factory, pipe)  \nRegister the write end of pipe in the event loop. protocol_factory must be a callable returning an asyncio protocol implementation. pipe is file-like object. Return pair (transport, protocol), where transport supports WriteTransport interface and protocol is an object instantiated by the protocol_factory. With SelectorEventLoop event loop, the pipe is set to non-blocking mode. \n"}, {"name": "asyncio.loop.create_connection()", "path": "library/asyncio-eventloop#asyncio.loop.create_connection", "type": "Asynchronous I/O", "text": " \ncoroutine loop.create_connection(protocol_factory, host=None, port=None, *, ssl=None, family=0, proto=0, flags=0, sock=None, local_addr=None, server_hostname=None, ssl_handshake_timeout=None, happy_eyeballs_delay=None, interleave=None)  \nOpen a streaming transport connection to a given address specified by host and port. The socket family can be either AF_INET or AF_INET6 depending on host (or the family argument, if provided). The socket type will be SOCK_STREAM. protocol_factory must be a callable returning an asyncio protocol implementation. This method will try to establish the connection in the background. When successful, it returns a (transport, protocol) pair. The chronological synopsis of the underlying operation is as follows:  The connection is established and a transport is created for it. \nprotocol_factory is called without arguments and is expected to return a protocol instance. The protocol instance is coupled with the transport by calling its connection_made() method. A (transport, protocol) tuple is returned on success.  The created transport is an implementation-dependent bidirectional stream. Other arguments:  \nssl: if given and not false, a SSL/TLS transport is created (by default a plain TCP transport is created). If ssl is a ssl.SSLContext object, this context is used to create the transport; if ssl is True, a default context returned from ssl.create_default_context() is used.  See also SSL/TLS security considerations   \nserver_hostname sets or overrides the hostname that the target server\u2019s certificate will be matched against. Should only be passed if ssl is not None. By default the value of the host argument is used. If host is empty, there is no default and you must pass a value for server_hostname. If server_hostname is an empty string, hostname matching is disabled (which is a serious security risk, allowing for potential man-in-the-middle attacks). \nfamily, proto, flags are the optional address family, protocol and flags to be passed through to getaddrinfo() for host resolution. If given, these should all be integers from the corresponding socket module constants. \nhappy_eyeballs_delay, if given, enables Happy Eyeballs for this connection. It should be a floating-point number representing the amount of time in seconds to wait for a connection attempt to complete, before starting the next attempt in parallel. This is the \u201cConnection Attempt Delay\u201d as defined in RFC 8305. A sensible default value recommended by the RFC is 0.25 (250 milliseconds). \ninterleave controls address reordering when a host name resolves to multiple IP addresses. If 0 or unspecified, no reordering is done, and addresses are tried in the order returned by getaddrinfo(). If a positive integer is specified, the addresses are interleaved by address family, and the given integer is interpreted as \u201cFirst Address Family Count\u201d as defined in RFC 8305. The default is 0 if happy_eyeballs_delay is not specified, and 1 if it is. \nsock, if given, should be an existing, already connected socket.socket object to be used by the transport. If sock is given, none of host, port, family, proto, flags, happy_eyeballs_delay, interleave and local_addr should be specified. \nlocal_addr, if given, is a (local_host, local_port) tuple used to bind the socket to locally. The local_host and local_port are looked up using getaddrinfo(), similarly to host and port. \nssl_handshake_timeout is (for a TLS connection) the time in seconds to wait for the TLS handshake to complete before aborting the connection. 60.0 seconds if None (default).   New in version 3.8: Added the happy_eyeballs_delay and interleave parameters. Happy Eyeballs Algorithm: Success with Dual-Stack Hosts. When a server\u2019s IPv4 path and protocol are working, but the server\u2019s IPv6 path and protocol are not working, a dual-stack client application experiences significant connection delay compared to an IPv4-only client. This is undesirable because it causes the dual- stack client to have a worse user experience. This document specifies requirements for algorithms that reduce this user-visible delay and provides an algorithm. For more information: https://tools.ietf.org/html/rfc6555   New in version 3.7: The ssl_handshake_timeout parameter.   Changed in version 3.6: The socket option TCP_NODELAY is set by default for all TCP connections.   Changed in version 3.5: Added support for SSL/TLS in ProactorEventLoop.   See also The open_connection() function is a high-level alternative API. It returns a pair of (StreamReader, StreamWriter) that can be used directly in async/await code.  \n"}, {"name": "asyncio.loop.create_datagram_endpoint()", "path": "library/asyncio-eventloop#asyncio.loop.create_datagram_endpoint", "type": "Asynchronous I/O", "text": " \ncoroutine loop.create_datagram_endpoint(protocol_factory, local_addr=None, remote_addr=None, *, family=0, proto=0, flags=0, reuse_address=None, reuse_port=None, allow_broadcast=None, sock=None)  \n Note The parameter reuse_address is no longer supported, as using SO_REUSEADDR poses a significant security concern for UDP. Explicitly passing reuse_address=True will raise an exception. When multiple processes with differing UIDs assign sockets to an identical UDP socket address with SO_REUSEADDR, incoming packets can become randomly distributed among the sockets. For supported platforms, reuse_port can be used as a replacement for similar functionality. With reuse_port, SO_REUSEPORT is used instead, which specifically prevents processes with differing UIDs from assigning sockets to the same socket address.  Create a datagram connection. The socket family can be either AF_INET, AF_INET6, or AF_UNIX, depending on host (or the family argument, if provided). The socket type will be SOCK_DGRAM. protocol_factory must be a callable returning a protocol implementation. A tuple of (transport, protocol) is returned on success. Other arguments:  \nlocal_addr, if given, is a (local_host, local_port) tuple used to bind the socket to locally. The local_host and local_port are looked up using getaddrinfo(). \nremote_addr, if given, is a (remote_host, remote_port) tuple used to connect the socket to a remote address. The remote_host and remote_port are looked up using getaddrinfo(). \nfamily, proto, flags are the optional address family, protocol and flags to be passed through to getaddrinfo() for host resolution. If given, these should all be integers from the corresponding socket module constants. \nreuse_port tells the kernel to allow this endpoint to be bound to the same port as other existing endpoints are bound to, so long as they all set this flag when being created. This option is not supported on Windows and some Unixes. If the SO_REUSEPORT constant is not defined then this capability is unsupported. \nallow_broadcast tells the kernel to allow this endpoint to send messages to the broadcast address. \nsock can optionally be specified in order to use a preexisting, already connected, socket.socket object to be used by the transport. If specified, local_addr and remote_addr should be omitted (must be None).  See UDP echo client protocol and UDP echo server protocol examples.  Changed in version 3.4.4: The family, proto, flags, reuse_address, reuse_port, *allow_broadcast, and sock parameters were added.   Changed in version 3.8.1: The reuse_address parameter is no longer supported due to security concerns.   Changed in version 3.8: Added support for Windows.  \n"}, {"name": "asyncio.loop.create_future()", "path": "library/asyncio-eventloop#asyncio.loop.create_future", "type": "Asynchronous I/O", "text": " \nloop.create_future()  \nCreate an asyncio.Future object attached to the event loop. This is the preferred way to create Futures in asyncio. This lets third-party event loops provide alternative implementations of the Future object (with better performance or instrumentation).  New in version 3.5.2.  \n"}, {"name": "asyncio.loop.create_server()", "path": "library/asyncio-eventloop#asyncio.loop.create_server", "type": "Asynchronous I/O", "text": " \ncoroutine loop.create_server(protocol_factory, host=None, port=None, *, family=socket.AF_UNSPEC, flags=socket.AI_PASSIVE, sock=None, backlog=100, ssl=None, reuse_address=None, reuse_port=None, ssl_handshake_timeout=None, start_serving=True)  \nCreate a TCP server (socket type SOCK_STREAM) listening on port of the host address. Returns a Server object. Arguments:  \nprotocol_factory must be a callable returning a protocol implementation. \nThe host parameter can be set to several types which determine where the server would be listening:  If host is a string, the TCP server is bound to a single network interface specified by host. If host is a sequence of strings, the TCP server is bound to all network interfaces specified by the sequence. If host is an empty string or None, all interfaces are assumed and a list of multiple sockets will be returned (most likely one for IPv4 and another one for IPv6).   \nfamily can be set to either socket.AF_INET or AF_INET6 to force the socket to use IPv4 or IPv6. If not set, the family will be determined from host name (defaults to AF_UNSPEC). \nflags is a bitmask for getaddrinfo(). \nsock can optionally be specified in order to use a preexisting socket object. If specified, host and port must not be specified. \nbacklog is the maximum number of queued connections passed to listen() (defaults to 100). \nssl can be set to an SSLContext instance to enable TLS over the accepted connections. \nreuse_address tells the kernel to reuse a local socket in TIME_WAIT state, without waiting for its natural timeout to expire. If not specified will automatically be set to True on Unix. \nreuse_port tells the kernel to allow this endpoint to be bound to the same port as other existing endpoints are bound to, so long as they all set this flag when being created. This option is not supported on Windows. \nssl_handshake_timeout is (for a TLS server) the time in seconds to wait for the TLS handshake to complete before aborting the connection. 60.0 seconds if None (default). \nstart_serving set to True (the default) causes the created server to start accepting connections immediately. When set to False, the user should await on Server.start_serving() or Server.serve_forever() to make the server to start accepting connections.   New in version 3.7: Added ssl_handshake_timeout and start_serving parameters.   Changed in version 3.6: The socket option TCP_NODELAY is set by default for all TCP connections.   Changed in version 3.5: Added support for SSL/TLS in ProactorEventLoop.   Changed in version 3.5.1: The host parameter can be a sequence of strings.   See also The start_server() function is a higher-level alternative API that returns a pair of StreamReader and StreamWriter that can be used in an async/await code.  \n"}, {"name": "asyncio.loop.create_task()", "path": "library/asyncio-eventloop#asyncio.loop.create_task", "type": "Asynchronous I/O", "text": " \nloop.create_task(coro, *, name=None)  \nSchedule the execution of a Coroutines. Return a Task object. Third-party event loops can use their own subclass of Task for interoperability. In this case, the result type is a subclass of Task. If the name argument is provided and not None, it is set as the name of the task using Task.set_name().  Changed in version 3.8: Added the name parameter.  \n"}, {"name": "asyncio.loop.create_unix_connection()", "path": "library/asyncio-eventloop#asyncio.loop.create_unix_connection", "type": "Asynchronous I/O", "text": " \ncoroutine loop.create_unix_connection(protocol_factory, path=None, *, ssl=None, sock=None, server_hostname=None, ssl_handshake_timeout=None)  \nCreate a Unix connection. The socket family will be AF_UNIX; socket type will be SOCK_STREAM. A tuple of (transport, protocol) is returned on success. path is the name of a Unix domain socket and is required, unless a sock parameter is specified. Abstract Unix sockets, str, bytes, and Path paths are supported. See the documentation of the loop.create_connection() method for information about arguments to this method. Availability: Unix.  New in version 3.7: The ssl_handshake_timeout parameter.   Changed in version 3.7: The path parameter can now be a path-like object.  \n"}, {"name": "asyncio.loop.create_unix_server()", "path": "library/asyncio-eventloop#asyncio.loop.create_unix_server", "type": "Asynchronous I/O", "text": " \ncoroutine loop.create_unix_server(protocol_factory, path=None, *, sock=None, backlog=100, ssl=None, ssl_handshake_timeout=None, start_serving=True)  \nSimilar to loop.create_server() but works with the AF_UNIX socket family. path is the name of a Unix domain socket, and is required, unless a sock argument is provided. Abstract Unix sockets, str, bytes, and Path paths are supported. See the documentation of the loop.create_server() method for information about arguments to this method. Availability: Unix.  New in version 3.7: The ssl_handshake_timeout and start_serving parameters.   Changed in version 3.7: The path parameter can now be a Path object.  \n"}, {"name": "asyncio.loop.default_exception_handler()", "path": "library/asyncio-eventloop#asyncio.loop.default_exception_handler", "type": "Asynchronous I/O", "text": " \nloop.default_exception_handler(context)  \nDefault exception handler. This is called when an exception occurs and no exception handler is set. This can be called by a custom exception handler that wants to defer to the default handler behavior. context parameter has the same meaning as in call_exception_handler(). \n"}, {"name": "asyncio.loop.getaddrinfo()", "path": "library/asyncio-eventloop#asyncio.loop.getaddrinfo", "type": "Asynchronous I/O", "text": " \ncoroutine loop.getaddrinfo(host, port, *, family=0, type=0, proto=0, flags=0)  \nAsynchronous version of socket.getaddrinfo(). \n"}, {"name": "asyncio.loop.getnameinfo()", "path": "library/asyncio-eventloop#asyncio.loop.getnameinfo", "type": "Asynchronous I/O", "text": " \ncoroutine loop.getnameinfo(sockaddr, flags=0)  \nAsynchronous version of socket.getnameinfo(). \n"}, {"name": "asyncio.loop.get_debug()", "path": "library/asyncio-eventloop#asyncio.loop.get_debug", "type": "Asynchronous I/O", "text": " \nloop.get_debug()  \nGet the debug mode (bool) of the event loop. The default value is True if the environment variable PYTHONASYNCIODEBUG is set to a non-empty string, False otherwise. \n"}, {"name": "asyncio.loop.get_exception_handler()", "path": "library/asyncio-eventloop#asyncio.loop.get_exception_handler", "type": "Asynchronous I/O", "text": " \nloop.get_exception_handler()  \nReturn the current exception handler, or None if no custom exception handler was set.  New in version 3.5.2.  \n"}, {"name": "asyncio.loop.get_task_factory()", "path": "library/asyncio-eventloop#asyncio.loop.get_task_factory", "type": "Asynchronous I/O", "text": " \nloop.get_task_factory()  \nReturn a task factory or None if the default one is in use. \n"}, {"name": "asyncio.loop.is_closed()", "path": "library/asyncio-eventloop#asyncio.loop.is_closed", "type": "Asynchronous I/O", "text": " \nloop.is_closed()  \nReturn True if the event loop was closed. \n"}, {"name": "asyncio.loop.is_running()", "path": "library/asyncio-eventloop#asyncio.loop.is_running", "type": "Asynchronous I/O", "text": " \nloop.is_running()  \nReturn True if the event loop is currently running. \n"}, {"name": "asyncio.loop.remove_reader()", "path": "library/asyncio-eventloop#asyncio.loop.remove_reader", "type": "Asynchronous I/O", "text": " \nloop.remove_reader(fd)  \nStop monitoring the fd file descriptor for read availability. \n"}, {"name": "asyncio.loop.remove_signal_handler()", "path": "library/asyncio-eventloop#asyncio.loop.remove_signal_handler", "type": "Asynchronous I/O", "text": " \nloop.remove_signal_handler(sig)  \nRemove the handler for the sig signal. Return True if the signal handler was removed, or False if no handler was set for the given signal. Availability: Unix. \n"}, {"name": "asyncio.loop.remove_writer()", "path": "library/asyncio-eventloop#asyncio.loop.remove_writer", "type": "Asynchronous I/O", "text": " \nloop.remove_writer(fd)  \nStop monitoring the fd file descriptor for write availability. \n"}, {"name": "asyncio.loop.run_forever()", "path": "library/asyncio-eventloop#asyncio.loop.run_forever", "type": "Asynchronous I/O", "text": " \nloop.run_forever()  \nRun the event loop until stop() is called. If stop() is called before run_forever() is called, the loop will poll the I/O selector once with a timeout of zero, run all callbacks scheduled in response to I/O events (and those that were already scheduled), and then exit. If stop() is called while run_forever() is running, the loop will run the current batch of callbacks and then exit. Note that new callbacks scheduled by callbacks will not run in this case; instead, they will run the next time run_forever() or run_until_complete() is called. \n"}, {"name": "asyncio.loop.run_in_executor()", "path": "library/asyncio-eventloop#asyncio.loop.run_in_executor", "type": "Asynchronous I/O", "text": " \nawaitable loop.run_in_executor(executor, func, *args)  \nArrange for func to be called in the specified executor. The executor argument should be an concurrent.futures.Executor instance. The default executor is used if executor is None. Example: import asyncio\nimport concurrent.futures\n\ndef blocking_io():\n    # File operations (such as logging) can block the\n    # event loop: run them in a thread pool.\n    with open('/dev/urandom', 'rb') as f:\n        return f.read(100)\n\ndef cpu_bound():\n    # CPU-bound operations will block the event loop:\n    # in general it is preferable to run them in a\n    # process pool.\n    return sum(i * i for i in range(10 ** 7))\n\nasync def main():\n    loop = asyncio.get_running_loop()\n\n    ## Options:\n\n    # 1. Run in the default loop's executor:\n    result = await loop.run_in_executor(\n        None, blocking_io)\n    print('default thread pool', result)\n\n    # 2. Run in a custom thread pool:\n    with concurrent.futures.ThreadPoolExecutor() as pool:\n        result = await loop.run_in_executor(\n            pool, blocking_io)\n        print('custom thread pool', result)\n\n    # 3. Run in a custom process pool:\n    with concurrent.futures.ProcessPoolExecutor() as pool:\n        result = await loop.run_in_executor(\n            pool, cpu_bound)\n        print('custom process pool', result)\n\nasyncio.run(main())\n This method returns a asyncio.Future object. Use functools.partial() to pass keyword arguments to func.  Changed in version 3.5.3: loop.run_in_executor() no longer configures the max_workers of the thread pool executor it creates, instead leaving it up to the thread pool executor (ThreadPoolExecutor) to set the default.  \n"}, {"name": "asyncio.loop.run_until_complete()", "path": "library/asyncio-eventloop#asyncio.loop.run_until_complete", "type": "Asynchronous I/O", "text": " \nloop.run_until_complete(future)  \nRun until the future (an instance of Future) has completed. If the argument is a coroutine object it is implicitly scheduled to run as a asyncio.Task. Return the Future\u2019s result or raise its exception. \n"}, {"name": "asyncio.loop.sendfile()", "path": "library/asyncio-eventloop#asyncio.loop.sendfile", "type": "Asynchronous I/O", "text": " \ncoroutine loop.sendfile(transport, file, offset=0, count=None, *, fallback=True)  \nSend a file over a transport. Return the total number of bytes sent. The method uses high-performance os.sendfile() if available. file must be a regular file object opened in binary mode. offset tells from where to start reading the file. If specified, count is the total number of bytes to transmit as opposed to sending the file until EOF is reached. File position is always updated, even when this method raises an error, and file.tell() can be used to obtain the actual number of bytes sent. fallback set to True makes asyncio to manually read and send the file when the platform does not support the sendfile system call (e.g. Windows or SSL socket on Unix). Raise SendfileNotAvailableError if the system does not support the sendfile syscall and fallback is False.  New in version 3.7.  \n"}, {"name": "asyncio.loop.set_debug()", "path": "library/asyncio-eventloop#asyncio.loop.set_debug", "type": "Asynchronous I/O", "text": " \nloop.set_debug(enabled: bool)  \nSet the debug mode of the event loop.  Changed in version 3.7: The new Python Development Mode can now also be used to enable the debug mode.  \n"}, {"name": "asyncio.loop.set_default_executor()", "path": "library/asyncio-eventloop#asyncio.loop.set_default_executor", "type": "Asynchronous I/O", "text": " \nloop.set_default_executor(executor)  \nSet executor as the default executor used by run_in_executor(). executor should be an instance of ThreadPoolExecutor.  Deprecated since version 3.8: Using an executor that is not an instance of ThreadPoolExecutor is deprecated and will trigger an error in Python 3.9.  executor must be an instance of concurrent.futures.ThreadPoolExecutor. \n"}, {"name": "asyncio.loop.set_exception_handler()", "path": "library/asyncio-eventloop#asyncio.loop.set_exception_handler", "type": "Asynchronous I/O", "text": " \nloop.set_exception_handler(handler)  \nSet handler as the new event loop exception handler. If handler is None, the default exception handler will be set. Otherwise, handler must be a callable with the signature matching (loop, context), where loop is a reference to the active event loop, and context is a dict object containing the details of the exception (see call_exception_handler() documentation for details about context). \n"}, {"name": "asyncio.loop.set_task_factory()", "path": "library/asyncio-eventloop#asyncio.loop.set_task_factory", "type": "Asynchronous I/O", "text": " \nloop.set_task_factory(factory)  \nSet a task factory that will be used by loop.create_task(). If factory is None the default task factory will be set. Otherwise, factory must be a callable with the signature matching (loop, coro), where loop is a reference to the active event loop, and coro is a coroutine object. The callable must return a asyncio.Future-compatible object. \n"}, {"name": "asyncio.loop.shutdown_asyncgens()", "path": "library/asyncio-eventloop#asyncio.loop.shutdown_asyncgens", "type": "Asynchronous I/O", "text": " \ncoroutine loop.shutdown_asyncgens()  \nSchedule all currently open asynchronous generator objects to close with an aclose() call. After calling this method, the event loop will issue a warning if a new asynchronous generator is iterated. This should be used to reliably finalize all scheduled asynchronous generators. Note that there is no need to call this function when asyncio.run() is used. Example: try:\n    loop.run_forever()\nfinally:\n    loop.run_until_complete(loop.shutdown_asyncgens())\n    loop.close()\n  New in version 3.6.  \n"}, {"name": "asyncio.loop.shutdown_default_executor()", "path": "library/asyncio-eventloop#asyncio.loop.shutdown_default_executor", "type": "Asynchronous I/O", "text": " \ncoroutine loop.shutdown_default_executor()  \nSchedule the closure of the default executor and wait for it to join all of the threads in the ThreadPoolExecutor. After calling this method, a RuntimeError will be raised if loop.run_in_executor() is called while using the default executor. Note that there is no need to call this function when asyncio.run() is used.  New in version 3.9.  \n"}, {"name": "asyncio.loop.sock_accept()", "path": "library/asyncio-eventloop#asyncio.loop.sock_accept", "type": "Asynchronous I/O", "text": " \ncoroutine loop.sock_accept(sock)  \nAccept a connection. Modeled after the blocking socket.accept() method. The socket must be bound to an address and listening for connections. The return value is a pair (conn, address) where conn is a new socket object usable to send and receive data on the connection, and address is the address bound to the socket on the other end of the connection. sock must be a non-blocking socket.  Changed in version 3.7: Even though the method was always documented as a coroutine method, before Python 3.7 it returned a Future. Since Python 3.7, this is an async def method.   See also loop.create_server() and start_server().  \n"}, {"name": "asyncio.loop.sock_connect()", "path": "library/asyncio-eventloop#asyncio.loop.sock_connect", "type": "Asynchronous I/O", "text": " \ncoroutine loop.sock_connect(sock, address)  \nConnect sock to a remote socket at address. Asynchronous version of socket.connect(). sock must be a non-blocking socket.  Changed in version 3.5.2: address no longer needs to be resolved. sock_connect will try to check if the address is already resolved by calling socket.inet_pton(). If not, loop.getaddrinfo() will be used to resolve the address.   See also loop.create_connection() and asyncio.open_connection().  \n"}, {"name": "asyncio.loop.sock_recv()", "path": "library/asyncio-eventloop#asyncio.loop.sock_recv", "type": "Asynchronous I/O", "text": " \ncoroutine loop.sock_recv(sock, nbytes)  \nReceive up to nbytes from sock. Asynchronous version of socket.recv(). Return the received data as a bytes object. sock must be a non-blocking socket.  Changed in version 3.7: Even though this method was always documented as a coroutine method, releases before Python 3.7 returned a Future. Since Python 3.7 this is an async def method.  \n"}, {"name": "asyncio.loop.sock_recv_into()", "path": "library/asyncio-eventloop#asyncio.loop.sock_recv_into", "type": "Asynchronous I/O", "text": " \ncoroutine loop.sock_recv_into(sock, buf)  \nReceive data from sock into the buf buffer. Modeled after the blocking socket.recv_into() method. Return the number of bytes written to the buffer. sock must be a non-blocking socket.  New in version 3.7.  \n"}, {"name": "asyncio.loop.sock_sendall()", "path": "library/asyncio-eventloop#asyncio.loop.sock_sendall", "type": "Asynchronous I/O", "text": " \ncoroutine loop.sock_sendall(sock, data)  \nSend data to the sock socket. Asynchronous version of socket.sendall(). This method continues to send to the socket until either all data in data has been sent or an error occurs. None is returned on success. On error, an exception is raised. Additionally, there is no way to determine how much data, if any, was successfully processed by the receiving end of the connection. sock must be a non-blocking socket.  Changed in version 3.7: Even though the method was always documented as a coroutine method, before Python 3.7 it returned an Future. Since Python 3.7, this is an async def method.  \n"}, {"name": "asyncio.loop.sock_sendfile()", "path": "library/asyncio-eventloop#asyncio.loop.sock_sendfile", "type": "Asynchronous I/O", "text": " \ncoroutine loop.sock_sendfile(sock, file, offset=0, count=None, *, fallback=True)  \nSend a file using high-performance os.sendfile if possible. Return the total number of bytes sent. Asynchronous version of socket.sendfile(). sock must be a non-blocking socket.SOCK_STREAM socket. file must be a regular file object open in binary mode. offset tells from where to start reading the file. If specified, count is the total number of bytes to transmit as opposed to sending the file until EOF is reached. File position is always updated, even when this method raises an error, and file.tell() can be used to obtain the actual number of bytes sent. fallback, when set to True, makes asyncio manually read and send the file when the platform does not support the sendfile syscall (e.g. Windows or SSL socket on Unix). Raise SendfileNotAvailableError if the system does not support sendfile syscall and fallback is False. sock must be a non-blocking socket.  New in version 3.7.  \n"}, {"name": "asyncio.loop.start_tls()", "path": "library/asyncio-eventloop#asyncio.loop.start_tls", "type": "Asynchronous I/O", "text": " \ncoroutine loop.start_tls(transport, protocol, sslcontext, *, server_side=False, server_hostname=None, ssl_handshake_timeout=None)  \nUpgrade an existing transport-based connection to TLS. Return a new transport instance, that the protocol must start using immediately after the await. The transport instance passed to the start_tls method should never be used again. Parameters:  \ntransport and protocol instances that methods like create_server() and create_connection() return. \nsslcontext: a configured instance of SSLContext. \nserver_side pass True when a server-side connection is being upgraded (like the one created by create_server()). \nserver_hostname: sets or overrides the host name that the target server\u2019s certificate will be matched against. \nssl_handshake_timeout is (for a TLS connection) the time in seconds to wait for the TLS handshake to complete before aborting the connection. 60.0 seconds if None (default).   New in version 3.7.  \n"}, {"name": "asyncio.loop.stop()", "path": "library/asyncio-eventloop#asyncio.loop.stop", "type": "Asynchronous I/O", "text": " \nloop.stop()  \nStop the event loop. \n"}, {"name": "asyncio.loop.subprocess_exec()", "path": "library/asyncio-eventloop#asyncio.loop.subprocess_exec", "type": "Asynchronous I/O", "text": " \ncoroutine loop.subprocess_exec(protocol_factory, *args, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, **kwargs)  \nCreate a subprocess from one or more string arguments specified by args. args must be a list of strings represented by:  \nstr; or bytes, encoded to the filesystem encoding.  The first string specifies the program executable, and the remaining strings specify the arguments. Together, string arguments form the argv of the program. This is similar to the standard library subprocess.Popen class called with shell=False and the list of strings passed as the first argument; however, where Popen takes a single argument which is list of strings, subprocess_exec takes multiple string arguments. The protocol_factory must be a callable returning a subclass of the asyncio.SubprocessProtocol class. Other parameters:  \nstdin can be any of these:  a file-like object representing a pipe to be connected to the subprocess\u2019s standard input stream using connect_write_pipe()\n the subprocess.PIPE constant (default) which will create a new pipe and connect it, the value None which will make the subprocess inherit the file descriptor from this process the subprocess.DEVNULL constant which indicates that the special os.devnull file will be used   \nstdout can be any of these:  a file-like object representing a pipe to be connected to the subprocess\u2019s standard output stream using connect_write_pipe()\n the subprocess.PIPE constant (default) which will create a new pipe and connect it, the value None which will make the subprocess inherit the file descriptor from this process the subprocess.DEVNULL constant which indicates that the special os.devnull file will be used   \nstderr can be any of these:  a file-like object representing a pipe to be connected to the subprocess\u2019s standard error stream using connect_write_pipe()\n the subprocess.PIPE constant (default) which will create a new pipe and connect it, the value None which will make the subprocess inherit the file descriptor from this process the subprocess.DEVNULL constant which indicates that the special os.devnull file will be used the subprocess.STDOUT constant which will connect the standard error stream to the process\u2019 standard output stream   \nAll other keyword arguments are passed to subprocess.Popen without interpretation, except for bufsize, universal_newlines, shell, text, encoding and errors, which should not be specified at all. The asyncio subprocess API does not support decoding the streams as text. bytes.decode() can be used to convert the bytes returned from the stream to text.   See the constructor of the subprocess.Popen class for documentation on other arguments. Returns a pair of (transport, protocol), where transport conforms to the asyncio.SubprocessTransport base class and protocol is an object instantiated by the protocol_factory. \n"}, {"name": "asyncio.loop.subprocess_shell()", "path": "library/asyncio-eventloop#asyncio.loop.subprocess_shell", "type": "Asynchronous I/O", "text": " \ncoroutine loop.subprocess_shell(protocol_factory, cmd, *, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, **kwargs)  \nCreate a subprocess from cmd, which can be a str or a bytes string encoded to the filesystem encoding, using the platform\u2019s \u201cshell\u201d syntax. This is similar to the standard library subprocess.Popen class called with shell=True. The protocol_factory must be a callable returning a subclass of the SubprocessProtocol class. See subprocess_exec() for more details about the remaining arguments. Returns a pair of (transport, protocol), where transport conforms to the SubprocessTransport base class and protocol is an object instantiated by the protocol_factory. \n"}, {"name": "asyncio.loop.time()", "path": "library/asyncio-eventloop#asyncio.loop.time", "type": "Asynchronous I/O", "text": " \nloop.time()  \nReturn the current time, as a float value, according to the event loop\u2019s internal monotonic clock. \n"}, {"name": "asyncio.MultiLoopChildWatcher", "path": "library/asyncio-policy#asyncio.MultiLoopChildWatcher", "type": "Asynchronous I/O", "text": " \nclass asyncio.MultiLoopChildWatcher  \nThis implementation registers a SIGCHLD signal handler on instantiation. That can break third-party code that installs a custom handler for SIGCHLD signal. The watcher avoids disrupting other code spawning processes by polling every process explicitly on a SIGCHLD signal. There is no limitation for running subprocesses from different threads once the watcher is installed. The solution is safe but it has a significant overhead when handling a big number of processes (O(n) each time a SIGCHLD is received).  New in version 3.8.  \n"}, {"name": "asyncio.new_event_loop()", "path": "library/asyncio-eventloop#asyncio.new_event_loop", "type": "Asynchronous I/O", "text": " \nasyncio.new_event_loop()  \nCreate a new event loop object. \n"}, {"name": "asyncio.open_connection()", "path": "library/asyncio-stream#asyncio.open_connection", "type": "Asynchronous I/O", "text": " \ncoroutine asyncio.open_connection(host=None, port=None, *, loop=None, limit=None, ssl=None, family=0, proto=0, flags=0, sock=None, local_addr=None, server_hostname=None, ssl_handshake_timeout=None)  \nEstablish a network connection and return a pair of (reader, writer) objects. The returned reader and writer objects are instances of StreamReader and StreamWriter classes. The loop argument is optional and can always be determined automatically when this function is awaited from a coroutine. limit determines the buffer size limit used by the returned StreamReader instance. By default the limit is set to 64 KiB. The rest of the arguments are passed directly to loop.create_connection().  New in version 3.7: The ssl_handshake_timeout parameter.  \n"}, {"name": "asyncio.open_unix_connection()", "path": "library/asyncio-stream#asyncio.open_unix_connection", "type": "Asynchronous I/O", "text": " \ncoroutine asyncio.open_unix_connection(path=None, *, loop=None, limit=None, ssl=None, sock=None, server_hostname=None, ssl_handshake_timeout=None)  \nEstablish a Unix socket connection and return a pair of (reader, writer). Similar to open_connection() but operates on Unix sockets. See also the documentation of loop.create_unix_connection(). Availability: Unix.  New in version 3.7: The ssl_handshake_timeout parameter.   Changed in version 3.7: The path parameter can now be a path-like object  \n"}, {"name": "asyncio.PidfdChildWatcher", "path": "library/asyncio-policy#asyncio.PidfdChildWatcher", "type": "Asynchronous I/O", "text": " \nclass asyncio.PidfdChildWatcher  \nThis implementation polls process file descriptors (pidfds) to await child process termination. In some respects, PidfdChildWatcher is a \u201cGoldilocks\u201d child watcher implementation. It doesn\u2019t require signals or threads, doesn\u2019t interfere with any processes launched outside the event loop, and scales linearly with the number of subprocesses launched by the event loop. The main disadvantage is that pidfds are specific to Linux, and only work on recent (5.3+) kernels.  New in version 3.9.  \n"}, {"name": "asyncio.PriorityQueue", "path": "library/asyncio-queue#asyncio.PriorityQueue", "type": "Asynchronous I/O", "text": " \nclass asyncio.PriorityQueue  \nA variant of Queue; retrieves entries in priority order (lowest first). Entries are typically tuples of the form (priority_number, data). \n"}, {"name": "asyncio.ProactorEventLoop", "path": "library/asyncio-eventloop#asyncio.ProactorEventLoop", "type": "Asynchronous I/O", "text": " \nclass asyncio.ProactorEventLoop  \nAn event loop for Windows that uses \u201cI/O Completion Ports\u201d (IOCP). Availability: Windows.  See also MSDN documentation on I/O Completion Ports.  \n"}, {"name": "asyncio.Protocol", "path": "library/asyncio-protocol#asyncio.Protocol", "type": "Asynchronous I/O", "text": " \nclass asyncio.Protocol(BaseProtocol)  \nThe base class for implementing streaming protocols (TCP, Unix sockets, etc). \n"}, {"name": "asyncio.Protocol.data_received()", "path": "library/asyncio-protocol#asyncio.Protocol.data_received", "type": "Asynchronous I/O", "text": " \nProtocol.data_received(data)  \nCalled when some data is received. data is a non-empty bytes object containing the incoming data. Whether the data is buffered, chunked or reassembled depends on the transport. In general, you shouldn\u2019t rely on specific semantics and instead make your parsing generic and flexible. However, data is always received in the correct order. The method can be called an arbitrary number of times while a connection is open. However, protocol.eof_received() is called at most once. Once eof_received() is called, data_received() is not called anymore. \n"}, {"name": "asyncio.Protocol.eof_received()", "path": "library/asyncio-protocol#asyncio.Protocol.eof_received", "type": "Asynchronous I/O", "text": " \nProtocol.eof_received()  \nCalled when the other end signals it won\u2019t send any more data (for example by calling transport.write_eof(), if the other end also uses asyncio). This method may return a false value (including None), in which case the transport will close itself. Conversely, if this method returns a true value, the protocol used determines whether to close the transport. Since the default implementation returns None, it implicitly closes the connection. Some transports, including SSL, don\u2019t support half-closed connections, in which case returning true from this method will result in the connection being closed. \n"}, {"name": "asyncio.Queue", "path": "library/asyncio-queue#asyncio.Queue", "type": "Asynchronous I/O", "text": " \nclass asyncio.Queue(maxsize=0, *, loop=None)  \nA first in, first out (FIFO) queue. If maxsize is less than or equal to zero, the queue size is infinite. If it is an integer greater than 0, then await put() blocks when the queue reaches maxsize until an item is removed by get(). Unlike the standard library threading queue, the size of the queue is always known and can be returned by calling the qsize() method.  Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.  This class is not thread safe.  \nmaxsize  \nNumber of items allowed in the queue. \n  \nempty()  \nReturn True if the queue is empty, False otherwise. \n  \nfull()  \nReturn True if there are maxsize items in the queue. If the queue was initialized with maxsize=0 (the default), then full() never returns True. \n  \ncoroutine get()  \nRemove and return an item from the queue. If queue is empty, wait until an item is available. \n  \nget_nowait()  \nReturn an item if one is immediately available, else raise QueueEmpty. \n  \ncoroutine join()  \nBlock until all items in the queue have been received and processed. The count of unfinished tasks goes up whenever an item is added to the queue. The count goes down whenever a consumer coroutine calls task_done() to indicate that the item was retrieved and all work on it is complete. When the count of unfinished tasks drops to zero, join() unblocks. \n  \ncoroutine put(item)  \nPut an item into the queue. If the queue is full, wait until a free slot is available before adding the item. \n  \nput_nowait(item)  \nPut an item into the queue without blocking. If no free slot is immediately available, raise QueueFull. \n  \nqsize()  \nReturn the number of items in the queue. \n  \ntask_done()  \nIndicate that a formerly enqueued task is complete. Used by queue consumers. For each get() used to fetch a task, a subsequent call to task_done() tells the queue that the processing on the task is complete. If a join() is currently blocking, it will resume when all items have been processed (meaning that a task_done() call was received for every item that had been put() into the queue). Raises ValueError if called more times than there were items placed in the queue. \n \n"}, {"name": "asyncio.Queue.empty()", "path": "library/asyncio-queue#asyncio.Queue.empty", "type": "Asynchronous I/O", "text": " \nempty()  \nReturn True if the queue is empty, False otherwise. \n"}, {"name": "asyncio.Queue.full()", "path": "library/asyncio-queue#asyncio.Queue.full", "type": "Asynchronous I/O", "text": " \nfull()  \nReturn True if there are maxsize items in the queue. If the queue was initialized with maxsize=0 (the default), then full() never returns True. \n"}, {"name": "asyncio.Queue.get()", "path": "library/asyncio-queue#asyncio.Queue.get", "type": "Asynchronous I/O", "text": " \ncoroutine get()  \nRemove and return an item from the queue. If queue is empty, wait until an item is available. \n"}, {"name": "asyncio.Queue.get_nowait()", "path": "library/asyncio-queue#asyncio.Queue.get_nowait", "type": "Asynchronous I/O", "text": " \nget_nowait()  \nReturn an item if one is immediately available, else raise QueueEmpty. \n"}, {"name": "asyncio.Queue.join()", "path": "library/asyncio-queue#asyncio.Queue.join", "type": "Asynchronous I/O", "text": " \ncoroutine join()  \nBlock until all items in the queue have been received and processed. The count of unfinished tasks goes up whenever an item is added to the queue. The count goes down whenever a consumer coroutine calls task_done() to indicate that the item was retrieved and all work on it is complete. When the count of unfinished tasks drops to zero, join() unblocks. \n"}, {"name": "asyncio.Queue.maxsize", "path": "library/asyncio-queue#asyncio.Queue.maxsize", "type": "Asynchronous I/O", "text": " \nmaxsize  \nNumber of items allowed in the queue. \n"}, {"name": "asyncio.Queue.put()", "path": "library/asyncio-queue#asyncio.Queue.put", "type": "Asynchronous I/O", "text": " \ncoroutine put(item)  \nPut an item into the queue. If the queue is full, wait until a free slot is available before adding the item. \n"}, {"name": "asyncio.Queue.put_nowait()", "path": "library/asyncio-queue#asyncio.Queue.put_nowait", "type": "Asynchronous I/O", "text": " \nput_nowait(item)  \nPut an item into the queue without blocking. If no free slot is immediately available, raise QueueFull. \n"}, {"name": "asyncio.Queue.qsize()", "path": "library/asyncio-queue#asyncio.Queue.qsize", "type": "Asynchronous I/O", "text": " \nqsize()  \nReturn the number of items in the queue. \n"}, {"name": "asyncio.Queue.task_done()", "path": "library/asyncio-queue#asyncio.Queue.task_done", "type": "Asynchronous I/O", "text": " \ntask_done()  \nIndicate that a formerly enqueued task is complete. Used by queue consumers. For each get() used to fetch a task, a subsequent call to task_done() tells the queue that the processing on the task is complete. If a join() is currently blocking, it will resume when all items have been processed (meaning that a task_done() call was received for every item that had been put() into the queue). Raises ValueError if called more times than there were items placed in the queue. \n"}, {"name": "asyncio.QueueEmpty", "path": "library/asyncio-queue#asyncio.QueueEmpty", "type": "Asynchronous I/O", "text": " \nexception asyncio.QueueEmpty  \nThis exception is raised when the get_nowait() method is called on an empty queue. \n"}, {"name": "asyncio.QueueFull", "path": "library/asyncio-queue#asyncio.QueueFull", "type": "Asynchronous I/O", "text": " \nexception asyncio.QueueFull  \nException raised when the put_nowait() method is called on a queue that has reached its maxsize. \n"}, {"name": "asyncio.ReadTransport", "path": "library/asyncio-protocol#asyncio.ReadTransport", "type": "Asynchronous I/O", "text": " \nclass asyncio.ReadTransport(BaseTransport)  \nA base transport for read-only connections. Instances of the ReadTransport class are returned from the loop.connect_read_pipe() event loop method and are also used by subprocess-related methods like loop.subprocess_exec(). \n"}, {"name": "asyncio.ReadTransport.is_reading()", "path": "library/asyncio-protocol#asyncio.ReadTransport.is_reading", "type": "Asynchronous I/O", "text": " \nReadTransport.is_reading()  \nReturn True if the transport is receiving new data.  New in version 3.7.  \n"}, {"name": "asyncio.ReadTransport.pause_reading()", "path": "library/asyncio-protocol#asyncio.ReadTransport.pause_reading", "type": "Asynchronous I/O", "text": " \nReadTransport.pause_reading()  \nPause the receiving end of the transport. No data will be passed to the protocol\u2019s protocol.data_received() method until resume_reading() is called.  Changed in version 3.7: The method is idempotent, i.e. it can be called when the transport is already paused or closed.  \n"}, {"name": "asyncio.ReadTransport.resume_reading()", "path": "library/asyncio-protocol#asyncio.ReadTransport.resume_reading", "type": "Asynchronous I/O", "text": " \nReadTransport.resume_reading()  \nResume the receiving end. The protocol\u2019s protocol.data_received() method will be called once again if some data is available for reading.  Changed in version 3.7: The method is idempotent, i.e. it can be called when the transport is already reading.  \n"}, {"name": "asyncio.run()", "path": "library/asyncio-task#asyncio.run", "type": "Asynchronous I/O", "text": " \nasyncio.run(coro, *, debug=False)  \nExecute the coroutine coro and return the result. This function runs the passed coroutine, taking care of managing the asyncio event loop, finalizing asynchronous generators, and closing the threadpool. This function cannot be called when another asyncio event loop is running in the same thread. If debug is True, the event loop will be run in debug mode. This function always creates a new event loop and closes it at the end. It should be used as a main entry point for asyncio programs, and should ideally only be called once. Example: async def main():\n    await asyncio.sleep(1)\n    print('hello')\n\nasyncio.run(main())\n  New in version 3.7.   Changed in version 3.9: Updated to use loop.shutdown_default_executor().   Note The source code for asyncio.run() can be found in Lib/asyncio/runners.py.  \n"}, {"name": "asyncio.run_coroutine_threadsafe()", "path": "library/asyncio-task#asyncio.run_coroutine_threadsafe", "type": "Asynchronous I/O", "text": " \nasyncio.run_coroutine_threadsafe(coro, loop)  \nSubmit a coroutine to the given event loop. Thread-safe. Return a concurrent.futures.Future to wait for the result from another OS thread. This function is meant to be called from a different OS thread than the one where the event loop is running. Example: # Create a coroutine\ncoro = asyncio.sleep(1, result=3)\n\n# Submit the coroutine to a given loop\nfuture = asyncio.run_coroutine_threadsafe(coro, loop)\n\n# Wait for the result with an optional timeout argument\nassert future.result(timeout) == 3\n If an exception is raised in the coroutine, the returned Future will be notified. It can also be used to cancel the task in the event loop: try:\n    result = future.result(timeout)\nexcept asyncio.TimeoutError:\n    print('The coroutine took too long, cancelling the task...')\n    future.cancel()\nexcept Exception as exc:\n    print(f'The coroutine raised an exception: {exc!r}')\nelse:\n    print(f'The coroutine returned: {result!r}')\n See the concurrency and multithreading section of the documentation. Unlike other asyncio functions this function requires the loop argument to be passed explicitly.  New in version 3.5.1.  \n"}, {"name": "asyncio.SafeChildWatcher", "path": "library/asyncio-policy#asyncio.SafeChildWatcher", "type": "Asynchronous I/O", "text": " \nclass asyncio.SafeChildWatcher  \nThis implementation uses active event loop from the main thread to handle SIGCHLD signal. If the main thread has no running event loop another thread cannot spawn a subprocess (RuntimeError is raised). The watcher avoids disrupting other code spawning processes by polling every process explicitly on a SIGCHLD signal. This solution is as safe as MultiLoopChildWatcher and has the same O(N) complexity but requires a running event loop in the main thread to work. \n"}, {"name": "asyncio.SelectorEventLoop", "path": "library/asyncio-eventloop#asyncio.SelectorEventLoop", "type": "Asynchronous I/O", "text": " \nclass asyncio.SelectorEventLoop  \nAn event loop based on the selectors module. Uses the most efficient selector available for the given platform. It is also possible to manually configure the exact selector implementation to be used: import asyncio\nimport selectors\n\nselector = selectors.SelectSelector()\nloop = asyncio.SelectorEventLoop(selector)\nasyncio.set_event_loop(loop)\n Availability: Unix, Windows. \n"}, {"name": "asyncio.Semaphore", "path": "library/asyncio-sync#asyncio.Semaphore", "type": "Asynchronous I/O", "text": " \nclass asyncio.Semaphore(value=1, *, loop=None)  \nA Semaphore object. Not thread-safe. A semaphore manages an internal counter which is decremented by each acquire() call and incremented by each release() call. The counter can never go below zero; when acquire() finds that it is zero, it blocks, waiting until some task calls release(). The optional value argument gives the initial value for the internal counter (1 by default). If the given value is less than 0 a ValueError is raised.  Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.  The preferred way to use a Semaphore is an async with statement: sem = asyncio.Semaphore(10)\n\n# ... later\nasync with sem:\n    # work with shared resource\n which is equivalent to: sem = asyncio.Semaphore(10)\n\n# ... later\nawait sem.acquire()\ntry:\n    # work with shared resource\nfinally:\n    sem.release()\n  \ncoroutine acquire()  \nAcquire a semaphore. If the internal counter is greater than zero, decrement it by one and return True immediately. If it is zero, wait until a release() is called and return True. \n  \nlocked()  \nReturns True if semaphore can not be acquired immediately. \n  \nrelease()  \nRelease a semaphore, incrementing the internal counter by one. Can wake up a task waiting to acquire the semaphore. Unlike BoundedSemaphore, Semaphore allows making more release() calls than acquire() calls. \n \n"}, {"name": "asyncio.Semaphore.acquire()", "path": "library/asyncio-sync#asyncio.Semaphore.acquire", "type": "Asynchronous I/O", "text": " \ncoroutine acquire()  \nAcquire a semaphore. If the internal counter is greater than zero, decrement it by one and return True immediately. If it is zero, wait until a release() is called and return True. \n"}, {"name": "asyncio.Semaphore.locked()", "path": "library/asyncio-sync#asyncio.Semaphore.locked", "type": "Asynchronous I/O", "text": " \nlocked()  \nReturns True if semaphore can not be acquired immediately. \n"}, {"name": "asyncio.Semaphore.release()", "path": "library/asyncio-sync#asyncio.Semaphore.release", "type": "Asynchronous I/O", "text": " \nrelease()  \nRelease a semaphore, incrementing the internal counter by one. Can wake up a task waiting to acquire the semaphore. Unlike BoundedSemaphore, Semaphore allows making more release() calls than acquire() calls. \n"}, {"name": "asyncio.SendfileNotAvailableError", "path": "library/asyncio-exceptions#asyncio.SendfileNotAvailableError", "type": "Asynchronous I/O", "text": " \nexception asyncio.SendfileNotAvailableError  \nThe \u201csendfile\u201d syscall is not available for the given socket or file type. A subclass of RuntimeError. \n"}, {"name": "asyncio.Server", "path": "library/asyncio-eventloop#asyncio.Server", "type": "Asynchronous I/O", "text": " \nclass asyncio.Server  \nServer objects are asynchronous context managers. When used in an async with statement, it\u2019s guaranteed that the Server object is closed and not accepting new connections when the async with statement is completed: srv = await loop.create_server(...)\n\nasync with srv:\n    # some code\n\n# At this point, srv is closed and no longer accepts new connections.\n  Changed in version 3.7: Server object is an asynchronous context manager since Python 3.7.   \nclose()  \nStop serving: close listening sockets and set the sockets attribute to None. The sockets that represent existing incoming client connections are left open. The server is closed asynchronously, use the wait_closed() coroutine to wait until the server is closed. \n  \nget_loop()  \nReturn the event loop associated with the server object.  New in version 3.7.  \n  \ncoroutine start_serving()  \nStart accepting connections. This method is idempotent, so it can be called when the server is already being serving. The start_serving keyword-only parameter to loop.create_server() and asyncio.start_server() allows creating a Server object that is not accepting connections initially. In this case Server.start_serving(), or Server.serve_forever() can be used to make the Server start accepting connections.  New in version 3.7.  \n  \ncoroutine serve_forever()  \nStart accepting connections until the coroutine is cancelled. Cancellation of serve_forever task causes the server to be closed. This method can be called if the server is already accepting connections. Only one serve_forever task can exist per one Server object. Example: async def client_connected(reader, writer):\n    # Communicate with the client with\n    # reader/writer streams.  For example:\n    await reader.readline()\n\nasync def main(host, port):\n    srv = await asyncio.start_server(\n        client_connected, host, port)\n    await srv.serve_forever()\n\nasyncio.run(main('127.0.0.1', 0))\n  New in version 3.7.  \n  \nis_serving()  \nReturn True if the server is accepting new connections.  New in version 3.7.  \n  \ncoroutine wait_closed()  \nWait until the close() method completes. \n  \nsockets  \nList of socket.socket objects the server is listening on.  Changed in version 3.7: Prior to Python 3.7 Server.sockets used to return an internal list of server sockets directly. In 3.7 a copy of that list is returned.  \n \n"}, {"name": "asyncio.Server.close()", "path": "library/asyncio-eventloop#asyncio.Server.close", "type": "Asynchronous I/O", "text": " \nclose()  \nStop serving: close listening sockets and set the sockets attribute to None. The sockets that represent existing incoming client connections are left open. The server is closed asynchronously, use the wait_closed() coroutine to wait until the server is closed. \n"}, {"name": "asyncio.Server.get_loop()", "path": "library/asyncio-eventloop#asyncio.Server.get_loop", "type": "Asynchronous I/O", "text": " \nget_loop()  \nReturn the event loop associated with the server object.  New in version 3.7.  \n"}, {"name": "asyncio.Server.is_serving()", "path": "library/asyncio-eventloop#asyncio.Server.is_serving", "type": "Asynchronous I/O", "text": " \nis_serving()  \nReturn True if the server is accepting new connections.  New in version 3.7.  \n"}, {"name": "asyncio.Server.serve_forever()", "path": "library/asyncio-eventloop#asyncio.Server.serve_forever", "type": "Asynchronous I/O", "text": " \ncoroutine serve_forever()  \nStart accepting connections until the coroutine is cancelled. Cancellation of serve_forever task causes the server to be closed. This method can be called if the server is already accepting connections. Only one serve_forever task can exist per one Server object. Example: async def client_connected(reader, writer):\n    # Communicate with the client with\n    # reader/writer streams.  For example:\n    await reader.readline()\n\nasync def main(host, port):\n    srv = await asyncio.start_server(\n        client_connected, host, port)\n    await srv.serve_forever()\n\nasyncio.run(main('127.0.0.1', 0))\n  New in version 3.7.  \n"}, {"name": "asyncio.Server.sockets", "path": "library/asyncio-eventloop#asyncio.Server.sockets", "type": "Asynchronous I/O", "text": " \nsockets  \nList of socket.socket objects the server is listening on.  Changed in version 3.7: Prior to Python 3.7 Server.sockets used to return an internal list of server sockets directly. In 3.7 a copy of that list is returned.  \n"}, {"name": "asyncio.Server.start_serving()", "path": "library/asyncio-eventloop#asyncio.Server.start_serving", "type": "Asynchronous I/O", "text": " \ncoroutine start_serving()  \nStart accepting connections. This method is idempotent, so it can be called when the server is already being serving. The start_serving keyword-only parameter to loop.create_server() and asyncio.start_server() allows creating a Server object that is not accepting connections initially. In this case Server.start_serving(), or Server.serve_forever() can be used to make the Server start accepting connections.  New in version 3.7.  \n"}, {"name": "asyncio.Server.wait_closed()", "path": "library/asyncio-eventloop#asyncio.Server.wait_closed", "type": "Asynchronous I/O", "text": " \ncoroutine wait_closed()  \nWait until the close() method completes. \n"}, {"name": "asyncio.set_child_watcher()", "path": "library/asyncio-policy#asyncio.set_child_watcher", "type": "Asynchronous I/O", "text": " \nasyncio.set_child_watcher(watcher)  \nSet the current child watcher to watcher for the current policy. watcher must implement methods defined in the AbstractChildWatcher base class. \n"}, {"name": "asyncio.set_event_loop()", "path": "library/asyncio-eventloop#asyncio.set_event_loop", "type": "Asynchronous I/O", "text": " \nasyncio.set_event_loop(loop)  \nSet loop as a current event loop for the current OS thread. \n"}, {"name": "asyncio.set_event_loop_policy()", "path": "library/asyncio-policy#asyncio.set_event_loop_policy", "type": "Asynchronous I/O", "text": " \nasyncio.set_event_loop_policy(policy)  \nSet the current process-wide policy to policy. If policy is set to None, the default policy is restored. \n"}, {"name": "asyncio.shield()", "path": "library/asyncio-task#asyncio.shield", "type": "Asynchronous I/O", "text": " \nawaitable asyncio.shield(aw, *, loop=None)  \nProtect an awaitable object from being cancelled. If aw is a coroutine it is automatically scheduled as a Task. The statement: res = await shield(something())\n is equivalent to: res = await something()\n except that if the coroutine containing it is cancelled, the Task running in something() is not cancelled. From the point of view of something(), the cancellation did not happen. Although its caller is still cancelled, so the \u201cawait\u201d expression still raises a CancelledError. If something() is cancelled by other means (i.e. from within itself) that would also cancel shield(). If it is desired to completely ignore cancellation (not recommended) the shield() function should be combined with a try/except clause, as follows: try:\n    res = await shield(something())\nexcept CancelledError:\n    res = None\n  Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.  \n"}, {"name": "asyncio.sleep()", "path": "library/asyncio-task#asyncio.sleep", "type": "Asynchronous I/O", "text": " \ncoroutine asyncio.sleep(delay, result=None, *, loop=None)  \nBlock for delay seconds. If result is provided, it is returned to the caller when the coroutine completes. sleep() always suspends the current task, allowing other tasks to run.  Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.  Example of coroutine displaying the current date every second for 5 seconds: import asyncio\nimport datetime\n\nasync def display_date():\n    loop = asyncio.get_running_loop()\n    end_time = loop.time() + 5.0\n    while True:\n        print(datetime.datetime.now())\n        if (loop.time() + 1.0) >= end_time:\n            break\n        await asyncio.sleep(1)\n\nasyncio.run(display_date())\n \n"}, {"name": "asyncio.start_server()", "path": "library/asyncio-stream#asyncio.start_server", "type": "Asynchronous I/O", "text": " \ncoroutine asyncio.start_server(client_connected_cb, host=None, port=None, *, loop=None, limit=None, family=socket.AF_UNSPEC, flags=socket.AI_PASSIVE, sock=None, backlog=100, ssl=None, reuse_address=None, reuse_port=None, ssl_handshake_timeout=None, start_serving=True)  \nStart a socket server. The client_connected_cb callback is called whenever a new client connection is established. It receives a (reader, writer) pair as two arguments, instances of the StreamReader and StreamWriter classes. client_connected_cb can be a plain callable or a coroutine function; if it is a coroutine function, it will be automatically scheduled as a Task. The loop argument is optional and can always be determined automatically when this method is awaited from a coroutine. limit determines the buffer size limit used by the returned StreamReader instance. By default the limit is set to 64 KiB. The rest of the arguments are passed directly to loop.create_server().  New in version 3.7: The ssl_handshake_timeout and start_serving parameters.  \n"}, {"name": "asyncio.start_unix_server()", "path": "library/asyncio-stream#asyncio.start_unix_server", "type": "Asynchronous I/O", "text": " \ncoroutine asyncio.start_unix_server(client_connected_cb, path=None, *, loop=None, limit=None, sock=None, backlog=100, ssl=None, ssl_handshake_timeout=None, start_serving=True)  \nStart a Unix socket server. Similar to start_server() but works with Unix sockets. See also the documentation of loop.create_unix_server(). Availability: Unix.  New in version 3.7: The ssl_handshake_timeout and start_serving parameters.   Changed in version 3.7: The path parameter can now be a path-like object.  \n"}, {"name": "asyncio.StreamReader", "path": "library/asyncio-stream#asyncio.StreamReader", "type": "Asynchronous I/O", "text": " \nclass asyncio.StreamReader  \nRepresents a reader object that provides APIs to read data from the IO stream. It is not recommended to instantiate StreamReader objects directly; use open_connection() and start_server() instead.  \ncoroutine read(n=-1)  \nRead up to n bytes. If n is not provided, or set to -1, read until EOF and return all read bytes. If EOF was received and the internal buffer is empty, return an empty bytes object. \n  \ncoroutine readline()  \nRead one line, where \u201cline\u201d is a sequence of bytes ending with \\n. If EOF is received and \\n was not found, the method returns partially read data. If EOF is received and the internal buffer is empty, return an empty bytes object. \n  \ncoroutine readexactly(n)  \nRead exactly n bytes. Raise an IncompleteReadError if EOF is reached before n can be read. Use the IncompleteReadError.partial attribute to get the partially read data. \n  \ncoroutine readuntil(separator=b'\\n')  \nRead data from the stream until separator is found. On success, the data and separator will be removed from the internal buffer (consumed). Returned data will include the separator at the end. If the amount of data read exceeds the configured stream limit, a LimitOverrunError exception is raised, and the data is left in the internal buffer and can be read again. If EOF is reached before the complete separator is found, an IncompleteReadError exception is raised, and the internal buffer is reset. The IncompleteReadError.partial attribute may contain a portion of the separator.  New in version 3.5.2.  \n  \nat_eof()  \nReturn True if the buffer is empty and feed_eof() was called. \n \n"}, {"name": "asyncio.StreamReader.at_eof()", "path": "library/asyncio-stream#asyncio.StreamReader.at_eof", "type": "Asynchronous I/O", "text": " \nat_eof()  \nReturn True if the buffer is empty and feed_eof() was called. \n"}, {"name": "asyncio.StreamReader.read()", "path": "library/asyncio-stream#asyncio.StreamReader.read", "type": "Asynchronous I/O", "text": " \ncoroutine read(n=-1)  \nRead up to n bytes. If n is not provided, or set to -1, read until EOF and return all read bytes. If EOF was received and the internal buffer is empty, return an empty bytes object. \n"}, {"name": "asyncio.StreamReader.readexactly()", "path": "library/asyncio-stream#asyncio.StreamReader.readexactly", "type": "Asynchronous I/O", "text": " \ncoroutine readexactly(n)  \nRead exactly n bytes. Raise an IncompleteReadError if EOF is reached before n can be read. Use the IncompleteReadError.partial attribute to get the partially read data. \n"}, {"name": "asyncio.StreamReader.readline()", "path": "library/asyncio-stream#asyncio.StreamReader.readline", "type": "Asynchronous I/O", "text": " \ncoroutine readline()  \nRead one line, where \u201cline\u201d is a sequence of bytes ending with \\n. If EOF is received and \\n was not found, the method returns partially read data. If EOF is received and the internal buffer is empty, return an empty bytes object. \n"}, {"name": "asyncio.StreamReader.readuntil()", "path": "library/asyncio-stream#asyncio.StreamReader.readuntil", "type": "Asynchronous I/O", "text": " \ncoroutine readuntil(separator=b'\\n')  \nRead data from the stream until separator is found. On success, the data and separator will be removed from the internal buffer (consumed). Returned data will include the separator at the end. If the amount of data read exceeds the configured stream limit, a LimitOverrunError exception is raised, and the data is left in the internal buffer and can be read again. If EOF is reached before the complete separator is found, an IncompleteReadError exception is raised, and the internal buffer is reset. The IncompleteReadError.partial attribute may contain a portion of the separator.  New in version 3.5.2.  \n"}, {"name": "asyncio.StreamWriter", "path": "library/asyncio-stream#asyncio.StreamWriter", "type": "Asynchronous I/O", "text": " \nclass asyncio.StreamWriter  \nRepresents a writer object that provides APIs to write data to the IO stream. It is not recommended to instantiate StreamWriter objects directly; use open_connection() and start_server() instead.  \nwrite(data)  \nThe method attempts to write the data to the underlying socket immediately. If that fails, the data is queued in an internal write buffer until it can be sent. The method should be used along with the drain() method: stream.write(data)\nawait stream.drain()\n \n  \nwritelines(data)  \nThe method writes a list (or any iterable) of bytes to the underlying socket immediately. If that fails, the data is queued in an internal write buffer until it can be sent. The method should be used along with the drain() method: stream.writelines(lines)\nawait stream.drain()\n \n  \nclose()  \nThe method closes the stream and the underlying socket. The method should be used along with the wait_closed() method: stream.close()\nawait stream.wait_closed()\n \n  \ncan_write_eof()  \nReturn True if the underlying transport supports the write_eof() method, False otherwise. \n  \nwrite_eof()  \nClose the write end of the stream after the buffered write data is flushed. \n  \ntransport  \nReturn the underlying asyncio transport. \n  \nget_extra_info(name, default=None)  \nAccess optional transport information; see BaseTransport.get_extra_info() for details. \n  \ncoroutine drain()  \nWait until it is appropriate to resume writing to the stream. Example: writer.write(data)\nawait writer.drain()\n This is a flow control method that interacts with the underlying IO write buffer. When the size of the buffer reaches the high watermark, drain() blocks until the size of the buffer is drained down to the low watermark and writing can be resumed. When there is nothing to wait for, the drain() returns immediately. \n  \nis_closing()  \nReturn True if the stream is closed or in the process of being closed.  New in version 3.7.  \n  \ncoroutine wait_closed()  \nWait until the stream is closed. Should be called after close() to wait until the underlying connection is closed.  New in version 3.7.  \n \n"}, {"name": "asyncio.StreamWriter.can_write_eof()", "path": "library/asyncio-stream#asyncio.StreamWriter.can_write_eof", "type": "Asynchronous I/O", "text": " \ncan_write_eof()  \nReturn True if the underlying transport supports the write_eof() method, False otherwise. \n"}, {"name": "asyncio.StreamWriter.close()", "path": "library/asyncio-stream#asyncio.StreamWriter.close", "type": "Asynchronous I/O", "text": " \nclose()  \nThe method closes the stream and the underlying socket. The method should be used along with the wait_closed() method: stream.close()\nawait stream.wait_closed()\n \n"}, {"name": "asyncio.StreamWriter.drain()", "path": "library/asyncio-stream#asyncio.StreamWriter.drain", "type": "Asynchronous I/O", "text": " \ncoroutine drain()  \nWait until it is appropriate to resume writing to the stream. Example: writer.write(data)\nawait writer.drain()\n This is a flow control method that interacts with the underlying IO write buffer. When the size of the buffer reaches the high watermark, drain() blocks until the size of the buffer is drained down to the low watermark and writing can be resumed. When there is nothing to wait for, the drain() returns immediately. \n"}, {"name": "asyncio.StreamWriter.get_extra_info()", "path": "library/asyncio-stream#asyncio.StreamWriter.get_extra_info", "type": "Asynchronous I/O", "text": " \nget_extra_info(name, default=None)  \nAccess optional transport information; see BaseTransport.get_extra_info() for details. \n"}, {"name": "asyncio.StreamWriter.is_closing()", "path": "library/asyncio-stream#asyncio.StreamWriter.is_closing", "type": "Asynchronous I/O", "text": " \nis_closing()  \nReturn True if the stream is closed or in the process of being closed.  New in version 3.7.  \n"}, {"name": "asyncio.StreamWriter.transport", "path": "library/asyncio-stream#asyncio.StreamWriter.transport", "type": "Asynchronous I/O", "text": " \ntransport  \nReturn the underlying asyncio transport. \n"}, {"name": "asyncio.StreamWriter.wait_closed()", "path": "library/asyncio-stream#asyncio.StreamWriter.wait_closed", "type": "Asynchronous I/O", "text": " \ncoroutine wait_closed()  \nWait until the stream is closed. Should be called after close() to wait until the underlying connection is closed.  New in version 3.7.  \n"}, {"name": "asyncio.StreamWriter.write()", "path": "library/asyncio-stream#asyncio.StreamWriter.write", "type": "Asynchronous I/O", "text": " \nwrite(data)  \nThe method attempts to write the data to the underlying socket immediately. If that fails, the data is queued in an internal write buffer until it can be sent. The method should be used along with the drain() method: stream.write(data)\nawait stream.drain()\n \n"}, {"name": "asyncio.StreamWriter.writelines()", "path": "library/asyncio-stream#asyncio.StreamWriter.writelines", "type": "Asynchronous I/O", "text": " \nwritelines(data)  \nThe method writes a list (or any iterable) of bytes to the underlying socket immediately. If that fails, the data is queued in an internal write buffer until it can be sent. The method should be used along with the drain() method: stream.writelines(lines)\nawait stream.drain()\n \n"}, {"name": "asyncio.StreamWriter.write_eof()", "path": "library/asyncio-stream#asyncio.StreamWriter.write_eof", "type": "Asynchronous I/O", "text": " \nwrite_eof()  \nClose the write end of the stream after the buffered write data is flushed. \n"}, {"name": "asyncio.SubprocessProtocol", "path": "library/asyncio-protocol#asyncio.SubprocessProtocol", "type": "Asynchronous I/O", "text": " \nclass asyncio.SubprocessProtocol(BaseProtocol)  \nThe base class for implementing protocols communicating with child processes (unidirectional pipes). \n"}, {"name": "asyncio.SubprocessProtocol.pipe_connection_lost()", "path": "library/asyncio-protocol#asyncio.SubprocessProtocol.pipe_connection_lost", "type": "Asynchronous I/O", "text": " \nSubprocessProtocol.pipe_connection_lost(fd, exc)  \nCalled when one of the pipes communicating with the child process is closed. fd is the integer file descriptor that was closed. \n"}, {"name": "asyncio.SubprocessProtocol.pipe_data_received()", "path": "library/asyncio-protocol#asyncio.SubprocessProtocol.pipe_data_received", "type": "Asynchronous I/O", "text": " \nSubprocessProtocol.pipe_data_received(fd, data)  \nCalled when the child process writes data into its stdout or stderr pipe. fd is the integer file descriptor of the pipe. data is a non-empty bytes object containing the received data. \n"}, {"name": "asyncio.SubprocessProtocol.process_exited()", "path": "library/asyncio-protocol#asyncio.SubprocessProtocol.process_exited", "type": "Asynchronous I/O", "text": " \nSubprocessProtocol.process_exited()  \nCalled when the child process has exited. \n"}, {"name": "asyncio.SubprocessTransport", "path": "library/asyncio-protocol#asyncio.SubprocessTransport", "type": "Asynchronous I/O", "text": " \nclass asyncio.SubprocessTransport(BaseTransport)  \nAn abstraction to represent a connection between a parent and its child OS process. Instances of the SubprocessTransport class are returned from event loop methods loop.subprocess_shell() and loop.subprocess_exec(). \n"}, {"name": "asyncio.SubprocessTransport.close()", "path": "library/asyncio-protocol#asyncio.SubprocessTransport.close", "type": "Asynchronous I/O", "text": " \nSubprocessTransport.close()  \nKill the subprocess by calling the kill() method. If the subprocess hasn\u2019t returned yet, and close transports of stdin, stdout, and stderr pipes. \n"}, {"name": "asyncio.SubprocessTransport.get_pid()", "path": "library/asyncio-protocol#asyncio.SubprocessTransport.get_pid", "type": "Asynchronous I/O", "text": " \nSubprocessTransport.get_pid()  \nReturn the subprocess process id as an integer. \n"}, {"name": "asyncio.SubprocessTransport.get_pipe_transport()", "path": "library/asyncio-protocol#asyncio.SubprocessTransport.get_pipe_transport", "type": "Asynchronous I/O", "text": " \nSubprocessTransport.get_pipe_transport(fd)  \nReturn the transport for the communication pipe corresponding to the integer file descriptor fd:  \n0: readable streaming transport of the standard input (stdin), or None if the subprocess was not created with stdin=PIPE\n \n1: writable streaming transport of the standard output (stdout), or None if the subprocess was not created with stdout=PIPE\n \n2: writable streaming transport of the standard error (stderr), or None if the subprocess was not created with stderr=PIPE\n other fd: None\n  \n"}, {"name": "asyncio.SubprocessTransport.get_returncode()", "path": "library/asyncio-protocol#asyncio.SubprocessTransport.get_returncode", "type": "Asynchronous I/O", "text": " \nSubprocessTransport.get_returncode()  \nReturn the subprocess return code as an integer or None if it hasn\u2019t returned, which is similar to the subprocess.Popen.returncode attribute. \n"}, {"name": "asyncio.SubprocessTransport.kill()", "path": "library/asyncio-protocol#asyncio.SubprocessTransport.kill", "type": "Asynchronous I/O", "text": " \nSubprocessTransport.kill()  \nKill the subprocess. On POSIX systems, the function sends SIGKILL to the subprocess. On Windows, this method is an alias for terminate(). See also subprocess.Popen.kill(). \n"}, {"name": "asyncio.SubprocessTransport.send_signal()", "path": "library/asyncio-protocol#asyncio.SubprocessTransport.send_signal", "type": "Asynchronous I/O", "text": " \nSubprocessTransport.send_signal(signal)  \nSend the signal number to the subprocess, as in subprocess.Popen.send_signal(). \n"}, {"name": "asyncio.SubprocessTransport.terminate()", "path": "library/asyncio-protocol#asyncio.SubprocessTransport.terminate", "type": "Asynchronous I/O", "text": " \nSubprocessTransport.terminate()  \nStop the subprocess. On POSIX systems, this method sends SIGTERM to the subprocess. On Windows, the Windows API function TerminateProcess() is called to stop the subprocess. See also subprocess.Popen.terminate(). \n"}, {"name": "asyncio.Task", "path": "library/asyncio-task#asyncio.Task", "type": "Asynchronous I/O", "text": " \nclass asyncio.Task(coro, *, loop=None, name=None)  \nA Future-like object that runs a Python coroutine. Not thread-safe. Tasks are used to run coroutines in event loops. If a coroutine awaits on a Future, the Task suspends the execution of the coroutine and waits for the completion of the Future. When the Future is done, the execution of the wrapped coroutine resumes. Event loops use cooperative scheduling: an event loop runs one Task at a time. While a Task awaits for the completion of a Future, the event loop runs other Tasks, callbacks, or performs IO operations. Use the high-level asyncio.create_task() function to create Tasks, or the low-level loop.create_task() or ensure_future() functions. Manual instantiation of Tasks is discouraged. To cancel a running Task use the cancel() method. Calling it will cause the Task to throw a CancelledError exception into the wrapped coroutine. If a coroutine is awaiting on a Future object during cancellation, the Future object will be cancelled. cancelled() can be used to check if the Task was cancelled. The method returns True if the wrapped coroutine did not suppress the CancelledError exception and was actually cancelled. asyncio.Task inherits from Future all of its APIs except Future.set_result() and Future.set_exception(). Tasks support the contextvars module. When a Task is created it copies the current context and later runs its coroutine in the copied context.  Changed in version 3.7: Added support for the contextvars module.   Changed in version 3.8: Added the name parameter.   Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.   \ncancel(msg=None)  \nRequest the Task to be cancelled. This arranges for a CancelledError exception to be thrown into the wrapped coroutine on the next cycle of the event loop. The coroutine then has a chance to clean up or even deny the request by suppressing the exception with a try \u2026 \u2026 except CancelledError \u2026 finally block. Therefore, unlike Future.cancel(), Task.cancel() does not guarantee that the Task will be cancelled, although suppressing cancellation completely is not common and is actively discouraged.  Changed in version 3.9: Added the msg parameter.  The following example illustrates how coroutines can intercept the cancellation request: async def cancel_me():\n    print('cancel_me(): before sleep')\n\n    try:\n        # Wait for 1 hour\n        await asyncio.sleep(3600)\n    except asyncio.CancelledError:\n        print('cancel_me(): cancel sleep')\n        raise\n    finally:\n        print('cancel_me(): after sleep')\n\nasync def main():\n    # Create a \"cancel_me\" Task\n    task = asyncio.create_task(cancel_me())\n\n    # Wait for 1 second\n    await asyncio.sleep(1)\n\n    task.cancel()\n    try:\n        await task\n    except asyncio.CancelledError:\n        print(\"main(): cancel_me is cancelled now\")\n\nasyncio.run(main())\n\n# Expected output:\n#\n#     cancel_me(): before sleep\n#     cancel_me(): cancel sleep\n#     cancel_me(): after sleep\n#     main(): cancel_me is cancelled now\n \n  \ncancelled()  \nReturn True if the Task is cancelled. The Task is cancelled when the cancellation was requested with cancel() and the wrapped coroutine propagated the CancelledError exception thrown into it. \n  \ndone()  \nReturn True if the Task is done. A Task is done when the wrapped coroutine either returned a value, raised an exception, or the Task was cancelled. \n  \nresult()  \nReturn the result of the Task. If the Task is done, the result of the wrapped coroutine is returned (or if the coroutine raised an exception, that exception is re-raised.) If the Task has been cancelled, this method raises a CancelledError exception. If the Task\u2019s result isn\u2019t yet available, this method raises a InvalidStateError exception. \n  \nexception()  \nReturn the exception of the Task. If the wrapped coroutine raised an exception that exception is returned. If the wrapped coroutine returned normally this method returns None. If the Task has been cancelled, this method raises a CancelledError exception. If the Task isn\u2019t done yet, this method raises an InvalidStateError exception. \n  \nadd_done_callback(callback, *, context=None)  \nAdd a callback to be run when the Task is done. This method should only be used in low-level callback-based code. See the documentation of Future.add_done_callback() for more details. \n  \nremove_done_callback(callback)  \nRemove callback from the callbacks list. This method should only be used in low-level callback-based code. See the documentation of Future.remove_done_callback() for more details. \n  \nget_stack(*, limit=None)  \nReturn the list of stack frames for this Task. If the wrapped coroutine is not done, this returns the stack where it is suspended. If the coroutine has completed successfully or was cancelled, this returns an empty list. If the coroutine was terminated by an exception, this returns the list of traceback frames. The frames are always ordered from oldest to newest. Only one stack frame is returned for a suspended coroutine. The optional limit argument sets the maximum number of frames to return; by default all available frames are returned. The ordering of the returned list differs depending on whether a stack or a traceback is returned: the newest frames of a stack are returned, but the oldest frames of a traceback are returned. (This matches the behavior of the traceback module.) \n  \nprint_stack(*, limit=None, file=None)  \nPrint the stack or traceback for this Task. This produces output similar to that of the traceback module for the frames retrieved by get_stack(). The limit argument is passed to get_stack() directly. The file argument is an I/O stream to which the output is written; by default output is written to sys.stderr. \n  \nget_coro()  \nReturn the coroutine object wrapped by the Task.  New in version 3.8.  \n  \nget_name()  \nReturn the name of the Task. If no name has been explicitly assigned to the Task, the default asyncio Task implementation generates a default name during instantiation.  New in version 3.8.  \n  \nset_name(value)  \nSet the name of the Task. The value argument can be any object, which is then converted to a string. In the default Task implementation, the name will be visible in the repr() output of a task object.  New in version 3.8.  \n \n"}, {"name": "asyncio.Task.add_done_callback()", "path": "library/asyncio-task#asyncio.Task.add_done_callback", "type": "Asynchronous I/O", "text": " \nadd_done_callback(callback, *, context=None)  \nAdd a callback to be run when the Task is done. This method should only be used in low-level callback-based code. See the documentation of Future.add_done_callback() for more details. \n"}, {"name": "asyncio.Task.cancel()", "path": "library/asyncio-task#asyncio.Task.cancel", "type": "Asynchronous I/O", "text": " \ncancel(msg=None)  \nRequest the Task to be cancelled. This arranges for a CancelledError exception to be thrown into the wrapped coroutine on the next cycle of the event loop. The coroutine then has a chance to clean up or even deny the request by suppressing the exception with a try \u2026 \u2026 except CancelledError \u2026 finally block. Therefore, unlike Future.cancel(), Task.cancel() does not guarantee that the Task will be cancelled, although suppressing cancellation completely is not common and is actively discouraged.  Changed in version 3.9: Added the msg parameter.  The following example illustrates how coroutines can intercept the cancellation request: async def cancel_me():\n    print('cancel_me(): before sleep')\n\n    try:\n        # Wait for 1 hour\n        await asyncio.sleep(3600)\n    except asyncio.CancelledError:\n        print('cancel_me(): cancel sleep')\n        raise\n    finally:\n        print('cancel_me(): after sleep')\n\nasync def main():\n    # Create a \"cancel_me\" Task\n    task = asyncio.create_task(cancel_me())\n\n    # Wait for 1 second\n    await asyncio.sleep(1)\n\n    task.cancel()\n    try:\n        await task\n    except asyncio.CancelledError:\n        print(\"main(): cancel_me is cancelled now\")\n\nasyncio.run(main())\n\n# Expected output:\n#\n#     cancel_me(): before sleep\n#     cancel_me(): cancel sleep\n#     cancel_me(): after sleep\n#     main(): cancel_me is cancelled now\n \n"}, {"name": "asyncio.Task.cancelled()", "path": "library/asyncio-task#asyncio.Task.cancelled", "type": "Asynchronous I/O", "text": " \ncancelled()  \nReturn True if the Task is cancelled. The Task is cancelled when the cancellation was requested with cancel() and the wrapped coroutine propagated the CancelledError exception thrown into it. \n"}, {"name": "asyncio.Task.done()", "path": "library/asyncio-task#asyncio.Task.done", "type": "Asynchronous I/O", "text": " \ndone()  \nReturn True if the Task is done. A Task is done when the wrapped coroutine either returned a value, raised an exception, or the Task was cancelled. \n"}, {"name": "asyncio.Task.exception()", "path": "library/asyncio-task#asyncio.Task.exception", "type": "Asynchronous I/O", "text": " \nexception()  \nReturn the exception of the Task. If the wrapped coroutine raised an exception that exception is returned. If the wrapped coroutine returned normally this method returns None. If the Task has been cancelled, this method raises a CancelledError exception. If the Task isn\u2019t done yet, this method raises an InvalidStateError exception. \n"}, {"name": "asyncio.Task.get_coro()", "path": "library/asyncio-task#asyncio.Task.get_coro", "type": "Asynchronous I/O", "text": " \nget_coro()  \nReturn the coroutine object wrapped by the Task.  New in version 3.8.  \n"}, {"name": "asyncio.Task.get_name()", "path": "library/asyncio-task#asyncio.Task.get_name", "type": "Asynchronous I/O", "text": " \nget_name()  \nReturn the name of the Task. If no name has been explicitly assigned to the Task, the default asyncio Task implementation generates a default name during instantiation.  New in version 3.8.  \n"}, {"name": "asyncio.Task.get_stack()", "path": "library/asyncio-task#asyncio.Task.get_stack", "type": "Asynchronous I/O", "text": " \nget_stack(*, limit=None)  \nReturn the list of stack frames for this Task. If the wrapped coroutine is not done, this returns the stack where it is suspended. If the coroutine has completed successfully or was cancelled, this returns an empty list. If the coroutine was terminated by an exception, this returns the list of traceback frames. The frames are always ordered from oldest to newest. Only one stack frame is returned for a suspended coroutine. The optional limit argument sets the maximum number of frames to return; by default all available frames are returned. The ordering of the returned list differs depending on whether a stack or a traceback is returned: the newest frames of a stack are returned, but the oldest frames of a traceback are returned. (This matches the behavior of the traceback module.) \n"}, {"name": "asyncio.Task.print_stack()", "path": "library/asyncio-task#asyncio.Task.print_stack", "type": "Asynchronous I/O", "text": " \nprint_stack(*, limit=None, file=None)  \nPrint the stack or traceback for this Task. This produces output similar to that of the traceback module for the frames retrieved by get_stack(). The limit argument is passed to get_stack() directly. The file argument is an I/O stream to which the output is written; by default output is written to sys.stderr. \n"}, {"name": "asyncio.Task.remove_done_callback()", "path": "library/asyncio-task#asyncio.Task.remove_done_callback", "type": "Asynchronous I/O", "text": " \nremove_done_callback(callback)  \nRemove callback from the callbacks list. This method should only be used in low-level callback-based code. See the documentation of Future.remove_done_callback() for more details. \n"}, {"name": "asyncio.Task.result()", "path": "library/asyncio-task#asyncio.Task.result", "type": "Asynchronous I/O", "text": " \nresult()  \nReturn the result of the Task. If the Task is done, the result of the wrapped coroutine is returned (or if the coroutine raised an exception, that exception is re-raised.) If the Task has been cancelled, this method raises a CancelledError exception. If the Task\u2019s result isn\u2019t yet available, this method raises a InvalidStateError exception. \n"}, {"name": "asyncio.Task.set_name()", "path": "library/asyncio-task#asyncio.Task.set_name", "type": "Asynchronous I/O", "text": " \nset_name(value)  \nSet the name of the Task. The value argument can be any object, which is then converted to a string. In the default Task implementation, the name will be visible in the repr() output of a task object.  New in version 3.8.  \n"}, {"name": "asyncio.ThreadedChildWatcher", "path": "library/asyncio-policy#asyncio.ThreadedChildWatcher", "type": "Asynchronous I/O", "text": " \nclass asyncio.ThreadedChildWatcher  \nThis implementation starts a new waiting thread for every subprocess spawn. It works reliably even when the asyncio event loop is run in a non-main OS thread. There is no noticeable overhead when handling a big number of children (O(1) each time a child terminates), but starting a thread per process requires extra memory. This watcher is used by default.  New in version 3.8.  \n"}, {"name": "asyncio.TimeoutError", "path": "library/asyncio-exceptions#asyncio.TimeoutError", "type": "Asynchronous I/O", "text": " \nexception asyncio.TimeoutError  \nThe operation has exceeded the given deadline.  Important This exception is different from the builtin TimeoutError exception.  \n"}, {"name": "asyncio.TimerHandle", "path": "library/asyncio-eventloop#asyncio.TimerHandle", "type": "Asynchronous I/O", "text": " \nclass asyncio.TimerHandle  \nA callback wrapper object returned by loop.call_later(), and loop.call_at(). This class is a subclass of Handle.  \nwhen()  \nReturn a scheduled callback time as float seconds. The time is an absolute timestamp, using the same time reference as loop.time().  New in version 3.7.  \n \n"}, {"name": "asyncio.TimerHandle.when()", "path": "library/asyncio-eventloop#asyncio.TimerHandle.when", "type": "Asynchronous I/O", "text": " \nwhen()  \nReturn a scheduled callback time as float seconds. The time is an absolute timestamp, using the same time reference as loop.time().  New in version 3.7.  \n"}, {"name": "asyncio.to_thread()", "path": "library/asyncio-task#asyncio.to_thread", "type": "Asynchronous I/O", "text": " \ncoroutine asyncio.to_thread(func, /, *args, **kwargs)  \nAsynchronously run function func in a separate thread. Any *args and **kwargs supplied for this function are directly passed to func. Also, the current contextvars.Context is propagated, allowing context variables from the event loop thread to be accessed in the separate thread. Return a coroutine that can be awaited to get the eventual result of func. This coroutine function is primarily intended to be used for executing IO-bound functions/methods that would otherwise block the event loop if they were ran in the main thread. For example: def blocking_io():\n    print(f\"start blocking_io at {time.strftime('%X')}\")\n    # Note that time.sleep() can be replaced with any blocking\n    # IO-bound operation, such as file operations.\n    time.sleep(1)\n    print(f\"blocking_io complete at {time.strftime('%X')}\")\n\nasync def main():\n    print(f\"started main at {time.strftime('%X')}\")\n\n    await asyncio.gather(\n        asyncio.to_thread(blocking_io),\n        asyncio.sleep(1))\n\n    print(f\"finished main at {time.strftime('%X')}\")\n\n\nasyncio.run(main())\n\n# Expected output:\n#\n# started main at 19:50:53\n# start blocking_io at 19:50:53\n# blocking_io complete at 19:50:54\n# finished main at 19:50:54\n Directly calling blocking_io() in any coroutine would block the event loop for its duration, resulting in an additional 1 second of run time. Instead, by using asyncio.to_thread(), we can run it in a separate thread without blocking the event loop.  Note Due to the GIL, asyncio.to_thread() can typically only be used to make IO-bound functions non-blocking. However, for extension modules that release the GIL or alternative Python implementations that don\u2019t have one, asyncio.to_thread() can also be used for CPU-bound functions.   New in version 3.9.  \n"}, {"name": "asyncio.Transport", "path": "library/asyncio-protocol#asyncio.Transport", "type": "Asynchronous I/O", "text": " \nclass asyncio.Transport(WriteTransport, ReadTransport)  \nInterface representing a bidirectional transport, such as a TCP connection. The user does not instantiate a transport directly; they call a utility function, passing it a protocol factory and other information necessary to create the transport and protocol. Instances of the Transport class are returned from or used by event loop methods like loop.create_connection(), loop.create_unix_connection(), loop.create_server(), loop.sendfile(), etc. \n"}, {"name": "asyncio.wait()", "path": "library/asyncio-task#asyncio.wait", "type": "Asynchronous I/O", "text": " \ncoroutine asyncio.wait(aws, *, loop=None, timeout=None, return_when=ALL_COMPLETED)  \nRun awaitable objects in the aws iterable concurrently and block until the condition specified by return_when. The aws iterable must not be empty. Returns two sets of Tasks/Futures: (done, pending). Usage: done, pending = await asyncio.wait(aws)\n timeout (a float or int), if specified, can be used to control the maximum number of seconds to wait before returning. Note that this function does not raise asyncio.TimeoutError. Futures or Tasks that aren\u2019t done when the timeout occurs are simply returned in the second set. return_when indicates when this function should return. It must be one of the following constants:   \nConstant Description   \nFIRST_COMPLETED The function will return when any future finishes or is cancelled.  \nFIRST_EXCEPTION The function will return when any future finishes by raising an exception. If no future raises an exception then it is equivalent to ALL_COMPLETED.  \nALL_COMPLETED The function will return when all futures finish or are cancelled.   Unlike wait_for(), wait() does not cancel the futures when a timeout occurs.  Deprecated since version 3.8: If any awaitable in aws is a coroutine, it is automatically scheduled as a Task. Passing coroutines objects to wait() directly is deprecated as it leads to confusing behavior.   Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.   Note wait() schedules coroutines as Tasks automatically and later returns those implicitly created Task objects in (done, pending) sets. Therefore the following code won\u2019t work as expected: async def foo():\n    return 42\n\ncoro = foo()\ndone, pending = await asyncio.wait({coro})\n\nif coro in done:\n    # This branch will never be run!\n Here is how the above snippet can be fixed: async def foo():\n    return 42\n\ntask = asyncio.create_task(foo())\ndone, pending = await asyncio.wait({task})\n\nif task in done:\n    # Everything will work as expected now.\n   Deprecated since version 3.8, will be removed in version 3.11: Passing coroutine objects to wait() directly is deprecated.  \n"}, {"name": "asyncio.wait_for()", "path": "library/asyncio-task#asyncio.wait_for", "type": "Asynchronous I/O", "text": " \ncoroutine asyncio.wait_for(aw, timeout, *, loop=None)  \nWait for the aw awaitable to complete with a timeout. If aw is a coroutine it is automatically scheduled as a Task. timeout can either be None or a float or int number of seconds to wait for. If timeout is None, block until the future completes. If a timeout occurs, it cancels the task and raises asyncio.TimeoutError. To avoid the task cancellation, wrap it in shield(). The function will wait until the future is actually cancelled, so the total wait time may exceed the timeout. If an exception happens during cancellation, it is propagated. If the wait is cancelled, the future aw is also cancelled.  Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.  Example: async def eternity():\n    # Sleep for one hour\n    await asyncio.sleep(3600)\n    print('yay!')\n\nasync def main():\n    # Wait for at most 1 second\n    try:\n        await asyncio.wait_for(eternity(), timeout=1.0)\n    except asyncio.TimeoutError:\n        print('timeout!')\n\nasyncio.run(main())\n\n# Expected output:\n#\n#     timeout!\n  Changed in version 3.7: When aw is cancelled due to a timeout, wait_for waits for aw to be cancelled. Previously, it raised asyncio.TimeoutError immediately.  \n"}, {"name": "asyncio.WindowsProactorEventLoopPolicy", "path": "library/asyncio-policy#asyncio.WindowsProactorEventLoopPolicy", "type": "Asynchronous I/O", "text": " \nclass asyncio.WindowsProactorEventLoopPolicy  \nAn alternative event loop policy that uses the ProactorEventLoop event loop implementation. Availability: Windows. \n"}, {"name": "asyncio.WindowsSelectorEventLoopPolicy", "path": "library/asyncio-policy#asyncio.WindowsSelectorEventLoopPolicy", "type": "Asynchronous I/O", "text": " \nclass asyncio.WindowsSelectorEventLoopPolicy  \nAn alternative event loop policy that uses the SelectorEventLoop event loop implementation. Availability: Windows. \n"}, {"name": "asyncio.wrap_future()", "path": "library/asyncio-future#asyncio.wrap_future", "type": "Asynchronous I/O", "text": " \nasyncio.wrap_future(future, *, loop=None)  \nWrap a concurrent.futures.Future object in a asyncio.Future object. \n"}, {"name": "asyncio.WriteTransport", "path": "library/asyncio-protocol#asyncio.WriteTransport", "type": "Asynchronous I/O", "text": " \nclass asyncio.WriteTransport(BaseTransport)  \nA base transport for write-only connections. Instances of the WriteTransport class are returned from the loop.connect_write_pipe() event loop method and are also used by subprocess-related methods like loop.subprocess_exec(). \n"}, {"name": "asyncio.WriteTransport.abort()", "path": "library/asyncio-protocol#asyncio.WriteTransport.abort", "type": "Asynchronous I/O", "text": " \nWriteTransport.abort()  \nClose the transport immediately, without waiting for pending operations to complete. Buffered data will be lost. No more data will be received. The protocol\u2019s protocol.connection_lost() method will eventually be called with None as its argument. \n"}, {"name": "asyncio.WriteTransport.can_write_eof()", "path": "library/asyncio-protocol#asyncio.WriteTransport.can_write_eof", "type": "Asynchronous I/O", "text": " \nWriteTransport.can_write_eof()  \nReturn True if the transport supports write_eof(), False if not. \n"}, {"name": "asyncio.WriteTransport.get_write_buffer_limits()", "path": "library/asyncio-protocol#asyncio.WriteTransport.get_write_buffer_limits", "type": "Asynchronous I/O", "text": " \nWriteTransport.get_write_buffer_limits()  \nGet the high and low watermarks for write flow control. Return a tuple (low, high) where low and high are positive number of bytes. Use set_write_buffer_limits() to set the limits.  New in version 3.4.2.  \n"}, {"name": "asyncio.WriteTransport.get_write_buffer_size()", "path": "library/asyncio-protocol#asyncio.WriteTransport.get_write_buffer_size", "type": "Asynchronous I/O", "text": " \nWriteTransport.get_write_buffer_size()  \nReturn the current size of the output buffer used by the transport. \n"}, {"name": "asyncio.WriteTransport.set_write_buffer_limits()", "path": "library/asyncio-protocol#asyncio.WriteTransport.set_write_buffer_limits", "type": "Asynchronous I/O", "text": " \nWriteTransport.set_write_buffer_limits(high=None, low=None)  \nSet the high and low watermarks for write flow control. These two values (measured in number of bytes) control when the protocol\u2019s protocol.pause_writing() and protocol.resume_writing() methods are called. If specified, the low watermark must be less than or equal to the high watermark. Neither high nor low can be negative. pause_writing() is called when the buffer size becomes greater than or equal to the high value. If writing has been paused, resume_writing() is called when the buffer size becomes less than or equal to the low value. The defaults are implementation-specific. If only the high watermark is given, the low watermark defaults to an implementation-specific value less than or equal to the high watermark. Setting high to zero forces low to zero as well, and causes pause_writing() to be called whenever the buffer becomes non-empty. Setting low to zero causes resume_writing() to be called only once the buffer is empty. Use of zero for either limit is generally sub-optimal as it reduces opportunities for doing I/O and computation concurrently. Use get_write_buffer_limits() to get the limits. \n"}, {"name": "asyncio.WriteTransport.write()", "path": "library/asyncio-protocol#asyncio.WriteTransport.write", "type": "Asynchronous I/O", "text": " \nWriteTransport.write(data)  \nWrite some data bytes to the transport. This method does not block; it buffers the data and arranges for it to be sent out asynchronously. \n"}, {"name": "asyncio.WriteTransport.writelines()", "path": "library/asyncio-protocol#asyncio.WriteTransport.writelines", "type": "Asynchronous I/O", "text": " \nWriteTransport.writelines(list_of_data)  \nWrite a list (or any iterable) of data bytes to the transport. This is functionally equivalent to calling write() on each element yielded by the iterable, but may be implemented more efficiently. \n"}, {"name": "asyncio.WriteTransport.write_eof()", "path": "library/asyncio-protocol#asyncio.WriteTransport.write_eof", "type": "Asynchronous I/O", "text": " \nWriteTransport.write_eof()  \nClose the write end of the transport after flushing all buffered data. Data may still be received. This method can raise NotImplementedError if the transport (e.g. SSL) doesn\u2019t support half-closed connections. \n"}, {"name": "asyncore", "path": "library/asyncore", "type": "Networking & Interprocess Communication", "text": "asyncore \u2014 Asynchronous socket handler Source code: Lib/asyncore.py  Deprecated since version 3.6: Please use asyncio instead.   Note This module exists for backwards compatibility only. For new code we recommend using asyncio.  This module provides the basic infrastructure for writing asynchronous socket service clients and servers. There are only two ways to have a program on a single processor do \u201cmore than one thing at a time.\u201d Multi-threaded programming is the simplest and most popular way to do it, but there is another very different technique, that lets you have nearly all the advantages of multi-threading, without actually using multiple threads. It\u2019s really only practical if your program is largely I/O bound. If your program is processor bound, then pre-emptive scheduled threads are probably what you really need. Network servers are rarely processor bound, however. If your operating system supports the select() system call in its I/O library (and nearly all do), then you can use it to juggle multiple communication channels at once; doing other work while your I/O is taking place in the \u201cbackground.\u201d Although this strategy can seem strange and complex, especially at first, it is in many ways easier to understand and control than multi-threaded programming. The asyncore module solves many of the difficult problems for you, making the task of building sophisticated high-performance network servers and clients a snap. For \u201cconversational\u201d applications and protocols the companion asynchat module is invaluable. The basic idea behind both modules is to create one or more network channels, instances of class asyncore.dispatcher and asynchat.async_chat. Creating the channels adds them to a global map, used by the loop() function if you do not provide it with your own map. Once the initial channel(s) is(are) created, calling the loop() function activates channel service, which continues until the last channel (including any that have been added to the map during asynchronous service) is closed.  \nasyncore.loop([timeout[, use_poll[, map[, count]]]])  \nEnter a polling loop that terminates after count passes or all open channels have been closed. All arguments are optional. The count parameter defaults to None, resulting in the loop terminating only when all channels have been closed. The timeout argument sets the timeout parameter for the appropriate select() or poll() call, measured in seconds; the default is 30 seconds. The use_poll parameter, if true, indicates that poll() should be used in preference to select() (the default is False). The map parameter is a dictionary whose items are the channels to watch. As channels are closed they are deleted from their map. If map is omitted, a global map is used. Channels (instances of asyncore.dispatcher, asynchat.async_chat and subclasses thereof) can freely be mixed in the map. \n  \nclass asyncore.dispatcher  \nThe dispatcher class is a thin wrapper around a low-level socket object. To make it more useful, it has a few methods for event-handling which are called from the asynchronous loop. Otherwise, it can be treated as a normal non-blocking socket object. The firing of low-level events at certain times or in certain connection states tells the asynchronous loop that certain higher-level events have taken place. For example, if we have asked for a socket to connect to another host, we know that the connection has been made when the socket becomes writable for the first time (at this point you know that you may write to it with the expectation of success). The implied higher-level events are:   \nEvent Description   \nhandle_connect() Implied by the first read or write event  \nhandle_close() Implied by a read event with no data available  \nhandle_accepted() Implied by a read event on a listening socket   During asynchronous processing, each mapped channel\u2019s readable() and writable() methods are used to determine whether the channel\u2019s socket should be added to the list of channels select()ed or poll()ed for read and write events. Thus, the set of channel events is larger than the basic socket events. The full set of methods that can be overridden in your subclass follows:  \nhandle_read()  \nCalled when the asynchronous loop detects that a read() call on the channel\u2019s socket will succeed. \n  \nhandle_write()  \nCalled when the asynchronous loop detects that a writable socket can be written. Often this method will implement the necessary buffering for performance. For example: def handle_write(self):\n    sent = self.send(self.buffer)\n    self.buffer = self.buffer[sent:]\n \n  \nhandle_expt()  \nCalled when there is out of band (OOB) data for a socket connection. This will almost never happen, as OOB is tenuously supported and rarely used. \n  \nhandle_connect()  \nCalled when the active opener\u2019s socket actually makes a connection. Might send a \u201cwelcome\u201d banner, or initiate a protocol negotiation with the remote endpoint, for example. \n  \nhandle_close()  \nCalled when the socket is closed. \n  \nhandle_error()  \nCalled when an exception is raised and not otherwise handled. The default version prints a condensed traceback. \n  \nhandle_accept()  \nCalled on listening channels (passive openers) when a connection can be established with a new remote endpoint that has issued a connect() call for the local endpoint. Deprecated in version 3.2; use handle_accepted() instead.  Deprecated since version 3.2.  \n  \nhandle_accepted(sock, addr)  \nCalled on listening channels (passive openers) when a connection has been established with a new remote endpoint that has issued a connect() call for the local endpoint. sock is a new socket object usable to send and receive data on the connection, and addr is the address bound to the socket on the other end of the connection.  New in version 3.2.  \n  \nreadable()  \nCalled each time around the asynchronous loop to determine whether a channel\u2019s socket should be added to the list on which read events can occur. The default method simply returns True, indicating that by default, all channels will be interested in read events. \n  \nwritable()  \nCalled each time around the asynchronous loop to determine whether a channel\u2019s socket should be added to the list on which write events can occur. The default method simply returns True, indicating that by default, all channels will be interested in write events. \n In addition, each channel delegates or extends many of the socket methods. Most of these are nearly identical to their socket partners.  \ncreate_socket(family=socket.AF_INET, type=socket.SOCK_STREAM)  \nThis is identical to the creation of a normal socket, and will use the same options for creation. Refer to the socket documentation for information on creating sockets.  Changed in version 3.3: family and type arguments can be omitted.  \n  \nconnect(address)  \nAs with the normal socket object, address is a tuple with the first element the host to connect to, and the second the port number. \n  \nsend(data)  \nSend data to the remote end-point of the socket. \n  \nrecv(buffer_size)  \nRead at most buffer_size bytes from the socket\u2019s remote end-point. An empty bytes object implies that the channel has been closed from the other end. Note that recv() may raise BlockingIOError , even though select.select() or select.poll() has reported the socket ready for reading. \n  \nlisten(backlog)  \nListen for connections made to the socket. The backlog argument specifies the maximum number of queued connections and should be at least 1; the maximum value is system-dependent (usually 5). \n  \nbind(address)  \nBind the socket to address. The socket must not already be bound. (The format of address depends on the address family \u2014 refer to the socket documentation for more information.) To mark the socket as re-usable (setting the SO_REUSEADDR option), call the dispatcher object\u2019s set_reuse_addr() method. \n  \naccept()  \nAccept a connection. The socket must be bound to an address and listening for connections. The return value can be either None or a pair (conn, address) where conn is a new socket object usable to send and receive data on the connection, and address is the address bound to the socket on the other end of the connection. When None is returned it means the connection didn\u2019t take place, in which case the server should just ignore this event and keep listening for further incoming connections. \n  \nclose()  \nClose the socket. All future operations on the socket object will fail. The remote end-point will receive no more data (after queued data is flushed). Sockets are automatically closed when they are garbage-collected. \n \n  \nclass asyncore.dispatcher_with_send  \nA dispatcher subclass which adds simple buffered output capability, useful for simple clients. For more sophisticated usage use asynchat.async_chat. \n  \nclass asyncore.file_dispatcher  \nA file_dispatcher takes a file descriptor or file object along with an optional map argument and wraps it for use with the poll() or loop() functions. If provided a file object or anything with a fileno() method, that method will be called and passed to the file_wrapper constructor. Availability: Unix. \n  \nclass asyncore.file_wrapper  \nA file_wrapper takes an integer file descriptor and calls os.dup() to duplicate the handle so that the original handle may be closed independently of the file_wrapper. This class implements sufficient methods to emulate a socket for use by the file_dispatcher class. Availability: Unix. \n asyncore Example basic HTTP client Here is a very basic HTTP client that uses the dispatcher class to implement its socket handling: import asyncore\n\nclass HTTPClient(asyncore.dispatcher):\n\n    def __init__(self, host, path):\n        asyncore.dispatcher.__init__(self)\n        self.create_socket()\n        self.connect( (host, 80) )\n        self.buffer = bytes('GET %s HTTP/1.0\\r\\nHost: %s\\r\\n\\r\\n' %\n                            (path, host), 'ascii')\n\n    def handle_connect(self):\n        pass\n\n    def handle_close(self):\n        self.close()\n\n    def handle_read(self):\n        print(self.recv(8192))\n\n    def writable(self):\n        return (len(self.buffer) > 0)\n\n    def handle_write(self):\n        sent = self.send(self.buffer)\n        self.buffer = self.buffer[sent:]\n\n\nclient = HTTPClient('www.python.org', '/')\nasyncore.loop()\n asyncore Example basic echo server Here is a basic echo server that uses the dispatcher class to accept connections and dispatches the incoming connections to a handler: import asyncore\n\nclass EchoHandler(asyncore.dispatcher_with_send):\n\n    def handle_read(self):\n        data = self.recv(8192)\n        if data:\n            self.send(data)\n\nclass EchoServer(asyncore.dispatcher):\n\n    def __init__(self, host, port):\n        asyncore.dispatcher.__init__(self)\n        self.create_socket()\n        self.set_reuse_addr()\n        self.bind((host, port))\n        self.listen(5)\n\n    def handle_accepted(self, sock, addr):\n        print('Incoming connection from %s' % repr(addr))\n        handler = EchoHandler(sock)\n\nserver = EchoServer('localhost', 8080)\nasyncore.loop()\n\n"}, {"name": "asyncore.dispatcher", "path": "library/asyncore#asyncore.dispatcher", "type": "Networking & Interprocess Communication", "text": " \nclass asyncore.dispatcher  \nThe dispatcher class is a thin wrapper around a low-level socket object. To make it more useful, it has a few methods for event-handling which are called from the asynchronous loop. Otherwise, it can be treated as a normal non-blocking socket object. The firing of low-level events at certain times or in certain connection states tells the asynchronous loop that certain higher-level events have taken place. For example, if we have asked for a socket to connect to another host, we know that the connection has been made when the socket becomes writable for the first time (at this point you know that you may write to it with the expectation of success). The implied higher-level events are:   \nEvent Description   \nhandle_connect() Implied by the first read or write event  \nhandle_close() Implied by a read event with no data available  \nhandle_accepted() Implied by a read event on a listening socket   During asynchronous processing, each mapped channel\u2019s readable() and writable() methods are used to determine whether the channel\u2019s socket should be added to the list of channels select()ed or poll()ed for read and write events. Thus, the set of channel events is larger than the basic socket events. The full set of methods that can be overridden in your subclass follows:  \nhandle_read()  \nCalled when the asynchronous loop detects that a read() call on the channel\u2019s socket will succeed. \n  \nhandle_write()  \nCalled when the asynchronous loop detects that a writable socket can be written. Often this method will implement the necessary buffering for performance. For example: def handle_write(self):\n    sent = self.send(self.buffer)\n    self.buffer = self.buffer[sent:]\n \n  \nhandle_expt()  \nCalled when there is out of band (OOB) data for a socket connection. This will almost never happen, as OOB is tenuously supported and rarely used. \n  \nhandle_connect()  \nCalled when the active opener\u2019s socket actually makes a connection. Might send a \u201cwelcome\u201d banner, or initiate a protocol negotiation with the remote endpoint, for example. \n  \nhandle_close()  \nCalled when the socket is closed. \n  \nhandle_error()  \nCalled when an exception is raised and not otherwise handled. The default version prints a condensed traceback. \n  \nhandle_accept()  \nCalled on listening channels (passive openers) when a connection can be established with a new remote endpoint that has issued a connect() call for the local endpoint. Deprecated in version 3.2; use handle_accepted() instead.  Deprecated since version 3.2.  \n  \nhandle_accepted(sock, addr)  \nCalled on listening channels (passive openers) when a connection has been established with a new remote endpoint that has issued a connect() call for the local endpoint. sock is a new socket object usable to send and receive data on the connection, and addr is the address bound to the socket on the other end of the connection.  New in version 3.2.  \n  \nreadable()  \nCalled each time around the asynchronous loop to determine whether a channel\u2019s socket should be added to the list on which read events can occur. The default method simply returns True, indicating that by default, all channels will be interested in read events. \n  \nwritable()  \nCalled each time around the asynchronous loop to determine whether a channel\u2019s socket should be added to the list on which write events can occur. The default method simply returns True, indicating that by default, all channels will be interested in write events. \n In addition, each channel delegates or extends many of the socket methods. Most of these are nearly identical to their socket partners.  \ncreate_socket(family=socket.AF_INET, type=socket.SOCK_STREAM)  \nThis is identical to the creation of a normal socket, and will use the same options for creation. Refer to the socket documentation for information on creating sockets.  Changed in version 3.3: family and type arguments can be omitted.  \n  \nconnect(address)  \nAs with the normal socket object, address is a tuple with the first element the host to connect to, and the second the port number. \n  \nsend(data)  \nSend data to the remote end-point of the socket. \n  \nrecv(buffer_size)  \nRead at most buffer_size bytes from the socket\u2019s remote end-point. An empty bytes object implies that the channel has been closed from the other end. Note that recv() may raise BlockingIOError , even though select.select() or select.poll() has reported the socket ready for reading. \n  \nlisten(backlog)  \nListen for connections made to the socket. The backlog argument specifies the maximum number of queued connections and should be at least 1; the maximum value is system-dependent (usually 5). \n  \nbind(address)  \nBind the socket to address. The socket must not already be bound. (The format of address depends on the address family \u2014 refer to the socket documentation for more information.) To mark the socket as re-usable (setting the SO_REUSEADDR option), call the dispatcher object\u2019s set_reuse_addr() method. \n  \naccept()  \nAccept a connection. The socket must be bound to an address and listening for connections. The return value can be either None or a pair (conn, address) where conn is a new socket object usable to send and receive data on the connection, and address is the address bound to the socket on the other end of the connection. When None is returned it means the connection didn\u2019t take place, in which case the server should just ignore this event and keep listening for further incoming connections. \n  \nclose()  \nClose the socket. All future operations on the socket object will fail. The remote end-point will receive no more data (after queued data is flushed). Sockets are automatically closed when they are garbage-collected. \n \n"}, {"name": "asyncore.dispatcher.accept()", "path": "library/asyncore#asyncore.dispatcher.accept", "type": "Networking & Interprocess Communication", "text": " \naccept()  \nAccept a connection. The socket must be bound to an address and listening for connections. The return value can be either None or a pair (conn, address) where conn is a new socket object usable to send and receive data on the connection, and address is the address bound to the socket on the other end of the connection. When None is returned it means the connection didn\u2019t take place, in which case the server should just ignore this event and keep listening for further incoming connections. \n"}, {"name": "asyncore.dispatcher.bind()", "path": "library/asyncore#asyncore.dispatcher.bind", "type": "Networking & Interprocess Communication", "text": " \nbind(address)  \nBind the socket to address. The socket must not already be bound. (The format of address depends on the address family \u2014 refer to the socket documentation for more information.) To mark the socket as re-usable (setting the SO_REUSEADDR option), call the dispatcher object\u2019s set_reuse_addr() method. \n"}, {"name": "asyncore.dispatcher.close()", "path": "library/asyncore#asyncore.dispatcher.close", "type": "Networking & Interprocess Communication", "text": " \nclose()  \nClose the socket. All future operations on the socket object will fail. The remote end-point will receive no more data (after queued data is flushed). Sockets are automatically closed when they are garbage-collected. \n"}, {"name": "asyncore.dispatcher.connect()", "path": "library/asyncore#asyncore.dispatcher.connect", "type": "Networking & Interprocess Communication", "text": " \nconnect(address)  \nAs with the normal socket object, address is a tuple with the first element the host to connect to, and the second the port number. \n"}, {"name": "asyncore.dispatcher.create_socket()", "path": "library/asyncore#asyncore.dispatcher.create_socket", "type": "Networking & Interprocess Communication", "text": " \ncreate_socket(family=socket.AF_INET, type=socket.SOCK_STREAM)  \nThis is identical to the creation of a normal socket, and will use the same options for creation. Refer to the socket documentation for information on creating sockets.  Changed in version 3.3: family and type arguments can be omitted.  \n"}, {"name": "asyncore.dispatcher.handle_accept()", "path": "library/asyncore#asyncore.dispatcher.handle_accept", "type": "Networking & Interprocess Communication", "text": " \nhandle_accept()  \nCalled on listening channels (passive openers) when a connection can be established with a new remote endpoint that has issued a connect() call for the local endpoint. Deprecated in version 3.2; use handle_accepted() instead.  Deprecated since version 3.2.  \n"}, {"name": "asyncore.dispatcher.handle_accepted()", "path": "library/asyncore#asyncore.dispatcher.handle_accepted", "type": "Networking & Interprocess Communication", "text": " \nhandle_accepted(sock, addr)  \nCalled on listening channels (passive openers) when a connection has been established with a new remote endpoint that has issued a connect() call for the local endpoint. sock is a new socket object usable to send and receive data on the connection, and addr is the address bound to the socket on the other end of the connection.  New in version 3.2.  \n"}, {"name": "asyncore.dispatcher.handle_close()", "path": "library/asyncore#asyncore.dispatcher.handle_close", "type": "Networking & Interprocess Communication", "text": " \nhandle_close()  \nCalled when the socket is closed. \n"}, {"name": "asyncore.dispatcher.handle_connect()", "path": "library/asyncore#asyncore.dispatcher.handle_connect", "type": "Networking & Interprocess Communication", "text": " \nhandle_connect()  \nCalled when the active opener\u2019s socket actually makes a connection. Might send a \u201cwelcome\u201d banner, or initiate a protocol negotiation with the remote endpoint, for example. \n"}, {"name": "asyncore.dispatcher.handle_error()", "path": "library/asyncore#asyncore.dispatcher.handle_error", "type": "Networking & Interprocess Communication", "text": " \nhandle_error()  \nCalled when an exception is raised and not otherwise handled. The default version prints a condensed traceback. \n"}, {"name": "asyncore.dispatcher.handle_expt()", "path": "library/asyncore#asyncore.dispatcher.handle_expt", "type": "Networking & Interprocess Communication", "text": " \nhandle_expt()  \nCalled when there is out of band (OOB) data for a socket connection. This will almost never happen, as OOB is tenuously supported and rarely used. \n"}, {"name": "asyncore.dispatcher.handle_read()", "path": "library/asyncore#asyncore.dispatcher.handle_read", "type": "Networking & Interprocess Communication", "text": " \nhandle_read()  \nCalled when the asynchronous loop detects that a read() call on the channel\u2019s socket will succeed. \n"}, {"name": "asyncore.dispatcher.handle_write()", "path": "library/asyncore#asyncore.dispatcher.handle_write", "type": "Networking & Interprocess Communication", "text": " \nhandle_write()  \nCalled when the asynchronous loop detects that a writable socket can be written. Often this method will implement the necessary buffering for performance. For example: def handle_write(self):\n    sent = self.send(self.buffer)\n    self.buffer = self.buffer[sent:]\n \n"}, {"name": "asyncore.dispatcher.listen()", "path": "library/asyncore#asyncore.dispatcher.listen", "type": "Networking & Interprocess Communication", "text": " \nlisten(backlog)  \nListen for connections made to the socket. The backlog argument specifies the maximum number of queued connections and should be at least 1; the maximum value is system-dependent (usually 5). \n"}, {"name": "asyncore.dispatcher.readable()", "path": "library/asyncore#asyncore.dispatcher.readable", "type": "Networking & Interprocess Communication", "text": " \nreadable()  \nCalled each time around the asynchronous loop to determine whether a channel\u2019s socket should be added to the list on which read events can occur. The default method simply returns True, indicating that by default, all channels will be interested in read events. \n"}, {"name": "asyncore.dispatcher.recv()", "path": "library/asyncore#asyncore.dispatcher.recv", "type": "Networking & Interprocess Communication", "text": " \nrecv(buffer_size)  \nRead at most buffer_size bytes from the socket\u2019s remote end-point. An empty bytes object implies that the channel has been closed from the other end. Note that recv() may raise BlockingIOError , even though select.select() or select.poll() has reported the socket ready for reading. \n"}, {"name": "asyncore.dispatcher.send()", "path": "library/asyncore#asyncore.dispatcher.send", "type": "Networking & Interprocess Communication", "text": " \nsend(data)  \nSend data to the remote end-point of the socket. \n"}, {"name": "asyncore.dispatcher.writable()", "path": "library/asyncore#asyncore.dispatcher.writable", "type": "Networking & Interprocess Communication", "text": " \nwritable()  \nCalled each time around the asynchronous loop to determine whether a channel\u2019s socket should be added to the list on which write events can occur. The default method simply returns True, indicating that by default, all channels will be interested in write events. \n"}, {"name": "asyncore.dispatcher_with_send", "path": "library/asyncore#asyncore.dispatcher_with_send", "type": "Networking & Interprocess Communication", "text": " \nclass asyncore.dispatcher_with_send  \nA dispatcher subclass which adds simple buffered output capability, useful for simple clients. For more sophisticated usage use asynchat.async_chat. \n"}, {"name": "asyncore.file_dispatcher", "path": "library/asyncore#asyncore.file_dispatcher", "type": "Networking & Interprocess Communication", "text": " \nclass asyncore.file_dispatcher  \nA file_dispatcher takes a file descriptor or file object along with an optional map argument and wraps it for use with the poll() or loop() functions. If provided a file object or anything with a fileno() method, that method will be called and passed to the file_wrapper constructor. Availability: Unix. \n"}, {"name": "asyncore.file_wrapper", "path": "library/asyncore#asyncore.file_wrapper", "type": "Networking & Interprocess Communication", "text": " \nclass asyncore.file_wrapper  \nA file_wrapper takes an integer file descriptor and calls os.dup() to duplicate the handle so that the original handle may be closed independently of the file_wrapper. This class implements sufficient methods to emulate a socket for use by the file_dispatcher class. Availability: Unix. \n"}, {"name": "asyncore.loop()", "path": "library/asyncore#asyncore.loop", "type": "Networking & Interprocess Communication", "text": " \nasyncore.loop([timeout[, use_poll[, map[, count]]]])  \nEnter a polling loop that terminates after count passes or all open channels have been closed. All arguments are optional. The count parameter defaults to None, resulting in the loop terminating only when all channels have been closed. The timeout argument sets the timeout parameter for the appropriate select() or poll() call, measured in seconds; the default is 30 seconds. The use_poll parameter, if true, indicates that poll() should be used in preference to select() (the default is False). The map parameter is a dictionary whose items are the channels to watch. As channels are closed they are deleted from their map. If map is omitted, a global map is used. Channels (instances of asyncore.dispatcher, asynchat.async_chat and subclasses thereof) can freely be mixed in the map. \n"}, {"name": "atexit", "path": "library/atexit", "type": "Runtime", "text": "atexit \u2014 Exit handlers The atexit module defines functions to register and unregister cleanup functions. Functions thus registered are automatically executed upon normal interpreter termination. atexit runs these functions in the reverse order in which they were registered; if you register A, B, and C, at interpreter termination time they will be run in the order C, B, A. Note: The functions registered via this module are not called when the program is killed by a signal not handled by Python, when a Python fatal internal error is detected, or when os._exit() is called.  Changed in version 3.7: When used with C-API subinterpreters, registered functions are local to the interpreter they were registered in.   \natexit.register(func, *args, **kwargs)  \nRegister func as a function to be executed at termination. Any optional arguments that are to be passed to func must be passed as arguments to register(). It is possible to register the same function and arguments more than once. At normal program termination (for instance, if sys.exit() is called or the main module\u2019s execution completes), all functions registered are called in last in, first out order. The assumption is that lower level modules will normally be imported before higher level modules and thus must be cleaned up later. If an exception is raised during execution of the exit handlers, a traceback is printed (unless SystemExit is raised) and the exception information is saved. After all exit handlers have had a chance to run the last exception to be raised is re-raised. This function returns func, which makes it possible to use it as a decorator. \n  \natexit.unregister(func)  \nRemove func from the list of functions to be run at interpreter shutdown. After calling unregister(), func is guaranteed not to be called when the interpreter shuts down, even if it was registered more than once. unregister() silently does nothing if func was not previously registered. \n  See also  \nModule readline\n\n\nUseful example of atexit to read and write readline history files.    atexit Example The following simple example demonstrates how a module can initialize a counter from a file when it is imported and save the counter\u2019s updated value automatically when the program terminates without relying on the application making an explicit call into this module at termination. try:\n    with open(\"counterfile\") as infile:\n        _count = int(infile.read())\nexcept FileNotFoundError:\n    _count = 0\n\ndef incrcounter(n):\n    global _count\n    _count = _count + n\n\ndef savecounter():\n    with open(\"counterfile\", \"w\") as outfile:\n        outfile.write(\"%d\" % _count)\n\nimport atexit\natexit.register(savecounter)\n Positional and keyword arguments may also be passed to register() to be passed along to the registered function when it is called: def goodbye(name, adjective):\n    print('Goodbye, %s, it was %s to meet you.' % (name, adjective))\n\nimport atexit\natexit.register(goodbye, 'Donny', 'nice')\n\n# or:\natexit.register(goodbye, adjective='nice', name='Donny')\n Usage as a decorator: import atexit\n\n@atexit.register\ndef goodbye():\n    print(\"You are now leaving the Python sector.\")\n This only works with functions that can be called without arguments.\n"}, {"name": "atexit.register()", "path": "library/atexit#atexit.register", "type": "Runtime", "text": " \natexit.register(func, *args, **kwargs)  \nRegister func as a function to be executed at termination. Any optional arguments that are to be passed to func must be passed as arguments to register(). It is possible to register the same function and arguments more than once. At normal program termination (for instance, if sys.exit() is called or the main module\u2019s execution completes), all functions registered are called in last in, first out order. The assumption is that lower level modules will normally be imported before higher level modules and thus must be cleaned up later. If an exception is raised during execution of the exit handlers, a traceback is printed (unless SystemExit is raised) and the exception information is saved. After all exit handlers have had a chance to run the last exception to be raised is re-raised. This function returns func, which makes it possible to use it as a decorator. \n"}, {"name": "atexit.unregister()", "path": "library/atexit#atexit.unregister", "type": "Runtime", "text": " \natexit.unregister(func)  \nRemove func from the list of functions to be run at interpreter shutdown. After calling unregister(), func is guaranteed not to be called when the interpreter shuts down, even if it was registered more than once. unregister() silently does nothing if func was not previously registered. \n"}, {"name": "AttributeError", "path": "library/exceptions#AttributeError", "type": "Built-in Exceptions", "text": " \nexception AttributeError  \nRaised when an attribute reference (see Attribute references) or assignment fails. (When an object does not support attribute references or attribute assignments at all, TypeError is raised.) \n"}, {"name": "audioop", "path": "library/audioop", "type": "Multimedia", "text": "audioop \u2014 Manipulate raw audio data The audioop module contains some useful operations on sound fragments. It operates on sound fragments consisting of signed integer samples 8, 16, 24 or 32 bits wide, stored in bytes-like objects. All scalar items are integers, unless specified otherwise.  Changed in version 3.4: Support for 24-bit samples was added. All functions now accept any bytes-like object. String input now results in an immediate error.  This module provides support for a-LAW, u-LAW and Intel/DVI ADPCM encodings. A few of the more complicated operations only take 16-bit samples, otherwise the sample size (in bytes) is always a parameter of the operation. The module defines the following variables and functions:  \nexception audioop.error  \nThis exception is raised on all errors, such as unknown number of bytes per sample, etc. \n  \naudioop.add(fragment1, fragment2, width)  \nReturn a fragment which is the addition of the two samples passed as parameters. width is the sample width in bytes, either 1, 2, 3 or 4. Both fragments should have the same length. Samples are truncated in case of overflow. \n  \naudioop.adpcm2lin(adpcmfragment, width, state)  \nDecode an Intel/DVI ADPCM coded fragment to a linear fragment. See the description of lin2adpcm() for details on ADPCM coding. Return a tuple (sample, newstate) where the sample has the width specified in width. \n  \naudioop.alaw2lin(fragment, width)  \nConvert sound fragments in a-LAW encoding to linearly encoded sound fragments. a-LAW encoding always uses 8 bits samples, so width refers only to the sample width of the output fragment here. \n  \naudioop.avg(fragment, width)  \nReturn the average over all samples in the fragment. \n  \naudioop.avgpp(fragment, width)  \nReturn the average peak-peak value over all samples in the fragment. No filtering is done, so the usefulness of this routine is questionable. \n  \naudioop.bias(fragment, width, bias)  \nReturn a fragment that is the original fragment with a bias added to each sample. Samples wrap around in case of overflow. \n  \naudioop.byteswap(fragment, width)  \n\u201cByteswap\u201d all samples in a fragment and returns the modified fragment. Converts big-endian samples to little-endian and vice versa.  New in version 3.4.  \n  \naudioop.cross(fragment, width)  \nReturn the number of zero crossings in the fragment passed as an argument. \n  \naudioop.findfactor(fragment, reference)  \nReturn a factor F such that rms(add(fragment, mul(reference, -F))) is minimal, i.e., return the factor with which you should multiply reference to make it match as well as possible to fragment. The fragments should both contain 2-byte samples. The time taken by this routine is proportional to len(fragment). \n  \naudioop.findfit(fragment, reference)  \nTry to match reference as well as possible to a portion of fragment (which should be the longer fragment). This is (conceptually) done by taking slices out of fragment, using findfactor() to compute the best match, and minimizing the result. The fragments should both contain 2-byte samples. Return a tuple (offset, factor) where offset is the (integer) offset into fragment where the optimal match started and factor is the (floating-point) factor as per findfactor(). \n  \naudioop.findmax(fragment, length)  \nSearch fragment for a slice of length length samples (not bytes!) with maximum energy, i.e., return i for which rms(fragment[i*2:(i+length)*2]) is maximal. The fragments should both contain 2-byte samples. The routine takes time proportional to len(fragment). \n  \naudioop.getsample(fragment, width, index)  \nReturn the value of sample index from the fragment. \n  \naudioop.lin2adpcm(fragment, width, state)  \nConvert samples to 4 bit Intel/DVI ADPCM encoding. ADPCM coding is an adaptive coding scheme, whereby each 4 bit number is the difference between one sample and the next, divided by a (varying) step. The Intel/DVI ADPCM algorithm has been selected for use by the IMA, so it may well become a standard. state is a tuple containing the state of the coder. The coder returns a tuple (adpcmfrag, newstate), and the newstate should be passed to the next call of lin2adpcm(). In the initial call, None can be passed as the state. adpcmfrag is the ADPCM coded fragment packed 2 4-bit values per byte. \n  \naudioop.lin2alaw(fragment, width)  \nConvert samples in the audio fragment to a-LAW encoding and return this as a bytes object. a-LAW is an audio encoding format whereby you get a dynamic range of about 13 bits using only 8 bit samples. It is used by the Sun audio hardware, among others. \n  \naudioop.lin2lin(fragment, width, newwidth)  \nConvert samples between 1-, 2-, 3- and 4-byte formats.  Note In some audio formats, such as .WAV files, 16, 24 and 32 bit samples are signed, but 8 bit samples are unsigned. So when converting to 8 bit wide samples for these formats, you need to also add 128 to the result: new_frames = audioop.lin2lin(frames, old_width, 1)\nnew_frames = audioop.bias(new_frames, 1, 128)\n The same, in reverse, has to be applied when converting from 8 to 16, 24 or 32 bit width samples.  \n  \naudioop.lin2ulaw(fragment, width)  \nConvert samples in the audio fragment to u-LAW encoding and return this as a bytes object. u-LAW is an audio encoding format whereby you get a dynamic range of about 14 bits using only 8 bit samples. It is used by the Sun audio hardware, among others. \n  \naudioop.max(fragment, width)  \nReturn the maximum of the absolute value of all samples in a fragment. \n  \naudioop.maxpp(fragment, width)  \nReturn the maximum peak-peak value in the sound fragment. \n  \naudioop.minmax(fragment, width)  \nReturn a tuple consisting of the minimum and maximum values of all samples in the sound fragment. \n  \naudioop.mul(fragment, width, factor)  \nReturn a fragment that has all samples in the original fragment multiplied by the floating-point value factor. Samples are truncated in case of overflow. \n  \naudioop.ratecv(fragment, width, nchannels, inrate, outrate, state[, weightA[, weightB]])  \nConvert the frame rate of the input fragment. state is a tuple containing the state of the converter. The converter returns a tuple (newfragment, newstate), and newstate should be passed to the next call of ratecv(). The initial call should pass None as the state. The weightA and weightB arguments are parameters for a simple digital filter and default to 1 and 0 respectively. \n  \naudioop.reverse(fragment, width)  \nReverse the samples in a fragment and returns the modified fragment. \n  \naudioop.rms(fragment, width)  \nReturn the root-mean-square of the fragment, i.e. sqrt(sum(S_i^2)/n). This is a measure of the power in an audio signal. \n  \naudioop.tomono(fragment, width, lfactor, rfactor)  \nConvert a stereo fragment to a mono fragment. The left channel is multiplied by lfactor and the right channel by rfactor before adding the two channels to give a mono signal. \n  \naudioop.tostereo(fragment, width, lfactor, rfactor)  \nGenerate a stereo fragment from a mono fragment. Each pair of samples in the stereo fragment are computed from the mono sample, whereby left channel samples are multiplied by lfactor and right channel samples by rfactor. \n  \naudioop.ulaw2lin(fragment, width)  \nConvert sound fragments in u-LAW encoding to linearly encoded sound fragments. u-LAW encoding always uses 8 bits samples, so width refers only to the sample width of the output fragment here. \n Note that operations such as mul() or max() make no distinction between mono and stereo fragments, i.e. all samples are treated equal. If this is a problem the stereo fragment should be split into two mono fragments first and recombined later. Here is an example of how to do that: def mul_stereo(sample, width, lfactor, rfactor):\n    lsample = audioop.tomono(sample, width, 1, 0)\n    rsample = audioop.tomono(sample, width, 0, 1)\n    lsample = audioop.mul(lsample, width, lfactor)\n    rsample = audioop.mul(rsample, width, rfactor)\n    lsample = audioop.tostereo(lsample, width, 1, 0)\n    rsample = audioop.tostereo(rsample, width, 0, 1)\n    return audioop.add(lsample, rsample, width)\n If you use the ADPCM coder to build network packets and you want your protocol to be stateless (i.e. to be able to tolerate packet loss) you should not only transmit the data but also the state. Note that you should send the initial state (the one you passed to lin2adpcm()) along to the decoder, not the final state (as returned by the coder). If you want to use struct.Struct to store the state in binary you can code the first element (the predicted value) in 16 bits and the second (the delta index) in 8. The ADPCM coders have never been tried against other ADPCM coders, only against themselves. It could well be that I misinterpreted the standards in which case they will not be interoperable with the respective standards. The find*() routines might look a bit funny at first sight. They are primarily meant to do echo cancellation. A reasonably fast way to do this is to pick the most energetic piece of the output sample, locate that in the input sample and subtract the whole output sample from the input sample: def echocancel(outputdata, inputdata):\n    pos = audioop.findmax(outputdata, 800)    # one tenth second\n    out_test = outputdata[pos*2:]\n    in_test = inputdata[pos*2:]\n    ipos, factor = audioop.findfit(in_test, out_test)\n    # Optional (for better cancellation):\n    # factor = audioop.findfactor(in_test[ipos*2:ipos*2+len(out_test)],\n    #              out_test)\n    prefill = '\\0'*(pos+ipos)*2\n    postfill = '\\0'*(len(inputdata)-len(prefill)-len(outputdata))\n    outputdata = prefill + audioop.mul(outputdata, 2, -factor) + postfill\n    return audioop.add(inputdata, outputdata, 2)\n\n"}, {"name": "audioop.add()", "path": "library/audioop#audioop.add", "type": "Multimedia", "text": " \naudioop.add(fragment1, fragment2, width)  \nReturn a fragment which is the addition of the two samples passed as parameters. width is the sample width in bytes, either 1, 2, 3 or 4. Both fragments should have the same length. Samples are truncated in case of overflow. \n"}, {"name": "audioop.adpcm2lin()", "path": "library/audioop#audioop.adpcm2lin", "type": "Multimedia", "text": " \naudioop.adpcm2lin(adpcmfragment, width, state)  \nDecode an Intel/DVI ADPCM coded fragment to a linear fragment. See the description of lin2adpcm() for details on ADPCM coding. Return a tuple (sample, newstate) where the sample has the width specified in width. \n"}, {"name": "audioop.alaw2lin()", "path": "library/audioop#audioop.alaw2lin", "type": "Multimedia", "text": " \naudioop.alaw2lin(fragment, width)  \nConvert sound fragments in a-LAW encoding to linearly encoded sound fragments. a-LAW encoding always uses 8 bits samples, so width refers only to the sample width of the output fragment here. \n"}, {"name": "audioop.avg()", "path": "library/audioop#audioop.avg", "type": "Multimedia", "text": " \naudioop.avg(fragment, width)  \nReturn the average over all samples in the fragment. \n"}, {"name": "audioop.avgpp()", "path": "library/audioop#audioop.avgpp", "type": "Multimedia", "text": " \naudioop.avgpp(fragment, width)  \nReturn the average peak-peak value over all samples in the fragment. No filtering is done, so the usefulness of this routine is questionable. \n"}, {"name": "audioop.bias()", "path": "library/audioop#audioop.bias", "type": "Multimedia", "text": " \naudioop.bias(fragment, width, bias)  \nReturn a fragment that is the original fragment with a bias added to each sample. Samples wrap around in case of overflow. \n"}, {"name": "audioop.byteswap()", "path": "library/audioop#audioop.byteswap", "type": "Multimedia", "text": " \naudioop.byteswap(fragment, width)  \n\u201cByteswap\u201d all samples in a fragment and returns the modified fragment. Converts big-endian samples to little-endian and vice versa.  New in version 3.4.  \n"}, {"name": "audioop.cross()", "path": "library/audioop#audioop.cross", "type": "Multimedia", "text": " \naudioop.cross(fragment, width)  \nReturn the number of zero crossings in the fragment passed as an argument. \n"}, {"name": "audioop.error", "path": "library/audioop#audioop.error", "type": "Multimedia", "text": " \nexception audioop.error  \nThis exception is raised on all errors, such as unknown number of bytes per sample, etc. \n"}, {"name": "audioop.findfactor()", "path": "library/audioop#audioop.findfactor", "type": "Multimedia", "text": " \naudioop.findfactor(fragment, reference)  \nReturn a factor F such that rms(add(fragment, mul(reference, -F))) is minimal, i.e., return the factor with which you should multiply reference to make it match as well as possible to fragment. The fragments should both contain 2-byte samples. The time taken by this routine is proportional to len(fragment). \n"}, {"name": "audioop.findfit()", "path": "library/audioop#audioop.findfit", "type": "Multimedia", "text": " \naudioop.findfit(fragment, reference)  \nTry to match reference as well as possible to a portion of fragment (which should be the longer fragment). This is (conceptually) done by taking slices out of fragment, using findfactor() to compute the best match, and minimizing the result. The fragments should both contain 2-byte samples. Return a tuple (offset, factor) where offset is the (integer) offset into fragment where the optimal match started and factor is the (floating-point) factor as per findfactor(). \n"}, {"name": "audioop.findmax()", "path": "library/audioop#audioop.findmax", "type": "Multimedia", "text": " \naudioop.findmax(fragment, length)  \nSearch fragment for a slice of length length samples (not bytes!) with maximum energy, i.e., return i for which rms(fragment[i*2:(i+length)*2]) is maximal. The fragments should both contain 2-byte samples. The routine takes time proportional to len(fragment). \n"}, {"name": "audioop.getsample()", "path": "library/audioop#audioop.getsample", "type": "Multimedia", "text": " \naudioop.getsample(fragment, width, index)  \nReturn the value of sample index from the fragment. \n"}, {"name": "audioop.lin2adpcm()", "path": "library/audioop#audioop.lin2adpcm", "type": "Multimedia", "text": " \naudioop.lin2adpcm(fragment, width, state)  \nConvert samples to 4 bit Intel/DVI ADPCM encoding. ADPCM coding is an adaptive coding scheme, whereby each 4 bit number is the difference between one sample and the next, divided by a (varying) step. The Intel/DVI ADPCM algorithm has been selected for use by the IMA, so it may well become a standard. state is a tuple containing the state of the coder. The coder returns a tuple (adpcmfrag, newstate), and the newstate should be passed to the next call of lin2adpcm(). In the initial call, None can be passed as the state. adpcmfrag is the ADPCM coded fragment packed 2 4-bit values per byte. \n"}, {"name": "audioop.lin2alaw()", "path": "library/audioop#audioop.lin2alaw", "type": "Multimedia", "text": " \naudioop.lin2alaw(fragment, width)  \nConvert samples in the audio fragment to a-LAW encoding and return this as a bytes object. a-LAW is an audio encoding format whereby you get a dynamic range of about 13 bits using only 8 bit samples. It is used by the Sun audio hardware, among others. \n"}, {"name": "audioop.lin2lin()", "path": "library/audioop#audioop.lin2lin", "type": "Multimedia", "text": " \naudioop.lin2lin(fragment, width, newwidth)  \nConvert samples between 1-, 2-, 3- and 4-byte formats.  Note In some audio formats, such as .WAV files, 16, 24 and 32 bit samples are signed, but 8 bit samples are unsigned. So when converting to 8 bit wide samples for these formats, you need to also add 128 to the result: new_frames = audioop.lin2lin(frames, old_width, 1)\nnew_frames = audioop.bias(new_frames, 1, 128)\n The same, in reverse, has to be applied when converting from 8 to 16, 24 or 32 bit width samples.  \n"}, {"name": "audioop.lin2ulaw()", "path": "library/audioop#audioop.lin2ulaw", "type": "Multimedia", "text": " \naudioop.lin2ulaw(fragment, width)  \nConvert samples in the audio fragment to u-LAW encoding and return this as a bytes object. u-LAW is an audio encoding format whereby you get a dynamic range of about 14 bits using only 8 bit samples. It is used by the Sun audio hardware, among others. \n"}, {"name": "audioop.max()", "path": "library/audioop#audioop.max", "type": "Multimedia", "text": " \naudioop.max(fragment, width)  \nReturn the maximum of the absolute value of all samples in a fragment. \n"}, {"name": "audioop.maxpp()", "path": "library/audioop#audioop.maxpp", "type": "Multimedia", "text": " \naudioop.maxpp(fragment, width)  \nReturn the maximum peak-peak value in the sound fragment. \n"}, {"name": "audioop.minmax()", "path": "library/audioop#audioop.minmax", "type": "Multimedia", "text": " \naudioop.minmax(fragment, width)  \nReturn a tuple consisting of the minimum and maximum values of all samples in the sound fragment. \n"}, {"name": "audioop.mul()", "path": "library/audioop#audioop.mul", "type": "Multimedia", "text": " \naudioop.mul(fragment, width, factor)  \nReturn a fragment that has all samples in the original fragment multiplied by the floating-point value factor. Samples are truncated in case of overflow. \n"}, {"name": "audioop.ratecv()", "path": "library/audioop#audioop.ratecv", "type": "Multimedia", "text": " \naudioop.ratecv(fragment, width, nchannels, inrate, outrate, state[, weightA[, weightB]])  \nConvert the frame rate of the input fragment. state is a tuple containing the state of the converter. The converter returns a tuple (newfragment, newstate), and newstate should be passed to the next call of ratecv(). The initial call should pass None as the state. The weightA and weightB arguments are parameters for a simple digital filter and default to 1 and 0 respectively. \n"}, {"name": "audioop.reverse()", "path": "library/audioop#audioop.reverse", "type": "Multimedia", "text": " \naudioop.reverse(fragment, width)  \nReverse the samples in a fragment and returns the modified fragment. \n"}, {"name": "audioop.rms()", "path": "library/audioop#audioop.rms", "type": "Multimedia", "text": " \naudioop.rms(fragment, width)  \nReturn the root-mean-square of the fragment, i.e. sqrt(sum(S_i^2)/n). This is a measure of the power in an audio signal. \n"}, {"name": "audioop.tomono()", "path": "library/audioop#audioop.tomono", "type": "Multimedia", "text": " \naudioop.tomono(fragment, width, lfactor, rfactor)  \nConvert a stereo fragment to a mono fragment. The left channel is multiplied by lfactor and the right channel by rfactor before adding the two channels to give a mono signal. \n"}, {"name": "audioop.tostereo()", "path": "library/audioop#audioop.tostereo", "type": "Multimedia", "text": " \naudioop.tostereo(fragment, width, lfactor, rfactor)  \nGenerate a stereo fragment from a mono fragment. Each pair of samples in the stereo fragment are computed from the mono sample, whereby left channel samples are multiplied by lfactor and right channel samples by rfactor. \n"}, {"name": "audioop.ulaw2lin()", "path": "library/audioop#audioop.ulaw2lin", "type": "Multimedia", "text": " \naudioop.ulaw2lin(fragment, width)  \nConvert sound fragments in u-LAW encoding to linearly encoded sound fragments. u-LAW encoding always uses 8 bits samples, so width refers only to the sample width of the output fragment here. \n"}, {"name": "Audit events table", "path": "library/audit_events", "type": "Debugging & Profiling", "text": "Audit events table This table contains all events raised by sys.audit() or PySys_Audit() calls throughout the CPython runtime and the standard library. These calls were added in 3.8.0 or later. See sys.addaudithook() and PySys_AddAuditHook() for information on handling these events.  CPython implementation detail: This table is generated from the CPython documentation, and may not represent events raised by other implementations. See your runtime specific documentation for actual events raised.    \nAudit event Arguments References   \narray.__new__ typecode, initializer [1]  \nbuiltins.breakpoint breakpointhook [1]  \nbuiltins.id id [1]  \nbuiltins.input prompt [1]  \nbuiltins.input/result result [1]  \ncode.__new__ code, filename, name, argcount, posonlyargcount, kwonlyargcount, nlocals, stacksize, flags [1]  \ncompile source, filename [1]  \ncpython.PyInterpreterState_Clear  [1]  \ncpython.PyInterpreterState_New  [1]  \ncpython._PySys_ClearAuditHooks  [1]  \ncpython.run_command command [1]  \ncpython.run_file filename [1]  \ncpython.run_interactivehook hook [1]  \ncpython.run_module module-name [1]  \ncpython.run_startup filename [1]  \ncpython.run_stdin  [1]  \nctypes.addressof obj [1]  \nctypes.call_function func_pointer, arguments [1]  \nctypes.cdata address [1]  \nctypes.cdata/buffer pointer, size, offset [1][2]  \nctypes.create_string_buffer init, size [1]  \nctypes.create_unicode_buffer init, size [1]  \nctypes.dlopen name [1]  \nctypes.dlsym library, name [1]  \nctypes.dlsym/handle handle, name [1]  \nctypes.get_errno  [1]  \nctypes.get_last_error  [1]  \nctypes.seh_exception code [1]  \nctypes.set_errno errno [1]  \nctypes.set_last_error error [1]  \nctypes.string_at address, size [1]  \nctypes.wstring_at address, size [1]  \nensurepip.bootstrap root [1]  \nexec code_object [1][2]  \nfcntl.fcntl fd, cmd, arg [1]  \nfcntl.flock fd, operation [1]  \nfcntl.ioctl fd, request, arg [1]  \nfcntl.lockf fd, cmd, len, start, whence [1]  \nftplib.connect self, host, port [1]  \nftplib.sendcmd self, cmd [1][2]  \nfunction.__new__ code [1]  \ngc.get_objects generation [1]  \ngc.get_referents objs [1]  \ngc.get_referrers objs [1]  \nglob.glob pathname, recursive [1][2]  \nimaplib.open self, host, port [1]  \nimaplib.send self, data [1]  \nimport module, filename, sys.path, sys.meta_path, sys.path_hooks [1]  \nmmap.__new__ fileno, length, access, offset [1]  \nmsvcrt.get_osfhandle fd [1]  \nmsvcrt.locking fd, mode, nbytes [1]  \nmsvcrt.open_osfhandle handle, flags [1]  \nnntplib.connect self, host, port [1][2]  \nnntplib.putline self, line [1][2]  \nobject.__delattr__ obj, name [1]  \nobject.__getattr__ obj, name [1]  \nobject.__setattr__ obj, name, value [1]  \nopen file, mode, flags [1][2][3]  \nos.add_dll_directory path [1]  \nos.chdir path [1][2]  \nos.chflags path, flags [1][2]  \nos.chmod path, mode, dir_fd [1][2][3]  \nos.chown path, uid, gid, dir_fd [1][2][3]  \nos.exec path, args, env [1]  \nos.fork  [1]  \nos.forkpty  [1]  \nos.fwalk top, topdown, onerror, follow_symlinks, dir_fd [1]  \nos.getxattr path, attribute [1]  \nos.kill pid, sig [1]  \nos.killpg pgid, sig [1]  \nos.link src, dst, src_dir_fd, dst_dir_fd [1]  \nos.listdir path [1]  \nos.listxattr path [1]  \nos.lockf fd, cmd, len [1]  \nos.mkdir path, mode, dir_fd [1][2]  \nos.posix_spawn path, argv, env [1][2]  \nos.putenv key, value [1]  \nos.remove path, dir_fd [1][2][3]  \nos.removexattr path, attribute [1]  \nos.rename src, dst, src_dir_fd, dst_dir_fd [1][2][3]  \nos.rmdir path, dir_fd [1]  \nos.scandir path [1]  \nos.setxattr path, attribute, value, flags [1]  \nos.spawn mode, path, args, env [1]  \nos.startfile path, operation [1]  \nos.symlink src, dst, dir_fd [1]  \nos.system command [1]  \nos.truncate fd, length [1][2]  \nos.unsetenv key [1]  \nos.utime path, times, ns, dir_fd [1]  \nos.walk top, topdown, onerror, followlinks [1]  \npathlib.Path.glob self, pattern [1]  \npathlib.Path.rglob self, pattern [1]  \npdb.Pdb  [1]  \npickle.find_class module, name [1]  \npoplib.connect self, host, port [1][2]  \npoplib.putline self, line [1][2]  \npty.spawn argv [1]  \nresource.prlimit pid, resource, limits [1]  \nresource.setrlimit resource, limits [1]  \nsetopencodehook  [1]  \nshutil.chown path, user, group [1]  \nshutil.copyfile src, dst [1][2][3]  \nshutil.copymode src, dst [1][2]  \nshutil.copystat src, dst [1][2]  \nshutil.copytree src, dst [1]  \nshutil.make_archive base_name, format, root_dir, base_dir [1]  \nshutil.move src, dst [1]  \nshutil.rmtree path [1]  \nshutil.unpack_archive filename, extract_dir, format [1]  \nsignal.pthread_kill thread_id, signalnum [1]  \nsmtplib.connect self, host, port [1]  \nsmtplib.send self, data [1]  \nsocket.__new__ self, family, type, protocol [1]  \nsocket.bind self, address [1]  \nsocket.connect self, address [1][2]  \nsocket.getaddrinfo host, port, family, type, protocol [1]  \nsocket.gethostbyaddr ip_address [1]  \nsocket.gethostbyname hostname [1][2]  \nsocket.gethostname  [1]  \nsocket.getnameinfo sockaddr [1]  \nsocket.getservbyname servicename, protocolname [1]  \nsocket.getservbyport port, protocolname [1]  \nsocket.sendmsg self, address [1]  \nsocket.sendto self, address [1]  \nsocket.sethostname name [1]  \nsqlite3.connect database [1]  \nsubprocess.Popen executable, args, cwd, env [1]  \nsys._current_frames  [1]  \nsys._getframe  [1]  \nsys.addaudithook  [1][2]  \nsys.excepthook hook, type, value, traceback [1]  \nsys.set_asyncgen_hooks_finalizer  [1]  \nsys.set_asyncgen_hooks_firstiter  [1]  \nsys.setprofile  [1]  \nsys.settrace  [1]  \nsys.unraisablehook hook, unraisable [1]  \nsyslog.closelog  [1]  \nsyslog.openlog ident, logoption, facility [1]  \nsyslog.setlogmask maskpri [1]  \nsyslog.syslog priority, message [1]  \ntelnetlib.Telnet.open self, host, port [1]  \ntelnetlib.Telnet.write self, buffer [1]  \ntempfile.mkdtemp fullpath [1][2]  \ntempfile.mkstemp fullpath [1][2][3]  \nurllib.Request fullurl, data, headers, method [1]  \nwebbrowser.open url [1]  \nwinreg.ConnectRegistry computer_name, key [1]  \nwinreg.CreateKey key, sub_key, access [1][2]  \nwinreg.DeleteKey key, sub_key, access [1][2]  \nwinreg.DeleteValue key, value [1]  \nwinreg.DisableReflectionKey key [1]  \nwinreg.EnableReflectionKey key [1]  \nwinreg.EnumKey key, index [1]  \nwinreg.EnumValue key, index [1]  \nwinreg.ExpandEnvironmentStrings str [1]  \nwinreg.LoadKey key, sub_key, file_name [1]  \nwinreg.OpenKey key, sub_key, access [1]  \nwinreg.OpenKey/result key [1][2][3]  \nwinreg.PyHKEY.Detach key [1]  \nwinreg.QueryInfoKey key [1]  \nwinreg.QueryReflectionKey key [1]  \nwinreg.QueryValue key, sub_key, value_name [1][2]  \nwinreg.SaveKey key, file_name [1]  \nwinreg.SetValue key, sub_key, type, value [1][2]   The following events are raised internally and do not correspond to any public API of CPython:   \nAudit event Arguments   \n_winapi.CreateFile file_name, desired_access, share_mode, creation_disposition, flags_and_attributes  \n_winapi.CreateJunction src_path, dst_path  \n_winapi.CreateNamedPipe name, open_mode, pipe_mode  \n_winapi.CreatePipe   \n_winapi.CreateProcess application_name, command_line, current_directory  \n_winapi.OpenProcess process_id, desired_access  \n_winapi.TerminateProcess handle, exit_code  \nctypes.PyObj_FromPtr obj  \n"}, {"name": "base64", "path": "library/base64", "type": "Internet Data", "text": "base64 \u2014 Base16, Base32, Base64, Base85 Data Encodings Source code: Lib/base64.py This module provides functions for encoding binary data to printable ASCII characters and decoding such encodings back to binary data. It provides encoding and decoding functions for the encodings specified in RFC 3548, which defines the Base16, Base32, and Base64 algorithms, and for the de-facto standard Ascii85 and Base85 encodings. The RFC 3548 encodings are suitable for encoding binary data so that it can safely sent by email, used as parts of URLs, or included as part of an HTTP POST request. The encoding algorithm is not the same as the uuencode program. There are two interfaces provided by this module. The modern interface supports encoding bytes-like objects to ASCII bytes, and decoding bytes-like objects or strings containing ASCII to bytes. Both base-64 alphabets defined in RFC 3548 (normal, and URL- and filesystem-safe) are supported. The legacy interface does not support decoding from strings, but it does provide functions for encoding and decoding to and from file objects. It only supports the Base64 standard alphabet, and it adds newlines every 76 characters as per RFC 2045. Note that if you are looking for RFC 2045 support you probably want to be looking at the email package instead.  Changed in version 3.3: ASCII-only Unicode strings are now accepted by the decoding functions of the modern interface.   Changed in version 3.4: Any bytes-like objects are now accepted by all encoding and decoding functions in this module. Ascii85/Base85 support added.  The modern interface provides:  \nbase64.b64encode(s, altchars=None)  \nEncode the bytes-like object s using Base64 and return the encoded bytes. Optional altchars must be a bytes-like object of at least length 2 (additional characters are ignored) which specifies an alternative alphabet for the + and / characters. This allows an application to e.g. generate URL or filesystem safe Base64 strings. The default is None, for which the standard Base64 alphabet is used. \n  \nbase64.b64decode(s, altchars=None, validate=False)  \nDecode the Base64 encoded bytes-like object or ASCII string s and return the decoded bytes. Optional altchars must be a bytes-like object or ASCII string of at least length 2 (additional characters are ignored) which specifies the alternative alphabet used instead of the + and / characters. A binascii.Error exception is raised if s is incorrectly padded. If validate is False (the default), characters that are neither in the normal base-64 alphabet nor the alternative alphabet are discarded prior to the padding check. If validate is True, these non-alphabet characters in the input result in a binascii.Error. \n  \nbase64.standard_b64encode(s)  \nEncode bytes-like object s using the standard Base64 alphabet and return the encoded bytes. \n  \nbase64.standard_b64decode(s)  \nDecode bytes-like object or ASCII string s using the standard Base64 alphabet and return the decoded bytes. \n  \nbase64.urlsafe_b64encode(s)  \nEncode bytes-like object s using the URL- and filesystem-safe alphabet, which substitutes - instead of + and _ instead of / in the standard Base64 alphabet, and return the encoded bytes. The result can still contain =. \n  \nbase64.urlsafe_b64decode(s)  \nDecode bytes-like object or ASCII string s using the URL- and filesystem-safe alphabet, which substitutes - instead of + and _ instead of / in the standard Base64 alphabet, and return the decoded bytes. \n  \nbase64.b32encode(s)  \nEncode the bytes-like object s using Base32 and return the encoded bytes. \n  \nbase64.b32decode(s, casefold=False, map01=None)  \nDecode the Base32 encoded bytes-like object or ASCII string s and return the decoded bytes. Optional casefold is a flag specifying whether a lowercase alphabet is acceptable as input. For security purposes, the default is False. RFC 3548 allows for optional mapping of the digit 0 (zero) to the letter O (oh), and for optional mapping of the digit 1 (one) to either the letter I (eye) or letter L (el). The optional argument map01 when not None, specifies which letter the digit 1 should be mapped to (when map01 is not None, the digit 0 is always mapped to the letter O). For security purposes the default is None, so that 0 and 1 are not allowed in the input. A binascii.Error is raised if s is incorrectly padded or if there are non-alphabet characters present in the input. \n  \nbase64.b16encode(s)  \nEncode the bytes-like object s using Base16 and return the encoded bytes. \n  \nbase64.b16decode(s, casefold=False)  \nDecode the Base16 encoded bytes-like object or ASCII string s and return the decoded bytes. Optional casefold is a flag specifying whether a lowercase alphabet is acceptable as input. For security purposes, the default is False. A binascii.Error is raised if s is incorrectly padded or if there are non-alphabet characters present in the input. \n  \nbase64.a85encode(b, *, foldspaces=False, wrapcol=0, pad=False, adobe=False)  \nEncode the bytes-like object b using Ascii85 and return the encoded bytes. foldspaces is an optional flag that uses the special short sequence \u2018y\u2019 instead of 4 consecutive spaces (ASCII 0x20) as supported by \u2018btoa\u2019. This feature is not supported by the \u201cstandard\u201d Ascii85 encoding. wrapcol controls whether the output should have newline (b'\\n') characters added to it. If this is non-zero, each output line will be at most this many characters long. pad controls whether the input is padded to a multiple of 4 before encoding. Note that the btoa implementation always pads. adobe controls whether the encoded byte sequence is framed with <~ and ~>, which is used by the Adobe implementation.  New in version 3.4.  \n  \nbase64.a85decode(b, *, foldspaces=False, adobe=False, ignorechars=b' \\t\\n\\r\\v')  \nDecode the Ascii85 encoded bytes-like object or ASCII string b and return the decoded bytes. foldspaces is a flag that specifies whether the \u2018y\u2019 short sequence should be accepted as shorthand for 4 consecutive spaces (ASCII 0x20). This feature is not supported by the \u201cstandard\u201d Ascii85 encoding. adobe controls whether the input sequence is in Adobe Ascii85 format (i.e. is framed with <~ and ~>). ignorechars should be a bytes-like object or ASCII string containing characters to ignore from the input. This should only contain whitespace characters, and by default contains all whitespace characters in ASCII.  New in version 3.4.  \n  \nbase64.b85encode(b, pad=False)  \nEncode the bytes-like object b using base85 (as used in e.g. git-style binary diffs) and return the encoded bytes. If pad is true, the input is padded with b'\\0' so its length is a multiple of 4 bytes before encoding.  New in version 3.4.  \n  \nbase64.b85decode(b)  \nDecode the base85-encoded bytes-like object or ASCII string b and return the decoded bytes. Padding is implicitly removed, if necessary.  New in version 3.4.  \n The legacy interface:  \nbase64.decode(input, output)  \nDecode the contents of the binary input file and write the resulting binary data to the output file. input and output must be file objects. input will be read until input.readline() returns an empty bytes object. \n  \nbase64.decodebytes(s)  \nDecode the bytes-like object s, which must contain one or more lines of base64 encoded data, and return the decoded bytes.  New in version 3.1.  \n  \nbase64.encode(input, output)  \nEncode the contents of the binary input file and write the resulting base64 encoded data to the output file. input and output must be file objects. input will be read until input.read() returns an empty bytes object. encode() inserts a newline character (b'\\n') after every 76 bytes of the output, as well as ensuring that the output always ends with a newline, as per RFC 2045 (MIME). \n  \nbase64.encodebytes(s)  \nEncode the bytes-like object s, which can contain arbitrary binary data, and return bytes containing the base64-encoded data, with newlines (b'\\n') inserted after every 76 bytes of output, and ensuring that there is a trailing newline, as per RFC 2045 (MIME).  New in version 3.1.  \n An example usage of the module: >>> import base64\n>>> encoded = base64.b64encode(b'data to be encoded')\n>>> encoded\nb'ZGF0YSB0byBiZSBlbmNvZGVk'\n>>> data = base64.b64decode(encoded)\n>>> data\nb'data to be encoded'\n  See also  \nModule binascii\n\n\nSupport module containing ASCII-to-binary and binary-to-ASCII conversions.  \nRFC 1521 - MIME (Multipurpose Internet Mail Extensions) Part One: Mechanisms for Specifying and Describing the Format of Internet Message Bodies\n\nSection 5.2, \u201cBase64 Content-Transfer-Encoding,\u201d provides the definition of the base64 encoding.   \n"}, {"name": "base64.a85decode()", "path": "library/base64#base64.a85decode", "type": "Internet Data", "text": " \nbase64.a85decode(b, *, foldspaces=False, adobe=False, ignorechars=b' \\t\\n\\r\\v')  \nDecode the Ascii85 encoded bytes-like object or ASCII string b and return the decoded bytes. foldspaces is a flag that specifies whether the \u2018y\u2019 short sequence should be accepted as shorthand for 4 consecutive spaces (ASCII 0x20). This feature is not supported by the \u201cstandard\u201d Ascii85 encoding. adobe controls whether the input sequence is in Adobe Ascii85 format (i.e. is framed with <~ and ~>). ignorechars should be a bytes-like object or ASCII string containing characters to ignore from the input. This should only contain whitespace characters, and by default contains all whitespace characters in ASCII.  New in version 3.4.  \n"}, {"name": "base64.a85encode()", "path": "library/base64#base64.a85encode", "type": "Internet Data", "text": " \nbase64.a85encode(b, *, foldspaces=False, wrapcol=0, pad=False, adobe=False)  \nEncode the bytes-like object b using Ascii85 and return the encoded bytes. foldspaces is an optional flag that uses the special short sequence \u2018y\u2019 instead of 4 consecutive spaces (ASCII 0x20) as supported by \u2018btoa\u2019. This feature is not supported by the \u201cstandard\u201d Ascii85 encoding. wrapcol controls whether the output should have newline (b'\\n') characters added to it. If this is non-zero, each output line will be at most this many characters long. pad controls whether the input is padded to a multiple of 4 before encoding. Note that the btoa implementation always pads. adobe controls whether the encoded byte sequence is framed with <~ and ~>, which is used by the Adobe implementation.  New in version 3.4.  \n"}, {"name": "base64.b16decode()", "path": "library/base64#base64.b16decode", "type": "Internet Data", "text": " \nbase64.b16decode(s, casefold=False)  \nDecode the Base16 encoded bytes-like object or ASCII string s and return the decoded bytes. Optional casefold is a flag specifying whether a lowercase alphabet is acceptable as input. For security purposes, the default is False. A binascii.Error is raised if s is incorrectly padded or if there are non-alphabet characters present in the input. \n"}, {"name": "base64.b16encode()", "path": "library/base64#base64.b16encode", "type": "Internet Data", "text": " \nbase64.b16encode(s)  \nEncode the bytes-like object s using Base16 and return the encoded bytes. \n"}, {"name": "base64.b32decode()", "path": "library/base64#base64.b32decode", "type": "Internet Data", "text": " \nbase64.b32decode(s, casefold=False, map01=None)  \nDecode the Base32 encoded bytes-like object or ASCII string s and return the decoded bytes. Optional casefold is a flag specifying whether a lowercase alphabet is acceptable as input. For security purposes, the default is False. RFC 3548 allows for optional mapping of the digit 0 (zero) to the letter O (oh), and for optional mapping of the digit 1 (one) to either the letter I (eye) or letter L (el). The optional argument map01 when not None, specifies which letter the digit 1 should be mapped to (when map01 is not None, the digit 0 is always mapped to the letter O). For security purposes the default is None, so that 0 and 1 are not allowed in the input. A binascii.Error is raised if s is incorrectly padded or if there are non-alphabet characters present in the input. \n"}, {"name": "base64.b32encode()", "path": "library/base64#base64.b32encode", "type": "Internet Data", "text": " \nbase64.b32encode(s)  \nEncode the bytes-like object s using Base32 and return the encoded bytes. \n"}, {"name": "base64.b64decode()", "path": "library/base64#base64.b64decode", "type": "Internet Data", "text": " \nbase64.b64decode(s, altchars=None, validate=False)  \nDecode the Base64 encoded bytes-like object or ASCII string s and return the decoded bytes. Optional altchars must be a bytes-like object or ASCII string of at least length 2 (additional characters are ignored) which specifies the alternative alphabet used instead of the + and / characters. A binascii.Error exception is raised if s is incorrectly padded. If validate is False (the default), characters that are neither in the normal base-64 alphabet nor the alternative alphabet are discarded prior to the padding check. If validate is True, these non-alphabet characters in the input result in a binascii.Error. \n"}, {"name": "base64.b64encode()", "path": "library/base64#base64.b64encode", "type": "Internet Data", "text": " \nbase64.b64encode(s, altchars=None)  \nEncode the bytes-like object s using Base64 and return the encoded bytes. Optional altchars must be a bytes-like object of at least length 2 (additional characters are ignored) which specifies an alternative alphabet for the + and / characters. This allows an application to e.g. generate URL or filesystem safe Base64 strings. The default is None, for which the standard Base64 alphabet is used. \n"}, {"name": "base64.b85decode()", "path": "library/base64#base64.b85decode", "type": "Internet Data", "text": " \nbase64.b85decode(b)  \nDecode the base85-encoded bytes-like object or ASCII string b and return the decoded bytes. Padding is implicitly removed, if necessary.  New in version 3.4.  \n"}, {"name": "base64.b85encode()", "path": "library/base64#base64.b85encode", "type": "Internet Data", "text": " \nbase64.b85encode(b, pad=False)  \nEncode the bytes-like object b using base85 (as used in e.g. git-style binary diffs) and return the encoded bytes. If pad is true, the input is padded with b'\\0' so its length is a multiple of 4 bytes before encoding.  New in version 3.4.  \n"}, {"name": "base64.decode()", "path": "library/base64#base64.decode", "type": "Internet Data", "text": " \nbase64.decode(input, output)  \nDecode the contents of the binary input file and write the resulting binary data to the output file. input and output must be file objects. input will be read until input.readline() returns an empty bytes object. \n"}, {"name": "base64.decodebytes()", "path": "library/base64#base64.decodebytes", "type": "Internet Data", "text": " \nbase64.decodebytes(s)  \nDecode the bytes-like object s, which must contain one or more lines of base64 encoded data, and return the decoded bytes.  New in version 3.1.  \n"}, {"name": "base64.encode()", "path": "library/base64#base64.encode", "type": "Internet Data", "text": " \nbase64.encode(input, output)  \nEncode the contents of the binary input file and write the resulting base64 encoded data to the output file. input and output must be file objects. input will be read until input.read() returns an empty bytes object. encode() inserts a newline character (b'\\n') after every 76 bytes of the output, as well as ensuring that the output always ends with a newline, as per RFC 2045 (MIME). \n"}, {"name": "base64.encodebytes()", "path": "library/base64#base64.encodebytes", "type": "Internet Data", "text": " \nbase64.encodebytes(s)  \nEncode the bytes-like object s, which can contain arbitrary binary data, and return bytes containing the base64-encoded data, with newlines (b'\\n') inserted after every 76 bytes of output, and ensuring that there is a trailing newline, as per RFC 2045 (MIME).  New in version 3.1.  \n"}, {"name": "base64.standard_b64decode()", "path": "library/base64#base64.standard_b64decode", "type": "Internet Data", "text": " \nbase64.standard_b64decode(s)  \nDecode bytes-like object or ASCII string s using the standard Base64 alphabet and return the decoded bytes. \n"}, {"name": "base64.standard_b64encode()", "path": "library/base64#base64.standard_b64encode", "type": "Internet Data", "text": " \nbase64.standard_b64encode(s)  \nEncode bytes-like object s using the standard Base64 alphabet and return the encoded bytes. \n"}, {"name": "base64.urlsafe_b64decode()", "path": "library/base64#base64.urlsafe_b64decode", "type": "Internet Data", "text": " \nbase64.urlsafe_b64decode(s)  \nDecode bytes-like object or ASCII string s using the URL- and filesystem-safe alphabet, which substitutes - instead of + and _ instead of / in the standard Base64 alphabet, and return the decoded bytes. \n"}, {"name": "base64.urlsafe_b64encode()", "path": "library/base64#base64.urlsafe_b64encode", "type": "Internet Data", "text": " \nbase64.urlsafe_b64encode(s)  \nEncode bytes-like object s using the URL- and filesystem-safe alphabet, which substitutes - instead of + and _ instead of / in the standard Base64 alphabet, and return the encoded bytes. The result can still contain =. \n"}, {"name": "BaseException", "path": "library/exceptions#BaseException", "type": "Built-in Exceptions", "text": " \nexception BaseException  \nThe base class for all built-in exceptions. It is not meant to be directly inherited by user-defined classes (for that, use Exception). If str() is called on an instance of this class, the representation of the argument(s) to the instance are returned, or the empty string when there were no arguments.  \nargs  \nThe tuple of arguments given to the exception constructor. Some built-in exceptions (like OSError) expect a certain number of arguments and assign a special meaning to the elements of this tuple, while others are usually called only with a single string giving an error message. \n  \nwith_traceback(tb)  \nThis method sets tb as the new traceback for the exception and returns the exception object. It is usually used in exception handling code like this: try:\n    ...\nexcept SomeException:\n    tb = sys.exc_info()[2]\n    raise OtherException(...).with_traceback(tb)\n \n \n"}, {"name": "BaseException.args", "path": "library/exceptions#BaseException.args", "type": "Built-in Exceptions", "text": " \nargs  \nThe tuple of arguments given to the exception constructor. Some built-in exceptions (like OSError) expect a certain number of arguments and assign a special meaning to the elements of this tuple, while others are usually called only with a single string giving an error message. \n"}, {"name": "BaseException.with_traceback()", "path": "library/exceptions#BaseException.with_traceback", "type": "Built-in Exceptions", "text": " \nwith_traceback(tb)  \nThis method sets tb as the new traceback for the exception and returns the exception object. It is usually used in exception handling code like this: try:\n    ...\nexcept SomeException:\n    tb = sys.exc_info()[2]\n    raise OtherException(...).with_traceback(tb)\n \n"}, {"name": "bdb", "path": "library/bdb", "type": "Debugging & Profiling", "text": "bdb \u2014 Debugger framework Source code: Lib/bdb.py The bdb module handles basic debugger functions, like setting breakpoints or managing execution via the debugger. The following exception is defined:  \nexception bdb.BdbQuit  \nException raised by the Bdb class for quitting the debugger. \n The bdb module also defines two classes:  \nclass bdb.Breakpoint(self, file, line, temporary=0, cond=None, funcname=None)  \nThis class implements temporary breakpoints, ignore counts, disabling and (re-)enabling, and conditionals. Breakpoints are indexed by number through a list called bpbynumber and by (file, line) pairs through bplist. The former points to a single instance of class Breakpoint. The latter points to a list of such instances since there may be more than one breakpoint per line. When creating a breakpoint, its associated filename should be in canonical form. If a funcname is defined, a breakpoint hit will be counted when the first line of that function is executed. A conditional breakpoint always counts a hit. Breakpoint instances have the following methods:  \ndeleteMe()  \nDelete the breakpoint from the list associated to a file/line. If it is the last breakpoint in that position, it also deletes the entry for the file/line. \n  \nenable()  \nMark the breakpoint as enabled. \n  \ndisable()  \nMark the breakpoint as disabled. \n  \nbpformat()  \nReturn a string with all the information about the breakpoint, nicely formatted:  The breakpoint number. If it is temporary or not. Its file,line position. The condition that causes a break. If it must be ignored the next N times. The breakpoint hit count.   New in version 3.2.  \n  \nbpprint(out=None)  \nPrint the output of bpformat() to the file out, or if it is None, to standard output. \n \n  \nclass bdb.Bdb(skip=None)  \nThe Bdb class acts as a generic Python debugger base class. This class takes care of the details of the trace facility; a derived class should implement user interaction. The standard debugger class (pdb.Pdb) is an example. The skip argument, if given, must be an iterable of glob-style module name patterns. The debugger will not step into frames that originate in a module that matches one of these patterns. Whether a frame is considered to originate in a certain module is determined by the __name__ in the frame globals.  New in version 3.1: The skip argument.  The following methods of Bdb normally don\u2019t need to be overridden.  \ncanonic(filename)  \nAuxiliary method for getting a filename in a canonical form, that is, as a case-normalized (on case-insensitive filesystems) absolute path, stripped of surrounding angle brackets. \n  \nreset()  \nSet the botframe, stopframe, returnframe and quitting attributes with values ready to start debugging. \n  \ntrace_dispatch(frame, event, arg)  \nThis function is installed as the trace function of debugged frames. Its return value is the new trace function (in most cases, that is, itself). The default implementation decides how to dispatch a frame, depending on the type of event (passed as a string) that is about to be executed. event can be one of the following:  \n\"line\": A new line of code is going to be executed. \n\"call\": A function is about to be called, or another code block entered. \n\"return\": A function or other code block is about to return. \n\"exception\": An exception has occurred. \n\"c_call\": A C function is about to be called. \n\"c_return\": A C function has returned. \n\"c_exception\": A C function has raised an exception.  For the Python events, specialized functions (see below) are called. For the C events, no action is taken. The arg parameter depends on the previous event. See the documentation for sys.settrace() for more information on the trace function. For more information on code and frame objects, refer to The standard type hierarchy. \n  \ndispatch_line(frame)  \nIf the debugger should stop on the current line, invoke the user_line() method (which should be overridden in subclasses). Raise a BdbQuit exception if the Bdb.quitting flag is set (which can be set from user_line()). Return a reference to the trace_dispatch() method for further tracing in that scope. \n  \ndispatch_call(frame, arg)  \nIf the debugger should stop on this function call, invoke the user_call() method (which should be overridden in subclasses). Raise a BdbQuit exception if the Bdb.quitting flag is set (which can be set from user_call()). Return a reference to the trace_dispatch() method for further tracing in that scope. \n  \ndispatch_return(frame, arg)  \nIf the debugger should stop on this function return, invoke the user_return() method (which should be overridden in subclasses). Raise a BdbQuit exception if the Bdb.quitting flag is set (which can be set from user_return()). Return a reference to the trace_dispatch() method for further tracing in that scope. \n  \ndispatch_exception(frame, arg)  \nIf the debugger should stop at this exception, invokes the user_exception() method (which should be overridden in subclasses). Raise a BdbQuit exception if the Bdb.quitting flag is set (which can be set from user_exception()). Return a reference to the trace_dispatch() method for further tracing in that scope. \n Normally derived classes don\u2019t override the following methods, but they may if they want to redefine the definition of stopping and breakpoints.  \nstop_here(frame)  \nThis method checks if the frame is somewhere below botframe in the call stack. botframe is the frame in which debugging started. \n  \nbreak_here(frame)  \nThis method checks if there is a breakpoint in the filename and line belonging to frame or, at least, in the current function. If the breakpoint is a temporary one, this method deletes it. \n  \nbreak_anywhere(frame)  \nThis method checks if there is a breakpoint in the filename of the current frame. \n Derived classes should override these methods to gain control over debugger operation.  \nuser_call(frame, argument_list)  \nThis method is called from dispatch_call() when there is the possibility that a break might be necessary anywhere inside the called function. \n  \nuser_line(frame)  \nThis method is called from dispatch_line() when either stop_here() or break_here() yields True. \n  \nuser_return(frame, return_value)  \nThis method is called from dispatch_return() when stop_here() yields True. \n  \nuser_exception(frame, exc_info)  \nThis method is called from dispatch_exception() when stop_here() yields True. \n  \ndo_clear(arg)  \nHandle how a breakpoint must be removed when it is a temporary one. This method must be implemented by derived classes. \n Derived classes and clients can call the following methods to affect the stepping state.  \nset_step()  \nStop after one line of code. \n  \nset_next(frame)  \nStop on the next line in or below the given frame. \n  \nset_return(frame)  \nStop when returning from the given frame. \n  \nset_until(frame)  \nStop when the line with the line no greater than the current one is reached or when returning from current frame. \n  \nset_trace([frame])  \nStart debugging from frame. If frame is not specified, debugging starts from caller\u2019s frame. \n  \nset_continue()  \nStop only at breakpoints or when finished. If there are no breakpoints, set the system trace function to None. \n  \nset_quit()  \nSet the quitting attribute to True. This raises BdbQuit in the next call to one of the dispatch_*() methods. \n Derived classes and clients can call the following methods to manipulate breakpoints. These methods return a string containing an error message if something went wrong, or None if all is well.  \nset_break(filename, lineno, temporary=0, cond, funcname)  \nSet a new breakpoint. If the lineno line doesn\u2019t exist for the filename passed as argument, return an error message. The filename should be in canonical form, as described in the canonic() method. \n  \nclear_break(filename, lineno)  \nDelete the breakpoints in filename and lineno. If none were set, an error message is returned. \n  \nclear_bpbynumber(arg)  \nDelete the breakpoint which has the index arg in the Breakpoint.bpbynumber. If arg is not numeric or out of range, return an error message. \n  \nclear_all_file_breaks(filename)  \nDelete all breakpoints in filename. If none were set, an error message is returned. \n  \nclear_all_breaks()  \nDelete all existing breakpoints. \n  \nget_bpbynumber(arg)  \nReturn a breakpoint specified by the given number. If arg is a string, it will be converted to a number. If arg is a non-numeric string, if the given breakpoint never existed or has been deleted, a ValueError is raised.  New in version 3.2.  \n  \nget_break(filename, lineno)  \nCheck if there is a breakpoint for lineno of filename. \n  \nget_breaks(filename, lineno)  \nReturn all breakpoints for lineno in filename, or an empty list if none are set. \n  \nget_file_breaks(filename)  \nReturn all breakpoints in filename, or an empty list if none are set. \n  \nget_all_breaks()  \nReturn all breakpoints that are set. \n Derived classes and clients can call the following methods to get a data structure representing a stack trace.  \nget_stack(f, t)  \nGet a list of records for a frame and all higher (calling) and lower frames, and the size of the higher part. \n  \nformat_stack_entry(frame_lineno, lprefix=': ')  \nReturn a string with information about a stack entry, identified by a (frame, lineno) tuple:  The canonical form of the filename which contains the frame. The function name, or \"<lambda>\". The input arguments. The return value. The line of code (if it exists).  \n The following two methods can be called by clients to use a debugger to debug a statement, given as a string.  \nrun(cmd, globals=None, locals=None)  \nDebug a statement executed via the exec() function. globals defaults to __main__.__dict__, locals defaults to globals. \n  \nruneval(expr, globals=None, locals=None)  \nDebug an expression executed via the eval() function. globals and locals have the same meaning as in run(). \n  \nrunctx(cmd, globals, locals)  \nFor backwards compatibility. Calls the run() method. \n  \nruncall(func, /, *args, **kwds)  \nDebug a single function call, and return its result. \n \n Finally, the module defines the following functions:  \nbdb.checkfuncname(b, frame)  \nCheck whether we should break here, depending on the way the breakpoint b was set. If it was set via line number, it checks if b.line is the same as the one in the frame also passed as argument. If the breakpoint was set via function name, we have to check we are in the right frame (the right function) and if we are in its first executable line. \n  \nbdb.effective(file, line, frame)  \nDetermine if there is an effective (active) breakpoint at this line of code. Return a tuple of the breakpoint and a boolean that indicates if it is ok to delete a temporary breakpoint. Return (None, None) if there is no matching breakpoint. \n  \nbdb.set_trace()  \nStart debugging with a Bdb instance from caller\u2019s frame. \n\n"}, {"name": "bdb.Bdb", "path": "library/bdb#bdb.Bdb", "type": "Debugging & Profiling", "text": " \nclass bdb.Bdb(skip=None)  \nThe Bdb class acts as a generic Python debugger base class. This class takes care of the details of the trace facility; a derived class should implement user interaction. The standard debugger class (pdb.Pdb) is an example. The skip argument, if given, must be an iterable of glob-style module name patterns. The debugger will not step into frames that originate in a module that matches one of these patterns. Whether a frame is considered to originate in a certain module is determined by the __name__ in the frame globals.  New in version 3.1: The skip argument.  The following methods of Bdb normally don\u2019t need to be overridden.  \ncanonic(filename)  \nAuxiliary method for getting a filename in a canonical form, that is, as a case-normalized (on case-insensitive filesystems) absolute path, stripped of surrounding angle brackets. \n  \nreset()  \nSet the botframe, stopframe, returnframe and quitting attributes with values ready to start debugging. \n  \ntrace_dispatch(frame, event, arg)  \nThis function is installed as the trace function of debugged frames. Its return value is the new trace function (in most cases, that is, itself). The default implementation decides how to dispatch a frame, depending on the type of event (passed as a string) that is about to be executed. event can be one of the following:  \n\"line\": A new line of code is going to be executed. \n\"call\": A function is about to be called, or another code block entered. \n\"return\": A function or other code block is about to return. \n\"exception\": An exception has occurred. \n\"c_call\": A C function is about to be called. \n\"c_return\": A C function has returned. \n\"c_exception\": A C function has raised an exception.  For the Python events, specialized functions (see below) are called. For the C events, no action is taken. The arg parameter depends on the previous event. See the documentation for sys.settrace() for more information on the trace function. For more information on code and frame objects, refer to The standard type hierarchy. \n  \ndispatch_line(frame)  \nIf the debugger should stop on the current line, invoke the user_line() method (which should be overridden in subclasses). Raise a BdbQuit exception if the Bdb.quitting flag is set (which can be set from user_line()). Return a reference to the trace_dispatch() method for further tracing in that scope. \n  \ndispatch_call(frame, arg)  \nIf the debugger should stop on this function call, invoke the user_call() method (which should be overridden in subclasses). Raise a BdbQuit exception if the Bdb.quitting flag is set (which can be set from user_call()). Return a reference to the trace_dispatch() method for further tracing in that scope. \n  \ndispatch_return(frame, arg)  \nIf the debugger should stop on this function return, invoke the user_return() method (which should be overridden in subclasses). Raise a BdbQuit exception if the Bdb.quitting flag is set (which can be set from user_return()). Return a reference to the trace_dispatch() method for further tracing in that scope. \n  \ndispatch_exception(frame, arg)  \nIf the debugger should stop at this exception, invokes the user_exception() method (which should be overridden in subclasses). Raise a BdbQuit exception if the Bdb.quitting flag is set (which can be set from user_exception()). Return a reference to the trace_dispatch() method for further tracing in that scope. \n Normally derived classes don\u2019t override the following methods, but they may if they want to redefine the definition of stopping and breakpoints.  \nstop_here(frame)  \nThis method checks if the frame is somewhere below botframe in the call stack. botframe is the frame in which debugging started. \n  \nbreak_here(frame)  \nThis method checks if there is a breakpoint in the filename and line belonging to frame or, at least, in the current function. If the breakpoint is a temporary one, this method deletes it. \n  \nbreak_anywhere(frame)  \nThis method checks if there is a breakpoint in the filename of the current frame. \n Derived classes should override these methods to gain control over debugger operation.  \nuser_call(frame, argument_list)  \nThis method is called from dispatch_call() when there is the possibility that a break might be necessary anywhere inside the called function. \n  \nuser_line(frame)  \nThis method is called from dispatch_line() when either stop_here() or break_here() yields True. \n  \nuser_return(frame, return_value)  \nThis method is called from dispatch_return() when stop_here() yields True. \n  \nuser_exception(frame, exc_info)  \nThis method is called from dispatch_exception() when stop_here() yields True. \n  \ndo_clear(arg)  \nHandle how a breakpoint must be removed when it is a temporary one. This method must be implemented by derived classes. \n Derived classes and clients can call the following methods to affect the stepping state.  \nset_step()  \nStop after one line of code. \n  \nset_next(frame)  \nStop on the next line in or below the given frame. \n  \nset_return(frame)  \nStop when returning from the given frame. \n  \nset_until(frame)  \nStop when the line with the line no greater than the current one is reached or when returning from current frame. \n  \nset_trace([frame])  \nStart debugging from frame. If frame is not specified, debugging starts from caller\u2019s frame. \n  \nset_continue()  \nStop only at breakpoints or when finished. If there are no breakpoints, set the system trace function to None. \n  \nset_quit()  \nSet the quitting attribute to True. This raises BdbQuit in the next call to one of the dispatch_*() methods. \n Derived classes and clients can call the following methods to manipulate breakpoints. These methods return a string containing an error message if something went wrong, or None if all is well.  \nset_break(filename, lineno, temporary=0, cond, funcname)  \nSet a new breakpoint. If the lineno line doesn\u2019t exist for the filename passed as argument, return an error message. The filename should be in canonical form, as described in the canonic() method. \n  \nclear_break(filename, lineno)  \nDelete the breakpoints in filename and lineno. If none were set, an error message is returned. \n  \nclear_bpbynumber(arg)  \nDelete the breakpoint which has the index arg in the Breakpoint.bpbynumber. If arg is not numeric or out of range, return an error message. \n  \nclear_all_file_breaks(filename)  \nDelete all breakpoints in filename. If none were set, an error message is returned. \n  \nclear_all_breaks()  \nDelete all existing breakpoints. \n  \nget_bpbynumber(arg)  \nReturn a breakpoint specified by the given number. If arg is a string, it will be converted to a number. If arg is a non-numeric string, if the given breakpoint never existed or has been deleted, a ValueError is raised.  New in version 3.2.  \n  \nget_break(filename, lineno)  \nCheck if there is a breakpoint for lineno of filename. \n  \nget_breaks(filename, lineno)  \nReturn all breakpoints for lineno in filename, or an empty list if none are set. \n  \nget_file_breaks(filename)  \nReturn all breakpoints in filename, or an empty list if none are set. \n  \nget_all_breaks()  \nReturn all breakpoints that are set. \n Derived classes and clients can call the following methods to get a data structure representing a stack trace.  \nget_stack(f, t)  \nGet a list of records for a frame and all higher (calling) and lower frames, and the size of the higher part. \n  \nformat_stack_entry(frame_lineno, lprefix=': ')  \nReturn a string with information about a stack entry, identified by a (frame, lineno) tuple:  The canonical form of the filename which contains the frame. The function name, or \"<lambda>\". The input arguments. The return value. The line of code (if it exists).  \n The following two methods can be called by clients to use a debugger to debug a statement, given as a string.  \nrun(cmd, globals=None, locals=None)  \nDebug a statement executed via the exec() function. globals defaults to __main__.__dict__, locals defaults to globals. \n  \nruneval(expr, globals=None, locals=None)  \nDebug an expression executed via the eval() function. globals and locals have the same meaning as in run(). \n  \nrunctx(cmd, globals, locals)  \nFor backwards compatibility. Calls the run() method. \n  \nruncall(func, /, *args, **kwds)  \nDebug a single function call, and return its result. \n \n"}, {"name": "bdb.Bdb.break_anywhere()", "path": "library/bdb#bdb.Bdb.break_anywhere", "type": "Debugging & Profiling", "text": " \nbreak_anywhere(frame)  \nThis method checks if there is a breakpoint in the filename of the current frame. \n"}, {"name": "bdb.Bdb.break_here()", "path": "library/bdb#bdb.Bdb.break_here", "type": "Debugging & Profiling", "text": " \nbreak_here(frame)  \nThis method checks if there is a breakpoint in the filename and line belonging to frame or, at least, in the current function. If the breakpoint is a temporary one, this method deletes it. \n"}, {"name": "bdb.Bdb.canonic()", "path": "library/bdb#bdb.Bdb.canonic", "type": "Debugging & Profiling", "text": " \ncanonic(filename)  \nAuxiliary method for getting a filename in a canonical form, that is, as a case-normalized (on case-insensitive filesystems) absolute path, stripped of surrounding angle brackets. \n"}, {"name": "bdb.Bdb.clear_all_breaks()", "path": "library/bdb#bdb.Bdb.clear_all_breaks", "type": "Debugging & Profiling", "text": " \nclear_all_breaks()  \nDelete all existing breakpoints. \n"}, {"name": "bdb.Bdb.clear_all_file_breaks()", "path": "library/bdb#bdb.Bdb.clear_all_file_breaks", "type": "Debugging & Profiling", "text": " \nclear_all_file_breaks(filename)  \nDelete all breakpoints in filename. If none were set, an error message is returned. \n"}, {"name": "bdb.Bdb.clear_bpbynumber()", "path": "library/bdb#bdb.Bdb.clear_bpbynumber", "type": "Debugging & Profiling", "text": " \nclear_bpbynumber(arg)  \nDelete the breakpoint which has the index arg in the Breakpoint.bpbynumber. If arg is not numeric or out of range, return an error message. \n"}, {"name": "bdb.Bdb.clear_break()", "path": "library/bdb#bdb.Bdb.clear_break", "type": "Debugging & Profiling", "text": " \nclear_break(filename, lineno)  \nDelete the breakpoints in filename and lineno. If none were set, an error message is returned. \n"}, {"name": "bdb.Bdb.dispatch_call()", "path": "library/bdb#bdb.Bdb.dispatch_call", "type": "Debugging & Profiling", "text": " \ndispatch_call(frame, arg)  \nIf the debugger should stop on this function call, invoke the user_call() method (which should be overridden in subclasses). Raise a BdbQuit exception if the Bdb.quitting flag is set (which can be set from user_call()). Return a reference to the trace_dispatch() method for further tracing in that scope. \n"}, {"name": "bdb.Bdb.dispatch_exception()", "path": "library/bdb#bdb.Bdb.dispatch_exception", "type": "Debugging & Profiling", "text": " \ndispatch_exception(frame, arg)  \nIf the debugger should stop at this exception, invokes the user_exception() method (which should be overridden in subclasses). Raise a BdbQuit exception if the Bdb.quitting flag is set (which can be set from user_exception()). Return a reference to the trace_dispatch() method for further tracing in that scope. \n"}, {"name": "bdb.Bdb.dispatch_line()", "path": "library/bdb#bdb.Bdb.dispatch_line", "type": "Debugging & Profiling", "text": " \ndispatch_line(frame)  \nIf the debugger should stop on the current line, invoke the user_line() method (which should be overridden in subclasses). Raise a BdbQuit exception if the Bdb.quitting flag is set (which can be set from user_line()). Return a reference to the trace_dispatch() method for further tracing in that scope. \n"}, {"name": "bdb.Bdb.dispatch_return()", "path": "library/bdb#bdb.Bdb.dispatch_return", "type": "Debugging & Profiling", "text": " \ndispatch_return(frame, arg)  \nIf the debugger should stop on this function return, invoke the user_return() method (which should be overridden in subclasses). Raise a BdbQuit exception if the Bdb.quitting flag is set (which can be set from user_return()). Return a reference to the trace_dispatch() method for further tracing in that scope. \n"}, {"name": "bdb.Bdb.do_clear()", "path": "library/bdb#bdb.Bdb.do_clear", "type": "Debugging & Profiling", "text": " \ndo_clear(arg)  \nHandle how a breakpoint must be removed when it is a temporary one. This method must be implemented by derived classes. \n"}, {"name": "bdb.Bdb.format_stack_entry()", "path": "library/bdb#bdb.Bdb.format_stack_entry", "type": "Debugging & Profiling", "text": " \nformat_stack_entry(frame_lineno, lprefix=': ')  \nReturn a string with information about a stack entry, identified by a (frame, lineno) tuple:  The canonical form of the filename which contains the frame. The function name, or \"<lambda>\". The input arguments. The return value. The line of code (if it exists).  \n"}, {"name": "bdb.Bdb.get_all_breaks()", "path": "library/bdb#bdb.Bdb.get_all_breaks", "type": "Debugging & Profiling", "text": " \nget_all_breaks()  \nReturn all breakpoints that are set. \n"}, {"name": "bdb.Bdb.get_bpbynumber()", "path": "library/bdb#bdb.Bdb.get_bpbynumber", "type": "Debugging & Profiling", "text": " \nget_bpbynumber(arg)  \nReturn a breakpoint specified by the given number. If arg is a string, it will be converted to a number. If arg is a non-numeric string, if the given breakpoint never existed or has been deleted, a ValueError is raised.  New in version 3.2.  \n"}, {"name": "bdb.Bdb.get_break()", "path": "library/bdb#bdb.Bdb.get_break", "type": "Debugging & Profiling", "text": " \nget_break(filename, lineno)  \nCheck if there is a breakpoint for lineno of filename. \n"}, {"name": "bdb.Bdb.get_breaks()", "path": "library/bdb#bdb.Bdb.get_breaks", "type": "Debugging & Profiling", "text": " \nget_breaks(filename, lineno)  \nReturn all breakpoints for lineno in filename, or an empty list if none are set. \n"}, {"name": "bdb.Bdb.get_file_breaks()", "path": "library/bdb#bdb.Bdb.get_file_breaks", "type": "Debugging & Profiling", "text": " \nget_file_breaks(filename)  \nReturn all breakpoints in filename, or an empty list if none are set. \n"}, {"name": "bdb.Bdb.get_stack()", "path": "library/bdb#bdb.Bdb.get_stack", "type": "Debugging & Profiling", "text": " \nget_stack(f, t)  \nGet a list of records for a frame and all higher (calling) and lower frames, and the size of the higher part. \n"}, {"name": "bdb.Bdb.reset()", "path": "library/bdb#bdb.Bdb.reset", "type": "Debugging & Profiling", "text": " \nreset()  \nSet the botframe, stopframe, returnframe and quitting attributes with values ready to start debugging. \n"}, {"name": "bdb.Bdb.run()", "path": "library/bdb#bdb.Bdb.run", "type": "Debugging & Profiling", "text": " \nrun(cmd, globals=None, locals=None)  \nDebug a statement executed via the exec() function. globals defaults to __main__.__dict__, locals defaults to globals. \n"}, {"name": "bdb.Bdb.runcall()", "path": "library/bdb#bdb.Bdb.runcall", "type": "Debugging & Profiling", "text": " \nruncall(func, /, *args, **kwds)  \nDebug a single function call, and return its result. \n"}, {"name": "bdb.Bdb.runctx()", "path": "library/bdb#bdb.Bdb.runctx", "type": "Debugging & Profiling", "text": " \nrunctx(cmd, globals, locals)  \nFor backwards compatibility. Calls the run() method. \n"}, {"name": "bdb.Bdb.runeval()", "path": "library/bdb#bdb.Bdb.runeval", "type": "Debugging & Profiling", "text": " \nruneval(expr, globals=None, locals=None)  \nDebug an expression executed via the eval() function. globals and locals have the same meaning as in run(). \n"}, {"name": "bdb.Bdb.set_break()", "path": "library/bdb#bdb.Bdb.set_break", "type": "Debugging & Profiling", "text": " \nset_break(filename, lineno, temporary=0, cond, funcname)  \nSet a new breakpoint. If the lineno line doesn\u2019t exist for the filename passed as argument, return an error message. The filename should be in canonical form, as described in the canonic() method. \n"}, {"name": "bdb.Bdb.set_continue()", "path": "library/bdb#bdb.Bdb.set_continue", "type": "Debugging & Profiling", "text": " \nset_continue()  \nStop only at breakpoints or when finished. If there are no breakpoints, set the system trace function to None. \n"}, {"name": "bdb.Bdb.set_next()", "path": "library/bdb#bdb.Bdb.set_next", "type": "Debugging & Profiling", "text": " \nset_next(frame)  \nStop on the next line in or below the given frame. \n"}, {"name": "bdb.Bdb.set_quit()", "path": "library/bdb#bdb.Bdb.set_quit", "type": "Debugging & Profiling", "text": " \nset_quit()  \nSet the quitting attribute to True. This raises BdbQuit in the next call to one of the dispatch_*() methods. \n"}, {"name": "bdb.Bdb.set_return()", "path": "library/bdb#bdb.Bdb.set_return", "type": "Debugging & Profiling", "text": " \nset_return(frame)  \nStop when returning from the given frame. \n"}, {"name": "bdb.Bdb.set_step()", "path": "library/bdb#bdb.Bdb.set_step", "type": "Debugging & Profiling", "text": " \nset_step()  \nStop after one line of code. \n"}, {"name": "bdb.Bdb.set_trace()", "path": "library/bdb#bdb.Bdb.set_trace", "type": "Debugging & Profiling", "text": " \nset_trace([frame])  \nStart debugging from frame. If frame is not specified, debugging starts from caller\u2019s frame. \n"}, {"name": "bdb.Bdb.set_until()", "path": "library/bdb#bdb.Bdb.set_until", "type": "Debugging & Profiling", "text": " \nset_until(frame)  \nStop when the line with the line no greater than the current one is reached or when returning from current frame. \n"}, {"name": "bdb.Bdb.stop_here()", "path": "library/bdb#bdb.Bdb.stop_here", "type": "Debugging & Profiling", "text": " \nstop_here(frame)  \nThis method checks if the frame is somewhere below botframe in the call stack. botframe is the frame in which debugging started. \n"}, {"name": "bdb.Bdb.trace_dispatch()", "path": "library/bdb#bdb.Bdb.trace_dispatch", "type": "Debugging & Profiling", "text": " \ntrace_dispatch(frame, event, arg)  \nThis function is installed as the trace function of debugged frames. Its return value is the new trace function (in most cases, that is, itself). The default implementation decides how to dispatch a frame, depending on the type of event (passed as a string) that is about to be executed. event can be one of the following:  \n\"line\": A new line of code is going to be executed. \n\"call\": A function is about to be called, or another code block entered. \n\"return\": A function or other code block is about to return. \n\"exception\": An exception has occurred. \n\"c_call\": A C function is about to be called. \n\"c_return\": A C function has returned. \n\"c_exception\": A C function has raised an exception.  For the Python events, specialized functions (see below) are called. For the C events, no action is taken. The arg parameter depends on the previous event. See the documentation for sys.settrace() for more information on the trace function. For more information on code and frame objects, refer to The standard type hierarchy. \n"}, {"name": "bdb.Bdb.user_call()", "path": "library/bdb#bdb.Bdb.user_call", "type": "Debugging & Profiling", "text": " \nuser_call(frame, argument_list)  \nThis method is called from dispatch_call() when there is the possibility that a break might be necessary anywhere inside the called function. \n"}, {"name": "bdb.Bdb.user_exception()", "path": "library/bdb#bdb.Bdb.user_exception", "type": "Debugging & Profiling", "text": " \nuser_exception(frame, exc_info)  \nThis method is called from dispatch_exception() when stop_here() yields True. \n"}, {"name": "bdb.Bdb.user_line()", "path": "library/bdb#bdb.Bdb.user_line", "type": "Debugging & Profiling", "text": " \nuser_line(frame)  \nThis method is called from dispatch_line() when either stop_here() or break_here() yields True. \n"}, {"name": "bdb.Bdb.user_return()", "path": "library/bdb#bdb.Bdb.user_return", "type": "Debugging & Profiling", "text": " \nuser_return(frame, return_value)  \nThis method is called from dispatch_return() when stop_here() yields True. \n"}, {"name": "bdb.BdbQuit", "path": "library/bdb#bdb.BdbQuit", "type": "Debugging & Profiling", "text": " \nexception bdb.BdbQuit  \nException raised by the Bdb class for quitting the debugger. \n"}, {"name": "bdb.Breakpoint", "path": "library/bdb#bdb.Breakpoint", "type": "Debugging & Profiling", "text": " \nclass bdb.Breakpoint(self, file, line, temporary=0, cond=None, funcname=None)  \nThis class implements temporary breakpoints, ignore counts, disabling and (re-)enabling, and conditionals. Breakpoints are indexed by number through a list called bpbynumber and by (file, line) pairs through bplist. The former points to a single instance of class Breakpoint. The latter points to a list of such instances since there may be more than one breakpoint per line. When creating a breakpoint, its associated filename should be in canonical form. If a funcname is defined, a breakpoint hit will be counted when the first line of that function is executed. A conditional breakpoint always counts a hit. Breakpoint instances have the following methods:  \ndeleteMe()  \nDelete the breakpoint from the list associated to a file/line. If it is the last breakpoint in that position, it also deletes the entry for the file/line. \n  \nenable()  \nMark the breakpoint as enabled. \n  \ndisable()  \nMark the breakpoint as disabled. \n  \nbpformat()  \nReturn a string with all the information about the breakpoint, nicely formatted:  The breakpoint number. If it is temporary or not. Its file,line position. The condition that causes a break. If it must be ignored the next N times. The breakpoint hit count.   New in version 3.2.  \n  \nbpprint(out=None)  \nPrint the output of bpformat() to the file out, or if it is None, to standard output. \n \n"}, {"name": "bdb.Breakpoint.bpformat()", "path": "library/bdb#bdb.Breakpoint.bpformat", "type": "Debugging & Profiling", "text": " \nbpformat()  \nReturn a string with all the information about the breakpoint, nicely formatted:  The breakpoint number. If it is temporary or not. Its file,line position. The condition that causes a break. If it must be ignored the next N times. The breakpoint hit count.   New in version 3.2.  \n"}, {"name": "bdb.Breakpoint.bpprint()", "path": "library/bdb#bdb.Breakpoint.bpprint", "type": "Debugging & Profiling", "text": " \nbpprint(out=None)  \nPrint the output of bpformat() to the file out, or if it is None, to standard output. \n"}, {"name": "bdb.Breakpoint.deleteMe()", "path": "library/bdb#bdb.Breakpoint.deleteMe", "type": "Debugging & Profiling", "text": " \ndeleteMe()  \nDelete the breakpoint from the list associated to a file/line. If it is the last breakpoint in that position, it also deletes the entry for the file/line. \n"}, {"name": "bdb.Breakpoint.disable()", "path": "library/bdb#bdb.Breakpoint.disable", "type": "Debugging & Profiling", "text": " \ndisable()  \nMark the breakpoint as disabled. \n"}, {"name": "bdb.Breakpoint.enable()", "path": "library/bdb#bdb.Breakpoint.enable", "type": "Debugging & Profiling", "text": " \nenable()  \nMark the breakpoint as enabled. \n"}, {"name": "bdb.checkfuncname()", "path": "library/bdb#bdb.checkfuncname", "type": "Debugging & Profiling", "text": " \nbdb.checkfuncname(b, frame)  \nCheck whether we should break here, depending on the way the breakpoint b was set. If it was set via line number, it checks if b.line is the same as the one in the frame also passed as argument. If the breakpoint was set via function name, we have to check we are in the right frame (the right function) and if we are in its first executable line. \n"}, {"name": "bdb.effective()", "path": "library/bdb#bdb.effective", "type": "Debugging & Profiling", "text": " \nbdb.effective(file, line, frame)  \nDetermine if there is an effective (active) breakpoint at this line of code. Return a tuple of the breakpoint and a boolean that indicates if it is ok to delete a temporary breakpoint. Return (None, None) if there is no matching breakpoint. \n"}, {"name": "bdb.set_trace()", "path": "library/bdb#bdb.set_trace", "type": "Debugging & Profiling", "text": " \nbdb.set_trace()  \nStart debugging with a Bdb instance from caller\u2019s frame. \n"}, {"name": "bin()", "path": "library/functions#bin", "type": "Built-in Functions", "text": " \nbin(x)  \nConvert an integer number to a binary string prefixed with \u201c0b\u201d. The result is a valid Python expression. If x is not a Python int object, it has to define an __index__() method that returns an integer. Some examples: >>> bin(3)\n'0b11'\n>>> bin(-10)\n'-0b1010'\n If prefix \u201c0b\u201d is desired or not, you can use either of the following ways. >>> format(14, '#b'), format(14, 'b')\n('0b1110', '1110')\n>>> f'{14:#b}', f'{14:b}'\n('0b1110', '1110')\n See also format() for more information. \n"}, {"name": "binascii", "path": "library/binascii", "type": "Internet Data", "text": "binascii \u2014 Convert between binary and ASCII The binascii module contains a number of methods to convert between binary and various ASCII-encoded binary representations. Normally, you will not use these functions directly but use wrapper modules like uu, base64, or binhex instead. The binascii module contains low-level functions written in C for greater speed that are used by the higher-level modules.  Note a2b_* functions accept Unicode strings containing only ASCII characters. Other functions only accept bytes-like objects (such as bytes, bytearray and other objects that support the buffer protocol).  Changed in version 3.3: ASCII-only unicode strings are now accepted by the a2b_* functions.   The binascii module defines the following functions:  \nbinascii.a2b_uu(string)  \nConvert a single line of uuencoded data back to binary and return the binary data. Lines normally contain 45 (binary) bytes, except for the last line. Line data may be followed by whitespace. \n  \nbinascii.b2a_uu(data, *, backtick=False)  \nConvert binary data to a line of ASCII characters, the return value is the converted line, including a newline char. The length of data should be at most 45. If backtick is true, zeros are represented by '`' instead of spaces.  Changed in version 3.7: Added the backtick parameter.  \n  \nbinascii.a2b_base64(string)  \nConvert a block of base64 data back to binary and return the binary data. More than one line may be passed at a time. \n  \nbinascii.b2a_base64(data, *, newline=True)  \nConvert binary data to a line of ASCII characters in base64 coding. The return value is the converted line, including a newline char if newline is true. The output of this function conforms to RFC 3548.  Changed in version 3.6: Added the newline parameter.  \n  \nbinascii.a2b_qp(data, header=False)  \nConvert a block of quoted-printable data back to binary and return the binary data. More than one line may be passed at a time. If the optional argument header is present and true, underscores will be decoded as spaces. \n  \nbinascii.b2a_qp(data, quotetabs=False, istext=True, header=False)  \nConvert binary data to a line(s) of ASCII characters in quoted-printable encoding. The return value is the converted line(s). If the optional argument quotetabs is present and true, all tabs and spaces will be encoded. If the optional argument istext is present and true, newlines are not encoded but trailing whitespace will be encoded. If the optional argument header is present and true, spaces will be encoded as underscores per RFC 1522. If the optional argument header is present and false, newline characters will be encoded as well; otherwise linefeed conversion might corrupt the binary data stream. \n  \nbinascii.a2b_hqx(string)  \nConvert binhex4 formatted ASCII data to binary, without doing RLE-decompression. The string should contain a complete number of binary bytes, or (in case of the last portion of the binhex4 data) have the remaining bits zero.  Deprecated since version 3.9.  \n  \nbinascii.rledecode_hqx(data)  \nPerform RLE-decompression on the data, as per the binhex4 standard. The algorithm uses 0x90 after a byte as a repeat indicator, followed by a count. A count of 0 specifies a byte value of 0x90. The routine returns the decompressed data, unless data input data ends in an orphaned repeat indicator, in which case the Incomplete exception is raised.  Changed in version 3.2: Accept only bytestring or bytearray objects as input.   Deprecated since version 3.9.  \n  \nbinascii.rlecode_hqx(data)  \nPerform binhex4 style RLE-compression on data and return the result.  Deprecated since version 3.9.  \n  \nbinascii.b2a_hqx(data)  \nPerform hexbin4 binary-to-ASCII translation and return the resulting string. The argument should already be RLE-coded, and have a length divisible by 3 (except possibly the last fragment).  Deprecated since version 3.9.  \n  \nbinascii.crc_hqx(data, value)  \nCompute a 16-bit CRC value of data, starting with value as the initial CRC, and return the result. This uses the CRC-CCITT polynomial x16 + x12 + x5 + 1, often represented as 0x1021. This CRC is used in the binhex4 format. \n  \nbinascii.crc32(data[, value])  \nCompute CRC-32, the 32-bit checksum of data, starting with an initial CRC of value. The default initial CRC is zero. The algorithm is consistent with the ZIP file checksum. Since the algorithm is designed for use as a checksum algorithm, it is not suitable for use as a general hash algorithm. Use as follows: print(binascii.crc32(b\"hello world\"))\n# Or, in two pieces:\ncrc = binascii.crc32(b\"hello\")\ncrc = binascii.crc32(b\" world\", crc)\nprint('crc32 = {:#010x}'.format(crc))\n  Changed in version 3.0: The result is always unsigned. To generate the same numeric value across all Python versions and platforms, use crc32(data) & 0xffffffff.  \n  \nbinascii.b2a_hex(data[, sep[, bytes_per_sep=1]])  \nbinascii.hexlify(data[, sep[, bytes_per_sep=1]])  \nReturn the hexadecimal representation of the binary data. Every byte of data is converted into the corresponding 2-digit hex representation. The returned bytes object is therefore twice as long as the length of data. Similar functionality (but returning a text string) is also conveniently accessible using the bytes.hex() method. If sep is specified, it must be a single character str or bytes object. It will be inserted in the output after every bytes_per_sep input bytes. Separator placement is counted from the right end of the output by default, if you wish to count from the left, supply a negative bytes_per_sep value. >>> import binascii\n>>> binascii.b2a_hex(b'\\xb9\\x01\\xef')\nb'b901ef'\n>>> binascii.hexlify(b'\\xb9\\x01\\xef', '-')\nb'b9-01-ef'\n>>> binascii.b2a_hex(b'\\xb9\\x01\\xef', b'_', 2)\nb'b9_01ef'\n>>> binascii.b2a_hex(b'\\xb9\\x01\\xef', b' ', -2)\nb'b901 ef'\n  Changed in version 3.8: The sep and bytes_per_sep parameters were added.  \n  \nbinascii.a2b_hex(hexstr)  \nbinascii.unhexlify(hexstr)  \nReturn the binary data represented by the hexadecimal string hexstr. This function is the inverse of b2a_hex(). hexstr must contain an even number of hexadecimal digits (which can be upper or lower case), otherwise an Error exception is raised. Similar functionality (accepting only text string arguments, but more liberal towards whitespace) is also accessible using the bytes.fromhex() class method. \n  \nexception binascii.Error  \nException raised on errors. These are usually programming errors. \n  \nexception binascii.Incomplete  \nException raised on incomplete data. These are usually not programming errors, but may be handled by reading a little more data and trying again. \n  See also  \nModule base64\n\n\nSupport for RFC compliant base64-style encoding in base 16, 32, 64, and 85.  \nModule binhex\n\n\nSupport for the binhex format used on the Macintosh.  \nModule uu\n\n\nSupport for UU encoding used on Unix.  \nModule quopri\n\n\nSupport for quoted-printable encoding used in MIME email messages.   \n"}, {"name": "binascii.a2b_base64()", "path": "library/binascii#binascii.a2b_base64", "type": "Internet Data", "text": " \nbinascii.a2b_base64(string)  \nConvert a block of base64 data back to binary and return the binary data. More than one line may be passed at a time. \n"}, {"name": "binascii.a2b_hex()", "path": "library/binascii#binascii.a2b_hex", "type": "Internet Data", "text": " \nbinascii.a2b_hex(hexstr)  \nbinascii.unhexlify(hexstr)  \nReturn the binary data represented by the hexadecimal string hexstr. This function is the inverse of b2a_hex(). hexstr must contain an even number of hexadecimal digits (which can be upper or lower case), otherwise an Error exception is raised. Similar functionality (accepting only text string arguments, but more liberal towards whitespace) is also accessible using the bytes.fromhex() class method. \n"}, {"name": "binascii.a2b_hqx()", "path": "library/binascii#binascii.a2b_hqx", "type": "Internet Data", "text": " \nbinascii.a2b_hqx(string)  \nConvert binhex4 formatted ASCII data to binary, without doing RLE-decompression. The string should contain a complete number of binary bytes, or (in case of the last portion of the binhex4 data) have the remaining bits zero.  Deprecated since version 3.9.  \n"}, {"name": "binascii.a2b_qp()", "path": "library/binascii#binascii.a2b_qp", "type": "Internet Data", "text": " \nbinascii.a2b_qp(data, header=False)  \nConvert a block of quoted-printable data back to binary and return the binary data. More than one line may be passed at a time. If the optional argument header is present and true, underscores will be decoded as spaces. \n"}, {"name": "binascii.a2b_uu()", "path": "library/binascii#binascii.a2b_uu", "type": "Internet Data", "text": " \nbinascii.a2b_uu(string)  \nConvert a single line of uuencoded data back to binary and return the binary data. Lines normally contain 45 (binary) bytes, except for the last line. Line data may be followed by whitespace. \n"}, {"name": "binascii.b2a_base64()", "path": "library/binascii#binascii.b2a_base64", "type": "Internet Data", "text": " \nbinascii.b2a_base64(data, *, newline=True)  \nConvert binary data to a line of ASCII characters in base64 coding. The return value is the converted line, including a newline char if newline is true. The output of this function conforms to RFC 3548.  Changed in version 3.6: Added the newline parameter.  \n"}, {"name": "binascii.b2a_hex()", "path": "library/binascii#binascii.b2a_hex", "type": "Internet Data", "text": " \nbinascii.b2a_hex(data[, sep[, bytes_per_sep=1]])  \nbinascii.hexlify(data[, sep[, bytes_per_sep=1]])  \nReturn the hexadecimal representation of the binary data. Every byte of data is converted into the corresponding 2-digit hex representation. The returned bytes object is therefore twice as long as the length of data. Similar functionality (but returning a text string) is also conveniently accessible using the bytes.hex() method. If sep is specified, it must be a single character str or bytes object. It will be inserted in the output after every bytes_per_sep input bytes. Separator placement is counted from the right end of the output by default, if you wish to count from the left, supply a negative bytes_per_sep value. >>> import binascii\n>>> binascii.b2a_hex(b'\\xb9\\x01\\xef')\nb'b901ef'\n>>> binascii.hexlify(b'\\xb9\\x01\\xef', '-')\nb'b9-01-ef'\n>>> binascii.b2a_hex(b'\\xb9\\x01\\xef', b'_', 2)\nb'b9_01ef'\n>>> binascii.b2a_hex(b'\\xb9\\x01\\xef', b' ', -2)\nb'b901 ef'\n  Changed in version 3.8: The sep and bytes_per_sep parameters were added.  \n"}, {"name": "binascii.b2a_hqx()", "path": "library/binascii#binascii.b2a_hqx", "type": "Internet Data", "text": " \nbinascii.b2a_hqx(data)  \nPerform hexbin4 binary-to-ASCII translation and return the resulting string. The argument should already be RLE-coded, and have a length divisible by 3 (except possibly the last fragment).  Deprecated since version 3.9.  \n"}, {"name": "binascii.b2a_qp()", "path": "library/binascii#binascii.b2a_qp", "type": "Internet Data", "text": " \nbinascii.b2a_qp(data, quotetabs=False, istext=True, header=False)  \nConvert binary data to a line(s) of ASCII characters in quoted-printable encoding. The return value is the converted line(s). If the optional argument quotetabs is present and true, all tabs and spaces will be encoded. If the optional argument istext is present and true, newlines are not encoded but trailing whitespace will be encoded. If the optional argument header is present and true, spaces will be encoded as underscores per RFC 1522. If the optional argument header is present and false, newline characters will be encoded as well; otherwise linefeed conversion might corrupt the binary data stream. \n"}, {"name": "binascii.b2a_uu()", "path": "library/binascii#binascii.b2a_uu", "type": "Internet Data", "text": " \nbinascii.b2a_uu(data, *, backtick=False)  \nConvert binary data to a line of ASCII characters, the return value is the converted line, including a newline char. The length of data should be at most 45. If backtick is true, zeros are represented by '`' instead of spaces.  Changed in version 3.7: Added the backtick parameter.  \n"}, {"name": "binascii.crc32()", "path": "library/binascii#binascii.crc32", "type": "Internet Data", "text": " \nbinascii.crc32(data[, value])  \nCompute CRC-32, the 32-bit checksum of data, starting with an initial CRC of value. The default initial CRC is zero. The algorithm is consistent with the ZIP file checksum. Since the algorithm is designed for use as a checksum algorithm, it is not suitable for use as a general hash algorithm. Use as follows: print(binascii.crc32(b\"hello world\"))\n# Or, in two pieces:\ncrc = binascii.crc32(b\"hello\")\ncrc = binascii.crc32(b\" world\", crc)\nprint('crc32 = {:#010x}'.format(crc))\n  Changed in version 3.0: The result is always unsigned. To generate the same numeric value across all Python versions and platforms, use crc32(data) & 0xffffffff.  \n"}, {"name": "binascii.crc_hqx()", "path": "library/binascii#binascii.crc_hqx", "type": "Internet Data", "text": " \nbinascii.crc_hqx(data, value)  \nCompute a 16-bit CRC value of data, starting with value as the initial CRC, and return the result. This uses the CRC-CCITT polynomial x16 + x12 + x5 + 1, often represented as 0x1021. This CRC is used in the binhex4 format. \n"}, {"name": "binascii.Error", "path": "library/binascii#binascii.Error", "type": "Internet Data", "text": " \nexception binascii.Error  \nException raised on errors. These are usually programming errors. \n"}, {"name": "binascii.hexlify()", "path": "library/binascii#binascii.hexlify", "type": "Internet Data", "text": " \nbinascii.b2a_hex(data[, sep[, bytes_per_sep=1]])  \nbinascii.hexlify(data[, sep[, bytes_per_sep=1]])  \nReturn the hexadecimal representation of the binary data. Every byte of data is converted into the corresponding 2-digit hex representation. The returned bytes object is therefore twice as long as the length of data. Similar functionality (but returning a text string) is also conveniently accessible using the bytes.hex() method. If sep is specified, it must be a single character str or bytes object. It will be inserted in the output after every bytes_per_sep input bytes. Separator placement is counted from the right end of the output by default, if you wish to count from the left, supply a negative bytes_per_sep value. >>> import binascii\n>>> binascii.b2a_hex(b'\\xb9\\x01\\xef')\nb'b901ef'\n>>> binascii.hexlify(b'\\xb9\\x01\\xef', '-')\nb'b9-01-ef'\n>>> binascii.b2a_hex(b'\\xb9\\x01\\xef', b'_', 2)\nb'b9_01ef'\n>>> binascii.b2a_hex(b'\\xb9\\x01\\xef', b' ', -2)\nb'b901 ef'\n  Changed in version 3.8: The sep and bytes_per_sep parameters were added.  \n"}, {"name": "binascii.Incomplete", "path": "library/binascii#binascii.Incomplete", "type": "Internet Data", "text": " \nexception binascii.Incomplete  \nException raised on incomplete data. These are usually not programming errors, but may be handled by reading a little more data and trying again. \n"}, {"name": "binascii.rlecode_hqx()", "path": "library/binascii#binascii.rlecode_hqx", "type": "Internet Data", "text": " \nbinascii.rlecode_hqx(data)  \nPerform binhex4 style RLE-compression on data and return the result.  Deprecated since version 3.9.  \n"}, {"name": "binascii.rledecode_hqx()", "path": "library/binascii#binascii.rledecode_hqx", "type": "Internet Data", "text": " \nbinascii.rledecode_hqx(data)  \nPerform RLE-decompression on the data, as per the binhex4 standard. The algorithm uses 0x90 after a byte as a repeat indicator, followed by a count. A count of 0 specifies a byte value of 0x90. The routine returns the decompressed data, unless data input data ends in an orphaned repeat indicator, in which case the Incomplete exception is raised.  Changed in version 3.2: Accept only bytestring or bytearray objects as input.   Deprecated since version 3.9.  \n"}, {"name": "binascii.unhexlify()", "path": "library/binascii#binascii.unhexlify", "type": "Internet Data", "text": " \nbinascii.a2b_hex(hexstr)  \nbinascii.unhexlify(hexstr)  \nReturn the binary data represented by the hexadecimal string hexstr. This function is the inverse of b2a_hex(). hexstr must contain an even number of hexadecimal digits (which can be upper or lower case), otherwise an Error exception is raised. Similar functionality (accepting only text string arguments, but more liberal towards whitespace) is also accessible using the bytes.fromhex() class method. \n"}, {"name": "binhex", "path": "library/binhex", "type": "Internet Data", "text": "binhex \u2014 Encode and decode binhex4 files Source code: Lib/binhex.py  Deprecated since version 3.9.  This module encodes and decodes files in binhex4 format, a format allowing representation of Macintosh files in ASCII. Only the data fork is handled. The binhex module defines the following functions:  \nbinhex.binhex(input, output)  \nConvert a binary file with filename input to binhex file output. The output parameter can either be a filename or a file-like object (any object supporting a write() and close() method). \n  \nbinhex.hexbin(input, output)  \nDecode a binhex file input. input may be a filename or a file-like object supporting read() and close() methods. The resulting file is written to a file named output, unless the argument is None in which case the output filename is read from the binhex file. \n The following exception is also defined:  \nexception binhex.Error  \nException raised when something can\u2019t be encoded using the binhex format (for example, a filename is too long to fit in the filename field), or when input is not properly encoded binhex data. \n  See also  \nModule binascii\n\n\nSupport module containing ASCII-to-binary and binary-to-ASCII conversions.    Notes There is an alternative, more powerful interface to the coder and decoder, see the source for details. If you code or decode textfiles on non-Macintosh platforms they will still use the old Macintosh newline convention (carriage-return as end of line).\n"}, {"name": "binhex.binhex()", "path": "library/binhex#binhex.binhex", "type": "Internet Data", "text": " \nbinhex.binhex(input, output)  \nConvert a binary file with filename input to binhex file output. The output parameter can either be a filename or a file-like object (any object supporting a write() and close() method). \n"}, {"name": "binhex.Error", "path": "library/binhex#binhex.Error", "type": "Internet Data", "text": " \nexception binhex.Error  \nException raised when something can\u2019t be encoded using the binhex format (for example, a filename is too long to fit in the filename field), or when input is not properly encoded binhex data. \n"}, {"name": "binhex.hexbin()", "path": "library/binhex#binhex.hexbin", "type": "Internet Data", "text": " \nbinhex.hexbin(input, output)  \nDecode a binhex file input. input may be a filename or a file-like object supporting read() and close() methods. The resulting file is written to a file named output, unless the argument is None in which case the output filename is read from the binhex file. \n"}, {"name": "bisect", "path": "library/bisect", "type": "Data Types", "text": "bisect \u2014 Array bisection algorithm Source code: Lib/bisect.py This module provides support for maintaining a list in sorted order without having to sort the list after each insertion. For long lists of items with expensive comparison operations, this can be an improvement over the more common approach. The module is called bisect because it uses a basic bisection algorithm to do its work. The source code may be most useful as a working example of the algorithm (the boundary conditions are already right!). The following functions are provided:  \nbisect.bisect_left(a, x, lo=0, hi=len(a))  \nLocate the insertion point for x in a to maintain sorted order. The parameters lo and hi may be used to specify a subset of the list which should be considered; by default the entire list is used. If x is already present in a, the insertion point will be before (to the left of) any existing entries. The return value is suitable for use as the first parameter to list.insert() assuming that a is already sorted. The returned insertion point i partitions the array a into two halves so that all(val < x for val in a[lo:i]) for the left side and all(val >= x for val in a[i:hi]) for the right side. \n  \nbisect.bisect_right(a, x, lo=0, hi=len(a))  \nbisect.bisect(a, x, lo=0, hi=len(a))  \nSimilar to bisect_left(), but returns an insertion point which comes after (to the right of) any existing entries of x in a. The returned insertion point i partitions the array a into two halves so that all(val <= x for val in a[lo:i]) for the left side and all(val > x for val in a[i:hi]) for the right side. \n  \nbisect.insort_left(a, x, lo=0, hi=len(a))  \nInsert x in a in sorted order. This is equivalent to a.insert(bisect.bisect_left(a, x, lo, hi), x) assuming that a is already sorted. Keep in mind that the O(log n) search is dominated by the slow O(n) insertion step. \n  \nbisect.insort_right(a, x, lo=0, hi=len(a))  \nbisect.insort(a, x, lo=0, hi=len(a))  \nSimilar to insort_left(), but inserting x in a after any existing entries of x. \n  See also SortedCollection recipe that uses bisect to build a full-featured collection class with straight-forward search methods and support for a key-function. The keys are precomputed to save unnecessary calls to the key function during searches.  Searching Sorted Lists The above bisect() functions are useful for finding insertion points but can be tricky or awkward to use for common searching tasks. The following five functions show how to transform them into the standard lookups for sorted lists: def index(a, x):\n    'Locate the leftmost value exactly equal to x'\n    i = bisect_left(a, x)\n    if i != len(a) and a[i] == x:\n        return i\n    raise ValueError\n\ndef find_lt(a, x):\n    'Find rightmost value less than x'\n    i = bisect_left(a, x)\n    if i:\n        return a[i-1]\n    raise ValueError\n\ndef find_le(a, x):\n    'Find rightmost value less than or equal to x'\n    i = bisect_right(a, x)\n    if i:\n        return a[i-1]\n    raise ValueError\n\ndef find_gt(a, x):\n    'Find leftmost value greater than x'\n    i = bisect_right(a, x)\n    if i != len(a):\n        return a[i]\n    raise ValueError\n\ndef find_ge(a, x):\n    'Find leftmost item greater than or equal to x'\n    i = bisect_left(a, x)\n    if i != len(a):\n        return a[i]\n    raise ValueError\n Other Examples The bisect() function can be useful for numeric table lookups. This example uses bisect() to look up a letter grade for an exam score (say) based on a set of ordered numeric breakpoints: 90 and up is an \u2018A\u2019, 80 to 89 is a \u2018B\u2019, and so on: >>> def grade(score, breakpoints=[60, 70, 80, 90], grades='FDCBA'):\n...     i = bisect(breakpoints, score)\n...     return grades[i]\n...\n>>> [grade(score) for score in [33, 99, 77, 70, 89, 90, 100]]\n['F', 'A', 'C', 'C', 'B', 'A', 'A']\n Unlike the sorted() function, it does not make sense for the bisect() functions to have key or reversed arguments because that would lead to an inefficient design (successive calls to bisect functions would not \u201cremember\u201d all of the previous key lookups). Instead, it is better to search a list of precomputed keys to find the index of the record in question: >>> data = [('red', 5), ('blue', 1), ('yellow', 8), ('black', 0)]\n>>> data.sort(key=lambda r: r[1])\n>>> keys = [r[1] for r in data]         # precomputed list of keys\n>>> data[bisect_left(keys, 0)]\n('black', 0)\n>>> data[bisect_left(keys, 1)]\n('blue', 1)\n>>> data[bisect_left(keys, 5)]\n('red', 5)\n>>> data[bisect_left(keys, 8)]\n('yellow', 8)\n\n"}, {"name": "bisect.bisect()", "path": "library/bisect#bisect.bisect", "type": "Data Types", "text": " \nbisect.bisect_right(a, x, lo=0, hi=len(a))  \nbisect.bisect(a, x, lo=0, hi=len(a))  \nSimilar to bisect_left(), but returns an insertion point which comes after (to the right of) any existing entries of x in a. The returned insertion point i partitions the array a into two halves so that all(val <= x for val in a[lo:i]) for the left side and all(val > x for val in a[i:hi]) for the right side. \n"}, {"name": "bisect.bisect_left()", "path": "library/bisect#bisect.bisect_left", "type": "Data Types", "text": " \nbisect.bisect_left(a, x, lo=0, hi=len(a))  \nLocate the insertion point for x in a to maintain sorted order. The parameters lo and hi may be used to specify a subset of the list which should be considered; by default the entire list is used. If x is already present in a, the insertion point will be before (to the left of) any existing entries. The return value is suitable for use as the first parameter to list.insert() assuming that a is already sorted. The returned insertion point i partitions the array a into two halves so that all(val < x for val in a[lo:i]) for the left side and all(val >= x for val in a[i:hi]) for the right side. \n"}, {"name": "bisect.bisect_right()", "path": "library/bisect#bisect.bisect_right", "type": "Data Types", "text": " \nbisect.bisect_right(a, x, lo=0, hi=len(a))  \nbisect.bisect(a, x, lo=0, hi=len(a))  \nSimilar to bisect_left(), but returns an insertion point which comes after (to the right of) any existing entries of x in a. The returned insertion point i partitions the array a into two halves so that all(val <= x for val in a[lo:i]) for the left side and all(val > x for val in a[i:hi]) for the right side. \n"}, {"name": "bisect.insort()", "path": "library/bisect#bisect.insort", "type": "Data Types", "text": " \nbisect.insort_right(a, x, lo=0, hi=len(a))  \nbisect.insort(a, x, lo=0, hi=len(a))  \nSimilar to insort_left(), but inserting x in a after any existing entries of x. \n"}, {"name": "bisect.insort_left()", "path": "library/bisect#bisect.insort_left", "type": "Data Types", "text": " \nbisect.insort_left(a, x, lo=0, hi=len(a))  \nInsert x in a in sorted order. This is equivalent to a.insert(bisect.bisect_left(a, x, lo, hi), x) assuming that a is already sorted. Keep in mind that the O(log n) search is dominated by the slow O(n) insertion step. \n"}, {"name": "bisect.insort_right()", "path": "library/bisect#bisect.insort_right", "type": "Data Types", "text": " \nbisect.insort_right(a, x, lo=0, hi=len(a))  \nbisect.insort(a, x, lo=0, hi=len(a))  \nSimilar to insort_left(), but inserting x in a after any existing entries of x. \n"}, {"name": "BlockingIOError", "path": "library/exceptions#BlockingIOError", "type": "Built-in Exceptions", "text": " \nexception BlockingIOError  \nRaised when an operation would block on an object (e.g. socket) set for non-blocking operation. Corresponds to errno EAGAIN, EALREADY, EWOULDBLOCK and EINPROGRESS. In addition to those of OSError, BlockingIOError can have one more attribute:  \ncharacters_written  \nAn integer containing the number of characters written to the stream before it blocked. This attribute is available when using the buffered I/O classes from the io module. \n \n"}, {"name": "BlockingIOError.characters_written", "path": "library/exceptions#BlockingIOError.characters_written", "type": "Built-in Exceptions", "text": " \ncharacters_written  \nAn integer containing the number of characters written to the stream before it blocked. This attribute is available when using the buffered I/O classes from the io module. \n"}, {"name": "bool", "path": "library/functions#bool", "type": "Built-in Functions", "text": " \nclass bool([x])  \nReturn a Boolean value, i.e. one of True or False. x is converted using the standard truth testing procedure. If x is false or omitted, this returns False; otherwise it returns True. The bool class is a subclass of int (see Numeric Types \u2014 int, float, complex). It cannot be subclassed further. Its only instances are False and True (see Boolean Values).  Changed in version 3.7: x is now a positional-only parameter.  \n"}, {"name": "breakpoint()", "path": "library/functions#breakpoint", "type": "Built-in Functions", "text": " \nbreakpoint(*args, **kws)  \nThis function drops you into the debugger at the call site. Specifically, it calls sys.breakpointhook(), passing args and kws straight through. By default, sys.breakpointhook() calls pdb.set_trace() expecting no arguments. In this case, it is purely a convenience function so you don\u2019t have to explicitly import pdb or type as much code to enter the debugger. However, sys.breakpointhook() can be set to some other function and breakpoint() will automatically call that, allowing you to drop into the debugger of choice. Raises an auditing event builtins.breakpoint with argument breakpointhook.  New in version 3.7.  \n"}, {"name": "BrokenPipeError", "path": "library/exceptions#BrokenPipeError", "type": "Built-in Exceptions", "text": " \nexception BrokenPipeError  \nA subclass of ConnectionError, raised when trying to write on a pipe while the other end has been closed, or trying to write on a socket which has been shutdown for writing. Corresponds to errno EPIPE and ESHUTDOWN. \n"}, {"name": "BufferError", "path": "library/exceptions#BufferError", "type": "Built-in Exceptions", "text": " \nexception BufferError  \nRaised when a buffer related operation cannot be performed. \n"}, {"name": "builtins", "path": "library/builtins", "type": "Runtime", "text": "builtins \u2014 Built-in objects This module provides direct access to all \u2018built-in\u2019 identifiers of Python; for example, builtins.open is the full name for the built-in function open(). See Built-in Functions and Built-in Constants for documentation. This module is not normally accessed explicitly by most applications, but can be useful in modules that provide objects with the same name as a built-in value, but in which the built-in of that name is also needed. For example, in a module that wants to implement an open() function that wraps the built-in open(), this module can be used directly: import builtins\n\ndef open(path):\n    f = builtins.open(path, 'r')\n    return UpperCaser(f)\n\nclass UpperCaser:\n    '''Wrapper around a file that converts output to upper-case.'''\n\n    def __init__(self, f):\n        self._f = f\n\n    def read(self, count=-1):\n        return self._f.read(count).upper()\n\n    # ...\n As an implementation detail, most modules have the name __builtins__ made available as part of their globals. The value of __builtins__ is normally either this module or the value of this module\u2019s __dict__ attribute. Since this is an implementation detail, it may not be used by alternate implementations of Python.\n"}, {"name": "bytearray", "path": "library/functions#bytearray", "type": "Built-in Functions", "text": " \nclass bytearray([source[, encoding[, errors]]])  \nReturn a new array of bytes. The bytearray class is a mutable sequence of integers in the range 0 <= x < 256. It has most of the usual methods of mutable sequences, described in Mutable Sequence Types, as well as most methods that the bytes type has, see Bytes and Bytearray Operations. The optional source parameter can be used to initialize the array in a few different ways:  If it is a string, you must also give the encoding (and optionally, errors) parameters; bytearray() then converts the string to bytes using str.encode(). If it is an integer, the array will have that size and will be initialized with null bytes. If it is an object conforming to the buffer interface, a read-only buffer of the object will be used to initialize the bytes array. If it is an iterable, it must be an iterable of integers in the range 0 <= x < 256, which are used as the initial contents of the array.  Without an argument, an array of size 0 is created. See also Binary Sequence Types \u2014 bytes, bytearray, memoryview and Bytearray Objects. \n"}, {"name": "bytearray", "path": "library/stdtypes#bytearray", "type": "Built-in Types", "text": " \nclass bytearray([source[, encoding[, errors]]])  \nThere is no dedicated literal syntax for bytearray objects, instead they are always created by calling the constructor:  Creating an empty instance: bytearray()\n Creating a zero-filled instance with a given length: bytearray(10)\n From an iterable of integers: bytearray(range(20))\n Copying existing binary data via the buffer protocol: bytearray(b'Hi!')\n  As bytearray objects are mutable, they support the mutable sequence operations in addition to the common bytes and bytearray operations described in Bytes and Bytearray Operations. Also see the bytearray built-in. Since 2 hexadecimal digits correspond precisely to a single byte, hexadecimal numbers are a commonly used format for describing binary data. Accordingly, the bytearray type has an additional class method to read data in that format:  \nclassmethod fromhex(string)  \nThis bytearray class method returns bytearray object, decoding the given string object. The string must contain two hexadecimal digits per byte, with ASCII whitespace being ignored. >>> bytearray.fromhex('2Ef0 F1f2  ')\nbytearray(b'.\\xf0\\xf1\\xf2')\n  Changed in version 3.7: bytearray.fromhex() now skips all ASCII whitespace in the string, not just spaces.  \n A reverse conversion function exists to transform a bytearray object into its hexadecimal representation.  \nhex([sep[, bytes_per_sep]])  \nReturn a string object containing two hexadecimal digits for each byte in the instance. >>> bytearray(b'\\xf0\\xf1\\xf2').hex()\n'f0f1f2'\n  New in version 3.5.   Changed in version 3.8: Similar to bytes.hex(), bytearray.hex() now supports optional sep and bytes_per_sep parameters to insert separators between bytes in the hex output.  \n \n"}, {"name": "bytearray.capitalize()", "path": "library/stdtypes#bytearray.capitalize", "type": "Built-in Types", "text": " \nbytes.capitalize()  \nbytearray.capitalize()  \nReturn a copy of the sequence with each byte interpreted as an ASCII character, and the first byte capitalized and the rest lowercased. Non-ASCII byte values are passed through unchanged.  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytearray.center()", "path": "library/stdtypes#bytearray.center", "type": "Built-in Types", "text": " \nbytes.center(width[, fillbyte])  \nbytearray.center(width[, fillbyte])  \nReturn a copy of the object centered in a sequence of length width. Padding is done using the specified fillbyte (default is an ASCII space). For bytes objects, the original sequence is returned if width is less than or equal to len(s).  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytearray.count()", "path": "library/stdtypes#bytearray.count", "type": "Built-in Types", "text": " \nbytes.count(sub[, start[, end]])  \nbytearray.count(sub[, start[, end]])  \nReturn the number of non-overlapping occurrences of subsequence sub in the range [start, end]. Optional arguments start and end are interpreted as in slice notation. The subsequence to search for may be any bytes-like object or an integer in the range 0 to 255.  Changed in version 3.3: Also accept an integer in the range 0 to 255 as the subsequence.  \n"}, {"name": "bytearray.decode()", "path": "library/stdtypes#bytearray.decode", "type": "Built-in Types", "text": " \nbytes.decode(encoding=\"utf-8\", errors=\"strict\")  \nbytearray.decode(encoding=\"utf-8\", errors=\"strict\")  \nReturn a string decoded from the given bytes. Default encoding is 'utf-8'. errors may be given to set a different error handling scheme. The default for errors is 'strict', meaning that encoding errors raise a UnicodeError. Other possible values are 'ignore', 'replace' and any other name registered via codecs.register_error(), see section Error Handlers. For a list of possible encodings, see section Standard Encodings. By default, the errors argument is not checked for best performances, but only used at the first decoding error. Enable the Python Development Mode, or use a debug build to check errors.  Note Passing the encoding argument to str allows decoding any bytes-like object directly, without needing to make a temporary bytes or bytearray object.   Changed in version 3.1: Added support for keyword arguments.   Changed in version 3.9: The errors is now checked in development mode and in debug mode.  \n"}, {"name": "bytearray.endswith()", "path": "library/stdtypes#bytearray.endswith", "type": "Built-in Types", "text": " \nbytes.endswith(suffix[, start[, end]])  \nbytearray.endswith(suffix[, start[, end]])  \nReturn True if the binary data ends with the specified suffix, otherwise return False. suffix can also be a tuple of suffixes to look for. With optional start, test beginning at that position. With optional end, stop comparing at that position. The suffix(es) to search for may be any bytes-like object. \n"}, {"name": "bytearray.expandtabs()", "path": "library/stdtypes#bytearray.expandtabs", "type": "Built-in Types", "text": " \nbytes.expandtabs(tabsize=8)  \nbytearray.expandtabs(tabsize=8)  \nReturn a copy of the sequence where all ASCII tab characters are replaced by one or more ASCII spaces, depending on the current column and the given tab size. Tab positions occur every tabsize bytes (default is 8, giving tab positions at columns 0, 8, 16 and so on). To expand the sequence, the current column is set to zero and the sequence is examined byte by byte. If the byte is an ASCII tab character (b'\\t'), one or more space characters are inserted in the result until the current column is equal to the next tab position. (The tab character itself is not copied.) If the current byte is an ASCII newline (b'\\n') or carriage return (b'\\r'), it is copied and the current column is reset to zero. Any other byte value is copied unchanged and the current column is incremented by one regardless of how the byte value is represented when printed: >>> b'01\\t012\\t0123\\t01234'.expandtabs()\nb'01      012     0123    01234'\n>>> b'01\\t012\\t0123\\t01234'.expandtabs(4)\nb'01  012 0123    01234'\n  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytearray.find()", "path": "library/stdtypes#bytearray.find", "type": "Built-in Types", "text": " \nbytes.find(sub[, start[, end]])  \nbytearray.find(sub[, start[, end]])  \nReturn the lowest index in the data where the subsequence sub is found, such that sub is contained in the slice s[start:end]. Optional arguments start and end are interpreted as in slice notation. Return -1 if sub is not found. The subsequence to search for may be any bytes-like object or an integer in the range 0 to 255.  Note The find() method should be used only if you need to know the position of sub. To check if sub is a substring or not, use the in operator: >>> b'Py' in b'Python'\nTrue\n   Changed in version 3.3: Also accept an integer in the range 0 to 255 as the subsequence.  \n"}, {"name": "bytearray.fromhex()", "path": "library/stdtypes#bytearray.fromhex", "type": "Built-in Types", "text": " \nclassmethod fromhex(string)  \nThis bytearray class method returns bytearray object, decoding the given string object. The string must contain two hexadecimal digits per byte, with ASCII whitespace being ignored. >>> bytearray.fromhex('2Ef0 F1f2  ')\nbytearray(b'.\\xf0\\xf1\\xf2')\n  Changed in version 3.7: bytearray.fromhex() now skips all ASCII whitespace in the string, not just spaces.  \n"}, {"name": "bytearray.hex()", "path": "library/stdtypes#bytearray.hex", "type": "Built-in Types", "text": " \nhex([sep[, bytes_per_sep]])  \nReturn a string object containing two hexadecimal digits for each byte in the instance. >>> bytearray(b'\\xf0\\xf1\\xf2').hex()\n'f0f1f2'\n  New in version 3.5.   Changed in version 3.8: Similar to bytes.hex(), bytearray.hex() now supports optional sep and bytes_per_sep parameters to insert separators between bytes in the hex output.  \n"}, {"name": "bytearray.index()", "path": "library/stdtypes#bytearray.index", "type": "Built-in Types", "text": " \nbytes.index(sub[, start[, end]])  \nbytearray.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the subsequence is not found. The subsequence to search for may be any bytes-like object or an integer in the range 0 to 255.  Changed in version 3.3: Also accept an integer in the range 0 to 255 as the subsequence.  \n"}, {"name": "bytearray.isalnum()", "path": "library/stdtypes#bytearray.isalnum", "type": "Built-in Types", "text": " \nbytes.isalnum()  \nbytearray.isalnum()  \nReturn True if all bytes in the sequence are alphabetical ASCII characters or ASCII decimal digits and the sequence is not empty, False otherwise. Alphabetic ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'. ASCII decimal digits are those byte values in the sequence b'0123456789'. For example: >>> b'ABCabc1'.isalnum()\nTrue\n>>> b'ABC abc1'.isalnum()\nFalse\n \n"}, {"name": "bytearray.isalpha()", "path": "library/stdtypes#bytearray.isalpha", "type": "Built-in Types", "text": " \nbytes.isalpha()  \nbytearray.isalpha()  \nReturn True if all bytes in the sequence are alphabetic ASCII characters and the sequence is not empty, False otherwise. Alphabetic ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'. For example: >>> b'ABCabc'.isalpha()\nTrue\n>>> b'ABCabc1'.isalpha()\nFalse\n \n"}, {"name": "bytearray.isascii()", "path": "library/stdtypes#bytearray.isascii", "type": "Built-in Types", "text": " \nbytes.isascii()  \nbytearray.isascii()  \nReturn True if the sequence is empty or all bytes in the sequence are ASCII, False otherwise. ASCII bytes are in the range 0-0x7F.  New in version 3.7.  \n"}, {"name": "bytearray.isdigit()", "path": "library/stdtypes#bytearray.isdigit", "type": "Built-in Types", "text": " \nbytes.isdigit()  \nbytearray.isdigit()  \nReturn True if all bytes in the sequence are ASCII decimal digits and the sequence is not empty, False otherwise. ASCII decimal digits are those byte values in the sequence b'0123456789'. For example: >>> b'1234'.isdigit()\nTrue\n>>> b'1.23'.isdigit()\nFalse\n \n"}, {"name": "bytearray.islower()", "path": "library/stdtypes#bytearray.islower", "type": "Built-in Types", "text": " \nbytes.islower()  \nbytearray.islower()  \nReturn True if there is at least one lowercase ASCII character in the sequence and no uppercase ASCII characters, False otherwise. For example: >>> b'hello world'.islower()\nTrue\n>>> b'Hello world'.islower()\nFalse\n Lowercase ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyz'. Uppercase ASCII characters are those byte values in the sequence b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'. \n"}, {"name": "bytearray.isspace()", "path": "library/stdtypes#bytearray.isspace", "type": "Built-in Types", "text": " \nbytes.isspace()  \nbytearray.isspace()  \nReturn True if all bytes in the sequence are ASCII whitespace and the sequence is not empty, False otherwise. ASCII whitespace characters are those byte values in the sequence b' \\t\\n\\r\\x0b\\f' (space, tab, newline, carriage return, vertical tab, form feed). \n"}, {"name": "bytearray.istitle()", "path": "library/stdtypes#bytearray.istitle", "type": "Built-in Types", "text": " \nbytes.istitle()  \nbytearray.istitle()  \nReturn True if the sequence is ASCII titlecase and the sequence is not empty, False otherwise. See bytes.title() for more details on the definition of \u201ctitlecase\u201d. For example: >>> b'Hello World'.istitle()\nTrue\n>>> b'Hello world'.istitle()\nFalse\n \n"}, {"name": "bytearray.isupper()", "path": "library/stdtypes#bytearray.isupper", "type": "Built-in Types", "text": " \nbytes.isupper()  \nbytearray.isupper()  \nReturn True if there is at least one uppercase alphabetic ASCII character in the sequence and no lowercase ASCII characters, False otherwise. For example: >>> b'HELLO WORLD'.isupper()\nTrue\n>>> b'Hello world'.isupper()\nFalse\n Lowercase ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyz'. Uppercase ASCII characters are those byte values in the sequence b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'. \n"}, {"name": "bytearray.join()", "path": "library/stdtypes#bytearray.join", "type": "Built-in Types", "text": " \nbytes.join(iterable)  \nbytearray.join(iterable)  \nReturn a bytes or bytearray object which is the concatenation of the binary data sequences in iterable. A TypeError will be raised if there are any values in iterable that are not bytes-like objects, including str objects. The separator between elements is the contents of the bytes or bytearray object providing this method. \n"}, {"name": "bytearray.ljust()", "path": "library/stdtypes#bytearray.ljust", "type": "Built-in Types", "text": " \nbytes.ljust(width[, fillbyte])  \nbytearray.ljust(width[, fillbyte])  \nReturn a copy of the object left justified in a sequence of length width. Padding is done using the specified fillbyte (default is an ASCII space). For bytes objects, the original sequence is returned if width is less than or equal to len(s).  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytearray.lower()", "path": "library/stdtypes#bytearray.lower", "type": "Built-in Types", "text": " \nbytes.lower()  \nbytearray.lower()  \nReturn a copy of the sequence with all the uppercase ASCII characters converted to their corresponding lowercase counterpart. For example: >>> b'Hello World'.lower()\nb'hello world'\n Lowercase ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyz'. Uppercase ASCII characters are those byte values in the sequence b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'.  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytearray.lstrip()", "path": "library/stdtypes#bytearray.lstrip", "type": "Built-in Types", "text": " \nbytes.lstrip([chars])  \nbytearray.lstrip([chars])  \nReturn a copy of the sequence with specified leading bytes removed. The chars argument is a binary sequence specifying the set of byte values to be removed - the name refers to the fact this method is usually used with ASCII characters. If omitted or None, the chars argument defaults to removing ASCII whitespace. The chars argument is not a prefix; rather, all combinations of its values are stripped: >>> b'   spacious   '.lstrip()\nb'spacious   '\n>>> b'www.example.com'.lstrip(b'cmowz.')\nb'example.com'\n The binary sequence of byte values to remove may be any bytes-like object. See removeprefix() for a method that will remove a single prefix string rather than all of a set of characters. For example: >>> b'Arthur: three!'.lstrip(b'Arthur: ')\nb'ee!'\n>>> b'Arthur: three!'.removeprefix(b'Arthur: ')\nb'three!'\n  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytearray.maketrans()", "path": "library/stdtypes#bytearray.maketrans", "type": "Built-in Types", "text": " \nstatic bytes.maketrans(from, to)  \nstatic bytearray.maketrans(from, to)  \nThis static method returns a translation table usable for bytes.translate() that will map each character in from into the character at the same position in to; from and to must both be bytes-like objects and have the same length.  New in version 3.1.  \n"}, {"name": "bytearray.partition()", "path": "library/stdtypes#bytearray.partition", "type": "Built-in Types", "text": " \nbytes.partition(sep)  \nbytearray.partition(sep)  \nSplit the sequence at the first occurrence of sep, and return a 3-tuple containing the part before the separator, the separator itself or its bytearray copy, and the part after the separator. If the separator is not found, return a 3-tuple containing a copy of the original sequence, followed by two empty bytes or bytearray objects. The separator to search for may be any bytes-like object. \n"}, {"name": "bytearray.removeprefix()", "path": "library/stdtypes#bytearray.removeprefix", "type": "Built-in Types", "text": " \nbytes.removeprefix(prefix, /)  \nbytearray.removeprefix(prefix, /)  \nIf the binary data starts with the prefix string, return bytes[len(prefix):]. Otherwise, return a copy of the original binary data: >>> b'TestHook'.removeprefix(b'Test')\nb'Hook'\n>>> b'BaseTestCase'.removeprefix(b'Test')\nb'BaseTestCase'\n The prefix may be any bytes-like object.  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.   New in version 3.9.  \n"}, {"name": "bytearray.removesuffix()", "path": "library/stdtypes#bytearray.removesuffix", "type": "Built-in Types", "text": " \nbytes.removesuffix(suffix, /)  \nbytearray.removesuffix(suffix, /)  \nIf the binary data ends with the suffix string and that suffix is not empty, return bytes[:-len(suffix)]. Otherwise, return a copy of the original binary data: >>> b'MiscTests'.removesuffix(b'Tests')\nb'Misc'\n>>> b'TmpDirMixin'.removesuffix(b'Tests')\nb'TmpDirMixin'\n The suffix may be any bytes-like object.  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.   New in version 3.9.  \n"}, {"name": "bytearray.replace()", "path": "library/stdtypes#bytearray.replace", "type": "Built-in Types", "text": " \nbytes.replace(old, new[, count])  \nbytearray.replace(old, new[, count])  \nReturn a copy of the sequence with all occurrences of subsequence old replaced by new. If the optional argument count is given, only the first count occurrences are replaced. The subsequence to search for and its replacement may be any bytes-like object.  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytearray.rfind()", "path": "library/stdtypes#bytearray.rfind", "type": "Built-in Types", "text": " \nbytes.rfind(sub[, start[, end]])  \nbytearray.rfind(sub[, start[, end]])  \nReturn the highest index in the sequence where the subsequence sub is found, such that sub is contained within s[start:end]. Optional arguments start and end are interpreted as in slice notation. Return -1 on failure. The subsequence to search for may be any bytes-like object or an integer in the range 0 to 255.  Changed in version 3.3: Also accept an integer in the range 0 to 255 as the subsequence.  \n"}, {"name": "bytearray.rindex()", "path": "library/stdtypes#bytearray.rindex", "type": "Built-in Types", "text": " \nbytes.rindex(sub[, start[, end]])  \nbytearray.rindex(sub[, start[, end]])  \nLike rfind() but raises ValueError when the subsequence sub is not found. The subsequence to search for may be any bytes-like object or an integer in the range 0 to 255.  Changed in version 3.3: Also accept an integer in the range 0 to 255 as the subsequence.  \n"}, {"name": "bytearray.rjust()", "path": "library/stdtypes#bytearray.rjust", "type": "Built-in Types", "text": " \nbytes.rjust(width[, fillbyte])  \nbytearray.rjust(width[, fillbyte])  \nReturn a copy of the object right justified in a sequence of length width. Padding is done using the specified fillbyte (default is an ASCII space). For bytes objects, the original sequence is returned if width is less than or equal to len(s).  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytearray.rpartition()", "path": "library/stdtypes#bytearray.rpartition", "type": "Built-in Types", "text": " \nbytes.rpartition(sep)  \nbytearray.rpartition(sep)  \nSplit the sequence at the last occurrence of sep, and return a 3-tuple containing the part before the separator, the separator itself or its bytearray copy, and the part after the separator. If the separator is not found, return a 3-tuple containing two empty bytes or bytearray objects, followed by a copy of the original sequence. The separator to search for may be any bytes-like object. \n"}, {"name": "bytearray.rsplit()", "path": "library/stdtypes#bytearray.rsplit", "type": "Built-in Types", "text": " \nbytes.rsplit(sep=None, maxsplit=-1)  \nbytearray.rsplit(sep=None, maxsplit=-1)  \nSplit the binary sequence into subsequences of the same type, using sep as the delimiter string. If maxsplit is given, at most maxsplit splits are done, the rightmost ones. If sep is not specified or None, any subsequence consisting solely of ASCII whitespace is a separator. Except for splitting from the right, rsplit() behaves like split() which is described in detail below. \n"}, {"name": "bytearray.rstrip()", "path": "library/stdtypes#bytearray.rstrip", "type": "Built-in Types", "text": " \nbytes.rstrip([chars])  \nbytearray.rstrip([chars])  \nReturn a copy of the sequence with specified trailing bytes removed. The chars argument is a binary sequence specifying the set of byte values to be removed - the name refers to the fact this method is usually used with ASCII characters. If omitted or None, the chars argument defaults to removing ASCII whitespace. The chars argument is not a suffix; rather, all combinations of its values are stripped: >>> b'   spacious   '.rstrip()\nb'   spacious'\n>>> b'mississippi'.rstrip(b'ipz')\nb'mississ'\n The binary sequence of byte values to remove may be any bytes-like object. See removesuffix() for a method that will remove a single suffix string rather than all of a set of characters. For example: >>> b'Monty Python'.rstrip(b' Python')\nb'M'\n>>> b'Monty Python'.removesuffix(b' Python')\nb'Monty'\n  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytearray.split()", "path": "library/stdtypes#bytearray.split", "type": "Built-in Types", "text": " \nbytes.split(sep=None, maxsplit=-1)  \nbytearray.split(sep=None, maxsplit=-1)  \nSplit the binary sequence into subsequences of the same type, using sep as the delimiter string. If maxsplit is given and non-negative, at most maxsplit splits are done (thus, the list will have at most maxsplit+1 elements). If maxsplit is not specified or is -1, then there is no limit on the number of splits (all possible splits are made). If sep is given, consecutive delimiters are not grouped together and are deemed to delimit empty subsequences (for example, b'1,,2'.split(b',') returns [b'1', b'', b'2']). The sep argument may consist of a multibyte sequence (for example, b'1<>2<>3'.split(b'<>') returns [b'1', b'2', b'3']). Splitting an empty sequence with a specified separator returns [b''] or [bytearray(b'')] depending on the type of object being split. The sep argument may be any bytes-like object. For example: >>> b'1,2,3'.split(b',')\n[b'1', b'2', b'3']\n>>> b'1,2,3'.split(b',', maxsplit=1)\n[b'1', b'2,3']\n>>> b'1,2,,3,'.split(b',')\n[b'1', b'2', b'', b'3', b'']\n If sep is not specified or is None, a different splitting algorithm is applied: runs of consecutive ASCII whitespace are regarded as a single separator, and the result will contain no empty strings at the start or end if the sequence has leading or trailing whitespace. Consequently, splitting an empty sequence or a sequence consisting solely of ASCII whitespace without a specified separator returns []. For example: >>> b'1 2 3'.split()\n[b'1', b'2', b'3']\n>>> b'1 2 3'.split(maxsplit=1)\n[b'1', b'2 3']\n>>> b'   1   2   3   '.split()\n[b'1', b'2', b'3']\n \n"}, {"name": "bytearray.splitlines()", "path": "library/stdtypes#bytearray.splitlines", "type": "Built-in Types", "text": " \nbytes.splitlines(keepends=False)  \nbytearray.splitlines(keepends=False)  \nReturn a list of the lines in the binary sequence, breaking at ASCII line boundaries. This method uses the universal newlines approach to splitting lines. Line breaks are not included in the resulting list unless keepends is given and true. For example: >>> b'ab c\\n\\nde fg\\rkl\\r\\n'.splitlines()\n[b'ab c', b'', b'de fg', b'kl']\n>>> b'ab c\\n\\nde fg\\rkl\\r\\n'.splitlines(keepends=True)\n[b'ab c\\n', b'\\n', b'de fg\\r', b'kl\\r\\n']\n Unlike split() when a delimiter string sep is given, this method returns an empty list for the empty string, and a terminal line break does not result in an extra line: >>> b\"\".split(b'\\n'), b\"Two lines\\n\".split(b'\\n')\n([b''], [b'Two lines', b''])\n>>> b\"\".splitlines(), b\"One line\\n\".splitlines()\n([], [b'One line'])\n \n"}, {"name": "bytearray.startswith()", "path": "library/stdtypes#bytearray.startswith", "type": "Built-in Types", "text": " \nbytes.startswith(prefix[, start[, end]])  \nbytearray.startswith(prefix[, start[, end]])  \nReturn True if the binary data starts with the specified prefix, otherwise return False. prefix can also be a tuple of prefixes to look for. With optional start, test beginning at that position. With optional end, stop comparing at that position. The prefix(es) to search for may be any bytes-like object. \n"}, {"name": "bytearray.strip()", "path": "library/stdtypes#bytearray.strip", "type": "Built-in Types", "text": " \nbytes.strip([chars])  \nbytearray.strip([chars])  \nReturn a copy of the sequence with specified leading and trailing bytes removed. The chars argument is a binary sequence specifying the set of byte values to be removed - the name refers to the fact this method is usually used with ASCII characters. If omitted or None, the chars argument defaults to removing ASCII whitespace. The chars argument is not a prefix or suffix; rather, all combinations of its values are stripped: >>> b'   spacious   '.strip()\nb'spacious'\n>>> b'www.example.com'.strip(b'cmowz.')\nb'example'\n The binary sequence of byte values to remove may be any bytes-like object.  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytearray.swapcase()", "path": "library/stdtypes#bytearray.swapcase", "type": "Built-in Types", "text": " \nbytes.swapcase()  \nbytearray.swapcase()  \nReturn a copy of the sequence with all the lowercase ASCII characters converted to their corresponding uppercase counterpart and vice-versa. For example: >>> b'Hello World'.swapcase()\nb'hELLO wORLD'\n Lowercase ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyz'. Uppercase ASCII characters are those byte values in the sequence b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'. Unlike str.swapcase(), it is always the case that bin.swapcase().swapcase() == bin for the binary versions. Case conversions are symmetrical in ASCII, even though that is not generally true for arbitrary Unicode code points.  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytearray.title()", "path": "library/stdtypes#bytearray.title", "type": "Built-in Types", "text": " \nbytes.title()  \nbytearray.title()  \nReturn a titlecased version of the binary sequence where words start with an uppercase ASCII character and the remaining characters are lowercase. Uncased byte values are left unmodified. For example: >>> b'Hello world'.title()\nb'Hello World'\n Lowercase ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyz'. Uppercase ASCII characters are those byte values in the sequence b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'. All other byte values are uncased. The algorithm uses a simple language-independent definition of a word as groups of consecutive letters. The definition works in many contexts but it means that apostrophes in contractions and possessives form word boundaries, which may not be the desired result: >>> b\"they're bill's friends from the UK\".title()\nb\"They'Re Bill'S Friends From The Uk\"\n A workaround for apostrophes can be constructed using regular expressions: >>> import re\n>>> def titlecase(s):\n...     return re.sub(rb\"[A-Za-z]+('[A-Za-z]+)?\",\n...                   lambda mo: mo.group(0)[0:1].upper() +\n...                              mo.group(0)[1:].lower(),\n...                   s)\n...\n>>> titlecase(b\"they're bill's friends.\")\nb\"They're Bill's Friends.\"\n  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytearray.translate()", "path": "library/stdtypes#bytearray.translate", "type": "Built-in Types", "text": " \nbytes.translate(table, /, delete=b'')  \nbytearray.translate(table, /, delete=b'')  \nReturn a copy of the bytes or bytearray object where all bytes occurring in the optional argument delete are removed, and the remaining bytes have been mapped through the given translation table, which must be a bytes object of length 256. You can use the bytes.maketrans() method to create a translation table. Set the table argument to None for translations that only delete characters: >>> b'read this short text'.translate(None, b'aeiou')\nb'rd ths shrt txt'\n  Changed in version 3.6: delete is now supported as a keyword argument.  \n"}, {"name": "bytearray.upper()", "path": "library/stdtypes#bytearray.upper", "type": "Built-in Types", "text": " \nbytes.upper()  \nbytearray.upper()  \nReturn a copy of the sequence with all the lowercase ASCII characters converted to their corresponding uppercase counterpart. For example: >>> b'Hello World'.upper()\nb'HELLO WORLD'\n Lowercase ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyz'. Uppercase ASCII characters are those byte values in the sequence b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'.  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytearray.zfill()", "path": "library/stdtypes#bytearray.zfill", "type": "Built-in Types", "text": " \nbytes.zfill(width)  \nbytearray.zfill(width)  \nReturn a copy of the sequence left filled with ASCII b'0' digits to make a sequence of length width. A leading sign prefix (b'+'/ b'-') is handled by inserting the padding after the sign character rather than before. For bytes objects, the original sequence is returned if width is less than or equal to len(seq). For example: >>> b\"42\".zfill(5)\nb'00042'\n>>> b\"-42\".zfill(5)\nb'-0042'\n  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytes", "path": "library/stdtypes#bytes", "type": "Built-in Types", "text": " \nclass bytes([source[, encoding[, errors]]])  \nFirstly, the syntax for bytes literals is largely the same as that for string literals, except that a b prefix is added:  Single quotes: b'still allows embedded \"double\" quotes'\n Double quotes: b\"still allows embedded 'single' quotes\". Triple quoted: b'''3 single quotes''', b\"\"\"3 double quotes\"\"\"\n  Only ASCII characters are permitted in bytes literals (regardless of the declared source code encoding). Any binary values over 127 must be entered into bytes literals using the appropriate escape sequence. As with string literals, bytes literals may also use a r prefix to disable processing of escape sequences. See String and Bytes literals for more about the various forms of bytes literal, including supported escape sequences. While bytes literals and representations are based on ASCII text, bytes objects actually behave like immutable sequences of integers, with each value in the sequence restricted such that 0 <= x < 256 (attempts to violate this restriction will trigger ValueError). This is done deliberately to emphasise that while many binary formats include ASCII based elements and can be usefully manipulated with some text-oriented algorithms, this is not generally the case for arbitrary binary data (blindly applying text processing algorithms to binary data formats that are not ASCII compatible will usually lead to data corruption). In addition to the literal forms, bytes objects can be created in a number of other ways:  A zero-filled bytes object of a specified length: bytes(10)\n From an iterable of integers: bytes(range(20))\n Copying existing binary data via the buffer protocol: bytes(obj)\n  Also see the bytes built-in. Since 2 hexadecimal digits correspond precisely to a single byte, hexadecimal numbers are a commonly used format for describing binary data. Accordingly, the bytes type has an additional class method to read data in that format:  \nclassmethod fromhex(string)  \nThis bytes class method returns a bytes object, decoding the given string object. The string must contain two hexadecimal digits per byte, with ASCII whitespace being ignored. >>> bytes.fromhex('2Ef0 F1f2  ')\nb'.\\xf0\\xf1\\xf2'\n  Changed in version 3.7: bytes.fromhex() now skips all ASCII whitespace in the string, not just spaces.  \n A reverse conversion function exists to transform a bytes object into its hexadecimal representation.  \nhex([sep[, bytes_per_sep]])  \nReturn a string object containing two hexadecimal digits for each byte in the instance. >>> b'\\xf0\\xf1\\xf2'.hex()\n'f0f1f2'\n If you want to make the hex string easier to read, you can specify a single character separator sep parameter to include in the output. By default between each byte. A second optional bytes_per_sep parameter controls the spacing. Positive values calculate the separator position from the right, negative values from the left. >>> value = b'\\xf0\\xf1\\xf2'\n>>> value.hex('-')\n'f0-f1-f2'\n>>> value.hex('_', 2)\n'f0_f1f2'\n>>> b'UUDDLRLRAB'.hex(' ', -4)\n'55554444 4c524c52 4142'\n  New in version 3.5.   Changed in version 3.8: bytes.hex() now supports optional sep and bytes_per_sep parameters to insert separators between bytes in the hex output.  \n \n"}, {"name": "bytes", "path": "library/functions#bytes", "type": "Built-in Functions", "text": " \nclass bytes([source[, encoding[, errors]]])  \nReturn a new \u201cbytes\u201d object, which is an immutable sequence of integers in the range 0 <= x < 256. bytes is an immutable version of bytearray \u2013 it has the same non-mutating methods and the same indexing and slicing behavior. Accordingly, constructor arguments are interpreted as for bytearray(). Bytes objects can also be created with literals, see String and Bytes literals. See also Binary Sequence Types \u2014 bytes, bytearray, memoryview, Bytes Objects, and Bytes and Bytearray Operations. \n"}, {"name": "bytes.capitalize()", "path": "library/stdtypes#bytes.capitalize", "type": "Built-in Types", "text": " \nbytes.capitalize()  \nbytearray.capitalize()  \nReturn a copy of the sequence with each byte interpreted as an ASCII character, and the first byte capitalized and the rest lowercased. Non-ASCII byte values are passed through unchanged.  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytes.center()", "path": "library/stdtypes#bytes.center", "type": "Built-in Types", "text": " \nbytes.center(width[, fillbyte])  \nbytearray.center(width[, fillbyte])  \nReturn a copy of the object centered in a sequence of length width. Padding is done using the specified fillbyte (default is an ASCII space). For bytes objects, the original sequence is returned if width is less than or equal to len(s).  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytes.count()", "path": "library/stdtypes#bytes.count", "type": "Built-in Types", "text": " \nbytes.count(sub[, start[, end]])  \nbytearray.count(sub[, start[, end]])  \nReturn the number of non-overlapping occurrences of subsequence sub in the range [start, end]. Optional arguments start and end are interpreted as in slice notation. The subsequence to search for may be any bytes-like object or an integer in the range 0 to 255.  Changed in version 3.3: Also accept an integer in the range 0 to 255 as the subsequence.  \n"}, {"name": "bytes.decode()", "path": "library/stdtypes#bytes.decode", "type": "Built-in Types", "text": " \nbytes.decode(encoding=\"utf-8\", errors=\"strict\")  \nbytearray.decode(encoding=\"utf-8\", errors=\"strict\")  \nReturn a string decoded from the given bytes. Default encoding is 'utf-8'. errors may be given to set a different error handling scheme. The default for errors is 'strict', meaning that encoding errors raise a UnicodeError. Other possible values are 'ignore', 'replace' and any other name registered via codecs.register_error(), see section Error Handlers. For a list of possible encodings, see section Standard Encodings. By default, the errors argument is not checked for best performances, but only used at the first decoding error. Enable the Python Development Mode, or use a debug build to check errors.  Note Passing the encoding argument to str allows decoding any bytes-like object directly, without needing to make a temporary bytes or bytearray object.   Changed in version 3.1: Added support for keyword arguments.   Changed in version 3.9: The errors is now checked in development mode and in debug mode.  \n"}, {"name": "bytes.endswith()", "path": "library/stdtypes#bytes.endswith", "type": "Built-in Types", "text": " \nbytes.endswith(suffix[, start[, end]])  \nbytearray.endswith(suffix[, start[, end]])  \nReturn True if the binary data ends with the specified suffix, otherwise return False. suffix can also be a tuple of suffixes to look for. With optional start, test beginning at that position. With optional end, stop comparing at that position. The suffix(es) to search for may be any bytes-like object. \n"}, {"name": "bytes.expandtabs()", "path": "library/stdtypes#bytes.expandtabs", "type": "Built-in Types", "text": " \nbytes.expandtabs(tabsize=8)  \nbytearray.expandtabs(tabsize=8)  \nReturn a copy of the sequence where all ASCII tab characters are replaced by one or more ASCII spaces, depending on the current column and the given tab size. Tab positions occur every tabsize bytes (default is 8, giving tab positions at columns 0, 8, 16 and so on). To expand the sequence, the current column is set to zero and the sequence is examined byte by byte. If the byte is an ASCII tab character (b'\\t'), one or more space characters are inserted in the result until the current column is equal to the next tab position. (The tab character itself is not copied.) If the current byte is an ASCII newline (b'\\n') or carriage return (b'\\r'), it is copied and the current column is reset to zero. Any other byte value is copied unchanged and the current column is incremented by one regardless of how the byte value is represented when printed: >>> b'01\\t012\\t0123\\t01234'.expandtabs()\nb'01      012     0123    01234'\n>>> b'01\\t012\\t0123\\t01234'.expandtabs(4)\nb'01  012 0123    01234'\n  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytes.find()", "path": "library/stdtypes#bytes.find", "type": "Built-in Types", "text": " \nbytes.find(sub[, start[, end]])  \nbytearray.find(sub[, start[, end]])  \nReturn the lowest index in the data where the subsequence sub is found, such that sub is contained in the slice s[start:end]. Optional arguments start and end are interpreted as in slice notation. Return -1 if sub is not found. The subsequence to search for may be any bytes-like object or an integer in the range 0 to 255.  Note The find() method should be used only if you need to know the position of sub. To check if sub is a substring or not, use the in operator: >>> b'Py' in b'Python'\nTrue\n   Changed in version 3.3: Also accept an integer in the range 0 to 255 as the subsequence.  \n"}, {"name": "bytes.fromhex()", "path": "library/stdtypes#bytes.fromhex", "type": "Built-in Types", "text": " \nclassmethod fromhex(string)  \nThis bytes class method returns a bytes object, decoding the given string object. The string must contain two hexadecimal digits per byte, with ASCII whitespace being ignored. >>> bytes.fromhex('2Ef0 F1f2  ')\nb'.\\xf0\\xf1\\xf2'\n  Changed in version 3.7: bytes.fromhex() now skips all ASCII whitespace in the string, not just spaces.  \n"}, {"name": "bytes.hex()", "path": "library/stdtypes#bytes.hex", "type": "Built-in Types", "text": " \nhex([sep[, bytes_per_sep]])  \nReturn a string object containing two hexadecimal digits for each byte in the instance. >>> b'\\xf0\\xf1\\xf2'.hex()\n'f0f1f2'\n If you want to make the hex string easier to read, you can specify a single character separator sep parameter to include in the output. By default between each byte. A second optional bytes_per_sep parameter controls the spacing. Positive values calculate the separator position from the right, negative values from the left. >>> value = b'\\xf0\\xf1\\xf2'\n>>> value.hex('-')\n'f0-f1-f2'\n>>> value.hex('_', 2)\n'f0_f1f2'\n>>> b'UUDDLRLRAB'.hex(' ', -4)\n'55554444 4c524c52 4142'\n  New in version 3.5.   Changed in version 3.8: bytes.hex() now supports optional sep and bytes_per_sep parameters to insert separators between bytes in the hex output.  \n"}, {"name": "bytes.index()", "path": "library/stdtypes#bytes.index", "type": "Built-in Types", "text": " \nbytes.index(sub[, start[, end]])  \nbytearray.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the subsequence is not found. The subsequence to search for may be any bytes-like object or an integer in the range 0 to 255.  Changed in version 3.3: Also accept an integer in the range 0 to 255 as the subsequence.  \n"}, {"name": "bytes.isalnum()", "path": "library/stdtypes#bytes.isalnum", "type": "Built-in Types", "text": " \nbytes.isalnum()  \nbytearray.isalnum()  \nReturn True if all bytes in the sequence are alphabetical ASCII characters or ASCII decimal digits and the sequence is not empty, False otherwise. Alphabetic ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'. ASCII decimal digits are those byte values in the sequence b'0123456789'. For example: >>> b'ABCabc1'.isalnum()\nTrue\n>>> b'ABC abc1'.isalnum()\nFalse\n \n"}, {"name": "bytes.isalpha()", "path": "library/stdtypes#bytes.isalpha", "type": "Built-in Types", "text": " \nbytes.isalpha()  \nbytearray.isalpha()  \nReturn True if all bytes in the sequence are alphabetic ASCII characters and the sequence is not empty, False otherwise. Alphabetic ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'. For example: >>> b'ABCabc'.isalpha()\nTrue\n>>> b'ABCabc1'.isalpha()\nFalse\n \n"}, {"name": "bytes.isascii()", "path": "library/stdtypes#bytes.isascii", "type": "Built-in Types", "text": " \nbytes.isascii()  \nbytearray.isascii()  \nReturn True if the sequence is empty or all bytes in the sequence are ASCII, False otherwise. ASCII bytes are in the range 0-0x7F.  New in version 3.7.  \n"}, {"name": "bytes.isdigit()", "path": "library/stdtypes#bytes.isdigit", "type": "Built-in Types", "text": " \nbytes.isdigit()  \nbytearray.isdigit()  \nReturn True if all bytes in the sequence are ASCII decimal digits and the sequence is not empty, False otherwise. ASCII decimal digits are those byte values in the sequence b'0123456789'. For example: >>> b'1234'.isdigit()\nTrue\n>>> b'1.23'.isdigit()\nFalse\n \n"}, {"name": "bytes.islower()", "path": "library/stdtypes#bytes.islower", "type": "Built-in Types", "text": " \nbytes.islower()  \nbytearray.islower()  \nReturn True if there is at least one lowercase ASCII character in the sequence and no uppercase ASCII characters, False otherwise. For example: >>> b'hello world'.islower()\nTrue\n>>> b'Hello world'.islower()\nFalse\n Lowercase ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyz'. Uppercase ASCII characters are those byte values in the sequence b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'. \n"}, {"name": "bytes.isspace()", "path": "library/stdtypes#bytes.isspace", "type": "Built-in Types", "text": " \nbytes.isspace()  \nbytearray.isspace()  \nReturn True if all bytes in the sequence are ASCII whitespace and the sequence is not empty, False otherwise. ASCII whitespace characters are those byte values in the sequence b' \\t\\n\\r\\x0b\\f' (space, tab, newline, carriage return, vertical tab, form feed). \n"}, {"name": "bytes.istitle()", "path": "library/stdtypes#bytes.istitle", "type": "Built-in Types", "text": " \nbytes.istitle()  \nbytearray.istitle()  \nReturn True if the sequence is ASCII titlecase and the sequence is not empty, False otherwise. See bytes.title() for more details on the definition of \u201ctitlecase\u201d. For example: >>> b'Hello World'.istitle()\nTrue\n>>> b'Hello world'.istitle()\nFalse\n \n"}, {"name": "bytes.isupper()", "path": "library/stdtypes#bytes.isupper", "type": "Built-in Types", "text": " \nbytes.isupper()  \nbytearray.isupper()  \nReturn True if there is at least one uppercase alphabetic ASCII character in the sequence and no lowercase ASCII characters, False otherwise. For example: >>> b'HELLO WORLD'.isupper()\nTrue\n>>> b'Hello world'.isupper()\nFalse\n Lowercase ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyz'. Uppercase ASCII characters are those byte values in the sequence b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'. \n"}, {"name": "bytes.join()", "path": "library/stdtypes#bytes.join", "type": "Built-in Types", "text": " \nbytes.join(iterable)  \nbytearray.join(iterable)  \nReturn a bytes or bytearray object which is the concatenation of the binary data sequences in iterable. A TypeError will be raised if there are any values in iterable that are not bytes-like objects, including str objects. The separator between elements is the contents of the bytes or bytearray object providing this method. \n"}, {"name": "bytes.ljust()", "path": "library/stdtypes#bytes.ljust", "type": "Built-in Types", "text": " \nbytes.ljust(width[, fillbyte])  \nbytearray.ljust(width[, fillbyte])  \nReturn a copy of the object left justified in a sequence of length width. Padding is done using the specified fillbyte (default is an ASCII space). For bytes objects, the original sequence is returned if width is less than or equal to len(s).  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytes.lower()", "path": "library/stdtypes#bytes.lower", "type": "Built-in Types", "text": " \nbytes.lower()  \nbytearray.lower()  \nReturn a copy of the sequence with all the uppercase ASCII characters converted to their corresponding lowercase counterpart. For example: >>> b'Hello World'.lower()\nb'hello world'\n Lowercase ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyz'. Uppercase ASCII characters are those byte values in the sequence b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'.  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytes.lstrip()", "path": "library/stdtypes#bytes.lstrip", "type": "Built-in Types", "text": " \nbytes.lstrip([chars])  \nbytearray.lstrip([chars])  \nReturn a copy of the sequence with specified leading bytes removed. The chars argument is a binary sequence specifying the set of byte values to be removed - the name refers to the fact this method is usually used with ASCII characters. If omitted or None, the chars argument defaults to removing ASCII whitespace. The chars argument is not a prefix; rather, all combinations of its values are stripped: >>> b'   spacious   '.lstrip()\nb'spacious   '\n>>> b'www.example.com'.lstrip(b'cmowz.')\nb'example.com'\n The binary sequence of byte values to remove may be any bytes-like object. See removeprefix() for a method that will remove a single prefix string rather than all of a set of characters. For example: >>> b'Arthur: three!'.lstrip(b'Arthur: ')\nb'ee!'\n>>> b'Arthur: three!'.removeprefix(b'Arthur: ')\nb'three!'\n  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytes.maketrans()", "path": "library/stdtypes#bytes.maketrans", "type": "Built-in Types", "text": " \nstatic bytes.maketrans(from, to)  \nstatic bytearray.maketrans(from, to)  \nThis static method returns a translation table usable for bytes.translate() that will map each character in from into the character at the same position in to; from and to must both be bytes-like objects and have the same length.  New in version 3.1.  \n"}, {"name": "bytes.partition()", "path": "library/stdtypes#bytes.partition", "type": "Built-in Types", "text": " \nbytes.partition(sep)  \nbytearray.partition(sep)  \nSplit the sequence at the first occurrence of sep, and return a 3-tuple containing the part before the separator, the separator itself or its bytearray copy, and the part after the separator. If the separator is not found, return a 3-tuple containing a copy of the original sequence, followed by two empty bytes or bytearray objects. The separator to search for may be any bytes-like object. \n"}, {"name": "bytes.removeprefix()", "path": "library/stdtypes#bytes.removeprefix", "type": "Built-in Types", "text": " \nbytes.removeprefix(prefix, /)  \nbytearray.removeprefix(prefix, /)  \nIf the binary data starts with the prefix string, return bytes[len(prefix):]. Otherwise, return a copy of the original binary data: >>> b'TestHook'.removeprefix(b'Test')\nb'Hook'\n>>> b'BaseTestCase'.removeprefix(b'Test')\nb'BaseTestCase'\n The prefix may be any bytes-like object.  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.   New in version 3.9.  \n"}, {"name": "bytes.removesuffix()", "path": "library/stdtypes#bytes.removesuffix", "type": "Built-in Types", "text": " \nbytes.removesuffix(suffix, /)  \nbytearray.removesuffix(suffix, /)  \nIf the binary data ends with the suffix string and that suffix is not empty, return bytes[:-len(suffix)]. Otherwise, return a copy of the original binary data: >>> b'MiscTests'.removesuffix(b'Tests')\nb'Misc'\n>>> b'TmpDirMixin'.removesuffix(b'Tests')\nb'TmpDirMixin'\n The suffix may be any bytes-like object.  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.   New in version 3.9.  \n"}, {"name": "bytes.replace()", "path": "library/stdtypes#bytes.replace", "type": "Built-in Types", "text": " \nbytes.replace(old, new[, count])  \nbytearray.replace(old, new[, count])  \nReturn a copy of the sequence with all occurrences of subsequence old replaced by new. If the optional argument count is given, only the first count occurrences are replaced. The subsequence to search for and its replacement may be any bytes-like object.  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytes.rfind()", "path": "library/stdtypes#bytes.rfind", "type": "Built-in Types", "text": " \nbytes.rfind(sub[, start[, end]])  \nbytearray.rfind(sub[, start[, end]])  \nReturn the highest index in the sequence where the subsequence sub is found, such that sub is contained within s[start:end]. Optional arguments start and end are interpreted as in slice notation. Return -1 on failure. The subsequence to search for may be any bytes-like object or an integer in the range 0 to 255.  Changed in version 3.3: Also accept an integer in the range 0 to 255 as the subsequence.  \n"}, {"name": "bytes.rindex()", "path": "library/stdtypes#bytes.rindex", "type": "Built-in Types", "text": " \nbytes.rindex(sub[, start[, end]])  \nbytearray.rindex(sub[, start[, end]])  \nLike rfind() but raises ValueError when the subsequence sub is not found. The subsequence to search for may be any bytes-like object or an integer in the range 0 to 255.  Changed in version 3.3: Also accept an integer in the range 0 to 255 as the subsequence.  \n"}, {"name": "bytes.rjust()", "path": "library/stdtypes#bytes.rjust", "type": "Built-in Types", "text": " \nbytes.rjust(width[, fillbyte])  \nbytearray.rjust(width[, fillbyte])  \nReturn a copy of the object right justified in a sequence of length width. Padding is done using the specified fillbyte (default is an ASCII space). For bytes objects, the original sequence is returned if width is less than or equal to len(s).  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytes.rpartition()", "path": "library/stdtypes#bytes.rpartition", "type": "Built-in Types", "text": " \nbytes.rpartition(sep)  \nbytearray.rpartition(sep)  \nSplit the sequence at the last occurrence of sep, and return a 3-tuple containing the part before the separator, the separator itself or its bytearray copy, and the part after the separator. If the separator is not found, return a 3-tuple containing two empty bytes or bytearray objects, followed by a copy of the original sequence. The separator to search for may be any bytes-like object. \n"}, {"name": "bytes.rsplit()", "path": "library/stdtypes#bytes.rsplit", "type": "Built-in Types", "text": " \nbytes.rsplit(sep=None, maxsplit=-1)  \nbytearray.rsplit(sep=None, maxsplit=-1)  \nSplit the binary sequence into subsequences of the same type, using sep as the delimiter string. If maxsplit is given, at most maxsplit splits are done, the rightmost ones. If sep is not specified or None, any subsequence consisting solely of ASCII whitespace is a separator. Except for splitting from the right, rsplit() behaves like split() which is described in detail below. \n"}, {"name": "bytes.rstrip()", "path": "library/stdtypes#bytes.rstrip", "type": "Built-in Types", "text": " \nbytes.rstrip([chars])  \nbytearray.rstrip([chars])  \nReturn a copy of the sequence with specified trailing bytes removed. The chars argument is a binary sequence specifying the set of byte values to be removed - the name refers to the fact this method is usually used with ASCII characters. If omitted or None, the chars argument defaults to removing ASCII whitespace. The chars argument is not a suffix; rather, all combinations of its values are stripped: >>> b'   spacious   '.rstrip()\nb'   spacious'\n>>> b'mississippi'.rstrip(b'ipz')\nb'mississ'\n The binary sequence of byte values to remove may be any bytes-like object. See removesuffix() for a method that will remove a single suffix string rather than all of a set of characters. For example: >>> b'Monty Python'.rstrip(b' Python')\nb'M'\n>>> b'Monty Python'.removesuffix(b' Python')\nb'Monty'\n  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytes.split()", "path": "library/stdtypes#bytes.split", "type": "Built-in Types", "text": " \nbytes.split(sep=None, maxsplit=-1)  \nbytearray.split(sep=None, maxsplit=-1)  \nSplit the binary sequence into subsequences of the same type, using sep as the delimiter string. If maxsplit is given and non-negative, at most maxsplit splits are done (thus, the list will have at most maxsplit+1 elements). If maxsplit is not specified or is -1, then there is no limit on the number of splits (all possible splits are made). If sep is given, consecutive delimiters are not grouped together and are deemed to delimit empty subsequences (for example, b'1,,2'.split(b',') returns [b'1', b'', b'2']). The sep argument may consist of a multibyte sequence (for example, b'1<>2<>3'.split(b'<>') returns [b'1', b'2', b'3']). Splitting an empty sequence with a specified separator returns [b''] or [bytearray(b'')] depending on the type of object being split. The sep argument may be any bytes-like object. For example: >>> b'1,2,3'.split(b',')\n[b'1', b'2', b'3']\n>>> b'1,2,3'.split(b',', maxsplit=1)\n[b'1', b'2,3']\n>>> b'1,2,,3,'.split(b',')\n[b'1', b'2', b'', b'3', b'']\n If sep is not specified or is None, a different splitting algorithm is applied: runs of consecutive ASCII whitespace are regarded as a single separator, and the result will contain no empty strings at the start or end if the sequence has leading or trailing whitespace. Consequently, splitting an empty sequence or a sequence consisting solely of ASCII whitespace without a specified separator returns []. For example: >>> b'1 2 3'.split()\n[b'1', b'2', b'3']\n>>> b'1 2 3'.split(maxsplit=1)\n[b'1', b'2 3']\n>>> b'   1   2   3   '.split()\n[b'1', b'2', b'3']\n \n"}, {"name": "bytes.splitlines()", "path": "library/stdtypes#bytes.splitlines", "type": "Built-in Types", "text": " \nbytes.splitlines(keepends=False)  \nbytearray.splitlines(keepends=False)  \nReturn a list of the lines in the binary sequence, breaking at ASCII line boundaries. This method uses the universal newlines approach to splitting lines. Line breaks are not included in the resulting list unless keepends is given and true. For example: >>> b'ab c\\n\\nde fg\\rkl\\r\\n'.splitlines()\n[b'ab c', b'', b'de fg', b'kl']\n>>> b'ab c\\n\\nde fg\\rkl\\r\\n'.splitlines(keepends=True)\n[b'ab c\\n', b'\\n', b'de fg\\r', b'kl\\r\\n']\n Unlike split() when a delimiter string sep is given, this method returns an empty list for the empty string, and a terminal line break does not result in an extra line: >>> b\"\".split(b'\\n'), b\"Two lines\\n\".split(b'\\n')\n([b''], [b'Two lines', b''])\n>>> b\"\".splitlines(), b\"One line\\n\".splitlines()\n([], [b'One line'])\n \n"}, {"name": "bytes.startswith()", "path": "library/stdtypes#bytes.startswith", "type": "Built-in Types", "text": " \nbytes.startswith(prefix[, start[, end]])  \nbytearray.startswith(prefix[, start[, end]])  \nReturn True if the binary data starts with the specified prefix, otherwise return False. prefix can also be a tuple of prefixes to look for. With optional start, test beginning at that position. With optional end, stop comparing at that position. The prefix(es) to search for may be any bytes-like object. \n"}, {"name": "bytes.strip()", "path": "library/stdtypes#bytes.strip", "type": "Built-in Types", "text": " \nbytes.strip([chars])  \nbytearray.strip([chars])  \nReturn a copy of the sequence with specified leading and trailing bytes removed. The chars argument is a binary sequence specifying the set of byte values to be removed - the name refers to the fact this method is usually used with ASCII characters. If omitted or None, the chars argument defaults to removing ASCII whitespace. The chars argument is not a prefix or suffix; rather, all combinations of its values are stripped: >>> b'   spacious   '.strip()\nb'spacious'\n>>> b'www.example.com'.strip(b'cmowz.')\nb'example'\n The binary sequence of byte values to remove may be any bytes-like object.  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytes.swapcase()", "path": "library/stdtypes#bytes.swapcase", "type": "Built-in Types", "text": " \nbytes.swapcase()  \nbytearray.swapcase()  \nReturn a copy of the sequence with all the lowercase ASCII characters converted to their corresponding uppercase counterpart and vice-versa. For example: >>> b'Hello World'.swapcase()\nb'hELLO wORLD'\n Lowercase ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyz'. Uppercase ASCII characters are those byte values in the sequence b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'. Unlike str.swapcase(), it is always the case that bin.swapcase().swapcase() == bin for the binary versions. Case conversions are symmetrical in ASCII, even though that is not generally true for arbitrary Unicode code points.  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytes.title()", "path": "library/stdtypes#bytes.title", "type": "Built-in Types", "text": " \nbytes.title()  \nbytearray.title()  \nReturn a titlecased version of the binary sequence where words start with an uppercase ASCII character and the remaining characters are lowercase. Uncased byte values are left unmodified. For example: >>> b'Hello world'.title()\nb'Hello World'\n Lowercase ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyz'. Uppercase ASCII characters are those byte values in the sequence b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'. All other byte values are uncased. The algorithm uses a simple language-independent definition of a word as groups of consecutive letters. The definition works in many contexts but it means that apostrophes in contractions and possessives form word boundaries, which may not be the desired result: >>> b\"they're bill's friends from the UK\".title()\nb\"They'Re Bill'S Friends From The Uk\"\n A workaround for apostrophes can be constructed using regular expressions: >>> import re\n>>> def titlecase(s):\n...     return re.sub(rb\"[A-Za-z]+('[A-Za-z]+)?\",\n...                   lambda mo: mo.group(0)[0:1].upper() +\n...                              mo.group(0)[1:].lower(),\n...                   s)\n...\n>>> titlecase(b\"they're bill's friends.\")\nb\"They're Bill's Friends.\"\n  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytes.translate()", "path": "library/stdtypes#bytes.translate", "type": "Built-in Types", "text": " \nbytes.translate(table, /, delete=b'')  \nbytearray.translate(table, /, delete=b'')  \nReturn a copy of the bytes or bytearray object where all bytes occurring in the optional argument delete are removed, and the remaining bytes have been mapped through the given translation table, which must be a bytes object of length 256. You can use the bytes.maketrans() method to create a translation table. Set the table argument to None for translations that only delete characters: >>> b'read this short text'.translate(None, b'aeiou')\nb'rd ths shrt txt'\n  Changed in version 3.6: delete is now supported as a keyword argument.  \n"}, {"name": "bytes.upper()", "path": "library/stdtypes#bytes.upper", "type": "Built-in Types", "text": " \nbytes.upper()  \nbytearray.upper()  \nReturn a copy of the sequence with all the lowercase ASCII characters converted to their corresponding uppercase counterpart. For example: >>> b'Hello World'.upper()\nb'HELLO WORLD'\n Lowercase ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyz'. Uppercase ASCII characters are those byte values in the sequence b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'.  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "bytes.zfill()", "path": "library/stdtypes#bytes.zfill", "type": "Built-in Types", "text": " \nbytes.zfill(width)  \nbytearray.zfill(width)  \nReturn a copy of the sequence left filled with ASCII b'0' digits to make a sequence of length width. A leading sign prefix (b'+'/ b'-') is handled by inserting the padding after the sign character rather than before. For bytes objects, the original sequence is returned if width is less than or equal to len(seq). For example: >>> b\"42\".zfill(5)\nb'00042'\n>>> b\"-42\".zfill(5)\nb'-0042'\n  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.  \n"}, {"name": "BytesWarning", "path": "library/exceptions#BytesWarning", "type": "Built-in Exceptions", "text": " \nexception BytesWarning  \nBase class for warnings related to bytes and bytearray. \n"}, {"name": "bz2", "path": "library/bz2", "type": "Data Compression", "text": "bz2 \u2014 Support for bzip2 compression Source code: Lib/bz2.py This module provides a comprehensive interface for compressing and decompressing data using the bzip2 compression algorithm. The bz2 module contains:  The open() function and BZ2File class for reading and writing compressed files. The BZ2Compressor and BZ2Decompressor classes for incremental (de)compression. The compress() and decompress() functions for one-shot (de)compression.  All of the classes in this module may safely be accessed from multiple threads. (De)compression of files  \nbz2.open(filename, mode='rb', compresslevel=9, encoding=None, errors=None, newline=None)  \nOpen a bzip2-compressed file in binary or text mode, returning a file object. As with the constructor for BZ2File, the filename argument can be an actual filename (a str or bytes object), or an existing file object to read from or write to. The mode argument can be any of 'r', 'rb', 'w', 'wb', 'x', 'xb', 'a' or 'ab' for binary mode, or 'rt', 'wt', 'xt', or 'at' for text mode. The default is 'rb'. The compresslevel argument is an integer from 1 to 9, as for the BZ2File constructor. For binary mode, this function is equivalent to the BZ2File constructor: BZ2File(filename, mode, compresslevel=compresslevel). In this case, the encoding, errors and newline arguments must not be provided. For text mode, a BZ2File object is created, and wrapped in an io.TextIOWrapper instance with the specified encoding, error handling behavior, and line ending(s).  New in version 3.3.   Changed in version 3.4: The 'x' (exclusive creation) mode was added.   Changed in version 3.6: Accepts a path-like object.  \n  \nclass bz2.BZ2File(filename, mode='r', *, compresslevel=9)  \nOpen a bzip2-compressed file in binary mode. If filename is a str or bytes object, open the named file directly. Otherwise, filename should be a file object, which will be used to read or write the compressed data. The mode argument can be either 'r' for reading (default), 'w' for overwriting, 'x' for exclusive creation, or 'a' for appending. These can equivalently be given as 'rb', 'wb', 'xb' and 'ab' respectively. If filename is a file object (rather than an actual file name), a mode of 'w' does not truncate the file, and is instead equivalent to 'a'. If mode is 'w' or 'a', compresslevel can be an integer between 1 and 9 specifying the level of compression: 1 produces the least compression, and 9 (default) produces the most compression. If mode is 'r', the input file may be the concatenation of multiple compressed streams. BZ2File provides all of the members specified by the io.BufferedIOBase, except for detach() and truncate(). Iteration and the with statement are supported. BZ2File also provides the following method:  \npeek([n])  \nReturn buffered data without advancing the file position. At least one byte of data will be returned (unless at EOF). The exact number of bytes returned is unspecified.  Note While calling peek() does not change the file position of the BZ2File, it may change the position of the underlying file object (e.g. if the BZ2File was constructed by passing a file object for filename).   New in version 3.3.  \n  Changed in version 3.1: Support for the with statement was added.   Changed in version 3.3: The fileno(), readable(), seekable(), writable(), read1() and readinto() methods were added.   Changed in version 3.3: Support was added for filename being a file object instead of an actual filename.   Changed in version 3.3: The 'a' (append) mode was added, along with support for reading multi-stream files.   Changed in version 3.4: The 'x' (exclusive creation) mode was added.   Changed in version 3.5: The read() method now accepts an argument of None.   Changed in version 3.6: Accepts a path-like object.   Changed in version 3.9: The buffering parameter has been removed. It was ignored and deprecated since Python 3.0. Pass an open file object to control how the file is opened. The compresslevel parameter became keyword-only.  \n Incremental (de)compression  \nclass bz2.BZ2Compressor(compresslevel=9)  \nCreate a new compressor object. This object may be used to compress data incrementally. For one-shot compression, use the compress() function instead. compresslevel, if given, must be an integer between 1 and 9. The default is 9.  \ncompress(data)  \nProvide data to the compressor object. Returns a chunk of compressed data if possible, or an empty byte string otherwise. When you have finished providing data to the compressor, call the flush() method to finish the compression process. \n  \nflush()  \nFinish the compression process. Returns the compressed data left in internal buffers. The compressor object may not be used after this method has been called. \n \n  \nclass bz2.BZ2Decompressor  \nCreate a new decompressor object. This object may be used to decompress data incrementally. For one-shot compression, use the decompress() function instead.  Note This class does not transparently handle inputs containing multiple compressed streams, unlike decompress() and BZ2File. If you need to decompress a multi-stream input with BZ2Decompressor, you must use a new decompressor for each stream.   \ndecompress(data, max_length=-1)  \nDecompress data (a bytes-like object), returning uncompressed data as bytes. Some of data may be buffered internally, for use in later calls to decompress(). The returned data should be concatenated with the output of any previous calls to decompress(). If max_length is nonnegative, returns at most max_length bytes of decompressed data. If this limit is reached and further output can be produced, the needs_input attribute will be set to False. In this case, the next call to decompress() may provide data as b'' to obtain more of the output. If all of the input data was decompressed and returned (either because this was less than max_length bytes, or because max_length was negative), the needs_input attribute will be set to True. Attempting to decompress data after the end of stream is reached raises an EOFError. Any data found after the end of the stream is ignored and saved in the unused_data attribute.  Changed in version 3.5: Added the max_length parameter.  \n  \neof  \nTrue if the end-of-stream marker has been reached.  New in version 3.3.  \n  \nunused_data  \nData found after the end of the compressed stream. If this attribute is accessed before the end of the stream has been reached, its value will be b''. \n  \nneeds_input  \nFalse if the decompress() method can provide more decompressed data before requiring new uncompressed input.  New in version 3.5.  \n \n One-shot (de)compression  \nbz2.compress(data, compresslevel=9)  \nCompress data, a bytes-like object. compresslevel, if given, must be an integer between 1 and 9. The default is 9. For incremental compression, use a BZ2Compressor instead. \n  \nbz2.decompress(data)  \nDecompress data, a bytes-like object. If data is the concatenation of multiple compressed streams, decompress all of the streams. For incremental decompression, use a BZ2Decompressor instead.  Changed in version 3.3: Support for multi-stream inputs was added.  \n Examples of usage Below are some examples of typical usage of the bz2 module. Using compress() and decompress() to demonstrate round-trip compression: >>> import bz2\n>>> data = b\"\"\"\\\n... Donec rhoncus quis sapien sit amet molestie. Fusce scelerisque vel augue\n... nec ullamcorper. Nam rutrum pretium placerat. Aliquam vel tristique lorem,\n... sit amet cursus ante. In interdum laoreet mi, sit amet ultrices purus\n... pulvinar a. Nam gravida euismod magna, non varius justo tincidunt feugiat.\n... Aliquam pharetra lacus non risus vehicula rutrum. Maecenas aliquam leo\n... felis. Pellentesque semper nunc sit amet nibh ullamcorper, ac elementum\n... dolor luctus. Curabitur lacinia mi ornare consectetur vestibulum.\"\"\"\n>>> c = bz2.compress(data)\n>>> len(data) / len(c)  # Data compression ratio\n1.513595166163142\n>>> d = bz2.decompress(c)\n>>> data == d  # Check equality to original object after round-trip\nTrue\n Using BZ2Compressor for incremental compression: >>> import bz2\n>>> def gen_data(chunks=10, chunksize=1000):\n...     \"\"\"Yield incremental blocks of chunksize bytes.\"\"\"\n...     for _ in range(chunks):\n...         yield b\"z\" * chunksize\n...\n>>> comp = bz2.BZ2Compressor()\n>>> out = b\"\"\n>>> for chunk in gen_data():\n...     # Provide data to the compressor object\n...     out = out + comp.compress(chunk)\n...\n>>> # Finish the compression process.  Call this once you have\n>>> # finished providing data to the compressor.\n>>> out = out + comp.flush()\n The example above uses a very \u201cnonrandom\u201d stream of data (a stream of b\u201dz\u201d chunks). Random data tends to compress poorly, while ordered, repetitive data usually yields a high compression ratio. Writing and reading a bzip2-compressed file in binary mode: >>> import bz2\n>>> data = b\"\"\"\\\n... Donec rhoncus quis sapien sit amet molestie. Fusce scelerisque vel augue\n... nec ullamcorper. Nam rutrum pretium placerat. Aliquam vel tristique lorem,\n... sit amet cursus ante. In interdum laoreet mi, sit amet ultrices purus\n... pulvinar a. Nam gravida euismod magna, non varius justo tincidunt feugiat.\n... Aliquam pharetra lacus non risus vehicula rutrum. Maecenas aliquam leo\n... felis. Pellentesque semper nunc sit amet nibh ullamcorper, ac elementum\n... dolor luctus. Curabitur lacinia mi ornare consectetur vestibulum.\"\"\"\n>>> with bz2.open(\"myfile.bz2\", \"wb\") as f:\n...     # Write compressed data to file\n...     unused = f.write(data)\n>>> with bz2.open(\"myfile.bz2\", \"rb\") as f:\n...     # Decompress data from file\n...     content = f.read()\n>>> content == data  # Check equality to original object after round-trip\nTrue\n\n"}, {"name": "bz2.BZ2Compressor", "path": "library/bz2#bz2.BZ2Compressor", "type": "Data Compression", "text": " \nclass bz2.BZ2Compressor(compresslevel=9)  \nCreate a new compressor object. This object may be used to compress data incrementally. For one-shot compression, use the compress() function instead. compresslevel, if given, must be an integer between 1 and 9. The default is 9.  \ncompress(data)  \nProvide data to the compressor object. Returns a chunk of compressed data if possible, or an empty byte string otherwise. When you have finished providing data to the compressor, call the flush() method to finish the compression process. \n  \nflush()  \nFinish the compression process. Returns the compressed data left in internal buffers. The compressor object may not be used after this method has been called. \n \n"}, {"name": "bz2.BZ2Compressor.compress()", "path": "library/bz2#bz2.BZ2Compressor.compress", "type": "Data Compression", "text": " \ncompress(data)  \nProvide data to the compressor object. Returns a chunk of compressed data if possible, or an empty byte string otherwise. When you have finished providing data to the compressor, call the flush() method to finish the compression process. \n"}, {"name": "bz2.BZ2Compressor.flush()", "path": "library/bz2#bz2.BZ2Compressor.flush", "type": "Data Compression", "text": " \nflush()  \nFinish the compression process. Returns the compressed data left in internal buffers. The compressor object may not be used after this method has been called. \n"}, {"name": "bz2.BZ2Decompressor", "path": "library/bz2#bz2.BZ2Decompressor", "type": "Data Compression", "text": " \nclass bz2.BZ2Decompressor  \nCreate a new decompressor object. This object may be used to decompress data incrementally. For one-shot compression, use the decompress() function instead.  Note This class does not transparently handle inputs containing multiple compressed streams, unlike decompress() and BZ2File. If you need to decompress a multi-stream input with BZ2Decompressor, you must use a new decompressor for each stream.   \ndecompress(data, max_length=-1)  \nDecompress data (a bytes-like object), returning uncompressed data as bytes. Some of data may be buffered internally, for use in later calls to decompress(). The returned data should be concatenated with the output of any previous calls to decompress(). If max_length is nonnegative, returns at most max_length bytes of decompressed data. If this limit is reached and further output can be produced, the needs_input attribute will be set to False. In this case, the next call to decompress() may provide data as b'' to obtain more of the output. If all of the input data was decompressed and returned (either because this was less than max_length bytes, or because max_length was negative), the needs_input attribute will be set to True. Attempting to decompress data after the end of stream is reached raises an EOFError. Any data found after the end of the stream is ignored and saved in the unused_data attribute.  Changed in version 3.5: Added the max_length parameter.  \n  \neof  \nTrue if the end-of-stream marker has been reached.  New in version 3.3.  \n  \nunused_data  \nData found after the end of the compressed stream. If this attribute is accessed before the end of the stream has been reached, its value will be b''. \n  \nneeds_input  \nFalse if the decompress() method can provide more decompressed data before requiring new uncompressed input.  New in version 3.5.  \n \n"}, {"name": "bz2.BZ2Decompressor.decompress()", "path": "library/bz2#bz2.BZ2Decompressor.decompress", "type": "Data Compression", "text": " \ndecompress(data, max_length=-1)  \nDecompress data (a bytes-like object), returning uncompressed data as bytes. Some of data may be buffered internally, for use in later calls to decompress(). The returned data should be concatenated with the output of any previous calls to decompress(). If max_length is nonnegative, returns at most max_length bytes of decompressed data. If this limit is reached and further output can be produced, the needs_input attribute will be set to False. In this case, the next call to decompress() may provide data as b'' to obtain more of the output. If all of the input data was decompressed and returned (either because this was less than max_length bytes, or because max_length was negative), the needs_input attribute will be set to True. Attempting to decompress data after the end of stream is reached raises an EOFError. Any data found after the end of the stream is ignored and saved in the unused_data attribute.  Changed in version 3.5: Added the max_length parameter.  \n"}, {"name": "bz2.BZ2Decompressor.eof", "path": "library/bz2#bz2.BZ2Decompressor.eof", "type": "Data Compression", "text": " \neof  \nTrue if the end-of-stream marker has been reached.  New in version 3.3.  \n"}, {"name": "bz2.BZ2Decompressor.needs_input", "path": "library/bz2#bz2.BZ2Decompressor.needs_input", "type": "Data Compression", "text": " \nneeds_input  \nFalse if the decompress() method can provide more decompressed data before requiring new uncompressed input.  New in version 3.5.  \n"}, {"name": "bz2.BZ2Decompressor.unused_data", "path": "library/bz2#bz2.BZ2Decompressor.unused_data", "type": "Data Compression", "text": " \nunused_data  \nData found after the end of the compressed stream. If this attribute is accessed before the end of the stream has been reached, its value will be b''. \n"}, {"name": "bz2.BZ2File", "path": "library/bz2#bz2.BZ2File", "type": "Data Compression", "text": " \nclass bz2.BZ2File(filename, mode='r', *, compresslevel=9)  \nOpen a bzip2-compressed file in binary mode. If filename is a str or bytes object, open the named file directly. Otherwise, filename should be a file object, which will be used to read or write the compressed data. The mode argument can be either 'r' for reading (default), 'w' for overwriting, 'x' for exclusive creation, or 'a' for appending. These can equivalently be given as 'rb', 'wb', 'xb' and 'ab' respectively. If filename is a file object (rather than an actual file name), a mode of 'w' does not truncate the file, and is instead equivalent to 'a'. If mode is 'w' or 'a', compresslevel can be an integer between 1 and 9 specifying the level of compression: 1 produces the least compression, and 9 (default) produces the most compression. If mode is 'r', the input file may be the concatenation of multiple compressed streams. BZ2File provides all of the members specified by the io.BufferedIOBase, except for detach() and truncate(). Iteration and the with statement are supported. BZ2File also provides the following method:  \npeek([n])  \nReturn buffered data without advancing the file position. At least one byte of data will be returned (unless at EOF). The exact number of bytes returned is unspecified.  Note While calling peek() does not change the file position of the BZ2File, it may change the position of the underlying file object (e.g. if the BZ2File was constructed by passing a file object for filename).   New in version 3.3.  \n  Changed in version 3.1: Support for the with statement was added.   Changed in version 3.3: The fileno(), readable(), seekable(), writable(), read1() and readinto() methods were added.   Changed in version 3.3: Support was added for filename being a file object instead of an actual filename.   Changed in version 3.3: The 'a' (append) mode was added, along with support for reading multi-stream files.   Changed in version 3.4: The 'x' (exclusive creation) mode was added.   Changed in version 3.5: The read() method now accepts an argument of None.   Changed in version 3.6: Accepts a path-like object.   Changed in version 3.9: The buffering parameter has been removed. It was ignored and deprecated since Python 3.0. Pass an open file object to control how the file is opened. The compresslevel parameter became keyword-only.  \n"}, {"name": "bz2.BZ2File.peek()", "path": "library/bz2#bz2.BZ2File.peek", "type": "Data Compression", "text": " \npeek([n])  \nReturn buffered data without advancing the file position. At least one byte of data will be returned (unless at EOF). The exact number of bytes returned is unspecified.  Note While calling peek() does not change the file position of the BZ2File, it may change the position of the underlying file object (e.g. if the BZ2File was constructed by passing a file object for filename).   New in version 3.3.  \n"}, {"name": "bz2.compress()", "path": "library/bz2#bz2.compress", "type": "Data Compression", "text": " \nbz2.compress(data, compresslevel=9)  \nCompress data, a bytes-like object. compresslevel, if given, must be an integer between 1 and 9. The default is 9. For incremental compression, use a BZ2Compressor instead. \n"}, {"name": "bz2.decompress()", "path": "library/bz2#bz2.decompress", "type": "Data Compression", "text": " \nbz2.decompress(data)  \nDecompress data, a bytes-like object. If data is the concatenation of multiple compressed streams, decompress all of the streams. For incremental decompression, use a BZ2Decompressor instead.  Changed in version 3.3: Support for multi-stream inputs was added.  \n"}, {"name": "bz2.open()", "path": "library/bz2#bz2.open", "type": "Data Compression", "text": " \nbz2.open(filename, mode='rb', compresslevel=9, encoding=None, errors=None, newline=None)  \nOpen a bzip2-compressed file in binary or text mode, returning a file object. As with the constructor for BZ2File, the filename argument can be an actual filename (a str or bytes object), or an existing file object to read from or write to. The mode argument can be any of 'r', 'rb', 'w', 'wb', 'x', 'xb', 'a' or 'ab' for binary mode, or 'rt', 'wt', 'xt', or 'at' for text mode. The default is 'rb'. The compresslevel argument is an integer from 1 to 9, as for the BZ2File constructor. For binary mode, this function is equivalent to the BZ2File constructor: BZ2File(filename, mode, compresslevel=compresslevel). In this case, the encoding, errors and newline arguments must not be provided. For text mode, a BZ2File object is created, and wrapped in an io.TextIOWrapper instance with the specified encoding, error handling behavior, and line ending(s).  New in version 3.3.   Changed in version 3.4: The 'x' (exclusive creation) mode was added.   Changed in version 3.6: Accepts a path-like object.  \n"}, {"name": "calendar", "path": "library/calendar", "type": "Data Types", "text": "calendar \u2014 General calendar-related functions Source code: Lib/calendar.py This module allows you to output calendars like the Unix cal program, and provides additional useful functions related to the calendar. By default, these calendars have Monday as the first day of the week, and Sunday as the last (the European convention). Use setfirstweekday() to set the first day of the week to Sunday (6) or to any other weekday. Parameters that specify dates are given as integers. For related functionality, see also the datetime and time modules. The functions and classes defined in this module use an idealized calendar, the current Gregorian calendar extended indefinitely in both directions. This matches the definition of the \u201cproleptic Gregorian\u201d calendar in Dershowitz and Reingold\u2019s book \u201cCalendrical Calculations\u201d, where it\u2019s the base calendar for all computations. Zero and negative years are interpreted as prescribed by the ISO 8601 standard. Year 0 is 1 BC, year -1 is 2 BC, and so on.  \nclass calendar.Calendar(firstweekday=0)  \nCreates a Calendar object. firstweekday is an integer specifying the first day of the week. 0 is Monday (the default), 6 is Sunday. A Calendar object provides several methods that can be used for preparing the calendar data for formatting. This class doesn\u2019t do any formatting itself. This is the job of subclasses. Calendar instances have the following methods:  \niterweekdays()  \nReturn an iterator for the week day numbers that will be used for one week. The first value from the iterator will be the same as the value of the firstweekday property. \n  \nitermonthdates(year, month)  \nReturn an iterator for the month month (1\u201312) in the year year. This iterator will return all days (as datetime.date objects) for the month and all days before the start of the month or after the end of the month that are required to get a complete week. \n  \nitermonthdays(year, month)  \nReturn an iterator for the month month in the year year similar to itermonthdates(), but not restricted by the datetime.date range. Days returned will simply be day of the month numbers. For the days outside of the specified month, the day number is 0. \n  \nitermonthdays2(year, month)  \nReturn an iterator for the month month in the year year similar to itermonthdates(), but not restricted by the datetime.date range. Days returned will be tuples consisting of a day of the month number and a week day number. \n  \nitermonthdays3(year, month)  \nReturn an iterator for the month month in the year year similar to itermonthdates(), but not restricted by the datetime.date range. Days returned will be tuples consisting of a year, a month and a day of the month numbers.  New in version 3.7.  \n  \nitermonthdays4(year, month)  \nReturn an iterator for the month month in the year year similar to itermonthdates(), but not restricted by the datetime.date range. Days returned will be tuples consisting of a year, a month, a day of the month, and a day of the week numbers.  New in version 3.7.  \n  \nmonthdatescalendar(year, month)  \nReturn a list of the weeks in the month month of the year as full weeks. Weeks are lists of seven datetime.date objects. \n  \nmonthdays2calendar(year, month)  \nReturn a list of the weeks in the month month of the year as full weeks. Weeks are lists of seven tuples of day numbers and weekday numbers. \n  \nmonthdayscalendar(year, month)  \nReturn a list of the weeks in the month month of the year as full weeks. Weeks are lists of seven day numbers. \n  \nyeardatescalendar(year, width=3)  \nReturn the data for the specified year ready for formatting. The return value is a list of month rows. Each month row contains up to width months (defaulting to 3). Each month contains between 4 and 6 weeks and each week contains 1\u20137 days. Days are datetime.date objects. \n  \nyeardays2calendar(year, width=3)  \nReturn the data for the specified year ready for formatting (similar to yeardatescalendar()). Entries in the week lists are tuples of day numbers and weekday numbers. Day numbers outside this month are zero. \n  \nyeardayscalendar(year, width=3)  \nReturn the data for the specified year ready for formatting (similar to yeardatescalendar()). Entries in the week lists are day numbers. Day numbers outside this month are zero. \n \n  \nclass calendar.TextCalendar(firstweekday=0)  \nThis class can be used to generate plain text calendars. TextCalendar instances have the following methods:  \nformatmonth(theyear, themonth, w=0, l=0)  \nReturn a month\u2019s calendar in a multi-line string. If w is provided, it specifies the width of the date columns, which are centered. If l is given, it specifies the number of lines that each week will use. Depends on the first weekday as specified in the constructor or set by the setfirstweekday() method. \n  \nprmonth(theyear, themonth, w=0, l=0)  \nPrint a month\u2019s calendar as returned by formatmonth(). \n  \nformatyear(theyear, w=2, l=1, c=6, m=3)  \nReturn a m-column calendar for an entire year as a multi-line string. Optional parameters w, l, and c are for date column width, lines per week, and number of spaces between month columns, respectively. Depends on the first weekday as specified in the constructor or set by the setfirstweekday() method. The earliest year for which a calendar can be generated is platform-dependent. \n  \npryear(theyear, w=2, l=1, c=6, m=3)  \nPrint the calendar for an entire year as returned by formatyear(). \n \n  \nclass calendar.HTMLCalendar(firstweekday=0)  \nThis class can be used to generate HTML calendars. HTMLCalendar instances have the following methods:  \nformatmonth(theyear, themonth, withyear=True)  \nReturn a month\u2019s calendar as an HTML table. If withyear is true the year will be included in the header, otherwise just the month name will be used. \n  \nformatyear(theyear, width=3)  \nReturn a year\u2019s calendar as an HTML table. width (defaulting to 3) specifies the number of months per row. \n  \nformatyearpage(theyear, width=3, css='calendar.css', encoding=None)  \nReturn a year\u2019s calendar as a complete HTML page. width (defaulting to 3) specifies the number of months per row. css is the name for the cascading style sheet to be used. None can be passed if no style sheet should be used. encoding specifies the encoding to be used for the output (defaulting to the system default encoding). \n HTMLCalendar has the following attributes you can override to customize the CSS classes used by the calendar:  \ncssclasses  \nA list of CSS classes used for each weekday. The default class list is: cssclasses = [\"mon\", \"tue\", \"wed\", \"thu\", \"fri\", \"sat\", \"sun\"]\n more styles can be added for each day: cssclasses = [\"mon text-bold\", \"tue\", \"wed\", \"thu\", \"fri\", \"sat\", \"sun red\"]\n Note that the length of this list must be seven items. \n  \ncssclass_noday  \nThe CSS class for a weekday occurring in the previous or coming month.  New in version 3.7.  \n  \ncssclasses_weekday_head  \nA list of CSS classes used for weekday names in the header row. The default is the same as cssclasses.  New in version 3.7.  \n  \ncssclass_month_head  \nThe month\u2019s head CSS class (used by formatmonthname()). The default value is \"month\".  New in version 3.7.  \n  \ncssclass_month  \nThe CSS class for the whole month\u2019s table (used by formatmonth()). The default value is \"month\".  New in version 3.7.  \n  \ncssclass_year  \nThe CSS class for the whole year\u2019s table of tables (used by formatyear()). The default value is \"year\".  New in version 3.7.  \n  \ncssclass_year_head  \nThe CSS class for the table head for the whole year (used by formatyear()). The default value is \"year\".  New in version 3.7.  \n Note that although the naming for the above described class attributes is singular (e.g. cssclass_month cssclass_noday), one can replace the single CSS class with a space separated list of CSS classes, for example: \"text-bold text-red\"\n Here is an example how HTMLCalendar can be customized: class CustomHTMLCal(calendar.HTMLCalendar):\n    cssclasses = [style + \" text-nowrap\" for style in\n                  calendar.HTMLCalendar.cssclasses]\n    cssclass_month_head = \"text-center month-head\"\n    cssclass_month = \"text-center month\"\n    cssclass_year = \"text-italic lead\"\n \n  \nclass calendar.LocaleTextCalendar(firstweekday=0, locale=None)  \nThis subclass of TextCalendar can be passed a locale name in the constructor and will return month and weekday names in the specified locale. If this locale includes an encoding all strings containing month and weekday names will be returned as unicode. \n  \nclass calendar.LocaleHTMLCalendar(firstweekday=0, locale=None)  \nThis subclass of HTMLCalendar can be passed a locale name in the constructor and will return month and weekday names in the specified locale. If this locale includes an encoding all strings containing month and weekday names will be returned as unicode. \n  Note The formatweekday() and formatmonthname() methods of these two classes temporarily change the current locale to the given locale. Because the current locale is a process-wide setting, they are not thread-safe.  For simple text calendars this module provides the following functions.  \ncalendar.setfirstweekday(weekday)  \nSets the weekday (0 is Monday, 6 is Sunday) to start each week. The values MONDAY, TUESDAY, WEDNESDAY, THURSDAY, FRIDAY, SATURDAY, and SUNDAY are provided for convenience. For example, to set the first weekday to Sunday: import calendar\ncalendar.setfirstweekday(calendar.SUNDAY)\n \n  \ncalendar.firstweekday()  \nReturns the current setting for the weekday to start each week. \n  \ncalendar.isleap(year)  \nReturns True if year is a leap year, otherwise False. \n  \ncalendar.leapdays(y1, y2)  \nReturns the number of leap years in the range from y1 to y2 (exclusive), where y1 and y2 are years. This function works for ranges spanning a century change. \n  \ncalendar.weekday(year, month, day)  \nReturns the day of the week (0 is Monday) for year (1970\u2013\u2026), month (1\u201312), day (1\u201331). \n  \ncalendar.weekheader(n)  \nReturn a header containing abbreviated weekday names. n specifies the width in characters for one weekday. \n  \ncalendar.monthrange(year, month)  \nReturns weekday of first day of the month and number of days in month, for the specified year and month. \n  \ncalendar.monthcalendar(year, month)  \nReturns a matrix representing a month\u2019s calendar. Each row represents a week; days outside of the month are represented by zeros. Each week begins with Monday unless set by setfirstweekday(). \n  \ncalendar.prmonth(theyear, themonth, w=0, l=0)  \nPrints a month\u2019s calendar as returned by month(). \n  \ncalendar.month(theyear, themonth, w=0, l=0)  \nReturns a month\u2019s calendar in a multi-line string using the formatmonth() of the TextCalendar class. \n  \ncalendar.prcal(year, w=0, l=0, c=6, m=3)  \nPrints the calendar for an entire year as returned by calendar(). \n  \ncalendar.calendar(year, w=2, l=1, c=6, m=3)  \nReturns a 3-column calendar for an entire year as a multi-line string using the formatyear() of the TextCalendar class. \n  \ncalendar.timegm(tuple)  \nAn unrelated but handy function that takes a time tuple such as returned by the gmtime() function in the time module, and returns the corresponding Unix timestamp value, assuming an epoch of 1970, and the POSIX encoding. In fact, time.gmtime() and timegm() are each others\u2019 inverse. \n The calendar module exports the following data attributes:  \ncalendar.day_name  \nAn array that represents the days of the week in the current locale. \n  \ncalendar.day_abbr  \nAn array that represents the abbreviated days of the week in the current locale. \n  \ncalendar.month_name  \nAn array that represents the months of the year in the current locale. This follows normal convention of January being month number 1, so it has a length of 13 and month_name[0] is the empty string. \n  \ncalendar.month_abbr  \nAn array that represents the abbreviated months of the year in the current locale. This follows normal convention of January being month number 1, so it has a length of 13 and month_abbr[0] is the empty string. \n  See also  \nModule datetime\n\n\nObject-oriented interface to dates and times with similar functionality to the time module.  \nModule time\n\n\nLow-level time related functions.   \n"}, {"name": "calendar.Calendar", "path": "library/calendar#calendar.Calendar", "type": "Data Types", "text": " \nclass calendar.Calendar(firstweekday=0)  \nCreates a Calendar object. firstweekday is an integer specifying the first day of the week. 0 is Monday (the default), 6 is Sunday. A Calendar object provides several methods that can be used for preparing the calendar data for formatting. This class doesn\u2019t do any formatting itself. This is the job of subclasses. Calendar instances have the following methods:  \niterweekdays()  \nReturn an iterator for the week day numbers that will be used for one week. The first value from the iterator will be the same as the value of the firstweekday property. \n  \nitermonthdates(year, month)  \nReturn an iterator for the month month (1\u201312) in the year year. This iterator will return all days (as datetime.date objects) for the month and all days before the start of the month or after the end of the month that are required to get a complete week. \n  \nitermonthdays(year, month)  \nReturn an iterator for the month month in the year year similar to itermonthdates(), but not restricted by the datetime.date range. Days returned will simply be day of the month numbers. For the days outside of the specified month, the day number is 0. \n  \nitermonthdays2(year, month)  \nReturn an iterator for the month month in the year year similar to itermonthdates(), but not restricted by the datetime.date range. Days returned will be tuples consisting of a day of the month number and a week day number. \n  \nitermonthdays3(year, month)  \nReturn an iterator for the month month in the year year similar to itermonthdates(), but not restricted by the datetime.date range. Days returned will be tuples consisting of a year, a month and a day of the month numbers.  New in version 3.7.  \n  \nitermonthdays4(year, month)  \nReturn an iterator for the month month in the year year similar to itermonthdates(), but not restricted by the datetime.date range. Days returned will be tuples consisting of a year, a month, a day of the month, and a day of the week numbers.  New in version 3.7.  \n  \nmonthdatescalendar(year, month)  \nReturn a list of the weeks in the month month of the year as full weeks. Weeks are lists of seven datetime.date objects. \n  \nmonthdays2calendar(year, month)  \nReturn a list of the weeks in the month month of the year as full weeks. Weeks are lists of seven tuples of day numbers and weekday numbers. \n  \nmonthdayscalendar(year, month)  \nReturn a list of the weeks in the month month of the year as full weeks. Weeks are lists of seven day numbers. \n  \nyeardatescalendar(year, width=3)  \nReturn the data for the specified year ready for formatting. The return value is a list of month rows. Each month row contains up to width months (defaulting to 3). Each month contains between 4 and 6 weeks and each week contains 1\u20137 days. Days are datetime.date objects. \n  \nyeardays2calendar(year, width=3)  \nReturn the data for the specified year ready for formatting (similar to yeardatescalendar()). Entries in the week lists are tuples of day numbers and weekday numbers. Day numbers outside this month are zero. \n  \nyeardayscalendar(year, width=3)  \nReturn the data for the specified year ready for formatting (similar to yeardatescalendar()). Entries in the week lists are day numbers. Day numbers outside this month are zero. \n \n"}, {"name": "calendar.calendar()", "path": "library/calendar#calendar.calendar", "type": "Data Types", "text": " \ncalendar.calendar(year, w=2, l=1, c=6, m=3)  \nReturns a 3-column calendar for an entire year as a multi-line string using the formatyear() of the TextCalendar class. \n"}, {"name": "calendar.Calendar.itermonthdates()", "path": "library/calendar#calendar.Calendar.itermonthdates", "type": "Data Types", "text": " \nitermonthdates(year, month)  \nReturn an iterator for the month month (1\u201312) in the year year. This iterator will return all days (as datetime.date objects) for the month and all days before the start of the month or after the end of the month that are required to get a complete week. \n"}, {"name": "calendar.Calendar.itermonthdays()", "path": "library/calendar#calendar.Calendar.itermonthdays", "type": "Data Types", "text": " \nitermonthdays(year, month)  \nReturn an iterator for the month month in the year year similar to itermonthdates(), but not restricted by the datetime.date range. Days returned will simply be day of the month numbers. For the days outside of the specified month, the day number is 0. \n"}, {"name": "calendar.Calendar.itermonthdays2()", "path": "library/calendar#calendar.Calendar.itermonthdays2", "type": "Data Types", "text": " \nitermonthdays2(year, month)  \nReturn an iterator for the month month in the year year similar to itermonthdates(), but not restricted by the datetime.date range. Days returned will be tuples consisting of a day of the month number and a week day number. \n"}, {"name": "calendar.Calendar.itermonthdays3()", "path": "library/calendar#calendar.Calendar.itermonthdays3", "type": "Data Types", "text": " \nitermonthdays3(year, month)  \nReturn an iterator for the month month in the year year similar to itermonthdates(), but not restricted by the datetime.date range. Days returned will be tuples consisting of a year, a month and a day of the month numbers.  New in version 3.7.  \n"}, {"name": "calendar.Calendar.itermonthdays4()", "path": "library/calendar#calendar.Calendar.itermonthdays4", "type": "Data Types", "text": " \nitermonthdays4(year, month)  \nReturn an iterator for the month month in the year year similar to itermonthdates(), but not restricted by the datetime.date range. Days returned will be tuples consisting of a year, a month, a day of the month, and a day of the week numbers.  New in version 3.7.  \n"}, {"name": "calendar.Calendar.iterweekdays()", "path": "library/calendar#calendar.Calendar.iterweekdays", "type": "Data Types", "text": " \niterweekdays()  \nReturn an iterator for the week day numbers that will be used for one week. The first value from the iterator will be the same as the value of the firstweekday property. \n"}, {"name": "calendar.Calendar.monthdatescalendar()", "path": "library/calendar#calendar.Calendar.monthdatescalendar", "type": "Data Types", "text": " \nmonthdatescalendar(year, month)  \nReturn a list of the weeks in the month month of the year as full weeks. Weeks are lists of seven datetime.date objects. \n"}, {"name": "calendar.Calendar.monthdays2calendar()", "path": "library/calendar#calendar.Calendar.monthdays2calendar", "type": "Data Types", "text": " \nmonthdays2calendar(year, month)  \nReturn a list of the weeks in the month month of the year as full weeks. Weeks are lists of seven tuples of day numbers and weekday numbers. \n"}, {"name": "calendar.Calendar.monthdayscalendar()", "path": "library/calendar#calendar.Calendar.monthdayscalendar", "type": "Data Types", "text": " \nmonthdayscalendar(year, month)  \nReturn a list of the weeks in the month month of the year as full weeks. Weeks are lists of seven day numbers. \n"}, {"name": "calendar.Calendar.yeardatescalendar()", "path": "library/calendar#calendar.Calendar.yeardatescalendar", "type": "Data Types", "text": " \nyeardatescalendar(year, width=3)  \nReturn the data for the specified year ready for formatting. The return value is a list of month rows. Each month row contains up to width months (defaulting to 3). Each month contains between 4 and 6 weeks and each week contains 1\u20137 days. Days are datetime.date objects. \n"}, {"name": "calendar.Calendar.yeardays2calendar()", "path": "library/calendar#calendar.Calendar.yeardays2calendar", "type": "Data Types", "text": " \nyeardays2calendar(year, width=3)  \nReturn the data for the specified year ready for formatting (similar to yeardatescalendar()). Entries in the week lists are tuples of day numbers and weekday numbers. Day numbers outside this month are zero. \n"}, {"name": "calendar.Calendar.yeardayscalendar()", "path": "library/calendar#calendar.Calendar.yeardayscalendar", "type": "Data Types", "text": " \nyeardayscalendar(year, width=3)  \nReturn the data for the specified year ready for formatting (similar to yeardatescalendar()). Entries in the week lists are day numbers. Day numbers outside this month are zero. \n"}, {"name": "calendar.day_abbr", "path": "library/calendar#calendar.day_abbr", "type": "Data Types", "text": " \ncalendar.day_abbr  \nAn array that represents the abbreviated days of the week in the current locale. \n"}, {"name": "calendar.day_name", "path": "library/calendar#calendar.day_name", "type": "Data Types", "text": " \ncalendar.day_name  \nAn array that represents the days of the week in the current locale. \n"}, {"name": "calendar.firstweekday()", "path": "library/calendar#calendar.firstweekday", "type": "Data Types", "text": " \ncalendar.firstweekday()  \nReturns the current setting for the weekday to start each week. \n"}, {"name": "calendar.HTMLCalendar", "path": "library/calendar#calendar.HTMLCalendar", "type": "Data Types", "text": " \nclass calendar.HTMLCalendar(firstweekday=0)  \nThis class can be used to generate HTML calendars. HTMLCalendar instances have the following methods:  \nformatmonth(theyear, themonth, withyear=True)  \nReturn a month\u2019s calendar as an HTML table. If withyear is true the year will be included in the header, otherwise just the month name will be used. \n  \nformatyear(theyear, width=3)  \nReturn a year\u2019s calendar as an HTML table. width (defaulting to 3) specifies the number of months per row. \n  \nformatyearpage(theyear, width=3, css='calendar.css', encoding=None)  \nReturn a year\u2019s calendar as a complete HTML page. width (defaulting to 3) specifies the number of months per row. css is the name for the cascading style sheet to be used. None can be passed if no style sheet should be used. encoding specifies the encoding to be used for the output (defaulting to the system default encoding). \n HTMLCalendar has the following attributes you can override to customize the CSS classes used by the calendar:  \ncssclasses  \nA list of CSS classes used for each weekday. The default class list is: cssclasses = [\"mon\", \"tue\", \"wed\", \"thu\", \"fri\", \"sat\", \"sun\"]\n more styles can be added for each day: cssclasses = [\"mon text-bold\", \"tue\", \"wed\", \"thu\", \"fri\", \"sat\", \"sun red\"]\n Note that the length of this list must be seven items. \n  \ncssclass_noday  \nThe CSS class for a weekday occurring in the previous or coming month.  New in version 3.7.  \n  \ncssclasses_weekday_head  \nA list of CSS classes used for weekday names in the header row. The default is the same as cssclasses.  New in version 3.7.  \n  \ncssclass_month_head  \nThe month\u2019s head CSS class (used by formatmonthname()). The default value is \"month\".  New in version 3.7.  \n  \ncssclass_month  \nThe CSS class for the whole month\u2019s table (used by formatmonth()). The default value is \"month\".  New in version 3.7.  \n  \ncssclass_year  \nThe CSS class for the whole year\u2019s table of tables (used by formatyear()). The default value is \"year\".  New in version 3.7.  \n  \ncssclass_year_head  \nThe CSS class for the table head for the whole year (used by formatyear()). The default value is \"year\".  New in version 3.7.  \n Note that although the naming for the above described class attributes is singular (e.g. cssclass_month cssclass_noday), one can replace the single CSS class with a space separated list of CSS classes, for example: \"text-bold text-red\"\n Here is an example how HTMLCalendar can be customized: class CustomHTMLCal(calendar.HTMLCalendar):\n    cssclasses = [style + \" text-nowrap\" for style in\n                  calendar.HTMLCalendar.cssclasses]\n    cssclass_month_head = \"text-center month-head\"\n    cssclass_month = \"text-center month\"\n    cssclass_year = \"text-italic lead\"\n \n"}, {"name": "calendar.HTMLCalendar.cssclasses", "path": "library/calendar#calendar.HTMLCalendar.cssclasses", "type": "Data Types", "text": " \ncssclasses  \nA list of CSS classes used for each weekday. The default class list is: cssclasses = [\"mon\", \"tue\", \"wed\", \"thu\", \"fri\", \"sat\", \"sun\"]\n more styles can be added for each day: cssclasses = [\"mon text-bold\", \"tue\", \"wed\", \"thu\", \"fri\", \"sat\", \"sun red\"]\n Note that the length of this list must be seven items. \n"}, {"name": "calendar.HTMLCalendar.cssclasses_weekday_head", "path": "library/calendar#calendar.HTMLCalendar.cssclasses_weekday_head", "type": "Data Types", "text": " \ncssclasses_weekday_head  \nA list of CSS classes used for weekday names in the header row. The default is the same as cssclasses.  New in version 3.7.  \n"}, {"name": "calendar.HTMLCalendar.cssclass_month", "path": "library/calendar#calendar.HTMLCalendar.cssclass_month", "type": "Data Types", "text": " \ncssclass_month  \nThe CSS class for the whole month\u2019s table (used by formatmonth()). The default value is \"month\".  New in version 3.7.  \n"}, {"name": "calendar.HTMLCalendar.cssclass_month_head", "path": "library/calendar#calendar.HTMLCalendar.cssclass_month_head", "type": "Data Types", "text": " \ncssclass_month_head  \nThe month\u2019s head CSS class (used by formatmonthname()). The default value is \"month\".  New in version 3.7.  \n"}, {"name": "calendar.HTMLCalendar.cssclass_noday", "path": "library/calendar#calendar.HTMLCalendar.cssclass_noday", "type": "Data Types", "text": " \ncssclass_noday  \nThe CSS class for a weekday occurring in the previous or coming month.  New in version 3.7.  \n"}, {"name": "calendar.HTMLCalendar.cssclass_year", "path": "library/calendar#calendar.HTMLCalendar.cssclass_year", "type": "Data Types", "text": " \ncssclass_year  \nThe CSS class for the whole year\u2019s table of tables (used by formatyear()). The default value is \"year\".  New in version 3.7.  \n"}, {"name": "calendar.HTMLCalendar.cssclass_year_head", "path": "library/calendar#calendar.HTMLCalendar.cssclass_year_head", "type": "Data Types", "text": " \ncssclass_year_head  \nThe CSS class for the table head for the whole year (used by formatyear()). The default value is \"year\".  New in version 3.7.  \n"}, {"name": "calendar.HTMLCalendar.formatmonth()", "path": "library/calendar#calendar.HTMLCalendar.formatmonth", "type": "Data Types", "text": " \nformatmonth(theyear, themonth, withyear=True)  \nReturn a month\u2019s calendar as an HTML table. If withyear is true the year will be included in the header, otherwise just the month name will be used. \n"}, {"name": "calendar.HTMLCalendar.formatyear()", "path": "library/calendar#calendar.HTMLCalendar.formatyear", "type": "Data Types", "text": " \nformatyear(theyear, width=3)  \nReturn a year\u2019s calendar as an HTML table. width (defaulting to 3) specifies the number of months per row. \n"}, {"name": "calendar.HTMLCalendar.formatyearpage()", "path": "library/calendar#calendar.HTMLCalendar.formatyearpage", "type": "Data Types", "text": " \nformatyearpage(theyear, width=3, css='calendar.css', encoding=None)  \nReturn a year\u2019s calendar as a complete HTML page. width (defaulting to 3) specifies the number of months per row. css is the name for the cascading style sheet to be used. None can be passed if no style sheet should be used. encoding specifies the encoding to be used for the output (defaulting to the system default encoding). \n"}, {"name": "calendar.isleap()", "path": "library/calendar#calendar.isleap", "type": "Data Types", "text": " \ncalendar.isleap(year)  \nReturns True if year is a leap year, otherwise False. \n"}, {"name": "calendar.leapdays()", "path": "library/calendar#calendar.leapdays", "type": "Data Types", "text": " \ncalendar.leapdays(y1, y2)  \nReturns the number of leap years in the range from y1 to y2 (exclusive), where y1 and y2 are years. This function works for ranges spanning a century change. \n"}, {"name": "calendar.LocaleHTMLCalendar", "path": "library/calendar#calendar.LocaleHTMLCalendar", "type": "Data Types", "text": " \nclass calendar.LocaleHTMLCalendar(firstweekday=0, locale=None)  \nThis subclass of HTMLCalendar can be passed a locale name in the constructor and will return month and weekday names in the specified locale. If this locale includes an encoding all strings containing month and weekday names will be returned as unicode. \n"}, {"name": "calendar.LocaleTextCalendar", "path": "library/calendar#calendar.LocaleTextCalendar", "type": "Data Types", "text": " \nclass calendar.LocaleTextCalendar(firstweekday=0, locale=None)  \nThis subclass of TextCalendar can be passed a locale name in the constructor and will return month and weekday names in the specified locale. If this locale includes an encoding all strings containing month and weekday names will be returned as unicode. \n"}, {"name": "calendar.month()", "path": "library/calendar#calendar.month", "type": "Data Types", "text": " \ncalendar.month(theyear, themonth, w=0, l=0)  \nReturns a month\u2019s calendar in a multi-line string using the formatmonth() of the TextCalendar class. \n"}, {"name": "calendar.monthcalendar()", "path": "library/calendar#calendar.monthcalendar", "type": "Data Types", "text": " \ncalendar.monthcalendar(year, month)  \nReturns a matrix representing a month\u2019s calendar. Each row represents a week; days outside of the month are represented by zeros. Each week begins with Monday unless set by setfirstweekday(). \n"}, {"name": "calendar.monthrange()", "path": "library/calendar#calendar.monthrange", "type": "Data Types", "text": " \ncalendar.monthrange(year, month)  \nReturns weekday of first day of the month and number of days in month, for the specified year and month. \n"}, {"name": "calendar.month_abbr", "path": "library/calendar#calendar.month_abbr", "type": "Data Types", "text": " \ncalendar.month_abbr  \nAn array that represents the abbreviated months of the year in the current locale. This follows normal convention of January being month number 1, so it has a length of 13 and month_abbr[0] is the empty string. \n"}, {"name": "calendar.month_name", "path": "library/calendar#calendar.month_name", "type": "Data Types", "text": " \ncalendar.month_name  \nAn array that represents the months of the year in the current locale. This follows normal convention of January being month number 1, so it has a length of 13 and month_name[0] is the empty string. \n"}, {"name": "calendar.prcal()", "path": "library/calendar#calendar.prcal", "type": "Data Types", "text": " \ncalendar.prcal(year, w=0, l=0, c=6, m=3)  \nPrints the calendar for an entire year as returned by calendar(). \n"}, {"name": "calendar.prmonth()", "path": "library/calendar#calendar.prmonth", "type": "Data Types", "text": " \ncalendar.prmonth(theyear, themonth, w=0, l=0)  \nPrints a month\u2019s calendar as returned by month(). \n"}, {"name": "calendar.setfirstweekday()", "path": "library/calendar#calendar.setfirstweekday", "type": "Data Types", "text": " \ncalendar.setfirstweekday(weekday)  \nSets the weekday (0 is Monday, 6 is Sunday) to start each week. The values MONDAY, TUESDAY, WEDNESDAY, THURSDAY, FRIDAY, SATURDAY, and SUNDAY are provided for convenience. For example, to set the first weekday to Sunday: import calendar\ncalendar.setfirstweekday(calendar.SUNDAY)\n \n"}, {"name": "calendar.TextCalendar", "path": "library/calendar#calendar.TextCalendar", "type": "Data Types", "text": " \nclass calendar.TextCalendar(firstweekday=0)  \nThis class can be used to generate plain text calendars. TextCalendar instances have the following methods:  \nformatmonth(theyear, themonth, w=0, l=0)  \nReturn a month\u2019s calendar in a multi-line string. If w is provided, it specifies the width of the date columns, which are centered. If l is given, it specifies the number of lines that each week will use. Depends on the first weekday as specified in the constructor or set by the setfirstweekday() method. \n  \nprmonth(theyear, themonth, w=0, l=0)  \nPrint a month\u2019s calendar as returned by formatmonth(). \n  \nformatyear(theyear, w=2, l=1, c=6, m=3)  \nReturn a m-column calendar for an entire year as a multi-line string. Optional parameters w, l, and c are for date column width, lines per week, and number of spaces between month columns, respectively. Depends on the first weekday as specified in the constructor or set by the setfirstweekday() method. The earliest year for which a calendar can be generated is platform-dependent. \n  \npryear(theyear, w=2, l=1, c=6, m=3)  \nPrint the calendar for an entire year as returned by formatyear(). \n \n"}, {"name": "calendar.TextCalendar.formatmonth()", "path": "library/calendar#calendar.TextCalendar.formatmonth", "type": "Data Types", "text": " \nformatmonth(theyear, themonth, w=0, l=0)  \nReturn a month\u2019s calendar in a multi-line string. If w is provided, it specifies the width of the date columns, which are centered. If l is given, it specifies the number of lines that each week will use. Depends on the first weekday as specified in the constructor or set by the setfirstweekday() method. \n"}, {"name": "calendar.TextCalendar.formatyear()", "path": "library/calendar#calendar.TextCalendar.formatyear", "type": "Data Types", "text": " \nformatyear(theyear, w=2, l=1, c=6, m=3)  \nReturn a m-column calendar for an entire year as a multi-line string. Optional parameters w, l, and c are for date column width, lines per week, and number of spaces between month columns, respectively. Depends on the first weekday as specified in the constructor or set by the setfirstweekday() method. The earliest year for which a calendar can be generated is platform-dependent. \n"}, {"name": "calendar.TextCalendar.prmonth()", "path": "library/calendar#calendar.TextCalendar.prmonth", "type": "Data Types", "text": " \nprmonth(theyear, themonth, w=0, l=0)  \nPrint a month\u2019s calendar as returned by formatmonth(). \n"}, {"name": "calendar.TextCalendar.pryear()", "path": "library/calendar#calendar.TextCalendar.pryear", "type": "Data Types", "text": " \npryear(theyear, w=2, l=1, c=6, m=3)  \nPrint the calendar for an entire year as returned by formatyear(). \n"}, {"name": "calendar.timegm()", "path": "library/calendar#calendar.timegm", "type": "Data Types", "text": " \ncalendar.timegm(tuple)  \nAn unrelated but handy function that takes a time tuple such as returned by the gmtime() function in the time module, and returns the corresponding Unix timestamp value, assuming an epoch of 1970, and the POSIX encoding. In fact, time.gmtime() and timegm() are each others\u2019 inverse. \n"}, {"name": "calendar.weekday()", "path": "library/calendar#calendar.weekday", "type": "Data Types", "text": " \ncalendar.weekday(year, month, day)  \nReturns the day of the week (0 is Monday) for year (1970\u2013\u2026), month (1\u201312), day (1\u201331). \n"}, {"name": "calendar.weekheader()", "path": "library/calendar#calendar.weekheader", "type": "Data Types", "text": " \ncalendar.weekheader(n)  \nReturn a header containing abbreviated weekday names. n specifies the width in characters for one weekday. \n"}, {"name": "callable()", "path": "library/functions#callable", "type": "Built-in Functions", "text": " \ncallable(object)  \nReturn True if the object argument appears callable, False if not. If this returns True, it is still possible that a call fails, but if it is False, calling object will never succeed. Note that classes are callable (calling a class returns a new instance); instances are callable if their class has a __call__() method.  New in version 3.2: This function was first removed in Python 3.0 and then brought back in Python 3.2.  \n"}, {"name": "cgi", "path": "library/cgi", "type": "Internet", "text": "cgi \u2014 Common Gateway Interface support Source code: Lib/cgi.py Support module for Common Gateway Interface (CGI) scripts. This module defines a number of utilities for use by CGI scripts written in Python. Introduction A CGI script is invoked by an HTTP server, usually to process user input submitted through an HTML <FORM> or <ISINDEX> element. Most often, CGI scripts live in the server\u2019s special cgi-bin directory. The HTTP server places all sorts of information about the request (such as the client\u2019s hostname, the requested URL, the query string, and lots of other goodies) in the script\u2019s shell environment, executes the script, and sends the script\u2019s output back to the client. The script\u2019s input is connected to the client too, and sometimes the form data is read this way; at other times the form data is passed via the \u201cquery string\u201d part of the URL. This module is intended to take care of the different cases and provide a simpler interface to the Python script. It also provides a number of utilities that help in debugging scripts, and the latest addition is support for file uploads from a form (if your browser supports it). The output of a CGI script should consist of two sections, separated by a blank line. The first section contains a number of headers, telling the client what kind of data is following. Python code to generate a minimal header section looks like this: print(\"Content-Type: text/html\")    # HTML is following\nprint()                             # blank line, end of headers\n The second section is usually HTML, which allows the client software to display nicely formatted text with header, in-line images, etc. Here\u2019s Python code that prints a simple piece of HTML: print(\"<TITLE>CGI script output</TITLE>\")\nprint(\"<H1>This is my first CGI script</H1>\")\nprint(\"Hello, world!\")\n Using the cgi module Begin by writing import cgi. When you write a new script, consider adding these lines: import cgitb\ncgitb.enable()\n This activates a special exception handler that will display detailed reports in the Web browser if any errors occur. If you\u2019d rather not show the guts of your program to users of your script, you can have the reports saved to files instead, with code like this: import cgitb\ncgitb.enable(display=0, logdir=\"/path/to/logdir\")\n It\u2019s very helpful to use this feature during script development. The reports produced by cgitb provide information that can save you a lot of time in tracking down bugs. You can always remove the cgitb line later when you have tested your script and are confident that it works correctly. To get at submitted form data, use the FieldStorage class. If the form contains non-ASCII characters, use the encoding keyword parameter set to the value of the encoding defined for the document. It is usually contained in the META tag in the HEAD section of the HTML document or by the Content-Type header). This reads the form contents from the standard input or the environment (depending on the value of various environment variables set according to the CGI standard). Since it may consume standard input, it should be instantiated only once. The FieldStorage instance can be indexed like a Python dictionary. It allows membership testing with the in operator, and also supports the standard dictionary method keys() and the built-in function len(). Form fields containing empty strings are ignored and do not appear in the dictionary; to keep such values, provide a true value for the optional keep_blank_values keyword parameter when creating the FieldStorage instance. For instance, the following code (which assumes that the Content-Type header and blank line have already been printed) checks that the fields name and addr are both set to a non-empty string: form = cgi.FieldStorage()\nif \"name\" not in form or \"addr\" not in form:\n    print(\"<H1>Error</H1>\")\n    print(\"Please fill in the name and addr fields.\")\n    return\nprint(\"<p>name:\", form[\"name\"].value)\nprint(\"<p>addr:\", form[\"addr\"].value)\n...further form processing here...\n Here the fields, accessed through form[key], are themselves instances of FieldStorage (or MiniFieldStorage, depending on the form encoding). The value attribute of the instance yields the string value of the field. The getvalue() method returns this string value directly; it also accepts an optional second argument as a default to return if the requested key is not present. If the submitted form data contains more than one field with the same name, the object retrieved by form[key] is not a FieldStorage or MiniFieldStorage instance but a list of such instances. Similarly, in this situation, form.getvalue(key) would return a list of strings. If you expect this possibility (when your HTML form contains multiple fields with the same name), use the getlist() method, which always returns a list of values (so that you do not need to special-case the single item case). For example, this code concatenates any number of username fields, separated by commas: value = form.getlist(\"username\")\nusernames = \",\".join(value)\n If a field represents an uploaded file, accessing the value via the value attribute or the getvalue() method reads the entire file in memory as bytes. This may not be what you want. You can test for an uploaded file by testing either the filename attribute or the file attribute. You can then read the data from the file attribute before it is automatically closed as part of the garbage collection of the FieldStorage instance (the read() and readline() methods will return bytes): fileitem = form[\"userfile\"]\nif fileitem.file:\n    # It's an uploaded file; count lines\n    linecount = 0\n    while True:\n        line = fileitem.file.readline()\n        if not line: break\n        linecount = linecount + 1\n FieldStorage objects also support being used in a with statement, which will automatically close them when done. If an error is encountered when obtaining the contents of an uploaded file (for example, when the user interrupts the form submission by clicking on a Back or Cancel button) the done attribute of the object for the field will be set to the value -1. The file upload draft standard entertains the possibility of uploading multiple files from one field (using a recursive multipart/* encoding). When this occurs, the item will be a dictionary-like FieldStorage item. This can be determined by testing its type attribute, which should be multipart/form-data (or perhaps another MIME type matching multipart/*). In this case, it can be iterated over recursively just like the top-level form object. When a form is submitted in the \u201cold\u201d format (as the query string or as a single data part of type application/x-www-form-urlencoded), the items will actually be instances of the class MiniFieldStorage. In this case, the list, file, and filename attributes are always None. A form submitted via POST that also has a query string will contain both FieldStorage and MiniFieldStorage items.  Changed in version 3.4: The file attribute is automatically closed upon the garbage collection of the creating FieldStorage instance.   Changed in version 3.5: Added support for the context management protocol to the FieldStorage class.  Higher Level Interface The previous section explains how to read CGI form data using the FieldStorage class. This section describes a higher level interface which was added to this class to allow one to do it in a more readable and intuitive way. The interface doesn\u2019t make the techniques described in previous sections obsolete \u2014 they are still useful to process file uploads efficiently, for example. The interface consists of two simple methods. Using the methods you can process form data in a generic way, without the need to worry whether only one or more values were posted under one name. In the previous section, you learned to write following code anytime you expected a user to post more than one value under one name: item = form.getvalue(\"item\")\nif isinstance(item, list):\n    # The user is requesting more than one item.\nelse:\n    # The user is requesting only one item.\n This situation is common for example when a form contains a group of multiple checkboxes with the same name: <input type=\"checkbox\" name=\"item\" value=\"1\" />\n<input type=\"checkbox\" name=\"item\" value=\"2\" />\n In most situations, however, there\u2019s only one form control with a particular name in a form and then you expect and need only one value associated with this name. So you write a script containing for example this code: user = form.getvalue(\"user\").upper()\n The problem with the code is that you should never expect that a client will provide valid input to your scripts. For example, if a curious user appends another user=foo pair to the query string, then the script would crash, because in this situation the getvalue(\"user\") method call returns a list instead of a string. Calling the upper() method on a list is not valid (since lists do not have a method of this name) and results in an AttributeError exception. Therefore, the appropriate way to read form data values was to always use the code which checks whether the obtained value is a single value or a list of values. That\u2019s annoying and leads to less readable scripts. A more convenient approach is to use the methods getfirst() and getlist() provided by this higher level interface.  \nFieldStorage.getfirst(name, default=None)  \nThis method always returns only one value associated with form field name. The method returns only the first value in case that more values were posted under such name. Please note that the order in which the values are received may vary from browser to browser and should not be counted on. 1 If no such form field or value exists then the method returns the value specified by the optional parameter default. This parameter defaults to None if not specified. \n  \nFieldStorage.getlist(name)  \nThis method always returns a list of values associated with form field name. The method returns an empty list if no such form field or value exists for name. It returns a list consisting of one item if only one such value exists. \n Using these methods you can write nice compact code: import cgi\nform = cgi.FieldStorage()\nuser = form.getfirst(\"user\", \"\").upper()    # This way it's safe.\nfor item in form.getlist(\"item\"):\n    do_something(item)\n Functions These are useful if you want more control, or if you want to employ some of the algorithms implemented in this module in other circumstances.  \ncgi.parse(fp=None, environ=os.environ, keep_blank_values=False, strict_parsing=False, separator=\"&\")  \nParse a query in the environment or from a file (the file defaults to sys.stdin). The keep_blank_values, strict_parsing and separator parameters are passed to urllib.parse.parse_qs() unchanged. \n  \ncgi.parse_multipart(fp, pdict, encoding=\"utf-8\", errors=\"replace\", separator=\"&\")  \nParse input of type multipart/form-data (for file uploads). Arguments are fp for the input file, pdict for a dictionary containing other parameters in the Content-Type header, and encoding, the request encoding. Returns a dictionary just like urllib.parse.parse_qs(): keys are the field names, each value is a list of values for that field. For non-file fields, the value is a list of strings. This is easy to use but not much good if you are expecting megabytes to be uploaded \u2014 in that case, use the FieldStorage class instead which is much more flexible.  Changed in version 3.7: Added the encoding and errors parameters. For non-file fields, the value is now a list of strings, not bytes.   Changed in version 3.9.2: Added the separator parameter.  \n  \ncgi.parse_header(string)  \nParse a MIME header (such as Content-Type) into a main value and a dictionary of parameters. \n  \ncgi.test()  \nRobust test CGI script, usable as main program. Writes minimal HTTP headers and formats all information provided to the script in HTML form. \n  \ncgi.print_environ()  \nFormat the shell environment in HTML. \n  \ncgi.print_form(form)  \nFormat a form in HTML. \n  \ncgi.print_directory()  \nFormat the current directory in HTML. \n  \ncgi.print_environ_usage()  \nPrint a list of useful (used by CGI) environment variables in HTML. \n Caring about security There\u2019s one important rule: if you invoke an external program (via the os.system() or os.popen() functions. or others with similar functionality), make very sure you don\u2019t pass arbitrary strings received from the client to the shell. This is a well-known security hole whereby clever hackers anywhere on the Web can exploit a gullible CGI script to invoke arbitrary shell commands. Even parts of the URL or field names cannot be trusted, since the request doesn\u2019t have to come from your form! To be on the safe side, if you must pass a string gotten from a form to a shell command, you should make sure the string contains only alphanumeric characters, dashes, underscores, and periods. Installing your CGI script on a Unix system Read the documentation for your HTTP server and check with your local system administrator to find the directory where CGI scripts should be installed; usually this is in a directory cgi-bin in the server tree. Make sure that your script is readable and executable by \u201cothers\u201d; the Unix file mode should be 0o755 octal (use chmod 0755 filename). Make sure that the first line of the script contains #! starting in column 1 followed by the pathname of the Python interpreter, for instance: #!/usr/local/bin/python\n Make sure the Python interpreter exists and is executable by \u201cothers\u201d. Make sure that any files your script needs to read or write are readable or writable, respectively, by \u201cothers\u201d \u2014 their mode should be 0o644 for readable and 0o666 for writable. This is because, for security reasons, the HTTP server executes your script as user \u201cnobody\u201d, without any special privileges. It can only read (write, execute) files that everybody can read (write, execute). The current directory at execution time is also different (it is usually the server\u2019s cgi-bin directory) and the set of environment variables is also different from what you get when you log in. In particular, don\u2019t count on the shell\u2019s search path for executables (PATH) or the Python module search path (PYTHONPATH) to be set to anything interesting. If you need to load modules from a directory which is not on Python\u2019s default module search path, you can change the path in your script, before importing other modules. For example: import sys\nsys.path.insert(0, \"/usr/home/joe/lib/python\")\nsys.path.insert(0, \"/usr/local/lib/python\")\n (This way, the directory inserted last will be searched first!) Instructions for non-Unix systems will vary; check your HTTP server\u2019s documentation (it will usually have a section on CGI scripts). Testing your CGI script Unfortunately, a CGI script will generally not run when you try it from the command line, and a script that works perfectly from the command line may fail mysteriously when run from the server. There\u2019s one reason why you should still test your script from the command line: if it contains a syntax error, the Python interpreter won\u2019t execute it at all, and the HTTP server will most likely send a cryptic error to the client. Assuming your script has no syntax errors, yet it does not work, you have no choice but to read the next section. Debugging CGI scripts First of all, check for trivial installation errors \u2014 reading the section above on installing your CGI script carefully can save you a lot of time. If you wonder whether you have understood the installation procedure correctly, try installing a copy of this module file (cgi.py) as a CGI script. When invoked as a script, the file will dump its environment and the contents of the form in HTML form. Give it the right mode etc, and send it a request. If it\u2019s installed in the standard cgi-bin directory, it should be possible to send it a request by entering a URL into your browser of the form: http://yourhostname/cgi-bin/cgi.py?name=Joe+Blow&addr=At+Home\n If this gives an error of type 404, the server cannot find the script \u2013 perhaps you need to install it in a different directory. If it gives another error, there\u2019s an installation problem that you should fix before trying to go any further. If you get a nicely formatted listing of the environment and form content (in this example, the fields should be listed as \u201caddr\u201d with value \u201cAt Home\u201d and \u201cname\u201d with value \u201cJoe Blow\u201d), the cgi.py script has been installed correctly. If you follow the same procedure for your own script, you should now be able to debug it. The next step could be to call the cgi module\u2019s test() function from your script: replace its main code with the single statement cgi.test()\n This should produce the same results as those gotten from installing the cgi.py file itself. When an ordinary Python script raises an unhandled exception (for whatever reason: of a typo in a module name, a file that can\u2019t be opened, etc.), the Python interpreter prints a nice traceback and exits. While the Python interpreter will still do this when your CGI script raises an exception, most likely the traceback will end up in one of the HTTP server\u2019s log files, or be discarded altogether. Fortunately, once you have managed to get your script to execute some code, you can easily send tracebacks to the Web browser using the cgitb module. If you haven\u2019t done so already, just add the lines: import cgitb\ncgitb.enable()\n to the top of your script. Then try running it again; when a problem occurs, you should see a detailed report that will likely make apparent the cause of the crash. If you suspect that there may be a problem in importing the cgitb module, you can use an even more robust approach (which only uses built-in modules): import sys\nsys.stderr = sys.stdout\nprint(\"Content-Type: text/plain\")\nprint()\n...your code here...\n This relies on the Python interpreter to print the traceback. The content type of the output is set to plain text, which disables all HTML processing. If your script works, the raw HTML will be displayed by your client. If it raises an exception, most likely after the first two lines have been printed, a traceback will be displayed. Because no HTML interpretation is going on, the traceback will be readable. Common problems and solutions  Most HTTP servers buffer the output from CGI scripts until the script is completed. This means that it is not possible to display a progress report on the client\u2019s display while the script is running. Check the installation instructions above. Check the HTTP server\u2019s log files. (tail -f logfile in a separate window may be useful!) Always check a script for syntax errors first, by doing something like python script.py. If your script does not have any syntax errors, try adding import cgitb;\ncgitb.enable() to the top of the script. When invoking external programs, make sure they can be found. Usually, this means using absolute path names \u2014 PATH is usually not set to a very useful value in a CGI script. When reading or writing external files, make sure they can be read or written by the userid under which your CGI script will be running: this is typically the userid under which the web server is running, or some explicitly specified userid for a web server\u2019s suexec feature. Don\u2019t try to give a CGI script a set-uid mode. This doesn\u2019t work on most systems, and is a security liability as well.  Footnotes  \n1  \nNote that some recent versions of the HTML specification do state what order the field values should be supplied in, but knowing whether a request was received from a conforming browser, or even from a browser at all, is tedious and error-prone.  \n"}, {"name": "cgi.FieldStorage.getfirst()", "path": "library/cgi#cgi.FieldStorage.getfirst", "type": "Internet", "text": " \nFieldStorage.getfirst(name, default=None)  \nThis method always returns only one value associated with form field name. The method returns only the first value in case that more values were posted under such name. Please note that the order in which the values are received may vary from browser to browser and should not be counted on. 1 If no such form field or value exists then the method returns the value specified by the optional parameter default. This parameter defaults to None if not specified. \n"}, {"name": "cgi.FieldStorage.getlist()", "path": "library/cgi#cgi.FieldStorage.getlist", "type": "Internet", "text": " \nFieldStorage.getlist(name)  \nThis method always returns a list of values associated with form field name. The method returns an empty list if no such form field or value exists for name. It returns a list consisting of one item if only one such value exists. \n"}, {"name": "cgi.parse()", "path": "library/cgi#cgi.parse", "type": "Internet", "text": " \ncgi.parse(fp=None, environ=os.environ, keep_blank_values=False, strict_parsing=False, separator=\"&\")  \nParse a query in the environment or from a file (the file defaults to sys.stdin). The keep_blank_values, strict_parsing and separator parameters are passed to urllib.parse.parse_qs() unchanged. \n"}, {"name": "cgi.parse_header()", "path": "library/cgi#cgi.parse_header", "type": "Internet", "text": " \ncgi.parse_header(string)  \nParse a MIME header (such as Content-Type) into a main value and a dictionary of parameters. \n"}, {"name": "cgi.parse_multipart()", "path": "library/cgi#cgi.parse_multipart", "type": "Internet", "text": " \ncgi.parse_multipart(fp, pdict, encoding=\"utf-8\", errors=\"replace\", separator=\"&\")  \nParse input of type multipart/form-data (for file uploads). Arguments are fp for the input file, pdict for a dictionary containing other parameters in the Content-Type header, and encoding, the request encoding. Returns a dictionary just like urllib.parse.parse_qs(): keys are the field names, each value is a list of values for that field. For non-file fields, the value is a list of strings. This is easy to use but not much good if you are expecting megabytes to be uploaded \u2014 in that case, use the FieldStorage class instead which is much more flexible.  Changed in version 3.7: Added the encoding and errors parameters. For non-file fields, the value is now a list of strings, not bytes.   Changed in version 3.9.2: Added the separator parameter.  \n"}, {"name": "cgi.print_directory()", "path": "library/cgi#cgi.print_directory", "type": "Internet", "text": " \ncgi.print_directory()  \nFormat the current directory in HTML. \n"}, {"name": "cgi.print_environ()", "path": "library/cgi#cgi.print_environ", "type": "Internet", "text": " \ncgi.print_environ()  \nFormat the shell environment in HTML. \n"}, {"name": "cgi.print_environ_usage()", "path": "library/cgi#cgi.print_environ_usage", "type": "Internet", "text": " \ncgi.print_environ_usage()  \nPrint a list of useful (used by CGI) environment variables in HTML. \n"}, {"name": "cgi.print_form()", "path": "library/cgi#cgi.print_form", "type": "Internet", "text": " \ncgi.print_form(form)  \nFormat a form in HTML. \n"}, {"name": "cgi.test()", "path": "library/cgi#cgi.test", "type": "Internet", "text": " \ncgi.test()  \nRobust test CGI script, usable as main program. Writes minimal HTTP headers and formats all information provided to the script in HTML form. \n"}, {"name": "cgitb", "path": "library/cgitb", "type": "Internet", "text": "cgitb \u2014 Traceback manager for CGI scripts Source code: Lib/cgitb.py The cgitb module provides a special exception handler for Python scripts. (Its name is a bit misleading. It was originally designed to display extensive traceback information in HTML for CGI scripts. It was later generalized to also display this information in plain text.) After this module is activated, if an uncaught exception occurs, a detailed, formatted report will be displayed. The report includes a traceback showing excerpts of the source code for each level, as well as the values of the arguments and local variables to currently running functions, to help you debug the problem. Optionally, you can save this information to a file instead of sending it to the browser. To enable this feature, simply add this to the top of your CGI script: import cgitb\ncgitb.enable()\n The options to the enable() function control whether the report is displayed in the browser and whether the report is logged to a file for later analysis.  \ncgitb.enable(display=1, logdir=None, context=5, format=\"html\")  \nThis function causes the cgitb module to take over the interpreter\u2019s default handling for exceptions by setting the value of sys.excepthook. The optional argument display defaults to 1 and can be set to 0 to suppress sending the traceback to the browser. If the argument logdir is present, the traceback reports are written to files. The value of logdir should be a directory where these files will be placed. The optional argument context is the number of lines of context to display around the current line of source code in the traceback; this defaults to 5. If the optional argument format is \"html\", the output is formatted as HTML. Any other value forces plain text output. The default value is \"html\". \n  \ncgitb.text(info, context=5)  \nThis function handles the exception described by info (a 3-tuple containing the result of sys.exc_info()), formatting its traceback as text and returning the result as a string. The optional argument context is the number of lines of context to display around the current line of source code in the traceback; this defaults to 5. \n  \ncgitb.html(info, context=5)  \nThis function handles the exception described by info (a 3-tuple containing the result of sys.exc_info()), formatting its traceback as HTML and returning the result as a string. The optional argument context is the number of lines of context to display around the current line of source code in the traceback; this defaults to 5. \n  \ncgitb.handler(info=None)  \nThis function handles an exception using the default settings (that is, show a report in the browser, but don\u2019t log to a file). This can be used when you\u2019ve caught an exception and want to report it using cgitb. The optional info argument should be a 3-tuple containing an exception type, exception value, and traceback object, exactly like the tuple returned by sys.exc_info(). If the info argument is not supplied, the current exception is obtained from sys.exc_info(). \n\n"}, {"name": "cgitb.enable()", "path": "library/cgitb#cgitb.enable", "type": "Internet", "text": " \ncgitb.enable(display=1, logdir=None, context=5, format=\"html\")  \nThis function causes the cgitb module to take over the interpreter\u2019s default handling for exceptions by setting the value of sys.excepthook. The optional argument display defaults to 1 and can be set to 0 to suppress sending the traceback to the browser. If the argument logdir is present, the traceback reports are written to files. The value of logdir should be a directory where these files will be placed. The optional argument context is the number of lines of context to display around the current line of source code in the traceback; this defaults to 5. If the optional argument format is \"html\", the output is formatted as HTML. Any other value forces plain text output. The default value is \"html\". \n"}, {"name": "cgitb.handler()", "path": "library/cgitb#cgitb.handler", "type": "Internet", "text": " \ncgitb.handler(info=None)  \nThis function handles an exception using the default settings (that is, show a report in the browser, but don\u2019t log to a file). This can be used when you\u2019ve caught an exception and want to report it using cgitb. The optional info argument should be a 3-tuple containing an exception type, exception value, and traceback object, exactly like the tuple returned by sys.exc_info(). If the info argument is not supplied, the current exception is obtained from sys.exc_info(). \n"}, {"name": "cgitb.html()", "path": "library/cgitb#cgitb.html", "type": "Internet", "text": " \ncgitb.html(info, context=5)  \nThis function handles the exception described by info (a 3-tuple containing the result of sys.exc_info()), formatting its traceback as HTML and returning the result as a string. The optional argument context is the number of lines of context to display around the current line of source code in the traceback; this defaults to 5. \n"}, {"name": "cgitb.text()", "path": "library/cgitb#cgitb.text", "type": "Internet", "text": " \ncgitb.text(info, context=5)  \nThis function handles the exception described by info (a 3-tuple containing the result of sys.exc_info()), formatting its traceback as text and returning the result as a string. The optional argument context is the number of lines of context to display around the current line of source code in the traceback; this defaults to 5. \n"}, {"name": "ChildProcessError", "path": "library/exceptions#ChildProcessError", "type": "Built-in Exceptions", "text": " \nexception ChildProcessError  \nRaised when an operation on a child process failed. Corresponds to errno ECHILD. \n"}, {"name": "chr()", "path": "library/functions#chr", "type": "Built-in Functions", "text": " \nchr(i)  \nReturn the string representing a character whose Unicode code point is the integer i. For example, chr(97) returns the string 'a', while chr(8364) returns the string '\u20ac'. This is the inverse of ord(). The valid range for the argument is from 0 through 1,114,111 (0x10FFFF in base 16). ValueError will be raised if i is outside that range. \n"}, {"name": "chunk", "path": "library/chunk", "type": "Multimedia", "text": "chunk \u2014 Read IFF chunked data Source code: Lib/chunk.py This module provides an interface for reading files that use EA IFF 85 chunks. 1 This format is used in at least the Audio Interchange File Format (AIFF/AIFF-C) and the Real Media File Format (RMFF). The WAVE audio file format is closely related and can also be read using this module. A chunk has the following structure:   \nOffset Length Contents   \n0 4 Chunk ID  \n4 4 Size of chunk in big-endian byte order, not including the header  \n8 n Data bytes, where n is the size given in the preceding field  \n8 + n 0 or 1 Pad byte needed if n is odd and chunk alignment is used   The ID is a 4-byte string which identifies the type of chunk. The size field (a 32-bit value, encoded using big-endian byte order) gives the size of the chunk data, not including the 8-byte header. Usually an IFF-type file consists of one or more chunks. The proposed usage of the Chunk class defined here is to instantiate an instance at the start of each chunk and read from the instance until it reaches the end, after which a new instance can be instantiated. At the end of the file, creating a new instance will fail with an EOFError exception.  \nclass chunk.Chunk(file, align=True, bigendian=True, inclheader=False)  \nClass which represents a chunk. The file argument is expected to be a file-like object. An instance of this class is specifically allowed. The only method that is needed is read(). If the methods seek() and tell() are present and don\u2019t raise an exception, they are also used. If these methods are present and raise an exception, they are expected to not have altered the object. If the optional argument align is true, chunks are assumed to be aligned on 2-byte boundaries. If align is false, no alignment is assumed. The default value is true. If the optional argument bigendian is false, the chunk size is assumed to be in little-endian order. This is needed for WAVE audio files. The default value is true. If the optional argument inclheader is true, the size given in the chunk header includes the size of the header. The default value is false. A Chunk object supports the following methods:  \ngetname()  \nReturns the name (ID) of the chunk. This is the first 4 bytes of the chunk. \n  \ngetsize()  \nReturns the size of the chunk. \n  \nclose()  \nClose and skip to the end of the chunk. This does not close the underlying file. \n The remaining methods will raise OSError if called after the close() method has been called. Before Python 3.3, they used to raise IOError, now an alias of OSError.  \nisatty()  \nReturns False. \n  \nseek(pos, whence=0)  \nSet the chunk\u2019s current position. The whence argument is optional and defaults to 0 (absolute file positioning); other values are 1 (seek relative to the current position) and 2 (seek relative to the file\u2019s end). There is no return value. If the underlying file does not allow seek, only forward seeks are allowed. \n  \ntell()  \nReturn the current position into the chunk. \n  \nread(size=-1)  \nRead at most size bytes from the chunk (less if the read hits the end of the chunk before obtaining size bytes). If the size argument is negative or omitted, read all data until the end of the chunk. An empty bytes object is returned when the end of the chunk is encountered immediately. \n  \nskip()  \nSkip to the end of the chunk. All further calls to read() for the chunk will return b''. If you are not interested in the contents of the chunk, this method should be called so that the file points to the start of the next chunk. \n \n Footnotes  \n1  \n\u201cEA IFF 85\u201d Standard for Interchange Format Files, Jerry Morrison, Electronic Arts, January 1985.  \n"}, {"name": "chunk.Chunk", "path": "library/chunk#chunk.Chunk", "type": "Multimedia", "text": " \nclass chunk.Chunk(file, align=True, bigendian=True, inclheader=False)  \nClass which represents a chunk. The file argument is expected to be a file-like object. An instance of this class is specifically allowed. The only method that is needed is read(). If the methods seek() and tell() are present and don\u2019t raise an exception, they are also used. If these methods are present and raise an exception, they are expected to not have altered the object. If the optional argument align is true, chunks are assumed to be aligned on 2-byte boundaries. If align is false, no alignment is assumed. The default value is true. If the optional argument bigendian is false, the chunk size is assumed to be in little-endian order. This is needed for WAVE audio files. The default value is true. If the optional argument inclheader is true, the size given in the chunk header includes the size of the header. The default value is false. A Chunk object supports the following methods:  \ngetname()  \nReturns the name (ID) of the chunk. This is the first 4 bytes of the chunk. \n  \ngetsize()  \nReturns the size of the chunk. \n  \nclose()  \nClose and skip to the end of the chunk. This does not close the underlying file. \n The remaining methods will raise OSError if called after the close() method has been called. Before Python 3.3, they used to raise IOError, now an alias of OSError.  \nisatty()  \nReturns False. \n  \nseek(pos, whence=0)  \nSet the chunk\u2019s current position. The whence argument is optional and defaults to 0 (absolute file positioning); other values are 1 (seek relative to the current position) and 2 (seek relative to the file\u2019s end). There is no return value. If the underlying file does not allow seek, only forward seeks are allowed. \n  \ntell()  \nReturn the current position into the chunk. \n  \nread(size=-1)  \nRead at most size bytes from the chunk (less if the read hits the end of the chunk before obtaining size bytes). If the size argument is negative or omitted, read all data until the end of the chunk. An empty bytes object is returned when the end of the chunk is encountered immediately. \n  \nskip()  \nSkip to the end of the chunk. All further calls to read() for the chunk will return b''. If you are not interested in the contents of the chunk, this method should be called so that the file points to the start of the next chunk. \n \n"}, {"name": "chunk.Chunk.close()", "path": "library/chunk#chunk.Chunk.close", "type": "Multimedia", "text": " \nclose()  \nClose and skip to the end of the chunk. This does not close the underlying file. \n"}, {"name": "chunk.Chunk.getname()", "path": "library/chunk#chunk.Chunk.getname", "type": "Multimedia", "text": " \ngetname()  \nReturns the name (ID) of the chunk. This is the first 4 bytes of the chunk. \n"}, {"name": "chunk.Chunk.getsize()", "path": "library/chunk#chunk.Chunk.getsize", "type": "Multimedia", "text": " \ngetsize()  \nReturns the size of the chunk. \n"}, {"name": "chunk.Chunk.isatty()", "path": "library/chunk#chunk.Chunk.isatty", "type": "Multimedia", "text": " \nisatty()  \nReturns False. \n"}, {"name": "chunk.Chunk.read()", "path": "library/chunk#chunk.Chunk.read", "type": "Multimedia", "text": " \nread(size=-1)  \nRead at most size bytes from the chunk (less if the read hits the end of the chunk before obtaining size bytes). If the size argument is negative or omitted, read all data until the end of the chunk. An empty bytes object is returned when the end of the chunk is encountered immediately. \n"}, {"name": "chunk.Chunk.seek()", "path": "library/chunk#chunk.Chunk.seek", "type": "Multimedia", "text": " \nseek(pos, whence=0)  \nSet the chunk\u2019s current position. The whence argument is optional and defaults to 0 (absolute file positioning); other values are 1 (seek relative to the current position) and 2 (seek relative to the file\u2019s end). There is no return value. If the underlying file does not allow seek, only forward seeks are allowed. \n"}, {"name": "chunk.Chunk.skip()", "path": "library/chunk#chunk.Chunk.skip", "type": "Multimedia", "text": " \nskip()  \nSkip to the end of the chunk. All further calls to read() for the chunk will return b''. If you are not interested in the contents of the chunk, this method should be called so that the file points to the start of the next chunk. \n"}, {"name": "chunk.Chunk.tell()", "path": "library/chunk#chunk.Chunk.tell", "type": "Multimedia", "text": " \ntell()  \nReturn the current position into the chunk. \n"}, {"name": "class.mro()", "path": "library/stdtypes#class.mro", "type": "Built-in Types", "text": " \nclass.mro()  \nThis method can be overridden by a metaclass to customize the method resolution order for its instances. It is called at class instantiation, and its result is stored in __mro__. \n"}, {"name": "class.__bases__", "path": "library/stdtypes#class.__bases__", "type": "Built-in Types", "text": " \nclass.__bases__  \nThe tuple of base classes of a class object. \n"}, {"name": "class.__mro__", "path": "library/stdtypes#class.__mro__", "type": "Built-in Types", "text": " \nclass.__mro__  \nThis attribute is a tuple of classes that are considered when looking for base classes during method resolution. \n"}, {"name": "class.__subclasses__()", "path": "library/stdtypes#class.__subclasses__", "type": "Built-in Types", "text": " \nclass.__subclasses__()  \nEach class keeps a list of weak references to its immediate subclasses. This method returns a list of all those references still alive. The list is in definition order. Example: >>> int.__subclasses__()\n[<class 'bool'>]\n \n"}, {"name": "classmethod()", "path": "library/functions#classmethod", "type": "Built-in Functions", "text": " \n@classmethod  \nTransform a method into a class method. A class method receives the class as implicit first argument, just like an instance method receives the instance. To declare a class method, use this idiom: class C:\n    @classmethod\n    def f(cls, arg1, arg2, ...): ...\n The @classmethod form is a function decorator \u2013 see Function definitions for details. A class method can be called either on the class (such as C.f()) or on an instance (such as C().f()). The instance is ignored except for its class. If a class method is called for a derived class, the derived class object is passed as the implied first argument. Class methods are different than C++ or Java static methods. If you want those, see staticmethod() in this section. For more information on class methods, see The standard type hierarchy.  Changed in version 3.9: Class methods can now wrap other descriptors such as property().  \n"}, {"name": "cmath", "path": "library/cmath", "type": "Numeric & Mathematical", "text": "cmath \u2014 Mathematical functions for complex numbers This module provides access to mathematical functions for complex numbers. The functions in this module accept integers, floating-point numbers or complex numbers as arguments. They will also accept any Python object that has either a __complex__() or a __float__() method: these methods are used to convert the object to a complex or floating-point number, respectively, and the function is then applied to the result of the conversion.  Note On platforms with hardware and system-level support for signed zeros, functions involving branch cuts are continuous on both sides of the branch cut: the sign of the zero distinguishes one side of the branch cut from the other. On platforms that do not support signed zeros the continuity is as specified below.  Conversions to and from polar coordinates A Python complex number z is stored internally using rectangular or Cartesian coordinates. It is completely determined by its real part z.real and its imaginary part z.imag. In other words: z == z.real + z.imag*1j\n Polar coordinates give an alternative way to represent a complex number. In polar coordinates, a complex number z is defined by the modulus r and the phase angle phi. The modulus r is the distance from z to the origin, while the phase phi is the counterclockwise angle, measured in radians, from the positive x-axis to the line segment that joins the origin to z. The following functions can be used to convert from the native rectangular coordinates to polar coordinates and back.  \ncmath.phase(x)  \nReturn the phase of x (also known as the argument of x), as a float. phase(x) is equivalent to math.atan2(x.imag,\nx.real). The result lies in the range [-\u03c0, \u03c0], and the branch cut for this operation lies along the negative real axis, continuous from above. On systems with support for signed zeros (which includes most systems in current use), this means that the sign of the result is the same as the sign of x.imag, even when x.imag is zero: >>> phase(complex(-1.0, 0.0))\n3.141592653589793\n>>> phase(complex(-1.0, -0.0))\n-3.141592653589793\n \n  Note The modulus (absolute value) of a complex number x can be computed using the built-in abs() function. There is no separate cmath module function for this operation.   \ncmath.polar(x)  \nReturn the representation of x in polar coordinates. Returns a pair (r, phi) where r is the modulus of x and phi is the phase of x. polar(x) is equivalent to (abs(x),\nphase(x)). \n  \ncmath.rect(r, phi)  \nReturn the complex number x with polar coordinates r and phi. Equivalent to r * (math.cos(phi) + math.sin(phi)*1j). \n Power and logarithmic functions  \ncmath.exp(x)  \nReturn e raised to the power x, where e is the base of natural logarithms. \n  \ncmath.log(x[, base])  \nReturns the logarithm of x to the given base. If the base is not specified, returns the natural logarithm of x. There is one branch cut, from 0 along the negative real axis to -\u221e, continuous from above. \n  \ncmath.log10(x)  \nReturn the base-10 logarithm of x. This has the same branch cut as log(). \n  \ncmath.sqrt(x)  \nReturn the square root of x. This has the same branch cut as log(). \n Trigonometric functions  \ncmath.acos(x)  \nReturn the arc cosine of x. There are two branch cuts: One extends right from 1 along the real axis to \u221e, continuous from below. The other extends left from -1 along the real axis to -\u221e, continuous from above. \n  \ncmath.asin(x)  \nReturn the arc sine of x. This has the same branch cuts as acos(). \n  \ncmath.atan(x)  \nReturn the arc tangent of x. There are two branch cuts: One extends from 1j along the imaginary axis to \u221ej, continuous from the right. The other extends from -1j along the imaginary axis to -\u221ej, continuous from the left. \n  \ncmath.cos(x)  \nReturn the cosine of x. \n  \ncmath.sin(x)  \nReturn the sine of x. \n  \ncmath.tan(x)  \nReturn the tangent of x. \n Hyperbolic functions  \ncmath.acosh(x)  \nReturn the inverse hyperbolic cosine of x. There is one branch cut, extending left from 1 along the real axis to -\u221e, continuous from above. \n  \ncmath.asinh(x)  \nReturn the inverse hyperbolic sine of x. There are two branch cuts: One extends from 1j along the imaginary axis to \u221ej, continuous from the right. The other extends from -1j along the imaginary axis to -\u221ej, continuous from the left. \n  \ncmath.atanh(x)  \nReturn the inverse hyperbolic tangent of x. There are two branch cuts: One extends from 1 along the real axis to \u221e, continuous from below. The other extends from -1 along the real axis to -\u221e, continuous from above. \n  \ncmath.cosh(x)  \nReturn the hyperbolic cosine of x. \n  \ncmath.sinh(x)  \nReturn the hyperbolic sine of x. \n  \ncmath.tanh(x)  \nReturn the hyperbolic tangent of x. \n Classification functions  \ncmath.isfinite(x)  \nReturn True if both the real and imaginary parts of x are finite, and False otherwise.  New in version 3.2.  \n  \ncmath.isinf(x)  \nReturn True if either the real or the imaginary part of x is an infinity, and False otherwise. \n  \ncmath.isnan(x)  \nReturn True if either the real or the imaginary part of x is a NaN, and False otherwise. \n  \ncmath.isclose(a, b, *, rel_tol=1e-09, abs_tol=0.0)  \nReturn True if the values a and b are close to each other and False otherwise. Whether or not two values are considered close is determined according to given absolute and relative tolerances. rel_tol is the relative tolerance \u2013 it is the maximum allowed difference between a and b, relative to the larger absolute value of a or b. For example, to set a tolerance of 5%, pass rel_tol=0.05. The default tolerance is 1e-09, which assures that the two values are the same within about 9 decimal digits. rel_tol must be greater than zero. abs_tol is the minimum absolute tolerance \u2013 useful for comparisons near zero. abs_tol must be at least zero. If no errors occur, the result will be: abs(a-b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol). The IEEE 754 special values of NaN, inf, and -inf will be handled according to IEEE rules. Specifically, NaN is not considered close to any other value, including NaN. inf and -inf are only considered close to themselves.  New in version 3.5.   See also PEP 485 \u2013 A function for testing approximate equality  \n Constants  \ncmath.pi  \nThe mathematical constant \u03c0, as a float. \n  \ncmath.e  \nThe mathematical constant e, as a float. \n  \ncmath.tau  \nThe mathematical constant \u03c4, as a float.  New in version 3.6.  \n  \ncmath.inf  \nFloating-point positive infinity. Equivalent to float('inf').  New in version 3.6.  \n  \ncmath.infj  \nComplex number with zero real part and positive infinity imaginary part. Equivalent to complex(0.0, float('inf')).  New in version 3.6.  \n  \ncmath.nan  \nA floating-point \u201cnot a number\u201d (NaN) value. Equivalent to float('nan').  New in version 3.6.  \n  \ncmath.nanj  \nComplex number with zero real part and NaN imaginary part. Equivalent to complex(0.0, float('nan')).  New in version 3.6.  \n Note that the selection of functions is similar, but not identical, to that in module math. The reason for having two modules is that some users aren\u2019t interested in complex numbers, and perhaps don\u2019t even know what they are. They would rather have math.sqrt(-1) raise an exception than return a complex number. Also note that the functions defined in cmath always return a complex number, even if the answer can be expressed as a real number (in which case the complex number has an imaginary part of zero). A note on branch cuts: They are curves along which the given function fails to be continuous. They are a necessary feature of many complex functions. It is assumed that if you need to compute with complex functions, you will understand about branch cuts. Consult almost any (not too elementary) book on complex variables for enlightenment. For information of the proper choice of branch cuts for numerical purposes, a good reference should be the following:  See also Kahan, W: Branch cuts for complex elementary functions; or, Much ado about nothing\u2019s sign bit. In Iserles, A., and Powell, M. (eds.), The state of the art in numerical analysis. Clarendon Press (1987) pp165\u2013211. \n"}, {"name": "cmath.acos()", "path": "library/cmath#cmath.acos", "type": "Numeric & Mathematical", "text": " \ncmath.acos(x)  \nReturn the arc cosine of x. There are two branch cuts: One extends right from 1 along the real axis to \u221e, continuous from below. The other extends left from -1 along the real axis to -\u221e, continuous from above. \n"}, {"name": "cmath.acosh()", "path": "library/cmath#cmath.acosh", "type": "Numeric & Mathematical", "text": " \ncmath.acosh(x)  \nReturn the inverse hyperbolic cosine of x. There is one branch cut, extending left from 1 along the real axis to -\u221e, continuous from above. \n"}, {"name": "cmath.asin()", "path": "library/cmath#cmath.asin", "type": "Numeric & Mathematical", "text": " \ncmath.asin(x)  \nReturn the arc sine of x. This has the same branch cuts as acos(). \n"}, {"name": "cmath.asinh()", "path": "library/cmath#cmath.asinh", "type": "Numeric & Mathematical", "text": " \ncmath.asinh(x)  \nReturn the inverse hyperbolic sine of x. There are two branch cuts: One extends from 1j along the imaginary axis to \u221ej, continuous from the right. The other extends from -1j along the imaginary axis to -\u221ej, continuous from the left. \n"}, {"name": "cmath.atan()", "path": "library/cmath#cmath.atan", "type": "Numeric & Mathematical", "text": " \ncmath.atan(x)  \nReturn the arc tangent of x. There are two branch cuts: One extends from 1j along the imaginary axis to \u221ej, continuous from the right. The other extends from -1j along the imaginary axis to -\u221ej, continuous from the left. \n"}, {"name": "cmath.atanh()", "path": "library/cmath#cmath.atanh", "type": "Numeric & Mathematical", "text": " \ncmath.atanh(x)  \nReturn the inverse hyperbolic tangent of x. There are two branch cuts: One extends from 1 along the real axis to \u221e, continuous from below. The other extends from -1 along the real axis to -\u221e, continuous from above. \n"}, {"name": "cmath.cos()", "path": "library/cmath#cmath.cos", "type": "Numeric & Mathematical", "text": " \ncmath.cos(x)  \nReturn the cosine of x. \n"}, {"name": "cmath.cosh()", "path": "library/cmath#cmath.cosh", "type": "Numeric & Mathematical", "text": " \ncmath.cosh(x)  \nReturn the hyperbolic cosine of x. \n"}, {"name": "cmath.e", "path": "library/cmath#cmath.e", "type": "Numeric & Mathematical", "text": " \ncmath.e  \nThe mathematical constant e, as a float. \n"}, {"name": "cmath.exp()", "path": "library/cmath#cmath.exp", "type": "Numeric & Mathematical", "text": " \ncmath.exp(x)  \nReturn e raised to the power x, where e is the base of natural logarithms. \n"}, {"name": "cmath.inf", "path": "library/cmath#cmath.inf", "type": "Numeric & Mathematical", "text": " \ncmath.inf  \nFloating-point positive infinity. Equivalent to float('inf').  New in version 3.6.  \n"}, {"name": "cmath.infj", "path": "library/cmath#cmath.infj", "type": "Numeric & Mathematical", "text": " \ncmath.infj  \nComplex number with zero real part and positive infinity imaginary part. Equivalent to complex(0.0, float('inf')).  New in version 3.6.  \n"}, {"name": "cmath.isclose()", "path": "library/cmath#cmath.isclose", "type": "Numeric & Mathematical", "text": " \ncmath.isclose(a, b, *, rel_tol=1e-09, abs_tol=0.0)  \nReturn True if the values a and b are close to each other and False otherwise. Whether or not two values are considered close is determined according to given absolute and relative tolerances. rel_tol is the relative tolerance \u2013 it is the maximum allowed difference between a and b, relative to the larger absolute value of a or b. For example, to set a tolerance of 5%, pass rel_tol=0.05. The default tolerance is 1e-09, which assures that the two values are the same within about 9 decimal digits. rel_tol must be greater than zero. abs_tol is the minimum absolute tolerance \u2013 useful for comparisons near zero. abs_tol must be at least zero. If no errors occur, the result will be: abs(a-b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol). The IEEE 754 special values of NaN, inf, and -inf will be handled according to IEEE rules. Specifically, NaN is not considered close to any other value, including NaN. inf and -inf are only considered close to themselves.  New in version 3.5.   See also PEP 485 \u2013 A function for testing approximate equality  \n"}, {"name": "cmath.isfinite()", "path": "library/cmath#cmath.isfinite", "type": "Numeric & Mathematical", "text": " \ncmath.isfinite(x)  \nReturn True if both the real and imaginary parts of x are finite, and False otherwise.  New in version 3.2.  \n"}, {"name": "cmath.isinf()", "path": "library/cmath#cmath.isinf", "type": "Numeric & Mathematical", "text": " \ncmath.isinf(x)  \nReturn True if either the real or the imaginary part of x is an infinity, and False otherwise. \n"}, {"name": "cmath.isnan()", "path": "library/cmath#cmath.isnan", "type": "Numeric & Mathematical", "text": " \ncmath.isnan(x)  \nReturn True if either the real or the imaginary part of x is a NaN, and False otherwise. \n"}, {"name": "cmath.log()", "path": "library/cmath#cmath.log", "type": "Numeric & Mathematical", "text": " \ncmath.log(x[, base])  \nReturns the logarithm of x to the given base. If the base is not specified, returns the natural logarithm of x. There is one branch cut, from 0 along the negative real axis to -\u221e, continuous from above. \n"}, {"name": "cmath.log10()", "path": "library/cmath#cmath.log10", "type": "Numeric & Mathematical", "text": " \ncmath.log10(x)  \nReturn the base-10 logarithm of x. This has the same branch cut as log(). \n"}, {"name": "cmath.nan", "path": "library/cmath#cmath.nan", "type": "Numeric & Mathematical", "text": " \ncmath.nan  \nA floating-point \u201cnot a number\u201d (NaN) value. Equivalent to float('nan').  New in version 3.6.  \n"}, {"name": "cmath.nanj", "path": "library/cmath#cmath.nanj", "type": "Numeric & Mathematical", "text": " \ncmath.nanj  \nComplex number with zero real part and NaN imaginary part. Equivalent to complex(0.0, float('nan')).  New in version 3.6.  \n"}, {"name": "cmath.phase()", "path": "library/cmath#cmath.phase", "type": "Numeric & Mathematical", "text": " \ncmath.phase(x)  \nReturn the phase of x (also known as the argument of x), as a float. phase(x) is equivalent to math.atan2(x.imag,\nx.real). The result lies in the range [-\u03c0, \u03c0], and the branch cut for this operation lies along the negative real axis, continuous from above. On systems with support for signed zeros (which includes most systems in current use), this means that the sign of the result is the same as the sign of x.imag, even when x.imag is zero: >>> phase(complex(-1.0, 0.0))\n3.141592653589793\n>>> phase(complex(-1.0, -0.0))\n-3.141592653589793\n \n"}, {"name": "cmath.pi", "path": "library/cmath#cmath.pi", "type": "Numeric & Mathematical", "text": " \ncmath.pi  \nThe mathematical constant \u03c0, as a float. \n"}, {"name": "cmath.polar()", "path": "library/cmath#cmath.polar", "type": "Numeric & Mathematical", "text": " \ncmath.polar(x)  \nReturn the representation of x in polar coordinates. Returns a pair (r, phi) where r is the modulus of x and phi is the phase of x. polar(x) is equivalent to (abs(x),\nphase(x)). \n"}, {"name": "cmath.rect()", "path": "library/cmath#cmath.rect", "type": "Numeric & Mathematical", "text": " \ncmath.rect(r, phi)  \nReturn the complex number x with polar coordinates r and phi. Equivalent to r * (math.cos(phi) + math.sin(phi)*1j). \n"}, {"name": "cmath.sin()", "path": "library/cmath#cmath.sin", "type": "Numeric & Mathematical", "text": " \ncmath.sin(x)  \nReturn the sine of x. \n"}, {"name": "cmath.sinh()", "path": "library/cmath#cmath.sinh", "type": "Numeric & Mathematical", "text": " \ncmath.sinh(x)  \nReturn the hyperbolic sine of x. \n"}, {"name": "cmath.sqrt()", "path": "library/cmath#cmath.sqrt", "type": "Numeric & Mathematical", "text": " \ncmath.sqrt(x)  \nReturn the square root of x. This has the same branch cut as log(). \n"}, {"name": "cmath.tan()", "path": "library/cmath#cmath.tan", "type": "Numeric & Mathematical", "text": " \ncmath.tan(x)  \nReturn the tangent of x. \n"}, {"name": "cmath.tanh()", "path": "library/cmath#cmath.tanh", "type": "Numeric & Mathematical", "text": " \ncmath.tanh(x)  \nReturn the hyperbolic tangent of x. \n"}, {"name": "cmath.tau", "path": "library/cmath#cmath.tau", "type": "Numeric & Mathematical", "text": " \ncmath.tau  \nThe mathematical constant \u03c4, as a float.  New in version 3.6.  \n"}, {"name": "cmd", "path": "library/cmd", "type": "Frameworks", "text": "cmd \u2014 Support for line-oriented command interpreters Source code: Lib/cmd.py The Cmd class provides a simple framework for writing line-oriented command interpreters. These are often useful for test harnesses, administrative tools, and prototypes that will later be wrapped in a more sophisticated interface.  \nclass cmd.Cmd(completekey='tab', stdin=None, stdout=None)  \nA Cmd instance or subclass instance is a line-oriented interpreter framework. There is no good reason to instantiate Cmd itself; rather, it\u2019s useful as a superclass of an interpreter class you define yourself in order to inherit Cmd\u2019s methods and encapsulate action methods. The optional argument completekey is the readline name of a completion key; it defaults to Tab. If completekey is not None and readline is available, command completion is done automatically. The optional arguments stdin and stdout specify the input and output file objects that the Cmd instance or subclass instance will use for input and output. If not specified, they will default to sys.stdin and sys.stdout. If you want a given stdin to be used, make sure to set the instance\u2019s use_rawinput attribute to False, otherwise stdin will be ignored. \n Cmd Objects A Cmd instance has the following methods:  \nCmd.cmdloop(intro=None)  \nRepeatedly issue a prompt, accept input, parse an initial prefix off the received input, and dispatch to action methods, passing them the remainder of the line as argument. The optional argument is a banner or intro string to be issued before the first prompt (this overrides the intro class attribute). If the readline module is loaded, input will automatically inherit bash-like history-list editing (e.g. Control-P scrolls back to the last command, Control-N forward to the next one, Control-F moves the cursor to the right non-destructively, Control-B moves the cursor to the left non-destructively, etc.). An end-of-file on input is passed back as the string 'EOF'. An interpreter instance will recognize a command name foo if and only if it has a method do_foo(). As a special case, a line beginning with the character '?' is dispatched to the method do_help(). As another special case, a line beginning with the character '!' is dispatched to the method do_shell() (if such a method is defined). This method will return when the postcmd() method returns a true value. The stop argument to postcmd() is the return value from the command\u2019s corresponding do_*() method. If completion is enabled, completing commands will be done automatically, and completing of commands args is done by calling complete_foo() with arguments text, line, begidx, and endidx. text is the string prefix we are attempting to match: all returned matches must begin with it. line is the current input line with leading whitespace removed, begidx and endidx are the beginning and ending indexes of the prefix text, which could be used to provide different completion depending upon which position the argument is in. All subclasses of Cmd inherit a predefined do_help(). This method, called with an argument 'bar', invokes the corresponding method help_bar(), and if that is not present, prints the docstring of do_bar(), if available. With no argument, do_help() lists all available help topics (that is, all commands with corresponding help_*() methods or commands that have docstrings), and also lists any undocumented commands. \n  \nCmd.onecmd(str)  \nInterpret the argument as though it had been typed in response to the prompt. This may be overridden, but should not normally need to be; see the precmd() and postcmd() methods for useful execution hooks. The return value is a flag indicating whether interpretation of commands by the interpreter should stop. If there is a do_*() method for the command str, the return value of that method is returned, otherwise the return value from the default() method is returned. \n  \nCmd.emptyline()  \nMethod called when an empty line is entered in response to the prompt. If this method is not overridden, it repeats the last nonempty command entered. \n  \nCmd.default(line)  \nMethod called on an input line when the command prefix is not recognized. If this method is not overridden, it prints an error message and returns. \n  \nCmd.completedefault(text, line, begidx, endidx)  \nMethod called to complete an input line when no command-specific complete_*() method is available. By default, it returns an empty list. \n  \nCmd.precmd(line)  \nHook method executed just before the command line line is interpreted, but after the input prompt is generated and issued. This method is a stub in Cmd; it exists to be overridden by subclasses. The return value is used as the command which will be executed by the onecmd() method; the precmd() implementation may re-write the command or simply return line unchanged. \n  \nCmd.postcmd(stop, line)  \nHook method executed just after a command dispatch is finished. This method is a stub in Cmd; it exists to be overridden by subclasses. line is the command line which was executed, and stop is a flag which indicates whether execution will be terminated after the call to postcmd(); this will be the return value of the onecmd() method. The return value of this method will be used as the new value for the internal flag which corresponds to stop; returning false will cause interpretation to continue. \n  \nCmd.preloop()  \nHook method executed once when cmdloop() is called. This method is a stub in Cmd; it exists to be overridden by subclasses. \n  \nCmd.postloop()  \nHook method executed once when cmdloop() is about to return. This method is a stub in Cmd; it exists to be overridden by subclasses. \n Instances of Cmd subclasses have some public instance variables:  \nCmd.prompt  \nThe prompt issued to solicit input. \n  \nCmd.identchars  \nThe string of characters accepted for the command prefix. \n  \nCmd.lastcmd  \nThe last nonempty command prefix seen. \n  \nCmd.cmdqueue  \nA list of queued input lines. The cmdqueue list is checked in cmdloop() when new input is needed; if it is nonempty, its elements will be processed in order, as if entered at the prompt. \n  \nCmd.intro  \nA string to issue as an intro or banner. May be overridden by giving the cmdloop() method an argument. \n  \nCmd.doc_header  \nThe header to issue if the help output has a section for documented commands. \n  \nCmd.misc_header  \nThe header to issue if the help output has a section for miscellaneous help topics (that is, there are help_*() methods without corresponding do_*() methods). \n  \nCmd.undoc_header  \nThe header to issue if the help output has a section for undocumented commands (that is, there are do_*() methods without corresponding help_*() methods). \n  \nCmd.ruler  \nThe character used to draw separator lines under the help-message headers. If empty, no ruler line is drawn. It defaults to '='. \n  \nCmd.use_rawinput  \nA flag, defaulting to true. If true, cmdloop() uses input() to display a prompt and read the next command; if false, sys.stdout.write() and sys.stdin.readline() are used. (This means that by importing readline, on systems that support it, the interpreter will automatically support Emacs-like line editing and command-history keystrokes.) \n Cmd Example The cmd module is mainly useful for building custom shells that let a user work with a program interactively. This section presents a simple example of how to build a shell around a few of the commands in the turtle module. Basic turtle commands such as forward() are added to a Cmd subclass with method named do_forward(). The argument is converted to a number and dispatched to the turtle module. The docstring is used in the help utility provided by the shell. The example also includes a basic record and playback facility implemented with the precmd() method which is responsible for converting the input to lowercase and writing the commands to a file. The do_playback() method reads the file and adds the recorded commands to the cmdqueue for immediate playback: import cmd, sys\nfrom turtle import *\n\nclass TurtleShell(cmd.Cmd):\n    intro = 'Welcome to the turtle shell.   Type help or ? to list commands.\\n'\n    prompt = '(turtle) '\n    file = None\n\n    # ----- basic turtle commands -----\n    def do_forward(self, arg):\n        'Move the turtle forward by the specified distance:  FORWARD 10'\n        forward(*parse(arg))\n    def do_right(self, arg):\n        'Turn turtle right by given number of degrees:  RIGHT 20'\n        right(*parse(arg))\n    def do_left(self, arg):\n        'Turn turtle left by given number of degrees:  LEFT 90'\n        left(*parse(arg))\n    def do_goto(self, arg):\n        'Move turtle to an absolute position with changing orientation.  GOTO 100 200'\n        goto(*parse(arg))\n    def do_home(self, arg):\n        'Return turtle to the home position:  HOME'\n        home()\n    def do_circle(self, arg):\n        'Draw circle with given radius an options extent and steps:  CIRCLE 50'\n        circle(*parse(arg))\n    def do_position(self, arg):\n        'Print the current turtle position:  POSITION'\n        print('Current position is %d %d\\n' % position())\n    def do_heading(self, arg):\n        'Print the current turtle heading in degrees:  HEADING'\n        print('Current heading is %d\\n' % (heading(),))\n    def do_color(self, arg):\n        'Set the color:  COLOR BLUE'\n        color(arg.lower())\n    def do_undo(self, arg):\n        'Undo (repeatedly) the last turtle action(s):  UNDO'\n    def do_reset(self, arg):\n        'Clear the screen and return turtle to center:  RESET'\n        reset()\n    def do_bye(self, arg):\n        'Stop recording, close the turtle window, and exit:  BYE'\n        print('Thank you for using Turtle')\n        self.close()\n        bye()\n        return True\n\n    # ----- record and playback -----\n    def do_record(self, arg):\n        'Save future commands to filename:  RECORD rose.cmd'\n        self.file = open(arg, 'w')\n    def do_playback(self, arg):\n        'Playback commands from a file:  PLAYBACK rose.cmd'\n        self.close()\n        with open(arg) as f:\n            self.cmdqueue.extend(f.read().splitlines())\n    def precmd(self, line):\n        line = line.lower()\n        if self.file and 'playback' not in line:\n            print(line, file=self.file)\n        return line\n    def close(self):\n        if self.file:\n            self.file.close()\n            self.file = None\n\ndef parse(arg):\n    'Convert a series of zero or more numbers to an argument tuple'\n    return tuple(map(int, arg.split()))\n\nif __name__ == '__main__':\n    TurtleShell().cmdloop()\n Here is a sample session with the turtle shell showing the help functions, using blank lines to repeat commands, and the simple record and playback facility: Welcome to the turtle shell.   Type help or ? to list commands.\n\n(turtle) ?\n\nDocumented commands (type help <topic>):\n========================================\nbye     color    goto     home  playback  record  right\ncircle  forward  heading  left  position  reset   undo\n\n(turtle) help forward\nMove the turtle forward by the specified distance:  FORWARD 10\n(turtle) record spiral.cmd\n(turtle) position\nCurrent position is 0 0\n\n(turtle) heading\nCurrent heading is 0\n\n(turtle) reset\n(turtle) circle 20\n(turtle) right 30\n(turtle) circle 40\n(turtle) right 30\n(turtle) circle 60\n(turtle) right 30\n(turtle) circle 80\n(turtle) right 30\n(turtle) circle 100\n(turtle) right 30\n(turtle) circle 120\n(turtle) right 30\n(turtle) circle 120\n(turtle) heading\nCurrent heading is 180\n\n(turtle) forward 100\n(turtle)\n(turtle) right 90\n(turtle) forward 100\n(turtle)\n(turtle) right 90\n(turtle) forward 400\n(turtle) right 90\n(turtle) forward 500\n(turtle) right 90\n(turtle) forward 400\n(turtle) right 90\n(turtle) forward 300\n(turtle) playback spiral.cmd\nCurrent position is 0 0\n\nCurrent heading is 0\n\nCurrent heading is 180\n\n(turtle) bye\nThank you for using Turtle\n\n"}, {"name": "cmd.Cmd", "path": "library/cmd#cmd.Cmd", "type": "Frameworks", "text": " \nclass cmd.Cmd(completekey='tab', stdin=None, stdout=None)  \nA Cmd instance or subclass instance is a line-oriented interpreter framework. There is no good reason to instantiate Cmd itself; rather, it\u2019s useful as a superclass of an interpreter class you define yourself in order to inherit Cmd\u2019s methods and encapsulate action methods. The optional argument completekey is the readline name of a completion key; it defaults to Tab. If completekey is not None and readline is available, command completion is done automatically. The optional arguments stdin and stdout specify the input and output file objects that the Cmd instance or subclass instance will use for input and output. If not specified, they will default to sys.stdin and sys.stdout. If you want a given stdin to be used, make sure to set the instance\u2019s use_rawinput attribute to False, otherwise stdin will be ignored. \n"}, {"name": "cmd.Cmd.cmdloop()", "path": "library/cmd#cmd.Cmd.cmdloop", "type": "Frameworks", "text": " \nCmd.cmdloop(intro=None)  \nRepeatedly issue a prompt, accept input, parse an initial prefix off the received input, and dispatch to action methods, passing them the remainder of the line as argument. The optional argument is a banner or intro string to be issued before the first prompt (this overrides the intro class attribute). If the readline module is loaded, input will automatically inherit bash-like history-list editing (e.g. Control-P scrolls back to the last command, Control-N forward to the next one, Control-F moves the cursor to the right non-destructively, Control-B moves the cursor to the left non-destructively, etc.). An end-of-file on input is passed back as the string 'EOF'. An interpreter instance will recognize a command name foo if and only if it has a method do_foo(). As a special case, a line beginning with the character '?' is dispatched to the method do_help(). As another special case, a line beginning with the character '!' is dispatched to the method do_shell() (if such a method is defined). This method will return when the postcmd() method returns a true value. The stop argument to postcmd() is the return value from the command\u2019s corresponding do_*() method. If completion is enabled, completing commands will be done automatically, and completing of commands args is done by calling complete_foo() with arguments text, line, begidx, and endidx. text is the string prefix we are attempting to match: all returned matches must begin with it. line is the current input line with leading whitespace removed, begidx and endidx are the beginning and ending indexes of the prefix text, which could be used to provide different completion depending upon which position the argument is in. All subclasses of Cmd inherit a predefined do_help(). This method, called with an argument 'bar', invokes the corresponding method help_bar(), and if that is not present, prints the docstring of do_bar(), if available. With no argument, do_help() lists all available help topics (that is, all commands with corresponding help_*() methods or commands that have docstrings), and also lists any undocumented commands. \n"}, {"name": "cmd.Cmd.cmdqueue", "path": "library/cmd#cmd.Cmd.cmdqueue", "type": "Frameworks", "text": " \nCmd.cmdqueue  \nA list of queued input lines. The cmdqueue list is checked in cmdloop() when new input is needed; if it is nonempty, its elements will be processed in order, as if entered at the prompt. \n"}, {"name": "cmd.Cmd.completedefault()", "path": "library/cmd#cmd.Cmd.completedefault", "type": "Frameworks", "text": " \nCmd.completedefault(text, line, begidx, endidx)  \nMethod called to complete an input line when no command-specific complete_*() method is available. By default, it returns an empty list. \n"}, {"name": "cmd.Cmd.default()", "path": "library/cmd#cmd.Cmd.default", "type": "Frameworks", "text": " \nCmd.default(line)  \nMethod called on an input line when the command prefix is not recognized. If this method is not overridden, it prints an error message and returns. \n"}, {"name": "cmd.Cmd.doc_header", "path": "library/cmd#cmd.Cmd.doc_header", "type": "Frameworks", "text": " \nCmd.doc_header  \nThe header to issue if the help output has a section for documented commands. \n"}, {"name": "cmd.Cmd.emptyline()", "path": "library/cmd#cmd.Cmd.emptyline", "type": "Frameworks", "text": " \nCmd.emptyline()  \nMethod called when an empty line is entered in response to the prompt. If this method is not overridden, it repeats the last nonempty command entered. \n"}, {"name": "cmd.Cmd.identchars", "path": "library/cmd#cmd.Cmd.identchars", "type": "Frameworks", "text": " \nCmd.identchars  \nThe string of characters accepted for the command prefix. \n"}, {"name": "cmd.Cmd.intro", "path": "library/cmd#cmd.Cmd.intro", "type": "Frameworks", "text": " \nCmd.intro  \nA string to issue as an intro or banner. May be overridden by giving the cmdloop() method an argument. \n"}, {"name": "cmd.Cmd.lastcmd", "path": "library/cmd#cmd.Cmd.lastcmd", "type": "Frameworks", "text": " \nCmd.lastcmd  \nThe last nonempty command prefix seen. \n"}, {"name": "cmd.Cmd.misc_header", "path": "library/cmd#cmd.Cmd.misc_header", "type": "Frameworks", "text": " \nCmd.misc_header  \nThe header to issue if the help output has a section for miscellaneous help topics (that is, there are help_*() methods without corresponding do_*() methods). \n"}, {"name": "cmd.Cmd.onecmd()", "path": "library/cmd#cmd.Cmd.onecmd", "type": "Frameworks", "text": " \nCmd.onecmd(str)  \nInterpret the argument as though it had been typed in response to the prompt. This may be overridden, but should not normally need to be; see the precmd() and postcmd() methods for useful execution hooks. The return value is a flag indicating whether interpretation of commands by the interpreter should stop. If there is a do_*() method for the command str, the return value of that method is returned, otherwise the return value from the default() method is returned. \n"}, {"name": "cmd.Cmd.postcmd()", "path": "library/cmd#cmd.Cmd.postcmd", "type": "Frameworks", "text": " \nCmd.postcmd(stop, line)  \nHook method executed just after a command dispatch is finished. This method is a stub in Cmd; it exists to be overridden by subclasses. line is the command line which was executed, and stop is a flag which indicates whether execution will be terminated after the call to postcmd(); this will be the return value of the onecmd() method. The return value of this method will be used as the new value for the internal flag which corresponds to stop; returning false will cause interpretation to continue. \n"}, {"name": "cmd.Cmd.postloop()", "path": "library/cmd#cmd.Cmd.postloop", "type": "Frameworks", "text": " \nCmd.postloop()  \nHook method executed once when cmdloop() is about to return. This method is a stub in Cmd; it exists to be overridden by subclasses. \n"}, {"name": "cmd.Cmd.precmd()", "path": "library/cmd#cmd.Cmd.precmd", "type": "Frameworks", "text": " \nCmd.precmd(line)  \nHook method executed just before the command line line is interpreted, but after the input prompt is generated and issued. This method is a stub in Cmd; it exists to be overridden by subclasses. The return value is used as the command which will be executed by the onecmd() method; the precmd() implementation may re-write the command or simply return line unchanged. \n"}, {"name": "cmd.Cmd.preloop()", "path": "library/cmd#cmd.Cmd.preloop", "type": "Frameworks", "text": " \nCmd.preloop()  \nHook method executed once when cmdloop() is called. This method is a stub in Cmd; it exists to be overridden by subclasses. \n"}, {"name": "cmd.Cmd.prompt", "path": "library/cmd#cmd.Cmd.prompt", "type": "Frameworks", "text": " \nCmd.prompt  \nThe prompt issued to solicit input. \n"}, {"name": "cmd.Cmd.ruler", "path": "library/cmd#cmd.Cmd.ruler", "type": "Frameworks", "text": " \nCmd.ruler  \nThe character used to draw separator lines under the help-message headers. If empty, no ruler line is drawn. It defaults to '='. \n"}, {"name": "cmd.Cmd.undoc_header", "path": "library/cmd#cmd.Cmd.undoc_header", "type": "Frameworks", "text": " \nCmd.undoc_header  \nThe header to issue if the help output has a section for undocumented commands (that is, there are do_*() methods without corresponding help_*() methods). \n"}, {"name": "cmd.Cmd.use_rawinput", "path": "library/cmd#cmd.Cmd.use_rawinput", "type": "Frameworks", "text": " \nCmd.use_rawinput  \nA flag, defaulting to true. If true, cmdloop() uses input() to display a prompt and read the next command; if false, sys.stdout.write() and sys.stdin.readline() are used. (This means that by importing readline, on systems that support it, the interpreter will automatically support Emacs-like line editing and command-history keystrokes.) \n"}, {"name": "code", "path": "library/code", "type": "Interpreters", "text": "code \u2014 Interpreter base classes Source code: Lib/code.py The code module provides facilities to implement read-eval-print loops in Python. Two classes and convenience functions are included which can be used to build applications which provide an interactive interpreter prompt.  \nclass code.InteractiveInterpreter(locals=None)  \nThis class deals with parsing and interpreter state (the user\u2019s namespace); it does not deal with input buffering or prompting or input file naming (the filename is always passed in explicitly). The optional locals argument specifies the dictionary in which code will be executed; it defaults to a newly created dictionary with key '__name__' set to '__console__' and key '__doc__' set to None. \n  \nclass code.InteractiveConsole(locals=None, filename=\"<console>\")  \nClosely emulate the behavior of the interactive Python interpreter. This class builds on InteractiveInterpreter and adds prompting using the familiar sys.ps1 and sys.ps2, and input buffering. \n  \ncode.interact(banner=None, readfunc=None, local=None, exitmsg=None)  \nConvenience function to run a read-eval-print loop. This creates a new instance of InteractiveConsole and sets readfunc to be used as the InteractiveConsole.raw_input() method, if provided. If local is provided, it is passed to the InteractiveConsole constructor for use as the default namespace for the interpreter loop. The interact() method of the instance is then run with banner and exitmsg passed as the banner and exit message to use, if provided. The console object is discarded after use.  Changed in version 3.6: Added exitmsg parameter.  \n  \ncode.compile_command(source, filename=\"<input>\", symbol=\"single\")  \nThis function is useful for programs that want to emulate Python\u2019s interpreter main loop (a.k.a. the read-eval-print loop). The tricky part is to determine when the user has entered an incomplete command that can be completed by entering more text (as opposed to a complete command or a syntax error). This function almost always makes the same decision as the real interpreter main loop. source is the source string; filename is the optional filename from which source was read, defaulting to '<input>'; and symbol is the optional grammar start symbol, which should be 'single' (the default), 'eval' or 'exec'. Returns a code object (the same as compile(source, filename, symbol)) if the command is complete and valid; None if the command is incomplete; raises SyntaxError if the command is complete and contains a syntax error, or raises OverflowError or ValueError if the command contains an invalid literal. \n Interactive Interpreter Objects  \nInteractiveInterpreter.runsource(source, filename=\"<input>\", symbol=\"single\")  \nCompile and run some source in the interpreter. Arguments are the same as for compile_command(); the default for filename is '<input>', and for symbol is 'single'. One of several things can happen:  The input is incorrect; compile_command() raised an exception (SyntaxError or OverflowError). A syntax traceback will be printed by calling the showsyntaxerror() method. runsource() returns False. The input is incomplete, and more input is required; compile_command() returned None. runsource() returns True. The input is complete; compile_command() returned a code object. The code is executed by calling the runcode() (which also handles run-time exceptions, except for SystemExit). runsource() returns False.  The return value can be used to decide whether to use sys.ps1 or sys.ps2 to prompt the next line. \n  \nInteractiveInterpreter.runcode(code)  \nExecute a code object. When an exception occurs, showtraceback() is called to display a traceback. All exceptions are caught except SystemExit, which is allowed to propagate. A note about KeyboardInterrupt: this exception may occur elsewhere in this code, and may not always be caught. The caller should be prepared to deal with it. \n  \nInteractiveInterpreter.showsyntaxerror(filename=None)  \nDisplay the syntax error that just occurred. This does not display a stack trace because there isn\u2019t one for syntax errors. If filename is given, it is stuffed into the exception instead of the default filename provided by Python\u2019s parser, because it always uses '<string>' when reading from a string. The output is written by the write() method. \n  \nInteractiveInterpreter.showtraceback()  \nDisplay the exception that just occurred. We remove the first stack item because it is within the interpreter object implementation. The output is written by the write() method.  Changed in version 3.5: The full chained traceback is displayed instead of just the primary traceback.  \n  \nInteractiveInterpreter.write(data)  \nWrite a string to the standard error stream (sys.stderr). Derived classes should override this to provide the appropriate output handling as needed. \n Interactive Console Objects The InteractiveConsole class is a subclass of InteractiveInterpreter, and so offers all the methods of the interpreter objects as well as the following additions.  \nInteractiveConsole.interact(banner=None, exitmsg=None)  \nClosely emulate the interactive Python console. The optional banner argument specify the banner to print before the first interaction; by default it prints a banner similar to the one printed by the standard Python interpreter, followed by the class name of the console object in parentheses (so as not to confuse this with the real interpreter \u2013 since it\u2019s so close!). The optional exitmsg argument specifies an exit message printed when exiting. Pass the empty string to suppress the exit message. If exitmsg is not given or None, a default message is printed.  Changed in version 3.4: To suppress printing any banner, pass an empty string.   Changed in version 3.6: Print an exit message when exiting.  \n  \nInteractiveConsole.push(line)  \nPush a line of source text to the interpreter. The line should not have a trailing newline; it may have internal newlines. The line is appended to a buffer and the interpreter\u2019s runsource() method is called with the concatenated contents of the buffer as source. If this indicates that the command was executed or invalid, the buffer is reset; otherwise, the command is incomplete, and the buffer is left as it was after the line was appended. The return value is True if more input is required, False if the line was dealt with in some way (this is the same as runsource()). \n  \nInteractiveConsole.resetbuffer()  \nRemove any unhandled source text from the input buffer. \n  \nInteractiveConsole.raw_input(prompt=\"\")  \nWrite a prompt and read a line. The returned line does not include the trailing newline. When the user enters the EOF key sequence, EOFError is raised. The base implementation reads from sys.stdin; a subclass may replace this with a different implementation. \n\n"}, {"name": "code.compile_command()", "path": "library/code#code.compile_command", "type": "Interpreters", "text": " \ncode.compile_command(source, filename=\"<input>\", symbol=\"single\")  \nThis function is useful for programs that want to emulate Python\u2019s interpreter main loop (a.k.a. the read-eval-print loop). The tricky part is to determine when the user has entered an incomplete command that can be completed by entering more text (as opposed to a complete command or a syntax error). This function almost always makes the same decision as the real interpreter main loop. source is the source string; filename is the optional filename from which source was read, defaulting to '<input>'; and symbol is the optional grammar start symbol, which should be 'single' (the default), 'eval' or 'exec'. Returns a code object (the same as compile(source, filename, symbol)) if the command is complete and valid; None if the command is incomplete; raises SyntaxError if the command is complete and contains a syntax error, or raises OverflowError or ValueError if the command contains an invalid literal. \n"}, {"name": "code.interact()", "path": "library/code#code.interact", "type": "Interpreters", "text": " \ncode.interact(banner=None, readfunc=None, local=None, exitmsg=None)  \nConvenience function to run a read-eval-print loop. This creates a new instance of InteractiveConsole and sets readfunc to be used as the InteractiveConsole.raw_input() method, if provided. If local is provided, it is passed to the InteractiveConsole constructor for use as the default namespace for the interpreter loop. The interact() method of the instance is then run with banner and exitmsg passed as the banner and exit message to use, if provided. The console object is discarded after use.  Changed in version 3.6: Added exitmsg parameter.  \n"}, {"name": "code.InteractiveConsole", "path": "library/code#code.InteractiveConsole", "type": "Interpreters", "text": " \nclass code.InteractiveConsole(locals=None, filename=\"<console>\")  \nClosely emulate the behavior of the interactive Python interpreter. This class builds on InteractiveInterpreter and adds prompting using the familiar sys.ps1 and sys.ps2, and input buffering. \n"}, {"name": "code.InteractiveConsole.interact()", "path": "library/code#code.InteractiveConsole.interact", "type": "Interpreters", "text": " \nInteractiveConsole.interact(banner=None, exitmsg=None)  \nClosely emulate the interactive Python console. The optional banner argument specify the banner to print before the first interaction; by default it prints a banner similar to the one printed by the standard Python interpreter, followed by the class name of the console object in parentheses (so as not to confuse this with the real interpreter \u2013 since it\u2019s so close!). The optional exitmsg argument specifies an exit message printed when exiting. Pass the empty string to suppress the exit message. If exitmsg is not given or None, a default message is printed.  Changed in version 3.4: To suppress printing any banner, pass an empty string.   Changed in version 3.6: Print an exit message when exiting.  \n"}, {"name": "code.InteractiveConsole.push()", "path": "library/code#code.InteractiveConsole.push", "type": "Interpreters", "text": " \nInteractiveConsole.push(line)  \nPush a line of source text to the interpreter. The line should not have a trailing newline; it may have internal newlines. The line is appended to a buffer and the interpreter\u2019s runsource() method is called with the concatenated contents of the buffer as source. If this indicates that the command was executed or invalid, the buffer is reset; otherwise, the command is incomplete, and the buffer is left as it was after the line was appended. The return value is True if more input is required, False if the line was dealt with in some way (this is the same as runsource()). \n"}, {"name": "code.InteractiveConsole.raw_input()", "path": "library/code#code.InteractiveConsole.raw_input", "type": "Interpreters", "text": " \nInteractiveConsole.raw_input(prompt=\"\")  \nWrite a prompt and read a line. The returned line does not include the trailing newline. When the user enters the EOF key sequence, EOFError is raised. The base implementation reads from sys.stdin; a subclass may replace this with a different implementation. \n"}, {"name": "code.InteractiveConsole.resetbuffer()", "path": "library/code#code.InteractiveConsole.resetbuffer", "type": "Interpreters", "text": " \nInteractiveConsole.resetbuffer()  \nRemove any unhandled source text from the input buffer. \n"}, {"name": "code.InteractiveInterpreter", "path": "library/code#code.InteractiveInterpreter", "type": "Interpreters", "text": " \nclass code.InteractiveInterpreter(locals=None)  \nThis class deals with parsing and interpreter state (the user\u2019s namespace); it does not deal with input buffering or prompting or input file naming (the filename is always passed in explicitly). The optional locals argument specifies the dictionary in which code will be executed; it defaults to a newly created dictionary with key '__name__' set to '__console__' and key '__doc__' set to None. \n"}, {"name": "code.InteractiveInterpreter.runcode()", "path": "library/code#code.InteractiveInterpreter.runcode", "type": "Interpreters", "text": " \nInteractiveInterpreter.runcode(code)  \nExecute a code object. When an exception occurs, showtraceback() is called to display a traceback. All exceptions are caught except SystemExit, which is allowed to propagate. A note about KeyboardInterrupt: this exception may occur elsewhere in this code, and may not always be caught. The caller should be prepared to deal with it. \n"}, {"name": "code.InteractiveInterpreter.runsource()", "path": "library/code#code.InteractiveInterpreter.runsource", "type": "Interpreters", "text": " \nInteractiveInterpreter.runsource(source, filename=\"<input>\", symbol=\"single\")  \nCompile and run some source in the interpreter. Arguments are the same as for compile_command(); the default for filename is '<input>', and for symbol is 'single'. One of several things can happen:  The input is incorrect; compile_command() raised an exception (SyntaxError or OverflowError). A syntax traceback will be printed by calling the showsyntaxerror() method. runsource() returns False. The input is incomplete, and more input is required; compile_command() returned None. runsource() returns True. The input is complete; compile_command() returned a code object. The code is executed by calling the runcode() (which also handles run-time exceptions, except for SystemExit). runsource() returns False.  The return value can be used to decide whether to use sys.ps1 or sys.ps2 to prompt the next line. \n"}, {"name": "code.InteractiveInterpreter.showsyntaxerror()", "path": "library/code#code.InteractiveInterpreter.showsyntaxerror", "type": "Interpreters", "text": " \nInteractiveInterpreter.showsyntaxerror(filename=None)  \nDisplay the syntax error that just occurred. This does not display a stack trace because there isn\u2019t one for syntax errors. If filename is given, it is stuffed into the exception instead of the default filename provided by Python\u2019s parser, because it always uses '<string>' when reading from a string. The output is written by the write() method. \n"}, {"name": "code.InteractiveInterpreter.showtraceback()", "path": "library/code#code.InteractiveInterpreter.showtraceback", "type": "Interpreters", "text": " \nInteractiveInterpreter.showtraceback()  \nDisplay the exception that just occurred. We remove the first stack item because it is within the interpreter object implementation. The output is written by the write() method.  Changed in version 3.5: The full chained traceback is displayed instead of just the primary traceback.  \n"}, {"name": "code.InteractiveInterpreter.write()", "path": "library/code#code.InteractiveInterpreter.write", "type": "Interpreters", "text": " \nInteractiveInterpreter.write(data)  \nWrite a string to the standard error stream (sys.stderr). Derived classes should override this to provide the appropriate output handling as needed. \n"}, {"name": "codecs", "path": "library/codecs", "type": "Binary Data", "text": "codecs \u2014 Codec registry and base classes Source code: Lib/codecs.py This module defines base classes for standard Python codecs (encoders and decoders) and provides access to the internal Python codec registry, which manages the codec and error handling lookup process. Most standard codecs are text encodings, which encode text to bytes, but there are also codecs provided that encode text to text, and bytes to bytes. Custom codecs may encode and decode between arbitrary types, but some module features are restricted to use specifically with text encodings, or with codecs that encode to bytes. The module defines the following functions for encoding and decoding with any codec:  \ncodecs.encode(obj, encoding='utf-8', errors='strict')  \nEncodes obj using the codec registered for encoding. Errors may be given to set the desired error handling scheme. The default error handler is 'strict' meaning that encoding errors raise ValueError (or a more codec specific subclass, such as UnicodeEncodeError). Refer to Codec Base Classes for more information on codec error handling. \n  \ncodecs.decode(obj, encoding='utf-8', errors='strict')  \nDecodes obj using the codec registered for encoding. Errors may be given to set the desired error handling scheme. The default error handler is 'strict' meaning that decoding errors raise ValueError (or a more codec specific subclass, such as UnicodeDecodeError). Refer to Codec Base Classes for more information on codec error handling. \n The full details for each codec can also be looked up directly:  \ncodecs.lookup(encoding)  \nLooks up the codec info in the Python codec registry and returns a CodecInfo object as defined below. Encodings are first looked up in the registry\u2019s cache. If not found, the list of registered search functions is scanned. If no CodecInfo object is found, a LookupError is raised. Otherwise, the CodecInfo object is stored in the cache and returned to the caller. \n  \nclass codecs.CodecInfo(encode, decode, streamreader=None, streamwriter=None, incrementalencoder=None, incrementaldecoder=None, name=None)  \nCodec details when looking up the codec registry. The constructor arguments are stored in attributes of the same name:  \nname  \nThe name of the encoding. \n  \nencode  \ndecode  \nThe stateless encoding and decoding functions. These must be functions or methods which have the same interface as the encode() and decode() methods of Codec instances (see Codec Interface). The functions or methods are expected to work in a stateless mode. \n  \nincrementalencoder  \nincrementaldecoder  \nIncremental encoder and decoder classes or factory functions. These have to provide the interface defined by the base classes IncrementalEncoder and IncrementalDecoder, respectively. Incremental codecs can maintain state. \n  \nstreamwriter  \nstreamreader  \nStream writer and reader classes or factory functions. These have to provide the interface defined by the base classes StreamWriter and StreamReader, respectively. Stream codecs can maintain state. \n \n To simplify access to the various codec components, the module provides these additional functions which use lookup() for the codec lookup:  \ncodecs.getencoder(encoding)  \nLook up the codec for the given encoding and return its encoder function. Raises a LookupError in case the encoding cannot be found. \n  \ncodecs.getdecoder(encoding)  \nLook up the codec for the given encoding and return its decoder function. Raises a LookupError in case the encoding cannot be found. \n  \ncodecs.getincrementalencoder(encoding)  \nLook up the codec for the given encoding and return its incremental encoder class or factory function. Raises a LookupError in case the encoding cannot be found or the codec doesn\u2019t support an incremental encoder. \n  \ncodecs.getincrementaldecoder(encoding)  \nLook up the codec for the given encoding and return its incremental decoder class or factory function. Raises a LookupError in case the encoding cannot be found or the codec doesn\u2019t support an incremental decoder. \n  \ncodecs.getreader(encoding)  \nLook up the codec for the given encoding and return its StreamReader class or factory function. Raises a LookupError in case the encoding cannot be found. \n  \ncodecs.getwriter(encoding)  \nLook up the codec for the given encoding and return its StreamWriter class or factory function. Raises a LookupError in case the encoding cannot be found. \n Custom codecs are made available by registering a suitable codec search function:  \ncodecs.register(search_function)  \nRegister a codec search function. Search functions are expected to take one argument, being the encoding name in all lower case letters with hyphens and spaces converted to underscores, and return a CodecInfo object. In case a search function cannot find a given encoding, it should return None.  Changed in version 3.9: Hyphens and spaces are converted to underscore.   Note Search function registration is not currently reversible, which may cause problems in some cases, such as unit testing or module reloading.  \n While the builtin open() and the associated io module are the recommended approach for working with encoded text files, this module provides additional utility functions and classes that allow the use of a wider range of codecs when working with binary files:  \ncodecs.open(filename, mode='r', encoding=None, errors='strict', buffering=-1)  \nOpen an encoded file using the given mode and return an instance of StreamReaderWriter, providing transparent encoding/decoding. The default file mode is 'r', meaning to open the file in read mode.  Note Underlying encoded files are always opened in binary mode. No automatic conversion of '\\n' is done on reading and writing. The mode argument may be any binary mode acceptable to the built-in open() function; the 'b' is automatically added.  encoding specifies the encoding which is to be used for the file. Any encoding that encodes to and decodes from bytes is allowed, and the data types supported by the file methods depend on the codec used. errors may be given to define the error handling. It defaults to 'strict' which causes a ValueError to be raised in case an encoding error occurs. buffering has the same meaning as for the built-in open() function. It defaults to -1 which means that the default buffer size will be used. \n  \ncodecs.EncodedFile(file, data_encoding, file_encoding=None, errors='strict')  \nReturn a StreamRecoder instance, a wrapped version of file which provides transparent transcoding. The original file is closed when the wrapped version is closed. Data written to the wrapped file is decoded according to the given data_encoding and then written to the original file as bytes using file_encoding. Bytes read from the original file are decoded according to file_encoding, and the result is encoded using data_encoding. If file_encoding is not given, it defaults to data_encoding. errors may be given to define the error handling. It defaults to 'strict', which causes ValueError to be raised in case an encoding error occurs. \n  \ncodecs.iterencode(iterator, encoding, errors='strict', **kwargs)  \nUses an incremental encoder to iteratively encode the input provided by iterator. This function is a generator. The errors argument (as well as any other keyword argument) is passed through to the incremental encoder. This function requires that the codec accept text str objects to encode. Therefore it does not support bytes-to-bytes encoders such as base64_codec. \n  \ncodecs.iterdecode(iterator, encoding, errors='strict', **kwargs)  \nUses an incremental decoder to iteratively decode the input provided by iterator. This function is a generator. The errors argument (as well as any other keyword argument) is passed through to the incremental decoder. This function requires that the codec accept bytes objects to decode. Therefore it does not support text-to-text encoders such as rot_13, although rot_13 may be used equivalently with iterencode(). \n The module also provides the following constants which are useful for reading and writing to platform dependent files:  \ncodecs.BOM  \ncodecs.BOM_BE  \ncodecs.BOM_LE  \ncodecs.BOM_UTF8  \ncodecs.BOM_UTF16  \ncodecs.BOM_UTF16_BE  \ncodecs.BOM_UTF16_LE  \ncodecs.BOM_UTF32  \ncodecs.BOM_UTF32_BE  \ncodecs.BOM_UTF32_LE  \nThese constants define various byte sequences, being Unicode byte order marks (BOMs) for several encodings. They are used in UTF-16 and UTF-32 data streams to indicate the byte order used, and in UTF-8 as a Unicode signature. BOM_UTF16 is either BOM_UTF16_BE or BOM_UTF16_LE depending on the platform\u2019s native byte order, BOM is an alias for BOM_UTF16, BOM_LE for BOM_UTF16_LE and BOM_BE for BOM_UTF16_BE. The others represent the BOM in UTF-8 and UTF-32 encodings. \n Codec Base Classes The codecs module defines a set of base classes which define the interfaces for working with codec objects, and can also be used as the basis for custom codec implementations. Each codec has to define four interfaces to make it usable as codec in Python: stateless encoder, stateless decoder, stream reader and stream writer. The stream reader and writers typically reuse the stateless encoder/decoder to implement the file protocols. Codec authors also need to define how the codec will handle encoding and decoding errors. Error Handlers To simplify and standardize error handling, codecs may implement different error handling schemes by accepting the errors string argument. The following string values are defined and implemented by all standard Python codecs:   \nValue Meaning   \n'strict' Raise UnicodeError (or a subclass); this is the default. Implemented in strict_errors().  \n'ignore' Ignore the malformed data and continue without further notice. Implemented in ignore_errors().   The following error handlers are only applicable to text encodings:   \nValue Meaning   \n'replace' Replace with a suitable replacement marker; Python will use the official U+FFFD REPLACEMENT CHARACTER for the built-in codecs on decoding, and \u2018?\u2019 on encoding. Implemented in replace_errors().  \n'xmlcharrefreplace' Replace with the appropriate XML character reference (only for encoding). Implemented in xmlcharrefreplace_errors().  \n'backslashreplace' Replace with backslashed escape sequences. Implemented in backslashreplace_errors().  \n'namereplace' Replace with \\N{...} escape sequences (only for encoding). Implemented in namereplace_errors().  \n'surrogateescape' On decoding, replace byte with individual surrogate code ranging from U+DC80 to U+DCFF. This code will then be turned back into the same byte when the 'surrogateescape' error handler is used when encoding the data. (See PEP 383 for more.)   In addition, the following error handler is specific to the given codecs:   \nValue Codecs Meaning   \n'surrogatepass' utf-8, utf-16, utf-32, utf-16-be, utf-16-le, utf-32-be, utf-32-le Allow encoding and decoding of surrogate codes. These codecs normally treat the presence of surrogates as an error.    New in version 3.1: The 'surrogateescape' and 'surrogatepass' error handlers.   Changed in version 3.4: The 'surrogatepass' error handlers now works with utf-16* and utf-32* codecs.   New in version 3.5: The 'namereplace' error handler.   Changed in version 3.5: The 'backslashreplace' error handlers now works with decoding and translating.  The set of allowed values can be extended by registering a new named error handler:  \ncodecs.register_error(name, error_handler)  \nRegister the error handling function error_handler under the name name. The error_handler argument will be called during encoding and decoding in case of an error, when name is specified as the errors parameter. For encoding, error_handler will be called with a UnicodeEncodeError instance, which contains information about the location of the error. The error handler must either raise this or a different exception, or return a tuple with a replacement for the unencodable part of the input and a position where encoding should continue. The replacement may be either str or bytes. If the replacement is bytes, the encoder will simply copy them into the output buffer. If the replacement is a string, the encoder will encode the replacement. Encoding continues on original input at the specified position. Negative position values will be treated as being relative to the end of the input string. If the resulting position is out of bound an IndexError will be raised. Decoding and translating works similarly, except UnicodeDecodeError or UnicodeTranslateError will be passed to the handler and that the replacement from the error handler will be put into the output directly. \n Previously registered error handlers (including the standard error handlers) can be looked up by name:  \ncodecs.lookup_error(name)  \nReturn the error handler previously registered under the name name. Raises a LookupError in case the handler cannot be found. \n The following standard error handlers are also made available as module level functions:  \ncodecs.strict_errors(exception)  \nImplements the 'strict' error handling: each encoding or decoding error raises a UnicodeError. \n  \ncodecs.replace_errors(exception)  \nImplements the 'replace' error handling (for text encodings only): substitutes '?' for encoding errors (to be encoded by the codec), and '\\ufffd' (the Unicode replacement character) for decoding errors. \n  \ncodecs.ignore_errors(exception)  \nImplements the 'ignore' error handling: malformed data is ignored and encoding or decoding is continued without further notice. \n  \ncodecs.xmlcharrefreplace_errors(exception)  \nImplements the 'xmlcharrefreplace' error handling (for encoding with text encodings only): the unencodable character is replaced by an appropriate XML character reference. \n  \ncodecs.backslashreplace_errors(exception)  \nImplements the 'backslashreplace' error handling (for text encodings only): malformed data is replaced by a backslashed escape sequence. \n  \ncodecs.namereplace_errors(exception)  \nImplements the 'namereplace' error handling (for encoding with text encodings only): the unencodable character is replaced by a \\N{...} escape sequence.  New in version 3.5.  \n Stateless Encoding and Decoding The base Codec class defines these methods which also define the function interfaces of the stateless encoder and decoder:  \nCodec.encode(input[, errors])  \nEncodes the object input and returns a tuple (output object, length consumed). For instance, text encoding converts a string object to a bytes object using a particular character set encoding (e.g., cp1252 or iso-8859-1). The errors argument defines the error handling to apply. It defaults to 'strict' handling. The method may not store state in the Codec instance. Use StreamWriter for codecs which have to keep state in order to make encoding efficient. The encoder must be able to handle zero length input and return an empty object of the output object type in this situation. \n  \nCodec.decode(input[, errors])  \nDecodes the object input and returns a tuple (output object, length consumed). For instance, for a text encoding, decoding converts a bytes object encoded using a particular character set encoding to a string object. For text encodings and bytes-to-bytes codecs, input must be a bytes object or one which provides the read-only buffer interface \u2013 for example, buffer objects and memory mapped files. The errors argument defines the error handling to apply. It defaults to 'strict' handling. The method may not store state in the Codec instance. Use StreamReader for codecs which have to keep state in order to make decoding efficient. The decoder must be able to handle zero length input and return an empty object of the output object type in this situation. \n Incremental Encoding and Decoding The IncrementalEncoder and IncrementalDecoder classes provide the basic interface for incremental encoding and decoding. Encoding/decoding the input isn\u2019t done with one call to the stateless encoder/decoder function, but with multiple calls to the encode()/decode() method of the incremental encoder/decoder. The incremental encoder/decoder keeps track of the encoding/decoding process during method calls. The joined output of calls to the encode()/decode() method is the same as if all the single inputs were joined into one, and this input was encoded/decoded with the stateless encoder/decoder. IncrementalEncoder Objects The IncrementalEncoder class is used for encoding an input in multiple steps. It defines the following methods which every incremental encoder must define in order to be compatible with the Python codec registry.  \nclass codecs.IncrementalEncoder(errors='strict')  \nConstructor for an IncrementalEncoder instance. All incremental encoders must provide this constructor interface. They are free to add additional keyword arguments, but only the ones defined here are used by the Python codec registry. The IncrementalEncoder may implement different error handling schemes by providing the errors keyword argument. See Error Handlers for possible values. The errors argument will be assigned to an attribute of the same name. Assigning to this attribute makes it possible to switch between different error handling strategies during the lifetime of the IncrementalEncoder object.  \nencode(object[, final])  \nEncodes object (taking the current state of the encoder into account) and returns the resulting encoded object. If this is the last call to encode() final must be true (the default is false). \n  \nreset()  \nReset the encoder to the initial state. The output is discarded: call .encode(object, final=True), passing an empty byte or text string if necessary, to reset the encoder and to get the output. \n  \ngetstate()  \nReturn the current state of the encoder which must be an integer. The implementation should make sure that 0 is the most common state. (States that are more complicated than integers can be converted into an integer by marshaling/pickling the state and encoding the bytes of the resulting string into an integer.) \n  \nsetstate(state)  \nSet the state of the encoder to state. state must be an encoder state returned by getstate(). \n \n IncrementalDecoder Objects The IncrementalDecoder class is used for decoding an input in multiple steps. It defines the following methods which every incremental decoder must define in order to be compatible with the Python codec registry.  \nclass codecs.IncrementalDecoder(errors='strict')  \nConstructor for an IncrementalDecoder instance. All incremental decoders must provide this constructor interface. They are free to add additional keyword arguments, but only the ones defined here are used by the Python codec registry. The IncrementalDecoder may implement different error handling schemes by providing the errors keyword argument. See Error Handlers for possible values. The errors argument will be assigned to an attribute of the same name. Assigning to this attribute makes it possible to switch between different error handling strategies during the lifetime of the IncrementalDecoder object.  \ndecode(object[, final])  \nDecodes object (taking the current state of the decoder into account) and returns the resulting decoded object. If this is the last call to decode() final must be true (the default is false). If final is true the decoder must decode the input completely and must flush all buffers. If this isn\u2019t possible (e.g. because of incomplete byte sequences at the end of the input) it must initiate error handling just like in the stateless case (which might raise an exception). \n  \nreset()  \nReset the decoder to the initial state. \n  \ngetstate()  \nReturn the current state of the decoder. This must be a tuple with two items, the first must be the buffer containing the still undecoded input. The second must be an integer and can be additional state info. (The implementation should make sure that 0 is the most common additional state info.) If this additional state info is 0 it must be possible to set the decoder to the state which has no input buffered and 0 as the additional state info, so that feeding the previously buffered input to the decoder returns it to the previous state without producing any output. (Additional state info that is more complicated than integers can be converted into an integer by marshaling/pickling the info and encoding the bytes of the resulting string into an integer.) \n  \nsetstate(state)  \nSet the state of the decoder to state. state must be a decoder state returned by getstate(). \n \n Stream Encoding and Decoding The StreamWriter and StreamReader classes provide generic working interfaces which can be used to implement new encoding submodules very easily. See encodings.utf_8 for an example of how this is done. StreamWriter Objects The StreamWriter class is a subclass of Codec and defines the following methods which every stream writer must define in order to be compatible with the Python codec registry.  \nclass codecs.StreamWriter(stream, errors='strict')  \nConstructor for a StreamWriter instance. All stream writers must provide this constructor interface. They are free to add additional keyword arguments, but only the ones defined here are used by the Python codec registry. The stream argument must be a file-like object open for writing text or binary data, as appropriate for the specific codec. The StreamWriter may implement different error handling schemes by providing the errors keyword argument. See Error Handlers for the standard error handlers the underlying stream codec may support. The errors argument will be assigned to an attribute of the same name. Assigning to this attribute makes it possible to switch between different error handling strategies during the lifetime of the StreamWriter object.  \nwrite(object)  \nWrites the object\u2019s contents encoded to the stream. \n  \nwritelines(list)  \nWrites the concatenated list of strings to the stream (possibly by reusing the write() method). The standard bytes-to-bytes codecs do not support this method. \n  \nreset()  \nResets the codec buffers used for keeping internal state. Calling this method should ensure that the data on the output is put into a clean state that allows appending of new fresh data without having to rescan the whole stream to recover state. \n \n In addition to the above methods, the StreamWriter must also inherit all other methods and attributes from the underlying stream. StreamReader Objects The StreamReader class is a subclass of Codec and defines the following methods which every stream reader must define in order to be compatible with the Python codec registry.  \nclass codecs.StreamReader(stream, errors='strict')  \nConstructor for a StreamReader instance. All stream readers must provide this constructor interface. They are free to add additional keyword arguments, but only the ones defined here are used by the Python codec registry. The stream argument must be a file-like object open for reading text or binary data, as appropriate for the specific codec. The StreamReader may implement different error handling schemes by providing the errors keyword argument. See Error Handlers for the standard error handlers the underlying stream codec may support. The errors argument will be assigned to an attribute of the same name. Assigning to this attribute makes it possible to switch between different error handling strategies during the lifetime of the StreamReader object. The set of allowed values for the errors argument can be extended with register_error().  \nread([size[, chars[, firstline]]])  \nDecodes data from the stream and returns the resulting object. The chars argument indicates the number of decoded code points or bytes to return. The read() method will never return more data than requested, but it might return less, if there is not enough available. The size argument indicates the approximate maximum number of encoded bytes or code points to read for decoding. The decoder can modify this setting as appropriate. The default value -1 indicates to read and decode as much as possible. This parameter is intended to prevent having to decode huge files in one step. The firstline flag indicates that it would be sufficient to only return the first line, if there are decoding errors on later lines. The method should use a greedy read strategy meaning that it should read as much data as is allowed within the definition of the encoding and the given size, e.g. if optional encoding endings or state markers are available on the stream, these should be read too. \n  \nreadline([size[, keepends]])  \nRead one line from the input stream and return the decoded data. size, if given, is passed as size argument to the stream\u2019s read() method. If keepends is false line-endings will be stripped from the lines returned. \n  \nreadlines([sizehint[, keepends]])  \nRead all lines available on the input stream and return them as a list of lines. Line-endings are implemented using the codec\u2019s decode() method and are included in the list entries if keepends is true. sizehint, if given, is passed as the size argument to the stream\u2019s read() method. \n  \nreset()  \nResets the codec buffers used for keeping internal state. Note that no stream repositioning should take place. This method is primarily intended to be able to recover from decoding errors. \n \n In addition to the above methods, the StreamReader must also inherit all other methods and attributes from the underlying stream. StreamReaderWriter Objects The StreamReaderWriter is a convenience class that allows wrapping streams which work in both read and write modes. The design is such that one can use the factory functions returned by the lookup() function to construct the instance.  \nclass codecs.StreamReaderWriter(stream, Reader, Writer, errors='strict')  \nCreates a StreamReaderWriter instance. stream must be a file-like object. Reader and Writer must be factory functions or classes providing the StreamReader and StreamWriter interface resp. Error handling is done in the same way as defined for the stream readers and writers. \n StreamReaderWriter instances define the combined interfaces of StreamReader and StreamWriter classes. They inherit all other methods and attributes from the underlying stream. StreamRecoder Objects The StreamRecoder translates data from one encoding to another, which is sometimes useful when dealing with different encoding environments. The design is such that one can use the factory functions returned by the lookup() function to construct the instance.  \nclass codecs.StreamRecoder(stream, encode, decode, Reader, Writer, errors='strict')  \nCreates a StreamRecoder instance which implements a two-way conversion: encode and decode work on the frontend \u2014 the data visible to code calling read() and write(), while Reader and Writer work on the backend \u2014 the data in stream. You can use these objects to do transparent transcodings, e.g., from Latin-1 to UTF-8 and back. The stream argument must be a file-like object. The encode and decode arguments must adhere to the Codec interface. Reader and Writer must be factory functions or classes providing objects of the StreamReader and StreamWriter interface respectively. Error handling is done in the same way as defined for the stream readers and writers. \n StreamRecoder instances define the combined interfaces of StreamReader and StreamWriter classes. They inherit all other methods and attributes from the underlying stream. Encodings and Unicode Strings are stored internally as sequences of code points in range 0x0\u20130x10FFFF. (See PEP 393 for more details about the implementation.) Once a string object is used outside of CPU and memory, endianness and how these arrays are stored as bytes become an issue. As with other codecs, serialising a string into a sequence of bytes is known as encoding, and recreating the string from the sequence of bytes is known as decoding. There are a variety of different text serialisation codecs, which are collectivity referred to as text encodings. The simplest text encoding (called 'latin-1' or 'iso-8859-1') maps the code points 0\u2013255 to the bytes 0x0\u20130xff, which means that a string object that contains code points above U+00FF can\u2019t be encoded with this codec. Doing so will raise a UnicodeEncodeError that looks like the following (although the details of the error message may differ): UnicodeEncodeError: 'latin-1' codec can't encode character '\\u1234' in\nposition 3: ordinal not in range(256). There\u2019s another group of encodings (the so called charmap encodings) that choose a different subset of all Unicode code points and how these code points are mapped to the bytes 0x0\u20130xff. To see how this is done simply open e.g. encodings/cp1252.py (which is an encoding that is used primarily on Windows). There\u2019s a string constant with 256 characters that shows you which character is mapped to which byte value. All of these encodings can only encode 256 of the 1114112 code points defined in Unicode. A simple and straightforward way that can store each Unicode code point, is to store each code point as four consecutive bytes. There are two possibilities: store the bytes in big endian or in little endian order. These two encodings are called UTF-32-BE and UTF-32-LE respectively. Their disadvantage is that if e.g. you use UTF-32-BE on a little endian machine you will always have to swap bytes on encoding and decoding. UTF-32 avoids this problem: bytes will always be in natural endianness. When these bytes are read by a CPU with a different endianness, then bytes have to be swapped though. To be able to detect the endianness of a UTF-16 or UTF-32 byte sequence, there\u2019s the so called BOM (\u201cByte Order Mark\u201d). This is the Unicode character U+FEFF. This character can be prepended to every UTF-16 or UTF-32 byte sequence. The byte swapped version of this character (0xFFFE) is an illegal character that may not appear in a Unicode text. So when the first character in an UTF-16 or UTF-32 byte sequence appears to be a U+FFFE the bytes have to be swapped on decoding. Unfortunately the character U+FEFF had a second purpose as a ZERO WIDTH NO-BREAK SPACE: a character that has no width and doesn\u2019t allow a word to be split. It can e.g. be used to give hints to a ligature algorithm. With Unicode 4.0 using U+FEFF as a ZERO WIDTH NO-BREAK SPACE has been deprecated (with U+2060 (WORD JOINER) assuming this role). Nevertheless Unicode software still must be able to handle U+FEFF in both roles: as a BOM it\u2019s a device to determine the storage layout of the encoded bytes, and vanishes once the byte sequence has been decoded into a string; as a ZERO WIDTH\nNO-BREAK SPACE it\u2019s a normal character that will be decoded like any other. There\u2019s another encoding that is able to encoding the full range of Unicode characters: UTF-8. UTF-8 is an 8-bit encoding, which means there are no issues with byte order in UTF-8. Each byte in a UTF-8 byte sequence consists of two parts: marker bits (the most significant bits) and payload bits. The marker bits are a sequence of zero to four 1 bits followed by a 0 bit. Unicode characters are encoded like this (with x being payload bits, which when concatenated give the Unicode character):   \nRange Encoding   \nU-00000000 \u2026 U-0000007F 0xxxxxxx  \nU-00000080 \u2026 U-000007FF 110xxxxx 10xxxxxx  \nU-00000800 \u2026 U-0000FFFF 1110xxxx 10xxxxxx 10xxxxxx  \nU-00010000 \u2026 U-0010FFFF 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx   The least significant bit of the Unicode character is the rightmost x bit. As UTF-8 is an 8-bit encoding no BOM is required and any U+FEFF character in the decoded string (even if it\u2019s the first character) is treated as a ZERO\nWIDTH NO-BREAK SPACE. Without external information it\u2019s impossible to reliably determine which encoding was used for encoding a string. Each charmap encoding can decode any random byte sequence. However that\u2019s not possible with UTF-8, as UTF-8 byte sequences have a structure that doesn\u2019t allow arbitrary byte sequences. To increase the reliability with which a UTF-8 encoding can be detected, Microsoft invented a variant of UTF-8 (that Python 2.5 calls \"utf-8-sig\") for its Notepad program: Before any of the Unicode characters is written to the file, a UTF-8 encoded BOM (which looks like this as a byte sequence: 0xef, 0xbb, 0xbf) is written. As it\u2019s rather improbable that any charmap encoded file starts with these byte values (which would e.g. map to in iso-8859-1), this increases the probability that a utf-8-sig encoding can be correctly guessed from the byte sequence. So here the BOM is not used to be able to determine the byte order used for generating the byte sequence, but as a signature that helps in guessing the encoding. On encoding the utf-8-sig codec will write 0xef, 0xbb, 0xbf as the first three bytes to the file. On decoding utf-8-sig will skip those three bytes if they appear as the first three bytes in the file. In UTF-8, the use of the BOM is discouraged and should generally be avoided. Standard Encodings Python comes with a number of codecs built-in, either implemented as C functions or with dictionaries as mapping tables. The following table lists the codecs by name, together with a few common aliases, and the languages for which the encoding is likely used. Neither the list of aliases nor the list of languages is meant to be exhaustive. Notice that spelling alternatives that only differ in case or use a hyphen instead of an underscore are also valid aliases; therefore, e.g. 'utf-8' is a valid alias for the 'utf_8' codec.  CPython implementation detail: Some common encodings can bypass the codecs lookup machinery to improve performance. These optimization opportunities are only recognized by CPython for a limited set of (case insensitive) aliases: utf-8, utf8, latin-1, latin1, iso-8859-1, iso8859-1, mbcs (Windows only), ascii, us-ascii, utf-16, utf16, utf-32, utf32, and the same using underscores instead of dashes. Using alternative aliases for these encodings may result in slower execution.  Changed in version 3.6: Optimization opportunity recognized for us-ascii.   Many of the character sets support the same languages. They vary in individual characters (e.g. whether the EURO SIGN is supported or not), and in the assignment of characters to code positions. For the European languages in particular, the following variants typically exist:  an ISO 8859 codeset a Microsoft Windows code page, which is typically derived from an 8859 codeset, but replaces control characters with additional graphic characters an IBM EBCDIC code page an IBM PC code page, which is ASCII compatible    \nCodec Aliases Languages   \nascii 646, us-ascii English  \nbig5 big5-tw, csbig5 Traditional Chinese  \nbig5hkscs big5-hkscs, hkscs Traditional Chinese  \ncp037 IBM037, IBM039 English  \ncp273 273, IBM273, csIBM273 \nGerman  New in version 3.4.    \ncp424 EBCDIC-CP-HE, IBM424 Hebrew  \ncp437 437, IBM437 English  \ncp500 EBCDIC-CP-BE, EBCDIC-CP-CH, IBM500 Western Europe  \ncp720  Arabic  \ncp737  Greek  \ncp775 IBM775 Baltic languages  \ncp850 850, IBM850 Western Europe  \ncp852 852, IBM852 Central and Eastern Europe  \ncp855 855, IBM855 Bulgarian, Byelorussian, Macedonian, Russian, Serbian  \ncp856  Hebrew  \ncp857 857, IBM857 Turkish  \ncp858 858, IBM858 Western Europe  \ncp860 860, IBM860 Portuguese  \ncp861 861, CP-IS, IBM861 Icelandic  \ncp862 862, IBM862 Hebrew  \ncp863 863, IBM863 Canadian  \ncp864 IBM864 Arabic  \ncp865 865, IBM865 Danish, Norwegian  \ncp866 866, IBM866 Russian  \ncp869 869, CP-GR, IBM869 Greek  \ncp874  Thai  \ncp875  Greek  \ncp932 932, ms932, mskanji, ms-kanji Japanese  \ncp949 949, ms949, uhc Korean  \ncp950 950, ms950 Traditional Chinese  \ncp1006  Urdu  \ncp1026 ibm1026 Turkish  \ncp1125 1125, ibm1125, cp866u, ruscii \nUkrainian  New in version 3.4.    \ncp1140 ibm1140 Western Europe  \ncp1250 windows-1250 Central and Eastern Europe  \ncp1251 windows-1251 Bulgarian, Byelorussian, Macedonian, Russian, Serbian  \ncp1252 windows-1252 Western Europe  \ncp1253 windows-1253 Greek  \ncp1254 windows-1254 Turkish  \ncp1255 windows-1255 Hebrew  \ncp1256 windows-1256 Arabic  \ncp1257 windows-1257 Baltic languages  \ncp1258 windows-1258 Vietnamese  \neuc_jp eucjp, ujis, u-jis Japanese  \neuc_jis_2004 jisx0213, eucjis2004 Japanese  \neuc_jisx0213 eucjisx0213 Japanese  \neuc_kr euckr, korean, ksc5601, ks_c-5601, ks_c-5601-1987, ksx1001, ks_x-1001 Korean  \ngb2312 chinese, csiso58gb231280, euc-cn, euccn, eucgb2312-cn, gb2312-1980, gb2312-80, iso-ir-58 Simplified Chinese  \ngbk 936, cp936, ms936 Unified Chinese  \ngb18030 gb18030-2000 Unified Chinese  \nhz hzgb, hz-gb, hz-gb-2312 Simplified Chinese  \niso2022_jp csiso2022jp, iso2022jp, iso-2022-jp Japanese  \niso2022_jp_1 iso2022jp-1, iso-2022-jp-1 Japanese  \niso2022_jp_2 iso2022jp-2, iso-2022-jp-2 Japanese, Korean, Simplified Chinese, Western Europe, Greek  \niso2022_jp_2004 iso2022jp-2004, iso-2022-jp-2004 Japanese  \niso2022_jp_3 iso2022jp-3, iso-2022-jp-3 Japanese  \niso2022_jp_ext iso2022jp-ext, iso-2022-jp-ext Japanese  \niso2022_kr csiso2022kr, iso2022kr, iso-2022-kr Korean  \nlatin_1 iso-8859-1, iso8859-1, 8859, cp819, latin, latin1, L1 Western Europe  \niso8859_2 iso-8859-2, latin2, L2 Central and Eastern Europe  \niso8859_3 iso-8859-3, latin3, L3 Esperanto, Maltese  \niso8859_4 iso-8859-4, latin4, L4 Baltic languages  \niso8859_5 iso-8859-5, cyrillic Bulgarian, Byelorussian, Macedonian, Russian, Serbian  \niso8859_6 iso-8859-6, arabic Arabic  \niso8859_7 iso-8859-7, greek, greek8 Greek  \niso8859_8 iso-8859-8, hebrew Hebrew  \niso8859_9 iso-8859-9, latin5, L5 Turkish  \niso8859_10 iso-8859-10, latin6, L6 Nordic languages  \niso8859_11 iso-8859-11, thai Thai languages  \niso8859_13 iso-8859-13, latin7, L7 Baltic languages  \niso8859_14 iso-8859-14, latin8, L8 Celtic languages  \niso8859_15 iso-8859-15, latin9, L9 Western Europe  \niso8859_16 iso-8859-16, latin10, L10 South-Eastern Europe  \njohab cp1361, ms1361 Korean  \nkoi8_r  Russian  \nkoi8_t  \nTajik  New in version 3.5.    \nkoi8_u  Ukrainian  \nkz1048 kz_1048, strk1048_2002, rk1048 \nKazakh  New in version 3.5.    \nmac_cyrillic maccyrillic Bulgarian, Byelorussian, Macedonian, Russian, Serbian  \nmac_greek macgreek Greek  \nmac_iceland maciceland Icelandic  \nmac_latin2 maclatin2, maccentraleurope, mac_centeuro Central and Eastern Europe  \nmac_roman macroman, macintosh Western Europe  \nmac_turkish macturkish Turkish  \nptcp154 csptcp154, pt154, cp154, cyrillic-asian Kazakh  \nshift_jis csshiftjis, shiftjis, sjis, s_jis Japanese  \nshift_jis_2004 shiftjis2004, sjis_2004, sjis2004 Japanese  \nshift_jisx0213 shiftjisx0213, sjisx0213, s_jisx0213 Japanese  \nutf_32 U32, utf32 all languages  \nutf_32_be UTF-32BE all languages  \nutf_32_le UTF-32LE all languages  \nutf_16 U16, utf16 all languages  \nutf_16_be UTF-16BE all languages  \nutf_16_le UTF-16LE all languages  \nutf_7 U7, unicode-1-1-utf-7 all languages  \nutf_8 U8, UTF, utf8, cp65001 all languages  \nutf_8_sig  all languages    Changed in version 3.4: The utf-16* and utf-32* encoders no longer allow surrogate code points (U+D800\u2013U+DFFF) to be encoded. The utf-32* decoders no longer decode byte sequences that correspond to surrogate code points.   Changed in version 3.8: cp65001 is now an alias to utf_8.  Python Specific Encodings A number of predefined codecs are specific to Python, so their codec names have no meaning outside Python. These are listed in the tables below based on the expected input and output types (note that while text encodings are the most common use case for codecs, the underlying codec infrastructure supports arbitrary data transforms rather than just text encodings). For asymmetric codecs, the stated meaning describes the encoding direction. Text Encodings The following codecs provide str to bytes encoding and bytes-like object to str decoding, similar to the Unicode text encodings.   \nCodec Aliases Meaning   \nidna  Implement RFC 3490, see also encodings.idna. Only errors='strict' is supported.  \nmbcs ansi, dbcs Windows only: Encode the operand according to the ANSI codepage (CP_ACP).  \noem  \nWindows only: Encode the operand according to the OEM codepage (CP_OEMCP).  New in version 3.6.    \npalmos  Encoding of PalmOS 3.5.  \npunycode  Implement RFC 3492. Stateful codecs are not supported.  \nraw_unicode_escape  Latin-1 encoding with \\uXXXX and \\UXXXXXXXX for other code points. Existing backslashes are not escaped in any way. It is used in the Python pickle protocol.  \nundefined  Raise an exception for all conversions, even empty strings. The error handler is ignored.  \nunicode_escape  Encoding suitable as the contents of a Unicode literal in ASCII-encoded Python source code, except that quotes are not escaped. Decode from Latin-1 source code. Beware that Python source code actually uses UTF-8 by default.    Changed in version 3.8: \u201cunicode_internal\u201d codec is removed.  Binary Transforms The following codecs provide binary transforms: bytes-like object to bytes mappings. They are not supported by bytes.decode() (which only produces str output).   \nCodec Aliases Meaning Encoder / decoder   \nbase64_codec 1 base64, base_64 \nConvert the operand to multiline MIME base64 (the result always includes a trailing '\\n').  Changed in version 3.4: accepts any bytes-like object as input for encoding and decoding   base64.encodebytes() / base64.decodebytes()  \nbz2_codec bz2 Compress the operand using bz2. bz2.compress() / bz2.decompress()  \nhex_codec hex Convert the operand to hexadecimal representation, with two digits per byte. binascii.b2a_hex() / binascii.a2b_hex()  \nquopri_codec quopri, quotedprintable, quoted_printable Convert the operand to MIME quoted printable. quopri.encode() with quotetabs=True / quopri.decode()  \nuu_codec uu Convert the operand using uuencode. uu.encode() / uu.decode()  \nzlib_codec zip, zlib Compress the operand using gzip. zlib.compress() / zlib.decompress()    \n1  \nIn addition to bytes-like objects, 'base64_codec' also accepts ASCII-only instances of str for decoding    New in version 3.2: Restoration of the binary transforms.   Changed in version 3.4: Restoration of the aliases for the binary transforms.  Text Transforms The following codec provides a text transform: a str to str mapping. It is not supported by str.encode() (which only produces bytes output).   \nCodec Aliases Meaning   \nrot_13 rot13 Return the Caesar-cypher encryption of the operand.    New in version 3.2: Restoration of the rot_13 text transform.   Changed in version 3.4: Restoration of the rot13 alias.  encodings.idna \u2014 Internationalized Domain Names in Applications This module implements RFC 3490 (Internationalized Domain Names in Applications) and RFC 3492 (Nameprep: A Stringprep Profile for Internationalized Domain Names (IDN)). It builds upon the punycode encoding and stringprep. If you need the IDNA 2008 standard from RFC 5891 and RFC 5895, use the third-party idna module <https://pypi.org/project/idna/>_. These RFCs together define a protocol to support non-ASCII characters in domain names. A domain name containing non-ASCII characters (such as www.Alliancefran\u00e7aise.nu) is converted into an ASCII-compatible encoding (ACE, such as www.xn--alliancefranaise-npb.nu). The ACE form of the domain name is then used in all places where arbitrary characters are not allowed by the protocol, such as DNS queries, HTTP Host fields, and so on. This conversion is carried out in the application; if possible invisible to the user: The application should transparently convert Unicode domain labels to IDNA on the wire, and convert back ACE labels to Unicode before presenting them to the user. Python supports this conversion in several ways: the idna codec performs conversion between Unicode and ACE, separating an input string into labels based on the separator characters defined in section 3.1 of RFC 3490 and converting each label to ACE as required, and conversely separating an input byte string into labels based on the . separator and converting any ACE labels found into unicode. Furthermore, the socket module transparently converts Unicode host names to ACE, so that applications need not be concerned about converting host names themselves when they pass them to the socket module. On top of that, modules that have host names as function parameters, such as http.client and ftplib, accept Unicode host names (http.client then also transparently sends an IDNA hostname in the Host field if it sends that field at all). When receiving host names from the wire (such as in reverse name lookup), no automatic conversion to Unicode is performed: applications wishing to present such host names to the user should decode them to Unicode. The module encodings.idna also implements the nameprep procedure, which performs certain normalizations on host names, to achieve case-insensitivity of international domain names, and to unify similar characters. The nameprep functions can be used directly if desired.  \nencodings.idna.nameprep(label)  \nReturn the nameprepped version of label. The implementation currently assumes query strings, so AllowUnassigned is true. \n  \nencodings.idna.ToASCII(label)  \nConvert a label to ASCII, as specified in RFC 3490. UseSTD3ASCIIRules is assumed to be false. \n  \nencodings.idna.ToUnicode(label)  \nConvert a label to Unicode, as specified in RFC 3490. \n encodings.mbcs \u2014 Windows ANSI codepage This module implements the ANSI codepage (CP_ACP). Availability: Windows only.  Changed in version 3.3: Support any error handler.   Changed in version 3.2: Before 3.2, the errors argument was ignored; 'replace' was always used to encode, and 'ignore' to decode.  encodings.utf_8_sig \u2014 UTF-8 codec with BOM signature This module implements a variant of the UTF-8 codec. On encoding, a UTF-8 encoded BOM will be prepended to the UTF-8 encoded bytes. For the stateful encoder this is only done once (on the first write to the byte stream). On decoding, an optional UTF-8 encoded BOM at the start of the data will be skipped.\n"}, {"name": "codecs.backslashreplace_errors()", "path": "library/codecs#codecs.backslashreplace_errors", "type": "Binary Data", "text": " \ncodecs.backslashreplace_errors(exception)  \nImplements the 'backslashreplace' error handling (for text encodings only): malformed data is replaced by a backslashed escape sequence. \n"}, {"name": "codecs.BOM", "path": "library/codecs#codecs.BOM", "type": "Binary Data", "text": " \ncodecs.BOM  \ncodecs.BOM_BE  \ncodecs.BOM_LE  \ncodecs.BOM_UTF8  \ncodecs.BOM_UTF16  \ncodecs.BOM_UTF16_BE  \ncodecs.BOM_UTF16_LE  \ncodecs.BOM_UTF32  \ncodecs.BOM_UTF32_BE  \ncodecs.BOM_UTF32_LE  \nThese constants define various byte sequences, being Unicode byte order marks (BOMs) for several encodings. They are used in UTF-16 and UTF-32 data streams to indicate the byte order used, and in UTF-8 as a Unicode signature. BOM_UTF16 is either BOM_UTF16_BE or BOM_UTF16_LE depending on the platform\u2019s native byte order, BOM is an alias for BOM_UTF16, BOM_LE for BOM_UTF16_LE and BOM_BE for BOM_UTF16_BE. The others represent the BOM in UTF-8 and UTF-32 encodings. \n"}, {"name": "codecs.BOM_BE", "path": "library/codecs#codecs.BOM_BE", "type": "Binary Data", "text": " \ncodecs.BOM  \ncodecs.BOM_BE  \ncodecs.BOM_LE  \ncodecs.BOM_UTF8  \ncodecs.BOM_UTF16  \ncodecs.BOM_UTF16_BE  \ncodecs.BOM_UTF16_LE  \ncodecs.BOM_UTF32  \ncodecs.BOM_UTF32_BE  \ncodecs.BOM_UTF32_LE  \nThese constants define various byte sequences, being Unicode byte order marks (BOMs) for several encodings. They are used in UTF-16 and UTF-32 data streams to indicate the byte order used, and in UTF-8 as a Unicode signature. BOM_UTF16 is either BOM_UTF16_BE or BOM_UTF16_LE depending on the platform\u2019s native byte order, BOM is an alias for BOM_UTF16, BOM_LE for BOM_UTF16_LE and BOM_BE for BOM_UTF16_BE. The others represent the BOM in UTF-8 and UTF-32 encodings. \n"}, {"name": "codecs.BOM_LE", "path": "library/codecs#codecs.BOM_LE", "type": "Binary Data", "text": " \ncodecs.BOM  \ncodecs.BOM_BE  \ncodecs.BOM_LE  \ncodecs.BOM_UTF8  \ncodecs.BOM_UTF16  \ncodecs.BOM_UTF16_BE  \ncodecs.BOM_UTF16_LE  \ncodecs.BOM_UTF32  \ncodecs.BOM_UTF32_BE  \ncodecs.BOM_UTF32_LE  \nThese constants define various byte sequences, being Unicode byte order marks (BOMs) for several encodings. They are used in UTF-16 and UTF-32 data streams to indicate the byte order used, and in UTF-8 as a Unicode signature. BOM_UTF16 is either BOM_UTF16_BE or BOM_UTF16_LE depending on the platform\u2019s native byte order, BOM is an alias for BOM_UTF16, BOM_LE for BOM_UTF16_LE and BOM_BE for BOM_UTF16_BE. The others represent the BOM in UTF-8 and UTF-32 encodings. \n"}, {"name": "codecs.BOM_UTF16", "path": "library/codecs#codecs.BOM_UTF16", "type": "Binary Data", "text": " \ncodecs.BOM  \ncodecs.BOM_BE  \ncodecs.BOM_LE  \ncodecs.BOM_UTF8  \ncodecs.BOM_UTF16  \ncodecs.BOM_UTF16_BE  \ncodecs.BOM_UTF16_LE  \ncodecs.BOM_UTF32  \ncodecs.BOM_UTF32_BE  \ncodecs.BOM_UTF32_LE  \nThese constants define various byte sequences, being Unicode byte order marks (BOMs) for several encodings. They are used in UTF-16 and UTF-32 data streams to indicate the byte order used, and in UTF-8 as a Unicode signature. BOM_UTF16 is either BOM_UTF16_BE or BOM_UTF16_LE depending on the platform\u2019s native byte order, BOM is an alias for BOM_UTF16, BOM_LE for BOM_UTF16_LE and BOM_BE for BOM_UTF16_BE. The others represent the BOM in UTF-8 and UTF-32 encodings. \n"}, {"name": "codecs.BOM_UTF16_BE", "path": "library/codecs#codecs.BOM_UTF16_BE", "type": "Binary Data", "text": " \ncodecs.BOM  \ncodecs.BOM_BE  \ncodecs.BOM_LE  \ncodecs.BOM_UTF8  \ncodecs.BOM_UTF16  \ncodecs.BOM_UTF16_BE  \ncodecs.BOM_UTF16_LE  \ncodecs.BOM_UTF32  \ncodecs.BOM_UTF32_BE  \ncodecs.BOM_UTF32_LE  \nThese constants define various byte sequences, being Unicode byte order marks (BOMs) for several encodings. They are used in UTF-16 and UTF-32 data streams to indicate the byte order used, and in UTF-8 as a Unicode signature. BOM_UTF16 is either BOM_UTF16_BE or BOM_UTF16_LE depending on the platform\u2019s native byte order, BOM is an alias for BOM_UTF16, BOM_LE for BOM_UTF16_LE and BOM_BE for BOM_UTF16_BE. The others represent the BOM in UTF-8 and UTF-32 encodings. \n"}, {"name": "codecs.BOM_UTF16_LE", "path": "library/codecs#codecs.BOM_UTF16_LE", "type": "Binary Data", "text": " \ncodecs.BOM  \ncodecs.BOM_BE  \ncodecs.BOM_LE  \ncodecs.BOM_UTF8  \ncodecs.BOM_UTF16  \ncodecs.BOM_UTF16_BE  \ncodecs.BOM_UTF16_LE  \ncodecs.BOM_UTF32  \ncodecs.BOM_UTF32_BE  \ncodecs.BOM_UTF32_LE  \nThese constants define various byte sequences, being Unicode byte order marks (BOMs) for several encodings. They are used in UTF-16 and UTF-32 data streams to indicate the byte order used, and in UTF-8 as a Unicode signature. BOM_UTF16 is either BOM_UTF16_BE or BOM_UTF16_LE depending on the platform\u2019s native byte order, BOM is an alias for BOM_UTF16, BOM_LE for BOM_UTF16_LE and BOM_BE for BOM_UTF16_BE. The others represent the BOM in UTF-8 and UTF-32 encodings. \n"}, {"name": "codecs.BOM_UTF32", "path": "library/codecs#codecs.BOM_UTF32", "type": "Binary Data", "text": " \ncodecs.BOM  \ncodecs.BOM_BE  \ncodecs.BOM_LE  \ncodecs.BOM_UTF8  \ncodecs.BOM_UTF16  \ncodecs.BOM_UTF16_BE  \ncodecs.BOM_UTF16_LE  \ncodecs.BOM_UTF32  \ncodecs.BOM_UTF32_BE  \ncodecs.BOM_UTF32_LE  \nThese constants define various byte sequences, being Unicode byte order marks (BOMs) for several encodings. They are used in UTF-16 and UTF-32 data streams to indicate the byte order used, and in UTF-8 as a Unicode signature. BOM_UTF16 is either BOM_UTF16_BE or BOM_UTF16_LE depending on the platform\u2019s native byte order, BOM is an alias for BOM_UTF16, BOM_LE for BOM_UTF16_LE and BOM_BE for BOM_UTF16_BE. The others represent the BOM in UTF-8 and UTF-32 encodings. \n"}, {"name": "codecs.BOM_UTF32_BE", "path": "library/codecs#codecs.BOM_UTF32_BE", "type": "Binary Data", "text": " \ncodecs.BOM  \ncodecs.BOM_BE  \ncodecs.BOM_LE  \ncodecs.BOM_UTF8  \ncodecs.BOM_UTF16  \ncodecs.BOM_UTF16_BE  \ncodecs.BOM_UTF16_LE  \ncodecs.BOM_UTF32  \ncodecs.BOM_UTF32_BE  \ncodecs.BOM_UTF32_LE  \nThese constants define various byte sequences, being Unicode byte order marks (BOMs) for several encodings. They are used in UTF-16 and UTF-32 data streams to indicate the byte order used, and in UTF-8 as a Unicode signature. BOM_UTF16 is either BOM_UTF16_BE or BOM_UTF16_LE depending on the platform\u2019s native byte order, BOM is an alias for BOM_UTF16, BOM_LE for BOM_UTF16_LE and BOM_BE for BOM_UTF16_BE. The others represent the BOM in UTF-8 and UTF-32 encodings. \n"}, {"name": "codecs.BOM_UTF32_LE", "path": "library/codecs#codecs.BOM_UTF32_LE", "type": "Binary Data", "text": " \ncodecs.BOM  \ncodecs.BOM_BE  \ncodecs.BOM_LE  \ncodecs.BOM_UTF8  \ncodecs.BOM_UTF16  \ncodecs.BOM_UTF16_BE  \ncodecs.BOM_UTF16_LE  \ncodecs.BOM_UTF32  \ncodecs.BOM_UTF32_BE  \ncodecs.BOM_UTF32_LE  \nThese constants define various byte sequences, being Unicode byte order marks (BOMs) for several encodings. They are used in UTF-16 and UTF-32 data streams to indicate the byte order used, and in UTF-8 as a Unicode signature. BOM_UTF16 is either BOM_UTF16_BE or BOM_UTF16_LE depending on the platform\u2019s native byte order, BOM is an alias for BOM_UTF16, BOM_LE for BOM_UTF16_LE and BOM_BE for BOM_UTF16_BE. The others represent the BOM in UTF-8 and UTF-32 encodings. \n"}, {"name": "codecs.BOM_UTF8", "path": "library/codecs#codecs.BOM_UTF8", "type": "Binary Data", "text": " \ncodecs.BOM  \ncodecs.BOM_BE  \ncodecs.BOM_LE  \ncodecs.BOM_UTF8  \ncodecs.BOM_UTF16  \ncodecs.BOM_UTF16_BE  \ncodecs.BOM_UTF16_LE  \ncodecs.BOM_UTF32  \ncodecs.BOM_UTF32_BE  \ncodecs.BOM_UTF32_LE  \nThese constants define various byte sequences, being Unicode byte order marks (BOMs) for several encodings. They are used in UTF-16 and UTF-32 data streams to indicate the byte order used, and in UTF-8 as a Unicode signature. BOM_UTF16 is either BOM_UTF16_BE or BOM_UTF16_LE depending on the platform\u2019s native byte order, BOM is an alias for BOM_UTF16, BOM_LE for BOM_UTF16_LE and BOM_BE for BOM_UTF16_BE. The others represent the BOM in UTF-8 and UTF-32 encodings. \n"}, {"name": "codecs.Codec.decode()", "path": "library/codecs#codecs.Codec.decode", "type": "Binary Data", "text": " \nCodec.decode(input[, errors])  \nDecodes the object input and returns a tuple (output object, length consumed). For instance, for a text encoding, decoding converts a bytes object encoded using a particular character set encoding to a string object. For text encodings and bytes-to-bytes codecs, input must be a bytes object or one which provides the read-only buffer interface \u2013 for example, buffer objects and memory mapped files. The errors argument defines the error handling to apply. It defaults to 'strict' handling. The method may not store state in the Codec instance. Use StreamReader for codecs which have to keep state in order to make decoding efficient. The decoder must be able to handle zero length input and return an empty object of the output object type in this situation. \n"}, {"name": "codecs.Codec.encode()", "path": "library/codecs#codecs.Codec.encode", "type": "Binary Data", "text": " \nCodec.encode(input[, errors])  \nEncodes the object input and returns a tuple (output object, length consumed). For instance, text encoding converts a string object to a bytes object using a particular character set encoding (e.g., cp1252 or iso-8859-1). The errors argument defines the error handling to apply. It defaults to 'strict' handling. The method may not store state in the Codec instance. Use StreamWriter for codecs which have to keep state in order to make encoding efficient. The encoder must be able to handle zero length input and return an empty object of the output object type in this situation. \n"}, {"name": "codecs.CodecInfo", "path": "library/codecs#codecs.CodecInfo", "type": "Binary Data", "text": " \nclass codecs.CodecInfo(encode, decode, streamreader=None, streamwriter=None, incrementalencoder=None, incrementaldecoder=None, name=None)  \nCodec details when looking up the codec registry. The constructor arguments are stored in attributes of the same name:  \nname  \nThe name of the encoding. \n  \nencode  \ndecode  \nThe stateless encoding and decoding functions. These must be functions or methods which have the same interface as the encode() and decode() methods of Codec instances (see Codec Interface). The functions or methods are expected to work in a stateless mode. \n  \nincrementalencoder  \nincrementaldecoder  \nIncremental encoder and decoder classes or factory functions. These have to provide the interface defined by the base classes IncrementalEncoder and IncrementalDecoder, respectively. Incremental codecs can maintain state. \n  \nstreamwriter  \nstreamreader  \nStream writer and reader classes or factory functions. These have to provide the interface defined by the base classes StreamWriter and StreamReader, respectively. Stream codecs can maintain state. \n \n"}, {"name": "codecs.CodecInfo.decode", "path": "library/codecs#codecs.CodecInfo.decode", "type": "Binary Data", "text": " \nencode  \ndecode  \nThe stateless encoding and decoding functions. These must be functions or methods which have the same interface as the encode() and decode() methods of Codec instances (see Codec Interface). The functions or methods are expected to work in a stateless mode. \n"}, {"name": "codecs.CodecInfo.encode", "path": "library/codecs#codecs.CodecInfo.encode", "type": "Binary Data", "text": " \nencode  \ndecode  \nThe stateless encoding and decoding functions. These must be functions or methods which have the same interface as the encode() and decode() methods of Codec instances (see Codec Interface). The functions or methods are expected to work in a stateless mode. \n"}, {"name": "codecs.CodecInfo.incrementaldecoder", "path": "library/codecs#codecs.CodecInfo.incrementaldecoder", "type": "Binary Data", "text": " \nincrementalencoder  \nincrementaldecoder  \nIncremental encoder and decoder classes or factory functions. These have to provide the interface defined by the base classes IncrementalEncoder and IncrementalDecoder, respectively. Incremental codecs can maintain state. \n"}, {"name": "codecs.CodecInfo.incrementalencoder", "path": "library/codecs#codecs.CodecInfo.incrementalencoder", "type": "Binary Data", "text": " \nincrementalencoder  \nincrementaldecoder  \nIncremental encoder and decoder classes or factory functions. These have to provide the interface defined by the base classes IncrementalEncoder and IncrementalDecoder, respectively. Incremental codecs can maintain state. \n"}, {"name": "codecs.CodecInfo.name", "path": "library/codecs#codecs.CodecInfo.name", "type": "Binary Data", "text": " \nname  \nThe name of the encoding. \n"}, {"name": "codecs.CodecInfo.streamreader", "path": "library/codecs#codecs.CodecInfo.streamreader", "type": "Binary Data", "text": " \nstreamwriter  \nstreamreader  \nStream writer and reader classes or factory functions. These have to provide the interface defined by the base classes StreamWriter and StreamReader, respectively. Stream codecs can maintain state. \n"}, {"name": "codecs.CodecInfo.streamwriter", "path": "library/codecs#codecs.CodecInfo.streamwriter", "type": "Binary Data", "text": " \nstreamwriter  \nstreamreader  \nStream writer and reader classes or factory functions. These have to provide the interface defined by the base classes StreamWriter and StreamReader, respectively. Stream codecs can maintain state. \n"}, {"name": "codecs.decode()", "path": "library/codecs#codecs.decode", "type": "Binary Data", "text": " \ncodecs.decode(obj, encoding='utf-8', errors='strict')  \nDecodes obj using the codec registered for encoding. Errors may be given to set the desired error handling scheme. The default error handler is 'strict' meaning that decoding errors raise ValueError (or a more codec specific subclass, such as UnicodeDecodeError). Refer to Codec Base Classes for more information on codec error handling. \n"}, {"name": "codecs.encode()", "path": "library/codecs#codecs.encode", "type": "Binary Data", "text": " \ncodecs.encode(obj, encoding='utf-8', errors='strict')  \nEncodes obj using the codec registered for encoding. Errors may be given to set the desired error handling scheme. The default error handler is 'strict' meaning that encoding errors raise ValueError (or a more codec specific subclass, such as UnicodeEncodeError). Refer to Codec Base Classes for more information on codec error handling. \n"}, {"name": "codecs.EncodedFile()", "path": "library/codecs#codecs.EncodedFile", "type": "Binary Data", "text": " \ncodecs.EncodedFile(file, data_encoding, file_encoding=None, errors='strict')  \nReturn a StreamRecoder instance, a wrapped version of file which provides transparent transcoding. The original file is closed when the wrapped version is closed. Data written to the wrapped file is decoded according to the given data_encoding and then written to the original file as bytes using file_encoding. Bytes read from the original file are decoded according to file_encoding, and the result is encoded using data_encoding. If file_encoding is not given, it defaults to data_encoding. errors may be given to define the error handling. It defaults to 'strict', which causes ValueError to be raised in case an encoding error occurs. \n"}, {"name": "codecs.getdecoder()", "path": "library/codecs#codecs.getdecoder", "type": "Binary Data", "text": " \ncodecs.getdecoder(encoding)  \nLook up the codec for the given encoding and return its decoder function. Raises a LookupError in case the encoding cannot be found. \n"}, {"name": "codecs.getencoder()", "path": "library/codecs#codecs.getencoder", "type": "Binary Data", "text": " \ncodecs.getencoder(encoding)  \nLook up the codec for the given encoding and return its encoder function. Raises a LookupError in case the encoding cannot be found. \n"}, {"name": "codecs.getincrementaldecoder()", "path": "library/codecs#codecs.getincrementaldecoder", "type": "Binary Data", "text": " \ncodecs.getincrementaldecoder(encoding)  \nLook up the codec for the given encoding and return its incremental decoder class or factory function. Raises a LookupError in case the encoding cannot be found or the codec doesn\u2019t support an incremental decoder. \n"}, {"name": "codecs.getincrementalencoder()", "path": "library/codecs#codecs.getincrementalencoder", "type": "Binary Data", "text": " \ncodecs.getincrementalencoder(encoding)  \nLook up the codec for the given encoding and return its incremental encoder class or factory function. Raises a LookupError in case the encoding cannot be found or the codec doesn\u2019t support an incremental encoder. \n"}, {"name": "codecs.getreader()", "path": "library/codecs#codecs.getreader", "type": "Binary Data", "text": " \ncodecs.getreader(encoding)  \nLook up the codec for the given encoding and return its StreamReader class or factory function. Raises a LookupError in case the encoding cannot be found. \n"}, {"name": "codecs.getwriter()", "path": "library/codecs#codecs.getwriter", "type": "Binary Data", "text": " \ncodecs.getwriter(encoding)  \nLook up the codec for the given encoding and return its StreamWriter class or factory function. Raises a LookupError in case the encoding cannot be found. \n"}, {"name": "codecs.ignore_errors()", "path": "library/codecs#codecs.ignore_errors", "type": "Binary Data", "text": " \ncodecs.ignore_errors(exception)  \nImplements the 'ignore' error handling: malformed data is ignored and encoding or decoding is continued without further notice. \n"}, {"name": "codecs.IncrementalDecoder", "path": "library/codecs#codecs.IncrementalDecoder", "type": "Binary Data", "text": " \nclass codecs.IncrementalDecoder(errors='strict')  \nConstructor for an IncrementalDecoder instance. All incremental decoders must provide this constructor interface. They are free to add additional keyword arguments, but only the ones defined here are used by the Python codec registry. The IncrementalDecoder may implement different error handling schemes by providing the errors keyword argument. See Error Handlers for possible values. The errors argument will be assigned to an attribute of the same name. Assigning to this attribute makes it possible to switch between different error handling strategies during the lifetime of the IncrementalDecoder object.  \ndecode(object[, final])  \nDecodes object (taking the current state of the decoder into account) and returns the resulting decoded object. If this is the last call to decode() final must be true (the default is false). If final is true the decoder must decode the input completely and must flush all buffers. If this isn\u2019t possible (e.g. because of incomplete byte sequences at the end of the input) it must initiate error handling just like in the stateless case (which might raise an exception). \n  \nreset()  \nReset the decoder to the initial state. \n  \ngetstate()  \nReturn the current state of the decoder. This must be a tuple with two items, the first must be the buffer containing the still undecoded input. The second must be an integer and can be additional state info. (The implementation should make sure that 0 is the most common additional state info.) If this additional state info is 0 it must be possible to set the decoder to the state which has no input buffered and 0 as the additional state info, so that feeding the previously buffered input to the decoder returns it to the previous state without producing any output. (Additional state info that is more complicated than integers can be converted into an integer by marshaling/pickling the info and encoding the bytes of the resulting string into an integer.) \n  \nsetstate(state)  \nSet the state of the decoder to state. state must be a decoder state returned by getstate(). \n \n"}, {"name": "codecs.IncrementalDecoder.decode()", "path": "library/codecs#codecs.IncrementalDecoder.decode", "type": "Binary Data", "text": " \ndecode(object[, final])  \nDecodes object (taking the current state of the decoder into account) and returns the resulting decoded object. If this is the last call to decode() final must be true (the default is false). If final is true the decoder must decode the input completely and must flush all buffers. If this isn\u2019t possible (e.g. because of incomplete byte sequences at the end of the input) it must initiate error handling just like in the stateless case (which might raise an exception). \n"}, {"name": "codecs.IncrementalDecoder.getstate()", "path": "library/codecs#codecs.IncrementalDecoder.getstate", "type": "Binary Data", "text": " \ngetstate()  \nReturn the current state of the decoder. This must be a tuple with two items, the first must be the buffer containing the still undecoded input. The second must be an integer and can be additional state info. (The implementation should make sure that 0 is the most common additional state info.) If this additional state info is 0 it must be possible to set the decoder to the state which has no input buffered and 0 as the additional state info, so that feeding the previously buffered input to the decoder returns it to the previous state without producing any output. (Additional state info that is more complicated than integers can be converted into an integer by marshaling/pickling the info and encoding the bytes of the resulting string into an integer.) \n"}, {"name": "codecs.IncrementalDecoder.reset()", "path": "library/codecs#codecs.IncrementalDecoder.reset", "type": "Binary Data", "text": " \nreset()  \nReset the decoder to the initial state. \n"}, {"name": "codecs.IncrementalDecoder.setstate()", "path": "library/codecs#codecs.IncrementalDecoder.setstate", "type": "Binary Data", "text": " \nsetstate(state)  \nSet the state of the decoder to state. state must be a decoder state returned by getstate(). \n"}, {"name": "codecs.IncrementalEncoder", "path": "library/codecs#codecs.IncrementalEncoder", "type": "Binary Data", "text": " \nclass codecs.IncrementalEncoder(errors='strict')  \nConstructor for an IncrementalEncoder instance. All incremental encoders must provide this constructor interface. They are free to add additional keyword arguments, but only the ones defined here are used by the Python codec registry. The IncrementalEncoder may implement different error handling schemes by providing the errors keyword argument. See Error Handlers for possible values. The errors argument will be assigned to an attribute of the same name. Assigning to this attribute makes it possible to switch between different error handling strategies during the lifetime of the IncrementalEncoder object.  \nencode(object[, final])  \nEncodes object (taking the current state of the encoder into account) and returns the resulting encoded object. If this is the last call to encode() final must be true (the default is false). \n  \nreset()  \nReset the encoder to the initial state. The output is discarded: call .encode(object, final=True), passing an empty byte or text string if necessary, to reset the encoder and to get the output. \n  \ngetstate()  \nReturn the current state of the encoder which must be an integer. The implementation should make sure that 0 is the most common state. (States that are more complicated than integers can be converted into an integer by marshaling/pickling the state and encoding the bytes of the resulting string into an integer.) \n  \nsetstate(state)  \nSet the state of the encoder to state. state must be an encoder state returned by getstate(). \n \n"}, {"name": "codecs.IncrementalEncoder.encode()", "path": "library/codecs#codecs.IncrementalEncoder.encode", "type": "Binary Data", "text": " \nencode(object[, final])  \nEncodes object (taking the current state of the encoder into account) and returns the resulting encoded object. If this is the last call to encode() final must be true (the default is false). \n"}, {"name": "codecs.IncrementalEncoder.getstate()", "path": "library/codecs#codecs.IncrementalEncoder.getstate", "type": "Binary Data", "text": " \ngetstate()  \nReturn the current state of the encoder which must be an integer. The implementation should make sure that 0 is the most common state. (States that are more complicated than integers can be converted into an integer by marshaling/pickling the state and encoding the bytes of the resulting string into an integer.) \n"}, {"name": "codecs.IncrementalEncoder.reset()", "path": "library/codecs#codecs.IncrementalEncoder.reset", "type": "Binary Data", "text": " \nreset()  \nReset the encoder to the initial state. The output is discarded: call .encode(object, final=True), passing an empty byte or text string if necessary, to reset the encoder and to get the output. \n"}, {"name": "codecs.IncrementalEncoder.setstate()", "path": "library/codecs#codecs.IncrementalEncoder.setstate", "type": "Binary Data", "text": " \nsetstate(state)  \nSet the state of the encoder to state. state must be an encoder state returned by getstate(). \n"}, {"name": "codecs.iterdecode()", "path": "library/codecs#codecs.iterdecode", "type": "Binary Data", "text": " \ncodecs.iterdecode(iterator, encoding, errors='strict', **kwargs)  \nUses an incremental decoder to iteratively decode the input provided by iterator. This function is a generator. The errors argument (as well as any other keyword argument) is passed through to the incremental decoder. This function requires that the codec accept bytes objects to decode. Therefore it does not support text-to-text encoders such as rot_13, although rot_13 may be used equivalently with iterencode(). \n"}, {"name": "codecs.iterencode()", "path": "library/codecs#codecs.iterencode", "type": "Binary Data", "text": " \ncodecs.iterencode(iterator, encoding, errors='strict', **kwargs)  \nUses an incremental encoder to iteratively encode the input provided by iterator. This function is a generator. The errors argument (as well as any other keyword argument) is passed through to the incremental encoder. This function requires that the codec accept text str objects to encode. Therefore it does not support bytes-to-bytes encoders such as base64_codec. \n"}, {"name": "codecs.lookup()", "path": "library/codecs#codecs.lookup", "type": "Binary Data", "text": " \ncodecs.lookup(encoding)  \nLooks up the codec info in the Python codec registry and returns a CodecInfo object as defined below. Encodings are first looked up in the registry\u2019s cache. If not found, the list of registered search functions is scanned. If no CodecInfo object is found, a LookupError is raised. Otherwise, the CodecInfo object is stored in the cache and returned to the caller. \n"}, {"name": "codecs.lookup_error()", "path": "library/codecs#codecs.lookup_error", "type": "Binary Data", "text": " \ncodecs.lookup_error(name)  \nReturn the error handler previously registered under the name name. Raises a LookupError in case the handler cannot be found. \n"}, {"name": "codecs.namereplace_errors()", "path": "library/codecs#codecs.namereplace_errors", "type": "Binary Data", "text": " \ncodecs.namereplace_errors(exception)  \nImplements the 'namereplace' error handling (for encoding with text encodings only): the unencodable character is replaced by a \\N{...} escape sequence.  New in version 3.5.  \n"}, {"name": "codecs.open()", "path": "library/codecs#codecs.open", "type": "Binary Data", "text": " \ncodecs.open(filename, mode='r', encoding=None, errors='strict', buffering=-1)  \nOpen an encoded file using the given mode and return an instance of StreamReaderWriter, providing transparent encoding/decoding. The default file mode is 'r', meaning to open the file in read mode.  Note Underlying encoded files are always opened in binary mode. No automatic conversion of '\\n' is done on reading and writing. The mode argument may be any binary mode acceptable to the built-in open() function; the 'b' is automatically added.  encoding specifies the encoding which is to be used for the file. Any encoding that encodes to and decodes from bytes is allowed, and the data types supported by the file methods depend on the codec used. errors may be given to define the error handling. It defaults to 'strict' which causes a ValueError to be raised in case an encoding error occurs. buffering has the same meaning as for the built-in open() function. It defaults to -1 which means that the default buffer size will be used. \n"}, {"name": "codecs.register()", "path": "library/codecs#codecs.register", "type": "Binary Data", "text": " \ncodecs.register(search_function)  \nRegister a codec search function. Search functions are expected to take one argument, being the encoding name in all lower case letters with hyphens and spaces converted to underscores, and return a CodecInfo object. In case a search function cannot find a given encoding, it should return None.  Changed in version 3.9: Hyphens and spaces are converted to underscore.   Note Search function registration is not currently reversible, which may cause problems in some cases, such as unit testing or module reloading.  \n"}, {"name": "codecs.register_error()", "path": "library/codecs#codecs.register_error", "type": "Binary Data", "text": " \ncodecs.register_error(name, error_handler)  \nRegister the error handling function error_handler under the name name. The error_handler argument will be called during encoding and decoding in case of an error, when name is specified as the errors parameter. For encoding, error_handler will be called with a UnicodeEncodeError instance, which contains information about the location of the error. The error handler must either raise this or a different exception, or return a tuple with a replacement for the unencodable part of the input and a position where encoding should continue. The replacement may be either str or bytes. If the replacement is bytes, the encoder will simply copy them into the output buffer. If the replacement is a string, the encoder will encode the replacement. Encoding continues on original input at the specified position. Negative position values will be treated as being relative to the end of the input string. If the resulting position is out of bound an IndexError will be raised. Decoding and translating works similarly, except UnicodeDecodeError or UnicodeTranslateError will be passed to the handler and that the replacement from the error handler will be put into the output directly. \n"}, {"name": "codecs.replace_errors()", "path": "library/codecs#codecs.replace_errors", "type": "Binary Data", "text": " \ncodecs.replace_errors(exception)  \nImplements the 'replace' error handling (for text encodings only): substitutes '?' for encoding errors (to be encoded by the codec), and '\\ufffd' (the Unicode replacement character) for decoding errors. \n"}, {"name": "codecs.StreamReader", "path": "library/codecs#codecs.StreamReader", "type": "Binary Data", "text": " \nclass codecs.StreamReader(stream, errors='strict')  \nConstructor for a StreamReader instance. All stream readers must provide this constructor interface. They are free to add additional keyword arguments, but only the ones defined here are used by the Python codec registry. The stream argument must be a file-like object open for reading text or binary data, as appropriate for the specific codec. The StreamReader may implement different error handling schemes by providing the errors keyword argument. See Error Handlers for the standard error handlers the underlying stream codec may support. The errors argument will be assigned to an attribute of the same name. Assigning to this attribute makes it possible to switch between different error handling strategies during the lifetime of the StreamReader object. The set of allowed values for the errors argument can be extended with register_error().  \nread([size[, chars[, firstline]]])  \nDecodes data from the stream and returns the resulting object. The chars argument indicates the number of decoded code points or bytes to return. The read() method will never return more data than requested, but it might return less, if there is not enough available. The size argument indicates the approximate maximum number of encoded bytes or code points to read for decoding. The decoder can modify this setting as appropriate. The default value -1 indicates to read and decode as much as possible. This parameter is intended to prevent having to decode huge files in one step. The firstline flag indicates that it would be sufficient to only return the first line, if there are decoding errors on later lines. The method should use a greedy read strategy meaning that it should read as much data as is allowed within the definition of the encoding and the given size, e.g. if optional encoding endings or state markers are available on the stream, these should be read too. \n  \nreadline([size[, keepends]])  \nRead one line from the input stream and return the decoded data. size, if given, is passed as size argument to the stream\u2019s read() method. If keepends is false line-endings will be stripped from the lines returned. \n  \nreadlines([sizehint[, keepends]])  \nRead all lines available on the input stream and return them as a list of lines. Line-endings are implemented using the codec\u2019s decode() method and are included in the list entries if keepends is true. sizehint, if given, is passed as the size argument to the stream\u2019s read() method. \n  \nreset()  \nResets the codec buffers used for keeping internal state. Note that no stream repositioning should take place. This method is primarily intended to be able to recover from decoding errors. \n \n"}, {"name": "codecs.StreamReader.read()", "path": "library/codecs#codecs.StreamReader.read", "type": "Binary Data", "text": " \nread([size[, chars[, firstline]]])  \nDecodes data from the stream and returns the resulting object. The chars argument indicates the number of decoded code points or bytes to return. The read() method will never return more data than requested, but it might return less, if there is not enough available. The size argument indicates the approximate maximum number of encoded bytes or code points to read for decoding. The decoder can modify this setting as appropriate. The default value -1 indicates to read and decode as much as possible. This parameter is intended to prevent having to decode huge files in one step. The firstline flag indicates that it would be sufficient to only return the first line, if there are decoding errors on later lines. The method should use a greedy read strategy meaning that it should read as much data as is allowed within the definition of the encoding and the given size, e.g. if optional encoding endings or state markers are available on the stream, these should be read too. \n"}, {"name": "codecs.StreamReader.readline()", "path": "library/codecs#codecs.StreamReader.readline", "type": "Binary Data", "text": " \nreadline([size[, keepends]])  \nRead one line from the input stream and return the decoded data. size, if given, is passed as size argument to the stream\u2019s read() method. If keepends is false line-endings will be stripped from the lines returned. \n"}, {"name": "codecs.StreamReader.readlines()", "path": "library/codecs#codecs.StreamReader.readlines", "type": "Binary Data", "text": " \nreadlines([sizehint[, keepends]])  \nRead all lines available on the input stream and return them as a list of lines. Line-endings are implemented using the codec\u2019s decode() method and are included in the list entries if keepends is true. sizehint, if given, is passed as the size argument to the stream\u2019s read() method. \n"}, {"name": "codecs.StreamReader.reset()", "path": "library/codecs#codecs.StreamReader.reset", "type": "Binary Data", "text": " \nreset()  \nResets the codec buffers used for keeping internal state. Note that no stream repositioning should take place. This method is primarily intended to be able to recover from decoding errors. \n"}, {"name": "codecs.StreamReaderWriter", "path": "library/codecs#codecs.StreamReaderWriter", "type": "Binary Data", "text": " \nclass codecs.StreamReaderWriter(stream, Reader, Writer, errors='strict')  \nCreates a StreamReaderWriter instance. stream must be a file-like object. Reader and Writer must be factory functions or classes providing the StreamReader and StreamWriter interface resp. Error handling is done in the same way as defined for the stream readers and writers. \n"}, {"name": "codecs.StreamRecoder", "path": "library/codecs#codecs.StreamRecoder", "type": "Binary Data", "text": " \nclass codecs.StreamRecoder(stream, encode, decode, Reader, Writer, errors='strict')  \nCreates a StreamRecoder instance which implements a two-way conversion: encode and decode work on the frontend \u2014 the data visible to code calling read() and write(), while Reader and Writer work on the backend \u2014 the data in stream. You can use these objects to do transparent transcodings, e.g., from Latin-1 to UTF-8 and back. The stream argument must be a file-like object. The encode and decode arguments must adhere to the Codec interface. Reader and Writer must be factory functions or classes providing objects of the StreamReader and StreamWriter interface respectively. Error handling is done in the same way as defined for the stream readers and writers. \n"}, {"name": "codecs.StreamWriter", "path": "library/codecs#codecs.StreamWriter", "type": "Binary Data", "text": " \nclass codecs.StreamWriter(stream, errors='strict')  \nConstructor for a StreamWriter instance. All stream writers must provide this constructor interface. They are free to add additional keyword arguments, but only the ones defined here are used by the Python codec registry. The stream argument must be a file-like object open for writing text or binary data, as appropriate for the specific codec. The StreamWriter may implement different error handling schemes by providing the errors keyword argument. See Error Handlers for the standard error handlers the underlying stream codec may support. The errors argument will be assigned to an attribute of the same name. Assigning to this attribute makes it possible to switch between different error handling strategies during the lifetime of the StreamWriter object.  \nwrite(object)  \nWrites the object\u2019s contents encoded to the stream. \n  \nwritelines(list)  \nWrites the concatenated list of strings to the stream (possibly by reusing the write() method). The standard bytes-to-bytes codecs do not support this method. \n  \nreset()  \nResets the codec buffers used for keeping internal state. Calling this method should ensure that the data on the output is put into a clean state that allows appending of new fresh data without having to rescan the whole stream to recover state. \n \n"}, {"name": "codecs.StreamWriter.reset()", "path": "library/codecs#codecs.StreamWriter.reset", "type": "Binary Data", "text": " \nreset()  \nResets the codec buffers used for keeping internal state. Calling this method should ensure that the data on the output is put into a clean state that allows appending of new fresh data without having to rescan the whole stream to recover state. \n"}, {"name": "codecs.StreamWriter.write()", "path": "library/codecs#codecs.StreamWriter.write", "type": "Binary Data", "text": " \nwrite(object)  \nWrites the object\u2019s contents encoded to the stream. \n"}, {"name": "codecs.StreamWriter.writelines()", "path": "library/codecs#codecs.StreamWriter.writelines", "type": "Binary Data", "text": " \nwritelines(list)  \nWrites the concatenated list of strings to the stream (possibly by reusing the write() method). The standard bytes-to-bytes codecs do not support this method. \n"}, {"name": "codecs.strict_errors()", "path": "library/codecs#codecs.strict_errors", "type": "Binary Data", "text": " \ncodecs.strict_errors(exception)  \nImplements the 'strict' error handling: each encoding or decoding error raises a UnicodeError. \n"}, {"name": "codecs.xmlcharrefreplace_errors()", "path": "library/codecs#codecs.xmlcharrefreplace_errors", "type": "Binary Data", "text": " \ncodecs.xmlcharrefreplace_errors(exception)  \nImplements the 'xmlcharrefreplace' error handling (for encoding with text encodings only): the unencodable character is replaced by an appropriate XML character reference. \n"}, {"name": "codeop", "path": "library/codeop", "type": "Interpreters", "text": "codeop \u2014 Compile Python code Source code: Lib/codeop.py The codeop module provides utilities upon which the Python read-eval-print loop can be emulated, as is done in the code module. As a result, you probably don\u2019t want to use the module directly; if you want to include such a loop in your program you probably want to use the code module instead. There are two parts to this job:  Being able to tell if a line of input completes a Python statement: in short, telling whether to print \u2018>>>\u2019 or \u2018...\u2019 next. Remembering which future statements the user has entered, so subsequent input can be compiled with these in effect.  The codeop module provides a way of doing each of these things, and a way of doing them both. To do just the former:  \ncodeop.compile_command(source, filename=\"<input>\", symbol=\"single\")  \nTries to compile source, which should be a string of Python code and return a code object if source is valid Python code. In that case, the filename attribute of the code object will be filename, which defaults to '<input>'. Returns None if source is not valid Python code, but is a prefix of valid Python code. If there is a problem with source, an exception will be raised. SyntaxError is raised if there is invalid Python syntax, and OverflowError or ValueError if there is an invalid literal. The symbol argument determines whether source is compiled as a statement ('single', the default), as a sequence of statements ('exec') or as an expression ('eval'). Any other value will cause ValueError to be raised.  Note It is possible (but not likely) that the parser stops parsing with a successful outcome before reaching the end of the source; in this case, trailing symbols may be ignored instead of causing an error. For example, a backslash followed by two newlines may be followed by arbitrary garbage. This will be fixed once the API for the parser is better.  \n  \nclass codeop.Compile  \nInstances of this class have __call__() methods identical in signature to the built-in function compile(), but with the difference that if the instance compiles program text containing a __future__ statement, the instance \u2018remembers\u2019 and compiles all subsequent program texts with the statement in force. \n  \nclass codeop.CommandCompiler  \nInstances of this class have __call__() methods identical in signature to compile_command(); the difference is that if the instance compiles program text containing a __future__ statement, the instance \u2018remembers\u2019 and compiles all subsequent program texts with the statement in force. \n\n"}, {"name": "codeop.CommandCompiler", "path": "library/codeop#codeop.CommandCompiler", "type": "Interpreters", "text": " \nclass codeop.CommandCompiler  \nInstances of this class have __call__() methods identical in signature to compile_command(); the difference is that if the instance compiles program text containing a __future__ statement, the instance \u2018remembers\u2019 and compiles all subsequent program texts with the statement in force. \n"}, {"name": "codeop.Compile", "path": "library/codeop#codeop.Compile", "type": "Interpreters", "text": " \nclass codeop.Compile  \nInstances of this class have __call__() methods identical in signature to the built-in function compile(), but with the difference that if the instance compiles program text containing a __future__ statement, the instance \u2018remembers\u2019 and compiles all subsequent program texts with the statement in force. \n"}, {"name": "codeop.compile_command()", "path": "library/codeop#codeop.compile_command", "type": "Interpreters", "text": " \ncodeop.compile_command(source, filename=\"<input>\", symbol=\"single\")  \nTries to compile source, which should be a string of Python code and return a code object if source is valid Python code. In that case, the filename attribute of the code object will be filename, which defaults to '<input>'. Returns None if source is not valid Python code, but is a prefix of valid Python code. If there is a problem with source, an exception will be raised. SyntaxError is raised if there is invalid Python syntax, and OverflowError or ValueError if there is an invalid literal. The symbol argument determines whether source is compiled as a statement ('single', the default), as a sequence of statements ('exec') or as an expression ('eval'). Any other value will cause ValueError to be raised.  Note It is possible (but not likely) that the parser stops parsing with a successful outcome before reaching the end of the source; in this case, trailing symbols may be ignored instead of causing an error. For example, a backslash followed by two newlines may be followed by arbitrary garbage. This will be fixed once the API for the parser is better.  \n"}, {"name": "collections", "path": "library/collections", "type": "Data Types", "text": "collections \u2014 Container datatypes Source code: Lib/collections/__init__.py This module implements specialized container datatypes providing alternatives to Python\u2019s general purpose built-in containers, dict, list, set, and tuple.  \nnamedtuple() factory function for creating tuple subclasses with named fields  \ndeque list-like container with fast appends and pops on either end  \nChainMap dict-like class for creating a single view of multiple mappings  \nCounter dict subclass for counting hashable objects  \nOrderedDict dict subclass that remembers the order entries were added  \ndefaultdict dict subclass that calls a factory function to supply missing values  \nUserDict wrapper around dictionary objects for easier dict subclassing  \nUserList wrapper around list objects for easier list subclassing  \nUserString wrapper around string objects for easier string subclassing    Deprecated since version 3.3, will be removed in version 3.10: Moved Collections Abstract Base Classes to the collections.abc module. For backwards compatibility, they continue to be visible in this module through Python 3.9.  ChainMap objects  New in version 3.3.  A ChainMap class is provided for quickly linking a number of mappings so they can be treated as a single unit. It is often much faster than creating a new dictionary and running multiple update() calls. The class can be used to simulate nested scopes and is useful in templating.  \nclass collections.ChainMap(*maps)  \nA ChainMap groups multiple dicts or other mappings together to create a single, updateable view. If no maps are specified, a single empty dictionary is provided so that a new chain always has at least one mapping. The underlying mappings are stored in a list. That list is public and can be accessed or updated using the maps attribute. There is no other state. Lookups search the underlying mappings successively until a key is found. In contrast, writes, updates, and deletions only operate on the first mapping. A ChainMap incorporates the underlying mappings by reference. So, if one of the underlying mappings gets updated, those changes will be reflected in ChainMap. All of the usual dictionary methods are supported. In addition, there is a maps attribute, a method for creating new subcontexts, and a property for accessing all but the first mapping:  \nmaps  \nA user updateable list of mappings. The list is ordered from first-searched to last-searched. It is the only stored state and can be modified to change which mappings are searched. The list should always contain at least one mapping. \n  \nnew_child(m=None)  \nReturns a new ChainMap containing a new map followed by all of the maps in the current instance. If m is specified, it becomes the new map at the front of the list of mappings; if not specified, an empty dict is used, so that a call to d.new_child() is equivalent to: ChainMap({}, *d.maps). This method is used for creating subcontexts that can be updated without altering values in any of the parent mappings.  Changed in version 3.4: The optional m parameter was added.  \n  \nparents  \nProperty returning a new ChainMap containing all of the maps in the current instance except the first one. This is useful for skipping the first map in the search. Use cases are similar to those for the nonlocal keyword used in nested scopes. The use cases also parallel those for the built-in super() function. A reference to d.parents is equivalent to: ChainMap(*d.maps[1:]). \n Note, the iteration order of a ChainMap() is determined by scanning the mappings last to first: >>> baseline = {'music': 'bach', 'art': 'rembrandt'}\n>>> adjustments = {'art': 'van gogh', 'opera': 'carmen'}\n>>> list(ChainMap(adjustments, baseline))\n['music', 'art', 'opera']\n This gives the same ordering as a series of dict.update() calls starting with the last mapping: >>> combined = baseline.copy()\n>>> combined.update(adjustments)\n>>> list(combined)\n['music', 'art', 'opera']\n  Changed in version 3.9: Added support for | and |= operators, specified in PEP 584.  \n  See also  The MultiContext class in the Enthought CodeTools package has options to support writing to any mapping in the chain. Django\u2019s Context class for templating is a read-only chain of mappings. It also features pushing and popping of contexts similar to the new_child() method and the parents property. The Nested Contexts recipe has options to control whether writes and other mutations apply only to the first mapping or to any mapping in the chain. A greatly simplified read-only version of Chainmap.   \nChainMap Examples and Recipes This section shows various approaches to working with chained maps. Example of simulating Python\u2019s internal lookup chain: import builtins\npylookup = ChainMap(locals(), globals(), vars(builtins))\n Example of letting user specified command-line arguments take precedence over environment variables which in turn take precedence over default values: import os, argparse\n\ndefaults = {'color': 'red', 'user': 'guest'}\n\nparser = argparse.ArgumentParser()\nparser.add_argument('-u', '--user')\nparser.add_argument('-c', '--color')\nnamespace = parser.parse_args()\ncommand_line_args = {k: v for k, v in vars(namespace).items() if v is not None}\n\ncombined = ChainMap(command_line_args, os.environ, defaults)\nprint(combined['color'])\nprint(combined['user'])\n Example patterns for using the ChainMap class to simulate nested contexts: c = ChainMap()        # Create root context\nd = c.new_child()     # Create nested child context\ne = c.new_child()     # Child of c, independent from d\ne.maps[0]             # Current context dictionary -- like Python's locals()\ne.maps[-1]            # Root context -- like Python's globals()\ne.parents             # Enclosing context chain -- like Python's nonlocals\n\nd['x'] = 1            # Set value in current context\nd['x']                # Get first key in the chain of contexts\ndel d['x']            # Delete from current context\nlist(d)               # All nested values\nk in d                # Check all nested values\nlen(d)                # Number of nested values\nd.items()             # All nested items\ndict(d)               # Flatten into a regular dictionary\n The ChainMap class only makes updates (writes and deletions) to the first mapping in the chain while lookups will search the full chain. However, if deep writes and deletions are desired, it is easy to make a subclass that updates keys found deeper in the chain: class DeepChainMap(ChainMap):\n    'Variant of ChainMap that allows direct updates to inner scopes'\n\n    def __setitem__(self, key, value):\n        for mapping in self.maps:\n            if key in mapping:\n                mapping[key] = value\n                return\n        self.maps[0][key] = value\n\n    def __delitem__(self, key):\n        for mapping in self.maps:\n            if key in mapping:\n                del mapping[key]\n                return\n        raise KeyError(key)\n\n>>> d = DeepChainMap({'zebra': 'black'}, {'elephant': 'blue'}, {'lion': 'yellow'})\n>>> d['lion'] = 'orange'         # update an existing key two levels down\n>>> d['snake'] = 'red'           # new keys get added to the topmost dict\n>>> del d['elephant']            # remove an existing key one level down\n>>> d                            # display result\nDeepChainMap({'zebra': 'black', 'snake': 'red'}, {}, {'lion': 'orange'})\n Counter objects A counter tool is provided to support convenient and rapid tallies. For example: >>> # Tally occurrences of words in a list\n>>> cnt = Counter()\n>>> for word in ['red', 'blue', 'red', 'green', 'blue', 'blue']:\n...     cnt[word] += 1\n>>> cnt\nCounter({'blue': 3, 'red': 2, 'green': 1})\n\n>>> # Find the ten most common words in Hamlet\n>>> import re\n>>> words = re.findall(r'\\w+', open('hamlet.txt').read().lower())\n>>> Counter(words).most_common(10)\n[('the', 1143), ('and', 966), ('to', 762), ('of', 669), ('i', 631),\n ('you', 554),  ('a', 546), ('my', 514), ('hamlet', 471), ('in', 451)]\n  \nclass collections.Counter([iterable-or-mapping])  \nA Counter is a dict subclass for counting hashable objects. It is a collection where elements are stored as dictionary keys and their counts are stored as dictionary values. Counts are allowed to be any integer value including zero or negative counts. The Counter class is similar to bags or multisets in other languages. Elements are counted from an iterable or initialized from another mapping (or counter): >>> c = Counter()                           # a new, empty counter\n>>> c = Counter('gallahad')                 # a new counter from an iterable\n>>> c = Counter({'red': 4, 'blue': 2})      # a new counter from a mapping\n>>> c = Counter(cats=4, dogs=8)             # a new counter from keyword args\n Counter objects have a dictionary interface except that they return a zero count for missing items instead of raising a KeyError: >>> c = Counter(['eggs', 'ham'])\n>>> c['bacon']                              # count of a missing element is zero\n0\n Setting a count to zero does not remove an element from a counter. Use del to remove it entirely: >>> c['sausage'] = 0                        # counter entry with a zero count\n>>> del c['sausage']                        # del actually removes the entry\n  New in version 3.1.   Changed in version 3.7: As a dict subclass, Counter Inherited the capability to remember insertion order. Math operations on Counter objects also preserve order. Results are ordered according to when an element is first encountered in the left operand and then by the order encountered in the right operand.  Counter objects support three methods beyond those available for all dictionaries:  \nelements()  \nReturn an iterator over elements repeating each as many times as its count. Elements are returned in the order first encountered. If an element\u2019s count is less than one, elements() will ignore it. >>> c = Counter(a=4, b=2, c=0, d=-2)\n>>> sorted(c.elements())\n['a', 'a', 'a', 'a', 'b', 'b']\n \n  \nmost_common([n])  \nReturn a list of the n most common elements and their counts from the most common to the least. If n is omitted or None, most_common() returns all elements in the counter. Elements with equal counts are ordered in the order first encountered: >>> Counter('abracadabra').most_common(3)\n[('a', 5), ('b', 2), ('r', 2)]\n \n  \nsubtract([iterable-or-mapping])  \nElements are subtracted from an iterable or from another mapping (or counter). Like dict.update() but subtracts counts instead of replacing them. Both inputs and outputs may be zero or negative. >>> c = Counter(a=4, b=2, c=0, d=-2)\n>>> d = Counter(a=1, b=2, c=3, d=4)\n>>> c.subtract(d)\n>>> c\nCounter({'a': 3, 'b': 0, 'c': -3, 'd': -6})\n  New in version 3.2.  \n The usual dictionary methods are available for Counter objects except for two which work differently for counters.  \nfromkeys(iterable)  \nThis class method is not implemented for Counter objects. \n  \nupdate([iterable-or-mapping])  \nElements are counted from an iterable or added-in from another mapping (or counter). Like dict.update() but adds counts instead of replacing them. Also, the iterable is expected to be a sequence of elements, not a sequence of (key, value) pairs. \n \n Common patterns for working with Counter objects: sum(c.values())                 # total of all counts\nc.clear()                       # reset all counts\nlist(c)                         # list unique elements\nset(c)                          # convert to a set\ndict(c)                         # convert to a regular dictionary\nc.items()                       # convert to a list of (elem, cnt) pairs\nCounter(dict(list_of_pairs))    # convert from a list of (elem, cnt) pairs\nc.most_common()[:-n-1:-1]       # n least common elements\n+c                              # remove zero and negative counts\n Several mathematical operations are provided for combining Counter objects to produce multisets (counters that have counts greater than zero). Addition and subtraction combine counters by adding or subtracting the counts of corresponding elements. Intersection and union return the minimum and maximum of corresponding counts. Each operation can accept inputs with signed counts, but the output will exclude results with counts of zero or less. >>> c = Counter(a=3, b=1)\n>>> d = Counter(a=1, b=2)\n>>> c + d                       # add two counters together:  c[x] + d[x]\nCounter({'a': 4, 'b': 3})\n>>> c - d                       # subtract (keeping only positive counts)\nCounter({'a': 2})\n>>> c & d                       # intersection:  min(c[x], d[x]) \nCounter({'a': 1, 'b': 1})\n>>> c | d                       # union:  max(c[x], d[x])\nCounter({'a': 3, 'b': 2})\n Unary addition and subtraction are shortcuts for adding an empty counter or subtracting from an empty counter. >>> c = Counter(a=2, b=-4)\n>>> +c\nCounter({'a': 2})\n>>> -c\nCounter({'b': 4})\n  New in version 3.3: Added support for unary plus, unary minus, and in-place multiset operations.   Note Counters were primarily designed to work with positive integers to represent running counts; however, care was taken to not unnecessarily preclude use cases needing other types or negative values. To help with those use cases, this section documents the minimum range and type restrictions.  The Counter class itself is a dictionary subclass with no restrictions on its keys and values. The values are intended to be numbers representing counts, but you could store anything in the value field. The most_common() method requires only that the values be orderable. For in-place operations such as c[key] += 1, the value type need only support addition and subtraction. So fractions, floats, and decimals would work and negative values are supported. The same is also true for update() and subtract() which allow negative and zero values for both inputs and outputs. The multiset methods are designed only for use cases with positive values. The inputs may be negative or zero, but only outputs with positive values are created. There are no type restrictions, but the value type needs to support addition, subtraction, and comparison. The elements() method requires integer counts. It ignores zero and negative counts.    See also  \nBag class in Smalltalk. Wikipedia entry for Multisets. \nC++ multisets tutorial with examples. For mathematical operations on multisets and their use cases, see Knuth, Donald. The Art of Computer Programming Volume II, Section 4.6.3, Exercise 19. \nTo enumerate all distinct multisets of a given size over a given set of elements, see itertools.combinations_with_replacement(): map(Counter, combinations_with_replacement('ABC', 2)) # --> AA AB AC BB BC CC\n    deque objects  \nclass collections.deque([iterable[, maxlen]])  \nReturns a new deque object initialized left-to-right (using append()) with data from iterable. If iterable is not specified, the new deque is empty. Deques are a generalization of stacks and queues (the name is pronounced \u201cdeck\u201d and is short for \u201cdouble-ended queue\u201d). Deques support thread-safe, memory efficient appends and pops from either side of the deque with approximately the same O(1) performance in either direction. Though list objects support similar operations, they are optimized for fast fixed-length operations and incur O(n) memory movement costs for pop(0) and insert(0, v) operations which change both the size and position of the underlying data representation. If maxlen is not specified or is None, deques may grow to an arbitrary length. Otherwise, the deque is bounded to the specified maximum length. Once a bounded length deque is full, when new items are added, a corresponding number of items are discarded from the opposite end. Bounded length deques provide functionality similar to the tail filter in Unix. They are also useful for tracking transactions and other pools of data where only the most recent activity is of interest. Deque objects support the following methods:  \nappend(x)  \nAdd x to the right side of the deque. \n  \nappendleft(x)  \nAdd x to the left side of the deque. \n  \nclear()  \nRemove all elements from the deque leaving it with length 0. \n  \ncopy()  \nCreate a shallow copy of the deque.  New in version 3.5.  \n  \ncount(x)  \nCount the number of deque elements equal to x.  New in version 3.2.  \n  \nextend(iterable)  \nExtend the right side of the deque by appending elements from the iterable argument. \n  \nextendleft(iterable)  \nExtend the left side of the deque by appending elements from iterable. Note, the series of left appends results in reversing the order of elements in the iterable argument. \n  \nindex(x[, start[, stop]])  \nReturn the position of x in the deque (at or after index start and before index stop). Returns the first match or raises ValueError if not found.  New in version 3.5.  \n  \ninsert(i, x)  \nInsert x into the deque at position i. If the insertion would cause a bounded deque to grow beyond maxlen, an IndexError is raised.  New in version 3.5.  \n  \npop()  \nRemove and return an element from the right side of the deque. If no elements are present, raises an IndexError. \n  \npopleft()  \nRemove and return an element from the left side of the deque. If no elements are present, raises an IndexError. \n  \nremove(value)  \nRemove the first occurrence of value. If not found, raises a ValueError. \n  \nreverse()  \nReverse the elements of the deque in-place and then return None.  New in version 3.2.  \n  \nrotate(n=1)  \nRotate the deque n steps to the right. If n is negative, rotate to the left. When the deque is not empty, rotating one step to the right is equivalent to d.appendleft(d.pop()), and rotating one step to the left is equivalent to d.append(d.popleft()). \n Deque objects also provide one read-only attribute:  \nmaxlen  \nMaximum size of a deque or None if unbounded.  New in version 3.1.  \n \n In addition to the above, deques support iteration, pickling, len(d), reversed(d), copy.copy(d), copy.deepcopy(d), membership testing with the in operator, and subscript references such as d[0] to access the first element. Indexed access is O(1) at both ends but slows to O(n) in the middle. For fast random access, use lists instead. Starting in version 3.5, deques support __add__(), __mul__(), and __imul__(). Example: >>> from collections import deque\n>>> d = deque('ghi')                 # make a new deque with three items\n>>> for elem in d:                   # iterate over the deque's elements\n...     print(elem.upper())\nG\nH\nI\n\n>>> d.append('j')                    # add a new entry to the right side\n>>> d.appendleft('f')                # add a new entry to the left side\n>>> d                                # show the representation of the deque\ndeque(['f', 'g', 'h', 'i', 'j'])\n\n>>> d.pop()                          # return and remove the rightmost item\n'j'\n>>> d.popleft()                      # return and remove the leftmost item\n'f'\n>>> list(d)                          # list the contents of the deque\n['g', 'h', 'i']\n>>> d[0]                             # peek at leftmost item\n'g'\n>>> d[-1]                            # peek at rightmost item\n'i'\n\n>>> list(reversed(d))                # list the contents of a deque in reverse\n['i', 'h', 'g']\n>>> 'h' in d                         # search the deque\nTrue\n>>> d.extend('jkl')                  # add multiple elements at once\n>>> d\ndeque(['g', 'h', 'i', 'j', 'k', 'l'])\n>>> d.rotate(1)                      # right rotation\n>>> d\ndeque(['l', 'g', 'h', 'i', 'j', 'k'])\n>>> d.rotate(-1)                     # left rotation\n>>> d\ndeque(['g', 'h', 'i', 'j', 'k', 'l'])\n\n>>> deque(reversed(d))               # make a new deque in reverse order\ndeque(['l', 'k', 'j', 'i', 'h', 'g'])\n>>> d.clear()                        # empty the deque\n>>> d.pop()                          # cannot pop from an empty deque\nTraceback (most recent call last):\n    File \"<pyshell#6>\", line 1, in -toplevel-\n        d.pop()\nIndexError: pop from an empty deque\n\n>>> d.extendleft('abc')              # extendleft() reverses the input order\n>>> d\ndeque(['c', 'b', 'a'])\n \ndeque Recipes This section shows various approaches to working with deques. Bounded length deques provide functionality similar to the tail filter in Unix: def tail(filename, n=10):\n    'Return the last n lines of a file'\n    with open(filename) as f:\n        return deque(f, n)\n Another approach to using deques is to maintain a sequence of recently added elements by appending to the right and popping to the left: def moving_average(iterable, n=3):\n    # moving_average([40, 30, 50, 46, 39, 44]) --> 40.0 42.0 45.0 43.0\n    # http://en.wikipedia.org/wiki/Moving_average\n    it = iter(iterable)\n    d = deque(itertools.islice(it, n-1))\n    d.appendleft(0)\n    s = sum(d)\n    for elem in it:\n        s += elem - d.popleft()\n        d.append(elem)\n        yield s / n\n A round-robin scheduler can be implemented with input iterators stored in a deque. Values are yielded from the active iterator in position zero. If that iterator is exhausted, it can be removed with popleft(); otherwise, it can be cycled back to the end with the rotate() method: def roundrobin(*iterables):\n    \"roundrobin('ABC', 'D', 'EF') --> A D E B F C\"\n    iterators = deque(map(iter, iterables))\n    while iterators:\n        try:\n            while True:\n                yield next(iterators[0])\n                iterators.rotate(-1)\n        except StopIteration:\n            # Remove an exhausted iterator.\n            iterators.popleft()\n The rotate() method provides a way to implement deque slicing and deletion. For example, a pure Python implementation of del d[n] relies on the rotate() method to position elements to be popped: def delete_nth(d, n):\n    d.rotate(-n)\n    d.popleft()\n    d.rotate(n)\n To implement deque slicing, use a similar approach applying rotate() to bring a target element to the left side of the deque. Remove old entries with popleft(), add new entries with extend(), and then reverse the rotation. With minor variations on that approach, it is easy to implement Forth style stack manipulations such as dup, drop, swap, over, pick, rot, and roll. defaultdict objects  \nclass collections.defaultdict([default_factory[, ...]])  \nReturns a new dictionary-like object. defaultdict is a subclass of the built-in dict class. It overrides one method and adds one writable instance variable. The remaining functionality is the same as for the dict class and is not documented here. The first argument provides the initial value for the default_factory attribute; it defaults to None. All remaining arguments are treated the same as if they were passed to the dict constructor, including keyword arguments. defaultdict objects support the following method in addition to the standard dict operations:  \n__missing__(key)  \nIf the default_factory attribute is None, this raises a KeyError exception with the key as argument. If default_factory is not None, it is called without arguments to provide a default value for the given key, this value is inserted in the dictionary for the key, and returned. If calling default_factory raises an exception this exception is propagated unchanged. This method is called by the __getitem__() method of the dict class when the requested key is not found; whatever it returns or raises is then returned or raised by __getitem__(). Note that __missing__() is not called for any operations besides __getitem__(). This means that get() will, like normal dictionaries, return None as a default rather than using default_factory. \n defaultdict objects support the following instance variable:  \ndefault_factory  \nThis attribute is used by the __missing__() method; it is initialized from the first argument to the constructor, if present, or to None, if absent. \n  Changed in version 3.9: Added merge (|) and update (|=) operators, specified in PEP 584.  \n \ndefaultdict Examples Using list as the default_factory, it is easy to group a sequence of key-value pairs into a dictionary of lists: >>> s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)]\n>>> d = defaultdict(list)\n>>> for k, v in s:\n...     d[k].append(v)\n...\n>>> sorted(d.items())\n[('blue', [2, 4]), ('red', [1]), ('yellow', [1, 3])]\n When each key is encountered for the first time, it is not already in the mapping; so an entry is automatically created using the default_factory function which returns an empty list. The list.append() operation then attaches the value to the new list. When keys are encountered again, the look-up proceeds normally (returning the list for that key) and the list.append() operation adds another value to the list. This technique is simpler and faster than an equivalent technique using dict.setdefault(): >>> d = {}\n>>> for k, v in s:\n...     d.setdefault(k, []).append(v)\n...\n>>> sorted(d.items())\n[('blue', [2, 4]), ('red', [1]), ('yellow', [1, 3])]\n Setting the default_factory to int makes the defaultdict useful for counting (like a bag or multiset in other languages): >>> s = 'mississippi'\n>>> d = defaultdict(int)\n>>> for k in s:\n...     d[k] += 1\n...\n>>> sorted(d.items())\n[('i', 4), ('m', 1), ('p', 2), ('s', 4)]\n When a letter is first encountered, it is missing from the mapping, so the default_factory function calls int() to supply a default count of zero. The increment operation then builds up the count for each letter. The function int() which always returns zero is just a special case of constant functions. A faster and more flexible way to create constant functions is to use a lambda function which can supply any constant value (not just zero): >>> def constant_factory(value):\n...     return lambda: value\n>>> d = defaultdict(constant_factory('<missing>'))\n>>> d.update(name='John', action='ran')\n>>> '%(name)s %(action)s to %(object)s' % d\n'John ran to <missing>'\n Setting the default_factory to set makes the defaultdict useful for building a dictionary of sets: >>> s = [('red', 1), ('blue', 2), ('red', 3), ('blue', 4), ('red', 1), ('blue', 4)]\n>>> d = defaultdict(set)\n>>> for k, v in s:\n...     d[k].add(v)\n...\n>>> sorted(d.items())\n[('blue', {2, 4}), ('red', {1, 3})]\n namedtuple() Factory Function for Tuples with Named Fields Named tuples assign meaning to each position in a tuple and allow for more readable, self-documenting code. They can be used wherever regular tuples are used, and they add the ability to access fields by name instead of position index.  \ncollections.namedtuple(typename, field_names, *, rename=False, defaults=None, module=None)  \nReturns a new tuple subclass named typename. The new subclass is used to create tuple-like objects that have fields accessible by attribute lookup as well as being indexable and iterable. Instances of the subclass also have a helpful docstring (with typename and field_names) and a helpful __repr__() method which lists the tuple contents in a name=value format. The field_names are a sequence of strings such as ['x', 'y']. Alternatively, field_names can be a single string with each fieldname separated by whitespace and/or commas, for example 'x y' or 'x, y'. Any valid Python identifier may be used for a fieldname except for names starting with an underscore. Valid identifiers consist of letters, digits, and underscores but do not start with a digit or underscore and cannot be a keyword such as class, for, return, global, pass, or raise. If rename is true, invalid fieldnames are automatically replaced with positional names. For example, ['abc', 'def', 'ghi', 'abc'] is converted to ['abc', '_1', 'ghi', '_3'], eliminating the keyword def and the duplicate fieldname abc. defaults can be None or an iterable of default values. Since fields with a default value must come after any fields without a default, the defaults are applied to the rightmost parameters. For example, if the fieldnames are ['x', 'y', 'z'] and the defaults are (1, 2), then x will be a required argument, y will default to 1, and z will default to 2. If module is defined, the __module__ attribute of the named tuple is set to that value. Named tuple instances do not have per-instance dictionaries, so they are lightweight and require no more memory than regular tuples. To support pickling, the named tuple class should be assigned to a variable that matches typename.  Changed in version 3.1: Added support for rename.   Changed in version 3.6: The verbose and rename parameters became keyword-only arguments.   Changed in version 3.6: Added the module parameter.   Changed in version 3.7: Removed the verbose parameter and the _source attribute.   Changed in version 3.7: Added the defaults parameter and the _field_defaults attribute.  \n >>> # Basic example\n>>> Point = namedtuple('Point', ['x', 'y'])\n>>> p = Point(11, y=22)     # instantiate with positional or keyword arguments\n>>> p[0] + p[1]             # indexable like the plain tuple (11, 22)\n33\n>>> x, y = p                # unpack like a regular tuple\n>>> x, y\n(11, 22)\n>>> p.x + p.y               # fields also accessible by name\n33\n>>> p                       # readable __repr__ with a name=value style\nPoint(x=11, y=22)\n Named tuples are especially useful for assigning field names to result tuples returned by the csv or sqlite3 modules: EmployeeRecord = namedtuple('EmployeeRecord', 'name, age, title, department, paygrade')\n\nimport csv\nfor emp in map(EmployeeRecord._make, csv.reader(open(\"employees.csv\", \"rb\"))):\n    print(emp.name, emp.title)\n\nimport sqlite3\nconn = sqlite3.connect('/companydata')\ncursor = conn.cursor()\ncursor.execute('SELECT name, age, title, department, paygrade FROM employees')\nfor emp in map(EmployeeRecord._make, cursor.fetchall()):\n    print(emp.name, emp.title)\n In addition to the methods inherited from tuples, named tuples support three additional methods and two attributes. To prevent conflicts with field names, the method and attribute names start with an underscore.  \nclassmethod somenamedtuple._make(iterable)  \nClass method that makes a new instance from an existing sequence or iterable. >>> t = [11, 22]\n>>> Point._make(t)\nPoint(x=11, y=22)\n \n  \nsomenamedtuple._asdict()  \nReturn a new dict which maps field names to their corresponding values: >>> p = Point(x=11, y=22)\n>>> p._asdict()\n{'x': 11, 'y': 22}\n  Changed in version 3.1: Returns an OrderedDict instead of a regular dict.   Changed in version 3.8: Returns a regular dict instead of an OrderedDict. As of Python 3.7, regular dicts are guaranteed to be ordered. If the extra features of OrderedDict are required, the suggested remediation is to cast the result to the desired type: OrderedDict(nt._asdict()).  \n  \nsomenamedtuple._replace(**kwargs)  \nReturn a new instance of the named tuple replacing specified fields with new values: >>> p = Point(x=11, y=22)\n>>> p._replace(x=33)\nPoint(x=33, y=22)\n\n>>> for partnum, record in inventory.items():\n...     inventory[partnum] = record._replace(price=newprices[partnum], timestamp=time.now())\n \n  \nsomenamedtuple._fields  \nTuple of strings listing the field names. Useful for introspection and for creating new named tuple types from existing named tuples. >>> p._fields            # view the field names\n('x', 'y')\n\n>>> Color = namedtuple('Color', 'red green blue')\n>>> Pixel = namedtuple('Pixel', Point._fields + Color._fields)\n>>> Pixel(11, 22, 128, 255, 0)\nPixel(x=11, y=22, red=128, green=255, blue=0)\n \n  \nsomenamedtuple._field_defaults  \nDictionary mapping field names to default values. >>> Account = namedtuple('Account', ['type', 'balance'], defaults=[0])\n>>> Account._field_defaults\n{'balance': 0}\n>>> Account('premium')\nAccount(type='premium', balance=0)\n \n To retrieve a field whose name is stored in a string, use the getattr() function: >>> getattr(p, 'x')\n11\n To convert a dictionary to a named tuple, use the double-star-operator (as described in Unpacking Argument Lists): >>> d = {'x': 11, 'y': 22}\n>>> Point(**d)\nPoint(x=11, y=22)\n Since a named tuple is a regular Python class, it is easy to add or change functionality with a subclass. Here is how to add a calculated field and a fixed-width print format: >>> class Point(namedtuple('Point', ['x', 'y'])):\n...     __slots__ = ()\n...     @property\n...     def hypot(self):\n...         return (self.x ** 2 + self.y ** 2) ** 0.5\n...     def __str__(self):\n...         return 'Point: x=%6.3f  y=%6.3f  hypot=%6.3f' % (self.x, self.y, self.hypot)\n\n>>> for p in Point(3, 4), Point(14, 5/7):\n...     print(p)\nPoint: x= 3.000  y= 4.000  hypot= 5.000\nPoint: x=14.000  y= 0.714  hypot=14.018\n The subclass shown above sets __slots__ to an empty tuple. This helps keep memory requirements low by preventing the creation of instance dictionaries. Subclassing is not useful for adding new, stored fields. Instead, simply create a new named tuple type from the _fields attribute: >>> Point3D = namedtuple('Point3D', Point._fields + ('z',))\n Docstrings can be customized by making direct assignments to the __doc__ fields: >>> Book = namedtuple('Book', ['id', 'title', 'authors'])\n>>> Book.__doc__ += ': Hardcover book in active collection'\n>>> Book.id.__doc__ = '13-digit ISBN'\n>>> Book.title.__doc__ = 'Title of first printing'\n>>> Book.authors.__doc__ = 'List of authors sorted by last name'\n  Changed in version 3.5: Property docstrings became writeable.   See also  \nSee typing.NamedTuple for a way to add type hints for named tuples. It also provides an elegant notation using the class keyword: class Component(NamedTuple):\n    part_number: int\n    weight: float\n    description: Optional[str] = None\n  See types.SimpleNamespace() for a mutable namespace based on an underlying dictionary instead of a tuple. The dataclasses module provides a decorator and functions for automatically adding generated special methods to user-defined classes.   OrderedDict objects Ordered dictionaries are just like regular dictionaries but have some extra capabilities relating to ordering operations. They have become less important now that the built-in dict class gained the ability to remember insertion order (this new behavior became guaranteed in Python 3.7). Some differences from dict still remain:  The regular dict was designed to be very good at mapping operations. Tracking insertion order was secondary. The OrderedDict was designed to be good at reordering operations. Space efficiency, iteration speed, and the performance of update operations were secondary. Algorithmically, OrderedDict can handle frequent reordering operations better than dict. This makes it suitable for tracking recent accesses (for example in an LRU cache). The equality operation for OrderedDict checks for matching order. The popitem() method of OrderedDict has a different signature. It accepts an optional argument to specify which item is popped. \nOrderedDict has a move_to_end() method to efficiently reposition an element to an endpoint. Until Python 3.8, dict lacked a __reversed__() method.   \nclass collections.OrderedDict([items])  \nReturn an instance of a dict subclass that has methods specialized for rearranging dictionary order.  New in version 3.1.   \npopitem(last=True)  \nThe popitem() method for ordered dictionaries returns and removes a (key, value) pair. The pairs are returned in LIFO order if last is true or FIFO order if false. \n  \nmove_to_end(key, last=True)  \nMove an existing key to either end of an ordered dictionary. The item is moved to the right end if last is true (the default) or to the beginning if last is false. Raises KeyError if the key does not exist: >>> d = OrderedDict.fromkeys('abcde')\n>>> d.move_to_end('b')\n>>> ''.join(d.keys())\n'acdeb'\n>>> d.move_to_end('b', last=False)\n>>> ''.join(d.keys())\n'bacde'\n  New in version 3.2.  \n \n In addition to the usual mapping methods, ordered dictionaries also support reverse iteration using reversed(). Equality tests between OrderedDict objects are order-sensitive and are implemented as list(od1.items())==list(od2.items()). Equality tests between OrderedDict objects and other Mapping objects are order-insensitive like regular dictionaries. This allows OrderedDict objects to be substituted anywhere a regular dictionary is used.  Changed in version 3.5: The items, keys, and values views of OrderedDict now support reverse iteration using reversed().   Changed in version 3.6: With the acceptance of PEP 468, order is retained for keyword arguments passed to the OrderedDict constructor and its update() method.   Changed in version 3.9: Added merge (|) and update (|=) operators, specified in PEP 584.  \nOrderedDict Examples and Recipes It is straightforward to create an ordered dictionary variant that remembers the order the keys were last inserted. If a new entry overwrites an existing entry, the original insertion position is changed and moved to the end: class LastUpdatedOrderedDict(OrderedDict):\n    'Store items in the order the keys were last added'\n\n    def __setitem__(self, key, value):\n        super().__setitem__(key, value)\n        self.move_to_end(key)\n An OrderedDict would also be useful for implementing variants of functools.lru_cache(): class LRU(OrderedDict):\n    'Limit size, evicting the least recently looked-up key when full'\n\n    def __init__(self, maxsize=128, /, *args, **kwds):\n        self.maxsize = maxsize\n        super().__init__(*args, **kwds)\n\n    def __getitem__(self, key):\n        value = super().__getitem__(key)\n        self.move_to_end(key)\n        return value\n\n    def __setitem__(self, key, value):\n        if key in self:\n            self.move_to_end(key)\n        super().__setitem__(key, value)\n        if len(self) > self.maxsize:\n            oldest = next(iter(self))\n            del self[oldest]\n UserDict objects The class, UserDict acts as a wrapper around dictionary objects. The need for this class has been partially supplanted by the ability to subclass directly from dict; however, this class can be easier to work with because the underlying dictionary is accessible as an attribute.  \nclass collections.UserDict([initialdata])  \nClass that simulates a dictionary. The instance\u2019s contents are kept in a regular dictionary, which is accessible via the data attribute of UserDict instances. If initialdata is provided, data is initialized with its contents; note that a reference to initialdata will not be kept, allowing it be used for other purposes. In addition to supporting the methods and operations of mappings, UserDict instances provide the following attribute:  \ndata  \nA real dictionary used to store the contents of the UserDict class. \n \n UserList objects This class acts as a wrapper around list objects. It is a useful base class for your own list-like classes which can inherit from them and override existing methods or add new ones. In this way, one can add new behaviors to lists. The need for this class has been partially supplanted by the ability to subclass directly from list; however, this class can be easier to work with because the underlying list is accessible as an attribute.  \nclass collections.UserList([list])  \nClass that simulates a list. The instance\u2019s contents are kept in a regular list, which is accessible via the data attribute of UserList instances. The instance\u2019s contents are initially set to a copy of list, defaulting to the empty list []. list can be any iterable, for example a real Python list or a UserList object. In addition to supporting the methods and operations of mutable sequences, UserList instances provide the following attribute:  \ndata  \nA real list object used to store the contents of the UserList class. \n \n Subclassing requirements: Subclasses of UserList are expected to offer a constructor which can be called with either no arguments or one argument. List operations which return a new sequence attempt to create an instance of the actual implementation class. To do so, it assumes that the constructor can be called with a single parameter, which is a sequence object used as a data source. If a derived class does not wish to comply with this requirement, all of the special methods supported by this class will need to be overridden; please consult the sources for information about the methods which need to be provided in that case. UserString objects The class, UserString acts as a wrapper around string objects. The need for this class has been partially supplanted by the ability to subclass directly from str; however, this class can be easier to work with because the underlying string is accessible as an attribute.  \nclass collections.UserString(seq)  \nClass that simulates a string object. The instance\u2019s content is kept in a regular string object, which is accessible via the data attribute of UserString instances. The instance\u2019s contents are initially set to a copy of seq. The seq argument can be any object which can be converted into a string using the built-in str() function. In addition to supporting the methods and operations of strings, UserString instances provide the following attribute:  \ndata  \nA real str object used to store the contents of the UserString class. \n  Changed in version 3.5: New methods __getnewargs__, __rmod__, casefold, format_map, isprintable, and maketrans.  \n\n"}, {"name": "collections.abc", "path": "library/collections.abc", "type": "Data Types", "text": "collections.abc \u2014 Abstract Base Classes for Containers  New in version 3.3: Formerly, this module was part of the collections module.  Source code: Lib/_collections_abc.py This module provides abstract base classes that can be used to test whether a class provides a particular interface; for example, whether it is hashable or whether it is a mapping. Collections Abstract Base Classes The collections module offers the following ABCs:   \nABC Inherits from Abstract Methods Mixin Methods   \nContainer  __contains__   \nHashable  __hash__   \nIterable  __iter__   \nIterator Iterable __next__ __iter__  \nReversible Iterable __reversed__   \nGenerator Iterator send, throw close, __iter__, __next__  \nSized  __len__   \nCallable  __call__   \nCollection Sized, Iterable, Container __contains__, __iter__, __len__   \nSequence Reversible, Collection __getitem__, __len__ __contains__, __iter__, __reversed__, index, and count  \nMutableSequence Sequence __getitem__, __setitem__, __delitem__, __len__, insert Inherited Sequence methods and append, reverse, extend, pop, remove, and __iadd__  \nByteString Sequence __getitem__, __len__ Inherited Sequence methods  \nSet Collection __contains__, __iter__, __len__ __le__, __lt__, __eq__, __ne__, __gt__, __ge__, __and__, __or__, __sub__, __xor__, and isdisjoint  \nMutableSet Set __contains__, __iter__, __len__, add, discard Inherited Set methods and clear, pop, remove, __ior__, __iand__, __ixor__, and __isub__  \nMapping Collection __getitem__, __iter__, __len__ __contains__, keys, items, values, get, __eq__, and __ne__  \nMutableMapping Mapping __getitem__, __setitem__, __delitem__, __iter__, __len__ Inherited Mapping methods and pop, popitem, clear, update, and setdefault  \nMappingView Sized  __len__  \nItemsView MappingView, Set  __contains__, __iter__  \nKeysView MappingView, Set  __contains__, __iter__  \nValuesView MappingView, Collection  __contains__, __iter__  \nAwaitable  __await__   \nCoroutine Awaitable send, throw close  \nAsyncIterable  __aiter__   \nAsyncIterator AsyncIterable __anext__ __aiter__  \nAsyncGenerator AsyncIterator asend, athrow aclose, __aiter__, __anext__    \nclass collections.abc.Container  \nABC for classes that provide the __contains__() method. \n  \nclass collections.abc.Hashable  \nABC for classes that provide the __hash__() method. \n  \nclass collections.abc.Sized  \nABC for classes that provide the __len__() method. \n  \nclass collections.abc.Callable  \nABC for classes that provide the __call__() method. \n  \nclass collections.abc.Iterable  \nABC for classes that provide the __iter__() method. Checking isinstance(obj, Iterable) detects classes that are registered as Iterable or that have an __iter__() method, but it does not detect classes that iterate with the __getitem__() method. The only reliable way to determine whether an object is iterable is to call iter(obj). \n  \nclass collections.abc.Collection  \nABC for sized iterable container classes.  New in version 3.6.  \n  \nclass collections.abc.Iterator  \nABC for classes that provide the __iter__() and __next__() methods. See also the definition of iterator. \n  \nclass collections.abc.Reversible  \nABC for iterable classes that also provide the __reversed__() method.  New in version 3.6.  \n  \nclass collections.abc.Generator  \nABC for generator classes that implement the protocol defined in PEP 342 that extends iterators with the send(), throw() and close() methods. See also the definition of generator.  New in version 3.5.  \n  \nclass collections.abc.Sequence  \nclass collections.abc.MutableSequence  \nclass collections.abc.ByteString  \nABCs for read-only and mutable sequences. Implementation note: Some of the mixin methods, such as __iter__(), __reversed__() and index(), make repeated calls to the underlying __getitem__() method. Consequently, if __getitem__() is implemented with constant access speed, the mixin methods will have linear performance; however, if the underlying method is linear (as it would be with a linked list), the mixins will have quadratic performance and will likely need to be overridden.  Changed in version 3.5: The index() method added support for stop and start arguments.  \n  \nclass collections.abc.Set  \nclass collections.abc.MutableSet  \nABCs for read-only and mutable sets. \n  \nclass collections.abc.Mapping  \nclass collections.abc.MutableMapping  \nABCs for read-only and mutable mappings. \n  \nclass collections.abc.MappingView  \nclass collections.abc.ItemsView  \nclass collections.abc.KeysView  \nclass collections.abc.ValuesView  \nABCs for mapping, items, keys, and values views. \n  \nclass collections.abc.Awaitable  \nABC for awaitable objects, which can be used in await expressions. Custom implementations must provide the __await__() method. Coroutine objects and instances of the Coroutine ABC are all instances of this ABC.  Note In CPython, generator-based coroutines (generators decorated with types.coroutine() or asyncio.coroutine()) are awaitables, even though they do not have an __await__() method. Using isinstance(gencoro, Awaitable) for them will return False. Use inspect.isawaitable() to detect them.   New in version 3.5.  \n  \nclass collections.abc.Coroutine  \nABC for coroutine compatible classes. These implement the following methods, defined in Coroutine Objects: send(), throw(), and close(). Custom implementations must also implement __await__(). All Coroutine instances are also instances of Awaitable. See also the definition of coroutine.  Note In CPython, generator-based coroutines (generators decorated with types.coroutine() or asyncio.coroutine()) are awaitables, even though they do not have an __await__() method. Using isinstance(gencoro, Coroutine) for them will return False. Use inspect.isawaitable() to detect them.   New in version 3.5.  \n  \nclass collections.abc.AsyncIterable  \nABC for classes that provide __aiter__ method. See also the definition of asynchronous iterable.  New in version 3.5.  \n  \nclass collections.abc.AsyncIterator  \nABC for classes that provide __aiter__ and __anext__ methods. See also the definition of asynchronous iterator.  New in version 3.5.  \n  \nclass collections.abc.AsyncGenerator  \nABC for asynchronous generator classes that implement the protocol defined in PEP 525 and PEP 492.  New in version 3.6.  \n These ABCs allow us to ask classes or instances if they provide particular functionality, for example: size = None\nif isinstance(myvar, collections.abc.Sized):\n    size = len(myvar)\n Several of the ABCs are also useful as mixins that make it easier to develop classes supporting container APIs. For example, to write a class supporting the full Set API, it is only necessary to supply the three underlying abstract methods: __contains__(), __iter__(), and __len__(). The ABC supplies the remaining methods such as __and__() and isdisjoint(): class ListBasedSet(collections.abc.Set):\n    ''' Alternate set implementation favoring space over speed\n        and not requiring the set elements to be hashable. '''\n    def __init__(self, iterable):\n        self.elements = lst = []\n        for value in iterable:\n            if value not in lst:\n                lst.append(value)\n\n    def __iter__(self):\n        return iter(self.elements)\n\n    def __contains__(self, value):\n        return value in self.elements\n\n    def __len__(self):\n        return len(self.elements)\n\ns1 = ListBasedSet('abcdef')\ns2 = ListBasedSet('defghi')\noverlap = s1 & s2            # The __and__() method is supported automatically\n Notes on using Set and MutableSet as a mixin:  Since some set operations create new sets, the default mixin methods need a way to create new instances from an iterable. The class constructor is assumed to have a signature in the form ClassName(iterable). That assumption is factored-out to an internal classmethod called _from_iterable() which calls cls(iterable) to produce a new set. If the Set mixin is being used in a class with a different constructor signature, you will need to override _from_iterable() with a classmethod or regular method that can construct new instances from an iterable argument. To override the comparisons (presumably for speed, as the semantics are fixed), redefine __le__() and __ge__(), then the other operations will automatically follow suit. The Set mixin provides a _hash() method to compute a hash value for the set; however, __hash__() is not defined because not all sets are hashable or immutable. To add set hashability using mixins, inherit from both Set() and Hashable(), then define __hash__ = Set._hash.   See also  \nOrderedSet recipe for an example built on MutableSet. For more about ABCs, see the abc module and PEP 3119.  \n"}, {"name": "collections.abc.AsyncGenerator", "path": "library/collections.abc#collections.abc.AsyncGenerator", "type": "Data Types", "text": " \nclass collections.abc.AsyncGenerator  \nABC for asynchronous generator classes that implement the protocol defined in PEP 525 and PEP 492.  New in version 3.6.  \n"}, {"name": "collections.abc.AsyncIterable", "path": "library/collections.abc#collections.abc.AsyncIterable", "type": "Data Types", "text": " \nclass collections.abc.AsyncIterable  \nABC for classes that provide __aiter__ method. See also the definition of asynchronous iterable.  New in version 3.5.  \n"}, {"name": "collections.abc.AsyncIterator", "path": "library/collections.abc#collections.abc.AsyncIterator", "type": "Data Types", "text": " \nclass collections.abc.AsyncIterator  \nABC for classes that provide __aiter__ and __anext__ methods. See also the definition of asynchronous iterator.  New in version 3.5.  \n"}, {"name": "collections.abc.Awaitable", "path": "library/collections.abc#collections.abc.Awaitable", "type": "Data Types", "text": " \nclass collections.abc.Awaitable  \nABC for awaitable objects, which can be used in await expressions. Custom implementations must provide the __await__() method. Coroutine objects and instances of the Coroutine ABC are all instances of this ABC.  Note In CPython, generator-based coroutines (generators decorated with types.coroutine() or asyncio.coroutine()) are awaitables, even though they do not have an __await__() method. Using isinstance(gencoro, Awaitable) for them will return False. Use inspect.isawaitable() to detect them.   New in version 3.5.  \n"}, {"name": "collections.abc.ByteString", "path": "library/collections.abc#collections.abc.ByteString", "type": "Data Types", "text": " \nclass collections.abc.Sequence  \nclass collections.abc.MutableSequence  \nclass collections.abc.ByteString  \nABCs for read-only and mutable sequences. Implementation note: Some of the mixin methods, such as __iter__(), __reversed__() and index(), make repeated calls to the underlying __getitem__() method. Consequently, if __getitem__() is implemented with constant access speed, the mixin methods will have linear performance; however, if the underlying method is linear (as it would be with a linked list), the mixins will have quadratic performance and will likely need to be overridden.  Changed in version 3.5: The index() method added support for stop and start arguments.  \n"}, {"name": "collections.abc.Callable", "path": "library/collections.abc#collections.abc.Callable", "type": "Data Types", "text": " \nclass collections.abc.Callable  \nABC for classes that provide the __call__() method. \n"}, {"name": "collections.abc.Collection", "path": "library/collections.abc#collections.abc.Collection", "type": "Data Types", "text": " \nclass collections.abc.Collection  \nABC for sized iterable container classes.  New in version 3.6.  \n"}, {"name": "collections.abc.Container", "path": "library/collections.abc#collections.abc.Container", "type": "Data Types", "text": " \nclass collections.abc.Container  \nABC for classes that provide the __contains__() method. \n"}, {"name": "collections.abc.Coroutine", "path": "library/collections.abc#collections.abc.Coroutine", "type": "Data Types", "text": " \nclass collections.abc.Coroutine  \nABC for coroutine compatible classes. These implement the following methods, defined in Coroutine Objects: send(), throw(), and close(). Custom implementations must also implement __await__(). All Coroutine instances are also instances of Awaitable. See also the definition of coroutine.  Note In CPython, generator-based coroutines (generators decorated with types.coroutine() or asyncio.coroutine()) are awaitables, even though they do not have an __await__() method. Using isinstance(gencoro, Coroutine) for them will return False. Use inspect.isawaitable() to detect them.   New in version 3.5.  \n"}, {"name": "collections.abc.Generator", "path": "library/collections.abc#collections.abc.Generator", "type": "Data Types", "text": " \nclass collections.abc.Generator  \nABC for generator classes that implement the protocol defined in PEP 342 that extends iterators with the send(), throw() and close() methods. See also the definition of generator.  New in version 3.5.  \n"}, {"name": "collections.abc.Hashable", "path": "library/collections.abc#collections.abc.Hashable", "type": "Data Types", "text": " \nclass collections.abc.Hashable  \nABC for classes that provide the __hash__() method. \n"}, {"name": "collections.abc.ItemsView", "path": "library/collections.abc#collections.abc.ItemsView", "type": "Data Types", "text": " \nclass collections.abc.MappingView  \nclass collections.abc.ItemsView  \nclass collections.abc.KeysView  \nclass collections.abc.ValuesView  \nABCs for mapping, items, keys, and values views. \n"}, {"name": "collections.abc.Iterable", "path": "library/collections.abc#collections.abc.Iterable", "type": "Data Types", "text": " \nclass collections.abc.Iterable  \nABC for classes that provide the __iter__() method. Checking isinstance(obj, Iterable) detects classes that are registered as Iterable or that have an __iter__() method, but it does not detect classes that iterate with the __getitem__() method. The only reliable way to determine whether an object is iterable is to call iter(obj). \n"}, {"name": "collections.abc.Iterator", "path": "library/collections.abc#collections.abc.Iterator", "type": "Data Types", "text": " \nclass collections.abc.Iterator  \nABC for classes that provide the __iter__() and __next__() methods. See also the definition of iterator. \n"}, {"name": "collections.abc.KeysView", "path": "library/collections.abc#collections.abc.KeysView", "type": "Data Types", "text": " \nclass collections.abc.MappingView  \nclass collections.abc.ItemsView  \nclass collections.abc.KeysView  \nclass collections.abc.ValuesView  \nABCs for mapping, items, keys, and values views. \n"}, {"name": "collections.abc.Mapping", "path": "library/collections.abc#collections.abc.Mapping", "type": "Data Types", "text": " \nclass collections.abc.Mapping  \nclass collections.abc.MutableMapping  \nABCs for read-only and mutable mappings. \n"}, {"name": "collections.abc.MappingView", "path": "library/collections.abc#collections.abc.MappingView", "type": "Data Types", "text": " \nclass collections.abc.MappingView  \nclass collections.abc.ItemsView  \nclass collections.abc.KeysView  \nclass collections.abc.ValuesView  \nABCs for mapping, items, keys, and values views. \n"}, {"name": "collections.abc.MutableMapping", "path": "library/collections.abc#collections.abc.MutableMapping", "type": "Data Types", "text": " \nclass collections.abc.Mapping  \nclass collections.abc.MutableMapping  \nABCs for read-only and mutable mappings. \n"}, {"name": "collections.abc.MutableSequence", "path": "library/collections.abc#collections.abc.MutableSequence", "type": "Data Types", "text": " \nclass collections.abc.Sequence  \nclass collections.abc.MutableSequence  \nclass collections.abc.ByteString  \nABCs for read-only and mutable sequences. Implementation note: Some of the mixin methods, such as __iter__(), __reversed__() and index(), make repeated calls to the underlying __getitem__() method. Consequently, if __getitem__() is implemented with constant access speed, the mixin methods will have linear performance; however, if the underlying method is linear (as it would be with a linked list), the mixins will have quadratic performance and will likely need to be overridden.  Changed in version 3.5: The index() method added support for stop and start arguments.  \n"}, {"name": "collections.abc.MutableSet", "path": "library/collections.abc#collections.abc.MutableSet", "type": "Data Types", "text": " \nclass collections.abc.Set  \nclass collections.abc.MutableSet  \nABCs for read-only and mutable sets. \n"}, {"name": "collections.abc.Reversible", "path": "library/collections.abc#collections.abc.Reversible", "type": "Data Types", "text": " \nclass collections.abc.Reversible  \nABC for iterable classes that also provide the __reversed__() method.  New in version 3.6.  \n"}, {"name": "collections.abc.Sequence", "path": "library/collections.abc#collections.abc.Sequence", "type": "Data Types", "text": " \nclass collections.abc.Sequence  \nclass collections.abc.MutableSequence  \nclass collections.abc.ByteString  \nABCs for read-only and mutable sequences. Implementation note: Some of the mixin methods, such as __iter__(), __reversed__() and index(), make repeated calls to the underlying __getitem__() method. Consequently, if __getitem__() is implemented with constant access speed, the mixin methods will have linear performance; however, if the underlying method is linear (as it would be with a linked list), the mixins will have quadratic performance and will likely need to be overridden.  Changed in version 3.5: The index() method added support for stop and start arguments.  \n"}, {"name": "collections.abc.Set", "path": "library/collections.abc#collections.abc.Set", "type": "Data Types", "text": " \nclass collections.abc.Set  \nclass collections.abc.MutableSet  \nABCs for read-only and mutable sets. \n"}, {"name": "collections.abc.Sized", "path": "library/collections.abc#collections.abc.Sized", "type": "Data Types", "text": " \nclass collections.abc.Sized  \nABC for classes that provide the __len__() method. \n"}, {"name": "collections.abc.ValuesView", "path": "library/collections.abc#collections.abc.ValuesView", "type": "Data Types", "text": " \nclass collections.abc.MappingView  \nclass collections.abc.ItemsView  \nclass collections.abc.KeysView  \nclass collections.abc.ValuesView  \nABCs for mapping, items, keys, and values views. \n"}, {"name": "collections.ChainMap", "path": "library/collections#collections.ChainMap", "type": "Data Types", "text": " \nclass collections.ChainMap(*maps)  \nA ChainMap groups multiple dicts or other mappings together to create a single, updateable view. If no maps are specified, a single empty dictionary is provided so that a new chain always has at least one mapping. The underlying mappings are stored in a list. That list is public and can be accessed or updated using the maps attribute. There is no other state. Lookups search the underlying mappings successively until a key is found. In contrast, writes, updates, and deletions only operate on the first mapping. A ChainMap incorporates the underlying mappings by reference. So, if one of the underlying mappings gets updated, those changes will be reflected in ChainMap. All of the usual dictionary methods are supported. In addition, there is a maps attribute, a method for creating new subcontexts, and a property for accessing all but the first mapping:  \nmaps  \nA user updateable list of mappings. The list is ordered from first-searched to last-searched. It is the only stored state and can be modified to change which mappings are searched. The list should always contain at least one mapping. \n  \nnew_child(m=None)  \nReturns a new ChainMap containing a new map followed by all of the maps in the current instance. If m is specified, it becomes the new map at the front of the list of mappings; if not specified, an empty dict is used, so that a call to d.new_child() is equivalent to: ChainMap({}, *d.maps). This method is used for creating subcontexts that can be updated without altering values in any of the parent mappings.  Changed in version 3.4: The optional m parameter was added.  \n  \nparents  \nProperty returning a new ChainMap containing all of the maps in the current instance except the first one. This is useful for skipping the first map in the search. Use cases are similar to those for the nonlocal keyword used in nested scopes. The use cases also parallel those for the built-in super() function. A reference to d.parents is equivalent to: ChainMap(*d.maps[1:]). \n Note, the iteration order of a ChainMap() is determined by scanning the mappings last to first: >>> baseline = {'music': 'bach', 'art': 'rembrandt'}\n>>> adjustments = {'art': 'van gogh', 'opera': 'carmen'}\n>>> list(ChainMap(adjustments, baseline))\n['music', 'art', 'opera']\n This gives the same ordering as a series of dict.update() calls starting with the last mapping: >>> combined = baseline.copy()\n>>> combined.update(adjustments)\n>>> list(combined)\n['music', 'art', 'opera']\n  Changed in version 3.9: Added support for | and |= operators, specified in PEP 584.  \n"}, {"name": "collections.ChainMap.maps", "path": "library/collections#collections.ChainMap.maps", "type": "Data Types", "text": " \nmaps  \nA user updateable list of mappings. The list is ordered from first-searched to last-searched. It is the only stored state and can be modified to change which mappings are searched. The list should always contain at least one mapping. \n"}, {"name": "collections.ChainMap.new_child()", "path": "library/collections#collections.ChainMap.new_child", "type": "Data Types", "text": " \nnew_child(m=None)  \nReturns a new ChainMap containing a new map followed by all of the maps in the current instance. If m is specified, it becomes the new map at the front of the list of mappings; if not specified, an empty dict is used, so that a call to d.new_child() is equivalent to: ChainMap({}, *d.maps). This method is used for creating subcontexts that can be updated without altering values in any of the parent mappings.  Changed in version 3.4: The optional m parameter was added.  \n"}, {"name": "collections.ChainMap.parents", "path": "library/collections#collections.ChainMap.parents", "type": "Data Types", "text": " \nparents  \nProperty returning a new ChainMap containing all of the maps in the current instance except the first one. This is useful for skipping the first map in the search. Use cases are similar to those for the nonlocal keyword used in nested scopes. The use cases also parallel those for the built-in super() function. A reference to d.parents is equivalent to: ChainMap(*d.maps[1:]). \n"}, {"name": "collections.Counter", "path": "library/collections#collections.Counter", "type": "Data Types", "text": " \nclass collections.Counter([iterable-or-mapping])  \nA Counter is a dict subclass for counting hashable objects. It is a collection where elements are stored as dictionary keys and their counts are stored as dictionary values. Counts are allowed to be any integer value including zero or negative counts. The Counter class is similar to bags or multisets in other languages. Elements are counted from an iterable or initialized from another mapping (or counter): >>> c = Counter()                           # a new, empty counter\n>>> c = Counter('gallahad')                 # a new counter from an iterable\n>>> c = Counter({'red': 4, 'blue': 2})      # a new counter from a mapping\n>>> c = Counter(cats=4, dogs=8)             # a new counter from keyword args\n Counter objects have a dictionary interface except that they return a zero count for missing items instead of raising a KeyError: >>> c = Counter(['eggs', 'ham'])\n>>> c['bacon']                              # count of a missing element is zero\n0\n Setting a count to zero does not remove an element from a counter. Use del to remove it entirely: >>> c['sausage'] = 0                        # counter entry with a zero count\n>>> del c['sausage']                        # del actually removes the entry\n  New in version 3.1.   Changed in version 3.7: As a dict subclass, Counter Inherited the capability to remember insertion order. Math operations on Counter objects also preserve order. Results are ordered according to when an element is first encountered in the left operand and then by the order encountered in the right operand.  Counter objects support three methods beyond those available for all dictionaries:  \nelements()  \nReturn an iterator over elements repeating each as many times as its count. Elements are returned in the order first encountered. If an element\u2019s count is less than one, elements() will ignore it. >>> c = Counter(a=4, b=2, c=0, d=-2)\n>>> sorted(c.elements())\n['a', 'a', 'a', 'a', 'b', 'b']\n \n  \nmost_common([n])  \nReturn a list of the n most common elements and their counts from the most common to the least. If n is omitted or None, most_common() returns all elements in the counter. Elements with equal counts are ordered in the order first encountered: >>> Counter('abracadabra').most_common(3)\n[('a', 5), ('b', 2), ('r', 2)]\n \n  \nsubtract([iterable-or-mapping])  \nElements are subtracted from an iterable or from another mapping (or counter). Like dict.update() but subtracts counts instead of replacing them. Both inputs and outputs may be zero or negative. >>> c = Counter(a=4, b=2, c=0, d=-2)\n>>> d = Counter(a=1, b=2, c=3, d=4)\n>>> c.subtract(d)\n>>> c\nCounter({'a': 3, 'b': 0, 'c': -3, 'd': -6})\n  New in version 3.2.  \n The usual dictionary methods are available for Counter objects except for two which work differently for counters.  \nfromkeys(iterable)  \nThis class method is not implemented for Counter objects. \n  \nupdate([iterable-or-mapping])  \nElements are counted from an iterable or added-in from another mapping (or counter). Like dict.update() but adds counts instead of replacing them. Also, the iterable is expected to be a sequence of elements, not a sequence of (key, value) pairs. \n \n"}, {"name": "collections.Counter.elements()", "path": "library/collections#collections.Counter.elements", "type": "Data Types", "text": " \nelements()  \nReturn an iterator over elements repeating each as many times as its count. Elements are returned in the order first encountered. If an element\u2019s count is less than one, elements() will ignore it. >>> c = Counter(a=4, b=2, c=0, d=-2)\n>>> sorted(c.elements())\n['a', 'a', 'a', 'a', 'b', 'b']\n \n"}, {"name": "collections.Counter.fromkeys()", "path": "library/collections#collections.Counter.fromkeys", "type": "Data Types", "text": " \nfromkeys(iterable)  \nThis class method is not implemented for Counter objects. \n"}, {"name": "collections.Counter.most_common()", "path": "library/collections#collections.Counter.most_common", "type": "Data Types", "text": " \nmost_common([n])  \nReturn a list of the n most common elements and their counts from the most common to the least. If n is omitted or None, most_common() returns all elements in the counter. Elements with equal counts are ordered in the order first encountered: >>> Counter('abracadabra').most_common(3)\n[('a', 5), ('b', 2), ('r', 2)]\n \n"}, {"name": "collections.Counter.subtract()", "path": "library/collections#collections.Counter.subtract", "type": "Data Types", "text": " \nsubtract([iterable-or-mapping])  \nElements are subtracted from an iterable or from another mapping (or counter). Like dict.update() but subtracts counts instead of replacing them. Both inputs and outputs may be zero or negative. >>> c = Counter(a=4, b=2, c=0, d=-2)\n>>> d = Counter(a=1, b=2, c=3, d=4)\n>>> c.subtract(d)\n>>> c\nCounter({'a': 3, 'b': 0, 'c': -3, 'd': -6})\n  New in version 3.2.  \n"}, {"name": "collections.Counter.update()", "path": "library/collections#collections.Counter.update", "type": "Data Types", "text": " \nupdate([iterable-or-mapping])  \nElements are counted from an iterable or added-in from another mapping (or counter). Like dict.update() but adds counts instead of replacing them. Also, the iterable is expected to be a sequence of elements, not a sequence of (key, value) pairs. \n"}, {"name": "collections.defaultdict", "path": "library/collections#collections.defaultdict", "type": "Data Types", "text": " \nclass collections.defaultdict([default_factory[, ...]])  \nReturns a new dictionary-like object. defaultdict is a subclass of the built-in dict class. It overrides one method and adds one writable instance variable. The remaining functionality is the same as for the dict class and is not documented here. The first argument provides the initial value for the default_factory attribute; it defaults to None. All remaining arguments are treated the same as if they were passed to the dict constructor, including keyword arguments. defaultdict objects support the following method in addition to the standard dict operations:  \n__missing__(key)  \nIf the default_factory attribute is None, this raises a KeyError exception with the key as argument. If default_factory is not None, it is called without arguments to provide a default value for the given key, this value is inserted in the dictionary for the key, and returned. If calling default_factory raises an exception this exception is propagated unchanged. This method is called by the __getitem__() method of the dict class when the requested key is not found; whatever it returns or raises is then returned or raised by __getitem__(). Note that __missing__() is not called for any operations besides __getitem__(). This means that get() will, like normal dictionaries, return None as a default rather than using default_factory. \n defaultdict objects support the following instance variable:  \ndefault_factory  \nThis attribute is used by the __missing__() method; it is initialized from the first argument to the constructor, if present, or to None, if absent. \n  Changed in version 3.9: Added merge (|) and update (|=) operators, specified in PEP 584.  \n"}, {"name": "collections.defaultdict.default_factory", "path": "library/collections#collections.defaultdict.default_factory", "type": "Data Types", "text": " \ndefault_factory  \nThis attribute is used by the __missing__() method; it is initialized from the first argument to the constructor, if present, or to None, if absent. \n"}, {"name": "collections.defaultdict.__missing__()", "path": "library/collections#collections.defaultdict.__missing__", "type": "Data Types", "text": " \n__missing__(key)  \nIf the default_factory attribute is None, this raises a KeyError exception with the key as argument. If default_factory is not None, it is called without arguments to provide a default value for the given key, this value is inserted in the dictionary for the key, and returned. If calling default_factory raises an exception this exception is propagated unchanged. This method is called by the __getitem__() method of the dict class when the requested key is not found; whatever it returns or raises is then returned or raised by __getitem__(). Note that __missing__() is not called for any operations besides __getitem__(). This means that get() will, like normal dictionaries, return None as a default rather than using default_factory. \n"}, {"name": "collections.deque", "path": "library/collections#collections.deque", "type": "Data Types", "text": " \nclass collections.deque([iterable[, maxlen]])  \nReturns a new deque object initialized left-to-right (using append()) with data from iterable. If iterable is not specified, the new deque is empty. Deques are a generalization of stacks and queues (the name is pronounced \u201cdeck\u201d and is short for \u201cdouble-ended queue\u201d). Deques support thread-safe, memory efficient appends and pops from either side of the deque with approximately the same O(1) performance in either direction. Though list objects support similar operations, they are optimized for fast fixed-length operations and incur O(n) memory movement costs for pop(0) and insert(0, v) operations which change both the size and position of the underlying data representation. If maxlen is not specified or is None, deques may grow to an arbitrary length. Otherwise, the deque is bounded to the specified maximum length. Once a bounded length deque is full, when new items are added, a corresponding number of items are discarded from the opposite end. Bounded length deques provide functionality similar to the tail filter in Unix. They are also useful for tracking transactions and other pools of data where only the most recent activity is of interest. Deque objects support the following methods:  \nappend(x)  \nAdd x to the right side of the deque. \n  \nappendleft(x)  \nAdd x to the left side of the deque. \n  \nclear()  \nRemove all elements from the deque leaving it with length 0. \n  \ncopy()  \nCreate a shallow copy of the deque.  New in version 3.5.  \n  \ncount(x)  \nCount the number of deque elements equal to x.  New in version 3.2.  \n  \nextend(iterable)  \nExtend the right side of the deque by appending elements from the iterable argument. \n  \nextendleft(iterable)  \nExtend the left side of the deque by appending elements from iterable. Note, the series of left appends results in reversing the order of elements in the iterable argument. \n  \nindex(x[, start[, stop]])  \nReturn the position of x in the deque (at or after index start and before index stop). Returns the first match or raises ValueError if not found.  New in version 3.5.  \n  \ninsert(i, x)  \nInsert x into the deque at position i. If the insertion would cause a bounded deque to grow beyond maxlen, an IndexError is raised.  New in version 3.5.  \n  \npop()  \nRemove and return an element from the right side of the deque. If no elements are present, raises an IndexError. \n  \npopleft()  \nRemove and return an element from the left side of the deque. If no elements are present, raises an IndexError. \n  \nremove(value)  \nRemove the first occurrence of value. If not found, raises a ValueError. \n  \nreverse()  \nReverse the elements of the deque in-place and then return None.  New in version 3.2.  \n  \nrotate(n=1)  \nRotate the deque n steps to the right. If n is negative, rotate to the left. When the deque is not empty, rotating one step to the right is equivalent to d.appendleft(d.pop()), and rotating one step to the left is equivalent to d.append(d.popleft()). \n Deque objects also provide one read-only attribute:  \nmaxlen  \nMaximum size of a deque or None if unbounded.  New in version 3.1.  \n \n"}, {"name": "collections.deque.append()", "path": "library/collections#collections.deque.append", "type": "Data Types", "text": " \nappend(x)  \nAdd x to the right side of the deque. \n"}, {"name": "collections.deque.appendleft()", "path": "library/collections#collections.deque.appendleft", "type": "Data Types", "text": " \nappendleft(x)  \nAdd x to the left side of the deque. \n"}, {"name": "collections.deque.clear()", "path": "library/collections#collections.deque.clear", "type": "Data Types", "text": " \nclear()  \nRemove all elements from the deque leaving it with length 0. \n"}, {"name": "collections.deque.copy()", "path": "library/collections#collections.deque.copy", "type": "Data Types", "text": " \ncopy()  \nCreate a shallow copy of the deque.  New in version 3.5.  \n"}, {"name": "collections.deque.count()", "path": "library/collections#collections.deque.count", "type": "Data Types", "text": " \ncount(x)  \nCount the number of deque elements equal to x.  New in version 3.2.  \n"}, {"name": "collections.deque.extend()", "path": "library/collections#collections.deque.extend", "type": "Data Types", "text": " \nextend(iterable)  \nExtend the right side of the deque by appending elements from the iterable argument. \n"}, {"name": "collections.deque.extendleft()", "path": "library/collections#collections.deque.extendleft", "type": "Data Types", "text": " \nextendleft(iterable)  \nExtend the left side of the deque by appending elements from iterable. Note, the series of left appends results in reversing the order of elements in the iterable argument. \n"}, {"name": "collections.deque.index()", "path": "library/collections#collections.deque.index", "type": "Data Types", "text": " \nindex(x[, start[, stop]])  \nReturn the position of x in the deque (at or after index start and before index stop). Returns the first match or raises ValueError if not found.  New in version 3.5.  \n"}, {"name": "collections.deque.insert()", "path": "library/collections#collections.deque.insert", "type": "Data Types", "text": " \ninsert(i, x)  \nInsert x into the deque at position i. If the insertion would cause a bounded deque to grow beyond maxlen, an IndexError is raised.  New in version 3.5.  \n"}, {"name": "collections.deque.maxlen", "path": "library/collections#collections.deque.maxlen", "type": "Data Types", "text": " \nmaxlen  \nMaximum size of a deque or None if unbounded.  New in version 3.1.  \n"}, {"name": "collections.deque.pop()", "path": "library/collections#collections.deque.pop", "type": "Data Types", "text": " \npop()  \nRemove and return an element from the right side of the deque. If no elements are present, raises an IndexError. \n"}, {"name": "collections.deque.popleft()", "path": "library/collections#collections.deque.popleft", "type": "Data Types", "text": " \npopleft()  \nRemove and return an element from the left side of the deque. If no elements are present, raises an IndexError. \n"}, {"name": "collections.deque.remove()", "path": "library/collections#collections.deque.remove", "type": "Data Types", "text": " \nremove(value)  \nRemove the first occurrence of value. If not found, raises a ValueError. \n"}, {"name": "collections.deque.reverse()", "path": "library/collections#collections.deque.reverse", "type": "Data Types", "text": " \nreverse()  \nReverse the elements of the deque in-place and then return None.  New in version 3.2.  \n"}, {"name": "collections.deque.rotate()", "path": "library/collections#collections.deque.rotate", "type": "Data Types", "text": " \nrotate(n=1)  \nRotate the deque n steps to the right. If n is negative, rotate to the left. When the deque is not empty, rotating one step to the right is equivalent to d.appendleft(d.pop()), and rotating one step to the left is equivalent to d.append(d.popleft()). \n"}, {"name": "collections.namedtuple()", "path": "library/collections#collections.namedtuple", "type": "Data Types", "text": " \ncollections.namedtuple(typename, field_names, *, rename=False, defaults=None, module=None)  \nReturns a new tuple subclass named typename. The new subclass is used to create tuple-like objects that have fields accessible by attribute lookup as well as being indexable and iterable. Instances of the subclass also have a helpful docstring (with typename and field_names) and a helpful __repr__() method which lists the tuple contents in a name=value format. The field_names are a sequence of strings such as ['x', 'y']. Alternatively, field_names can be a single string with each fieldname separated by whitespace and/or commas, for example 'x y' or 'x, y'. Any valid Python identifier may be used for a fieldname except for names starting with an underscore. Valid identifiers consist of letters, digits, and underscores but do not start with a digit or underscore and cannot be a keyword such as class, for, return, global, pass, or raise. If rename is true, invalid fieldnames are automatically replaced with positional names. For example, ['abc', 'def', 'ghi', 'abc'] is converted to ['abc', '_1', 'ghi', '_3'], eliminating the keyword def and the duplicate fieldname abc. defaults can be None or an iterable of default values. Since fields with a default value must come after any fields without a default, the defaults are applied to the rightmost parameters. For example, if the fieldnames are ['x', 'y', 'z'] and the defaults are (1, 2), then x will be a required argument, y will default to 1, and z will default to 2. If module is defined, the __module__ attribute of the named tuple is set to that value. Named tuple instances do not have per-instance dictionaries, so they are lightweight and require no more memory than regular tuples. To support pickling, the named tuple class should be assigned to a variable that matches typename.  Changed in version 3.1: Added support for rename.   Changed in version 3.6: The verbose and rename parameters became keyword-only arguments.   Changed in version 3.6: Added the module parameter.   Changed in version 3.7: Removed the verbose parameter and the _source attribute.   Changed in version 3.7: Added the defaults parameter and the _field_defaults attribute.  \n"}, {"name": "collections.OrderedDict", "path": "library/collections#collections.OrderedDict", "type": "Data Types", "text": " \nclass collections.OrderedDict([items])  \nReturn an instance of a dict subclass that has methods specialized for rearranging dictionary order.  New in version 3.1.   \npopitem(last=True)  \nThe popitem() method for ordered dictionaries returns and removes a (key, value) pair. The pairs are returned in LIFO order if last is true or FIFO order if false. \n  \nmove_to_end(key, last=True)  \nMove an existing key to either end of an ordered dictionary. The item is moved to the right end if last is true (the default) or to the beginning if last is false. Raises KeyError if the key does not exist: >>> d = OrderedDict.fromkeys('abcde')\n>>> d.move_to_end('b')\n>>> ''.join(d.keys())\n'acdeb'\n>>> d.move_to_end('b', last=False)\n>>> ''.join(d.keys())\n'bacde'\n  New in version 3.2.  \n \n"}, {"name": "collections.OrderedDict.move_to_end()", "path": "library/collections#collections.OrderedDict.move_to_end", "type": "Data Types", "text": " \nmove_to_end(key, last=True)  \nMove an existing key to either end of an ordered dictionary. The item is moved to the right end if last is true (the default) or to the beginning if last is false. Raises KeyError if the key does not exist: >>> d = OrderedDict.fromkeys('abcde')\n>>> d.move_to_end('b')\n>>> ''.join(d.keys())\n'acdeb'\n>>> d.move_to_end('b', last=False)\n>>> ''.join(d.keys())\n'bacde'\n  New in version 3.2.  \n"}, {"name": "collections.OrderedDict.popitem()", "path": "library/collections#collections.OrderedDict.popitem", "type": "Data Types", "text": " \npopitem(last=True)  \nThe popitem() method for ordered dictionaries returns and removes a (key, value) pair. The pairs are returned in LIFO order if last is true or FIFO order if false. \n"}, {"name": "collections.somenamedtuple._asdict()", "path": "library/collections#collections.somenamedtuple._asdict", "type": "Data Types", "text": " \nsomenamedtuple._asdict()  \nReturn a new dict which maps field names to their corresponding values: >>> p = Point(x=11, y=22)\n>>> p._asdict()\n{'x': 11, 'y': 22}\n  Changed in version 3.1: Returns an OrderedDict instead of a regular dict.   Changed in version 3.8: Returns a regular dict instead of an OrderedDict. As of Python 3.7, regular dicts are guaranteed to be ordered. If the extra features of OrderedDict are required, the suggested remediation is to cast the result to the desired type: OrderedDict(nt._asdict()).  \n"}, {"name": "collections.somenamedtuple._fields", "path": "library/collections#collections.somenamedtuple._fields", "type": "Data Types", "text": " \nsomenamedtuple._fields  \nTuple of strings listing the field names. Useful for introspection and for creating new named tuple types from existing named tuples. >>> p._fields            # view the field names\n('x', 'y')\n\n>>> Color = namedtuple('Color', 'red green blue')\n>>> Pixel = namedtuple('Pixel', Point._fields + Color._fields)\n>>> Pixel(11, 22, 128, 255, 0)\nPixel(x=11, y=22, red=128, green=255, blue=0)\n \n"}, {"name": "collections.somenamedtuple._field_defaults", "path": "library/collections#collections.somenamedtuple._field_defaults", "type": "Data Types", "text": " \nsomenamedtuple._field_defaults  \nDictionary mapping field names to default values. >>> Account = namedtuple('Account', ['type', 'balance'], defaults=[0])\n>>> Account._field_defaults\n{'balance': 0}\n>>> Account('premium')\nAccount(type='premium', balance=0)\n \n"}, {"name": "collections.somenamedtuple._make()", "path": "library/collections#collections.somenamedtuple._make", "type": "Data Types", "text": " \nclassmethod somenamedtuple._make(iterable)  \nClass method that makes a new instance from an existing sequence or iterable. >>> t = [11, 22]\n>>> Point._make(t)\nPoint(x=11, y=22)\n \n"}, {"name": "collections.somenamedtuple._replace()", "path": "library/collections#collections.somenamedtuple._replace", "type": "Data Types", "text": " \nsomenamedtuple._replace(**kwargs)  \nReturn a new instance of the named tuple replacing specified fields with new values: >>> p = Point(x=11, y=22)\n>>> p._replace(x=33)\nPoint(x=33, y=22)\n\n>>> for partnum, record in inventory.items():\n...     inventory[partnum] = record._replace(price=newprices[partnum], timestamp=time.now())\n \n"}, {"name": "collections.UserDict", "path": "library/collections#collections.UserDict", "type": "Data Types", "text": " \nclass collections.UserDict([initialdata])  \nClass that simulates a dictionary. The instance\u2019s contents are kept in a regular dictionary, which is accessible via the data attribute of UserDict instances. If initialdata is provided, data is initialized with its contents; note that a reference to initialdata will not be kept, allowing it be used for other purposes. In addition to supporting the methods and operations of mappings, UserDict instances provide the following attribute:  \ndata  \nA real dictionary used to store the contents of the UserDict class. \n \n"}, {"name": "collections.UserDict.data", "path": "library/collections#collections.UserDict.data", "type": "Data Types", "text": " \ndata  \nA real dictionary used to store the contents of the UserDict class. \n"}, {"name": "collections.UserList", "path": "library/collections#collections.UserList", "type": "Data Types", "text": " \nclass collections.UserList([list])  \nClass that simulates a list. The instance\u2019s contents are kept in a regular list, which is accessible via the data attribute of UserList instances. The instance\u2019s contents are initially set to a copy of list, defaulting to the empty list []. list can be any iterable, for example a real Python list or a UserList object. In addition to supporting the methods and operations of mutable sequences, UserList instances provide the following attribute:  \ndata  \nA real list object used to store the contents of the UserList class. \n \n"}, {"name": "collections.UserList.data", "path": "library/collections#collections.UserList.data", "type": "Data Types", "text": " \ndata  \nA real list object used to store the contents of the UserList class. \n"}, {"name": "collections.UserString", "path": "library/collections#collections.UserString", "type": "Data Types", "text": " \nclass collections.UserString(seq)  \nClass that simulates a string object. The instance\u2019s content is kept in a regular string object, which is accessible via the data attribute of UserString instances. The instance\u2019s contents are initially set to a copy of seq. The seq argument can be any object which can be converted into a string using the built-in str() function. In addition to supporting the methods and operations of strings, UserString instances provide the following attribute:  \ndata  \nA real str object used to store the contents of the UserString class. \n  Changed in version 3.5: New methods __getnewargs__, __rmod__, casefold, format_map, isprintable, and maketrans.  \n"}, {"name": "collections.UserString.data", "path": "library/collections#collections.UserString.data", "type": "Data Types", "text": " \ndata  \nA real str object used to store the contents of the UserString class. \n"}, {"name": "colorsys", "path": "library/colorsys", "type": "Multimedia", "text": "colorsys \u2014 Conversions between color systems Source code: Lib/colorsys.py The colorsys module defines bidirectional conversions of color values between colors expressed in the RGB (Red Green Blue) color space used in computer monitors and three other coordinate systems: YIQ, HLS (Hue Lightness Saturation) and HSV (Hue Saturation Value). Coordinates in all of these color spaces are floating point values. In the YIQ space, the Y coordinate is between 0 and 1, but the I and Q coordinates can be positive or negative. In all other spaces, the coordinates are all between 0 and 1.  See also More information about color spaces can be found at https://poynton.ca/ColorFAQ.html and https://www.cambridgeincolour.com/tutorials/color-spaces.htm.  The colorsys module defines the following functions:  \ncolorsys.rgb_to_yiq(r, g, b)  \nConvert the color from RGB coordinates to YIQ coordinates. \n  \ncolorsys.yiq_to_rgb(y, i, q)  \nConvert the color from YIQ coordinates to RGB coordinates. \n  \ncolorsys.rgb_to_hls(r, g, b)  \nConvert the color from RGB coordinates to HLS coordinates. \n  \ncolorsys.hls_to_rgb(h, l, s)  \nConvert the color from HLS coordinates to RGB coordinates. \n  \ncolorsys.rgb_to_hsv(r, g, b)  \nConvert the color from RGB coordinates to HSV coordinates. \n  \ncolorsys.hsv_to_rgb(h, s, v)  \nConvert the color from HSV coordinates to RGB coordinates. \n Example: >>> import colorsys\n>>> colorsys.rgb_to_hsv(0.2, 0.4, 0.4)\n(0.5, 0.5, 0.4)\n>>> colorsys.hsv_to_rgb(0.5, 0.5, 0.4)\n(0.2, 0.4, 0.4)\n\n"}, {"name": "colorsys.hls_to_rgb()", "path": "library/colorsys#colorsys.hls_to_rgb", "type": "Multimedia", "text": " \ncolorsys.hls_to_rgb(h, l, s)  \nConvert the color from HLS coordinates to RGB coordinates. \n"}, {"name": "colorsys.hsv_to_rgb()", "path": "library/colorsys#colorsys.hsv_to_rgb", "type": "Multimedia", "text": " \ncolorsys.hsv_to_rgb(h, s, v)  \nConvert the color from HSV coordinates to RGB coordinates. \n"}, {"name": "colorsys.rgb_to_hls()", "path": "library/colorsys#colorsys.rgb_to_hls", "type": "Multimedia", "text": " \ncolorsys.rgb_to_hls(r, g, b)  \nConvert the color from RGB coordinates to HLS coordinates. \n"}, {"name": "colorsys.rgb_to_hsv()", "path": "library/colorsys#colorsys.rgb_to_hsv", "type": "Multimedia", "text": " \ncolorsys.rgb_to_hsv(r, g, b)  \nConvert the color from RGB coordinates to HSV coordinates. \n"}, {"name": "colorsys.rgb_to_yiq()", "path": "library/colorsys#colorsys.rgb_to_yiq", "type": "Multimedia", "text": " \ncolorsys.rgb_to_yiq(r, g, b)  \nConvert the color from RGB coordinates to YIQ coordinates. \n"}, {"name": "colorsys.yiq_to_rgb()", "path": "library/colorsys#colorsys.yiq_to_rgb", "type": "Multimedia", "text": " \ncolorsys.yiq_to_rgb(y, i, q)  \nConvert the color from YIQ coordinates to RGB coordinates. \n"}, {"name": "compile()", "path": "library/functions#compile", "type": "Built-in Functions", "text": " \ncompile(source, filename, mode, flags=0, dont_inherit=False, optimize=-1)  \nCompile the source into a code or AST object. Code objects can be executed by exec() or eval(). source can either be a normal string, a byte string, or an AST object. Refer to the ast module documentation for information on how to work with AST objects. The filename argument should give the file from which the code was read; pass some recognizable value if it wasn\u2019t read from a file ('<string>' is commonly used). The mode argument specifies what kind of code must be compiled; it can be 'exec' if source consists of a sequence of statements, 'eval' if it consists of a single expression, or 'single' if it consists of a single interactive statement (in the latter case, expression statements that evaluate to something other than None will be printed). The optional arguments flags and dont_inherit control which compiler options should be activated and which future features should be allowed. If neither is present (or both are zero) the code is compiled with the same flags that affect the code that is calling compile(). If the flags argument is given and dont_inherit is not (or is zero) then the compiler options and the future statements specified by the flags argument are used in addition to those that would be used anyway. If dont_inherit is a non-zero integer then the flags argument is it \u2013 the flags (future features and compiler options) in the surrounding code are ignored. Compiler options and future statements are specified by bits which can be bitwise ORed together to specify multiple options. The bitfield required to specify a given future feature can be found as the compiler_flag attribute on the _Feature instance in the __future__ module. Compiler flags can be found in ast module, with PyCF_ prefix. The argument optimize specifies the optimization level of the compiler; the default value of -1 selects the optimization level of the interpreter as given by -O options. Explicit levels are 0 (no optimization; __debug__ is true), 1 (asserts are removed, __debug__ is false) or 2 (docstrings are removed too). This function raises SyntaxError if the compiled source is invalid, and ValueError if the source contains null bytes. If you want to parse Python code into its AST representation, see ast.parse().\nRaises an auditing event compile with arguments source and filename. This event may also be raised by implicit compilation.  Note When compiling a string with multi-line code in 'single' or 'eval' mode, input must be terminated by at least one newline character. This is to facilitate detection of incomplete and complete statements in the code module.   Warning It is possible to crash the Python interpreter with a sufficiently large/complex string when compiling to an AST object due to stack depth limitations in Python\u2019s AST compiler.   Changed in version 3.2: Allowed use of Windows and Mac newlines. Also input in 'exec' mode does not have to end in a newline anymore. Added the optimize parameter.   Changed in version 3.5: Previously, TypeError was raised when null bytes were encountered in source.   New in version 3.8: ast.PyCF_ALLOW_TOP_LEVEL_AWAIT can now be passed in flags to enable support for top-level await, async for, and async with.  \n"}, {"name": "compileall", "path": "library/compileall", "type": "Language", "text": "compileall \u2014 Byte-compile Python libraries Source code: Lib/compileall.py This module provides some utility functions to support installing Python libraries. These functions compile Python source files in a directory tree. This module can be used to create the cached byte-code files at library installation time, which makes them available for use even by users who don\u2019t have write permission to the library directories. Command-line use This module can work as a script (using python -m compileall) to compile Python sources.  \ndirectory ...  \nfile ...  \nPositional arguments are files to compile or directories that contain source files, traversed recursively. If no argument is given, behave as if the command line was -l <directories from sys.path>. \n  \n-l  \nDo not recurse into subdirectories, only compile source code files directly contained in the named or implied directories. \n  \n-f  \nForce rebuild even if timestamps are up-to-date. \n  \n-q  \nDo not print the list of files compiled. If passed once, error messages will still be printed. If passed twice (-qq), all output is suppressed. \n  \n-d destdir  \nDirectory prepended to the path to each file being compiled. This will appear in compilation time tracebacks, and is also compiled in to the byte-code file, where it will be used in tracebacks and other messages in cases where the source file does not exist at the time the byte-code file is executed. \n  \n-s strip_prefix \n  \n-p prepend_prefix  \nRemove (-s) or append (-p) the given prefix of paths recorded in the .pyc files. Cannot be combined with -d. \n  \n-x regex  \nregex is used to search the full path to each file considered for compilation, and if the regex produces a match, the file is skipped. \n  \n-i list  \nRead the file list and add each line that it contains to the list of files and directories to compile. If list is -, read lines from stdin. \n  \n-b  \nWrite the byte-code files to their legacy locations and names, which may overwrite byte-code files created by another version of Python. The default is to write files to their PEP 3147 locations and names, which allows byte-code files from multiple versions of Python to coexist. \n  \n-r  \nControl the maximum recursion level for subdirectories. If this is given, then -l option will not be taken into account. python -m compileall <directory> -r 0 is equivalent to python -m compileall <directory> -l. \n  \n-j N  \nUse N workers to compile the files within the given directory. If 0 is used, then the result of os.cpu_count() will be used. \n  \n--invalidation-mode [timestamp|checked-hash|unchecked-hash]  \nControl how the generated byte-code files are invalidated at runtime. The timestamp value, means that .pyc files with the source timestamp and size embedded will be generated. The checked-hash and unchecked-hash values cause hash-based pycs to be generated. Hash-based pycs embed a hash of the source file contents rather than a timestamp. See Cached bytecode invalidation for more information on how Python validates bytecode cache files at runtime. The default is timestamp if the SOURCE_DATE_EPOCH environment variable is not set, and checked-hash if the SOURCE_DATE_EPOCH environment variable is set. \n  \n-o level  \nCompile with the given optimization level. May be used multiple times to compile for multiple levels at a time (for example, compileall -o 1 -o 2). \n  \n-e dir  \nIgnore symlinks pointing outside the given directory. \n  \n--hardlink-dupes  \nIf two .pyc files with different optimization level have the same content, use hard links to consolidate duplicate files. \n  Changed in version 3.2: Added the -i, -b and -h options.   Changed in version 3.5: Added the -j, -r, and -qq options. -q option was changed to a multilevel value. -b will always produce a byte-code file ending in .pyc, never .pyo.   Changed in version 3.7: Added the --invalidation-mode option.   Changed in version 3.9: Added the -s, -p, -e and --hardlink-dupes options. Raised the default recursion limit from 10 to sys.getrecursionlimit(). Added the possibility to specify the -o option multiple times.  There is no command-line option to control the optimization level used by the compile() function, because the Python interpreter itself already provides the option: python -O -m compileall. Similarly, the compile() function respects the sys.pycache_prefix setting. The generated bytecode cache will only be useful if compile() is run with the same sys.pycache_prefix (if any) that will be used at runtime. Public functions  \ncompileall.compile_dir(dir, maxlevels=sys.getrecursionlimit(), ddir=None, force=False, rx=None, quiet=0, legacy=False, optimize=-1, workers=1, invalidation_mode=None, *, stripdir=None, prependdir=None, limit_sl_dest=None, hardlink_dupes=False)  \nRecursively descend the directory tree named by dir, compiling all .py files along the way. Return a true value if all the files compiled successfully, and a false value otherwise. The maxlevels parameter is used to limit the depth of the recursion; it defaults to sys.getrecursionlimit(). If ddir is given, it is prepended to the path to each file being compiled for use in compilation time tracebacks, and is also compiled in to the byte-code file, where it will be used in tracebacks and other messages in cases where the source file does not exist at the time the byte-code file is executed. If force is true, modules are re-compiled even if the timestamps are up to date. If rx is given, its search method is called on the complete path to each file considered for compilation, and if it returns a true value, the file is skipped. If quiet is False or 0 (the default), the filenames and other information are printed to standard out. Set to 1, only errors are printed. Set to 2, all output is suppressed. If legacy is true, byte-code files are written to their legacy locations and names, which may overwrite byte-code files created by another version of Python. The default is to write files to their PEP 3147 locations and names, which allows byte-code files from multiple versions of Python to coexist. optimize specifies the optimization level for the compiler. It is passed to the built-in compile() function. Accepts also a sequence of optimization levels which lead to multiple compilations of one .py file in one call. The argument workers specifies how many workers are used to compile files in parallel. The default is to not use multiple workers. If the platform can\u2019t use multiple workers and workers argument is given, then sequential compilation will be used as a fallback. If workers is 0, the number of cores in the system is used. If workers is lower than 0, a ValueError will be raised. invalidation_mode should be a member of the py_compile.PycInvalidationMode enum and controls how the generated pycs are invalidated at runtime. The stripdir, prependdir and limit_sl_dest arguments correspond to the -s, -p and -e options described above. They may be specified as str, bytes or os.PathLike. If hardlink_dupes is true and two .pyc files with different optimization level have the same content, use hard links to consolidate duplicate files.  Changed in version 3.2: Added the legacy and optimize parameter.   Changed in version 3.5: Added the workers parameter.   Changed in version 3.5: quiet parameter was changed to a multilevel value.   Changed in version 3.5: The legacy parameter only writes out .pyc files, not .pyo files no matter what the value of optimize is.   Changed in version 3.6: Accepts a path-like object.   Changed in version 3.7: The invalidation_mode parameter was added.   Changed in version 3.7.2: The invalidation_mode parameter\u2019s default value is updated to None.   Changed in version 3.8: Setting workers to 0 now chooses the optimal number of cores.   Changed in version 3.9: Added stripdir, prependdir, limit_sl_dest and hardlink_dupes arguments. Default value of maxlevels was changed from 10 to sys.getrecursionlimit()  \n  \ncompileall.compile_file(fullname, ddir=None, force=False, rx=None, quiet=0, legacy=False, optimize=-1, invalidation_mode=None, *, stripdir=None, prependdir=None, limit_sl_dest=None, hardlink_dupes=False)  \nCompile the file with path fullname. Return a true value if the file compiled successfully, and a false value otherwise. If ddir is given, it is prepended to the path to the file being compiled for use in compilation time tracebacks, and is also compiled in to the byte-code file, where it will be used in tracebacks and other messages in cases where the source file does not exist at the time the byte-code file is executed. If rx is given, its search method is passed the full path name to the file being compiled, and if it returns a true value, the file is not compiled and True is returned. If quiet is False or 0 (the default), the filenames and other information are printed to standard out. Set to 1, only errors are printed. Set to 2, all output is suppressed. If legacy is true, byte-code files are written to their legacy locations and names, which may overwrite byte-code files created by another version of Python. The default is to write files to their PEP 3147 locations and names, which allows byte-code files from multiple versions of Python to coexist. optimize specifies the optimization level for the compiler. It is passed to the built-in compile() function. Accepts also a sequence of optimization levels which lead to multiple compilations of one .py file in one call. invalidation_mode should be a member of the py_compile.PycInvalidationMode enum and controls how the generated pycs are invalidated at runtime. The stripdir, prependdir and limit_sl_dest arguments correspond to the -s, -p and -e options described above. They may be specified as str, bytes or os.PathLike. If hardlink_dupes is true and two .pyc files with different optimization level have the same content, use hard links to consolidate duplicate files.  New in version 3.2.   Changed in version 3.5: quiet parameter was changed to a multilevel value.   Changed in version 3.5: The legacy parameter only writes out .pyc files, not .pyo files no matter what the value of optimize is.   Changed in version 3.7: The invalidation_mode parameter was added.   Changed in version 3.7.2: The invalidation_mode parameter\u2019s default value is updated to None.   Changed in version 3.9: Added stripdir, prependdir, limit_sl_dest and hardlink_dupes arguments.  \n  \ncompileall.compile_path(skip_curdir=True, maxlevels=0, force=False, quiet=0, legacy=False, optimize=-1, invalidation_mode=None)  \nByte-compile all the .py files found along sys.path. Return a true value if all the files compiled successfully, and a false value otherwise. If skip_curdir is true (the default), the current directory is not included in the search. All other parameters are passed to the compile_dir() function. Note that unlike the other compile functions, maxlevels defaults to 0.  Changed in version 3.2: Added the legacy and optimize parameter.   Changed in version 3.5: quiet parameter was changed to a multilevel value.   Changed in version 3.5: The legacy parameter only writes out .pyc files, not .pyo files no matter what the value of optimize is.   Changed in version 3.7: The invalidation_mode parameter was added.   Changed in version 3.7.2: The invalidation_mode parameter\u2019s default value is updated to None.  \n To force a recompile of all the .py files in the Lib/ subdirectory and all its subdirectories: import compileall\n\ncompileall.compile_dir('Lib/', force=True)\n\n# Perform same compilation, excluding files in .svn directories.\nimport re\ncompileall.compile_dir('Lib/', rx=re.compile(r'[/\\\\][.]svn'), force=True)\n\n# pathlib.Path objects can also be used.\nimport pathlib\ncompileall.compile_dir(pathlib.Path('Lib/'), force=True)\n  See also  \nModule py_compile\n\n\nByte-compile a single source file.   \n"}, {"name": "compileall.compile_dir()", "path": "library/compileall#compileall.compile_dir", "type": "Language", "text": " \ncompileall.compile_dir(dir, maxlevels=sys.getrecursionlimit(), ddir=None, force=False, rx=None, quiet=0, legacy=False, optimize=-1, workers=1, invalidation_mode=None, *, stripdir=None, prependdir=None, limit_sl_dest=None, hardlink_dupes=False)  \nRecursively descend the directory tree named by dir, compiling all .py files along the way. Return a true value if all the files compiled successfully, and a false value otherwise. The maxlevels parameter is used to limit the depth of the recursion; it defaults to sys.getrecursionlimit(). If ddir is given, it is prepended to the path to each file being compiled for use in compilation time tracebacks, and is also compiled in to the byte-code file, where it will be used in tracebacks and other messages in cases where the source file does not exist at the time the byte-code file is executed. If force is true, modules are re-compiled even if the timestamps are up to date. If rx is given, its search method is called on the complete path to each file considered for compilation, and if it returns a true value, the file is skipped. If quiet is False or 0 (the default), the filenames and other information are printed to standard out. Set to 1, only errors are printed. Set to 2, all output is suppressed. If legacy is true, byte-code files are written to their legacy locations and names, which may overwrite byte-code files created by another version of Python. The default is to write files to their PEP 3147 locations and names, which allows byte-code files from multiple versions of Python to coexist. optimize specifies the optimization level for the compiler. It is passed to the built-in compile() function. Accepts also a sequence of optimization levels which lead to multiple compilations of one .py file in one call. The argument workers specifies how many workers are used to compile files in parallel. The default is to not use multiple workers. If the platform can\u2019t use multiple workers and workers argument is given, then sequential compilation will be used as a fallback. If workers is 0, the number of cores in the system is used. If workers is lower than 0, a ValueError will be raised. invalidation_mode should be a member of the py_compile.PycInvalidationMode enum and controls how the generated pycs are invalidated at runtime. The stripdir, prependdir and limit_sl_dest arguments correspond to the -s, -p and -e options described above. They may be specified as str, bytes or os.PathLike. If hardlink_dupes is true and two .pyc files with different optimization level have the same content, use hard links to consolidate duplicate files.  Changed in version 3.2: Added the legacy and optimize parameter.   Changed in version 3.5: Added the workers parameter.   Changed in version 3.5: quiet parameter was changed to a multilevel value.   Changed in version 3.5: The legacy parameter only writes out .pyc files, not .pyo files no matter what the value of optimize is.   Changed in version 3.6: Accepts a path-like object.   Changed in version 3.7: The invalidation_mode parameter was added.   Changed in version 3.7.2: The invalidation_mode parameter\u2019s default value is updated to None.   Changed in version 3.8: Setting workers to 0 now chooses the optimal number of cores.   Changed in version 3.9: Added stripdir, prependdir, limit_sl_dest and hardlink_dupes arguments. Default value of maxlevels was changed from 10 to sys.getrecursionlimit()  \n"}, {"name": "compileall.compile_file()", "path": "library/compileall#compileall.compile_file", "type": "Language", "text": " \ncompileall.compile_file(fullname, ddir=None, force=False, rx=None, quiet=0, legacy=False, optimize=-1, invalidation_mode=None, *, stripdir=None, prependdir=None, limit_sl_dest=None, hardlink_dupes=False)  \nCompile the file with path fullname. Return a true value if the file compiled successfully, and a false value otherwise. If ddir is given, it is prepended to the path to the file being compiled for use in compilation time tracebacks, and is also compiled in to the byte-code file, where it will be used in tracebacks and other messages in cases where the source file does not exist at the time the byte-code file is executed. If rx is given, its search method is passed the full path name to the file being compiled, and if it returns a true value, the file is not compiled and True is returned. If quiet is False or 0 (the default), the filenames and other information are printed to standard out. Set to 1, only errors are printed. Set to 2, all output is suppressed. If legacy is true, byte-code files are written to their legacy locations and names, which may overwrite byte-code files created by another version of Python. The default is to write files to their PEP 3147 locations and names, which allows byte-code files from multiple versions of Python to coexist. optimize specifies the optimization level for the compiler. It is passed to the built-in compile() function. Accepts also a sequence of optimization levels which lead to multiple compilations of one .py file in one call. invalidation_mode should be a member of the py_compile.PycInvalidationMode enum and controls how the generated pycs are invalidated at runtime. The stripdir, prependdir and limit_sl_dest arguments correspond to the -s, -p and -e options described above. They may be specified as str, bytes or os.PathLike. If hardlink_dupes is true and two .pyc files with different optimization level have the same content, use hard links to consolidate duplicate files.  New in version 3.2.   Changed in version 3.5: quiet parameter was changed to a multilevel value.   Changed in version 3.5: The legacy parameter only writes out .pyc files, not .pyo files no matter what the value of optimize is.   Changed in version 3.7: The invalidation_mode parameter was added.   Changed in version 3.7.2: The invalidation_mode parameter\u2019s default value is updated to None.   Changed in version 3.9: Added stripdir, prependdir, limit_sl_dest and hardlink_dupes arguments.  \n"}, {"name": "compileall.compile_path()", "path": "library/compileall#compileall.compile_path", "type": "Language", "text": " \ncompileall.compile_path(skip_curdir=True, maxlevels=0, force=False, quiet=0, legacy=False, optimize=-1, invalidation_mode=None)  \nByte-compile all the .py files found along sys.path. Return a true value if all the files compiled successfully, and a false value otherwise. If skip_curdir is true (the default), the current directory is not included in the search. All other parameters are passed to the compile_dir() function. Note that unlike the other compile functions, maxlevels defaults to 0.  Changed in version 3.2: Added the legacy and optimize parameter.   Changed in version 3.5: quiet parameter was changed to a multilevel value.   Changed in version 3.5: The legacy parameter only writes out .pyc files, not .pyo files no matter what the value of optimize is.   Changed in version 3.7: The invalidation_mode parameter was added.   Changed in version 3.7.2: The invalidation_mode parameter\u2019s default value is updated to None.  \n"}, {"name": "complex", "path": "library/functions#complex", "type": "Built-in Functions", "text": " \nclass complex([real[, imag]])  \nReturn a complex number with the value real + imag*1j or convert a string or number to a complex number. If the first parameter is a string, it will be interpreted as a complex number and the function must be called without a second parameter. The second parameter can never be a string. Each argument may be any numeric type (including complex). If imag is omitted, it defaults to zero and the constructor serves as a numeric conversion like int and float. If both arguments are omitted, returns 0j. For a general Python object x, complex(x) delegates to x.__complex__(). If __complex__() is not defined then it falls back to __float__(). If __float__() is not defined then it falls back to __index__().  Note When converting from a string, the string must not contain whitespace around the central + or - operator. For example, complex('1+2j') is fine, but complex('1 + 2j') raises ValueError.  The complex type is described in Numeric Types \u2014 int, float, complex.  Changed in version 3.6: Grouping digits with underscores as in code literals is allowed.   Changed in version 3.8: Falls back to __index__() if __complex__() and __float__() are not defined.  \n"}, {"name": "concurrent.futures", "path": "library/concurrent.futures", "type": "Concurrent Execution", "text": "concurrent.futures \u2014 Launching parallel tasks  New in version 3.2.  Source code: Lib/concurrent/futures/thread.py and Lib/concurrent/futures/process.py The concurrent.futures module provides a high-level interface for asynchronously executing callables. The asynchronous execution can be performed with threads, using ThreadPoolExecutor, or separate processes, using ProcessPoolExecutor. Both implement the same interface, which is defined by the abstract Executor class. Executor Objects  \nclass concurrent.futures.Executor  \nAn abstract class that provides methods to execute calls asynchronously. It should not be used directly, but through its concrete subclasses.  \nsubmit(fn, /, *args, **kwargs)  \nSchedules the callable, fn, to be executed as fn(*args **kwargs) and returns a Future object representing the execution of the callable. with ThreadPoolExecutor(max_workers=1) as executor:\n    future = executor.submit(pow, 323, 1235)\n    print(future.result())\n \n  \nmap(func, *iterables, timeout=None, chunksize=1)  \nSimilar to map(func, *iterables) except:  the iterables are collected immediately rather than lazily; \nfunc is executed asynchronously and several calls to func may be made concurrently.  The returned iterator raises a concurrent.futures.TimeoutError if __next__() is called and the result isn\u2019t available after timeout seconds from the original call to Executor.map(). timeout can be an int or a float. If timeout is not specified or None, there is no limit to the wait time. If a func call raises an exception, then that exception will be raised when its value is retrieved from the iterator. When using ProcessPoolExecutor, this method chops iterables into a number of chunks which it submits to the pool as separate tasks. The (approximate) size of these chunks can be specified by setting chunksize to a positive integer. For very long iterables, using a large value for chunksize can significantly improve performance compared to the default size of 1. With ThreadPoolExecutor, chunksize has no effect.  Changed in version 3.5: Added the chunksize argument.  \n  \nshutdown(wait=True, *, cancel_futures=False)  \nSignal the executor that it should free any resources that it is using when the currently pending futures are done executing. Calls to Executor.submit() and Executor.map() made after shutdown will raise RuntimeError. If wait is True then this method will not return until all the pending futures are done executing and the resources associated with the executor have been freed. If wait is False then this method will return immediately and the resources associated with the executor will be freed when all pending futures are done executing. Regardless of the value of wait, the entire Python program will not exit until all pending futures are done executing. If cancel_futures is True, this method will cancel all pending futures that the executor has not started running. Any futures that are completed or running won\u2019t be cancelled, regardless of the value of cancel_futures. If both cancel_futures and wait are True, all futures that the executor has started running will be completed prior to this method returning. The remaining futures are cancelled. You can avoid having to call this method explicitly if you use the with statement, which will shutdown the Executor (waiting as if Executor.shutdown() were called with wait set to True): import shutil\nwith ThreadPoolExecutor(max_workers=4) as e:\n    e.submit(shutil.copy, 'src1.txt', 'dest1.txt')\n    e.submit(shutil.copy, 'src2.txt', 'dest2.txt')\n    e.submit(shutil.copy, 'src3.txt', 'dest3.txt')\n    e.submit(shutil.copy, 'src4.txt', 'dest4.txt')\n  Changed in version 3.9: Added cancel_futures.  \n \n ThreadPoolExecutor ThreadPoolExecutor is an Executor subclass that uses a pool of threads to execute calls asynchronously. Deadlocks can occur when the callable associated with a Future waits on the results of another Future. For example: import time\ndef wait_on_b():\n    time.sleep(5)\n    print(b.result())  # b will never complete because it is waiting on a.\n    return 5\n\ndef wait_on_a():\n    time.sleep(5)\n    print(a.result())  # a will never complete because it is waiting on b.\n    return 6\n\n\nexecutor = ThreadPoolExecutor(max_workers=2)\na = executor.submit(wait_on_b)\nb = executor.submit(wait_on_a)\n And: def wait_on_future():\n    f = executor.submit(pow, 5, 2)\n    # This will never complete because there is only one worker thread and\n    # it is executing this function.\n    print(f.result())\n\nexecutor = ThreadPoolExecutor(max_workers=1)\nexecutor.submit(wait_on_future)\n  \nclass concurrent.futures.ThreadPoolExecutor(max_workers=None, thread_name_prefix='', initializer=None, initargs=())  \nAn Executor subclass that uses a pool of at most max_workers threads to execute calls asynchronously. initializer is an optional callable that is called at the start of each worker thread; initargs is a tuple of arguments passed to the initializer. Should initializer raise an exception, all currently pending jobs will raise a BrokenThreadPool, as well as any attempt to submit more jobs to the pool.  Changed in version 3.5: If max_workers is None or not given, it will default to the number of processors on the machine, multiplied by 5, assuming that ThreadPoolExecutor is often used to overlap I/O instead of CPU work and the number of workers should be higher than the number of workers for ProcessPoolExecutor.   New in version 3.6: The thread_name_prefix argument was added to allow users to control the threading.Thread names for worker threads created by the pool for easier debugging.   Changed in version 3.7: Added the initializer and initargs arguments.   Changed in version 3.8: Default value of max_workers is changed to min(32, os.cpu_count() + 4). This default value preserves at least 5 workers for I/O bound tasks. It utilizes at most 32 CPU cores for CPU bound tasks which release the GIL. And it avoids using very large resources implicitly on many-core machines. ThreadPoolExecutor now reuses idle worker threads before starting max_workers worker threads too.  \n ThreadPoolExecutor Example import concurrent.futures\nimport urllib.request\n\nURLS = ['http://www.foxnews.com/',\n        'http://www.cnn.com/',\n        'http://europe.wsj.com/',\n        'http://www.bbc.co.uk/',\n        'http://some-made-up-domain.com/']\n\n# Retrieve a single page and report the URL and contents\ndef load_url(url, timeout):\n    with urllib.request.urlopen(url, timeout=timeout) as conn:\n        return conn.read()\n\n# We can use a with statement to ensure threads are cleaned up promptly\nwith concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n    # Start the load operations and mark each future with its URL\n    future_to_url = {executor.submit(load_url, url, 60): url for url in URLS}\n    for future in concurrent.futures.as_completed(future_to_url):\n        url = future_to_url[future]\n        try:\n            data = future.result()\n        except Exception as exc:\n            print('%r generated an exception: %s' % (url, exc))\n        else:\n            print('%r page is %d bytes' % (url, len(data)))\n ProcessPoolExecutor The ProcessPoolExecutor class is an Executor subclass that uses a pool of processes to execute calls asynchronously. ProcessPoolExecutor uses the multiprocessing module, which allows it to side-step the Global Interpreter Lock but also means that only picklable objects can be executed and returned. The __main__ module must be importable by worker subprocesses. This means that ProcessPoolExecutor will not work in the interactive interpreter. Calling Executor or Future methods from a callable submitted to a ProcessPoolExecutor will result in deadlock.  \nclass concurrent.futures.ProcessPoolExecutor(max_workers=None, mp_context=None, initializer=None, initargs=())  \nAn Executor subclass that executes calls asynchronously using a pool of at most max_workers processes. If max_workers is None or not given, it will default to the number of processors on the machine. If max_workers is less than or equal to 0, then a ValueError will be raised. On Windows, max_workers must be less than or equal to 61. If it is not then ValueError will be raised. If max_workers is None, then the default chosen will be at most 61, even if more processors are available. mp_context can be a multiprocessing context or None. It will be used to launch the workers. If mp_context is None or not given, the default multiprocessing context is used. initializer is an optional callable that is called at the start of each worker process; initargs is a tuple of arguments passed to the initializer. Should initializer raise an exception, all currently pending jobs will raise a BrokenProcessPool, as well as any attempt to submit more jobs to the pool.  Changed in version 3.3: When one of the worker processes terminates abruptly, a BrokenProcessPool error is now raised. Previously, behaviour was undefined but operations on the executor or its futures would often freeze or deadlock.   Changed in version 3.7: The mp_context argument was added to allow users to control the start_method for worker processes created by the pool. Added the initializer and initargs arguments.  \n ProcessPoolExecutor Example import concurrent.futures\nimport math\n\nPRIMES = [\n    112272535095293,\n    112582705942171,\n    112272535095293,\n    115280095190773,\n    115797848077099,\n    1099726899285419]\n\ndef is_prime(n):\n    if n < 2:\n        return False\n    if n == 2:\n        return True\n    if n % 2 == 0:\n        return False\n\n    sqrt_n = int(math.floor(math.sqrt(n)))\n    for i in range(3, sqrt_n + 1, 2):\n        if n % i == 0:\n            return False\n    return True\n\ndef main():\n    with concurrent.futures.ProcessPoolExecutor() as executor:\n        for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)):\n            print('%d is prime: %s' % (number, prime))\n\nif __name__ == '__main__':\n    main()\n Future Objects The Future class encapsulates the asynchronous execution of a callable. Future instances are created by Executor.submit().  \nclass concurrent.futures.Future  \nEncapsulates the asynchronous execution of a callable. Future instances are created by Executor.submit() and should not be created directly except for testing.  \ncancel()  \nAttempt to cancel the call. If the call is currently being executed or finished running and cannot be cancelled then the method will return False, otherwise the call will be cancelled and the method will return True. \n  \ncancelled()  \nReturn True if the call was successfully cancelled. \n  \nrunning()  \nReturn True if the call is currently being executed and cannot be cancelled. \n  \ndone()  \nReturn True if the call was successfully cancelled or finished running. \n  \nresult(timeout=None)  \nReturn the value returned by the call. If the call hasn\u2019t yet completed then this method will wait up to timeout seconds. If the call hasn\u2019t completed in timeout seconds, then a concurrent.futures.TimeoutError will be raised. timeout can be an int or float. If timeout is not specified or None, there is no limit to the wait time. If the future is cancelled before completing then CancelledError will be raised. If the call raised, this method will raise the same exception. \n  \nexception(timeout=None)  \nReturn the exception raised by the call. If the call hasn\u2019t yet completed then this method will wait up to timeout seconds. If the call hasn\u2019t completed in timeout seconds, then a concurrent.futures.TimeoutError will be raised. timeout can be an int or float. If timeout is not specified or None, there is no limit to the wait time. If the future is cancelled before completing then CancelledError will be raised. If the call completed without raising, None is returned. \n  \nadd_done_callback(fn)  \nAttaches the callable fn to the future. fn will be called, with the future as its only argument, when the future is cancelled or finishes running. Added callables are called in the order that they were added and are always called in a thread belonging to the process that added them. If the callable raises an Exception subclass, it will be logged and ignored. If the callable raises a BaseException subclass, the behavior is undefined. If the future has already completed or been cancelled, fn will be called immediately. \n The following Future methods are meant for use in unit tests and Executor implementations.  \nset_running_or_notify_can