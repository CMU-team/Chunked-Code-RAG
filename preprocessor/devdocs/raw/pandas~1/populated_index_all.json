[{"name": "10 minutes to pandas", "path": "user_guide/10min", "type": "Manual", "text": "10 minutes to pandas This is a short introduction to pandas, geared mainly for new users. You can see more complex recipes in the Cookbook. Customarily, we import as follows: \nIn [1]: import numpy as np\n\nIn [2]: import pandas as pd\n   Object creation See the Intro to data structures section. Creating a Series by passing a list of values, letting pandas create a default integer index: \nIn [3]: s = pd.Series([1, 3, 5, np.nan, 6, 8])\n\nIn [4]: s\nOut[4]: \n0    1.0\n1    3.0\n2    5.0\n3    NaN\n4    6.0\n5    8.0\ndtype: float64\n  Creating a DataFrame by passing a NumPy array, with a datetime index and labeled columns: \nIn [5]: dates = pd.date_range(\"20130101\", periods=6)\n\nIn [6]: dates\nOut[6]: \nDatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04',\n               '2013-01-05', '2013-01-06'],\n              dtype='datetime64[ns]', freq='D')\n\nIn [7]: df = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list(\"ABCD\"))\n\nIn [8]: df\nOut[8]: \n                   A         B         C         D\n2013-01-01  0.469112 -0.282863 -1.509059 -1.135632\n2013-01-02  1.212112 -0.173215  0.119209 -1.044236\n2013-01-03 -0.861849 -2.104569 -0.494929  1.071804\n2013-01-04  0.721555 -0.706771 -1.039575  0.271860\n2013-01-05 -0.424972  0.567020  0.276232 -1.087401\n2013-01-06 -0.673690  0.113648 -1.478427  0.524988\n  Creating a DataFrame by passing a dictionary of objects that can be converted into a series-like structure: \nIn [9]: df2 = pd.DataFrame(\n   ...:     {\n   ...:         \"A\": 1.0,\n   ...:         \"B\": pd.Timestamp(\"20130102\"),\n   ...:         \"C\": pd.Series(1, index=list(range(4)), dtype=\"float32\"),\n   ...:         \"D\": np.array([3] * 4, dtype=\"int32\"),\n   ...:         \"E\": pd.Categorical([\"test\", \"train\", \"test\", \"train\"]),\n   ...:         \"F\": \"foo\",\n   ...:     }\n   ...: )\n   ...: \n\nIn [10]: df2\nOut[10]: \n     A          B    C  D      E    F\n0  1.0 2013-01-02  1.0  3   test  foo\n1  1.0 2013-01-02  1.0  3  train  foo\n2  1.0 2013-01-02  1.0  3   test  foo\n3  1.0 2013-01-02  1.0  3  train  foo\n  The columns of the resulting DataFrame have different dtypes: \nIn [11]: df2.dtypes\nOut[11]: \nA           float64\nB    datetime64[ns]\nC           float32\nD             int32\nE          category\nF            object\ndtype: object\n  If you\u2019re using IPython, tab completion for column names (as well as public attributes) is automatically enabled. Here\u2019s a subset of the attributes that will be completed: \nIn [12]: df2.<TAB>  # noqa: E225, E999\ndf2.A                  df2.bool\ndf2.abs                df2.boxplot\ndf2.add                df2.C\ndf2.add_prefix         df2.clip\ndf2.add_suffix         df2.columns\ndf2.align              df2.copy\ndf2.all                df2.count\ndf2.any                df2.combine\ndf2.append             df2.D\ndf2.apply              df2.describe\ndf2.applymap           df2.diff\ndf2.B                  df2.duplicated\n  As you can see, the columns A, B, C, and D are automatically tab completed. E and F are there as well; the rest of the attributes have been truncated for brevity.   Viewing data See the Basics section. Here is how to view the top and bottom rows of the frame: \nIn [13]: df.head()\nOut[13]: \n                   A         B         C         D\n2013-01-01  0.469112 -0.282863 -1.509059 -1.135632\n2013-01-02  1.212112 -0.173215  0.119209 -1.044236\n2013-01-03 -0.861849 -2.104569 -0.494929  1.071804\n2013-01-04  0.721555 -0.706771 -1.039575  0.271860\n2013-01-05 -0.424972  0.567020  0.276232 -1.087401\n\nIn [14]: df.tail(3)\nOut[14]: \n                   A         B         C         D\n2013-01-04  0.721555 -0.706771 -1.039575  0.271860\n2013-01-05 -0.424972  0.567020  0.276232 -1.087401\n2013-01-06 -0.673690  0.113648 -1.478427  0.524988\n  Display the index, columns: \nIn [15]: df.index\nOut[15]: \nDatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04',\n               '2013-01-05', '2013-01-06'],\n              dtype='datetime64[ns]', freq='D')\n\nIn [16]: df.columns\nOut[16]: Index(['A', 'B', 'C', 'D'], dtype='object')\n  DataFrame.to_numpy() gives a NumPy representation of the underlying data. Note that this can be an expensive operation when your DataFrame has columns with different data types, which comes down to a fundamental difference between pandas and NumPy: NumPy arrays have one dtype for the entire array, while pandas DataFrames have one dtype per column. When you call DataFrame.to_numpy(), pandas will find the NumPy dtype that can hold all of the dtypes in the DataFrame. This may end up being object, which requires casting every value to a Python object. For df, our DataFrame of all floating-point values, DataFrame.to_numpy() is fast and doesn\u2019t require copying data: \nIn [17]: df.to_numpy()\nOut[17]: \narray([[ 0.4691, -0.2829, -1.5091, -1.1356],\n       [ 1.2121, -0.1732,  0.1192, -1.0442],\n       [-0.8618, -2.1046, -0.4949,  1.0718],\n       [ 0.7216, -0.7068, -1.0396,  0.2719],\n       [-0.425 ,  0.567 ,  0.2762, -1.0874],\n       [-0.6737,  0.1136, -1.4784,  0.525 ]])\n  For df2, the DataFrame with multiple dtypes, DataFrame.to_numpy() is relatively expensive: \nIn [18]: df2.to_numpy()\nOut[18]: \narray([[1.0, Timestamp('2013-01-02 00:00:00'), 1.0, 3, 'test', 'foo'],\n       [1.0, Timestamp('2013-01-02 00:00:00'), 1.0, 3, 'train', 'foo'],\n       [1.0, Timestamp('2013-01-02 00:00:00'), 1.0, 3, 'test', 'foo'],\n       [1.0, Timestamp('2013-01-02 00:00:00'), 1.0, 3, 'train', 'foo']],\n      dtype=object)\n   Note DataFrame.to_numpy() does not include the index or column labels in the output.  describe() shows a quick statistic summary of your data: \nIn [19]: df.describe()\nOut[19]: \n              A         B         C         D\ncount  6.000000  6.000000  6.000000  6.000000\nmean   0.073711 -0.431125 -0.687758 -0.233103\nstd    0.843157  0.922818  0.779887  0.973118\nmin   -0.861849 -2.104569 -1.509059 -1.135632\n25%   -0.611510 -0.600794 -1.368714 -1.076610\n50%    0.022070 -0.228039 -0.767252 -0.386188\n75%    0.658444  0.041933 -0.034326  0.461706\nmax    1.212112  0.567020  0.276232  1.071804\n  Transposing your data: \nIn [20]: df.T\nOut[20]: \n   2013-01-01  2013-01-02  2013-01-03  2013-01-04  2013-01-05  2013-01-06\nA    0.469112    1.212112   -0.861849    0.721555   -0.424972   -0.673690\nB   -0.282863   -0.173215   -2.104569   -0.706771    0.567020    0.113648\nC   -1.509059    0.119209   -0.494929   -1.039575    0.276232   -1.478427\nD   -1.135632   -1.044236    1.071804    0.271860   -1.087401    0.524988\n  Sorting by an axis: \nIn [21]: df.sort_index(axis=1, ascending=False)\nOut[21]: \n                   D         C         B         A\n2013-01-01 -1.135632 -1.509059 -0.282863  0.469112\n2013-01-02 -1.044236  0.119209 -0.173215  1.212112\n2013-01-03  1.071804 -0.494929 -2.104569 -0.861849\n2013-01-04  0.271860 -1.039575 -0.706771  0.721555\n2013-01-05 -1.087401  0.276232  0.567020 -0.424972\n2013-01-06  0.524988 -1.478427  0.113648 -0.673690\n  Sorting by values: \nIn [22]: df.sort_values(by=\"B\")\nOut[22]: \n                   A         B         C         D\n2013-01-03 -0.861849 -2.104569 -0.494929  1.071804\n2013-01-04  0.721555 -0.706771 -1.039575  0.271860\n2013-01-01  0.469112 -0.282863 -1.509059 -1.135632\n2013-01-02  1.212112 -0.173215  0.119209 -1.044236\n2013-01-06 -0.673690  0.113648 -1.478427  0.524988\n2013-01-05 -0.424972  0.567020  0.276232 -1.087401\n    Selection  Note While standard Python / NumPy expressions for selecting and setting are intuitive and come in handy for interactive work, for production code, we recommend the optimized pandas data access methods, .at, .iat, .loc and .iloc.  See the indexing documentation Indexing and Selecting Data and MultiIndex / Advanced Indexing.  Getting Selecting a single column, which yields a Series, equivalent to df.A: \nIn [23]: df[\"A\"]\nOut[23]: \n2013-01-01    0.469112\n2013-01-02    1.212112\n2013-01-03   -0.861849\n2013-01-04    0.721555\n2013-01-05   -0.424972\n2013-01-06   -0.673690\nFreq: D, Name: A, dtype: float64\n  Selecting via [], which slices the rows: \nIn [24]: df[0:3]\nOut[24]: \n                   A         B         C         D\n2013-01-01  0.469112 -0.282863 -1.509059 -1.135632\n2013-01-02  1.212112 -0.173215  0.119209 -1.044236\n2013-01-03 -0.861849 -2.104569 -0.494929  1.071804\n\nIn [25]: df[\"20130102\":\"20130104\"]\nOut[25]: \n                   A         B         C         D\n2013-01-02  1.212112 -0.173215  0.119209 -1.044236\n2013-01-03 -0.861849 -2.104569 -0.494929  1.071804\n2013-01-04  0.721555 -0.706771 -1.039575  0.271860\n    Selection by label See more in Selection by Label. For getting a cross section using a label: \nIn [26]: df.loc[dates[0]]\nOut[26]: \nA    0.469112\nB   -0.282863\nC   -1.509059\nD   -1.135632\nName: 2013-01-01 00:00:00, dtype: float64\n  Selecting on a multi-axis by label: \nIn [27]: df.loc[:, [\"A\", \"B\"]]\nOut[27]: \n                   A         B\n2013-01-01  0.469112 -0.282863\n2013-01-02  1.212112 -0.173215\n2013-01-03 -0.861849 -2.104569\n2013-01-04  0.721555 -0.706771\n2013-01-05 -0.424972  0.567020\n2013-01-06 -0.673690  0.113648\n  Showing label slicing, both endpoints are included: \nIn [28]: df.loc[\"20130102\":\"20130104\", [\"A\", \"B\"]]\nOut[28]: \n                   A         B\n2013-01-02  1.212112 -0.173215\n2013-01-03 -0.861849 -2.104569\n2013-01-04  0.721555 -0.706771\n  Reduction in the dimensions of the returned object: \nIn [29]: df.loc[\"20130102\", [\"A\", \"B\"]]\nOut[29]: \nA    1.212112\nB   -0.173215\nName: 2013-01-02 00:00:00, dtype: float64\n  For getting a scalar value: \nIn [30]: df.loc[dates[0], \"A\"]\nOut[30]: 0.4691122999071863\n  For getting fast access to a scalar (equivalent to the prior method): \nIn [31]: df.at[dates[0], \"A\"]\nOut[31]: 0.4691122999071863\n    Selection by position See more in Selection by Position. Select via the position of the passed integers: \nIn [32]: df.iloc[3]\nOut[32]: \nA    0.721555\nB   -0.706771\nC   -1.039575\nD    0.271860\nName: 2013-01-04 00:00:00, dtype: float64\n  By integer slices, acting similar to NumPy/Python: \nIn [33]: df.iloc[3:5, 0:2]\nOut[33]: \n                   A         B\n2013-01-04  0.721555 -0.706771\n2013-01-05 -0.424972  0.567020\n  By lists of integer position locations, similar to the NumPy/Python style: \nIn [34]: df.iloc[[1, 2, 4], [0, 2]]\nOut[34]: \n                   A         C\n2013-01-02  1.212112  0.119209\n2013-01-03 -0.861849 -0.494929\n2013-01-05 -0.424972  0.276232\n  For slicing rows explicitly: \nIn [35]: df.iloc[1:3, :]\nOut[35]: \n                   A         B         C         D\n2013-01-02  1.212112 -0.173215  0.119209 -1.044236\n2013-01-03 -0.861849 -2.104569 -0.494929  1.071804\n  For slicing columns explicitly: \nIn [36]: df.iloc[:, 1:3]\nOut[36]: \n                   B         C\n2013-01-01 -0.282863 -1.509059\n2013-01-02 -0.173215  0.119209\n2013-01-03 -2.104569 -0.494929\n2013-01-04 -0.706771 -1.039575\n2013-01-05  0.567020  0.276232\n2013-01-06  0.113648 -1.478427\n  For getting a value explicitly: \nIn [37]: df.iloc[1, 1]\nOut[37]: -0.17321464905330858\n  For getting fast access to a scalar (equivalent to the prior method): \nIn [38]: df.iat[1, 1]\nOut[38]: -0.17321464905330858\n    Boolean indexing Using a single column\u2019s values to select data: \nIn [39]: df[df[\"A\"] > 0]\nOut[39]: \n                   A         B         C         D\n2013-01-01  0.469112 -0.282863 -1.509059 -1.135632\n2013-01-02  1.212112 -0.173215  0.119209 -1.044236\n2013-01-04  0.721555 -0.706771 -1.039575  0.271860\n  Selecting values from a DataFrame where a boolean condition is met: \nIn [40]: df[df > 0]\nOut[40]: \n                   A         B         C         D\n2013-01-01  0.469112       NaN       NaN       NaN\n2013-01-02  1.212112       NaN  0.119209       NaN\n2013-01-03       NaN       NaN       NaN  1.071804\n2013-01-04  0.721555       NaN       NaN  0.271860\n2013-01-05       NaN  0.567020  0.276232       NaN\n2013-01-06       NaN  0.113648       NaN  0.524988\n  Using the isin() method for filtering: \nIn [41]: df2 = df.copy()\n\nIn [42]: df2[\"E\"] = [\"one\", \"one\", \"two\", \"three\", \"four\", \"three\"]\n\nIn [43]: df2\nOut[43]: \n                   A         B         C         D      E\n2013-01-01  0.469112 -0.282863 -1.509059 -1.135632    one\n2013-01-02  1.212112 -0.173215  0.119209 -1.044236    one\n2013-01-03 -0.861849 -2.104569 -0.494929  1.071804    two\n2013-01-04  0.721555 -0.706771 -1.039575  0.271860  three\n2013-01-05 -0.424972  0.567020  0.276232 -1.087401   four\n2013-01-06 -0.673690  0.113648 -1.478427  0.524988  three\n\nIn [44]: df2[df2[\"E\"].isin([\"two\", \"four\"])]\nOut[44]: \n                   A         B         C         D     E\n2013-01-03 -0.861849 -2.104569 -0.494929  1.071804   two\n2013-01-05 -0.424972  0.567020  0.276232 -1.087401  four\n    Setting Setting a new column automatically aligns the data by the indexes: \nIn [45]: s1 = pd.Series([1, 2, 3, 4, 5, 6], index=pd.date_range(\"20130102\", periods=6))\n\nIn [46]: s1\nOut[46]: \n2013-01-02    1\n2013-01-03    2\n2013-01-04    3\n2013-01-05    4\n2013-01-06    5\n2013-01-07    6\nFreq: D, dtype: int64\n\nIn [47]: df[\"F\"] = s1\n  Setting values by label: \nIn [48]: df.at[dates[0], \"A\"] = 0\n  Setting values by position: \nIn [49]: df.iat[0, 1] = 0\n  Setting by assigning with a NumPy array: \nIn [50]: df.loc[:, \"D\"] = np.array([5] * len(df))\n  The result of the prior setting operations: \nIn [51]: df\nOut[51]: \n                   A         B         C  D    F\n2013-01-01  0.000000  0.000000 -1.509059  5  NaN\n2013-01-02  1.212112 -0.173215  0.119209  5  1.0\n2013-01-03 -0.861849 -2.104569 -0.494929  5  2.0\n2013-01-04  0.721555 -0.706771 -1.039575  5  3.0\n2013-01-05 -0.424972  0.567020  0.276232  5  4.0\n2013-01-06 -0.673690  0.113648 -1.478427  5  5.0\n  A where operation with setting: \nIn [52]: df2 = df.copy()\n\nIn [53]: df2[df2 > 0] = -df2\n\nIn [54]: df2\nOut[54]: \n                   A         B         C  D    F\n2013-01-01  0.000000  0.000000 -1.509059 -5  NaN\n2013-01-02 -1.212112 -0.173215 -0.119209 -5 -1.0\n2013-01-03 -0.861849 -2.104569 -0.494929 -5 -2.0\n2013-01-04 -0.721555 -0.706771 -1.039575 -5 -3.0\n2013-01-05 -0.424972 -0.567020 -0.276232 -5 -4.0\n2013-01-06 -0.673690 -0.113648 -1.478427 -5 -5.0\n     Missing data pandas primarily uses the value np.nan to represent missing data. It is by default not included in computations. See the Missing Data section. Reindexing allows you to change/add/delete the index on a specified axis. This returns a copy of the data: \nIn [55]: df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + [\"E\"])\n\nIn [56]: df1.loc[dates[0] : dates[1], \"E\"] = 1\n\nIn [57]: df1\nOut[57]: \n                   A         B         C  D    F    E\n2013-01-01  0.000000  0.000000 -1.509059  5  NaN  1.0\n2013-01-02  1.212112 -0.173215  0.119209  5  1.0  1.0\n2013-01-03 -0.861849 -2.104569 -0.494929  5  2.0  NaN\n2013-01-04  0.721555 -0.706771 -1.039575  5  3.0  NaN\n  To drop any rows that have missing data: \nIn [58]: df1.dropna(how=\"any\")\nOut[58]: \n                   A         B         C  D    F    E\n2013-01-02  1.212112 -0.173215  0.119209  5  1.0  1.0\n  Filling missing data: \nIn [59]: df1.fillna(value=5)\nOut[59]: \n                   A         B         C  D    F    E\n2013-01-01  0.000000  0.000000 -1.509059  5  5.0  1.0\n2013-01-02  1.212112 -0.173215  0.119209  5  1.0  1.0\n2013-01-03 -0.861849 -2.104569 -0.494929  5  2.0  5.0\n2013-01-04  0.721555 -0.706771 -1.039575  5  3.0  5.0\n  To get the boolean mask where values are nan: \nIn [60]: pd.isna(df1)\nOut[60]: \n                A      B      C      D      F      E\n2013-01-01  False  False  False  False   True  False\n2013-01-02  False  False  False  False  False  False\n2013-01-03  False  False  False  False  False   True\n2013-01-04  False  False  False  False  False   True\n    Operations See the Basic section on Binary Ops.  Stats Operations in general exclude missing data. Performing a descriptive statistic: \nIn [61]: df.mean()\nOut[61]: \nA   -0.004474\nB   -0.383981\nC   -0.687758\nD    5.000000\nF    3.000000\ndtype: float64\n  Same operation on the other axis: \nIn [62]: df.mean(1)\nOut[62]: \n2013-01-01    0.872735\n2013-01-02    1.431621\n2013-01-03    0.707731\n2013-01-04    1.395042\n2013-01-05    1.883656\n2013-01-06    1.592306\nFreq: D, dtype: float64\n  Operating with objects that have different dimensionality and need alignment. In addition, pandas automatically broadcasts along the specified dimension: \nIn [63]: s = pd.Series([1, 3, 5, np.nan, 6, 8], index=dates).shift(2)\n\nIn [64]: s\nOut[64]: \n2013-01-01    NaN\n2013-01-02    NaN\n2013-01-03    1.0\n2013-01-04    3.0\n2013-01-05    5.0\n2013-01-06    NaN\nFreq: D, dtype: float64\n\nIn [65]: df.sub(s, axis=\"index\")\nOut[65]: \n                   A         B         C    D    F\n2013-01-01       NaN       NaN       NaN  NaN  NaN\n2013-01-02       NaN       NaN       NaN  NaN  NaN\n2013-01-03 -1.861849 -3.104569 -1.494929  4.0  1.0\n2013-01-04 -2.278445 -3.706771 -4.039575  2.0  0.0\n2013-01-05 -5.424972 -4.432980 -4.723768  0.0 -1.0\n2013-01-06       NaN       NaN       NaN  NaN  NaN\n    Apply Applying functions to the data: \nIn [66]: df.apply(np.cumsum)\nOut[66]: \n                   A         B         C   D     F\n2013-01-01  0.000000  0.000000 -1.509059   5   NaN\n2013-01-02  1.212112 -0.173215 -1.389850  10   1.0\n2013-01-03  0.350263 -2.277784 -1.884779  15   3.0\n2013-01-04  1.071818 -2.984555 -2.924354  20   6.0\n2013-01-05  0.646846 -2.417535 -2.648122  25  10.0\n2013-01-06 -0.026844 -2.303886 -4.126549  30  15.0\n\nIn [67]: df.apply(lambda x: x.max() - x.min())\nOut[67]: \nA    2.073961\nB    2.671590\nC    1.785291\nD    0.000000\nF    4.000000\ndtype: float64\n    Histogramming See more at Histogramming and Discretization. \nIn [68]: s = pd.Series(np.random.randint(0, 7, size=10))\n\nIn [69]: s\nOut[69]: \n0    4\n1    2\n2    1\n3    2\n4    6\n5    4\n6    4\n7    6\n8    4\n9    4\ndtype: int64\n\nIn [70]: s.value_counts()\nOut[70]: \n4    5\n2    2\n6    2\n1    1\ndtype: int64\n    String Methods Series is equipped with a set of string processing methods in the str attribute that make it easy to operate on each element of the array, as in the code snippet below. Note that pattern-matching in str generally uses regular expressions by default (and in some cases always uses them). See more at Vectorized String Methods. \nIn [71]: s = pd.Series([\"A\", \"B\", \"C\", \"Aaba\", \"Baca\", np.nan, \"CABA\", \"dog\", \"cat\"])\n\nIn [72]: s.str.lower()\nOut[72]: \n0       a\n1       b\n2       c\n3    aaba\n4    baca\n5     NaN\n6    caba\n7     dog\n8     cat\ndtype: object\n     Merge  Concat pandas provides various facilities for easily combining together Series and DataFrame objects with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations. See the Merging section. Concatenating pandas objects together with concat(): \nIn [73]: df = pd.DataFrame(np.random.randn(10, 4))\n\nIn [74]: df\nOut[74]: \n          0         1         2         3\n0 -0.548702  1.467327 -1.015962 -0.483075\n1  1.637550 -1.217659 -0.291519 -1.745505\n2 -0.263952  0.991460 -0.919069  0.266046\n3 -0.709661  1.669052  1.037882 -1.705775\n4 -0.919854 -0.042379  1.247642 -0.009920\n5  0.290213  0.495767  0.362949  1.548106\n6 -1.131345 -0.089329  0.337863 -0.945867\n7 -0.932132  1.956030  0.017587 -0.016692\n8 -0.575247  0.254161 -1.143704  0.215897\n9  1.193555 -0.077118 -0.408530 -0.862495\n\n# break it into pieces\nIn [75]: pieces = [df[:3], df[3:7], df[7:]]\n\nIn [76]: pd.concat(pieces)\nOut[76]: \n          0         1         2         3\n0 -0.548702  1.467327 -1.015962 -0.483075\n1  1.637550 -1.217659 -0.291519 -1.745505\n2 -0.263952  0.991460 -0.919069  0.266046\n3 -0.709661  1.669052  1.037882 -1.705775\n4 -0.919854 -0.042379  1.247642 -0.009920\n5  0.290213  0.495767  0.362949  1.548106\n6 -1.131345 -0.089329  0.337863 -0.945867\n7 -0.932132  1.956030  0.017587 -0.016692\n8 -0.575247  0.254161 -1.143704  0.215897\n9  1.193555 -0.077118 -0.408530 -0.862495\n   Note Adding a column to a DataFrame is relatively fast. However, adding a row requires a copy, and may be expensive. We recommend passing a pre-built list of records to the DataFrame constructor instead of building a DataFrame by iteratively appending records to it.    Join SQL style merges. See the Database style joining section. \nIn [77]: left = pd.DataFrame({\"key\": [\"foo\", \"foo\"], \"lval\": [1, 2]})\n\nIn [78]: right = pd.DataFrame({\"key\": [\"foo\", \"foo\"], \"rval\": [4, 5]})\n\nIn [79]: left\nOut[79]: \n   key  lval\n0  foo     1\n1  foo     2\n\nIn [80]: right\nOut[80]: \n   key  rval\n0  foo     4\n1  foo     5\n\nIn [81]: pd.merge(left, right, on=\"key\")\nOut[81]: \n   key  lval  rval\n0  foo     1     4\n1  foo     1     5\n2  foo     2     4\n3  foo     2     5\n  Another example that can be given is: \nIn [82]: left = pd.DataFrame({\"key\": [\"foo\", \"bar\"], \"lval\": [1, 2]})\n\nIn [83]: right = pd.DataFrame({\"key\": [\"foo\", \"bar\"], \"rval\": [4, 5]})\n\nIn [84]: left\nOut[84]: \n   key  lval\n0  foo     1\n1  bar     2\n\nIn [85]: right\nOut[85]: \n   key  rval\n0  foo     4\n1  bar     5\n\nIn [86]: pd.merge(left, right, on=\"key\")\nOut[86]: \n   key  lval  rval\n0  foo     1     4\n1  bar     2     5\n     Grouping By \u201cgroup by\u201d we are referring to a process involving one or more of the following steps:  \n Splitting the data into groups based on some criteria Applying a function to each group independently Combining the results into a data structure  \n See the Grouping section. \nIn [87]: df = pd.DataFrame(\n   ....:     {\n   ....:         \"A\": [\"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"foo\"],\n   ....:         \"B\": [\"one\", \"one\", \"two\", \"three\", \"two\", \"two\", \"one\", \"three\"],\n   ....:         \"C\": np.random.randn(8),\n   ....:         \"D\": np.random.randn(8),\n   ....:     }\n   ....: )\n   ....: \n\nIn [88]: df\nOut[88]: \n     A      B         C         D\n0  foo    one  1.346061 -1.577585\n1  bar    one  1.511763  0.396823\n2  foo    two  1.627081 -0.105381\n3  bar  three -0.990582 -0.532532\n4  foo    two -0.441652  1.453749\n5  bar    two  1.211526  1.208843\n6  foo    one  0.268520 -0.080952\n7  foo  three  0.024580 -0.264610\n  Grouping and then applying the sum() function to the resulting groups: \nIn [89]: df.groupby(\"A\").sum()\nOut[89]: \n            C         D\nA                      \nbar  1.732707  1.073134\nfoo  2.824590 -0.574779\n  Grouping by multiple columns forms a hierarchical index, and again we can apply the sum() function: \nIn [90]: df.groupby([\"A\", \"B\"]).sum()\nOut[90]: \n                  C         D\nA   B                        \nbar one    1.511763  0.396823\n    three -0.990582 -0.532532\n    two    1.211526  1.208843\nfoo one    1.614581 -1.658537\n    three  0.024580 -0.264610\n    two    1.185429  1.348368\n    Reshaping See the sections on Hierarchical Indexing and Reshaping.  Stack \nIn [91]: tuples = list(\n   ....:     zip(\n   ....:         *[\n   ....:             [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"],\n   ....:             [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"],\n   ....:         ]\n   ....:     )\n   ....: )\n   ....: \n\nIn [92]: index = pd.MultiIndex.from_tuples(tuples, names=[\"first\", \"second\"])\n\nIn [93]: df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=[\"A\", \"B\"])\n\nIn [94]: df2 = df[:4]\n\nIn [95]: df2\nOut[95]: \n                     A         B\nfirst second                    \nbar   one    -0.727965 -0.589346\n      two     0.339969 -0.693205\nbaz   one    -0.339355  0.593616\n      two     0.884345  1.591431\n  The stack() method \u201ccompresses\u201d a level in the DataFrame\u2019s columns: \nIn [96]: stacked = df2.stack()\n\nIn [97]: stacked\nOut[97]: \nfirst  second   \nbar    one     A   -0.727965\n               B   -0.589346\n       two     A    0.339969\n               B   -0.693205\nbaz    one     A   -0.339355\n               B    0.593616\n       two     A    0.884345\n               B    1.591431\ndtype: float64\n  With a \u201cstacked\u201d DataFrame or Series (having a MultiIndex as the index), the inverse operation of stack() is unstack(), which by default unstacks the last level: \nIn [98]: stacked.unstack()\nOut[98]: \n                     A         B\nfirst second                    \nbar   one    -0.727965 -0.589346\n      two     0.339969 -0.693205\nbaz   one    -0.339355  0.593616\n      two     0.884345  1.591431\n\nIn [99]: stacked.unstack(1)\nOut[99]: \nsecond        one       two\nfirst                      \nbar   A -0.727965  0.339969\n      B -0.589346 -0.693205\nbaz   A -0.339355  0.884345\n      B  0.593616  1.591431\n\nIn [100]: stacked.unstack(0)\nOut[100]: \nfirst          bar       baz\nsecond                      \none    A -0.727965 -0.339355\n       B -0.589346  0.593616\ntwo    A  0.339969  0.884345\n       B -0.693205  1.591431\n    Pivot tables See the section on Pivot Tables. \nIn [101]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"A\": [\"one\", \"one\", \"two\", \"three\"] * 3,\n   .....:         \"B\": [\"A\", \"B\", \"C\"] * 4,\n   .....:         \"C\": [\"foo\", \"foo\", \"foo\", \"bar\", \"bar\", \"bar\"] * 2,\n   .....:         \"D\": np.random.randn(12),\n   .....:         \"E\": np.random.randn(12),\n   .....:     }\n   .....: )\n   .....: \n\nIn [102]: df\nOut[102]: \n        A  B    C         D         E\n0     one  A  foo -1.202872  0.047609\n1     one  B  foo -1.814470 -0.136473\n2     two  C  foo  1.018601 -0.561757\n3   three  A  bar -0.595447 -1.623033\n4     one  B  bar  1.395433  0.029399\n5     one  C  bar -0.392670 -0.542108\n6     two  A  foo  0.007207  0.282696\n7   three  B  foo  1.928123 -0.087302\n8     one  C  foo -0.055224 -1.575170\n9     one  A  bar  2.395985  1.771208\n10    two  B  bar  1.552825  0.816482\n11  three  C  bar  0.166599  1.100230\n  We can produce pivot tables from this data very easily: \nIn [103]: pd.pivot_table(df, values=\"D\", index=[\"A\", \"B\"], columns=[\"C\"])\nOut[103]: \nC             bar       foo\nA     B                    \none   A  2.395985 -1.202872\n      B  1.395433 -1.814470\n      C -0.392670 -0.055224\nthree A -0.595447       NaN\n      B       NaN  1.928123\n      C  0.166599       NaN\ntwo   A       NaN  0.007207\n      B  1.552825       NaN\n      C       NaN  1.018601\n     Time series pandas has simple, powerful, and efficient functionality for performing resampling operations during frequency conversion (e.g., converting secondly data into 5-minutely data). This is extremely common in, but not limited to, financial applications. See the Time Series section. \nIn [104]: rng = pd.date_range(\"1/1/2012\", periods=100, freq=\"S\")\n\nIn [105]: ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)\n\nIn [106]: ts.resample(\"5Min\").sum()\nOut[106]: \n2012-01-01    24182\nFreq: 5T, dtype: int64\n  Time zone representation: \nIn [107]: rng = pd.date_range(\"3/6/2012 00:00\", periods=5, freq=\"D\")\n\nIn [108]: ts = pd.Series(np.random.randn(len(rng)), rng)\n\nIn [109]: ts\nOut[109]: \n2012-03-06    1.857704\n2012-03-07   -1.193545\n2012-03-08    0.677510\n2012-03-09   -0.153931\n2012-03-10    0.520091\nFreq: D, dtype: float64\n\nIn [110]: ts_utc = ts.tz_localize(\"UTC\")\n\nIn [111]: ts_utc\nOut[111]: \n2012-03-06 00:00:00+00:00    1.857704\n2012-03-07 00:00:00+00:00   -1.193545\n2012-03-08 00:00:00+00:00    0.677510\n2012-03-09 00:00:00+00:00   -0.153931\n2012-03-10 00:00:00+00:00    0.520091\nFreq: D, dtype: float64\n  Converting to another time zone: \nIn [112]: ts_utc.tz_convert(\"US/Eastern\")\nOut[112]: \n2012-03-05 19:00:00-05:00    1.857704\n2012-03-06 19:00:00-05:00   -1.193545\n2012-03-07 19:00:00-05:00    0.677510\n2012-03-08 19:00:00-05:00   -0.153931\n2012-03-09 19:00:00-05:00    0.520091\nFreq: D, dtype: float64\n  Converting between time span representations: \nIn [113]: rng = pd.date_range(\"1/1/2012\", periods=5, freq=\"M\")\n\nIn [114]: ts = pd.Series(np.random.randn(len(rng)), index=rng)\n\nIn [115]: ts\nOut[115]: \n2012-01-31   -1.475051\n2012-02-29    0.722570\n2012-03-31   -0.322646\n2012-04-30   -1.601631\n2012-05-31    0.778033\nFreq: M, dtype: float64\n\nIn [116]: ps = ts.to_period()\n\nIn [117]: ps\nOut[117]: \n2012-01   -1.475051\n2012-02    0.722570\n2012-03   -0.322646\n2012-04   -1.601631\n2012-05    0.778033\nFreq: M, dtype: float64\n\nIn [118]: ps.to_timestamp()\nOut[118]: \n2012-01-01   -1.475051\n2012-02-01    0.722570\n2012-03-01   -0.322646\n2012-04-01   -1.601631\n2012-05-01    0.778033\nFreq: MS, dtype: float64\n  Converting between period and timestamp enables some convenient arithmetic functions to be used. In the following example, we convert a quarterly frequency with year ending in November to 9am of the end of the month following the quarter end: \nIn [119]: prng = pd.period_range(\"1990Q1\", \"2000Q4\", freq=\"Q-NOV\")\n\nIn [120]: ts = pd.Series(np.random.randn(len(prng)), prng)\n\nIn [121]: ts.index = (prng.asfreq(\"M\", \"e\") + 1).asfreq(\"H\", \"s\") + 9\n\nIn [122]: ts.head()\nOut[122]: \n1990-03-01 09:00   -0.289342\n1990-06-01 09:00    0.233141\n1990-09-01 09:00   -0.223540\n1990-12-01 09:00    0.542054\n1991-03-01 09:00   -0.688585\nFreq: H, dtype: float64\n    Categoricals pandas can include categorical data in a DataFrame. For full docs, see the categorical introduction and the API documentation. \nIn [123]: df = pd.DataFrame(\n   .....:     {\"id\": [1, 2, 3, 4, 5, 6], \"raw_grade\": [\"a\", \"b\", \"b\", \"a\", \"a\", \"e\"]}\n   .....: )\n   .....: \n  Converting the raw grades to a categorical data type: \nIn [124]: df[\"grade\"] = df[\"raw_grade\"].astype(\"category\")\n\nIn [125]: df[\"grade\"]\nOut[125]: \n0    a\n1    b\n2    b\n3    a\n4    a\n5    e\nName: grade, dtype: category\nCategories (3, object): ['a', 'b', 'e']\n  Rename the categories to more meaningful names (assigning to Series.cat.categories() is in place!): \nIn [126]: df[\"grade\"].cat.categories = [\"very good\", \"good\", \"very bad\"]\n  Reorder the categories and simultaneously add the missing categories (methods under Series.cat() return a new Series by default): \nIn [127]: df[\"grade\"] = df[\"grade\"].cat.set_categories(\n   .....:     [\"very bad\", \"bad\", \"medium\", \"good\", \"very good\"]\n   .....: )\n   .....: \n\nIn [128]: df[\"grade\"]\nOut[128]: \n0    very good\n1         good\n2         good\n3    very good\n4    very good\n5     very bad\nName: grade, dtype: category\nCategories (5, object): ['very bad', 'bad', 'medium', 'good', 'very good']\n  Sorting is per order in the categories, not lexical order: \nIn [129]: df.sort_values(by=\"grade\")\nOut[129]: \n   id raw_grade      grade\n5   6         e   very bad\n1   2         b       good\n2   3         b       good\n0   1         a  very good\n3   4         a  very good\n4   5         a  very good\n  Grouping by a categorical column also shows empty categories: \nIn [130]: df.groupby(\"grade\").size()\nOut[130]: \ngrade\nvery bad     1\nbad          0\nmedium       0\ngood         2\nvery good    3\ndtype: int64\n    Plotting See the Plotting docs. We use the standard convention for referencing the matplotlib API: \nIn [131]: import matplotlib.pyplot as plt\n\nIn [132]: plt.close(\"all\")\n  The close() method is used to close a figure window: \nIn [133]: ts = pd.Series(np.random.randn(1000), index=pd.date_range(\"1/1/2000\", periods=1000))\n\nIn [134]: ts = ts.cumsum()\n\nIn [135]: ts.plot();\n   If running under Jupyter Notebook, the plot will appear on plot(). Otherwise use matplotlib.pyplot.show to show it or matplotlib.pyplot.savefig to write it to a file. \nIn [136]: plt.show();\n  On a DataFrame, the plot() method is a convenience to plot all of the columns with labels: \nIn [137]: df = pd.DataFrame(\n   .....:     np.random.randn(1000, 4), index=ts.index, columns=[\"A\", \"B\", \"C\", \"D\"]\n   .....: )\n   .....: \n\nIn [138]: df = df.cumsum()\n\nIn [139]: plt.figure();\n\nIn [140]: df.plot();\n\nIn [141]: plt.legend(loc='best');\n     Getting data in/out  CSV Writing to a csv file: \nIn [142]: df.to_csv(\"foo.csv\")\n  Reading from a csv file: \nIn [143]: pd.read_csv(\"foo.csv\")\nOut[143]: \n     Unnamed: 0          A          B          C          D\n0    2000-01-01   0.350262   0.843315   1.798556   0.782234\n1    2000-01-02  -0.586873   0.034907   1.923792  -0.562651\n2    2000-01-03  -1.245477  -0.963406   2.269575  -1.612566\n3    2000-01-04  -0.252830  -0.498066   3.176886  -1.275581\n4    2000-01-05  -1.044057   0.118042   2.768571   0.386039\n..          ...        ...        ...        ...        ...\n995  2002-09-22 -48.017654  31.474551  69.146374 -47.541670\n996  2002-09-23 -47.207912  32.627390  68.505254 -48.828331\n997  2002-09-24 -48.907133  31.990402  67.310924 -49.391051\n998  2002-09-25 -50.146062  33.716770  67.717434 -49.037577\n999  2002-09-26 -49.724318  33.479952  68.108014 -48.822030\n\n[1000 rows x 5 columns]\n    HDF5 Reading and writing to HDFStores. Writing to a HDF5 Store: \nIn [144]: df.to_hdf(\"foo.h5\", \"df\")\n  Reading from a HDF5 Store: \nIn [145]: pd.read_hdf(\"foo.h5\", \"df\")\nOut[145]: \n                    A          B          C          D\n2000-01-01   0.350262   0.843315   1.798556   0.782234\n2000-01-02  -0.586873   0.034907   1.923792  -0.562651\n2000-01-03  -1.245477  -0.963406   2.269575  -1.612566\n2000-01-04  -0.252830  -0.498066   3.176886  -1.275581\n2000-01-05  -1.044057   0.118042   2.768571   0.386039\n...               ...        ...        ...        ...\n2002-09-22 -48.017654  31.474551  69.146374 -47.541670\n2002-09-23 -47.207912  32.627390  68.505254 -48.828331\n2002-09-24 -48.907133  31.990402  67.310924 -49.391051\n2002-09-25 -50.146062  33.716770  67.717434 -49.037577\n2002-09-26 -49.724318  33.479952  68.108014 -48.822030\n\n[1000 rows x 4 columns]\n    Excel Reading and writing to MS Excel. Writing to an excel file: \nIn [146]: df.to_excel(\"foo.xlsx\", sheet_name=\"Sheet1\")\n  Reading from an excel file: \nIn [147]: pd.read_excel(\"foo.xlsx\", \"Sheet1\", index_col=None, na_values=[\"NA\"])\nOut[147]: \n    Unnamed: 0          A          B          C          D\n0   2000-01-01   0.350262   0.843315   1.798556   0.782234\n1   2000-01-02  -0.586873   0.034907   1.923792  -0.562651\n2   2000-01-03  -1.245477  -0.963406   2.269575  -1.612566\n3   2000-01-04  -0.252830  -0.498066   3.176886  -1.275581\n4   2000-01-05  -1.044057   0.118042   2.768571   0.386039\n..         ...        ...        ...        ...        ...\n995 2002-09-22 -48.017654  31.474551  69.146374 -47.541670\n996 2002-09-23 -47.207912  32.627390  68.505254 -48.828331\n997 2002-09-24 -48.907133  31.990402  67.310924 -49.391051\n998 2002-09-25 -50.146062  33.716770  67.717434 -49.037577\n999 2002-09-26 -49.724318  33.479952  68.108014 -48.822030\n\n[1000 rows x 5 columns]\n     Gotchas If you are attempting to perform an operation you might see an exception like: \n>>> if pd.Series([False, True, False]):\n...     print(\"I was true\")\nTraceback\n    ...\nValueError: The truth value of an array is ambiguous. Use a.empty, a.any() or a.all().\n  See Comparisons for an explanation and what to do. See Gotchas as well. \n"}, {"name": "API reference", "path": "reference/index", "type": "General functions", "text": "API reference This page gives an overview of all public pandas objects, functions and methods. All classes and functions exposed in pandas.* namespace are public. Some subpackages are public which include pandas.errors, pandas.plotting, and pandas.testing. Public functions in pandas.io and pandas.tseries submodules are mentioned in the documentation. pandas.api.types subpackage holds some public functions related to data types in pandas.  Warning The pandas.core, pandas.compat, and pandas.util top-level modules are PRIVATE. Stable functionality in such modules is not guaranteed.    \nInput/output Pickling Flat file Clipboard Excel JSON HTML XML Latex HDFStore: PyTables (HDF5) Feather Parquet ORC SAS SPSS SQL Google BigQuery STATA   \nGeneral functions Data manipulations Top-level missing data Top-level dealing with numeric data Top-level dealing with datetimelike data Top-level dealing with Interval data Top-level evaluation Hashing Testing   \nSeries Constructor Attributes Conversion Indexing, iteration Binary operator functions Function application, GroupBy & window Computations / descriptive stats Reindexing / selection / label manipulation Missing data handling Reshaping, sorting Combining / comparing / joining / merging Time Series-related Accessors Plotting Serialization / IO / conversion   \nDataFrame Constructor Attributes and underlying data Conversion Indexing, iteration Binary operator functions Function application, GroupBy & window Computations / descriptive stats Reindexing / selection / label manipulation Missing data handling Reshaping, sorting, transposing Combining / comparing / joining / merging Time Series-related Flags Metadata Plotting Sparse accessor Serialization / IO / conversion   \npandas arrays, scalars, and data types pandas.array Datetime data Timedelta data Timespan data Period Interval data Nullable integer Categorical data Sparse data Text data Boolean data with missing values   \nIndex objects Index Numeric Index CategoricalIndex IntervalIndex MultiIndex DatetimeIndex TimedeltaIndex PeriodIndex   \nDate offsets DateOffset BusinessDay BusinessHour CustomBusinessDay CustomBusinessHour MonthEnd MonthBegin BusinessMonthEnd BusinessMonthBegin CustomBusinessMonthEnd CustomBusinessMonthBegin SemiMonthEnd SemiMonthBegin Week WeekOfMonth LastWeekOfMonth BQuarterEnd BQuarterBegin QuarterEnd QuarterBegin BYearEnd BYearBegin YearEnd YearBegin FY5253 FY5253Quarter Easter Tick Day Hour Minute Second Milli Micro Nano   \nFrequencies pandas.tseries.frequencies.to_offset   \nWindow Rolling window functions Weighted window functions Expanding window functions Exponentially-weighted window functions Window indexer   \nGroupBy Indexing, iteration Function application Computations / descriptive stats   \nResampling Indexing, iteration Function application Upsampling Computations / descriptive stats   \nStyle Styler constructor Styler properties Style application Builtin styles Style export and import   \nPlotting pandas.plotting.andrews_curves pandas.plotting.autocorrelation_plot pandas.plotting.bootstrap_plot pandas.plotting.boxplot pandas.plotting.deregister_matplotlib_converters pandas.plotting.lag_plot pandas.plotting.parallel_coordinates pandas.plotting.plot_params pandas.plotting.radviz pandas.plotting.register_matplotlib_converters pandas.plotting.scatter_matrix pandas.plotting.table   \nGeneral utility functions Working with options Testing functions Exceptions and warnings Data types related functionality Bug report function   \nExtensions pandas.api.extensions.register_extension_dtype pandas.api.extensions.register_dataframe_accessor pandas.api.extensions.register_series_accessor pandas.api.extensions.register_index_accessor pandas.api.extensions.ExtensionDtype pandas.api.extensions.ExtensionArray pandas.arrays.PandasArray pandas.api.indexers.check_array_indexer    \n"}, {"name": "Categorical data", "path": "user_guide/categorical", "type": "Manual", "text": "Categorical data This is an introduction to pandas categorical data type, including a short comparison with R\u2019s factor. Categoricals are a pandas data type corresponding to categorical variables in statistics. A categorical variable takes on a limited, and usually fixed, number of possible values (categories; levels in R). Examples are gender, social class, blood type, country affiliation, observation time or rating via Likert scales. In contrast to statistical categorical variables, categorical data might have an order (e.g. \u2018strongly agree\u2019 vs \u2018agree\u2019 or \u2018first observation\u2019 vs. \u2018second observation\u2019), but numerical operations (additions, divisions, \u2026) are not possible. All values of categorical data are either in categories or np.nan. Order is defined by the order of categories, not lexical order of the values. Internally, the data structure consists of a categories array and an integer array of codes which point to the real value in the categories array. The categorical data type is useful in the following cases:  A string variable consisting of only a few different values. Converting such a string variable to a categorical variable will save some memory, see here. The lexical order of a variable is not the same as the logical order (\u201cone\u201d, \u201ctwo\u201d, \u201cthree\u201d). By converting to a categorical and specifying an order on the categories, sorting and min/max will use the logical order instead of the lexical order, see here. As a signal to other Python libraries that this column should be treated as a categorical variable (e.g. to use suitable statistical methods or plot types).  See also the API docs on categoricals.  Object creation  Series creation Categorical Series or columns in a DataFrame can be created in several ways: By specifying dtype=\"category\" when constructing a Series: \nIn [1]: s = pd.Series([\"a\", \"b\", \"c\", \"a\"], dtype=\"category\")\n\nIn [2]: s\nOut[2]: \n0    a\n1    b\n2    c\n3    a\ndtype: category\nCategories (3, object): ['a', 'b', 'c']\n  By converting an existing Series or column to a category dtype: \nIn [3]: df = pd.DataFrame({\"A\": [\"a\", \"b\", \"c\", \"a\"]})\n\nIn [4]: df[\"B\"] = df[\"A\"].astype(\"category\")\n\nIn [5]: df\nOut[5]: \n   A  B\n0  a  a\n1  b  b\n2  c  c\n3  a  a\n  By using special functions, such as cut(), which groups data into discrete bins. See the example on tiling in the docs. \nIn [6]: df = pd.DataFrame({\"value\": np.random.randint(0, 100, 20)})\n\nIn [7]: labels = [\"{0} - {1}\".format(i, i + 9) for i in range(0, 100, 10)]\n\nIn [8]: df[\"group\"] = pd.cut(df.value, range(0, 105, 10), right=False, labels=labels)\n\nIn [9]: df.head(10)\nOut[9]: \n   value    group\n0     65  60 - 69\n1     49  40 - 49\n2     56  50 - 59\n3     43  40 - 49\n4     43  40 - 49\n5     91  90 - 99\n6     32  30 - 39\n7     87  80 - 89\n8     36  30 - 39\n9      8    0 - 9\n  By passing a pandas.Categorical object to a Series or assigning it to a DataFrame. \nIn [10]: raw_cat = pd.Categorical(\n   ....:     [\"a\", \"b\", \"c\", \"a\"], categories=[\"b\", \"c\", \"d\"], ordered=False\n   ....: )\n   ....: \n\nIn [11]: s = pd.Series(raw_cat)\n\nIn [12]: s\nOut[12]: \n0    NaN\n1      b\n2      c\n3    NaN\ndtype: category\nCategories (3, object): ['b', 'c', 'd']\n\nIn [13]: df = pd.DataFrame({\"A\": [\"a\", \"b\", \"c\", \"a\"]})\n\nIn [14]: df[\"B\"] = raw_cat\n\nIn [15]: df\nOut[15]: \n   A    B\n0  a  NaN\n1  b    b\n2  c    c\n3  a  NaN\n  Categorical data has a specific category dtype: \nIn [16]: df.dtypes\nOut[16]: \nA      object\nB    category\ndtype: object\n    DataFrame creation Similar to the previous section where a single column was converted to categorical, all columns in a DataFrame can be batch converted to categorical either during or after construction. This can be done during construction by specifying dtype=\"category\" in the DataFrame constructor: \nIn [17]: df = pd.DataFrame({\"A\": list(\"abca\"), \"B\": list(\"bccd\")}, dtype=\"category\")\n\nIn [18]: df.dtypes\nOut[18]: \nA    category\nB    category\ndtype: object\n  Note that the categories present in each column differ; the conversion is done column by column, so only labels present in a given column are categories: \nIn [19]: df[\"A\"]\nOut[19]: \n0    a\n1    b\n2    c\n3    a\nName: A, dtype: category\nCategories (3, object): ['a', 'b', 'c']\n\nIn [20]: df[\"B\"]\nOut[20]: \n0    b\n1    c\n2    c\n3    d\nName: B, dtype: category\nCategories (3, object): ['b', 'c', 'd']\n  Analogously, all columns in an existing DataFrame can be batch converted using DataFrame.astype(): \nIn [21]: df = pd.DataFrame({\"A\": list(\"abca\"), \"B\": list(\"bccd\")})\n\nIn [22]: df_cat = df.astype(\"category\")\n\nIn [23]: df_cat.dtypes\nOut[23]: \nA    category\nB    category\ndtype: object\n  This conversion is likewise done column by column: \nIn [24]: df_cat[\"A\"]\nOut[24]: \n0    a\n1    b\n2    c\n3    a\nName: A, dtype: category\nCategories (3, object): ['a', 'b', 'c']\n\nIn [25]: df_cat[\"B\"]\nOut[25]: \n0    b\n1    c\n2    c\n3    d\nName: B, dtype: category\nCategories (3, object): ['b', 'c', 'd']\n    Controlling behavior In the examples above where we passed dtype='category', we used the default behavior:  Categories are inferred from the data. Categories are unordered.  To control those behaviors, instead of passing 'category', use an instance of CategoricalDtype. \nIn [26]: from pandas.api.types import CategoricalDtype\n\nIn [27]: s = pd.Series([\"a\", \"b\", \"c\", \"a\"])\n\nIn [28]: cat_type = CategoricalDtype(categories=[\"b\", \"c\", \"d\"], ordered=True)\n\nIn [29]: s_cat = s.astype(cat_type)\n\nIn [30]: s_cat\nOut[30]: \n0    NaN\n1      b\n2      c\n3    NaN\ndtype: category\nCategories (3, object): ['b' < 'c' < 'd']\n  Similarly, a CategoricalDtype can be used with a DataFrame to ensure that categories are consistent among all columns. \nIn [31]: from pandas.api.types import CategoricalDtype\n\nIn [32]: df = pd.DataFrame({\"A\": list(\"abca\"), \"B\": list(\"bccd\")})\n\nIn [33]: cat_type = CategoricalDtype(categories=list(\"abcd\"), ordered=True)\n\nIn [34]: df_cat = df.astype(cat_type)\n\nIn [35]: df_cat[\"A\"]\nOut[35]: \n0    a\n1    b\n2    c\n3    a\nName: A, dtype: category\nCategories (4, object): ['a' < 'b' < 'c' < 'd']\n\nIn [36]: df_cat[\"B\"]\nOut[36]: \n0    b\n1    c\n2    c\n3    d\nName: B, dtype: category\nCategories (4, object): ['a' < 'b' < 'c' < 'd']\n   Note To perform table-wise conversion, where all labels in the entire DataFrame are used as categories for each column, the categories parameter can be determined programmatically by categories = pd.unique(df.to_numpy().ravel()).  If you already have codes and categories, you can use the from_codes() constructor to save the factorize step during normal constructor mode: \nIn [37]: splitter = np.random.choice([0, 1], 5, p=[0.5, 0.5])\n\nIn [38]: s = pd.Series(pd.Categorical.from_codes(splitter, categories=[\"train\", \"test\"]))\n    Regaining original data To get back to the original Series or NumPy array, use Series.astype(original_dtype) or np.asarray(categorical): \nIn [39]: s = pd.Series([\"a\", \"b\", \"c\", \"a\"])\n\nIn [40]: s\nOut[40]: \n0    a\n1    b\n2    c\n3    a\ndtype: object\n\nIn [41]: s2 = s.astype(\"category\")\n\nIn [42]: s2\nOut[42]: \n0    a\n1    b\n2    c\n3    a\ndtype: category\nCategories (3, object): ['a', 'b', 'c']\n\nIn [43]: s2.astype(str)\nOut[43]: \n0    a\n1    b\n2    c\n3    a\ndtype: object\n\nIn [44]: np.asarray(s2)\nOut[44]: array(['a', 'b', 'c', 'a'], dtype=object)\n   Note In contrast to R\u2019s factor function, categorical data is not converting input values to strings; categories will end up the same data type as the original values.   Note In contrast to R\u2019s factor function, there is currently no way to assign/change labels at creation time. Use categories to change the categories after creation time.     CategoricalDtype A categorical\u2019s type is fully described by  categories: a sequence of unique values and no missing values ordered: a boolean  This information can be stored in a CategoricalDtype. The categories argument is optional, which implies that the actual categories should be inferred from whatever is present in the data when the pandas.Categorical is created. The categories are assumed to be unordered by default. \nIn [45]: from pandas.api.types import CategoricalDtype\n\nIn [46]: CategoricalDtype([\"a\", \"b\", \"c\"])\nOut[46]: CategoricalDtype(categories=['a', 'b', 'c'], ordered=False)\n\nIn [47]: CategoricalDtype([\"a\", \"b\", \"c\"], ordered=True)\nOut[47]: CategoricalDtype(categories=['a', 'b', 'c'], ordered=True)\n\nIn [48]: CategoricalDtype()\nOut[48]: CategoricalDtype(categories=None, ordered=False)\n  A CategoricalDtype can be used in any place pandas expects a dtype. For example pandas.read_csv(), pandas.DataFrame.astype(), or in the Series constructor.  Note As a convenience, you can use the string 'category' in place of a CategoricalDtype when you want the default behavior of the categories being unordered, and equal to the set values present in the array. In other words, dtype='category' is equivalent to dtype=CategoricalDtype().   Equality semantics Two instances of CategoricalDtype compare equal whenever they have the same categories and order. When comparing two unordered categoricals, the order of the categories is not considered. \nIn [49]: c1 = CategoricalDtype([\"a\", \"b\", \"c\"], ordered=False)\n\n# Equal, since order is not considered when ordered=False\nIn [50]: c1 == CategoricalDtype([\"b\", \"c\", \"a\"], ordered=False)\nOut[50]: True\n\n# Unequal, since the second CategoricalDtype is ordered\nIn [51]: c1 == CategoricalDtype([\"a\", \"b\", \"c\"], ordered=True)\nOut[51]: False\n  All instances of CategoricalDtype compare equal to the string 'category'. \nIn [52]: c1 == \"category\"\nOut[52]: True\n   Warning Since dtype='category' is essentially CategoricalDtype(None, False), and since all instances CategoricalDtype compare equal to 'category', all instances of CategoricalDtype compare equal to a CategoricalDtype(None, False), regardless of categories or ordered.     Description Using describe() on categorical data will produce similar output to a Series or DataFrame of type string. \nIn [53]: cat = pd.Categorical([\"a\", \"c\", \"c\", np.nan], categories=[\"b\", \"a\", \"c\"])\n\nIn [54]: df = pd.DataFrame({\"cat\": cat, \"s\": [\"a\", \"c\", \"c\", np.nan]})\n\nIn [55]: df.describe()\nOut[55]: \n       cat  s\ncount    3  3\nunique   2  2\ntop      c  c\nfreq     2  2\n\nIn [56]: df[\"cat\"].describe()\nOut[56]: \ncount     3\nunique    2\ntop       c\nfreq      2\nName: cat, dtype: object\n    Working with categories Categorical data has a categories and a ordered property, which list their possible values and whether the ordering matters or not. These properties are exposed as s.cat.categories and s.cat.ordered. If you don\u2019t manually specify categories and ordering, they are inferred from the passed arguments. \nIn [57]: s = pd.Series([\"a\", \"b\", \"c\", \"a\"], dtype=\"category\")\n\nIn [58]: s.cat.categories\nOut[58]: Index(['a', 'b', 'c'], dtype='object')\n\nIn [59]: s.cat.ordered\nOut[59]: False\n  It\u2019s also possible to pass in the categories in a specific order: \nIn [60]: s = pd.Series(pd.Categorical([\"a\", \"b\", \"c\", \"a\"], categories=[\"c\", \"b\", \"a\"]))\n\nIn [61]: s.cat.categories\nOut[61]: Index(['c', 'b', 'a'], dtype='object')\n\nIn [62]: s.cat.ordered\nOut[62]: False\n   Note New categorical data are not automatically ordered. You must explicitly pass ordered=True to indicate an ordered Categorical.   Note The result of unique() is not always the same as Series.cat.categories, because Series.unique() has a couple of guarantees, namely that it returns categories in the order of appearance, and it only includes values that are actually present. \nIn [63]: s = pd.Series(list(\"babc\")).astype(CategoricalDtype(list(\"abcd\")))\n\nIn [64]: s\nOut[64]: \n0    b\n1    a\n2    b\n3    c\ndtype: category\nCategories (4, object): ['a', 'b', 'c', 'd']\n\n# categories\nIn [65]: s.cat.categories\nOut[65]: Index(['a', 'b', 'c', 'd'], dtype='object')\n\n# uniques\nIn [66]: s.unique()\nOut[66]: \n['b', 'a', 'c']\nCategories (4, object): ['a', 'b', 'c', 'd']\n    Renaming categories Renaming categories is done by assigning new values to the Series.cat.categories property or by using the rename_categories() method: \nIn [67]: s = pd.Series([\"a\", \"b\", \"c\", \"a\"], dtype=\"category\")\n\nIn [68]: s\nOut[68]: \n0    a\n1    b\n2    c\n3    a\ndtype: category\nCategories (3, object): ['a', 'b', 'c']\n\nIn [69]: s.cat.categories = [\"Group %s\" % g for g in s.cat.categories]\n\nIn [70]: s\nOut[70]: \n0    Group a\n1    Group b\n2    Group c\n3    Group a\ndtype: category\nCategories (3, object): ['Group a', 'Group b', 'Group c']\n\nIn [71]: s = s.cat.rename_categories([1, 2, 3])\n\nIn [72]: s\nOut[72]: \n0    1\n1    2\n2    3\n3    1\ndtype: category\nCategories (3, int64): [1, 2, 3]\n\n# You can also pass a dict-like object to map the renaming\nIn [73]: s = s.cat.rename_categories({1: \"x\", 2: \"y\", 3: \"z\"})\n\nIn [74]: s\nOut[74]: \n0    x\n1    y\n2    z\n3    x\ndtype: category\nCategories (3, object): ['x', 'y', 'z']\n   Note In contrast to R\u2019s factor, categorical data can have categories of other types than string.   Note Be aware that assigning new categories is an inplace operation, while most other operations under Series.cat per default return a new Series of dtype category.  Categories must be unique or a ValueError is raised: \nIn [75]: try:\n   ....:     s.cat.categories = [1, 1, 1]\n   ....: except ValueError as e:\n   ....:     print(\"ValueError:\", str(e))\n   ....: \nValueError: Categorical categories must be unique\n  Categories must also not be NaN or a ValueError is raised: \nIn [76]: try:\n   ....:     s.cat.categories = [1, 2, np.nan]\n   ....: except ValueError as e:\n   ....:     print(\"ValueError:\", str(e))\n   ....: \nValueError: Categorical categories cannot be null\n    Appending new categories Appending categories can be done by using the add_categories() method: \nIn [77]: s = s.cat.add_categories([4])\n\nIn [78]: s.cat.categories\nOut[78]: Index(['x', 'y', 'z', 4], dtype='object')\n\nIn [79]: s\nOut[79]: \n0    x\n1    y\n2    z\n3    x\ndtype: category\nCategories (4, object): ['x', 'y', 'z', 4]\n    Removing categories Removing categories can be done by using the remove_categories() method. Values which are removed are replaced by np.nan.: \nIn [80]: s = s.cat.remove_categories([4])\n\nIn [81]: s\nOut[81]: \n0    x\n1    y\n2    z\n3    x\ndtype: category\nCategories (3, object): ['x', 'y', 'z']\n    Removing unused categories Removing unused categories can also be done: \nIn [82]: s = pd.Series(pd.Categorical([\"a\", \"b\", \"a\"], categories=[\"a\", \"b\", \"c\", \"d\"]))\n\nIn [83]: s\nOut[83]: \n0    a\n1    b\n2    a\ndtype: category\nCategories (4, object): ['a', 'b', 'c', 'd']\n\nIn [84]: s.cat.remove_unused_categories()\nOut[84]: \n0    a\n1    b\n2    a\ndtype: category\nCategories (2, object): ['a', 'b']\n    Setting categories If you want to do remove and add new categories in one step (which has some speed advantage), or simply set the categories to a predefined scale, use set_categories(). \nIn [85]: s = pd.Series([\"one\", \"two\", \"four\", \"-\"], dtype=\"category\")\n\nIn [86]: s\nOut[86]: \n0     one\n1     two\n2    four\n3       -\ndtype: category\nCategories (4, object): ['-', 'four', 'one', 'two']\n\nIn [87]: s = s.cat.set_categories([\"one\", \"two\", \"three\", \"four\"])\n\nIn [88]: s\nOut[88]: \n0     one\n1     two\n2    four\n3     NaN\ndtype: category\nCategories (4, object): ['one', 'two', 'three', 'four']\n   Note Be aware that Categorical.set_categories() cannot know whether some category is omitted intentionally or because it is misspelled or (under Python3) due to a type difference (e.g., NumPy S1 dtype and Python strings). This can result in surprising behaviour!     Sorting and order If categorical data is ordered (s.cat.ordered == True), then the order of the categories has a meaning and certain operations are possible. If the categorical is unordered, .min()/.max() will raise a TypeError. \nIn [89]: s = pd.Series(pd.Categorical([\"a\", \"b\", \"c\", \"a\"], ordered=False))\n\nIn [90]: s.sort_values(inplace=True)\n\nIn [91]: s = pd.Series([\"a\", \"b\", \"c\", \"a\"]).astype(CategoricalDtype(ordered=True))\n\nIn [92]: s.sort_values(inplace=True)\n\nIn [93]: s\nOut[93]: \n0    a\n3    a\n1    b\n2    c\ndtype: category\nCategories (3, object): ['a' < 'b' < 'c']\n\nIn [94]: s.min(), s.max()\nOut[94]: ('a', 'c')\n  You can set categorical data to be ordered by using as_ordered() or unordered by using as_unordered(). These will by default return a new object. \nIn [95]: s.cat.as_ordered()\nOut[95]: \n0    a\n3    a\n1    b\n2    c\ndtype: category\nCategories (3, object): ['a' < 'b' < 'c']\n\nIn [96]: s.cat.as_unordered()\nOut[96]: \n0    a\n3    a\n1    b\n2    c\ndtype: category\nCategories (3, object): ['a', 'b', 'c']\n  Sorting will use the order defined by categories, not any lexical order present on the data type. This is even true for strings and numeric data: \nIn [97]: s = pd.Series([1, 2, 3, 1], dtype=\"category\")\n\nIn [98]: s = s.cat.set_categories([2, 3, 1], ordered=True)\n\nIn [99]: s\nOut[99]: \n0    1\n1    2\n2    3\n3    1\ndtype: category\nCategories (3, int64): [2 < 3 < 1]\n\nIn [100]: s.sort_values(inplace=True)\n\nIn [101]: s\nOut[101]: \n1    2\n2    3\n0    1\n3    1\ndtype: category\nCategories (3, int64): [2 < 3 < 1]\n\nIn [102]: s.min(), s.max()\nOut[102]: (2, 1)\n   Reordering Reordering the categories is possible via the Categorical.reorder_categories() and the Categorical.set_categories() methods. For Categorical.reorder_categories(), all old categories must be included in the new categories and no new categories are allowed. This will necessarily make the sort order the same as the categories order. \nIn [103]: s = pd.Series([1, 2, 3, 1], dtype=\"category\")\n\nIn [104]: s = s.cat.reorder_categories([2, 3, 1], ordered=True)\n\nIn [105]: s\nOut[105]: \n0    1\n1    2\n2    3\n3    1\ndtype: category\nCategories (3, int64): [2 < 3 < 1]\n\nIn [106]: s.sort_values(inplace=True)\n\nIn [107]: s\nOut[107]: \n1    2\n2    3\n0    1\n3    1\ndtype: category\nCategories (3, int64): [2 < 3 < 1]\n\nIn [108]: s.min(), s.max()\nOut[108]: (2, 1)\n   Note Note the difference between assigning new categories and reordering the categories: the first renames categories and therefore the individual values in the Series, but if the first position was sorted last, the renamed value will still be sorted last. Reordering means that the way values are sorted is different afterwards, but not that individual values in the Series are changed.   Note If the Categorical is not ordered, Series.min() and Series.max() will raise TypeError. Numeric operations like +, -, *, / and operations based on them (e.g. Series.median(), which would need to compute the mean between two values if the length of an array is even) do not work and raise a TypeError.    Multi column sorting A categorical dtyped column will participate in a multi-column sort in a similar manner to other columns. The ordering of the categorical is determined by the categories of that column. \nIn [109]: dfs = pd.DataFrame(\n   .....:     {\n   .....:         \"A\": pd.Categorical(\n   .....:             list(\"bbeebbaa\"),\n   .....:             categories=[\"e\", \"a\", \"b\"],\n   .....:             ordered=True,\n   .....:         ),\n   .....:         \"B\": [1, 2, 1, 2, 2, 1, 2, 1],\n   .....:     }\n   .....: )\n   .....: \n\nIn [110]: dfs.sort_values(by=[\"A\", \"B\"])\nOut[110]: \n   A  B\n2  e  1\n3  e  2\n7  a  1\n6  a  2\n0  b  1\n5  b  1\n1  b  2\n4  b  2\n  Reordering the categories changes a future sort. \nIn [111]: dfs[\"A\"] = dfs[\"A\"].cat.reorder_categories([\"a\", \"b\", \"e\"])\n\nIn [112]: dfs.sort_values(by=[\"A\", \"B\"])\nOut[112]: \n   A  B\n7  a  1\n6  a  2\n0  b  1\n5  b  1\n1  b  2\n4  b  2\n2  e  1\n3  e  2\n     Comparisons Comparing categorical data with other objects is possible in three cases:  Comparing equality (== and !=) to a list-like object (list, Series, array, \u2026) of the same length as the categorical data. All comparisons (==, !=, >, >=, <, and <=) of categorical data to another categorical Series, when ordered==True and the categories are the same. All comparisons of a categorical data to a scalar.  All other comparisons, especially \u201cnon-equality\u201d comparisons of two categoricals with different categories or a categorical with any list-like object, will raise a TypeError.  Note Any \u201cnon-equality\u201d comparisons of categorical data with a Series, np.array, list or categorical data with different categories or ordering will raise a TypeError because custom categories ordering could be interpreted in two ways: one with taking into account the ordering and one without.  \nIn [113]: cat = pd.Series([1, 2, 3]).astype(CategoricalDtype([3, 2, 1], ordered=True))\n\nIn [114]: cat_base = pd.Series([2, 2, 2]).astype(CategoricalDtype([3, 2, 1], ordered=True))\n\nIn [115]: cat_base2 = pd.Series([2, 2, 2]).astype(CategoricalDtype(ordered=True))\n\nIn [116]: cat\nOut[116]: \n0    1\n1    2\n2    3\ndtype: category\nCategories (3, int64): [3 < 2 < 1]\n\nIn [117]: cat_base\nOut[117]: \n0    2\n1    2\n2    2\ndtype: category\nCategories (3, int64): [3 < 2 < 1]\n\nIn [118]: cat_base2\nOut[118]: \n0    2\n1    2\n2    2\ndtype: category\nCategories (1, int64): [2]\n  Comparing to a categorical with the same categories and ordering or to a scalar works: \nIn [119]: cat > cat_base\nOut[119]: \n0     True\n1    False\n2    False\ndtype: bool\n\nIn [120]: cat > 2\nOut[120]: \n0     True\n1    False\n2    False\ndtype: bool\n  Equality comparisons work with any list-like object of same length and scalars: \nIn [121]: cat == cat_base\nOut[121]: \n0    False\n1     True\n2    False\ndtype: bool\n\nIn [122]: cat == np.array([1, 2, 3])\nOut[122]: \n0    True\n1    True\n2    True\ndtype: bool\n\nIn [123]: cat == 2\nOut[123]: \n0    False\n1     True\n2    False\ndtype: bool\n  This doesn\u2019t work because the categories are not the same: \nIn [124]: try:\n   .....:     cat > cat_base2\n   .....: except TypeError as e:\n   .....:     print(\"TypeError:\", str(e))\n   .....: \nTypeError: Categoricals can only be compared if 'categories' are the same.\n  If you want to do a \u201cnon-equality\u201d comparison of a categorical series with a list-like object which is not categorical data, you need to be explicit and convert the categorical data back to the original values: \nIn [125]: base = np.array([1, 2, 3])\n\nIn [126]: try:\n   .....:     cat > base\n   .....: except TypeError as e:\n   .....:     print(\"TypeError:\", str(e))\n   .....: \nTypeError: Cannot compare a Categorical for op __gt__ with type <class 'numpy.ndarray'>.\nIf you want to compare values, use 'np.asarray(cat) <op> other'.\n\nIn [127]: np.asarray(cat) > base\nOut[127]: array([False, False, False])\n  When you compare two unordered categoricals with the same categories, the order is not considered: \nIn [128]: c1 = pd.Categorical([\"a\", \"b\"], categories=[\"a\", \"b\"], ordered=False)\n\nIn [129]: c2 = pd.Categorical([\"a\", \"b\"], categories=[\"b\", \"a\"], ordered=False)\n\nIn [130]: c1 == c2\nOut[130]: array([ True,  True])\n    Operations Apart from Series.min(), Series.max() and Series.mode(), the following operations are possible with categorical data: Series methods like Series.value_counts() will use all categories, even if some categories are not present in the data: \nIn [131]: s = pd.Series(pd.Categorical([\"a\", \"b\", \"c\", \"c\"], categories=[\"c\", \"a\", \"b\", \"d\"]))\n\nIn [132]: s.value_counts()\nOut[132]: \nc    2\na    1\nb    1\nd    0\ndtype: int64\n  DataFrame methods like DataFrame.sum() also show \u201cunused\u201d categories. \nIn [133]: columns = pd.Categorical(\n   .....:     [\"One\", \"One\", \"Two\"], categories=[\"One\", \"Two\", \"Three\"], ordered=True\n   .....: )\n   .....: \n\nIn [134]: df = pd.DataFrame(\n   .....:     data=[[1, 2, 3], [4, 5, 6]],\n   .....:     columns=pd.MultiIndex.from_arrays([[\"A\", \"B\", \"B\"], columns]),\n   .....: )\n   .....: \n\nIn [135]: df.groupby(axis=1, level=1).sum()\nOut[135]: \n   One  Two  Three\n0    3    3      0\n1    9    6      0\n  Groupby will also show \u201cunused\u201d categories: \nIn [136]: cats = pd.Categorical(\n   .....:     [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\"], categories=[\"a\", \"b\", \"c\", \"d\"]\n   .....: )\n   .....: \n\nIn [137]: df = pd.DataFrame({\"cats\": cats, \"values\": [1, 2, 2, 2, 3, 4, 5]})\n\nIn [138]: df.groupby(\"cats\").mean()\nOut[138]: \n      values\ncats        \na        1.0\nb        2.0\nc        4.0\nd        NaN\n\nIn [139]: cats2 = pd.Categorical([\"a\", \"a\", \"b\", \"b\"], categories=[\"a\", \"b\", \"c\"])\n\nIn [140]: df2 = pd.DataFrame(\n   .....:     {\n   .....:         \"cats\": cats2,\n   .....:         \"B\": [\"c\", \"d\", \"c\", \"d\"],\n   .....:         \"values\": [1, 2, 3, 4],\n   .....:     }\n   .....: )\n   .....: \n\nIn [141]: df2.groupby([\"cats\", \"B\"]).mean()\nOut[141]: \n        values\ncats B        \na    c     1.0\n     d     2.0\nb    c     3.0\n     d     4.0\nc    c     NaN\n     d     NaN\n  Pivot tables: \nIn [142]: raw_cat = pd.Categorical([\"a\", \"a\", \"b\", \"b\"], categories=[\"a\", \"b\", \"c\"])\n\nIn [143]: df = pd.DataFrame({\"A\": raw_cat, \"B\": [\"c\", \"d\", \"c\", \"d\"], \"values\": [1, 2, 3, 4]})\n\nIn [144]: pd.pivot_table(df, values=\"values\", index=[\"A\", \"B\"])\nOut[144]: \n     values\nA B        \na c       1\n  d       2\nb c       3\n  d       4\n    Data munging The optimized pandas data access methods .loc, .iloc, .at, and .iat, work as normal. The only difference is the return type (for getting) and that only values already in categories can be assigned.  Getting If the slicing operation returns either a DataFrame or a column of type Series, the category dtype is preserved. \nIn [145]: idx = pd.Index([\"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\"])\n\nIn [146]: cats = pd.Series([\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\"], dtype=\"category\", index=idx)\n\nIn [147]: values = [1, 2, 2, 2, 3, 4, 5]\n\nIn [148]: df = pd.DataFrame({\"cats\": cats, \"values\": values}, index=idx)\n\nIn [149]: df.iloc[2:4, :]\nOut[149]: \n  cats  values\nj    b       2\nk    b       2\n\nIn [150]: df.iloc[2:4, :].dtypes\nOut[150]: \ncats      category\nvalues       int64\ndtype: object\n\nIn [151]: df.loc[\"h\":\"j\", \"cats\"]\nOut[151]: \nh    a\ni    b\nj    b\nName: cats, dtype: category\nCategories (3, object): ['a', 'b', 'c']\n\nIn [152]: df[df[\"cats\"] == \"b\"]\nOut[152]: \n  cats  values\ni    b       2\nj    b       2\nk    b       2\n  An example where the category type is not preserved is if you take one single row: the resulting Series is of dtype object: \n# get the complete \"h\" row as a Series\nIn [153]: df.loc[\"h\", :]\nOut[153]: \ncats      a\nvalues    1\nName: h, dtype: object\n  Returning a single item from categorical data will also return the value, not a categorical of length \u201c1\u201d. \nIn [154]: df.iat[0, 0]\nOut[154]: 'a'\n\nIn [155]: df[\"cats\"].cat.categories = [\"x\", \"y\", \"z\"]\n\nIn [156]: df.at[\"h\", \"cats\"]  # returns a string\nOut[156]: 'x'\n   Note The is in contrast to R\u2019s factor function, where factor(c(1,2,3))[1] returns a single value factor.  To get a single value Series of type category, you pass in a list with a single value: \nIn [157]: df.loc[[\"h\"], \"cats\"]\nOut[157]: \nh    x\nName: cats, dtype: category\nCategories (3, object): ['x', 'y', 'z']\n    String and datetime accessors The accessors .dt and .str will work if the s.cat.categories are of an appropriate type: \nIn [158]: str_s = pd.Series(list(\"aabb\"))\n\nIn [159]: str_cat = str_s.astype(\"category\")\n\nIn [160]: str_cat\nOut[160]: \n0    a\n1    a\n2    b\n3    b\ndtype: category\nCategories (2, object): ['a', 'b']\n\nIn [161]: str_cat.str.contains(\"a\")\nOut[161]: \n0     True\n1     True\n2    False\n3    False\ndtype: bool\n\nIn [162]: date_s = pd.Series(pd.date_range(\"1/1/2015\", periods=5))\n\nIn [163]: date_cat = date_s.astype(\"category\")\n\nIn [164]: date_cat\nOut[164]: \n0   2015-01-01\n1   2015-01-02\n2   2015-01-03\n3   2015-01-04\n4   2015-01-05\ndtype: category\nCategories (5, datetime64[ns]): [2015-01-01, 2015-01-02, 2015-01-03, 2015-01-04, 2015-01-05]\n\nIn [165]: date_cat.dt.day\nOut[165]: \n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n   Note The returned Series (or DataFrame) is of the same type as if you used the .str.<method> / .dt.<method> on a Series of that type (and not of type category!).  That means, that the returned values from methods and properties on the accessors of a Series and the returned values from methods and properties on the accessors of this Series transformed to one of type category will be equal: \nIn [166]: ret_s = str_s.str.contains(\"a\")\n\nIn [167]: ret_cat = str_cat.str.contains(\"a\")\n\nIn [168]: ret_s.dtype == ret_cat.dtype\nOut[168]: True\n\nIn [169]: ret_s == ret_cat\nOut[169]: \n0    True\n1    True\n2    True\n3    True\ndtype: bool\n   Note The work is done on the categories and then a new Series is constructed. This has some performance implication if you have a Series of type string, where lots of elements are repeated (i.e. the number of unique elements in the Series is a lot smaller than the length of the Series). In this case it can be faster to convert the original Series to one of type category and use .str.<method> or .dt.<property> on that.    Setting Setting values in a categorical column (or Series) works as long as the value is included in the categories: \nIn [170]: idx = pd.Index([\"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\"])\n\nIn [171]: cats = pd.Categorical([\"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\"], categories=[\"a\", \"b\"])\n\nIn [172]: values = [1, 1, 1, 1, 1, 1, 1]\n\nIn [173]: df = pd.DataFrame({\"cats\": cats, \"values\": values}, index=idx)\n\nIn [174]: df.iloc[2:4, :] = [[\"b\", 2], [\"b\", 2]]\n\nIn [175]: df\nOut[175]: \n  cats  values\nh    a       1\ni    a       1\nj    b       2\nk    b       2\nl    a       1\nm    a       1\nn    a       1\n\nIn [176]: try:\n   .....:     df.iloc[2:4, :] = [[\"c\", 3], [\"c\", 3]]\n   .....: except TypeError as e:\n   .....:     print(\"TypeError:\", str(e))\n   .....: \nTypeError: Cannot setitem on a Categorical with a new category, set the categories first\n  Setting values by assigning categorical data will also check that the categories match: \nIn [177]: df.loc[\"j\":\"k\", \"cats\"] = pd.Categorical([\"a\", \"a\"], categories=[\"a\", \"b\"])\n\nIn [178]: df\nOut[178]: \n  cats  values\nh    a       1\ni    a       1\nj    a       2\nk    a       2\nl    a       1\nm    a       1\nn    a       1\n\nIn [179]: try:\n   .....:     df.loc[\"j\":\"k\", \"cats\"] = pd.Categorical([\"b\", \"b\"], categories=[\"a\", \"b\", \"c\"])\n   .....: except TypeError as e:\n   .....:     print(\"TypeError:\", str(e))\n   .....: \nTypeError: Cannot set a Categorical with another, without identical categories\n  Assigning a Categorical to parts of a column of other types will use the values: \nIn [180]: df = pd.DataFrame({\"a\": [1, 1, 1, 1, 1], \"b\": [\"a\", \"a\", \"a\", \"a\", \"a\"]})\n\nIn [181]: df.loc[1:2, \"a\"] = pd.Categorical([\"b\", \"b\"], categories=[\"a\", \"b\"])\n\nIn [182]: df.loc[2:3, \"b\"] = pd.Categorical([\"b\", \"b\"], categories=[\"a\", \"b\"])\n\nIn [183]: df\nOut[183]: \n   a  b\n0  1  a\n1  b  a\n2  b  b\n3  1  b\n4  1  a\n\nIn [184]: df.dtypes\nOut[184]: \na    object\nb    object\ndtype: object\n    Merging / concatenation By default, combining Series or DataFrames which contain the same categories results in category dtype, otherwise results will depend on the dtype of the underlying categories. Merges that result in non-categorical dtypes will likely have higher memory usage. Use .astype or union_categoricals to ensure category results. \nIn [185]: from pandas.api.types import union_categoricals\n\n# same categories\nIn [186]: s1 = pd.Series([\"a\", \"b\"], dtype=\"category\")\n\nIn [187]: s2 = pd.Series([\"a\", \"b\", \"a\"], dtype=\"category\")\n\nIn [188]: pd.concat([s1, s2])\nOut[188]: \n0    a\n1    b\n0    a\n1    b\n2    a\ndtype: category\nCategories (2, object): ['a', 'b']\n\n# different categories\nIn [189]: s3 = pd.Series([\"b\", \"c\"], dtype=\"category\")\n\nIn [190]: pd.concat([s1, s3])\nOut[190]: \n0    a\n1    b\n0    b\n1    c\ndtype: object\n\n# Output dtype is inferred based on categories values\nIn [191]: int_cats = pd.Series([1, 2], dtype=\"category\")\n\nIn [192]: float_cats = pd.Series([3.0, 4.0], dtype=\"category\")\n\nIn [193]: pd.concat([int_cats, float_cats])\nOut[193]: \n0    1.0\n1    2.0\n0    3.0\n1    4.0\ndtype: float64\n\nIn [194]: pd.concat([s1, s3]).astype(\"category\")\nOut[194]: \n0    a\n1    b\n0    b\n1    c\ndtype: category\nCategories (3, object): ['a', 'b', 'c']\n\nIn [195]: union_categoricals([s1.array, s3.array])\nOut[195]: \n['a', 'b', 'b', 'c']\nCategories (3, object): ['a', 'b', 'c']\n  The following table summarizes the results of merging Categoricals:         \narg1 arg2 identical result    \ncategory category True category  \ncategory (object) category (object) False object (dtype is inferred)  \ncategory (int) category (float) False float (dtype is inferred)    See also the section on merge dtypes for notes about preserving merge dtypes and performance.   Unioning If you want to combine categoricals that do not necessarily have the same categories, the union_categoricals() function will combine a list-like of categoricals. The new categories will be the union of the categories being combined. \nIn [196]: from pandas.api.types import union_categoricals\n\nIn [197]: a = pd.Categorical([\"b\", \"c\"])\n\nIn [198]: b = pd.Categorical([\"a\", \"b\"])\n\nIn [199]: union_categoricals([a, b])\nOut[199]: \n['b', 'c', 'a', 'b']\nCategories (3, object): ['b', 'c', 'a']\n  By default, the resulting categories will be ordered as they appear in the data. If you want the categories to be lexsorted, use sort_categories=True argument. \nIn [200]: union_categoricals([a, b], sort_categories=True)\nOut[200]: \n['b', 'c', 'a', 'b']\nCategories (3, object): ['a', 'b', 'c']\n  union_categoricals also works with the \u201ceasy\u201d case of combining two categoricals of the same categories and order information (e.g. what you could also append for). \nIn [201]: a = pd.Categorical([\"a\", \"b\"], ordered=True)\n\nIn [202]: b = pd.Categorical([\"a\", \"b\", \"a\"], ordered=True)\n\nIn [203]: union_categoricals([a, b])\nOut[203]: \n['a', 'b', 'a', 'b', 'a']\nCategories (2, object): ['a' < 'b']\n  The below raises TypeError because the categories are ordered and not identical. \nIn [1]: a = pd.Categorical([\"a\", \"b\"], ordered=True)\nIn [2]: b = pd.Categorical([\"a\", \"b\", \"c\"], ordered=True)\nIn [3]: union_categoricals([a, b])\nOut[3]:\nTypeError: to union ordered Categoricals, all categories must be the same\n  Ordered categoricals with different categories or orderings can be combined by using the ignore_ordered=True argument. \nIn [204]: a = pd.Categorical([\"a\", \"b\", \"c\"], ordered=True)\n\nIn [205]: b = pd.Categorical([\"c\", \"b\", \"a\"], ordered=True)\n\nIn [206]: union_categoricals([a, b], ignore_order=True)\nOut[206]: \n['a', 'b', 'c', 'c', 'b', 'a']\nCategories (3, object): ['a', 'b', 'c']\n  union_categoricals() also works with a CategoricalIndex, or Series containing categorical data, but note that the resulting array will always be a plain Categorical: \nIn [207]: a = pd.Series([\"b\", \"c\"], dtype=\"category\")\n\nIn [208]: b = pd.Series([\"a\", \"b\"], dtype=\"category\")\n\nIn [209]: union_categoricals([a, b])\nOut[209]: \n['b', 'c', 'a', 'b']\nCategories (3, object): ['b', 'c', 'a']\n   Note union_categoricals may recode the integer codes for categories when combining categoricals. This is likely what you want, but if you are relying on the exact numbering of the categories, be aware. \nIn [210]: c1 = pd.Categorical([\"b\", \"c\"])\n\nIn [211]: c2 = pd.Categorical([\"a\", \"b\"])\n\nIn [212]: c1\nOut[212]: \n['b', 'c']\nCategories (2, object): ['b', 'c']\n\n# \"b\" is coded to 0\nIn [213]: c1.codes\nOut[213]: array([0, 1], dtype=int8)\n\nIn [214]: c2\nOut[214]: \n['a', 'b']\nCategories (2, object): ['a', 'b']\n\n# \"b\" is coded to 1\nIn [215]: c2.codes\nOut[215]: array([0, 1], dtype=int8)\n\nIn [216]: c = union_categoricals([c1, c2])\n\nIn [217]: c\nOut[217]: \n['b', 'c', 'a', 'b']\nCategories (3, object): ['b', 'c', 'a']\n\n# \"b\" is coded to 0 throughout, same as c1, different from c2\nIn [218]: c.codes\nOut[218]: array([0, 1, 2, 0], dtype=int8)\n      Getting data in/out You can write data that contains category dtypes to a HDFStore. See here for an example and caveats. It is also possible to write data to and reading data from Stata format files. See here for an example and caveats. Writing to a CSV file will convert the data, effectively removing any information about the categorical (categories and ordering). So if you read back the CSV file you have to convert the relevant columns back to category and assign the right categories and categories ordering. \nIn [219]: import io\n\nIn [220]: s = pd.Series(pd.Categorical([\"a\", \"b\", \"b\", \"a\", \"a\", \"d\"]))\n\n# rename the categories\nIn [221]: s.cat.categories = [\"very good\", \"good\", \"bad\"]\n\n# reorder the categories and add missing categories\nIn [222]: s = s.cat.set_categories([\"very bad\", \"bad\", \"medium\", \"good\", \"very good\"])\n\nIn [223]: df = pd.DataFrame({\"cats\": s, \"vals\": [1, 2, 3, 4, 5, 6]})\n\nIn [224]: csv = io.StringIO()\n\nIn [225]: df.to_csv(csv)\n\nIn [226]: df2 = pd.read_csv(io.StringIO(csv.getvalue()))\n\nIn [227]: df2.dtypes\nOut[227]: \nUnnamed: 0     int64\ncats          object\nvals           int64\ndtype: object\n\nIn [228]: df2[\"cats\"]\nOut[228]: \n0    very good\n1         good\n2         good\n3    very good\n4    very good\n5          bad\nName: cats, dtype: object\n\n# Redo the category\nIn [229]: df2[\"cats\"] = df2[\"cats\"].astype(\"category\")\n\nIn [230]: df2[\"cats\"].cat.set_categories(\n   .....:     [\"very bad\", \"bad\", \"medium\", \"good\", \"very good\"], inplace=True\n   .....: )\n   .....: \n\nIn [231]: df2.dtypes\nOut[231]: \nUnnamed: 0       int64\ncats          category\nvals             int64\ndtype: object\n\nIn [232]: df2[\"cats\"]\nOut[232]: \n0    very good\n1         good\n2         good\n3    very good\n4    very good\n5          bad\nName: cats, dtype: category\nCategories (5, object): ['very bad', 'bad', 'medium', 'good', 'very good']\n  The same holds for writing to a SQL database with to_sql.   Missing data pandas primarily uses the value np.nan to represent missing data. It is by default not included in computations. See the Missing Data section. Missing values should not be included in the Categorical\u2019s categories, only in the values. Instead, it is understood that NaN is different, and is always a possibility. When working with the Categorical\u2019s codes, missing values will always have a code of -1. \nIn [233]: s = pd.Series([\"a\", \"b\", np.nan, \"a\"], dtype=\"category\")\n\n# only two categories\nIn [234]: s\nOut[234]: \n0      a\n1      b\n2    NaN\n3      a\ndtype: category\nCategories (2, object): ['a', 'b']\n\nIn [235]: s.cat.codes\nOut[235]: \n0    0\n1    1\n2   -1\n3    0\ndtype: int8\n  Methods for working with missing data, e.g. isna(), fillna(), dropna(), all work normally: \nIn [236]: s = pd.Series([\"a\", \"b\", np.nan], dtype=\"category\")\n\nIn [237]: s\nOut[237]: \n0      a\n1      b\n2    NaN\ndtype: category\nCategories (2, object): ['a', 'b']\n\nIn [238]: pd.isna(s)\nOut[238]: \n0    False\n1    False\n2     True\ndtype: bool\n\nIn [239]: s.fillna(\"a\")\nOut[239]: \n0    a\n1    b\n2    a\ndtype: category\nCategories (2, object): ['a', 'b']\n    Differences to R\u2019s factor\n The following differences to R\u2019s factor functions can be observed:  R\u2019s levels are named categories. R\u2019s levels are always of type string, while categories in pandas can be of any dtype. It\u2019s not possible to specify labels at creation time. Use s.cat.rename_categories(new_labels) afterwards. In contrast to R\u2019s factor function, using categorical data as the sole input to create a new categorical series will not remove unused categories but create a new categorical series which is equal to the passed in one! R allows for missing values to be included in its levels (pandas\u2019 categories). pandas does not allow NaN categories, but missing values can still be in the values.    Gotchas  Memory usage The memory usage of a Categorical is proportional to the number of categories plus the length of the data. In contrast, an object dtype is a constant times the length of the data. \nIn [240]: s = pd.Series([\"foo\", \"bar\"] * 1000)\n\n# object dtype\nIn [241]: s.nbytes\nOut[241]: 16000\n\n# category dtype\nIn [242]: s.astype(\"category\").nbytes\nOut[242]: 2016\n   Note If the number of categories approaches the length of the data, the Categorical will use nearly the same or more memory than an equivalent object dtype representation. \nIn [243]: s = pd.Series([\"foo%04d\" % i for i in range(2000)])\n\n# object dtype\nIn [244]: s.nbytes\nOut[244]: 16000\n\n# category dtype\nIn [245]: s.astype(\"category\").nbytes\nOut[245]: 20000\n     \nCategorical is not a numpy array Currently, categorical data and the underlying Categorical is implemented as a Python object and not as a low-level NumPy array dtype. This leads to some problems. NumPy itself doesn\u2019t know about the new dtype: \nIn [246]: try:\n   .....:     np.dtype(\"category\")\n   .....: except TypeError as e:\n   .....:     print(\"TypeError:\", str(e))\n   .....: \nTypeError: data type 'category' not understood\n\nIn [247]: dtype = pd.Categorical([\"a\"]).dtype\n\nIn [248]: try:\n   .....:     np.dtype(dtype)\n   .....: except TypeError as e:\n   .....:     print(\"TypeError:\", str(e))\n   .....: \nTypeError: Cannot interpret 'CategoricalDtype(categories=['a'], ordered=False)' as a data type\n  Dtype comparisons work: \nIn [249]: dtype == np.str_\nOut[249]: False\n\nIn [250]: np.str_ == dtype\nOut[250]: False\n  To check if a Series contains Categorical data, use hasattr(s, 'cat'): \nIn [251]: hasattr(pd.Series([\"a\"], dtype=\"category\"), \"cat\")\nOut[251]: True\n\nIn [252]: hasattr(pd.Series([\"a\"]), \"cat\")\nOut[252]: False\n  Using NumPy functions on a Series of type category should not work as Categoricals are not numeric data (even in the case that .categories is numeric). \nIn [253]: s = pd.Series(pd.Categorical([1, 2, 3, 4]))\n\nIn [254]: try:\n   .....:     np.sum(s)\n   .....: except TypeError as e:\n   .....:     print(\"TypeError:\", str(e))\n   .....: \nTypeError: 'Categorical' with dtype category does not support reduction 'sum'\n   Note If such a function works, please file a bug at https://github.com/pandas-dev/pandas!    dtype in apply pandas currently does not preserve the dtype in apply functions: If you apply along rows you get a Series of object dtype (same as getting a row -> getting one element will return a basic type) and applying along columns will also convert to object. NaN values are unaffected. You can use fillna to handle missing values before applying a function. \nIn [255]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"a\": [1, 2, 3, 4],\n   .....:         \"b\": [\"a\", \"b\", \"c\", \"d\"],\n   .....:         \"cats\": pd.Categorical([1, 2, 3, 2]),\n   .....:     }\n   .....: )\n   .....: \n\nIn [256]: df.apply(lambda row: type(row[\"cats\"]), axis=1)\nOut[256]: \n0    <class 'int'>\n1    <class 'int'>\n2    <class 'int'>\n3    <class 'int'>\ndtype: object\n\nIn [257]: df.apply(lambda col: col.dtype, axis=0)\nOut[257]: \na          int64\nb         object\ncats    category\ndtype: object\n    Categorical index CategoricalIndex is a type of index that is useful for supporting indexing with duplicates. This is a container around a Categorical and allows efficient indexing and storage of an index with a large number of duplicated elements. See the advanced indexing docs for a more detailed explanation. Setting the index will create a CategoricalIndex: \nIn [258]: cats = pd.Categorical([1, 2, 3, 4], categories=[4, 2, 3, 1])\n\nIn [259]: strings = [\"a\", \"b\", \"c\", \"d\"]\n\nIn [260]: values = [4, 2, 3, 1]\n\nIn [261]: df = pd.DataFrame({\"strings\": strings, \"values\": values}, index=cats)\n\nIn [262]: df.index\nOut[262]: CategoricalIndex([1, 2, 3, 4], categories=[4, 2, 3, 1], ordered=False, dtype='category')\n\n# This now sorts by the categories order\nIn [263]: df.sort_index()\nOut[263]: \n  strings  values\n4       d       1\n2       b       2\n3       c       3\n1       a       4\n    Side effects Constructing a Series from a Categorical will not copy the input Categorical. This means that changes to the Series will in most cases change the original Categorical: \nIn [264]: cat = pd.Categorical([1, 2, 3, 10], categories=[1, 2, 3, 4, 10])\n\nIn [265]: s = pd.Series(cat, name=\"cat\")\n\nIn [266]: cat\nOut[266]: \n[1, 2, 3, 10]\nCategories (5, int64): [1, 2, 3, 4, 10]\n\nIn [267]: s.iloc[0:2] = 10\n\nIn [268]: cat\nOut[268]: \n[10, 10, 3, 10]\nCategories (5, int64): [1, 2, 3, 4, 10]\n\nIn [269]: df = pd.DataFrame(s)\n\nIn [270]: df[\"cat\"].cat.categories = [1, 2, 3, 4, 5]\n\nIn [271]: cat\nOut[271]: \n[10, 10, 3, 10]\nCategories (5, int64): [1, 2, 3, 4, 10]\n  Use copy=True to prevent such a behaviour or simply don\u2019t reuse Categoricals: \nIn [272]: cat = pd.Categorical([1, 2, 3, 10], categories=[1, 2, 3, 4, 10])\n\nIn [273]: s = pd.Series(cat, name=\"cat\", copy=True)\n\nIn [274]: cat\nOut[274]: \n[1, 2, 3, 10]\nCategories (5, int64): [1, 2, 3, 4, 10]\n\nIn [275]: s.iloc[0:2] = 10\n\nIn [276]: cat\nOut[276]: \n[1, 2, 3, 10]\nCategories (5, int64): [1, 2, 3, 4, 10]\n   Note This also happens in some cases when you supply a NumPy array instead of a Categorical: using an int array (e.g. np.array([1,2,3,4])) will exhibit the same behavior, while using a string array (e.g. np.array([\"a\",\"b\",\"c\",\"a\"])) will not.   \n"}, {"name": "Chart Visualization", "path": "user_guide/visualization", "type": "Manual", "text": "Chart Visualization This section demonstrates visualization through charting. For information on visualization of tabular data please see the section on Table Visualization. We use the standard convention for referencing the matplotlib API: \nIn [1]: import matplotlib.pyplot as plt\n\nIn [2]: plt.close(\"all\")\n  We provide the basics in pandas to easily create decent looking plots. See the ecosystem section for visualization libraries that go beyond the basics documented here.  Note All calls to np.random are seeded with 123456.   Basic plotting: plot\n We will demonstrate the basics, see the cookbook for some advanced strategies. The plot method on Series and DataFrame is just a simple wrapper around plt.plot(): \nIn [3]: ts = pd.Series(np.random.randn(1000), index=pd.date_range(\"1/1/2000\", periods=1000))\n\nIn [4]: ts = ts.cumsum()\n\nIn [5]: ts.plot();\n   If the index consists of dates, it calls gcf().autofmt_xdate() to try to format the x-axis nicely as per above. On DataFrame, plot() is a convenience to plot all of the columns with labels: \nIn [6]: df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list(\"ABCD\"))\n\nIn [7]: df = df.cumsum()\n\nIn [8]: plt.figure();\n\nIn [9]: df.plot();\n   You can plot one column versus another using the x and y keywords in plot(): \nIn [10]: df3 = pd.DataFrame(np.random.randn(1000, 2), columns=[\"B\", \"C\"]).cumsum()\n\nIn [11]: df3[\"A\"] = pd.Series(list(range(len(df))))\n\nIn [12]: df3.plot(x=\"A\", y=\"B\");\n    Note For more formatting and styling options, see formatting below.    Other plots Plotting methods allow for a handful of plot styles other than the default line plot. These methods can be provided as the kind keyword argument to plot(), and include:  \u2018bar\u2019 or \u2018barh\u2019 for bar plots \u2018hist\u2019 for histogram \u2018box\u2019 for boxplot \u2018kde\u2019 or \u2018density\u2019 for density plots \u2018area\u2019 for area plots \u2018scatter\u2019 for scatter plots \u2018hexbin\u2019 for hexagonal bin plots \u2018pie\u2019 for pie plots  For example, a bar plot can be created the following way: \nIn [13]: plt.figure();\n\nIn [14]: df.iloc[5].plot(kind=\"bar\");\n   You can also create these other plots using the methods DataFrame.plot.<kind> instead of providing the kind keyword argument. This makes it easier to discover plot methods and the specific arguments they use: \nIn [15]: df = pd.DataFrame()\n\nIn [16]: df.plot.<TAB>  # noqa: E225, E999\ndf.plot.area     df.plot.barh     df.plot.density  df.plot.hist     df.plot.line     df.plot.scatter\ndf.plot.bar      df.plot.box      df.plot.hexbin   df.plot.kde      df.plot.pie\n  In addition to these kind s, there are the DataFrame.hist(), and DataFrame.boxplot() methods, which use a separate interface. Finally, there are several plotting functions in pandas.plotting that take a Series or DataFrame as an argument. These include:  Scatter Matrix Andrews Curves Parallel Coordinates Lag Plot Autocorrelation Plot Bootstrap Plot RadViz  Plots may also be adorned with errorbars or tables.  Bar plots For labeled, non-time series data, you may wish to produce a bar plot: \nIn [17]: plt.figure();\n\nIn [18]: df.iloc[5].plot.bar();\n\nIn [19]: plt.axhline(0, color=\"k\");\n   Calling a DataFrame\u2019s plot.bar() method produces a multiple bar plot: \nIn [20]: df2 = pd.DataFrame(np.random.rand(10, 4), columns=[\"a\", \"b\", \"c\", \"d\"])\n\nIn [21]: df2.plot.bar();\n   To produce a stacked bar plot, pass stacked=True: \nIn [22]: df2.plot.bar(stacked=True);\n   To get horizontal bar plots, use the barh method: \nIn [23]: df2.plot.barh(stacked=True);\n     Histograms Histograms can be drawn by using the DataFrame.plot.hist() and Series.plot.hist() methods. \nIn [24]: df4 = pd.DataFrame(\n   ....:     {\n   ....:         \"a\": np.random.randn(1000) + 1,\n   ....:         \"b\": np.random.randn(1000),\n   ....:         \"c\": np.random.randn(1000) - 1,\n   ....:     },\n   ....:     columns=[\"a\", \"b\", \"c\"],\n   ....: )\n   ....: \n\nIn [25]: plt.figure();\n\nIn [26]: df4.plot.hist(alpha=0.5);\n   A histogram can be stacked using stacked=True. Bin size can be changed using the bins keyword. \nIn [27]: plt.figure();\n\nIn [28]: df4.plot.hist(stacked=True, bins=20);\n   You can pass other keywords supported by matplotlib hist. For example, horizontal and cumulative histograms can be drawn by orientation='horizontal' and cumulative=True. \nIn [29]: plt.figure();\n\nIn [30]: df4[\"a\"].plot.hist(orientation=\"horizontal\", cumulative=True);\n   See the hist method and the matplotlib hist documentation for more. The existing interface DataFrame.hist to plot histogram still can be used. \nIn [31]: plt.figure();\n\nIn [32]: df[\"A\"].diff().hist();\n   DataFrame.hist() plots the histograms of the columns on multiple subplots: \nIn [33]: plt.figure();\n\nIn [34]: df.diff().hist(color=\"k\", alpha=0.5, bins=50);\n   The by keyword can be specified to plot grouped histograms: \nIn [35]: data = pd.Series(np.random.randn(1000))\n\nIn [36]: data.hist(by=np.random.randint(0, 4, 1000), figsize=(6, 4));\n   In addition, the by keyword can also be specified in DataFrame.plot.hist().  Changed in version 1.4.0.  \nIn [37]: data = pd.DataFrame(\n   ....:     {\n   ....:         \"a\": np.random.choice([\"x\", \"y\", \"z\"], 1000),\n   ....:         \"b\": np.random.choice([\"e\", \"f\", \"g\"], 1000),\n   ....:         \"c\": np.random.randn(1000),\n   ....:         \"d\": np.random.randn(1000) - 1,\n   ....:     },\n   ....: )\n   ....: \n\nIn [38]: data.plot.hist(by=[\"a\", \"b\"], figsize=(10, 5));\n     Box plots Boxplot can be drawn calling Series.plot.box() and DataFrame.plot.box(), or DataFrame.boxplot() to visualize the distribution of values within each column. For instance, here is a boxplot representing five trials of 10 observations of a uniform random variable on [0,1). \nIn [39]: df = pd.DataFrame(np.random.rand(10, 5), columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\nIn [40]: df.plot.box();\n   Boxplot can be colorized by passing color keyword. You can pass a dict whose keys are boxes, whiskers, medians and caps. If some keys are missing in the dict, default colors are used for the corresponding artists. Also, boxplot has sym keyword to specify fliers style. When you pass other type of arguments via color keyword, it will be directly passed to matplotlib for all the boxes, whiskers, medians and caps colorization. The colors are applied to every boxes to be drawn. If you want more complicated colorization, you can get each drawn artists by passing return_type. \nIn [41]: color = {\n   ....:     \"boxes\": \"DarkGreen\",\n   ....:     \"whiskers\": \"DarkOrange\",\n   ....:     \"medians\": \"DarkBlue\",\n   ....:     \"caps\": \"Gray\",\n   ....: }\n   ....: \n\nIn [42]: df.plot.box(color=color, sym=\"r+\");\n   Also, you can pass other keywords supported by matplotlib boxplot. For example, horizontal and custom-positioned boxplot can be drawn by vert=False and positions keywords. \nIn [43]: df.plot.box(vert=False, positions=[1, 4, 5, 6, 8]);\n   See the boxplot method and the matplotlib boxplot documentation for more. The existing interface DataFrame.boxplot to plot boxplot still can be used. \nIn [44]: df = pd.DataFrame(np.random.rand(10, 5))\n\nIn [45]: plt.figure();\n\nIn [46]: bp = df.boxplot()\n   You can create a stratified boxplot using the by keyword argument to create groupings. For instance, \nIn [47]: df = pd.DataFrame(np.random.rand(10, 2), columns=[\"Col1\", \"Col2\"])\n\nIn [48]: df[\"X\"] = pd.Series([\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\"])\n\nIn [49]: plt.figure();\n\nIn [50]: bp = df.boxplot(by=\"X\")\n   You can also pass a subset of columns to plot, as well as group by multiple columns: \nIn [51]: df = pd.DataFrame(np.random.rand(10, 3), columns=[\"Col1\", \"Col2\", \"Col3\"])\n\nIn [52]: df[\"X\"] = pd.Series([\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\"])\n\nIn [53]: df[\"Y\"] = pd.Series([\"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\"])\n\nIn [54]: plt.figure();\n\nIn [55]: bp = df.boxplot(column=[\"Col1\", \"Col2\"], by=[\"X\", \"Y\"])\n   You could also create groupings with DataFrame.plot.box(), for instance:  Changed in version 1.4.0.  \nIn [56]: df = pd.DataFrame(np.random.rand(10, 3), columns=[\"Col1\", \"Col2\", \"Col3\"])\n\nIn [57]: df[\"X\"] = pd.Series([\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\"])\n\nIn [58]: plt.figure();\n\nIn [59]: bp = df.plot.box(column=[\"Col1\", \"Col2\"], by=\"X\")\n   In boxplot, the return type can be controlled by the return_type, keyword. The valid choices are {\"axes\", \"dict\", \"both\", None}. Faceting, created by DataFrame.boxplot with the by keyword, will affect the output type as well:        \nreturn_type Faceted Output type    \nNone No axes  \nNone Yes 2-D ndarray of axes  \n'axes' No axes  \n'axes' Yes Series of axes  \n'dict' No dict of artists  \n'dict' Yes Series of dicts of artists  \n'both' No namedtuple  \n'both' Yes Series of namedtuples    Groupby.boxplot always returns a Series of return_type. \nIn [60]: np.random.seed(1234)\n\nIn [61]: df_box = pd.DataFrame(np.random.randn(50, 2))\n\nIn [62]: df_box[\"g\"] = np.random.choice([\"A\", \"B\"], size=50)\n\nIn [63]: df_box.loc[df_box[\"g\"] == \"B\", 1] += 3\n\nIn [64]: bp = df_box.boxplot(by=\"g\")\n   The subplots above are split by the numeric columns first, then the value of the g column. Below the subplots are first split by the value of g, then by the numeric columns. \nIn [65]: bp = df_box.groupby(\"g\").boxplot()\n     Area plot You can create area plots with Series.plot.area() and DataFrame.plot.area(). Area plots are stacked by default. To produce stacked area plot, each column must be either all positive or all negative values. When input data contains NaN, it will be automatically filled by 0. If you want to drop or fill by different values, use dataframe.dropna() or dataframe.fillna() before calling plot. \nIn [66]: df = pd.DataFrame(np.random.rand(10, 4), columns=[\"a\", \"b\", \"c\", \"d\"])\n\nIn [67]: df.plot.area();\n   To produce an unstacked plot, pass stacked=False. Alpha value is set to 0.5 unless otherwise specified: \nIn [68]: df.plot.area(stacked=False);\n     Scatter plot Scatter plot can be drawn by using the DataFrame.plot.scatter() method. Scatter plot requires numeric columns for the x and y axes. These can be specified by the x and y keywords. \nIn [69]: df = pd.DataFrame(np.random.rand(50, 4), columns=[\"a\", \"b\", \"c\", \"d\"])\n\nIn [70]: df[\"species\"] = pd.Categorical(\n   ....:     [\"setosa\"] * 20 + [\"versicolor\"] * 20 + [\"virginica\"] * 10\n   ....: )\n   ....: \n\nIn [71]: df.plot.scatter(x=\"a\", y=\"b\");\n   To plot multiple column groups in a single axes, repeat plot method specifying target ax. It is recommended to specify color and label keywords to distinguish each groups. \nIn [72]: ax = df.plot.scatter(x=\"a\", y=\"b\", color=\"DarkBlue\", label=\"Group 1\")\n\nIn [73]: df.plot.scatter(x=\"c\", y=\"d\", color=\"DarkGreen\", label=\"Group 2\", ax=ax);\n   The keyword c may be given as the name of a column to provide colors for each point: \nIn [74]: df.plot.scatter(x=\"a\", y=\"b\", c=\"c\", s=50);\n   If a categorical column is passed to c, then a discrete colorbar will be produced:  New in version 1.3.0.  \nIn [75]: df.plot.scatter(x=\"a\", y=\"b\", c=\"species\", cmap=\"viridis\", s=50);\n   You can pass other keywords supported by matplotlib scatter. The example below shows a bubble chart using a column of the DataFrame as the bubble size. \nIn [76]: df.plot.scatter(x=\"a\", y=\"b\", s=df[\"c\"] * 200);\n   See the scatter method and the matplotlib scatter documentation for more.   Hexagonal bin plot You can create hexagonal bin plots with DataFrame.plot.hexbin(). Hexbin plots can be a useful alternative to scatter plots if your data are too dense to plot each point individually. \nIn [77]: df = pd.DataFrame(np.random.randn(1000, 2), columns=[\"a\", \"b\"])\n\nIn [78]: df[\"b\"] = df[\"b\"] + np.arange(1000)\n\nIn [79]: df.plot.hexbin(x=\"a\", y=\"b\", gridsize=25);\n   A useful keyword argument is gridsize; it controls the number of hexagons in the x-direction, and defaults to 100. A larger gridsize means more, smaller bins. By default, a histogram of the counts around each (x, y) point is computed. You can specify alternative aggregations by passing values to the C and reduce_C_function arguments. C specifies the value at each (x, y) point and reduce_C_function is a function of one argument that reduces all the values in a bin to a single number (e.g. mean, max, sum, std). In this example the positions are given by columns a and b, while the value is given by column z. The bins are aggregated with NumPy\u2019s max function. \nIn [80]: df = pd.DataFrame(np.random.randn(1000, 2), columns=[\"a\", \"b\"])\n\nIn [81]: df[\"b\"] = df[\"b\"] + np.arange(1000)\n\nIn [82]: df[\"z\"] = np.random.uniform(0, 3, 1000)\n\nIn [83]: df.plot.hexbin(x=\"a\", y=\"b\", C=\"z\", reduce_C_function=np.max, gridsize=25);\n   See the hexbin method and the matplotlib hexbin documentation for more.   Pie plot You can create a pie plot with DataFrame.plot.pie() or Series.plot.pie(). If your data includes any NaN, they will be automatically filled with 0. A ValueError will be raised if there are any negative values in your data. \nIn [84]: series = pd.Series(3 * np.random.rand(4), index=[\"a\", \"b\", \"c\", \"d\"], name=\"series\")\n\nIn [85]: series.plot.pie(figsize=(6, 6));\n   For pie plots it\u2019s best to use square figures, i.e. a figure aspect ratio 1. You can create the figure with equal width and height, or force the aspect ratio to be equal after plotting by calling ax.set_aspect('equal') on the returned axes object. Note that pie plot with DataFrame requires that you either specify a target column by the y argument or subplots=True. When y is specified, pie plot of selected column will be drawn. If subplots=True is specified, pie plots for each column are drawn as subplots. A legend will be drawn in each pie plots by default; specify legend=False to hide it. \nIn [86]: df = pd.DataFrame(\n   ....:     3 * np.random.rand(4, 2), index=[\"a\", \"b\", \"c\", \"d\"], columns=[\"x\", \"y\"]\n   ....: )\n   ....: \n\nIn [87]: df.plot.pie(subplots=True, figsize=(8, 4));\n   You can use the labels and colors keywords to specify the labels and colors of each wedge.  Warning Most pandas plots use the label and color arguments (note the lack of \u201cs\u201d on those). To be consistent with matplotlib.pyplot.pie() you must use labels and colors.  If you want to hide wedge labels, specify labels=None. If fontsize is specified, the value will be applied to wedge labels. Also, other keywords supported by matplotlib.pyplot.pie() can be used. \nIn [88]: series.plot.pie(\n   ....:     labels=[\"AA\", \"BB\", \"CC\", \"DD\"],\n   ....:     colors=[\"r\", \"g\", \"b\", \"c\"],\n   ....:     autopct=\"%.2f\",\n   ....:     fontsize=20,\n   ....:     figsize=(6, 6),\n   ....: );\n   ....: \n   If you pass values whose sum total is less than 1.0, matplotlib draws a semicircle. \nIn [89]: series = pd.Series([0.1] * 4, index=[\"a\", \"b\", \"c\", \"d\"], name=\"series2\")\n\nIn [90]: series.plot.pie(figsize=(6, 6));\n   See the matplotlib pie documentation for more.    Plotting with missing data pandas tries to be pragmatic about plotting DataFrames or Series that contain missing data. Missing values are dropped, left out, or filled depending on the plot type.       \nPlot Type NaN Handling    \nLine Leave gaps at NaNs  \nLine (stacked) Fill 0\u2019s  \nBar Fill 0\u2019s  \nScatter Drop NaNs  \nHistogram Drop NaNs (column-wise)  \nBox Drop NaNs (column-wise)  \nArea Fill 0\u2019s  \nKDE Drop NaNs (column-wise)  \nHexbin Drop NaNs  \nPie Fill 0\u2019s    If any of these defaults are not what you want, or if you want to be explicit about how missing values are handled, consider using fillna() or dropna() before plotting.   Plotting tools These functions can be imported from pandas.plotting and take a Series or DataFrame as an argument.  Scatter matrix plot You can create a scatter plot matrix using the scatter_matrix method in pandas.plotting: \nIn [91]: from pandas.plotting import scatter_matrix\n\nIn [92]: df = pd.DataFrame(np.random.randn(1000, 4), columns=[\"a\", \"b\", \"c\", \"d\"])\n\nIn [93]: scatter_matrix(df, alpha=0.2, figsize=(6, 6), diagonal=\"kde\");\n     Density plot You can create density plots using the Series.plot.kde() and DataFrame.plot.kde() methods. \nIn [94]: ser = pd.Series(np.random.randn(1000))\n\nIn [95]: ser.plot.kde();\n     Andrews curves Andrews curves allow one to plot multivariate data as a large number of curves that are created using the attributes of samples as coefficients for Fourier series, see the Wikipedia entry for more information. By coloring these curves differently for each class it is possible to visualize data clustering. Curves belonging to samples of the same class will usually be closer together and form larger structures. Note: The \u201cIris\u201d dataset is available here. \nIn [96]: from pandas.plotting import andrews_curves\n\nIn [97]: data = pd.read_csv(\"data/iris.data\")\n\nIn [98]: plt.figure();\n\nIn [99]: andrews_curves(data, \"Name\");\n     Parallel coordinates Parallel coordinates is a plotting technique for plotting multivariate data, see the Wikipedia entry for an introduction. Parallel coordinates allows one to see clusters in data and to estimate other statistics visually. Using parallel coordinates points are represented as connected line segments. Each vertical line represents one attribute. One set of connected line segments represents one data point. Points that tend to cluster will appear closer together. \nIn [100]: from pandas.plotting import parallel_coordinates\n\nIn [101]: data = pd.read_csv(\"data/iris.data\")\n\nIn [102]: plt.figure();\n\nIn [103]: parallel_coordinates(data, \"Name\");\n     Lag plot Lag plots are used to check if a data set or time series is random. Random data should not exhibit any structure in the lag plot. Non-random structure implies that the underlying data are not random. The lag argument may be passed, and when lag=1 the plot is essentially data[:-1] vs. data[1:]. \nIn [104]: from pandas.plotting import lag_plot\n\nIn [105]: plt.figure();\n\nIn [106]: spacing = np.linspace(-99 * np.pi, 99 * np.pi, num=1000)\n\nIn [107]: data = pd.Series(0.1 * np.random.rand(1000) + 0.9 * np.sin(spacing))\n\nIn [108]: lag_plot(data);\n     Autocorrelation plot Autocorrelation plots are often used for checking randomness in time series. This is done by computing autocorrelations for data values at varying time lags. If time series is random, such autocorrelations should be near zero for any and all time-lag separations. If time series is non-random then one or more of the autocorrelations will be significantly non-zero. The horizontal lines displayed in the plot correspond to 95% and 99% confidence bands. The dashed line is 99% confidence band. See the Wikipedia entry for more about autocorrelation plots. \nIn [109]: from pandas.plotting import autocorrelation_plot\n\nIn [110]: plt.figure();\n\nIn [111]: spacing = np.linspace(-9 * np.pi, 9 * np.pi, num=1000)\n\nIn [112]: data = pd.Series(0.7 * np.random.rand(1000) + 0.3 * np.sin(spacing))\n\nIn [113]: autocorrelation_plot(data);\n     Bootstrap plot Bootstrap plots are used to visually assess the uncertainty of a statistic, such as mean, median, midrange, etc. A random subset of a specified size is selected from a data set, the statistic in question is computed for this subset and the process is repeated a specified number of times. Resulting plots and histograms are what constitutes the bootstrap plot. \nIn [114]: from pandas.plotting import bootstrap_plot\n\nIn [115]: data = pd.Series(np.random.rand(1000))\n\nIn [116]: bootstrap_plot(data, size=50, samples=500, color=\"grey\");\n     RadViz RadViz is a way of visualizing multi-variate data. It is based on a simple spring tension minimization algorithm. Basically you set up a bunch of points in a plane. In our case they are equally spaced on a unit circle. Each point represents a single attribute. You then pretend that each sample in the data set is attached to each of these points by a spring, the stiffness of which is proportional to the numerical value of that attribute (they are normalized to unit interval). The point in the plane, where our sample settles to (where the forces acting on our sample are at an equilibrium) is where a dot representing our sample will be drawn. Depending on which class that sample belongs it will be colored differently. See the R package Radviz for more information. Note: The \u201cIris\u201d dataset is available here. \nIn [117]: from pandas.plotting import radviz\n\nIn [118]: data = pd.read_csv(\"data/iris.data\")\n\nIn [119]: plt.figure();\n\nIn [120]: radviz(data, \"Name\");\n      Plot formatting  Setting the plot style From version 1.5 and up, matplotlib offers a range of pre-configured plotting styles. Setting the style can be used to easily give plots the general look that you want. Setting the style is as easy as calling matplotlib.style.use(my_plot_style) before creating your plot. For example you could write matplotlib.style.use('ggplot') for ggplot-style plots. You can see the various available style names at matplotlib.style.available and it\u2019s very easy to try them out.   General plot style arguments Most plotting methods have a set of keyword arguments that control the layout and formatting of the returned plot: \nIn [121]: plt.figure();\n\nIn [122]: ts.plot(style=\"k--\", label=\"Series\");\n   For each kind of plot (e.g. line, bar, scatter) any additional arguments keywords are passed along to the corresponding matplotlib function (ax.plot(), ax.bar(), ax.scatter()). These can be used to control additional styling, beyond what pandas provides.   Controlling the legend You may set the legend argument to False to hide the legend, which is shown by default. \nIn [123]: df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list(\"ABCD\"))\n\nIn [124]: df = df.cumsum()\n\nIn [125]: df.plot(legend=False);\n     Controlling the labels  New in version 1.1.0.  You may set the xlabel and ylabel arguments to give the plot custom labels for x and y axis. By default, pandas will pick up index name as xlabel, while leaving it empty for ylabel. \nIn [126]: df.plot();\n\nIn [127]: df.plot(xlabel=\"new x\", ylabel=\"new y\");\n     Scales You may pass logy to get a log-scale Y axis. \nIn [128]: ts = pd.Series(np.random.randn(1000), index=pd.date_range(\"1/1/2000\", periods=1000))\n\nIn [129]: ts = np.exp(ts.cumsum())\n\nIn [130]: ts.plot(logy=True);\n   See also the logx and loglog keyword arguments.   Plotting on a secondary y-axis To plot data on a secondary y-axis, use the secondary_y keyword: \nIn [131]: df[\"A\"].plot();\n\nIn [132]: df[\"B\"].plot(secondary_y=True, style=\"g\");\n   To plot some columns in a DataFrame, give the column names to the secondary_y keyword: \nIn [133]: plt.figure();\n\nIn [134]: ax = df.plot(secondary_y=[\"A\", \"B\"])\n\nIn [135]: ax.set_ylabel(\"CD scale\");\n\nIn [136]: ax.right_ax.set_ylabel(\"AB scale\");\n   Note that the columns plotted on the secondary y-axis is automatically marked with \u201c(right)\u201d in the legend. To turn off the automatic marking, use the mark_right=False keyword: \nIn [137]: plt.figure();\n\nIn [138]: df.plot(secondary_y=[\"A\", \"B\"], mark_right=False);\n     Custom formatters for timeseries plots  Changed in version 1.0.0.  pandas provides custom formatters for timeseries plots. These change the formatting of the axis labels for dates and times. By default, the custom formatters are applied only to plots created by pandas with DataFrame.plot() or Series.plot(). To have them apply to all plots, including those made by matplotlib, set the option pd.options.plotting.matplotlib.register_converters = True or use pandas.plotting.register_matplotlib_converters().   Suppressing tick resolution adjustment pandas includes automatic tick resolution adjustment for regular frequency time-series data. For limited cases where pandas cannot infer the frequency information (e.g., in an externally created twinx), you can choose to suppress this behavior for alignment purposes. Here is the default behavior, notice how the x-axis tick labeling is performed: \nIn [139]: plt.figure();\n\nIn [140]: df[\"A\"].plot();\n   Using the x_compat parameter, you can suppress this behavior: \nIn [141]: plt.figure();\n\nIn [142]: df[\"A\"].plot(x_compat=True);\n   If you have more than one plot that needs to be suppressed, the use method in pandas.plotting.plot_params can be used in a with statement: \nIn [143]: plt.figure();\n\nIn [144]: with pd.plotting.plot_params.use(\"x_compat\", True):\n   .....:     df[\"A\"].plot(color=\"r\")\n   .....:     df[\"B\"].plot(color=\"g\")\n   .....:     df[\"C\"].plot(color=\"b\")\n   .....: \n     Automatic date tick adjustment TimedeltaIndex now uses the native matplotlib tick locator methods, it is useful to call the automatic date tick adjustment from matplotlib for figures whose ticklabels overlap. See the autofmt_xdate method and the matplotlib documentation for more.   Subplots Each Series in a DataFrame can be plotted on a different axis with the subplots keyword: \nIn [145]: df.plot(subplots=True, figsize=(6, 6));\n     Using layout and targeting multiple axes The layout of subplots can be specified by the layout keyword. It can accept (rows, columns). The layout keyword can be used in hist and boxplot also. If the input is invalid, a ValueError will be raised. The number of axes which can be contained by rows x columns specified by layout must be larger than the number of required subplots. If layout can contain more axes than required, blank axes are not drawn. Similar to a NumPy array\u2019s reshape method, you can use -1 for one dimension to automatically calculate the number of rows or columns needed, given the other. \nIn [146]: df.plot(subplots=True, layout=(2, 3), figsize=(6, 6), sharex=False);\n   The above example is identical to using: \nIn [147]: df.plot(subplots=True, layout=(2, -1), figsize=(6, 6), sharex=False);\n  The required number of columns (3) is inferred from the number of series to plot and the given number of rows (2). You can pass multiple axes created beforehand as list-like via ax keyword. This allows more complicated layouts. The passed axes must be the same number as the subplots being drawn. When multiple axes are passed via the ax keyword, layout, sharex and sharey keywords don\u2019t affect to the output. You should explicitly pass sharex=False and sharey=False, otherwise you will see a warning. \nIn [148]: fig, axes = plt.subplots(4, 4, figsize=(9, 9))\n\nIn [149]: plt.subplots_adjust(wspace=0.5, hspace=0.5)\n\nIn [150]: target1 = [axes[0][0], axes[1][1], axes[2][2], axes[3][3]]\n\nIn [151]: target2 = [axes[3][0], axes[2][1], axes[1][2], axes[0][3]]\n\nIn [152]: df.plot(subplots=True, ax=target1, legend=False, sharex=False, sharey=False);\n\nIn [153]: (-df).plot(subplots=True, ax=target2, legend=False, sharex=False, sharey=False);\n   Another option is passing an ax argument to Series.plot() to plot on a particular axis: \nIn [154]: fig, axes = plt.subplots(nrows=2, ncols=2)\n\nIn [155]: plt.subplots_adjust(wspace=0.2, hspace=0.5)\n\nIn [156]: df[\"A\"].plot(ax=axes[0, 0]);\n\nIn [157]: axes[0, 0].set_title(\"A\");\n\nIn [158]: df[\"B\"].plot(ax=axes[0, 1]);\n\nIn [159]: axes[0, 1].set_title(\"B\");\n\nIn [160]: df[\"C\"].plot(ax=axes[1, 0]);\n\nIn [161]: axes[1, 0].set_title(\"C\");\n\nIn [162]: df[\"D\"].plot(ax=axes[1, 1]);\n\nIn [163]: axes[1, 1].set_title(\"D\");\n     Plotting with error bars Plotting with error bars is supported in DataFrame.plot() and Series.plot(). Horizontal and vertical error bars can be supplied to the xerr and yerr keyword arguments to plot(). The error values can be specified using a variety of formats:  As a DataFrame or dict of errors with column names matching the columns attribute of the plotting DataFrame or matching the name attribute of the Series. As a str indicating which of the columns of plotting DataFrame contain the error values. As raw values (list, tuple, or np.ndarray). Must be the same length as the plotting DataFrame/Series.  Here is an example of one way to easily plot group means with standard deviations from the raw data. \n# Generate the data\nIn [164]: ix3 = pd.MultiIndex.from_arrays(\n   .....:     [\n   .....:         [\"a\", \"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\", \"b\"],\n   .....:         [\"foo\", \"foo\", \"foo\", \"bar\", \"bar\", \"foo\", \"foo\", \"bar\", \"bar\", \"bar\"],\n   .....:     ],\n   .....:     names=[\"letter\", \"word\"],\n   .....: )\n   .....: \n\nIn [165]: df3 = pd.DataFrame(\n   .....:     {\n   .....:         \"data1\": [9, 3, 2, 4, 3, 2, 4, 6, 3, 2],\n   .....:         \"data2\": [9, 6, 5, 7, 5, 4, 5, 6, 5, 1],\n   .....:     },\n   .....:     index=ix3,\n   .....: )\n   .....: \n\n# Group by index labels and take the means and standard deviations\n# for each group\nIn [166]: gp3 = df3.groupby(level=(\"letter\", \"word\"))\n\nIn [167]: means = gp3.mean()\n\nIn [168]: errors = gp3.std()\n\nIn [169]: means\nOut[169]: \n                data1     data2\nletter word                    \na      bar   3.500000  6.000000\n       foo   4.666667  6.666667\nb      bar   3.666667  4.000000\n       foo   3.000000  4.500000\n\nIn [170]: errors\nOut[170]: \n                data1     data2\nletter word                    \na      bar   0.707107  1.414214\n       foo   3.785939  2.081666\nb      bar   2.081666  2.645751\n       foo   1.414214  0.707107\n\n# Plot\nIn [171]: fig, ax = plt.subplots()\n\nIn [172]: means.plot.bar(yerr=errors, ax=ax, capsize=4, rot=0);\n   Asymmetrical error bars are also supported, however raw error values must be provided in this case. For a N length Series, a 2xN array should be provided indicating lower and upper (or left and right) errors. For a MxN DataFrame, asymmetrical errors should be in a Mx2xN array. Here is an example of one way to plot the min/max range using asymmetrical error bars. \nIn [173]: mins = gp3.min()\n\nIn [174]: maxs = gp3.max()\n\n# errors should be positive, and defined in the order of lower, upper\nIn [175]: errors = [[means[c] - mins[c], maxs[c] - means[c]] for c in df3.columns]\n\n# Plot\nIn [176]: fig, ax = plt.subplots()\n\nIn [177]: means.plot.bar(yerr=errors, ax=ax, capsize=4, rot=0);\n     Plotting tables Plotting with matplotlib table is now supported in DataFrame.plot() and Series.plot() with a table keyword. The table keyword can accept bool, DataFrame or Series. The simple way to draw a table is to specify table=True. Data will be transposed to meet matplotlib\u2019s default layout. \nIn [178]: fig, ax = plt.subplots(1, 1, figsize=(7, 6.5))\n\nIn [179]: df = pd.DataFrame(np.random.rand(5, 3), columns=[\"a\", \"b\", \"c\"])\n\nIn [180]: ax.xaxis.tick_top()  # Display x-axis ticks on top.\n\nIn [181]: df.plot(table=True, ax=ax);\n   Also, you can pass a different DataFrame or Series to the table keyword. The data will be drawn as displayed in print method (not transposed automatically). If required, it should be transposed manually as seen in the example below. \nIn [182]: fig, ax = plt.subplots(1, 1, figsize=(7, 6.75))\n\nIn [183]: ax.xaxis.tick_top()  # Display x-axis ticks on top.\n\nIn [184]: df.plot(table=np.round(df.T, 2), ax=ax);\n   There also exists a helper function pandas.plotting.table, which creates a table from DataFrame or Series, and adds it to an matplotlib.Axes instance. This function can accept keywords which the matplotlib table has. \nIn [185]: from pandas.plotting import table\n\nIn [186]: fig, ax = plt.subplots(1, 1)\n\nIn [187]: table(ax, np.round(df.describe(), 2), loc=\"upper right\", colWidths=[0.2, 0.2, 0.2]);\n\nIn [188]: df.plot(ax=ax, ylim=(0, 2), legend=None);\n   Note: You can get table instances on the axes using axes.tables property for further decorations. See the matplotlib table documentation for more.   Colormaps A potential issue when plotting a large number of columns is that it can be difficult to distinguish some series due to repetition in the default colors. To remedy this, DataFrame plotting supports the use of the colormap argument, which accepts either a Matplotlib colormap or a string that is a name of a colormap registered with Matplotlib. A visualization of the default matplotlib colormaps is available here. As matplotlib does not directly support colormaps for line-based plots, the colors are selected based on an even spacing determined by the number of columns in the DataFrame. There is no consideration made for background color, so some colormaps will produce lines that are not easily visible. To use the cubehelix colormap, we can pass colormap='cubehelix'. \nIn [189]: df = pd.DataFrame(np.random.randn(1000, 10), index=ts.index)\n\nIn [190]: df = df.cumsum()\n\nIn [191]: plt.figure();\n\nIn [192]: df.plot(colormap=\"cubehelix\");\n   Alternatively, we can pass the colormap itself: \nIn [193]: from matplotlib import cm\n\nIn [194]: plt.figure();\n\nIn [195]: df.plot(colormap=cm.cubehelix);\n   Colormaps can also be used other plot types, like bar charts: \nIn [196]: dd = pd.DataFrame(np.random.randn(10, 10)).applymap(abs)\n\nIn [197]: dd = dd.cumsum()\n\nIn [198]: plt.figure();\n\nIn [199]: dd.plot.bar(colormap=\"Greens\");\n   Parallel coordinates charts: \nIn [200]: plt.figure();\n\nIn [201]: parallel_coordinates(data, \"Name\", colormap=\"gist_rainbow\");\n   Andrews curves charts: \nIn [202]: plt.figure();\n\nIn [203]: andrews_curves(data, \"Name\", colormap=\"winter\");\n      Plotting directly with matplotlib In some situations it may still be preferable or necessary to prepare plots directly with matplotlib, for instance when a certain type of plot or customization is not (yet) supported by pandas. Series and DataFrame objects behave like arrays and can therefore be passed directly to matplotlib functions without explicit casts. pandas also automatically registers formatters and locators that recognize date indices, thereby extending date and time support to practically all plot types available in matplotlib. Although this formatting does not provide the same level of refinement you would get when plotting via pandas, it can be faster when plotting a large number of points. \nIn [204]: price = pd.Series(\n   .....:     np.random.randn(150).cumsum(),\n   .....:     index=pd.date_range(\"2000-1-1\", periods=150, freq=\"B\"),\n   .....: )\n   .....: \n\nIn [205]: ma = price.rolling(20).mean()\n\nIn [206]: mstd = price.rolling(20).std()\n\nIn [207]: plt.figure();\n\nIn [208]: plt.plot(price.index, price, \"k\");\n\nIn [209]: plt.plot(ma.index, ma, \"b\");\n\nIn [210]: plt.fill_between(mstd.index, ma - 2 * mstd, ma + 2 * mstd, color=\"b\", alpha=0.2);\n     Plotting backends Starting in version 0.25, pandas can be extended with third-party plotting backends. The main idea is letting users select a plotting backend different than the provided one based on Matplotlib. This can be done by passing \u2018backend.module\u2019 as the argument backend in plot function. For example: \n>>> Series([1, 2, 3]).plot(backend=\"backend.module\")\n  Alternatively, you can also set this option globally, do you don\u2019t need to specify the keyword in each plot call. For example: \n>>> pd.set_option(\"plotting.backend\", \"backend.module\")\n>>> pd.Series([1, 2, 3]).plot()\n  Or: \n>>> pd.options.plotting.backend = \"backend.module\"\n>>> pd.Series([1, 2, 3]).plot()\n  This would be more or less equivalent to: \n>>> import backend.module\n>>> backend.module.plot(pd.Series([1, 2, 3]))\n  The backend module can then use other visualization tools (Bokeh, Altair, hvplot,\u2026) to generate the plots. Some libraries implementing a backend for pandas are listed on the ecosystem Visualization page. Developers guide can be found at https://pandas.pydata.org/docs/dev/development/extending.html#plotting-backends \n"}, {"name": "Computational tools", "path": "user_guide/computation", "type": "Manual", "text": "Computational tools  Statistical functions  Percent change Series and DataFrame have a method pct_change() to compute the percent change over a given number of periods (using fill_method to fill NA/null values before computing the percent change). \nIn [1]: ser = pd.Series(np.random.randn(8))\n\nIn [2]: ser.pct_change()\nOut[2]: \n0         NaN\n1   -1.602976\n2    4.334938\n3   -0.247456\n4   -2.067345\n5   -1.142903\n6   -1.688214\n7   -9.759729\ndtype: float64\n  \nIn [3]: df = pd.DataFrame(np.random.randn(10, 4))\n\nIn [4]: df.pct_change(periods=3)\nOut[4]: \n          0         1         2         3\n0       NaN       NaN       NaN       NaN\n1       NaN       NaN       NaN       NaN\n2       NaN       NaN       NaN       NaN\n3 -0.218320 -1.054001  1.987147 -0.510183\n4 -0.439121 -1.816454  0.649715 -4.822809\n5 -0.127833 -3.042065 -5.866604 -1.776977\n6 -2.596833 -1.959538 -2.111697 -3.798900\n7 -0.117826 -2.169058  0.036094 -0.067696\n8  2.492606 -1.357320 -1.205802 -1.558697\n9 -1.012977  2.324558 -1.003744 -0.371806\n    Covariance Series.cov() can be used to compute covariance between series (excluding missing values). \nIn [5]: s1 = pd.Series(np.random.randn(1000))\n\nIn [6]: s2 = pd.Series(np.random.randn(1000))\n\nIn [7]: s1.cov(s2)\nOut[7]: 0.0006801088174310875\n  Analogously, DataFrame.cov() to compute pairwise covariances among the series in the DataFrame, also excluding NA/null values.  Note Assuming the missing data are missing at random this results in an estimate for the covariance matrix which is unbiased. However, for many applications this estimate may not be acceptable because the estimated covariance matrix is not guaranteed to be positive semi-definite. This could lead to estimated correlations having absolute values which are greater than one, and/or a non-invertible covariance matrix. See Estimation of covariance matrices for more details.  \nIn [8]: frame = pd.DataFrame(np.random.randn(1000, 5), columns=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n\nIn [9]: frame.cov()\nOut[9]: \n          a         b         c         d         e\na  1.000882 -0.003177 -0.002698 -0.006889  0.031912\nb -0.003177  1.024721  0.000191  0.009212  0.000857\nc -0.002698  0.000191  0.950735 -0.031743 -0.005087\nd -0.006889  0.009212 -0.031743  1.002983 -0.047952\ne  0.031912  0.000857 -0.005087 -0.047952  1.042487\n  DataFrame.cov also supports an optional min_periods keyword that specifies the required minimum number of observations for each column pair in order to have a valid result. \nIn [10]: frame = pd.DataFrame(np.random.randn(20, 3), columns=[\"a\", \"b\", \"c\"])\n\nIn [11]: frame.loc[frame.index[:5], \"a\"] = np.nan\n\nIn [12]: frame.loc[frame.index[5:10], \"b\"] = np.nan\n\nIn [13]: frame.cov()\nOut[13]: \n          a         b         c\na  1.123670 -0.412851  0.018169\nb -0.412851  1.154141  0.305260\nc  0.018169  0.305260  1.301149\n\nIn [14]: frame.cov(min_periods=12)\nOut[14]: \n          a         b         c\na  1.123670       NaN  0.018169\nb       NaN  1.154141  0.305260\nc  0.018169  0.305260  1.301149\n    Correlation Correlation may be computed using the corr() method. Using the method parameter, several methods for computing correlations are provided:       \nMethod name Description    \npearson (default) Standard correlation coefficient  \nkendall Kendall Tau correlation coefficient  \nspearman Spearman rank correlation coefficient    All of these are currently computed using pairwise complete observations. Wikipedia has articles covering the above correlation coefficients:  Pearson correlation coefficient Kendall rank correlation coefficient Spearman\u2019s rank correlation coefficient   Note Please see the caveats associated with this method of calculating correlation matrices in the covariance section.  \nIn [15]: frame = pd.DataFrame(np.random.randn(1000, 5), columns=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n\nIn [16]: frame.iloc[::2] = np.nan\n\n# Series with Series\nIn [17]: frame[\"a\"].corr(frame[\"b\"])\nOut[17]: 0.013479040400098775\n\nIn [18]: frame[\"a\"].corr(frame[\"b\"], method=\"spearman\")\nOut[18]: -0.007289885159540637\n\n# Pairwise correlation of DataFrame columns\nIn [19]: frame.corr()\nOut[19]: \n          a         b         c         d         e\na  1.000000  0.013479 -0.049269 -0.042239 -0.028525\nb  0.013479  1.000000 -0.020433 -0.011139  0.005654\nc -0.049269 -0.020433  1.000000  0.018587 -0.054269\nd -0.042239 -0.011139  0.018587  1.000000 -0.017060\ne -0.028525  0.005654 -0.054269 -0.017060  1.000000\n  Note that non-numeric columns will be automatically excluded from the correlation calculation. Like cov, corr also supports the optional min_periods keyword: \nIn [20]: frame = pd.DataFrame(np.random.randn(20, 3), columns=[\"a\", \"b\", \"c\"])\n\nIn [21]: frame.loc[frame.index[:5], \"a\"] = np.nan\n\nIn [22]: frame.loc[frame.index[5:10], \"b\"] = np.nan\n\nIn [23]: frame.corr()\nOut[23]: \n          a         b         c\na  1.000000 -0.121111  0.069544\nb -0.121111  1.000000  0.051742\nc  0.069544  0.051742  1.000000\n\nIn [24]: frame.corr(min_periods=12)\nOut[24]: \n          a         b         c\na  1.000000       NaN  0.069544\nb       NaN  1.000000  0.051742\nc  0.069544  0.051742  1.000000\n  The method argument can also be a callable for a generic correlation calculation. In this case, it should be a single function that produces a single value from two ndarray inputs. Suppose we wanted to compute the correlation based on histogram intersection: \n# histogram intersection\nIn [25]: def histogram_intersection(a, b):\n   ....:     return np.minimum(np.true_divide(a, a.sum()), np.true_divide(b, b.sum())).sum()\n   ....: \n\nIn [26]: frame.corr(method=histogram_intersection)\nOut[26]: \n          a          b          c\na  1.000000  -6.404882  -2.058431\nb -6.404882   1.000000 -19.255743\nc -2.058431 -19.255743   1.000000\n  A related method corrwith() is implemented on DataFrame to compute the correlation between like-labeled Series contained in different DataFrame objects. \nIn [27]: index = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n\nIn [28]: columns = [\"one\", \"two\", \"three\", \"four\"]\n\nIn [29]: df1 = pd.DataFrame(np.random.randn(5, 4), index=index, columns=columns)\n\nIn [30]: df2 = pd.DataFrame(np.random.randn(4, 4), index=index[:4], columns=columns)\n\nIn [31]: df1.corrwith(df2)\nOut[31]: \none     -0.125501\ntwo     -0.493244\nthree    0.344056\nfour     0.004183\ndtype: float64\n\nIn [32]: df2.corrwith(df1, axis=1)\nOut[32]: \na   -0.675817\nb    0.458296\nc    0.190809\nd   -0.186275\ne         NaN\ndtype: float64\n    Data ranking The rank() method produces a data ranking with ties being assigned the mean of the ranks (by default) for the group: \nIn [33]: s = pd.Series(np.random.randn(5), index=list(\"abcde\"))\n\nIn [34]: s[\"d\"] = s[\"b\"]  # so there's a tie\n\nIn [35]: s.rank()\nOut[35]: \na    5.0\nb    2.5\nc    1.0\nd    2.5\ne    4.0\ndtype: float64\n  rank() is also a DataFrame method and can rank either the rows (axis=0) or the columns (axis=1). NaN values are excluded from the ranking. \nIn [36]: df = pd.DataFrame(np.random.randn(10, 6))\n\nIn [37]: df[4] = df[2][:5]  # some ties\n\nIn [38]: df\nOut[38]: \n          0         1         2         3         4         5\n0 -0.904948 -1.163537 -1.457187  0.135463 -1.457187  0.294650\n1 -0.976288 -0.244652 -0.748406 -0.999601 -0.748406 -0.800809\n2  0.401965  1.460840  1.256057  1.308127  1.256057  0.876004\n3  0.205954  0.369552 -0.669304  0.038378 -0.669304  1.140296\n4 -0.477586 -0.730705 -1.129149 -0.601463 -1.129149 -0.211196\n5 -1.092970 -0.689246  0.908114  0.204848       NaN  0.463347\n6  0.376892  0.959292  0.095572 -0.593740       NaN -0.069180\n7 -1.002601  1.957794 -0.120708  0.094214       NaN -1.467422\n8 -0.547231  0.664402 -0.519424 -0.073254       NaN -1.263544\n9 -0.250277 -0.237428 -1.056443  0.419477       NaN  1.375064\n\nIn [39]: df.rank(1)\nOut[39]: \n     0    1    2    3    4    5\n0  4.0  3.0  1.5  5.0  1.5  6.0\n1  2.0  6.0  4.5  1.0  4.5  3.0\n2  1.0  6.0  3.5  5.0  3.5  2.0\n3  4.0  5.0  1.5  3.0  1.5  6.0\n4  5.0  3.0  1.5  4.0  1.5  6.0\n5  1.0  2.0  5.0  3.0  NaN  4.0\n6  4.0  5.0  3.0  1.0  NaN  2.0\n7  2.0  5.0  3.0  4.0  NaN  1.0\n8  2.0  5.0  3.0  4.0  NaN  1.0\n9  2.0  3.0  1.0  4.0  NaN  5.0\n  rank optionally takes a parameter ascending which by default is true; when false, data is reverse-ranked, with larger values assigned a smaller rank. rank supports different tie-breaking methods, specified with the method parameter:  \n average : average rank of tied group min : lowest rank in the group max : highest rank in the group first : ranks assigned in the order they appear in the array  \n   Windowing functions See the window operations user guide for an overview of windowing functions.  \n"}, {"name": "Cookbook", "path": "user_guide/cookbook", "type": "Manual", "text": "Cookbook This is a repository for short and sweet examples and links for useful pandas recipes. We encourage users to add to this documentation. Adding interesting links and/or inline examples to this section is a great First Pull Request. Simplified, condensed, new-user friendly, in-line examples have been inserted where possible to augment the Stack-Overflow and GitHub links. Many of the links contain expanded information, above what the in-line examples offer. pandas (pd) and NumPy (np) are the only two abbreviated imported modules. The rest are kept explicitly imported for newer users.  Idioms These are some neat pandas idioms if-then/if-then-else on one column, and assignment to another one or more columns: \nIn [1]: df = pd.DataFrame(\n   ...:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ...: )\n   ...: \n\nIn [2]: df\nOut[2]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n   if-then\u2026 An if-then on one column \nIn [3]: df.loc[df.AAA >= 5, \"BBB\"] = -1\n\nIn [4]: df\nOut[4]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   -1   50\n2    6   -1  -30\n3    7   -1  -50\n  An if-then with assignment to 2 columns: \nIn [5]: df.loc[df.AAA >= 5, [\"BBB\", \"CCC\"]] = 555\n\nIn [6]: df\nOut[6]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5  555  555\n2    6  555  555\n3    7  555  555\n  Add another line with different logic, to do the -else \nIn [7]: df.loc[df.AAA < 5, [\"BBB\", \"CCC\"]] = 2000\n\nIn [8]: df\nOut[8]: \n   AAA   BBB   CCC\n0    4  2000  2000\n1    5   555   555\n2    6   555   555\n3    7   555   555\n  Or use pandas where after you\u2019ve set up a mask \nIn [9]: df_mask = pd.DataFrame(\n   ...:     {\"AAA\": [True] * 4, \"BBB\": [False] * 4, \"CCC\": [True, False] * 2}\n   ...: )\n   ...: \n\nIn [10]: df.where(df_mask, -1000)\nOut[10]: \n   AAA   BBB   CCC\n0    4 -1000  2000\n1    5 -1000 -1000\n2    6 -1000   555\n3    7 -1000 -1000\n  if-then-else using NumPy\u2019s where() \nIn [11]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [12]: df\nOut[12]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [13]: df[\"logic\"] = np.where(df[\"AAA\"] > 5, \"high\", \"low\")\n\nIn [14]: df\nOut[14]: \n   AAA  BBB  CCC logic\n0    4   10  100   low\n1    5   20   50   low\n2    6   30  -30  high\n3    7   40  -50  high\n    Splitting Split a frame with a boolean criterion \nIn [15]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [16]: df\nOut[16]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [17]: df[df.AAA <= 5]\nOut[17]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n\nIn [18]: df[df.AAA > 5]\nOut[18]: \n   AAA  BBB  CCC\n2    6   30  -30\n3    7   40  -50\n    Building criteria Select with multi-column criteria \nIn [19]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [20]: df\nOut[20]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n  \u2026and (without assignment returns a Series) \nIn [21]: df.loc[(df[\"BBB\"] < 25) & (df[\"CCC\"] >= -40), \"AAA\"]\nOut[21]: \n0    4\n1    5\nName: AAA, dtype: int64\n  \u2026or (without assignment returns a Series) \nIn [22]: df.loc[(df[\"BBB\"] > 25) | (df[\"CCC\"] >= -40), \"AAA\"]\nOut[22]: \n0    4\n1    5\n2    6\n3    7\nName: AAA, dtype: int64\n  \u2026or (with assignment modifies the DataFrame.) \nIn [23]: df.loc[(df[\"BBB\"] > 25) | (df[\"CCC\"] >= 75), \"AAA\"] = 0.1\n\nIn [24]: df\nOut[24]: \n   AAA  BBB  CCC\n0  0.1   10  100\n1  5.0   20   50\n2  0.1   30  -30\n3  0.1   40  -50\n  Select rows with data closest to certain value using argsort \nIn [25]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [26]: df\nOut[26]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [27]: aValue = 43.0\n\nIn [28]: df.loc[(df.CCC - aValue).abs().argsort()]\nOut[28]: \n   AAA  BBB  CCC\n1    5   20   50\n0    4   10  100\n2    6   30  -30\n3    7   40  -50\n  Dynamically reduce a list of criteria using a binary operators \nIn [29]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [30]: df\nOut[30]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [31]: Crit1 = df.AAA <= 5.5\n\nIn [32]: Crit2 = df.BBB == 10.0\n\nIn [33]: Crit3 = df.CCC > -40.0\n  One could hard code: \nIn [34]: AllCrit = Crit1 & Crit2 & Crit3\n  \u2026Or it can be done with a list of dynamically built criteria \nIn [35]: import functools\n\nIn [36]: CritList = [Crit1, Crit2, Crit3]\n\nIn [37]: AllCrit = functools.reduce(lambda x, y: x & y, CritList)\n\nIn [38]: df[AllCrit]\nOut[38]: \n   AAA  BBB  CCC\n0    4   10  100\n     Selection  Dataframes The indexing docs. Using both row labels and value conditionals \nIn [39]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [40]: df\nOut[40]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [41]: df[(df.AAA <= 6) & (df.index.isin([0, 2, 4]))]\nOut[41]: \n   AAA  BBB  CCC\n0    4   10  100\n2    6   30  -30\n  Use loc for label-oriented slicing and iloc positional slicing GH2904 \nIn [42]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]},\n   ....:     index=[\"foo\", \"bar\", \"boo\", \"kar\"],\n   ....: )\n   ....: \n  There are 2 explicit slicing methods, with a third general case  Positional-oriented (Python slicing style : exclusive of end) Label-oriented (Non-Python slicing style : inclusive of end) General (Either slicing style : depends on if the slice contains labels or positions)  \nIn [43]: df.loc[\"bar\":\"kar\"]  # Label\nOut[43]: \n     AAA  BBB  CCC\nbar    5   20   50\nboo    6   30  -30\nkar    7   40  -50\n\n# Generic\nIn [44]: df[0:3]\nOut[44]: \n     AAA  BBB  CCC\nfoo    4   10  100\nbar    5   20   50\nboo    6   30  -30\n\nIn [45]: df[\"bar\":\"kar\"]\nOut[45]: \n     AAA  BBB  CCC\nbar    5   20   50\nboo    6   30  -30\nkar    7   40  -50\n  Ambiguity arises when an index consists of integers with a non-zero start or non-unit increment. \nIn [46]: data = {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n\nIn [47]: df2 = pd.DataFrame(data=data, index=[1, 2, 3, 4])  # Note index starts at 1.\n\nIn [48]: df2.iloc[1:3]  # Position-oriented\nOut[48]: \n   AAA  BBB  CCC\n2    5   20   50\n3    6   30  -30\n\nIn [49]: df2.loc[1:3]  # Label-oriented\nOut[49]: \n   AAA  BBB  CCC\n1    4   10  100\n2    5   20   50\n3    6   30  -30\n  Using inverse operator (~) to take the complement of a mask \nIn [50]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [51]: df\nOut[51]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [52]: df[~((df.AAA <= 6) & (df.index.isin([0, 2, 4])))]\nOut[52]: \n   AAA  BBB  CCC\n1    5   20   50\n3    7   40  -50\n    New columns Efficiently and dynamically creating new columns using applymap \nIn [53]: df = pd.DataFrame({\"AAA\": [1, 2, 1, 3], \"BBB\": [1, 1, 2, 2], \"CCC\": [2, 1, 3, 1]})\n\nIn [54]: df\nOut[54]: \n   AAA  BBB  CCC\n0    1    1    2\n1    2    1    1\n2    1    2    3\n3    3    2    1\n\nIn [55]: source_cols = df.columns  # Or some subset would work too\n\nIn [56]: new_cols = [str(x) + \"_cat\" for x in source_cols]\n\nIn [57]: categories = {1: \"Alpha\", 2: \"Beta\", 3: \"Charlie\"}\n\nIn [58]: df[new_cols] = df[source_cols].applymap(categories.get)\n\nIn [59]: df\nOut[59]: \n   AAA  BBB  CCC  AAA_cat BBB_cat  CCC_cat\n0    1    1    2    Alpha   Alpha     Beta\n1    2    1    1     Beta   Alpha    Alpha\n2    1    2    3    Alpha    Beta  Charlie\n3    3    2    1  Charlie    Beta    Alpha\n  Keep other columns when using min() with groupby \nIn [60]: df = pd.DataFrame(\n   ....:     {\"AAA\": [1, 1, 1, 2, 2, 2, 3, 3], \"BBB\": [2, 1, 3, 4, 5, 1, 2, 3]}\n   ....: )\n   ....: \n\nIn [61]: df\nOut[61]: \n   AAA  BBB\n0    1    2\n1    1    1\n2    1    3\n3    2    4\n4    2    5\n5    2    1\n6    3    2\n7    3    3\n  Method 1 : idxmin() to get the index of the minimums \nIn [62]: df.loc[df.groupby(\"AAA\")[\"BBB\"].idxmin()]\nOut[62]: \n   AAA  BBB\n1    1    1\n5    2    1\n6    3    2\n  Method 2 : sort then take first of each \nIn [63]: df.sort_values(by=\"BBB\").groupby(\"AAA\", as_index=False).first()\nOut[63]: \n   AAA  BBB\n0    1    1\n1    2    1\n2    3    2\n  Notice the same results, with the exception of the index.    Multiindexing The multindexing docs. Creating a MultiIndex from a labeled frame \nIn [64]: df = pd.DataFrame(\n   ....:     {\n   ....:         \"row\": [0, 1, 2],\n   ....:         \"One_X\": [1.1, 1.1, 1.1],\n   ....:         \"One_Y\": [1.2, 1.2, 1.2],\n   ....:         \"Two_X\": [1.11, 1.11, 1.11],\n   ....:         \"Two_Y\": [1.22, 1.22, 1.22],\n   ....:     }\n   ....: )\n   ....: \n\nIn [65]: df\nOut[65]: \n   row  One_X  One_Y  Two_X  Two_Y\n0    0    1.1    1.2   1.11   1.22\n1    1    1.1    1.2   1.11   1.22\n2    2    1.1    1.2   1.11   1.22\n\n# As Labelled Index\nIn [66]: df = df.set_index(\"row\")\n\nIn [67]: df\nOut[67]: \n     One_X  One_Y  Two_X  Two_Y\nrow                            \n0      1.1    1.2   1.11   1.22\n1      1.1    1.2   1.11   1.22\n2      1.1    1.2   1.11   1.22\n\n# With Hierarchical Columns\nIn [68]: df.columns = pd.MultiIndex.from_tuples([tuple(c.split(\"_\")) for c in df.columns])\n\nIn [69]: df\nOut[69]: \n     One        Two      \n       X    Y     X     Y\nrow                      \n0    1.1  1.2  1.11  1.22\n1    1.1  1.2  1.11  1.22\n2    1.1  1.2  1.11  1.22\n\n# Now stack & Reset\nIn [70]: df = df.stack(0).reset_index(1)\n\nIn [71]: df\nOut[71]: \n    level_1     X     Y\nrow                    \n0       One  1.10  1.20\n0       Two  1.11  1.22\n1       One  1.10  1.20\n1       Two  1.11  1.22\n2       One  1.10  1.20\n2       Two  1.11  1.22\n\n# And fix the labels (Notice the label 'level_1' got added automatically)\nIn [72]: df.columns = [\"Sample\", \"All_X\", \"All_Y\"]\n\nIn [73]: df\nOut[73]: \n    Sample  All_X  All_Y\nrow                     \n0      One   1.10   1.20\n0      Two   1.11   1.22\n1      One   1.10   1.20\n1      Two   1.11   1.22\n2      One   1.10   1.20\n2      Two   1.11   1.22\n   Arithmetic Performing arithmetic with a MultiIndex that needs broadcasting \nIn [74]: cols = pd.MultiIndex.from_tuples(\n   ....:     [(x, y) for x in [\"A\", \"B\", \"C\"] for y in [\"O\", \"I\"]]\n   ....: )\n   ....: \n\nIn [75]: df = pd.DataFrame(np.random.randn(2, 6), index=[\"n\", \"m\"], columns=cols)\n\nIn [76]: df\nOut[76]: \n          A                   B                   C          \n          O         I         O         I         O         I\nn  0.469112 -0.282863 -1.509059 -1.135632  1.212112 -0.173215\nm  0.119209 -1.044236 -0.861849 -2.104569 -0.494929  1.071804\n\nIn [77]: df = df.div(df[\"C\"], level=1)\n\nIn [78]: df\nOut[78]: \n          A                   B              C     \n          O         I         O         I    O    I\nn  0.387021  1.633022 -1.244983  6.556214  1.0  1.0\nm -0.240860 -0.974279  1.741358 -1.963577  1.0  1.0\n    Slicing Slicing a MultiIndex with xs \nIn [79]: coords = [(\"AA\", \"one\"), (\"AA\", \"six\"), (\"BB\", \"one\"), (\"BB\", \"two\"), (\"BB\", \"six\")]\n\nIn [80]: index = pd.MultiIndex.from_tuples(coords)\n\nIn [81]: df = pd.DataFrame([11, 22, 33, 44, 55], index, [\"MyData\"])\n\nIn [82]: df\nOut[82]: \n        MyData\nAA one      11\n   six      22\nBB one      33\n   two      44\n   six      55\n  To take the cross section of the 1st level and 1st axis the index: \n# Note : level and axis are optional, and default to zero\nIn [83]: df.xs(\"BB\", level=0, axis=0)\nOut[83]: \n     MyData\none      33\ntwo      44\nsix      55\n  \u2026and now the 2nd level of the 1st axis. \nIn [84]: df.xs(\"six\", level=1, axis=0)\nOut[84]: \n    MyData\nAA      22\nBB      55\n  Slicing a MultiIndex with xs, method #2 \nIn [85]: import itertools\n\nIn [86]: index = list(itertools.product([\"Ada\", \"Quinn\", \"Violet\"], [\"Comp\", \"Math\", \"Sci\"]))\n\nIn [87]: headr = list(itertools.product([\"Exams\", \"Labs\"], [\"I\", \"II\"]))\n\nIn [88]: indx = pd.MultiIndex.from_tuples(index, names=[\"Student\", \"Course\"])\n\nIn [89]: cols = pd.MultiIndex.from_tuples(headr)  # Notice these are un-named\n\nIn [90]: data = [[70 + x + y + (x * y) % 3 for x in range(4)] for y in range(9)]\n\nIn [91]: df = pd.DataFrame(data, indx, cols)\n\nIn [92]: df\nOut[92]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nAda     Comp      70  71   72  73\n        Math      71  73   75  74\n        Sci       72  75   75  75\nQuinn   Comp      73  74   75  76\n        Math      74  76   78  77\n        Sci       75  78   78  78\nViolet  Comp      76  77   78  79\n        Math      77  79   81  80\n        Sci       78  81   81  81\n\nIn [93]: All = slice(None)\n\nIn [94]: df.loc[\"Violet\"]\nOut[94]: \n       Exams     Labs    \n           I  II    I  II\nCourse                   \nComp      76  77   78  79\nMath      77  79   81  80\nSci       78  81   81  81\n\nIn [95]: df.loc[(All, \"Math\"), All]\nOut[95]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nAda     Math      71  73   75  74\nQuinn   Math      74  76   78  77\nViolet  Math      77  79   81  80\n\nIn [96]: df.loc[(slice(\"Ada\", \"Quinn\"), \"Math\"), All]\nOut[96]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nAda     Math      71  73   75  74\nQuinn   Math      74  76   78  77\n\nIn [97]: df.loc[(All, \"Math\"), (\"Exams\")]\nOut[97]: \n                 I  II\nStudent Course        \nAda     Math    71  73\nQuinn   Math    74  76\nViolet  Math    77  79\n\nIn [98]: df.loc[(All, \"Math\"), (All, \"II\")]\nOut[98]: \n               Exams Labs\n                  II   II\nStudent Course           \nAda     Math      73   74\nQuinn   Math      76   77\nViolet  Math      79   80\n  Setting portions of a MultiIndex with xs   Sorting Sort by specific column or an ordered list of columns, with a MultiIndex \nIn [99]: df.sort_values(by=(\"Labs\", \"II\"), ascending=False)\nOut[99]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nViolet  Sci       78  81   81  81\n        Math      77  79   81  80\n        Comp      76  77   78  79\nQuinn   Sci       75  78   78  78\n        Math      74  76   78  77\n        Comp      73  74   75  76\nAda     Sci       72  75   75  75\n        Math      71  73   75  74\n        Comp      70  71   72  73\n  Partial selection, the need for sortedness GH2995   Levels Prepending a level to a multiindex Flatten Hierarchical columns    Missing data The missing data docs. Fill forward a reversed timeseries \nIn [100]: df = pd.DataFrame(\n   .....:     np.random.randn(6, 1),\n   .....:     index=pd.date_range(\"2013-08-01\", periods=6, freq=\"B\"),\n   .....:     columns=list(\"A\"),\n   .....: )\n   .....: \n\nIn [101]: df.loc[df.index[3], \"A\"] = np.nan\n\nIn [102]: df\nOut[102]: \n                   A\n2013-08-01  0.721555\n2013-08-02 -0.706771\n2013-08-05 -1.039575\n2013-08-06       NaN\n2013-08-07 -0.424972\n2013-08-08  0.567020\n\nIn [103]: df.reindex(df.index[::-1]).ffill()\nOut[103]: \n                   A\n2013-08-08  0.567020\n2013-08-07 -0.424972\n2013-08-06 -0.424972\n2013-08-05 -1.039575\n2013-08-02 -0.706771\n2013-08-01  0.721555\n  cumsum reset at NaN values  Replace Using replace with backrefs    Grouping The grouping docs. Basic grouping with apply Unlike agg, apply\u2019s callable is passed a sub-DataFrame which gives you access to all the columns \nIn [104]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"animal\": \"cat dog cat fish dog cat cat\".split(),\n   .....:         \"size\": list(\"SSMMMLL\"),\n   .....:         \"weight\": [8, 10, 11, 1, 20, 12, 12],\n   .....:         \"adult\": [False] * 5 + [True] * 2,\n   .....:     }\n   .....: )\n   .....: \n\nIn [105]: df\nOut[105]: \n  animal size  weight  adult\n0    cat    S       8  False\n1    dog    S      10  False\n2    cat    M      11  False\n3   fish    M       1  False\n4    dog    M      20  False\n5    cat    L      12   True\n6    cat    L      12   True\n\n# List the size of the animals with the highest weight.\nIn [106]: df.groupby(\"animal\").apply(lambda subf: subf[\"size\"][subf[\"weight\"].idxmax()])\nOut[106]: \nanimal\ncat     L\ndog     M\nfish    M\ndtype: object\n  Using get_group \nIn [107]: gb = df.groupby([\"animal\"])\n\nIn [108]: gb.get_group(\"cat\")\nOut[108]: \n  animal size  weight  adult\n0    cat    S       8  False\n2    cat    M      11  False\n5    cat    L      12   True\n6    cat    L      12   True\n  Apply to different items in a group \nIn [109]: def GrowUp(x):\n   .....:     avg_weight = sum(x[x[\"size\"] == \"S\"].weight * 1.5)\n   .....:     avg_weight += sum(x[x[\"size\"] == \"M\"].weight * 1.25)\n   .....:     avg_weight += sum(x[x[\"size\"] == \"L\"].weight)\n   .....:     avg_weight /= len(x)\n   .....:     return pd.Series([\"L\", avg_weight, True], index=[\"size\", \"weight\", \"adult\"])\n   .....: \n\nIn [110]: expected_df = gb.apply(GrowUp)\n\nIn [111]: expected_df\nOut[111]: \n       size   weight  adult\nanimal                     \ncat       L  12.4375   True\ndog       L  20.0000   True\nfish      L   1.2500   True\n  Expanding apply \nIn [112]: S = pd.Series([i / 100.0 for i in range(1, 11)])\n\nIn [113]: def cum_ret(x, y):\n   .....:     return x * (1 + y)\n   .....: \n\nIn [114]: def red(x):\n   .....:     return functools.reduce(cum_ret, x, 1.0)\n   .....: \n\nIn [115]: S.expanding().apply(red, raw=True)\nOut[115]: \n0    1.010000\n1    1.030200\n2    1.061106\n3    1.103550\n4    1.158728\n5    1.228251\n6    1.314229\n7    1.419367\n8    1.547110\n9    1.701821\ndtype: float64\n  Replacing some values with mean of the rest of a group \nIn [116]: df = pd.DataFrame({\"A\": [1, 1, 2, 2], \"B\": [1, -1, 1, 2]})\n\nIn [117]: gb = df.groupby(\"A\")\n\nIn [118]: def replace(g):\n   .....:     mask = g < 0\n   .....:     return g.where(mask, g[~mask].mean())\n   .....: \n\nIn [119]: gb.transform(replace)\nOut[119]: \n     B\n0  1.0\n1 -1.0\n2  1.5\n3  1.5\n  Sort groups by aggregated data \nIn [120]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"code\": [\"foo\", \"bar\", \"baz\"] * 2,\n   .....:         \"data\": [0.16, -0.21, 0.33, 0.45, -0.59, 0.62],\n   .....:         \"flag\": [False, True] * 3,\n   .....:     }\n   .....: )\n   .....: \n\nIn [121]: code_groups = df.groupby(\"code\")\n\nIn [122]: agg_n_sort_order = code_groups[[\"data\"]].transform(sum).sort_values(by=\"data\")\n\nIn [123]: sorted_df = df.loc[agg_n_sort_order.index]\n\nIn [124]: sorted_df\nOut[124]: \n  code  data   flag\n1  bar -0.21   True\n4  bar -0.59  False\n0  foo  0.16  False\n3  foo  0.45   True\n2  baz  0.33  False\n5  baz  0.62   True\n  Create multiple aggregated columns \nIn [125]: rng = pd.date_range(start=\"2014-10-07\", periods=10, freq=\"2min\")\n\nIn [126]: ts = pd.Series(data=list(range(10)), index=rng)\n\nIn [127]: def MyCust(x):\n   .....:     if len(x) > 2:\n   .....:         return x[1] * 1.234\n   .....:     return pd.NaT\n   .....: \n\nIn [128]: mhc = {\"Mean\": np.mean, \"Max\": np.max, \"Custom\": MyCust}\n\nIn [129]: ts.resample(\"5min\").apply(mhc)\nOut[129]: \n                     Mean  Max Custom\n2014-10-07 00:00:00   1.0    2  1.234\n2014-10-07 00:05:00   3.5    4    NaT\n2014-10-07 00:10:00   6.0    7  7.404\n2014-10-07 00:15:00   8.5    9    NaT\n\nIn [130]: ts\nOut[130]: \n2014-10-07 00:00:00    0\n2014-10-07 00:02:00    1\n2014-10-07 00:04:00    2\n2014-10-07 00:06:00    3\n2014-10-07 00:08:00    4\n2014-10-07 00:10:00    5\n2014-10-07 00:12:00    6\n2014-10-07 00:14:00    7\n2014-10-07 00:16:00    8\n2014-10-07 00:18:00    9\nFreq: 2T, dtype: int64\n  Create a value counts column and reassign back to the DataFrame \nIn [131]: df = pd.DataFrame(\n   .....:     {\"Color\": \"Red Red Red Blue\".split(), \"Value\": [100, 150, 50, 50]}\n   .....: )\n   .....: \n\nIn [132]: df\nOut[132]: \n  Color  Value\n0   Red    100\n1   Red    150\n2   Red     50\n3  Blue     50\n\nIn [133]: df[\"Counts\"] = df.groupby([\"Color\"]).transform(len)\n\nIn [134]: df\nOut[134]: \n  Color  Value  Counts\n0   Red    100       3\n1   Red    150       3\n2   Red     50       3\n3  Blue     50       1\n  Shift groups of the values in a column based on the index \nIn [135]: df = pd.DataFrame(\n   .....:     {\"line_race\": [10, 10, 8, 10, 10, 8], \"beyer\": [99, 102, 103, 103, 88, 100]},\n   .....:     index=[\n   .....:         \"Last Gunfighter\",\n   .....:         \"Last Gunfighter\",\n   .....:         \"Last Gunfighter\",\n   .....:         \"Paynter\",\n   .....:         \"Paynter\",\n   .....:         \"Paynter\",\n   .....:     ],\n   .....: )\n   .....: \n\nIn [136]: df\nOut[136]: \n                 line_race  beyer\nLast Gunfighter         10     99\nLast Gunfighter         10    102\nLast Gunfighter          8    103\nPaynter                 10    103\nPaynter                 10     88\nPaynter                  8    100\n\nIn [137]: df[\"beyer_shifted\"] = df.groupby(level=0)[\"beyer\"].shift(1)\n\nIn [138]: df\nOut[138]: \n                 line_race  beyer  beyer_shifted\nLast Gunfighter         10     99            NaN\nLast Gunfighter         10    102           99.0\nLast Gunfighter          8    103          102.0\nPaynter                 10    103            NaN\nPaynter                 10     88          103.0\nPaynter                  8    100           88.0\n  Select row with maximum value from each group \nIn [139]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"host\": [\"other\", \"other\", \"that\", \"this\", \"this\"],\n   .....:         \"service\": [\"mail\", \"web\", \"mail\", \"mail\", \"web\"],\n   .....:         \"no\": [1, 2, 1, 2, 1],\n   .....:     }\n   .....: ).set_index([\"host\", \"service\"])\n   .....: \n\nIn [140]: mask = df.groupby(level=0).agg(\"idxmax\")\n\nIn [141]: df_count = df.loc[mask[\"no\"]].reset_index()\n\nIn [142]: df_count\nOut[142]: \n    host service  no\n0  other     web   2\n1   that    mail   1\n2   this    mail   2\n  Grouping like Python\u2019s itertools.groupby \nIn [143]: df = pd.DataFrame([0, 1, 0, 1, 1, 1, 0, 1, 1], columns=[\"A\"])\n\nIn [144]: df[\"A\"].groupby((df[\"A\"] != df[\"A\"].shift()).cumsum()).groups\nOut[144]: {1: [0], 2: [1], 3: [2], 4: [3, 4, 5], 5: [6], 6: [7, 8]}\n\nIn [145]: df[\"A\"].groupby((df[\"A\"] != df[\"A\"].shift()).cumsum()).cumsum()\nOut[145]: \n0    0\n1    1\n2    0\n3    1\n4    2\n5    3\n6    0\n7    1\n8    2\nName: A, dtype: int64\n   Expanding data Alignment and to-date Rolling Computation window based on values instead of counts Rolling Mean by Time Interval   Splitting Splitting a frame Create a list of dataframes, split using a delineation based on logic included in rows. \nIn [146]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"Case\": [\"A\", \"A\", \"A\", \"B\", \"A\", \"A\", \"B\", \"A\", \"A\"],\n   .....:         \"Data\": np.random.randn(9),\n   .....:     }\n   .....: )\n   .....: \n\nIn [147]: dfs = list(\n   .....:     zip(\n   .....:         *df.groupby(\n   .....:             (1 * (df[\"Case\"] == \"B\"))\n   .....:             .cumsum()\n   .....:             .rolling(window=3, min_periods=1)\n   .....:             .median()\n   .....:         )\n   .....:     )\n   .....: )[-1]\n   .....: \n\nIn [148]: dfs[0]\nOut[148]: \n  Case      Data\n0    A  0.276232\n1    A -1.087401\n2    A -0.673690\n3    B  0.113648\n\nIn [149]: dfs[1]\nOut[149]: \n  Case      Data\n4    A -1.478427\n5    A  0.524988\n6    B  0.404705\n\nIn [150]: dfs[2]\nOut[150]: \n  Case      Data\n7    A  0.577046\n8    A -1.715002\n    Pivot The Pivot docs. Partial sums and subtotals \nIn [151]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"Province\": [\"ON\", \"QC\", \"BC\", \"AL\", \"AL\", \"MN\", \"ON\"],\n   .....:         \"City\": [\n   .....:             \"Toronto\",\n   .....:             \"Montreal\",\n   .....:             \"Vancouver\",\n   .....:             \"Calgary\",\n   .....:             \"Edmonton\",\n   .....:             \"Winnipeg\",\n   .....:             \"Windsor\",\n   .....:         ],\n   .....:         \"Sales\": [13, 6, 16, 8, 4, 3, 1],\n   .....:     }\n   .....: )\n   .....: \n\nIn [152]: table = pd.pivot_table(\n   .....:     df,\n   .....:     values=[\"Sales\"],\n   .....:     index=[\"Province\"],\n   .....:     columns=[\"City\"],\n   .....:     aggfunc=np.sum,\n   .....:     margins=True,\n   .....: )\n   .....: \n\nIn [153]: table.stack(\"City\")\nOut[153]: \n                    Sales\nProvince City            \nAL       All         12.0\n         Calgary      8.0\n         Edmonton     4.0\nBC       All         16.0\n         Vancouver   16.0\n...                   ...\nAll      Montreal     6.0\n         Toronto     13.0\n         Vancouver   16.0\n         Windsor      1.0\n         Winnipeg     3.0\n\n[20 rows x 1 columns]\n  Frequency table like plyr in R \nIn [154]: grades = [48, 99, 75, 80, 42, 80, 72, 68, 36, 78]\n\nIn [155]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"ID\": [\"x%d\" % r for r in range(10)],\n   .....:         \"Gender\": [\"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"M\", \"M\"],\n   .....:         \"ExamYear\": [\n   .....:             \"2007\",\n   .....:             \"2007\",\n   .....:             \"2007\",\n   .....:             \"2008\",\n   .....:             \"2008\",\n   .....:             \"2008\",\n   .....:             \"2008\",\n   .....:             \"2009\",\n   .....:             \"2009\",\n   .....:             \"2009\",\n   .....:         ],\n   .....:         \"Class\": [\n   .....:             \"algebra\",\n   .....:             \"stats\",\n   .....:             \"bio\",\n   .....:             \"algebra\",\n   .....:             \"algebra\",\n   .....:             \"stats\",\n   .....:             \"stats\",\n   .....:             \"algebra\",\n   .....:             \"bio\",\n   .....:             \"bio\",\n   .....:         ],\n   .....:         \"Participated\": [\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"no\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:         ],\n   .....:         \"Passed\": [\"yes\" if x > 50 else \"no\" for x in grades],\n   .....:         \"Employed\": [\n   .....:             True,\n   .....:             True,\n   .....:             True,\n   .....:             False,\n   .....:             False,\n   .....:             False,\n   .....:             False,\n   .....:             True,\n   .....:             True,\n   .....:             False,\n   .....:         ],\n   .....:         \"Grade\": grades,\n   .....:     }\n   .....: )\n   .....: \n\nIn [156]: df.groupby(\"ExamYear\").agg(\n   .....:     {\n   .....:         \"Participated\": lambda x: x.value_counts()[\"yes\"],\n   .....:         \"Passed\": lambda x: sum(x == \"yes\"),\n   .....:         \"Employed\": lambda x: sum(x),\n   .....:         \"Grade\": lambda x: sum(x) / len(x),\n   .....:     }\n   .....: )\n   .....: \nOut[156]: \n          Participated  Passed  Employed      Grade\nExamYear                                           \n2007                 3       2         3  74.000000\n2008                 3       3         0  68.500000\n2009                 3       2         2  60.666667\n  Plot pandas DataFrame with year over year data To create year and month cross tabulation: \nIn [157]: df = pd.DataFrame(\n   .....:     {\"value\": np.random.randn(36)},\n   .....:     index=pd.date_range(\"2011-01-01\", freq=\"M\", periods=36),\n   .....: )\n   .....: \n\nIn [158]: pd.pivot_table(\n   .....:     df, index=df.index.month, columns=df.index.year, values=\"value\", aggfunc=\"sum\"\n   .....: )\n   .....: \nOut[158]: \n        2011      2012      2013\n1  -1.039268 -0.968914  2.565646\n2  -0.370647 -1.294524  1.431256\n3  -1.157892  0.413738  1.340309\n4  -1.344312  0.276662 -1.170299\n5   0.844885 -0.472035 -0.226169\n6   1.075770 -0.013960  0.410835\n7  -0.109050 -0.362543  0.813850\n8   1.643563 -0.006154  0.132003\n9  -1.469388 -0.923061 -0.827317\n10  0.357021  0.895717 -0.076467\n11 -0.674600  0.805244 -1.187678\n12 -1.776904 -1.206412  1.130127\n    Apply Rolling apply to organize - Turning embedded lists into a MultiIndex frame \nIn [159]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"A\": [[2, 4, 8, 16], [100, 200], [10, 20, 30]],\n   .....:         \"B\": [[\"a\", \"b\", \"c\"], [\"jj\", \"kk\"], [\"ccc\"]],\n   .....:     },\n   .....:     index=[\"I\", \"II\", \"III\"],\n   .....: )\n   .....: \n\nIn [160]: def SeriesFromSubList(aList):\n   .....:     return pd.Series(aList)\n   .....: \n\nIn [161]: df_orgz = pd.concat(\n   .....:     {ind: row.apply(SeriesFromSubList) for ind, row in df.iterrows()}\n   .....: )\n   .....: \n\nIn [162]: df_orgz\nOut[162]: \n         0     1     2     3\nI   A    2     4     8  16.0\n    B    a     b     c   NaN\nII  A  100   200   NaN   NaN\n    B   jj    kk   NaN   NaN\nIII A   10  20.0  30.0   NaN\n    B  ccc   NaN   NaN   NaN\n  Rolling apply with a DataFrame returning a Series Rolling Apply to multiple columns where function calculates a Series before a Scalar from the Series is returned \nIn [163]: df = pd.DataFrame(\n   .....:     data=np.random.randn(2000, 2) / 10000,\n   .....:     index=pd.date_range(\"2001-01-01\", periods=2000),\n   .....:     columns=[\"A\", \"B\"],\n   .....: )\n   .....: \n\nIn [164]: df\nOut[164]: \n                   A         B\n2001-01-01 -0.000144 -0.000141\n2001-01-02  0.000161  0.000102\n2001-01-03  0.000057  0.000088\n2001-01-04 -0.000221  0.000097\n2001-01-05 -0.000201 -0.000041\n...              ...       ...\n2006-06-19  0.000040 -0.000235\n2006-06-20 -0.000123 -0.000021\n2006-06-21 -0.000113  0.000114\n2006-06-22  0.000136  0.000109\n2006-06-23  0.000027  0.000030\n\n[2000 rows x 2 columns]\n\nIn [165]: def gm(df, const):\n   .....:     v = ((((df[\"A\"] + df[\"B\"]) + 1).cumprod()) - 1) * const\n   .....:     return v.iloc[-1]\n   .....: \n\nIn [166]: s = pd.Series(\n   .....:     {\n   .....:         df.index[i]: gm(df.iloc[i: min(i + 51, len(df) - 1)], 5)\n   .....:         for i in range(len(df) - 50)\n   .....:     }\n   .....: )\n   .....: \n\nIn [167]: s\nOut[167]: \n2001-01-01    0.000930\n2001-01-02    0.002615\n2001-01-03    0.001281\n2001-01-04    0.001117\n2001-01-05    0.002772\n                ...   \n2006-04-30    0.003296\n2006-05-01    0.002629\n2006-05-02    0.002081\n2006-05-03    0.004247\n2006-05-04    0.003928\nLength: 1950, dtype: float64\n  Rolling apply with a DataFrame returning a Scalar Rolling Apply to multiple columns where function returns a Scalar (Volume Weighted Average Price) \nIn [168]: rng = pd.date_range(start=\"2014-01-01\", periods=100)\n\nIn [169]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"Open\": np.random.randn(len(rng)),\n   .....:         \"Close\": np.random.randn(len(rng)),\n   .....:         \"Volume\": np.random.randint(100, 2000, len(rng)),\n   .....:     },\n   .....:     index=rng,\n   .....: )\n   .....: \n\nIn [170]: df\nOut[170]: \n                Open     Close  Volume\n2014-01-01 -1.611353 -0.492885    1219\n2014-01-02 -3.000951  0.445794    1054\n2014-01-03 -0.138359 -0.076081    1381\n2014-01-04  0.301568  1.198259    1253\n2014-01-05  0.276381 -0.669831    1728\n...              ...       ...     ...\n2014-04-06 -0.040338  0.937843    1188\n2014-04-07  0.359661 -0.285908    1864\n2014-04-08  0.060978  1.714814     941\n2014-04-09  1.759055 -0.455942    1065\n2014-04-10  0.138185 -1.147008    1453\n\n[100 rows x 3 columns]\n\nIn [171]: def vwap(bars):\n   .....:     return (bars.Close * bars.Volume).sum() / bars.Volume.sum()\n   .....: \n\nIn [172]: window = 5\n\nIn [173]: s = pd.concat(\n   .....:     [\n   .....:         (pd.Series(vwap(df.iloc[i: i + window]), index=[df.index[i + window]]))\n   .....:         for i in range(len(df) - window)\n   .....:     ]\n   .....: )\n   .....: \n\nIn [174]: s.round(2)\nOut[174]: \n2014-01-06    0.02\n2014-01-07    0.11\n2014-01-08    0.10\n2014-01-09    0.07\n2014-01-10   -0.29\n              ... \n2014-04-06   -0.63\n2014-04-07   -0.02\n2014-04-08   -0.03\n2014-04-09    0.34\n2014-04-10    0.29\nLength: 95, dtype: float64\n     Timeseries Between times Using indexer between time Constructing a datetime range that excludes weekends and includes only certain times Vectorized Lookup Aggregation and plotting time series Turn a matrix with hours in columns and days in rows into a continuous row sequence in the form of a time series. How to rearrange a Python pandas DataFrame? Dealing with duplicates when reindexing a timeseries to a specified frequency Calculate the first day of the month for each entry in a DatetimeIndex \nIn [175]: dates = pd.date_range(\"2000-01-01\", periods=5)\n\nIn [176]: dates.to_period(freq=\"M\").to_timestamp()\nOut[176]: \nDatetimeIndex(['2000-01-01', '2000-01-01', '2000-01-01', '2000-01-01',\n               '2000-01-01'],\n              dtype='datetime64[ns]', freq=None)\n   Resampling The Resample docs. Using Grouper instead of TimeGrouper for time grouping of values Time grouping with some missing values Valid frequency arguments to Grouper Timeseries Grouping using a MultiIndex Using TimeGrouper and another grouping to create subgroups, then apply a custom function GH3791 Resampling with custom periods Resample intraday frame without adding new days Resample minute data Resample with groupby    Merge The Join docs. Concatenate two dataframes with overlapping index (emulate R rbind) \nIn [177]: rng = pd.date_range(\"2000-01-01\", periods=6)\n\nIn [178]: df1 = pd.DataFrame(np.random.randn(6, 3), index=rng, columns=[\"A\", \"B\", \"C\"])\n\nIn [179]: df2 = df1.copy()\n  Depending on df construction, ignore_index may be needed \nIn [180]: df = pd.concat([df1, df2], ignore_index=True)\n\nIn [181]: df\nOut[181]: \n           A         B         C\n0  -0.870117 -0.479265 -0.790855\n1   0.144817  1.726395 -0.464535\n2  -0.821906  1.597605  0.187307\n3  -0.128342 -1.511638 -0.289858\n4   0.399194 -1.430030 -0.639760\n5   1.115116 -2.012600  1.810662\n6  -0.870117 -0.479265 -0.790855\n7   0.144817  1.726395 -0.464535\n8  -0.821906  1.597605  0.187307\n9  -0.128342 -1.511638 -0.289858\n10  0.399194 -1.430030 -0.639760\n11  1.115116 -2.012600  1.810662\n  Self Join of a DataFrame GH2996 \nIn [182]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"Area\": [\"A\"] * 5 + [\"C\"] * 2,\n   .....:         \"Bins\": [110] * 2 + [160] * 3 + [40] * 2,\n   .....:         \"Test_0\": [0, 1, 0, 1, 2, 0, 1],\n   .....:         \"Data\": np.random.randn(7),\n   .....:     }\n   .....: )\n   .....: \n\nIn [183]: df\nOut[183]: \n  Area  Bins  Test_0      Data\n0    A   110       0 -0.433937\n1    A   110       1 -0.160552\n2    A   160       0  0.744434\n3    A   160       1  1.754213\n4    A   160       2  0.000850\n5    C    40       0  0.342243\n6    C    40       1  1.070599\n\nIn [184]: df[\"Test_1\"] = df[\"Test_0\"] - 1\n\nIn [185]: pd.merge(\n   .....:     df,\n   .....:     df,\n   .....:     left_on=[\"Bins\", \"Area\", \"Test_0\"],\n   .....:     right_on=[\"Bins\", \"Area\", \"Test_1\"],\n   .....:     suffixes=(\"_L\", \"_R\"),\n   .....: )\n   .....: \nOut[185]: \n  Area  Bins  Test_0_L    Data_L  Test_1_L  Test_0_R    Data_R  Test_1_R\n0    A   110         0 -0.433937        -1         1 -0.160552         0\n1    A   160         0  0.744434        -1         1  1.754213         0\n2    A   160         1  1.754213         0         2  0.000850         1\n3    C    40         0  0.342243        -1         1  1.070599         0\n  How to set the index and join KDB like asof join Join with a criteria based on the values Using searchsorted to merge based on values inside a range   Plotting The Plotting docs. Make Matplotlib look like R Setting x-axis major and minor labels Plotting multiple charts in an IPython Jupyter notebook Creating a multi-line plot Plotting a heatmap Annotate a time-series plot Annotate a time-series plot #2 Generate Embedded plots in excel files using Pandas, Vincent and xlsxwriter Boxplot for each quartile of a stratifying variable \nIn [186]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"stratifying_var\": np.random.uniform(0, 100, 20),\n   .....:         \"price\": np.random.normal(100, 5, 20),\n   .....:     }\n   .....: )\n   .....: \n\nIn [187]: df[\"quartiles\"] = pd.qcut(\n   .....:     df[\"stratifying_var\"], 4, labels=[\"0-25%\", \"25-50%\", \"50-75%\", \"75-100%\"]\n   .....: )\n   .....: \n\nIn [188]: df.boxplot(column=\"price\", by=\"quartiles\")\nOut[188]: <AxesSubplot:title={'center':'price'}, xlabel='quartiles'>\n     Data in/out Performance comparison of SQL vs HDF5  CSV The CSV docs read_csv in action appending to a csv Reading a csv chunk-by-chunk Reading only certain rows of a csv chunk-by-chunk Reading the first few lines of a frame Reading a file that is compressed but not by gzip/bz2 (the native compressed formats which read_csv understands). This example shows a WinZipped file, but is a general application of opening the file within a context manager and using that handle to read. See here Inferring dtypes from a file Dealing with bad lines GH2886 Write a multi-row index CSV without writing duplicates  Reading multiple files to create a single DataFrame The best way to combine multiple files into a single DataFrame is to read the individual frames one by one, put all of the individual frames into a list, and then combine the frames in the list using pd.concat(): \nIn [189]: for i in range(3):\n   .....:     data = pd.DataFrame(np.random.randn(10, 4))\n   .....:     data.to_csv(\"file_{}.csv\".format(i))\n   .....: \n\nIn [190]: files = [\"file_0.csv\", \"file_1.csv\", \"file_2.csv\"]\n\nIn [191]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n  You can use the same approach to read all files matching a pattern. Here is an example using glob: \nIn [192]: import glob\n\nIn [193]: import os\n\nIn [194]: files = glob.glob(\"file_*.csv\")\n\nIn [195]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n  Finally, this strategy will work with the other pd.read_*(...) functions described in the io docs.   Parsing date components in multi-columns Parsing date components in multi-columns is faster with a format \nIn [196]: i = pd.date_range(\"20000101\", periods=10000)\n\nIn [197]: df = pd.DataFrame({\"year\": i.year, \"month\": i.month, \"day\": i.day})\n\nIn [198]: df.head()\nOut[198]: \n   year  month  day\n0  2000      1    1\n1  2000      1    2\n2  2000      1    3\n3  2000      1    4\n4  2000      1    5\n\nIn [199]: %timeit pd.to_datetime(df.year * 10000 + df.month * 100 + df.day, format='%Y%m%d')\n   .....: ds = df.apply(lambda x: \"%04d%02d%02d\" % (x[\"year\"], x[\"month\"], x[\"day\"]), axis=1)\n   .....: ds.head()\n   .....: %timeit pd.to_datetime(ds)\n   .....: \n8.7 ms +- 765 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n2.1 ms +- 419 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n    Skip row between header and data \nIn [200]: data = \"\"\";;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....: ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....: ;;;;\n   .....: date;Param1;Param2;Param4;Param5\n   .....:     ;m\u00b2;\u00b0C;m\u00b2;m\n   .....: ;;;;\n   .....: 01.01.1990 00:00;1;1;2;3\n   .....: 01.01.1990 01:00;5;3;4;5\n   .....: 01.01.1990 02:00;9;5;6;7\n   .....: 01.01.1990 03:00;13;7;8;9\n   .....: 01.01.1990 04:00;17;9;10;11\n   .....: 01.01.1990 05:00;21;11;12;13\n   .....: \"\"\"\n   .....: \n   Option 1: pass rows explicitly to skip rows \nIn [201]: from io import StringIO\n\nIn [202]: pd.read_csv(\n   .....:     StringIO(data),\n   .....:     sep=\";\",\n   .....:     skiprows=[11, 12],\n   .....:     index_col=0,\n   .....:     parse_dates=True,\n   .....:     header=10,\n   .....: )\n   .....: \nOut[202]: \n                     Param1  Param2  Param4  Param5\ndate                                               \n1990-01-01 00:00:00       1       1       2       3\n1990-01-01 01:00:00       5       3       4       5\n1990-01-01 02:00:00       9       5       6       7\n1990-01-01 03:00:00      13       7       8       9\n1990-01-01 04:00:00      17       9      10      11\n1990-01-01 05:00:00      21      11      12      13\n    Option 2: read column names and then data \nIn [203]: pd.read_csv(StringIO(data), sep=\";\", header=10, nrows=10).columns\nOut[203]: Index(['date', 'Param1', 'Param2', 'Param4', 'Param5'], dtype='object')\n\nIn [204]: columns = pd.read_csv(StringIO(data), sep=\";\", header=10, nrows=10).columns\n\nIn [205]: pd.read_csv(\n   .....:     StringIO(data), sep=\";\", index_col=0, header=12, parse_dates=True, names=columns\n   .....: )\n   .....: \nOut[205]: \n                     Param1  Param2  Param4  Param5\ndate                                               \n1990-01-01 00:00:00       1       1       2       3\n1990-01-01 01:00:00       5       3       4       5\n1990-01-01 02:00:00       9       5       6       7\n1990-01-01 03:00:00      13       7       8       9\n1990-01-01 04:00:00      17       9      10      11\n1990-01-01 05:00:00      21      11      12      13\n      SQL The SQL docs Reading from databases with SQL   Excel The Excel docs Reading from a filelike handle Modifying formatting in XlsxWriter output Loading only visible sheets GH19842#issuecomment-892150745   HTML Reading HTML tables from a server that cannot handle the default request header   HDFStore The HDFStores docs Simple queries with a Timestamp Index Managing heterogeneous data using a linked multiple table hierarchy GH3032 Merging on-disk tables with millions of rows Avoiding inconsistencies when writing to a store from multiple processes/threads De-duplicating a large store by chunks, essentially a recursive reduction operation. Shows a function for taking in data from csv file and creating a store by chunks, with date parsing as well. See here Creating a store chunk-by-chunk from a csv file Appending to a store, while creating a unique index Large Data work flows Reading in a sequence of files, then providing a global unique index to a store while appending Groupby on a HDFStore with low group density Groupby on a HDFStore with high group density Hierarchical queries on a HDFStore Counting with a HDFStore Troubleshoot HDFStore exceptions Setting min_itemsize with strings Using ptrepack to create a completely-sorted-index on a store Storing Attributes to a group node \nIn [206]: df = pd.DataFrame(np.random.randn(8, 3))\n\nIn [207]: store = pd.HDFStore(\"test.h5\")\n\nIn [208]: store.put(\"df\", df)\n\n# you can store an arbitrary Python object via pickle\nIn [209]: store.get_storer(\"df\").attrs.my_attribute = {\"A\": 10}\n\nIn [210]: store.get_storer(\"df\").attrs.my_attribute\nOut[210]: {'A': 10}\n  You can create or load a HDFStore in-memory by passing the driver parameter to PyTables. Changes are only written to disk when the HDFStore is closed. \nIn [211]: store = pd.HDFStore(\"test.h5\", \"w\", driver=\"H5FD_CORE\")\n\nIn [212]: df = pd.DataFrame(np.random.randn(8, 3))\n\nIn [213]: store[\"test\"] = df\n\n# only after closing the store, data is written to disk:\nIn [214]: store.close()\n    Binary files pandas readily accepts NumPy record arrays, if you need to read in a binary file consisting of an array of C structs. For example, given this C program in a file called main.c compiled with gcc main.c -std=gnu99 on a 64-bit machine, \n#include <stdio.h>\n#include <stdint.h>\n\ntypedef struct _Data\n{\n    int32_t count;\n    double avg;\n    float scale;\n} Data;\n\nint main(int argc, const char *argv[])\n{\n    size_t n = 10;\n    Data d[n];\n\n    for (int i = 0; i < n; ++i)\n    {\n        d[i].count = i;\n        d[i].avg = i + 1.0;\n        d[i].scale = (float) i + 2.0f;\n    }\n\n    FILE *file = fopen(\"binary.dat\", \"wb\");\n    fwrite(&d, sizeof(Data), n, file);\n    fclose(file);\n\n    return 0;\n}\n  the following Python code will read the binary file 'binary.dat' into a pandas DataFrame, where each element of the struct corresponds to a column in the frame: \nnames = \"count\", \"avg\", \"scale\"\n\n# note that the offsets are larger than the size of the type because of\n# struct padding\noffsets = 0, 8, 16\nformats = \"i4\", \"f8\", \"f4\"\ndt = np.dtype({\"names\": names, \"offsets\": offsets, \"formats\": formats}, align=True)\ndf = pd.DataFrame(np.fromfile(\"binary.dat\", dt))\n   Note The offsets of the structure elements may be different depending on the architecture of the machine on which the file was created. Using a raw binary file format like this for general data storage is not recommended, as it is not cross platform. We recommended either HDF5 or parquet, both of which are supported by pandas\u2019 IO facilities.     Computation Numerical integration (sample-based) of a time series  Correlation Often it\u2019s useful to obtain the lower (or upper) triangular form of a correlation matrix calculated from DataFrame.corr(). This can be achieved by passing a boolean mask to where as follows: \nIn [215]: df = pd.DataFrame(np.random.random(size=(100, 5)))\n\nIn [216]: corr_mat = df.corr()\n\nIn [217]: mask = np.tril(np.ones_like(corr_mat, dtype=np.bool_), k=-1)\n\nIn [218]: corr_mat.where(mask)\nOut[218]: \n          0         1         2        3   4\n0       NaN       NaN       NaN      NaN NaN\n1 -0.079861       NaN       NaN      NaN NaN\n2 -0.236573  0.183801       NaN      NaN NaN\n3 -0.013795 -0.051975  0.037235      NaN NaN\n4 -0.031974  0.118342 -0.073499 -0.02063 NaN\n  The method argument within DataFrame.corr can accept a callable in addition to the named correlation types. Here we compute the distance correlation matrix for a DataFrame object. \nIn [219]: def distcorr(x, y):\n   .....:     n = len(x)\n   .....:     a = np.zeros(shape=(n, n))\n   .....:     b = np.zeros(shape=(n, n))\n   .....:     for i in range(n):\n   .....:         for j in range(i + 1, n):\n   .....:             a[i, j] = abs(x[i] - x[j])\n   .....:             b[i, j] = abs(y[i] - y[j])\n   .....:     a += a.T\n   .....:     b += b.T\n   .....:     a_bar = np.vstack([np.nanmean(a, axis=0)] * n)\n   .....:     b_bar = np.vstack([np.nanmean(b, axis=0)] * n)\n   .....:     A = a - a_bar - a_bar.T + np.full(shape=(n, n), fill_value=a_bar.mean())\n   .....:     B = b - b_bar - b_bar.T + np.full(shape=(n, n), fill_value=b_bar.mean())\n   .....:     cov_ab = np.sqrt(np.nansum(A * B)) / n\n   .....:     std_a = np.sqrt(np.sqrt(np.nansum(A ** 2)) / n)\n   .....:     std_b = np.sqrt(np.sqrt(np.nansum(B ** 2)) / n)\n   .....:     return cov_ab / std_a / std_b\n   .....: \n\nIn [220]: df = pd.DataFrame(np.random.normal(size=(100, 3)))\n\nIn [221]: df.corr(method=distcorr)\nOut[221]: \n          0         1         2\n0  1.000000  0.197613  0.216328\n1  0.197613  1.000000  0.208749\n2  0.216328  0.208749  1.000000\n     Timedeltas The Timedeltas docs. Using timedeltas \nIn [222]: import datetime\n\nIn [223]: s = pd.Series(pd.date_range(\"2012-1-1\", periods=3, freq=\"D\"))\n\nIn [224]: s - s.max()\nOut[224]: \n0   -2 days\n1   -1 days\n2    0 days\ndtype: timedelta64[ns]\n\nIn [225]: s.max() - s\nOut[225]: \n0   2 days\n1   1 days\n2   0 days\ndtype: timedelta64[ns]\n\nIn [226]: s - datetime.datetime(2011, 1, 1, 3, 5)\nOut[226]: \n0   364 days 20:55:00\n1   365 days 20:55:00\n2   366 days 20:55:00\ndtype: timedelta64[ns]\n\nIn [227]: s + datetime.timedelta(minutes=5)\nOut[227]: \n0   2012-01-01 00:05:00\n1   2012-01-02 00:05:00\n2   2012-01-03 00:05:00\ndtype: datetime64[ns]\n\nIn [228]: datetime.datetime(2011, 1, 1, 3, 5) - s\nOut[228]: \n0   -365 days +03:05:00\n1   -366 days +03:05:00\n2   -367 days +03:05:00\ndtype: timedelta64[ns]\n\nIn [229]: datetime.timedelta(minutes=5) + s\nOut[229]: \n0   2012-01-01 00:05:00\n1   2012-01-02 00:05:00\n2   2012-01-03 00:05:00\ndtype: datetime64[ns]\n  Adding and subtracting deltas and dates \nIn [230]: deltas = pd.Series([datetime.timedelta(days=i) for i in range(3)])\n\nIn [231]: df = pd.DataFrame({\"A\": s, \"B\": deltas})\n\nIn [232]: df\nOut[232]: \n           A      B\n0 2012-01-01 0 days\n1 2012-01-02 1 days\n2 2012-01-03 2 days\n\nIn [233]: df[\"New Dates\"] = df[\"A\"] + df[\"B\"]\n\nIn [234]: df[\"Delta\"] = df[\"A\"] - df[\"New Dates\"]\n\nIn [235]: df\nOut[235]: \n           A      B  New Dates   Delta\n0 2012-01-01 0 days 2012-01-01  0 days\n1 2012-01-02 1 days 2012-01-03 -1 days\n2 2012-01-03 2 days 2012-01-05 -2 days\n\nIn [236]: df.dtypes\nOut[236]: \nA             datetime64[ns]\nB            timedelta64[ns]\nNew Dates     datetime64[ns]\nDelta        timedelta64[ns]\ndtype: object\n  Another example Values can be set to NaT using np.nan, similar to datetime \nIn [237]: y = s - s.shift()\n\nIn [238]: y\nOut[238]: \n0      NaT\n1   1 days\n2   1 days\ndtype: timedelta64[ns]\n\nIn [239]: y[1] = np.nan\n\nIn [240]: y\nOut[240]: \n0      NaT\n1      NaT\n2   1 days\ndtype: timedelta64[ns]\n    Creating example data To create a dataframe from every combination of some given values, like R\u2019s expand.grid() function, we can create a dict where the keys are column names and the values are lists of the data values: \nIn [241]: def expand_grid(data_dict):\n   .....:     rows = itertools.product(*data_dict.values())\n   .....:     return pd.DataFrame.from_records(rows, columns=data_dict.keys())\n   .....: \n\nIn [242]: df = expand_grid(\n   .....:     {\"height\": [60, 70], \"weight\": [100, 140, 180], \"sex\": [\"Male\", \"Female\"]}\n   .....: )\n   .....: \n\nIn [243]: df\nOut[243]: \n    height  weight     sex\n0       60     100    Male\n1       60     100  Female\n2       60     140    Male\n3       60     140  Female\n4       60     180    Male\n5       60     180  Female\n6       70     100    Male\n7       70     100  Female\n8       70     140    Male\n9       70     140  Female\n10      70     180    Male\n11      70     180  Female\n  \n"}, {"name": "DataFrame", "path": "reference/frame", "type": "DataFrame", "text": "DataFrame  Constructor       \nDataFrame([data, index, columns, dtype, copy]) Two-dimensional, size-mutable, potentially heterogeneous tabular data.      Attributes and underlying data Axes       \nDataFrame.index The index (row labels) of the DataFrame.  \nDataFrame.columns The column labels of the DataFrame.          \nDataFrame.dtypes Return the dtypes in the DataFrame.  \nDataFrame.info([verbose, buf, max_cols, ...]) Print a concise summary of a DataFrame.  \nDataFrame.select_dtypes([include, exclude]) Return a subset of the DataFrame's columns based on the column dtypes.  \nDataFrame.values Return a Numpy representation of the DataFrame.  \nDataFrame.axes Return a list representing the axes of the DataFrame.  \nDataFrame.ndim Return an int representing the number of axes / array dimensions.  \nDataFrame.size Return an int representing the number of elements in this object.  \nDataFrame.shape Return a tuple representing the dimensionality of the DataFrame.  \nDataFrame.memory_usage([index, deep]) Return the memory usage of each column in bytes.  \nDataFrame.empty Indicator whether Series/DataFrame is empty.  \nDataFrame.set_flags(*[, copy, ...]) Return a new object with updated flags.      Conversion       \nDataFrame.astype(dtype[, copy, errors]) Cast a pandas object to a specified dtype dtype.  \nDataFrame.convert_dtypes([infer_objects, ...]) Convert columns to best possible dtypes using dtypes supporting pd.NA.  \nDataFrame.infer_objects() Attempt to infer better dtypes for object columns.  \nDataFrame.copy([deep]) Make a copy of this object's indices and data.  \nDataFrame.bool() Return the bool of a single element Series or DataFrame.      Indexing, iteration       \nDataFrame.head([n]) Return the first n rows.  \nDataFrame.at Access a single value for a row/column label pair.  \nDataFrame.iat Access a single value for a row/column pair by integer position.  \nDataFrame.loc Access a group of rows and columns by label(s) or a boolean array.  \nDataFrame.iloc Purely integer-location based indexing for selection by position.  \nDataFrame.insert(loc, column, value[, ...]) Insert column into DataFrame at specified location.  \nDataFrame.__iter__() Iterate over info axis.  \nDataFrame.items() Iterate over (column name, Series) pairs.  \nDataFrame.iteritems() Iterate over (column name, Series) pairs.  \nDataFrame.keys() Get the 'info axis' (see Indexing for more).  \nDataFrame.iterrows() Iterate over DataFrame rows as (index, Series) pairs.  \nDataFrame.itertuples([index, name]) Iterate over DataFrame rows as namedtuples.  \nDataFrame.lookup(row_labels, col_labels) (DEPRECATED) Label-based \"fancy indexing\" function for DataFrame.  \nDataFrame.pop(item) Return item and drop from frame.  \nDataFrame.tail([n]) Return the last n rows.  \nDataFrame.xs(key[, axis, level, drop_level]) Return cross-section from the Series/DataFrame.  \nDataFrame.get(key[, default]) Get item from object for given key (ex: DataFrame column).  \nDataFrame.isin(values) Whether each element in the DataFrame is contained in values.  \nDataFrame.where(cond[, other, inplace, ...]) Replace values where the condition is False.  \nDataFrame.mask(cond[, other, inplace, axis, ...]) Replace values where the condition is True.  \nDataFrame.query(expr[, inplace]) Query the columns of a DataFrame with a boolean expression.    For more information on .at, .iat, .loc, and .iloc, see the indexing documentation.   Binary operator functions       \nDataFrame.add(other[, axis, level, fill_value]) Get Addition of dataframe and other, element-wise (binary operator add).  \nDataFrame.sub(other[, axis, level, fill_value]) Get Subtraction of dataframe and other, element-wise (binary operator sub).  \nDataFrame.mul(other[, axis, level, fill_value]) Get Multiplication of dataframe and other, element-wise (binary operator mul).  \nDataFrame.div(other[, axis, level, fill_value]) Get Floating division of dataframe and other, element-wise (binary operator truediv).  \nDataFrame.truediv(other[, axis, level, ...]) Get Floating division of dataframe and other, element-wise (binary operator truediv).  \nDataFrame.floordiv(other[, axis, level, ...]) Get Integer division of dataframe and other, element-wise (binary operator floordiv).  \nDataFrame.mod(other[, axis, level, fill_value]) Get Modulo of dataframe and other, element-wise (binary operator mod).  \nDataFrame.pow(other[, axis, level, fill_value]) Get Exponential power of dataframe and other, element-wise (binary operator pow).  \nDataFrame.dot(other) Compute the matrix multiplication between the DataFrame and other.  \nDataFrame.radd(other[, axis, level, fill_value]) Get Addition of dataframe and other, element-wise (binary operator radd).  \nDataFrame.rsub(other[, axis, level, fill_value]) Get Subtraction of dataframe and other, element-wise (binary operator rsub).  \nDataFrame.rmul(other[, axis, level, fill_value]) Get Multiplication of dataframe and other, element-wise (binary operator rmul).  \nDataFrame.rdiv(other[, axis, level, fill_value]) Get Floating division of dataframe and other, element-wise (binary operator rtruediv).  \nDataFrame.rtruediv(other[, axis, level, ...]) Get Floating division of dataframe and other, element-wise (binary operator rtruediv).  \nDataFrame.rfloordiv(other[, axis, level, ...]) Get Integer division of dataframe and other, element-wise (binary operator rfloordiv).  \nDataFrame.rmod(other[, axis, level, fill_value]) Get Modulo of dataframe and other, element-wise (binary operator rmod).  \nDataFrame.rpow(other[, axis, level, fill_value]) Get Exponential power of dataframe and other, element-wise (binary operator rpow).  \nDataFrame.lt(other[, axis, level]) Get Less than of dataframe and other, element-wise (binary operator lt).  \nDataFrame.gt(other[, axis, level]) Get Greater than of dataframe and other, element-wise (binary operator gt).  \nDataFrame.le(other[, axis, level]) Get Less than or equal to of dataframe and other, element-wise (binary operator le).  \nDataFrame.ge(other[, axis, level]) Get Greater than or equal to of dataframe and other, element-wise (binary operator ge).  \nDataFrame.ne(other[, axis, level]) Get Not equal to of dataframe and other, element-wise (binary operator ne).  \nDataFrame.eq(other[, axis, level]) Get Equal to of dataframe and other, element-wise (binary operator eq).  \nDataFrame.combine(other, func[, fill_value, ...]) Perform column-wise combine with another DataFrame.  \nDataFrame.combine_first(other) Update null elements with value in the same location in other.      Function application, GroupBy & window       \nDataFrame.apply(func[, axis, raw, ...]) Apply a function along an axis of the DataFrame.  \nDataFrame.applymap(func[, na_action]) Apply a function to a Dataframe elementwise.  \nDataFrame.pipe(func, *args, **kwargs) Apply chainable functions that expect Series or DataFrames.  \nDataFrame.agg([func, axis]) Aggregate using one or more operations over the specified axis.  \nDataFrame.aggregate([func, axis]) Aggregate using one or more operations over the specified axis.  \nDataFrame.transform(func[, axis]) Call func on self producing a DataFrame with the same axis shape as self.  \nDataFrame.groupby([by, axis, level, ...]) Group DataFrame using a mapper or by a Series of columns.  \nDataFrame.rolling(window[, min_periods, ...]) Provide rolling window calculations.  \nDataFrame.expanding([min_periods, center, ...]) Provide expanding window calculations.  \nDataFrame.ewm([com, span, halflife, alpha, ...]) Provide exponentially weighted (EW) calculations.      Computations / descriptive stats       \nDataFrame.abs() Return a Series/DataFrame with absolute numeric value of each element.  \nDataFrame.all([axis, bool_only, skipna, level]) Return whether all elements are True, potentially over an axis.  \nDataFrame.any([axis, bool_only, skipna, level]) Return whether any element is True, potentially over an axis.  \nDataFrame.clip([lower, upper, axis, inplace]) Trim values at input threshold(s).  \nDataFrame.corr([method, min_periods]) Compute pairwise correlation of columns, excluding NA/null values.  \nDataFrame.corrwith(other[, axis, drop, method]) Compute pairwise correlation.  \nDataFrame.count([axis, level, numeric_only]) Count non-NA cells for each column or row.  \nDataFrame.cov([min_periods, ddof]) Compute pairwise covariance of columns, excluding NA/null values.  \nDataFrame.cummax([axis, skipna]) Return cumulative maximum over a DataFrame or Series axis.  \nDataFrame.cummin([axis, skipna]) Return cumulative minimum over a DataFrame or Series axis.  \nDataFrame.cumprod([axis, skipna]) Return cumulative product over a DataFrame or Series axis.  \nDataFrame.cumsum([axis, skipna]) Return cumulative sum over a DataFrame or Series axis.  \nDataFrame.describe([percentiles, include, ...]) Generate descriptive statistics.  \nDataFrame.diff([periods, axis]) First discrete difference of element.  \nDataFrame.eval(expr[, inplace]) Evaluate a string describing operations on DataFrame columns.  \nDataFrame.kurt([axis, skipna, level, ...]) Return unbiased kurtosis over requested axis.  \nDataFrame.kurtosis([axis, skipna, level, ...]) Return unbiased kurtosis over requested axis.  \nDataFrame.mad([axis, skipna, level]) Return the mean absolute deviation of the values over the requested axis.  \nDataFrame.max([axis, skipna, level, ...]) Return the maximum of the values over the requested axis.  \nDataFrame.mean([axis, skipna, level, ...]) Return the mean of the values over the requested axis.  \nDataFrame.median([axis, skipna, level, ...]) Return the median of the values over the requested axis.  \nDataFrame.min([axis, skipna, level, ...]) Return the minimum of the values over the requested axis.  \nDataFrame.mode([axis, numeric_only, dropna]) Get the mode(s) of each element along the selected axis.  \nDataFrame.pct_change([periods, fill_method, ...]) Percentage change between the current and a prior element.  \nDataFrame.prod([axis, skipna, level, ...]) Return the product of the values over the requested axis.  \nDataFrame.product([axis, skipna, level, ...]) Return the product of the values over the requested axis.  \nDataFrame.quantile([q, axis, numeric_only, ...]) Return values at the given quantile over requested axis.  \nDataFrame.rank([axis, method, numeric_only, ...]) Compute numerical data ranks (1 through n) along axis.  \nDataFrame.round([decimals]) Round a DataFrame to a variable number of decimal places.  \nDataFrame.sem([axis, skipna, level, ddof, ...]) Return unbiased standard error of the mean over requested axis.  \nDataFrame.skew([axis, skipna, level, ...]) Return unbiased skew over requested axis.  \nDataFrame.sum([axis, skipna, level, ...]) Return the sum of the values over the requested axis.  \nDataFrame.std([axis, skipna, level, ddof, ...]) Return sample standard deviation over requested axis.  \nDataFrame.var([axis, skipna, level, ddof, ...]) Return unbiased variance over requested axis.  \nDataFrame.nunique([axis, dropna]) Count number of distinct elements in specified axis.  \nDataFrame.value_counts([subset, normalize, ...]) Return a Series containing counts of unique rows in the DataFrame.      Reindexing / selection / label manipulation       \nDataFrame.add_prefix(prefix) Prefix labels with string prefix.  \nDataFrame.add_suffix(suffix) Suffix labels with string suffix.  \nDataFrame.align(other[, join, axis, level, ...]) Align two objects on their axes with the specified join method.  \nDataFrame.at_time(time[, asof, axis]) Select values at particular time of day (e.g., 9:30AM).  \nDataFrame.between_time(start_time, end_time) Select values between particular times of the day (e.g., 9:00-9:30 AM).  \nDataFrame.drop([labels, axis, index, ...]) Drop specified labels from rows or columns.  \nDataFrame.drop_duplicates([subset, keep, ...]) Return DataFrame with duplicate rows removed.  \nDataFrame.duplicated([subset, keep]) Return boolean Series denoting duplicate rows.  \nDataFrame.equals(other) Test whether two objects contain the same elements.  \nDataFrame.filter([items, like, regex, axis]) Subset the dataframe rows or columns according to the specified index labels.  \nDataFrame.first(offset) Select initial periods of time series data based on a date offset.  \nDataFrame.head([n]) Return the first n rows.  \nDataFrame.idxmax([axis, skipna]) Return index of first occurrence of maximum over requested axis.  \nDataFrame.idxmin([axis, skipna]) Return index of first occurrence of minimum over requested axis.  \nDataFrame.last(offset) Select final periods of time series data based on a date offset.  \nDataFrame.reindex([labels, index, columns, ...]) Conform Series/DataFrame to new index with optional filling logic.  \nDataFrame.reindex_like(other[, method, ...]) Return an object with matching indices as other object.  \nDataFrame.rename([mapper, index, columns, ...]) Alter axes labels.  \nDataFrame.rename_axis([mapper, index, ...]) Set the name of the axis for the index or columns.  \nDataFrame.reset_index([level, drop, ...]) Reset the index, or a level of it.  \nDataFrame.sample([n, frac, replace, ...]) Return a random sample of items from an axis of object.  \nDataFrame.set_axis(labels[, axis, inplace]) Assign desired index to given axis.  \nDataFrame.set_index(keys[, drop, append, ...]) Set the DataFrame index using existing columns.  \nDataFrame.tail([n]) Return the last n rows.  \nDataFrame.take(indices[, axis, is_copy]) Return the elements in the given positional indices along an axis.  \nDataFrame.truncate([before, after, axis, copy]) Truncate a Series or DataFrame before and after some index value.      Missing data handling       \nDataFrame.backfill([axis, inplace, limit, ...]) Synonym for DataFrame.fillna() with method='bfill'.  \nDataFrame.bfill([axis, inplace, limit, downcast]) Synonym for DataFrame.fillna() with method='bfill'.  \nDataFrame.dropna([axis, how, thresh, ...]) Remove missing values.  \nDataFrame.ffill([axis, inplace, limit, downcast]) Synonym for DataFrame.fillna() with method='ffill'.  \nDataFrame.fillna([value, method, axis, ...]) Fill NA/NaN values using the specified method.  \nDataFrame.interpolate([method, axis, limit, ...]) Fill NaN values using an interpolation method.  \nDataFrame.isna() Detect missing values.  \nDataFrame.isnull() DataFrame.isnull is an alias for DataFrame.isna.  \nDataFrame.notna() Detect existing (non-missing) values.  \nDataFrame.notnull() DataFrame.notnull is an alias for DataFrame.notna.  \nDataFrame.pad([axis, inplace, limit, downcast]) Synonym for DataFrame.fillna() with method='ffill'.  \nDataFrame.replace([to_replace, value, ...]) Replace values given in to_replace with value.      Reshaping, sorting, transposing       \nDataFrame.droplevel(level[, axis]) Return Series/DataFrame with requested index / column level(s) removed.  \nDataFrame.pivot([index, columns, values]) Return reshaped DataFrame organized by given index / column values.  \nDataFrame.pivot_table([values, index, ...]) Create a spreadsheet-style pivot table as a DataFrame.  \nDataFrame.reorder_levels(order[, axis]) Rearrange index levels using input order.  \nDataFrame.sort_values(by[, axis, ascending, ...]) Sort by the values along either axis.  \nDataFrame.sort_index([axis, level, ...]) Sort object by labels (along an axis).  \nDataFrame.nlargest(n, columns[, keep]) Return the first n rows ordered by columns in descending order.  \nDataFrame.nsmallest(n, columns[, keep]) Return the first n rows ordered by columns in ascending order.  \nDataFrame.swaplevel([i, j, axis]) Swap levels i and j in a MultiIndex.  \nDataFrame.stack([level, dropna]) Stack the prescribed level(s) from columns to index.  \nDataFrame.unstack([level, fill_value]) Pivot a level of the (necessarily hierarchical) index labels.  \nDataFrame.swapaxes(axis1, axis2[, copy]) Interchange axes and swap values axes appropriately.  \nDataFrame.melt([id_vars, value_vars, ...]) Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.  \nDataFrame.explode(column[, ignore_index]) Transform each element of a list-like to a row, replicating index values.  \nDataFrame.squeeze([axis]) Squeeze 1 dimensional axis objects into scalars.  \nDataFrame.to_xarray() Return an xarray object from the pandas object.  \nDataFrame.T   \nDataFrame.transpose(*args[, copy]) Transpose index and columns.      Combining / comparing / joining / merging       \nDataFrame.append(other[, ignore_index, ...]) Append rows of other to the end of caller, returning a new object.  \nDataFrame.assign(**kwargs) Assign new columns to a DataFrame.  \nDataFrame.compare(other[, align_axis, ...]) Compare to another DataFrame and show the differences.  \nDataFrame.join(other[, on, how, lsuffix, ...]) Join columns of another DataFrame.  \nDataFrame.merge(right[, how, on, left_on, ...]) Merge DataFrame or named Series objects with a database-style join.  \nDataFrame.update(other[, join, overwrite, ...]) Modify in place using non-NA values from another DataFrame.      Time Series-related       \nDataFrame.asfreq(freq[, method, how, ...]) Convert time series to specified frequency.  \nDataFrame.asof(where[, subset]) Return the last row(s) without any NaNs before where.  \nDataFrame.shift([periods, freq, axis, ...]) Shift index by desired number of periods with an optional time freq.  \nDataFrame.slice_shift([periods, axis]) (DEPRECATED) Equivalent to shift without copying data.  \nDataFrame.tshift([periods, freq, axis]) (DEPRECATED) Shift the time index, using the index's frequency if available.  \nDataFrame.first_valid_index() Return index for first non-NA value or None, if no NA value is found.  \nDataFrame.last_valid_index() Return index for last non-NA value or None, if no NA value is found.  \nDataFrame.resample(rule[, axis, closed, ...]) Resample time-series data.  \nDataFrame.to_period([freq, axis, copy]) Convert DataFrame from DatetimeIndex to PeriodIndex.  \nDataFrame.to_timestamp([freq, how, axis, copy]) Cast to DatetimeIndex of timestamps, at beginning of period.  \nDataFrame.tz_convert(tz[, axis, level, copy]) Convert tz-aware axis to target time zone.  \nDataFrame.tz_localize(tz[, axis, level, ...]) Localize tz-naive index of a Series or DataFrame to target time zone.      Flags Flags refer to attributes of the pandas object. Properties of the dataset (like the date is was recorded, the URL it was accessed from, etc.) should be stored in DataFrame.attrs.       \nFlags(obj, *, allows_duplicate_labels) Flags that apply to pandas objects.      Metadata DataFrame.attrs is a dictionary for storing global metadata for this DataFrame.  Warning DataFrame.attrs is considered experimental and may change without warning.        \nDataFrame.attrs Dictionary of global attributes of this dataset.      Plotting DataFrame.plot is both a callable method and a namespace attribute for specific plotting methods of the form DataFrame.plot.<kind>.       \nDataFrame.plot([x, y, kind, ax, ....]) DataFrame plotting accessor and method          \nDataFrame.plot.area([x, y]) Draw a stacked area plot.  \nDataFrame.plot.bar([x, y]) Vertical bar plot.  \nDataFrame.plot.barh([x, y]) Make a horizontal bar plot.  \nDataFrame.plot.box([by]) Make a box plot of the DataFrame columns.  \nDataFrame.plot.density([bw_method, ind]) Generate Kernel Density Estimate plot using Gaussian kernels.  \nDataFrame.plot.hexbin(x, y[, C, ...]) Generate a hexagonal binning plot.  \nDataFrame.plot.hist([by, bins]) Draw one histogram of the DataFrame's columns.  \nDataFrame.plot.kde([bw_method, ind]) Generate Kernel Density Estimate plot using Gaussian kernels.  \nDataFrame.plot.line([x, y]) Plot Series or DataFrame as lines.  \nDataFrame.plot.pie(**kwargs) Generate a pie plot.  \nDataFrame.plot.scatter(x, y[, s, c]) Create a scatter plot with varying marker point size and color.          \nDataFrame.boxplot([column, by, ax, ...]) Make a box plot from DataFrame columns.  \nDataFrame.hist([column, by, grid, ...]) Make a histogram of the DataFrame's columns.      Sparse accessor Sparse-dtype specific methods and attributes are provided under the DataFrame.sparse accessor.       \nDataFrame.sparse.density Ratio of non-sparse points to total (dense) data points.          \nDataFrame.sparse.from_spmatrix(data[, ...]) Create a new DataFrame from a scipy sparse matrix.  \nDataFrame.sparse.to_coo() Return the contents of the frame as a sparse SciPy COO matrix.  \nDataFrame.sparse.to_dense() Convert a DataFrame with sparse values to dense.      Serialization / IO / conversion       \nDataFrame.from_dict(data[, orient, dtype, ...]) Construct DataFrame from dict of array-like or dicts.  \nDataFrame.from_records(data[, index, ...]) Convert structured or record ndarray to DataFrame.  \nDataFrame.to_parquet([path, engine, ...]) Write a DataFrame to the binary parquet format.  \nDataFrame.to_pickle(path[, compression, ...]) Pickle (serialize) object to file.  \nDataFrame.to_csv([path_or_buf, sep, na_rep, ...]) Write object to a comma-separated values (csv) file.  \nDataFrame.to_hdf(path_or_buf, key[, mode, ...]) Write the contained data to an HDF5 file using HDFStore.  \nDataFrame.to_sql(name, con[, schema, ...]) Write records stored in a DataFrame to a SQL database.  \nDataFrame.to_dict([orient, into]) Convert the DataFrame to a dictionary.  \nDataFrame.to_excel(excel_writer[, ...]) Write object to an Excel sheet.  \nDataFrame.to_json([path_or_buf, orient, ...]) Convert the object to a JSON string.  \nDataFrame.to_html([buf, columns, col_space, ...]) Render a DataFrame as an HTML table.  \nDataFrame.to_feather(path, **kwargs) Write a DataFrame to the binary Feather format.  \nDataFrame.to_latex([buf, columns, ...]) Render object to a LaTeX tabular, longtable, or nested table.  \nDataFrame.to_stata(path[, convert_dates, ...]) Export DataFrame object to Stata dta format.  \nDataFrame.to_gbq(destination_table[, ...]) Write a DataFrame to a Google BigQuery table.  \nDataFrame.to_records([index, column_dtypes, ...]) Convert DataFrame to a NumPy record array.  \nDataFrame.to_string([buf, columns, ...]) Render a DataFrame to a console-friendly tabular output.  \nDataFrame.to_clipboard([excel, sep]) Copy object to the system clipboard.  \nDataFrame.to_markdown([buf, mode, index, ...]) Print DataFrame in Markdown-friendly format.  \nDataFrame.style Returns a Styler object.    \n"}, {"name": "Date offsets", "path": "reference/offset_frequency", "type": "Data offsets", "text": "Date offsets  DateOffset       \nDateOffset Standard kind of date increment used for a date range.     Properties       \nDateOffset.freqstr   \nDateOffset.kwds   \nDateOffset.name   \nDateOffset.nanos   \nDateOffset.normalize   \nDateOffset.rule_code   \nDateOffset.n   \nDateOffset.is_month_start   \nDateOffset.is_month_end       Methods       \nDateOffset.apply   \nDateOffset.apply_index(other)   \nDateOffset.copy   \nDateOffset.isAnchored   \nDateOffset.onOffset   \nDateOffset.is_anchored   \nDateOffset.is_on_offset   \nDateOffset.__call__(*args, **kwargs) Call self as a function.  \nDateOffset.is_month_start   \nDateOffset.is_month_end   \nDateOffset.is_quarter_start   \nDateOffset.is_quarter_end   \nDateOffset.is_year_start   \nDateOffset.is_year_end        BusinessDay       \nBusinessDay DateOffset subclass representing possibly n business days.    Alias:       \nBDay alias of pandas._libs.tslibs.offsets.BusinessDay     Properties       \nBusinessDay.freqstr   \nBusinessDay.kwds   \nBusinessDay.name   \nBusinessDay.nanos   \nBusinessDay.normalize   \nBusinessDay.rule_code   \nBusinessDay.n   \nBusinessDay.weekmask   \nBusinessDay.holidays   \nBusinessDay.calendar       Methods       \nBusinessDay.apply   \nBusinessDay.apply_index(other)   \nBusinessDay.copy   \nBusinessDay.isAnchored   \nBusinessDay.onOffset   \nBusinessDay.is_anchored   \nBusinessDay.is_on_offset   \nBusinessDay.__call__(*args, **kwargs) Call self as a function.  \nBusinessDay.is_month_start   \nBusinessDay.is_month_end   \nBusinessDay.is_quarter_start   \nBusinessDay.is_quarter_end   \nBusinessDay.is_year_start   \nBusinessDay.is_year_end        BusinessHour       \nBusinessHour DateOffset subclass representing possibly n business hours.     Properties       \nBusinessHour.freqstr   \nBusinessHour.kwds   \nBusinessHour.name   \nBusinessHour.nanos   \nBusinessHour.normalize   \nBusinessHour.rule_code   \nBusinessHour.n   \nBusinessHour.start   \nBusinessHour.end   \nBusinessHour.weekmask   \nBusinessHour.holidays   \nBusinessHour.calendar       Methods       \nBusinessHour.apply   \nBusinessHour.apply_index(other)   \nBusinessHour.copy   \nBusinessHour.isAnchored   \nBusinessHour.onOffset   \nBusinessHour.is_anchored   \nBusinessHour.is_on_offset   \nBusinessHour.__call__(*args, **kwargs) Call self as a function.  \nBusinessHour.is_month_start   \nBusinessHour.is_month_end   \nBusinessHour.is_quarter_start   \nBusinessHour.is_quarter_end   \nBusinessHour.is_year_start   \nBusinessHour.is_year_end        CustomBusinessDay       \nCustomBusinessDay DateOffset subclass representing custom business days excluding holidays.    Alias:       \nCDay alias of pandas._libs.tslibs.offsets.CustomBusinessDay     Properties       \nCustomBusinessDay.freqstr   \nCustomBusinessDay.kwds   \nCustomBusinessDay.name   \nCustomBusinessDay.nanos   \nCustomBusinessDay.normalize   \nCustomBusinessDay.rule_code   \nCustomBusinessDay.n   \nCustomBusinessDay.weekmask   \nCustomBusinessDay.calendar   \nCustomBusinessDay.holidays       Methods       \nCustomBusinessDay.apply_index   \nCustomBusinessDay.apply   \nCustomBusinessDay.copy   \nCustomBusinessDay.isAnchored   \nCustomBusinessDay.onOffset   \nCustomBusinessDay.is_anchored   \nCustomBusinessDay.is_on_offset   \nCustomBusinessDay.__call__(*args, **kwargs) Call self as a function.  \nCustomBusinessDay.is_month_start   \nCustomBusinessDay.is_month_end   \nCustomBusinessDay.is_quarter_start   \nCustomBusinessDay.is_quarter_end   \nCustomBusinessDay.is_year_start   \nCustomBusinessDay.is_year_end        CustomBusinessHour       \nCustomBusinessHour DateOffset subclass representing possibly n custom business days.     Properties       \nCustomBusinessHour.freqstr   \nCustomBusinessHour.kwds   \nCustomBusinessHour.name   \nCustomBusinessHour.nanos   \nCustomBusinessHour.normalize   \nCustomBusinessHour.rule_code   \nCustomBusinessHour.n   \nCustomBusinessHour.weekmask   \nCustomBusinessHour.calendar   \nCustomBusinessHour.holidays   \nCustomBusinessHour.start   \nCustomBusinessHour.end       Methods       \nCustomBusinessHour.apply   \nCustomBusinessHour.apply_index(other)   \nCustomBusinessHour.copy   \nCustomBusinessHour.isAnchored   \nCustomBusinessHour.onOffset   \nCustomBusinessHour.is_anchored   \nCustomBusinessHour.is_on_offset   \nCustomBusinessHour.__call__(*args, **kwargs) Call self as a function.  \nCustomBusinessHour.is_month_start   \nCustomBusinessHour.is_month_end   \nCustomBusinessHour.is_quarter_start   \nCustomBusinessHour.is_quarter_end   \nCustomBusinessHour.is_year_start   \nCustomBusinessHour.is_year_end        MonthEnd       \nMonthEnd DateOffset of one month end.     Properties       \nMonthEnd.freqstr   \nMonthEnd.kwds   \nMonthEnd.name   \nMonthEnd.nanos   \nMonthEnd.normalize   \nMonthEnd.rule_code   \nMonthEnd.n       Methods       \nMonthEnd.apply   \nMonthEnd.apply_index(other)   \nMonthEnd.copy   \nMonthEnd.isAnchored   \nMonthEnd.onOffset   \nMonthEnd.is_anchored   \nMonthEnd.is_on_offset   \nMonthEnd.__call__(*args, **kwargs) Call self as a function.  \nMonthEnd.is_month_start   \nMonthEnd.is_month_end   \nMonthEnd.is_quarter_start   \nMonthEnd.is_quarter_end   \nMonthEnd.is_year_start   \nMonthEnd.is_year_end        MonthBegin       \nMonthBegin DateOffset of one month at beginning.     Properties       \nMonthBegin.freqstr   \nMonthBegin.kwds   \nMonthBegin.name   \nMonthBegin.nanos   \nMonthBegin.normalize   \nMonthBegin.rule_code   \nMonthBegin.n       Methods       \nMonthBegin.apply   \nMonthBegin.apply_index(other)   \nMonthBegin.copy   \nMonthBegin.isAnchored   \nMonthBegin.onOffset   \nMonthBegin.is_anchored   \nMonthBegin.is_on_offset   \nMonthBegin.__call__(*args, **kwargs) Call self as a function.  \nMonthBegin.is_month_start   \nMonthBegin.is_month_end   \nMonthBegin.is_quarter_start   \nMonthBegin.is_quarter_end   \nMonthBegin.is_year_start   \nMonthBegin.is_year_end        BusinessMonthEnd       \nBusinessMonthEnd DateOffset increments between the last business day of the month.    Alias:       \nBMonthEnd alias of pandas._libs.tslibs.offsets.BusinessMonthEnd     Properties       \nBusinessMonthEnd.freqstr   \nBusinessMonthEnd.kwds   \nBusinessMonthEnd.name   \nBusinessMonthEnd.nanos   \nBusinessMonthEnd.normalize   \nBusinessMonthEnd.rule_code   \nBusinessMonthEnd.n       Methods       \nBusinessMonthEnd.apply   \nBusinessMonthEnd.apply_index(other)   \nBusinessMonthEnd.copy   \nBusinessMonthEnd.isAnchored   \nBusinessMonthEnd.onOffset   \nBusinessMonthEnd.is_anchored   \nBusinessMonthEnd.is_on_offset   \nBusinessMonthEnd.__call__(*args, **kwargs) Call self as a function.  \nBusinessMonthEnd.is_month_start   \nBusinessMonthEnd.is_month_end   \nBusinessMonthEnd.is_quarter_start   \nBusinessMonthEnd.is_quarter_end   \nBusinessMonthEnd.is_year_start   \nBusinessMonthEnd.is_year_end        BusinessMonthBegin       \nBusinessMonthBegin DateOffset of one month at the first business day.    Alias:       \nBMonthBegin alias of pandas._libs.tslibs.offsets.BusinessMonthBegin     Properties       \nBusinessMonthBegin.freqstr   \nBusinessMonthBegin.kwds   \nBusinessMonthBegin.name   \nBusinessMonthBegin.nanos   \nBusinessMonthBegin.normalize   \nBusinessMonthBegin.rule_code   \nBusinessMonthBegin.n       Methods       \nBusinessMonthBegin.apply   \nBusinessMonthBegin.apply_index(other)   \nBusinessMonthBegin.copy   \nBusinessMonthBegin.isAnchored   \nBusinessMonthBegin.onOffset   \nBusinessMonthBegin.is_anchored   \nBusinessMonthBegin.is_on_offset   \nBusinessMonthBegin.__call__(*args, **kwargs) Call self as a function.  \nBusinessMonthBegin.is_month_start   \nBusinessMonthBegin.is_month_end   \nBusinessMonthBegin.is_quarter_start   \nBusinessMonthBegin.is_quarter_end   \nBusinessMonthBegin.is_year_start   \nBusinessMonthBegin.is_year_end        CustomBusinessMonthEnd       \nCustomBusinessMonthEnd \nAttributes     Alias:       \nCBMonthEnd alias of pandas._libs.tslibs.offsets.CustomBusinessMonthEnd     Properties       \nCustomBusinessMonthEnd.freqstr   \nCustomBusinessMonthEnd.kwds   \nCustomBusinessMonthEnd.m_offset   \nCustomBusinessMonthEnd.name   \nCustomBusinessMonthEnd.nanos   \nCustomBusinessMonthEnd.normalize   \nCustomBusinessMonthEnd.rule_code   \nCustomBusinessMonthEnd.n   \nCustomBusinessMonthEnd.weekmask   \nCustomBusinessMonthEnd.calendar   \nCustomBusinessMonthEnd.holidays       Methods       \nCustomBusinessMonthEnd.apply   \nCustomBusinessMonthEnd.apply_index(other)   \nCustomBusinessMonthEnd.copy   \nCustomBusinessMonthEnd.isAnchored   \nCustomBusinessMonthEnd.onOffset   \nCustomBusinessMonthEnd.is_anchored   \nCustomBusinessMonthEnd.is_on_offset   \nCustomBusinessMonthEnd.__call__(*args, **kwargs) Call self as a function.  \nCustomBusinessMonthEnd.is_month_start   \nCustomBusinessMonthEnd.is_month_end   \nCustomBusinessMonthEnd.is_quarter_start   \nCustomBusinessMonthEnd.is_quarter_end   \nCustomBusinessMonthEnd.is_year_start   \nCustomBusinessMonthEnd.is_year_end        CustomBusinessMonthBegin       \nCustomBusinessMonthBegin \nAttributes     Alias:       \nCBMonthBegin alias of pandas._libs.tslibs.offsets.CustomBusinessMonthBegin     Properties       \nCustomBusinessMonthBegin.freqstr   \nCustomBusinessMonthBegin.kwds   \nCustomBusinessMonthBegin.m_offset   \nCustomBusinessMonthBegin.name   \nCustomBusinessMonthBegin.nanos   \nCustomBusinessMonthBegin.normalize   \nCustomBusinessMonthBegin.rule_code   \nCustomBusinessMonthBegin.n   \nCustomBusinessMonthBegin.weekmask   \nCustomBusinessMonthBegin.calendar   \nCustomBusinessMonthBegin.holidays       Methods       \nCustomBusinessMonthBegin.apply   \nCustomBusinessMonthBegin.apply_index(other)   \nCustomBusinessMonthBegin.copy   \nCustomBusinessMonthBegin.isAnchored   \nCustomBusinessMonthBegin.onOffset   \nCustomBusinessMonthBegin.is_anchored   \nCustomBusinessMonthBegin.is_on_offset   \nCustomBusinessMonthBegin.__call__(*args, ...) Call self as a function.  \nCustomBusinessMonthBegin.is_month_start   \nCustomBusinessMonthBegin.is_month_end   \nCustomBusinessMonthBegin.is_quarter_start   \nCustomBusinessMonthBegin.is_quarter_end   \nCustomBusinessMonthBegin.is_year_start   \nCustomBusinessMonthBegin.is_year_end        SemiMonthEnd       \nSemiMonthEnd Two DateOffset's per month repeating on the last day of the month and day_of_month.     Properties       \nSemiMonthEnd.freqstr   \nSemiMonthEnd.kwds   \nSemiMonthEnd.name   \nSemiMonthEnd.nanos   \nSemiMonthEnd.normalize   \nSemiMonthEnd.rule_code   \nSemiMonthEnd.n   \nSemiMonthEnd.day_of_month       Methods       \nSemiMonthEnd.apply   \nSemiMonthEnd.apply_index(other)   \nSemiMonthEnd.copy   \nSemiMonthEnd.isAnchored   \nSemiMonthEnd.onOffset   \nSemiMonthEnd.is_anchored   \nSemiMonthEnd.is_on_offset   \nSemiMonthEnd.__call__(*args, **kwargs) Call self as a function.  \nSemiMonthEnd.is_month_start   \nSemiMonthEnd.is_month_end   \nSemiMonthEnd.is_quarter_start   \nSemiMonthEnd.is_quarter_end   \nSemiMonthEnd.is_year_start   \nSemiMonthEnd.is_year_end        SemiMonthBegin       \nSemiMonthBegin Two DateOffset's per month repeating on the first day of the month and day_of_month.     Properties       \nSemiMonthBegin.freqstr   \nSemiMonthBegin.kwds   \nSemiMonthBegin.name   \nSemiMonthBegin.nanos   \nSemiMonthBegin.normalize   \nSemiMonthBegin.rule_code   \nSemiMonthBegin.n   \nSemiMonthBegin.day_of_month       Methods       \nSemiMonthBegin.apply   \nSemiMonthBegin.apply_index(other)   \nSemiMonthBegin.copy   \nSemiMonthBegin.isAnchored   \nSemiMonthBegin.onOffset   \nSemiMonthBegin.is_anchored   \nSemiMonthBegin.is_on_offset   \nSemiMonthBegin.__call__(*args, **kwargs) Call self as a function.  \nSemiMonthBegin.is_month_start   \nSemiMonthBegin.is_month_end   \nSemiMonthBegin.is_quarter_start   \nSemiMonthBegin.is_quarter_end   \nSemiMonthBegin.is_year_start   \nSemiMonthBegin.is_year_end        Week       \nWeek Weekly offset.     Properties       \nWeek.freqstr   \nWeek.kwds   \nWeek.name   \nWeek.nanos   \nWeek.normalize   \nWeek.rule_code   \nWeek.n   \nWeek.weekday       Methods       \nWeek.apply   \nWeek.apply_index(other)   \nWeek.copy   \nWeek.isAnchored   \nWeek.onOffset   \nWeek.is_anchored   \nWeek.is_on_offset   \nWeek.__call__(*args, **kwargs) Call self as a function.  \nWeek.is_month_start   \nWeek.is_month_end   \nWeek.is_quarter_start   \nWeek.is_quarter_end   \nWeek.is_year_start   \nWeek.is_year_end        WeekOfMonth       \nWeekOfMonth Describes monthly dates like \"the Tuesday of the 2nd week of each month\".     Properties       \nWeekOfMonth.freqstr   \nWeekOfMonth.kwds   \nWeekOfMonth.name   \nWeekOfMonth.nanos   \nWeekOfMonth.normalize   \nWeekOfMonth.rule_code   \nWeekOfMonth.n   \nWeekOfMonth.week       Methods       \nWeekOfMonth.apply   \nWeekOfMonth.apply_index(other)   \nWeekOfMonth.copy   \nWeekOfMonth.isAnchored   \nWeekOfMonth.onOffset   \nWeekOfMonth.is_anchored   \nWeekOfMonth.is_on_offset   \nWeekOfMonth.__call__(*args, **kwargs) Call self as a function.  \nWeekOfMonth.weekday   \nWeekOfMonth.is_month_start   \nWeekOfMonth.is_month_end   \nWeekOfMonth.is_quarter_start   \nWeekOfMonth.is_quarter_end   \nWeekOfMonth.is_year_start   \nWeekOfMonth.is_year_end        LastWeekOfMonth       \nLastWeekOfMonth Describes monthly dates in last week of month like \"the last Tuesday of each month\".     Properties       \nLastWeekOfMonth.freqstr   \nLastWeekOfMonth.kwds   \nLastWeekOfMonth.name   \nLastWeekOfMonth.nanos   \nLastWeekOfMonth.normalize   \nLastWeekOfMonth.rule_code   \nLastWeekOfMonth.n   \nLastWeekOfMonth.weekday   \nLastWeekOfMonth.week       Methods       \nLastWeekOfMonth.apply   \nLastWeekOfMonth.apply_index(other)   \nLastWeekOfMonth.copy   \nLastWeekOfMonth.isAnchored   \nLastWeekOfMonth.onOffset   \nLastWeekOfMonth.is_anchored   \nLastWeekOfMonth.is_on_offset   \nLastWeekOfMonth.__call__(*args, **kwargs) Call self as a function.  \nLastWeekOfMonth.is_month_start   \nLastWeekOfMonth.is_month_end   \nLastWeekOfMonth.is_quarter_start   \nLastWeekOfMonth.is_quarter_end   \nLastWeekOfMonth.is_year_start   \nLastWeekOfMonth.is_year_end        BQuarterEnd       \nBQuarterEnd DateOffset increments between the last business day of each Quarter.     Properties       \nBQuarterEnd.freqstr   \nBQuarterEnd.kwds   \nBQuarterEnd.name   \nBQuarterEnd.nanos   \nBQuarterEnd.normalize   \nBQuarterEnd.rule_code   \nBQuarterEnd.n   \nBQuarterEnd.startingMonth       Methods       \nBQuarterEnd.apply   \nBQuarterEnd.apply_index(other)   \nBQuarterEnd.copy   \nBQuarterEnd.isAnchored   \nBQuarterEnd.onOffset   \nBQuarterEnd.is_anchored   \nBQuarterEnd.is_on_offset   \nBQuarterEnd.__call__(*args, **kwargs) Call self as a function.  \nBQuarterEnd.is_month_start   \nBQuarterEnd.is_month_end   \nBQuarterEnd.is_quarter_start   \nBQuarterEnd.is_quarter_end   \nBQuarterEnd.is_year_start   \nBQuarterEnd.is_year_end        BQuarterBegin       \nBQuarterBegin DateOffset increments between the first business day of each Quarter.     Properties       \nBQuarterBegin.freqstr   \nBQuarterBegin.kwds   \nBQuarterBegin.name   \nBQuarterBegin.nanos   \nBQuarterBegin.normalize   \nBQuarterBegin.rule_code   \nBQuarterBegin.n   \nBQuarterBegin.startingMonth       Methods       \nBQuarterBegin.apply   \nBQuarterBegin.apply_index(other)   \nBQuarterBegin.copy   \nBQuarterBegin.isAnchored   \nBQuarterBegin.onOffset   \nBQuarterBegin.is_anchored   \nBQuarterBegin.is_on_offset   \nBQuarterBegin.__call__(*args, **kwargs) Call self as a function.  \nBQuarterBegin.is_month_start   \nBQuarterBegin.is_month_end   \nBQuarterBegin.is_quarter_start   \nBQuarterBegin.is_quarter_end   \nBQuarterBegin.is_year_start   \nBQuarterBegin.is_year_end        QuarterEnd       \nQuarterEnd DateOffset increments between Quarter end dates.     Properties       \nQuarterEnd.freqstr   \nQuarterEnd.kwds   \nQuarterEnd.name   \nQuarterEnd.nanos   \nQuarterEnd.normalize   \nQuarterEnd.rule_code   \nQuarterEnd.n   \nQuarterEnd.startingMonth       Methods       \nQuarterEnd.apply   \nQuarterEnd.apply_index(other)   \nQuarterEnd.copy   \nQuarterEnd.isAnchored   \nQuarterEnd.onOffset   \nQuarterEnd.is_anchored   \nQuarterEnd.is_on_offset   \nQuarterEnd.__call__(*args, **kwargs) Call self as a function.  \nQuarterEnd.is_month_start   \nQuarterEnd.is_month_end   \nQuarterEnd.is_quarter_start   \nQuarterEnd.is_quarter_end   \nQuarterEnd.is_year_start   \nQuarterEnd.is_year_end        QuarterBegin       \nQuarterBegin DateOffset increments between Quarter start dates.     Properties       \nQuarterBegin.freqstr   \nQuarterBegin.kwds   \nQuarterBegin.name   \nQuarterBegin.nanos   \nQuarterBegin.normalize   \nQuarterBegin.rule_code   \nQuarterBegin.n   \nQuarterBegin.startingMonth       Methods       \nQuarterBegin.apply   \nQuarterBegin.apply_index(other)   \nQuarterBegin.copy   \nQuarterBegin.isAnchored   \nQuarterBegin.onOffset   \nQuarterBegin.is_anchored   \nQuarterBegin.is_on_offset   \nQuarterBegin.__call__(*args, **kwargs) Call self as a function.  \nQuarterBegin.is_month_start   \nQuarterBegin.is_month_end   \nQuarterBegin.is_quarter_start   \nQuarterBegin.is_quarter_end   \nQuarterBegin.is_year_start   \nQuarterBegin.is_year_end        BYearEnd       \nBYearEnd DateOffset increments between the last business day of the year.     Properties       \nBYearEnd.freqstr   \nBYearEnd.kwds   \nBYearEnd.name   \nBYearEnd.nanos   \nBYearEnd.normalize   \nBYearEnd.rule_code   \nBYearEnd.n   \nBYearEnd.month       Methods       \nBYearEnd.apply   \nBYearEnd.apply_index(other)   \nBYearEnd.copy   \nBYearEnd.isAnchored   \nBYearEnd.onOffset   \nBYearEnd.is_anchored   \nBYearEnd.is_on_offset   \nBYearEnd.__call__(*args, **kwargs) Call self as a function.  \nBYearEnd.is_month_start   \nBYearEnd.is_month_end   \nBYearEnd.is_quarter_start   \nBYearEnd.is_quarter_end   \nBYearEnd.is_year_start   \nBYearEnd.is_year_end        BYearBegin       \nBYearBegin DateOffset increments between the first business day of the year.     Properties       \nBYearBegin.freqstr   \nBYearBegin.kwds   \nBYearBegin.name   \nBYearBegin.nanos   \nBYearBegin.normalize   \nBYearBegin.rule_code   \nBYearBegin.n   \nBYearBegin.month       Methods       \nBYearBegin.apply   \nBYearBegin.apply_index(other)   \nBYearBegin.copy   \nBYearBegin.isAnchored   \nBYearBegin.onOffset   \nBYearBegin.is_anchored   \nBYearBegin.is_on_offset   \nBYearBegin.__call__(*args, **kwargs) Call self as a function.  \nBYearBegin.is_month_start   \nBYearBegin.is_month_end   \nBYearBegin.is_quarter_start   \nBYearBegin.is_quarter_end   \nBYearBegin.is_year_start   \nBYearBegin.is_year_end        YearEnd       \nYearEnd DateOffset increments between calendar year ends.     Properties       \nYearEnd.freqstr   \nYearEnd.kwds   \nYearEnd.name   \nYearEnd.nanos   \nYearEnd.normalize   \nYearEnd.rule_code   \nYearEnd.n   \nYearEnd.month       Methods       \nYearEnd.apply   \nYearEnd.apply_index(other)   \nYearEnd.copy   \nYearEnd.isAnchored   \nYearEnd.onOffset   \nYearEnd.is_anchored   \nYearEnd.is_on_offset   \nYearEnd.__call__(*args, **kwargs) Call self as a function.  \nYearEnd.is_month_start   \nYearEnd.is_month_end   \nYearEnd.is_quarter_start   \nYearEnd.is_quarter_end   \nYearEnd.is_year_start   \nYearEnd.is_year_end        YearBegin       \nYearBegin DateOffset increments between calendar year begin dates.     Properties       \nYearBegin.freqstr   \nYearBegin.kwds   \nYearBegin.name   \nYearBegin.nanos   \nYearBegin.normalize   \nYearBegin.rule_code   \nYearBegin.n   \nYearBegin.month       Methods       \nYearBegin.apply   \nYearBegin.apply_index(other)   \nYearBegin.copy   \nYearBegin.isAnchored   \nYearBegin.onOffset   \nYearBegin.is_anchored   \nYearBegin.is_on_offset   \nYearBegin.__call__(*args, **kwargs) Call self as a function.  \nYearBegin.is_month_start   \nYearBegin.is_month_end   \nYearBegin.is_quarter_start   \nYearBegin.is_quarter_end   \nYearBegin.is_year_start   \nYearBegin.is_year_end        FY5253       \nFY5253 Describes 52-53 week fiscal year.     Properties       \nFY5253.freqstr   \nFY5253.kwds   \nFY5253.name   \nFY5253.nanos   \nFY5253.normalize   \nFY5253.rule_code   \nFY5253.n   \nFY5253.startingMonth   \nFY5253.variation   \nFY5253.weekday       Methods       \nFY5253.apply   \nFY5253.apply_index(other)   \nFY5253.copy   \nFY5253.get_rule_code_suffix   \nFY5253.get_year_end   \nFY5253.isAnchored   \nFY5253.onOffset   \nFY5253.is_anchored   \nFY5253.is_on_offset   \nFY5253.__call__(*args, **kwargs) Call self as a function.  \nFY5253.is_month_start   \nFY5253.is_month_end   \nFY5253.is_quarter_start   \nFY5253.is_quarter_end   \nFY5253.is_year_start   \nFY5253.is_year_end        FY5253Quarter       \nFY5253Quarter DateOffset increments between business quarter dates for 52-53 week fiscal year (also known as a 4-4-5 calendar).     Properties       \nFY5253Quarter.freqstr   \nFY5253Quarter.kwds   \nFY5253Quarter.name   \nFY5253Quarter.nanos   \nFY5253Quarter.normalize   \nFY5253Quarter.rule_code   \nFY5253Quarter.n   \nFY5253Quarter.qtr_with_extra_week   \nFY5253Quarter.startingMonth   \nFY5253Quarter.variation   \nFY5253Quarter.weekday       Methods       \nFY5253Quarter.apply   \nFY5253Quarter.apply_index(other)   \nFY5253Quarter.copy   \nFY5253Quarter.get_rule_code_suffix   \nFY5253Quarter.get_weeks   \nFY5253Quarter.isAnchored   \nFY5253Quarter.onOffset   \nFY5253Quarter.is_anchored   \nFY5253Quarter.is_on_offset   \nFY5253Quarter.year_has_extra_week   \nFY5253Quarter.__call__(*args, **kwargs) Call self as a function.  \nFY5253Quarter.is_month_start   \nFY5253Quarter.is_month_end   \nFY5253Quarter.is_quarter_start   \nFY5253Quarter.is_quarter_end   \nFY5253Quarter.is_year_start   \nFY5253Quarter.is_year_end        Easter       \nEaster DateOffset for the Easter holiday using logic defined in dateutil.     Properties       \nEaster.freqstr   \nEaster.kwds   \nEaster.name   \nEaster.nanos   \nEaster.normalize   \nEaster.rule_code   \nEaster.n       Methods       \nEaster.apply   \nEaster.apply_index(other)   \nEaster.copy   \nEaster.isAnchored   \nEaster.onOffset   \nEaster.is_anchored   \nEaster.is_on_offset   \nEaster.__call__(*args, **kwargs) Call self as a function.  \nEaster.is_month_start   \nEaster.is_month_end   \nEaster.is_quarter_start   \nEaster.is_quarter_end   \nEaster.is_year_start   \nEaster.is_year_end        Tick       \nTick \nAttributes      Properties       \nTick.delta   \nTick.freqstr   \nTick.kwds   \nTick.name   \nTick.nanos   \nTick.normalize   \nTick.rule_code   \nTick.n       Methods       \nTick.copy   \nTick.isAnchored   \nTick.onOffset   \nTick.is_anchored   \nTick.is_on_offset   \nTick.__call__(*args, **kwargs) Call self as a function.  \nTick.apply   \nTick.apply_index(other)   \nTick.is_month_start   \nTick.is_month_end   \nTick.is_quarter_start   \nTick.is_quarter_end   \nTick.is_year_start   \nTick.is_year_end        Day       \nDay \nAttributes      Properties       \nDay.delta   \nDay.freqstr   \nDay.kwds   \nDay.name   \nDay.nanos   \nDay.normalize   \nDay.rule_code   \nDay.n       Methods       \nDay.copy   \nDay.isAnchored   \nDay.onOffset   \nDay.is_anchored   \nDay.is_on_offset   \nDay.__call__(*args, **kwargs) Call self as a function.  \nDay.apply   \nDay.apply_index(other)   \nDay.is_month_start   \nDay.is_month_end   \nDay.is_quarter_start   \nDay.is_quarter_end   \nDay.is_year_start   \nDay.is_year_end        Hour       \nHour \nAttributes      Properties       \nHour.delta   \nHour.freqstr   \nHour.kwds   \nHour.name   \nHour.nanos   \nHour.normalize   \nHour.rule_code   \nHour.n       Methods       \nHour.copy   \nHour.isAnchored   \nHour.onOffset   \nHour.is_anchored   \nHour.is_on_offset   \nHour.__call__(*args, **kwargs) Call self as a function.  \nHour.apply   \nHour.apply_index(other)   \nHour.is_month_start   \nHour.is_month_end   \nHour.is_quarter_start   \nHour.is_quarter_end   \nHour.is_year_start   \nHour.is_year_end        Minute       \nMinute \nAttributes      Properties       \nMinute.delta   \nMinute.freqstr   \nMinute.kwds   \nMinute.name   \nMinute.nanos   \nMinute.normalize   \nMinute.rule_code   \nMinute.n       Methods       \nMinute.copy   \nMinute.isAnchored   \nMinute.onOffset   \nMinute.is_anchored   \nMinute.is_on_offset   \nMinute.__call__(*args, **kwargs) Call self as a function.  \nMinute.apply   \nMinute.apply_index(other)   \nMinute.is_month_start   \nMinute.is_month_end   \nMinute.is_quarter_start   \nMinute.is_quarter_end   \nMinute.is_year_start   \nMinute.is_year_end        Second       \nSecond \nAttributes      Properties       \nSecond.delta   \nSecond.freqstr   \nSecond.kwds   \nSecond.name   \nSecond.nanos   \nSecond.normalize   \nSecond.rule_code   \nSecond.n       Methods       \nSecond.copy   \nSecond.isAnchored   \nSecond.onOffset   \nSecond.is_anchored   \nSecond.is_on_offset   \nSecond.__call__(*args, **kwargs) Call self as a function.  \nSecond.apply   \nSecond.apply_index(other)   \nSecond.is_month_start   \nSecond.is_month_end   \nSecond.is_quarter_start   \nSecond.is_quarter_end   \nSecond.is_year_start   \nSecond.is_year_end        Milli       \nMilli \nAttributes      Properties       \nMilli.delta   \nMilli.freqstr   \nMilli.kwds   \nMilli.name   \nMilli.nanos   \nMilli.normalize   \nMilli.rule_code   \nMilli.n       Methods       \nMilli.copy   \nMilli.isAnchored   \nMilli.onOffset   \nMilli.is_anchored   \nMilli.is_on_offset   \nMilli.__call__(*args, **kwargs) Call self as a function.  \nMilli.apply   \nMilli.apply_index(other)   \nMilli.is_month_start   \nMilli.is_month_end   \nMilli.is_quarter_start   \nMilli.is_quarter_end   \nMilli.is_year_start   \nMilli.is_year_end        Micro       \nMicro \nAttributes      Properties       \nMicro.delta   \nMicro.freqstr   \nMicro.kwds   \nMicro.name   \nMicro.nanos   \nMicro.normalize   \nMicro.rule_code   \nMicro.n       Methods       \nMicro.copy   \nMicro.isAnchored   \nMicro.onOffset   \nMicro.is_anchored   \nMicro.is_on_offset   \nMicro.__call__(*args, **kwargs) Call self as a function.  \nMicro.apply   \nMicro.apply_index(other)   \nMicro.is_month_start   \nMicro.is_month_end   \nMicro.is_quarter_start   \nMicro.is_quarter_end   \nMicro.is_year_start   \nMicro.is_year_end        Nano       \nNano \nAttributes      Properties       \nNano.delta   \nNano.freqstr   \nNano.kwds   \nNano.name   \nNano.nanos   \nNano.normalize   \nNano.rule_code   \nNano.n       Methods       \nNano.copy   \nNano.isAnchored   \nNano.onOffset   \nNano.is_anchored   \nNano.is_on_offset   \nNano.__call__(*args, **kwargs) Call self as a function.  \nNano.apply   \nNano.apply_index(other)   \nNano.is_month_start   \nNano.is_month_end   \nNano.is_quarter_start   \nNano.is_quarter_end   \nNano.is_year_start   \nNano.is_year_end      \n"}, {"name": "Duplicate Labels", "path": "user_guide/duplicates", "type": "Manual", "text": "Duplicate Labels Index objects are not required to be unique; you can have duplicate row or column labels. This may be a bit confusing at first. If you\u2019re familiar with SQL, you know that row labels are similar to a primary key on a table, and you would never want duplicates in a SQL table. But one of pandas\u2019 roles is to clean messy, real-world data before it goes to some downstream system. And real-world data has duplicates, even in fields that are supposed to be unique. This section describes how duplicate labels change the behavior of certain operations, and how prevent duplicates from arising during operations, or to detect them if they do. \nIn [1]: import pandas as pd\n\nIn [2]: import numpy as np\n   Consequences of Duplicate Labels Some pandas methods (Series.reindex() for example) just don\u2019t work with duplicates present. The output can\u2019t be determined, and so pandas raises. \nIn [3]: s1 = pd.Series([0, 1, 2], index=[\"a\", \"b\", \"b\"])\n\nIn [4]: s1.reindex([\"a\", \"b\", \"c\"])\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nInput In [4], in <module>\n----> 1 s1.reindex([\"a\", \"b\", \"c\"])\n\nFile /pandas/pandas/core/series.py:4669, in Series.reindex(self, *args, **kwargs)\n   4665         raise TypeError(\n   4666             \"'index' passed as both positional and keyword argument\"\n   4667         )\n   4668     kwargs.update({\"index\": index})\n-> 4669 return super().reindex(**kwargs)\n\nFile /pandas/pandas/core/generic.py:4974, in NDFrame.reindex(self, *args, **kwargs)\n   4971     return self._reindex_multi(axes, copy, fill_value)\n   4973 # perform the reindex on the axes\n-> 4974 return self._reindex_axes(\n   4975     axes, level, limit, tolerance, method, fill_value, copy\n   4976 ).__finalize__(self, method=\"reindex\")\n\nFile /pandas/pandas/core/generic.py:4994, in NDFrame._reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy)\n   4989 new_index, indexer = ax.reindex(\n   4990     labels, level=level, limit=limit, tolerance=tolerance, method=method\n   4991 )\n   4993 axis = self._get_axis_number(a)\n-> 4994 obj = obj._reindex_with_indexers(\n   4995     {axis: [new_index, indexer]},\n   4996     fill_value=fill_value,\n   4997     copy=copy,\n   4998     allow_dups=False,\n   4999 )\n   5000 # If we've made a copy once, no need to make another one\n   5001 copy = False\n\nFile /pandas/pandas/core/generic.py:5040, in NDFrame._reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups)\n   5037     indexer = ensure_platform_int(indexer)\n   5039 # TODO: speed up on homogeneous DataFrame objects (see _reindex_multi)\n-> 5040 new_data = new_data.reindex_indexer(\n   5041     index,\n   5042     indexer,\n   5043     axis=baxis,\n   5044     fill_value=fill_value,\n   5045     allow_dups=allow_dups,\n   5046     copy=copy,\n   5047 )\n   5048 # If we've made a copy once, no need to make another one\n   5049 copy = False\n\nFile /pandas/pandas/core/internals/managers.py:679, in BaseBlockManager.reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice, use_na_proxy)\n    677 # some axes don't allow reindexing with dups\n    678 if not allow_dups:\n--> 679     self.axes[axis]._validate_can_reindex(indexer)\n    681 if axis >= self.ndim:\n    682     raise IndexError(\"Requested axis not found in manager\")\n\nFile /pandas/pandas/core/indexes/base.py:4107, in Index._validate_can_reindex(self, indexer)\n   4105 # trying to reindex on an axis with duplicates\n   4106 if not self._index_as_unique and len(indexer):\n-> 4107     raise ValueError(\"cannot reindex on an axis with duplicate labels\")\n\nValueError: cannot reindex on an axis with duplicate labels\n  Other methods, like indexing, can give very surprising results. Typically indexing with a scalar will reduce dimensionality. Slicing a DataFrame with a scalar will return a Series. Slicing a Series with a scalar will return a scalar. But with duplicates, this isn\u2019t the case. \nIn [5]: df1 = pd.DataFrame([[0, 1, 2], [3, 4, 5]], columns=[\"A\", \"A\", \"B\"])\n\nIn [6]: df1\nOut[6]: \n   A  A  B\n0  0  1  2\n1  3  4  5\n  We have duplicates in the columns. If we slice 'B', we get back a Series \nIn [7]: df1[\"B\"]  # a series\nOut[7]: \n0    2\n1    5\nName: B, dtype: int64\n  But slicing 'A' returns a DataFrame \nIn [8]: df1[\"A\"]  # a DataFrame\nOut[8]: \n   A  A\n0  0  1\n1  3  4\n  This applies to row labels as well \nIn [9]: df2 = pd.DataFrame({\"A\": [0, 1, 2]}, index=[\"a\", \"a\", \"b\"])\n\nIn [10]: df2\nOut[10]: \n   A\na  0\na  1\nb  2\n\nIn [11]: df2.loc[\"b\", \"A\"]  # a scalar\nOut[11]: 2\n\nIn [12]: df2.loc[\"a\", \"A\"]  # a Series\nOut[12]: \na    0\na    1\nName: A, dtype: int64\n    Duplicate Label Detection You can check whether an Index (storing the row or column labels) is unique with Index.is_unique: \nIn [13]: df2\nOut[13]: \n   A\na  0\na  1\nb  2\n\nIn [14]: df2.index.is_unique\nOut[14]: False\n\nIn [15]: df2.columns.is_unique\nOut[15]: True\n   Note Checking whether an index is unique is somewhat expensive for large datasets. pandas does cache this result, so re-checking on the same index is very fast.  Index.duplicated() will return a boolean ndarray indicating whether a label is repeated. \nIn [16]: df2.index.duplicated()\nOut[16]: array([False,  True, False])\n  Which can be used as a boolean filter to drop duplicate rows. \nIn [17]: df2.loc[~df2.index.duplicated(), :]\nOut[17]: \n   A\na  0\nb  2\n  If you need additional logic to handle duplicate labels, rather than just dropping the repeats, using groupby() on the index is a common trick. For example, we\u2019ll resolve duplicates by taking the average of all rows with the same label. \nIn [18]: df2.groupby(level=0).mean()\nOut[18]: \n     A\na  0.5\nb  2.0\n    Disallowing Duplicate Labels  New in version 1.2.0.  As noted above, handling duplicates is an important feature when reading in raw data. That said, you may want to avoid introducing duplicates as part of a data processing pipeline (from methods like pandas.concat(), rename(), etc.). Both Series and DataFrame disallow duplicate labels by calling .set_flags(allows_duplicate_labels=False). (the default is to allow them). If there are duplicate labels, an exception will be raised. \nIn [19]: pd.Series([0, 1, 2], index=[\"a\", \"b\", \"b\"]).set_flags(allows_duplicate_labels=False)\n---------------------------------------------------------------------------\nDuplicateLabelError                       Traceback (most recent call last)\nInput In [19], in <module>\n----> 1 pd.Series([0, 1, 2], index=[\"a\", \"b\", \"b\"]).set_flags(allows_duplicate_labels=False)\n\nFile /pandas/pandas/core/generic.py:438, in NDFrame.set_flags(self, copy, allows_duplicate_labels)\n    436 df = self.copy(deep=copy)\n    437 if allows_duplicate_labels is not None:\n--> 438     df.flags[\"allows_duplicate_labels\"] = allows_duplicate_labels\n    439 return df\n\nFile /pandas/pandas/core/flags.py:105, in Flags.__setitem__(self, key, value)\n    103 if key not in self._keys:\n    104     raise ValueError(f\"Unknown flag {key}. Must be one of {self._keys}\")\n--> 105 setattr(self, key, value)\n\nFile /pandas/pandas/core/flags.py:92, in Flags.allows_duplicate_labels(self, value)\n     90 if not value:\n     91     for ax in obj.axes:\n---> 92         ax._maybe_check_unique()\n     94 self._allows_duplicate_labels = value\n\nFile /pandas/pandas/core/indexes/base.py:715, in Index._maybe_check_unique(self)\n    712 duplicates = self._format_duplicate_message()\n    713 msg += f\"\\n{duplicates}\"\n--> 715 raise DuplicateLabelError(msg)\n\nDuplicateLabelError: Index has duplicates.\n      positions\nlabel          \nb        [1, 2]\n  This applies to both row and column labels for a DataFrame \nIn [20]: pd.DataFrame([[0, 1, 2], [3, 4, 5]], columns=[\"A\", \"B\", \"C\"],).set_flags(\n   ....:     allows_duplicate_labels=False\n   ....: )\n   ....: \nOut[20]: \n   A  B  C\n0  0  1  2\n1  3  4  5\n  This attribute can be checked or set with allows_duplicate_labels, which indicates whether that object can have duplicate labels. \nIn [21]: df = pd.DataFrame({\"A\": [0, 1, 2, 3]}, index=[\"x\", \"y\", \"X\", \"Y\"]).set_flags(\n   ....:     allows_duplicate_labels=False\n   ....: )\n   ....: \n\nIn [22]: df\nOut[22]: \n   A\nx  0\ny  1\nX  2\nY  3\n\nIn [23]: df.flags.allows_duplicate_labels\nOut[23]: False\n  DataFrame.set_flags() can be used to return a new DataFrame with attributes like allows_duplicate_labels set to some value \nIn [24]: df2 = df.set_flags(allows_duplicate_labels=True)\n\nIn [25]: df2.flags.allows_duplicate_labels\nOut[25]: True\n  The new DataFrame returned is a view on the same data as the old DataFrame. Or the property can just be set directly on the same object \nIn [26]: df2.flags.allows_duplicate_labels = False\n\nIn [27]: df2.flags.allows_duplicate_labels\nOut[27]: False\n  When processing raw, messy data you might initially read in the messy data (which potentially has duplicate labels), deduplicate, and then disallow duplicates going forward, to ensure that your data pipeline doesn\u2019t introduce duplicates. \n>>> raw = pd.read_csv(\"...\")\n>>> deduplicated = raw.groupby(level=0).first()  # remove duplicates\n>>> deduplicated.flags.allows_duplicate_labels = False  # disallow going forward\n  Setting allows_duplicate_labels=True on a Series or DataFrame with duplicate labels or performing an operation that introduces duplicate labels on a Series or DataFrame that disallows duplicates will raise an errors.DuplicateLabelError. \nIn [28]: df.rename(str.upper)\n---------------------------------------------------------------------------\nDuplicateLabelError                       Traceback (most recent call last)\nInput In [28], in <module>\n----> 1 df.rename(str.upper)\n\nFile /pandas/pandas/core/frame.py:5085, in DataFrame.rename(self, mapper, index, columns, axis, copy, inplace, level, errors)\n   4966 def rename(\n   4967     self,\n   4968     mapper: Renamer | None = None,\n   (...)\n   4976     errors: str = \"ignore\",\n   4977 ) -> DataFrame | None:\n   4978     \"\"\"\n   4979     Alter axes labels.\n   4980 \n   (...)\n   5083     4  3  6\n   5084     \"\"\"\n-> 5085     return super()._rename(\n   5086         mapper=mapper,\n   5087         index=index,\n   5088         columns=columns,\n   5089         axis=axis,\n   5090         copy=copy,\n   5091         inplace=inplace,\n   5092         level=level,\n   5093         errors=errors,\n   5094     )\n\nFile /pandas/pandas/core/generic.py:1171, in NDFrame._rename(self, mapper, index, columns, axis, copy, inplace, level, errors)\n   1169     return None\n   1170 else:\n-> 1171     return result.__finalize__(self, method=\"rename\")\n\nFile /pandas/pandas/core/generic.py:5549, in NDFrame.__finalize__(self, other, method, **kwargs)\n   5546 for name in other.attrs:\n   5547     self.attrs[name] = other.attrs[name]\n-> 5549 self.flags.allows_duplicate_labels = other.flags.allows_duplicate_labels\n   5550 # For subclasses using _metadata.\n   5551 for name in set(self._metadata) & set(other._metadata):\n\nFile /pandas/pandas/core/flags.py:92, in Flags.allows_duplicate_labels(self, value)\n     90 if not value:\n     91     for ax in obj.axes:\n---> 92         ax._maybe_check_unique()\n     94 self._allows_duplicate_labels = value\n\nFile /pandas/pandas/core/indexes/base.py:715, in Index._maybe_check_unique(self)\n    712 duplicates = self._format_duplicate_message()\n    713 msg += f\"\\n{duplicates}\"\n--> 715 raise DuplicateLabelError(msg)\n\nDuplicateLabelError: Index has duplicates.\n      positions\nlabel          \nX        [0, 2]\nY        [1, 3]\n  This error message contains the labels that are duplicated, and the numeric positions of all the duplicates (including the \u201coriginal\u201d) in the Series or DataFrame  Duplicate Label Propagation In general, disallowing duplicates is \u201csticky\u201d. It\u2019s preserved through operations. \nIn [29]: s1 = pd.Series(0, index=[\"a\", \"b\"]).set_flags(allows_duplicate_labels=False)\n\nIn [30]: s1\nOut[30]: \na    0\nb    0\ndtype: int64\n\nIn [31]: s1.head().rename({\"a\": \"b\"})\n---------------------------------------------------------------------------\nDuplicateLabelError                       Traceback (most recent call last)\nInput In [31], in <module>\n----> 1 s1.head().rename({\"a\": \"b\"})\n\nFile /pandas/pandas/core/series.py:4598, in Series.rename(self, index, axis, copy, inplace, level, errors)\n   4595     axis = self._get_axis_number(axis)\n   4597 if callable(index) or is_dict_like(index):\n-> 4598     return super()._rename(\n   4599         index, copy=copy, inplace=inplace, level=level, errors=errors\n   4600     )\n   4601 else:\n   4602     return self._set_name(index, inplace=inplace)\n\nFile /pandas/pandas/core/generic.py:1171, in NDFrame._rename(self, mapper, index, columns, axis, copy, inplace, level, errors)\n   1169     return None\n   1170 else:\n-> 1171     return result.__finalize__(self, method=\"rename\")\n\nFile /pandas/pandas/core/generic.py:5549, in NDFrame.__finalize__(self, other, method, **kwargs)\n   5546 for name in other.attrs:\n   5547     self.attrs[name] = other.attrs[name]\n-> 5549 self.flags.allows_duplicate_labels = other.flags.allows_duplicate_labels\n   5550 # For subclasses using _metadata.\n   5551 for name in set(self._metadata) & set(other._metadata):\n\nFile /pandas/pandas/core/flags.py:92, in Flags.allows_duplicate_labels(self, value)\n     90 if not value:\n     91     for ax in obj.axes:\n---> 92         ax._maybe_check_unique()\n     94 self._allows_duplicate_labels = value\n\nFile /pandas/pandas/core/indexes/base.py:715, in Index._maybe_check_unique(self)\n    712 duplicates = self._format_duplicate_message()\n    713 msg += f\"\\n{duplicates}\"\n--> 715 raise DuplicateLabelError(msg)\n\nDuplicateLabelError: Index has duplicates.\n      positions\nlabel          \nb        [0, 1]\n   Warning This is an experimental feature. Currently, many methods fail to propagate the allows_duplicate_labels value. In future versions it is expected that every method taking or returning one or more DataFrame or Series objects will propagate allows_duplicate_labels.   \n"}, {"name": "Enhancing performance", "path": "user_guide/enhancingperf", "type": "Manual", "text": "Enhancing performance In this part of the tutorial, we will investigate how to speed up certain functions operating on pandas DataFrames using three different techniques: Cython, Numba and pandas.eval(). We will see a speed improvement of ~200 when we use Cython and Numba on a test function operating row-wise on the DataFrame. Using pandas.eval() we will speed up a sum by an order of ~2.  Note In addition to following the steps in this tutorial, users interested in enhancing performance are highly encouraged to install the recommended dependencies for pandas. These dependencies are often not installed by default, but will offer speed improvements if present.   Cython (writing C extensions for pandas) For many use cases writing pandas in pure Python and NumPy is sufficient. In some computationally heavy applications however, it can be possible to achieve sizable speed-ups by offloading work to cython. This tutorial assumes you have refactored as much as possible in Python, for example by trying to remove for-loops and making use of NumPy vectorization. It\u2019s always worth optimising in Python first. This tutorial walks through a \u201ctypical\u201d process of cythonizing a slow computation. We use an example from the Cython documentation but in the context of pandas. Our final cythonized solution is around 100 times faster than the pure Python solution.  Pure Python We have a DataFrame to which we want to apply a function row-wise. \nIn [1]: df = pd.DataFrame(\n   ...:     {\n   ...:         \"a\": np.random.randn(1000),\n   ...:         \"b\": np.random.randn(1000),\n   ...:         \"N\": np.random.randint(100, 1000, (1000)),\n   ...:         \"x\": \"x\",\n   ...:     }\n   ...: )\n   ...: \n\nIn [2]: df\nOut[2]: \n            a         b    N  x\n0    0.469112 -0.218470  585  x\n1   -0.282863 -0.061645  841  x\n2   -1.509059 -0.723780  251  x\n3   -1.135632  0.551225  972  x\n4    1.212112 -0.497767  181  x\n..        ...       ...  ... ..\n995 -1.512743  0.874737  374  x\n996  0.933753  1.120790  246  x\n997 -0.308013  0.198768  157  x\n998 -0.079915  1.757555  977  x\n999 -1.010589 -1.115680  770  x\n\n[1000 rows x 4 columns]\n  Here\u2019s the function in pure Python: \nIn [3]: def f(x):\n   ...:     return x * (x - 1)\n   ...: \n\nIn [4]: def integrate_f(a, b, N):\n   ...:     s = 0\n   ...:     dx = (b - a) / N\n   ...:     for i in range(N):\n   ...:         s += f(a + i * dx)\n   ...:     return s * dx\n   ...: \n  We achieve our result by using apply (row-wise): \nIn [7]: %timeit df.apply(lambda x: integrate_f(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1)\n10 loops, best of 3: 174 ms per loop\n  But clearly this isn\u2019t fast enough for us. Let\u2019s take a look and see where the time is spent during this operation (limited to the most time consuming four calls) using the prun ipython magic function: \nIn [5]: %prun -l 4 df.apply(lambda x: integrate_f(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1)  # noqa E999\n         638348 function calls (638330 primitive calls) in 0.230 seconds\n\n   Ordered by: internal time\n   List reduced from 228 to 4 due to restriction <4>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n     1000    0.123    0.000    0.177    0.000 <ipython-input-4-c2a74e076cf0>:1(integrate_f)\n   552423    0.054    0.000    0.054    0.000 <ipython-input-3-c138bdd570e3>:1(f)\n     3000    0.007    0.000    0.032    0.000 series.py:944(__getitem__)\n     3000    0.004    0.000    0.019    0.000 series.py:1053(_get_value)\n  By far the majority of time is spend inside either integrate_f or f, hence we\u2019ll concentrate our efforts cythonizing these two functions.   Plain Cython First we\u2019re going to need to import the Cython magic function to IPython: \nIn [6]: %load_ext Cython\n  Now, let\u2019s simply copy our functions over to Cython as is (the suffix is here to distinguish between function versions): \nIn [7]: %%cython\n   ...: def f_plain(x):\n   ...:     return x * (x - 1)\n   ...: def integrate_f_plain(a, b, N):\n   ...:     s = 0\n   ...:     dx = (b - a) / N\n   ...:     for i in range(N):\n   ...:         s += f_plain(a + i * dx)\n   ...:     return s * dx\n   ...: \n   Note If you\u2019re having trouble pasting the above into your ipython, you may need to be using bleeding edge IPython for paste to play well with cell magics.  \nIn [4]: %timeit df.apply(lambda x: integrate_f_plain(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1)\n10 loops, best of 3: 85.5 ms per loop\n  Already this has shaved a third off, not too bad for a simple copy and paste.   Adding type We get another huge improvement simply by providing type information: \nIn [8]: %%cython\n   ...: cdef double f_typed(double x) except? -2:\n   ...:     return x * (x - 1)\n   ...: cpdef double integrate_f_typed(double a, double b, int N):\n   ...:     cdef int i\n   ...:     cdef double s, dx\n   ...:     s = 0\n   ...:     dx = (b - a) / N\n   ...:     for i in range(N):\n   ...:         s += f_typed(a + i * dx)\n   ...:     return s * dx\n   ...: \n  \nIn [4]: %timeit df.apply(lambda x: integrate_f_typed(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1)\n10 loops, best of 3: 20.3 ms per loop\n  Now, we\u2019re talking! It\u2019s now over ten times faster than the original Python implementation, and we haven\u2019t really modified the code. Let\u2019s have another look at what\u2019s eating up time: \nIn [9]: %prun -l 4 df.apply(lambda x: integrate_f_typed(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1)\n         85918 function calls (85900 primitive calls) in 0.034 seconds\n\n   Ordered by: internal time\n   List reduced from 221 to 4 due to restriction <4>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n     3000    0.004    0.000    0.021    0.000 series.py:944(__getitem__)\n     3000    0.003    0.000    0.012    0.000 series.py:1053(_get_value)\n    16189    0.002    0.000    0.002    0.000 {built-in method builtins.isinstance}\n     3000    0.002    0.000    0.006    0.000 base.py:3577(get_loc)\n    Using ndarray It\u2019s calling series\u2026 a lot! It\u2019s creating a Series from each row, and get-ting from both the index and the series (three times for each row). Function calls are expensive in Python, so maybe we could minimize these by cythonizing the apply part.  Note We are now passing ndarrays into the Cython function, fortunately Cython plays very nicely with NumPy.  \nIn [10]: %%cython\n   ....: cimport numpy as np\n   ....: import numpy as np\n   ....: cdef double f_typed(double x) except? -2:\n   ....:     return x * (x - 1)\n   ....: cpdef double integrate_f_typed(double a, double b, int N):\n   ....:     cdef int i\n   ....:     cdef double s, dx\n   ....:     s = 0\n   ....:     dx = (b - a) / N\n   ....:     for i in range(N):\n   ....:         s += f_typed(a + i * dx)\n   ....:     return s * dx\n   ....: cpdef np.ndarray[double] apply_integrate_f(np.ndarray col_a, np.ndarray col_b,\n   ....:                                            np.ndarray col_N):\n   ....:     assert (col_a.dtype == np.float_\n   ....:             and col_b.dtype == np.float_ and col_N.dtype == np.int_)\n   ....:     cdef Py_ssize_t i, n = len(col_N)\n   ....:     assert (len(col_a) == len(col_b) == n)\n   ....:     cdef np.ndarray[double] res = np.empty(n)\n   ....:     for i in range(len(col_a)):\n   ....:         res[i] = integrate_f_typed(col_a[i], col_b[i], col_N[i])\n   ....:     return res\n   ....: \n  The implementation is simple, it creates an array of zeros and loops over the rows, applying our integrate_f_typed, and putting this in the zeros array.  Warning You can not pass a Series directly as a ndarray typed parameter to a Cython function. Instead pass the actual ndarray using the Series.to_numpy(). The reason is that the Cython definition is specific to an ndarray and not the passed Series. So, do not do this: \napply_integrate_f(df[\"a\"], df[\"b\"], df[\"N\"])\n  But rather, use Series.to_numpy() to get the underlying ndarray: \napply_integrate_f(df[\"a\"].to_numpy(), df[\"b\"].to_numpy(), df[\"N\"].to_numpy())\n    Note Loops like this would be extremely slow in Python, but in Cython looping over NumPy arrays is fast.  \nIn [4]: %timeit apply_integrate_f(df[\"a\"].to_numpy(), df[\"b\"].to_numpy(), df[\"N\"].to_numpy())\n1000 loops, best of 3: 1.25 ms per loop\n  We\u2019ve gotten another big improvement. Let\u2019s check again where the time is spent: \nIn [11]: %prun -l 4 apply_integrate_f(df[\"a\"].to_numpy(), df[\"b\"].to_numpy(), df[\"N\"].to_numpy())\n         200 function calls in 0.001 seconds\n\n   Ordered by: internal time\n   List reduced from 53 to 4 due to restriction <4>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n        1    0.001    0.001    0.001    0.001 {built-in method _cython_magic_f5cd3d072b0d379f774a53a3036c6b65.apply_integrate_f}\n        3    0.000    0.000    0.000    0.000 frame.py:3464(__getitem__)\n        1    0.000    0.000    0.001    0.001 {built-in method builtins.exec}\n        3    0.000    0.000    0.000    0.000 managers.py:1016(iget)\n  As one might expect, the majority of the time is now spent in apply_integrate_f, so if we wanted to make anymore efficiencies we must continue to concentrate our efforts here.   More advanced techniques There is still hope for improvement. Here\u2019s an example of using some more advanced Cython techniques: \nIn [12]: %%cython\n   ....: cimport cython\n   ....: cimport numpy as np\n   ....: import numpy as np\n   ....: cdef double f_typed(double x) except? -2:\n   ....:     return x * (x - 1)\n   ....: cpdef double integrate_f_typed(double a, double b, int N):\n   ....:     cdef int i\n   ....:     cdef double s, dx\n   ....:     s = 0\n   ....:     dx = (b - a) / N\n   ....:     for i in range(N):\n   ....:         s += f_typed(a + i * dx)\n   ....:     return s * dx\n   ....: @cython.boundscheck(False)\n   ....: @cython.wraparound(False)\n   ....: cpdef np.ndarray[double] apply_integrate_f_wrap(np.ndarray[double] col_a,\n   ....:                                                 np.ndarray[double] col_b,\n   ....:                                                 np.ndarray[int] col_N):\n   ....:     cdef int i, n = len(col_N)\n   ....:     assert len(col_a) == len(col_b) == n\n   ....:     cdef np.ndarray[double] res = np.empty(n)\n   ....:     for i in range(n):\n   ....:         res[i] = integrate_f_typed(col_a[i], col_b[i], col_N[i])\n   ....:     return res\n   ....: \n  \nIn [4]: %timeit apply_integrate_f_wrap(df[\"a\"].to_numpy(), df[\"b\"].to_numpy(), df[\"N\"].to_numpy())\n1000 loops, best of 3: 987 us per loop\n  Even faster, with the caveat that a bug in our Cython code (an off-by-one error, for example) might cause a segfault because memory access isn\u2019t checked. For more about boundscheck and wraparound, see the Cython docs on compiler directives.    Numba (JIT compilation) An alternative to statically compiling Cython code is to use a dynamic just-in-time (JIT) compiler with Numba. Numba allows you to write a pure Python function which can be JIT compiled to native machine instructions, similar in performance to C, C++ and Fortran, by decorating your function with @jit. Numba works by generating optimized machine code using the LLVM compiler infrastructure at import time, runtime, or statically (using the included pycc tool). Numba supports compilation of Python to run on either CPU or GPU hardware and is designed to integrate with the Python scientific software stack.  Note The @jit compilation will add overhead to the runtime of the function, so performance benefits may not be realized especially when using small data sets. Consider caching your function to avoid compilation overhead each time your function is run.  Numba can be used in 2 ways with pandas:  Specify the engine=\"numba\" keyword in select pandas methods Define your own Python function decorated with @jit and pass the underlying NumPy array of Series or Dataframe (using to_numpy()) into the function   pandas Numba Engine If Numba is installed, one can specify engine=\"numba\" in select pandas methods to execute the method using Numba. Methods that support engine=\"numba\" will also have an engine_kwargs keyword that accepts a dictionary that allows one to specify \"nogil\", \"nopython\" and \"parallel\" keys with boolean values to pass into the @jit decorator. If engine_kwargs is not specified, it defaults to {\"nogil\": False, \"nopython\": True, \"parallel\": False} unless otherwise specified. In terms of performance, the first time a function is run using the Numba engine will be slow as Numba will have some function compilation overhead. However, the JIT compiled functions are cached, and subsequent calls will be fast. In general, the Numba engine is performant with a larger amount of data points (e.g. 1+ million). \nIn [1]: data = pd.Series(range(1_000_000))  # noqa: E225\n\nIn [2]: roll = data.rolling(10)\n\nIn [3]: def f(x):\n   ...:     return np.sum(x) + 5\n# Run the first time, compilation time will affect performance\nIn [4]: %timeit -r 1 -n 1 roll.apply(f, engine='numba', raw=True)\n1.23 s \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1 loop each)\n# Function is cached and performance will improve\nIn [5]: %timeit roll.apply(f, engine='numba', raw=True)\n188 ms \u00b1 1.93 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [6]: %timeit roll.apply(f, engine='cython', raw=True)\n3.92 s \u00b1 59 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n    Custom Function Examples A custom Python function decorated with @jit can be used with pandas objects by passing their NumPy array representations with to_numpy(). \nimport numba\n\n\n@numba.jit\ndef f_plain(x):\n    return x * (x - 1)\n\n\n@numba.jit\ndef integrate_f_numba(a, b, N):\n    s = 0\n    dx = (b - a) / N\n    for i in range(N):\n        s += f_plain(a + i * dx)\n    return s * dx\n\n\n@numba.jit\ndef apply_integrate_f_numba(col_a, col_b, col_N):\n    n = len(col_N)\n    result = np.empty(n, dtype=\"float64\")\n    assert len(col_a) == len(col_b) == n\n    for i in range(n):\n        result[i] = integrate_f_numba(col_a[i], col_b[i], col_N[i])\n    return result\n\n\ndef compute_numba(df):\n    result = apply_integrate_f_numba(\n        df[\"a\"].to_numpy(), df[\"b\"].to_numpy(), df[\"N\"].to_numpy()\n    )\n    return pd.Series(result, index=df.index, name=\"result\")\n  \nIn [4]: %timeit compute_numba(df)\n1000 loops, best of 3: 798 us per loop\n  In this example, using Numba was faster than Cython. Numba can also be used to write vectorized functions that do not require the user to explicitly loop over the observations of a vector; a vectorized function will be applied to each row automatically. Consider the following example of doubling each observation: \nimport numba\n\n\ndef double_every_value_nonumba(x):\n    return x * 2\n\n\n@numba.vectorize\ndef double_every_value_withnumba(x):  # noqa E501\n    return x * 2\n  \n# Custom function without numba\nIn [5]: %timeit df[\"col1_doubled\"] = df[\"a\"].apply(double_every_value_nonumba)  # noqa E501\n1000 loops, best of 3: 797 us per loop\n\n# Standard implementation (faster than a custom function)\nIn [6]: %timeit df[\"col1_doubled\"] = df[\"a\"] * 2\n1000 loops, best of 3: 233 us per loop\n\n# Custom function with numba\nIn [7]: %timeit df[\"col1_doubled\"] = double_every_value_withnumba(df[\"a\"].to_numpy())\n1000 loops, best of 3: 145 us per loop\n    Caveats Numba is best at accelerating functions that apply numerical functions to NumPy arrays. If you try to @jit a function that contains unsupported Python or NumPy code, compilation will revert object mode which will mostly likely not speed up your function. If you would prefer that Numba throw an error if it cannot compile a function in a way that speeds up your code, pass Numba the argument nopython=True (e.g. @jit(nopython=True)). For more on troubleshooting Numba modes, see the Numba troubleshooting page. Using parallel=True (e.g. @jit(parallel=True)) may result in a SIGABRT if the threading layer leads to unsafe behavior. You can first specify a safe threading layer before running a JIT function with parallel=True. Generally if the you encounter a segfault (SIGSEGV) while using Numba, please report the issue to the Numba issue tracker.    Expression evaluation via eval()\n The top-level function pandas.eval() implements expression evaluation of Series and DataFrame objects.  Note To benefit from using eval() you need to install numexpr. See the recommended dependencies section for more details.  The point of using eval() for expression evaluation rather than plain Python is two-fold: 1) large DataFrame objects are evaluated more efficiently and 2) large arithmetic and boolean expressions are evaluated all at once by the underlying engine (by default numexpr is used for evaluation).  Note You should not use eval() for simple expressions or for expressions involving small DataFrames. In fact, eval() is many orders of magnitude slower for smaller expressions/objects than plain ol\u2019 Python. A good rule of thumb is to only use eval() when you have a DataFrame with more than 10,000 rows.  eval() supports all arithmetic expressions supported by the engine in addition to some extensions available only in pandas.  Note The larger the frame and the larger the expression the more speedup you will see from using eval().   Supported syntax These operations are supported by pandas.eval():  Arithmetic operations except for the left shift (<<) and right shift (>>) operators, e.g., df + 2 * pi / s ** 4 % 42 - the_golden_ratio Comparison operations, including chained comparisons, e.g., 2 < df < df2 Boolean operations, e.g., df < df2 and df3 < df4 or not df_bool list and tuple literals, e.g., [1, 2] or (1, 2) Attribute access, e.g., df.a Subscript expressions, e.g., df[0] Simple variable evaluation, e.g., pd.eval(\"df\") (this is not very useful) Math functions: sin, cos, exp, log, expm1, log1p, sqrt, sinh, cosh, tanh, arcsin, arccos, arctan, arccosh, arcsinh, arctanh, abs, arctan2 and log10.  This Python syntax is not allowed:  \nExpressions  \n Function calls other than math functions. is/is not operations if expressions lambda expressions list/set/dict comprehensions Literal dict and set expressions yield expressions Generator expressions Boolean expressions consisting of only scalar values  \n  \nStatements  \n Neither simple nor compound statements are allowed. This includes things like for, while, and if.  \n     \neval() examples pandas.eval() works well with expressions containing large arrays. First let\u2019s create a few decent-sized arrays to play with: \nIn [13]: nrows, ncols = 20000, 100\n\nIn [14]: df1, df2, df3, df4 = [pd.DataFrame(np.random.randn(nrows, ncols)) for _ in range(4)]\n  Now let\u2019s compare adding them together using plain ol\u2019 Python versus eval(): \nIn [15]: %timeit df1 + df2 + df3 + df4\n8.17 ms +- 266 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n  \nIn [16]: %timeit pd.eval(\"df1 + df2 + df3 + df4\")\n5.46 ms +- 230 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n  Now let\u2019s do the same thing but with comparisons: \nIn [17]: %timeit (df1 > 0) & (df2 > 0) & (df3 > 0) & (df4 > 0)\n6.45 ms +- 277 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n  \nIn [18]: %timeit pd.eval(\"(df1 > 0) & (df2 > 0) & (df3 > 0) & (df4 > 0)\")\n6.07 ms +- 225 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n  eval() also works with unaligned pandas objects: \nIn [19]: s = pd.Series(np.random.randn(50))\n\nIn [20]: %timeit df1 + df2 + df3 + df4 + s\n23.1 ms +- 1.25 ms per loop (mean +- std. dev. of 7 runs, 10 loops each)\n  \nIn [21]: %timeit pd.eval(\"df1 + df2 + df3 + df4 + s\")\n5.87 ms +- 127 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n   Note Operations such as  \n\n1 and 2  # would parse to 1 & 2, but should evaluate to 2\n3 or 4  # would parse to 3 | 4, but should evaluate to 3\n~1  # this is okay, but slower when using eval\n  \n should be performed in Python. An exception will be raised if you try to perform any boolean/bitwise operations with scalar operands that are not of type bool or np.bool_. Again, you should perform these kinds of operations in plain Python.    The DataFrame.eval method In addition to the top level pandas.eval() function you can also evaluate an expression in the \u201ccontext\u201d of a DataFrame. \nIn [22]: df = pd.DataFrame(np.random.randn(5, 2), columns=[\"a\", \"b\"])\n\nIn [23]: df.eval(\"a + b\")\nOut[23]: \n0   -0.246747\n1    0.867786\n2   -1.626063\n3   -1.134978\n4   -1.027798\ndtype: float64\n  Any expression that is a valid pandas.eval() expression is also a valid DataFrame.eval() expression, with the added benefit that you don\u2019t have to prefix the name of the DataFrame to the column(s) you\u2019re interested in evaluating. In addition, you can perform assignment of columns within an expression. This allows for formulaic evaluation. The assignment target can be a new column name or an existing column name, and it must be a valid Python identifier. The inplace keyword determines whether this assignment will performed on the original DataFrame or return a copy with the new column. \nIn [24]: df = pd.DataFrame(dict(a=range(5), b=range(5, 10)))\n\nIn [25]: df.eval(\"c = a + b\", inplace=True)\n\nIn [26]: df.eval(\"d = a + b + c\", inplace=True)\n\nIn [27]: df.eval(\"a = 1\", inplace=True)\n\nIn [28]: df\nOut[28]: \n   a  b   c   d\n0  1  5   5  10\n1  1  6   7  14\n2  1  7   9  18\n3  1  8  11  22\n4  1  9  13  26\n  When inplace is set to False, the default, a copy of the DataFrame with the new or modified columns is returned and the original frame is unchanged. \nIn [29]: df\nOut[29]: \n   a  b   c   d\n0  1  5   5  10\n1  1  6   7  14\n2  1  7   9  18\n3  1  8  11  22\n4  1  9  13  26\n\nIn [30]: df.eval(\"e = a - c\", inplace=False)\nOut[30]: \n   a  b   c   d   e\n0  1  5   5  10  -4\n1  1  6   7  14  -6\n2  1  7   9  18  -8\n3  1  8  11  22 -10\n4  1  9  13  26 -12\n\nIn [31]: df\nOut[31]: \n   a  b   c   d\n0  1  5   5  10\n1  1  6   7  14\n2  1  7   9  18\n3  1  8  11  22\n4  1  9  13  26\n  As a convenience, multiple assignments can be performed by using a multi-line string. \nIn [32]: df.eval(\n   ....:     \"\"\"\n   ....: c = a + b\n   ....: d = a + b + c\n   ....: a = 1\"\"\",\n   ....:     inplace=False,\n   ....: )\n   ....: \nOut[32]: \n   a  b   c   d\n0  1  5   6  12\n1  1  6   7  14\n2  1  7   8  16\n3  1  8   9  18\n4  1  9  10  20\n  The equivalent in standard Python would be \nIn [33]: df = pd.DataFrame(dict(a=range(5), b=range(5, 10)))\n\nIn [34]: df[\"c\"] = df[\"a\"] + df[\"b\"]\n\nIn [35]: df[\"d\"] = df[\"a\"] + df[\"b\"] + df[\"c\"]\n\nIn [36]: df[\"a\"] = 1\n\nIn [37]: df\nOut[37]: \n   a  b   c   d\n0  1  5   5  10\n1  1  6   7  14\n2  1  7   9  18\n3  1  8  11  22\n4  1  9  13  26\n  The query method has a inplace keyword which determines whether the query modifies the original frame. \nIn [38]: df = pd.DataFrame(dict(a=range(5), b=range(5, 10)))\n\nIn [39]: df.query(\"a > 2\")\nOut[39]: \n   a  b\n3  3  8\n4  4  9\n\nIn [40]: df.query(\"a > 2\", inplace=True)\n\nIn [41]: df\nOut[41]: \n   a  b\n3  3  8\n4  4  9\n    Local variables You must explicitly reference any local variable that you want to use in an expression by placing the @ character in front of the name. For example, \nIn [42]: df = pd.DataFrame(np.random.randn(5, 2), columns=list(\"ab\"))\n\nIn [43]: newcol = np.random.randn(len(df))\n\nIn [44]: df.eval(\"b + @newcol\")\nOut[44]: \n0   -0.173926\n1    2.493083\n2   -0.881831\n3   -0.691045\n4    1.334703\ndtype: float64\n\nIn [45]: df.query(\"b < @newcol\")\nOut[45]: \n          a         b\n0  0.863987 -0.115998\n2 -2.621419 -1.297879\n  If you don\u2019t prefix the local variable with @, pandas will raise an exception telling you the variable is undefined. When using DataFrame.eval() and DataFrame.query(), this allows you to have a local variable and a DataFrame column with the same name in an expression. \nIn [46]: a = np.random.randn()\n\nIn [47]: df.query(\"@a < a\")\nOut[47]: \n          a         b\n0  0.863987 -0.115998\n\nIn [48]: df.loc[a < df[\"a\"]]  # same as the previous expression\nOut[48]: \n          a         b\n0  0.863987 -0.115998\n  With pandas.eval() you cannot use the @ prefix at all, because it isn\u2019t defined in that context. pandas will let you know this if you try to use @ in a top-level call to pandas.eval(). For example, \nIn [49]: a, b = 1, 2\n\nIn [50]: pd.eval(\"@a + b\")\nTraceback (most recent call last):\n\n  File /opt/conda/envs/pandas/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3251 in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n\n  Input In [50] in <module>\n    pd.eval(\"@a + b\")\n\n  File /pandas/pandas/core/computation/eval.py:339 in eval\n    _check_for_locals(expr, level, parser)\n\n  File /pandas/pandas/core/computation/eval.py:163 in _check_for_locals\n    raise SyntaxError(msg)\n\n  File <string>\nSyntaxError: The '@' prefix is not allowed in top-level eval calls.\nplease refer to your variables by name without the '@' prefix.\n  In this case, you should simply refer to the variables like you would in standard Python. \nIn [51]: pd.eval(\"a + b\")\nOut[51]: 3\n    \npandas.eval() parsers There are two different parsers and two different engines you can use as the backend. The default 'pandas' parser allows a more intuitive syntax for expressing query-like operations (comparisons, conjunctions and disjunctions). In particular, the precedence of the & and | operators is made equal to the precedence of the corresponding boolean operations and and or. For example, the above conjunction can be written without parentheses. Alternatively, you can use the 'python' parser to enforce strict Python semantics. \nIn [52]: expr = \"(df1 > 0) & (df2 > 0) & (df3 > 0) & (df4 > 0)\"\n\nIn [53]: x = pd.eval(expr, parser=\"python\")\n\nIn [54]: expr_no_parens = \"df1 > 0 & df2 > 0 & df3 > 0 & df4 > 0\"\n\nIn [55]: y = pd.eval(expr_no_parens, parser=\"pandas\")\n\nIn [56]: np.all(x == y)\nOut[56]: True\n  The same expression can be \u201canded\u201d together with the word and as well: \nIn [57]: expr = \"(df1 > 0) & (df2 > 0) & (df3 > 0) & (df4 > 0)\"\n\nIn [58]: x = pd.eval(expr, parser=\"python\")\n\nIn [59]: expr_with_ands = \"df1 > 0 and df2 > 0 and df3 > 0 and df4 > 0\"\n\nIn [60]: y = pd.eval(expr_with_ands, parser=\"pandas\")\n\nIn [61]: np.all(x == y)\nOut[61]: True\n  The and and or operators here have the same precedence that they would in vanilla Python.   \npandas.eval() backends There\u2019s also the option to make eval() operate identical to plain ol\u2019 Python.  Note Using the 'python' engine is generally not useful, except for testing other evaluation engines against it. You will achieve no performance benefits using eval() with engine='python' and in fact may incur a performance hit.  You can see this by using pandas.eval() with the 'python' engine. It is a bit slower (not by much) than evaluating the same expression in Python \nIn [62]: %timeit df1 + df2 + df3 + df4\n7.12 ms +- 296 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n  \nIn [63]: %timeit pd.eval(\"df1 + df2 + df3 + df4\", engine=\"python\")\n8.41 ms +- 266 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n    \npandas.eval() performance eval() is intended to speed up certain kinds of operations. In particular, those operations involving complex expressions with large DataFrame/Series objects should see a significant performance benefit. Here is a plot showing the running time of pandas.eval() as function of the size of the frame involved in the computation. The two lines are two different engines.   Note Operations with smallish objects (around 15k-20k rows) are faster using plain Python:  \n \n  This plot was created using a DataFrame with 3 columns each containing floating point values generated using numpy.random.randn().   Technical minutia regarding expression evaluation Expressions that would result in an object dtype or involve datetime operations (because of NaT) must be evaluated in Python space. The main reason for this behavior is to maintain backwards compatibility with versions of NumPy < 1.7. In those versions of NumPy a call to ndarray.astype(str) will truncate any strings that are more than 60 characters in length. Second, we can\u2019t pass object arrays to numexpr thus string comparisons must be evaluated in Python space. The upshot is that this only applies to object-dtype expressions. So, if you have an expression\u2013for example \nIn [64]: df = pd.DataFrame(\n   ....:     {\"strings\": np.repeat(list(\"cba\"), 3), \"nums\": np.repeat(range(3), 3)}\n   ....: )\n   ....: \n\nIn [65]: df\nOut[65]: \n  strings  nums\n0       c     0\n1       c     0\n2       c     0\n3       b     1\n4       b     1\n5       b     1\n6       a     2\n7       a     2\n8       a     2\n\nIn [66]: df.query(\"strings == 'a' and nums == 1\")\nOut[66]: \nEmpty DataFrame\nColumns: [strings, nums]\nIndex: []\n  the numeric part of the comparison (nums == 1) will be evaluated by numexpr. In general, DataFrame.query()/pandas.eval() will evaluate the subexpressions that can be evaluated by numexpr and those that must be evaluated in Python space transparently to the user. This is done by inferring the result type of an expression from its arguments and operators.  \n"}, {"name": "Essential basic functionality", "path": "user_guide/basics", "type": "Manual", "text": "Essential basic functionality Here we discuss a lot of the essential functionality common to the pandas data structures. To begin, let\u2019s create some example objects like we did in the 10 minutes to pandas section: \nIn [1]: index = pd.date_range(\"1/1/2000\", periods=8)\n\nIn [2]: s = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n\nIn [3]: df = pd.DataFrame(np.random.randn(8, 3), index=index, columns=[\"A\", \"B\", \"C\"])\n   Head and tail To view a small sample of a Series or DataFrame object, use the head() and tail() methods. The default number of elements to display is five, but you may pass a custom number. \nIn [4]: long_series = pd.Series(np.random.randn(1000))\n\nIn [5]: long_series.head()\nOut[5]: \n0   -1.157892\n1   -1.344312\n2    0.844885\n3    1.075770\n4   -0.109050\ndtype: float64\n\nIn [6]: long_series.tail(3)\nOut[6]: \n997   -0.289388\n998   -1.020544\n999    0.589993\ndtype: float64\n    Attributes and underlying data pandas objects have a number of attributes enabling you to access the metadata  shape: gives the axis dimensions of the object, consistent with ndarray \n Axis labels\n\n Series: index (only axis) DataFrame: index (rows) and columns      Note, these attributes can be safely assigned to! \nIn [7]: df[:2]\nOut[7]: \n                   A         B         C\n2000-01-01 -0.173215  0.119209 -1.044236\n2000-01-02 -0.861849 -2.104569 -0.494929\n\nIn [8]: df.columns = [x.lower() for x in df.columns]\n\nIn [9]: df\nOut[9]: \n                   a         b         c\n2000-01-01 -0.173215  0.119209 -1.044236\n2000-01-02 -0.861849 -2.104569 -0.494929\n2000-01-03  1.071804  0.721555 -0.706771\n2000-01-04 -1.039575  0.271860 -0.424972\n2000-01-05  0.567020  0.276232 -1.087401\n2000-01-06 -0.673690  0.113648 -1.478427\n2000-01-07  0.524988  0.404705  0.577046\n2000-01-08 -1.715002 -1.039268 -0.370647\n  pandas objects (Index, Series, DataFrame) can be thought of as containers for arrays, which hold the actual data and do the actual computation. For many types, the underlying array is a numpy.ndarray. However, pandas and 3rd party libraries may extend NumPy\u2019s type system to add support for custom arrays (see dtypes). To get the actual data inside a Index or Series, use the .array property \nIn [10]: s.array\nOut[10]: \n<PandasArray>\n[ 0.4691122999071863, -0.2828633443286633, -1.5090585031735124,\n -1.1356323710171934,  1.2121120250208506]\nLength: 5, dtype: float64\n\nIn [11]: s.index.array\nOut[11]: \n<PandasArray>\n['a', 'b', 'c', 'd', 'e']\nLength: 5, dtype: object\n  array will always be an ExtensionArray. The exact details of what an ExtensionArray is and why pandas uses them are a bit beyond the scope of this introduction. See dtypes for more. If you know you need a NumPy array, use to_numpy() or numpy.asarray(). \nIn [12]: s.to_numpy()\nOut[12]: array([ 0.4691, -0.2829, -1.5091, -1.1356,  1.2121])\n\nIn [13]: np.asarray(s)\nOut[13]: array([ 0.4691, -0.2829, -1.5091, -1.1356,  1.2121])\n  When the Series or Index is backed by an ExtensionArray, to_numpy() may involve copying data and coercing values. See dtypes for more. to_numpy() gives some control over the dtype of the resulting numpy.ndarray. For example, consider datetimes with timezones. NumPy doesn\u2019t have a dtype to represent timezone-aware datetimes, so there are two possibly useful representations:  An object-dtype numpy.ndarray with Timestamp objects, each with the correct tz A datetime64[ns] -dtype numpy.ndarray, where the values have been converted to UTC and the timezone discarded  Timezones may be preserved with dtype=object \nIn [14]: ser = pd.Series(pd.date_range(\"2000\", periods=2, tz=\"CET\"))\n\nIn [15]: ser.to_numpy(dtype=object)\nOut[15]: \narray([Timestamp('2000-01-01 00:00:00+0100', tz='CET'),\n       Timestamp('2000-01-02 00:00:00+0100', tz='CET')], dtype=object)\n  Or thrown away with dtype='datetime64[ns]' \nIn [16]: ser.to_numpy(dtype=\"datetime64[ns]\")\nOut[16]: \narray(['1999-12-31T23:00:00.000000000', '2000-01-01T23:00:00.000000000'],\n      dtype='datetime64[ns]')\n  Getting the \u201craw data\u201d inside a DataFrame is possibly a bit more complex. When your DataFrame only has a single data type for all the columns, DataFrame.to_numpy() will return the underlying data: \nIn [17]: df.to_numpy()\nOut[17]: \narray([[-0.1732,  0.1192, -1.0442],\n       [-0.8618, -2.1046, -0.4949],\n       [ 1.0718,  0.7216, -0.7068],\n       [-1.0396,  0.2719, -0.425 ],\n       [ 0.567 ,  0.2762, -1.0874],\n       [-0.6737,  0.1136, -1.4784],\n       [ 0.525 ,  0.4047,  0.577 ],\n       [-1.715 , -1.0393, -0.3706]])\n  If a DataFrame contains homogeneously-typed data, the ndarray can actually be modified in-place, and the changes will be reflected in the data structure. For heterogeneous data (e.g. some of the DataFrame\u2019s columns are not all the same dtype), this will not be the case. The values attribute itself, unlike the axis labels, cannot be assigned to.  Note When working with heterogeneous data, the dtype of the resulting ndarray will be chosen to accommodate all of the data involved. For example, if strings are involved, the result will be of object dtype. If there are only floats and integers, the resulting array will be of float dtype.  In the past, pandas recommended Series.values or DataFrame.values for extracting the data from a Series or DataFrame. You\u2019ll still find references to these in old code bases and online. Going forward, we recommend avoiding .values and using .array or .to_numpy(). .values has the following drawbacks:  When your Series contains an extension type, it\u2019s unclear whether Series.values returns a NumPy array or the extension array. Series.array will always return an ExtensionArray, and will never copy data. Series.to_numpy() will always return a NumPy array, potentially at the cost of copying / coercing values. When your DataFrame contains a mixture of data types, DataFrame.values may involve copying data and coercing values to a common dtype, a relatively expensive operation. DataFrame.to_numpy(), being a method, makes it clearer that the returned NumPy array may not be a view on the same data in the DataFrame.    Accelerated operations pandas has support for accelerating certain types of binary numerical and boolean operations using the numexpr library and the bottleneck libraries. These libraries are especially useful when dealing with large data sets, and provide large speedups. numexpr uses smart chunking, caching, and multiple cores. bottleneck is a set of specialized cython routines that are especially fast when dealing with arrays that have nans. Here is a sample (using 100 column x 100,000 row DataFrames):         \nOperation 0.11.0 (ms) Prior Version (ms) Ratio to Prior    \ndf1 > df2 13.32 125.35 0.1063  \ndf1 * df2 21.71 36.63 0.5928  \ndf1 + df2 22.04 36.50 0.6039    You are highly encouraged to install both libraries. See the section Recommended Dependencies for more installation info. These are both enabled to be used by default, you can control this by setting the options: \npd.set_option(\"compute.use_bottleneck\", False)\npd.set_option(\"compute.use_numexpr\", False)\n    Flexible binary operations With binary operations between pandas data structures, there are two key points of interest:  Broadcasting behavior between higher- (e.g. DataFrame) and lower-dimensional (e.g. Series) objects. Missing data in computations.  We will demonstrate how to manage these issues independently, though they can be handled simultaneously.  Matching / broadcasting behavior DataFrame has the methods add(), sub(), mul(), div() and related functions radd(), rsub(), \u2026 for carrying out binary operations. For broadcasting behavior, Series input is of primary interest. Using these functions, you can use to either match on the index or columns via the axis keyword: \nIn [18]: df = pd.DataFrame(\n   ....:     {\n   ....:         \"one\": pd.Series(np.random.randn(3), index=[\"a\", \"b\", \"c\"]),\n   ....:         \"two\": pd.Series(np.random.randn(4), index=[\"a\", \"b\", \"c\", \"d\"]),\n   ....:         \"three\": pd.Series(np.random.randn(3), index=[\"b\", \"c\", \"d\"]),\n   ....:     }\n   ....: )\n   ....: \n\nIn [19]: df\nOut[19]: \n        one       two     three\na  1.394981  1.772517       NaN\nb  0.343054  1.912123 -0.050390\nc  0.695246  1.478369  1.227435\nd       NaN  0.279344 -0.613172\n\nIn [20]: row = df.iloc[1]\n\nIn [21]: column = df[\"two\"]\n\nIn [22]: df.sub(row, axis=\"columns\")\nOut[22]: \n        one       two     three\na  1.051928 -0.139606       NaN\nb  0.000000  0.000000  0.000000\nc  0.352192 -0.433754  1.277825\nd       NaN -1.632779 -0.562782\n\nIn [23]: df.sub(row, axis=1)\nOut[23]: \n        one       two     three\na  1.051928 -0.139606       NaN\nb  0.000000  0.000000  0.000000\nc  0.352192 -0.433754  1.277825\nd       NaN -1.632779 -0.562782\n\nIn [24]: df.sub(column, axis=\"index\")\nOut[24]: \n        one  two     three\na -0.377535  0.0       NaN\nb -1.569069  0.0 -1.962513\nc -0.783123  0.0 -0.250933\nd       NaN  0.0 -0.892516\n\nIn [25]: df.sub(column, axis=0)\nOut[25]: \n        one  two     three\na -0.377535  0.0       NaN\nb -1.569069  0.0 -1.962513\nc -0.783123  0.0 -0.250933\nd       NaN  0.0 -0.892516\n  Furthermore you can align a level of a MultiIndexed DataFrame with a Series. \nIn [26]: dfmi = df.copy()\n\nIn [27]: dfmi.index = pd.MultiIndex.from_tuples(\n   ....:     [(1, \"a\"), (1, \"b\"), (1, \"c\"), (2, \"a\")], names=[\"first\", \"second\"]\n   ....: )\n   ....: \n\nIn [28]: dfmi.sub(column, axis=0, level=\"second\")\nOut[28]: \n                   one       two     three\nfirst second                              \n1     a      -0.377535  0.000000       NaN\n      b      -1.569069  0.000000 -1.962513\n      c      -0.783123  0.000000 -0.250933\n2     a            NaN -1.493173 -2.385688\n  Series and Index also support the divmod() builtin. This function takes the floor division and modulo operation at the same time returning a two-tuple of the same type as the left hand side. For example: \nIn [29]: s = pd.Series(np.arange(10))\n\nIn [30]: s\nOut[30]: \n0    0\n1    1\n2    2\n3    3\n4    4\n5    5\n6    6\n7    7\n8    8\n9    9\ndtype: int64\n\nIn [31]: div, rem = divmod(s, 3)\n\nIn [32]: div\nOut[32]: \n0    0\n1    0\n2    0\n3    1\n4    1\n5    1\n6    2\n7    2\n8    2\n9    3\ndtype: int64\n\nIn [33]: rem\nOut[33]: \n0    0\n1    1\n2    2\n3    0\n4    1\n5    2\n6    0\n7    1\n8    2\n9    0\ndtype: int64\n\nIn [34]: idx = pd.Index(np.arange(10))\n\nIn [35]: idx\nOut[35]: Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64')\n\nIn [36]: div, rem = divmod(idx, 3)\n\nIn [37]: div\nOut[37]: Int64Index([0, 0, 0, 1, 1, 1, 2, 2, 2, 3], dtype='int64')\n\nIn [38]: rem\nOut[38]: Int64Index([0, 1, 2, 0, 1, 2, 0, 1, 2, 0], dtype='int64')\n  We can also do elementwise divmod(): \nIn [39]: div, rem = divmod(s, [2, 2, 3, 3, 4, 4, 5, 5, 6, 6])\n\nIn [40]: div\nOut[40]: \n0    0\n1    0\n2    0\n3    1\n4    1\n5    1\n6    1\n7    1\n8    1\n9    1\ndtype: int64\n\nIn [41]: rem\nOut[41]: \n0    0\n1    1\n2    2\n3    0\n4    0\n5    1\n6    1\n7    2\n8    2\n9    3\ndtype: int64\n    Missing data / operations with fill values In Series and DataFrame, the arithmetic functions have the option of inputting a fill_value, namely a value to substitute when at most one of the values at a location are missing. For example, when adding two DataFrame objects, you may wish to treat NaN as 0 unless both DataFrames are missing that value, in which case the result will be NaN (you can later replace NaN with some other value using fillna if you wish). \nIn [42]: df\nOut[42]: \n        one       two     three\na  1.394981  1.772517       NaN\nb  0.343054  1.912123 -0.050390\nc  0.695246  1.478369  1.227435\nd       NaN  0.279344 -0.613172\n\nIn [43]: df2\nOut[43]: \n        one       two     three\na  1.394981  1.772517  1.000000\nb  0.343054  1.912123 -0.050390\nc  0.695246  1.478369  1.227435\nd       NaN  0.279344 -0.613172\n\nIn [44]: df + df2\nOut[44]: \n        one       two     three\na  2.789963  3.545034       NaN\nb  0.686107  3.824246 -0.100780\nc  1.390491  2.956737  2.454870\nd       NaN  0.558688 -1.226343\n\nIn [45]: df.add(df2, fill_value=0)\nOut[45]: \n        one       two     three\na  2.789963  3.545034  1.000000\nb  0.686107  3.824246 -0.100780\nc  1.390491  2.956737  2.454870\nd       NaN  0.558688 -1.226343\n    Flexible comparisons Series and DataFrame have the binary comparison methods eq, ne, lt, gt, le, and ge whose behavior is analogous to the binary arithmetic operations described above: \nIn [46]: df.gt(df2)\nOut[46]: \n     one    two  three\na  False  False  False\nb  False  False  False\nc  False  False  False\nd  False  False  False\n\nIn [47]: df2.ne(df)\nOut[47]: \n     one    two  three\na  False  False   True\nb  False  False  False\nc  False  False  False\nd   True  False  False\n  These operations produce a pandas object of the same type as the left-hand-side input that is of dtype bool. These boolean objects can be used in indexing operations, see the section on Boolean indexing.   Boolean reductions You can apply the reductions: empty, any(), all(), and bool() to provide a way to summarize a boolean result. \nIn [48]: (df > 0).all()\nOut[48]: \none      False\ntwo       True\nthree    False\ndtype: bool\n\nIn [49]: (df > 0).any()\nOut[49]: \none      True\ntwo      True\nthree    True\ndtype: bool\n  You can reduce to a final boolean value. \nIn [50]: (df > 0).any().any()\nOut[50]: True\n  You can test if a pandas object is empty, via the empty property. \nIn [51]: df.empty\nOut[51]: False\n\nIn [52]: pd.DataFrame(columns=list(\"ABC\")).empty\nOut[52]: True\n  To evaluate single-element pandas objects in a boolean context, use the method bool(): \nIn [53]: pd.Series([True]).bool()\nOut[53]: True\n\nIn [54]: pd.Series([False]).bool()\nOut[54]: False\n\nIn [55]: pd.DataFrame([[True]]).bool()\nOut[55]: True\n\nIn [56]: pd.DataFrame([[False]]).bool()\nOut[56]: False\n   Warning You might be tempted to do the following: \n>>> if df:\n...     pass\n  Or \n>>> df and df2\n  These will both raise errors, as you are trying to compare multiple values.: \nValueError: The truth value of an array is ambiguous. Use a.empty, a.any() or a.all().\n   See gotchas for a more detailed discussion.   Comparing if objects are equivalent Often you may find that there is more than one way to compute the same result. As a simple example, consider df + df and df * 2. To test that these two computations produce the same result, given the tools shown above, you might imagine using (df + df == df * 2).all(). But in fact, this expression is False: \nIn [57]: df + df == df * 2\nOut[57]: \n     one   two  three\na   True  True  False\nb   True  True   True\nc   True  True   True\nd  False  True   True\n\nIn [58]: (df + df == df * 2).all()\nOut[58]: \none      False\ntwo       True\nthree    False\ndtype: bool\n  Notice that the boolean DataFrame df + df == df * 2 contains some False values! This is because NaNs do not compare as equals: \nIn [59]: np.nan == np.nan\nOut[59]: False\n  So, NDFrames (such as Series and DataFrames) have an equals() method for testing equality, with NaNs in corresponding locations treated as equal. \nIn [60]: (df + df).equals(df * 2)\nOut[60]: True\n  Note that the Series or DataFrame index needs to be in the same order for equality to be True: \nIn [61]: df1 = pd.DataFrame({\"col\": [\"foo\", 0, np.nan]})\n\nIn [62]: df2 = pd.DataFrame({\"col\": [np.nan, 0, \"foo\"]}, index=[2, 1, 0])\n\nIn [63]: df1.equals(df2)\nOut[63]: False\n\nIn [64]: df1.equals(df2.sort_index())\nOut[64]: True\n    Comparing array-like objects You can conveniently perform element-wise comparisons when comparing a pandas data structure with a scalar value: \nIn [65]: pd.Series([\"foo\", \"bar\", \"baz\"]) == \"foo\"\nOut[65]: \n0     True\n1    False\n2    False\ndtype: bool\n\nIn [66]: pd.Index([\"foo\", \"bar\", \"baz\"]) == \"foo\"\nOut[66]: array([ True, False, False])\n  pandas also handles element-wise comparisons between different array-like objects of the same length: \nIn [67]: pd.Series([\"foo\", \"bar\", \"baz\"]) == pd.Index([\"foo\", \"bar\", \"qux\"])\nOut[67]: \n0     True\n1     True\n2    False\ndtype: bool\n\nIn [68]: pd.Series([\"foo\", \"bar\", \"baz\"]) == np.array([\"foo\", \"bar\", \"qux\"])\nOut[68]: \n0     True\n1     True\n2    False\ndtype: bool\n  Trying to compare Index or Series objects of different lengths will raise a ValueError: \nIn [55]: pd.Series(['foo', 'bar', 'baz']) == pd.Series(['foo', 'bar'])\nValueError: Series lengths must match to compare\n\nIn [56]: pd.Series(['foo', 'bar', 'baz']) == pd.Series(['foo'])\nValueError: Series lengths must match to compare\n  Note that this is different from the NumPy behavior where a comparison can be broadcast: \nIn [69]: np.array([1, 2, 3]) == np.array([2])\nOut[69]: array([False,  True, False])\n  or it can return False if broadcasting can not be done: \nIn [70]: np.array([1, 2, 3]) == np.array([1, 2])\nOut[70]: False\n    Combining overlapping data sets A problem occasionally arising is the combination of two similar data sets where values in one are preferred over the other. An example would be two data series representing a particular economic indicator where one is considered to be of \u201chigher quality\u201d. However, the lower quality series might extend further back in history or have more complete data coverage. As such, we would like to combine two DataFrame objects where missing values in one DataFrame are conditionally filled with like-labeled values from the other DataFrame. The function implementing this operation is combine_first(), which we illustrate: \nIn [71]: df1 = pd.DataFrame(\n   ....:     {\"A\": [1.0, np.nan, 3.0, 5.0, np.nan], \"B\": [np.nan, 2.0, 3.0, np.nan, 6.0]}\n   ....: )\n   ....: \n\nIn [72]: df2 = pd.DataFrame(\n   ....:     {\n   ....:         \"A\": [5.0, 2.0, 4.0, np.nan, 3.0, 7.0],\n   ....:         \"B\": [np.nan, np.nan, 3.0, 4.0, 6.0, 8.0],\n   ....:     }\n   ....: )\n   ....: \n\nIn [73]: df1\nOut[73]: \n     A    B\n0  1.0  NaN\n1  NaN  2.0\n2  3.0  3.0\n3  5.0  NaN\n4  NaN  6.0\n\nIn [74]: df2\nOut[74]: \n     A    B\n0  5.0  NaN\n1  2.0  NaN\n2  4.0  3.0\n3  NaN  4.0\n4  3.0  6.0\n5  7.0  8.0\n\nIn [75]: df1.combine_first(df2)\nOut[75]: \n     A    B\n0  1.0  NaN\n1  2.0  2.0\n2  3.0  3.0\n3  5.0  4.0\n4  3.0  6.0\n5  7.0  8.0\n    General DataFrame combine The combine_first() method above calls the more general DataFrame.combine(). This method takes another DataFrame and a combiner function, aligns the input DataFrame and then passes the combiner function pairs of Series (i.e., columns whose names are the same). So, for instance, to reproduce combine_first() as above: \nIn [76]: def combiner(x, y):\n   ....:     return np.where(pd.isna(x), y, x)\n   ....: \n\nIn [77]: df1.combine(df2, combiner)\nOut[77]: \n     A    B\n0  1.0  NaN\n1  2.0  2.0\n2  3.0  3.0\n3  5.0  4.0\n4  3.0  6.0\n5  7.0  8.0\n     Descriptive statistics There exists a large number of methods for computing descriptive statistics and other related operations on Series, DataFrame. Most of these are aggregations (hence producing a lower-dimensional result) like sum(), mean(), and quantile(), but some of them, like cumsum() and cumprod(), produce an object of the same size. Generally speaking, these methods take an axis argument, just like ndarray.{sum, std, \u2026}, but the axis can be specified by name or integer:  Series: no axis argument needed DataFrame: \u201cindex\u201d (axis=0, default), \u201ccolumns\u201d (axis=1)  For example: \nIn [78]: df\nOut[78]: \n        one       two     three\na  1.394981  1.772517       NaN\nb  0.343054  1.912123 -0.050390\nc  0.695246  1.478369  1.227435\nd       NaN  0.279344 -0.613172\n\nIn [79]: df.mean(0)\nOut[79]: \none      0.811094\ntwo      1.360588\nthree    0.187958\ndtype: float64\n\nIn [80]: df.mean(1)\nOut[80]: \na    1.583749\nb    0.734929\nc    1.133683\nd   -0.166914\ndtype: float64\n  All such methods have a skipna option signaling whether to exclude missing data (True by default): \nIn [81]: df.sum(0, skipna=False)\nOut[81]: \none           NaN\ntwo      5.442353\nthree         NaN\ndtype: float64\n\nIn [82]: df.sum(axis=1, skipna=True)\nOut[82]: \na    3.167498\nb    2.204786\nc    3.401050\nd   -0.333828\ndtype: float64\n  Combined with the broadcasting / arithmetic behavior, one can describe various statistical procedures, like standardization (rendering data zero mean and standard deviation of 1), very concisely: \nIn [83]: ts_stand = (df - df.mean()) / df.std()\n\nIn [84]: ts_stand.std()\nOut[84]: \none      1.0\ntwo      1.0\nthree    1.0\ndtype: float64\n\nIn [85]: xs_stand = df.sub(df.mean(1), axis=0).div(df.std(1), axis=0)\n\nIn [86]: xs_stand.std(1)\nOut[86]: \na    1.0\nb    1.0\nc    1.0\nd    1.0\ndtype: float64\n  Note that methods like cumsum() and cumprod() preserve the location of NaN values. This is somewhat different from expanding() and rolling() since NaN behavior is furthermore dictated by a min_periods parameter. \nIn [87]: df.cumsum()\nOut[87]: \n        one       two     three\na  1.394981  1.772517       NaN\nb  1.738035  3.684640 -0.050390\nc  2.433281  5.163008  1.177045\nd       NaN  5.442353  0.563873\n  Here is a quick reference summary table of common functions. Each also takes an optional level parameter which applies only if the object has a hierarchical index.       \nFunction Description    \ncount Number of non-NA observations  \nsum Sum of values  \nmean Mean of values  \nmad Mean absolute deviation  \nmedian Arithmetic median of values  \nmin Minimum  \nmax Maximum  \nmode Mode  \nabs Absolute Value  \nprod Product of values  \nstd Bessel-corrected sample standard deviation  \nvar Unbiased variance  \nsem Standard error of the mean  \nskew Sample skewness (3rd moment)  \nkurt Sample kurtosis (4th moment)  \nquantile Sample quantile (value at %)  \ncumsum Cumulative sum  \ncumprod Cumulative product  \ncummax Cumulative maximum  \ncummin Cumulative minimum    Note that by chance some NumPy methods, like mean, std, and sum, will exclude NAs on Series input by default: \nIn [88]: np.mean(df[\"one\"])\nOut[88]: 0.8110935116651192\n\nIn [89]: np.mean(df[\"one\"].to_numpy())\nOut[89]: nan\n  Series.nunique() will return the number of unique non-NA values in a Series: \nIn [90]: series = pd.Series(np.random.randn(500))\n\nIn [91]: series[20:500] = np.nan\n\nIn [92]: series[10:20] = 5\n\nIn [93]: series.nunique()\nOut[93]: 11\n   Summarizing data: describe There is a convenient describe() function which computes a variety of summary statistics about a Series or the columns of a DataFrame (excluding NAs of course): \nIn [94]: series = pd.Series(np.random.randn(1000))\n\nIn [95]: series[::2] = np.nan\n\nIn [96]: series.describe()\nOut[96]: \ncount    500.000000\nmean      -0.021292\nstd        1.015906\nmin       -2.683763\n25%       -0.699070\n50%       -0.069718\n75%        0.714483\nmax        3.160915\ndtype: float64\n\nIn [97]: frame = pd.DataFrame(np.random.randn(1000, 5), columns=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n\nIn [98]: frame.iloc[::2] = np.nan\n\nIn [99]: frame.describe()\nOut[99]: \n                a           b           c           d           e\ncount  500.000000  500.000000  500.000000  500.000000  500.000000\nmean     0.033387    0.030045   -0.043719   -0.051686    0.005979\nstd      1.017152    0.978743    1.025270    1.015988    1.006695\nmin     -3.000951   -2.637901   -3.303099   -3.159200   -3.188821\n25%     -0.647623   -0.576449   -0.712369   -0.691338   -0.691115\n50%      0.047578   -0.021499   -0.023888   -0.032652   -0.025363\n75%      0.729907    0.775880    0.618896    0.670047    0.649748\nmax      2.740139    2.752332    3.004229    2.728702    3.240991\n  You can select specific percentiles to include in the output: \nIn [100]: series.describe(percentiles=[0.05, 0.25, 0.75, 0.95])\nOut[100]: \ncount    500.000000\nmean      -0.021292\nstd        1.015906\nmin       -2.683763\n5%        -1.645423\n25%       -0.699070\n50%       -0.069718\n75%        0.714483\n95%        1.711409\nmax        3.160915\ndtype: float64\n  By default, the median is always included. For a non-numerical Series object, describe() will give a simple summary of the number of unique values and most frequently occurring values: \nIn [101]: s = pd.Series([\"a\", \"a\", \"b\", \"b\", \"a\", \"a\", np.nan, \"c\", \"d\", \"a\"])\n\nIn [102]: s.describe()\nOut[102]: \ncount     9\nunique    4\ntop       a\nfreq      5\ndtype: object\n  Note that on a mixed-type DataFrame object, describe() will restrict the summary to include only numerical columns or, if none are, only categorical columns: \nIn [103]: frame = pd.DataFrame({\"a\": [\"Yes\", \"Yes\", \"No\", \"No\"], \"b\": range(4)})\n\nIn [104]: frame.describe()\nOut[104]: \n              b\ncount  4.000000\nmean   1.500000\nstd    1.290994\nmin    0.000000\n25%    0.750000\n50%    1.500000\n75%    2.250000\nmax    3.000000\n  This behavior can be controlled by providing a list of types as include/exclude arguments. The special value all can also be used: \nIn [105]: frame.describe(include=[\"object\"])\nOut[105]: \n          a\ncount     4\nunique    2\ntop     Yes\nfreq      2\n\nIn [106]: frame.describe(include=[\"number\"])\nOut[106]: \n              b\ncount  4.000000\nmean   1.500000\nstd    1.290994\nmin    0.000000\n25%    0.750000\n50%    1.500000\n75%    2.250000\nmax    3.000000\n\nIn [107]: frame.describe(include=\"all\")\nOut[107]: \n          a         b\ncount     4  4.000000\nunique    2       NaN\ntop     Yes       NaN\nfreq      2       NaN\nmean    NaN  1.500000\nstd     NaN  1.290994\nmin     NaN  0.000000\n25%     NaN  0.750000\n50%     NaN  1.500000\n75%     NaN  2.250000\nmax     NaN  3.000000\n  That feature relies on select_dtypes. Refer to there for details about accepted inputs.   Index of min/max values The idxmin() and idxmax() functions on Series and DataFrame compute the index labels with the minimum and maximum corresponding values: \nIn [108]: s1 = pd.Series(np.random.randn(5))\n\nIn [109]: s1\nOut[109]: \n0    1.118076\n1   -0.352051\n2   -1.242883\n3   -1.277155\n4   -0.641184\ndtype: float64\n\nIn [110]: s1.idxmin(), s1.idxmax()\nOut[110]: (3, 0)\n\nIn [111]: df1 = pd.DataFrame(np.random.randn(5, 3), columns=[\"A\", \"B\", \"C\"])\n\nIn [112]: df1\nOut[112]: \n          A         B         C\n0 -0.327863 -0.946180 -0.137570\n1 -0.186235 -0.257213 -0.486567\n2 -0.507027 -0.871259 -0.111110\n3  2.000339 -2.430505  0.089759\n4 -0.321434 -0.033695  0.096271\n\nIn [113]: df1.idxmin(axis=0)\nOut[113]: \nA    2\nB    3\nC    1\ndtype: int64\n\nIn [114]: df1.idxmax(axis=1)\nOut[114]: \n0    C\n1    A\n2    C\n3    A\n4    C\ndtype: object\n  When there are multiple rows (or columns) matching the minimum or maximum value, idxmin() and idxmax() return the first matching index: \nIn [115]: df3 = pd.DataFrame([2, 1, 1, 3, np.nan], columns=[\"A\"], index=list(\"edcba\"))\n\nIn [116]: df3\nOut[116]: \n     A\ne  2.0\nd  1.0\nc  1.0\nb  3.0\na  NaN\n\nIn [117]: df3[\"A\"].idxmin()\nOut[117]: 'd'\n   Note idxmin and idxmax are called argmin and argmax in NumPy.    Value counts (histogramming) / mode The value_counts() Series method and top-level function computes a histogram of a 1D array of values. It can also be used as a function on regular arrays: \nIn [118]: data = np.random.randint(0, 7, size=50)\n\nIn [119]: data\nOut[119]: \narray([6, 6, 2, 3, 5, 3, 2, 5, 4, 5, 4, 3, 4, 5, 0, 2, 0, 4, 2, 0, 3, 2,\n       2, 5, 6, 5, 3, 4, 6, 4, 3, 5, 6, 4, 3, 6, 2, 6, 6, 2, 3, 4, 2, 1,\n       6, 2, 6, 1, 5, 4])\n\nIn [120]: s = pd.Series(data)\n\nIn [121]: s.value_counts()\nOut[121]: \n6    10\n2    10\n4     9\n3     8\n5     8\n0     3\n1     2\ndtype: int64\n\nIn [122]: pd.value_counts(data)\nOut[122]: \n6    10\n2    10\n4     9\n3     8\n5     8\n0     3\n1     2\ndtype: int64\n   New in version 1.1.0.  The value_counts() method can be used to count combinations across multiple columns. By default all columns are used but a subset can be selected using the subset argument. \nIn [123]: data = {\"a\": [1, 2, 3, 4], \"b\": [\"x\", \"x\", \"y\", \"y\"]}\n\nIn [124]: frame = pd.DataFrame(data)\n\nIn [125]: frame.value_counts()\nOut[125]: \na  b\n1  x    1\n2  x    1\n3  y    1\n4  y    1\ndtype: int64\n  Similarly, you can get the most frequently occurring value(s), i.e. the mode, of the values in a Series or DataFrame: \nIn [126]: s5 = pd.Series([1, 1, 3, 3, 3, 5, 5, 7, 7, 7])\n\nIn [127]: s5.mode()\nOut[127]: \n0    3\n1    7\ndtype: int64\n\nIn [128]: df5 = pd.DataFrame(\n   .....:     {\n   .....:         \"A\": np.random.randint(0, 7, size=50),\n   .....:         \"B\": np.random.randint(-10, 15, size=50),\n   .....:     }\n   .....: )\n   .....: \n\nIn [129]: df5.mode()\nOut[129]: \n     A   B\n0  1.0  -9\n1  NaN  10\n2  NaN  13\n    Discretization and quantiling Continuous values can be discretized using the cut() (bins based on values) and qcut() (bins based on sample quantiles) functions: \nIn [130]: arr = np.random.randn(20)\n\nIn [131]: factor = pd.cut(arr, 4)\n\nIn [132]: factor\nOut[132]: \n[(-0.251, 0.464], (-0.968, -0.251], (0.464, 1.179], (-0.251, 0.464], (-0.968, -0.251], ..., (-0.251, 0.464], (-0.968, -0.251], (-0.968, -0.251], (-0.968, -0.251], (-0.968, -0.251]]\nLength: 20\nCategories (4, interval[float64, right]): [(-0.968, -0.251] < (-0.251, 0.464] < (0.464, 1.179] <\n                                           (1.179, 1.893]]\n\nIn [133]: factor = pd.cut(arr, [-5, -1, 0, 1, 5])\n\nIn [134]: factor\nOut[134]: \n[(0, 1], (-1, 0], (0, 1], (0, 1], (-1, 0], ..., (-1, 0], (-1, 0], (-1, 0], (-1, 0], (-1, 0]]\nLength: 20\nCategories (4, interval[int64, right]): [(-5, -1] < (-1, 0] < (0, 1] < (1, 5]]\n  qcut() computes sample quantiles. For example, we could slice up some normally distributed data into equal-size quartiles like so: \nIn [135]: arr = np.random.randn(30)\n\nIn [136]: factor = pd.qcut(arr, [0, 0.25, 0.5, 0.75, 1])\n\nIn [137]: factor\nOut[137]: \n[(0.569, 1.184], (-2.278, -0.301], (-2.278, -0.301], (0.569, 1.184], (0.569, 1.184], ..., (-0.301, 0.569], (1.184, 2.346], (1.184, 2.346], (-0.301, 0.569], (-2.278, -0.301]]\nLength: 30\nCategories (4, interval[float64, right]): [(-2.278, -0.301] < (-0.301, 0.569] < (0.569, 1.184] <\n                                           (1.184, 2.346]]\n\nIn [138]: pd.value_counts(factor)\nOut[138]: \n(-2.278, -0.301]    8\n(1.184, 2.346]      8\n(-0.301, 0.569]     7\n(0.569, 1.184]      7\ndtype: int64\n  We can also pass infinite values to define the bins: \nIn [139]: arr = np.random.randn(20)\n\nIn [140]: factor = pd.cut(arr, [-np.inf, 0, np.inf])\n\nIn [141]: factor\nOut[141]: \n[(-inf, 0.0], (0.0, inf], (0.0, inf], (-inf, 0.0], (-inf, 0.0], ..., (-inf, 0.0], (-inf, 0.0], (-inf, 0.0], (0.0, inf], (0.0, inf]]\nLength: 20\nCategories (2, interval[float64, right]): [(-inf, 0.0] < (0.0, inf]]\n     Function application To apply your own or another library\u2019s functions to pandas objects, you should be aware of the three methods below. The appropriate method to use depends on whether your function expects to operate on an entire DataFrame or Series, row- or column-wise, or elementwise.  Tablewise Function Application: pipe() Row or Column-wise Function Application: apply() Aggregation API: agg() and transform() Applying Elementwise Functions: applymap()   Tablewise function application DataFrames and Series can be passed into functions. However, if the function needs to be called in a chain, consider using the pipe() method. First some setup: \nIn [142]: def extract_city_name(df):\n   .....:     \"\"\"\n   .....:     Chicago, IL -> Chicago for city_name column\n   .....:     \"\"\"\n   .....:     df[\"city_name\"] = df[\"city_and_code\"].str.split(\",\").str.get(0)\n   .....:     return df\n   .....: \n\nIn [143]: def add_country_name(df, country_name=None):\n   .....:     \"\"\"\n   .....:     Chicago -> Chicago-US for city_name column\n   .....:     \"\"\"\n   .....:     col = \"city_name\"\n   .....:     df[\"city_and_country\"] = df[col] + country_name\n   .....:     return df\n   .....: \n\nIn [144]: df_p = pd.DataFrame({\"city_and_code\": [\"Chicago, IL\"]})\n  extract_city_name and add_country_name are functions taking and returning DataFrames. Now compare the following: \nIn [145]: add_country_name(extract_city_name(df_p), country_name=\"US\")\nOut[145]: \n  city_and_code city_name city_and_country\n0   Chicago, IL   Chicago        ChicagoUS\n  Is equivalent to: \nIn [146]: df_p.pipe(extract_city_name).pipe(add_country_name, country_name=\"US\")\nOut[146]: \n  city_and_code city_name city_and_country\n0   Chicago, IL   Chicago        ChicagoUS\n  pandas encourages the second style, which is known as method chaining. pipe makes it easy to use your own or another library\u2019s functions in method chains, alongside pandas\u2019 methods. In the example above, the functions extract_city_name and add_country_name each expected a DataFrame as the first positional argument. What if the function you wish to apply takes its data as, say, the second argument? In this case, provide pipe with a tuple of (callable, data_keyword). .pipe will route the DataFrame to the argument specified in the tuple. For example, we can fit a regression using statsmodels. Their API expects a formula first and a DataFrame as the second argument, data. We pass in the function, keyword pair (sm.ols, 'data') to pipe: \nIn [147]: import statsmodels.formula.api as sm\n\nIn [148]: bb = pd.read_csv(\"data/baseball.csv\", index_col=\"id\")\n\nIn [149]: (\n   .....:     bb.query(\"h > 0\")\n   .....:     .assign(ln_h=lambda df: np.log(df.h))\n   .....:     .pipe((sm.ols, \"data\"), \"hr ~ ln_h + year + g + C(lg)\")\n   .....:     .fit()\n   .....:     .summary()\n   .....: )\n   .....: \nOut[149]: \n<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                     hr   R-squared:                       0.685\nModel:                            OLS   Adj. R-squared:                  0.665\nMethod:                 Least Squares   F-statistic:                     34.28\nDate:                Sat, 22 Jan 2022   Prob (F-statistic):           3.48e-15\nTime:                        10:50:02   Log-Likelihood:                -205.92\nNo. Observations:                  68   AIC:                             421.8\nDf Residuals:                      63   BIC:                             432.9\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept   -8484.7720   4664.146     -1.819      0.074   -1.78e+04     835.780\nC(lg)[T.NL]    -2.2736      1.325     -1.716      0.091      -4.922       0.375\nln_h           -1.3542      0.875     -1.547      0.127      -3.103       0.395\nyear            4.2277      2.324      1.819      0.074      -0.417       8.872\ng               0.1841      0.029      6.258      0.000       0.125       0.243\n==============================================================================\nOmnibus:                       10.875   Durbin-Watson:                   1.999\nProb(Omnibus):                  0.004   Jarque-Bera (JB):               17.298\nSkew:                           0.537   Prob(JB):                     0.000175\nKurtosis:                       5.225   Cond. No.                     1.49e+07\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.49e+07. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\"\n  The pipe method is inspired by unix pipes and more recently dplyr and magrittr, which have introduced the popular (%>%) (read pipe) operator for R. The implementation of pipe here is quite clean and feels right at home in Python. We encourage you to view the source code of pipe().   Row or column-wise function application Arbitrary functions can be applied along the axes of a DataFrame using the apply() method, which, like the descriptive statistics methods, takes an optional axis argument: \nIn [150]: df.apply(np.mean)\nOut[150]: \none      0.811094\ntwo      1.360588\nthree    0.187958\ndtype: float64\n\nIn [151]: df.apply(np.mean, axis=1)\nOut[151]: \na    1.583749\nb    0.734929\nc    1.133683\nd   -0.166914\ndtype: float64\n\nIn [152]: df.apply(lambda x: x.max() - x.min())\nOut[152]: \none      1.051928\ntwo      1.632779\nthree    1.840607\ndtype: float64\n\nIn [153]: df.apply(np.cumsum)\nOut[153]: \n        one       two     three\na  1.394981  1.772517       NaN\nb  1.738035  3.684640 -0.050390\nc  2.433281  5.163008  1.177045\nd       NaN  5.442353  0.563873\n\nIn [154]: df.apply(np.exp)\nOut[154]: \n        one       two     three\na  4.034899  5.885648       NaN\nb  1.409244  6.767440  0.950858\nc  2.004201  4.385785  3.412466\nd       NaN  1.322262  0.541630\n  The apply() method will also dispatch on a string method name. \nIn [155]: df.apply(\"mean\")\nOut[155]: \none      0.811094\ntwo      1.360588\nthree    0.187958\ndtype: float64\n\nIn [156]: df.apply(\"mean\", axis=1)\nOut[156]: \na    1.583749\nb    0.734929\nc    1.133683\nd   -0.166914\ndtype: float64\n  The return type of the function passed to apply() affects the type of the final output from DataFrame.apply for the default behaviour:  If the applied function returns a Series, the final output is a DataFrame. The columns match the index of the Series returned by the applied function. If the applied function returns any other type, the final output is a Series.  This default behaviour can be overridden using the result_type, which accepts three options: reduce, broadcast, and expand. These will determine how list-likes return values expand (or not) to a DataFrame. apply() combined with some cleverness can be used to answer many questions about a data set. For example, suppose we wanted to extract the date where the maximum value for each column occurred: \nIn [157]: tsdf = pd.DataFrame(\n   .....:     np.random.randn(1000, 3),\n   .....:     columns=[\"A\", \"B\", \"C\"],\n   .....:     index=pd.date_range(\"1/1/2000\", periods=1000),\n   .....: )\n   .....: \n\nIn [158]: tsdf.apply(lambda x: x.idxmax())\nOut[158]: \nA   2000-08-06\nB   2001-01-18\nC   2001-07-18\ndtype: datetime64[ns]\n  You may also pass additional arguments and keyword arguments to the apply() method. For instance, consider the following function you would like to apply: \ndef subtract_and_divide(x, sub, divide=1):\n    return (x - sub) / divide\n  You may then apply this function as follows: \ndf.apply(subtract_and_divide, args=(5,), divide=3)\n  Another useful feature is the ability to pass Series methods to carry out some Series operation on each column or row: \nIn [159]: tsdf\nOut[159]: \n                   A         B         C\n2000-01-01 -0.158131 -0.232466  0.321604\n2000-01-02 -1.810340 -3.105758  0.433834\n2000-01-03 -1.209847 -1.156793 -0.136794\n2000-01-04       NaN       NaN       NaN\n2000-01-05       NaN       NaN       NaN\n2000-01-06       NaN       NaN       NaN\n2000-01-07       NaN       NaN       NaN\n2000-01-08 -0.653602  0.178875  1.008298\n2000-01-09  1.007996  0.462824  0.254472\n2000-01-10  0.307473  0.600337  1.643950\n\nIn [160]: tsdf.apply(pd.Series.interpolate)\nOut[160]: \n                   A         B         C\n2000-01-01 -0.158131 -0.232466  0.321604\n2000-01-02 -1.810340 -3.105758  0.433834\n2000-01-03 -1.209847 -1.156793 -0.136794\n2000-01-04 -1.098598 -0.889659  0.092225\n2000-01-05 -0.987349 -0.622526  0.321243\n2000-01-06 -0.876100 -0.355392  0.550262\n2000-01-07 -0.764851 -0.088259  0.779280\n2000-01-08 -0.653602  0.178875  1.008298\n2000-01-09  1.007996  0.462824  0.254472\n2000-01-10  0.307473  0.600337  1.643950\n  Finally, apply() takes an argument raw which is False by default, which converts each row or column into a Series before applying the function. When set to True, the passed function will instead receive an ndarray object, which has positive performance implications if you do not need the indexing functionality.   Aggregation API The aggregation API allows one to express possibly multiple aggregation operations in a single concise way. This API is similar across pandas objects, see groupby API, the window API, and the resample API. The entry point for aggregation is DataFrame.aggregate(), or the alias DataFrame.agg(). We will use a similar starting frame from above: \nIn [161]: tsdf = pd.DataFrame(\n   .....:     np.random.randn(10, 3),\n   .....:     columns=[\"A\", \"B\", \"C\"],\n   .....:     index=pd.date_range(\"1/1/2000\", periods=10),\n   .....: )\n   .....: \n\nIn [162]: tsdf.iloc[3:7] = np.nan\n\nIn [163]: tsdf\nOut[163]: \n                   A         B         C\n2000-01-01  1.257606  1.004194  0.167574\n2000-01-02 -0.749892  0.288112 -0.757304\n2000-01-03 -0.207550 -0.298599  0.116018\n2000-01-04       NaN       NaN       NaN\n2000-01-05       NaN       NaN       NaN\n2000-01-06       NaN       NaN       NaN\n2000-01-07       NaN       NaN       NaN\n2000-01-08  0.814347 -0.257623  0.869226\n2000-01-09 -0.250663 -1.206601  0.896839\n2000-01-10  2.169758 -1.333363  0.283157\n  Using a single function is equivalent to apply(). You can also pass named methods as strings. These will return a Series of the aggregated output: \nIn [164]: tsdf.agg(np.sum)\nOut[164]: \nA    3.033606\nB   -1.803879\nC    1.575510\ndtype: float64\n\nIn [165]: tsdf.agg(\"sum\")\nOut[165]: \nA    3.033606\nB   -1.803879\nC    1.575510\ndtype: float64\n\n# these are equivalent to a ``.sum()`` because we are aggregating\n# on a single function\nIn [166]: tsdf.sum()\nOut[166]: \nA    3.033606\nB   -1.803879\nC    1.575510\ndtype: float64\n  Single aggregations on a Series this will return a scalar value: \nIn [167]: tsdf[\"A\"].agg(\"sum\")\nOut[167]: 3.033606102414146\n   Aggregating with multiple functions You can pass multiple aggregation arguments as a list. The results of each of the passed functions will be a row in the resulting DataFrame. These are naturally named from the aggregation function. \nIn [168]: tsdf.agg([\"sum\"])\nOut[168]: \n            A         B        C\nsum  3.033606 -1.803879  1.57551\n  Multiple functions yield multiple rows: \nIn [169]: tsdf.agg([\"sum\", \"mean\"])\nOut[169]: \n             A         B         C\nsum   3.033606 -1.803879  1.575510\nmean  0.505601 -0.300647  0.262585\n  On a Series, multiple functions return a Series, indexed by the function names: \nIn [170]: tsdf[\"A\"].agg([\"sum\", \"mean\"])\nOut[170]: \nsum     3.033606\nmean    0.505601\nName: A, dtype: float64\n  Passing a lambda function will yield a <lambda> named row: \nIn [171]: tsdf[\"A\"].agg([\"sum\", lambda x: x.mean()])\nOut[171]: \nsum         3.033606\n<lambda>    0.505601\nName: A, dtype: float64\n  Passing a named function will yield that name for the row: \nIn [172]: def mymean(x):\n   .....:     return x.mean()\n   .....: \n\nIn [173]: tsdf[\"A\"].agg([\"sum\", mymean])\nOut[173]: \nsum       3.033606\nmymean    0.505601\nName: A, dtype: float64\n    Aggregating with a dict Passing a dictionary of column names to a scalar or a list of scalars, to DataFrame.agg allows you to customize which functions are applied to which columns. Note that the results are not in any particular order, you can use an OrderedDict instead to guarantee ordering. \nIn [174]: tsdf.agg({\"A\": \"mean\", \"B\": \"sum\"})\nOut[174]: \nA    0.505601\nB   -1.803879\ndtype: float64\n  Passing a list-like will generate a DataFrame output. You will get a matrix-like output of all of the aggregators. The output will consist of all unique functions. Those that are not noted for a particular column will be NaN: \nIn [175]: tsdf.agg({\"A\": [\"mean\", \"min\"], \"B\": \"sum\"})\nOut[175]: \n             A         B\nmean  0.505601       NaN\nmin  -0.749892       NaN\nsum        NaN -1.803879\n    Mixed dtypes  Deprecated since version 1.4.0: Attempting to determine which columns cannot be aggregated and silently dropping them from the results is deprecated and will be removed in a future version. If any porition of the columns or operations provided fail, the call to .agg will raise.  When presented with mixed dtypes that cannot aggregate, .agg will only take the valid aggregations. This is similar to how .groupby.agg works. \nIn [176]: mdf = pd.DataFrame(\n   .....:     {\n   .....:         \"A\": [1, 2, 3],\n   .....:         \"B\": [1.0, 2.0, 3.0],\n   .....:         \"C\": [\"foo\", \"bar\", \"baz\"],\n   .....:         \"D\": pd.date_range(\"20130101\", periods=3),\n   .....:     }\n   .....: )\n   .....: \n\nIn [177]: mdf.dtypes\nOut[177]: \nA             int64\nB           float64\nC            object\nD    datetime64[ns]\ndtype: object\n  \nIn [178]: mdf.agg([\"min\", \"sum\"])\nOut[178]: \n     A    B          C          D\nmin  1  1.0        bar 2013-01-01\nsum  6  6.0  foobarbaz        NaT\n    Custom describe With .agg() it is possible to easily create a custom describe function, similar to the built in describe function. \nIn [179]: from functools import partial\n\nIn [180]: q_25 = partial(pd.Series.quantile, q=0.25)\n\nIn [181]: q_25.__name__ = \"25%\"\n\nIn [182]: q_75 = partial(pd.Series.quantile, q=0.75)\n\nIn [183]: q_75.__name__ = \"75%\"\n\nIn [184]: tsdf.agg([\"count\", \"mean\", \"std\", \"min\", q_25, \"median\", q_75, \"max\"])\nOut[184]: \n               A         B         C\ncount   6.000000  6.000000  6.000000\nmean    0.505601 -0.300647  0.262585\nstd     1.103362  0.887508  0.606860\nmin    -0.749892 -1.333363 -0.757304\n25%    -0.239885 -0.979600  0.128907\nmedian  0.303398 -0.278111  0.225365\n75%     1.146791  0.151678  0.722709\nmax     2.169758  1.004194  0.896839\n     Transform API The transform() method returns an object that is indexed the same (same size) as the original. This API allows you to provide multiple operations at the same time rather than one-by-one. Its API is quite similar to the .agg API. We create a frame similar to the one used in the above sections. \nIn [185]: tsdf = pd.DataFrame(\n   .....:     np.random.randn(10, 3),\n   .....:     columns=[\"A\", \"B\", \"C\"],\n   .....:     index=pd.date_range(\"1/1/2000\", periods=10),\n   .....: )\n   .....: \n\nIn [186]: tsdf.iloc[3:7] = np.nan\n\nIn [187]: tsdf\nOut[187]: \n                   A         B         C\n2000-01-01 -0.428759 -0.864890 -0.675341\n2000-01-02 -0.168731  1.338144 -1.279321\n2000-01-03 -1.621034  0.438107  0.903794\n2000-01-04       NaN       NaN       NaN\n2000-01-05       NaN       NaN       NaN\n2000-01-06       NaN       NaN       NaN\n2000-01-07       NaN       NaN       NaN\n2000-01-08  0.254374 -1.240447 -0.201052\n2000-01-09 -0.157795  0.791197 -1.144209\n2000-01-10 -0.030876  0.371900  0.061932\n  Transform the entire frame. .transform() allows input functions as: a NumPy function, a string function name or a user defined function. \nIn [188]: tsdf.transform(np.abs)\nOut[188]: \n                   A         B         C\n2000-01-01  0.428759  0.864890  0.675341\n2000-01-02  0.168731  1.338144  1.279321\n2000-01-03  1.621034  0.438107  0.903794\n2000-01-04       NaN       NaN       NaN\n2000-01-05       NaN       NaN       NaN\n2000-01-06       NaN       NaN       NaN\n2000-01-07       NaN       NaN       NaN\n2000-01-08  0.254374  1.240447  0.201052\n2000-01-09  0.157795  0.791197  1.144209\n2000-01-10  0.030876  0.371900  0.061932\n\nIn [189]: tsdf.transform(\"abs\")\nOut[189]: \n                   A         B         C\n2000-01-01  0.428759  0.864890  0.675341\n2000-01-02  0.168731  1.338144  1.279321\n2000-01-03  1.621034  0.438107  0.903794\n2000-01-04       NaN       NaN       NaN\n2000-01-05       NaN       NaN       NaN\n2000-01-06       NaN       NaN       NaN\n2000-01-07       NaN       NaN       NaN\n2000-01-08  0.254374  1.240447  0.201052\n2000-01-09  0.157795  0.791197  1.144209\n2000-01-10  0.030876  0.371900  0.061932\n\nIn [190]: tsdf.transform(lambda x: x.abs())\nOut[190]: \n                   A         B         C\n2000-01-01  0.428759  0.864890  0.675341\n2000-01-02  0.168731  1.338144  1.279321\n2000-01-03  1.621034  0.438107  0.903794\n2000-01-04       NaN       NaN       NaN\n2000-01-05       NaN       NaN       NaN\n2000-01-06       NaN       NaN       NaN\n2000-01-07       NaN       NaN       NaN\n2000-01-08  0.254374  1.240447  0.201052\n2000-01-09  0.157795  0.791197  1.144209\n2000-01-10  0.030876  0.371900  0.061932\n  Here transform() received a single function; this is equivalent to a ufunc application. \nIn [191]: np.abs(tsdf)\nOut[191]: \n                   A         B         C\n2000-01-01  0.428759  0.864890  0.675341\n2000-01-02  0.168731  1.338144  1.279321\n2000-01-03  1.621034  0.438107  0.903794\n2000-01-04       NaN       NaN       NaN\n2000-01-05       NaN       NaN       NaN\n2000-01-06       NaN       NaN       NaN\n2000-01-07       NaN       NaN       NaN\n2000-01-08  0.254374  1.240447  0.201052\n2000-01-09  0.157795  0.791197  1.144209\n2000-01-10  0.030876  0.371900  0.061932\n  Passing a single function to .transform() with a Series will yield a single Series in return. \nIn [192]: tsdf[\"A\"].transform(np.abs)\nOut[192]: \n2000-01-01    0.428759\n2000-01-02    0.168731\n2000-01-03    1.621034\n2000-01-04         NaN\n2000-01-05         NaN\n2000-01-06         NaN\n2000-01-07         NaN\n2000-01-08    0.254374\n2000-01-09    0.157795\n2000-01-10    0.030876\nFreq: D, Name: A, dtype: float64\n   Transform with multiple functions Passing multiple functions will yield a column MultiIndexed DataFrame. The first level will be the original frame column names; the second level will be the names of the transforming functions. \nIn [193]: tsdf.transform([np.abs, lambda x: x + 1])\nOut[193]: \n                   A                   B                   C          \n            absolute  <lambda>  absolute  <lambda>  absolute  <lambda>\n2000-01-01  0.428759  0.571241  0.864890  0.135110  0.675341  0.324659\n2000-01-02  0.168731  0.831269  1.338144  2.338144  1.279321 -0.279321\n2000-01-03  1.621034 -0.621034  0.438107  1.438107  0.903794  1.903794\n2000-01-04       NaN       NaN       NaN       NaN       NaN       NaN\n2000-01-05       NaN       NaN       NaN       NaN       NaN       NaN\n2000-01-06       NaN       NaN       NaN       NaN       NaN       NaN\n2000-01-07       NaN       NaN       NaN       NaN       NaN       NaN\n2000-01-08  0.254374  1.254374  1.240447 -0.240447  0.201052  0.798948\n2000-01-09  0.157795  0.842205  0.791197  1.791197  1.144209 -0.144209\n2000-01-10  0.030876  0.969124  0.371900  1.371900  0.061932  1.061932\n  Passing multiple functions to a Series will yield a DataFrame. The resulting column names will be the transforming functions. \nIn [194]: tsdf[\"A\"].transform([np.abs, lambda x: x + 1])\nOut[194]: \n            absolute  <lambda>\n2000-01-01  0.428759  0.571241\n2000-01-02  0.168731  0.831269\n2000-01-03  1.621034 -0.621034\n2000-01-04       NaN       NaN\n2000-01-05       NaN       NaN\n2000-01-06       NaN       NaN\n2000-01-07       NaN       NaN\n2000-01-08  0.254374  1.254374\n2000-01-09  0.157795  0.842205\n2000-01-10  0.030876  0.969124\n    Transforming with a dict Passing a dict of functions will allow selective transforming per column. \nIn [195]: tsdf.transform({\"A\": np.abs, \"B\": lambda x: x + 1})\nOut[195]: \n                   A         B\n2000-01-01  0.428759  0.135110\n2000-01-02  0.168731  2.338144\n2000-01-03  1.621034  1.438107\n2000-01-04       NaN       NaN\n2000-01-05       NaN       NaN\n2000-01-06       NaN       NaN\n2000-01-07       NaN       NaN\n2000-01-08  0.254374 -0.240447\n2000-01-09  0.157795  1.791197\n2000-01-10  0.030876  1.371900\n  Passing a dict of lists will generate a MultiIndexed DataFrame with these selective transforms. \nIn [196]: tsdf.transform({\"A\": np.abs, \"B\": [lambda x: x + 1, \"sqrt\"]})\nOut[196]: \n                   A         B          \n            absolute  <lambda>      sqrt\n2000-01-01  0.428759  0.135110       NaN\n2000-01-02  0.168731  2.338144  1.156782\n2000-01-03  1.621034  1.438107  0.661897\n2000-01-04       NaN       NaN       NaN\n2000-01-05       NaN       NaN       NaN\n2000-01-06       NaN       NaN       NaN\n2000-01-07       NaN       NaN       NaN\n2000-01-08  0.254374 -0.240447       NaN\n2000-01-09  0.157795  1.791197  0.889493\n2000-01-10  0.030876  1.371900  0.609836\n     Applying elementwise functions Since not all functions can be vectorized (accept NumPy arrays and return another array or value), the methods applymap() on DataFrame and analogously map() on Series accept any Python function taking a single value and returning a single value. For example: \nIn [197]: df4\nOut[197]: \n        one       two     three\na  1.394981  1.772517       NaN\nb  0.343054  1.912123 -0.050390\nc  0.695246  1.478369  1.227435\nd       NaN  0.279344 -0.613172\n\nIn [198]: def f(x):\n   .....:     return len(str(x))\n   .....: \n\nIn [199]: df4[\"one\"].map(f)\nOut[199]: \na    18\nb    19\nc    18\nd     3\nName: one, dtype: int64\n\nIn [200]: df4.applymap(f)\nOut[200]: \n   one  two  three\na   18   17      3\nb   19   18     20\nc   18   18     16\nd    3   19     19\n  Series.map() has an additional feature; it can be used to easily \u201clink\u201d or \u201cmap\u201d values defined by a secondary series. This is closely related to merging/joining functionality: \nIn [201]: s = pd.Series(\n   .....:     [\"six\", \"seven\", \"six\", \"seven\", \"six\"], index=[\"a\", \"b\", \"c\", \"d\", \"e\"]\n   .....: )\n   .....: \n\nIn [202]: t = pd.Series({\"six\": 6.0, \"seven\": 7.0})\n\nIn [203]: s\nOut[203]: \na      six\nb    seven\nc      six\nd    seven\ne      six\ndtype: object\n\nIn [204]: s.map(t)\nOut[204]: \na    6.0\nb    7.0\nc    6.0\nd    7.0\ne    6.0\ndtype: float64\n     Reindexing and altering labels reindex() is the fundamental data alignment method in pandas. It is used to implement nearly all other features relying on label-alignment functionality. To reindex means to conform the data to match a given set of labels along a particular axis. This accomplishes several things:  Reorders the existing data to match a new set of labels Inserts missing value (NA) markers in label locations where no data for that label existed If specified, fill data for missing labels using logic (highly relevant to working with time series data)  Here is a simple example: \nIn [205]: s = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n\nIn [206]: s\nOut[206]: \na    1.695148\nb    1.328614\nc    1.234686\nd   -0.385845\ne   -1.326508\ndtype: float64\n\nIn [207]: s.reindex([\"e\", \"b\", \"f\", \"d\"])\nOut[207]: \ne   -1.326508\nb    1.328614\nf         NaN\nd   -0.385845\ndtype: float64\n  Here, the f label was not contained in the Series and hence appears as NaN in the result. With a DataFrame, you can simultaneously reindex the index and columns: \nIn [208]: df\nOut[208]: \n        one       two     three\na  1.394981  1.772517       NaN\nb  0.343054  1.912123 -0.050390\nc  0.695246  1.478369  1.227435\nd       NaN  0.279344 -0.613172\n\nIn [209]: df.reindex(index=[\"c\", \"f\", \"b\"], columns=[\"three\", \"two\", \"one\"])\nOut[209]: \n      three       two       one\nc  1.227435  1.478369  0.695246\nf       NaN       NaN       NaN\nb -0.050390  1.912123  0.343054\n  You may also use reindex with an axis keyword: \nIn [210]: df.reindex([\"c\", \"f\", \"b\"], axis=\"index\")\nOut[210]: \n        one       two     three\nc  0.695246  1.478369  1.227435\nf       NaN       NaN       NaN\nb  0.343054  1.912123 -0.050390\n  Note that the Index objects containing the actual axis labels can be shared between objects. So if we have a Series and a DataFrame, the following can be done: \nIn [211]: rs = s.reindex(df.index)\n\nIn [212]: rs\nOut[212]: \na    1.695148\nb    1.328614\nc    1.234686\nd   -0.385845\ndtype: float64\n\nIn [213]: rs.index is df.index\nOut[213]: True\n  This means that the reindexed Series\u2019s index is the same Python object as the DataFrame\u2019s index. DataFrame.reindex() also supports an \u201caxis-style\u201d calling convention, where you specify a single labels argument and the axis it applies to. \nIn [214]: df.reindex([\"c\", \"f\", \"b\"], axis=\"index\")\nOut[214]: \n        one       two     three\nc  0.695246  1.478369  1.227435\nf       NaN       NaN       NaN\nb  0.343054  1.912123 -0.050390\n\nIn [215]: df.reindex([\"three\", \"two\", \"one\"], axis=\"columns\")\nOut[215]: \n      three       two       one\na       NaN  1.772517  1.394981\nb -0.050390  1.912123  0.343054\nc  1.227435  1.478369  0.695246\nd -0.613172  0.279344       NaN\n   See also MultiIndex / Advanced Indexing is an even more concise way of doing reindexing.   Note When writing performance-sensitive code, there is a good reason to spend some time becoming a reindexing ninja: many operations are faster on pre-aligned data. Adding two unaligned DataFrames internally triggers a reindexing step. For exploratory analysis you will hardly notice the difference (because reindex has been heavily optimized), but when CPU cycles matter sprinkling a few explicit reindex calls here and there can have an impact.   Reindexing to align with another object You may wish to take an object and reindex its axes to be labeled the same as another object. While the syntax for this is straightforward albeit verbose, it is a common enough operation that the reindex_like() method is available to make this simpler: \nIn [216]: df2\nOut[216]: \n        one       two\na  1.394981  1.772517\nb  0.343054  1.912123\nc  0.695246  1.478369\n\nIn [217]: df3\nOut[217]: \n        one       two\na  0.583888  0.051514\nb -0.468040  0.191120\nc -0.115848 -0.242634\n\nIn [218]: df.reindex_like(df2)\nOut[218]: \n        one       two\na  1.394981  1.772517\nb  0.343054  1.912123\nc  0.695246  1.478369\n    Aligning objects with each other with align\n The align() method is the fastest way to simultaneously align two objects. It supports a join argument (related to joining and merging):  \n join='outer': take the union of the indexes (default) join='left': use the calling object\u2019s index join='right': use the passed object\u2019s index join='inner': intersect the indexes  \n It returns a tuple with both of the reindexed Series: \nIn [219]: s = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n\nIn [220]: s1 = s[:4]\n\nIn [221]: s2 = s[1:]\n\nIn [222]: s1.align(s2)\nOut[222]: \n(a   -0.186646\n b   -1.692424\n c   -0.303893\n d   -1.425662\n e         NaN\n dtype: float64,\n a         NaN\n b   -1.692424\n c   -0.303893\n d   -1.425662\n e    1.114285\n dtype: float64)\n\nIn [223]: s1.align(s2, join=\"inner\")\nOut[223]: \n(b   -1.692424\n c   -0.303893\n d   -1.425662\n dtype: float64,\n b   -1.692424\n c   -0.303893\n d   -1.425662\n dtype: float64)\n\nIn [224]: s1.align(s2, join=\"left\")\nOut[224]: \n(a   -0.186646\n b   -1.692424\n c   -0.303893\n d   -1.425662\n dtype: float64,\n a         NaN\n b   -1.692424\n c   -0.303893\n d   -1.425662\n dtype: float64)\n  For DataFrames, the join method will be applied to both the index and the columns by default: \nIn [225]: df.align(df2, join=\"inner\")\nOut[225]: \n(        one       two\n a  1.394981  1.772517\n b  0.343054  1.912123\n c  0.695246  1.478369,\n         one       two\n a  1.394981  1.772517\n b  0.343054  1.912123\n c  0.695246  1.478369)\n  You can also pass an axis option to only align on the specified axis: \nIn [226]: df.align(df2, join=\"inner\", axis=0)\nOut[226]: \n(        one       two     three\n a  1.394981  1.772517       NaN\n b  0.343054  1.912123 -0.050390\n c  0.695246  1.478369  1.227435,\n         one       two\n a  1.394981  1.772517\n b  0.343054  1.912123\n c  0.695246  1.478369)\n  If you pass a Series to DataFrame.align(), you can choose to align both objects either on the DataFrame\u2019s index or columns using the axis argument: \nIn [227]: df.align(df2.iloc[0], axis=1)\nOut[227]: \n(        one     three       two\n a  1.394981       NaN  1.772517\n b  0.343054 -0.050390  1.912123\n c  0.695246  1.227435  1.478369\n d       NaN -0.613172  0.279344,\n one      1.394981\n three         NaN\n two      1.772517\n Name: a, dtype: float64)\n    Filling while reindexing reindex() takes an optional parameter method which is a filling method chosen from the following table:       \nMethod Action    \npad / ffill Fill values forward  \nbfill / backfill Fill values backward  \nnearest Fill from the nearest index value    We illustrate these fill methods on a simple Series: \nIn [228]: rng = pd.date_range(\"1/3/2000\", periods=8)\n\nIn [229]: ts = pd.Series(np.random.randn(8), index=rng)\n\nIn [230]: ts2 = ts[[0, 3, 6]]\n\nIn [231]: ts\nOut[231]: \n2000-01-03    0.183051\n2000-01-04    0.400528\n2000-01-05   -0.015083\n2000-01-06    2.395489\n2000-01-07    1.414806\n2000-01-08    0.118428\n2000-01-09    0.733639\n2000-01-10   -0.936077\nFreq: D, dtype: float64\n\nIn [232]: ts2\nOut[232]: \n2000-01-03    0.183051\n2000-01-06    2.395489\n2000-01-09    0.733639\nFreq: 3D, dtype: float64\n\nIn [233]: ts2.reindex(ts.index)\nOut[233]: \n2000-01-03    0.183051\n2000-01-04         NaN\n2000-01-05         NaN\n2000-01-06    2.395489\n2000-01-07         NaN\n2000-01-08         NaN\n2000-01-09    0.733639\n2000-01-10         NaN\nFreq: D, dtype: float64\n\nIn [234]: ts2.reindex(ts.index, method=\"ffill\")\nOut[234]: \n2000-01-03    0.183051\n2000-01-04    0.183051\n2000-01-05    0.183051\n2000-01-06    2.395489\n2000-01-07    2.395489\n2000-01-08    2.395489\n2000-01-09    0.733639\n2000-01-10    0.733639\nFreq: D, dtype: float64\n\nIn [235]: ts2.reindex(ts.index, method=\"bfill\")\nOut[235]: \n2000-01-03    0.183051\n2000-01-04    2.395489\n2000-01-05    2.395489\n2000-01-06    2.395489\n2000-01-07    0.733639\n2000-01-08    0.733639\n2000-01-09    0.733639\n2000-01-10         NaN\nFreq: D, dtype: float64\n\nIn [236]: ts2.reindex(ts.index, method=\"nearest\")\nOut[236]: \n2000-01-03    0.183051\n2000-01-04    0.183051\n2000-01-05    2.395489\n2000-01-06    2.395489\n2000-01-07    2.395489\n2000-01-08    0.733639\n2000-01-09    0.733639\n2000-01-10    0.733639\nFreq: D, dtype: float64\n  These methods require that the indexes are ordered increasing or decreasing. Note that the same result could have been achieved using fillna (except for method='nearest') or interpolate: \nIn [237]: ts2.reindex(ts.index).fillna(method=\"ffill\")\nOut[237]: \n2000-01-03    0.183051\n2000-01-04    0.183051\n2000-01-05    0.183051\n2000-01-06    2.395489\n2000-01-07    2.395489\n2000-01-08    2.395489\n2000-01-09    0.733639\n2000-01-10    0.733639\nFreq: D, dtype: float64\n  reindex() will raise a ValueError if the index is not monotonically increasing or decreasing. fillna() and interpolate() will not perform any checks on the order of the index.   Limits on filling while reindexing The limit and tolerance arguments provide additional control over filling while reindexing. Limit specifies the maximum count of consecutive matches: \nIn [238]: ts2.reindex(ts.index, method=\"ffill\", limit=1)\nOut[238]: \n2000-01-03    0.183051\n2000-01-04    0.183051\n2000-01-05         NaN\n2000-01-06    2.395489\n2000-01-07    2.395489\n2000-01-08         NaN\n2000-01-09    0.733639\n2000-01-10    0.733639\nFreq: D, dtype: float64\n  In contrast, tolerance specifies the maximum distance between the index and indexer values: \nIn [239]: ts2.reindex(ts.index, method=\"ffill\", tolerance=\"1 day\")\nOut[239]: \n2000-01-03    0.183051\n2000-01-04    0.183051\n2000-01-05         NaN\n2000-01-06    2.395489\n2000-01-07    2.395489\n2000-01-08         NaN\n2000-01-09    0.733639\n2000-01-10    0.733639\nFreq: D, dtype: float64\n  Notice that when used on a DatetimeIndex, TimedeltaIndex or PeriodIndex, tolerance will coerced into a Timedelta if possible. This allows you to specify tolerance with appropriate strings.   Dropping labels from an axis A method closely related to reindex is the drop() function. It removes a set of labels from an axis: \nIn [240]: df\nOut[240]: \n        one       two     three\na  1.394981  1.772517       NaN\nb  0.343054  1.912123 -0.050390\nc  0.695246  1.478369  1.227435\nd       NaN  0.279344 -0.613172\n\nIn [241]: df.drop([\"a\", \"d\"], axis=0)\nOut[241]: \n        one       two     three\nb  0.343054  1.912123 -0.050390\nc  0.695246  1.478369  1.227435\n\nIn [242]: df.drop([\"one\"], axis=1)\nOut[242]: \n        two     three\na  1.772517       NaN\nb  1.912123 -0.050390\nc  1.478369  1.227435\nd  0.279344 -0.613172\n  Note that the following also works, but is a bit less obvious / clean: \nIn [243]: df.reindex(df.index.difference([\"a\", \"d\"]))\nOut[243]: \n        one       two     three\nb  0.343054  1.912123 -0.050390\nc  0.695246  1.478369  1.227435\n    Renaming / mapping labels The rename() method allows you to relabel an axis based on some mapping (a dict or Series) or an arbitrary function. \nIn [244]: s\nOut[244]: \na   -0.186646\nb   -1.692424\nc   -0.303893\nd   -1.425662\ne    1.114285\ndtype: float64\n\nIn [245]: s.rename(str.upper)\nOut[245]: \nA   -0.186646\nB   -1.692424\nC   -0.303893\nD   -1.425662\nE    1.114285\ndtype: float64\n  If you pass a function, it must return a value when called with any of the labels (and must produce a set of unique values). A dict or Series can also be used: \nIn [246]: df.rename(\n   .....:     columns={\"one\": \"foo\", \"two\": \"bar\"},\n   .....:     index={\"a\": \"apple\", \"b\": \"banana\", \"d\": \"durian\"},\n   .....: )\n   .....: \nOut[246]: \n             foo       bar     three\napple   1.394981  1.772517       NaN\nbanana  0.343054  1.912123 -0.050390\nc       0.695246  1.478369  1.227435\ndurian       NaN  0.279344 -0.613172\n  If the mapping doesn\u2019t include a column/index label, it isn\u2019t renamed. Note that extra labels in the mapping don\u2019t throw an error. DataFrame.rename() also supports an \u201caxis-style\u201d calling convention, where you specify a single mapper and the axis to apply that mapping to. \nIn [247]: df.rename({\"one\": \"foo\", \"two\": \"bar\"}, axis=\"columns\")\nOut[247]: \n        foo       bar     three\na  1.394981  1.772517       NaN\nb  0.343054  1.912123 -0.050390\nc  0.695246  1.478369  1.227435\nd       NaN  0.279344 -0.613172\n\nIn [248]: df.rename({\"a\": \"apple\", \"b\": \"banana\", \"d\": \"durian\"}, axis=\"index\")\nOut[248]: \n             one       two     three\napple   1.394981  1.772517       NaN\nbanana  0.343054  1.912123 -0.050390\nc       0.695246  1.478369  1.227435\ndurian       NaN  0.279344 -0.613172\n  The rename() method also provides an inplace named parameter that is by default False and copies the underlying data. Pass inplace=True to rename the data in place. Finally, rename() also accepts a scalar or list-like for altering the Series.name attribute. \nIn [249]: s.rename(\"scalar-name\")\nOut[249]: \na   -0.186646\nb   -1.692424\nc   -0.303893\nd   -1.425662\ne    1.114285\nName: scalar-name, dtype: float64\n  The methods DataFrame.rename_axis() and Series.rename_axis() allow specific names of a MultiIndex to be changed (as opposed to the labels). \nIn [250]: df = pd.DataFrame(\n   .....:     {\"x\": [1, 2, 3, 4, 5, 6], \"y\": [10, 20, 30, 40, 50, 60]},\n   .....:     index=pd.MultiIndex.from_product(\n   .....:         [[\"a\", \"b\", \"c\"], [1, 2]], names=[\"let\", \"num\"]\n   .....:     ),\n   .....: )\n   .....: \n\nIn [251]: df\nOut[251]: \n         x   y\nlet num       \na   1    1  10\n    2    2  20\nb   1    3  30\n    2    4  40\nc   1    5  50\n    2    6  60\n\nIn [252]: df.rename_axis(index={\"let\": \"abc\"})\nOut[252]: \n         x   y\nabc num       \na   1    1  10\n    2    2  20\nb   1    3  30\n    2    4  40\nc   1    5  50\n    2    6  60\n\nIn [253]: df.rename_axis(index=str.upper)\nOut[253]: \n         x   y\nLET NUM       \na   1    1  10\n    2    2  20\nb   1    3  30\n    2    4  40\nc   1    5  50\n    2    6  60\n     Iteration The behavior of basic iteration over pandas objects depends on the type. When iterating over a Series, it is regarded as array-like, and basic iteration produces the values. DataFrames follow the dict-like convention of iterating over the \u201ckeys\u201d of the objects. In short, basic iteration (for i in object) produces:  Series: values DataFrame: column labels  Thus, for example, iterating over a DataFrame gives you the column names: \nIn [254]: df = pd.DataFrame(\n   .....:     {\"col1\": np.random.randn(3), \"col2\": np.random.randn(3)}, index=[\"a\", \"b\", \"c\"]\n   .....: )\n   .....: \n\nIn [255]: for col in df:\n   .....:     print(col)\n   .....: \ncol1\ncol2\n  pandas objects also have the dict-like items() method to iterate over the (key, value) pairs. To iterate over the rows of a DataFrame, you can use the following methods:  iterrows(): Iterate over the rows of a DataFrame as (index, Series) pairs. This converts the rows to Series objects, which can change the dtypes and has some performance implications. itertuples(): Iterate over the rows of a DataFrame as namedtuples of the values. This is a lot faster than iterrows(), and is in most cases preferable to use to iterate over the values of a DataFrame.   Warning Iterating through pandas objects is generally slow. In many cases, iterating manually over the rows is not needed and can be avoided with one of the following approaches:  Look for a vectorized solution: many operations can be performed using built-in methods or NumPy functions, (boolean) indexing, \u2026 When you have a function that cannot work on the full DataFrame/Series at once, it is better to use apply() instead of iterating over the values. See the docs on function application. If you need to do iterative manipulations on the values but performance is important, consider writing the inner loop with cython or numba. See the enhancing performance section for some examples of this approach.    Warning You should never modify something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect! For example, in the following case setting the value has no effect: \nIn [256]: df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"]})\n\nIn [257]: for index, row in df.iterrows():\n   .....:     row[\"a\"] = 10\n   .....: \n\nIn [258]: df\nOut[258]: \n   a  b\n0  1  a\n1  2  b\n2  3  c\n    items Consistent with the dict-like interface, items() iterates through key-value pairs:  Series: (index, scalar value) pairs DataFrame: (column, Series) pairs  For example: \nIn [259]: for label, ser in df.items():\n   .....:     print(label)\n   .....:     print(ser)\n   .....: \na\n0    1\n1    2\n2    3\nName: a, dtype: int64\nb\n0    a\n1    b\n2    c\nName: b, dtype: object\n    iterrows iterrows() allows you to iterate through the rows of a DataFrame as Series objects. It returns an iterator yielding each index value along with a Series containing the data in each row: \nIn [260]: for row_index, row in df.iterrows():\n   .....:     print(row_index, row, sep=\"\\n\")\n   .....: \n0\na    1\nb    a\nName: 0, dtype: object\n1\na    2\nb    b\nName: 1, dtype: object\n2\na    3\nb    c\nName: 2, dtype: object\n   Note Because iterrows() returns a Series for each row, it does not preserve dtypes across the rows (dtypes are preserved across columns for DataFrames). For example, \nIn [261]: df_orig = pd.DataFrame([[1, 1.5]], columns=[\"int\", \"float\"])\n\nIn [262]: df_orig.dtypes\nOut[262]: \nint        int64\nfloat    float64\ndtype: object\n\nIn [263]: row = next(df_orig.iterrows())[1]\n\nIn [264]: row\nOut[264]: \nint      1.0\nfloat    1.5\nName: 0, dtype: float64\n  All values in row, returned as a Series, are now upcasted to floats, also the original integer value in column x: \nIn [265]: row[\"int\"].dtype\nOut[265]: dtype('float64')\n\nIn [266]: df_orig[\"int\"].dtype\nOut[266]: dtype('int64')\n  To preserve dtypes while iterating over the rows, it is better to use itertuples() which returns namedtuples of the values and which is generally much faster than iterrows().  For instance, a contrived way to transpose the DataFrame would be: \nIn [267]: df2 = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\n\nIn [268]: print(df2)\n   x  y\n0  1  4\n1  2  5\n2  3  6\n\nIn [269]: print(df2.T)\n   0  1  2\nx  1  2  3\ny  4  5  6\n\nIn [270]: df2_t = pd.DataFrame({idx: values for idx, values in df2.iterrows()})\n\nIn [271]: print(df2_t)\n   0  1  2\nx  1  2  3\ny  4  5  6\n    itertuples The itertuples() method will return an iterator yielding a namedtuple for each row in the DataFrame. The first element of the tuple will be the row\u2019s corresponding index value, while the remaining values are the row values. For instance: \nIn [272]: for row in df.itertuples():\n   .....:     print(row)\n   .....: \nPandas(Index=0, a=1, b='a')\nPandas(Index=1, a=2, b='b')\nPandas(Index=2, a=3, b='c')\n  This method does not convert the row to a Series object; it merely returns the values inside a namedtuple. Therefore, itertuples() preserves the data type of the values and is generally faster as iterrows().  Note The column names will be renamed to positional names if they are invalid Python identifiers, repeated, or start with an underscore. With a large number of columns (>255), regular tuples are returned.     .dt accessor Series has an accessor to succinctly return datetime like properties for the values of the Series, if it is a datetime/period like Series. This will return a Series, indexed like the existing Series. \n# datetime\nIn [273]: s = pd.Series(pd.date_range(\"20130101 09:10:12\", periods=4))\n\nIn [274]: s\nOut[274]: \n0   2013-01-01 09:10:12\n1   2013-01-02 09:10:12\n2   2013-01-03 09:10:12\n3   2013-01-04 09:10:12\ndtype: datetime64[ns]\n\nIn [275]: s.dt.hour\nOut[275]: \n0    9\n1    9\n2    9\n3    9\ndtype: int64\n\nIn [276]: s.dt.second\nOut[276]: \n0    12\n1    12\n2    12\n3    12\ndtype: int64\n\nIn [277]: s.dt.day\nOut[277]: \n0    1\n1    2\n2    3\n3    4\ndtype: int64\n  This enables nice expressions like this: \nIn [278]: s[s.dt.day == 2]\nOut[278]: \n1   2013-01-02 09:10:12\ndtype: datetime64[ns]\n  You can easily produces tz aware transformations: \nIn [279]: stz = s.dt.tz_localize(\"US/Eastern\")\n\nIn [280]: stz\nOut[280]: \n0   2013-01-01 09:10:12-05:00\n1   2013-01-02 09:10:12-05:00\n2   2013-01-03 09:10:12-05:00\n3   2013-01-04 09:10:12-05:00\ndtype: datetime64[ns, US/Eastern]\n\nIn [281]: stz.dt.tz\nOut[281]: <DstTzInfo 'US/Eastern' LMT-1 day, 19:04:00 STD>\n  You can also chain these types of operations: \nIn [282]: s.dt.tz_localize(\"UTC\").dt.tz_convert(\"US/Eastern\")\nOut[282]: \n0   2013-01-01 04:10:12-05:00\n1   2013-01-02 04:10:12-05:00\n2   2013-01-03 04:10:12-05:00\n3   2013-01-04 04:10:12-05:00\ndtype: datetime64[ns, US/Eastern]\n  You can also format datetime values as strings with Series.dt.strftime() which supports the same format as the standard strftime(). \n# DatetimeIndex\nIn [283]: s = pd.Series(pd.date_range(\"20130101\", periods=4))\n\nIn [284]: s\nOut[284]: \n0   2013-01-01\n1   2013-01-02\n2   2013-01-03\n3   2013-01-04\ndtype: datetime64[ns]\n\nIn [285]: s.dt.strftime(\"%Y/%m/%d\")\nOut[285]: \n0    2013/01/01\n1    2013/01/02\n2    2013/01/03\n3    2013/01/04\ndtype: object\n  \n# PeriodIndex\nIn [286]: s = pd.Series(pd.period_range(\"20130101\", periods=4))\n\nIn [287]: s\nOut[287]: \n0    2013-01-01\n1    2013-01-02\n2    2013-01-03\n3    2013-01-04\ndtype: period[D]\n\nIn [288]: s.dt.strftime(\"%Y/%m/%d\")\nOut[288]: \n0    2013/01/01\n1    2013/01/02\n2    2013/01/03\n3    2013/01/04\ndtype: object\n  The .dt accessor works for period and timedelta dtypes. \n# period\nIn [289]: s = pd.Series(pd.period_range(\"20130101\", periods=4, freq=\"D\"))\n\nIn [290]: s\nOut[290]: \n0    2013-01-01\n1    2013-01-02\n2    2013-01-03\n3    2013-01-04\ndtype: period[D]\n\nIn [291]: s.dt.year\nOut[291]: \n0    2013\n1    2013\n2    2013\n3    2013\ndtype: int64\n\nIn [292]: s.dt.day\nOut[292]: \n0    1\n1    2\n2    3\n3    4\ndtype: int64\n  \n# timedelta\nIn [293]: s = pd.Series(pd.timedelta_range(\"1 day 00:00:05\", periods=4, freq=\"s\"))\n\nIn [294]: s\nOut[294]: \n0   1 days 00:00:05\n1   1 days 00:00:06\n2   1 days 00:00:07\n3   1 days 00:00:08\ndtype: timedelta64[ns]\n\nIn [295]: s.dt.days\nOut[295]: \n0    1\n1    1\n2    1\n3    1\ndtype: int64\n\nIn [296]: s.dt.seconds\nOut[296]: \n0    5\n1    6\n2    7\n3    8\ndtype: int64\n\nIn [297]: s.dt.components\nOut[297]: \n   days  hours  minutes  seconds  milliseconds  microseconds  nanoseconds\n0     1      0        0        5             0             0            0\n1     1      0        0        6             0             0            0\n2     1      0        0        7             0             0            0\n3     1      0        0        8             0             0            0\n   Note Series.dt will raise a TypeError if you access with a non-datetime-like values.    Vectorized string methods Series is equipped with a set of string processing methods that make it easy to operate on each element of the array. Perhaps most importantly, these methods exclude missing/NA values automatically. These are accessed via the Series\u2019s str attribute and generally have names matching the equivalent (scalar) built-in string methods. For example:  \n\nIn [298]: s = pd.Series(\n   .....:     [\"A\", \"B\", \"C\", \"Aaba\", \"Baca\", np.nan, \"CABA\", \"dog\", \"cat\"], dtype=\"string\"\n   .....: )\n   .....: \n\nIn [299]: s.str.lower()\nOut[299]: \n0       a\n1       b\n2       c\n3    aaba\n4    baca\n5    <NA>\n6    caba\n7     dog\n8     cat\ndtype: string\n  \n Powerful pattern-matching methods are provided as well, but note that pattern-matching generally uses regular expressions by default (and in some cases always uses them).  Note Prior to pandas 1.0, string methods were only available on object -dtype Series. pandas 1.0 added the StringDtype which is dedicated to strings. See Text data types for more.  Please see Vectorized String Methods for a complete description.   Sorting pandas supports three kinds of sorting: sorting by index labels, sorting by column values, and sorting by a combination of both.  By index The Series.sort_index() and DataFrame.sort_index() methods are used to sort a pandas object by its index levels. \nIn [300]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"one\": pd.Series(np.random.randn(3), index=[\"a\", \"b\", \"c\"]),\n   .....:         \"two\": pd.Series(np.random.randn(4), index=[\"a\", \"b\", \"c\", \"d\"]),\n   .....:         \"three\": pd.Series(np.random.randn(3), index=[\"b\", \"c\", \"d\"]),\n   .....:     }\n   .....: )\n   .....: \n\nIn [301]: unsorted_df = df.reindex(\n   .....:     index=[\"a\", \"d\", \"c\", \"b\"], columns=[\"three\", \"two\", \"one\"]\n   .....: )\n   .....: \n\nIn [302]: unsorted_df\nOut[302]: \n      three       two       one\na       NaN -1.152244  0.562973\nd -0.252916 -0.109597       NaN\nc  1.273388 -0.167123  0.640382\nb -0.098217  0.009797 -1.299504\n\n# DataFrame\nIn [303]: unsorted_df.sort_index()\nOut[303]: \n      three       two       one\na       NaN -1.152244  0.562973\nb -0.098217  0.009797 -1.299504\nc  1.273388 -0.167123  0.640382\nd -0.252916 -0.109597       NaN\n\nIn [304]: unsorted_df.sort_index(ascending=False)\nOut[304]: \n      three       two       one\nd -0.252916 -0.109597       NaN\nc  1.273388 -0.167123  0.640382\nb -0.098217  0.009797 -1.299504\na       NaN -1.152244  0.562973\n\nIn [305]: unsorted_df.sort_index(axis=1)\nOut[305]: \n        one     three       two\na  0.562973       NaN -1.152244\nd       NaN -0.252916 -0.109597\nc  0.640382  1.273388 -0.167123\nb -1.299504 -0.098217  0.009797\n\n# Series\nIn [306]: unsorted_df[\"three\"].sort_index()\nOut[306]: \na         NaN\nb   -0.098217\nc    1.273388\nd   -0.252916\nName: three, dtype: float64\n   New in version 1.1.0.  Sorting by index also supports a key parameter that takes a callable function to apply to the index being sorted. For MultiIndex objects, the key is applied per-level to the levels specified by level. \nIn [307]: s1 = pd.DataFrame({\"a\": [\"B\", \"a\", \"C\"], \"b\": [1, 2, 3], \"c\": [2, 3, 4]}).set_index(\n   .....:     list(\"ab\")\n   .....: )\n   .....: \n\nIn [308]: s1\nOut[308]: \n     c\na b   \nB 1  2\na 2  3\nC 3  4\n  \nIn [309]: s1.sort_index(level=\"a\")\nOut[309]: \n     c\na b   \nB 1  2\nC 3  4\na 2  3\n\nIn [310]: s1.sort_index(level=\"a\", key=lambda idx: idx.str.lower())\nOut[310]: \n     c\na b   \na 2  3\nB 1  2\nC 3  4\n  For information on key sorting by value, see value sorting.   By values The Series.sort_values() method is used to sort a Series by its values. The DataFrame.sort_values() method is used to sort a DataFrame by its column or row values. The optional by parameter to DataFrame.sort_values() may used to specify one or more columns to use to determine the sorted order. \nIn [311]: df1 = pd.DataFrame(\n   .....:     {\"one\": [2, 1, 1, 1], \"two\": [1, 3, 2, 4], \"three\": [5, 4, 3, 2]}\n   .....: )\n   .....: \n\nIn [312]: df1.sort_values(by=\"two\")\nOut[312]: \n   one  two  three\n0    2    1      5\n2    1    2      3\n1    1    3      4\n3    1    4      2\n  The by parameter can take a list of column names, e.g.: \nIn [313]: df1[[\"one\", \"two\", \"three\"]].sort_values(by=[\"one\", \"two\"])\nOut[313]: \n   one  two  three\n2    1    2      3\n1    1    3      4\n3    1    4      2\n0    2    1      5\n  These methods have special treatment of NA values via the na_position argument: \nIn [314]: s[2] = np.nan\n\nIn [315]: s.sort_values()\nOut[315]: \n0       A\n3    Aaba\n1       B\n4    Baca\n6    CABA\n8     cat\n7     dog\n2    <NA>\n5    <NA>\ndtype: string\n\nIn [316]: s.sort_values(na_position=\"first\")\nOut[316]: \n2    <NA>\n5    <NA>\n0       A\n3    Aaba\n1       B\n4    Baca\n6    CABA\n8     cat\n7     dog\ndtype: string\n   New in version 1.1.0.  Sorting also supports a key parameter that takes a callable function to apply to the values being sorted. \nIn [317]: s1 = pd.Series([\"B\", \"a\", \"C\"])\n  \nIn [318]: s1.sort_values()\nOut[318]: \n0    B\n2    C\n1    a\ndtype: object\n\nIn [319]: s1.sort_values(key=lambda x: x.str.lower())\nOut[319]: \n1    a\n0    B\n2    C\ndtype: object\n  key will be given the Series of values and should return a Series or array of the same shape with the transformed values. For DataFrame objects, the key is applied per column, so the key should still expect a Series and return a Series, e.g. \nIn [320]: df = pd.DataFrame({\"a\": [\"B\", \"a\", \"C\"], \"b\": [1, 2, 3]})\n  \nIn [321]: df.sort_values(by=\"a\")\nOut[321]: \n   a  b\n0  B  1\n2  C  3\n1  a  2\n\nIn [322]: df.sort_values(by=\"a\", key=lambda col: col.str.lower())\nOut[322]: \n   a  b\n1  a  2\n0  B  1\n2  C  3\n  The name or type of each column can be used to apply different functions to different columns.   By indexes and values Strings passed as the by parameter to DataFrame.sort_values() may refer to either columns or index level names. \n# Build MultiIndex\nIn [323]: idx = pd.MultiIndex.from_tuples(\n   .....:     [(\"a\", 1), (\"a\", 2), (\"a\", 2), (\"b\", 2), (\"b\", 1), (\"b\", 1)]\n   .....: )\n   .....: \n\nIn [324]: idx.names = [\"first\", \"second\"]\n\n# Build DataFrame\nIn [325]: df_multi = pd.DataFrame({\"A\": np.arange(6, 0, -1)}, index=idx)\n\nIn [326]: df_multi\nOut[326]: \n              A\nfirst second   \na     1       6\n      2       5\n      2       4\nb     2       3\n      1       2\n      1       1\n  Sort by \u2018second\u2019 (index) and \u2018A\u2019 (column) \nIn [327]: df_multi.sort_values(by=[\"second\", \"A\"])\nOut[327]: \n              A\nfirst second   \nb     1       1\n      1       2\na     1       6\nb     2       3\na     2       4\n      2       5\n   Note If a string matches both a column name and an index level name then a warning is issued and the column takes precedence. This will result in an ambiguity error in a future version.    searchsorted Series has the searchsorted() method, which works similarly to numpy.ndarray.searchsorted(). \nIn [328]: ser = pd.Series([1, 2, 3])\n\nIn [329]: ser.searchsorted([0, 3])\nOut[329]: array([0, 2])\n\nIn [330]: ser.searchsorted([0, 4])\nOut[330]: array([0, 3])\n\nIn [331]: ser.searchsorted([1, 3], side=\"right\")\nOut[331]: array([1, 3])\n\nIn [332]: ser.searchsorted([1, 3], side=\"left\")\nOut[332]: array([0, 2])\n\nIn [333]: ser = pd.Series([3, 1, 2])\n\nIn [334]: ser.searchsorted([0, 3], sorter=np.argsort(ser))\nOut[334]: array([0, 2])\n    smallest / largest values Series has the nsmallest() and nlargest() methods which return the smallest or largest \\(n\\) values. For a large Series this can be much faster than sorting the entire Series and calling head(n) on the result. \nIn [335]: s = pd.Series(np.random.permutation(10))\n\nIn [336]: s\nOut[336]: \n0    2\n1    0\n2    3\n3    7\n4    1\n5    5\n6    9\n7    6\n8    8\n9    4\ndtype: int64\n\nIn [337]: s.sort_values()\nOut[337]: \n1    0\n4    1\n0    2\n2    3\n9    4\n5    5\n7    6\n3    7\n8    8\n6    9\ndtype: int64\n\nIn [338]: s.nsmallest(3)\nOut[338]: \n1    0\n4    1\n0    2\ndtype: int64\n\nIn [339]: s.nlargest(3)\nOut[339]: \n6    9\n8    8\n3    7\ndtype: int64\n  DataFrame also has the nlargest and nsmallest methods. \nIn [340]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"a\": [-2, -1, 1, 10, 8, 11, -1],\n   .....:         \"b\": list(\"abdceff\"),\n   .....:         \"c\": [1.0, 2.0, 4.0, 3.2, np.nan, 3.0, 4.0],\n   .....:     }\n   .....: )\n   .....: \n\nIn [341]: df.nlargest(3, \"a\")\nOut[341]: \n    a  b    c\n5  11  f  3.0\n3  10  c  3.2\n4   8  e  NaN\n\nIn [342]: df.nlargest(5, [\"a\", \"c\"])\nOut[342]: \n    a  b    c\n5  11  f  3.0\n3  10  c  3.2\n4   8  e  NaN\n2   1  d  4.0\n6  -1  f  4.0\n\nIn [343]: df.nsmallest(3, \"a\")\nOut[343]: \n   a  b    c\n0 -2  a  1.0\n1 -1  b  2.0\n6 -1  f  4.0\n\nIn [344]: df.nsmallest(5, [\"a\", \"c\"])\nOut[344]: \n   a  b    c\n0 -2  a  1.0\n1 -1  b  2.0\n6 -1  f  4.0\n2  1  d  4.0\n4  8  e  NaN\n    Sorting by a MultiIndex column You must be explicit about sorting when the column is a MultiIndex, and fully specify all levels to by. \nIn [345]: df1.columns = pd.MultiIndex.from_tuples(\n   .....:     [(\"a\", \"one\"), (\"a\", \"two\"), (\"b\", \"three\")]\n   .....: )\n   .....: \n\nIn [346]: df1.sort_values(by=(\"a\", \"two\"))\nOut[346]: \n    a         b\n  one two three\n0   2   1     5\n2   1   2     3\n1   1   3     4\n3   1   4     2\n     Copying The copy() method on pandas objects copies the underlying data (though not the axis indexes, since they are immutable) and returns a new object. Note that it is seldom necessary to copy objects. For example, there are only a handful of ways to alter a DataFrame in-place:  Inserting, deleting, or modifying a column. Assigning to the index or columns attributes. For homogeneous data, directly modifying the values via the values attribute or advanced indexing.  To be clear, no pandas method has the side effect of modifying your data; almost every method returns a new object, leaving the original object untouched. If the data is modified, it is because you did so explicitly.   dtypes For the most part, pandas uses NumPy arrays and dtypes for Series or individual columns of a DataFrame. NumPy provides support for float, int, bool, timedelta64[ns] and datetime64[ns] (note that NumPy does not support timezone-aware datetimes). pandas and third-party libraries extend NumPy\u2019s type system in a few places. This section describes the extensions pandas has made internally. See Extension types for how to write your own extension that works with pandas. See Extension data types for a list of third-party libraries that have implemented an extension. The following table lists all of pandas extension types. For methods requiring dtype arguments, strings can be specified as indicated. See the respective documentation sections for more on each type.            \nKind of Data Data Type Scalar Array String Aliases    \ntz-aware datetime DatetimeTZDtype Timestamp arrays.DatetimeArray 'datetime64[ns, <tz>]'  \nCategorical CategoricalDtype (none) Categorical 'category'  \nperiod (time spans) PeriodDtype Period arrays.PeriodArray 'Period[<freq>]' 'period[<freq>]',  \nsparse SparseDtype (none) arrays.SparseArray 'Sparse', 'Sparse[int]', 'Sparse[float]'  \nintervals IntervalDtype Interval arrays.IntervalArray 'interval', 'Interval', 'Interval[<numpy_dtype>]', 'Interval[datetime64[ns, <tz>]]', 'Interval[timedelta64[<freq>]]'  \nnullable integer Int64Dtype, \u2026 (none) arrays.IntegerArray 'Int8', 'Int16', 'Int32', 'Int64', 'UInt8', 'UInt16', 'UInt32', 'UInt64'  \nStrings StringDtype str arrays.StringArray 'string'  \nBoolean (with NA) BooleanDtype bool arrays.BooleanArray 'boolean'    pandas has two ways to store strings.  object dtype, which can hold any Python object, including strings. StringDtype, which is dedicated to strings.  Generally, we recommend using StringDtype. See Text data types for more. Finally, arbitrary objects may be stored using the object dtype, but should be avoided to the extent possible (for performance and interoperability with other libraries and methods. See object conversion). A convenient dtypes attribute for DataFrame returns a Series with the data type of each column. \nIn [347]: dft = pd.DataFrame(\n   .....:     {\n   .....:         \"A\": np.random.rand(3),\n   .....:         \"B\": 1,\n   .....:         \"C\": \"foo\",\n   .....:         \"D\": pd.Timestamp(\"20010102\"),\n   .....:         \"E\": pd.Series([1.0] * 3).astype(\"float32\"),\n   .....:         \"F\": False,\n   .....:         \"G\": pd.Series([1] * 3, dtype=\"int8\"),\n   .....:     }\n   .....: )\n   .....: \n\nIn [348]: dft\nOut[348]: \n          A  B    C          D    E      F  G\n0  0.035962  1  foo 2001-01-02  1.0  False  1\n1  0.701379  1  foo 2001-01-02  1.0  False  1\n2  0.281885  1  foo 2001-01-02  1.0  False  1\n\nIn [349]: dft.dtypes\nOut[349]: \nA           float64\nB             int64\nC            object\nD    datetime64[ns]\nE           float32\nF              bool\nG              int8\ndtype: object\n  On a Series object, use the dtype attribute. \nIn [350]: dft[\"A\"].dtype\nOut[350]: dtype('float64')\n  If a pandas object contains data with multiple dtypes in a single column, the dtype of the column will be chosen to accommodate all of the data types (object is the most general). \n# these ints are coerced to floats\nIn [351]: pd.Series([1, 2, 3, 4, 5, 6.0])\nOut[351]: \n0    1.0\n1    2.0\n2    3.0\n3    4.0\n4    5.0\n5    6.0\ndtype: float64\n\n# string data forces an ``object`` dtype\nIn [352]: pd.Series([1, 2, 3, 6.0, \"foo\"])\nOut[352]: \n0      1\n1      2\n2      3\n3    6.0\n4    foo\ndtype: object\n  The number of columns of each type in a DataFrame can be found by calling DataFrame.dtypes.value_counts(). \nIn [353]: dft.dtypes.value_counts()\nOut[353]: \nfloat64           1\nint64             1\nobject            1\ndatetime64[ns]    1\nfloat32           1\nbool              1\nint8              1\ndtype: int64\n  Numeric dtypes will propagate and can coexist in DataFrames. If a dtype is passed (either directly via the dtype keyword, a passed ndarray, or a passed Series), then it will be preserved in DataFrame operations. Furthermore, different numeric dtypes will NOT be combined. The following example will give you a taste. \nIn [354]: df1 = pd.DataFrame(np.random.randn(8, 1), columns=[\"A\"], dtype=\"float32\")\n\nIn [355]: df1\nOut[355]: \n          A\n0  0.224364\n1  1.890546\n2  0.182879\n3  0.787847\n4 -0.188449\n5  0.667715\n6 -0.011736\n7 -0.399073\n\nIn [356]: df1.dtypes\nOut[356]: \nA    float32\ndtype: object\n\nIn [357]: df2 = pd.DataFrame(\n   .....:     {\n   .....:         \"A\": pd.Series(np.random.randn(8), dtype=\"float16\"),\n   .....:         \"B\": pd.Series(np.random.randn(8)),\n   .....:         \"C\": pd.Series(np.array(np.random.randn(8), dtype=\"uint8\")),\n   .....:     }\n   .....: )\n   .....: \n\nIn [358]: df2\nOut[358]: \n          A         B    C\n0  0.823242  0.256090    0\n1  1.607422  1.426469    0\n2 -0.333740 -0.416203  255\n3 -0.063477  1.139976    0\n4 -1.014648 -1.193477    0\n5  0.678711  0.096706    0\n6 -0.040863 -1.956850    1\n7 -0.357422 -0.714337    0\n\nIn [359]: df2.dtypes\nOut[359]: \nA    float16\nB    float64\nC      uint8\ndtype: object\n   defaults By default integer types are int64 and float types are float64, regardless of platform (32-bit or 64-bit). The following will all result in int64 dtypes. \nIn [360]: pd.DataFrame([1, 2], columns=[\"a\"]).dtypes\nOut[360]: \na    int64\ndtype: object\n\nIn [361]: pd.DataFrame({\"a\": [1, 2]}).dtypes\nOut[361]: \na    int64\ndtype: object\n\nIn [362]: pd.DataFrame({\"a\": 1}, index=list(range(2))).dtypes\nOut[362]: \na    int64\ndtype: object\n  Note that Numpy will choose platform-dependent types when creating arrays. The following WILL result in int32 on 32-bit platform. \nIn [363]: frame = pd.DataFrame(np.array([1, 2]))\n    upcasting Types can potentially be upcasted when combined with other types, meaning they are promoted from the current type (e.g. int to float). \nIn [364]: df3 = df1.reindex_like(df2).fillna(value=0.0) + df2\n\nIn [365]: df3\nOut[365]: \n          A         B      C\n0  1.047606  0.256090    0.0\n1  3.497968  1.426469    0.0\n2 -0.150862 -0.416203  255.0\n3  0.724370  1.139976    0.0\n4 -1.203098 -1.193477    0.0\n5  1.346426  0.096706    0.0\n6 -0.052599 -1.956850    1.0\n7 -0.756495 -0.714337    0.0\n\nIn [366]: df3.dtypes\nOut[366]: \nA    float32\nB    float64\nC    float64\ndtype: object\n  DataFrame.to_numpy() will return the lower-common-denominator of the dtypes, meaning the dtype that can accommodate ALL of the types in the resulting homogeneous dtyped NumPy array. This can force some upcasting. \nIn [367]: df3.to_numpy().dtype\nOut[367]: dtype('float64')\n    astype You can use the astype() method to explicitly convert dtypes from one to another. These will by default return a copy, even if the dtype was unchanged (pass copy=False to change this behavior). In addition, they will raise an exception if the astype operation is invalid. Upcasting is always according to the NumPy rules. If two different dtypes are involved in an operation, then the more general one will be used as the result of the operation. \nIn [368]: df3\nOut[368]: \n          A         B      C\n0  1.047606  0.256090    0.0\n1  3.497968  1.426469    0.0\n2 -0.150862 -0.416203  255.0\n3  0.724370  1.139976    0.0\n4 -1.203098 -1.193477    0.0\n5  1.346426  0.096706    0.0\n6 -0.052599 -1.956850    1.0\n7 -0.756495 -0.714337    0.0\n\nIn [369]: df3.dtypes\nOut[369]: \nA    float32\nB    float64\nC    float64\ndtype: object\n\n# conversion of dtypes\nIn [370]: df3.astype(\"float32\").dtypes\nOut[370]: \nA    float32\nB    float32\nC    float32\ndtype: object\n  Convert a subset of columns to a specified type using astype(). \nIn [371]: dft = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9]})\n\nIn [372]: dft[[\"a\", \"b\"]] = dft[[\"a\", \"b\"]].astype(np.uint8)\n\nIn [373]: dft\nOut[373]: \n   a  b  c\n0  1  4  7\n1  2  5  8\n2  3  6  9\n\nIn [374]: dft.dtypes\nOut[374]: \na    uint8\nb    uint8\nc    int64\ndtype: object\n  Convert certain columns to a specific dtype by passing a dict to astype(). \nIn [375]: dft1 = pd.DataFrame({\"a\": [1, 0, 1], \"b\": [4, 5, 6], \"c\": [7, 8, 9]})\n\nIn [376]: dft1 = dft1.astype({\"a\": np.bool_, \"c\": np.float64})\n\nIn [377]: dft1\nOut[377]: \n       a  b    c\n0   True  4  7.0\n1  False  5  8.0\n2   True  6  9.0\n\nIn [378]: dft1.dtypes\nOut[378]: \na       bool\nb      int64\nc    float64\ndtype: object\n   Note When trying to convert a subset of columns to a specified type using astype() and loc(), upcasting occurs. loc() tries to fit in what we are assigning to the current dtypes, while [] will overwrite them taking the dtype from the right hand side. Therefore the following piece of code produces the unintended result. \nIn [379]: dft = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9]})\n\nIn [380]: dft.loc[:, [\"a\", \"b\"]].astype(np.uint8).dtypes\nOut[380]: \na    uint8\nb    uint8\ndtype: object\n\nIn [381]: dft.loc[:, [\"a\", \"b\"]] = dft.loc[:, [\"a\", \"b\"]].astype(np.uint8)\n\nIn [382]: dft.dtypes\nOut[382]: \na    int64\nb    int64\nc    int64\ndtype: object\n     object conversion pandas offers various functions to try to force conversion of types from the object dtype to other types. In cases where the data is already of the correct type, but stored in an object array, the DataFrame.infer_objects() and Series.infer_objects() methods can be used to soft convert to the correct type.  \n\nIn [383]: import datetime\n\nIn [384]: df = pd.DataFrame(\n   .....:     [\n   .....:         [1, 2],\n   .....:         [\"a\", \"b\"],\n   .....:         [datetime.datetime(2016, 3, 2), datetime.datetime(2016, 3, 2)],\n   .....:     ]\n   .....: )\n   .....: \n\nIn [385]: df = df.T\n\nIn [386]: df\nOut[386]: \n   0  1          2\n0  1  a 2016-03-02\n1  2  b 2016-03-02\n\nIn [387]: df.dtypes\nOut[387]: \n0            object\n1            object\n2    datetime64[ns]\ndtype: object\n  \n Because the data was transposed the original inference stored all columns as object, which infer_objects will correct.  \n\nIn [388]: df.infer_objects().dtypes\nOut[388]: \n0             int64\n1            object\n2    datetime64[ns]\ndtype: object\n  \n The following functions are available for one dimensional object arrays or scalars to perform hard conversion of objects to a specified type:  \nto_numeric() (conversion to numeric dtypes) \nIn [389]: m = [\"1.1\", 2, 3]\n\nIn [390]: pd.to_numeric(m)\nOut[390]: array([1.1, 2. , 3. ])\n   \nto_datetime() (conversion to datetime objects) \nIn [391]: import datetime\n\nIn [392]: m = [\"2016-07-09\", datetime.datetime(2016, 3, 2)]\n\nIn [393]: pd.to_datetime(m)\nOut[393]: DatetimeIndex(['2016-07-09', '2016-03-02'], dtype='datetime64[ns]', freq=None)\n   \nto_timedelta() (conversion to timedelta objects) \nIn [394]: m = [\"5us\", pd.Timedelta(\"1day\")]\n\nIn [395]: pd.to_timedelta(m)\nOut[395]: TimedeltaIndex(['0 days 00:00:00.000005', '1 days 00:00:00'], dtype='timedelta64[ns]', freq=None)\n    To force a conversion, we can pass in an errors argument, which specifies how pandas should deal with elements that cannot be converted to desired dtype or object. By default, errors='raise', meaning that any errors encountered will be raised during the conversion process. However, if errors='coerce', these errors will be ignored and pandas will convert problematic elements to pd.NaT (for datetime and timedelta) or np.nan (for numeric). This might be useful if you are reading in data which is mostly of the desired dtype (e.g. numeric, datetime), but occasionally has non-conforming elements intermixed that you want to represent as missing: \nIn [396]: import datetime\n\nIn [397]: m = [\"apple\", datetime.datetime(2016, 3, 2)]\n\nIn [398]: pd.to_datetime(m, errors=\"coerce\")\nOut[398]: DatetimeIndex(['NaT', '2016-03-02'], dtype='datetime64[ns]', freq=None)\n\nIn [399]: m = [\"apple\", 2, 3]\n\nIn [400]: pd.to_numeric(m, errors=\"coerce\")\nOut[400]: array([nan,  2.,  3.])\n\nIn [401]: m = [\"apple\", pd.Timedelta(\"1day\")]\n\nIn [402]: pd.to_timedelta(m, errors=\"coerce\")\nOut[402]: TimedeltaIndex([NaT, '1 days'], dtype='timedelta64[ns]', freq=None)\n  The errors parameter has a third option of errors='ignore', which will simply return the passed in data if it encounters any errors with the conversion to a desired data type: \nIn [403]: import datetime\n\nIn [404]: m = [\"apple\", datetime.datetime(2016, 3, 2)]\n\nIn [405]: pd.to_datetime(m, errors=\"ignore\")\nOut[405]: Index(['apple', 2016-03-02 00:00:00], dtype='object')\n\nIn [406]: m = [\"apple\", 2, 3]\n\nIn [407]: pd.to_numeric(m, errors=\"ignore\")\nOut[407]: array(['apple', 2, 3], dtype=object)\n\nIn [408]: m = [\"apple\", pd.Timedelta(\"1day\")]\n\nIn [409]: pd.to_timedelta(m, errors=\"ignore\")\nOut[409]: array(['apple', Timedelta('1 days 00:00:00')], dtype=object)\n  In addition to object conversion, to_numeric() provides another argument downcast, which gives the option of downcasting the newly (or already) numeric data to a smaller dtype, which can conserve memory: \nIn [410]: m = [\"1\", 2, 3]\n\nIn [411]: pd.to_numeric(m, downcast=\"integer\")  # smallest signed int dtype\nOut[411]: array([1, 2, 3], dtype=int8)\n\nIn [412]: pd.to_numeric(m, downcast=\"signed\")  # same as 'integer'\nOut[412]: array([1, 2, 3], dtype=int8)\n\nIn [413]: pd.to_numeric(m, downcast=\"unsigned\")  # smallest unsigned int dtype\nOut[413]: array([1, 2, 3], dtype=uint8)\n\nIn [414]: pd.to_numeric(m, downcast=\"float\")  # smallest float dtype\nOut[414]: array([1., 2., 3.], dtype=float32)\n  As these methods apply only to one-dimensional arrays, lists or scalars; they cannot be used directly on multi-dimensional objects such as DataFrames. However, with apply(), we can \u201capply\u201d the function over each column efficiently: \nIn [415]: import datetime\n\nIn [416]: df = pd.DataFrame([[\"2016-07-09\", datetime.datetime(2016, 3, 2)]] * 2, dtype=\"O\")\n\nIn [417]: df\nOut[417]: \n            0                    1\n0  2016-07-09  2016-03-02 00:00:00\n1  2016-07-09  2016-03-02 00:00:00\n\nIn [418]: df.apply(pd.to_datetime)\nOut[418]: \n           0          1\n0 2016-07-09 2016-03-02\n1 2016-07-09 2016-03-02\n\nIn [419]: df = pd.DataFrame([[\"1.1\", 2, 3]] * 2, dtype=\"O\")\n\nIn [420]: df\nOut[420]: \n     0  1  2\n0  1.1  2  3\n1  1.1  2  3\n\nIn [421]: df.apply(pd.to_numeric)\nOut[421]: \n     0  1  2\n0  1.1  2  3\n1  1.1  2  3\n\nIn [422]: df = pd.DataFrame([[\"5us\", pd.Timedelta(\"1day\")]] * 2, dtype=\"O\")\n\nIn [423]: df\nOut[423]: \n     0                1\n0  5us  1 days 00:00:00\n1  5us  1 days 00:00:00\n\nIn [424]: df.apply(pd.to_timedelta)\nOut[424]: \n                       0      1\n0 0 days 00:00:00.000005 1 days\n1 0 days 00:00:00.000005 1 days\n    gotchas Performing selection operations on integer type data can easily upcast the data to floating. The dtype of the input data will be preserved in cases where nans are not introduced. See also Support for integer NA. \nIn [425]: dfi = df3.astype(\"int32\")\n\nIn [426]: dfi[\"E\"] = 1\n\nIn [427]: dfi\nOut[427]: \n   A  B    C  E\n0  1  0    0  1\n1  3  1    0  1\n2  0  0  255  1\n3  0  1    0  1\n4 -1 -1    0  1\n5  1  0    0  1\n6  0 -1    1  1\n7  0  0    0  1\n\nIn [428]: dfi.dtypes\nOut[428]: \nA    int32\nB    int32\nC    int32\nE    int64\ndtype: object\n\nIn [429]: casted = dfi[dfi > 0]\n\nIn [430]: casted\nOut[430]: \n     A    B      C  E\n0  1.0  NaN    NaN  1\n1  3.0  1.0    NaN  1\n2  NaN  NaN  255.0  1\n3  NaN  1.0    NaN  1\n4  NaN  NaN    NaN  1\n5  1.0  NaN    NaN  1\n6  NaN  NaN    1.0  1\n7  NaN  NaN    NaN  1\n\nIn [431]: casted.dtypes\nOut[431]: \nA    float64\nB    float64\nC    float64\nE      int64\ndtype: object\n  While float dtypes are unchanged. \nIn [432]: dfa = df3.copy()\n\nIn [433]: dfa[\"A\"] = dfa[\"A\"].astype(\"float32\")\n\nIn [434]: dfa.dtypes\nOut[434]: \nA    float32\nB    float64\nC    float64\ndtype: object\n\nIn [435]: casted = dfa[df2 > 0]\n\nIn [436]: casted\nOut[436]: \n          A         B      C\n0  1.047606  0.256090    NaN\n1  3.497968  1.426469    NaN\n2       NaN       NaN  255.0\n3       NaN  1.139976    NaN\n4       NaN       NaN    NaN\n5  1.346426  0.096706    NaN\n6       NaN       NaN    1.0\n7       NaN       NaN    NaN\n\nIn [437]: casted.dtypes\nOut[437]: \nA    float32\nB    float64\nC    float64\ndtype: object\n     Selecting columns based on dtype\n The select_dtypes() method implements subsetting of columns based on their dtype. First, let\u2019s create a DataFrame with a slew of different dtypes: \nIn [438]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"string\": list(\"abc\"),\n   .....:         \"int64\": list(range(1, 4)),\n   .....:         \"uint8\": np.arange(3, 6).astype(\"u1\"),\n   .....:         \"float64\": np.arange(4.0, 7.0),\n   .....:         \"bool1\": [True, False, True],\n   .....:         \"bool2\": [False, True, False],\n   .....:         \"dates\": pd.date_range(\"now\", periods=3),\n   .....:         \"category\": pd.Series(list(\"ABC\")).astype(\"category\"),\n   .....:     }\n   .....: )\n   .....: \n\nIn [439]: df[\"tdeltas\"] = df.dates.diff()\n\nIn [440]: df[\"uint64\"] = np.arange(3, 6).astype(\"u8\")\n\nIn [441]: df[\"other_dates\"] = pd.date_range(\"20130101\", periods=3)\n\nIn [442]: df[\"tz_aware_dates\"] = pd.date_range(\"20130101\", periods=3, tz=\"US/Eastern\")\n\nIn [443]: df\nOut[443]: \n  string  int64  uint8  float64  bool1  bool2                      dates category tdeltas  uint64 other_dates            tz_aware_dates\n0      a      1      3      4.0   True  False 2022-01-22 10:50:03.741897        A     NaT       3  2013-01-01 2013-01-01 00:00:00-05:00\n1      b      2      4      5.0  False   True 2022-01-23 10:50:03.741897        B  1 days       4  2013-01-02 2013-01-02 00:00:00-05:00\n2      c      3      5      6.0   True  False 2022-01-24 10:50:03.741897        C  1 days       5  2013-01-03 2013-01-03 00:00:00-05:00\n  And the dtypes: \nIn [444]: df.dtypes\nOut[444]: \nstring                                object\nint64                                  int64\nuint8                                  uint8\nfloat64                              float64\nbool1                                   bool\nbool2                                   bool\ndates                         datetime64[ns]\ncategory                            category\ntdeltas                      timedelta64[ns]\nuint64                                uint64\nother_dates                   datetime64[ns]\ntz_aware_dates    datetime64[ns, US/Eastern]\ndtype: object\n  select_dtypes() has two parameters include and exclude that allow you to say \u201cgive me the columns with these dtypes\u201d (include) and/or \u201cgive the columns without these dtypes\u201d (exclude). For example, to select bool columns: \nIn [445]: df.select_dtypes(include=[bool])\nOut[445]: \n   bool1  bool2\n0   True  False\n1  False   True\n2   True  False\n  You can also pass the name of a dtype in the NumPy dtype hierarchy: \nIn [446]: df.select_dtypes(include=[\"bool\"])\nOut[446]: \n   bool1  bool2\n0   True  False\n1  False   True\n2   True  False\n  select_dtypes() also works with generic dtypes as well. For example, to select all numeric and boolean columns while excluding unsigned integers: \nIn [447]: df.select_dtypes(include=[\"number\", \"bool\"], exclude=[\"unsignedinteger\"])\nOut[447]: \n   int64  float64  bool1  bool2 tdeltas\n0      1      4.0   True  False     NaT\n1      2      5.0  False   True  1 days\n2      3      6.0   True  False  1 days\n  To select string columns you must use the object dtype: \nIn [448]: df.select_dtypes(include=[\"object\"])\nOut[448]: \n  string\n0      a\n1      b\n2      c\n  To see all the child dtypes of a generic dtype like numpy.number you can define a function that returns a tree of child dtypes: \nIn [449]: def subdtypes(dtype):\n   .....:     subs = dtype.__subclasses__()\n   .....:     if not subs:\n   .....:         return dtype\n   .....:     return [dtype, [subdtypes(dt) for dt in subs]]\n   .....: \n  All NumPy dtypes are subclasses of numpy.generic: \nIn [450]: subdtypes(np.generic)\nOut[450]: \n[numpy.generic,\n [[numpy.number,\n   [[numpy.integer,\n     [[numpy.signedinteger,\n       [numpy.int8,\n        numpy.int16,\n        numpy.int32,\n        numpy.int64,\n        numpy.longlong,\n        numpy.timedelta64]],\n      [numpy.unsignedinteger,\n       [numpy.uint8,\n        numpy.uint16,\n        numpy.uint32,\n        numpy.uint64,\n        numpy.ulonglong]]]],\n    [numpy.inexact,\n     [[numpy.floating,\n       [numpy.float16, numpy.float32, numpy.float64, numpy.float128]],\n      [numpy.complexfloating,\n       [numpy.complex64, numpy.complex128, numpy.complex256]]]]]],\n  [numpy.flexible,\n   [[numpy.character, [numpy.bytes_, numpy.str_]],\n    [numpy.void, [numpy.record]]]],\n  numpy.bool_,\n  numpy.datetime64,\n  numpy.object_]]\n   Note pandas also defines the types category, and datetime64[ns, tz], which are not integrated into the normal NumPy hierarchy and won\u2019t show up with the above function.  \n"}, {"name": "Extensions", "path": "reference/extensions", "type": "Extensions", "text": "Extensions These are primarily intended for library authors looking to extend pandas objects.       \napi.extensions.register_extension_dtype(cls) Register an ExtensionType with pandas as class decorator.  \napi.extensions.register_dataframe_accessor(name) Register a custom accessor on DataFrame objects.  \napi.extensions.register_series_accessor(name) Register a custom accessor on Series objects.  \napi.extensions.register_index_accessor(name) Register a custom accessor on Index objects.  \napi.extensions.ExtensionDtype() A custom data type, to be paired with an ExtensionArray.          \napi.extensions.ExtensionArray() Abstract base class for custom 1-D array types.  \narrays.PandasArray(values[, copy]) A pandas ExtensionArray for NumPy data.    Additionally, we have some utility methods for ensuring your object behaves correctly.       \napi.indexers.check_array_indexer(array, indexer) Check if indexer is a valid array indexer for array.    The sentinel pandas.api.extensions.no_default is used as the default value in some methods. Use an is comparison to check if the user provides a non-default value.\n"}, {"name": "Frequently Asked Questions (FAQ)", "path": "user_guide/gotchas", "type": "Manual", "text": "Frequently Asked Questions (FAQ)  DataFrame memory usage The memory usage of a DataFrame (including the index) is shown when calling the info(). A configuration option, display.memory_usage (see the list of options), specifies if the DataFrame\u2019s memory usage will be displayed when invoking the df.info() method. For example, the memory usage of the DataFrame below is shown when calling info(): \nIn [1]: dtypes = [\n   ...:     \"int64\",\n   ...:     \"float64\",\n   ...:     \"datetime64[ns]\",\n   ...:     \"timedelta64[ns]\",\n   ...:     \"complex128\",\n   ...:     \"object\",\n   ...:     \"bool\",\n   ...: ]\n   ...: \n\nIn [2]: n = 5000\n\nIn [3]: data = {t: np.random.randint(100, size=n).astype(t) for t in dtypes}\n\nIn [4]: df = pd.DataFrame(data)\n\nIn [5]: df[\"categorical\"] = df[\"object\"].astype(\"category\")\n\nIn [6]: df.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5000 entries, 0 to 4999\nData columns (total 8 columns):\n #   Column           Non-Null Count  Dtype          \n---  ------           --------------  -----          \n 0   int64            5000 non-null   int64          \n 1   float64          5000 non-null   float64        \n 2   datetime64[ns]   5000 non-null   datetime64[ns] \n 3   timedelta64[ns]  5000 non-null   timedelta64[ns]\n 4   complex128       5000 non-null   complex128     \n 5   object           5000 non-null   object         \n 6   bool             5000 non-null   bool           \n 7   categorical      5000 non-null   category       \ndtypes: bool(1), category(1), complex128(1), datetime64[ns](1), float64(1), int64(1), object(1), timedelta64[ns](1)\nmemory usage: 288.2+ KB\n  The + symbol indicates that the true memory usage could be higher, because pandas does not count the memory used by values in columns with dtype=object. Passing memory_usage='deep' will enable a more accurate memory usage report, accounting for the full usage of the contained objects. This is optional as it can be expensive to do this deeper introspection. \nIn [7]: df.info(memory_usage=\"deep\")\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5000 entries, 0 to 4999\nData columns (total 8 columns):\n #   Column           Non-Null Count  Dtype          \n---  ------           --------------  -----          \n 0   int64            5000 non-null   int64          \n 1   float64          5000 non-null   float64        \n 2   datetime64[ns]   5000 non-null   datetime64[ns] \n 3   timedelta64[ns]  5000 non-null   timedelta64[ns]\n 4   complex128       5000 non-null   complex128     \n 5   object           5000 non-null   object         \n 6   bool             5000 non-null   bool           \n 7   categorical      5000 non-null   category       \ndtypes: bool(1), category(1), complex128(1), datetime64[ns](1), float64(1), int64(1), object(1), timedelta64[ns](1)\nmemory usage: 424.7 KB\n  By default the display option is set to True but can be explicitly overridden by passing the memory_usage argument when invoking df.info(). The memory usage of each column can be found by calling the memory_usage() method. This returns a Series with an index represented by column names and memory usage of each column shown in bytes. For the DataFrame above, the memory usage of each column and the total memory usage can be found with the memory_usage method: \nIn [8]: df.memory_usage()\nOut[8]: \nIndex                128\nint64              40000\nfloat64            40000\ndatetime64[ns]     40000\ntimedelta64[ns]    40000\ncomplex128         80000\nobject             40000\nbool                5000\ncategorical         9968\ndtype: int64\n\n# total memory usage of dataframe\nIn [9]: df.memory_usage().sum()\nOut[9]: 295096\n  By default the memory usage of the DataFrame\u2019s index is shown in the returned Series, the memory usage of the index can be suppressed by passing the index=False argument: \nIn [10]: df.memory_usage(index=False)\nOut[10]: \nint64              40000\nfloat64            40000\ndatetime64[ns]     40000\ntimedelta64[ns]    40000\ncomplex128         80000\nobject             40000\nbool                5000\ncategorical         9968\ndtype: int64\n  The memory usage displayed by the info() method utilizes the memory_usage() method to determine the memory usage of a DataFrame while also formatting the output in human-readable units (base-2 representation; i.e. 1KB = 1024 bytes). See also Categorical Memory Usage.   Using if/truth statements with pandas pandas follows the NumPy convention of raising an error when you try to convert something to a bool. This happens in an if-statement or when using the boolean operations: and, or, and not. It is not clear what the result of the following code should be: \n>>> if pd.Series([False, True, False]):\n...     pass\n  Should it be True because it\u2019s not zero-length, or False because there are False values? It is unclear, so instead, pandas raises a ValueError: \n>>> if pd.Series([False, True, False]):\n...     print(\"I was true\")\nTraceback\n    ...\nValueError: The truth value of an array is ambiguous. Use a.empty, a.any() or a.all().\n  You need to explicitly choose what you want to do with the DataFrame, e.g. use any(), all() or empty(). Alternatively, you might want to compare if the pandas object is None: \n>>> if pd.Series([False, True, False]) is not None:\n...     print(\"I was not None\")\nI was not None\n  Below is how to check if any of the values are True: \n>>> if pd.Series([False, True, False]).any():\n...     print(\"I am any\")\nI am any\n  To evaluate single-element pandas objects in a boolean context, use the method bool(): \nIn [11]: pd.Series([True]).bool()\nOut[11]: True\n\nIn [12]: pd.Series([False]).bool()\nOut[12]: False\n\nIn [13]: pd.DataFrame([[True]]).bool()\nOut[13]: True\n\nIn [14]: pd.DataFrame([[False]]).bool()\nOut[14]: False\n   Bitwise boolean Bitwise boolean operators like == and != return a boolean Series, which is almost always what you want anyways. \n>>> s = pd.Series(range(5))\n>>> s == 4\n0    False\n1    False\n2    False\n3    False\n4     True\ndtype: bool\n  See boolean comparisons for more examples.   Using the in operator Using the Python in operator on a Series tests for membership in the index, not membership among the values. \nIn [15]: s = pd.Series(range(5), index=list(\"abcde\"))\n\nIn [16]: 2 in s\nOut[16]: False\n\nIn [17]: 'b' in s\nOut[17]: True\n  If this behavior is surprising, keep in mind that using in on a Python dictionary tests keys, not values, and Series are dict-like. To test for membership in the values, use the method isin(): \nIn [18]: s.isin([2])\nOut[18]: \na    False\nb    False\nc     True\nd    False\ne    False\ndtype: bool\n\nIn [19]: s.isin([2]).any()\nOut[19]: True\n  For DataFrames, likewise, in applies to the column axis, testing for membership in the list of column names.    Mutating with User Defined Function (UDF) methods This section applies to pandas methods that take a UDF. In particular, the methods .apply, .aggregate, .transform, and .filter. It is a general rule in programming that one should not mutate a container while it is being iterated over. Mutation will invalidate the iterator, causing unexpected behavior. Consider the example: \nIn [20]: values = [0, 1, 2, 3, 4, 5]\n\nIn [21]: n_removed = 0\n\nIn [22]: for k, value in enumerate(values):\n   ....:     idx = k - n_removed\n   ....:     if value % 2 == 1:\n   ....:         del values[idx]\n   ....:         n_removed += 1\n   ....:     else:\n   ....:         values[idx] = value + 1\n   ....: \n\nIn [23]: values\nOut[23]: [1, 4, 5]\n  One probably would have expected that the result would be [1, 3, 5]. When using a pandas method that takes a UDF, internally pandas is often iterating over the DataFrame or other pandas object. Therefore, if the UDF mutates (changes) the DataFrame, unexpected behavior can arise. Here is a similar example with DataFrame.apply(): \nIn [24]: def f(s):\n   ....:     s.pop(\"a\")\n   ....:     return s\n   ....: \n\nIn [25]: df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n\nIn [26]: try:\n   ....:     df.apply(f, axis=\"columns\")\n   ....: except Exception as err:\n   ....:     print(repr(err))\n   ....: \nKeyError('a')\n  To resolve this issue, one can make a copy so that the mutation does not apply to the container being iterated over. \nIn [27]: values = [0, 1, 2, 3, 4, 5]\n\nIn [28]: n_removed = 0\n\nIn [29]: for k, value in enumerate(values.copy()):\n   ....:     idx = k - n_removed\n   ....:     if value % 2 == 1:\n   ....:         del values[idx]\n   ....:         n_removed += 1\n   ....:     else:\n   ....:         values[idx] = value + 1\n   ....: \n\nIn [30]: values\nOut[30]: [1, 3, 5]\n  \nIn [31]: def f(s):\n   ....:     s = s.copy()\n   ....:     s.pop(\"a\")\n   ....:     return s\n   ....: \n\nIn [32]: df = pd.DataFrame({\"a\": [1, 2, 3], 'b': [4, 5, 6]})\n\nIn [33]: df.apply(f, axis=\"columns\")\nOut[33]: \n   b\n0  4\n1  5\n2  6\n    \nNaN, Integer NA values and NA type promotions  Choice of NA representation For lack of NA (missing) support from the ground up in NumPy and Python in general, we were given the difficult choice between either:  A masked array solution: an array of data and an array of boolean values indicating whether a value is there or is missing. Using a special sentinel value, bit pattern, or set of sentinel values to denote NA across the dtypes.  For many reasons we chose the latter. After years of production use it has proven, at least in my opinion, to be the best decision given the state of affairs in NumPy and Python in general. The special value NaN (Not-A-Number) is used everywhere as the NA value, and there are API functions isna and notna which can be used across the dtypes to detect NA values. However, it comes with it a couple of trade-offs which I most certainly have not ignored.   Support for integer NA\n In the absence of high performance NA support being built into NumPy from the ground up, the primary casualty is the ability to represent NAs in integer arrays. For example: \nIn [34]: s = pd.Series([1, 2, 3, 4, 5], index=list(\"abcde\"))\n\nIn [35]: s\nOut[35]: \na    1\nb    2\nc    3\nd    4\ne    5\ndtype: int64\n\nIn [36]: s.dtype\nOut[36]: dtype('int64')\n\nIn [37]: s2 = s.reindex([\"a\", \"b\", \"c\", \"f\", \"u\"])\n\nIn [38]: s2\nOut[38]: \na    1.0\nb    2.0\nc    3.0\nf    NaN\nu    NaN\ndtype: float64\n\nIn [39]: s2.dtype\nOut[39]: dtype('float64')\n  This trade-off is made largely for memory and performance reasons, and also so that the resulting Series continues to be \u201cnumeric\u201d. If you need to represent integers with possibly missing values, use one of the nullable-integer extension dtypes provided by pandas  Int8Dtype Int16Dtype Int32Dtype Int64Dtype  \nIn [40]: s_int = pd.Series([1, 2, 3, 4, 5], index=list(\"abcde\"), dtype=pd.Int64Dtype())\n\nIn [41]: s_int\nOut[41]: \na    1\nb    2\nc    3\nd    4\ne    5\ndtype: Int64\n\nIn [42]: s_int.dtype\nOut[42]: Int64Dtype()\n\nIn [43]: s2_int = s_int.reindex([\"a\", \"b\", \"c\", \"f\", \"u\"])\n\nIn [44]: s2_int\nOut[44]: \na       1\nb       2\nc       3\nf    <NA>\nu    <NA>\ndtype: Int64\n\nIn [45]: s2_int.dtype\nOut[45]: Int64Dtype()\n  See Nullable integer data type for more.   \nNA type promotions When introducing NAs into an existing Series or DataFrame via reindex() or some other means, boolean and integer types will be promoted to a different dtype in order to store the NAs. The promotions are summarized in this table:       \nTypeclass Promotion dtype for storing NAs    \nfloating no change  \nobject no change  \ninteger cast to float64  \nboolean cast to object    While this may seem like a heavy trade-off, I have found very few cases where this is an issue in practice i.e. storing values greater than 2**53. Some explanation for the motivation is in the next section.   Why not make NumPy like R? Many people have suggested that NumPy should simply emulate the NA support present in the more domain-specific statistical programming language R. Part of the reason is the NumPy type hierarchy:       \nTypeclass Dtypes    \nnumpy.floating float16, float32, float64, float128  \nnumpy.integer int8, int16, int32, int64  \nnumpy.unsignedinteger uint8, uint16, uint32, uint64  \nnumpy.object_ object_  \nnumpy.bool_ bool_  \nnumpy.character string_, unicode_    The R language, by contrast, only has a handful of built-in data types: integer, numeric (floating-point), character, and boolean. NA types are implemented by reserving special bit patterns for each type to be used as the missing value. While doing this with the full NumPy type hierarchy would be possible, it would be a more substantial trade-off (especially for the 8- and 16-bit data types) and implementation undertaking. An alternate approach is that of using masked arrays. A masked array is an array of data with an associated boolean mask denoting whether each value should be considered NA or not. I am personally not in love with this approach as I feel that overall it places a fairly heavy burden on the user and the library implementer. Additionally, it exacts a fairly high performance cost when working with numerical data compared with the simple approach of using NaN. Thus, I have chosen the Pythonic \u201cpracticality beats purity\u201d approach and traded integer NA capability for a much simpler approach of using a special value in float and object arrays to denote NA, and promoting integer arrays to floating when NAs must be introduced.    Differences with NumPy For Series and DataFrame objects, var() normalizes by N-1 to produce unbiased estimates of the sample variance, while NumPy\u2019s var normalizes by N, which measures the variance of the sample. Note that cov() normalizes by N-1 in both pandas and NumPy.   Thread-safety As of pandas 0.11, pandas is not 100% thread safe. The known issues relate to the copy() method. If you are doing a lot of copying of DataFrame objects shared among threads, we recommend holding locks inside the threads where the data copying occurs. See this link for more information.   Byte-ordering issues Occasionally you may have to deal with data that were created on a machine with a different byte order than the one on which you are running Python. A common symptom of this issue is an error like: \nTraceback\n    ...\nValueError: Big-endian buffer not supported on little-endian compiler\n  To deal with this issue you should convert the underlying NumPy array to the native system byte order before passing it to Series or DataFrame constructors using something similar to the following: \nIn [46]: x = np.array(list(range(10)), \">i4\")  # big endian\n\nIn [47]: newx = x.byteswap().newbyteorder()  # force native byteorder\n\nIn [48]: s = pd.Series(newx)\n  See the NumPy documentation on byte order for more details. \n"}, {"name": "General functions", "path": "reference/general_functions", "type": "Input/output", "text": "General functions  Data manipulations       \nmelt(frame[, id_vars, value_vars, var_name, ...]) Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.  \npivot(data[, index, columns, values]) Return reshaped DataFrame organized by given index / column values.  \npivot_table(data[, values, index, columns, ...]) Create a spreadsheet-style pivot table as a DataFrame.  \ncrosstab(index, columns[, values, rownames, ...]) Compute a simple cross tabulation of two (or more) factors.  \ncut(x, bins[, right, labels, retbins, ...]) Bin values into discrete intervals.  \nqcut(x, q[, labels, retbins, precision, ...]) Quantile-based discretization function.  \nmerge(left, right[, how, on, left_on, ...]) Merge DataFrame or named Series objects with a database-style join.  \nmerge_ordered(left, right[, on, left_on, ...]) Perform a merge for ordered data with optional filling/interpolation.  \nmerge_asof(left, right[, on, left_on, ...]) Perform a merge by key distance.  \nconcat(objs[, axis, join, ignore_index, ...]) Concatenate pandas objects along a particular axis with optional set logic along the other axes.  \nget_dummies(data[, prefix, prefix_sep, ...]) Convert categorical variable into dummy/indicator variables.  \nfactorize(values[, sort, na_sentinel, size_hint]) Encode the object as an enumerated type or categorical variable.  \nunique(values) Return unique values based on a hash table.  \nwide_to_long(df, stubnames, i, j[, sep, suffix]) Unpivot a DataFrame from wide to long format.      Top-level missing data       \nisna(obj) Detect missing values for an array-like object.  \nisnull(obj) Detect missing values for an array-like object.  \nnotna(obj) Detect non-missing values for an array-like object.  \nnotnull(obj) Detect non-missing values for an array-like object.      Top-level dealing with numeric data       \nto_numeric(arg[, errors, downcast]) Convert argument to a numeric type.      Top-level dealing with datetimelike data       \nto_datetime(arg[, errors, dayfirst, ...]) Convert argument to datetime.  \nto_timedelta(arg[, unit, errors]) Convert argument to timedelta.  \ndate_range([start, end, periods, freq, tz, ...]) Return a fixed frequency DatetimeIndex.  \nbdate_range([start, end, periods, freq, tz, ...]) Return a fixed frequency DatetimeIndex, with business day as the default frequency.  \nperiod_range([start, end, periods, freq, name]) Return a fixed frequency PeriodIndex.  \ntimedelta_range([start, end, periods, freq, ...]) Return a fixed frequency TimedeltaIndex, with day as the default frequency.  \ninfer_freq(index[, warn]) Infer the most likely frequency given the input index.      Top-level dealing with Interval data       \ninterval_range([start, end, periods, freq, ...]) Return a fixed frequency IntervalIndex.      Top-level evaluation       \neval(expr[, parser, engine, truediv, ...]) Evaluate a Python expression as a string using various backends.      Hashing       \nutil.hash_array(vals[, encoding, hash_key, ...]) Given a 1d array, return an array of deterministic integers.  \nutil.hash_pandas_object(obj[, index, ...]) Return a data hash of the Index/Series/DataFrame.      Testing       \ntest([extra_args]) Run the pandas test suite using pytest.    \n"}, {"name": "General utility functions", "path": "reference/general_utility_functions", "type": "Input/output", "text": "General utility functions  Working with options       \ndescribe_option(pat[, _print_desc]) Prints the description for one or more registered options.  \nreset_option(pat) Reset one or more options to their default value.  \nget_option(pat) Retrieves the value of the specified option.  \nset_option(pat, value) Sets the value of the specified option.  \noption_context(*args) Context manager to temporarily set options in the with statement context.      Testing functions       \ntesting.assert_frame_equal(left, right[, ...]) Check that left and right DataFrame are equal.  \ntesting.assert_series_equal(left, right[, ...]) Check that left and right Series are equal.  \ntesting.assert_index_equal(left, right[, ...]) Check that left and right Index are equal.  \ntesting.assert_extension_array_equal(left, right) Check that left and right ExtensionArrays are equal.      Exceptions and warnings       \nerrors.AbstractMethodError(class_instance[, ...]) Raise this error instead of NotImplementedError for abstract methods while keeping compatibility with Python 2 and Python 3.  \nerrors.AccessorRegistrationWarning Warning for attribute conflicts in accessor registration.  \nerrors.DtypeWarning Warning raised when reading different dtypes in a column from a file.  \nerrors.DuplicateLabelError Error raised when an operation would introduce duplicate labels.  \nerrors.EmptyDataError Exception that is thrown in pd.read_csv (by both the C and Python engines) when empty data or header is encountered.  \nerrors.InvalidIndexError Exception raised when attempting to use an invalid index key.  \nerrors.IntCastingNaNError Raised when attempting an astype operation on an array with NaN to an integer dtype.  \nerrors.MergeError Error raised when problems arise during merging due to problems with input data.  \nerrors.NullFrequencyError Error raised when a null freq attribute is used in an operation that needs a non-null frequency, particularly DatetimeIndex.shift, TimedeltaIndex.shift, PeriodIndex.shift.  \nerrors.NumbaUtilError Error raised for unsupported Numba engine routines.  \nerrors.OptionError Exception for pandas.options, backwards compatible with KeyError checks.  \nerrors.OutOfBoundsDatetime   \nerrors.OutOfBoundsTimedelta Raised when encountering a timedelta value that cannot be represented as a timedelta64[ns].  \nerrors.ParserError Exception that is raised by an error encountered in parsing file contents.  \nerrors.ParserWarning Warning raised when reading a file that doesn't use the default 'c' parser.  \nerrors.PerformanceWarning Warning raised when there is a possible performance impact.  \nerrors.UnsortedIndexError Error raised when attempting to get a slice of a MultiIndex, and the index has not been lexsorted.  \nerrors.UnsupportedFunctionCall Exception raised when attempting to call a numpy function on a pandas object, but that function is not supported by the object e.g.      Data types related functionality       \napi.types.union_categoricals(to_union[, ...]) Combine list-like of Categorical-like, unioning categories.  \napi.types.infer_dtype Efficiently infer the type of a passed val, or list-like array of values.  \napi.types.pandas_dtype(dtype) Convert input into a pandas only dtype object or a numpy dtype object.     Dtype introspection       \napi.types.is_bool_dtype(arr_or_dtype) Check whether the provided array or dtype is of a boolean dtype.  \napi.types.is_categorical_dtype(arr_or_dtype) Check whether an array-like or dtype is of the Categorical dtype.  \napi.types.is_complex_dtype(arr_or_dtype) Check whether the provided array or dtype is of a complex dtype.  \napi.types.is_datetime64_any_dtype(arr_or_dtype) Check whether the provided array or dtype is of the datetime64 dtype.  \napi.types.is_datetime64_dtype(arr_or_dtype) Check whether an array-like or dtype is of the datetime64 dtype.  \napi.types.is_datetime64_ns_dtype(arr_or_dtype) Check whether the provided array or dtype is of the datetime64[ns] dtype.  \napi.types.is_datetime64tz_dtype(arr_or_dtype) Check whether an array-like or dtype is of a DatetimeTZDtype dtype.  \napi.types.is_extension_type(arr) (DEPRECATED) Check whether an array-like is of a pandas extension class instance.  \napi.types.is_extension_array_dtype(arr_or_dtype) Check if an object is a pandas extension array type.  \napi.types.is_float_dtype(arr_or_dtype) Check whether the provided array or dtype is of a float dtype.  \napi.types.is_int64_dtype(arr_or_dtype) Check whether the provided array or dtype is of the int64 dtype.  \napi.types.is_integer_dtype(arr_or_dtype) Check whether the provided array or dtype is of an integer dtype.  \napi.types.is_interval_dtype(arr_or_dtype) Check whether an array-like or dtype is of the Interval dtype.  \napi.types.is_numeric_dtype(arr_or_dtype) Check whether the provided array or dtype is of a numeric dtype.  \napi.types.is_object_dtype(arr_or_dtype) Check whether an array-like or dtype is of the object dtype.  \napi.types.is_period_dtype(arr_or_dtype) Check whether an array-like or dtype is of the Period dtype.  \napi.types.is_signed_integer_dtype(arr_or_dtype) Check whether the provided array or dtype is of a signed integer dtype.  \napi.types.is_string_dtype(arr_or_dtype) Check whether the provided array or dtype is of the string dtype.  \napi.types.is_timedelta64_dtype(arr_or_dtype) Check whether an array-like or dtype is of the timedelta64 dtype.  \napi.types.is_timedelta64_ns_dtype(arr_or_dtype) Check whether the provided array or dtype is of the timedelta64[ns] dtype.  \napi.types.is_unsigned_integer_dtype(arr_or_dtype) Check whether the provided array or dtype is of an unsigned integer dtype.  \napi.types.is_sparse(arr) Check whether an array-like is a 1-D pandas sparse array.      Iterable introspection       \napi.types.is_dict_like(obj) Check if the object is dict-like.  \napi.types.is_file_like(obj) Check if the object is a file-like object.  \napi.types.is_list_like Check if the object is list-like.  \napi.types.is_named_tuple(obj) Check if the object is a named tuple.  \napi.types.is_iterator Check if the object is an iterator.      Scalar introspection       \napi.types.is_bool Return True if given object is boolean.  \napi.types.is_categorical(arr) Check whether an array-like is a Categorical instance.  \napi.types.is_complex Return True if given object is complex.  \napi.types.is_float Return True if given object is float.  \napi.types.is_hashable(obj) Return True if hash(obj) will succeed, False otherwise.  \napi.types.is_integer Return True if given object is integer.  \napi.types.is_interval   \napi.types.is_number(obj) Check if the object is a number.  \napi.types.is_re(obj) Check if the object is a regex pattern instance.  \napi.types.is_re_compilable(obj) Check if the object can be compiled into a regex pattern instance.  \napi.types.is_scalar Return True if given object is scalar.       Bug report function       \nshow_versions([as_json]) Provide useful information, important for bug reports.    \n"}, {"name": "Group by: split-apply-combine", "path": "user_guide/groupby", "type": "Manual", "text": "Group by: split-apply-combine By \u201cgroup by\u201d we are referring to a process involving one or more of the following steps:  Splitting the data into groups based on some criteria. Applying a function to each group independently. Combining the results into a data structure.  Out of these, the split step is the most straightforward. In fact, in many situations we may wish to split the data set into groups and do something with those groups. In the apply step, we might wish to do one of the following:  \nAggregation: compute a summary statistic (or statistics) for each group. Some examples:  \n Compute group sums or means. Compute group sizes / counts.  \n  \nTransformation: perform some group-specific computations and return a like-indexed object. Some examples:  \n Standardize data (zscore) within a group. Filling NAs within groups with a value derived from each group.  \n  \nFiltration: discard some groups, according to a group-wise computation that evaluates True or False. Some examples:  \n Discard data that belongs to groups with only a few members. Filter out data based on the group sum or mean.  \n  Some combination of the above: GroupBy will examine the results of the apply step and try to return a sensibly combined result if it doesn\u2019t fit into either of the above two categories.  Since the set of object instance methods on pandas data structures are generally rich and expressive, we often simply want to invoke, say, a DataFrame function on each group. The name GroupBy should be quite familiar to those who have used a SQL-based tool (or itertools), in which you can write code like: \nSELECT Column1, Column2, mean(Column3), sum(Column4)\nFROM SomeTable\nGROUP BY Column1, Column2\n  We aim to make operations like this natural and easy to express using pandas. We\u2019ll address each area of GroupBy functionality then provide some non-trivial examples / use cases. See the cookbook for some advanced strategies.  Splitting an object into groups pandas objects can be split on any of their axes. The abstract definition of grouping is to provide a mapping of labels to group names. To create a GroupBy object (more on what the GroupBy object is later), you may do the following: \nIn [1]: df = pd.DataFrame(\n   ...:     [\n   ...:         (\"bird\", \"Falconiformes\", 389.0),\n   ...:         (\"bird\", \"Psittaciformes\", 24.0),\n   ...:         (\"mammal\", \"Carnivora\", 80.2),\n   ...:         (\"mammal\", \"Primates\", np.nan),\n   ...:         (\"mammal\", \"Carnivora\", 58),\n   ...:     ],\n   ...:     index=[\"falcon\", \"parrot\", \"lion\", \"monkey\", \"leopard\"],\n   ...:     columns=(\"class\", \"order\", \"max_speed\"),\n   ...: )\n   ...: \n\nIn [2]: df\nOut[2]: \n          class           order  max_speed\nfalcon     bird   Falconiformes      389.0\nparrot     bird  Psittaciformes       24.0\nlion     mammal       Carnivora       80.2\nmonkey   mammal        Primates        NaN\nleopard  mammal       Carnivora       58.0\n\n# default is axis=0\nIn [3]: grouped = df.groupby(\"class\")\n\nIn [4]: grouped = df.groupby(\"order\", axis=\"columns\")\n\nIn [5]: grouped = df.groupby([\"class\", \"order\"])\n  The mapping can be specified many different ways:  A Python function, to be called on each of the axis labels. A list or NumPy array of the same length as the selected axis. A dict or Series, providing a label -> group name mapping. For DataFrame objects, a string indicating either a column name or an index level name to be used to group. df.groupby('A') is just syntactic sugar for df.groupby(df['A']). A list of any of the above things.  Collectively we refer to the grouping objects as the keys. For example, consider the following DataFrame:  Note A string passed to groupby may refer to either a column or an index level. If a string matches both a column name and an index level name, a ValueError will be raised.  \nIn [6]: df = pd.DataFrame(\n   ...:     {\n   ...:         \"A\": [\"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"foo\"],\n   ...:         \"B\": [\"one\", \"one\", \"two\", \"three\", \"two\", \"two\", \"one\", \"three\"],\n   ...:         \"C\": np.random.randn(8),\n   ...:         \"D\": np.random.randn(8),\n   ...:     }\n   ...: )\n   ...: \n\nIn [7]: df\nOut[7]: \n     A      B         C         D\n0  foo    one  0.469112 -0.861849\n1  bar    one -0.282863 -2.104569\n2  foo    two -1.509059 -0.494929\n3  bar  three -1.135632  1.071804\n4  foo    two  1.212112  0.721555\n5  bar    two -0.173215 -0.706771\n6  foo    one  0.119209 -1.039575\n7  foo  three -1.044236  0.271860\n  On a DataFrame, we obtain a GroupBy object by calling groupby(). We could naturally group by either the A or B columns, or both: \nIn [8]: grouped = df.groupby(\"A\")\n\nIn [9]: grouped = df.groupby([\"A\", \"B\"])\n  If we also have a MultiIndex on columns A and B, we can group by all but the specified columns \nIn [10]: df2 = df.set_index([\"A\", \"B\"])\n\nIn [11]: grouped = df2.groupby(level=df2.index.names.difference([\"B\"]))\n\nIn [12]: grouped.sum()\nOut[12]: \n            C         D\nA                      \nbar -1.591710 -1.739537\nfoo -0.752861 -1.402938\n  These will split the DataFrame on its index (rows). We could also split by the columns: \nIn [13]: def get_letter_type(letter):\n   ....:     if letter.lower() in 'aeiou':\n   ....:         return 'vowel'\n   ....:     else:\n   ....:         return 'consonant'\n   ....: \n\nIn [14]: grouped = df.groupby(get_letter_type, axis=1)\n  pandas Index objects support duplicate values. If a non-unique index is used as the group key in a groupby operation, all values for the same index value will be considered to be in one group and thus the output of aggregation functions will only contain unique index values: \nIn [15]: lst = [1, 2, 3, 1, 2, 3]\n\nIn [16]: s = pd.Series([1, 2, 3, 10, 20, 30], lst)\n\nIn [17]: grouped = s.groupby(level=0)\n\nIn [18]: grouped.first()\nOut[18]: \n1    1\n2    2\n3    3\ndtype: int64\n\nIn [19]: grouped.last()\nOut[19]: \n1    10\n2    20\n3    30\ndtype: int64\n\nIn [20]: grouped.sum()\nOut[20]: \n1    11\n2    22\n3    33\ndtype: int64\n  Note that no splitting occurs until it\u2019s needed. Creating the GroupBy object only verifies that you\u2019ve passed a valid mapping.  Note Many kinds of complicated data manipulations can be expressed in terms of GroupBy operations (though can\u2019t be guaranteed to be the most efficient). You can get quite creative with the label mapping functions.   GroupBy sorting By default the group keys are sorted during the groupby operation. You may however pass sort=False for potential speedups: \nIn [21]: df2 = pd.DataFrame({\"X\": [\"B\", \"B\", \"A\", \"A\"], \"Y\": [1, 2, 3, 4]})\n\nIn [22]: df2.groupby([\"X\"]).sum()\nOut[22]: \n   Y\nX   \nA  7\nB  3\n\nIn [23]: df2.groupby([\"X\"], sort=False).sum()\nOut[23]: \n   Y\nX   \nB  3\nA  7\n  Note that groupby will preserve the order in which observations are sorted within each group. For example, the groups created by groupby() below are in the order they appeared in the original DataFrame: \nIn [24]: df3 = pd.DataFrame({\"X\": [\"A\", \"B\", \"A\", \"B\"], \"Y\": [1, 4, 3, 2]})\n\nIn [25]: df3.groupby([\"X\"]).get_group(\"A\")\nOut[25]: \n   X  Y\n0  A  1\n2  A  3\n\nIn [26]: df3.groupby([\"X\"]).get_group(\"B\")\nOut[26]: \n   X  Y\n1  B  4\n3  B  2\n   New in version 1.1.0.   GroupBy dropna By default NA values are excluded from group keys during the groupby operation. However, in case you want to include NA values in group keys, you could pass dropna=False to achieve it. \nIn [27]: df_list = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]\n\nIn [28]: df_dropna = pd.DataFrame(df_list, columns=[\"a\", \"b\", \"c\"])\n\nIn [29]: df_dropna\nOut[29]: \n   a    b  c\n0  1  2.0  3\n1  1  NaN  4\n2  2  1.0  3\n3  1  2.0  2\n  \n# Default ``dropna`` is set to True, which will exclude NaNs in keys\nIn [30]: df_dropna.groupby(by=[\"b\"], dropna=True).sum()\nOut[30]: \n     a  c\nb        \n1.0  2  3\n2.0  2  5\n\n# In order to allow NaN in keys, set ``dropna`` to False\nIn [31]: df_dropna.groupby(by=[\"b\"], dropna=False).sum()\nOut[31]: \n     a  c\nb        \n1.0  2  3\n2.0  2  5\nNaN  1  4\n  The default setting of dropna argument is True which means NA are not included in group keys.    GroupBy object attributes The groups attribute is a dict whose keys are the computed unique groups and corresponding values being the axis labels belonging to each group. In the above example we have: \nIn [32]: df.groupby(\"A\").groups\nOut[32]: {'bar': [1, 3, 5], 'foo': [0, 2, 4, 6, 7]}\n\nIn [33]: df.groupby(get_letter_type, axis=1).groups\nOut[33]: {'consonant': ['B', 'C', 'D'], 'vowel': ['A']}\n  Calling the standard Python len function on the GroupBy object just returns the length of the groups dict, so it is largely just a convenience: \nIn [34]: grouped = df.groupby([\"A\", \"B\"])\n\nIn [35]: grouped.groups\nOut[35]: {('bar', 'one'): [1], ('bar', 'three'): [3], ('bar', 'two'): [5], ('foo', 'one'): [0, 6], ('foo', 'three'): [7], ('foo', 'two'): [2, 4]}\n\nIn [36]: len(grouped)\nOut[36]: 6\n  GroupBy will tab complete column names (and other attributes): \nIn [37]: df\nOut[37]: \n               height      weight  gender\n2000-01-01  42.849980  157.500553    male\n2000-01-02  49.607315  177.340407    male\n2000-01-03  56.293531  171.524640    male\n2000-01-04  48.421077  144.251986  female\n2000-01-05  46.556882  152.526206    male\n2000-01-06  68.448851  168.272968  female\n2000-01-07  70.757698  136.431469    male\n2000-01-08  58.909500  176.499753  female\n2000-01-09  76.435631  174.094104  female\n2000-01-10  45.306120  177.540920    male\n\nIn [38]: gb = df.groupby(\"gender\")\n  \nIn [39]: gb.<TAB>  # noqa: E225, E999\ngb.agg        gb.boxplot    gb.cummin     gb.describe   gb.filter     gb.get_group  gb.height     gb.last       gb.median     gb.ngroups    gb.plot       gb.rank       gb.std        gb.transform\ngb.aggregate  gb.count      gb.cumprod    gb.dtype      gb.first      gb.groups     gb.hist       gb.max        gb.min        gb.nth        gb.prod       gb.resample   gb.sum        gb.var\ngb.apply      gb.cummax     gb.cumsum     gb.fillna     gb.gender     gb.head       gb.indices    gb.mean       gb.name       gb.ohlc       gb.quantile   gb.size       gb.tail       gb.weight\n    GroupBy with MultiIndex With hierarchically-indexed data, it\u2019s quite natural to group by one of the levels of the hierarchy. Let\u2019s create a Series with a two-level MultiIndex. \nIn [40]: arrays = [\n   ....:     [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"],\n   ....:     [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"],\n   ....: ]\n   ....: \n\nIn [41]: index = pd.MultiIndex.from_arrays(arrays, names=[\"first\", \"second\"])\n\nIn [42]: s = pd.Series(np.random.randn(8), index=index)\n\nIn [43]: s\nOut[43]: \nfirst  second\nbar    one      -0.919854\n       two      -0.042379\nbaz    one       1.247642\n       two      -0.009920\nfoo    one       0.290213\n       two       0.495767\nqux    one       0.362949\n       two       1.548106\ndtype: float64\n  We can then group by one of the levels in s. \nIn [44]: grouped = s.groupby(level=0)\n\nIn [45]: grouped.sum()\nOut[45]: \nfirst\nbar   -0.962232\nbaz    1.237723\nfoo    0.785980\nqux    1.911055\ndtype: float64\n  If the MultiIndex has names specified, these can be passed instead of the level number: \nIn [46]: s.groupby(level=\"second\").sum()\nOut[46]: \nsecond\none    0.980950\ntwo    1.991575\ndtype: float64\n  Grouping with multiple levels is supported. \nIn [47]: s\nOut[47]: \nfirst  second  third\nbar    doo     one     -1.131345\n               two     -0.089329\nbaz    bee     one      0.337863\n               two     -0.945867\nfoo    bop     one     -0.932132\n               two      1.956030\nqux    bop     one      0.017587\n               two     -0.016692\ndtype: float64\n\nIn [48]: s.groupby(level=[\"first\", \"second\"]).sum()\nOut[48]: \nfirst  second\nbar    doo      -1.220674\nbaz    bee      -0.608004\nfoo    bop       1.023898\nqux    bop       0.000895\ndtype: float64\n  Index level names may be supplied as keys. \nIn [49]: s.groupby([\"first\", \"second\"]).sum()\nOut[49]: \nfirst  second\nbar    doo      -1.220674\nbaz    bee      -0.608004\nfoo    bop       1.023898\nqux    bop       0.000895\ndtype: float64\n  More on the sum function and aggregation later.   Grouping DataFrame with Index levels and columns A DataFrame may be grouped by a combination of columns and index levels by specifying the column names as strings and the index levels as pd.Grouper objects. \nIn [50]: arrays = [\n   ....:     [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"],\n   ....:     [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"],\n   ....: ]\n   ....: \n\nIn [51]: index = pd.MultiIndex.from_arrays(arrays, names=[\"first\", \"second\"])\n\nIn [52]: df = pd.DataFrame({\"A\": [1, 1, 1, 1, 2, 2, 3, 3], \"B\": np.arange(8)}, index=index)\n\nIn [53]: df\nOut[53]: \n              A  B\nfirst second      \nbar   one     1  0\n      two     1  1\nbaz   one     1  2\n      two     1  3\nfoo   one     2  4\n      two     2  5\nqux   one     3  6\n      two     3  7\n  The following example groups df by the second index level and the A column. \nIn [54]: df.groupby([pd.Grouper(level=1), \"A\"]).sum()\nOut[54]: \n          B\nsecond A   \none    1  2\n       2  4\n       3  6\ntwo    1  4\n       2  5\n       3  7\n  Index levels may also be specified by name. \nIn [55]: df.groupby([pd.Grouper(level=\"second\"), \"A\"]).sum()\nOut[55]: \n          B\nsecond A   \none    1  2\n       2  4\n       3  6\ntwo    1  4\n       2  5\n       3  7\n  Index level names may be specified as keys directly to groupby. \nIn [56]: df.groupby([\"second\", \"A\"]).sum()\nOut[56]: \n          B\nsecond A   \none    1  2\n       2  4\n       3  6\ntwo    1  4\n       2  5\n       3  7\n    DataFrame column selection in GroupBy Once you have created the GroupBy object from a DataFrame, you might want to do something different for each of the columns. Thus, using [] similar to getting a column from a DataFrame, you can do: \nIn [57]: df = pd.DataFrame(\n   ....:     {\n   ....:         \"A\": [\"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"foo\"],\n   ....:         \"B\": [\"one\", \"one\", \"two\", \"three\", \"two\", \"two\", \"one\", \"three\"],\n   ....:         \"C\": np.random.randn(8),\n   ....:         \"D\": np.random.randn(8),\n   ....:     }\n   ....: )\n   ....: \n\nIn [58]: df\nOut[58]: \n     A      B         C         D\n0  foo    one -0.575247  1.346061\n1  bar    one  0.254161  1.511763\n2  foo    two -1.143704  1.627081\n3  bar  three  0.215897 -0.990582\n4  foo    two  1.193555 -0.441652\n5  bar    two -0.077118  1.211526\n6  foo    one -0.408530  0.268520\n7  foo  three -0.862495  0.024580\n\nIn [59]: grouped = df.groupby([\"A\"])\n\nIn [60]: grouped_C = grouped[\"C\"]\n\nIn [61]: grouped_D = grouped[\"D\"]\n  This is mainly syntactic sugar for the alternative and much more verbose: \nIn [62]: df[\"C\"].groupby(df[\"A\"])\nOut[62]: <pandas.core.groupby.generic.SeriesGroupBy object at 0x7fe2b9449c10>\n  Additionally this method avoids recomputing the internal grouping information derived from the passed key.    Iterating through groups With the GroupBy object in hand, iterating through the grouped data is very natural and functions similarly to itertools.groupby(): \nIn [63]: grouped = df.groupby('A')\n\nIn [64]: for name, group in grouped:\n   ....:     print(name)\n   ....:     print(group)\n   ....: \nbar\n     A      B         C         D\n1  bar    one  0.254161  1.511763\n3  bar  three  0.215897 -0.990582\n5  bar    two -0.077118  1.211526\nfoo\n     A      B         C         D\n0  foo    one -0.575247  1.346061\n2  foo    two -1.143704  1.627081\n4  foo    two  1.193555 -0.441652\n6  foo    one -0.408530  0.268520\n7  foo  three -0.862495  0.024580\n  In the case of grouping by multiple keys, the group name will be a tuple: \nIn [65]: for name, group in df.groupby(['A', 'B']):\n   ....:     print(name)\n   ....:     print(group)\n   ....: \n('bar', 'one')\n     A    B         C         D\n1  bar  one  0.254161  1.511763\n('bar', 'three')\n     A      B         C         D\n3  bar  three  0.215897 -0.990582\n('bar', 'two')\n     A    B         C         D\n5  bar  two -0.077118  1.211526\n('foo', 'one')\n     A    B         C         D\n0  foo  one -0.575247  1.346061\n6  foo  one -0.408530  0.268520\n('foo', 'three')\n     A      B         C        D\n7  foo  three -0.862495  0.02458\n('foo', 'two')\n     A    B         C         D\n2  foo  two -1.143704  1.627081\n4  foo  two  1.193555 -0.441652\n  See Iterating through groups.   Selecting a group A single group can be selected using get_group(): \nIn [66]: grouped.get_group(\"bar\")\nOut[66]: \n     A      B         C         D\n1  bar    one  0.254161  1.511763\n3  bar  three  0.215897 -0.990582\n5  bar    two -0.077118  1.211526\n  Or for an object grouped on multiple columns: \nIn [67]: df.groupby([\"A\", \"B\"]).get_group((\"bar\", \"one\"))\nOut[67]: \n     A    B         C         D\n1  bar  one  0.254161  1.511763\n    Aggregation Once the GroupBy object has been created, several methods are available to perform a computation on the grouped data. These operations are similar to the aggregating API, window API, and resample API. An obvious one is aggregation via the aggregate() or equivalently agg() method: \nIn [68]: grouped = df.groupby(\"A\")\n\nIn [69]: grouped.aggregate(np.sum)\nOut[69]: \n            C         D\nA                      \nbar  0.392940  1.732707\nfoo -1.796421  2.824590\n\nIn [70]: grouped = df.groupby([\"A\", \"B\"])\n\nIn [71]: grouped.aggregate(np.sum)\nOut[71]: \n                  C         D\nA   B                        \nbar one    0.254161  1.511763\n    three  0.215897 -0.990582\n    two   -0.077118  1.211526\nfoo one   -0.983776  1.614581\n    three -0.862495  0.024580\n    two    0.049851  1.185429\n  As you can see, the result of the aggregation will have the group names as the new index along the grouped axis. In the case of multiple keys, the result is a MultiIndex by default, though this can be changed by using the as_index option: \nIn [72]: grouped = df.groupby([\"A\", \"B\"], as_index=False)\n\nIn [73]: grouped.aggregate(np.sum)\nOut[73]: \n     A      B         C         D\n0  bar    one  0.254161  1.511763\n1  bar  three  0.215897 -0.990582\n2  bar    two -0.077118  1.211526\n3  foo    one -0.983776  1.614581\n4  foo  three -0.862495  0.024580\n5  foo    two  0.049851  1.185429\n\nIn [74]: df.groupby(\"A\", as_index=False).sum()\nOut[74]: \n     A         C         D\n0  bar  0.392940  1.732707\n1  foo -1.796421  2.824590\n  Note that you could use the reset_index DataFrame function to achieve the same result as the column names are stored in the resulting MultiIndex: \nIn [75]: df.groupby([\"A\", \"B\"]).sum().reset_index()\nOut[75]: \n     A      B         C         D\n0  bar    one  0.254161  1.511763\n1  bar  three  0.215897 -0.990582\n2  bar    two -0.077118  1.211526\n3  foo    one -0.983776  1.614581\n4  foo  three -0.862495  0.024580\n5  foo    two  0.049851  1.185429\n  Another simple aggregation example is to compute the size of each group. This is included in GroupBy as the size method. It returns a Series whose index are the group names and whose values are the sizes of each group. \nIn [76]: grouped.size()\nOut[76]: \n     A      B  size\n0  bar    one     1\n1  bar  three     1\n2  bar    two     1\n3  foo    one     2\n4  foo  three     1\n5  foo    two     2\n  \nIn [77]: grouped.describe()\nOut[77]: \n      C                                                              ...         D                                                            \n  count      mean       std       min       25%       50%       75%  ...      mean       std       min       25%       50%       75%       max\n0   1.0  0.254161       NaN  0.254161  0.254161  0.254161  0.254161  ...  1.511763       NaN  1.511763  1.511763  1.511763  1.511763  1.511763\n1   1.0  0.215897       NaN  0.215897  0.215897  0.215897  0.215897  ... -0.990582       NaN -0.990582 -0.990582 -0.990582 -0.990582 -0.990582\n2   1.0 -0.077118       NaN -0.077118 -0.077118 -0.077118 -0.077118  ...  1.211526       NaN  1.211526  1.211526  1.211526  1.211526  1.211526\n3   2.0 -0.491888  0.117887 -0.575247 -0.533567 -0.491888 -0.450209  ...  0.807291  0.761937  0.268520  0.537905  0.807291  1.076676  1.346061\n4   1.0 -0.862495       NaN -0.862495 -0.862495 -0.862495 -0.862495  ...  0.024580       NaN  0.024580  0.024580  0.024580  0.024580  0.024580\n5   2.0  0.024925  1.652692 -1.143704 -0.559389  0.024925  0.609240  ...  0.592714  1.462816 -0.441652  0.075531  0.592714  1.109898  1.627081\n\n[6 rows x 16 columns]\n  Another aggregation example is to compute the number of unique values of each group. This is similar to the value_counts function, except that it only counts unique values. \nIn [78]: ll = [['foo', 1], ['foo', 2], ['foo', 2], ['bar', 1], ['bar', 1]]\n\nIn [79]: df4 = pd.DataFrame(ll, columns=[\"A\", \"B\"])\n\nIn [80]: df4\nOut[80]: \n     A  B\n0  foo  1\n1  foo  2\n2  foo  2\n3  bar  1\n4  bar  1\n\nIn [81]: df4.groupby(\"A\")[\"B\"].nunique()\nOut[81]: \nA\nbar    1\nfoo    2\nName: B, dtype: int64\n   Note Aggregation functions will not return the groups that you are aggregating over if they are named columns, when as_index=True, the default. The grouped columns will be the indices of the returned object. Passing as_index=False will return the groups that you are aggregating over, if they are named columns.  Aggregating functions are the ones that reduce the dimension of the returned objects. Some common aggregating functions are tabulated below:       \nFunction Description    \nmean() Compute mean of groups  \nsum() Compute sum of group values  \nsize() Compute group sizes  \ncount() Compute count of group  \nstd() Standard deviation of groups  \nvar() Compute variance of groups  \nsem() Standard error of the mean of groups  \ndescribe() Generates descriptive statistics  \nfirst() Compute first of group values  \nlast() Compute last of group values  \nnth() Take nth value, or a subset if n is a list  \nmin() Compute min of group values  \nmax() Compute max of group values    The aggregating functions above will exclude NA values. Any function which reduces a Series to a scalar value is an aggregation function and will work, a trivial example is df.groupby('A').agg(lambda ser: 1). Note that nth() can act as a reducer or a filter, see here.  Applying multiple functions at once With grouped Series you can also pass a list or dict of functions to do aggregation with, outputting a DataFrame: \nIn [82]: grouped = df.groupby(\"A\")\n\nIn [83]: grouped[\"C\"].agg([np.sum, np.mean, np.std])\nOut[83]: \n          sum      mean       std\nA                                \nbar  0.392940  0.130980  0.181231\nfoo -1.796421 -0.359284  0.912265\n  On a grouped DataFrame, you can pass a list of functions to apply to each column, which produces an aggregated result with a hierarchical index: \nIn [84]: grouped[[\"C\", \"D\"]].agg([np.sum, np.mean, np.std])\nOut[84]: \n            C                             D                    \n          sum      mean       std       sum      mean       std\nA                                                              \nbar  0.392940  0.130980  0.181231  1.732707  0.577569  1.366330\nfoo -1.796421 -0.359284  0.912265  2.824590  0.564918  0.884785\n  The resulting aggregations are named for the functions themselves. If you need to rename, then you can add in a chained operation for a Series like this: \nIn [85]: (\n   ....:     grouped[\"C\"]\n   ....:     .agg([np.sum, np.mean, np.std])\n   ....:     .rename(columns={\"sum\": \"foo\", \"mean\": \"bar\", \"std\": \"baz\"})\n   ....: )\n   ....: \nOut[85]: \n          foo       bar       baz\nA                                \nbar  0.392940  0.130980  0.181231\nfoo -1.796421 -0.359284  0.912265\n  For a grouped DataFrame, you can rename in a similar manner: \nIn [86]: (\n   ....:     grouped[[\"C\", \"D\"]].agg([np.sum, np.mean, np.std]).rename(\n   ....:         columns={\"sum\": \"foo\", \"mean\": \"bar\", \"std\": \"baz\"}\n   ....:     )\n   ....: )\n   ....: \nOut[86]: \n            C                             D                    \n          foo       bar       baz       foo       bar       baz\nA                                                              \nbar  0.392940  0.130980  0.181231  1.732707  0.577569  1.366330\nfoo -1.796421 -0.359284  0.912265  2.824590  0.564918  0.884785\n   Note In general, the output column names should be unique. You can\u2019t apply the same function (or two functions with the same name) to the same column. \nIn [87]: grouped[\"C\"].agg([\"sum\", \"sum\"])\nOut[87]: \n          sum       sum\nA                      \nbar  0.392940  0.392940\nfoo -1.796421 -1.796421\n  pandas does allow you to provide multiple lambdas. In this case, pandas will mangle the name of the (nameless) lambda functions, appending _<i> to each subsequent lambda. \nIn [88]: grouped[\"C\"].agg([lambda x: x.max() - x.min(), lambda x: x.median() - x.mean()])\nOut[88]: \n     <lambda_0>  <lambda_1>\nA                          \nbar    0.331279    0.084917\nfoo    2.337259   -0.215962\n     Named aggregation  New in version 0.25.0.  To support column-specific aggregation with control over the output column names, pandas accepts the special syntax in GroupBy.agg(), known as \u201cnamed aggregation\u201d, where  The keywords are the output column names The values are tuples whose first element is the column to select and the second element is the aggregation to apply to that column. pandas provides the pandas.NamedAgg namedtuple with the fields ['column', 'aggfunc'] to make it clearer what the arguments are. As usual, the aggregation can be a callable or a string alias.  \nIn [89]: animals = pd.DataFrame(\n   ....:     {\n   ....:         \"kind\": [\"cat\", \"dog\", \"cat\", \"dog\"],\n   ....:         \"height\": [9.1, 6.0, 9.5, 34.0],\n   ....:         \"weight\": [7.9, 7.5, 9.9, 198.0],\n   ....:     }\n   ....: )\n   ....: \n\nIn [90]: animals\nOut[90]: \n  kind  height  weight\n0  cat     9.1     7.9\n1  dog     6.0     7.5\n2  cat     9.5     9.9\n3  dog    34.0   198.0\n\nIn [91]: animals.groupby(\"kind\").agg(\n   ....:     min_height=pd.NamedAgg(column=\"height\", aggfunc=\"min\"),\n   ....:     max_height=pd.NamedAgg(column=\"height\", aggfunc=\"max\"),\n   ....:     average_weight=pd.NamedAgg(column=\"weight\", aggfunc=np.mean),\n   ....: )\n   ....: \nOut[91]: \n      min_height  max_height  average_weight\nkind                                        \ncat          9.1         9.5            8.90\ndog          6.0        34.0          102.75\n  pandas.NamedAgg is just a namedtuple. Plain tuples are allowed as well. \nIn [92]: animals.groupby(\"kind\").agg(\n   ....:     min_height=(\"height\", \"min\"),\n   ....:     max_height=(\"height\", \"max\"),\n   ....:     average_weight=(\"weight\", np.mean),\n   ....: )\n   ....: \nOut[92]: \n      min_height  max_height  average_weight\nkind                                        \ncat          9.1         9.5            8.90\ndog          6.0        34.0          102.75\n  If your desired output column names are not valid Python keywords, construct a dictionary and unpack the keyword arguments \nIn [93]: animals.groupby(\"kind\").agg(\n   ....:     **{\n   ....:         \"total weight\": pd.NamedAgg(column=\"weight\", aggfunc=sum)\n   ....:     }\n   ....: )\n   ....: \nOut[93]: \n      total weight\nkind              \ncat           17.8\ndog          205.5\n  Additional keyword arguments are not passed through to the aggregation functions. Only pairs of (column, aggfunc) should be passed as **kwargs. If your aggregation functions requires additional arguments, partially apply them with functools.partial().  Note For Python 3.5 and earlier, the order of **kwargs in a functions was not preserved. This means that the output column ordering would not be consistent. To ensure consistent ordering, the keys (and so output columns) will always be sorted for Python 3.5.  Named aggregation is also valid for Series groupby aggregations. In this case there\u2019s no column selection, so the values are just the functions. \nIn [94]: animals.groupby(\"kind\").height.agg(\n   ....:     min_height=\"min\",\n   ....:     max_height=\"max\",\n   ....: )\n   ....: \nOut[94]: \n      min_height  max_height\nkind                        \ncat          9.1         9.5\ndog          6.0        34.0\n    Applying different functions to DataFrame columns By passing a dict to aggregate you can apply a different aggregation to the columns of a DataFrame: \nIn [95]: grouped.agg({\"C\": np.sum, \"D\": lambda x: np.std(x, ddof=1)})\nOut[95]: \n            C         D\nA                      \nbar  0.392940  1.366330\nfoo -1.796421  0.884785\n  The function names can also be strings. In order for a string to be valid it must be either implemented on GroupBy or available via dispatching: \nIn [96]: grouped.agg({\"C\": \"sum\", \"D\": \"std\"})\nOut[96]: \n            C         D\nA                      \nbar  0.392940  1.366330\nfoo -1.796421  0.884785\n    Cython-optimized aggregation functions Some common aggregations, currently only sum, mean, std, and sem, have optimized Cython implementations: \nIn [97]: df.groupby(\"A\").sum()\nOut[97]: \n            C         D\nA                      \nbar  0.392940  1.732707\nfoo -1.796421  2.824590\n\nIn [98]: df.groupby([\"A\", \"B\"]).mean()\nOut[98]: \n                  C         D\nA   B                        \nbar one    0.254161  1.511763\n    three  0.215897 -0.990582\n    two   -0.077118  1.211526\nfoo one   -0.491888  0.807291\n    three -0.862495  0.024580\n    two    0.024925  0.592714\n  Of course sum and mean are implemented on pandas objects, so the above code would work even without the special versions via dispatching (see below).   Aggregations with User-Defined Functions Users can also provide their own functions for custom aggregations. When aggregating with a User-Defined Function (UDF), the UDF should not mutate the provided Series, see Mutating with User Defined Function (UDF) methods for more information. \nIn [99]: animals.groupby(\"kind\")[[\"height\"]].agg(lambda x: set(x))\nOut[99]: \n           height\nkind             \ncat    {9.1, 9.5}\ndog   {34.0, 6.0}\n  The resulting dtype will reflect that of the aggregating function. If the results from different groups have different dtypes, then a common dtype will be determined in the same way as DataFrame construction. \nIn [100]: animals.groupby(\"kind\")[[\"height\"]].agg(lambda x: x.astype(int).sum())\nOut[100]: \n      height\nkind        \ncat       18\ndog       40\n     Transformation The transform method returns an object that is indexed the same (same size) as the one being grouped. The transform function must:  Return a result that is either the same size as the group chunk or broadcastable to the size of the group chunk (e.g., a scalar, grouped.transform(lambda x: x.iloc[-1])). Operate column-by-column on the group chunk. The transform is applied to the first group chunk using chunk.apply. Not perform in-place operations on the group chunk. Group chunks should be treated as immutable, and changes to a group chunk may produce unexpected results. For example, when using fillna, inplace must be False (grouped.transform(lambda x: x.fillna(inplace=False))). (Optionally) operates on the entire group chunk. If this is supported, a fast path is used starting from the second chunk.  Similar to Aggregations with User-Defined Functions, the resulting dtype will reflect that of the transformation function. If the results from different groups have different dtypes, then a common dtype will be determined in the same way as DataFrame construction. Suppose we wished to standardize the data within each group: \nIn [101]: index = pd.date_range(\"10/1/1999\", periods=1100)\n\nIn [102]: ts = pd.Series(np.random.normal(0.5, 2, 1100), index)\n\nIn [103]: ts = ts.rolling(window=100, min_periods=100).mean().dropna()\n\nIn [104]: ts.head()\nOut[104]: \n2000-01-08    0.779333\n2000-01-09    0.778852\n2000-01-10    0.786476\n2000-01-11    0.782797\n2000-01-12    0.798110\nFreq: D, dtype: float64\n\nIn [105]: ts.tail()\nOut[105]: \n2002-09-30    0.660294\n2002-10-01    0.631095\n2002-10-02    0.673601\n2002-10-03    0.709213\n2002-10-04    0.719369\nFreq: D, dtype: float64\n\nIn [106]: transformed = ts.groupby(lambda x: x.year).transform(\n   .....:     lambda x: (x - x.mean()) / x.std()\n   .....: )\n   .....: \n  We would expect the result to now have mean 0 and standard deviation 1 within each group, which we can easily check: \n# Original Data\nIn [107]: grouped = ts.groupby(lambda x: x.year)\n\nIn [108]: grouped.mean()\nOut[108]: \n2000    0.442441\n2001    0.526246\n2002    0.459365\ndtype: float64\n\nIn [109]: grouped.std()\nOut[109]: \n2000    0.131752\n2001    0.210945\n2002    0.128753\ndtype: float64\n\n# Transformed Data\nIn [110]: grouped_trans = transformed.groupby(lambda x: x.year)\n\nIn [111]: grouped_trans.mean()\nOut[111]: \n2000    1.193722e-15\n2001    1.945476e-15\n2002    1.272949e-15\ndtype: float64\n\nIn [112]: grouped_trans.std()\nOut[112]: \n2000    1.0\n2001    1.0\n2002    1.0\ndtype: float64\n  We can also visually compare the original and transformed data sets. \nIn [113]: compare = pd.DataFrame({\"Original\": ts, \"Transformed\": transformed})\n\nIn [114]: compare.plot()\nOut[114]: <AxesSubplot:>\n   Transformation functions that have lower dimension outputs are broadcast to match the shape of the input array. \nIn [115]: ts.groupby(lambda x: x.year).transform(lambda x: x.max() - x.min())\nOut[115]: \n2000-01-08    0.623893\n2000-01-09    0.623893\n2000-01-10    0.623893\n2000-01-11    0.623893\n2000-01-12    0.623893\n                ...   \n2002-09-30    0.558275\n2002-10-01    0.558275\n2002-10-02    0.558275\n2002-10-03    0.558275\n2002-10-04    0.558275\nFreq: D, Length: 1001, dtype: float64\n  Alternatively, the built-in methods could be used to produce the same outputs. \nIn [116]: max = ts.groupby(lambda x: x.year).transform(\"max\")\n\nIn [117]: min = ts.groupby(lambda x: x.year).transform(\"min\")\n\nIn [118]: max - min\nOut[118]: \n2000-01-08    0.623893\n2000-01-09    0.623893\n2000-01-10    0.623893\n2000-01-11    0.623893\n2000-01-12    0.623893\n                ...   \n2002-09-30    0.558275\n2002-10-01    0.558275\n2002-10-02    0.558275\n2002-10-03    0.558275\n2002-10-04    0.558275\nFreq: D, Length: 1001, dtype: float64\n  Another common data transform is to replace missing data with the group mean. \nIn [119]: data_df\nOut[119]: \n            A         B         C\n0    1.539708 -1.166480  0.533026\n1    1.302092 -0.505754       NaN\n2   -0.371983  1.104803 -0.651520\n3   -1.309622  1.118697 -1.161657\n4   -1.924296  0.396437  0.812436\n..        ...       ...       ...\n995 -0.093110  0.683847 -0.774753\n996 -0.185043  1.438572       NaN\n997 -0.394469 -0.642343  0.011374\n998 -1.174126  1.857148       NaN\n999  0.234564  0.517098  0.393534\n\n[1000 rows x 3 columns]\n\nIn [120]: countries = np.array([\"US\", \"UK\", \"GR\", \"JP\"])\n\nIn [121]: key = countries[np.random.randint(0, 4, 1000)]\n\nIn [122]: grouped = data_df.groupby(key)\n\n# Non-NA count in each group\nIn [123]: grouped.count()\nOut[123]: \n      A    B    C\nGR  209  217  189\nJP  240  255  217\nUK  216  231  193\nUS  239  250  217\n\nIn [124]: transformed = grouped.transform(lambda x: x.fillna(x.mean()))\n  We can verify that the group means have not changed in the transformed data and that the transformed data contains no NAs. \nIn [125]: grouped_trans = transformed.groupby(key)\n\nIn [126]: grouped.mean()  # original group means\nOut[126]: \n           A         B         C\nGR -0.098371 -0.015420  0.068053\nJP  0.069025  0.023100 -0.077324\nUK  0.034069 -0.052580 -0.116525\nUS  0.058664 -0.020399  0.028603\n\nIn [127]: grouped_trans.mean()  # transformation did not change group means\nOut[127]: \n           A         B         C\nGR -0.098371 -0.015420  0.068053\nJP  0.069025  0.023100 -0.077324\nUK  0.034069 -0.052580 -0.116525\nUS  0.058664 -0.020399  0.028603\n\nIn [128]: grouped.count()  # original has some missing data points\nOut[128]: \n      A    B    C\nGR  209  217  189\nJP  240  255  217\nUK  216  231  193\nUS  239  250  217\n\nIn [129]: grouped_trans.count()  # counts after transformation\nOut[129]: \n      A    B    C\nGR  228  228  228\nJP  267  267  267\nUK  247  247  247\nUS  258  258  258\n\nIn [130]: grouped_trans.size()  # Verify non-NA count equals group size\nOut[130]: \nGR    228\nJP    267\nUK    247\nUS    258\ndtype: int64\n   Note Some functions will automatically transform the input when applied to a GroupBy object, but returning an object of the same shape as the original. Passing as_index=False will not affect these transformation methods. For example: fillna, ffill, bfill, shift.. \nIn [131]: grouped.ffill()\nOut[131]: \n            A         B         C\n0    1.539708 -1.166480  0.533026\n1    1.302092 -0.505754  0.533026\n2   -0.371983  1.104803 -0.651520\n3   -1.309622  1.118697 -1.161657\n4   -1.924296  0.396437  0.812436\n..        ...       ...       ...\n995 -0.093110  0.683847 -0.774753\n996 -0.185043  1.438572 -0.774753\n997 -0.394469 -0.642343  0.011374\n998 -1.174126  1.857148 -0.774753\n999  0.234564  0.517098  0.393534\n\n[1000 rows x 3 columns]\n    Window and resample operations It is possible to use resample(), expanding() and rolling() as methods on groupbys. The example below will apply the rolling() method on the samples of the column B based on the groups of column A. \nIn [132]: df_re = pd.DataFrame({\"A\": [1] * 10 + [5] * 10, \"B\": np.arange(20)})\n\nIn [133]: df_re\nOut[133]: \n    A   B\n0   1   0\n1   1   1\n2   1   2\n3   1   3\n4   1   4\n.. ..  ..\n15  5  15\n16  5  16\n17  5  17\n18  5  18\n19  5  19\n\n[20 rows x 2 columns]\n\nIn [134]: df_re.groupby(\"A\").rolling(4).B.mean()\nOut[134]: \nA    \n1  0      NaN\n   1      NaN\n   2      NaN\n   3      1.5\n   4      2.5\n         ... \n5  15    13.5\n   16    14.5\n   17    15.5\n   18    16.5\n   19    17.5\nName: B, Length: 20, dtype: float64\n  The expanding() method will accumulate a given operation (sum() in the example) for all the members of each particular group. \nIn [135]: df_re.groupby(\"A\").expanding().sum()\nOut[135]: \n          B\nA          \n1 0     0.0\n  1     1.0\n  2     3.0\n  3     6.0\n  4    10.0\n...     ...\n5 15   75.0\n  16   91.0\n  17  108.0\n  18  126.0\n  19  145.0\n\n[20 rows x 1 columns]\n  Suppose you want to use the resample() method to get a daily frequency in each group of your dataframe and wish to complete the missing values with the ffill() method. \nIn [136]: df_re = pd.DataFrame(\n   .....:     {\n   .....:         \"date\": pd.date_range(start=\"2016-01-01\", periods=4, freq=\"W\"),\n   .....:         \"group\": [1, 1, 2, 2],\n   .....:         \"val\": [5, 6, 7, 8],\n   .....:     }\n   .....: ).set_index(\"date\")\n   .....: \n\nIn [137]: df_re\nOut[137]: \n            group  val\ndate                  \n2016-01-03      1    5\n2016-01-10      1    6\n2016-01-17      2    7\n2016-01-24      2    8\n\nIn [138]: df_re.groupby(\"group\").resample(\"1D\").ffill()\nOut[138]: \n                  group  val\ngroup date                  \n1     2016-01-03      1    5\n      2016-01-04      1    5\n      2016-01-05      1    5\n      2016-01-06      1    5\n      2016-01-07      1    5\n...                 ...  ...\n2     2016-01-20      2    7\n      2016-01-21      2    7\n      2016-01-22      2    7\n      2016-01-23      2    7\n      2016-01-24      2    8\n\n[16 rows x 2 columns]\n     Filtration The filter method returns a subset of the original object. Suppose we want to take only elements that belong to groups with a group sum greater than 2. \nIn [139]: sf = pd.Series([1, 1, 2, 3, 3, 3])\n\nIn [140]: sf.groupby(sf).filter(lambda x: x.sum() > 2)\nOut[140]: \n3    3\n4    3\n5    3\ndtype: int64\n  The argument of filter must be a function that, applied to the group as a whole, returns True or False. Another useful operation is filtering out elements that belong to groups with only a couple members. \nIn [141]: dff = pd.DataFrame({\"A\": np.arange(8), \"B\": list(\"aabbbbcc\")})\n\nIn [142]: dff.groupby(\"B\").filter(lambda x: len(x) > 2)\nOut[142]: \n   A  B\n2  2  b\n3  3  b\n4  4  b\n5  5  b\n  Alternatively, instead of dropping the offending groups, we can return a like-indexed objects where the groups that do not pass the filter are filled with NaNs. \nIn [143]: dff.groupby(\"B\").filter(lambda x: len(x) > 2, dropna=False)\nOut[143]: \n     A    B\n0  NaN  NaN\n1  NaN  NaN\n2  2.0    b\n3  3.0    b\n4  4.0    b\n5  5.0    b\n6  NaN  NaN\n7  NaN  NaN\n  For DataFrames with multiple columns, filters should explicitly specify a column as the filter criterion. \nIn [144]: dff[\"C\"] = np.arange(8)\n\nIn [145]: dff.groupby(\"B\").filter(lambda x: len(x[\"C\"]) > 2)\nOut[145]: \n   A  B  C\n2  2  b  2\n3  3  b  3\n4  4  b  4\n5  5  b  5\n   Note Some functions when applied to a groupby object will act as a filter on the input, returning a reduced shape of the original (and potentially eliminating groups), but with the index unchanged. Passing as_index=False will not affect these transformation methods. For example: head, tail. \nIn [146]: dff.groupby(\"B\").head(2)\nOut[146]: \n   A  B  C\n0  0  a  0\n1  1  a  1\n2  2  b  2\n3  3  b  3\n6  6  c  6\n7  7  c  7\n     Dispatching to instance methods When doing an aggregation or transformation, you might just want to call an instance method on each data group. This is pretty easy to do by passing lambda functions: \nIn [147]: grouped = df.groupby(\"A\")\n\nIn [148]: grouped.agg(lambda x: x.std())\nOut[148]: \n            C         D\nA                      \nbar  0.181231  1.366330\nfoo  0.912265  0.884785\n  But, it\u2019s rather verbose and can be untidy if you need to pass additional arguments. Using a bit of metaprogramming cleverness, GroupBy now has the ability to \u201cdispatch\u201d method calls to the groups: \nIn [149]: grouped.std()\nOut[149]: \n            C         D\nA                      \nbar  0.181231  1.366330\nfoo  0.912265  0.884785\n  What is actually happening here is that a function wrapper is being generated. When invoked, it takes any passed arguments and invokes the function with any arguments on each group (in the above example, the std function). The results are then combined together much in the style of agg and transform (it actually uses apply to infer the gluing, documented next). This enables some operations to be carried out rather succinctly: \nIn [150]: tsdf = pd.DataFrame(\n   .....:     np.random.randn(1000, 3),\n   .....:     index=pd.date_range(\"1/1/2000\", periods=1000),\n   .....:     columns=[\"A\", \"B\", \"C\"],\n   .....: )\n   .....: \n\nIn [151]: tsdf.iloc[::2] = np.nan\n\nIn [152]: grouped = tsdf.groupby(lambda x: x.year)\n\nIn [153]: grouped.fillna(method=\"pad\")\nOut[153]: \n                   A         B         C\n2000-01-01       NaN       NaN       NaN\n2000-01-02 -0.353501 -0.080957 -0.876864\n2000-01-03 -0.353501 -0.080957 -0.876864\n2000-01-04  0.050976  0.044273 -0.559849\n2000-01-05  0.050976  0.044273 -0.559849\n...              ...       ...       ...\n2002-09-22  0.005011  0.053897 -1.026922\n2002-09-23  0.005011  0.053897 -1.026922\n2002-09-24 -0.456542 -1.849051  1.559856\n2002-09-25 -0.456542 -1.849051  1.559856\n2002-09-26  1.123162  0.354660  1.128135\n\n[1000 rows x 3 columns]\n  In this example, we chopped the collection of time series into yearly chunks then independently called fillna on the groups. The nlargest and nsmallest methods work on Series style groupbys: \nIn [154]: s = pd.Series([9, 8, 7, 5, 19, 1, 4.2, 3.3])\n\nIn [155]: g = pd.Series(list(\"abababab\"))\n\nIn [156]: gb = s.groupby(g)\n\nIn [157]: gb.nlargest(3)\nOut[157]: \na  4    19.0\n   0     9.0\n   2     7.0\nb  1     8.0\n   3     5.0\n   7     3.3\ndtype: float64\n\nIn [158]: gb.nsmallest(3)\nOut[158]: \na  6    4.2\n   2    7.0\n   0    9.0\nb  5    1.0\n   7    3.3\n   3    5.0\ndtype: float64\n    Flexible apply\n Some operations on the grouped data might not fit into either the aggregate or transform categories. Or, you may simply want GroupBy to infer how to combine the results. For these, use the apply function, which can be substituted for both aggregate and transform in many standard use cases. However, apply can handle some exceptional use cases, for example: \nIn [159]: df\nOut[159]: \n     A      B         C         D\n0  foo    one -0.575247  1.346061\n1  bar    one  0.254161  1.511763\n2  foo    two -1.143704  1.627081\n3  bar  three  0.215897 -0.990582\n4  foo    two  1.193555 -0.441652\n5  bar    two -0.077118  1.211526\n6  foo    one -0.408530  0.268520\n7  foo  three -0.862495  0.024580\n\nIn [160]: grouped = df.groupby(\"A\")\n\n# could also just call .describe()\nIn [161]: grouped[\"C\"].apply(lambda x: x.describe())\nOut[161]: \nA         \nbar  count    3.000000\n     mean     0.130980\n     std      0.181231\n     min     -0.077118\n     25%      0.069390\n                ...   \nfoo  min     -1.143704\n     25%     -0.862495\n     50%     -0.575247\n     75%     -0.408530\n     max      1.193555\nName: C, Length: 16, dtype: float64\n  The dimension of the returned result can also change: \nIn [162]: grouped = df.groupby('A')['C']\n\nIn [163]: def f(group):\n   .....:     return pd.DataFrame({'original': group,\n   .....:                          'demeaned': group - group.mean()})\n   .....: \n\nIn [164]: grouped.apply(f)\nOut[164]: \n   original  demeaned\n0 -0.575247 -0.215962\n1  0.254161  0.123181\n2 -1.143704 -0.784420\n3  0.215897  0.084917\n4  1.193555  1.552839\n5 -0.077118 -0.208098\n6 -0.408530 -0.049245\n7 -0.862495 -0.503211\n  apply on a Series can operate on a returned value from the applied function, that is itself a series, and possibly upcast the result to a DataFrame: \nIn [165]: def f(x):\n   .....:     return pd.Series([x, x ** 2], index=[\"x\", \"x^2\"])\n   .....: \n\nIn [166]: s = pd.Series(np.random.rand(5))\n\nIn [167]: s\nOut[167]: \n0    0.321438\n1    0.493496\n2    0.139505\n3    0.910103\n4    0.194158\ndtype: float64\n\nIn [168]: s.apply(f)\nOut[168]: \n          x       x^2\n0  0.321438  0.103323\n1  0.493496  0.243538\n2  0.139505  0.019462\n3  0.910103  0.828287\n4  0.194158  0.037697\n   Note apply can act as a reducer, transformer, or filter function, depending on exactly what is passed to it. So depending on the path taken, and exactly what you are grouping. Thus the grouped columns(s) may be included in the output as well as set the indices.  Similar to Aggregations with User-Defined Functions, the resulting dtype will reflect that of the apply function. If the results from different groups have different dtypes, then a common dtype will be determined in the same way as DataFrame construction.   Numba Accelerated Routines  New in version 1.1.  If Numba is installed as an optional dependency, the transform and aggregate methods support engine='numba' and engine_kwargs arguments. See enhancing performance with Numba for general usage of the arguments and performance considerations. The function signature must start with values, index exactly as the data belonging to each group will be passed into values, and the group index will be passed into index.  Warning When using engine='numba', there will be no \u201cfall back\u201d behavior internally. The group data and group index will be passed as NumPy arrays to the JITed user defined function, and no alternative execution attempts will be tried.    Other useful features  Automatic exclusion of \u201cnuisance\u201d columns Again consider the example DataFrame we\u2019ve been looking at: \nIn [169]: df\nOut[169]: \n     A      B         C         D\n0  foo    one -0.575247  1.346061\n1  bar    one  0.254161  1.511763\n2  foo    two -1.143704  1.627081\n3  bar  three  0.215897 -0.990582\n4  foo    two  1.193555 -0.441652\n5  bar    two -0.077118  1.211526\n6  foo    one -0.408530  0.268520\n7  foo  three -0.862495  0.024580\n  Suppose we wish to compute the standard deviation grouped by the A column. There is a slight problem, namely that we don\u2019t care about the data in column B. We refer to this as a \u201cnuisance\u201d column. If the passed aggregation function can\u2019t be applied to some columns, the troublesome columns will be (silently) dropped. Thus, this does not pose any problems: \nIn [170]: df.groupby(\"A\").std()\nOut[170]: \n            C         D\nA                      \nbar  0.181231  1.366330\nfoo  0.912265  0.884785\n  Note that df.groupby('A').colname.std(). is more efficient than df.groupby('A').std().colname, so if the result of an aggregation function is only interesting over one column (here colname), it may be filtered before applying the aggregation function.  Note Any object column, also if it contains numerical values such as Decimal objects, is considered as a \u201cnuisance\u201d columns. They are excluded from aggregate functions automatically in groupby. If you do wish to include decimal or object columns in an aggregation with other non-nuisance data types, you must do so explicitly.  \nIn [171]: from decimal import Decimal\n\nIn [172]: df_dec = pd.DataFrame(\n   .....:     {\n   .....:         \"id\": [1, 2, 1, 2],\n   .....:         \"int_column\": [1, 2, 3, 4],\n   .....:         \"dec_column\": [\n   .....:             Decimal(\"0.50\"),\n   .....:             Decimal(\"0.15\"),\n   .....:             Decimal(\"0.25\"),\n   .....:             Decimal(\"0.40\"),\n   .....:         ],\n   .....:     }\n   .....: )\n   .....: \n\n# Decimal columns can be sum'd explicitly by themselves...\nIn [173]: df_dec.groupby([\"id\"])[[\"dec_column\"]].sum()\nOut[173]: \n   dec_column\nid           \n1        0.75\n2        0.55\n\n# ...but cannot be combined with standard data types or they will be excluded\nIn [174]: df_dec.groupby([\"id\"])[[\"int_column\", \"dec_column\"]].sum()\nOut[174]: \n    int_column\nid            \n1            4\n2            6\n\n# Use .agg function to aggregate over standard and \"nuisance\" data types\n# at the same time\nIn [175]: df_dec.groupby([\"id\"]).agg({\"int_column\": \"sum\", \"dec_column\": \"sum\"})\nOut[175]: \n    int_column dec_column\nid                       \n1            4       0.75\n2            6       0.55\n    Handling of (un)observed Categorical values When using a Categorical grouper (as a single grouper, or as part of multiple groupers), the observed keyword controls whether to return a cartesian product of all possible groupers values (observed=False) or only those that are observed groupers (observed=True). Show all values: \nIn [176]: pd.Series([1, 1, 1]).groupby(\n   .....:     pd.Categorical([\"a\", \"a\", \"a\"], categories=[\"a\", \"b\"]), observed=False\n   .....: ).count()\n   .....: \nOut[176]: \na    3\nb    0\ndtype: int64\n  Show only the observed values: \nIn [177]: pd.Series([1, 1, 1]).groupby(\n   .....:     pd.Categorical([\"a\", \"a\", \"a\"], categories=[\"a\", \"b\"]), observed=True\n   .....: ).count()\n   .....: \nOut[177]: \na    3\ndtype: int64\n  The returned dtype of the grouped will always include all of the categories that were grouped. \nIn [178]: s = (\n   .....:     pd.Series([1, 1, 1])\n   .....:     .groupby(pd.Categorical([\"a\", \"a\", \"a\"], categories=[\"a\", \"b\"]), observed=False)\n   .....:     .count()\n   .....: )\n   .....: \n\nIn [179]: s.index.dtype\nOut[179]: CategoricalDtype(categories=['a', 'b'], ordered=False)\n    NA and NaT group handling If there are any NaN or NaT values in the grouping key, these will be automatically excluded. In other words, there will never be an \u201cNA group\u201d or \u201cNaT group\u201d. This was not the case in older versions of pandas, but users were generally discarding the NA group anyway (and supporting it was an implementation headache).   Grouping with ordered factors Categorical variables represented as instance of pandas\u2019s Categorical class can be used as group keys. If so, the order of the levels will be preserved: \nIn [180]: data = pd.Series(np.random.randn(100))\n\nIn [181]: factor = pd.qcut(data, [0, 0.25, 0.5, 0.75, 1.0])\n\nIn [182]: data.groupby(factor).mean()\nOut[182]: \n(-2.645, -0.523]   -1.362896\n(-0.523, 0.0296]   -0.260266\n(0.0296, 0.654]     0.361802\n(0.654, 2.21]       1.073801\ndtype: float64\n    Grouping with a grouper specification You may need to specify a bit more data to properly group. You can use the pd.Grouper to provide this local control. \nIn [183]: import datetime\n\nIn [184]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"Branch\": \"A A A A A A A B\".split(),\n   .....:         \"Buyer\": \"Carl Mark Carl Carl Joe Joe Joe Carl\".split(),\n   .....:         \"Quantity\": [1, 3, 5, 1, 8, 1, 9, 3],\n   .....:         \"Date\": [\n   .....:             datetime.datetime(2013, 1, 1, 13, 0),\n   .....:             datetime.datetime(2013, 1, 1, 13, 5),\n   .....:             datetime.datetime(2013, 10, 1, 20, 0),\n   .....:             datetime.datetime(2013, 10, 2, 10, 0),\n   .....:             datetime.datetime(2013, 10, 1, 20, 0),\n   .....:             datetime.datetime(2013, 10, 2, 10, 0),\n   .....:             datetime.datetime(2013, 12, 2, 12, 0),\n   .....:             datetime.datetime(2013, 12, 2, 14, 0),\n   .....:         ],\n   .....:     }\n   .....: )\n   .....: \n\nIn [185]: df\nOut[185]: \n  Branch Buyer  Quantity                Date\n0      A  Carl         1 2013-01-01 13:00:00\n1      A  Mark         3 2013-01-01 13:05:00\n2      A  Carl         5 2013-10-01 20:00:00\n3      A  Carl         1 2013-10-02 10:00:00\n4      A   Joe         8 2013-10-01 20:00:00\n5      A   Joe         1 2013-10-02 10:00:00\n6      A   Joe         9 2013-12-02 12:00:00\n7      B  Carl         3 2013-12-02 14:00:00\n  Groupby a specific column with the desired frequency. This is like resampling. \nIn [186]: df.groupby([pd.Grouper(freq=\"1M\", key=\"Date\"), \"Buyer\"]).sum()\nOut[186]: \n                  Quantity\nDate       Buyer          \n2013-01-31 Carl          1\n           Mark          3\n2013-10-31 Carl          6\n           Joe           9\n2013-12-31 Carl          3\n           Joe           9\n  You have an ambiguous specification in that you have a named index and a column that could be potential groupers. \nIn [187]: df = df.set_index(\"Date\")\n\nIn [188]: df[\"Date\"] = df.index + pd.offsets.MonthEnd(2)\n\nIn [189]: df.groupby([pd.Grouper(freq=\"6M\", key=\"Date\"), \"Buyer\"]).sum()\nOut[189]: \n                  Quantity\nDate       Buyer          \n2013-02-28 Carl          1\n           Mark          3\n2014-02-28 Carl          9\n           Joe          18\n\nIn [190]: df.groupby([pd.Grouper(freq=\"6M\", level=\"Date\"), \"Buyer\"]).sum()\nOut[190]: \n                  Quantity\nDate       Buyer          \n2013-01-31 Carl          1\n           Mark          3\n2014-01-31 Carl          9\n           Joe          18\n    Taking the first rows of each group Just like for a DataFrame or Series you can call head and tail on a groupby: \nIn [191]: df = pd.DataFrame([[1, 2], [1, 4], [5, 6]], columns=[\"A\", \"B\"])\n\nIn [192]: df\nOut[192]: \n   A  B\n0  1  2\n1  1  4\n2  5  6\n\nIn [193]: g = df.groupby(\"A\")\n\nIn [194]: g.head(1)\nOut[194]: \n   A  B\n0  1  2\n2  5  6\n\nIn [195]: g.tail(1)\nOut[195]: \n   A  B\n1  1  4\n2  5  6\n  This shows the first or last n rows from each group.   Taking the nth row of each group To select from a DataFrame or Series the nth item, use nth(). This is a reduction method, and will return a single row (or no row) per group if you pass an int for n: \nIn [196]: df = pd.DataFrame([[1, np.nan], [1, 4], [5, 6]], columns=[\"A\", \"B\"])\n\nIn [197]: g = df.groupby(\"A\")\n\nIn [198]: g.nth(0)\nOut[198]: \n     B\nA     \n1  NaN\n5  6.0\n\nIn [199]: g.nth(-1)\nOut[199]: \n     B\nA     \n1  4.0\n5  6.0\n\nIn [200]: g.nth(1)\nOut[200]: \n     B\nA     \n1  4.0\n  If you want to select the nth not-null item, use the dropna kwarg. For a DataFrame this should be either 'any' or 'all' just like you would pass to dropna: \n# nth(0) is the same as g.first()\nIn [201]: g.nth(0, dropna=\"any\")\nOut[201]: \n     B\nA     \n1  4.0\n5  6.0\n\nIn [202]: g.first()\nOut[202]: \n     B\nA     \n1  4.0\n5  6.0\n\n# nth(-1) is the same as g.last()\nIn [203]: g.nth(-1, dropna=\"any\")  # NaNs denote group exhausted when using dropna\nOut[203]: \n     B\nA     \n1  4.0\n5  6.0\n\nIn [204]: g.last()\nOut[204]: \n     B\nA     \n1  4.0\n5  6.0\n\nIn [205]: g.B.nth(0, dropna=\"all\")\nOut[205]: \nA\n1    4.0\n5    6.0\nName: B, dtype: float64\n  As with other methods, passing as_index=False, will achieve a filtration, which returns the grouped row. \nIn [206]: df = pd.DataFrame([[1, np.nan], [1, 4], [5, 6]], columns=[\"A\", \"B\"])\n\nIn [207]: g = df.groupby(\"A\", as_index=False)\n\nIn [208]: g.nth(0)\nOut[208]: \n   A    B\n0  1  NaN\n2  5  6.0\n\nIn [209]: g.nth(-1)\nOut[209]: \n   A    B\n1  1  4.0\n2  5  6.0\n  You can also select multiple rows from each group by specifying multiple nth values as a list of ints. \nIn [210]: business_dates = pd.date_range(start=\"4/1/2014\", end=\"6/30/2014\", freq=\"B\")\n\nIn [211]: df = pd.DataFrame(1, index=business_dates, columns=[\"a\", \"b\"])\n\n# get the first, 4th, and last date index for each month\nIn [212]: df.groupby([df.index.year, df.index.month]).nth([0, 3, -1])\nOut[212]: \n        a  b\n2014 4  1  1\n     4  1  1\n     4  1  1\n     5  1  1\n     5  1  1\n     5  1  1\n     6  1  1\n     6  1  1\n     6  1  1\n    Enumerate group items To see the order in which each row appears within its group, use the cumcount method: \nIn [213]: dfg = pd.DataFrame(list(\"aaabba\"), columns=[\"A\"])\n\nIn [214]: dfg\nOut[214]: \n   A\n0  a\n1  a\n2  a\n3  b\n4  b\n5  a\n\nIn [215]: dfg.groupby(\"A\").cumcount()\nOut[215]: \n0    0\n1    1\n2    2\n3    0\n4    1\n5    3\ndtype: int64\n\nIn [216]: dfg.groupby(\"A\").cumcount(ascending=False)\nOut[216]: \n0    3\n1    2\n2    1\n3    1\n4    0\n5    0\ndtype: int64\n    Enumerate groups To see the ordering of the groups (as opposed to the order of rows within a group given by cumcount) you can use ngroup(). Note that the numbers given to the groups match the order in which the groups would be seen when iterating over the groupby object, not the order they are first observed. \nIn [217]: dfg = pd.DataFrame(list(\"aaabba\"), columns=[\"A\"])\n\nIn [218]: dfg\nOut[218]: \n   A\n0  a\n1  a\n2  a\n3  b\n4  b\n5  a\n\nIn [219]: dfg.groupby(\"A\").ngroup()\nOut[219]: \n0    0\n1    0\n2    0\n3    1\n4    1\n5    0\ndtype: int64\n\nIn [220]: dfg.groupby(\"A\").ngroup(ascending=False)\nOut[220]: \n0    1\n1    1\n2    1\n3    0\n4    0\n5    1\ndtype: int64\n    Plotting Groupby also works with some plotting methods. For example, suppose we suspect that some features in a DataFrame may differ by group, in this case, the values in column 1 where the group is \u201cB\u201d are 3 higher on average. \nIn [221]: np.random.seed(1234)\n\nIn [222]: df = pd.DataFrame(np.random.randn(50, 2))\n\nIn [223]: df[\"g\"] = np.random.choice([\"A\", \"B\"], size=50)\n\nIn [224]: df.loc[df[\"g\"] == \"B\", 1] += 3\n  We can easily visualize this with a boxplot: \nIn [225]: df.groupby(\"g\").boxplot()\nOut[225]: \nA         AxesSubplot(0.1,0.15;0.363636x0.75)\nB    AxesSubplot(0.536364,0.15;0.363636x0.75)\ndtype: object\n   The result of calling boxplot is a dictionary whose keys are the values of our grouping column g (\u201cA\u201d and \u201cB\u201d). The values of the resulting dictionary can be controlled by the return_type keyword of boxplot. See the visualization documentation for more.  Warning For historical reasons, df.groupby(\"g\").boxplot() is not equivalent to df.boxplot(by=\"g\"). See here for an explanation.    Piping function calls Similar to the functionality provided by DataFrame and Series, functions that take GroupBy objects can be chained together using a pipe method to allow for a cleaner, more readable syntax. To read about .pipe in general terms, see here. Combining .groupby and .pipe is often useful when you need to reuse GroupBy objects. As an example, imagine having a DataFrame with columns for stores, products, revenue and quantity sold. We\u2019d like to do a groupwise calculation of prices (i.e. revenue/quantity) per store and per product. We could do this in a multi-step operation, but expressing it in terms of piping can make the code more readable. First we set the data: \nIn [226]: n = 1000\n\nIn [227]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"Store\": np.random.choice([\"Store_1\", \"Store_2\"], n),\n   .....:         \"Product\": np.random.choice([\"Product_1\", \"Product_2\"], n),\n   .....:         \"Revenue\": (np.random.random(n) * 50 + 10).round(2),\n   .....:         \"Quantity\": np.random.randint(1, 10, size=n),\n   .....:     }\n   .....: )\n   .....: \n\nIn [228]: df.head(2)\nOut[228]: \n     Store    Product  Revenue  Quantity\n0  Store_2  Product_1    26.12         1\n1  Store_2  Product_1    28.86         1\n  Now, to find prices per store/product, we can simply do: \nIn [229]: (\n   .....:     df.groupby([\"Store\", \"Product\"])\n   .....:     .pipe(lambda grp: grp.Revenue.sum() / grp.Quantity.sum())\n   .....:     .unstack()\n   .....:     .round(2)\n   .....: )\n   .....: \nOut[229]: \nProduct  Product_1  Product_2\nStore                        \nStore_1       6.82       7.05\nStore_2       6.30       6.64\n  Piping can also be expressive when you want to deliver a grouped object to some arbitrary function, for example: \nIn [230]: def mean(groupby):\n   .....:     return groupby.mean()\n   .....: \n\nIn [231]: df.groupby([\"Store\", \"Product\"]).pipe(mean)\nOut[231]: \n                     Revenue  Quantity\nStore   Product                       \nStore_1 Product_1  34.622727  5.075758\n        Product_2  35.482815  5.029630\nStore_2 Product_1  32.972837  5.237589\n        Product_2  34.684360  5.224000\n  where mean takes a GroupBy object and finds the mean of the Revenue and Quantity columns respectively for each Store-Product combination. The mean function can be any function that takes in a GroupBy object; the .pipe will pass the GroupBy object as a parameter into the function you specify.    Examples  Regrouping by factor Regroup columns of a DataFrame according to their sum, and sum the aggregated ones. \nIn [232]: df = pd.DataFrame({\"a\": [1, 0, 0], \"b\": [0, 1, 0], \"c\": [1, 0, 0], \"d\": [2, 3, 4]})\n\nIn [233]: df\nOut[233]: \n   a  b  c  d\n0  1  0  1  2\n1  0  1  0  3\n2  0  0  0  4\n\nIn [234]: df.groupby(df.sum(), axis=1).sum()\nOut[234]: \n   1  9\n0  2  2\n1  1  3\n2  0  4\n    Multi-column factorization By using ngroup(), we can extract information about the groups in a way similar to factorize() (as described further in the reshaping API) but which applies naturally to multiple columns of mixed type and different sources. This can be useful as an intermediate categorical-like step in processing, when the relationships between the group rows are more important than their content, or as input to an algorithm which only accepts the integer encoding. (For more information about support in pandas for full categorical data, see the Categorical introduction and the API documentation.) \nIn [235]: dfg = pd.DataFrame({\"A\": [1, 1, 2, 3, 2], \"B\": list(\"aaaba\")})\n\nIn [236]: dfg\nOut[236]: \n   A  B\n0  1  a\n1  1  a\n2  2  a\n3  3  b\n4  2  a\n\nIn [237]: dfg.groupby([\"A\", \"B\"]).ngroup()\nOut[237]: \n0    0\n1    0\n2    1\n3    2\n4    1\ndtype: int64\n\nIn [238]: dfg.groupby([\"A\", [0, 0, 0, 1, 1]]).ngroup()\nOut[238]: \n0    0\n1    0\n2    1\n3    3\n4    2\ndtype: int64\n    Groupby by indexer to \u2018resample\u2019 data Resampling produces new hypothetical samples (resamples) from already existing observed data or from a model that generates data. These new samples are similar to the pre-existing samples. In order to resample to work on indices that are non-datetimelike, the following procedure can be utilized. In the following examples, df.index // 5 returns a binary array which is used to determine what gets selected for the groupby operation.  Note The below example shows how we can downsample by consolidation of samples into fewer samples. Here by using df.index // 5, we are aggregating the samples in bins. By applying std() function, we aggregate the information contained in many samples into a small subset of values which is their standard deviation thereby reducing the number of samples.  \nIn [239]: df = pd.DataFrame(np.random.randn(10, 2))\n\nIn [240]: df\nOut[240]: \n          0         1\n0 -0.793893  0.321153\n1  0.342250  1.618906\n2 -0.975807  1.918201\n3 -0.810847 -1.405919\n4 -1.977759  0.461659\n5  0.730057 -1.316938\n6 -0.751328  0.528290\n7 -0.257759 -1.081009\n8  0.505895 -1.701948\n9 -1.006349  0.020208\n\nIn [241]: df.index // 5\nOut[241]: Int64Index([0, 0, 0, 0, 0, 1, 1, 1, 1, 1], dtype='int64')\n\nIn [242]: df.groupby(df.index // 5).std()\nOut[242]: \n          0         1\n0  0.823647  1.312912\n1  0.760109  0.942941\n    Returning a Series to propagate names Group DataFrame columns, compute a set of metrics and return a named Series. The Series name is used as the name for the column index. This is especially useful in conjunction with reshaping operations such as stacking in which the column index name will be used as the name of the inserted column: \nIn [243]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"a\": [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2],\n   .....:         \"b\": [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1],\n   .....:         \"c\": [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n   .....:         \"d\": [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1],\n   .....:     }\n   .....: )\n   .....: \n\nIn [244]: def compute_metrics(x):\n   .....:     result = {\"b_sum\": x[\"b\"].sum(), \"c_mean\": x[\"c\"].mean()}\n   .....:     return pd.Series(result, name=\"metrics\")\n   .....: \n\nIn [245]: result = df.groupby(\"a\").apply(compute_metrics)\n\nIn [246]: result\nOut[246]: \nmetrics  b_sum  c_mean\na                     \n0          2.0     0.5\n1          2.0     0.5\n2          2.0     0.5\n\nIn [247]: result.stack()\nOut[247]: \na  metrics\n0  b_sum      2.0\n   c_mean     0.5\n1  b_sum      2.0\n   c_mean     0.5\n2  b_sum      2.0\n   c_mean     0.5\ndtype: float64\n   \n"}, {"name": "GroupBy", "path": "reference/groupby", "type": "GroupBy", "text": "GroupBy GroupBy objects are returned by groupby calls: pandas.DataFrame.groupby(), pandas.Series.groupby(), etc.  Indexing, iteration       \nGroupBy.__iter__() Groupby iterator.  \nGroupBy.groups Dict {group name -> group labels}.  \nGroupBy.indices Dict {group name -> group indices}.  \nGroupBy.get_group(name[, obj]) Construct DataFrame from group with provided name.          \nGrouper(*args, **kwargs) A Grouper allows the user to specify a groupby instruction for an object.      Function application       \nGroupBy.apply(func, *args, **kwargs) Apply function func group-wise and combine the results together.  \nGroupBy.agg(func, *args, **kwargs)   \nSeriesGroupBy.aggregate([func, engine, ...]) Aggregate using one or more operations over the specified axis.  \nDataFrameGroupBy.aggregate([func, engine, ...]) Aggregate using one or more operations over the specified axis.  \nSeriesGroupBy.transform(func, *args[, ...]) Call function producing a like-indexed Series on each group and return a Series having the same indexes as the original object filled with the transformed values.  \nDataFrameGroupBy.transform(func, *args[, ...]) Call function producing a like-indexed DataFrame on each group and return a DataFrame having the same indexes as the original object filled with the transformed values.  \nGroupBy.pipe(func, *args, **kwargs) Apply a function func with arguments to this GroupBy object and return the function's result.      Computations / descriptive stats       \nGroupBy.all([skipna]) Return True if all values in the group are truthful, else False.  \nGroupBy.any([skipna]) Return True if any value in the group is truthful, else False.  \nGroupBy.bfill([limit]) Backward fill the values.  \nGroupBy.backfill([limit]) Backward fill the values.  \nGroupBy.count() Compute count of group, excluding missing values.  \nGroupBy.cumcount([ascending]) Number each item in each group from 0 to the length of that group - 1.  \nGroupBy.cummax([axis]) Cumulative max for each group.  \nGroupBy.cummin([axis]) Cumulative min for each group.  \nGroupBy.cumprod([axis]) Cumulative product for each group.  \nGroupBy.cumsum([axis]) Cumulative sum for each group.  \nGroupBy.ffill([limit]) Forward fill the values.  \nGroupBy.first([numeric_only, min_count]) Compute first of group values.  \nGroupBy.head([n]) Return first n rows of each group.  \nGroupBy.last([numeric_only, min_count]) Compute last of group values.  \nGroupBy.max([numeric_only, min_count]) Compute max of group values.  \nGroupBy.mean([numeric_only, engine, ...]) Compute mean of groups, excluding missing values.  \nGroupBy.median([numeric_only]) Compute median of groups, excluding missing values.  \nGroupBy.min([numeric_only, min_count]) Compute min of group values.  \nGroupBy.ngroup([ascending]) Number each group from 0 to the number of groups - 1.  \nGroupBy.nth(n[, dropna]) Take the nth row from each group if n is an int, otherwise a subset of rows.  \nGroupBy.ohlc() Compute open, high, low and close values of a group, excluding missing values.  \nGroupBy.pad([limit]) Forward fill the values.  \nGroupBy.prod([numeric_only, min_count]) Compute prod of group values.  \nGroupBy.rank([method, ascending, na_option, ...]) Provide the rank of values within each group.  \nGroupBy.pct_change([periods, fill_method, ...]) Calculate pct_change of each value to previous entry in group.  \nGroupBy.size() Compute group sizes.  \nGroupBy.sem([ddof]) Compute standard error of the mean of groups, excluding missing values.  \nGroupBy.std([ddof, engine, engine_kwargs]) Compute standard deviation of groups, excluding missing values.  \nGroupBy.sum([numeric_only, min_count, ...]) Compute sum of group values.  \nGroupBy.var([ddof, engine, engine_kwargs]) Compute variance of groups, excluding missing values.  \nGroupBy.tail([n]) Return last n rows of each group.    The following methods are available in both SeriesGroupBy and DataFrameGroupBy objects, but may differ slightly, usually in that the DataFrameGroupBy version usually permits the specification of an axis argument, and often an argument indicating whether to restrict application to columns of a specific data type.       \nDataFrameGroupBy.all([skipna]) Return True if all values in the group are truthful, else False.  \nDataFrameGroupBy.any([skipna]) Return True if any value in the group is truthful, else False.  \nDataFrameGroupBy.backfill([limit]) Backward fill the values.  \nDataFrameGroupBy.bfill([limit]) Backward fill the values.  \nDataFrameGroupBy.corr Compute pairwise correlation of columns, excluding NA/null values.  \nDataFrameGroupBy.count() Compute count of group, excluding missing values.  \nDataFrameGroupBy.cov Compute pairwise covariance of columns, excluding NA/null values.  \nDataFrameGroupBy.cumcount([ascending]) Number each item in each group from 0 to the length of that group - 1.  \nDataFrameGroupBy.cummax([axis]) Cumulative max for each group.  \nDataFrameGroupBy.cummin([axis]) Cumulative min for each group.  \nDataFrameGroupBy.cumprod([axis]) Cumulative product for each group.  \nDataFrameGroupBy.cumsum([axis]) Cumulative sum for each group.  \nDataFrameGroupBy.describe(**kwargs) Generate descriptive statistics.  \nDataFrameGroupBy.diff First discrete difference of element.  \nDataFrameGroupBy.ffill([limit]) Forward fill the values.  \nDataFrameGroupBy.fillna Fill NA/NaN values using the specified method.  \nDataFrameGroupBy.filter(func[, dropna]) Return a copy of a DataFrame excluding filtered elements.  \nDataFrameGroupBy.hist Make a histogram of the DataFrame's columns.  \nDataFrameGroupBy.idxmax([axis, skipna]) Return index of first occurrence of maximum over requested axis.  \nDataFrameGroupBy.idxmin([axis, skipna]) Return index of first occurrence of minimum over requested axis.  \nDataFrameGroupBy.mad Return the mean absolute deviation of the values over the requested axis.  \nDataFrameGroupBy.nunique([dropna]) Return DataFrame with counts of unique elements in each position.  \nDataFrameGroupBy.pad([limit]) Forward fill the values.  \nDataFrameGroupBy.pct_change([periods, ...]) Calculate pct_change of each value to previous entry in group.  \nDataFrameGroupBy.plot Class implementing the .plot attribute for groupby objects.  \nDataFrameGroupBy.quantile([q, interpolation]) Return group values at the given quantile, a la numpy.percentile.  \nDataFrameGroupBy.rank([method, ascending, ...]) Provide the rank of values within each group.  \nDataFrameGroupBy.resample(rule, *args, **kwargs) Provide resampling when using a TimeGrouper.  \nDataFrameGroupBy.sample([n, frac, replace, ...]) Return a random sample of items from each group.  \nDataFrameGroupBy.shift([periods, freq, ...]) Shift each group by periods observations.  \nDataFrameGroupBy.size() Compute group sizes.  \nDataFrameGroupBy.skew Return unbiased skew over requested axis.  \nDataFrameGroupBy.take Return the elements in the given positional indices along an axis.  \nDataFrameGroupBy.tshift (DEPRECATED) Shift the time index, using the index's frequency if available.  \nDataFrameGroupBy.value_counts([subset, ...]) Return a Series or DataFrame containing counts of unique rows.    The following methods are available only for SeriesGroupBy objects.       \nSeriesGroupBy.hist Draw histogram of the input series using matplotlib.  \nSeriesGroupBy.nlargest([n, keep]) Return the largest n elements.  \nSeriesGroupBy.nsmallest([n, keep]) Return the smallest n elements.  \nSeriesGroupBy.nunique([dropna]) Return number of unique elements in the group.  \nSeriesGroupBy.unique Return unique values of Series object.  \nSeriesGroupBy.value_counts([normalize, ...])   \nSeriesGroupBy.is_monotonic_increasing Alias for is_monotonic.  \nSeriesGroupBy.is_monotonic_decreasing Return boolean if values in the object are monotonic_decreasing.    The following methods are available only for DataFrameGroupBy objects.       \nDataFrameGroupBy.corrwith Compute pairwise correlation.  \nDataFrameGroupBy.boxplot([subplots, column, ...]) Make box plots from DataFrameGroupBy data.    \n"}, {"name": "Index objects", "path": "reference/indexing", "type": "Index Objects", "text": "Index objects  Index Many of these methods or variants thereof are available on the objects that contain an index (Series/DataFrame) and those should most likely be used before calling these methods directly.       \nIndex([data, dtype, copy, name, tupleize_cols]) Immutable sequence used for indexing and alignment.     Properties       \nIndex.values Return an array representing the data in the Index.  \nIndex.is_monotonic Alias for is_monotonic_increasing.  \nIndex.is_monotonic_increasing Return if the index is monotonic increasing (only equal or increasing) values.  \nIndex.is_monotonic_decreasing Return if the index is monotonic decreasing (only equal or decreasing) values.  \nIndex.is_unique Return if the index has unique values.  \nIndex.has_duplicates Check if the Index has duplicate values.  \nIndex.hasnans Return True if there are any NaNs.  \nIndex.dtype Return the dtype object of the underlying data.  \nIndex.inferred_type Return a string of the type inferred from the values.  \nIndex.is_all_dates Whether or not the index values only consist of dates.  \nIndex.shape Return a tuple of the shape of the underlying data.  \nIndex.name Return Index or MultiIndex name.  \nIndex.names   \nIndex.nbytes Return the number of bytes in the underlying data.  \nIndex.ndim Number of dimensions of the underlying data, by definition 1.  \nIndex.size Return the number of elements in the underlying data.  \nIndex.empty   \nIndex.T Return the transpose, which is by definition self.  \nIndex.memory_usage([deep]) Memory usage of the values.      Modifying and computations       \nIndex.all(*args, **kwargs) Return whether all elements are Truthy.  \nIndex.any(*args, **kwargs) Return whether any element is Truthy.  \nIndex.argmin([axis, skipna]) Return int position of the smallest value in the Series.  \nIndex.argmax([axis, skipna]) Return int position of the largest value in the Series.  \nIndex.copy([name, deep, dtype, names]) Make a copy of this object.  \nIndex.delete(loc) Make new Index with passed location(-s) deleted.  \nIndex.drop(labels[, errors]) Make new Index with passed list of labels deleted.  \nIndex.drop_duplicates([keep]) Return Index with duplicate values removed.  \nIndex.duplicated([keep]) Indicate duplicate index values.  \nIndex.equals(other) Determine if two Index object are equal.  \nIndex.factorize([sort, na_sentinel]) Encode the object as an enumerated type or categorical variable.  \nIndex.identical(other) Similar to equals, but checks that object attributes and types are also equal.  \nIndex.insert(loc, item) Make new Index inserting new item at location.  \nIndex.is_(other) More flexible, faster check like is but that works through views.  \nIndex.is_boolean() Check if the Index only consists of booleans.  \nIndex.is_categorical() Check if the Index holds categorical data.  \nIndex.is_floating() Check if the Index is a floating type.  \nIndex.is_integer() Check if the Index only consists of integers.  \nIndex.is_interval() Check if the Index holds Interval objects.  \nIndex.is_mixed() Check if the Index holds data with mixed data types.  \nIndex.is_numeric() Check if the Index only consists of numeric data.  \nIndex.is_object() Check if the Index is of the object dtype.  \nIndex.min([axis, skipna]) Return the minimum value of the Index.  \nIndex.max([axis, skipna]) Return the maximum value of the Index.  \nIndex.reindex(target[, method, level, ...]) Create index with target's values.  \nIndex.rename(name[, inplace]) Alter Index or MultiIndex name.  \nIndex.repeat(repeats[, axis]) Repeat elements of a Index.  \nIndex.where(cond[, other]) Replace values where the condition is False.  \nIndex.take(indices[, axis, allow_fill, ...]) Return a new Index of the values selected by the indices.  \nIndex.putmask(mask, value) Return a new Index of the values set with the mask.  \nIndex.unique([level]) Return unique values in the index.  \nIndex.nunique([dropna]) Return number of unique elements in the object.  \nIndex.value_counts([normalize, sort, ...]) Return a Series containing counts of unique values.      Compatibility with MultiIndex       \nIndex.set_names(names[, level, inplace]) Set Index or MultiIndex name.  \nIndex.droplevel([level]) Return index with requested level(s) removed.      Missing values       \nIndex.fillna([value, downcast]) Fill NA/NaN values with the specified value.  \nIndex.dropna([how]) Return Index without NA/NaN values.  \nIndex.isna() Detect missing values.  \nIndex.notna() Detect existing (non-missing) values.      Conversion       \nIndex.astype(dtype[, copy]) Create an Index with values cast to dtypes.  \nIndex.item() Return the first element of the underlying data as a Python scalar.  \nIndex.map(mapper[, na_action]) Map values using an input mapping or function.  \nIndex.ravel([order]) Return an ndarray of the flattened values of the underlying data.  \nIndex.to_list() Return a list of the values.  \nIndex.to_native_types([slicer]) (DEPRECATED) Format specified values of self and return them.  \nIndex.to_series([index, name]) Create a Series with both index and values equal to the index keys.  \nIndex.to_frame([index, name]) Create a DataFrame with a column containing the Index.  \nIndex.view([cls])       Sorting       \nIndex.argsort(*args, **kwargs) Return the integer indices that would sort the index.  \nIndex.searchsorted(value[, side, sorter]) Find indices where elements should be inserted to maintain order.  \nIndex.sort_values([return_indexer, ...]) Return a sorted copy of the index.      Time-specific operations       \nIndex.shift([periods, freq]) Shift index by desired number of time frequency increments.      Combining / joining / set operations       \nIndex.append(other) Append a collection of Index options together.  \nIndex.join(other[, how, level, ...]) Compute join_index and indexers to conform data structures to the new index.  \nIndex.intersection(other[, sort]) Form the intersection of two Index objects.  \nIndex.union(other[, sort]) Form the union of two Index objects.  \nIndex.difference(other[, sort]) Return a new Index with elements of index not in other.  \nIndex.symmetric_difference(other[, ...]) Compute the symmetric difference of two Index objects.      Selecting       \nIndex.asof(label) Return the label from the index, or, if not present, the previous one.  \nIndex.asof_locs(where, mask) Return the locations (indices) of labels in the index.  \nIndex.get_indexer(target[, method, limit, ...]) Compute indexer and mask for new index given the current index.  \nIndex.get_indexer_for(target) Guaranteed return of an indexer even when non-unique.  \nIndex.get_indexer_non_unique(target) Compute indexer and mask for new index given the current index.  \nIndex.get_level_values(level) Return an Index of values for requested level.  \nIndex.get_loc(key[, method, tolerance]) Get integer location, slice or boolean mask for requested label.  \nIndex.get_slice_bound(label, side[, kind]) Calculate slice bound that corresponds to given label.  \nIndex.get_value(series, key) Fast lookup of value from 1-dimensional ndarray.  \nIndex.isin(values[, level]) Return a boolean array where the index values are in values.  \nIndex.slice_indexer([start, end, step, kind]) Compute the slice indexer for input labels and step.  \nIndex.slice_locs([start, end, step, kind]) Compute slice locations for input labels.       Numeric Index       \nRangeIndex([start, stop, step, dtype, copy, ...]) Immutable Index implementing a monotonic integer range.  \nInt64Index([data, dtype, copy, name]) (DEPRECATED) Immutable sequence used for indexing and alignment.  \nUInt64Index([data, dtype, copy, name]) (DEPRECATED) Immutable sequence used for indexing and alignment.  \nFloat64Index([data, dtype, copy, name]) (DEPRECATED) Immutable sequence used for indexing and alignment.          \nRangeIndex.start The value of the start parameter (0 if this was not supplied).  \nRangeIndex.stop The value of the stop parameter.  \nRangeIndex.step The value of the step parameter (1 if this was not supplied).  \nRangeIndex.from_range(data[, name, dtype]) Create RangeIndex from a range object.      CategoricalIndex       \nCategoricalIndex([data, categories, ...]) Index based on an underlying Categorical.     Categorical components       \nCategoricalIndex.codes The category codes of this categorical.  \nCategoricalIndex.categories The categories of this categorical.  \nCategoricalIndex.ordered Whether the categories have an ordered relationship.  \nCategoricalIndex.rename_categories(*args, ...) Rename categories.  \nCategoricalIndex.reorder_categories(*args, ...) Reorder categories as specified in new_categories.  \nCategoricalIndex.add_categories(*args, **kwargs) Add new categories.  \nCategoricalIndex.remove_categories(*args, ...) Remove the specified categories.  \nCategoricalIndex.remove_unused_categories(...) Remove categories which are not used.  \nCategoricalIndex.set_categories(*args, **kwargs) Set the categories to the specified new_categories.  \nCategoricalIndex.as_ordered(*args, **kwargs) Set the Categorical to be ordered.  \nCategoricalIndex.as_unordered(*args, **kwargs) Set the Categorical to be unordered.      Modifying and computations       \nCategoricalIndex.map(mapper) Map values using input an input mapping or function.  \nCategoricalIndex.equals(other) Determine if two CategoricalIndex objects contain the same elements.       IntervalIndex       \nIntervalIndex(data[, closed, dtype, copy, ...]) Immutable index of intervals that are closed on the same side.     IntervalIndex components       \nIntervalIndex.from_arrays(left, right[, ...]) Construct from two arrays defining the left and right bounds.  \nIntervalIndex.from_tuples(data[, closed, ...]) Construct an IntervalIndex from an array-like of tuples.  \nIntervalIndex.from_breaks(breaks[, closed, ...]) Construct an IntervalIndex from an array of splits.  \nIntervalIndex.left   \nIntervalIndex.right   \nIntervalIndex.mid   \nIntervalIndex.closed Whether the intervals are closed on the left-side, right-side, both or neither.  \nIntervalIndex.length   \nIntervalIndex.values Return an array representing the data in the Index.  \nIntervalIndex.is_empty Indicates if an interval is empty, meaning it contains no points.  \nIntervalIndex.is_non_overlapping_monotonic Return True if the IntervalArray is non-overlapping (no Intervals share points) and is either monotonic increasing or monotonic decreasing, else False.  \nIntervalIndex.is_overlapping Return True if the IntervalIndex has overlapping intervals, else False.  \nIntervalIndex.get_loc(key[, method, tolerance]) Get integer location, slice or boolean mask for requested label.  \nIntervalIndex.get_indexer(target[, method, ...]) Compute indexer and mask for new index given the current index.  \nIntervalIndex.set_closed(*args, **kwargs) Return an IntervalArray identical to the current one, but closed on the specified side.  \nIntervalIndex.contains(*args, **kwargs) Check elementwise if the Intervals contain the value.  \nIntervalIndex.overlaps(*args, **kwargs) Check elementwise if an Interval overlaps the values in the IntervalArray.  \nIntervalIndex.to_tuples(*args, **kwargs) Return an ndarray of tuples of the form (left, right).       MultiIndex       \nMultiIndex([levels, codes, sortorder, ...]) A multi-level, or hierarchical, index object for pandas objects.          \nIndexSlice Create an object to more easily perform multi-index slicing.     MultiIndex constructors       \nMultiIndex.from_arrays(arrays[, sortorder, ...]) Convert arrays to MultiIndex.  \nMultiIndex.from_tuples(tuples[, sortorder, ...]) Convert list of tuples to MultiIndex.  \nMultiIndex.from_product(iterables[, ...]) Make a MultiIndex from the cartesian product of multiple iterables.  \nMultiIndex.from_frame(df[, sortorder, names]) Make a MultiIndex from a DataFrame.      MultiIndex properties       \nMultiIndex.names Names of levels in MultiIndex.  \nMultiIndex.levels   \nMultiIndex.codes   \nMultiIndex.nlevels Integer number of levels in this MultiIndex.  \nMultiIndex.levshape A tuple with the length of each level.  \nMultiIndex.dtypes Return the dtypes as a Series for the underlying MultiIndex.      MultiIndex components       \nMultiIndex.set_levels(levels[, level, ...]) Set new levels on MultiIndex.  \nMultiIndex.set_codes(codes[, level, ...]) Set new codes on MultiIndex.  \nMultiIndex.to_flat_index() Convert a MultiIndex to an Index of Tuples containing the level values.  \nMultiIndex.to_frame([index, name]) Create a DataFrame with the levels of the MultiIndex as columns.  \nMultiIndex.sortlevel([level, ascending, ...]) Sort MultiIndex at the requested level.  \nMultiIndex.droplevel([level]) Return index with requested level(s) removed.  \nMultiIndex.swaplevel([i, j]) Swap level i with level j.  \nMultiIndex.reorder_levels(order) Rearrange levels using input order.  \nMultiIndex.remove_unused_levels() Create new MultiIndex from current that removes unused levels.      MultiIndex selecting       \nMultiIndex.get_loc(key[, method]) Get location for a label or a tuple of labels.  \nMultiIndex.get_locs(seq) Get location for a sequence of labels.  \nMultiIndex.get_loc_level(key[, level, ...]) Get location and sliced index for requested label(s)/level(s).  \nMultiIndex.get_indexer(target[, method, ...]) Compute indexer and mask for new index given the current index.  \nMultiIndex.get_level_values(level) Return vector of label values for requested level.       DatetimeIndex       \nDatetimeIndex([data, freq, tz, normalize, ...]) Immutable ndarray-like of datetime64 data.     Time/date components       \nDatetimeIndex.year The year of the datetime.  \nDatetimeIndex.month The month as January=1, December=12.  \nDatetimeIndex.day The day of the datetime.  \nDatetimeIndex.hour The hours of the datetime.  \nDatetimeIndex.minute The minutes of the datetime.  \nDatetimeIndex.second The seconds of the datetime.  \nDatetimeIndex.microsecond The microseconds of the datetime.  \nDatetimeIndex.nanosecond The nanoseconds of the datetime.  \nDatetimeIndex.date Returns numpy array of python datetime.date objects.  \nDatetimeIndex.time Returns numpy array of datetime.time objects.  \nDatetimeIndex.timetz Returns numpy array of datetime.time objects with timezone information.  \nDatetimeIndex.dayofyear The ordinal day of the year.  \nDatetimeIndex.day_of_year The ordinal day of the year.  \nDatetimeIndex.weekofyear (DEPRECATED) The week ordinal of the year.  \nDatetimeIndex.week (DEPRECATED) The week ordinal of the year.  \nDatetimeIndex.dayofweek The day of the week with Monday=0, Sunday=6.  \nDatetimeIndex.day_of_week The day of the week with Monday=0, Sunday=6.  \nDatetimeIndex.weekday The day of the week with Monday=0, Sunday=6.  \nDatetimeIndex.quarter The quarter of the date.  \nDatetimeIndex.tz Return the timezone.  \nDatetimeIndex.freq Return the frequency object if it is set, otherwise None.  \nDatetimeIndex.freqstr Return the frequency object as a string if its set, otherwise None.  \nDatetimeIndex.is_month_start Indicates whether the date is the first day of the month.  \nDatetimeIndex.is_month_end Indicates whether the date is the last day of the month.  \nDatetimeIndex.is_quarter_start Indicator for whether the date is the first day of a quarter.  \nDatetimeIndex.is_quarter_end Indicator for whether the date is the last day of a quarter.  \nDatetimeIndex.is_year_start Indicate whether the date is the first day of a year.  \nDatetimeIndex.is_year_end Indicate whether the date is the last day of the year.  \nDatetimeIndex.is_leap_year Boolean indicator if the date belongs to a leap year.  \nDatetimeIndex.inferred_freq Tries to return a string representing a frequency guess, generated by infer_freq.      Selecting       \nDatetimeIndex.indexer_at_time(time[, asof]) Return index locations of values at particular time of day (e.g.  \nDatetimeIndex.indexer_between_time(...[, ...]) Return index locations of values between particular times of day (e.g., 9:00-9:30AM).      Time-specific operations       \nDatetimeIndex.normalize(*args, **kwargs) Convert times to midnight.  \nDatetimeIndex.strftime(*args, **kwargs) Convert to Index using specified date_format.  \nDatetimeIndex.snap([freq]) Snap time stamps to nearest occurring frequency.  \nDatetimeIndex.tz_convert(tz) Convert tz-aware Datetime Array/Index from one time zone to another.  \nDatetimeIndex.tz_localize(tz[, ambiguous, ...]) Localize tz-naive Datetime Array/Index to tz-aware Datetime Array/Index.  \nDatetimeIndex.round(*args, **kwargs) Perform round operation on the data to the specified freq.  \nDatetimeIndex.floor(*args, **kwargs) Perform floor operation on the data to the specified freq.  \nDatetimeIndex.ceil(*args, **kwargs) Perform ceil operation on the data to the specified freq.  \nDatetimeIndex.month_name(*args, **kwargs) Return the month names of the DateTimeIndex with specified locale.  \nDatetimeIndex.day_name(*args, **kwargs) Return the day names of the DateTimeIndex with specified locale.      Conversion       \nDatetimeIndex.to_period(*args, **kwargs) Cast to PeriodArray/Index at a particular frequency.  \nDatetimeIndex.to_perioddelta(freq) Calculate TimedeltaArray of difference between index values and index converted to PeriodArray at specified freq.  \nDatetimeIndex.to_pydatetime(*args, **kwargs) Return Datetime Array/Index as object ndarray of datetime.datetime objects.  \nDatetimeIndex.to_series([keep_tz, index, name]) Create a Series with both index and values equal to the index keys useful with map for returning an indexer based on an index.  \nDatetimeIndex.to_frame([index, name]) Create a DataFrame with a column containing the Index.      Methods       \nDatetimeIndex.mean(*args, **kwargs) Return the mean value of the Array.  \nDatetimeIndex.std(*args, **kwargs) Return sample standard deviation over requested axis.       TimedeltaIndex       \nTimedeltaIndex([data, unit, freq, closed, ...]) Immutable ndarray of timedelta64 data, represented internally as int64, and which can be boxed to timedelta objects.     Components       \nTimedeltaIndex.days Number of days for each element.  \nTimedeltaIndex.seconds Number of seconds (>= 0 and less than 1 day) for each element.  \nTimedeltaIndex.microseconds Number of microseconds (>= 0 and less than 1 second) for each element.  \nTimedeltaIndex.nanoseconds Number of nanoseconds (>= 0 and less than 1 microsecond) for each element.  \nTimedeltaIndex.components Return a dataframe of the components (days, hours, minutes, seconds, milliseconds, microseconds, nanoseconds) of the Timedeltas.  \nTimedeltaIndex.inferred_freq Tries to return a string representing a frequency guess, generated by infer_freq.      Conversion       \nTimedeltaIndex.to_pytimedelta(*args, **kwargs) Return Timedelta Array/Index as object ndarray of datetime.timedelta objects.  \nTimedeltaIndex.to_series([index, name]) Create a Series with both index and values equal to the index keys.  \nTimedeltaIndex.round(*args, **kwargs) Perform round operation on the data to the specified freq.  \nTimedeltaIndex.floor(*args, **kwargs) Perform floor operation on the data to the specified freq.  \nTimedeltaIndex.ceil(*args, **kwargs) Perform ceil operation on the data to the specified freq.  \nTimedeltaIndex.to_frame([index, name]) Create a DataFrame with a column containing the Index.      Methods       \nTimedeltaIndex.mean(*args, **kwargs) Return the mean value of the Array.       PeriodIndex       \nPeriodIndex([data, ordinal, freq, dtype, ...]) Immutable ndarray holding ordinal values indicating regular periods in time.     Properties       \nPeriodIndex.day The days of the period.  \nPeriodIndex.dayofweek The day of the week with Monday=0, Sunday=6.  \nPeriodIndex.day_of_week The day of the week with Monday=0, Sunday=6.  \nPeriodIndex.dayofyear The ordinal day of the year.  \nPeriodIndex.day_of_year The ordinal day of the year.  \nPeriodIndex.days_in_month The number of days in the month.  \nPeriodIndex.daysinmonth The number of days in the month.  \nPeriodIndex.end_time   \nPeriodIndex.freq Return the frequency object if it is set, otherwise None.  \nPeriodIndex.freqstr Return the frequency object as a string if its set, otherwise None.  \nPeriodIndex.hour The hour of the period.  \nPeriodIndex.is_leap_year Logical indicating if the date belongs to a leap year.  \nPeriodIndex.minute The minute of the period.  \nPeriodIndex.month The month as January=1, December=12.  \nPeriodIndex.quarter The quarter of the date.  \nPeriodIndex.qyear   \nPeriodIndex.second The second of the period.  \nPeriodIndex.start_time   \nPeriodIndex.week The week ordinal of the year.  \nPeriodIndex.weekday The day of the week with Monday=0, Sunday=6.  \nPeriodIndex.weekofyear The week ordinal of the year.  \nPeriodIndex.year The year of the period.      Methods       \nPeriodIndex.asfreq([freq, how]) Convert the PeriodArray to the specified frequency freq.  \nPeriodIndex.strftime(*args, **kwargs) Convert to Index using specified date_format.  \nPeriodIndex.to_timestamp([freq, how]) Cast to DatetimeArray/Index.     \n"}, {"name": "Indexing and selecting data", "path": "user_guide/indexing", "type": "Manual", "text": "Indexing and selecting data The axis labeling information in pandas objects serves many purposes:  Identifies data (i.e. provides metadata) using known indicators, important for analysis, visualization, and interactive console display. Enables automatic and explicit data alignment. Allows intuitive getting and setting of subsets of the data set.  In this section, we will focus on the final point: namely, how to slice, dice, and generally get and set subsets of pandas objects. The primary focus will be on Series and DataFrame as they have received more development attention in this area.  Note The Python and NumPy indexing operators [] and attribute operator . provide quick and easy access to pandas data structures across a wide range of use cases. This makes interactive work intuitive, as there\u2019s little new to learn if you already know how to deal with Python dictionaries and NumPy arrays. However, since the type of the data to be accessed isn\u2019t known in advance, directly using standard operators has some optimization limits. For production code, we recommended that you take advantage of the optimized pandas data access methods exposed in this chapter.   Warning Whether a copy or a reference is returned for a setting operation, may depend on the context. This is sometimes called chained assignment and should be avoided. See Returning a View versus Copy.  See the MultiIndex / Advanced Indexing for MultiIndex and more advanced indexing documentation. See the cookbook for some advanced strategies.  Different choices for indexing Object selection has had a number of user-requested additions in order to support more explicit location based indexing. pandas now supports three types of multi-axis indexing.  \n.loc is primarily label based, but may also be used with a boolean array. .loc will raise KeyError when the items are not found. Allowed inputs are:  \n A single label, e.g. 5 or 'a' (Note that 5 is interpreted as a label of the index. This use is not an integer position along the index.). A list or array of labels ['a', 'b', 'c']. A slice object with labels 'a':'f' (Note that contrary to usual Python slices, both the start and the stop are included, when present in the index! See Slicing with labels and Endpoints are inclusive.) A boolean array (any NA values will be treated as False). A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above).  \n See more at Selection by Label.  \n.iloc is primarily integer position based (from 0 to length-1 of the axis), but may also be used with a boolean array. .iloc will raise IndexError if a requested indexer is out-of-bounds, except slice indexers which allow out-of-bounds indexing. (this conforms with Python/NumPy slice semantics). Allowed inputs are:  \n An integer e.g. 5. A list or array of integers [4, 3, 0]. A slice object with ints 1:7. A boolean array (any NA values will be treated as False). A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above).  \n See more at Selection by Position, Advanced Indexing and Advanced Hierarchical.  .loc, .iloc, and also [] indexing can accept a callable as indexer. See more at Selection By Callable.  Getting values from an object with multi-axes selection uses the following notation (using .loc as an example, but the following applies to .iloc as well). Any of the axes accessors may be the null slice :. Axes left out of the specification are assumed to be :, e.g. p.loc['a'] is equivalent to p.loc['a', :, :].       \nObject Type Indexers    \nSeries s.loc[indexer]  \nDataFrame df.loc[row_indexer,column_indexer]      Basics As mentioned when introducing the data structures in the last section, the primary function of indexing with [] (a.k.a. __getitem__ for those familiar with implementing class behavior in Python) is selecting out lower-dimensional slices. The following table shows return type values when indexing pandas objects with []:        \nObject Type Selection Return Value Type    \nSeries series[label] scalar value  \nDataFrame frame[colname] Series corresponding to colname    Here we construct a simple time series data set to use for illustrating the indexing functionality: \nIn [1]: dates = pd.date_range('1/1/2000', periods=8)\n\nIn [2]: df = pd.DataFrame(np.random.randn(8, 4),\n   ...:                   index=dates, columns=['A', 'B', 'C', 'D'])\n   ...: \n\nIn [3]: df\nOut[3]: \n                   A         B         C         D\n2000-01-01  0.469112 -0.282863 -1.509059 -1.135632\n2000-01-02  1.212112 -0.173215  0.119209 -1.044236\n2000-01-03 -0.861849 -2.104569 -0.494929  1.071804\n2000-01-04  0.721555 -0.706771 -1.039575  0.271860\n2000-01-05 -0.424972  0.567020  0.276232 -1.087401\n2000-01-06 -0.673690  0.113648 -1.478427  0.524988\n2000-01-07  0.404705  0.577046 -1.715002 -1.039268\n2000-01-08 -0.370647 -1.157892 -1.344312  0.844885\n   Note None of the indexing functionality is time series specific unless specifically stated.  Thus, as per above, we have the most basic indexing using []: \nIn [4]: s = df['A']\n\nIn [5]: s[dates[5]]\nOut[5]: -0.6736897080883706\n  You can pass a list of columns to [] to select columns in that order. If a column is not contained in the DataFrame, an exception will be raised. Multiple columns can also be set in this manner: \nIn [6]: df\nOut[6]: \n                   A         B         C         D\n2000-01-01  0.469112 -0.282863 -1.509059 -1.135632\n2000-01-02  1.212112 -0.173215  0.119209 -1.044236\n2000-01-03 -0.861849 -2.104569 -0.494929  1.071804\n2000-01-04  0.721555 -0.706771 -1.039575  0.271860\n2000-01-05 -0.424972  0.567020  0.276232 -1.087401\n2000-01-06 -0.673690  0.113648 -1.478427  0.524988\n2000-01-07  0.404705  0.577046 -1.715002 -1.039268\n2000-01-08 -0.370647 -1.157892 -1.344312  0.844885\n\nIn [7]: df[['B', 'A']] = df[['A', 'B']]\n\nIn [8]: df\nOut[8]: \n                   A         B         C         D\n2000-01-01 -0.282863  0.469112 -1.509059 -1.135632\n2000-01-02 -0.173215  1.212112  0.119209 -1.044236\n2000-01-03 -2.104569 -0.861849 -0.494929  1.071804\n2000-01-04 -0.706771  0.721555 -1.039575  0.271860\n2000-01-05  0.567020 -0.424972  0.276232 -1.087401\n2000-01-06  0.113648 -0.673690 -1.478427  0.524988\n2000-01-07  0.577046  0.404705 -1.715002 -1.039268\n2000-01-08 -1.157892 -0.370647 -1.344312  0.844885\n  You may find this useful for applying a transform (in-place) to a subset of the columns.  Warning pandas aligns all AXES when setting Series and DataFrame from .loc, and .iloc. This will not modify df because the column alignment is before value assignment. \nIn [9]: df[['A', 'B']]\nOut[9]: \n                   A         B\n2000-01-01 -0.282863  0.469112\n2000-01-02 -0.173215  1.212112\n2000-01-03 -2.104569 -0.861849\n2000-01-04 -0.706771  0.721555\n2000-01-05  0.567020 -0.424972\n2000-01-06  0.113648 -0.673690\n2000-01-07  0.577046  0.404705\n2000-01-08 -1.157892 -0.370647\n\nIn [10]: df.loc[:, ['B', 'A']] = df[['A', 'B']]\n\nIn [11]: df[['A', 'B']]\nOut[11]: \n                   A         B\n2000-01-01 -0.282863  0.469112\n2000-01-02 -0.173215  1.212112\n2000-01-03 -2.104569 -0.861849\n2000-01-04 -0.706771  0.721555\n2000-01-05  0.567020 -0.424972\n2000-01-06  0.113648 -0.673690\n2000-01-07  0.577046  0.404705\n2000-01-08 -1.157892 -0.370647\n  The correct way to swap column values is by using raw values: \nIn [12]: df.loc[:, ['B', 'A']] = df[['A', 'B']].to_numpy()\n\nIn [13]: df[['A', 'B']]\nOut[13]: \n                   A         B\n2000-01-01  0.469112 -0.282863\n2000-01-02  1.212112 -0.173215\n2000-01-03 -0.861849 -2.104569\n2000-01-04  0.721555 -0.706771\n2000-01-05 -0.424972  0.567020\n2000-01-06 -0.673690  0.113648\n2000-01-07  0.404705  0.577046\n2000-01-08 -0.370647 -1.157892\n     Attribute access You may access an index on a Series or column on a DataFrame directly as an attribute: \nIn [14]: sa = pd.Series([1, 2, 3], index=list('abc'))\n\nIn [15]: dfa = df.copy()\n  \nIn [16]: sa.b\nOut[16]: 2\n\nIn [17]: dfa.A\nOut[17]: \n2000-01-01    0.469112\n2000-01-02    1.212112\n2000-01-03   -0.861849\n2000-01-04    0.721555\n2000-01-05   -0.424972\n2000-01-06   -0.673690\n2000-01-07    0.404705\n2000-01-08   -0.370647\nFreq: D, Name: A, dtype: float64\n  \nIn [18]: sa.a = 5\n\nIn [19]: sa\nOut[19]: \na    5\nb    2\nc    3\ndtype: int64\n\nIn [20]: dfa.A = list(range(len(dfa.index)))  # ok if A already exists\n\nIn [21]: dfa\nOut[21]: \n            A         B         C         D\n2000-01-01  0 -0.282863 -1.509059 -1.135632\n2000-01-02  1 -0.173215  0.119209 -1.044236\n2000-01-03  2 -2.104569 -0.494929  1.071804\n2000-01-04  3 -0.706771 -1.039575  0.271860\n2000-01-05  4  0.567020  0.276232 -1.087401\n2000-01-06  5  0.113648 -1.478427  0.524988\n2000-01-07  6  0.577046 -1.715002 -1.039268\n2000-01-08  7 -1.157892 -1.344312  0.844885\n\nIn [22]: dfa['A'] = list(range(len(dfa.index)))  # use this form to create a new column\n\nIn [23]: dfa\nOut[23]: \n            A         B         C         D\n2000-01-01  0 -0.282863 -1.509059 -1.135632\n2000-01-02  1 -0.173215  0.119209 -1.044236\n2000-01-03  2 -2.104569 -0.494929  1.071804\n2000-01-04  3 -0.706771 -1.039575  0.271860\n2000-01-05  4  0.567020  0.276232 -1.087401\n2000-01-06  5  0.113648 -1.478427  0.524988\n2000-01-07  6  0.577046 -1.715002 -1.039268\n2000-01-08  7 -1.157892 -1.344312  0.844885\n   Warning  You can use this access only if the index element is a valid Python identifier, e.g. s.1 is not allowed. See here for an explanation of valid identifiers. The attribute will not be available if it conflicts with an existing method name, e.g. s.min is not allowed, but s['min'] is possible. Similarly, the attribute will not be available if it conflicts with any of the following list: index, major_axis, minor_axis, items. In any of these cases, standard indexing will still work, e.g. s['1'], s['min'], and s['index'] will access the corresponding element or column.   If you are using the IPython environment, you may also use tab-completion to see these accessible attributes. You can also assign a dict to a row of a DataFrame: \nIn [24]: x = pd.DataFrame({'x': [1, 2, 3], 'y': [3, 4, 5]})\n\nIn [25]: x.iloc[1] = {'x': 9, 'y': 99}\n\nIn [26]: x\nOut[26]: \n   x   y\n0  1   3\n1  9  99\n2  3   5\n  You can use attribute access to modify an existing element of a Series or column of a DataFrame, but be careful; if you try to use attribute access to create a new column, it creates a new attribute rather than a new column. In 0.21.0 and later, this will raise a UserWarning: \nIn [1]: df = pd.DataFrame({'one': [1., 2., 3.]})\nIn [2]: df.two = [4, 5, 6]\nUserWarning: Pandas doesn't allow Series to be assigned into nonexistent columns - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute_access\nIn [3]: df\nOut[3]:\n   one\n0  1.0\n1  2.0\n2  3.0\n    Slicing ranges The most robust and consistent way of slicing ranges along arbitrary axes is described in the Selection by Position section detailing the .iloc method. For now, we explain the semantics of slicing using the [] operator. With Series, the syntax works exactly as with an ndarray, returning a slice of the values and the corresponding labels: \nIn [27]: s[:5]\nOut[27]: \n2000-01-01    0.469112\n2000-01-02    1.212112\n2000-01-03   -0.861849\n2000-01-04    0.721555\n2000-01-05   -0.424972\nFreq: D, Name: A, dtype: float64\n\nIn [28]: s[::2]\nOut[28]: \n2000-01-01    0.469112\n2000-01-03   -0.861849\n2000-01-05   -0.424972\n2000-01-07    0.404705\nFreq: 2D, Name: A, dtype: float64\n\nIn [29]: s[::-1]\nOut[29]: \n2000-01-08   -0.370647\n2000-01-07    0.404705\n2000-01-06   -0.673690\n2000-01-05   -0.424972\n2000-01-04    0.721555\n2000-01-03   -0.861849\n2000-01-02    1.212112\n2000-01-01    0.469112\nFreq: -1D, Name: A, dtype: float64\n  Note that setting works as well: \nIn [30]: s2 = s.copy()\n\nIn [31]: s2[:5] = 0\n\nIn [32]: s2\nOut[32]: \n2000-01-01    0.000000\n2000-01-02    0.000000\n2000-01-03    0.000000\n2000-01-04    0.000000\n2000-01-05    0.000000\n2000-01-06   -0.673690\n2000-01-07    0.404705\n2000-01-08   -0.370647\nFreq: D, Name: A, dtype: float64\n  With DataFrame, slicing inside of [] slices the rows. This is provided largely as a convenience since it is such a common operation. \nIn [33]: df[:3]\nOut[33]: \n                   A         B         C         D\n2000-01-01  0.469112 -0.282863 -1.509059 -1.135632\n2000-01-02  1.212112 -0.173215  0.119209 -1.044236\n2000-01-03 -0.861849 -2.104569 -0.494929  1.071804\n\nIn [34]: df[::-1]\nOut[34]: \n                   A         B         C         D\n2000-01-08 -0.370647 -1.157892 -1.344312  0.844885\n2000-01-07  0.404705  0.577046 -1.715002 -1.039268\n2000-01-06 -0.673690  0.113648 -1.478427  0.524988\n2000-01-05 -0.424972  0.567020  0.276232 -1.087401\n2000-01-04  0.721555 -0.706771 -1.039575  0.271860\n2000-01-03 -0.861849 -2.104569 -0.494929  1.071804\n2000-01-02  1.212112 -0.173215  0.119209 -1.044236\n2000-01-01  0.469112 -0.282863 -1.509059 -1.135632\n    Selection by label  Warning Whether a copy or a reference is returned for a setting operation, may depend on the context. This is sometimes called chained assignment and should be avoided. See Returning a View versus Copy.   Warning  \n.loc is strict when you present slicers that are not compatible (or convertible) with the index type. For example using integers in a DatetimeIndex. These will raise a TypeError. \n \nIn [35]: dfl = pd.DataFrame(np.random.randn(5, 4),\n   ....:                    columns=list('ABCD'),\n   ....:                    index=pd.date_range('20130101', periods=5))\n   ....: \n\nIn [36]: dfl\nOut[36]: \n                   A         B         C         D\n2013-01-01  1.075770 -0.109050  1.643563 -1.469388\n2013-01-02  0.357021 -0.674600 -1.776904 -0.968914\n2013-01-03 -1.294524  0.413738  0.276662 -0.472035\n2013-01-04 -0.013960 -0.362543 -0.006154 -0.923061\n2013-01-05  0.895717  0.805244 -1.206412  2.565646\n  \nIn [4]: dfl.loc[2:3]\nTypeError: cannot do slice indexing on <class 'pandas.tseries.index.DatetimeIndex'> with these indexers [2] of <type 'int'>\n  String likes in slicing can be convertible to the type of the index and lead to natural slicing. \nIn [37]: dfl.loc['20130102':'20130104']\nOut[37]: \n                   A         B         C         D\n2013-01-02  0.357021 -0.674600 -1.776904 -0.968914\n2013-01-03 -1.294524  0.413738  0.276662 -0.472035\n2013-01-04 -0.013960 -0.362543 -0.006154 -0.923061\n    Warning  Changed in version 1.0.0.  pandas will raise a KeyError if indexing with a list with missing labels. See list-like Using loc with missing keys in a list is Deprecated.  pandas provides a suite of methods in order to have purely label based indexing. This is a strict inclusion based protocol. Every label asked for must be in the index, or a KeyError will be raised. When slicing, both the start bound AND the stop bound are included, if present in the index. Integers are valid labels, but they refer to the label and not the position. The .loc attribute is the primary access method. The following are valid inputs:  A single label, e.g. 5 or 'a' (Note that 5 is interpreted as a label of the index. This use is not an integer position along the index.). A list or array of labels ['a', 'b', 'c']. A slice object with labels 'a':'f' (Note that contrary to usual Python slices, both the start and the stop are included, when present in the index! See Slicing with labels. A boolean array. A callable, see Selection By Callable.  \nIn [38]: s1 = pd.Series(np.random.randn(6), index=list('abcdef'))\n\nIn [39]: s1\nOut[39]: \na    1.431256\nb    1.340309\nc   -1.170299\nd   -0.226169\ne    0.410835\nf    0.813850\ndtype: float64\n\nIn [40]: s1.loc['c':]\nOut[40]: \nc   -1.170299\nd   -0.226169\ne    0.410835\nf    0.813850\ndtype: float64\n\nIn [41]: s1.loc['b']\nOut[41]: 1.3403088497993827\n  Note that setting works as well: \nIn [42]: s1.loc['c':] = 0\n\nIn [43]: s1\nOut[43]: \na    1.431256\nb    1.340309\nc    0.000000\nd    0.000000\ne    0.000000\nf    0.000000\ndtype: float64\n  With a DataFrame: \nIn [44]: df1 = pd.DataFrame(np.random.randn(6, 4),\n   ....:                    index=list('abcdef'),\n   ....:                    columns=list('ABCD'))\n   ....: \n\nIn [45]: df1\nOut[45]: \n          A         B         C         D\na  0.132003 -0.827317 -0.076467 -1.187678\nb  1.130127 -1.436737 -1.413681  1.607920\nc  1.024180  0.569605  0.875906 -2.211372\nd  0.974466 -2.006747 -0.410001 -0.078638\ne  0.545952 -1.219217 -1.226825  0.769804\nf -1.281247 -0.727707 -0.121306 -0.097883\n\nIn [46]: df1.loc[['a', 'b', 'd'], :]\nOut[46]: \n          A         B         C         D\na  0.132003 -0.827317 -0.076467 -1.187678\nb  1.130127 -1.436737 -1.413681  1.607920\nd  0.974466 -2.006747 -0.410001 -0.078638\n  Accessing via label slices: \nIn [47]: df1.loc['d':, 'A':'C']\nOut[47]: \n          A         B         C\nd  0.974466 -2.006747 -0.410001\ne  0.545952 -1.219217 -1.226825\nf -1.281247 -0.727707 -0.121306\n  For getting a cross section using a label (equivalent to df.xs('a')): \nIn [48]: df1.loc['a']\nOut[48]: \nA    0.132003\nB   -0.827317\nC   -0.076467\nD   -1.187678\nName: a, dtype: float64\n  For getting values with a boolean array: \nIn [49]: df1.loc['a'] > 0\nOut[49]: \nA     True\nB    False\nC    False\nD    False\nName: a, dtype: bool\n\nIn [50]: df1.loc[:, df1.loc['a'] > 0]\nOut[50]: \n          A\na  0.132003\nb  1.130127\nc  1.024180\nd  0.974466\ne  0.545952\nf -1.281247\n  NA values in a boolean array propagate as False:  Changed in version 1.0.2.  \nIn [51]: mask = pd.array([True, False, True, False, pd.NA, False], dtype=\"boolean\")\n\nIn [52]: mask\nOut[52]: \n<BooleanArray>\n[True, False, True, False, <NA>, False]\nLength: 6, dtype: boolean\n\nIn [53]: df1[mask]\nOut[53]: \n          A         B         C         D\na  0.132003 -0.827317 -0.076467 -1.187678\nc  1.024180  0.569605  0.875906 -2.211372\n  For getting a value explicitly: \n# this is also equivalent to ``df1.at['a','A']``\nIn [54]: df1.loc['a', 'A']\nOut[54]: 0.13200317033032932\n   Slicing with labels When using .loc with slices, if both the start and the stop labels are present in the index, then elements located between the two (including them) are returned: \nIn [55]: s = pd.Series(list('abcde'), index=[0, 3, 2, 5, 4])\n\nIn [56]: s.loc[3:5]\nOut[56]: \n3    b\n2    c\n5    d\ndtype: object\n  If at least one of the two is absent, but the index is sorted, and can be compared against start and stop labels, then slicing will still work as expected, by selecting labels which rank between the two: \nIn [57]: s.sort_index()\nOut[57]: \n0    a\n2    c\n3    b\n4    e\n5    d\ndtype: object\n\nIn [58]: s.sort_index().loc[1:6]\nOut[58]: \n2    c\n3    b\n4    e\n5    d\ndtype: object\n  However, if at least one of the two is absent and the index is not sorted, an error will be raised (since doing otherwise would be computationally expensive, as well as potentially ambiguous for mixed type indexes). For instance, in the above example, s.loc[1:6] would raise KeyError. For the rationale behind this behavior, see Endpoints are inclusive. \nIn [59]: s = pd.Series(list('abcdef'), index=[0, 3, 2, 5, 4, 2])\n\nIn [60]: s.loc[3:5]\nOut[60]: \n3    b\n2    c\n5    d\ndtype: object\n  Also, if the index has duplicate labels and either the start or the stop label is duplicated, an error will be raised. For instance, in the above example, s.loc[2:5] would raise a KeyError. For more information about duplicate labels, see Duplicate Labels.    Selection by position  Warning Whether a copy or a reference is returned for a setting operation, may depend on the context. This is sometimes called chained assignment and should be avoided. See Returning a View versus Copy.  pandas provides a suite of methods in order to get purely integer based indexing. The semantics follow closely Python and NumPy slicing. These are 0-based indexing. When slicing, the start bound is included, while the upper bound is excluded. Trying to use a non-integer, even a valid label will raise an IndexError. The .iloc attribute is the primary access method. The following are valid inputs:  An integer e.g. 5. A list or array of integers [4, 3, 0]. A slice object with ints 1:7. A boolean array. A callable, see Selection By Callable.  \nIn [61]: s1 = pd.Series(np.random.randn(5), index=list(range(0, 10, 2)))\n\nIn [62]: s1\nOut[62]: \n0    0.695775\n2    0.341734\n4    0.959726\n6   -1.110336\n8   -0.619976\ndtype: float64\n\nIn [63]: s1.iloc[:3]\nOut[63]: \n0    0.695775\n2    0.341734\n4    0.959726\ndtype: float64\n\nIn [64]: s1.iloc[3]\nOut[64]: -1.110336102891167\n  Note that setting works as well: \nIn [65]: s1.iloc[:3] = 0\n\nIn [66]: s1\nOut[66]: \n0    0.000000\n2    0.000000\n4    0.000000\n6   -1.110336\n8   -0.619976\ndtype: float64\n  With a DataFrame: \nIn [67]: df1 = pd.DataFrame(np.random.randn(6, 4),\n   ....:                    index=list(range(0, 12, 2)),\n   ....:                    columns=list(range(0, 8, 2)))\n   ....: \n\nIn [68]: df1\nOut[68]: \n           0         2         4         6\n0   0.149748 -0.732339  0.687738  0.176444\n2   0.403310 -0.154951  0.301624 -2.179861\n4  -1.369849 -0.954208  1.462696 -1.743161\n6  -0.826591 -0.345352  1.314232  0.690579\n8   0.995761  2.396780  0.014871  3.357427\n10 -0.317441 -1.236269  0.896171 -0.487602\n  Select via integer slicing: \nIn [69]: df1.iloc[:3]\nOut[69]: \n          0         2         4         6\n0  0.149748 -0.732339  0.687738  0.176444\n2  0.403310 -0.154951  0.301624 -2.179861\n4 -1.369849 -0.954208  1.462696 -1.743161\n\nIn [70]: df1.iloc[1:5, 2:4]\nOut[70]: \n          4         6\n2  0.301624 -2.179861\n4  1.462696 -1.743161\n6  1.314232  0.690579\n8  0.014871  3.357427\n  Select via integer list: \nIn [71]: df1.iloc[[1, 3, 5], [1, 3]]\nOut[71]: \n           2         6\n2  -0.154951 -2.179861\n6  -0.345352  0.690579\n10 -1.236269 -0.487602\n  \nIn [72]: df1.iloc[1:3, :]\nOut[72]: \n          0         2         4         6\n2  0.403310 -0.154951  0.301624 -2.179861\n4 -1.369849 -0.954208  1.462696 -1.743161\n  \nIn [73]: df1.iloc[:, 1:3]\nOut[73]: \n           2         4\n0  -0.732339  0.687738\n2  -0.154951  0.301624\n4  -0.954208  1.462696\n6  -0.345352  1.314232\n8   2.396780  0.014871\n10 -1.236269  0.896171\n  \n# this is also equivalent to ``df1.iat[1,1]``\nIn [74]: df1.iloc[1, 1]\nOut[74]: -0.1549507744249032\n  For getting a cross section using an integer position (equiv to df.xs(1)): \nIn [75]: df1.iloc[1]\nOut[75]: \n0    0.403310\n2   -0.154951\n4    0.301624\n6   -2.179861\nName: 2, dtype: float64\n  Out of range slice indexes are handled gracefully just as in Python/NumPy. \n# these are allowed in Python/NumPy.\nIn [76]: x = list('abcdef')\n\nIn [77]: x\nOut[77]: ['a', 'b', 'c', 'd', 'e', 'f']\n\nIn [78]: x[4:10]\nOut[78]: ['e', 'f']\n\nIn [79]: x[8:10]\nOut[79]: []\n\nIn [80]: s = pd.Series(x)\n\nIn [81]: s\nOut[81]: \n0    a\n1    b\n2    c\n3    d\n4    e\n5    f\ndtype: object\n\nIn [82]: s.iloc[4:10]\nOut[82]: \n4    e\n5    f\ndtype: object\n\nIn [83]: s.iloc[8:10]\nOut[83]: Series([], dtype: object)\n  Note that using slices that go out of bounds can result in an empty axis (e.g. an empty DataFrame being returned). \nIn [84]: dfl = pd.DataFrame(np.random.randn(5, 2), columns=list('AB'))\n\nIn [85]: dfl\nOut[85]: \n          A         B\n0 -0.082240 -2.182937\n1  0.380396  0.084844\n2  0.432390  1.519970\n3 -0.493662  0.600178\n4  0.274230  0.132885\n\nIn [86]: dfl.iloc[:, 2:3]\nOut[86]: \nEmpty DataFrame\nColumns: []\nIndex: [0, 1, 2, 3, 4]\n\nIn [87]: dfl.iloc[:, 1:3]\nOut[87]: \n          B\n0 -2.182937\n1  0.084844\n2  1.519970\n3  0.600178\n4  0.132885\n\nIn [88]: dfl.iloc[4:6]\nOut[88]: \n         A         B\n4  0.27423  0.132885\n  A single indexer that is out of bounds will raise an IndexError. A list of indexers where any element is out of bounds will raise an IndexError. \n>>> dfl.iloc[[4, 5, 6]]\nIndexError: positional indexers are out-of-bounds\n\n>>> dfl.iloc[:, 4]\nIndexError: single positional indexer is out-of-bounds\n    Selection by callable .loc, .iloc, and also [] indexing can accept a callable as indexer. The callable must be a function with one argument (the calling Series or DataFrame) that returns valid output for indexing. \nIn [89]: df1 = pd.DataFrame(np.random.randn(6, 4),\n   ....:                    index=list('abcdef'),\n   ....:                    columns=list('ABCD'))\n   ....: \n\nIn [90]: df1\nOut[90]: \n          A         B         C         D\na -0.023688  2.410179  1.450520  0.206053\nb -0.251905 -2.213588  1.063327  1.266143\nc  0.299368 -0.863838  0.408204 -1.048089\nd -0.025747 -0.988387  0.094055  1.262731\ne  1.289997  0.082423 -0.055758  0.536580\nf -0.489682  0.369374 -0.034571 -2.484478\n\nIn [91]: df1.loc[lambda df: df['A'] > 0, :]\nOut[91]: \n          A         B         C         D\nc  0.299368 -0.863838  0.408204 -1.048089\ne  1.289997  0.082423 -0.055758  0.536580\n\nIn [92]: df1.loc[:, lambda df: ['A', 'B']]\nOut[92]: \n          A         B\na -0.023688  2.410179\nb -0.251905 -2.213588\nc  0.299368 -0.863838\nd -0.025747 -0.988387\ne  1.289997  0.082423\nf -0.489682  0.369374\n\nIn [93]: df1.iloc[:, lambda df: [0, 1]]\nOut[93]: \n          A         B\na -0.023688  2.410179\nb -0.251905 -2.213588\nc  0.299368 -0.863838\nd -0.025747 -0.988387\ne  1.289997  0.082423\nf -0.489682  0.369374\n\nIn [94]: df1[lambda df: df.columns[0]]\nOut[94]: \na   -0.023688\nb   -0.251905\nc    0.299368\nd   -0.025747\ne    1.289997\nf   -0.489682\nName: A, dtype: float64\n  You can use callable indexing in Series. \nIn [95]: df1['A'].loc[lambda s: s > 0]\nOut[95]: \nc    0.299368\ne    1.289997\nName: A, dtype: float64\n  Using these methods / indexers, you can chain data selection operations without using a temporary variable. \nIn [96]: bb = pd.read_csv('data/baseball.csv', index_col='id')\n\nIn [97]: (bb.groupby(['year', 'team']).sum()\n   ....:    .loc[lambda df: df['r'] > 100])\n   ....: \nOut[97]: \n           stint    g    ab    r    h  X2b  X3b  hr    rbi    sb   cs   bb     so   ibb   hbp    sh    sf  gidp\nyear team                                                                                                      \n2007 CIN       6  379   745  101  203   35    2  36  125.0  10.0  1.0  105  127.0  14.0   1.0   1.0  15.0  18.0\n     DET       5  301  1062  162  283   54    4  37  144.0  24.0  7.0   97  176.0   3.0  10.0   4.0   8.0  28.0\n     HOU       4  311   926  109  218   47    6  14   77.0  10.0  4.0   60  212.0   3.0   9.0  16.0   6.0  17.0\n     LAN      11  413  1021  153  293   61    3  36  154.0   7.0  5.0  114  141.0   8.0   9.0   3.0   8.0  29.0\n     NYN      13  622  1854  240  509  101    3  61  243.0  22.0  4.0  174  310.0  24.0  23.0  18.0  15.0  48.0\n     SFN       5  482  1305  198  337   67    6  40  171.0  26.0  7.0  235  188.0  51.0   8.0  16.0   6.0  41.0\n     TEX       2  198   729  115  200   40    4  28  115.0  21.0  4.0   73  140.0   4.0   5.0   2.0   8.0  16.0\n     TOR       4  459  1408  187  378   96    2  58  223.0   4.0  2.0  190  265.0  16.0  12.0   4.0  16.0  38.0\n    Combining positional and label-based indexing If you wish to get the 0th and the 2nd elements from the index in the \u2018A\u2019 column, you can do: \nIn [98]: dfd = pd.DataFrame({'A': [1, 2, 3],\n   ....:                     'B': [4, 5, 6]},\n   ....:                    index=list('abc'))\n   ....: \n\nIn [99]: dfd\nOut[99]: \n   A  B\na  1  4\nb  2  5\nc  3  6\n\nIn [100]: dfd.loc[dfd.index[[0, 2]], 'A']\nOut[100]: \na    1\nc    3\nName: A, dtype: int64\n  This can also be expressed using .iloc, by explicitly getting locations on the indexers, and using positional indexing to select things. \nIn [101]: dfd.iloc[[0, 2], dfd.columns.get_loc('A')]\nOut[101]: \na    1\nc    3\nName: A, dtype: int64\n  For getting multiple indexers, using .get_indexer: \nIn [102]: dfd.iloc[[0, 2], dfd.columns.get_indexer(['A', 'B'])]\nOut[102]: \n   A  B\na  1  4\nc  3  6\n    Indexing with list with missing labels is deprecated  Warning  Changed in version 1.0.0.  Using .loc or [] with a list with one or more missing labels will no longer reindex, in favor of .reindex.  In prior versions, using .loc[list-of-labels] would work as long as at least 1 of the keys was found (otherwise it would raise a KeyError). This behavior was changed and will now raise a KeyError if at least one label is missing. The recommended alternative is to use .reindex(). For example. \nIn [103]: s = pd.Series([1, 2, 3])\n\nIn [104]: s\nOut[104]: \n0    1\n1    2\n2    3\ndtype: int64\n  Selection with all keys found is unchanged. \nIn [105]: s.loc[[1, 2]]\nOut[105]: \n1    2\n2    3\ndtype: int64\n  Previous behavior \nIn [4]: s.loc[[1, 2, 3]]\nOut[4]:\n1    2.0\n2    3.0\n3    NaN\ndtype: float64\n  Current behavior \nIn [4]: s.loc[[1, 2, 3]]\nPassing list-likes to .loc with any non-matching elements will raise\nKeyError in the future, you can use .reindex() as an alternative.\n\nSee the documentation here:\nhttps://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n\nOut[4]:\n1    2.0\n2    3.0\n3    NaN\ndtype: float64\n   Reindexing The idiomatic way to achieve selecting potentially not-found elements is via .reindex(). See also the section on reindexing. \nIn [106]: s.reindex([1, 2, 3])\nOut[106]: \n1    2.0\n2    3.0\n3    NaN\ndtype: float64\n  Alternatively, if you want to select only valid keys, the following is idiomatic and efficient; it is guaranteed to preserve the dtype of the selection. \nIn [107]: labels = [1, 2, 3]\n\nIn [108]: s.loc[s.index.intersection(labels)]\nOut[108]: \n1    2\n2    3\ndtype: int64\n  Having a duplicated index will raise for a .reindex(): \nIn [109]: s = pd.Series(np.arange(4), index=['a', 'a', 'b', 'c'])\n\nIn [110]: labels = ['c', 'd']\n  \nIn [17]: s.reindex(labels)\nValueError: cannot reindex on an axis with duplicate labels\n  Generally, you can intersect the desired labels with the current axis, and then reindex. \nIn [111]: s.loc[s.index.intersection(labels)].reindex(labels)\nOut[111]: \nc    3.0\nd    NaN\ndtype: float64\n  However, this would still raise if your resulting index is duplicated. \nIn [41]: labels = ['a', 'd']\n\nIn [42]: s.loc[s.index.intersection(labels)].reindex(labels)\nValueError: cannot reindex on an axis with duplicate labels\n     Selecting random samples A random selection of rows or columns from a Series or DataFrame with the sample() method. The method will sample rows by default, and accepts a specific number of rows/columns to return, or a fraction of rows. \nIn [112]: s = pd.Series([0, 1, 2, 3, 4, 5])\n\n# When no arguments are passed, returns 1 row.\nIn [113]: s.sample()\nOut[113]: \n4    4\ndtype: int64\n\n# One may specify either a number of rows:\nIn [114]: s.sample(n=3)\nOut[114]: \n0    0\n4    4\n1    1\ndtype: int64\n\n# Or a fraction of the rows:\nIn [115]: s.sample(frac=0.5)\nOut[115]: \n5    5\n3    3\n1    1\ndtype: int64\n  By default, sample will return each row at most once, but one can also sample with replacement using the replace option: \nIn [116]: s = pd.Series([0, 1, 2, 3, 4, 5])\n\n# Without replacement (default):\nIn [117]: s.sample(n=6, replace=False)\nOut[117]: \n0    0\n1    1\n5    5\n3    3\n2    2\n4    4\ndtype: int64\n\n# With replacement:\nIn [118]: s.sample(n=6, replace=True)\nOut[118]: \n0    0\n4    4\n3    3\n2    2\n4    4\n4    4\ndtype: int64\n  By default, each row has an equal probability of being selected, but if you want rows to have different probabilities, you can pass the sample function sampling weights as weights. These weights can be a list, a NumPy array, or a Series, but they must be of the same length as the object you are sampling. Missing values will be treated as a weight of zero, and inf values are not allowed. If weights do not sum to 1, they will be re-normalized by dividing all weights by the sum of the weights. For example: \nIn [119]: s = pd.Series([0, 1, 2, 3, 4, 5])\n\nIn [120]: example_weights = [0, 0, 0.2, 0.2, 0.2, 0.4]\n\nIn [121]: s.sample(n=3, weights=example_weights)\nOut[121]: \n5    5\n4    4\n3    3\ndtype: int64\n\n# Weights will be re-normalized automatically\nIn [122]: example_weights2 = [0.5, 0, 0, 0, 0, 0]\n\nIn [123]: s.sample(n=1, weights=example_weights2)\nOut[123]: \n0    0\ndtype: int64\n  When applied to a DataFrame, you can use a column of the DataFrame as sampling weights (provided you are sampling rows and not columns) by simply passing the name of the column as a string. \nIn [124]: df2 = pd.DataFrame({'col1': [9, 8, 7, 6],\n   .....:                     'weight_column': [0.5, 0.4, 0.1, 0]})\n   .....: \n\nIn [125]: df2.sample(n=3, weights='weight_column')\nOut[125]: \n   col1  weight_column\n1     8            0.4\n0     9            0.5\n2     7            0.1\n  sample also allows users to sample columns instead of rows using the axis argument. \nIn [126]: df3 = pd.DataFrame({'col1': [1, 2, 3], 'col2': [2, 3, 4]})\n\nIn [127]: df3.sample(n=1, axis=1)\nOut[127]: \n   col1\n0     1\n1     2\n2     3\n  Finally, one can also set a seed for sample\u2019s random number generator using the random_state argument, which will accept either an integer (as a seed) or a NumPy RandomState object. \nIn [128]: df4 = pd.DataFrame({'col1': [1, 2, 3], 'col2': [2, 3, 4]})\n\n# With a given seed, the sample will always draw the same rows.\nIn [129]: df4.sample(n=2, random_state=2)\nOut[129]: \n   col1  col2\n2     3     4\n1     2     3\n\nIn [130]: df4.sample(n=2, random_state=2)\nOut[130]: \n   col1  col2\n2     3     4\n1     2     3\n    Setting with enlargement The .loc/[] operations can perform enlargement when setting a non-existent key for that axis. In the Series case this is effectively an appending operation. \nIn [131]: se = pd.Series([1, 2, 3])\n\nIn [132]: se\nOut[132]: \n0    1\n1    2\n2    3\ndtype: int64\n\nIn [133]: se[5] = 5.\n\nIn [134]: se\nOut[134]: \n0    1.0\n1    2.0\n2    3.0\n5    5.0\ndtype: float64\n  A DataFrame can be enlarged on either axis via .loc. \nIn [135]: dfi = pd.DataFrame(np.arange(6).reshape(3, 2),\n   .....:                    columns=['A', 'B'])\n   .....: \n\nIn [136]: dfi\nOut[136]: \n   A  B\n0  0  1\n1  2  3\n2  4  5\n\nIn [137]: dfi.loc[:, 'C'] = dfi.loc[:, 'A']\n\nIn [138]: dfi\nOut[138]: \n   A  B  C\n0  0  1  0\n1  2  3  2\n2  4  5  4\n  This is like an append operation on the DataFrame. \nIn [139]: dfi.loc[3] = 5\n\nIn [140]: dfi\nOut[140]: \n   A  B  C\n0  0  1  0\n1  2  3  2\n2  4  5  4\n3  5  5  5\n    Fast scalar value getting and setting Since indexing with [] must handle a lot of cases (single-label access, slicing, boolean indexing, etc.), it has a bit of overhead in order to figure out what you\u2019re asking for. If you only want to access a scalar value, the fastest way is to use the at and iat methods, which are implemented on all of the data structures. Similarly to loc, at provides label based scalar lookups, while, iat provides integer based lookups analogously to iloc \nIn [141]: s.iat[5]\nOut[141]: 5\n\nIn [142]: df.at[dates[5], 'A']\nOut[142]: -0.6736897080883706\n\nIn [143]: df.iat[3, 0]\nOut[143]: 0.7215551622443669\n  You can also set using these same indexers. \nIn [144]: df.at[dates[5], 'E'] = 7\n\nIn [145]: df.iat[3, 0] = 7\n  at may enlarge the object in-place as above if the indexer is missing. \nIn [146]: df.at[dates[-1] + pd.Timedelta('1 day'), 0] = 7\n\nIn [147]: df\nOut[147]: \n                   A         B         C         D    E    0\n2000-01-01  0.469112 -0.282863 -1.509059 -1.135632  NaN  NaN\n2000-01-02  1.212112 -0.173215  0.119209 -1.044236  NaN  NaN\n2000-01-03 -0.861849 -2.104569 -0.494929  1.071804  NaN  NaN\n2000-01-04  7.000000 -0.706771 -1.039575  0.271860  NaN  NaN\n2000-01-05 -0.424972  0.567020  0.276232 -1.087401  NaN  NaN\n2000-01-06 -0.673690  0.113648 -1.478427  0.524988  7.0  NaN\n2000-01-07  0.404705  0.577046 -1.715002 -1.039268  NaN  NaN\n2000-01-08 -0.370647 -1.157892 -1.344312  0.844885  NaN  NaN\n2000-01-09       NaN       NaN       NaN       NaN  NaN  7.0\n    Boolean indexing Another common operation is the use of boolean vectors to filter the data. The operators are: | for or, & for and, and ~ for not. These must be grouped by using parentheses, since by default Python will evaluate an expression such as df['A'] > 2 & df['B'] < 3 as df['A'] > (2 & df['B']) < 3, while the desired evaluation order is (df['A'] > 2) & (df['B'] < 3). Using a boolean vector to index a Series works exactly as in a NumPy ndarray: \nIn [148]: s = pd.Series(range(-3, 4))\n\nIn [149]: s\nOut[149]: \n0   -3\n1   -2\n2   -1\n3    0\n4    1\n5    2\n6    3\ndtype: int64\n\nIn [150]: s[s > 0]\nOut[150]: \n4    1\n5    2\n6    3\ndtype: int64\n\nIn [151]: s[(s < -1) | (s > 0.5)]\nOut[151]: \n0   -3\n1   -2\n4    1\n5    2\n6    3\ndtype: int64\n\nIn [152]: s[~(s < 0)]\nOut[152]: \n3    0\n4    1\n5    2\n6    3\ndtype: int64\n  You may select rows from a DataFrame using a boolean vector the same length as the DataFrame\u2019s index (for example, something derived from one of the columns of the DataFrame): \nIn [153]: df[df['A'] > 0]\nOut[153]: \n                   A         B         C         D   E   0\n2000-01-01  0.469112 -0.282863 -1.509059 -1.135632 NaN NaN\n2000-01-02  1.212112 -0.173215  0.119209 -1.044236 NaN NaN\n2000-01-04  7.000000 -0.706771 -1.039575  0.271860 NaN NaN\n2000-01-07  0.404705  0.577046 -1.715002 -1.039268 NaN NaN\n  List comprehensions and the map method of Series can also be used to produce more complex criteria: \nIn [154]: df2 = pd.DataFrame({'a': ['one', 'one', 'two', 'three', 'two', 'one', 'six'],\n   .....:                     'b': ['x', 'y', 'y', 'x', 'y', 'x', 'x'],\n   .....:                     'c': np.random.randn(7)})\n   .....: \n\n# only want 'two' or 'three'\nIn [155]: criterion = df2['a'].map(lambda x: x.startswith('t'))\n\nIn [156]: df2[criterion]\nOut[156]: \n       a  b         c\n2    two  y  0.041290\n3  three  x  0.361719\n4    two  y -0.238075\n\n# equivalent but slower\nIn [157]: df2[[x.startswith('t') for x in df2['a']]]\nOut[157]: \n       a  b         c\n2    two  y  0.041290\n3  three  x  0.361719\n4    two  y -0.238075\n\n# Multiple criteria\nIn [158]: df2[criterion & (df2['b'] == 'x')]\nOut[158]: \n       a  b         c\n3  three  x  0.361719\n  With the choice methods Selection by Label, Selection by Position, and Advanced Indexing you may select along more than one axis using boolean vectors combined with other indexing expressions. \nIn [159]: df2.loc[criterion & (df2['b'] == 'x'), 'b':'c']\nOut[159]: \n   b         c\n3  x  0.361719\n   Warning iloc supports two kinds of boolean indexing. If the indexer is a boolean Series, an error will be raised. For instance, in the following example, df.iloc[s.values, 1] is ok. The boolean indexer is an array. But df.iloc[s, 1] would raise ValueError. \nIn [160]: df = pd.DataFrame([[1, 2], [3, 4], [5, 6]],\n   .....:                   index=list('abc'),\n   .....:                   columns=['A', 'B'])\n   .....: \n\nIn [161]: s = (df['A'] > 2)\n\nIn [162]: s\nOut[162]: \na    False\nb     True\nc     True\nName: A, dtype: bool\n\nIn [163]: df.loc[s, 'B']\nOut[163]: \nb    4\nc    6\nName: B, dtype: int64\n\nIn [164]: df.iloc[s.values, 1]\nOut[164]: \nb    4\nc    6\nName: B, dtype: int64\n     Indexing with isin Consider the isin() method of Series, which returns a boolean vector that is true wherever the Series elements exist in the passed list. This allows you to select rows where one or more columns have values you want: \nIn [165]: s = pd.Series(np.arange(5), index=np.arange(5)[::-1], dtype='int64')\n\nIn [166]: s\nOut[166]: \n4    0\n3    1\n2    2\n1    3\n0    4\ndtype: int64\n\nIn [167]: s.isin([2, 4, 6])\nOut[167]: \n4    False\n3    False\n2     True\n1    False\n0     True\ndtype: bool\n\nIn [168]: s[s.isin([2, 4, 6])]\nOut[168]: \n2    2\n0    4\ndtype: int64\n  The same method is available for Index objects and is useful for the cases when you don\u2019t know which of the sought labels are in fact present: \nIn [169]: s[s.index.isin([2, 4, 6])]\nOut[169]: \n4    0\n2    2\ndtype: int64\n\n# compare it to the following\nIn [170]: s.reindex([2, 4, 6])\nOut[170]: \n2    2.0\n4    0.0\n6    NaN\ndtype: float64\n  In addition to that, MultiIndex allows selecting a separate level to use in the membership check: \nIn [171]: s_mi = pd.Series(np.arange(6),\n   .....:                  index=pd.MultiIndex.from_product([[0, 1], ['a', 'b', 'c']]))\n   .....: \n\nIn [172]: s_mi\nOut[172]: \n0  a    0\n   b    1\n   c    2\n1  a    3\n   b    4\n   c    5\ndtype: int64\n\nIn [173]: s_mi.iloc[s_mi.index.isin([(1, 'a'), (2, 'b'), (0, 'c')])]\nOut[173]: \n0  c    2\n1  a    3\ndtype: int64\n\nIn [174]: s_mi.iloc[s_mi.index.isin(['a', 'c', 'e'], level=1)]\nOut[174]: \n0  a    0\n   c    2\n1  a    3\n   c    5\ndtype: int64\n  DataFrame also has an isin() method. When calling isin, pass a set of values as either an array or dict. If values is an array, isin returns a DataFrame of booleans that is the same shape as the original DataFrame, with True wherever the element is in the sequence of values. \nIn [175]: df = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': ['a', 'b', 'f', 'n'],\n   .....:                    'ids2': ['a', 'n', 'c', 'n']})\n   .....: \n\nIn [176]: values = ['a', 'b', 1, 3]\n\nIn [177]: df.isin(values)\nOut[177]: \n    vals    ids   ids2\n0   True   True   True\n1  False   True  False\n2   True  False  False\n3  False  False  False\n  Oftentimes you\u2019ll want to match certain values with certain columns. Just make values a dict where the key is the column, and the value is a list of items you want to check for. \nIn [178]: values = {'ids': ['a', 'b'], 'vals': [1, 3]}\n\nIn [179]: df.isin(values)\nOut[179]: \n    vals    ids   ids2\n0   True   True  False\n1  False   True  False\n2   True  False  False\n3  False  False  False\n  To return the DataFrame of booleans where the values are not in the original DataFrame, use the ~ operator: \nIn [180]: values = {'ids': ['a', 'b'], 'vals': [1, 3]}\n\nIn [181]: ~df.isin(values)\nOut[181]: \n    vals    ids  ids2\n0  False  False  True\n1   True  False  True\n2  False   True  True\n3   True   True  True\n  Combine DataFrame\u2019s isin with the any() and all() methods to quickly select subsets of your data that meet a given criteria. To select a row where each column meets its own criterion: \nIn [182]: values = {'ids': ['a', 'b'], 'ids2': ['a', 'c'], 'vals': [1, 3]}\n\nIn [183]: row_mask = df.isin(values).all(1)\n\nIn [184]: df[row_mask]\nOut[184]: \n   vals ids ids2\n0     1   a    a\n    The where() Method and Masking Selecting values from a Series with a boolean vector generally returns a subset of the data. To guarantee that selection output has the same shape as the original data, you can use the where method in Series and DataFrame. To return only the selected rows: \nIn [185]: s[s > 0]\nOut[185]: \n3    1\n2    2\n1    3\n0    4\ndtype: int64\n  To return a Series of the same shape as the original: \nIn [186]: s.where(s > 0)\nOut[186]: \n4    NaN\n3    1.0\n2    2.0\n1    3.0\n0    4.0\ndtype: float64\n  Selecting values from a DataFrame with a boolean criterion now also preserves input data shape. where is used under the hood as the implementation. The code below is equivalent to df.where(df < 0). \nIn [187]: df[df < 0]\nOut[187]: \n                   A         B         C         D\n2000-01-01 -2.104139 -1.309525       NaN       NaN\n2000-01-02 -0.352480       NaN -1.192319       NaN\n2000-01-03 -0.864883       NaN -0.227870       NaN\n2000-01-04       NaN -1.222082       NaN -1.233203\n2000-01-05       NaN -0.605656 -1.169184       NaN\n2000-01-06       NaN -0.948458       NaN -0.684718\n2000-01-07 -2.670153 -0.114722       NaN -0.048048\n2000-01-08       NaN       NaN -0.048788 -0.808838\n  In addition, where takes an optional other argument for replacement of values where the condition is False, in the returned copy. \nIn [188]: df.where(df < 0, -df)\nOut[188]: \n                   A         B         C         D\n2000-01-01 -2.104139 -1.309525 -0.485855 -0.245166\n2000-01-02 -0.352480 -0.390389 -1.192319 -1.655824\n2000-01-03 -0.864883 -0.299674 -0.227870 -0.281059\n2000-01-04 -0.846958 -1.222082 -0.600705 -1.233203\n2000-01-05 -0.669692 -0.605656 -1.169184 -0.342416\n2000-01-06 -0.868584 -0.948458 -2.297780 -0.684718\n2000-01-07 -2.670153 -0.114722 -0.168904 -0.048048\n2000-01-08 -0.801196 -1.392071 -0.048788 -0.808838\n  You may wish to set values based on some boolean criteria. This can be done intuitively like so: \nIn [189]: s2 = s.copy()\n\nIn [190]: s2[s2 < 0] = 0\n\nIn [191]: s2\nOut[191]: \n4    0\n3    1\n2    2\n1    3\n0    4\ndtype: int64\n\nIn [192]: df2 = df.copy()\n\nIn [193]: df2[df2 < 0] = 0\n\nIn [194]: df2\nOut[194]: \n                   A         B         C         D\n2000-01-01  0.000000  0.000000  0.485855  0.245166\n2000-01-02  0.000000  0.390389  0.000000  1.655824\n2000-01-03  0.000000  0.299674  0.000000  0.281059\n2000-01-04  0.846958  0.000000  0.600705  0.000000\n2000-01-05  0.669692  0.000000  0.000000  0.342416\n2000-01-06  0.868584  0.000000  2.297780  0.000000\n2000-01-07  0.000000  0.000000  0.168904  0.000000\n2000-01-08  0.801196  1.392071  0.000000  0.000000\n  By default, where returns a modified copy of the data. There is an optional parameter inplace so that the original data can be modified without creating a copy: \nIn [195]: df_orig = df.copy()\n\nIn [196]: df_orig.where(df > 0, -df, inplace=True)\n\nIn [197]: df_orig\nOut[197]: \n                   A         B         C         D\n2000-01-01  2.104139  1.309525  0.485855  0.245166\n2000-01-02  0.352480  0.390389  1.192319  1.655824\n2000-01-03  0.864883  0.299674  0.227870  0.281059\n2000-01-04  0.846958  1.222082  0.600705  1.233203\n2000-01-05  0.669692  0.605656  1.169184  0.342416\n2000-01-06  0.868584  0.948458  2.297780  0.684718\n2000-01-07  2.670153  0.114722  0.168904  0.048048\n2000-01-08  0.801196  1.392071  0.048788  0.808838\n   Note The signature for DataFrame.where() differs from numpy.where(). Roughly df1.where(m, df2) is equivalent to np.where(m, df1, df2). \nIn [198]: df.where(df < 0, -df) == np.where(df < 0, df, -df)\nOut[198]: \n               A     B     C     D\n2000-01-01  True  True  True  True\n2000-01-02  True  True  True  True\n2000-01-03  True  True  True  True\n2000-01-04  True  True  True  True\n2000-01-05  True  True  True  True\n2000-01-06  True  True  True  True\n2000-01-07  True  True  True  True\n2000-01-08  True  True  True  True\n   Alignment Furthermore, where aligns the input boolean condition (ndarray or DataFrame), such that partial selection with setting is possible. This is analogous to partial setting via .loc (but on the contents rather than the axis labels). \nIn [199]: df2 = df.copy()\n\nIn [200]: df2[df2[1:4] > 0] = 3\n\nIn [201]: df2\nOut[201]: \n                   A         B         C         D\n2000-01-01 -2.104139 -1.309525  0.485855  0.245166\n2000-01-02 -0.352480  3.000000 -1.192319  3.000000\n2000-01-03 -0.864883  3.000000 -0.227870  3.000000\n2000-01-04  3.000000 -1.222082  3.000000 -1.233203\n2000-01-05  0.669692 -0.605656 -1.169184  0.342416\n2000-01-06  0.868584 -0.948458  2.297780 -0.684718\n2000-01-07 -2.670153 -0.114722  0.168904 -0.048048\n2000-01-08  0.801196  1.392071 -0.048788 -0.808838\n  Where can also accept axis and level parameters to align the input when performing the where. \nIn [202]: df2 = df.copy()\n\nIn [203]: df2.where(df2 > 0, df2['A'], axis='index')\nOut[203]: \n                   A         B         C         D\n2000-01-01 -2.104139 -2.104139  0.485855  0.245166\n2000-01-02 -0.352480  0.390389 -0.352480  1.655824\n2000-01-03 -0.864883  0.299674 -0.864883  0.281059\n2000-01-04  0.846958  0.846958  0.600705  0.846958\n2000-01-05  0.669692  0.669692  0.669692  0.342416\n2000-01-06  0.868584  0.868584  2.297780  0.868584\n2000-01-07 -2.670153 -2.670153  0.168904 -2.670153\n2000-01-08  0.801196  1.392071  0.801196  0.801196\n  This is equivalent to (but faster than) the following. \nIn [204]: df2 = df.copy()\n\nIn [205]: df.apply(lambda x, y: x.where(x > 0, y), y=df['A'])\nOut[205]: \n                   A         B         C         D\n2000-01-01 -2.104139 -2.104139  0.485855  0.245166\n2000-01-02 -0.352480  0.390389 -0.352480  1.655824\n2000-01-03 -0.864883  0.299674 -0.864883  0.281059\n2000-01-04  0.846958  0.846958  0.600705  0.846958\n2000-01-05  0.669692  0.669692  0.669692  0.342416\n2000-01-06  0.868584  0.868584  2.297780  0.868584\n2000-01-07 -2.670153 -2.670153  0.168904 -2.670153\n2000-01-08  0.801196  1.392071  0.801196  0.801196\n  where can accept a callable as condition and other arguments. The function must be with one argument (the calling Series or DataFrame) and that returns valid output as condition and other argument. \nIn [206]: df3 = pd.DataFrame({'A': [1, 2, 3],\n   .....:                     'B': [4, 5, 6],\n   .....:                     'C': [7, 8, 9]})\n   .....: \n\nIn [207]: df3.where(lambda x: x > 4, lambda x: x + 10)\nOut[207]: \n    A   B  C\n0  11  14  7\n1  12   5  8\n2  13   6  9\n   Mask mask() is the inverse boolean operation of where. \nIn [208]: s.mask(s >= 0)\nOut[208]: \n4   NaN\n3   NaN\n2   NaN\n1   NaN\n0   NaN\ndtype: float64\n\nIn [209]: df.mask(df >= 0)\nOut[209]: \n                   A         B         C         D\n2000-01-01 -2.104139 -1.309525       NaN       NaN\n2000-01-02 -0.352480       NaN -1.192319       NaN\n2000-01-03 -0.864883       NaN -0.227870       NaN\n2000-01-04       NaN -1.222082       NaN -1.233203\n2000-01-05       NaN -0.605656 -1.169184       NaN\n2000-01-06       NaN -0.948458       NaN -0.684718\n2000-01-07 -2.670153 -0.114722       NaN -0.048048\n2000-01-08       NaN       NaN -0.048788 -0.808838\n     Setting with enlargement conditionally using numpy()\n An alternative to where() is to use numpy.where(). Combined with setting a new column, you can use it to enlarge a DataFrame where the values are determined conditionally. Consider you have two choices to choose from in the following DataFrame. And you want to set a new column color to \u2018green\u2019 when the second column has \u2018Z\u2019. You can do the following: \nIn [210]: df = pd.DataFrame({'col1': list('ABBC'), 'col2': list('ZZXY')})\n\nIn [211]: df['color'] = np.where(df['col2'] == 'Z', 'green', 'red')\n\nIn [212]: df\nOut[212]: \n  col1 col2  color\n0    A    Z  green\n1    B    Z  green\n2    B    X    red\n3    C    Y    red\n  If you have multiple conditions, you can use numpy.select() to achieve that. Say corresponding to three conditions there are three choice of colors, with a fourth color as a fallback, you can do the following. \nIn [213]: conditions = [\n   .....:     (df['col2'] == 'Z') & (df['col1'] == 'A'),\n   .....:     (df['col2'] == 'Z') & (df['col1'] == 'B'),\n   .....:     (df['col1'] == 'B')\n   .....: ]\n   .....: \n\nIn [214]: choices = ['yellow', 'blue', 'purple']\n\nIn [215]: df['color'] = np.select(conditions, choices, default='black')\n\nIn [216]: df\nOut[216]: \n  col1 col2   color\n0    A    Z  yellow\n1    B    Z    blue\n2    B    X  purple\n3    C    Y   black\n    The query() Method DataFrame objects have a query() method that allows selection using an expression. You can get the value of the frame where column b has values between the values of columns a and c. For example: \nIn [217]: n = 10\n\nIn [218]: df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))\n\nIn [219]: df\nOut[219]: \n          a         b         c\n0  0.438921  0.118680  0.863670\n1  0.138138  0.577363  0.686602\n2  0.595307  0.564592  0.520630\n3  0.913052  0.926075  0.616184\n4  0.078718  0.854477  0.898725\n5  0.076404  0.523211  0.591538\n6  0.792342  0.216974  0.564056\n7  0.397890  0.454131  0.915716\n8  0.074315  0.437913  0.019794\n9  0.559209  0.502065  0.026437\n\n# pure python\nIn [220]: df[(df['a'] < df['b']) & (df['b'] < df['c'])]\nOut[220]: \n          a         b         c\n1  0.138138  0.577363  0.686602\n4  0.078718  0.854477  0.898725\n5  0.076404  0.523211  0.591538\n7  0.397890  0.454131  0.915716\n\n# query\nIn [221]: df.query('(a < b) & (b < c)')\nOut[221]: \n          a         b         c\n1  0.138138  0.577363  0.686602\n4  0.078718  0.854477  0.898725\n5  0.076404  0.523211  0.591538\n7  0.397890  0.454131  0.915716\n  Do the same thing but fall back on a named index if there is no column with the name a. \nIn [222]: df = pd.DataFrame(np.random.randint(n / 2, size=(n, 2)), columns=list('bc'))\n\nIn [223]: df.index.name = 'a'\n\nIn [224]: df\nOut[224]: \n   b  c\na      \n0  0  4\n1  0  1\n2  3  4\n3  4  3\n4  1  4\n5  0  3\n6  0  1\n7  3  4\n8  2  3\n9  1  1\n\nIn [225]: df.query('a < b and b < c')\nOut[225]: \n   b  c\na      \n2  3  4\n  If instead you don\u2019t want to or cannot name your index, you can use the name index in your query expression: \nIn [226]: df = pd.DataFrame(np.random.randint(n, size=(n, 2)), columns=list('bc'))\n\nIn [227]: df\nOut[227]: \n   b  c\n0  3  1\n1  3  0\n2  5  6\n3  5  2\n4  7  4\n5  0  1\n6  2  5\n7  0  1\n8  6  0\n9  7  9\n\nIn [228]: df.query('index < b < c')\nOut[228]: \n   b  c\n2  5  6\n   Note If the name of your index overlaps with a column name, the column name is given precedence. For example, \nIn [229]: df = pd.DataFrame({'a': np.random.randint(5, size=5)})\n\nIn [230]: df.index.name = 'a'\n\nIn [231]: df.query('a > 2')  # uses the column 'a', not the index\nOut[231]: \n   a\na   \n1  3\n3  3\n  You can still use the index in a query expression by using the special identifier \u2018index\u2019: \nIn [232]: df.query('index > 2')\nOut[232]: \n   a\na   \n3  3\n4  2\n  If for some reason you have a column named index, then you can refer to the index as ilevel_0 as well, but at this point you should consider renaming your columns to something less ambiguous.   \nMultiIndex query() Syntax You can also use the levels of a DataFrame with a MultiIndex as if they were columns in the frame: \nIn [233]: n = 10\n\nIn [234]: colors = np.random.choice(['red', 'green'], size=n)\n\nIn [235]: foods = np.random.choice(['eggs', 'ham'], size=n)\n\nIn [236]: colors\nOut[236]: \narray(['red', 'red', 'red', 'green', 'green', 'green', 'green', 'green',\n       'green', 'green'], dtype='<U5')\n\nIn [237]: foods\nOut[237]: \narray(['ham', 'ham', 'eggs', 'eggs', 'eggs', 'ham', 'ham', 'eggs', 'eggs',\n       'eggs'], dtype='<U4')\n\nIn [238]: index = pd.MultiIndex.from_arrays([colors, foods], names=['color', 'food'])\n\nIn [239]: df = pd.DataFrame(np.random.randn(n, 2), index=index)\n\nIn [240]: df\nOut[240]: \n                   0         1\ncolor food                    \nred   ham   0.194889 -0.381994\n      ham   0.318587  2.089075\n      eggs -0.728293 -0.090255\ngreen eggs -0.748199  1.318931\n      eggs -2.029766  0.792652\n      ham   0.461007 -0.542749\n      ham  -0.305384 -0.479195\n      eggs  0.095031 -0.270099\n      eggs -0.707140 -0.773882\n      eggs  0.229453  0.304418\n\nIn [241]: df.query('color == \"red\"')\nOut[241]: \n                   0         1\ncolor food                    \nred   ham   0.194889 -0.381994\n      ham   0.318587  2.089075\n      eggs -0.728293 -0.090255\n  If the levels of the MultiIndex are unnamed, you can refer to them using special names: \nIn [242]: df.index.names = [None, None]\n\nIn [243]: df\nOut[243]: \n                   0         1\nred   ham   0.194889 -0.381994\n      ham   0.318587  2.089075\n      eggs -0.728293 -0.090255\ngreen eggs -0.748199  1.318931\n      eggs -2.029766  0.792652\n      ham   0.461007 -0.542749\n      ham  -0.305384 -0.479195\n      eggs  0.095031 -0.270099\n      eggs -0.707140 -0.773882\n      eggs  0.229453  0.304418\n\nIn [244]: df.query('ilevel_0 == \"red\"')\nOut[244]: \n                 0         1\nred ham   0.194889 -0.381994\n    ham   0.318587  2.089075\n    eggs -0.728293 -0.090255\n  The convention is ilevel_0, which means \u201cindex level 0\u201d for the 0th level of the index.   \nquery() Use Cases A use case for query() is when you have a collection of DataFrame objects that have a subset of column names (or index levels/names) in common. You can pass the same query to both frames without having to specify which frame you\u2019re interested in querying \nIn [245]: df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))\n\nIn [246]: df\nOut[246]: \n          a         b         c\n0  0.224283  0.736107  0.139168\n1  0.302827  0.657803  0.713897\n2  0.611185  0.136624  0.984960\n3  0.195246  0.123436  0.627712\n4  0.618673  0.371660  0.047902\n5  0.480088  0.062993  0.185760\n6  0.568018  0.483467  0.445289\n7  0.309040  0.274580  0.587101\n8  0.258993  0.477769  0.370255\n9  0.550459  0.840870  0.304611\n\nIn [247]: df2 = pd.DataFrame(np.random.rand(n + 2, 3), columns=df.columns)\n\nIn [248]: df2\nOut[248]: \n           a         b         c\n0   0.357579  0.229800  0.596001\n1   0.309059  0.957923  0.965663\n2   0.123102  0.336914  0.318616\n3   0.526506  0.323321  0.860813\n4   0.518736  0.486514  0.384724\n5   0.190804  0.505723  0.614533\n6   0.891939  0.623977  0.676639\n7   0.480559  0.378528  0.460858\n8   0.420223  0.136404  0.141295\n9   0.732206  0.419540  0.604675\n10  0.604466  0.848974  0.896165\n11  0.589168  0.920046  0.732716\n\nIn [249]: expr = '0.0 <= a <= c <= 0.5'\n\nIn [250]: map(lambda frame: frame.query(expr), [df, df2])\nOut[250]: <map at 0x7fe27a78f280>\n    \nquery() Python versus pandas Syntax Comparison Full numpy-like syntax: \nIn [251]: df = pd.DataFrame(np.random.randint(n, size=(n, 3)), columns=list('abc'))\n\nIn [252]: df\nOut[252]: \n   a  b  c\n0  7  8  9\n1  1  0  7\n2  2  7  2\n3  6  2  2\n4  2  6  3\n5  3  8  2\n6  1  7  2\n7  5  1  5\n8  9  8  0\n9  1  5  0\n\nIn [253]: df.query('(a < b) & (b < c)')\nOut[253]: \n   a  b  c\n0  7  8  9\n\nIn [254]: df[(df['a'] < df['b']) & (df['b'] < df['c'])]\nOut[254]: \n   a  b  c\n0  7  8  9\n  Slightly nicer by removing the parentheses (comparison operators bind tighter than & and |): \nIn [255]: df.query('a < b & b < c')\nOut[255]: \n   a  b  c\n0  7  8  9\n  Use English instead of symbols: \nIn [256]: df.query('a < b and b < c')\nOut[256]: \n   a  b  c\n0  7  8  9\n  Pretty close to how you might write it on paper: \nIn [257]: df.query('a < b < c')\nOut[257]: \n   a  b  c\n0  7  8  9\n    The in and not in operators query() also supports special use of Python\u2019s in and not in comparison operators, providing a succinct syntax for calling the isin method of a Series or DataFrame. \n# get all rows where columns \"a\" and \"b\" have overlapping values\nIn [258]: df = pd.DataFrame({'a': list('aabbccddeeff'), 'b': list('aaaabbbbcccc'),\n   .....:                    'c': np.random.randint(5, size=12),\n   .....:                    'd': np.random.randint(9, size=12)})\n   .....: \n\nIn [259]: df\nOut[259]: \n    a  b  c  d\n0   a  a  2  6\n1   a  a  4  7\n2   b  a  1  6\n3   b  a  2  1\n4   c  b  3  6\n5   c  b  0  2\n6   d  b  3  3\n7   d  b  2  1\n8   e  c  4  3\n9   e  c  2  0\n10  f  c  0  6\n11  f  c  1  2\n\nIn [260]: df.query('a in b')\nOut[260]: \n   a  b  c  d\n0  a  a  2  6\n1  a  a  4  7\n2  b  a  1  6\n3  b  a  2  1\n4  c  b  3  6\n5  c  b  0  2\n\n# How you'd do it in pure Python\nIn [261]: df[df['a'].isin(df['b'])]\nOut[261]: \n   a  b  c  d\n0  a  a  2  6\n1  a  a  4  7\n2  b  a  1  6\n3  b  a  2  1\n4  c  b  3  6\n5  c  b  0  2\n\nIn [262]: df.query('a not in b')\nOut[262]: \n    a  b  c  d\n6   d  b  3  3\n7   d  b  2  1\n8   e  c  4  3\n9   e  c  2  0\n10  f  c  0  6\n11  f  c  1  2\n\n# pure Python\nIn [263]: df[~df['a'].isin(df['b'])]\nOut[263]: \n    a  b  c  d\n6   d  b  3  3\n7   d  b  2  1\n8   e  c  4  3\n9   e  c  2  0\n10  f  c  0  6\n11  f  c  1  2\n  You can combine this with other expressions for very succinct queries: \n# rows where cols a and b have overlapping values\n# and col c's values are less than col d's\nIn [264]: df.query('a in b and c < d')\nOut[264]: \n   a  b  c  d\n0  a  a  2  6\n1  a  a  4  7\n2  b  a  1  6\n4  c  b  3  6\n5  c  b  0  2\n\n# pure Python\nIn [265]: df[df['b'].isin(df['a']) & (df['c'] < df['d'])]\nOut[265]: \n    a  b  c  d\n0   a  a  2  6\n1   a  a  4  7\n2   b  a  1  6\n4   c  b  3  6\n5   c  b  0  2\n10  f  c  0  6\n11  f  c  1  2\n   Note Note that in and not in are evaluated in Python, since numexpr has no equivalent of this operation. However, only the in/not in expression itself is evaluated in vanilla Python. For example, in the expression \ndf.query('a in b + c + d')\n  (b + c + d) is evaluated by numexpr and then the in operation is evaluated in plain Python. In general, any operations that can be evaluated using numexpr will be.    Special use of the == operator with list objects Comparing a list of values to a column using ==/!= works similarly to in/not in. \nIn [266]: df.query('b == [\"a\", \"b\", \"c\"]')\nOut[266]: \n    a  b  c  d\n0   a  a  2  6\n1   a  a  4  7\n2   b  a  1  6\n3   b  a  2  1\n4   c  b  3  6\n5   c  b  0  2\n6   d  b  3  3\n7   d  b  2  1\n8   e  c  4  3\n9   e  c  2  0\n10  f  c  0  6\n11  f  c  1  2\n\n# pure Python\nIn [267]: df[df['b'].isin([\"a\", \"b\", \"c\"])]\nOut[267]: \n    a  b  c  d\n0   a  a  2  6\n1   a  a  4  7\n2   b  a  1  6\n3   b  a  2  1\n4   c  b  3  6\n5   c  b  0  2\n6   d  b  3  3\n7   d  b  2  1\n8   e  c  4  3\n9   e  c  2  0\n10  f  c  0  6\n11  f  c  1  2\n\nIn [268]: df.query('c == [1, 2]')\nOut[268]: \n    a  b  c  d\n0   a  a  2  6\n2   b  a  1  6\n3   b  a  2  1\n7   d  b  2  1\n9   e  c  2  0\n11  f  c  1  2\n\nIn [269]: df.query('c != [1, 2]')\nOut[269]: \n    a  b  c  d\n1   a  a  4  7\n4   c  b  3  6\n5   c  b  0  2\n6   d  b  3  3\n8   e  c  4  3\n10  f  c  0  6\n\n# using in/not in\nIn [270]: df.query('[1, 2] in c')\nOut[270]: \n    a  b  c  d\n0   a  a  2  6\n2   b  a  1  6\n3   b  a  2  1\n7   d  b  2  1\n9   e  c  2  0\n11  f  c  1  2\n\nIn [271]: df.query('[1, 2] not in c')\nOut[271]: \n    a  b  c  d\n1   a  a  4  7\n4   c  b  3  6\n5   c  b  0  2\n6   d  b  3  3\n8   e  c  4  3\n10  f  c  0  6\n\n# pure Python\nIn [272]: df[df['c'].isin([1, 2])]\nOut[272]: \n    a  b  c  d\n0   a  a  2  6\n2   b  a  1  6\n3   b  a  2  1\n7   d  b  2  1\n9   e  c  2  0\n11  f  c  1  2\n    Boolean operators You can negate boolean expressions with the word not or the ~ operator. \nIn [273]: df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))\n\nIn [274]: df['bools'] = np.random.rand(len(df)) > 0.5\n\nIn [275]: df.query('~bools')\nOut[275]: \n          a         b         c  bools\n2  0.697753  0.212799  0.329209  False\n7  0.275396  0.691034  0.826619  False\n8  0.190649  0.558748  0.262467  False\n\nIn [276]: df.query('not bools')\nOut[276]: \n          a         b         c  bools\n2  0.697753  0.212799  0.329209  False\n7  0.275396  0.691034  0.826619  False\n8  0.190649  0.558748  0.262467  False\n\nIn [277]: df.query('not bools') == df[~df['bools']]\nOut[277]: \n      a     b     c  bools\n2  True  True  True   True\n7  True  True  True   True\n8  True  True  True   True\n  Of course, expressions can be arbitrarily complex too: \n# short query syntax\nIn [278]: shorter = df.query('a < b < c and (not bools) or bools > 2')\n\n# equivalent in pure Python\nIn [279]: longer = df[(df['a'] < df['b'])\n   .....:             & (df['b'] < df['c'])\n   .....:             & (~df['bools'])\n   .....:             | (df['bools'] > 2)]\n   .....: \n\nIn [280]: shorter\nOut[280]: \n          a         b         c  bools\n7  0.275396  0.691034  0.826619  False\n\nIn [281]: longer\nOut[281]: \n          a         b         c  bools\n7  0.275396  0.691034  0.826619  False\n\nIn [282]: shorter == longer\nOut[282]: \n      a     b     c  bools\n7  True  True  True   True\n    Performance of query()\n DataFrame.query() using numexpr is slightly faster than Python for large frames.   Note You will only see the performance benefits of using the numexpr engine with DataFrame.query() if your frame has more than approximately 200,000 rows.  \n \n  This plot was created using a DataFrame with 3 columns each containing floating point values generated using numpy.random.randn().    Duplicate data If you want to identify and remove duplicate rows in a DataFrame, there are two methods that will help: duplicated and drop_duplicates. Each takes as an argument the columns to use to identify duplicated rows.  duplicated returns a boolean vector whose length is the number of rows, and which indicates whether a row is duplicated. drop_duplicates removes duplicate rows.  By default, the first observed row of a duplicate set is considered unique, but each method has a keep parameter to specify targets to be kept.  keep='first' (default): mark / drop duplicates except for the first occurrence. keep='last': mark / drop duplicates except for the last occurrence. keep=False: mark / drop all duplicates.  \nIn [283]: df2 = pd.DataFrame({'a': ['one', 'one', 'two', 'two', 'two', 'three', 'four'],\n   .....:                     'b': ['x', 'y', 'x', 'y', 'x', 'x', 'x'],\n   .....:                     'c': np.random.randn(7)})\n   .....: \n\nIn [284]: df2\nOut[284]: \n       a  b         c\n0    one  x -1.067137\n1    one  y  0.309500\n2    two  x -0.211056\n3    two  y -1.842023\n4    two  x -0.390820\n5  three  x -1.964475\n6   four  x  1.298329\n\nIn [285]: df2.duplicated('a')\nOut[285]: \n0    False\n1     True\n2    False\n3     True\n4     True\n5    False\n6    False\ndtype: bool\n\nIn [286]: df2.duplicated('a', keep='last')\nOut[286]: \n0     True\n1    False\n2     True\n3     True\n4    False\n5    False\n6    False\ndtype: bool\n\nIn [287]: df2.duplicated('a', keep=False)\nOut[287]: \n0     True\n1     True\n2     True\n3     True\n4     True\n5    False\n6    False\ndtype: bool\n\nIn [288]: df2.drop_duplicates('a')\nOut[288]: \n       a  b         c\n0    one  x -1.067137\n2    two  x -0.211056\n5  three  x -1.964475\n6   four  x  1.298329\n\nIn [289]: df2.drop_duplicates('a', keep='last')\nOut[289]: \n       a  b         c\n1    one  y  0.309500\n4    two  x -0.390820\n5  three  x -1.964475\n6   four  x  1.298329\n\nIn [290]: df2.drop_duplicates('a', keep=False)\nOut[290]: \n       a  b         c\n5  three  x -1.964475\n6   four  x  1.298329\n  Also, you can pass a list of columns to identify duplications. \nIn [291]: df2.duplicated(['a', 'b'])\nOut[291]: \n0    False\n1    False\n2    False\n3    False\n4     True\n5    False\n6    False\ndtype: bool\n\nIn [292]: df2.drop_duplicates(['a', 'b'])\nOut[292]: \n       a  b         c\n0    one  x -1.067137\n1    one  y  0.309500\n2    two  x -0.211056\n3    two  y -1.842023\n5  three  x -1.964475\n6   four  x  1.298329\n  To drop duplicates by index value, use Index.duplicated then perform slicing. The same set of options are available for the keep parameter. \nIn [293]: df3 = pd.DataFrame({'a': np.arange(6),\n   .....:                     'b': np.random.randn(6)},\n   .....:                    index=['a', 'a', 'b', 'c', 'b', 'a'])\n   .....: \n\nIn [294]: df3\nOut[294]: \n   a         b\na  0  1.440455\na  1  2.456086\nb  2  1.038402\nc  3 -0.894409\nb  4  0.683536\na  5  3.082764\n\nIn [295]: df3.index.duplicated()\nOut[295]: array([False,  True, False, False,  True,  True])\n\nIn [296]: df3[~df3.index.duplicated()]\nOut[296]: \n   a         b\na  0  1.440455\nb  2  1.038402\nc  3 -0.894409\n\nIn [297]: df3[~df3.index.duplicated(keep='last')]\nOut[297]: \n   a         b\nc  3 -0.894409\nb  4  0.683536\na  5  3.082764\n\nIn [298]: df3[~df3.index.duplicated(keep=False)]\nOut[298]: \n   a         b\nc  3 -0.894409\n    Dictionary-like get() method Each of Series or DataFrame have a get method which can return a default value. \nIn [299]: s = pd.Series([1, 2, 3], index=['a', 'b', 'c'])\n\nIn [300]: s.get('a')  # equivalent to s['a']\nOut[300]: 1\n\nIn [301]: s.get('x', default=-1)\nOut[301]: -1\n    Looking up values by index/column labels Sometimes you want to extract a set of values given a sequence of row labels and column labels, this can be achieved by pandas.factorize and NumPy indexing. For instance: \nIn [302]: df = pd.DataFrame({'col': [\"A\", \"A\", \"B\", \"B\"],\n   .....:                    'A': [80, 23, np.nan, 22],\n   .....:                    'B': [80, 55, 76, 67]})\n   .....: \n\nIn [303]: df\nOut[303]: \n  col     A   B\n0   A  80.0  80\n1   A  23.0  55\n2   B   NaN  76\n3   B  22.0  67\n\nIn [304]: idx, cols = pd.factorize(df['col'])\n\nIn [305]: df.reindex(cols, axis=1).to_numpy()[np.arange(len(df)), idx]\nOut[305]: array([80., 23., 76., 67.])\n  Formerly this could be achieved with the dedicated DataFrame.lookup method which was deprecated in version 1.2.0.   Index objects The pandas Index class and its subclasses can be viewed as implementing an ordered multiset. Duplicates are allowed. However, if you try to convert an Index object with duplicate entries into a set, an exception will be raised. Index also provides the infrastructure necessary for lookups, data alignment, and reindexing. The easiest way to create an Index directly is to pass a list or other sequence to Index: \nIn [306]: index = pd.Index(['e', 'd', 'a', 'b'])\n\nIn [307]: index\nOut[307]: Index(['e', 'd', 'a', 'b'], dtype='object')\n\nIn [308]: 'd' in index\nOut[308]: True\n  You can also pass a name to be stored in the index: \nIn [309]: index = pd.Index(['e', 'd', 'a', 'b'], name='something')\n\nIn [310]: index.name\nOut[310]: 'something'\n  The name, if set, will be shown in the console display: \nIn [311]: index = pd.Index(list(range(5)), name='rows')\n\nIn [312]: columns = pd.Index(['A', 'B', 'C'], name='cols')\n\nIn [313]: df = pd.DataFrame(np.random.randn(5, 3), index=index, columns=columns)\n\nIn [314]: df\nOut[314]: \ncols         A         B         C\nrows                              \n0     1.295989 -1.051694  1.340429\n1    -2.366110  0.428241  0.387275\n2     0.433306  0.929548  0.278094\n3     2.154730 -0.315628  0.264223\n4     1.126818  1.132290 -0.353310\n\nIn [315]: df['A']\nOut[315]: \nrows\n0    1.295989\n1   -2.366110\n2    0.433306\n3    2.154730\n4    1.126818\nName: A, dtype: float64\n   Setting metadata Indexes are \u201cmostly immutable\u201d, but it is possible to set and change their name attribute. You can use the rename, set_names to set these attributes directly, and they default to returning a copy. See Advanced Indexing for usage of MultiIndexes. \nIn [316]: ind = pd.Index([1, 2, 3])\n\nIn [317]: ind.rename(\"apple\")\nOut[317]: Int64Index([1, 2, 3], dtype='int64', name='apple')\n\nIn [318]: ind\nOut[318]: Int64Index([1, 2, 3], dtype='int64')\n\nIn [319]: ind.set_names([\"apple\"], inplace=True)\n\nIn [320]: ind.name = \"bob\"\n\nIn [321]: ind\nOut[321]: Int64Index([1, 2, 3], dtype='int64', name='bob')\n  set_names, set_levels, and set_codes also take an optional level argument \nIn [322]: index = pd.MultiIndex.from_product([range(3), ['one', 'two']], names=['first', 'second'])\n\nIn [323]: index\nOut[323]: \nMultiIndex([(0, 'one'),\n            (0, 'two'),\n            (1, 'one'),\n            (1, 'two'),\n            (2, 'one'),\n            (2, 'two')],\n           names=['first', 'second'])\n\nIn [324]: index.levels[1]\nOut[324]: Index(['one', 'two'], dtype='object', name='second')\n\nIn [325]: index.set_levels([\"a\", \"b\"], level=1)\nOut[325]: \nMultiIndex([(0, 'a'),\n            (0, 'b'),\n            (1, 'a'),\n            (1, 'b'),\n            (2, 'a'),\n            (2, 'b')],\n           names=['first', 'second'])\n    Set operations on Index objects The two main operations are union and intersection. Difference is provided via the .difference() method. \nIn [326]: a = pd.Index(['c', 'b', 'a'])\n\nIn [327]: b = pd.Index(['c', 'e', 'd'])\n\nIn [328]: a.difference(b)\nOut[328]: Index(['a', 'b'], dtype='object')\n  Also available is the symmetric_difference operation, which returns elements that appear in either idx1 or idx2, but not in both. This is equivalent to the Index created by idx1.difference(idx2).union(idx2.difference(idx1)), with duplicates dropped. \nIn [329]: idx1 = pd.Index([1, 2, 3, 4])\n\nIn [330]: idx2 = pd.Index([2, 3, 4, 5])\n\nIn [331]: idx1.symmetric_difference(idx2)\nOut[331]: Int64Index([1, 5], dtype='int64')\n   Note The resulting index from a set operation will be sorted in ascending order.  When performing Index.union() between indexes with different dtypes, the indexes must be cast to a common dtype. Typically, though not always, this is object dtype. The exception is when performing a union between integer and float data. In this case, the integer values are converted to float \nIn [332]: idx1 = pd.Index([0, 1, 2])\n\nIn [333]: idx2 = pd.Index([0.5, 1.5])\n\nIn [334]: idx1.union(idx2)\nOut[334]: Float64Index([0.0, 0.5, 1.0, 1.5, 2.0], dtype='float64')\n    Missing values  Important Even though Index can hold missing values (NaN), it should be avoided if you do not want any unexpected results. For example, some operations exclude missing values implicitly.  Index.fillna fills missing values with specified scalar value. \nIn [335]: idx1 = pd.Index([1, np.nan, 3, 4])\n\nIn [336]: idx1\nOut[336]: Float64Index([1.0, nan, 3.0, 4.0], dtype='float64')\n\nIn [337]: idx1.fillna(2)\nOut[337]: Float64Index([1.0, 2.0, 3.0, 4.0], dtype='float64')\n\nIn [338]: idx2 = pd.DatetimeIndex([pd.Timestamp('2011-01-01'),\n   .....:                          pd.NaT,\n   .....:                          pd.Timestamp('2011-01-03')])\n   .....: \n\nIn [339]: idx2\nOut[339]: DatetimeIndex(['2011-01-01', 'NaT', '2011-01-03'], dtype='datetime64[ns]', freq=None)\n\nIn [340]: idx2.fillna(pd.Timestamp('2011-01-02'))\nOut[340]: DatetimeIndex(['2011-01-01', '2011-01-02', '2011-01-03'], dtype='datetime64[ns]', freq=None)\n     Set / reset index Occasionally you will load or create a data set into a DataFrame and want to add an index after you\u2019ve already done so. There are a couple of different ways.  Set an index DataFrame has a set_index() method which takes a column name (for a regular Index) or a list of column names (for a MultiIndex). To create a new, re-indexed DataFrame: \nIn [341]: data\nOut[341]: \n     a    b  c    d\n0  bar  one  z  1.0\n1  bar  two  y  2.0\n2  foo  one  x  3.0\n3  foo  two  w  4.0\n\nIn [342]: indexed1 = data.set_index('c')\n\nIn [343]: indexed1\nOut[343]: \n     a    b    d\nc               \nz  bar  one  1.0\ny  bar  two  2.0\nx  foo  one  3.0\nw  foo  two  4.0\n\nIn [344]: indexed2 = data.set_index(['a', 'b'])\n\nIn [345]: indexed2\nOut[345]: \n         c    d\na   b          \nbar one  z  1.0\n    two  y  2.0\nfoo one  x  3.0\n    two  w  4.0\n  The append keyword option allow you to keep the existing index and append the given columns to a MultiIndex: \nIn [346]: frame = data.set_index('c', drop=False)\n\nIn [347]: frame = frame.set_index(['a', 'b'], append=True)\n\nIn [348]: frame\nOut[348]: \n           c    d\nc a   b          \nz bar one  z  1.0\ny bar two  y  2.0\nx foo one  x  3.0\nw foo two  w  4.0\n  Other options in set_index allow you not drop the index columns or to add the index in-place (without creating a new object): \nIn [349]: data.set_index('c', drop=False)\nOut[349]: \n     a    b  c    d\nc                  \nz  bar  one  z  1.0\ny  bar  two  y  2.0\nx  foo  one  x  3.0\nw  foo  two  w  4.0\n\nIn [350]: data.set_index(['a', 'b'], inplace=True)\n\nIn [351]: data\nOut[351]: \n         c    d\na   b          \nbar one  z  1.0\n    two  y  2.0\nfoo one  x  3.0\n    two  w  4.0\n    Reset the index As a convenience, there is a new function on DataFrame called reset_index() which transfers the index values into the DataFrame\u2019s columns and sets a simple integer index. This is the inverse operation of set_index(). \nIn [352]: data\nOut[352]: \n         c    d\na   b          \nbar one  z  1.0\n    two  y  2.0\nfoo one  x  3.0\n    two  w  4.0\n\nIn [353]: data.reset_index()\nOut[353]: \n     a    b  c    d\n0  bar  one  z  1.0\n1  bar  two  y  2.0\n2  foo  one  x  3.0\n3  foo  two  w  4.0\n  The output is more similar to a SQL table or a record array. The names for the columns derived from the index are the ones stored in the names attribute. You can use the level keyword to remove only a portion of the index: \nIn [354]: frame\nOut[354]: \n           c    d\nc a   b          \nz bar one  z  1.0\ny bar two  y  2.0\nx foo one  x  3.0\nw foo two  w  4.0\n\nIn [355]: frame.reset_index(level=1)\nOut[355]: \n         a  c    d\nc b               \nz one  bar  z  1.0\ny two  bar  y  2.0\nx one  foo  x  3.0\nw two  foo  w  4.0\n  reset_index takes an optional parameter drop which if true simply discards the index, instead of putting index values in the DataFrame\u2019s columns.   Adding an ad hoc index If you create an index yourself, you can just assign it to the index field: \ndata.index = index\n     Returning a view versus a copy When setting values in a pandas object, care must be taken to avoid what is called chained indexing. Here is an example. \nIn [356]: dfmi = pd.DataFrame([list('abcd'),\n   .....:                      list('efgh'),\n   .....:                      list('ijkl'),\n   .....:                      list('mnop')],\n   .....:                     columns=pd.MultiIndex.from_product([['one', 'two'],\n   .....:                                                         ['first', 'second']]))\n   .....: \n\nIn [357]: dfmi\nOut[357]: \n    one          two       \n  first second first second\n0     a      b     c      d\n1     e      f     g      h\n2     i      j     k      l\n3     m      n     o      p\n  Compare these two access methods: \nIn [358]: dfmi['one']['second']\nOut[358]: \n0    b\n1    f\n2    j\n3    n\nName: second, dtype: object\n  \nIn [359]: dfmi.loc[:, ('one', 'second')]\nOut[359]: \n0    b\n1    f\n2    j\n3    n\nName: (one, second), dtype: object\n  These both yield the same results, so which should you use? It is instructive to understand the order of operations on these and why method 2 (.loc) is much preferred over method 1 (chained []). dfmi['one'] selects the first level of the columns and returns a DataFrame that is singly-indexed. Then another Python operation dfmi_with_one['second'] selects the series indexed by 'second'. This is indicated by the variable dfmi_with_one because pandas sees these operations as separate events. e.g. separate calls to __getitem__, so it has to treat them as linear operations, they happen one after another. Contrast this to df.loc[:,('one','second')] which passes a nested tuple of (slice(None),('one','second')) to a single call to __getitem__. This allows pandas to deal with this as a single entity. Furthermore this order of operations can be significantly faster, and allows one to index both axes if so desired.  Why does assignment fail when using chained indexing? The problem in the previous section is just a performance issue. What\u2019s up with the SettingWithCopy warning? We don\u2019t usually throw warnings around when you do something that might cost a few extra milliseconds! But it turns out that assigning to the product of chained indexing has inherently unpredictable results. To see this, think about how the Python interpreter executes this code: \ndfmi.loc[:, ('one', 'second')] = value\n# becomes\ndfmi.loc.__setitem__((slice(None), ('one', 'second')), value)\n  But this code is handled differently: \ndfmi['one']['second'] = value\n# becomes\ndfmi.__getitem__('one').__setitem__('second', value)\n  See that __getitem__ in there? Outside of simple cases, it\u2019s very hard to predict whether it will return a view or a copy (it depends on the memory layout of the array, about which pandas makes no guarantees), and therefore whether the __setitem__ will modify dfmi or a temporary object that gets thrown out immediately afterward. That\u2019s what SettingWithCopy is warning you about!  Note You may be wondering whether we should be concerned about the loc property in the first example. But dfmi.loc is guaranteed to be dfmi itself with modified indexing behavior, so dfmi.loc.__getitem__ / dfmi.loc.__setitem__ operate on dfmi directly. Of course, dfmi.loc.__getitem__(idx) may be a view or a copy of dfmi.  Sometimes a SettingWithCopy warning will arise at times when there\u2019s no obvious chained indexing going on. These are the bugs that SettingWithCopy is designed to catch! pandas is probably trying to warn you that you\u2019ve done this: \ndef do_something(df):\n    foo = df[['bar', 'baz']]  # Is foo a view? A copy? Nobody knows!\n    # ... many lines here ...\n    # We don't know whether this will modify df or not!\n    foo['quux'] = value\n    return foo\n  Yikes!   Evaluation order matters When you use chained indexing, the order and type of the indexing operation partially determine whether the result is a slice into the original object, or a copy of the slice. pandas has the SettingWithCopyWarning because assigning to a copy of a slice is frequently not intentional, but a mistake caused by chained indexing returning a copy where a slice was expected. If you would like pandas to be more or less trusting about assignment to a chained indexing expression, you can set the option mode.chained_assignment to one of these values:  'warn', the default, means a SettingWithCopyWarning is printed. 'raise' means pandas will raise a SettingWithCopyException you have to deal with. None will suppress the warnings entirely.  \nIn [360]: dfb = pd.DataFrame({'a': ['one', 'one', 'two',\n   .....:                           'three', 'two', 'one', 'six'],\n   .....:                     'c': np.arange(7)})\n   .....: \n\n# This will show the SettingWithCopyWarning\n# but the frame values will be set\nIn [361]: dfb['c'][dfb['a'].str.startswith('o')] = 42\n  This however is operating on a copy and will not work. \n>>> pd.set_option('mode.chained_assignment','warn')\n>>> dfb[dfb['a'].str.startswith('o')]['c'] = 42\nTraceback (most recent call last)\n     ...\nSettingWithCopyWarning:\n     A value is trying to be set on a copy of a slice from a DataFrame.\n     Try using .loc[row_index,col_indexer] = value instead\n  A chained assignment can also crop up in setting in a mixed dtype frame.  Note These setting rules apply to all of .loc/.iloc.  The following is the recommended access method using .loc for multiple items (using mask) and a single item using a fixed index: \nIn [362]: dfc = pd.DataFrame({'a': ['one', 'one', 'two',\n   .....:                           'three', 'two', 'one', 'six'],\n   .....:                     'c': np.arange(7)})\n   .....: \n\nIn [363]: dfd = dfc.copy()\n\n# Setting multiple items using a mask\nIn [364]: mask = dfd['a'].str.startswith('o')\n\nIn [365]: dfd.loc[mask, 'c'] = 42\n\nIn [366]: dfd\nOut[366]: \n       a   c\n0    one  42\n1    one  42\n2    two   2\n3  three   3\n4    two   4\n5    one  42\n6    six   6\n\n# Setting a single item\nIn [367]: dfd = dfc.copy()\n\nIn [368]: dfd.loc[2, 'a'] = 11\n\nIn [369]: dfd\nOut[369]: \n       a  c\n0    one  0\n1    one  1\n2     11  2\n3  three  3\n4    two  4\n5    one  5\n6    six  6\n  The following can work at times, but it is not guaranteed to, and therefore should be avoided: \nIn [370]: dfd = dfc.copy()\n\nIn [371]: dfd['a'][2] = 111\n\nIn [372]: dfd\nOut[372]: \n       a  c\n0    one  0\n1    one  1\n2    111  2\n3  three  3\n4    two  4\n5    one  5\n6    six  6\n  Last, the subsequent example will not work at all, and so should be avoided: \n>>> pd.set_option('mode.chained_assignment','raise')\n>>> dfd.loc[0]['a'] = 1111\nTraceback (most recent call last)\n     ...\nSettingWithCopyException:\n     A value is trying to be set on a copy of a slice from a DataFrame.\n     Try using .loc[row_index,col_indexer] = value instead\n   Warning The chained assignment warnings / exceptions are aiming to inform the user of a possibly invalid assignment. There may be false positives; situations where a chained assignment is inadvertently reported.   \n"}, {"name": "Input/output", "path": "reference/io", "type": "Input/output", "text": "Input/output  Pickling       \nread_pickle(filepath_or_buffer[, ...]) Load pickled pandas object (or any object) from file.  \nDataFrame.to_pickle(path[, compression, ...]) Pickle (serialize) object to file.      Flat file       \nread_table(filepath_or_buffer[, sep, ...]) Read general delimited file into DataFrame.  \nread_csv(filepath_or_buffer[, sep, ...]) Read a comma-separated values (csv) file into DataFrame.  \nDataFrame.to_csv([path_or_buf, sep, na_rep, ...]) Write object to a comma-separated values (csv) file.  \nread_fwf(filepath_or_buffer[, colspecs, ...]) Read a table of fixed-width formatted lines into DataFrame.      Clipboard       \nread_clipboard([sep]) Read text from clipboard and pass to read_csv.  \nDataFrame.to_clipboard([excel, sep]) Copy object to the system clipboard.      Excel       \nread_excel(io[, sheet_name, header, names, ...]) Read an Excel file into a pandas DataFrame.  \nDataFrame.to_excel(excel_writer[, ...]) Write object to an Excel sheet.  \nExcelFile.parse([sheet_name, header, names, ...]) Parse specified sheet(s) into a DataFrame.          \nStyler.to_excel(excel_writer[, sheet_name, ...]) Write Styler to an Excel sheet.          \nExcelWriter(path[, engine, date_format, ...]) Class for writing DataFrame objects into excel sheets.      JSON       \nread_json([path_or_buf, orient, typ, dtype, ...]) Convert a JSON string to pandas object.  \njson_normalize(data[, record_path, meta, ...]) Normalize semi-structured JSON data into a flat table.  \nDataFrame.to_json([path_or_buf, orient, ...]) Convert the object to a JSON string.          \nbuild_table_schema(data[, index, ...]) Create a Table schema from data.      HTML       \nread_html(io[, match, flavor, header, ...]) Read HTML tables into a list of DataFrame objects.  \nDataFrame.to_html([buf, columns, col_space, ...]) Render a DataFrame as an HTML table.          \nStyler.to_html([buf, table_uuid, ...]) Write Styler to a file, buffer or string in HTML-CSS format.      XML       \nread_xml(path_or_buffer[, xpath, ...]) Read XML document into a DataFrame object.  \nDataFrame.to_xml([path_or_buffer, index, ...]) Render a DataFrame to an XML document.      Latex       \nDataFrame.to_latex([buf, columns, ...]) Render object to a LaTeX tabular, longtable, or nested table.          \nStyler.to_latex([buf, column_format, ...]) Write Styler to a file, buffer or string in LaTeX format.      HDFStore: PyTables (HDF5)       \nread_hdf(path_or_buf[, key, mode, errors, ...]) Read from the store, close it if we opened it.  \nHDFStore.put(key, value[, format, index, ...]) Store object in HDFStore.  \nHDFStore.append(key, value[, format, axes, ...]) Append to Table in file.  \nHDFStore.get(key) Retrieve pandas object stored in file.  \nHDFStore.select(key[, where, start, stop, ...]) Retrieve pandas object stored in file, optionally based on where criteria.  \nHDFStore.info() Print detailed information on the store.  \nHDFStore.keys([include]) Return a list of keys corresponding to objects stored in HDFStore.  \nHDFStore.groups() Return a list of all the top-level nodes.  \nHDFStore.walk([where]) Walk the pytables group hierarchy for pandas objects.     Warning One can store a subclass of DataFrame or Series to HDF5, but the type of the subclass is lost upon storing.    Feather       \nread_feather(path[, columns, use_threads, ...]) Load a feather-format object from the file path.  \nDataFrame.to_feather(path, **kwargs) Write a DataFrame to the binary Feather format.      Parquet       \nread_parquet(path[, engine, columns, ...]) Load a parquet object from the file path, returning a DataFrame.  \nDataFrame.to_parquet([path, engine, ...]) Write a DataFrame to the binary parquet format.      ORC       \nread_orc(path[, columns]) Load an ORC object from the file path, returning a DataFrame.      SAS       \nread_sas(filepath_or_buffer[, format, ...]) Read SAS files stored as either XPORT or SAS7BDAT format files.      SPSS       \nread_spss(path[, usecols, convert_categoricals]) Load an SPSS file from the file path, returning a DataFrame.      SQL       \nread_sql_table(table_name, con[, schema, ...]) Read SQL database table into a DataFrame.  \nread_sql_query(sql, con[, index_col, ...]) Read SQL query into a DataFrame.  \nread_sql(sql, con[, index_col, ...]) Read SQL query or database table into a DataFrame.  \nDataFrame.to_sql(name, con[, schema, ...]) Write records stored in a DataFrame to a SQL database.      Google BigQuery       \nread_gbq(query[, project_id, index_col, ...]) Load data from Google BigQuery.      STATA       \nread_stata(filepath_or_buffer[, ...]) Read Stata file into DataFrame.  \nDataFrame.to_stata(path[, convert_dates, ...]) Export DataFrame object to Stata dta format.          \nStataReader.data_label Return data label of Stata file.  \nStataReader.value_labels() Return a dict, associating each variable name a dict, associating each value its corresponding label.  \nStataReader.variable_labels() Return variable labels as a dict, associating each variable name with corresponding label.  \nStataWriter.write_file() Export DataFrame object to Stata dta format.    \n"}, {"name": "Intro to data structures", "path": "user_guide/dsintro", "type": "Manual", "text": "Intro to data structures We\u2019ll start with a quick, non-comprehensive overview of the fundamental data structures in pandas to get you started. The fundamental behavior about data types, indexing, and axis labeling / alignment apply across all of the objects. To get started, import NumPy and load pandas into your namespace: \nIn [1]: import numpy as np\n\nIn [2]: import pandas as pd\n  Here is a basic tenet to keep in mind: data alignment is intrinsic. The link between labels and data will not be broken unless done so explicitly by you. We\u2019ll give a brief intro to the data structures, then consider all of the broad categories of functionality and methods in separate sections.  Series Series is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the index. The basic method to create a Series is to call: \n>>> s = pd.Series(data, index=index)\n  Here, data can be many different things:  a Python dict an ndarray a scalar value (like 5)  The passed index is a list of axis labels. Thus, this separates into a few cases depending on what data is: From ndarray If data is an ndarray, index must be the same length as data. If no index is passed, one will be created having values [0, ..., len(data) - 1]. \nIn [3]: s = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n\nIn [4]: s\nOut[4]: \na    0.469112\nb   -0.282863\nc   -1.509059\nd   -1.135632\ne    1.212112\ndtype: float64\n\nIn [5]: s.index\nOut[5]: Index(['a', 'b', 'c', 'd', 'e'], dtype='object')\n\nIn [6]: pd.Series(np.random.randn(5))\nOut[6]: \n0   -0.173215\n1    0.119209\n2   -1.044236\n3   -0.861849\n4   -2.104569\ndtype: float64\n   Note pandas supports non-unique index values. If an operation that does not support duplicate index values is attempted, an exception will be raised at that time. The reason for being lazy is nearly all performance-based (there are many instances in computations, like parts of GroupBy, where the index is not used).  From dict Series can be instantiated from dicts: \nIn [7]: d = {\"b\": 1, \"a\": 0, \"c\": 2}\n\nIn [8]: pd.Series(d)\nOut[8]: \nb    1\na    0\nc    2\ndtype: int64\n   Note When the data is a dict, and an index is not passed, the Series index will be ordered by the dict\u2019s insertion order, if you\u2019re using Python version >= 3.6 and pandas version >= 0.23. If you\u2019re using Python < 3.6 or pandas < 0.23, and an index is not passed, the Series index will be the lexically ordered list of dict keys.  In the example above, if you were on a Python version lower than 3.6 or a pandas version lower than 0.23, the Series would be ordered by the lexical order of the dict keys (i.e. ['a', 'b', 'c'] rather than ['b', 'a', 'c']). If an index is passed, the values in data corresponding to the labels in the index will be pulled out. \nIn [9]: d = {\"a\": 0.0, \"b\": 1.0, \"c\": 2.0}\n\nIn [10]: pd.Series(d)\nOut[10]: \na    0.0\nb    1.0\nc    2.0\ndtype: float64\n\nIn [11]: pd.Series(d, index=[\"b\", \"c\", \"d\", \"a\"])\nOut[11]: \nb    1.0\nc    2.0\nd    NaN\na    0.0\ndtype: float64\n   Note NaN (not a number) is the standard missing data marker used in pandas.  From scalar value If data is a scalar value, an index must be provided. The value will be repeated to match the length of index. \nIn [12]: pd.Series(5.0, index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\nOut[12]: \na    5.0\nb    5.0\nc    5.0\nd    5.0\ne    5.0\ndtype: float64\n   Series is ndarray-like Series acts very similarly to a ndarray, and is a valid argument to most NumPy functions. However, operations such as slicing will also slice the index. \nIn [13]: s[0]\nOut[13]: 0.4691122999071863\n\nIn [14]: s[:3]\nOut[14]: \na    0.469112\nb   -0.282863\nc   -1.509059\ndtype: float64\n\nIn [15]: s[s > s.median()]\nOut[15]: \na    0.469112\ne    1.212112\ndtype: float64\n\nIn [16]: s[[4, 3, 1]]\nOut[16]: \ne    1.212112\nd   -1.135632\nb   -0.282863\ndtype: float64\n\nIn [17]: np.exp(s)\nOut[17]: \na    1.598575\nb    0.753623\nc    0.221118\nd    0.321219\ne    3.360575\ndtype: float64\n   Note We will address array-based indexing like s[[4, 3, 1]] in section on indexing.  Like a NumPy array, a pandas Series has a dtype. \nIn [18]: s.dtype\nOut[18]: dtype('float64')\n  This is often a NumPy dtype. However, pandas and 3rd-party libraries extend NumPy\u2019s type system in a few places, in which case the dtype would be an ExtensionDtype. Some examples within pandas are Categorical data and Nullable integer data type. See dtypes for more. If you need the actual array backing a Series, use Series.array. \nIn [19]: s.array\nOut[19]: \n<PandasArray>\n[ 0.4691122999071863, -0.2828633443286633, -1.5090585031735124,\n -1.1356323710171934,  1.2121120250208506]\nLength: 5, dtype: float64\n  Accessing the array can be useful when you need to do some operation without the index (to disable automatic alignment, for example). Series.array will always be an ExtensionArray. Briefly, an ExtensionArray is a thin wrapper around one or more concrete arrays like a numpy.ndarray. pandas knows how to take an ExtensionArray and store it in a Series or a column of a DataFrame. See dtypes for more. While Series is ndarray-like, if you need an actual ndarray, then use Series.to_numpy(). \nIn [20]: s.to_numpy()\nOut[20]: array([ 0.4691, -0.2829, -1.5091, -1.1356,  1.2121])\n  Even if the Series is backed by a ExtensionArray, Series.to_numpy() will return a NumPy ndarray.   Series is dict-like A Series is like a fixed-size dict in that you can get and set values by index label: \nIn [21]: s[\"a\"]\nOut[21]: 0.4691122999071863\n\nIn [22]: s[\"e\"] = 12.0\n\nIn [23]: s\nOut[23]: \na     0.469112\nb    -0.282863\nc    -1.509059\nd    -1.135632\ne    12.000000\ndtype: float64\n\nIn [24]: \"e\" in s\nOut[24]: True\n\nIn [25]: \"f\" in s\nOut[25]: False\n  If a label is not contained, an exception is raised: \n>>> s[\"f\"]\nKeyError: 'f'\n  Using the get method, a missing label will return None or specified default: \nIn [26]: s.get(\"f\")\n\nIn [27]: s.get(\"f\", np.nan)\nOut[27]: nan\n  See also the section on attribute access.   Vectorized operations and label alignment with Series When working with raw NumPy arrays, looping through value-by-value is usually not necessary. The same is true when working with Series in pandas. Series can also be passed into most NumPy methods expecting an ndarray. \nIn [28]: s + s\nOut[28]: \na     0.938225\nb    -0.565727\nc    -3.018117\nd    -2.271265\ne    24.000000\ndtype: float64\n\nIn [29]: s * 2\nOut[29]: \na     0.938225\nb    -0.565727\nc    -3.018117\nd    -2.271265\ne    24.000000\ndtype: float64\n\nIn [30]: np.exp(s)\nOut[30]: \na         1.598575\nb         0.753623\nc         0.221118\nd         0.321219\ne    162754.791419\ndtype: float64\n  A key difference between Series and ndarray is that operations between Series automatically align the data based on label. Thus, you can write computations without giving consideration to whether the Series involved have the same labels. \nIn [31]: s[1:] + s[:-1]\nOut[31]: \na         NaN\nb   -0.565727\nc   -3.018117\nd   -2.271265\ne         NaN\ndtype: float64\n  The result of an operation between unaligned Series will have the union of the indexes involved. If a label is not found in one Series or the other, the result will be marked as missing NaN. Being able to write code without doing any explicit data alignment grants immense freedom and flexibility in interactive data analysis and research. The integrated data alignment features of the pandas data structures set pandas apart from the majority of related tools for working with labeled data.  Note In general, we chose to make the default result of operations between differently indexed objects yield the union of the indexes in order to avoid loss of information. Having an index label, though the data is missing, is typically important information as part of a computation. You of course have the option of dropping labels with missing data via the dropna function.    Name attribute Series can also have a name attribute: \nIn [32]: s = pd.Series(np.random.randn(5), name=\"something\")\n\nIn [33]: s\nOut[33]: \n0   -0.494929\n1    1.071804\n2    0.721555\n3   -0.706771\n4   -1.039575\nName: something, dtype: float64\n\nIn [34]: s.name\nOut[34]: 'something'\n  The Series name will be assigned automatically in many cases, in particular when taking 1D slices of DataFrame as you will see below. You can rename a Series with the pandas.Series.rename() method. \nIn [35]: s2 = s.rename(\"different\")\n\nIn [36]: s2.name\nOut[36]: 'different'\n  Note that s and s2 refer to different objects.    DataFrame DataFrame is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dict of Series objects. It is generally the most commonly used pandas object. Like Series, DataFrame accepts many different kinds of input:  Dict of 1D ndarrays, lists, dicts, or Series 2-D numpy.ndarray Structured or record ndarray A Series Another DataFrame  Along with the data, you can optionally pass index (row labels) and columns (column labels) arguments. If you pass an index and / or columns, you are guaranteeing the index and / or columns of the resulting DataFrame. Thus, a dict of Series plus a specific index will discard all data not matching up to the passed index. If axis labels are not passed, they will be constructed from the input data based on common sense rules.  Note When the data is a dict, and columns is not specified, the DataFrame columns will be ordered by the dict\u2019s insertion order, if you are using Python version >= 3.6 and pandas >= 0.23. If you are using Python < 3.6 or pandas < 0.23, and columns is not specified, the DataFrame columns will be the lexically ordered list of dict keys.   From dict of Series or dicts The resulting index will be the union of the indexes of the various Series. If there are any nested dicts, these will first be converted to Series. If no columns are passed, the columns will be the ordered list of dict keys. \nIn [37]: d = {\n   ....:     \"one\": pd.Series([1.0, 2.0, 3.0], index=[\"a\", \"b\", \"c\"]),\n   ....:     \"two\": pd.Series([1.0, 2.0, 3.0, 4.0], index=[\"a\", \"b\", \"c\", \"d\"]),\n   ....: }\n   ....: \n\nIn [38]: df = pd.DataFrame(d)\n\nIn [39]: df\nOut[39]: \n   one  two\na  1.0  1.0\nb  2.0  2.0\nc  3.0  3.0\nd  NaN  4.0\n\nIn [40]: pd.DataFrame(d, index=[\"d\", \"b\", \"a\"])\nOut[40]: \n   one  two\nd  NaN  4.0\nb  2.0  2.0\na  1.0  1.0\n\nIn [41]: pd.DataFrame(d, index=[\"d\", \"b\", \"a\"], columns=[\"two\", \"three\"])\nOut[41]: \n   two three\nd  4.0   NaN\nb  2.0   NaN\na  1.0   NaN\n  The row and column labels can be accessed respectively by accessing the index and columns attributes:  Note When a particular set of columns is passed along with a dict of data, the passed columns override the keys in the dict.  \nIn [42]: df.index\nOut[42]: Index(['a', 'b', 'c', 'd'], dtype='object')\n\nIn [43]: df.columns\nOut[43]: Index(['one', 'two'], dtype='object')\n    From dict of ndarrays / lists The ndarrays must all be the same length. If an index is passed, it must clearly also be the same length as the arrays. If no index is passed, the result will be range(n), where n is the array length. \nIn [44]: d = {\"one\": [1.0, 2.0, 3.0, 4.0], \"two\": [4.0, 3.0, 2.0, 1.0]}\n\nIn [45]: pd.DataFrame(d)\nOut[45]: \n   one  two\n0  1.0  4.0\n1  2.0  3.0\n2  3.0  2.0\n3  4.0  1.0\n\nIn [46]: pd.DataFrame(d, index=[\"a\", \"b\", \"c\", \"d\"])\nOut[46]: \n   one  two\na  1.0  4.0\nb  2.0  3.0\nc  3.0  2.0\nd  4.0  1.0\n    From structured or record array This case is handled identically to a dict of arrays. \nIn [47]: data = np.zeros((2,), dtype=[(\"A\", \"i4\"), (\"B\", \"f4\"), (\"C\", \"a10\")])\n\nIn [48]: data[:] = [(1, 2.0, \"Hello\"), (2, 3.0, \"World\")]\n\nIn [49]: pd.DataFrame(data)\nOut[49]: \n   A    B         C\n0  1  2.0  b'Hello'\n1  2  3.0  b'World'\n\nIn [50]: pd.DataFrame(data, index=[\"first\", \"second\"])\nOut[50]: \n        A    B         C\nfirst   1  2.0  b'Hello'\nsecond  2  3.0  b'World'\n\nIn [51]: pd.DataFrame(data, columns=[\"C\", \"A\", \"B\"])\nOut[51]: \n          C  A    B\n0  b'Hello'  1  2.0\n1  b'World'  2  3.0\n   Note DataFrame is not intended to work exactly like a 2-dimensional NumPy ndarray.    From a list of dicts \nIn [52]: data2 = [{\"a\": 1, \"b\": 2}, {\"a\": 5, \"b\": 10, \"c\": 20}]\n\nIn [53]: pd.DataFrame(data2)\nOut[53]: \n   a   b     c\n0  1   2   NaN\n1  5  10  20.0\n\nIn [54]: pd.DataFrame(data2, index=[\"first\", \"second\"])\nOut[54]: \n        a   b     c\nfirst   1   2   NaN\nsecond  5  10  20.0\n\nIn [55]: pd.DataFrame(data2, columns=[\"a\", \"b\"])\nOut[55]: \n   a   b\n0  1   2\n1  5  10\n    From a dict of tuples You can automatically create a MultiIndexed frame by passing a tuples dictionary. \nIn [56]: pd.DataFrame(\n   ....:     {\n   ....:         (\"a\", \"b\"): {(\"A\", \"B\"): 1, (\"A\", \"C\"): 2},\n   ....:         (\"a\", \"a\"): {(\"A\", \"C\"): 3, (\"A\", \"B\"): 4},\n   ....:         (\"a\", \"c\"): {(\"A\", \"B\"): 5, (\"A\", \"C\"): 6},\n   ....:         (\"b\", \"a\"): {(\"A\", \"C\"): 7, (\"A\", \"B\"): 8},\n   ....:         (\"b\", \"b\"): {(\"A\", \"D\"): 9, (\"A\", \"B\"): 10},\n   ....:     }\n   ....: )\n   ....: \nOut[56]: \n       a              b      \n       b    a    c    a     b\nA B  1.0  4.0  5.0  8.0  10.0\n  C  2.0  3.0  6.0  7.0   NaN\n  D  NaN  NaN  NaN  NaN   9.0\n    From a Series The result will be a DataFrame with the same index as the input Series, and with one column whose name is the original name of the Series (only if no other column name provided).   From a list of namedtuples The field names of the first namedtuple in the list determine the columns of the DataFrame. The remaining namedtuples (or tuples) are simply unpacked and their values are fed into the rows of the DataFrame. If any of those tuples is shorter than the first namedtuple then the later columns in the corresponding row are marked as missing values. If any are longer than the first namedtuple, a ValueError is raised. \nIn [57]: from collections import namedtuple\n\nIn [58]: Point = namedtuple(\"Point\", \"x y\")\n\nIn [59]: pd.DataFrame([Point(0, 0), Point(0, 3), (2, 3)])\nOut[59]: \n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nIn [60]: Point3D = namedtuple(\"Point3D\", \"x y z\")\n\nIn [61]: pd.DataFrame([Point3D(0, 0, 0), Point3D(0, 3, 5), Point(2, 3)])\nOut[61]: \n   x  y    z\n0  0  0  0.0\n1  0  3  5.0\n2  2  3  NaN\n    From a list of dataclasses  New in version 1.1.0.  Data Classes as introduced in PEP557, can be passed into the DataFrame constructor. Passing a list of dataclasses is equivalent to passing a list of dictionaries. Please be aware, that all values in the list should be dataclasses, mixing types in the list would result in a TypeError. \nIn [62]: from dataclasses import make_dataclass\n\nIn [63]: Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n\nIn [64]: pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\nOut[64]: \n   x  y\n0  0  0\n1  0  3\n2  2  3\n  Missing data Much more will be said on this topic in the Missing data section. To construct a DataFrame with missing data, we use np.nan to represent missing values. Alternatively, you may pass a numpy.MaskedArray as the data argument to the DataFrame constructor, and its masked entries will be considered missing.   Alternate constructors DataFrame.from_dict DataFrame.from_dict takes a dict of dicts or a dict of array-like sequences and returns a DataFrame. It operates like the DataFrame constructor except for the orient parameter which is 'columns' by default, but which can be set to 'index' in order to use the dict keys as row labels. \nIn [65]: pd.DataFrame.from_dict(dict([(\"A\", [1, 2, 3]), (\"B\", [4, 5, 6])]))\nOut[65]: \n   A  B\n0  1  4\n1  2  5\n2  3  6\n  If you pass orient='index', the keys will be the row labels. In this case, you can also pass the desired column names: \nIn [66]: pd.DataFrame.from_dict(\n   ....:     dict([(\"A\", [1, 2, 3]), (\"B\", [4, 5, 6])]),\n   ....:     orient=\"index\",\n   ....:     columns=[\"one\", \"two\", \"three\"],\n   ....: )\n   ....: \nOut[66]: \n   one  two  three\nA    1    2      3\nB    4    5      6\n  DataFrame.from_records DataFrame.from_records takes a list of tuples or an ndarray with structured dtype. It works analogously to the normal DataFrame constructor, except that the resulting DataFrame index may be a specific field of the structured dtype. For example: \nIn [67]: data\nOut[67]: \narray([(1, 2., b'Hello'), (2, 3., b'World')],\n      dtype=[('A', '<i4'), ('B', '<f4'), ('C', 'S10')])\n\nIn [68]: pd.DataFrame.from_records(data, index=\"C\")\nOut[68]: \n          A    B\nC               \nb'Hello'  1  2.0\nb'World'  2  3.0\n    Column selection, addition, deletion You can treat a DataFrame semantically like a dict of like-indexed Series objects. Getting, setting, and deleting columns works with the same syntax as the analogous dict operations: \nIn [69]: df[\"one\"]\nOut[69]: \na    1.0\nb    2.0\nc    3.0\nd    NaN\nName: one, dtype: float64\n\nIn [70]: df[\"three\"] = df[\"one\"] * df[\"two\"]\n\nIn [71]: df[\"flag\"] = df[\"one\"] > 2\n\nIn [72]: df\nOut[72]: \n   one  two  three   flag\na  1.0  1.0    1.0  False\nb  2.0  2.0    4.0  False\nc  3.0  3.0    9.0   True\nd  NaN  4.0    NaN  False\n  Columns can be deleted or popped like with a dict: \nIn [73]: del df[\"two\"]\n\nIn [74]: three = df.pop(\"three\")\n\nIn [75]: df\nOut[75]: \n   one   flag\na  1.0  False\nb  2.0  False\nc  3.0   True\nd  NaN  False\n  When inserting a scalar value, it will naturally be propagated to fill the column: \nIn [76]: df[\"foo\"] = \"bar\"\n\nIn [77]: df\nOut[77]: \n   one   flag  foo\na  1.0  False  bar\nb  2.0  False  bar\nc  3.0   True  bar\nd  NaN  False  bar\n  When inserting a Series that does not have the same index as the DataFrame, it will be conformed to the DataFrame\u2019s index: \nIn [78]: df[\"one_trunc\"] = df[\"one\"][:2]\n\nIn [79]: df\nOut[79]: \n   one   flag  foo  one_trunc\na  1.0  False  bar        1.0\nb  2.0  False  bar        2.0\nc  3.0   True  bar        NaN\nd  NaN  False  bar        NaN\n  You can insert raw ndarrays but their length must match the length of the DataFrame\u2019s index. By default, columns get inserted at the end. The insert function is available to insert at a particular location in the columns: \nIn [80]: df.insert(1, \"bar\", df[\"one\"])\n\nIn [81]: df\nOut[81]: \n   one  bar   flag  foo  one_trunc\na  1.0  1.0  False  bar        1.0\nb  2.0  2.0  False  bar        2.0\nc  3.0  3.0   True  bar        NaN\nd  NaN  NaN  False  bar        NaN\n    Assigning new columns in method chains Inspired by dplyr\u2019s mutate verb, DataFrame has an assign() method that allows you to easily create new columns that are potentially derived from existing columns. \nIn [82]: iris = pd.read_csv(\"data/iris.data\")\n\nIn [83]: iris.head()\nOut[83]: \n   SepalLength  SepalWidth  PetalLength  PetalWidth         Name\n0          5.1         3.5          1.4         0.2  Iris-setosa\n1          4.9         3.0          1.4         0.2  Iris-setosa\n2          4.7         3.2          1.3         0.2  Iris-setosa\n3          4.6         3.1          1.5         0.2  Iris-setosa\n4          5.0         3.6          1.4         0.2  Iris-setosa\n\nIn [84]: iris.assign(sepal_ratio=iris[\"SepalWidth\"] / iris[\"SepalLength\"]).head()\nOut[84]: \n   SepalLength  SepalWidth  PetalLength  PetalWidth         Name  sepal_ratio\n0          5.1         3.5          1.4         0.2  Iris-setosa     0.686275\n1          4.9         3.0          1.4         0.2  Iris-setosa     0.612245\n2          4.7         3.2          1.3         0.2  Iris-setosa     0.680851\n3          4.6         3.1          1.5         0.2  Iris-setosa     0.673913\n4          5.0         3.6          1.4         0.2  Iris-setosa     0.720000\n  In the example above, we inserted a precomputed value. We can also pass in a function of one argument to be evaluated on the DataFrame being assigned to. \nIn [85]: iris.assign(sepal_ratio=lambda x: (x[\"SepalWidth\"] / x[\"SepalLength\"])).head()\nOut[85]: \n   SepalLength  SepalWidth  PetalLength  PetalWidth         Name  sepal_ratio\n0          5.1         3.5          1.4         0.2  Iris-setosa     0.686275\n1          4.9         3.0          1.4         0.2  Iris-setosa     0.612245\n2          4.7         3.2          1.3         0.2  Iris-setosa     0.680851\n3          4.6         3.1          1.5         0.2  Iris-setosa     0.673913\n4          5.0         3.6          1.4         0.2  Iris-setosa     0.720000\n  assign always returns a copy of the data, leaving the original DataFrame untouched. Passing a callable, as opposed to an actual value to be inserted, is useful when you don\u2019t have a reference to the DataFrame at hand. This is common when using assign in a chain of operations. For example, we can limit the DataFrame to just those observations with a Sepal Length greater than 5, calculate the ratio, and plot: \nIn [86]: (\n   ....:     iris.query(\"SepalLength > 5\")\n   ....:     .assign(\n   ....:         SepalRatio=lambda x: x.SepalWidth / x.SepalLength,\n   ....:         PetalRatio=lambda x: x.PetalWidth / x.PetalLength,\n   ....:     )\n   ....:     .plot(kind=\"scatter\", x=\"SepalRatio\", y=\"PetalRatio\")\n   ....: )\n   ....: \nOut[86]: <AxesSubplot:xlabel='SepalRatio', ylabel='PetalRatio'>\n   Since a function is passed in, the function is computed on the DataFrame being assigned to. Importantly, this is the DataFrame that\u2019s been filtered to those rows with sepal length greater than 5. The filtering happens first, and then the ratio calculations. This is an example where we didn\u2019t have a reference to the filtered DataFrame available. The function signature for assign is simply **kwargs. The keys are the column names for the new fields, and the values are either a value to be inserted (for example, a Series or NumPy array), or a function of one argument to be called on the DataFrame. A copy of the original DataFrame is returned, with the new values inserted. Starting with Python 3.6 the order of **kwargs is preserved. This allows for dependent assignment, where an expression later in **kwargs can refer to a column created earlier in the same assign(). \nIn [87]: dfa = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\nIn [88]: dfa.assign(C=lambda x: x[\"A\"] + x[\"B\"], D=lambda x: x[\"A\"] + x[\"C\"])\nOut[88]: \n   A  B  C   D\n0  1  4  5   6\n1  2  5  7   9\n2  3  6  9  12\n  In the second expression, x['C'] will refer to the newly created column, that\u2019s equal to dfa['A'] + dfa['B'].   Indexing / selection The basics of indexing are as follows:        \nOperation Syntax Result    \nSelect column df[col] Series  \nSelect row by label df.loc[label] Series  \nSelect row by integer location df.iloc[loc] Series  \nSlice rows df[5:10] DataFrame  \nSelect rows by boolean vector df[bool_vec] DataFrame    Row selection, for example, returns a Series whose index is the columns of the DataFrame: \nIn [89]: df.loc[\"b\"]\nOut[89]: \none            2.0\nbar            2.0\nflag         False\nfoo            bar\none_trunc      2.0\nName: b, dtype: object\n\nIn [90]: df.iloc[2]\nOut[90]: \none           3.0\nbar           3.0\nflag         True\nfoo           bar\none_trunc     NaN\nName: c, dtype: object\n  For a more exhaustive treatment of sophisticated label-based indexing and slicing, see the section on indexing. We will address the fundamentals of reindexing / conforming to new sets of labels in the section on reindexing.   Data alignment and arithmetic Data alignment between DataFrame objects automatically align on both the columns and the index (row labels). Again, the resulting object will have the union of the column and row labels. \nIn [91]: df = pd.DataFrame(np.random.randn(10, 4), columns=[\"A\", \"B\", \"C\", \"D\"])\n\nIn [92]: df2 = pd.DataFrame(np.random.randn(7, 3), columns=[\"A\", \"B\", \"C\"])\n\nIn [93]: df + df2\nOut[93]: \n          A         B         C   D\n0  0.045691 -0.014138  1.380871 NaN\n1 -0.955398 -1.501007  0.037181 NaN\n2 -0.662690  1.534833 -0.859691 NaN\n3 -2.452949  1.237274 -0.133712 NaN\n4  1.414490  1.951676 -2.320422 NaN\n5 -0.494922 -1.649727 -1.084601 NaN\n6 -1.047551 -0.748572 -0.805479 NaN\n7       NaN       NaN       NaN NaN\n8       NaN       NaN       NaN NaN\n9       NaN       NaN       NaN NaN\n  When doing an operation between DataFrame and Series, the default behavior is to align the Series index on the DataFrame columns, thus broadcasting row-wise. For example: \nIn [94]: df - df.iloc[0]\nOut[94]: \n          A         B         C         D\n0  0.000000  0.000000  0.000000  0.000000\n1 -1.359261 -0.248717 -0.453372 -1.754659\n2  0.253128  0.829678  0.010026 -1.991234\n3 -1.311128  0.054325 -1.724913 -1.620544\n4  0.573025  1.500742 -0.676070  1.367331\n5 -1.741248  0.781993 -1.241620 -2.053136\n6 -1.240774 -0.869551 -0.153282  0.000430\n7 -0.743894  0.411013 -0.929563 -0.282386\n8 -1.194921  1.320690  0.238224 -1.482644\n9  2.293786  1.856228  0.773289 -1.446531\n  For explicit control over the matching and broadcasting behavior, see the section on flexible binary operations. Operations with scalars are just as you would expect: \nIn [95]: df * 5 + 2\nOut[95]: \n           A         B         C          D\n0   3.359299 -0.124862  4.835102   3.381160\n1  -3.437003 -1.368449  2.568242  -5.392133\n2   4.624938  4.023526  4.885230  -6.575010\n3  -3.196342  0.146766 -3.789461  -4.721559\n4   6.224426  7.378849  1.454750  10.217815\n5  -5.346940  3.785103 -1.373001  -6.884519\n6  -2.844569 -4.472618  4.068691   3.383309\n7  -0.360173  1.930201  0.187285   1.969232\n8  -2.615303  6.478587  6.026220  -4.032059\n9  14.828230  9.156280  8.701544  -3.851494\n\nIn [96]: 1 / df\nOut[96]: \n          A          B         C           D\n0  3.678365  -2.353094  1.763605    3.620145\n1 -0.919624  -1.484363  8.799067   -0.676395\n2  1.904807   2.470934  1.732964   -0.583090\n3 -0.962215  -2.697986 -0.863638   -0.743875\n4  1.183593   0.929567 -9.170108    0.608434\n5 -0.680555   2.800959 -1.482360   -0.562777\n6 -1.032084  -0.772485  2.416988    3.614523\n7 -2.118489 -71.634509 -2.758294 -162.507295\n8 -1.083352   1.116424  1.241860   -0.828904\n9  0.389765   0.698687  0.746097   -0.854483\n\nIn [97]: df ** 4\nOut[97]: \n           A             B         C             D\n0   0.005462  3.261689e-02  0.103370  5.822320e-03\n1   1.398165  2.059869e-01  0.000167  4.777482e+00\n2   0.075962  2.682596e-02  0.110877  8.650845e+00\n3   1.166571  1.887302e-02  1.797515  3.265879e+00\n4   0.509555  1.339298e+00  0.000141  7.297019e+00\n5   4.661717  1.624699e-02  0.207103  9.969092e+00\n6   0.881334  2.808277e+00  0.029302  5.858632e-03\n7   0.049647  3.797614e-08  0.017276  1.433866e-09\n8   0.725974  6.437005e-01  0.420446  2.118275e+00\n9  43.329821  4.196326e+00  3.227153  1.875802e+00\n  Boolean operators work as well: \nIn [98]: df1 = pd.DataFrame({\"a\": [1, 0, 1], \"b\": [0, 1, 1]}, dtype=bool)\n\nIn [99]: df2 = pd.DataFrame({\"a\": [0, 1, 1], \"b\": [1, 1, 0]}, dtype=bool)\n\nIn [100]: df1 & df2\nOut[100]: \n       a      b\n0  False  False\n1  False   True\n2   True  False\n\nIn [101]: df1 | df2\nOut[101]: \n      a     b\n0  True  True\n1  True  True\n2  True  True\n\nIn [102]: df1 ^ df2\nOut[102]: \n       a      b\n0   True   True\n1   True  False\n2  False   True\n\nIn [103]: -df1\nOut[103]: \n       a      b\n0  False   True\n1   True  False\n2  False  False\n    Transposing To transpose, access the T attribute (also the transpose function), similar to an ndarray: \n# only show the first 5 rows\nIn [104]: df[:5].T\nOut[104]: \n          0         1         2         3         4\nA  0.271860 -1.087401  0.524988 -1.039268  0.844885\nB -0.424972 -0.673690  0.404705 -0.370647  1.075770\nC  0.567020  0.113648  0.577046 -1.157892 -0.109050\nD  0.276232 -1.478427 -1.715002 -1.344312  1.643563\n    DataFrame interoperability with NumPy functions Elementwise NumPy ufuncs (log, exp, sqrt, \u2026) and various other NumPy functions can be used with no issues on Series and DataFrame, assuming the data within are numeric: \nIn [105]: np.exp(df)\nOut[105]: \n           A         B         C         D\n0   1.312403  0.653788  1.763006  1.318154\n1   0.337092  0.509824  1.120358  0.227996\n2   1.690438  1.498861  1.780770  0.179963\n3   0.353713  0.690288  0.314148  0.260719\n4   2.327710  2.932249  0.896686  5.173571\n5   0.230066  1.429065  0.509360  0.169161\n6   0.379495  0.274028  1.512461  1.318720\n7   0.623732  0.986137  0.695904  0.993865\n8   0.397301  2.449092  2.237242  0.299269\n9  13.009059  4.183951  3.820223  0.310274\n\nIn [106]: np.asarray(df)\nOut[106]: \narray([[ 0.2719, -0.425 ,  0.567 ,  0.2762],\n       [-1.0874, -0.6737,  0.1136, -1.4784],\n       [ 0.525 ,  0.4047,  0.577 , -1.715 ],\n       [-1.0393, -0.3706, -1.1579, -1.3443],\n       [ 0.8449,  1.0758, -0.109 ,  1.6436],\n       [-1.4694,  0.357 , -0.6746, -1.7769],\n       [-0.9689, -1.2945,  0.4137,  0.2767],\n       [-0.472 , -0.014 , -0.3625, -0.0062],\n       [-0.9231,  0.8957,  0.8052, -1.2064],\n       [ 2.5656,  1.4313,  1.3403, -1.1703]])\n  DataFrame is not intended to be a drop-in replacement for ndarray as its indexing semantics and data model are quite different in places from an n-dimensional array. Series implements __array_ufunc__, which allows it to work with NumPy\u2019s universal functions. The ufunc is applied to the underlying array in a Series. \nIn [107]: ser = pd.Series([1, 2, 3, 4])\n\nIn [108]: np.exp(ser)\nOut[108]: \n0     2.718282\n1     7.389056\n2    20.085537\n3    54.598150\ndtype: float64\n   Changed in version 0.25.0: When multiple Series are passed to a ufunc, they are aligned before performing the operation.  Like other parts of the library, pandas will automatically align labeled inputs as part of a ufunc with multiple inputs. For example, using numpy.remainder() on two Series with differently ordered labels will align before the operation. \nIn [109]: ser1 = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n\nIn [110]: ser2 = pd.Series([1, 3, 5], index=[\"b\", \"a\", \"c\"])\n\nIn [111]: ser1\nOut[111]: \na    1\nb    2\nc    3\ndtype: int64\n\nIn [112]: ser2\nOut[112]: \nb    1\na    3\nc    5\ndtype: int64\n\nIn [113]: np.remainder(ser1, ser2)\nOut[113]: \na    1\nb    0\nc    3\ndtype: int64\n  As usual, the union of the two indices is taken, and non-overlapping values are filled with missing values. \nIn [114]: ser3 = pd.Series([2, 4, 6], index=[\"b\", \"c\", \"d\"])\n\nIn [115]: ser3\nOut[115]: \nb    2\nc    4\nd    6\ndtype: int64\n\nIn [116]: np.remainder(ser1, ser3)\nOut[116]: \na    NaN\nb    0.0\nc    3.0\nd    NaN\ndtype: float64\n  When a binary ufunc is applied to a Series and Index, the Series implementation takes precedence and a Series is returned. \nIn [117]: ser = pd.Series([1, 2, 3])\n\nIn [118]: idx = pd.Index([4, 5, 6])\n\nIn [119]: np.maximum(ser, idx)\nOut[119]: \n0    4\n1    5\n2    6\ndtype: int64\n  NumPy ufuncs are safe to apply to Series backed by non-ndarray arrays, for example arrays.SparseArray (see Sparse calculation). If possible, the ufunc is applied without converting the underlying data to an ndarray.   Console display Very large DataFrames will be truncated to display them in the console. You can also get a summary using info(). (Here I am reading a CSV version of the baseball dataset from the plyr R package): \nIn [120]: baseball = pd.read_csv(\"data/baseball.csv\")\n\nIn [121]: print(baseball)\n       id     player  year  stint team  lg   g   ab   r    h  X2b  X3b  hr   rbi   sb   cs  bb    so  ibb  hbp   sh   sf  gidp\n0   88641  womacto01  2006      2  CHN  NL  19   50   6   14    1    0   1   2.0  1.0  1.0   4   4.0  0.0  0.0  3.0  0.0   0.0\n1   88643  schilcu01  2006      1  BOS  AL  31    2   0    1    0    0   0   0.0  0.0  0.0   0   1.0  0.0  0.0  0.0  0.0   0.0\n..    ...        ...   ...    ...  ...  ..  ..  ...  ..  ...  ...  ...  ..   ...  ...  ...  ..   ...  ...  ...  ...  ...   ...\n98  89533   aloumo01  2007      1  NYN  NL  87  328  51  112   19    1  13  49.0  3.0  0.0  27  30.0  5.0  2.0  0.0  3.0  13.0\n99  89534  alomasa02  2007      1  NYN  NL   8   22   1    3    1    0   0   0.0  0.0  0.0   0   3.0  0.0  0.0  0.0  0.0   0.0\n\n[100 rows x 23 columns]\n\nIn [122]: baseball.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 100 entries, 0 to 99\nData columns (total 23 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   id      100 non-null    int64  \n 1   player  100 non-null    object \n 2   year    100 non-null    int64  \n 3   stint   100 non-null    int64  \n 4   team    100 non-null    object \n 5   lg      100 non-null    object \n 6   g       100 non-null    int64  \n 7   ab      100 non-null    int64  \n 8   r       100 non-null    int64  \n 9   h       100 non-null    int64  \n 10  X2b     100 non-null    int64  \n 11  X3b     100 non-null    int64  \n 12  hr      100 non-null    int64  \n 13  rbi     100 non-null    float64\n 14  sb      100 non-null    float64\n 15  cs      100 non-null    float64\n 16  bb      100 non-null    int64  \n 17  so      100 non-null    float64\n 18  ibb     100 non-null    float64\n 19  hbp     100 non-null    float64\n 20  sh      100 non-null    float64\n 21  sf      100 non-null    float64\n 22  gidp    100 non-null    float64\ndtypes: float64(9), int64(11), object(3)\nmemory usage: 18.1+ KB\n  However, using to_string will return a string representation of the DataFrame in tabular form, though it won\u2019t always fit the console width: \nIn [123]: print(baseball.iloc[-20:, :12].to_string())\n       id     player  year  stint team  lg    g   ab   r    h  X2b  X3b\n80  89474  finlest01  2007      1  COL  NL   43   94   9   17    3    0\n81  89480  embreal01  2007      1  OAK  AL    4    0   0    0    0    0\n82  89481  edmonji01  2007      1  SLN  NL  117  365  39   92   15    2\n83  89482  easleda01  2007      1  NYN  NL   76  193  24   54    6    0\n84  89489  delgaca01  2007      1  NYN  NL  139  538  71  139   30    0\n85  89493  cormirh01  2007      1  CIN  NL    6    0   0    0    0    0\n86  89494  coninje01  2007      2  NYN  NL   21   41   2    8    2    0\n87  89495  coninje01  2007      1  CIN  NL   80  215  23   57   11    1\n88  89497  clemero02  2007      1  NYA  AL    2    2   0    1    0    0\n89  89498  claytro01  2007      2  BOS  AL    8    6   1    0    0    0\n90  89499  claytro01  2007      1  TOR  AL   69  189  23   48   14    0\n91  89501  cirilje01  2007      2  ARI  NL   28   40   6    8    4    0\n92  89502  cirilje01  2007      1  MIN  AL   50  153  18   40    9    2\n93  89521  bondsba01  2007      1  SFN  NL  126  340  75   94   14    0\n94  89523  biggicr01  2007      1  HOU  NL  141  517  68  130   31    3\n95  89525  benitar01  2007      2  FLO  NL   34    0   0    0    0    0\n96  89526  benitar01  2007      1  SFN  NL   19    0   0    0    0    0\n97  89530  ausmubr01  2007      1  HOU  NL  117  349  38   82   16    3\n98  89533   aloumo01  2007      1  NYN  NL   87  328  51  112   19    1\n99  89534  alomasa02  2007      1  NYN  NL    8   22   1    3    1    0\n  Wide DataFrames will be printed across multiple rows by default: \nIn [124]: pd.DataFrame(np.random.randn(3, 12))\nOut[124]: \n         0         1         2         3         4         5         6         7         8         9         10        11\n0 -1.226825  0.769804 -1.281247 -0.727707 -0.121306 -0.097883  0.695775  0.341734  0.959726 -1.110336 -0.619976  0.149748\n1 -0.732339  0.687738  0.176444  0.403310 -0.154951  0.301624 -2.179861 -1.369849 -0.954208  1.462696 -1.743161 -0.826591\n2 -0.345352  1.314232  0.690579  0.995761  2.396780  0.014871  3.357427 -0.317441 -1.236269  0.896171 -0.487602 -0.082240\n  You can change how much to print on a single row by setting the display.width option: \nIn [125]: pd.set_option(\"display.width\", 40)  # default is 80\n\nIn [126]: pd.DataFrame(np.random.randn(3, 12))\nOut[126]: \n         0         1         2         3         4         5         6         7         8         9         10        11\n0 -2.182937  0.380396  0.084844  0.432390  1.519970 -0.493662  0.600178  0.274230  0.132885 -0.023688  2.410179  1.450520\n1  0.206053 -0.251905 -2.213588  1.063327  1.266143  0.299368 -0.863838  0.408204 -1.048089 -0.025747 -0.988387  0.094055\n2  1.262731  1.289997  0.082423 -0.055758  0.536580 -0.489682  0.369374 -0.034571 -2.484478 -0.281461  0.030711  0.109121\n  You can adjust the max width of the individual columns by setting display.max_colwidth \nIn [127]: datafile = {\n   .....:     \"filename\": [\"filename_01\", \"filename_02\"],\n   .....:     \"path\": [\n   .....:         \"media/user_name/storage/folder_01/filename_01\",\n   .....:         \"media/user_name/storage/folder_02/filename_02\",\n   .....:     ],\n   .....: }\n   .....: \n\nIn [128]: pd.set_option(\"display.max_colwidth\", 30)\n\nIn [129]: pd.DataFrame(datafile)\nOut[129]: \n      filename                           path\n0  filename_01  media/user_name/storage/fo...\n1  filename_02  media/user_name/storage/fo...\n\nIn [130]: pd.set_option(\"display.max_colwidth\", 100)\n\nIn [131]: pd.DataFrame(datafile)\nOut[131]: \n      filename                                           path\n0  filename_01  media/user_name/storage/folder_01/filename_01\n1  filename_02  media/user_name/storage/folder_02/filename_02\n  You can also disable this feature via the expand_frame_repr option. This will print the table in one block.   DataFrame column attribute access and IPython completion If a DataFrame column label is a valid Python variable name, the column can be accessed like an attribute: \nIn [132]: df = pd.DataFrame({\"foo1\": np.random.randn(5), \"foo2\": np.random.randn(5)})\n\nIn [133]: df\nOut[133]: \n       foo1      foo2\n0  1.126203  0.781836\n1 -0.977349 -1.071357\n2  1.474071  0.441153\n3 -0.064034  2.353925\n4 -1.282782  0.583787\n\nIn [134]: df.foo1\nOut[134]: \n0    1.126203\n1   -0.977349\n2    1.474071\n3   -0.064034\n4   -1.282782\nName: foo1, dtype: float64\n  The columns are also connected to the IPython completion mechanism so they can be tab-completed: \nIn [5]: df.foo<TAB>  # noqa: E225, E999\ndf.foo1  df.foo2\n   \n"}, {"name": "IO tools (text, CSV, HDF5, \u2026)", "path": "user_guide/io", "type": "Manual", "text": "IO tools (text, CSV, HDF5, \u2026) The pandas I/O API is a set of top level reader functions accessed like pandas.read_csv() that generally return a pandas object. The corresponding writer functions are object methods that are accessed like DataFrame.to_csv(). Below is a table containing available readers and writers.         \nFormat Type Data Description Reader Writer    \ntext CSV read_csv to_csv  \ntext Fixed-Width Text File read_fwf   \ntext JSON read_json to_json  \ntext HTML read_html to_html  \ntext LaTeX  Styler.to_latex  \ntext XML read_xml to_xml  \ntext Local clipboard read_clipboard to_clipboard  \nbinary MS Excel read_excel to_excel  \nbinary OpenDocument read_excel   \nbinary HDF5 Format read_hdf to_hdf  \nbinary Feather Format read_feather to_feather  \nbinary Parquet Format read_parquet to_parquet  \nbinary ORC Format read_orc   \nbinary Stata read_stata to_stata  \nbinary SAS read_sas   \nbinary SPSS read_spss   \nbinary Python Pickle Format read_pickle to_pickle  \nSQL SQL read_sql to_sql  \nSQL Google BigQuery read_gbq to_gbq    Here is an informal performance comparison for some of these IO methods.  Note For examples that use the StringIO class, make sure you import it with from io import StringIO for Python 3.   CSV & text files The workhorse function for reading text files (a.k.a. flat files) is read_csv(). See the cookbook for some advanced strategies.  Parsing options read_csv() accepts the following common arguments:  Basic  filepath_or_buffer:various\n\n\nEither a path to a file (a str, pathlib.Path, or py:py._path.local.LocalPath), URL (including http, ftp, and S3 locations), or any object with a read() method (such as an open file or StringIO).  sep:str, defaults to ',' for read_csv(), \\t for read_table()\n\n\nDelimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python\u2019s builtin sniffer tool, csv.Sniffer. In addition, separators longer than 1 character and different from '\\s+' will be interpreted as regular expressions and will also force the use of the Python parsing engine. Note that regex delimiters are prone to ignoring quoted data. Regex example: '\\\\r\\\\t'.  delimiter:str, default None\n\n\nAlternative argument name for sep.  delim_whitespace:boolean, default False\n\n\nSpecifies whether or not whitespace (e.g. ' ' or '\\t') will be used as the delimiter. Equivalent to setting sep='\\s+'. If this option is set to True, nothing should be passed in for the delimiter parameter.     Column and index locations and names  header:int or list of ints, default 'infer'\n\n\nRow number(s) to use as the column names, and the start of the data. Default behavior is to infer the column names: if no names are passed the behavior is identical to header=0 and column names are inferred from the first line of the file, if column names are passed explicitly then the behavior is identical to header=None. Explicitly pass header=0 to be able to replace existing names. The header can be a list of ints that specify row locations for a MultiIndex on the columns e.g. [0,1,3]. Intervening rows that are not specified will be skipped (e.g. 2 in this example is skipped). Note that this parameter ignores commented lines and empty lines if skip_blank_lines=True, so header=0 denotes the first line of data rather than the first line of the file.  names:array-like, default None\n\n\nList of column names to use. If file contains no header row, then you should explicitly pass header=None. Duplicates in this list are not allowed.  index_col:int, str, sequence of int / str, or False, optional, default None\n\n\nColumn(s) to use as the row labels of the DataFrame, either given as string name or column index. If a sequence of int / str is given, a MultiIndex is used. Note: index_col=False can be used to force pandas to not use the first column as the index, e.g. when you have a malformed file with delimiters at the end of each line. The default value of None instructs pandas to guess. If the number of fields in the column header row is equal to the number of fields in the body of the data file, then a default index is used. If it is larger, then the first columns are used as index so that the remaining number of fields in the body are equal to the number of fields in the header. The first row after the header is used to determine the number of columns, which will go into the index. If the subsequent rows contain less columns than the first row, they are filled with NaN. This can be avoided through usecols. This ensures that the columns are taken as is and the trailing data are ignored.  usecols:list-like or callable, default None\n\n\nReturn a subset of the columns. If list-like, all elements must either be positional (i.e. integer indices into the document columns) or strings that correspond to column names provided either by the user in names or inferred from the document header row(s). If names are given, the document header row(s) are not taken into account. For example, a valid list-like usecols parameter would be [0, 1, 2] or ['foo', 'bar', 'baz']. Element order is ignored, so usecols=[0, 1] is the same as [1, 0]. To instantiate a DataFrame from data with element order preserved use pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']] for columns in ['foo', 'bar'] order or pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']] for ['bar', 'foo'] order. If callable, the callable function will be evaluated against the column names, returning names where the callable function evaluates to True: \nIn [1]: import pandas as pd\n\nIn [2]: from io import StringIO\n\nIn [3]: data = \"col1,col2,col3\\na,b,1\\na,b,2\\nc,d,3\"\n\nIn [4]: pd.read_csv(StringIO(data))\nOut[4]: \n  col1 col2  col3\n0    a    b     1\n1    a    b     2\n2    c    d     3\n\nIn [5]: pd.read_csv(StringIO(data), usecols=lambda x: x.upper() in [\"COL1\", \"COL3\"])\nOut[5]: \n  col1  col3\n0    a     1\n1    a     2\n2    c     3\n  Using this parameter results in much faster parsing time and lower memory usage when using the c engine. The Python engine loads the data first before deciding which columns to drop.  squeeze:boolean, default False\n\n\nIf the parsed data only contains one column then return a Series.  Deprecated since version 1.4.0: Append .squeeze(\"columns\") to the call to {func_name} to squeeze the data.   prefix:str, default None\n\n\nPrefix to add to column numbers when no header, e.g. \u2018X\u2019 for X0, X1, \u2026  Deprecated since version 1.4.0: Use a list comprehension on the DataFrame\u2019s columns after calling read_csv.  \nIn [6]: data = \"col1,col2,col3\\na,b,1\"\n\nIn [7]: df = pd.read_csv(StringIO(data))\n\nIn [8]: df.columns = [f\"pre_{col}\" for col in df.columns]\n\nIn [9]: df\nOut[9]: \n  pre_col1 pre_col2  pre_col3\n0        a        b         1\n   mangle_dupe_cols:boolean, default True\n\n\nDuplicate columns will be specified as \u2018X\u2019, \u2018X.1\u2019\u2026\u2019X.N\u2019, rather than \u2018X\u2019\u2026\u2019X\u2019. Passing in False will cause data to be overwritten if there are duplicate names in the columns.     General parsing configuration  dtype:Type name or dict of column -> type, default None\n\n\nData type for data or columns. E.g. {'a': np.float64, 'b': np.int32} (unsupported with engine='python'). Use str or object together with suitable na_values settings to preserve and not interpret dtype.  engine:{'c', 'python', 'pyarrow'}\n\n\nParser engine to use. The C and pyarrow engines are faster, while the python engine is currently more feature-complete. Multithreading is currently only supported by the pyarrow engine.  New in version 1.4.0: The \u201cpyarrow\u201d engine was added as an experimental engine, and some features are unsupported, or may not work correctly, with this engine.   converters:dict, default None\n\n\nDict of functions for converting values in certain columns. Keys can either be integers or column labels.  true_values:list, default None\n\n\nValues to consider as True.  false_values:list, default None\n\n\nValues to consider as False.  skipinitialspace:boolean, default False\n\n\nSkip spaces after delimiter.  skiprows:list-like or integer, default None\n\n\nLine numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file. If callable, the callable function will be evaluated against the row indices, returning True if the row should be skipped and False otherwise: \nIn [10]: data = \"col1,col2,col3\\na,b,1\\na,b,2\\nc,d,3\"\n\nIn [11]: pd.read_csv(StringIO(data))\nOut[11]: \n  col1 col2  col3\n0    a    b     1\n1    a    b     2\n2    c    d     3\n\nIn [12]: pd.read_csv(StringIO(data), skiprows=lambda x: x % 2 != 0)\nOut[12]: \n  col1 col2  col3\n0    a    b     2\n   skipfooter:int, default 0\n\n\nNumber of lines at bottom of file to skip (unsupported with engine=\u2019c\u2019).  nrows:int, default None\n\n\nNumber of rows of file to read. Useful for reading pieces of large files.  low_memory:boolean, default True\n\n\nInternally process the file in chunks, resulting in lower memory use while parsing, but possibly mixed type inference. To ensure no mixed types either set False, or specify the type with the dtype parameter. Note that the entire file is read into a single DataFrame regardless, use the chunksize or iterator parameter to return the data in chunks. (Only valid with C parser)  memory_map:boolean, default False\n\n\nIf a filepath is provided for filepath_or_buffer, map the file object directly onto memory and access the data directly from there. Using this option can improve performance because there is no longer any I/O overhead.     NA and missing data handling  na_values:scalar, str, list-like, or dict, default None\n\n\nAdditional strings to recognize as NA/NaN. If dict passed, specific per-column NA values. See na values const below for a list of the values interpreted as NaN by default.  keep_default_na:boolean, default True\n\n\nWhether or not to include the default NaN values when parsing the data. Depending on whether na_values is passed in, the behavior is as follows:  If keep_default_na is True, and na_values are specified, na_values is appended to the default NaN values used for parsing. If keep_default_na is True, and na_values are not specified, only the default NaN values are used for parsing. If keep_default_na is False, and na_values are specified, only the NaN values specified na_values are used for parsing. If keep_default_na is False, and na_values are not specified, no strings will be parsed as NaN.  Note that if na_filter is passed in as False, the keep_default_na and na_values parameters will be ignored.  na_filter:boolean, default True\n\n\nDetect missing value markers (empty strings and the value of na_values). In data without any NAs, passing na_filter=False can improve the performance of reading a large file.  verbose:boolean, default False\n\n\nIndicate number of NA values placed in non-numeric columns.  skip_blank_lines:boolean, default True\n\n\nIf True, skip over blank lines rather than interpreting as NaN values.     Datetime handling  parse_dates:boolean or list of ints or names or list of lists or dict, default False.\n\n\n If True -> try parsing the index. If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a separate date column. If [[1, 3]] -> combine columns 1 and 3 and parse as a single date column. If {'foo': [1, 3]} -> parse columns 1, 3 as date and call result \u2018foo\u2019. A fast-path exists for iso8601-formatted dates.   infer_datetime_format:boolean, default False\n\n\nIf True and parse_dates is enabled for a column, attempt to infer the datetime format to speed up the processing.  keep_date_col:boolean, default False\n\n\nIf True and parse_dates specifies combining multiple columns then keep the original columns.  date_parser:function, default None\n\n\nFunction to use for converting a sequence of string columns to an array of datetime instances. The default uses dateutil.parser.parser to do the conversion. pandas will try to call date_parser in three different ways, advancing to the next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates) as arguments; 2) concatenate (row-wise) the string values from the columns defined by parse_dates into a single array and pass that; and 3) call date_parser once for each row using one or more strings (corresponding to the columns defined by parse_dates) as arguments.  dayfirst:boolean, default False\n\n\nDD/MM format dates, international and European format.  cache_dates:boolean, default True\n\n\nIf True, use a cache of unique, converted dates to apply the datetime conversion. May produce significant speed-up when parsing duplicate date strings, especially ones with timezone offsets.  New in version 0.25.0.      Iteration  iterator:boolean, default False\n\n\nReturn TextFileReader object for iteration or getting chunks with get_chunk().  chunksize:int, default None\n\n\nReturn TextFileReader object for iteration. See iterating and chunking below.     Quoting, compression, and file format  compression:{'infer', 'gzip', 'bz2', 'zip', 'xz', 'zstd', None, dict}, default 'infer'\n\n\nFor on-the-fly decompression of on-disk data. If \u2018infer\u2019, then use gzip, bz2, zip, xz, or zstandard if filepath_or_buffer is path-like ending in \u2018.gz\u2019, \u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, \u2018.zst\u2019, respectively, and no decompression otherwise. If using \u2018zip\u2019, the ZIP file must contain only one data file to be read in. Set to None for no decompression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, or zstandard.ZstdDecompressor. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.  Changed in version 1.1.0: dict option extended to support gzip and bz2.   Changed in version 1.2.0: Previous versions forwarded dict entries for \u2018gzip\u2019 to gzip.open.   thousands:str, default None\n\n\nThousands separator.  decimal:str, default '.'\n\n\nCharacter to recognize as decimal point. E.g. use ',' for European data.  float_precision:string, default None\n\n\nSpecifies which converter the C engine should use for floating-point values. The options are None for the ordinary converter, high for the high-precision converter, and round_trip for the round-trip converter.  lineterminator:str (length 1), default None\n\n\nCharacter to break file into lines. Only valid with C parser.  quotechar:str (length 1)\n\n\nThe character used to denote the start and end of a quoted item. Quoted items can include the delimiter and it will be ignored.  quoting:int or csv.QUOTE_* instance, default 0\n\n\nControl field quoting behavior per csv.QUOTE_* constants. Use one of QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).  doublequote:boolean, default True\n\n\nWhen quotechar is specified and quoting is not QUOTE_NONE, indicate whether or not to interpret two consecutive quotechar elements inside a field as a single quotechar element.  escapechar:str (length 1), default None\n\n\nOne-character string used to escape delimiter when quoting is QUOTE_NONE.  comment:str, default None\n\n\nIndicates remainder of line should not be parsed. If found at the beginning of a line, the line will be ignored altogether. This parameter must be a single character. Like empty lines (as long as skip_blank_lines=True), fully commented lines are ignored by the parameter header but not by skiprows. For example, if comment='#', parsing \u2018#empty\\na,b,c\\n1,2,3\u2019 with header=0 will result in \u2018a,b,c\u2019 being treated as the header.  encoding:str, default None\n\n\nEncoding to use for UTF when reading/writing (e.g. 'utf-8'). List of Python standard encodings.  dialect:str or csv.Dialect instance, default None\n\n\nIf provided, this parameter will override values (default or not) for the following parameters: delimiter, doublequote, escapechar, skipinitialspace, quotechar, and quoting. If it is necessary to override values, a ParserWarning will be issued. See csv.Dialect documentation for more details.     Error handling  error_bad_lines:boolean, optional, default None\n\n\nLines with too many fields (e.g. a csv line with too many commas) will by default cause an exception to be raised, and no DataFrame will be returned. If False, then these \u201cbad lines\u201d will dropped from the DataFrame that is returned. See bad lines below.  Deprecated since version 1.3.0: The on_bad_lines parameter should be used instead to specify behavior upon encountering a bad line instead.   warn_bad_lines:boolean, optional, default None\n\n\nIf error_bad_lines is False, and warn_bad_lines is True, a warning for each \u201cbad line\u201d will be output.  Deprecated since version 1.3.0: The on_bad_lines parameter should be used instead to specify behavior upon encountering a bad line instead.   on_bad_lines:(\u2018error\u2019, \u2018warn\u2019, \u2018skip\u2019), default \u2018error\u2019\n\n\nSpecifies what to do upon encountering a bad line (a line with too many fields). Allowed values are :  \n \u2018error\u2019, raise an ParserError when a bad line is encountered. \u2018warn\u2019, print a warning when a bad line is encountered and skip that line. \u2018skip\u2019, skip bad lines without raising or warning when they are encountered.  \n  New in version 1.3.0.       Specifying column data types You can indicate the data type for the whole DataFrame or individual columns: \nIn [13]: import numpy as np\n\nIn [14]: data = \"a,b,c,d\\n1,2,3,4\\n5,6,7,8\\n9,10,11\"\n\nIn [15]: print(data)\na,b,c,d\n1,2,3,4\n5,6,7,8\n9,10,11\n\nIn [16]: df = pd.read_csv(StringIO(data), dtype=object)\n\nIn [17]: df\nOut[17]: \n   a   b   c    d\n0  1   2   3    4\n1  5   6   7    8\n2  9  10  11  NaN\n\nIn [18]: df[\"a\"][0]\nOut[18]: '1'\n\nIn [19]: df = pd.read_csv(StringIO(data), dtype={\"b\": object, \"c\": np.float64, \"d\": \"Int64\"})\n\nIn [20]: df.dtypes\nOut[20]: \na      int64\nb     object\nc    float64\nd      Int64\ndtype: object\n  Fortunately, pandas offers more than one way to ensure that your column(s) contain only one dtype. If you\u2019re unfamiliar with these concepts, you can see here to learn more about dtypes, and here to learn more about object conversion in pandas. For instance, you can use the converters argument of read_csv(): \nIn [21]: data = \"col_1\\n1\\n2\\n'A'\\n4.22\"\n\nIn [22]: df = pd.read_csv(StringIO(data), converters={\"col_1\": str})\n\nIn [23]: df\nOut[23]: \n  col_1\n0     1\n1     2\n2   'A'\n3  4.22\n\nIn [24]: df[\"col_1\"].apply(type).value_counts()\nOut[24]: \n<class 'str'>    4\nName: col_1, dtype: int64\n  Or you can use the to_numeric() function to coerce the dtypes after reading in the data, \nIn [25]: df2 = pd.read_csv(StringIO(data))\n\nIn [26]: df2[\"col_1\"] = pd.to_numeric(df2[\"col_1\"], errors=\"coerce\")\n\nIn [27]: df2\nOut[27]: \n   col_1\n0   1.00\n1   2.00\n2    NaN\n3   4.22\n\nIn [28]: df2[\"col_1\"].apply(type).value_counts()\nOut[28]: \n<class 'float'>    4\nName: col_1, dtype: int64\n  which will convert all valid parsing to floats, leaving the invalid parsing as NaN. Ultimately, how you deal with reading in columns containing mixed dtypes depends on your specific needs. In the case above, if you wanted to NaN out the data anomalies, then to_numeric() is probably your best option. However, if you wanted for all the data to be coerced, no matter the type, then using the converters argument of read_csv() would certainly be worth trying.  Note In some cases, reading in abnormal data with columns containing mixed dtypes will result in an inconsistent dataset. If you rely on pandas to infer the dtypes of your columns, the parsing engine will go and infer the dtypes for different chunks of the data, rather than the whole dataset at once. Consequently, you can end up with column(s) with mixed dtypes. For example, \nIn [29]: col_1 = list(range(500000)) + [\"a\", \"b\"] + list(range(500000))\n\nIn [30]: df = pd.DataFrame({\"col_1\": col_1})\n\nIn [31]: df.to_csv(\"foo.csv\")\n\nIn [32]: mixed_df = pd.read_csv(\"foo.csv\")\n\nIn [33]: mixed_df[\"col_1\"].apply(type).value_counts()\nOut[33]: \n<class 'int'>    737858\n<class 'str'>    262144\nName: col_1, dtype: int64\n\nIn [34]: mixed_df[\"col_1\"].dtype\nOut[34]: dtype('O')\n  will result with mixed_df containing an int dtype for certain chunks of the column, and str for others due to the mixed dtypes from the data that was read in. It is important to note that the overall column will be marked with a dtype of object, which is used for columns with mixed dtypes.    Specifying categorical dtype Categorical columns can be parsed directly by specifying dtype='category' or dtype=CategoricalDtype(categories, ordered). \nIn [35]: data = \"col1,col2,col3\\na,b,1\\na,b,2\\nc,d,3\"\n\nIn [36]: pd.read_csv(StringIO(data))\nOut[36]: \n  col1 col2  col3\n0    a    b     1\n1    a    b     2\n2    c    d     3\n\nIn [37]: pd.read_csv(StringIO(data)).dtypes\nOut[37]: \ncol1    object\ncol2    object\ncol3     int64\ndtype: object\n\nIn [38]: pd.read_csv(StringIO(data), dtype=\"category\").dtypes\nOut[38]: \ncol1    category\ncol2    category\ncol3    category\ndtype: object\n  Individual columns can be parsed as a Categorical using a dict specification: \nIn [39]: pd.read_csv(StringIO(data), dtype={\"col1\": \"category\"}).dtypes\nOut[39]: \ncol1    category\ncol2      object\ncol3       int64\ndtype: object\n  Specifying dtype='category' will result in an unordered Categorical whose categories are the unique values observed in the data. For more control on the categories and order, create a CategoricalDtype ahead of time, and pass that for that column\u2019s dtype. \nIn [40]: from pandas.api.types import CategoricalDtype\n\nIn [41]: dtype = CategoricalDtype([\"d\", \"c\", \"b\", \"a\"], ordered=True)\n\nIn [42]: pd.read_csv(StringIO(data), dtype={\"col1\": dtype}).dtypes\nOut[42]: \ncol1    category\ncol2      object\ncol3       int64\ndtype: object\n  When using dtype=CategoricalDtype, \u201cunexpected\u201d values outside of dtype.categories are treated as missing values. \nIn [43]: dtype = CategoricalDtype([\"a\", \"b\", \"d\"])  # No 'c'\n\nIn [44]: pd.read_csv(StringIO(data), dtype={\"col1\": dtype}).col1\nOut[44]: \n0      a\n1      a\n2    NaN\nName: col1, dtype: category\nCategories (3, object): ['a', 'b', 'd']\n  This matches the behavior of Categorical.set_categories().  Note With dtype='category', the resulting categories will always be parsed as strings (object dtype). If the categories are numeric they can be converted using the to_numeric() function, or as appropriate, another converter such as to_datetime(). When dtype is a CategoricalDtype with homogeneous categories ( all numeric, all datetimes, etc.), the conversion is done automatically. \nIn [45]: df = pd.read_csv(StringIO(data), dtype=\"category\")\n\nIn [46]: df.dtypes\nOut[46]: \ncol1    category\ncol2    category\ncol3    category\ndtype: object\n\nIn [47]: df[\"col3\"]\nOut[47]: \n0    1\n1    2\n2    3\nName: col3, dtype: category\nCategories (3, object): ['1', '2', '3']\n\nIn [48]: df[\"col3\"].cat.categories = pd.to_numeric(df[\"col3\"].cat.categories)\n\nIn [49]: df[\"col3\"]\nOut[49]: \n0    1\n1    2\n2    3\nName: col3, dtype: category\nCategories (3, int64): [1, 2, 3]\n     Naming and using columns  Handling column names A file may or may not have a header row. pandas assumes the first row should be used as the column names: \nIn [50]: data = \"a,b,c\\n1,2,3\\n4,5,6\\n7,8,9\"\n\nIn [51]: print(data)\na,b,c\n1,2,3\n4,5,6\n7,8,9\n\nIn [52]: pd.read_csv(StringIO(data))\nOut[52]: \n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n  By specifying the names argument in conjunction with header you can indicate other names to use and whether or not to throw away the header row (if any): \nIn [53]: print(data)\na,b,c\n1,2,3\n4,5,6\n7,8,9\n\nIn [54]: pd.read_csv(StringIO(data), names=[\"foo\", \"bar\", \"baz\"], header=0)\nOut[54]: \n   foo  bar  baz\n0    1    2    3\n1    4    5    6\n2    7    8    9\n\nIn [55]: pd.read_csv(StringIO(data), names=[\"foo\", \"bar\", \"baz\"], header=None)\nOut[55]: \n  foo bar baz\n0   a   b   c\n1   1   2   3\n2   4   5   6\n3   7   8   9\n  If the header is in a row other than the first, pass the row number to header. This will skip the preceding rows: \nIn [56]: data = \"skip this skip it\\na,b,c\\n1,2,3\\n4,5,6\\n7,8,9\"\n\nIn [57]: pd.read_csv(StringIO(data), header=1)\nOut[57]: \n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n   Note Default behavior is to infer the column names: if no names are passed the behavior is identical to header=0 and column names are inferred from the first non-blank line of the file, if column names are passed explicitly then the behavior is identical to header=None.     Duplicate names parsing If the file or header contains duplicate names, pandas will by default distinguish between them so as to prevent overwriting data: \nIn [58]: data = \"a,b,a\\n0,1,2\\n3,4,5\"\n\nIn [59]: pd.read_csv(StringIO(data))\nOut[59]: \n   a  b  a.1\n0  0  1    2\n1  3  4    5\n  There is no more duplicate data because mangle_dupe_cols=True by default, which modifies a series of duplicate columns \u2018X\u2019, \u2026, \u2018X\u2019 to become \u2018X\u2019, \u2018X.1\u2019, \u2026, \u2018X.N\u2019. If mangle_dupe_cols=False, duplicate data can arise: \nIn [2]: data = 'a,b,a\\n0,1,2\\n3,4,5'\nIn [3]: pd.read_csv(StringIO(data), mangle_dupe_cols=False)\nOut[3]:\n   a  b  a\n0  2  1  2\n1  5  4  5\n  To prevent users from encountering this problem with duplicate data, a ValueError exception is raised if mangle_dupe_cols != True: \nIn [2]: data = 'a,b,a\\n0,1,2\\n3,4,5'\nIn [3]: pd.read_csv(StringIO(data), mangle_dupe_cols=False)\n...\nValueError: Setting mangle_dupe_cols=False is not supported yet\n   Filtering columns (usecols) The usecols argument allows you to select any subset of the columns in a file, either using the column names, position numbers or a callable: \nIn [60]: data = \"a,b,c,d\\n1,2,3,foo\\n4,5,6,bar\\n7,8,9,baz\"\n\nIn [61]: pd.read_csv(StringIO(data))\nOut[61]: \n   a  b  c    d\n0  1  2  3  foo\n1  4  5  6  bar\n2  7  8  9  baz\n\nIn [62]: pd.read_csv(StringIO(data), usecols=[\"b\", \"d\"])\nOut[62]: \n   b    d\n0  2  foo\n1  5  bar\n2  8  baz\n\nIn [63]: pd.read_csv(StringIO(data), usecols=[0, 2, 3])\nOut[63]: \n   a  c    d\n0  1  3  foo\n1  4  6  bar\n2  7  9  baz\n\nIn [64]: pd.read_csv(StringIO(data), usecols=lambda x: x.upper() in [\"A\", \"C\"])\nOut[64]: \n   a  c\n0  1  3\n1  4  6\n2  7  9\n  The usecols argument can also be used to specify which columns not to use in the final result: \nIn [65]: pd.read_csv(StringIO(data), usecols=lambda x: x not in [\"a\", \"c\"])\nOut[65]: \n   b    d\n0  2  foo\n1  5  bar\n2  8  baz\n  In this case, the callable is specifying that we exclude the \u201ca\u201d and \u201cc\u201d columns from the output.    Comments and empty lines  Ignoring line comments and empty lines If the comment parameter is specified, then completely commented lines will be ignored. By default, completely blank lines will be ignored as well. \nIn [66]: data = \"\\na,b,c\\n  \\n# commented line\\n1,2,3\\n\\n4,5,6\"\n\nIn [67]: print(data)\n\na,b,c\n  \n# commented line\n1,2,3\n\n4,5,6\n\nIn [68]: pd.read_csv(StringIO(data), comment=\"#\")\nOut[68]: \n   a  b  c\n0  1  2  3\n1  4  5  6\n  If skip_blank_lines=False, then read_csv will not ignore blank lines: \nIn [69]: data = \"a,b,c\\n\\n1,2,3\\n\\n\\n4,5,6\"\n\nIn [70]: pd.read_csv(StringIO(data), skip_blank_lines=False)\nOut[70]: \n     a    b    c\n0  NaN  NaN  NaN\n1  1.0  2.0  3.0\n2  NaN  NaN  NaN\n3  NaN  NaN  NaN\n4  4.0  5.0  6.0\n   Warning The presence of ignored lines might create ambiguities involving line numbers; the parameter header uses row numbers (ignoring commented/empty lines), while skiprows uses line numbers (including commented/empty lines): \nIn [71]: data = \"#comment\\na,b,c\\nA,B,C\\n1,2,3\"\n\nIn [72]: pd.read_csv(StringIO(data), comment=\"#\", header=1)\nOut[72]: \n   A  B  C\n0  1  2  3\n\nIn [73]: data = \"A,B,C\\n#comment\\na,b,c\\n1,2,3\"\n\nIn [74]: pd.read_csv(StringIO(data), comment=\"#\", skiprows=2)\nOut[74]: \n   a  b  c\n0  1  2  3\n  If both header and skiprows are specified, header will be relative to the end of skiprows. For example:  \nIn [75]: data = (\n   ....:     \"# empty\\n\"\n   ....:     \"# second empty line\\n\"\n   ....:     \"# third emptyline\\n\"\n   ....:     \"X,Y,Z\\n\"\n   ....:     \"1,2,3\\n\"\n   ....:     \"A,B,C\\n\"\n   ....:     \"1,2.,4.\\n\"\n   ....:     \"5.,NaN,10.0\\n\"\n   ....: )\n   ....: \n\nIn [76]: print(data)\n# empty\n# second empty line\n# third emptyline\nX,Y,Z\n1,2,3\nA,B,C\n1,2.,4.\n5.,NaN,10.0\n\n\nIn [77]: pd.read_csv(StringIO(data), comment=\"#\", skiprows=4, header=1)\nOut[77]: \n     A    B     C\n0  1.0  2.0   4.0\n1  5.0  NaN  10.0\n    Comments Sometimes comments or meta data may be included in a file: \nIn [78]: print(open(\"tmp.csv\").read())\nID,level,category\nPatient1,123000,x # really unpleasant\nPatient2,23000,y # wouldn't take his medicine\nPatient3,1234018,z # awesome\n  By default, the parser includes the comments in the output: \nIn [79]: df = pd.read_csv(\"tmp.csv\")\n\nIn [80]: df\nOut[80]: \n         ID    level                        category\n0  Patient1   123000           x # really unpleasant\n1  Patient2    23000  y # wouldn't take his medicine\n2  Patient3  1234018                     z # awesome\n  We can suppress the comments using the comment keyword: \nIn [81]: df = pd.read_csv(\"tmp.csv\", comment=\"#\")\n\nIn [82]: df\nOut[82]: \n         ID    level category\n0  Patient1   123000       x \n1  Patient2    23000       y \n2  Patient3  1234018       z \n     Dealing with Unicode data The encoding argument should be used for encoded unicode data, which will result in byte strings being decoded to unicode in the result: \nIn [83]: from io import BytesIO\n\nIn [84]: data = b\"word,length\\n\" b\"Tr\\xc3\\xa4umen,7\\n\" b\"Gr\\xc3\\xbc\\xc3\\x9fe,5\"\n\nIn [85]: data = data.decode(\"utf8\").encode(\"latin-1\")\n\nIn [86]: df = pd.read_csv(BytesIO(data), encoding=\"latin-1\")\n\nIn [87]: df\nOut[87]: \n      word  length\n0  Tr\u00e4umen       7\n1    Gr\u00fc\u00dfe       5\n\nIn [88]: df[\"word\"][1]\nOut[88]: 'Gr\u00fc\u00dfe'\n  Some formats which encode all characters as multiple bytes, like UTF-16, won\u2019t parse correctly at all without specifying the encoding. Full list of Python standard encodings.   Index columns and trailing delimiters If a file has one more column of data than the number of column names, the first column will be used as the DataFrame\u2019s row names: \nIn [89]: data = \"a,b,c\\n4,apple,bat,5.7\\n8,orange,cow,10\"\n\nIn [90]: pd.read_csv(StringIO(data))\nOut[90]: \n        a    b     c\n4   apple  bat   5.7\n8  orange  cow  10.0\n  \nIn [91]: data = \"index,a,b,c\\n4,apple,bat,5.7\\n8,orange,cow,10\"\n\nIn [92]: pd.read_csv(StringIO(data), index_col=0)\nOut[92]: \n            a    b     c\nindex                   \n4       apple  bat   5.7\n8      orange  cow  10.0\n  Ordinarily, you can achieve this behavior using the index_col option. There are some exception cases when a file has been prepared with delimiters at the end of each data line, confusing the parser. To explicitly disable the index column inference and discard the last column, pass index_col=False: \nIn [93]: data = \"a,b,c\\n4,apple,bat,\\n8,orange,cow,\"\n\nIn [94]: print(data)\na,b,c\n4,apple,bat,\n8,orange,cow,\n\nIn [95]: pd.read_csv(StringIO(data))\nOut[95]: \n        a    b   c\n4   apple  bat NaN\n8  orange  cow NaN\n\nIn [96]: pd.read_csv(StringIO(data), index_col=False)\nOut[96]: \n   a       b    c\n0  4   apple  bat\n1  8  orange  cow\n  If a subset of data is being parsed using the usecols option, the index_col specification is based on that subset, not the original data. \nIn [97]: data = \"a,b,c\\n4,apple,bat,\\n8,orange,cow,\"\n\nIn [98]: print(data)\na,b,c\n4,apple,bat,\n8,orange,cow,\n\nIn [99]: pd.read_csv(StringIO(data), usecols=[\"b\", \"c\"])\nOut[99]: \n     b   c\n4  bat NaN\n8  cow NaN\n\nIn [100]: pd.read_csv(StringIO(data), usecols=[\"b\", \"c\"], index_col=0)\nOut[100]: \n     b   c\n4  bat NaN\n8  cow NaN\n    Date Handling  Specifying date columns To better facilitate working with datetime data, read_csv() uses the keyword arguments parse_dates and date_parser to allow users to specify a variety of columns and date/time formats to turn the input text data into datetime objects. The simplest case is to just pass in parse_dates=True: \n# Use a column as an index, and parse it as dates.\nIn [101]: df = pd.read_csv(\"foo.csv\", index_col=0, parse_dates=True)\n\nIn [102]: df\nOut[102]: \n            A  B  C\ndate               \n2009-01-01  a  1  2\n2009-01-02  b  3  4\n2009-01-03  c  4  5\n\n# These are Python datetime objects\nIn [103]: df.index\nOut[103]: DatetimeIndex(['2009-01-01', '2009-01-02', '2009-01-03'], dtype='datetime64[ns]', name='date', freq=None)\n  It is often the case that we may want to store date and time data separately, or store various date fields separately. the parse_dates keyword can be used to specify a combination of columns to parse the dates and/or times from. You can specify a list of column lists to parse_dates, the resulting date columns will be prepended to the output (so as to not affect the existing column order) and the new column names will be the concatenation of the component column names: \nIn [104]: print(open(\"tmp.csv\").read())\nKORD,19990127, 19:00:00, 18:56:00, 0.8100\nKORD,19990127, 20:00:00, 19:56:00, 0.0100\nKORD,19990127, 21:00:00, 20:56:00, -0.5900\nKORD,19990127, 21:00:00, 21:18:00, -0.9900\nKORD,19990127, 22:00:00, 21:56:00, -0.5900\nKORD,19990127, 23:00:00, 22:56:00, -0.5900\n\nIn [105]: df = pd.read_csv(\"tmp.csv\", header=None, parse_dates=[[1, 2], [1, 3]])\n\nIn [106]: df\nOut[106]: \n                  1_2                 1_3     0     4\n0 1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  0.81\n1 1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  0.01\n2 1999-01-27 21:00:00 1999-01-27 20:56:00  KORD -0.59\n3 1999-01-27 21:00:00 1999-01-27 21:18:00  KORD -0.99\n4 1999-01-27 22:00:00 1999-01-27 21:56:00  KORD -0.59\n5 1999-01-27 23:00:00 1999-01-27 22:56:00  KORD -0.59\n  By default the parser removes the component date columns, but you can choose to retain them via the keep_date_col keyword: \nIn [107]: df = pd.read_csv(\n   .....:     \"tmp.csv\", header=None, parse_dates=[[1, 2], [1, 3]], keep_date_col=True\n   .....: )\n   .....: \n\nIn [108]: df\nOut[108]: \n                  1_2                 1_3     0         1          2          3     4\n0 1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  19990127   19:00:00   18:56:00  0.81\n1 1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  19990127   20:00:00   19:56:00  0.01\n2 1999-01-27 21:00:00 1999-01-27 20:56:00  KORD  19990127   21:00:00   20:56:00 -0.59\n3 1999-01-27 21:00:00 1999-01-27 21:18:00  KORD  19990127   21:00:00   21:18:00 -0.99\n4 1999-01-27 22:00:00 1999-01-27 21:56:00  KORD  19990127   22:00:00   21:56:00 -0.59\n5 1999-01-27 23:00:00 1999-01-27 22:56:00  KORD  19990127   23:00:00   22:56:00 -0.59\n  Note that if you wish to combine multiple columns into a single date column, a nested list must be used. In other words, parse_dates=[1, 2] indicates that the second and third columns should each be parsed as separate date columns while parse_dates=[[1, 2]] means the two columns should be parsed into a single column. You can also use a dict to specify custom name columns: \nIn [109]: date_spec = {\"nominal\": [1, 2], \"actual\": [1, 3]}\n\nIn [110]: df = pd.read_csv(\"tmp.csv\", header=None, parse_dates=date_spec)\n\nIn [111]: df\nOut[111]: \n              nominal              actual     0     4\n0 1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  0.81\n1 1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  0.01\n2 1999-01-27 21:00:00 1999-01-27 20:56:00  KORD -0.59\n3 1999-01-27 21:00:00 1999-01-27 21:18:00  KORD -0.99\n4 1999-01-27 22:00:00 1999-01-27 21:56:00  KORD -0.59\n5 1999-01-27 23:00:00 1999-01-27 22:56:00  KORD -0.59\n  It is important to remember that if multiple text columns are to be parsed into a single date column, then a new column is prepended to the data. The index_col specification is based off of this new set of columns rather than the original data columns: \nIn [112]: date_spec = {\"nominal\": [1, 2], \"actual\": [1, 3]}\n\nIn [113]: df = pd.read_csv(\n   .....:     \"tmp.csv\", header=None, parse_dates=date_spec, index_col=0\n   .....: )  # index is the nominal column\n   .....: \n\nIn [114]: df\nOut[114]: \n                                 actual     0     4\nnominal                                            \n1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  0.81\n1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  0.01\n1999-01-27 21:00:00 1999-01-27 20:56:00  KORD -0.59\n1999-01-27 21:00:00 1999-01-27 21:18:00  KORD -0.99\n1999-01-27 22:00:00 1999-01-27 21:56:00  KORD -0.59\n1999-01-27 23:00:00 1999-01-27 22:56:00  KORD -0.59\n   Note If a column or index contains an unparsable date, the entire column or index will be returned unaltered as an object data type. For non-standard datetime parsing, use to_datetime() after pd.read_csv.   Note read_csv has a fast_path for parsing datetime strings in iso8601 format, e.g \u201c2000-01-01T00:01:02+00:00\u201d and similar variations. If you can arrange for your data to store datetimes in this format, load times will be significantly faster, ~20x has been observed.    Date parsing functions Finally, the parser allows you to specify a custom date_parser function to take full advantage of the flexibility of the date parsing API: \nIn [115]: df = pd.read_csv(\n   .....:     \"tmp.csv\", header=None, parse_dates=date_spec, date_parser=pd.to_datetime\n   .....: )\n   .....: \n\nIn [116]: df\nOut[116]: \n              nominal              actual     0     4\n0 1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  0.81\n1 1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  0.01\n2 1999-01-27 21:00:00 1999-01-27 20:56:00  KORD -0.59\n3 1999-01-27 21:00:00 1999-01-27 21:18:00  KORD -0.99\n4 1999-01-27 22:00:00 1999-01-27 21:56:00  KORD -0.59\n5 1999-01-27 23:00:00 1999-01-27 22:56:00  KORD -0.59\n  pandas will try to call the date_parser function in three different ways. If an exception is raised, the next one is tried:  date_parser is first called with one or more arrays as arguments, as defined using parse_dates (e.g., date_parser(['2013', '2013'], ['1', '2'])). If #1 fails, date_parser is called with all the columns concatenated row-wise into a single array (e.g., date_parser(['2013 1', '2013 2'])).  Note that performance-wise, you should try these methods of parsing dates in order:  Try to infer the format using infer_datetime_format=True (see section below). If you know the format, use pd.to_datetime(): date_parser=lambda x: pd.to_datetime(x, format=...). If you have a really non-standard format, use a custom date_parser function. For optimal performance, this should be vectorized, i.e., it should accept arrays as arguments.    Parsing a CSV with mixed timezones pandas cannot natively represent a column or index with mixed timezones. If your CSV file contains columns with a mixture of timezones, the default result will be an object-dtype column with strings, even with parse_dates. \nIn [117]: content = \"\"\"\\\n   .....: a\n   .....: 2000-01-01T00:00:00+05:00\n   .....: 2000-01-01T00:00:00+06:00\"\"\"\n   .....: \n\nIn [118]: df = pd.read_csv(StringIO(content), parse_dates=[\"a\"])\n\nIn [119]: df[\"a\"]\nOut[119]: \n0    2000-01-01 00:00:00+05:00\n1    2000-01-01 00:00:00+06:00\nName: a, dtype: object\n  To parse the mixed-timezone values as a datetime column, pass a partially-applied to_datetime() with utc=True as the date_parser. \nIn [120]: df = pd.read_csv(\n   .....:     StringIO(content),\n   .....:     parse_dates=[\"a\"],\n   .....:     date_parser=lambda col: pd.to_datetime(col, utc=True),\n   .....: )\n   .....: \n\nIn [121]: df[\"a\"]\nOut[121]: \n0   1999-12-31 19:00:00+00:00\n1   1999-12-31 18:00:00+00:00\nName: a, dtype: datetime64[ns, UTC]\n    Inferring datetime format If you have parse_dates enabled for some or all of your columns, and your datetime strings are all formatted the same way, you may get a large speed up by setting infer_datetime_format=True. If set, pandas will attempt to guess the format of your datetime strings, and then use a faster means of parsing the strings. 5-10x parsing speeds have been observed. pandas will fallback to the usual parsing if either the format cannot be guessed or the format that was guessed cannot properly parse the entire column of strings. So in general, infer_datetime_format should not have any negative consequences if enabled. Here are some examples of datetime strings that can be guessed (All representing December 30th, 2011 at 00:00:00):  \u201c20111230\u201d \u201c2011/12/30\u201d \u201c20111230 00:00:00\u201d \u201c12/30/2011 00:00:00\u201d \u201c30/Dec/2011 00:00:00\u201d \u201c30/December/2011 00:00:00\u201d  Note that infer_datetime_format is sensitive to dayfirst. With dayfirst=True, it will guess \u201c01/12/2011\u201d to be December 1st. With dayfirst=False (default) it will guess \u201c01/12/2011\u201d to be January 12th. \n# Try to infer the format for the index column\nIn [122]: df = pd.read_csv(\n   .....:     \"foo.csv\",\n   .....:     index_col=0,\n   .....:     parse_dates=True,\n   .....:     infer_datetime_format=True,\n   .....: )\n   .....: \n\nIn [123]: df\nOut[123]: \n            A  B  C\ndate               \n2009-01-01  a  1  2\n2009-01-02  b  3  4\n2009-01-03  c  4  5\n    International date formats While US date formats tend to be MM/DD/YYYY, many international formats use DD/MM/YYYY instead. For convenience, a dayfirst keyword is provided: \nIn [124]: print(open(\"tmp.csv\").read())\ndate,value,cat\n1/6/2000,5,a\n2/6/2000,10,b\n3/6/2000,15,c\n\nIn [125]: pd.read_csv(\"tmp.csv\", parse_dates=[0])\nOut[125]: \n        date  value cat\n0 2000-01-06      5   a\n1 2000-02-06     10   b\n2 2000-03-06     15   c\n\nIn [126]: pd.read_csv(\"tmp.csv\", dayfirst=True, parse_dates=[0])\nOut[126]: \n        date  value cat\n0 2000-06-01      5   a\n1 2000-06-02     10   b\n2 2000-06-03     15   c\n    Writing CSVs to binary file objects  New in version 1.2.0.  df.to_csv(..., mode=\"wb\") allows writing a CSV to a file object opened binary mode. In most cases, it is not necessary to specify mode as Pandas will auto-detect whether the file object is opened in text or binary mode. \nIn [127]: import io\n\nIn [128]: data = pd.DataFrame([0, 1, 2])\n\nIn [129]: buffer = io.BytesIO()\n\nIn [130]: data.to_csv(buffer, encoding=\"utf-8\", compression=\"gzip\")\n     Specifying method for floating-point conversion The parameter float_precision can be specified in order to use a specific floating-point converter during parsing with the C engine. The options are the ordinary converter, the high-precision converter, and the round-trip converter (which is guaranteed to round-trip values after writing to a file). For example: \nIn [131]: val = \"0.3066101993807095471566981359501369297504425048828125\"\n\nIn [132]: data = \"a,b,c\\n1,2,{0}\".format(val)\n\nIn [133]: abs(\n   .....:     pd.read_csv(\n   .....:         StringIO(data),\n   .....:         engine=\"c\",\n   .....:         float_precision=None,\n   .....:     )[\"c\"][0] - float(val)\n   .....: )\n   .....: \nOut[133]: 5.551115123125783e-17\n\nIn [134]: abs(\n   .....:     pd.read_csv(\n   .....:         StringIO(data),\n   .....:         engine=\"c\",\n   .....:         float_precision=\"high\",\n   .....:     )[\"c\"][0] - float(val)\n   .....: )\n   .....: \nOut[134]: 5.551115123125783e-17\n\nIn [135]: abs(\n   .....:     pd.read_csv(StringIO(data), engine=\"c\", float_precision=\"round_trip\")[\"c\"][0]\n   .....:     - float(val)\n   .....: )\n   .....: \nOut[135]: 0.0\n    Thousand separators For large numbers that have been written with a thousands separator, you can set the thousands keyword to a string of length 1 so that integers will be parsed correctly: By default, numbers with a thousands separator will be parsed as strings: \nIn [136]: print(open(\"tmp.csv\").read())\nID|level|category\nPatient1|123,000|x\nPatient2|23,000|y\nPatient3|1,234,018|z\n\nIn [137]: df = pd.read_csv(\"tmp.csv\", sep=\"|\")\n\nIn [138]: df\nOut[138]: \n         ID      level category\n0  Patient1    123,000        x\n1  Patient2     23,000        y\n2  Patient3  1,234,018        z\n\nIn [139]: df.level.dtype\nOut[139]: dtype('O')\n  The thousands keyword allows integers to be parsed correctly: \nIn [140]: print(open(\"tmp.csv\").read())\nID|level|category\nPatient1|123,000|x\nPatient2|23,000|y\nPatient3|1,234,018|z\n\nIn [141]: df = pd.read_csv(\"tmp.csv\", sep=\"|\", thousands=\",\")\n\nIn [142]: df\nOut[142]: \n         ID    level category\n0  Patient1   123000        x\n1  Patient2    23000        y\n2  Patient3  1234018        z\n\nIn [143]: df.level.dtype\nOut[143]: dtype('int64')\n    NA values To control which values are parsed as missing values (which are signified by NaN), specify a string in na_values. If you specify a list of strings, then all values in it are considered to be missing values. If you specify a number (a float, like 5.0 or an integer like 5), the corresponding equivalent values will also imply a missing value (in this case effectively [5.0, 5] are recognized as NaN). To completely override the default values that are recognized as missing, specify keep_default_na=False. The default NaN recognized values are ['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN', '#N/A N/A', '#N/A', 'N/A',\n'n/a', 'NA', '<NA>', '#NA', 'NULL', 'null', 'NaN', '-NaN', 'nan', '-nan', '']. Let us consider some examples: \npd.read_csv(\"path_to_file.csv\", na_values=[5])\n  In the example above 5 and 5.0 will be recognized as NaN, in addition to the defaults. A string will first be interpreted as a numerical 5, then as a NaN. \npd.read_csv(\"path_to_file.csv\", keep_default_na=False, na_values=[\"\"])\n  Above, only an empty field will be recognized as NaN. \npd.read_csv(\"path_to_file.csv\", keep_default_na=False, na_values=[\"NA\", \"0\"])\n  Above, both NA and 0 as strings are NaN. \npd.read_csv(\"path_to_file.csv\", na_values=[\"Nope\"])\n  The default values, in addition to the string \"Nope\" are recognized as NaN.   Infinity inf like values will be parsed as np.inf (positive infinity), and -inf as -np.inf (negative infinity). These will ignore the case of the value, meaning Inf, will also be parsed as np.inf.   Returning Series Using the squeeze keyword, the parser will return output with a single column as a Series:  Deprecated since version 1.4.0: Users should append .squeeze(\"columns\") to the DataFrame returned by read_csv instead.  \nIn [144]: print(open(\"tmp.csv\").read())\nlevel\nPatient1,123000\nPatient2,23000\nPatient3,1234018\n\nIn [145]: output = pd.read_csv(\"tmp.csv\", squeeze=True)\n\nIn [146]: output\nOut[146]: \nPatient1     123000\nPatient2      23000\nPatient3    1234018\nName: level, dtype: int64\n\nIn [147]: type(output)\nOut[147]: pandas.core.series.Series\n    Boolean values The common values True, False, TRUE, and FALSE are all recognized as boolean. Occasionally you might want to recognize other values as being boolean. To do this, use the true_values and false_values options as follows: \nIn [148]: data = \"a,b,c\\n1,Yes,2\\n3,No,4\"\n\nIn [149]: print(data)\na,b,c\n1,Yes,2\n3,No,4\n\nIn [150]: pd.read_csv(StringIO(data))\nOut[150]: \n   a    b  c\n0  1  Yes  2\n1  3   No  4\n\nIn [151]: pd.read_csv(StringIO(data), true_values=[\"Yes\"], false_values=[\"No\"])\nOut[151]: \n   a      b  c\n0  1   True  2\n1  3  False  4\n    Handling \u201cbad\u201d lines Some files may have malformed lines with too few fields or too many. Lines with too few fields will have NA values filled in the trailing fields. Lines with too many fields will raise an error by default: \nIn [152]: data = \"a,b,c\\n1,2,3\\n4,5,6,7\\n8,9,10\"\n\nIn [153]: pd.read_csv(StringIO(data))\n---------------------------------------------------------------------------\nParserError                               Traceback (most recent call last)\nInput In [153], in <module>\n----> 1 pd.read_csv(StringIO(data))\n\nFile /pandas/pandas/util/_decorators.py:311, in deprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper(*args, **kwargs)\n    305 if len(args) > num_allow_args:\n    306     warnings.warn(\n    307         msg.format(arguments=arguments),\n    308         FutureWarning,\n    309         stacklevel=stacklevel,\n    310     )\n--> 311 return func(*args, **kwargs)\n\nFile /pandas/pandas/io/parsers/readers.py:680, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\n    665 kwds_defaults = _refine_defaults_read(\n    666     dialect,\n    667     delimiter,\n   (...)\n    676     defaults={\"delimiter\": \",\"},\n    677 )\n    678 kwds.update(kwds_defaults)\n--> 680 return _read(filepath_or_buffer, kwds)\n\nFile /pandas/pandas/io/parsers/readers.py:581, in _read(filepath_or_buffer, kwds)\n    578     return parser\n    580 with parser:\n--> 581     return parser.read(nrows)\n\nFile /pandas/pandas/io/parsers/readers.py:1250, in TextFileReader.read(self, nrows)\n   1248 nrows = validate_integer(\"nrows\", nrows)\n   1249 try:\n-> 1250     index, columns, col_dict = self._engine.read(nrows)\n   1251 except Exception:\n   1252     self.close()\n\nFile /pandas/pandas/io/parsers/c_parser_wrapper.py:225, in CParserWrapper.read(self, nrows)\n    223 try:\n    224     if self.low_memory:\n--> 225         chunks = self._reader.read_low_memory(nrows)\n    226         # destructive to chunks\n    227         data = _concatenate_chunks(chunks)\n\nFile /pandas/pandas/_libs/parsers.pyx:805, in pandas._libs.parsers.TextReader.read_low_memory()\n\nFile /pandas/pandas/_libs/parsers.pyx:861, in pandas._libs.parsers.TextReader._read_rows()\n\nFile /pandas/pandas/_libs/parsers.pyx:847, in pandas._libs.parsers.TextReader._tokenize_rows()\n\nFile /pandas/pandas/_libs/parsers.pyx:1960, in pandas._libs.parsers.raise_parser_error()\n\nParserError: Error tokenizing data. C error: Expected 3 fields in line 3, saw 4\n  You can elect to skip bad lines: \nIn [29]: pd.read_csv(StringIO(data), on_bad_lines=\"warn\")\nSkipping line 3: expected 3 fields, saw 4\n\nOut[29]:\n   a  b   c\n0  1  2   3\n1  8  9  10\n  Or pass a callable function to handle the bad line if engine=\"python\". The bad line will be a list of strings that was split by the sep: \nIn [29]: external_list = []\n\nIn [30]: def bad_lines_func(line):\n    ...:     external_list.append(line)\n    ...:     return line[-3:]\n\nIn [31]: pd.read_csv(StringIO(data), on_bad_lines=bad_lines_func, engine=\"python\")\nOut[31]:\n   a  b   c\n0  1  2   3\n1  5  6   7\n2  8  9  10\n\nIn [32]: external_list\nOut[32]: [4, 5, 6, 7]\n\n.. versionadded:: 1.4.0\n  You can also use the usecols parameter to eliminate extraneous column data that appear in some lines but not others: \nIn [33]: pd.read_csv(StringIO(data), usecols=[0, 1, 2])\n\n Out[33]:\n    a  b   c\n 0  1  2   3\n 1  4  5   6\n 2  8  9  10\n  In case you want to keep all data including the lines with too many fields, you can specify a sufficient number of names. This ensures that lines with not enough fields are filled with NaN. \nIn [34]: pd.read_csv(StringIO(data), names=['a', 'b', 'c', 'd'])\n\nOut[34]:\n    a  b   c  d\n 0  1  2   3  NaN\n 1  4  5   6  7\n 2  8  9  10  NaN\n    Dialect The dialect keyword gives greater flexibility in specifying the file format. By default it uses the Excel dialect but you can specify either the dialect name or a csv.Dialect instance. Suppose you had data with unenclosed quotes: \nIn [154]: print(data)\nlabel1,label2,label3\nindex1,\"a,c,e\nindex2,b,d,f\n  By default, read_csv uses the Excel dialect and treats the double quote as the quote character, which causes it to fail when it finds a newline before it finds the closing double quote. We can get around this using dialect: \nIn [155]: import csv\n\nIn [156]: dia = csv.excel()\n\nIn [157]: dia.quoting = csv.QUOTE_NONE\n\nIn [158]: pd.read_csv(StringIO(data), dialect=dia)\nOut[158]: \n       label1 label2 label3\nindex1     \"a      c      e\nindex2      b      d      f\n  All of the dialect options can be specified separately by keyword arguments: \nIn [159]: data = \"a,b,c~1,2,3~4,5,6\"\n\nIn [160]: pd.read_csv(StringIO(data), lineterminator=\"~\")\nOut[160]: \n   a  b  c\n0  1  2  3\n1  4  5  6\n  Another common dialect option is skipinitialspace, to skip any whitespace after a delimiter: \nIn [161]: data = \"a, b, c\\n1, 2, 3\\n4, 5, 6\"\n\nIn [162]: print(data)\na, b, c\n1, 2, 3\n4, 5, 6\n\nIn [163]: pd.read_csv(StringIO(data), skipinitialspace=True)\nOut[163]: \n   a  b  c\n0  1  2  3\n1  4  5  6\n  The parsers make every attempt to \u201cdo the right thing\u201d and not be fragile. Type inference is a pretty big deal. If a column can be coerced to integer dtype without altering the contents, the parser will do so. Any non-numeric columns will come through as object dtype as with the rest of pandas objects.   Quoting and Escape Characters Quotes (and other escape characters) in embedded fields can be handled in any number of ways. One way is to use backslashes; to properly parse this data, you should pass the escapechar option: \nIn [164]: data = 'a,b\\n\"hello, \\\\\"Bob\\\\\", nice to see you\",5'\n\nIn [165]: print(data)\na,b\n\"hello, \\\"Bob\\\", nice to see you\",5\n\nIn [166]: pd.read_csv(StringIO(data), escapechar=\"\\\\\")\nOut[166]: \n                               a  b\n0  hello, \"Bob\", nice to see you  5\n    Files with fixed width columns While read_csv() reads delimited data, the read_fwf() function works with data files that have known and fixed column widths. The function parameters to read_fwf are largely the same as read_csv with two extra parameters, and a different usage of the delimiter parameter:  colspecs: A list of pairs (tuples) giving the extents of the fixed-width fields of each line as half-open intervals (i.e., [from, to[ ). String value \u2018infer\u2019 can be used to instruct the parser to try detecting the column specifications from the first 100 rows of the data. Default behavior, if not specified, is to infer. widths: A list of field widths which can be used instead of \u2018colspecs\u2019 if the intervals are contiguous. delimiter: Characters to consider as filler characters in the fixed-width file. Can be used to specify the filler character of the fields if it is not spaces (e.g., \u2018~\u2019).  Consider a typical fixed-width data file: \nIn [167]: print(open(\"bar.csv\").read())\nid8141    360.242940   149.910199   11950.7\nid1594    444.953632   166.985655   11788.4\nid1849    364.136849   183.628767   11806.2\nid1230    413.836124   184.375703   11916.8\nid1948    502.953953   173.237159   12468.3\n  In order to parse this file into a DataFrame, we simply need to supply the column specifications to the read_fwf function along with the file name: \n# Column specifications are a list of half-intervals\nIn [168]: colspecs = [(0, 6), (8, 20), (21, 33), (34, 43)]\n\nIn [169]: df = pd.read_fwf(\"bar.csv\", colspecs=colspecs, header=None, index_col=0)\n\nIn [170]: df\nOut[170]: \n                 1           2        3\n0                                      \nid8141  360.242940  149.910199  11950.7\nid1594  444.953632  166.985655  11788.4\nid1849  364.136849  183.628767  11806.2\nid1230  413.836124  184.375703  11916.8\nid1948  502.953953  173.237159  12468.3\n  Note how the parser automatically picks column names X.<column number> when header=None argument is specified. Alternatively, you can supply just the column widths for contiguous columns: \n# Widths are a list of integers\nIn [171]: widths = [6, 14, 13, 10]\n\nIn [172]: df = pd.read_fwf(\"bar.csv\", widths=widths, header=None)\n\nIn [173]: df\nOut[173]: \n        0           1           2        3\n0  id8141  360.242940  149.910199  11950.7\n1  id1594  444.953632  166.985655  11788.4\n2  id1849  364.136849  183.628767  11806.2\n3  id1230  413.836124  184.375703  11916.8\n4  id1948  502.953953  173.237159  12468.3\n  The parser will take care of extra white spaces around the columns so it\u2019s ok to have extra separation between the columns in the file. By default, read_fwf will try to infer the file\u2019s colspecs by using the first 100 rows of the file. It can do it only in cases when the columns are aligned and correctly separated by the provided delimiter (default delimiter is whitespace). \nIn [174]: df = pd.read_fwf(\"bar.csv\", header=None, index_col=0)\n\nIn [175]: df\nOut[175]: \n                 1           2        3\n0                                      \nid8141  360.242940  149.910199  11950.7\nid1594  444.953632  166.985655  11788.4\nid1849  364.136849  183.628767  11806.2\nid1230  413.836124  184.375703  11916.8\nid1948  502.953953  173.237159  12468.3\n  read_fwf supports the dtype parameter for specifying the types of parsed columns to be different from the inferred type. \nIn [176]: pd.read_fwf(\"bar.csv\", header=None, index_col=0).dtypes\nOut[176]: \n1    float64\n2    float64\n3    float64\ndtype: object\n\nIn [177]: pd.read_fwf(\"bar.csv\", header=None, dtype={2: \"object\"}).dtypes\nOut[177]: \n0     object\n1    float64\n2     object\n3    float64\ndtype: object\n    Indexes  Files with an \u201cimplicit\u201d index column Consider a file with one less entry in the header than the number of data column: \nIn [178]: print(open(\"foo.csv\").read())\nA,B,C\n20090101,a,1,2\n20090102,b,3,4\n20090103,c,4,5\n  In this special case, read_csv assumes that the first column is to be used as the index of the DataFrame: \nIn [179]: pd.read_csv(\"foo.csv\")\nOut[179]: \n          A  B  C\n20090101  a  1  2\n20090102  b  3  4\n20090103  c  4  5\n  Note that the dates weren\u2019t automatically parsed. In that case you would need to do as before: \nIn [180]: df = pd.read_csv(\"foo.csv\", parse_dates=True)\n\nIn [181]: df.index\nOut[181]: DatetimeIndex(['2009-01-01', '2009-01-02', '2009-01-03'], dtype='datetime64[ns]', freq=None)\n    Reading an index with a MultiIndex\n Suppose you have data indexed by two columns: \nIn [182]: print(open(\"data/mindex_ex.csv\").read())\nyear,indiv,zit,xit\n1977,\"A\",1.2,.6\n1977,\"B\",1.5,.5\n1977,\"C\",1.7,.8\n1978,\"A\",.2,.06\n1978,\"B\",.7,.2\n1978,\"C\",.8,.3\n1978,\"D\",.9,.5\n1978,\"E\",1.4,.9\n1979,\"C\",.2,.15\n1979,\"D\",.14,.05\n1979,\"E\",.5,.15\n1979,\"F\",1.2,.5\n1979,\"G\",3.4,1.9\n1979,\"H\",5.4,2.7\n1979,\"I\",6.4,1.2\n  The index_col argument to read_csv can take a list of column numbers to turn multiple columns into a MultiIndex for the index of the returned object: \nIn [183]: df = pd.read_csv(\"data/mindex_ex.csv\", index_col=[0, 1])\n\nIn [184]: df\nOut[184]: \n             zit   xit\nyear indiv            \n1977 A      1.20  0.60\n     B      1.50  0.50\n     C      1.70  0.80\n1978 A      0.20  0.06\n     B      0.70  0.20\n     C      0.80  0.30\n     D      0.90  0.50\n     E      1.40  0.90\n1979 C      0.20  0.15\n     D      0.14  0.05\n     E      0.50  0.15\n     F      1.20  0.50\n     G      3.40  1.90\n     H      5.40  2.70\n     I      6.40  1.20\n\nIn [185]: df.loc[1978]\nOut[185]: \n       zit   xit\nindiv           \nA      0.2  0.06\nB      0.7  0.20\nC      0.8  0.30\nD      0.9  0.50\nE      1.4  0.90\n    Reading columns with a MultiIndex\n By specifying list of row locations for the header argument, you can read in a MultiIndex for the columns. Specifying non-consecutive rows will skip the intervening rows. \nIn [186]: from pandas._testing import makeCustomDataframe as mkdf\n\nIn [187]: df = mkdf(5, 3, r_idx_nlevels=2, c_idx_nlevels=4)\n\nIn [188]: df.to_csv(\"mi.csv\")\n\nIn [189]: print(open(\"mi.csv\").read())\nC0,,C_l0_g0,C_l0_g1,C_l0_g2\nC1,,C_l1_g0,C_l1_g1,C_l1_g2\nC2,,C_l2_g0,C_l2_g1,C_l2_g2\nC3,,C_l3_g0,C_l3_g1,C_l3_g2\nR0,R1,,,\nR_l0_g0,R_l1_g0,R0C0,R0C1,R0C2\nR_l0_g1,R_l1_g1,R1C0,R1C1,R1C2\nR_l0_g2,R_l1_g2,R2C0,R2C1,R2C2\nR_l0_g3,R_l1_g3,R3C0,R3C1,R3C2\nR_l0_g4,R_l1_g4,R4C0,R4C1,R4C2\n\n\nIn [190]: pd.read_csv(\"mi.csv\", header=[0, 1, 2, 3], index_col=[0, 1])\nOut[190]: \nC0              C_l0_g0 C_l0_g1 C_l0_g2\nC1              C_l1_g0 C_l1_g1 C_l1_g2\nC2              C_l2_g0 C_l2_g1 C_l2_g2\nC3              C_l3_g0 C_l3_g1 C_l3_g2\nR0      R1                             \nR_l0_g0 R_l1_g0    R0C0    R0C1    R0C2\nR_l0_g1 R_l1_g1    R1C0    R1C1    R1C2\nR_l0_g2 R_l1_g2    R2C0    R2C1    R2C2\nR_l0_g3 R_l1_g3    R3C0    R3C1    R3C2\nR_l0_g4 R_l1_g4    R4C0    R4C1    R4C2\n  read_csv is also able to interpret a more common format of multi-columns indices. \nIn [191]: print(open(\"mi2.csv\").read())\n,a,a,a,b,c,c\n,q,r,s,t,u,v\none,1,2,3,4,5,6\ntwo,7,8,9,10,11,12\n\nIn [192]: pd.read_csv(\"mi2.csv\", header=[0, 1], index_col=0)\nOut[192]: \n     a         b   c    \n     q  r  s   t   u   v\none  1  2  3   4   5   6\ntwo  7  8  9  10  11  12\n  Note: If an index_col is not specified (e.g. you don\u2019t have an index, or wrote it with df.to_csv(..., index=False), then any names on the columns index will be lost.    Automatically \u201csniffing\u201d the delimiter read_csv is capable of inferring delimited (not necessarily comma-separated) files, as pandas uses the csv.Sniffer class of the csv module. For this, you have to specify sep=None. \nIn [193]: print(open(\"tmp2.sv\").read())\n:0:1:2:3\n0:0.4691122999071863:-0.2828633443286633:-1.5090585031735124:-1.1356323710171934\n1:1.2121120250208506:-0.17321464905330858:0.11920871129693428:-1.0442359662799567\n2:-0.8618489633477999:-2.1045692188948086:-0.4949292740687813:1.071803807037338\n3:0.7215551622443669:-0.7067711336300845:-1.0395749851146963:0.27185988554282986\n4:-0.42497232978883753:0.567020349793672:0.27623201927771873:-1.0874006912859915\n5:-0.6736897080883706:0.1136484096888855:-1.4784265524372235:0.5249876671147047\n6:0.4047052186802365:0.5770459859204836:-1.7150020161146375:-1.0392684835147725\n7:-0.3706468582364464:-1.1578922506419993:-1.344311812731667:0.8448851414248841\n8:1.0757697837155533:-0.10904997528022223:1.6435630703622064:-1.4693879595399115\n9:0.35702056413309086:-0.6746001037299882:-1.776903716971867:-0.9689138124473498\n\n\nIn [194]: pd.read_csv(\"tmp2.sv\", sep=None, engine=\"python\")\nOut[194]: \n   Unnamed: 0         0         1         2         3\n0           0  0.469112 -0.282863 -1.509059 -1.135632\n1           1  1.212112 -0.173215  0.119209 -1.044236\n2           2 -0.861849 -2.104569 -0.494929  1.071804\n3           3  0.721555 -0.706771 -1.039575  0.271860\n4           4 -0.424972  0.567020  0.276232 -1.087401\n5           5 -0.673690  0.113648 -1.478427  0.524988\n6           6  0.404705  0.577046 -1.715002 -1.039268\n7           7 -0.370647 -1.157892 -1.344312  0.844885\n8           8  1.075770 -0.109050  1.643563 -1.469388\n9           9  0.357021 -0.674600 -1.776904 -0.968914\n    Reading multiple files to create a single DataFrame It\u2019s best to use concat() to combine multiple files. See the cookbook for an example.   Iterating through files chunk by chunk Suppose you wish to iterate through a (potentially very large) file lazily rather than reading the entire file into memory, such as the following: \nIn [195]: print(open(\"tmp.sv\").read())\n|0|1|2|3\n0|0.4691122999071863|-0.2828633443286633|-1.5090585031735124|-1.1356323710171934\n1|1.2121120250208506|-0.17321464905330858|0.11920871129693428|-1.0442359662799567\n2|-0.8618489633477999|-2.1045692188948086|-0.4949292740687813|1.071803807037338\n3|0.7215551622443669|-0.7067711336300845|-1.0395749851146963|0.27185988554282986\n4|-0.42497232978883753|0.567020349793672|0.27623201927771873|-1.0874006912859915\n5|-0.6736897080883706|0.1136484096888855|-1.4784265524372235|0.5249876671147047\n6|0.4047052186802365|0.5770459859204836|-1.7150020161146375|-1.0392684835147725\n7|-0.3706468582364464|-1.1578922506419993|-1.344311812731667|0.8448851414248841\n8|1.0757697837155533|-0.10904997528022223|1.6435630703622064|-1.4693879595399115\n9|0.35702056413309086|-0.6746001037299882|-1.776903716971867|-0.9689138124473498\n\n\nIn [196]: table = pd.read_csv(\"tmp.sv\", sep=\"|\")\n\nIn [197]: table\nOut[197]: \n   Unnamed: 0         0         1         2         3\n0           0  0.469112 -0.282863 -1.509059 -1.135632\n1           1  1.212112 -0.173215  0.119209 -1.044236\n2           2 -0.861849 -2.104569 -0.494929  1.071804\n3           3  0.721555 -0.706771 -1.039575  0.271860\n4           4 -0.424972  0.567020  0.276232 -1.087401\n5           5 -0.673690  0.113648 -1.478427  0.524988\n6           6  0.404705  0.577046 -1.715002 -1.039268\n7           7 -0.370647 -1.157892 -1.344312  0.844885\n8           8  1.075770 -0.109050  1.643563 -1.469388\n9           9  0.357021 -0.674600 -1.776904 -0.968914\n  By specifying a chunksize to read_csv, the return value will be an iterable object of type TextFileReader: \nIn [198]: with pd.read_csv(\"tmp.sv\", sep=\"|\", chunksize=4) as reader:\n   .....:     reader\n   .....:     for chunk in reader:\n   .....:         print(chunk)\n   .....: \n   Unnamed: 0         0         1         2         3\n0           0  0.469112 -0.282863 -1.509059 -1.135632\n1           1  1.212112 -0.173215  0.119209 -1.044236\n2           2 -0.861849 -2.104569 -0.494929  1.071804\n3           3  0.721555 -0.706771 -1.039575  0.271860\n   Unnamed: 0         0         1         2         3\n4           4 -0.424972  0.567020  0.276232 -1.087401\n5           5 -0.673690  0.113648 -1.478427  0.524988\n6           6  0.404705  0.577046 -1.715002 -1.039268\n7           7 -0.370647 -1.157892 -1.344312  0.844885\n   Unnamed: 0         0        1         2         3\n8           8  1.075770 -0.10905  1.643563 -1.469388\n9           9  0.357021 -0.67460 -1.776904 -0.968914\n   Changed in version 1.2: read_csv/json/sas return a context-manager when iterating through a file.  Specifying iterator=True will also return the TextFileReader object: \nIn [199]: with pd.read_csv(\"tmp.sv\", sep=\"|\", iterator=True) as reader:\n   .....:     reader.get_chunk(5)\n   .....: \n    Specifying the parser engine Pandas currently supports three engines, the C engine, the python engine, and an experimental pyarrow engine (requires the pyarrow package). In general, the pyarrow engine is fastest on larger workloads and is equivalent in speed to the C engine on most other workloads. The python engine tends to be slower than the pyarrow and C engines on most workloads. However, the pyarrow engine is much less robust than the C engine, which lacks a few features compared to the Python engine. Where possible, pandas uses the C parser (specified as engine='c'), but it may fall back to Python if C-unsupported options are specified. Currently, options unsupported by the C and pyarrow engines include:  sep other than a single character (e.g. regex separators) skipfooter sep=None with delim_whitespace=False  Specifying any of the above options will produce a ParserWarning unless the python engine is selected explicitly using engine='python'. Options that are unsupported by the pyarrow engine which are not covered by the list above include:  float_precision chunksize comment nrows thousands memory_map dialect warn_bad_lines error_bad_lines on_bad_lines delim_whitespace quoting lineterminator converters decimal iterator dayfirst infer_datetime_format verbose skipinitialspace low_memory  Specifying these options with engine='pyarrow' will raise a ValueError.   Reading/writing remote files You can pass in a URL to read or write remote files to many of pandas\u2019 IO functions - the following example shows reading a CSV file: \ndf = pd.read_csv(\"https://download.bls.gov/pub/time.series/cu/cu.item\", sep=\"\\t\")\n   New in version 1.3.0.  A custom header can be sent alongside HTTP(s) requests by passing a dictionary of header key value mappings to the storage_options keyword argument as shown below: \nheaders = {\"User-Agent\": \"pandas\"}\ndf = pd.read_csv(\n    \"https://download.bls.gov/pub/time.series/cu/cu.item\",\n    sep=\"\\t\",\n    storage_options=headers\n)\n  All URLs which are not local files or HTTP(s) are handled by fsspec, if installed, and its various filesystem implementations (including Amazon S3, Google Cloud, SSH, FTP, webHDFS\u2026). Some of these implementations will require additional packages to be installed, for example S3 URLs require the s3fs library: \ndf = pd.read_json(\"s3://pandas-test/adatafile.json\")\n  When dealing with remote storage systems, you might need extra configuration with environment variables or config files in special locations. For example, to access data in your S3 bucket, you will need to define credentials in one of the several ways listed in the S3Fs documentation. The same is true for several of the storage backends, and you should follow the links at fsimpl1 for implementations built into fsspec and fsimpl2 for those not included in the main fsspec distribution. You can also pass parameters directly to the backend driver. For example, if you do not have S3 credentials, you can still access public data by specifying an anonymous connection, such as  New in version 1.2.0.  \npd.read_csv(\n    \"s3://ncei-wcsd-archive/data/processed/SH1305/18kHz/SaKe2013\"\n    \"-D20130523-T080854_to_SaKe2013-D20130523-T085643.csv\",\n    storage_options={\"anon\": True},\n)\n  fsspec also allows complex URLs, for accessing data in compressed archives, local caching of files, and more. To locally cache the above example, you would modify the call to \npd.read_csv(\n    \"simplecache::s3://ncei-wcsd-archive/data/processed/SH1305/18kHz/\"\n    \"SaKe2013-D20130523-T080854_to_SaKe2013-D20130523-T085643.csv\",\n    storage_options={\"s3\": {\"anon\": True}},\n)\n  where we specify that the \u201canon\u201d parameter is meant for the \u201cs3\u201d part of the implementation, not to the caching implementation. Note that this caches to a temporary directory for the duration of the session only, but you can also specify a permanent store.   Writing out data  Writing to CSV format The Series and DataFrame objects have an instance method to_csv which allows storing the contents of the object as a comma-separated-values file. The function takes a number of arguments. Only the first is required.  path_or_buf: A string path to the file to write or a file object. If a file object it must be opened with newline='' sep : Field delimiter for the output file (default \u201c,\u201d) na_rep: A string representation of a missing value (default \u2018\u2019) float_format: Format string for floating point numbers columns: Columns to write (default None) header: Whether to write out the column names (default True) index: whether to write row (index) names (default True) index_label: Column label(s) for index column(s) if desired. If None (default), and header and index are True, then the index names are used. (A sequence should be given if the DataFrame uses MultiIndex). mode : Python write mode, default \u2018w\u2019 encoding: a string representing the encoding to use if the contents are non-ASCII, for Python versions prior to 3 line_terminator: Character sequence denoting line end (default os.linesep) quoting: Set quoting rules as in csv module (default csv.QUOTE_MINIMAL). Note that if you have set a float_format then floats are converted to strings and csv.QUOTE_NONNUMERIC will treat them as non-numeric quotechar: Character used to quote fields (default \u2018\u201d\u2019) doublequote: Control quoting of quotechar in fields (default True) escapechar: Character used to escape sep and quotechar when appropriate (default None) chunksize: Number of rows to write at a time date_format: Format string for datetime objects    Writing a formatted string The DataFrame object has an instance method to_string which allows control over the string representation of the object. All arguments are optional:  buf default None, for example a StringIO object columns default None, which columns to write col_space default None, minimum width of each column. na_rep default NaN, representation of NA value formatters default None, a dictionary (by column) of functions each of which takes a single argument and returns a formatted string float_format default None, a function which takes a single (float) argument and returns a formatted string; to be applied to floats in the DataFrame. sparsify default True, set to False for a DataFrame with a hierarchical index to print every MultiIndex key at each row. index_names default True, will print the names of the indices index default True, will print the index (ie, row labels) header default True, will print the column labels justify default left, will print column headers left- or right-justified  The Series object also has a to_string method, but with only the buf, na_rep, float_format arguments. There is also a length argument which, if set to True, will additionally output the length of the Series.     JSON Read and write JSON format files and strings.  Writing JSON A Series or DataFrame can be converted to a valid JSON string. Use to_json with optional parameters:  path_or_buf : the pathname or buffer to write the output This can be None in which case a JSON string is returned \norient :  \nSeries:\n\n default is index allowed values are {split, records, index}   \nDataFrame:\n\n default is columns allowed values are {split, records, index, columns, values, table}    The format of the JSON string       \nsplit dict like {index -> [index], columns -> [columns], data -> [values]}  \nrecords list like [{column -> value}, \u2026 , {column -> value}]  \nindex dict like {index -> {column -> value}}  \ncolumns dict like {column -> {index -> value}}  \nvalues just the values array  \ntable adhering to the JSON Table Schema     date_format : string, type of date conversion, \u2018epoch\u2019 for timestamp, \u2018iso\u2019 for ISO8601. double_precision : The number of decimal places to use when encoding floating point values, default 10. force_ascii : force encoded string to be ASCII, default True. date_unit : The time unit to encode to, governs timestamp and ISO8601 precision. One of \u2018s\u2019, \u2018ms\u2019, \u2018us\u2019 or \u2018ns\u2019 for seconds, milliseconds, microseconds and nanoseconds respectively. Default \u2018ms\u2019. default_handler : The handler to call if an object cannot otherwise be converted to a suitable format for JSON. Takes a single argument, which is the object to convert, and returns a serializable object. lines : If records orient, then will write each record per line as json.  Note NaN\u2019s, NaT\u2019s and None will be converted to null and datetime objects will be converted based on the date_format and date_unit parameters. \nIn [200]: dfj = pd.DataFrame(np.random.randn(5, 2), columns=list(\"AB\"))\n\nIn [201]: json = dfj.to_json()\n\nIn [202]: json\nOut[202]: '{\"A\":{\"0\":-1.2945235903,\"1\":0.2766617129,\"2\":-0.0139597524,\"3\":-0.0061535699,\"4\":0.8957173022},\"B\":{\"0\":0.4137381054,\"1\":-0.472034511,\"2\":-0.3625429925,\"3\":-0.923060654,\"4\":0.8052440254}}'\n   Orient options There are a number of different options for the format of the resulting JSON file / string. Consider the following DataFrame and Series: \nIn [203]: dfjo = pd.DataFrame(\n   .....:     dict(A=range(1, 4), B=range(4, 7), C=range(7, 10)),\n   .....:     columns=list(\"ABC\"),\n   .....:     index=list(\"xyz\"),\n   .....: )\n   .....: \n\nIn [204]: dfjo\nOut[204]: \n   A  B  C\nx  1  4  7\ny  2  5  8\nz  3  6  9\n\nIn [205]: sjo = pd.Series(dict(x=15, y=16, z=17), name=\"D\")\n\nIn [206]: sjo\nOut[206]: \nx    15\ny    16\nz    17\nName: D, dtype: int64\n  Column oriented (the default for DataFrame) serializes the data as nested JSON objects with column labels acting as the primary index: \nIn [207]: dfjo.to_json(orient=\"columns\")\nOut[207]: '{\"A\":{\"x\":1,\"y\":2,\"z\":3},\"B\":{\"x\":4,\"y\":5,\"z\":6},\"C\":{\"x\":7,\"y\":8,\"z\":9}}'\n\n# Not available for Series\n  Index oriented (the default for Series) similar to column oriented but the index labels are now primary: \nIn [208]: dfjo.to_json(orient=\"index\")\nOut[208]: '{\"x\":{\"A\":1,\"B\":4,\"C\":7},\"y\":{\"A\":2,\"B\":5,\"C\":8},\"z\":{\"A\":3,\"B\":6,\"C\":9}}'\n\nIn [209]: sjo.to_json(orient=\"index\")\nOut[209]: '{\"x\":15,\"y\":16,\"z\":17}'\n  Record oriented serializes the data to a JSON array of column -> value records, index labels are not included. This is useful for passing DataFrame data to plotting libraries, for example the JavaScript library d3.js: \nIn [210]: dfjo.to_json(orient=\"records\")\nOut[210]: '[{\"A\":1,\"B\":4,\"C\":7},{\"A\":2,\"B\":5,\"C\":8},{\"A\":3,\"B\":6,\"C\":9}]'\n\nIn [211]: sjo.to_json(orient=\"records\")\nOut[211]: '[15,16,17]'\n  Value oriented is a bare-bones option which serializes to nested JSON arrays of values only, column and index labels are not included: \nIn [212]: dfjo.to_json(orient=\"values\")\nOut[212]: '[[1,4,7],[2,5,8],[3,6,9]]'\n\n# Not available for Series\n  Split oriented serializes to a JSON object containing separate entries for values, index and columns. Name is also included for Series: \nIn [213]: dfjo.to_json(orient=\"split\")\nOut[213]: '{\"columns\":[\"A\",\"B\",\"C\"],\"index\":[\"x\",\"y\",\"z\"],\"data\":[[1,4,7],[2,5,8],[3,6,9]]}'\n\nIn [214]: sjo.to_json(orient=\"split\")\nOut[214]: '{\"name\":\"D\",\"index\":[\"x\",\"y\",\"z\"],\"data\":[15,16,17]}'\n  Table oriented serializes to the JSON Table Schema, allowing for the preservation of metadata including but not limited to dtypes and index names.  Note Any orient option that encodes to a JSON object will not preserve the ordering of index and column labels during round-trip serialization. If you wish to preserve label ordering use the split option as it uses ordered containers.    Date handling Writing in ISO date format: \nIn [215]: dfd = pd.DataFrame(np.random.randn(5, 2), columns=list(\"AB\"))\n\nIn [216]: dfd[\"date\"] = pd.Timestamp(\"20130101\")\n\nIn [217]: dfd = dfd.sort_index(axis=1, ascending=False)\n\nIn [218]: json = dfd.to_json(date_format=\"iso\")\n\nIn [219]: json\nOut[219]: '{\"date\":{\"0\":\"2013-01-01T00:00:00.000Z\",\"1\":\"2013-01-01T00:00:00.000Z\",\"2\":\"2013-01-01T00:00:00.000Z\",\"3\":\"2013-01-01T00:00:00.000Z\",\"4\":\"2013-01-01T00:00:00.000Z\"},\"B\":{\"0\":2.5656459463,\"1\":1.3403088498,\"2\":-0.2261692849,\"3\":0.8138502857,\"4\":-0.8273169356},\"A\":{\"0\":-1.2064117817,\"1\":1.4312559863,\"2\":-1.1702987971,\"3\":0.4108345112,\"4\":0.1320031703}}'\n  Writing in ISO date format, with microseconds: \nIn [220]: json = dfd.to_json(date_format=\"iso\", date_unit=\"us\")\n\nIn [221]: json\nOut[221]: '{\"date\":{\"0\":\"2013-01-01T00:00:00.000000Z\",\"1\":\"2013-01-01T00:00:00.000000Z\",\"2\":\"2013-01-01T00:00:00.000000Z\",\"3\":\"2013-01-01T00:00:00.000000Z\",\"4\":\"2013-01-01T00:00:00.000000Z\"},\"B\":{\"0\":2.5656459463,\"1\":1.3403088498,\"2\":-0.2261692849,\"3\":0.8138502857,\"4\":-0.8273169356},\"A\":{\"0\":-1.2064117817,\"1\":1.4312559863,\"2\":-1.1702987971,\"3\":0.4108345112,\"4\":0.1320031703}}'\n  Epoch timestamps, in seconds: \nIn [222]: json = dfd.to_json(date_format=\"epoch\", date_unit=\"s\")\n\nIn [223]: json\nOut[223]: '{\"date\":{\"0\":1356998400,\"1\":1356998400,\"2\":1356998400,\"3\":1356998400,\"4\":1356998400},\"B\":{\"0\":2.5656459463,\"1\":1.3403088498,\"2\":-0.2261692849,\"3\":0.8138502857,\"4\":-0.8273169356},\"A\":{\"0\":-1.2064117817,\"1\":1.4312559863,\"2\":-1.1702987971,\"3\":0.4108345112,\"4\":0.1320031703}}'\n  Writing to a file, with a date index and a date column: \nIn [224]: dfj2 = dfj.copy()\n\nIn [225]: dfj2[\"date\"] = pd.Timestamp(\"20130101\")\n\nIn [226]: dfj2[\"ints\"] = list(range(5))\n\nIn [227]: dfj2[\"bools\"] = True\n\nIn [228]: dfj2.index = pd.date_range(\"20130101\", periods=5)\n\nIn [229]: dfj2.to_json(\"test.json\")\n\nIn [230]: with open(\"test.json\") as fh:\n   .....:     print(fh.read())\n   .....: \n{\"A\":{\"1356998400000\":-1.2945235903,\"1357084800000\":0.2766617129,\"1357171200000\":-0.0139597524,\"1357257600000\":-0.0061535699,\"1357344000000\":0.8957173022},\"B\":{\"1356998400000\":0.4137381054,\"1357084800000\":-0.472034511,\"1357171200000\":-0.3625429925,\"1357257600000\":-0.923060654,\"1357344000000\":0.8052440254},\"date\":{\"1356998400000\":1356998400000,\"1357084800000\":1356998400000,\"1357171200000\":1356998400000,\"1357257600000\":1356998400000,\"1357344000000\":1356998400000},\"ints\":{\"1356998400000\":0,\"1357084800000\":1,\"1357171200000\":2,\"1357257600000\":3,\"1357344000000\":4},\"bools\":{\"1356998400000\":true,\"1357084800000\":true,\"1357171200000\":true,\"1357257600000\":true,\"1357344000000\":true}}\n    Fallback behavior If the JSON serializer cannot handle the container contents directly it will fall back in the following manner:  if the dtype is unsupported (e.g. np.complex_) then the default_handler, if provided, will be called for each value, otherwise an exception is raised. \nif an object is unsupported it will attempt the following:  \n check if the object has defined a toDict method and call it. A toDict method should return a dict which will then be JSON serialized. invoke the default_handler if one was provided. convert the object to a dict by traversing its contents. However this will often fail with an OverflowError or give unexpected results.  \n   In general the best approach for unsupported objects or dtypes is to provide a default_handler. For example: \n>>> DataFrame([1.0, 2.0, complex(1.0, 2.0)]).to_json()  # raises\nRuntimeError: Unhandled numpy dtype 15\n  can be dealt with by specifying a simple default_handler: \nIn [231]: pd.DataFrame([1.0, 2.0, complex(1.0, 2.0)]).to_json(default_handler=str)\nOut[231]: '{\"0\":{\"0\":\"(1+0j)\",\"1\":\"(2+0j)\",\"2\":\"(1+2j)\"}}'\n     Reading JSON Reading a JSON string to pandas object can take a number of parameters. The parser will try to parse a DataFrame if typ is not supplied or is None. To explicitly force Series parsing, pass typ=series  filepath_or_buffer : a VALID JSON string or file handle / StringIO. The string could be a URL. Valid URL schemes include http, ftp, S3, and file. For file URLs, a host is expected. For instance, a local file could be file ://localhost/path/to/table.json typ : type of object to recover (series or frame), default \u2018frame\u2019 \norient :  Series :\n\n default is index allowed values are {split, records, index}   DataFrame\n\n default is columns allowed values are {split, records, index, columns, values, table}    The format of the JSON string       \nsplit dict like {index -> [index], columns -> [columns], data -> [values]}  \nrecords list like [{column -> value}, \u2026 , {column -> value}]  \nindex dict like {index -> {column -> value}}  \ncolumns dict like {column -> {index -> value}}  \nvalues just the values array  \ntable adhering to the JSON Table Schema     dtype : if True, infer dtypes, if a dict of column to dtype, then use those, if False, then don\u2019t infer dtypes at all, default is True, apply only to the data. convert_axes : boolean, try to convert the axes to the proper dtypes, default is True convert_dates : a list of columns to parse for dates; If True, then try to parse date-like columns, default is True. keep_default_dates : boolean, default True. If parsing dates, then parse the default date-like columns. numpy : direct decoding to NumPy arrays. default is False; Supports numeric data only, although labels may be non-numeric. Also note that the JSON ordering MUST be the same for each term if numpy=True. precise_float : boolean, default False. Set to enable usage of higher precision (strtod) function when decoding string to double values. Default (False) is to use fast but less precise builtin functionality. date_unit : string, the timestamp unit to detect if converting dates. Default None. By default the timestamp precision will be detected, if this is not desired then pass one of \u2018s\u2019, \u2018ms\u2019, \u2018us\u2019 or \u2018ns\u2019 to force timestamp precision to seconds, milliseconds, microseconds or nanoseconds respectively. lines : reads file as one json object per line. encoding : The encoding to use to decode py3 bytes. chunksize : when used in combination with lines=True, return a JsonReader which reads in chunksize lines per iteration.  The parser will raise one of ValueError/TypeError/AssertionError if the JSON is not parseable. If a non-default orient was used when encoding to JSON be sure to pass the same option here so that decoding produces sensible results, see Orient Options for an overview.  Data conversion The default of convert_axes=True, dtype=True, and convert_dates=True will try to parse the axes, and all of the data into appropriate types, including dates. If you need to override specific dtypes, pass a dict to dtype. convert_axes should only be set to False if you need to preserve string-like numbers (e.g. \u20181\u2019, \u20182\u2019) in an axes.  Note Large integer values may be converted to dates if convert_dates=True and the data and / or column labels appear \u2018date-like\u2019. The exact threshold depends on the date_unit specified. \u2018date-like\u2019 means that the column label meets one of the following criteria:  \n it ends with '_at' it ends with '_time' it begins with 'timestamp' it is 'modified' it is 'date'  \n   Warning When reading JSON data, automatic coercing into dtypes has some quirks:  \n an index can be reconstructed in a different order from serialization, that is, the returned order is not guaranteed to be the same as before serialization a column that was float data will be converted to integer if it can be done safely, e.g. a column of 1. bool columns will be converted to integer on reconstruction  \n Thus there are times where you may want to specify specific dtypes via the dtype keyword argument.  Reading from a JSON string: \nIn [232]: pd.read_json(json)\nOut[232]: \n        date         B         A\n0 2013-01-01  2.565646 -1.206412\n1 2013-01-01  1.340309  1.431256\n2 2013-01-01 -0.226169 -1.170299\n3 2013-01-01  0.813850  0.410835\n4 2013-01-01 -0.827317  0.132003\n  Reading from a file: \nIn [233]: pd.read_json(\"test.json\")\nOut[233]: \n                   A         B       date  ints  bools\n2013-01-01 -1.294524  0.413738 2013-01-01     0   True\n2013-01-02  0.276662 -0.472035 2013-01-01     1   True\n2013-01-03 -0.013960 -0.362543 2013-01-01     2   True\n2013-01-04 -0.006154 -0.923061 2013-01-01     3   True\n2013-01-05  0.895717  0.805244 2013-01-01     4   True\n  Don\u2019t convert any data (but still convert axes and dates): \nIn [234]: pd.read_json(\"test.json\", dtype=object).dtypes\nOut[234]: \nA        object\nB        object\ndate     object\nints     object\nbools    object\ndtype: object\n  Specify dtypes for conversion: \nIn [235]: pd.read_json(\"test.json\", dtype={\"A\": \"float32\", \"bools\": \"int8\"}).dtypes\nOut[235]: \nA               float32\nB               float64\ndate     datetime64[ns]\nints              int64\nbools              int8\ndtype: object\n  Preserve string indices: \nIn [236]: si = pd.DataFrame(\n   .....:     np.zeros((4, 4)), columns=list(range(4)), index=[str(i) for i in range(4)]\n   .....: )\n   .....: \n\nIn [237]: si\nOut[237]: \n     0    1    2    3\n0  0.0  0.0  0.0  0.0\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  0.0  0.0  0.0  0.0\n\nIn [238]: si.index\nOut[238]: Index(['0', '1', '2', '3'], dtype='object')\n\nIn [239]: si.columns\nOut[239]: Int64Index([0, 1, 2, 3], dtype='int64')\n\nIn [240]: json = si.to_json()\n\nIn [241]: sij = pd.read_json(json, convert_axes=False)\n\nIn [242]: sij\nOut[242]: \n   0  1  2  3\n0  0  0  0  0\n1  0  0  0  0\n2  0  0  0  0\n3  0  0  0  0\n\nIn [243]: sij.index\nOut[243]: Index(['0', '1', '2', '3'], dtype='object')\n\nIn [244]: sij.columns\nOut[244]: Index(['0', '1', '2', '3'], dtype='object')\n  Dates written in nanoseconds need to be read back in nanoseconds: \nIn [245]: json = dfj2.to_json(date_unit=\"ns\")\n\n# Try to parse timestamps as milliseconds -> Won't Work\nIn [246]: dfju = pd.read_json(json, date_unit=\"ms\")\n\nIn [247]: dfju\nOut[247]: \n                            A         B                 date  ints  bools\n1356998400000000000 -1.294524  0.413738  1356998400000000000     0   True\n1357084800000000000  0.276662 -0.472035  1356998400000000000     1   True\n1357171200000000000 -0.013960 -0.362543  1356998400000000000     2   True\n1357257600000000000 -0.006154 -0.923061  1356998400000000000     3   True\n1357344000000000000  0.895717  0.805244  1356998400000000000     4   True\n\n# Let pandas detect the correct precision\nIn [248]: dfju = pd.read_json(json)\n\nIn [249]: dfju\nOut[249]: \n                   A         B       date  ints  bools\n2013-01-01 -1.294524  0.413738 2013-01-01     0   True\n2013-01-02  0.276662 -0.472035 2013-01-01     1   True\n2013-01-03 -0.013960 -0.362543 2013-01-01     2   True\n2013-01-04 -0.006154 -0.923061 2013-01-01     3   True\n2013-01-05  0.895717  0.805244 2013-01-01     4   True\n\n# Or specify that all timestamps are in nanoseconds\nIn [250]: dfju = pd.read_json(json, date_unit=\"ns\")\n\nIn [251]: dfju\nOut[251]: \n                   A         B       date  ints  bools\n2013-01-01 -1.294524  0.413738 2013-01-01     0   True\n2013-01-02  0.276662 -0.472035 2013-01-01     1   True\n2013-01-03 -0.013960 -0.362543 2013-01-01     2   True\n2013-01-04 -0.006154 -0.923061 2013-01-01     3   True\n2013-01-05  0.895717  0.805244 2013-01-01     4   True\n    The Numpy parameter  Note This param has been deprecated as of version 1.0.0 and will raise a FutureWarning. This supports numeric data only. Index and columns labels may be non-numeric, e.g. strings, dates etc.  If numpy=True is passed to read_json an attempt will be made to sniff an appropriate dtype during deserialization and to subsequently decode directly to NumPy arrays, bypassing the need for intermediate Python objects. This can provide speedups if you are deserialising a large amount of numeric data: \nIn [252]: randfloats = np.random.uniform(-100, 1000, 10000)\n\nIn [253]: randfloats.shape = (1000, 10)\n\nIn [254]: dffloats = pd.DataFrame(randfloats, columns=list(\"ABCDEFGHIJ\"))\n\nIn [255]: jsonfloats = dffloats.to_json()\n  \nIn [256]: %timeit pd.read_json(jsonfloats)\n7.95 ms +- 597 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n  \nIn [257]: %timeit pd.read_json(jsonfloats, numpy=True)\n5.89 ms +- 407 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n  The speedup is less noticeable for smaller datasets: \nIn [258]: jsonfloats = dffloats.head(100).to_json()\n  \nIn [259]: %timeit pd.read_json(jsonfloats)\n5.39 ms +- 316 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n  \nIn [260]: %timeit pd.read_json(jsonfloats, numpy=True)\n4.76 ms +- 436 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n   Warning Direct NumPy decoding makes a number of assumptions and may fail or produce unexpected output if these assumptions are not satisfied:  \n data is numeric. data is uniform. The dtype is sniffed from the first value decoded. A ValueError may be raised, or incorrect output may be produced if this condition is not satisfied. labels are ordered. Labels are only read from the first container, it is assumed that each subsequent row / column has been encoded in the same order. This should be satisfied if the data was encoded using to_json but may not be the case if the JSON is from another source.  \n     Normalization pandas provides a utility function to take a dict or list of dicts and normalize this semi-structured data into a flat table. \nIn [261]: data = [\n   .....:     {\"id\": 1, \"name\": {\"first\": \"Coleen\", \"last\": \"Volk\"}},\n   .....:     {\"name\": {\"given\": \"Mark\", \"family\": \"Regner\"}},\n   .....:     {\"id\": 2, \"name\": \"Faye Raker\"},\n   .....: ]\n   .....: \n\nIn [262]: pd.json_normalize(data)\nOut[262]: \n    id name.first name.last name.given name.family        name\n0  1.0     Coleen      Volk        NaN         NaN         NaN\n1  NaN        NaN       NaN       Mark      Regner         NaN\n2  2.0        NaN       NaN        NaN         NaN  Faye Raker\n  \nIn [263]: data = [\n   .....:     {\n   .....:         \"state\": \"Florida\",\n   .....:         \"shortname\": \"FL\",\n   .....:         \"info\": {\"governor\": \"Rick Scott\"},\n   .....:         \"county\": [\n   .....:             {\"name\": \"Dade\", \"population\": 12345},\n   .....:             {\"name\": \"Broward\", \"population\": 40000},\n   .....:             {\"name\": \"Palm Beach\", \"population\": 60000},\n   .....:         ],\n   .....:     },\n   .....:     {\n   .....:         \"state\": \"Ohio\",\n   .....:         \"shortname\": \"OH\",\n   .....:         \"info\": {\"governor\": \"John Kasich\"},\n   .....:         \"county\": [\n   .....:             {\"name\": \"Summit\", \"population\": 1234},\n   .....:             {\"name\": \"Cuyahoga\", \"population\": 1337},\n   .....:         ],\n   .....:     },\n   .....: ]\n   .....: \n\nIn [264]: pd.json_normalize(data, \"county\", [\"state\", \"shortname\", [\"info\", \"governor\"]])\nOut[264]: \n         name  population    state shortname info.governor\n0        Dade       12345  Florida        FL    Rick Scott\n1     Broward       40000  Florida        FL    Rick Scott\n2  Palm Beach       60000  Florida        FL    Rick Scott\n3      Summit        1234     Ohio        OH   John Kasich\n4    Cuyahoga        1337     Ohio        OH   John Kasich\n  The max_level parameter provides more control over which level to end normalization. With max_level=1 the following snippet normalizes until 1st nesting level of the provided dict. \nIn [265]: data = [\n   .....:     {\n   .....:         \"CreatedBy\": {\"Name\": \"User001\"},\n   .....:         \"Lookup\": {\n   .....:             \"TextField\": \"Some text\",\n   .....:             \"UserField\": {\"Id\": \"ID001\", \"Name\": \"Name001\"},\n   .....:         },\n   .....:         \"Image\": {\"a\": \"b\"},\n   .....:     }\n   .....: ]\n   .....: \n\nIn [266]: pd.json_normalize(data, max_level=1)\nOut[266]: \n  CreatedBy.Name Lookup.TextField                    Lookup.UserField Image.a\n0        User001        Some text  {'Id': 'ID001', 'Name': 'Name001'}       b\n    Line delimited json pandas is able to read and write line-delimited json files that are common in data processing pipelines using Hadoop or Spark. For line-delimited json files, pandas can also return an iterator which reads in chunksize lines at a time. This can be useful for large files or to read from a stream. \nIn [267]: jsonl = \"\"\"\n   .....:     {\"a\": 1, \"b\": 2}\n   .....:     {\"a\": 3, \"b\": 4}\n   .....: \"\"\"\n   .....: \n\nIn [268]: df = pd.read_json(jsonl, lines=True)\n\nIn [269]: df\nOut[269]: \n   a  b\n0  1  2\n1  3  4\n\nIn [270]: df.to_json(orient=\"records\", lines=True)\nOut[270]: '{\"a\":1,\"b\":2}\\n{\"a\":3,\"b\":4}\\n'\n\n# reader is an iterator that returns ``chunksize`` lines each iteration\nIn [271]: with pd.read_json(StringIO(jsonl), lines=True, chunksize=1) as reader:\n   .....:     reader\n   .....:     for chunk in reader:\n   .....:         print(chunk)\n   .....: \nEmpty DataFrame\nColumns: []\nIndex: []\n   a  b\n0  1  2\n   a  b\n1  3  4\n    Table schema Table Schema is a spec for describing tabular datasets as a JSON object. The JSON includes information on the field names, types, and other attributes. You can use the orient table to build a JSON string with two fields, schema and data. \nIn [272]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"A\": [1, 2, 3],\n   .....:         \"B\": [\"a\", \"b\", \"c\"],\n   .....:         \"C\": pd.date_range(\"2016-01-01\", freq=\"d\", periods=3),\n   .....:     },\n   .....:     index=pd.Index(range(3), name=\"idx\"),\n   .....: )\n   .....: \n\nIn [273]: df\nOut[273]: \n     A  B          C\nidx                 \n0    1  a 2016-01-01\n1    2  b 2016-01-02\n2    3  c 2016-01-03\n\nIn [274]: df.to_json(orient=\"table\", date_format=\"iso\")\nOut[274]: '{\"schema\":{\"fields\":[{\"name\":\"idx\",\"type\":\"integer\"},{\"name\":\"A\",\"type\":\"integer\"},{\"name\":\"B\",\"type\":\"string\"},{\"name\":\"C\",\"type\":\"datetime\"}],\"primaryKey\":[\"idx\"],\"pandas_version\":\"1.4.0\"},\"data\":[{\"idx\":0,\"A\":1,\"B\":\"a\",\"C\":\"2016-01-01T00:00:00.000Z\"},{\"idx\":1,\"A\":2,\"B\":\"b\",\"C\":\"2016-01-02T00:00:00.000Z\"},{\"idx\":2,\"A\":3,\"B\":\"c\",\"C\":\"2016-01-03T00:00:00.000Z\"}]}'\n  The schema field contains the fields key, which itself contains a list of column name to type pairs, including the Index or MultiIndex (see below for a list of types). The schema field also contains a primaryKey field if the (Multi)index is unique. The second field, data, contains the serialized data with the records orient. The index is included, and any datetimes are ISO 8601 formatted, as required by the Table Schema spec. The full list of types supported are described in the Table Schema spec. This table shows the mapping from pandas types:       \npandas type Table Schema type    \nint64 integer  \nfloat64 number  \nbool boolean  \ndatetime64[ns] datetime  \ntimedelta64[ns] duration  \ncategorical any  \nobject str    A few notes on the generated table schema:  The schema object contains a pandas_version field. This contains the version of pandas\u2019 dialect of the schema, and will be incremented with each revision. \nAll dates are converted to UTC when serializing. Even timezone naive values, which are treated as UTC with an offset of 0. \nIn [275]: from pandas.io.json import build_table_schema\n\nIn [276]: s = pd.Series(pd.date_range(\"2016\", periods=4))\n\nIn [277]: build_table_schema(s)\nOut[277]: \n{'fields': [{'name': 'index', 'type': 'integer'},\n  {'name': 'values', 'type': 'datetime'}],\n 'primaryKey': ['index'],\n 'pandas_version': '1.4.0'}\n   \ndatetimes with a timezone (before serializing), include an additional field tz with the time zone name (e.g. 'US/Central'). \nIn [278]: s_tz = pd.Series(pd.date_range(\"2016\", periods=12, tz=\"US/Central\"))\n\nIn [279]: build_table_schema(s_tz)\nOut[279]: \n{'fields': [{'name': 'index', 'type': 'integer'},\n  {'name': 'values', 'type': 'datetime', 'tz': 'US/Central'}],\n 'primaryKey': ['index'],\n 'pandas_version': '1.4.0'}\n   \nPeriods are converted to timestamps before serialization, and so have the same behavior of being converted to UTC. In addition, periods will contain and additional field freq with the period\u2019s frequency, e.g. 'A-DEC'. \nIn [280]: s_per = pd.Series(1, index=pd.period_range(\"2016\", freq=\"A-DEC\", periods=4))\n\nIn [281]: build_table_schema(s_per)\nOut[281]: \n{'fields': [{'name': 'index', 'type': 'datetime', 'freq': 'A-DEC'},\n  {'name': 'values', 'type': 'integer'}],\n 'primaryKey': ['index'],\n 'pandas_version': '1.4.0'}\n   \nCategoricals use the any type and an enum constraint listing the set of possible values. Additionally, an ordered field is included: \nIn [282]: s_cat = pd.Series(pd.Categorical([\"a\", \"b\", \"a\"]))\n\nIn [283]: build_table_schema(s_cat)\nOut[283]: \n{'fields': [{'name': 'index', 'type': 'integer'},\n  {'name': 'values',\n   'type': 'any',\n   'constraints': {'enum': ['a', 'b']},\n   'ordered': False}],\n 'primaryKey': ['index'],\n 'pandas_version': '1.4.0'}\n   \nA primaryKey field, containing an array of labels, is included if the index is unique: \nIn [284]: s_dupe = pd.Series([1, 2], index=[1, 1])\n\nIn [285]: build_table_schema(s_dupe)\nOut[285]: \n{'fields': [{'name': 'index', 'type': 'integer'},\n  {'name': 'values', 'type': 'integer'}],\n 'pandas_version': '1.4.0'}\n   \nThe primaryKey behavior is the same with MultiIndexes, but in this case the primaryKey is an array: \nIn [286]: s_multi = pd.Series(1, index=pd.MultiIndex.from_product([(\"a\", \"b\"), (0, 1)]))\n\nIn [287]: build_table_schema(s_multi)\nOut[287]: \n{'fields': [{'name': 'level_0', 'type': 'string'},\n  {'name': 'level_1', 'type': 'integer'},\n  {'name': 'values', 'type': 'integer'}],\n 'primaryKey': FrozenList(['level_0', 'level_1']),\n 'pandas_version': '1.4.0'}\n   \nThe default naming roughly follows these rules:  \n For series, the object.name is used. If that\u2019s none, then the name is values For DataFrames, the stringified version of the column name is used For Index (not MultiIndex), index.name is used, with a fallback to index if that is None. For MultiIndex, mi.names is used. If any level has no name, then level_<i> is used.  \n   read_json also accepts orient='table' as an argument. This allows for the preservation of metadata such as dtypes and index names in a round-trippable manner.  \n\nIn [288]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"foo\": [1, 2, 3, 4],\n   .....:         \"bar\": [\"a\", \"b\", \"c\", \"d\"],\n   .....:         \"baz\": pd.date_range(\"2018-01-01\", freq=\"d\", periods=4),\n   .....:         \"qux\": pd.Categorical([\"a\", \"b\", \"c\", \"c\"]),\n   .....:     },\n   .....:     index=pd.Index(range(4), name=\"idx\"),\n   .....: )\n   .....: \n\nIn [289]: df\nOut[289]: \n     foo bar        baz qux\nidx                        \n0      1   a 2018-01-01   a\n1      2   b 2018-01-02   b\n2      3   c 2018-01-03   c\n3      4   d 2018-01-04   c\n\nIn [290]: df.dtypes\nOut[290]: \nfoo             int64\nbar            object\nbaz    datetime64[ns]\nqux          category\ndtype: object\n\nIn [291]: df.to_json(\"test.json\", orient=\"table\")\n\nIn [292]: new_df = pd.read_json(\"test.json\", orient=\"table\")\n\nIn [293]: new_df\nOut[293]: \n     foo bar        baz qux\nidx                        \n0      1   a 2018-01-01   a\n1      2   b 2018-01-02   b\n2      3   c 2018-01-03   c\n3      4   d 2018-01-04   c\n\nIn [294]: new_df.dtypes\nOut[294]: \nfoo             int64\nbar            object\nbaz    datetime64[ns]\nqux          category\ndtype: object\n  \n Please note that the literal string \u2018index\u2019 as the name of an Index is not round-trippable, nor are any names beginning with 'level_' within a MultiIndex. These are used by default in DataFrame.to_json() to indicate missing values and the subsequent read cannot distinguish the intent. \nIn [295]: df.index.name = \"index\"\n\nIn [296]: df.to_json(\"test.json\", orient=\"table\")\n\nIn [297]: new_df = pd.read_json(\"test.json\", orient=\"table\")\n\nIn [298]: print(new_df.index.name)\nNone\n  When using orient='table' along with user-defined ExtensionArray, the generated schema will contain an additional extDtype key in the respective fields element. This extra key is not standard but does enable JSON roundtrips for extension types (e.g. read_json(df.to_json(orient=\"table\"), orient=\"table\")). The extDtype key carries the name of the extension, if you have properly registered the ExtensionDtype, pandas will use said name to perform a lookup into the registry and re-convert the serialized data into your custom dtype.    HTML  Reading HTML content  Warning We highly encourage you to read the HTML Table Parsing gotchas below regarding the issues surrounding the BeautifulSoup4/html5lib/lxml parsers.  The top-level read_html() function can accept an HTML string/file/URL and will parse HTML tables into list of pandas DataFrames. Let\u2019s look at a few examples.  Note read_html returns a list of DataFrame objects, even if there is only a single table contained in the HTML content.  Read a URL with no options: \nIn [299]: url = \"https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list\"\n\nIn [300]: dfs = pd.read_html(url)\n\nIn [301]: dfs\nOut[301]: \n[                         Bank NameBank           CityCity StateSt  CertCert              Acquiring InstitutionAI Closing DateClosing  FundFund\n 0                    Almena State Bank             Almena      KS     15426                          Equity Bank    October 23, 2020     10538\n 1           First City Bank of Florida  Fort Walton Beach      FL     16748            United Fidelity Bank, fsb    October 16, 2020     10537\n 2                 The First State Bank      Barboursville      WV     14361                       MVB Bank, Inc.       April 3, 2020     10536\n 3                   Ericson State Bank            Ericson      NE     18265           Farmers and Merchants Bank   February 14, 2020     10535\n 4     City National Bank of New Jersey             Newark      NJ     21111                      Industrial Bank    November 1, 2019     10534\n ..                                 ...                ...     ...       ...                                  ...                 ...       ...\n 558                 Superior Bank, FSB           Hinsdale      IL     32646                Superior Federal, FSB       July 27, 2001      6004\n 559                Malta National Bank              Malta      OH      6629                    North Valley Bank         May 3, 2001      4648\n 560    First Alliance Bank & Trust Co.         Manchester      NH     34264  Southern New Hampshire Bank & Trust    February 2, 2001      4647\n 561  National State Bank of Metropolis         Metropolis      IL      3815              Banterra Bank of Marion   December 14, 2000      4646\n 562                   Bank of Honolulu           Honolulu      HI     21029                   Bank of the Orient    October 13, 2000      4645\n \n [563 rows x 7 columns]]\n   Note The data from the above URL changes every Monday so the resulting data above and the data below may be slightly different.  Read in the content of the file from the above URL and pass it to read_html as a string: \nIn [302]: with open(file_path, \"r\") as f:\n   .....:     dfs = pd.read_html(f.read())\n   .....: \n\nIn [303]: dfs\nOut[303]: \n[                                    Bank Name          City  ST  ...                Acquiring Institution       Closing Date       Updated Date\n 0    Banks of Wisconsin d/b/a Bank of Kenosha       Kenosha  WI  ...                North Shore Bank, FSB       May 31, 2013       May 31, 2013\n 1                        Central Arizona Bank    Scottsdale  AZ  ...                   Western State Bank       May 14, 2013       May 20, 2013\n 2                                Sunrise Bank      Valdosta  GA  ...                         Synovus Bank       May 10, 2013       May 21, 2013\n 3                       Pisgah Community Bank     Asheville  NC  ...                   Capital Bank, N.A.       May 10, 2013       May 14, 2013\n 4                         Douglas County Bank  Douglasville  GA  ...                  Hamilton State Bank     April 26, 2013       May 16, 2013\n ..                                        ...           ...  ..  ...                                  ...                ...                ...\n 501                        Superior Bank, FSB      Hinsdale  IL  ...                Superior Federal, FSB      July 27, 2001       June 5, 2012\n 502                       Malta National Bank         Malta  OH  ...                    North Valley Bank        May 3, 2001  November 18, 2002\n 503           First Alliance Bank & Trust Co.    Manchester  NH  ...  Southern New Hampshire Bank & Trust   February 2, 2001  February 18, 2003\n 504         National State Bank of Metropolis    Metropolis  IL  ...              Banterra Bank of Marion  December 14, 2000     March 17, 2005\n 505                          Bank of Honolulu      Honolulu  HI  ...                   Bank of the Orient   October 13, 2000     March 17, 2005\n \n [506 rows x 7 columns]]\n  You can even pass in an instance of StringIO if you so desire: \nIn [304]: with open(file_path, \"r\") as f:\n   .....:     sio = StringIO(f.read())\n   .....: \n\nIn [305]: dfs = pd.read_html(sio)\n\nIn [306]: dfs\nOut[306]: \n[                                    Bank Name          City  ST  ...                Acquiring Institution       Closing Date       Updated Date\n 0    Banks of Wisconsin d/b/a Bank of Kenosha       Kenosha  WI  ...                North Shore Bank, FSB       May 31, 2013       May 31, 2013\n 1                        Central Arizona Bank    Scottsdale  AZ  ...                   Western State Bank       May 14, 2013       May 20, 2013\n 2                                Sunrise Bank      Valdosta  GA  ...                         Synovus Bank       May 10, 2013       May 21, 2013\n 3                       Pisgah Community Bank     Asheville  NC  ...                   Capital Bank, N.A.       May 10, 2013       May 14, 2013\n 4                         Douglas County Bank  Douglasville  GA  ...                  Hamilton State Bank     April 26, 2013       May 16, 2013\n ..                                        ...           ...  ..  ...                                  ...                ...                ...\n 501                        Superior Bank, FSB      Hinsdale  IL  ...                Superior Federal, FSB      July 27, 2001       June 5, 2012\n 502                       Malta National Bank         Malta  OH  ...                    North Valley Bank        May 3, 2001  November 18, 2002\n 503           First Alliance Bank & Trust Co.    Manchester  NH  ...  Southern New Hampshire Bank & Trust   February 2, 2001  February 18, 2003\n 504         National State Bank of Metropolis    Metropolis  IL  ...              Banterra Bank of Marion  December 14, 2000     March 17, 2005\n 505                          Bank of Honolulu      Honolulu  HI  ...                   Bank of the Orient   October 13, 2000     March 17, 2005\n \n [506 rows x 7 columns]]\n   Note The following examples are not run by the IPython evaluator due to the fact that having so many network-accessing functions slows down the documentation build. If you spot an error or an example that doesn\u2019t run, please do not hesitate to report it over on pandas GitHub issues page.  Read a URL and match a table that contains specific text: \nmatch = \"Metcalf Bank\"\ndf_list = pd.read_html(url, match=match)\n  Specify a header row (by default <th> or <td> elements located within a <thead> are used to form the column index, if multiple rows are contained within <thead> then a MultiIndex is created); if specified, the header row is taken from the data minus the parsed header elements (<th> elements). \ndfs = pd.read_html(url, header=0)\n  Specify an index column: \ndfs = pd.read_html(url, index_col=0)\n  Specify a number of rows to skip: \ndfs = pd.read_html(url, skiprows=0)\n  Specify a number of rows to skip using a list (range works as well): \ndfs = pd.read_html(url, skiprows=range(2))\n  Specify an HTML attribute: \ndfs1 = pd.read_html(url, attrs={\"id\": \"table\"})\ndfs2 = pd.read_html(url, attrs={\"class\": \"sortable\"})\nprint(np.array_equal(dfs1[0], dfs2[0]))  # Should be True\n  Specify values that should be converted to NaN: \ndfs = pd.read_html(url, na_values=[\"No Acquirer\"])\n  Specify whether to keep the default set of NaN values: \ndfs = pd.read_html(url, keep_default_na=False)\n  Specify converters for columns. This is useful for numerical text data that has leading zeros. By default columns that are numerical are cast to numeric types and the leading zeros are lost. To avoid this, we can convert these columns to strings. \nurl_mcc = \"https://en.wikipedia.org/wiki/Mobile_country_code\"\ndfs = pd.read_html(\n    url_mcc,\n    match=\"Telekom Albania\",\n    header=0,\n    converters={\"MNC\": str},\n)\n  Use some combination of the above: \ndfs = pd.read_html(url, match=\"Metcalf Bank\", index_col=0)\n  Read in pandas to_html output (with some loss of floating point precision): \ndf = pd.DataFrame(np.random.randn(2, 2))\ns = df.to_html(float_format=\"{0:.40g}\".format)\ndfin = pd.read_html(s, index_col=0)\n  The lxml backend will raise an error on a failed parse if that is the only parser you provide. If you only have a single parser you can provide just a string, but it is considered good practice to pass a list with one string if, for example, the function expects a sequence of strings. You may use: \ndfs = pd.read_html(url, \"Metcalf Bank\", index_col=0, flavor=[\"lxml\"])\n  Or you could pass flavor='lxml' without a list: \ndfs = pd.read_html(url, \"Metcalf Bank\", index_col=0, flavor=\"lxml\")\n  However, if you have bs4 and html5lib installed and pass None or ['lxml',\n'bs4'] then the parse will most likely succeed. Note that as soon as a parse succeeds, the function will return. \ndfs = pd.read_html(url, \"Metcalf Bank\", index_col=0, flavor=[\"lxml\", \"bs4\"])\n    Writing to HTML files DataFrame objects have an instance method to_html which renders the contents of the DataFrame as an HTML table. The function arguments are as in the method to_string described above.  Note Not all of the possible options for DataFrame.to_html are shown here for brevity\u2019s sake. See to_html() for the full set of options.  \nIn [307]: df = pd.DataFrame(np.random.randn(2, 2))\n\nIn [308]: df\nOut[308]: \n          0         1\n0 -0.184744  0.496971\n1 -0.856240  1.857977\n\nIn [309]: print(df.to_html())  # raw html\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.184744</td>\n      <td>0.496971</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.856240</td>\n      <td>1.857977</td>\n    </tr>\n  </tbody>\n</table>\n  HTML:     0 1     0 -0.184744 0.496971   1 -0.856240 1.857977   \nThe columns argument will limit the columns shown: \nIn [310]: print(df.to_html(columns=[0]))\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.184744</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.856240</td>\n    </tr>\n  </tbody>\n</table>\n  HTML:     0     0 -0.184744   1 -0.856240   \nfloat_format takes a Python callable to control the precision of floating point values: \nIn [311]: print(df.to_html(float_format=\"{0:.10f}\".format))\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.1847438576</td>\n      <td>0.4969711327</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.8562396763</td>\n      <td>1.8579766508</td>\n    </tr>\n  </tbody>\n</table>\n  HTML:     0 1     0 -0.1847438576 0.4969711327   1 -0.8562396763 1.8579766508   \nbold_rows will make the row labels bold by default, but you can turn that off: \nIn [312]: print(df.to_html(bold_rows=False))\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>-0.184744</td>\n      <td>0.496971</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>-0.856240</td>\n      <td>1.857977</td>\n    </tr>\n  </tbody>\n</table>\n      0 1     0 -0.184744 0.496971   1 -0.856240 1.857977   \nThe classes argument provides the ability to give the resulting HTML table CSS classes. Note that these classes are appended to the existing 'dataframe' class. \nIn [313]: print(df.to_html(classes=[\"awesome_table_class\", \"even_more_awesome_class\"]))\n<table border=\"1\" class=\"dataframe awesome_table_class even_more_awesome_class\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.184744</td>\n      <td>0.496971</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.856240</td>\n      <td>1.857977</td>\n    </tr>\n  </tbody>\n</table>\n  The render_links argument provides the ability to add hyperlinks to cells that contain URLs. \nIn [314]: url_df = pd.DataFrame(\n   .....:     {\n   .....:         \"name\": [\"Python\", \"pandas\"],\n   .....:         \"url\": [\"https://www.python.org/\", \"https://pandas.pydata.org\"],\n   .....:     }\n   .....: )\n   .....: \n\nIn [315]: print(url_df.to_html(render_links=True))\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Python</td>\n      <td><a href=\"https://www.python.org/\" target=\"_blank\">https://www.python.org/</a></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>pandas</td>\n      <td><a href=\"https://pandas.pydata.org\" target=\"_blank\">https://pandas.pydata.org</a></td>\n    </tr>\n  </tbody>\n</table>\n  HTML:     name url     0 Python https://www.python.org/   1 pandas https://pandas.pydata.org   \nFinally, the escape argument allows you to control whether the \u201c<\u201d, \u201c>\u201d and \u201c&\u201d characters escaped in the resulting HTML (by default it is True). So to get the HTML without escaped characters pass escape=False \nIn [316]: df = pd.DataFrame({\"a\": list(\"&<>\"), \"b\": np.random.randn(3)})\n  Escaped: \nIn [317]: print(df.to_html())\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&amp;</td>\n      <td>-0.474063</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;</td>\n      <td>-0.230305</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&gt;</td>\n      <td>-0.400654</td>\n    </tr>\n  </tbody>\n</table>\n      a b     0 & -0.474063   1 < -0.230305   2 > -0.400654   \nNot escaped: \nIn [318]: print(df.to_html(escape=False))\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&</td>\n      <td>-0.474063</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td><</td>\n      <td>-0.230305</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>></td>\n      <td>-0.400654</td>\n    </tr>\n  </tbody>\n</table>\n      a b     0 & -0.474063   1 < -0.230305   2 > -0.400654   \n Note Some browsers may not show a difference in the rendering of the previous two HTML tables.    HTML Table Parsing Gotchas There are some versioning issues surrounding the libraries that are used to parse HTML tables in the top-level pandas io function read_html. Issues with lxml  \nBenefits  \n lxml is very fast. lxml requires Cython to install correctly.  \n  \nDrawbacks  \n lxml does not make any guarantees about the results of its parse unless it is given strictly valid markup. In light of the above, we have chosen to allow you, the user, to use the lxml backend, but this backend will use html5lib if lxml fails to parse It is therefore highly recommended that you install both BeautifulSoup4 and html5lib, so that you will still get a valid result (provided everything else is valid) even if lxml fails.  \n   Issues with BeautifulSoup4 using lxml as a backend  The above issues hold here as well since BeautifulSoup4 is essentially just a wrapper around a parser backend.  Issues with BeautifulSoup4 using html5lib as a backend  \nBenefits  \n html5lib is far more lenient than lxml and consequently deals with real-life markup in a much saner way rather than just, e.g., dropping an element without notifying you. html5lib generates valid HTML5 markup from invalid markup automatically. This is extremely important for parsing HTML tables, since it guarantees a valid document. However, that does NOT mean that it is \u201ccorrect\u201d, since the process of fixing markup does not have a single definition. html5lib is pure Python and requires no additional build steps beyond its own installation.  \n  \nDrawbacks  \n The biggest drawback to using html5lib is that it is slow as molasses. However consider the fact that many tables on the web are not big enough for the parsing algorithm runtime to matter. It is more likely that the bottleneck will be in the process of reading the raw text from the URL over the web, i.e., IO (input-output). For very large tables, this might not be true.  \n      LaTeX  New in version 1.3.0.  Currently there are no methods to read from LaTeX, only output methods.  Writing to LaTeX files  Note DataFrame and Styler objects currently have a to_latex method. We recommend using the Styler.to_latex() method over DataFrame.to_latex() due to the former\u2019s greater flexibility with conditional styling, and the latter\u2019s possible future deprecation.  Review the documentation for Styler.to_latex, which gives examples of conditional styling and explains the operation of its keyword arguments. For simple application the following pattern is sufficient. \nIn [319]: df = pd.DataFrame([[1, 2], [3, 4]], index=[\"a\", \"b\"], columns=[\"c\", \"d\"])\n\nIn [320]: print(df.style.to_latex())\n\\begin{tabular}{lrr}\n & c & d \\\\\na & 1 & 2 \\\\\nb & 3 & 4 \\\\\n\\end{tabular}\n  To format values before output, chain the Styler.format method. \nIn [321]: print(df.style.format(\"\u20ac {}\").to_latex())\n\\begin{tabular}{lrr}\n & c & d \\\\\na & \u20ac 1 & \u20ac 2 \\\\\nb & \u20ac 3 & \u20ac 4 \\\\\n\\end{tabular}\n     XML  Reading XML  New in version 1.3.0.  The top-level read_xml() function can accept an XML string/file/URL and will parse nodes and attributes into a pandas DataFrame.  Note Since there is no standard XML structure where design types can vary in many ways, read_xml works best with flatter, shallow versions. If an XML document is deeply nested, use the stylesheet feature to transform XML into a flatter version.  Let\u2019s look at a few examples. Read an XML string: \nIn [322]: xml = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n   .....: <bookstore>\n   .....:   <book category=\"cooking\">\n   .....:     <title lang=\"en\">Everyday Italian</title>\n   .....:     <author>Giada De Laurentiis</author>\n   .....:     <year>2005</year>\n   .....:     <price>30.00</price>\n   .....:   </book>\n   .....:   <book category=\"children\">\n   .....:     <title lang=\"en\">Harry Potter</title>\n   .....:     <author>J K. Rowling</author>\n   .....:     <year>2005</year>\n   .....:     <price>29.99</price>\n   .....:   </book>\n   .....:   <book category=\"web\">\n   .....:     <title lang=\"en\">Learning XML</title>\n   .....:     <author>Erik T. Ray</author>\n   .....:     <year>2003</year>\n   .....:     <price>39.95</price>\n   .....:   </book>\n   .....: </bookstore>\"\"\"\n   .....: \n\nIn [323]: df = pd.read_xml(xml)\n\nIn [324]: df\nOut[324]: \n   category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99\n2       web      Learning XML          Erik T. Ray  2003  39.95\n  Read a URL with no options: \nIn [325]: df = pd.read_xml(\"https://www.w3schools.com/xml/books.xml\")\n\nIn [326]: df\nOut[326]: \n   category              title                  author  year  price      cover\n0   cooking   Everyday Italian     Giada De Laurentiis  2005  30.00       None\n1  children       Harry Potter            J K. Rowling  2005  29.99       None\n2       web  XQuery Kick Start  Vaidyanathan Nagarajan  2003  49.99       None\n3       web       Learning XML             Erik T. Ray  2003  39.95  paperback\n  Read in the content of the \u201cbooks.xml\u201d file and pass it to read_xml as a string: \nIn [327]: with open(file_path, \"r\") as f:\n   .....:     df = pd.read_xml(f.read())\n   .....: \n\nIn [328]: df\nOut[328]: \n   category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99\n2       web      Learning XML          Erik T. Ray  2003  39.95\n  Read in the content of the \u201cbooks.xml\u201d as instance of StringIO or BytesIO and pass it to read_xml: \nIn [329]: with open(file_path, \"r\") as f:\n   .....:     sio = StringIO(f.read())\n   .....: \n\nIn [330]: df = pd.read_xml(sio)\n\nIn [331]: df\nOut[331]: \n   category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99\n2       web      Learning XML          Erik T. Ray  2003  39.95\n  \nIn [332]: with open(file_path, \"rb\") as f:\n   .....:     bio = BytesIO(f.read())\n   .....: \n\nIn [333]: df = pd.read_xml(bio)\n\nIn [334]: df\nOut[334]: \n   category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99\n2       web      Learning XML          Erik T. Ray  2003  39.95\n  Even read XML from AWS S3 buckets such as Python Software Foundation\u2019s IRS 990 Form: \nIn [335]: df = pd.read_xml(\n   .....:     \"s3://irs-form-990/201923199349319487_public.xml\",\n   .....:     xpath=\".//irs:Form990PartVIISectionAGrp\",\n   .....:     namespaces={\"irs\": \"http://www.irs.gov/efile\"}\n   .....: )\n   .....: \n\nIn [336]: df\nOut[336]: \n               PersonNm                                 TitleTxt  ...  OtherCompensationAmt  HighestCompensatedEmployeeInd\n0           Naomi Ceder                                    Chair  ...                     0                           None\n1          Van Lindberg              Vice Chair, General Counsel  ...                     0                           None\n2           Kurt Kaiser                                Treasurer  ...                     0                           None\n3         Ewa Jodlowska        Secretary, Director of Operations  ...                     0                           None\n4        Thomas Wouters                     Director, Vice Chair  ...                     0                           None\n..                  ...                                      ...  ...                   ...                            ...\n16        Kenneth Reitz                                 Director  ...                     0                           None\n17     Jeffrey Triplett                                 Director  ...                     0                           None\n18    Betsy Waliszewski   Assistant Secretary, Event Coordinator  ...                     0                           None\n19     Guido van Rossum                                President  ...                     0                           None\n20  Ernest W Durbin III  Director of Infrastructure, PyCon Chair  ...                     0                              X\n\n[21 rows x 10 columns]\n  With lxml as default parser, you access the full-featured XML library that extends Python\u2019s ElementTree API. One powerful tool is ability to query nodes selectively or conditionally with more expressive XPath: \nIn [337]: df = pd.read_xml(file_path, xpath=\"//book[year=2005]\")\n\nIn [338]: df\nOut[338]: \n   category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99\n  Specify only elements or only attributes to parse: \nIn [339]: df = pd.read_xml(file_path, elems_only=True)\n\nIn [340]: df\nOut[340]: \n              title               author  year  price\n0  Everyday Italian  Giada De Laurentiis  2005  30.00\n1      Harry Potter         J K. Rowling  2005  29.99\n2      Learning XML          Erik T. Ray  2003  39.95\n  \nIn [341]: df = pd.read_xml(file_path, attrs_only=True)\n\nIn [342]: df\nOut[342]: \n   category\n0   cooking\n1  children\n2       web\n  XML documents can have namespaces with prefixes and default namespaces without prefixes both of which are denoted with a special attribute xmlns. In order to parse by node under a namespace context, xpath must reference a prefix. For example, below XML contains a namespace with prefix, doc, and URI at https://example.com. In order to parse doc:row nodes, namespaces must be used. \nIn [343]: xml = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n   .....: <doc:data xmlns:doc=\"https://example.com\">\n   .....:   <doc:row>\n   .....:     <doc:shape>square</doc:shape>\n   .....:     <doc:degrees>360</doc:degrees>\n   .....:     <doc:sides>4.0</doc:sides>\n   .....:   </doc:row>\n   .....:   <doc:row>\n   .....:     <doc:shape>circle</doc:shape>\n   .....:     <doc:degrees>360</doc:degrees>\n   .....:     <doc:sides/>\n   .....:   </doc:row>\n   .....:   <doc:row>\n   .....:     <doc:shape>triangle</doc:shape>\n   .....:     <doc:degrees>180</doc:degrees>\n   .....:     <doc:sides>3.0</doc:sides>\n   .....:   </doc:row>\n   .....: </doc:data>\"\"\"\n   .....: \n\nIn [344]: df = pd.read_xml(xml,\n   .....:                  xpath=\"//doc:row\",\n   .....:                  namespaces={\"doc\": \"https://example.com\"})\n   .....: \n\nIn [345]: df\nOut[345]: \n      shape  degrees  sides\n0    square      360    4.0\n1    circle      360    NaN\n2  triangle      180    3.0\n  Similarly, an XML document can have a default namespace without prefix. Failing to assign a temporary prefix will return no nodes and raise a ValueError. But assigning any temporary name to correct URI allows parsing by nodes. \nIn [346]: xml = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n   .....: <data xmlns=\"https://example.com\">\n   .....:  <row>\n   .....:    <shape>square</shape>\n   .....:    <degrees>360</degrees>\n   .....:    <sides>4.0</sides>\n   .....:  </row>\n   .....:  <row>\n   .....:    <shape>circle</shape>\n   .....:    <degrees>360</degrees>\n   .....:    <sides/>\n   .....:  </row>\n   .....:  <row>\n   .....:    <shape>triangle</shape>\n   .....:    <degrees>180</degrees>\n   .....:    <sides>3.0</sides>\n   .....:  </row>\n   .....: </data>\"\"\"\n   .....: \n\nIn [347]: df = pd.read_xml(xml,\n   .....:                  xpath=\"//pandas:row\",\n   .....:                  namespaces={\"pandas\": \"https://example.com\"})\n   .....: \n\nIn [348]: df\nOut[348]: \n      shape  degrees  sides\n0    square      360    4.0\n1    circle      360    NaN\n2  triangle      180    3.0\n  However, if XPath does not reference node names such as default, /*, then namespaces is not required. With lxml as parser, you can flatten nested XML documents with an XSLT script which also can be string/file/URL types. As background, XSLT is a special-purpose language written in a special XML file that can transform original XML documents into other XML, HTML, even text (CSV, JSON, etc.) using an XSLT processor. For example, consider this somewhat nested structure of Chicago \u201cL\u201d Rides where station and rides elements encapsulate data in their own sections. With below XSLT, lxml can transform original nested document into a flatter output (as shown below for demonstration) for easier parse into DataFrame: \nIn [349]: xml = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n   .....:  <response>\n   .....:   <row>\n   .....:     <station id=\"40850\" name=\"Library\"/>\n   .....:     <month>2020-09-01T00:00:00</month>\n   .....:     <rides>\n   .....:       <avg_weekday_rides>864.2</avg_weekday_rides>\n   .....:       <avg_saturday_rides>534</avg_saturday_rides>\n   .....:       <avg_sunday_holiday_rides>417.2</avg_sunday_holiday_rides>\n   .....:     </rides>\n   .....:   </row>\n   .....:   <row>\n   .....:     <station id=\"41700\" name=\"Washington/Wabash\"/>\n   .....:     <month>2020-09-01T00:00:00</month>\n   .....:     <rides>\n   .....:       <avg_weekday_rides>2707.4</avg_weekday_rides>\n   .....:       <avg_saturday_rides>1909.8</avg_saturday_rides>\n   .....:       <avg_sunday_holiday_rides>1438.6</avg_sunday_holiday_rides>\n   .....:     </rides>\n   .....:   </row>\n   .....:   <row>\n   .....:     <station id=\"40380\" name=\"Clark/Lake\"/>\n   .....:     <month>2020-09-01T00:00:00</month>\n   .....:     <rides>\n   .....:       <avg_weekday_rides>2949.6</avg_weekday_rides>\n   .....:       <avg_saturday_rides>1657</avg_saturday_rides>\n   .....:       <avg_sunday_holiday_rides>1453.8</avg_sunday_holiday_rides>\n   .....:     </rides>\n   .....:   </row>\n   .....:  </response>\"\"\"\n   .....: \n\nIn [350]: xsl = \"\"\"<xsl:stylesheet version=\"1.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\">\n   .....:    <xsl:output method=\"xml\" omit-xml-declaration=\"no\" indent=\"yes\"/>\n   .....:    <xsl:strip-space elements=\"*\"/>\n   .....:    <xsl:template match=\"/response\">\n   .....:       <xsl:copy>\n   .....:         <xsl:apply-templates select=\"row\"/>\n   .....:       </xsl:copy>\n   .....:    </xsl:template>\n   .....:    <xsl:template match=\"row\">\n   .....:       <xsl:copy>\n   .....:         <station_id><xsl:value-of select=\"station/@id\"/></station_id>\n   .....:         <station_name><xsl:value-of select=\"station/@name\"/></station_name>\n   .....:         <xsl:copy-of select=\"month|rides/*\"/>\n   .....:       </xsl:copy>\n   .....:    </xsl:template>\n   .....:  </xsl:stylesheet>\"\"\"\n   .....: \n\nIn [351]: output = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n   .....:  <response>\n   .....:    <row>\n   .....:       <station_id>40850</station_id>\n   .....:       <station_name>Library</station_name>\n   .....:       <month>2020-09-01T00:00:00</month>\n   .....:       <avg_weekday_rides>864.2</avg_weekday_rides>\n   .....:       <avg_saturday_rides>534</avg_saturday_rides>\n   .....:       <avg_sunday_holiday_rides>417.2</avg_sunday_holiday_rides>\n   .....:    </row>\n   .....:    <row>\n   .....:       <station_id>41700</station_id>\n   .....:       <station_name>Washington/Wabash</station_name>\n   .....:       <month>2020-09-01T00:00:00</month>\n   .....:       <avg_weekday_rides>2707.4</avg_weekday_rides>\n   .....:       <avg_saturday_rides>1909.8</avg_saturday_rides>\n   .....:       <avg_sunday_holiday_rides>1438.6</avg_sunday_holiday_rides>\n   .....:    </row>\n   .....:    <row>\n   .....:       <station_id>40380</station_id>\n   .....:       <station_name>Clark/Lake</station_name>\n   .....:       <month>2020-09-01T00:00:00</month>\n   .....:       <avg_weekday_rides>2949.6</avg_weekday_rides>\n   .....:       <avg_saturday_rides>1657</avg_saturday_rides>\n   .....:       <avg_sunday_holiday_rides>1453.8</avg_sunday_holiday_rides>\n   .....:    </row>\n   .....:  </response>\"\"\"\n   .....: \n\nIn [352]: df = pd.read_xml(xml, stylesheet=xsl)\n\nIn [353]: df\nOut[353]: \n   station_id       station_name                month  avg_weekday_rides  avg_saturday_rides  avg_sunday_holiday_rides\n0       40850            Library  2020-09-01T00:00:00              864.2               534.0                     417.2\n1       41700  Washington/Wabash  2020-09-01T00:00:00             2707.4              1909.8                    1438.6\n2       40380         Clark/Lake  2020-09-01T00:00:00             2949.6              1657.0                    1453.8\n    Writing XML  New in version 1.3.0.  DataFrame objects have an instance method to_xml which renders the contents of the DataFrame as an XML document.  Note This method does not support special properties of XML including DTD, CData, XSD schemas, processing instructions, comments, and others. Only namespaces at the root level is supported. However, stylesheet allows design changes after initial output.  Let\u2019s look at a few examples. Write an XML without options: \nIn [354]: geom_df = pd.DataFrame(\n   .....:     {\n   .....:         \"shape\": [\"square\", \"circle\", \"triangle\"],\n   .....:         \"degrees\": [360, 360, 180],\n   .....:         \"sides\": [4, np.nan, 3],\n   .....:     }\n   .....: )\n   .....: \n\nIn [355]: print(geom_df.to_xml())\n<?xml version='1.0' encoding='utf-8'?>\n<data>\n  <row>\n    <index>0</index>\n    <shape>square</shape>\n    <degrees>360</degrees>\n    <sides>4.0</sides>\n  </row>\n  <row>\n    <index>1</index>\n    <shape>circle</shape>\n    <degrees>360</degrees>\n    <sides/>\n  </row>\n  <row>\n    <index>2</index>\n    <shape>triangle</shape>\n    <degrees>180</degrees>\n    <sides>3.0</sides>\n  </row>\n</data>\n  Write an XML with new root and row name: \nIn [356]: print(geom_df.to_xml(root_name=\"geometry\", row_name=\"objects\"))\n<?xml version='1.0' encoding='utf-8'?>\n<geometry>\n  <objects>\n    <index>0</index>\n    <shape>square</shape>\n    <degrees>360</degrees>\n    <sides>4.0</sides>\n  </objects>\n  <objects>\n    <index>1</index>\n    <shape>circle</shape>\n    <degrees>360</degrees>\n    <sides/>\n  </objects>\n  <objects>\n    <index>2</index>\n    <shape>triangle</shape>\n    <degrees>180</degrees>\n    <sides>3.0</sides>\n  </objects>\n</geometry>\n  Write an attribute-centric XML: \nIn [357]: print(geom_df.to_xml(attr_cols=geom_df.columns.tolist()))\n<?xml version='1.0' encoding='utf-8'?>\n<data>\n  <row index=\"0\" shape=\"square\" degrees=\"360\" sides=\"4.0\"/>\n  <row index=\"1\" shape=\"circle\" degrees=\"360\"/>\n  <row index=\"2\" shape=\"triangle\" degrees=\"180\" sides=\"3.0\"/>\n</data>\n  Write a mix of elements and attributes: \nIn [358]: print(\n   .....:     geom_df.to_xml(\n   .....:         index=False,\n   .....:         attr_cols=['shape'],\n   .....:         elem_cols=['degrees', 'sides'])\n   .....: )\n   .....: \n<?xml version='1.0' encoding='utf-8'?>\n<data>\n  <row shape=\"square\">\n    <degrees>360</degrees>\n    <sides>4.0</sides>\n  </row>\n  <row shape=\"circle\">\n    <degrees>360</degrees>\n    <sides/>\n  </row>\n  <row shape=\"triangle\">\n    <degrees>180</degrees>\n    <sides>3.0</sides>\n  </row>\n</data>\n  Any DataFrames with hierarchical columns will be flattened for XML element names with levels delimited by underscores: \nIn [359]: ext_geom_df = pd.DataFrame(\n   .....:     {\n   .....:         \"type\": [\"polygon\", \"other\", \"polygon\"],\n   .....:         \"shape\": [\"square\", \"circle\", \"triangle\"],\n   .....:         \"degrees\": [360, 360, 180],\n   .....:         \"sides\": [4, np.nan, 3],\n   .....:     }\n   .....: )\n   .....: \n\nIn [360]: pvt_df = ext_geom_df.pivot_table(index='shape',\n   .....:                                  columns='type',\n   .....:                                  values=['degrees', 'sides'],\n   .....:                                  aggfunc='sum')\n   .....: \n\nIn [361]: pvt_df\nOut[361]: \n         degrees         sides        \ntype       other polygon other polygon\nshape                                 \ncircle     360.0     NaN   0.0     NaN\nsquare       NaN   360.0   NaN     4.0\ntriangle     NaN   180.0   NaN     3.0\n\nIn [362]: print(pvt_df.to_xml())\n<?xml version='1.0' encoding='utf-8'?>\n<data>\n  <row>\n    <shape>circle</shape>\n    <degrees_other>360.0</degrees_other>\n    <degrees_polygon/>\n    <sides_other>0.0</sides_other>\n    <sides_polygon/>\n  </row>\n  <row>\n    <shape>square</shape>\n    <degrees_other/>\n    <degrees_polygon>360.0</degrees_polygon>\n    <sides_other/>\n    <sides_polygon>4.0</sides_polygon>\n  </row>\n  <row>\n    <shape>triangle</shape>\n    <degrees_other/>\n    <degrees_polygon>180.0</degrees_polygon>\n    <sides_other/>\n    <sides_polygon>3.0</sides_polygon>\n  </row>\n</data>\n  Write an XML with default namespace: \nIn [363]: print(geom_df.to_xml(namespaces={\"\": \"https://example.com\"}))\n<?xml version='1.0' encoding='utf-8'?>\n<data xmlns=\"https://example.com\">\n  <row>\n    <index>0</index>\n    <shape>square</shape>\n    <degrees>360</degrees>\n    <sides>4.0</sides>\n  </row>\n  <row>\n    <index>1</index>\n    <shape>circle</shape>\n    <degrees>360</degrees>\n    <sides/>\n  </row>\n  <row>\n    <index>2</index>\n    <shape>triangle</shape>\n    <degrees>180</degrees>\n    <sides>3.0</sides>\n  </row>\n</data>\n  Write an XML with namespace prefix: \nIn [364]: print(\n   .....:     geom_df.to_xml(namespaces={\"doc\": \"https://example.com\"},\n   .....:                    prefix=\"doc\")\n   .....: )\n   .....: \n<?xml version='1.0' encoding='utf-8'?>\n<doc:data xmlns:doc=\"https://example.com\">\n  <doc:row>\n    <doc:index>0</doc:index>\n    <doc:shape>square</doc:shape>\n    <doc:degrees>360</doc:degrees>\n    <doc:sides>4.0</doc:sides>\n  </doc:row>\n  <doc:row>\n    <doc:index>1</doc:index>\n    <doc:shape>circle</doc:shape>\n    <doc:degrees>360</doc:degrees>\n    <doc:sides/>\n  </doc:row>\n  <doc:row>\n    <doc:index>2</doc:index>\n    <doc:shape>triangle</doc:shape>\n    <doc:degrees>180</doc:degrees>\n    <doc:sides>3.0</doc:sides>\n  </doc:row>\n</doc:data>\n  Write an XML without declaration or pretty print: \nIn [365]: print(\n   .....:     geom_df.to_xml(xml_declaration=False,\n   .....:                    pretty_print=False)\n   .....: )\n   .....: \n<data><row><index>0</index><shape>square</shape><degrees>360</degrees><sides>4.0</sides></row><row><index>1</index><shape>circle</shape><degrees>360</degrees><sides/></row><row><index>2</index><shape>triangle</shape><degrees>180</degrees><sides>3.0</sides></row></data>\n  Write an XML and transform with stylesheet: \nIn [366]: xsl = \"\"\"<xsl:stylesheet version=\"1.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\">\n   .....:    <xsl:output method=\"xml\" omit-xml-declaration=\"no\" indent=\"yes\"/>\n   .....:    <xsl:strip-space elements=\"*\"/>\n   .....:    <xsl:template match=\"/data\">\n   .....:      <geometry>\n   .....:        <xsl:apply-templates select=\"row\"/>\n   .....:      </geometry>\n   .....:    </xsl:template>\n   .....:    <xsl:template match=\"row\">\n   .....:      <object index=\"{index}\">\n   .....:        <xsl:if test=\"shape!='circle'\">\n   .....:            <xsl:attribute name=\"type\">polygon</xsl:attribute>\n   .....:        </xsl:if>\n   .....:        <xsl:copy-of select=\"shape\"/>\n   .....:        <property>\n   .....:            <xsl:copy-of select=\"degrees|sides\"/>\n   .....:        </property>\n   .....:      </object>\n   .....:    </xsl:template>\n   .....:  </xsl:stylesheet>\"\"\"\n   .....: \n\nIn [367]: print(geom_df.to_xml(stylesheet=xsl))\n<?xml version=\"1.0\"?>\n<geometry>\n  <object index=\"0\" type=\"polygon\">\n    <shape>square</shape>\n    <property>\n      <degrees>360</degrees>\n      <sides>4.0</sides>\n    </property>\n  </object>\n  <object index=\"1\">\n    <shape>circle</shape>\n    <property>\n      <degrees>360</degrees>\n      <sides/>\n    </property>\n  </object>\n  <object index=\"2\" type=\"polygon\">\n    <shape>triangle</shape>\n    <property>\n      <degrees>180</degrees>\n      <sides>3.0</sides>\n    </property>\n  </object>\n</geometry>\n    XML Final Notes  All XML documents adhere to W3C specifications. Both etree and lxml parsers will fail to parse any markup document that is not well-formed or follows XML syntax rules. Do be aware HTML is not an XML document unless it follows XHTML specs. However, other popular markup types including KML, XAML, RSS, MusicML, MathML are compliant XML schemas. For above reason, if your application builds XML prior to pandas operations, use appropriate DOM libraries like etree and lxml to build the necessary document and not by string concatenation or regex adjustments. Always remember XML is a special text file with markup rules. With very large XML files (several hundred MBs to GBs), XPath and XSLT can become memory-intensive operations. Be sure to have enough available RAM for reading and writing to large XML files (roughly about 5 times the size of text). Because XSLT is a programming language, use it with caution since such scripts can pose a security risk in your environment and can run large or infinite recursive operations. Always test scripts on small fragments before full run. The etree parser supports all functionality of both read_xml and to_xml except for complex XPath and any XSLT. Though limited in features, etree is still a reliable and capable parser and tree builder. Its performance may trail lxml to a certain degree for larger files but relatively unnoticeable on small to medium size files.     Excel files The read_excel() method can read Excel 2007+ (.xlsx) files using the openpyxl Python module. Excel 2003 (.xls) files can be read using xlrd. Binary Excel (.xlsb) files can be read using pyxlsb. The to_excel() instance method is used for saving a DataFrame to Excel. Generally the semantics are similar to working with csv data. See the cookbook for some advanced strategies.  Warning The xlwt package for writing old-style .xls excel files is no longer maintained. The xlrd package is now only for reading old-style .xls files. Before pandas 1.3.0, the default argument engine=None to read_excel() would result in using the xlrd engine in many cases, including new Excel 2007+ (.xlsx) files. pandas will now default to using the openpyxl engine. It is strongly encouraged to install openpyxl to read Excel 2007+ (.xlsx) files. Please do not report issues when using ``xlrd`` to read ``.xlsx`` files. This is no longer supported, switch to using openpyxl instead. Attempting to use the the xlwt engine will raise a FutureWarning unless the option io.excel.xls.writer is set to \"xlwt\". While this option is now deprecated and will also raise a FutureWarning, it can be globally set and the warning suppressed. Users are recommended to write .xlsx files using the openpyxl engine instead.   Reading Excel files In the most basic use-case, read_excel takes a path to an Excel file, and the sheet_name indicating which sheet to parse. \n# Returns a DataFrame\npd.read_excel(\"path_to_file.xls\", sheet_name=\"Sheet1\")\n   \nExcelFile class To facilitate working with multiple sheets from the same file, the ExcelFile class can be used to wrap the file and can be passed into read_excel There will be a performance benefit for reading multiple sheets as the file is read into memory only once. \nxlsx = pd.ExcelFile(\"path_to_file.xls\")\ndf = pd.read_excel(xlsx, \"Sheet1\")\n  The ExcelFile class can also be used as a context manager. \nwith pd.ExcelFile(\"path_to_file.xls\") as xls:\n    df1 = pd.read_excel(xls, \"Sheet1\")\n    df2 = pd.read_excel(xls, \"Sheet2\")\n  The sheet_names property will generate a list of the sheet names in the file. The primary use-case for an ExcelFile is parsing multiple sheets with different parameters: \ndata = {}\n# For when Sheet1's format differs from Sheet2\nwith pd.ExcelFile(\"path_to_file.xls\") as xls:\n    data[\"Sheet1\"] = pd.read_excel(xls, \"Sheet1\", index_col=None, na_values=[\"NA\"])\n    data[\"Sheet2\"] = pd.read_excel(xls, \"Sheet2\", index_col=1)\n  Note that if the same parsing parameters are used for all sheets, a list of sheet names can simply be passed to read_excel with no loss in performance. \n# using the ExcelFile class\ndata = {}\nwith pd.ExcelFile(\"path_to_file.xls\") as xls:\n    data[\"Sheet1\"] = pd.read_excel(xls, \"Sheet1\", index_col=None, na_values=[\"NA\"])\n    data[\"Sheet2\"] = pd.read_excel(xls, \"Sheet2\", index_col=None, na_values=[\"NA\"])\n\n# equivalent using the read_excel function\ndata = pd.read_excel(\n    \"path_to_file.xls\", [\"Sheet1\", \"Sheet2\"], index_col=None, na_values=[\"NA\"]\n)\n  ExcelFile can also be called with a xlrd.book.Book object as a parameter. This allows the user to control how the excel file is read. For example, sheets can be loaded on demand by calling xlrd.open_workbook() with on_demand=True. \nimport xlrd\n\nxlrd_book = xlrd.open_workbook(\"path_to_file.xls\", on_demand=True)\nwith pd.ExcelFile(xlrd_book) as xls:\n    df1 = pd.read_excel(xls, \"Sheet1\")\n    df2 = pd.read_excel(xls, \"Sheet2\")\n    Specifying sheets  Note The second argument is sheet_name, not to be confused with ExcelFile.sheet_names.   Note An ExcelFile\u2019s attribute sheet_names provides access to a list of sheets.   The arguments sheet_name allows specifying the sheet or sheets to read. The default value for sheet_name is 0, indicating to read the first sheet Pass a string to refer to the name of a particular sheet in the workbook. Pass an integer to refer to the index of a sheet. Indices follow Python convention, beginning at 0. Pass a list of either strings or integers, to return a dictionary of specified sheets. Pass a None to return a dictionary of all available sheets.  \n# Returns a DataFrame\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", index_col=None, na_values=[\"NA\"])\n  Using the sheet index: \n# Returns a DataFrame\npd.read_excel(\"path_to_file.xls\", 0, index_col=None, na_values=[\"NA\"])\n  Using all default values: \n# Returns a DataFrame\npd.read_excel(\"path_to_file.xls\")\n  Using None to get all sheets: \n# Returns a dictionary of DataFrames\npd.read_excel(\"path_to_file.xls\", sheet_name=None)\n  Using a list to get multiple sheets: \n# Returns the 1st and 4th sheet, as a dictionary of DataFrames.\npd.read_excel(\"path_to_file.xls\", sheet_name=[\"Sheet1\", 3])\n  read_excel can read more than one sheet, by setting sheet_name to either a list of sheet names, a list of sheet positions, or None to read all sheets. Sheets can be specified by sheet index or sheet name, using an integer or string, respectively.   Reading a MultiIndex\n read_excel can read a MultiIndex index, by passing a list of columns to index_col and a MultiIndex column by passing a list of rows to header. If either the index or columns have serialized level names those will be read in as well by specifying the rows/columns that make up the levels. For example, to read in a MultiIndex index without names: \nIn [368]: df = pd.DataFrame(\n   .....:     {\"a\": [1, 2, 3, 4], \"b\": [5, 6, 7, 8]},\n   .....:     index=pd.MultiIndex.from_product([[\"a\", \"b\"], [\"c\", \"d\"]]),\n   .....: )\n   .....: \n\nIn [369]: df.to_excel(\"path_to_file.xlsx\")\n\nIn [370]: df = pd.read_excel(\"path_to_file.xlsx\", index_col=[0, 1])\n\nIn [371]: df\nOut[371]: \n     a  b\na c  1  5\n  d  2  6\nb c  3  7\n  d  4  8\n  If the index has level names, they will parsed as well, using the same parameters. \nIn [372]: df.index = df.index.set_names([\"lvl1\", \"lvl2\"])\n\nIn [373]: df.to_excel(\"path_to_file.xlsx\")\n\nIn [374]: df = pd.read_excel(\"path_to_file.xlsx\", index_col=[0, 1])\n\nIn [375]: df\nOut[375]: \n           a  b\nlvl1 lvl2      \na    c     1  5\n     d     2  6\nb    c     3  7\n     d     4  8\n  If the source file has both MultiIndex index and columns, lists specifying each should be passed to index_col and header: \nIn [376]: df.columns = pd.MultiIndex.from_product([[\"a\"], [\"b\", \"d\"]], names=[\"c1\", \"c2\"])\n\nIn [377]: df.to_excel(\"path_to_file.xlsx\")\n\nIn [378]: df = pd.read_excel(\"path_to_file.xlsx\", index_col=[0, 1], header=[0, 1])\n\nIn [379]: df\nOut[379]: \nc1         a   \nc2         b  d\nlvl1 lvl2      \na    c     1  5\n     d     2  6\nb    c     3  7\n     d     4  8\n    Parsing specific columns It is often the case that users will insert columns to do temporary computations in Excel and you may not want to read in those columns. read_excel takes a usecols keyword to allow you to specify a subset of columns to parse.  Changed in version 1.0.0.  Passing in an integer for usecols will no longer work. Please pass in a list of ints from 0 to usecols inclusive instead. You can specify a comma-delimited set of Excel columns and ranges as a string: \npd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=\"A,C:E\")\n  If usecols is a list of integers, then it is assumed to be the file column indices to be parsed. \npd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=[0, 2, 3])\n  Element order is ignored, so usecols=[0, 1] is the same as [1, 0]. If usecols is a list of strings, it is assumed that each string corresponds to a column name provided either by the user in names or inferred from the document header row(s). Those strings define which columns will be parsed: \npd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=[\"foo\", \"bar\"])\n  Element order is ignored, so usecols=['baz', 'joe'] is the same as ['joe', 'baz']. If usecols is callable, the callable function will be evaluated against the column names, returning names where the callable function evaluates to True. \npd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=lambda x: x.isalpha())\n    Parsing dates Datetime-like values are normally automatically converted to the appropriate dtype when reading the excel file. But if you have a column of strings that look like dates (but are not actually formatted as dates in excel), you can use the parse_dates keyword to parse those strings to datetimes: \npd.read_excel(\"path_to_file.xls\", \"Sheet1\", parse_dates=[\"date_strings\"])\n    Cell converters It is possible to transform the contents of Excel cells via the converters option. For instance, to convert a column to boolean: \npd.read_excel(\"path_to_file.xls\", \"Sheet1\", converters={\"MyBools\": bool})\n  This options handles missing values and treats exceptions in the converters as missing data. Transformations are applied cell by cell rather than to the column as a whole, so the array dtype is not guaranteed. For instance, a column of integers with missing values cannot be transformed to an array with integer dtype, because NaN is strictly a float. You can manually mask missing data to recover integer dtype: \ndef cfun(x):\n    return int(x) if x else -1\n\n\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", converters={\"MyInts\": cfun})\n    Dtype specifications As an alternative to converters, the type for an entire column can be specified using the dtype keyword, which takes a dictionary mapping column names to types. To interpret data with no type inference, use the type str or object. \npd.read_excel(\"path_to_file.xls\", dtype={\"MyInts\": \"int64\", \"MyText\": str})\n     Writing Excel files  Writing Excel files to disk To write a DataFrame object to a sheet of an Excel file, you can use the to_excel instance method. The arguments are largely the same as to_csv described above, the first argument being the name of the excel file, and the optional second argument the name of the sheet to which the DataFrame should be written. For example: \ndf.to_excel(\"path_to_file.xlsx\", sheet_name=\"Sheet1\")\n  Files with a .xls extension will be written using xlwt and those with a .xlsx extension will be written using xlsxwriter (if available) or openpyxl. The DataFrame will be written in a way that tries to mimic the REPL output. The index_label will be placed in the second row instead of the first. You can place it in the first row by setting the merge_cells option in to_excel() to False: \ndf.to_excel(\"path_to_file.xlsx\", index_label=\"label\", merge_cells=False)\n  In order to write separate DataFrames to separate sheets in a single Excel file, one can pass an ExcelWriter. \nwith pd.ExcelWriter(\"path_to_file.xlsx\") as writer:\n    df1.to_excel(writer, sheet_name=\"Sheet1\")\n    df2.to_excel(writer, sheet_name=\"Sheet2\")\n    Writing Excel files to memory pandas supports writing Excel files to buffer-like objects such as StringIO or BytesIO using ExcelWriter. \nfrom io import BytesIO\n\nbio = BytesIO()\n\n# By setting the 'engine' in the ExcelWriter constructor.\nwriter = pd.ExcelWriter(bio, engine=\"xlsxwriter\")\ndf.to_excel(writer, sheet_name=\"Sheet1\")\n\n# Save the workbook\nwriter.save()\n\n# Seek to the beginning and read to copy the workbook to a variable in memory\nbio.seek(0)\nworkbook = bio.read()\n   Note engine is optional but recommended. Setting the engine determines the version of workbook produced. Setting engine='xlrd' will produce an Excel 2003-format workbook (xls). Using either 'openpyxl' or 'xlsxwriter' will produce an Excel 2007-format workbook (xlsx). If omitted, an Excel 2007-formatted workbook is produced.     Excel writer engines  Deprecated since version 1.2.0: As the xlwt package is no longer maintained, the xlwt engine will be removed from a future version of pandas. This is the only engine in pandas that supports writing to .xls files.  pandas chooses an Excel writer via two methods:  the engine keyword argument the filename extension (via the default specified in config options)  By default, pandas uses the XlsxWriter for .xlsx, openpyxl for .xlsm, and xlwt for .xls files. If you have multiple engines installed, you can set the default engine through setting the config options io.excel.xlsx.writer and io.excel.xls.writer. pandas will fall back on openpyxl for .xlsx files if Xlsxwriter is not available. To specify which writer you want to use, you can pass an engine keyword argument to to_excel and to ExcelWriter. The built-in engines are:  openpyxl: version 2.4 or higher is required xlsxwriter xlwt  \n# By setting the 'engine' in the DataFrame 'to_excel()' methods.\ndf.to_excel(\"path_to_file.xlsx\", sheet_name=\"Sheet1\", engine=\"xlsxwriter\")\n\n# By setting the 'engine' in the ExcelWriter constructor.\nwriter = pd.ExcelWriter(\"path_to_file.xlsx\", engine=\"xlsxwriter\")\n\n# Or via pandas configuration.\nfrom pandas import options  # noqa: E402\n\noptions.io.excel.xlsx.writer = \"xlsxwriter\"\n\ndf.to_excel(\"path_to_file.xlsx\", sheet_name=\"Sheet1\")\n    Style and formatting The look and feel of Excel worksheets created from pandas can be modified using the following parameters on the DataFrame\u2019s to_excel method.  float_format : Format string for floating point numbers (default None). freeze_panes : A tuple of two integers representing the bottommost row and rightmost column to freeze. Each of these parameters is one-based, so (1, 1) will freeze the first row and first column (default None).  Using the Xlsxwriter engine provides many options for controlling the format of an Excel worksheet created with the to_excel method. Excellent examples can be found in the Xlsxwriter documentation here: https://xlsxwriter.readthedocs.io/working_with_pandas.html    OpenDocument Spreadsheets  New in version 0.25.  The read_excel() method can also read OpenDocument spreadsheets using the odfpy module. The semantics and features for reading OpenDocument spreadsheets match what can be done for Excel files using engine='odf'. \n# Returns a DataFrame\npd.read_excel(\"path_to_file.ods\", engine=\"odf\")\n   Note Currently pandas only supports reading OpenDocument spreadsheets. Writing is not implemented.    Binary Excel (.xlsb) files  New in version 1.0.0.  The read_excel() method can also read binary Excel files using the pyxlsb module. The semantics and features for reading binary Excel files mostly match what can be done for Excel files using engine='pyxlsb'. pyxlsb does not recognize datetime types in files and will return floats instead. \n# Returns a DataFrame\npd.read_excel(\"path_to_file.xlsb\", engine=\"pyxlsb\")\n   Note Currently pandas only supports reading binary Excel files. Writing is not implemented.    Clipboard A handy way to grab data is to use the read_clipboard() method, which takes the contents of the clipboard buffer and passes them to the read_csv method. For instance, you can copy the following text to the clipboard (CTRL-C on many operating systems): \n  A B C\nx 1 4 p\ny 2 5 q\nz 3 6 r\n  And then import the data directly to a DataFrame by calling: \n>>> clipdf = pd.read_clipboard()\n>>> clipdf\n  A B C\nx 1 4 p\ny 2 5 q\nz 3 6 r\n  The to_clipboard method can be used to write the contents of a DataFrame to the clipboard. Following which you can paste the clipboard contents into other applications (CTRL-V on many operating systems). Here we illustrate writing a DataFrame into clipboard and reading it back. \n>>> df = pd.DataFrame(\n...     {\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"C\": [\"p\", \"q\", \"r\"]}, index=[\"x\", \"y\", \"z\"]\n... )\n\n>>> df\n  A B C\nx 1 4 p\ny 2 5 q\nz 3 6 r\n>>> df.to_clipboard()\n>>> pd.read_clipboard()\n  A B C\nx 1 4 p\ny 2 5 q\nz 3 6 r\n  We can see that we got the same content back, which we had earlier written to the clipboard.  Note You may need to install xclip or xsel (with PyQt5, PyQt4 or qtpy) on Linux to use these methods.    Pickling All pandas objects are equipped with to_pickle methods which use Python\u2019s cPickle module to save data structures to disk using the pickle format. \nIn [380]: df\nOut[380]: \nc1         a   \nc2         b  d\nlvl1 lvl2      \na    c     1  5\n     d     2  6\nb    c     3  7\n     d     4  8\n\nIn [381]: df.to_pickle(\"foo.pkl\")\n  The read_pickle function in the pandas namespace can be used to load any pickled pandas object (or any other pickled object) from file: \nIn [382]: pd.read_pickle(\"foo.pkl\")\nOut[382]: \nc1         a   \nc2         b  d\nlvl1 lvl2      \na    c     1  5\n     d     2  6\nb    c     3  7\n     d     4  8\n   Warning Loading pickled data received from untrusted sources can be unsafe. See: https://docs.python.org/3/library/pickle.html   Warning read_pickle() is only guaranteed backwards compatible back to pandas version 0.20.3   Compressed pickle files read_pickle(), DataFrame.to_pickle() and Series.to_pickle() can read and write compressed pickle files. The compression types of gzip, bz2, xz, zstd are supported for reading and writing. The zip file format only supports reading and must contain only one data file to be read. The compression type can be an explicit parameter or be inferred from the file extension. If \u2018infer\u2019, then use gzip, bz2, zip, xz, zstd if filename ends in '.gz', '.bz2', '.zip', '.xz', or '.zst', respectively. The compression parameter can also be a dict in order to pass options to the compression protocol. It must have a 'method' key set to the name of the compression protocol, which must be one of {'zip', 'gzip', 'bz2', 'xz', 'zstd'}. All other key-value pairs are passed to the underlying compression library. \nIn [383]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"A\": np.random.randn(1000),\n   .....:         \"B\": \"foo\",\n   .....:         \"C\": pd.date_range(\"20130101\", periods=1000, freq=\"s\"),\n   .....:     }\n   .....: )\n   .....: \n\nIn [384]: df\nOut[384]: \n            A    B                   C\n0   -0.288267  foo 2013-01-01 00:00:00\n1   -0.084905  foo 2013-01-01 00:00:01\n2    0.004772  foo 2013-01-01 00:00:02\n3    1.382989  foo 2013-01-01 00:00:03\n4    0.343635  foo 2013-01-01 00:00:04\n..        ...  ...                 ...\n995 -0.220893  foo 2013-01-01 00:16:35\n996  0.492996  foo 2013-01-01 00:16:36\n997 -0.461625  foo 2013-01-01 00:16:37\n998  1.361779  foo 2013-01-01 00:16:38\n999 -1.197988  foo 2013-01-01 00:16:39\n\n[1000 rows x 3 columns]\n  Using an explicit compression type: \nIn [385]: df.to_pickle(\"data.pkl.compress\", compression=\"gzip\")\n\nIn [386]: rt = pd.read_pickle(\"data.pkl.compress\", compression=\"gzip\")\n\nIn [387]: rt\nOut[387]: \n            A    B                   C\n0   -0.288267  foo 2013-01-01 00:00:00\n1   -0.084905  foo 2013-01-01 00:00:01\n2    0.004772  foo 2013-01-01 00:00:02\n3    1.382989  foo 2013-01-01 00:00:03\n4    0.343635  foo 2013-01-01 00:00:04\n..        ...  ...                 ...\n995 -0.220893  foo 2013-01-01 00:16:35\n996  0.492996  foo 2013-01-01 00:16:36\n997 -0.461625  foo 2013-01-01 00:16:37\n998  1.361779  foo 2013-01-01 00:16:38\n999 -1.197988  foo 2013-01-01 00:16:39\n\n[1000 rows x 3 columns]\n  Inferring compression type from the extension: \nIn [388]: df.to_pickle(\"data.pkl.xz\", compression=\"infer\")\n\nIn [389]: rt = pd.read_pickle(\"data.pkl.xz\", compression=\"infer\")\n\nIn [390]: rt\nOut[390]: \n            A    B                   C\n0   -0.288267  foo 2013-01-01 00:00:00\n1   -0.084905  foo 2013-01-01 00:00:01\n2    0.004772  foo 2013-01-01 00:00:02\n3    1.382989  foo 2013-01-01 00:00:03\n4    0.343635  foo 2013-01-01 00:00:04\n..        ...  ...                 ...\n995 -0.220893  foo 2013-01-01 00:16:35\n996  0.492996  foo 2013-01-01 00:16:36\n997 -0.461625  foo 2013-01-01 00:16:37\n998  1.361779  foo 2013-01-01 00:16:38\n999 -1.197988  foo 2013-01-01 00:16:39\n\n[1000 rows x 3 columns]\n  The default is to \u2018infer\u2019: \nIn [391]: df.to_pickle(\"data.pkl.gz\")\n\nIn [392]: rt = pd.read_pickle(\"data.pkl.gz\")\n\nIn [393]: rt\nOut[393]: \n            A    B                   C\n0   -0.288267  foo 2013-01-01 00:00:00\n1   -0.084905  foo 2013-01-01 00:00:01\n2    0.004772  foo 2013-01-01 00:00:02\n3    1.382989  foo 2013-01-01 00:00:03\n4    0.343635  foo 2013-01-01 00:00:04\n..        ...  ...                 ...\n995 -0.220893  foo 2013-01-01 00:16:35\n996  0.492996  foo 2013-01-01 00:16:36\n997 -0.461625  foo 2013-01-01 00:16:37\n998  1.361779  foo 2013-01-01 00:16:38\n999 -1.197988  foo 2013-01-01 00:16:39\n\n[1000 rows x 3 columns]\n\nIn [394]: df[\"A\"].to_pickle(\"s1.pkl.bz2\")\n\nIn [395]: rt = pd.read_pickle(\"s1.pkl.bz2\")\n\nIn [396]: rt\nOut[396]: \n0     -0.288267\n1     -0.084905\n2      0.004772\n3      1.382989\n4      0.343635\n         ...   \n995   -0.220893\n996    0.492996\n997   -0.461625\n998    1.361779\n999   -1.197988\nName: A, Length: 1000, dtype: float64\n  Passing options to the compression protocol in order to speed up compression: \nIn [397]: df.to_pickle(\"data.pkl.gz\", compression={\"method\": \"gzip\", \"compresslevel\": 1})\n     msgpack pandas support for msgpack has been removed in version 1.0.0. It is recommended to use pickle instead. Alternatively, you can also the Arrow IPC serialization format for on-the-wire transmission of pandas objects. For documentation on pyarrow, see here.   HDF5 (PyTables) HDFStore is a dict-like object which reads and writes pandas using the high performance HDF5 format using the excellent PyTables library. See the cookbook for some advanced strategies  Warning pandas uses PyTables for reading and writing HDF5 files, which allows serializing object-dtype data with pickle. Loading pickled data received from untrusted sources can be unsafe. See: https://docs.python.org/3/library/pickle.html for more.  \nIn [398]: store = pd.HDFStore(\"store.h5\")\n\nIn [399]: print(store)\n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n  Objects can be written to the file just like adding key-value pairs to a dict: \nIn [400]: index = pd.date_range(\"1/1/2000\", periods=8)\n\nIn [401]: s = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n\nIn [402]: df = pd.DataFrame(np.random.randn(8, 3), index=index, columns=[\"A\", \"B\", \"C\"])\n\n# store.put('s', s) is an equivalent method\nIn [403]: store[\"s\"] = s\n\nIn [404]: store[\"df\"] = df\n\nIn [405]: store\nOut[405]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n  In a current or later Python session, you can retrieve stored objects: \n# store.get('df') is an equivalent method\nIn [406]: store[\"df\"]\nOut[406]: \n                   A         B         C\n2000-01-01  1.334065  0.521036  0.930384\n2000-01-02 -1.613932  1.088104 -0.632963\n2000-01-03 -0.585314 -0.275038 -0.937512\n2000-01-04  0.632369 -1.249657  0.975593\n2000-01-05  1.060617 -0.143682  0.218423\n2000-01-06  3.050329  1.317933 -0.963725\n2000-01-07 -0.539452 -0.771133  0.023751\n2000-01-08  0.649464 -1.736427  0.197288\n\n# dotted (attribute) access provides get as well\nIn [407]: store.df\nOut[407]: \n                   A         B         C\n2000-01-01  1.334065  0.521036  0.930384\n2000-01-02 -1.613932  1.088104 -0.632963\n2000-01-03 -0.585314 -0.275038 -0.937512\n2000-01-04  0.632369 -1.249657  0.975593\n2000-01-05  1.060617 -0.143682  0.218423\n2000-01-06  3.050329  1.317933 -0.963725\n2000-01-07 -0.539452 -0.771133  0.023751\n2000-01-08  0.649464 -1.736427  0.197288\n  Deletion of the object specified by the key: \n# store.remove('df') is an equivalent method\nIn [408]: del store[\"df\"]\n\nIn [409]: store\nOut[409]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n  Closing a Store and using a context manager: \nIn [410]: store.close()\n\nIn [411]: store\nOut[411]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n\nIn [412]: store.is_open\nOut[412]: False\n\n# Working with, and automatically closing the store using a context manager\nIn [413]: with pd.HDFStore(\"store.h5\") as store:\n   .....:     store.keys()\n   .....: \n   Read/write API HDFStore supports a top-level API using read_hdf for reading and to_hdf for writing, similar to how read_csv and to_csv work. \nIn [414]: df_tl = pd.DataFrame({\"A\": list(range(5)), \"B\": list(range(5))})\n\nIn [415]: df_tl.to_hdf(\"store_tl.h5\", \"table\", append=True)\n\nIn [416]: pd.read_hdf(\"store_tl.h5\", \"table\", where=[\"index>2\"])\nOut[416]: \n   A  B\n3  3  3\n4  4  4\n  HDFStore will by default not drop rows that are all missing. This behavior can be changed by setting dropna=True. \nIn [417]: df_with_missing = pd.DataFrame(\n   .....:     {\n   .....:         \"col1\": [0, np.nan, 2],\n   .....:         \"col2\": [1, np.nan, np.nan],\n   .....:     }\n   .....: )\n   .....: \n\nIn [418]: df_with_missing\nOut[418]: \n   col1  col2\n0   0.0   1.0\n1   NaN   NaN\n2   2.0   NaN\n\nIn [419]: df_with_missing.to_hdf(\"file.h5\", \"df_with_missing\", format=\"table\", mode=\"w\")\n\nIn [420]: pd.read_hdf(\"file.h5\", \"df_with_missing\")\nOut[420]: \n   col1  col2\n0   0.0   1.0\n1   NaN   NaN\n2   2.0   NaN\n\nIn [421]: df_with_missing.to_hdf(\n   .....:     \"file.h5\", \"df_with_missing\", format=\"table\", mode=\"w\", dropna=True\n   .....: )\n   .....: \n\nIn [422]: pd.read_hdf(\"file.h5\", \"df_with_missing\")\nOut[422]: \n   col1  col2\n0   0.0   1.0\n2   2.0   NaN\n    Fixed format The examples above show storing using put, which write the HDF5 to PyTables in a fixed array format, called the fixed format. These types of stores are not appendable once written (though you can simply remove them and rewrite). Nor are they queryable; they must be retrieved in their entirety. They also do not support dataframes with non-unique column names. The fixed format stores offer very fast writing and slightly faster reading than table stores. This format is specified by default when using put or to_hdf or by format='fixed' or format='f'.  Warning A fixed format will raise a TypeError if you try to retrieve using a where: \n>>> pd.DataFrame(np.random.randn(10, 2)).to_hdf(\"test_fixed.h5\", \"df\")\n>>> pd.read_hdf(\"test_fixed.h5\", \"df\", where=\"index>5\")\nTypeError: cannot pass a where specification when reading a fixed format.\n           this store must be selected in its entirety\n     Table format HDFStore supports another PyTables format on disk, the table format. Conceptually a table is shaped very much like a DataFrame, with rows and columns. A table may be appended to in the same or other sessions. In addition, delete and query type operations are supported. This format is specified by format='table' or format='t' to append or put or to_hdf. This format can be set as an option as well pd.set_option('io.hdf.default_format','table') to enable put/append/to_hdf to by default store in the table format. \nIn [423]: store = pd.HDFStore(\"store.h5\")\n\nIn [424]: df1 = df[0:4]\n\nIn [425]: df2 = df[4:]\n\n# append data (creates a table automatically)\nIn [426]: store.append(\"df\", df1)\n\nIn [427]: store.append(\"df\", df2)\n\nIn [428]: store\nOut[428]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n\n# select the entire object\nIn [429]: store.select(\"df\")\nOut[429]: \n                   A         B         C\n2000-01-01  1.334065  0.521036  0.930384\n2000-01-02 -1.613932  1.088104 -0.632963\n2000-01-03 -0.585314 -0.275038 -0.937512\n2000-01-04  0.632369 -1.249657  0.975593\n2000-01-05  1.060617 -0.143682  0.218423\n2000-01-06  3.050329  1.317933 -0.963725\n2000-01-07 -0.539452 -0.771133  0.023751\n2000-01-08  0.649464 -1.736427  0.197288\n\n# the type of stored data\nIn [430]: store.root.df._v_attrs.pandas_type\nOut[430]: 'frame_table'\n   Note You can also create a table by passing format='table' or format='t' to a put operation.    Hierarchical keys Keys to a store can be specified as a string. These can be in a hierarchical path-name like format (e.g. foo/bar/bah), which will generate a hierarchy of sub-stores (or Groups in PyTables parlance). Keys can be specified without the leading \u2018/\u2019 and are always absolute (e.g. \u2018foo\u2019 refers to \u2018/foo\u2019). Removal operations can remove everything in the sub-store and below, so be careful. \nIn [431]: store.put(\"foo/bar/bah\", df)\n\nIn [432]: store.append(\"food/orange\", df)\n\nIn [433]: store.append(\"food/apple\", df)\n\nIn [434]: store\nOut[434]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n\n# a list of keys are returned\nIn [435]: store.keys()\nOut[435]: ['/df', '/food/apple', '/food/orange', '/foo/bar/bah']\n\n# remove all nodes under this level\nIn [436]: store.remove(\"food\")\n\nIn [437]: store\nOut[437]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n  You can walk through the group hierarchy using the walk method which will yield a tuple for each group key along with the relative keys of its contents. \nIn [438]: for (path, subgroups, subkeys) in store.walk():\n   .....:     for subgroup in subgroups:\n   .....:         print(\"GROUP: {}/{}\".format(path, subgroup))\n   .....:     for subkey in subkeys:\n   .....:         key = \"/\".join([path, subkey])\n   .....:         print(\"KEY: {}\".format(key))\n   .....:         print(store.get(key))\n   .....: \nGROUP: /foo\nKEY: /df\n                   A         B         C\n2000-01-01  1.334065  0.521036  0.930384\n2000-01-02 -1.613932  1.088104 -0.632963\n2000-01-03 -0.585314 -0.275038 -0.937512\n2000-01-04  0.632369 -1.249657  0.975593\n2000-01-05  1.060617 -0.143682  0.218423\n2000-01-06  3.050329  1.317933 -0.963725\n2000-01-07 -0.539452 -0.771133  0.023751\n2000-01-08  0.649464 -1.736427  0.197288\nGROUP: /foo/bar\nKEY: /foo/bar/bah\n                   A         B         C\n2000-01-01  1.334065  0.521036  0.930384\n2000-01-02 -1.613932  1.088104 -0.632963\n2000-01-03 -0.585314 -0.275038 -0.937512\n2000-01-04  0.632369 -1.249657  0.975593\n2000-01-05  1.060617 -0.143682  0.218423\n2000-01-06  3.050329  1.317933 -0.963725\n2000-01-07 -0.539452 -0.771133  0.023751\n2000-01-08  0.649464 -1.736427  0.197288\n   Warning Hierarchical keys cannot be retrieved as dotted (attribute) access as described above for items stored under the root node. \nIn [8]: store.foo.bar.bah\nAttributeError: 'HDFStore' object has no attribute 'foo'\n\n# you can directly access the actual PyTables node but using the root node\nIn [9]: store.root.foo.bar.bah\nOut[9]:\n/foo/bar/bah (Group) ''\n  children := ['block0_items' (Array), 'block0_values' (Array), 'axis0' (Array), 'axis1' (Array)]\n  Instead, use explicit string based keys: \nIn [439]: store[\"foo/bar/bah\"]\nOut[439]: \n                   A         B         C\n2000-01-01  1.334065  0.521036  0.930384\n2000-01-02 -1.613932  1.088104 -0.632963\n2000-01-03 -0.585314 -0.275038 -0.937512\n2000-01-04  0.632369 -1.249657  0.975593\n2000-01-05  1.060617 -0.143682  0.218423\n2000-01-06  3.050329  1.317933 -0.963725\n2000-01-07 -0.539452 -0.771133  0.023751\n2000-01-08  0.649464 -1.736427  0.197288\n     Storing types  Storing mixed types in a table Storing mixed-dtype data is supported. Strings are stored as a fixed-width using the maximum size of the appended column. Subsequent attempts at appending longer strings will raise a ValueError. Passing min_itemsize={`values`: size} as a parameter to append will set a larger minimum for the string columns. Storing floats,\nstrings, ints, bools, datetime64 are currently supported. For string columns, passing nan_rep = 'nan' to append will change the default nan representation on disk (which converts to/from np.nan), this defaults to nan. \nIn [440]: df_mixed = pd.DataFrame(\n   .....:     {\n   .....:         \"A\": np.random.randn(8),\n   .....:         \"B\": np.random.randn(8),\n   .....:         \"C\": np.array(np.random.randn(8), dtype=\"float32\"),\n   .....:         \"string\": \"string\",\n   .....:         \"int\": 1,\n   .....:         \"bool\": True,\n   .....:         \"datetime64\": pd.Timestamp(\"20010102\"),\n   .....:     },\n   .....:     index=list(range(8)),\n   .....: )\n   .....: \n\nIn [441]: df_mixed.loc[df_mixed.index[3:5], [\"A\", \"B\", \"string\", \"datetime64\"]] = np.nan\n\nIn [442]: store.append(\"df_mixed\", df_mixed, min_itemsize={\"values\": 50})\n\nIn [443]: df_mixed1 = store.select(\"df_mixed\")\n\nIn [444]: df_mixed1\nOut[444]: \n          A         B         C  string  int  bool datetime64\n0 -0.116008  0.743946 -0.398501  string    1  True 2001-01-02\n1  0.592375 -0.533097 -0.677311  string    1  True 2001-01-02\n2  0.476481 -0.140850 -0.874991  string    1  True 2001-01-02\n3       NaN       NaN -1.167564     NaN    1  True        NaT\n4       NaN       NaN -0.593353     NaN    1  True        NaT\n5  0.852727  0.463819  0.146262  string    1  True 2001-01-02\n6 -1.177365  0.793644 -0.131959  string    1  True 2001-01-02\n7  1.236988  0.221252  0.089012  string    1  True 2001-01-02\n\nIn [445]: df_mixed1.dtypes.value_counts()\nOut[445]: \nfloat64           2\nfloat32           1\nobject            1\nint64             1\nbool              1\ndatetime64[ns]    1\ndtype: int64\n\n# we have provided a minimum string column size\nIn [446]: store.root.df_mixed.table\nOut[446]: \n/df_mixed/table (Table(8,)) ''\n  description := {\n  \"index\": Int64Col(shape=(), dflt=0, pos=0),\n  \"values_block_0\": Float64Col(shape=(2,), dflt=0.0, pos=1),\n  \"values_block_1\": Float32Col(shape=(1,), dflt=0.0, pos=2),\n  \"values_block_2\": StringCol(itemsize=50, shape=(1,), dflt=b'', pos=3),\n  \"values_block_3\": Int64Col(shape=(1,), dflt=0, pos=4),\n  \"values_block_4\": BoolCol(shape=(1,), dflt=False, pos=5),\n  \"values_block_5\": Int64Col(shape=(1,), dflt=0, pos=6)}\n  byteorder := 'little'\n  chunkshape := (689,)\n  autoindex := True\n  colindexes := {\n    \"index\": Index(6, medium, shuffle, zlib(1)).is_csi=False}\n    Storing MultiIndex DataFrames Storing MultiIndex DataFrames as tables is very similar to storing/selecting from homogeneous index DataFrames. \nIn [447]: index = pd.MultiIndex(\n   .....:     levels=[[\"foo\", \"bar\", \"baz\", \"qux\"], [\"one\", \"two\", \"three\"]],\n   .....:     codes=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3], [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],\n   .....:     names=[\"foo\", \"bar\"],\n   .....: )\n   .....: \n\nIn [448]: df_mi = pd.DataFrame(np.random.randn(10, 3), index=index, columns=[\"A\", \"B\", \"C\"])\n\nIn [449]: df_mi\nOut[449]: \n                  A         B         C\nfoo bar                                \nfoo one    0.667450  0.169405 -1.358046\n    two   -0.105563  0.492195  0.076693\n    three  0.213685 -0.285283 -1.210529\nbar one   -1.408386  0.941577 -0.342447\n    two    0.222031  0.052607  2.093214\nbaz two    1.064908  1.778161 -0.913867\n    three -0.030004 -0.399846 -1.234765\nqux one    0.081323 -0.268494  0.168016\n    two   -0.898283 -0.218499  1.408028\n    three -1.267828 -0.689263  0.520995\n\nIn [450]: store.append(\"df_mi\", df_mi)\n\nIn [451]: store.select(\"df_mi\")\nOut[451]: \n                  A         B         C\nfoo bar                                \nfoo one    0.667450  0.169405 -1.358046\n    two   -0.105563  0.492195  0.076693\n    three  0.213685 -0.285283 -1.210529\nbar one   -1.408386  0.941577 -0.342447\n    two    0.222031  0.052607  2.093214\nbaz two    1.064908  1.778161 -0.913867\n    three -0.030004 -0.399846 -1.234765\nqux one    0.081323 -0.268494  0.168016\n    two   -0.898283 -0.218499  1.408028\n    three -1.267828 -0.689263  0.520995\n\n# the levels are automatically included as data columns\nIn [452]: store.select(\"df_mi\", \"foo=bar\")\nOut[452]: \n                A         B         C\nfoo bar                              \nbar one -1.408386  0.941577 -0.342447\n    two  0.222031  0.052607  2.093214\n   Note The index keyword is reserved and cannot be use as a level name.     Querying  Querying a table select and delete operations have an optional criterion that can be specified to select/delete only a subset of the data. This allows one to have a very large on-disk table and retrieve only a portion of the data. A query is specified using the Term class under the hood, as a boolean expression.  index and columns are supported indexers of DataFrames. if data_columns are specified, these can be used as additional indexers. level name in a MultiIndex, with default name level_0, level_1, \u2026 if not provided.  Valid comparison operators are: =, ==, !=, >, >=, <, <= Valid boolean expressions are combined with:  | : or & : and ( and ) : for grouping  These rules are similar to how boolean expressions are used in pandas for indexing.  Note  = will be automatically expanded to the comparison operator == ~ is the not operator, but can only be used in very limited circumstances If a list/tuple of expressions is passed they will be combined via &   The following are valid expressions:  'index >= date' \"columns = ['A', 'D']\" \"columns in ['A', 'D']\" 'columns = A' 'columns == A' \"~(columns = ['A', 'B'])\" 'index > df.index[3] & string = \"bar\"' '(index > df.index[3] & index <= df.index[6]) | string = \"bar\"' \"ts >= Timestamp('2012-02-01')\" \"major_axis>=20130101\"  The indexers are on the left-hand side of the sub-expression: columns, major_axis, ts The right-hand side of the sub-expression (after a comparison operator) can be:  functions that will be evaluated, e.g. Timestamp('2012-02-01') strings, e.g. \"bar\" date-like, e.g. 20130101, or \"20130101\" lists, e.g. \"['A', 'B']\" variables that are defined in the local names space, e.g. date   Note Passing a string to a query by interpolating it into the query expression is not recommended. Simply assign the string of interest to a variable and use that variable in an expression. For example, do this \nstring = \"HolyMoly'\"\nstore.select(\"df\", \"index == string\")\n  instead of this \nstring = \"HolyMoly'\"\nstore.select('df', f'index == {string}')\n  The latter will not work and will raise a SyntaxError.Note that there\u2019s a single quote followed by a double quote in the string variable. If you must interpolate, use the '%r' format specifier \nstore.select(\"df\", \"index == %r\" % string)\n  which will quote string.  Here are some examples: \nIn [453]: dfq = pd.DataFrame(\n   .....:     np.random.randn(10, 4),\n   .....:     columns=list(\"ABCD\"),\n   .....:     index=pd.date_range(\"20130101\", periods=10),\n   .....: )\n   .....: \n\nIn [454]: store.append(\"dfq\", dfq, format=\"table\", data_columns=True)\n  Use boolean expressions, with in-line function evaluation. \nIn [455]: store.select(\"dfq\", \"index>pd.Timestamp('20130104') & columns=['A', 'B']\")\nOut[455]: \n                   A         B\n2013-01-05 -1.083889  0.811865\n2013-01-06 -0.402227  1.618922\n2013-01-07  0.948196  0.183573\n2013-01-08 -1.043530 -0.708145\n2013-01-09  0.813949  1.508891\n2013-01-10  1.176488 -1.246093\n  Use inline column reference. \nIn [456]: store.select(\"dfq\", where=\"A>0 or C>0\")\nOut[456]: \n                   A         B         C         D\n2013-01-01  0.620028  0.159416 -0.263043 -0.639244\n2013-01-04 -0.536722  1.005707  0.296917  0.139796\n2013-01-05 -1.083889  0.811865  1.648435 -0.164377\n2013-01-07  0.948196  0.183573  0.145277  0.308146\n2013-01-08 -1.043530 -0.708145  1.430905 -0.850136\n2013-01-09  0.813949  1.508891 -1.556154  0.187597\n2013-01-10  1.176488 -1.246093 -0.002726 -0.444249\n  The columns keyword can be supplied to select a list of columns to be returned, this is equivalent to passing a 'columns=list_of_columns_to_filter': \nIn [457]: store.select(\"df\", \"columns=['A', 'B']\")\nOut[457]: \n                   A         B\n2000-01-01  1.334065  0.521036\n2000-01-02 -1.613932  1.088104\n2000-01-03 -0.585314 -0.275038\n2000-01-04  0.632369 -1.249657\n2000-01-05  1.060617 -0.143682\n2000-01-06  3.050329  1.317933\n2000-01-07 -0.539452 -0.771133\n2000-01-08  0.649464 -1.736427\n  start and stop parameters can be specified to limit the total search space. These are in terms of the total number of rows in a table.  Note select will raise a ValueError if the query expression has an unknown variable reference. Usually this means that you are trying to select on a column that is not a data_column. select will raise a SyntaxError if the query expression is not valid.    Query timedelta64[ns] You can store and query using the timedelta64[ns] type. Terms can be specified in the format: <float>(<unit>), where float may be signed (and fractional), and unit can be D,s,ms,us,ns for the timedelta. Here\u2019s an example: \nIn [458]: from datetime import timedelta\n\nIn [459]: dftd = pd.DataFrame(\n   .....:     {\n   .....:         \"A\": pd.Timestamp(\"20130101\"),\n   .....:         \"B\": [\n   .....:             pd.Timestamp(\"20130101\") + timedelta(days=i, seconds=10)\n   .....:             for i in range(10)\n   .....:         ],\n   .....:     }\n   .....: )\n   .....: \n\nIn [460]: dftd[\"C\"] = dftd[\"A\"] - dftd[\"B\"]\n\nIn [461]: dftd\nOut[461]: \n           A                   B                  C\n0 2013-01-01 2013-01-01 00:00:10  -1 days +23:59:50\n1 2013-01-01 2013-01-02 00:00:10  -2 days +23:59:50\n2 2013-01-01 2013-01-03 00:00:10  -3 days +23:59:50\n3 2013-01-01 2013-01-04 00:00:10  -4 days +23:59:50\n4 2013-01-01 2013-01-05 00:00:10  -5 days +23:59:50\n5 2013-01-01 2013-01-06 00:00:10  -6 days +23:59:50\n6 2013-01-01 2013-01-07 00:00:10  -7 days +23:59:50\n7 2013-01-01 2013-01-08 00:00:10  -8 days +23:59:50\n8 2013-01-01 2013-01-09 00:00:10  -9 days +23:59:50\n9 2013-01-01 2013-01-10 00:00:10 -10 days +23:59:50\n\nIn [462]: store.append(\"dftd\", dftd, data_columns=True)\n\nIn [463]: store.select(\"dftd\", \"C<'-3.5D'\")\nOut[463]: \n           A                   B                  C\n4 2013-01-01 2013-01-05 00:00:10  -5 days +23:59:50\n5 2013-01-01 2013-01-06 00:00:10  -6 days +23:59:50\n6 2013-01-01 2013-01-07 00:00:10  -7 days +23:59:50\n7 2013-01-01 2013-01-08 00:00:10  -8 days +23:59:50\n8 2013-01-01 2013-01-09 00:00:10  -9 days +23:59:50\n9 2013-01-01 2013-01-10 00:00:10 -10 days +23:59:50\n    Query MultiIndex Selecting from a MultiIndex can be achieved by using the name of the level. \nIn [464]: df_mi.index.names\nOut[464]: FrozenList(['foo', 'bar'])\n\nIn [465]: store.select(\"df_mi\", \"foo=baz and bar=two\")\nOut[465]: \n                A         B         C\nfoo bar                              \nbaz two  1.064908  1.778161 -0.913867\n  If the MultiIndex levels names are None, the levels are automatically made available via the level_n keyword with n the level of the MultiIndex you want to select from. \nIn [466]: index = pd.MultiIndex(\n   .....:     levels=[[\"foo\", \"bar\", \"baz\", \"qux\"], [\"one\", \"two\", \"three\"]],\n   .....:     codes=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3], [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],\n   .....: )\n   .....: \n\nIn [467]: df_mi_2 = pd.DataFrame(np.random.randn(10, 3), index=index, columns=[\"A\", \"B\", \"C\"])\n\nIn [468]: df_mi_2\nOut[468]: \n                  A         B         C\nfoo one    0.856838  1.491776  0.001283\n    two    0.701816 -1.097917  0.102588\n    three  0.661740  0.443531  0.559313\nbar one   -0.459055 -1.222598 -0.455304\n    two   -0.781163  0.826204 -0.530057\nbaz two    0.296135  1.366810  1.073372\n    three -0.994957  0.755314  2.119746\nqux one   -2.628174 -0.089460 -0.133636\n    two    0.337920 -0.634027  0.421107\n    three  0.604303  1.053434  1.109090\n\nIn [469]: store.append(\"df_mi_2\", df_mi_2)\n\n# the levels are automatically included as data columns with keyword level_n\nIn [470]: store.select(\"df_mi_2\", \"level_0=foo and level_1=two\")\nOut[470]: \n                A         B         C\nfoo two  0.701816 -1.097917  0.102588\n    Indexing You can create/modify an index for a table with create_table_index after data is already in the table (after and append/put operation). Creating a table index is highly encouraged. This will speed your queries a great deal when you use a select with the indexed dimension as the where.  Note Indexes are automagically created on the indexables and any data columns you specify. This behavior can be turned off by passing index=False to append.  \n# we have automagically already created an index (in the first section)\nIn [471]: i = store.root.df.table.cols.index.index\n\nIn [472]: i.optlevel, i.kind\nOut[472]: (6, 'medium')\n\n# change an index by passing new parameters\nIn [473]: store.create_table_index(\"df\", optlevel=9, kind=\"full\")\n\nIn [474]: i = store.root.df.table.cols.index.index\n\nIn [475]: i.optlevel, i.kind\nOut[475]: (9, 'full')\n  Oftentimes when appending large amounts of data to a store, it is useful to turn off index creation for each append, then recreate at the end. \nIn [476]: df_1 = pd.DataFrame(np.random.randn(10, 2), columns=list(\"AB\"))\n\nIn [477]: df_2 = pd.DataFrame(np.random.randn(10, 2), columns=list(\"AB\"))\n\nIn [478]: st = pd.HDFStore(\"appends.h5\", mode=\"w\")\n\nIn [479]: st.append(\"df\", df_1, data_columns=[\"B\"], index=False)\n\nIn [480]: st.append(\"df\", df_2, data_columns=[\"B\"], index=False)\n\nIn [481]: st.get_storer(\"df\").table\nOut[481]: \n/df/table (Table(20,)) ''\n  description := {\n  \"index\": Int64Col(shape=(), dflt=0, pos=0),\n  \"values_block_0\": Float64Col(shape=(1,), dflt=0.0, pos=1),\n  \"B\": Float64Col(shape=(), dflt=0.0, pos=2)}\n  byteorder := 'little'\n  chunkshape := (2730,)\n  Then create the index when finished appending. \nIn [482]: st.create_table_index(\"df\", columns=[\"B\"], optlevel=9, kind=\"full\")\n\nIn [483]: st.get_storer(\"df\").table\nOut[483]: \n/df/table (Table(20,)) ''\n  description := {\n  \"index\": Int64Col(shape=(), dflt=0, pos=0),\n  \"values_block_0\": Float64Col(shape=(1,), dflt=0.0, pos=1),\n  \"B\": Float64Col(shape=(), dflt=0.0, pos=2)}\n  byteorder := 'little'\n  chunkshape := (2730,)\n  autoindex := True\n  colindexes := {\n    \"B\": Index(9, full, shuffle, zlib(1)).is_csi=True}\n\nIn [484]: st.close()\n  See here for how to create a completely-sorted-index (CSI) on an existing store.   Query via data columns You can designate (and index) certain columns that you want to be able to perform queries (other than the indexable columns, which you can always query). For instance say you want to perform this common operation, on-disk, and return just the frame that matches this query. You can specify data_columns = True to force all columns to be data_columns. \nIn [485]: df_dc = df.copy()\n\nIn [486]: df_dc[\"string\"] = \"foo\"\n\nIn [487]: df_dc.loc[df_dc.index[4:6], \"string\"] = np.nan\n\nIn [488]: df_dc.loc[df_dc.index[7:9], \"string\"] = \"bar\"\n\nIn [489]: df_dc[\"string2\"] = \"cool\"\n\nIn [490]: df_dc.loc[df_dc.index[1:3], [\"B\", \"C\"]] = 1.0\n\nIn [491]: df_dc\nOut[491]: \n                   A         B         C string string2\n2000-01-01  1.334065  0.521036  0.930384    foo    cool\n2000-01-02 -1.613932  1.000000  1.000000    foo    cool\n2000-01-03 -0.585314  1.000000  1.000000    foo    cool\n2000-01-04  0.632369 -1.249657  0.975593    foo    cool\n2000-01-05  1.060617 -0.143682  0.218423    NaN    cool\n2000-01-06  3.050329  1.317933 -0.963725    NaN    cool\n2000-01-07 -0.539452 -0.771133  0.023751    foo    cool\n2000-01-08  0.649464 -1.736427  0.197288    bar    cool\n\n# on-disk operations\nIn [492]: store.append(\"df_dc\", df_dc, data_columns=[\"B\", \"C\", \"string\", \"string2\"])\n\nIn [493]: store.select(\"df_dc\", where=\"B > 0\")\nOut[493]: \n                   A         B         C string string2\n2000-01-01  1.334065  0.521036  0.930384    foo    cool\n2000-01-02 -1.613932  1.000000  1.000000    foo    cool\n2000-01-03 -0.585314  1.000000  1.000000    foo    cool\n2000-01-06  3.050329  1.317933 -0.963725    NaN    cool\n\n# getting creative\nIn [494]: store.select(\"df_dc\", \"B > 0 & C > 0 & string == foo\")\nOut[494]: \n                   A         B         C string string2\n2000-01-01  1.334065  0.521036  0.930384    foo    cool\n2000-01-02 -1.613932  1.000000  1.000000    foo    cool\n2000-01-03 -0.585314  1.000000  1.000000    foo    cool\n\n# this is in-memory version of this type of selection\nIn [495]: df_dc[(df_dc.B > 0) & (df_dc.C > 0) & (df_dc.string == \"foo\")]\nOut[495]: \n                   A         B         C string string2\n2000-01-01  1.334065  0.521036  0.930384    foo    cool\n2000-01-02 -1.613932  1.000000  1.000000    foo    cool\n2000-01-03 -0.585314  1.000000  1.000000    foo    cool\n\n# we have automagically created this index and the B/C/string/string2\n# columns are stored separately as ``PyTables`` columns\nIn [496]: store.root.df_dc.table\nOut[496]: \n/df_dc/table (Table(8,)) ''\n  description := {\n  \"index\": Int64Col(shape=(), dflt=0, pos=0),\n  \"values_block_0\": Float64Col(shape=(1,), dflt=0.0, pos=1),\n  \"B\": Float64Col(shape=(), dflt=0.0, pos=2),\n  \"C\": Float64Col(shape=(), dflt=0.0, pos=3),\n  \"string\": StringCol(itemsize=3, shape=(), dflt=b'', pos=4),\n  \"string2\": StringCol(itemsize=4, shape=(), dflt=b'', pos=5)}\n  byteorder := 'little'\n  chunkshape := (1680,)\n  autoindex := True\n  colindexes := {\n    \"index\": Index(6, medium, shuffle, zlib(1)).is_csi=False,\n    \"B\": Index(6, medium, shuffle, zlib(1)).is_csi=False,\n    \"C\": Index(6, medium, shuffle, zlib(1)).is_csi=False,\n    \"string\": Index(6, medium, shuffle, zlib(1)).is_csi=False,\n    \"string2\": Index(6, medium, shuffle, zlib(1)).is_csi=False}\n  There is some performance degradation by making lots of columns into data columns, so it is up to the user to designate these. In addition, you cannot change data columns (nor indexables) after the first append/put operation (Of course you can simply read in the data and create a new table!).   Iterator You can pass iterator=True or chunksize=number_in_a_chunk to select and select_as_multiple to return an iterator on the results. The default is 50,000 rows returned in a chunk. \nIn [497]: for df in store.select(\"df\", chunksize=3):\n   .....:     print(df)\n   .....: \n                   A         B         C\n2000-01-01  1.334065  0.521036  0.930384\n2000-01-02 -1.613932  1.088104 -0.632963\n2000-01-03 -0.585314 -0.275038 -0.937512\n                   A         B         C\n2000-01-04  0.632369 -1.249657  0.975593\n2000-01-05  1.060617 -0.143682  0.218423\n2000-01-06  3.050329  1.317933 -0.963725\n                   A         B         C\n2000-01-07 -0.539452 -0.771133  0.023751\n2000-01-08  0.649464 -1.736427  0.197288\n   Note You can also use the iterator with read_hdf which will open, then automatically close the store when finished iterating. \nfor df in pd.read_hdf(\"store.h5\", \"df\", chunksize=3):\n    print(df)\n   Note, that the chunksize keyword applies to the source rows. So if you are doing a query, then the chunksize will subdivide the total rows in the table and the query applied, returning an iterator on potentially unequal sized chunks. Here is a recipe for generating a query and using it to create equal sized return chunks. \nIn [498]: dfeq = pd.DataFrame({\"number\": np.arange(1, 11)})\n\nIn [499]: dfeq\nOut[499]: \n   number\n0       1\n1       2\n2       3\n3       4\n4       5\n5       6\n6       7\n7       8\n8       9\n9      10\n\nIn [500]: store.append(\"dfeq\", dfeq, data_columns=[\"number\"])\n\nIn [501]: def chunks(l, n):\n   .....:     return [l[i: i + n] for i in range(0, len(l), n)]\n   .....: \n\nIn [502]: evens = [2, 4, 6, 8, 10]\n\nIn [503]: coordinates = store.select_as_coordinates(\"dfeq\", \"number=evens\")\n\nIn [504]: for c in chunks(coordinates, 2):\n   .....:     print(store.select(\"dfeq\", where=c))\n   .....: \n   number\n1       2\n3       4\n   number\n5       6\n7       8\n   number\n9      10\n    Advanced queries  Select a single column To retrieve a single indexable or data column, use the method select_column. This will, for example, enable you to get the index very quickly. These return a Series of the result, indexed by the row number. These do not currently accept the where selector. \nIn [505]: store.select_column(\"df_dc\", \"index\")\nOut[505]: \n0   2000-01-01\n1   2000-01-02\n2   2000-01-03\n3   2000-01-04\n4   2000-01-05\n5   2000-01-06\n6   2000-01-07\n7   2000-01-08\nName: index, dtype: datetime64[ns]\n\nIn [506]: store.select_column(\"df_dc\", \"string\")\nOut[506]: \n0    foo\n1    foo\n2    foo\n3    foo\n4    NaN\n5    NaN\n6    foo\n7    bar\nName: string, dtype: object\n    Selecting coordinates Sometimes you want to get the coordinates (a.k.a the index locations) of your query. This returns an Int64Index of the resulting locations. These coordinates can also be passed to subsequent where operations. \nIn [507]: df_coord = pd.DataFrame(\n   .....:     np.random.randn(1000, 2), index=pd.date_range(\"20000101\", periods=1000)\n   .....: )\n   .....: \n\nIn [508]: store.append(\"df_coord\", df_coord)\n\nIn [509]: c = store.select_as_coordinates(\"df_coord\", \"index > 20020101\")\n\nIn [510]: c\nOut[510]: \nInt64Index([732, 733, 734, 735, 736, 737, 738, 739, 740, 741,\n            ...\n            990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n           dtype='int64', length=268)\n\nIn [511]: store.select(\"df_coord\", where=c)\nOut[511]: \n                   0         1\n2002-01-02 -0.165548  0.646989\n2002-01-03  0.782753 -0.123409\n2002-01-04 -0.391932 -0.740915\n2002-01-05  1.211070 -0.668715\n2002-01-06  0.341987 -0.685867\n...              ...       ...\n2002-09-22  1.788110 -0.405908\n2002-09-23 -0.801912  0.768460\n2002-09-24  0.466284 -0.457411\n2002-09-25 -0.364060  0.785367\n2002-09-26 -1.463093  1.187315\n\n[268 rows x 2 columns]\n    Selecting using a where mask Sometime your query can involve creating a list of rows to select. Usually this mask would be a resulting index from an indexing operation. This example selects the months of a datetimeindex which are 5. \nIn [512]: df_mask = pd.DataFrame(\n   .....:     np.random.randn(1000, 2), index=pd.date_range(\"20000101\", periods=1000)\n   .....: )\n   .....: \n\nIn [513]: store.append(\"df_mask\", df_mask)\n\nIn [514]: c = store.select_column(\"df_mask\", \"index\")\n\nIn [515]: where = c[pd.DatetimeIndex(c).month == 5].index\n\nIn [516]: store.select(\"df_mask\", where=where)\nOut[516]: \n                   0         1\n2000-05-01  1.735883 -2.615261\n2000-05-02  0.422173  2.425154\n2000-05-03  0.632453 -0.165640\n2000-05-04 -1.017207 -0.005696\n2000-05-05  0.299606  0.070606\n...              ...       ...\n2002-05-27  0.234503  1.199126\n2002-05-28 -3.021833 -1.016828\n2002-05-29  0.522794  0.063465\n2002-05-30 -1.653736  0.031709\n2002-05-31 -0.968402 -0.393583\n\n[93 rows x 2 columns]\n    Storer object If you want to inspect the stored object, retrieve via get_storer. You could use this programmatically to say get the number of rows in an object. \nIn [517]: store.get_storer(\"df_dc\").nrows\nOut[517]: 8\n     Multiple table queries The methods append_to_multiple and select_as_multiple can perform appending/selecting from multiple tables at once. The idea is to have one table (call it the selector table) that you index most/all of the columns, and perform your queries. The other table(s) are data tables with an index matching the selector table\u2019s index. You can then perform a very fast query on the selector table, yet get lots of data back. This method is similar to having a very wide table, but enables more efficient queries. The append_to_multiple method splits a given single DataFrame into multiple tables according to d, a dictionary that maps the table names to a list of \u2018columns\u2019 you want in that table. If None is used in place of a list, that table will have the remaining unspecified columns of the given DataFrame. The argument selector defines which table is the selector table (which you can make queries from). The argument dropna will drop rows from the input DataFrame to ensure tables are synchronized. This means that if a row for one of the tables being written to is entirely np.NaN, that row will be dropped from all tables. If dropna is False, THE USER IS RESPONSIBLE FOR SYNCHRONIZING THE TABLES. Remember that entirely np.Nan rows are not written to the HDFStore, so if you choose to call dropna=False, some tables may have more rows than others, and therefore select_as_multiple may not work or it may return unexpected results. \nIn [518]: df_mt = pd.DataFrame(\n   .....:     np.random.randn(8, 6),\n   .....:     index=pd.date_range(\"1/1/2000\", periods=8),\n   .....:     columns=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"],\n   .....: )\n   .....: \n\nIn [519]: df_mt[\"foo\"] = \"bar\"\n\nIn [520]: df_mt.loc[df_mt.index[1], (\"A\", \"B\")] = np.nan\n\n# you can also create the tables individually\nIn [521]: store.append_to_multiple(\n   .....:     {\"df1_mt\": [\"A\", \"B\"], \"df2_mt\": None}, df_mt, selector=\"df1_mt\"\n   .....: )\n   .....: \n\nIn [522]: store\nOut[522]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n\n# individual tables were created\nIn [523]: store.select(\"df1_mt\")\nOut[523]: \n                   A         B\n2000-01-01  1.251079 -0.362628\n2000-01-02       NaN       NaN\n2000-01-03  0.719421 -0.448886\n2000-01-04  1.140998 -0.877922\n2000-01-05  1.043605  1.798494\n2000-01-06 -0.467812 -0.027965\n2000-01-07  0.150568  0.754820\n2000-01-08 -0.596306 -0.910022\n\nIn [524]: store.select(\"df2_mt\")\nOut[524]: \n                   C         D         E         F  foo\n2000-01-01  1.602451 -0.221229  0.712403  0.465927  bar\n2000-01-02 -0.525571  0.851566 -0.681308 -0.549386  bar\n2000-01-03 -0.044171  1.396628  1.041242 -1.588171  bar\n2000-01-04  0.463351 -0.861042 -2.192841 -1.025263  bar\n2000-01-05 -1.954845 -1.712882 -0.204377 -1.608953  bar\n2000-01-06  1.601542 -0.417884 -2.757922 -0.307713  bar\n2000-01-07 -1.935461  1.007668  0.079529 -1.459471  bar\n2000-01-08 -1.057072 -0.864360 -1.124870  1.732966  bar\n\n# as a multiple\nIn [525]: store.select_as_multiple(\n   .....:     [\"df1_mt\", \"df2_mt\"],\n   .....:     where=[\"A>0\", \"B>0\"],\n   .....:     selector=\"df1_mt\",\n   .....: )\n   .....: \nOut[525]: \n                   A         B         C         D         E         F  foo\n2000-01-05  1.043605  1.798494 -1.954845 -1.712882 -0.204377 -1.608953  bar\n2000-01-07  0.150568  0.754820 -1.935461  1.007668  0.079529 -1.459471  bar\n     Delete from a table You can delete from a table selectively by specifying a where. In deleting rows, it is important to understand the PyTables deletes rows by erasing the rows, then moving the following data. Thus deleting can potentially be a very expensive operation depending on the orientation of your data. To get optimal performance, it\u2019s worthwhile to have the dimension you are deleting be the first of the indexables. Data is ordered (on the disk) in terms of the indexables. Here\u2019s a simple use case. You store panel-type data, with dates in the major_axis and ids in the minor_axis. The data is then interleaved like this:  \n date_1\n\n id_1 id_2 . id_n     \n date_2\n\n id_1 . id_n      It should be clear that a delete operation on the major_axis will be fairly quick, as one chunk is removed, then the following data moved. On the other hand a delete operation on the minor_axis will be very expensive. In this case it would almost certainly be faster to rewrite the table using a where that selects all but the missing data.  Warning Please note that HDF5 DOES NOT RECLAIM SPACE in the h5 files automatically. Thus, repeatedly deleting (or removing nodes) and adding again, WILL TEND TO INCREASE THE FILE SIZE. To repack and clean the file, use ptrepack.    Notes & caveats  Compression PyTables allows the stored data to be compressed. This applies to all kinds of stores, not just tables. Two parameters are used to control compression: complevel and complib.  complevel specifies if and how hard data is to be compressed. complevel=0 and complevel=None disables compression and 0<complevel<10 enables compression. \ncomplib specifies which compression library to use. If nothing is specified the default library zlib is used. A compression library usually optimizes for either good compression rates or speed and the results will depend on the type of data. Which type of compression to choose depends on your specific needs and data. The list of supported compression libraries:  zlib: The default compression library. A classic in terms of compression, achieves good compression rates but is somewhat slow. lzo: Fast compression and decompression. bzip2: Good compression rates. \nblosc: Fast compression and decompression. Support for alternative blosc compressors:  blosc:blosclz This is the default compressor for blosc blosc:lz4: A compact, very popular and fast compressor. blosc:lz4hc: A tweaked version of LZ4, produces better compression ratios at the expense of speed. blosc:snappy: A popular compressor used in many places. blosc:zlib: A classic; somewhat slower than the previous ones, but achieving better compression ratios. blosc:zstd: An extremely well balanced codec; it provides the best compression ratios among the others above, and at reasonably fast speed.    If complib is defined as something other than the listed libraries a ValueError exception is issued.    Note If the library specified with the complib option is missing on your platform, compression defaults to zlib without further ado.  Enable compression for all objects within the file: \nstore_compressed = pd.HDFStore(\n    \"store_compressed.h5\", complevel=9, complib=\"blosc:blosclz\"\n)\n  Or on-the-fly compression (this only applies to tables) in stores where compression is not enabled: \nstore.append(\"df\", df, complib=\"zlib\", complevel=5)\n    ptrepack PyTables offers better write performance when tables are compressed after they are written, as opposed to turning on compression at the very beginning. You can use the supplied PyTables utility ptrepack. In addition, ptrepack can change compression levels after the fact. \nptrepack --chunkshape=auto --propindexes --complevel=9 --complib=blosc in.h5 out.h5\n  Furthermore ptrepack in.h5 out.h5 will repack the file to allow you to reuse previously deleted space. Alternatively, one can simply remove the file and write again, or use the copy method.   Caveats  Warning HDFStore is not-threadsafe for writing. The underlying PyTables only supports concurrent reads (via threading or processes). If you need reading and writing at the same time, you need to serialize these operations in a single thread in a single process. You will corrupt your data otherwise. See the (GH2397) for more information.   If you use locks to manage write access between multiple processes, you may want to use fsync() before releasing write locks. For convenience you can use store.flush(fsync=True) to do this for you. Once a table is created columns (DataFrame) are fixed; only exactly the same columns can be appended Be aware that timezones (e.g., pytz.timezone('US/Eastern')) are not necessarily equal across timezone versions. So if data is localized to a specific timezone in the HDFStore using one version of a timezone library and that data is updated with another version, the data will be converted to UTC since these timezones are not considered equal. Either use the same version of timezone library or use tz_convert with the updated timezone definition.   Warning PyTables will show a NaturalNameWarning if a column name cannot be used as an attribute selector. Natural identifiers contain only letters, numbers, and underscores, and may not begin with a number. Other identifiers cannot be used in a where clause and are generally a bad idea.     DataTypes HDFStore will map an object dtype to the PyTables underlying dtype. This means the following types are known to work:       \nType Represents missing values    \nfloating : float64, float32, float16 np.nan  \ninteger : int64, int32, int8, uint64,uint32, uint8   \nboolean   \ndatetime64[ns] NaT  \ntimedelta64[ns] NaT  \ncategorical : see the section below   \nobject : strings np.nan    unicode columns are not supported, and WILL FAIL.  Categorical data You can write data that contains category dtypes to a HDFStore. Queries work the same as if it was an object array. However, the category dtyped data is stored in a more efficient manner. \nIn [526]: dfcat = pd.DataFrame(\n   .....:     {\"A\": pd.Series(list(\"aabbcdba\")).astype(\"category\"), \"B\": np.random.randn(8)}\n   .....: )\n   .....: \n\nIn [527]: dfcat\nOut[527]: \n   A         B\n0  a  0.477849\n1  a  0.283128\n2  b -2.045700\n3  b -0.338206\n4  c -0.423113\n5  d  2.314361\n6  b -0.033100\n7  a -0.965461\n\nIn [528]: dfcat.dtypes\nOut[528]: \nA    category\nB     float64\ndtype: object\n\nIn [529]: cstore = pd.HDFStore(\"cats.h5\", mode=\"w\")\n\nIn [530]: cstore.append(\"dfcat\", dfcat, format=\"table\", data_columns=[\"A\"])\n\nIn [531]: result = cstore.select(\"dfcat\", where=\"A in ['b', 'c']\")\n\nIn [532]: result\nOut[532]: \n   A         B\n2  b -2.045700\n3  b -0.338206\n4  c -0.423113\n6  b -0.033100\n\nIn [533]: result.dtypes\nOut[533]: \nA    category\nB     float64\ndtype: object\n    String columns min_itemsize The underlying implementation of HDFStore uses a fixed column width (itemsize) for string columns. A string column itemsize is calculated as the maximum of the length of data (for that column) that is passed to the HDFStore, in the first append. Subsequent appends, may introduce a string for a column larger than the column can hold, an Exception will be raised (otherwise you could have a silent truncation of these columns, leading to loss of information). In the future we may relax this and allow a user-specified truncation to occur. Pass min_itemsize on the first table creation to a-priori specify the minimum length of a particular string column. min_itemsize can be an integer, or a dict mapping a column name to an integer. You can pass values as a key to allow all indexables or data_columns to have this min_itemsize. Passing a min_itemsize dict will cause all passed columns to be created as data_columns automatically.  Note If you are not passing any data_columns, then the min_itemsize will be the maximum of the length of any string passed  \nIn [534]: dfs = pd.DataFrame({\"A\": \"foo\", \"B\": \"bar\"}, index=list(range(5)))\n\nIn [535]: dfs\nOut[535]: \n     A    B\n0  foo  bar\n1  foo  bar\n2  foo  bar\n3  foo  bar\n4  foo  bar\n\n# A and B have a size of 30\nIn [536]: store.append(\"dfs\", dfs, min_itemsize=30)\n\nIn [537]: store.get_storer(\"dfs\").table\nOut[537]: \n/dfs/table (Table(5,)) ''\n  description := {\n  \"index\": Int64Col(shape=(), dflt=0, pos=0),\n  \"values_block_0\": StringCol(itemsize=30, shape=(2,), dflt=b'', pos=1)}\n  byteorder := 'little'\n  chunkshape := (963,)\n  autoindex := True\n  colindexes := {\n    \"index\": Index(6, medium, shuffle, zlib(1)).is_csi=False}\n\n# A is created as a data_column with a size of 30\n# B is size is calculated\nIn [538]: store.append(\"dfs2\", dfs, min_itemsize={\"A\": 30})\n\nIn [539]: store.get_storer(\"dfs2\").table\nOut[539]: \n/dfs2/table (Table(5,)) ''\n  description := {\n  \"index\": Int64Col(shape=(), dflt=0, pos=0),\n  \"values_block_0\": StringCol(itemsize=3, shape=(1,), dflt=b'', pos=1),\n  \"A\": StringCol(itemsize=30, shape=(), dflt=b'', pos=2)}\n  byteorder := 'little'\n  chunkshape := (1598,)\n  autoindex := True\n  colindexes := {\n    \"index\": Index(6, medium, shuffle, zlib(1)).is_csi=False,\n    \"A\": Index(6, medium, shuffle, zlib(1)).is_csi=False}\n  nan_rep String columns will serialize a np.nan (a missing value) with the nan_rep string representation. This defaults to the string value nan. You could inadvertently turn an actual nan value into a missing value. \nIn [540]: dfss = pd.DataFrame({\"A\": [\"foo\", \"bar\", \"nan\"]})\n\nIn [541]: dfss\nOut[541]: \n     A\n0  foo\n1  bar\n2  nan\n\nIn [542]: store.append(\"dfss\", dfss)\n\nIn [543]: store.select(\"dfss\")\nOut[543]: \n     A\n0  foo\n1  bar\n2  NaN\n\n# here you need to specify a different nan rep\nIn [544]: store.append(\"dfss2\", dfss, nan_rep=\"_nan_\")\n\nIn [545]: store.select(\"dfss2\")\nOut[545]: \n     A\n0  foo\n1  bar\n2  nan\n     External compatibility HDFStore writes table format objects in specific formats suitable for producing loss-less round trips to pandas objects. For external compatibility, HDFStore can read native PyTables format tables. It is possible to write an HDFStore object that can easily be imported into R using the rhdf5 library (Package website). Create a table format store like this: \nIn [546]: df_for_r = pd.DataFrame(\n   .....:     {\n   .....:         \"first\": np.random.rand(100),\n   .....:         \"second\": np.random.rand(100),\n   .....:         \"class\": np.random.randint(0, 2, (100,)),\n   .....:     },\n   .....:     index=range(100),\n   .....: )\n   .....: \n\nIn [547]: df_for_r.head()\nOut[547]: \n      first    second  class\n0  0.864919  0.852910      0\n1  0.030579  0.412962      1\n2  0.015226  0.978410      0\n3  0.498512  0.686761      0\n4  0.232163  0.328185      1\n\nIn [548]: store_export = pd.HDFStore(\"export.h5\")\n\nIn [549]: store_export.append(\"df_for_r\", df_for_r, data_columns=df_dc.columns)\n\nIn [550]: store_export\nOut[550]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: export.h5\n  In R this file can be read into a data.frame object using the rhdf5 library. The following example function reads the corresponding column names and data values from the values and assembles them into a data.frame: \n# Load values and column names for all datasets from corresponding nodes and\n# insert them into one data.frame object.\n\nlibrary(rhdf5)\n\nloadhdf5data <- function(h5File) {\n\nlisting <- h5ls(h5File)\n# Find all data nodes, values are stored in *_values and corresponding column\n# titles in *_items\ndata_nodes <- grep(\"_values\", listing$name)\nname_nodes <- grep(\"_items\", listing$name)\ndata_paths = paste(listing$group[data_nodes], listing$name[data_nodes], sep = \"/\")\nname_paths = paste(listing$group[name_nodes], listing$name[name_nodes], sep = \"/\")\ncolumns = list()\nfor (idx in seq(data_paths)) {\n  # NOTE: matrices returned by h5read have to be transposed to obtain\n  # required Fortran order!\n  data <- data.frame(t(h5read(h5File, data_paths[idx])))\n  names <- t(h5read(h5File, name_paths[idx]))\n  entry <- data.frame(data)\n  colnames(entry) <- names\n  columns <- append(columns, entry)\n}\n\ndata <- data.frame(columns)\n\nreturn(data)\n}\n  Now you can import the DataFrame into R: \n> data = loadhdf5data(\"transfer.hdf5\")\n> head(data)\n         first    second class\n1 0.4170220047 0.3266449     0\n2 0.7203244934 0.5270581     0\n3 0.0001143748 0.8859421     1\n4 0.3023325726 0.3572698     1\n5 0.1467558908 0.9085352     1\n6 0.0923385948 0.6233601     1\n   Note The R function lists the entire HDF5 file\u2019s contents and assembles the data.frame object from all matching nodes, so use this only as a starting point if you have stored multiple DataFrame objects to a single HDF5 file.    Performance  tables format come with a writing performance penalty as compared to fixed stores. The benefit is the ability to append/delete and query (potentially very large amounts of data). Write times are generally longer as compared with regular stores. Query times can be quite fast, especially on an indexed axis. You can pass chunksize=<int> to append, specifying the write chunksize (default is 50000). This will significantly lower your memory usage on writing. You can pass expectedrows=<int> to the first append, to set the TOTAL number of rows that PyTables will expect. This will optimize read/write performance. Duplicate rows can be written to tables, but are filtered out in selection (with the last items being selected; thus a table is unique on major, minor pairs) A PerformanceWarning will be raised if you are attempting to store types that will be pickled by PyTables (rather than stored as endemic types). See Here for more information and some solutions.     Feather Feather provides binary columnar serialization for data frames. It is designed to make reading and writing data frames efficient, and to make sharing data across data analysis languages easy. Feather is designed to faithfully serialize and de-serialize DataFrames, supporting all of the pandas dtypes, including extension dtypes such as categorical and datetime with tz. Several caveats:  The format will NOT write an Index, or MultiIndex for the DataFrame and will raise an error if a non-default one is provided. You can .reset_index() to store the index or .reset_index(drop=True) to ignore it. Duplicate column names and non-string columns names are not supported Actual Python objects in object dtype columns are not supported. These will raise a helpful error message on an attempt at serialization.  See the Full Documentation. \nIn [551]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"a\": list(\"abc\"),\n   .....:         \"b\": list(range(1, 4)),\n   .....:         \"c\": np.arange(3, 6).astype(\"u1\"),\n   .....:         \"d\": np.arange(4.0, 7.0, dtype=\"float64\"),\n   .....:         \"e\": [True, False, True],\n   .....:         \"f\": pd.Categorical(list(\"abc\")),\n   .....:         \"g\": pd.date_range(\"20130101\", periods=3),\n   .....:         \"h\": pd.date_range(\"20130101\", periods=3, tz=\"US/Eastern\"),\n   .....:         \"i\": pd.date_range(\"20130101\", periods=3, freq=\"ns\"),\n   .....:     }\n   .....: )\n   .....: \n\nIn [552]: df\nOut[552]: \n   a  b  c    d      e  f          g                         h                             i\n0  a  1  3  4.0   True  a 2013-01-01 2013-01-01 00:00:00-05:00 2013-01-01 00:00:00.000000000\n1  b  2  4  5.0  False  b 2013-01-02 2013-01-02 00:00:00-05:00 2013-01-01 00:00:00.000000001\n2  c  3  5  6.0   True  c 2013-01-03 2013-01-03 00:00:00-05:00 2013-01-01 00:00:00.000000002\n\nIn [553]: df.dtypes\nOut[553]: \na                        object\nb                         int64\nc                         uint8\nd                       float64\ne                          bool\nf                      category\ng                datetime64[ns]\nh    datetime64[ns, US/Eastern]\ni                datetime64[ns]\ndtype: object\n  Write to a feather file. \nIn [554]: df.to_feather(\"example.feather\")\n  Read from a feather file. \nIn [555]: result = pd.read_feather(\"example.feather\")\n\nIn [556]: result\nOut[556]: \n   a  b  c    d      e  f          g                         h                             i\n0  a  1  3  4.0   True  a 2013-01-01 2013-01-01 00:00:00-05:00 2013-01-01 00:00:00.000000000\n1  b  2  4  5.0  False  b 2013-01-02 2013-01-02 00:00:00-05:00 2013-01-01 00:00:00.000000001\n2  c  3  5  6.0   True  c 2013-01-03 2013-01-03 00:00:00-05:00 2013-01-01 00:00:00.000000002\n\n# we preserve dtypes\nIn [557]: result.dtypes\nOut[557]: \na                        object\nb                         int64\nc                         uint8\nd                       float64\ne                          bool\nf                      category\ng                datetime64[ns]\nh    datetime64[ns, US/Eastern]\ni                datetime64[ns]\ndtype: object\n    Parquet Apache Parquet provides a partitioned binary columnar serialization for data frames. It is designed to make reading and writing data frames efficient, and to make sharing data across data analysis languages easy. Parquet can use a variety of compression techniques to shrink the file size as much as possible while still maintaining good read performance. Parquet is designed to faithfully serialize and de-serialize DataFrame s, supporting all of the pandas dtypes, including extension dtypes such as datetime with tz. Several caveats.  Duplicate column names and non-string columns names are not supported. The pyarrow engine always writes the index to the output, but fastparquet only writes non-default indexes. This extra column can cause problems for non-pandas consumers that are not expecting it. You can force including or omitting indexes with the index argument, regardless of the underlying engine. Index level names, if specified, must be strings. In the pyarrow engine, categorical dtypes for non-string types can be serialized to parquet, but will de-serialize as their primitive dtype. The pyarrow engine preserves the ordered flag of categorical dtypes with string types. fastparquet does not preserve the ordered flag. Non supported types include Interval and actual Python object types. These will raise a helpful error message on an attempt at serialization. Period type is supported with pyarrow >= 0.16.0. The pyarrow engine preserves extension data types such as the nullable integer and string data type (requiring pyarrow >= 0.16.0, and requiring the extension type to implement the needed protocols, see the extension types documentation).  You can specify an engine to direct the serialization. This can be one of pyarrow, or fastparquet, or auto. If the engine is NOT specified, then the pd.options.io.parquet.engine option is checked; if this is also auto, then pyarrow is tried, and falling back to fastparquet. See the documentation for pyarrow and fastparquet.  Note These engines are very similar and should read/write nearly identical parquet format files. Currently pyarrow does not support timedelta data, fastparquet>=0.1.4 supports timezone aware datetimes. These libraries differ by having different underlying dependencies (fastparquet by using numba, while pyarrow uses a c-library).  \nIn [558]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"a\": list(\"abc\"),\n   .....:         \"b\": list(range(1, 4)),\n   .....:         \"c\": np.arange(3, 6).astype(\"u1\"),\n   .....:         \"d\": np.arange(4.0, 7.0, dtype=\"float64\"),\n   .....:         \"e\": [True, False, True],\n   .....:         \"f\": pd.date_range(\"20130101\", periods=3),\n   .....:         \"g\": pd.date_range(\"20130101\", periods=3, tz=\"US/Eastern\"),\n   .....:         \"h\": pd.Categorical(list(\"abc\")),\n   .....:         \"i\": pd.Categorical(list(\"abc\"), ordered=True),\n   .....:     }\n   .....: )\n   .....: \n\nIn [559]: df\nOut[559]: \n   a  b  c    d      e          f                         g  h  i\n0  a  1  3  4.0   True 2013-01-01 2013-01-01 00:00:00-05:00  a  a\n1  b  2  4  5.0  False 2013-01-02 2013-01-02 00:00:00-05:00  b  b\n2  c  3  5  6.0   True 2013-01-03 2013-01-03 00:00:00-05:00  c  c\n\nIn [560]: df.dtypes\nOut[560]: \na                        object\nb                         int64\nc                         uint8\nd                       float64\ne                          bool\nf                datetime64[ns]\ng    datetime64[ns, US/Eastern]\nh                      category\ni                      category\ndtype: object\n  Write to a parquet file. \nIn [561]: df.to_parquet(\"example_pa.parquet\", engine=\"pyarrow\")\n\nIn [562]: df.to_parquet(\"example_fp.parquet\", engine=\"fastparquet\")\n  Read from a parquet file. \nIn [563]: result = pd.read_parquet(\"example_fp.parquet\", engine=\"fastparquet\")\n\nIn [564]: result = pd.read_parquet(\"example_pa.parquet\", engine=\"pyarrow\")\n\nIn [565]: result.dtypes\nOut[565]: \na                        object\nb                         int64\nc                         uint8\nd                       float64\ne                          bool\nf                datetime64[ns]\ng    datetime64[ns, US/Eastern]\nh                      category\ni                      category\ndtype: object\n  Read only certain columns of a parquet file. \nIn [566]: result = pd.read_parquet(\n   .....:     \"example_fp.parquet\",\n   .....:     engine=\"fastparquet\",\n   .....:     columns=[\"a\", \"b\"],\n   .....: )\n   .....: \n\nIn [567]: result = pd.read_parquet(\n   .....:     \"example_pa.parquet\",\n   .....:     engine=\"pyarrow\",\n   .....:     columns=[\"a\", \"b\"],\n   .....: )\n   .....: \n\nIn [568]: result.dtypes\nOut[568]: \na    object\nb     int64\ndtype: object\n   Handling indexes Serializing a DataFrame to parquet may include the implicit index as one or more columns in the output file. Thus, this code: \nIn [569]: df = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\n\nIn [570]: df.to_parquet(\"test.parquet\", engine=\"pyarrow\")\n  creates a parquet file with three columns if you use pyarrow for serialization: a, b, and __index_level_0__. If you\u2019re using fastparquet, the index may or may not be written to the file. This unexpected extra column causes some databases like Amazon Redshift to reject the file, because that column doesn\u2019t exist in the target table. If you want to omit a dataframe\u2019s indexes when writing, pass index=False to to_parquet(): \nIn [571]: df.to_parquet(\"test.parquet\", index=False)\n  This creates a parquet file with just the two expected columns, a and b. If your DataFrame has a custom index, you won\u2019t get it back when you load this file into a DataFrame. Passing index=True will always write the index, even if that\u2019s not the underlying engine\u2019s default behavior.   Partitioning Parquet files Parquet supports partitioning of data based on the values of one or more columns. \nIn [572]: df = pd.DataFrame({\"a\": [0, 0, 1, 1], \"b\": [0, 1, 0, 1]})\n\nIn [573]: df.to_parquet(path=\"test\", engine=\"pyarrow\", partition_cols=[\"a\"], compression=None)\n  The path specifies the parent directory to which data will be saved. The partition_cols are the column names by which the dataset will be partitioned. Columns are partitioned in the order they are given. The partition splits are determined by the unique values in the partition columns. The above example creates a partitioned dataset that may look like: \ntest\n\u251c\u2500\u2500 a=0\n\u2502   \u251c\u2500\u2500 0bac803e32dc42ae83fddfd029cbdebc.parquet\n\u2502   \u2514\u2500\u2500  ...\n\u2514\u2500\u2500 a=1\n    \u251c\u2500\u2500 e6ab24a4f45147b49b54a662f0c412a3.parquet\n    \u2514\u2500\u2500 ...\n     ORC  New in version 1.0.0.  Similar to the parquet format, the ORC Format is a binary columnar serialization for data frames. It is designed to make reading data frames efficient. pandas provides only a reader for the ORC format, read_orc(). This requires the pyarrow library.  Warning  It is highly recommended to install pyarrow using conda due to some issues occurred by pyarrow. read_orc() is not supported on Windows yet, you can find valid environments on install optional dependencies.     SQL queries The pandas.io.sql module provides a collection of query wrappers to both facilitate data retrieval and to reduce dependency on DB-specific API. Database abstraction is provided by SQLAlchemy if installed. In addition you will need a driver library for your database. Examples of such drivers are psycopg2 for PostgreSQL or pymysql for MySQL. For SQLite this is included in Python\u2019s standard library by default. You can find an overview of supported drivers for each SQL dialect in the SQLAlchemy docs. If SQLAlchemy is not installed, a fallback is only provided for sqlite (and for mysql for backwards compatibility, but this is deprecated and will be removed in a future version). This mode requires a Python database adapter which respect the Python DB-API. See also some cookbook examples for some advanced strategies. The key functions are:       \nread_sql_table(table_name, con[, schema, ...]) Read SQL database table into a DataFrame.  \nread_sql_query(sql, con[, index_col, ...]) Read SQL query into a DataFrame.  \nread_sql(sql, con[, index_col, ...]) Read SQL query or database table into a DataFrame.  \nDataFrame.to_sql(name, con[, schema, ...]) Write records stored in a DataFrame to a SQL database.     Note The function read_sql() is a convenience wrapper around read_sql_table() and read_sql_query() (and for backward compatibility) and will delegate to specific function depending on the provided input (database table name or sql query). Table names do not need to be quoted if they have special characters.  In the following example, we use the SQlite SQL database engine. You can use a temporary SQLite database where data are stored in \u201cmemory\u201d. To connect with SQLAlchemy you use the create_engine() function to create an engine object from database URI. You only need to create the engine once per database you are connecting to. For more information on create_engine() and the URI formatting, see the examples below and the SQLAlchemy documentation \nIn [574]: from sqlalchemy import create_engine\n\n# Create your engine.\nIn [575]: engine = create_engine(\"sqlite:///:memory:\")\n  If you want to manage your own connections you can pass one of those instead. The example below opens a connection to the database using a Python context manager that automatically closes the connection after the block has completed. See the SQLAlchemy docs for an explanation of how the database connection is handled. \nwith engine.connect() as conn, conn.begin():\n    data = pd.read_sql_table(\"data\", conn)\n   Warning When you open a connection to a database you are also responsible for closing it. Side effects of leaving a connection open may include locking the database or other breaking behaviour.   Writing DataFrames Assuming the following data is in a DataFrame data, we can insert it into the database using to_sql().          \nid Date Col_1 Col_2 Col_3    \n26 2012-10-18 X 25.7 True  \n42 2012-10-19 Y -12.4 False  \n63 2012-10-20 Z 5.73 True    \nIn [576]: data\nOut[576]: \n   id       Date Col_1  Col_2  Col_3\n0  26 2010-10-18     X  27.50   True\n1  42 2010-10-19     Y -12.50  False\n2  63 2010-10-20     Z   5.73   True\n\nIn [577]: data.to_sql(\"data\", engine)\nOut[577]: 3\n  With some databases, writing large DataFrames can result in errors due to packet size limitations being exceeded. This can be avoided by setting the chunksize parameter when calling to_sql. For example, the following writes data to the database in batches of 1000 rows at a time: \nIn [578]: data.to_sql(\"data_chunked\", engine, chunksize=1000)\nOut[578]: 3\n   SQL data types to_sql() will try to map your data to an appropriate SQL data type based on the dtype of the data. When you have columns of dtype object, pandas will try to infer the data type. You can always override the default type by specifying the desired SQL type of any of the columns by using the dtype argument. This argument needs a dictionary mapping column names to SQLAlchemy types (or strings for the sqlite3 fallback mode). For example, specifying to use the sqlalchemy String type instead of the default Text type for string columns: \nIn [579]: from sqlalchemy.types import String\n\nIn [580]: data.to_sql(\"data_dtype\", engine, dtype={\"Col_1\": String})\nOut[580]: 3\n   Note Due to the limited support for timedelta\u2019s in the different database flavors, columns with type timedelta64 will be written as integer values as nanoseconds to the database and a warning will be raised.   Note Columns of category dtype will be converted to the dense representation as you would get with np.asarray(categorical) (e.g. for string categories this gives an array of strings). Because of this, reading the database table back in does not generate a categorical.     Datetime data types Using SQLAlchemy, to_sql() is capable of writing datetime data that is timezone naive or timezone aware. However, the resulting data stored in the database ultimately depends on the supported data type for datetime data of the database system being used. The following table lists supported data types for datetime data for some common databases. Other database dialects may have different data types for datetime data.        \nDatabase SQL Datetime Types Timezone Support    \nSQLite TEXT No  \nMySQL TIMESTAMP or DATETIME No  \nPostgreSQL TIMESTAMP or TIMESTAMP WITH TIME ZONE Yes    When writing timezone aware data to databases that do not support timezones, the data will be written as timezone naive timestamps that are in local time with respect to the timezone. read_sql_table() is also capable of reading datetime data that is timezone aware or naive. When reading TIMESTAMP WITH TIME ZONE types, pandas will convert the data to UTC.  Insertion method The parameter method controls the SQL insertion clause used. Possible values are:  None: Uses standard SQL INSERT clause (one per row). 'multi': Pass multiple values in a single INSERT clause. It uses a special SQL syntax not supported by all backends. This usually provides better performance for analytic databases like Presto and Redshift, but has worse performance for traditional SQL backend if the table contains many columns. For more information check the SQLAlchemy documentation. callable with signature (pd_table, conn, keys, data_iter): This can be used to implement a more performant insertion method based on specific backend dialect features.  Example of a callable using PostgreSQL COPY clause: \n# Alternative to_sql() *method* for DBs that support COPY FROM\nimport csv\nfrom io import StringIO\n\ndef psql_insert_copy(table, conn, keys, data_iter):\n    \"\"\"\n    Execute SQL statement inserting data\n\n    Parameters\n    ----------\n    table : pandas.io.sql.SQLTable\n    conn : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection\n    keys : list of str\n        Column names\n    data_iter : Iterable that iterates the values to be inserted\n    \"\"\"\n    # gets a DBAPI connection that can provide a cursor\n    dbapi_conn = conn.connection\n    with dbapi_conn.cursor() as cur:\n        s_buf = StringIO()\n        writer = csv.writer(s_buf)\n        writer.writerows(data_iter)\n        s_buf.seek(0)\n\n        columns = ', '.join(['\"{}\"'.format(k) for k in keys])\n        if table.schema:\n            table_name = '{}.{}'.format(table.schema, table.name)\n        else:\n            table_name = table.name\n\n        sql = 'COPY {} ({}) FROM STDIN WITH CSV'.format(\n            table_name, columns)\n        cur.copy_expert(sql=sql, file=s_buf)\n     Reading tables read_sql_table() will read a database table given the table name and optionally a subset of columns to read.  Note In order to use read_sql_table(), you must have the SQLAlchemy optional dependency installed.  \nIn [581]: pd.read_sql_table(\"data\", engine)\nOut[581]: \n   index  id       Date Col_1  Col_2  Col_3\n0      0  26 2010-10-18     X  27.50   True\n1      1  42 2010-10-19     Y -12.50  False\n2      2  63 2010-10-20     Z   5.73   True\n   Note Note that pandas infers column dtypes from query outputs, and not by looking up data types in the physical database schema. For example, assume userid is an integer column in a table. Then, intuitively, select userid ... will return integer-valued series, while select cast(userid as text) ... will return object-valued (str) series. Accordingly, if the query output is empty, then all resulting columns will be returned as object-valued (since they are most general). If you foresee that your query will sometimes generate an empty result, you may want to explicitly typecast afterwards to ensure dtype integrity.  You can also specify the name of the column as the DataFrame index, and specify a subset of columns to be read. \nIn [582]: pd.read_sql_table(\"data\", engine, index_col=\"id\")\nOut[582]: \n    index       Date Col_1  Col_2  Col_3\nid                                      \n26      0 2010-10-18     X  27.50   True\n42      1 2010-10-19     Y -12.50  False\n63      2 2010-10-20     Z   5.73   True\n\nIn [583]: pd.read_sql_table(\"data\", engine, columns=[\"Col_1\", \"Col_2\"])\nOut[583]: \n  Col_1  Col_2\n0     X  27.50\n1     Y -12.50\n2     Z   5.73\n  And you can explicitly force columns to be parsed as dates: \nIn [584]: pd.read_sql_table(\"data\", engine, parse_dates=[\"Date\"])\nOut[584]: \n   index  id       Date Col_1  Col_2  Col_3\n0      0  26 2010-10-18     X  27.50   True\n1      1  42 2010-10-19     Y -12.50  False\n2      2  63 2010-10-20     Z   5.73   True\n  If needed you can explicitly specify a format string, or a dict of arguments to pass to pandas.to_datetime(): \npd.read_sql_table(\"data\", engine, parse_dates={\"Date\": \"%Y-%m-%d\"})\npd.read_sql_table(\n    \"data\",\n    engine,\n    parse_dates={\"Date\": {\"format\": \"%Y-%m-%d %H:%M:%S\"}},\n)\n  You can check if a table exists using has_table()   Schema support Reading from and writing to different schema\u2019s is supported through the schema keyword in the read_sql_table() and to_sql() functions. Note however that this depends on the database flavor (sqlite does not have schema\u2019s). For example: \ndf.to_sql(\"table\", engine, schema=\"other_schema\")\npd.read_sql_table(\"table\", engine, schema=\"other_schema\")\n    Querying You can query using raw SQL in the read_sql_query() function. In this case you must use the SQL variant appropriate for your database. When using SQLAlchemy, you can also pass SQLAlchemy Expression language constructs, which are database-agnostic. \nIn [585]: pd.read_sql_query(\"SELECT * FROM data\", engine)\nOut[585]: \n   index  id                        Date Col_1  Col_2  Col_3\n0      0  26  2010-10-18 00:00:00.000000     X  27.50      1\n1      1  42  2010-10-19 00:00:00.000000     Y -12.50      0\n2      2  63  2010-10-20 00:00:00.000000     Z   5.73      1\n  Of course, you can specify a more \u201ccomplex\u201d query. \nIn [586]: pd.read_sql_query(\"SELECT id, Col_1, Col_2 FROM data WHERE id = 42;\", engine)\nOut[586]: \n   id Col_1  Col_2\n0  42     Y  -12.5\n  The read_sql_query() function supports a chunksize argument. Specifying this will return an iterator through chunks of the query result: \nIn [587]: df = pd.DataFrame(np.random.randn(20, 3), columns=list(\"abc\"))\n\nIn [588]: df.to_sql(\"data_chunks\", engine, index=False)\nOut[588]: 20\n  \nIn [589]: for chunk in pd.read_sql_query(\"SELECT * FROM data_chunks\", engine, chunksize=5):\n   .....:     print(chunk)\n   .....: \n          a         b         c\n0  0.092961 -0.674003  1.104153\n1 -0.092732 -0.156246 -0.585167\n2 -0.358119 -0.862331 -1.672907\n3  0.550313 -1.507513 -0.617232\n4  0.650576  1.033221  0.492464\n          a         b         c\n0 -1.627786 -0.692062  1.039548\n1 -1.802313 -0.890905 -0.881794\n2  0.630492  0.016739  0.014500\n3 -0.438358  0.647275 -0.052075\n4  0.673137  1.227539  0.203534\n          a         b         c\n0  0.861658  0.867852 -0.465016\n1  1.547012 -0.947189 -1.241043\n2  0.070470  0.901320  0.937577\n3  0.295770  1.420548 -0.005283\n4 -1.518598 -0.730065  0.226497\n          a         b         c\n0 -2.061465  0.632115  0.853619\n1  2.719155  0.139018  0.214557\n2 -1.538924 -0.366973 -0.748801\n3 -0.478137 -1.559153 -3.097759\n4 -2.320335 -0.221090  0.119763\n  You can also run a plain query without creating a DataFrame with execute(). This is useful for queries that don\u2019t return values, such as INSERT. This is functionally equivalent to calling execute on the SQLAlchemy engine or db connection object. Again, you must use the SQL syntax variant appropriate for your database. \nfrom pandas.io import sql\n\nsql.execute(\"SELECT * FROM table_name\", engine)\nsql.execute(\n    \"INSERT INTO table_name VALUES(?, ?, ?)\", engine, params=[(\"id\", 1, 12.2, True)]\n)\n    Engine connection examples To connect with SQLAlchemy you use the create_engine() function to create an engine object from database URI. You only need to create the engine once per database you are connecting to. \nfrom sqlalchemy import create_engine\n\nengine = create_engine(\"postgresql://scott:tiger@localhost:5432/mydatabase\")\n\nengine = create_engine(\"mysql+mysqldb://scott:tiger@localhost/foo\")\n\nengine = create_engine(\"oracle://scott:tiger@127.0.0.1:1521/sidname\")\n\nengine = create_engine(\"mssql+pyodbc://mydsn\")\n\n# sqlite://<nohostname>/<path>\n# where <path> is relative:\nengine = create_engine(\"sqlite:///foo.db\")\n\n# or absolute, starting with a slash:\nengine = create_engine(\"sqlite:////absolute/path/to/foo.db\")\n  For more information see the examples the SQLAlchemy documentation   Advanced SQLAlchemy queries You can use SQLAlchemy constructs to describe your query. Use sqlalchemy.text() to specify query parameters in a backend-neutral way \nIn [590]: import sqlalchemy as sa\n\nIn [591]: pd.read_sql(\n   .....:     sa.text(\"SELECT * FROM data where Col_1=:col1\"), engine, params={\"col1\": \"X\"}\n   .....: )\n   .....: \nOut[591]: \n   index  id                        Date Col_1  Col_2  Col_3\n0      0  26  2010-10-18 00:00:00.000000     X   27.5      1\n  If you have an SQLAlchemy description of your database you can express where conditions using SQLAlchemy expressions \nIn [592]: metadata = sa.MetaData()\n\nIn [593]: data_table = sa.Table(\n   .....:     \"data\",\n   .....:     metadata,\n   .....:     sa.Column(\"index\", sa.Integer),\n   .....:     sa.Column(\"Date\", sa.DateTime),\n   .....:     sa.Column(\"Col_1\", sa.String),\n   .....:     sa.Column(\"Col_2\", sa.Float),\n   .....:     sa.Column(\"Col_3\", sa.Boolean),\n   .....: )\n   .....: \n\nIn [594]: pd.read_sql(sa.select([data_table]).where(data_table.c.Col_3 is True), engine)\nOut[594]: \nEmpty DataFrame\nColumns: [index, Date, Col_1, Col_2, Col_3]\nIndex: []\n  You can combine SQLAlchemy expressions with parameters passed to read_sql() using sqlalchemy.bindparam() \nIn [595]: import datetime as dt\n\nIn [596]: expr = sa.select([data_table]).where(data_table.c.Date > sa.bindparam(\"date\"))\n\nIn [597]: pd.read_sql(expr, engine, params={\"date\": dt.datetime(2010, 10, 18)})\nOut[597]: \n   index       Date Col_1  Col_2  Col_3\n0      1 2010-10-19     Y -12.50  False\n1      2 2010-10-20     Z   5.73   True\n    Sqlite fallback The use of sqlite is supported without using SQLAlchemy. This mode requires a Python database adapter which respect the Python DB-API. You can create connections like so: \nimport sqlite3\n\ncon = sqlite3.connect(\":memory:\")\n  And then issue the following queries: \ndata.to_sql(\"data\", con)\npd.read_sql_query(\"SELECT * FROM data\", con)\n     Google BigQuery  Warning Starting in 0.20.0, pandas has split off Google BigQuery support into the separate package pandas-gbq. You can pip install pandas-gbq to get it.  The pandas-gbq package provides functionality to read/write from Google BigQuery. pandas integrates with this external package. if pandas-gbq is installed, you can use the pandas methods pd.read_gbq and DataFrame.to_gbq, which will call the respective functions from pandas-gbq. Full documentation can be found here.   Stata format  Writing to stata format The method to_stata() will write a DataFrame into a .dta file. The format version of this file is always 115 (Stata 12). \nIn [598]: df = pd.DataFrame(np.random.randn(10, 2), columns=list(\"AB\"))\n\nIn [599]: df.to_stata(\"stata.dta\")\n  Stata data files have limited data type support; only strings with 244 or fewer characters, int8, int16, int32, float32 and float64 can be stored in .dta files. Additionally, Stata reserves certain values to represent missing data. Exporting a non-missing value that is outside of the permitted range in Stata for a particular data type will retype the variable to the next larger size. For example, int8 values are restricted to lie between -127 and 100 in Stata, and so variables with values above 100 will trigger a conversion to int16. nan values in floating points data types are stored as the basic missing data type (. in Stata).  Note It is not possible to export missing data values for integer data types.  The Stata writer gracefully handles other data types including int64, bool, uint8, uint16, uint32 by casting to the smallest supported type that can represent the data. For example, data with a type of uint8 will be cast to int8 if all values are less than 100 (the upper bound for non-missing int8 data in Stata), or, if values are outside of this range, the variable is cast to int16.  Warning Conversion from int64 to float64 may result in a loss of precision if int64 values are larger than 2**53.   Warning StataWriter and to_stata() only support fixed width strings containing up to 244 characters, a limitation imposed by the version 115 dta file format. Attempting to write Stata dta files with strings longer than 244 characters raises a ValueError.    Reading from Stata format The top-level function read_stata will read a dta file and return either a DataFrame or a StataReader that can be used to read the file incrementally. \nIn [600]: pd.read_stata(\"stata.dta\")\nOut[600]: \n   index         A         B\n0      0  0.608228  1.064810\n1      1 -0.780506 -2.736887\n2      2  0.143539  1.170191\n3      3 -1.573076  0.075792\n4      4 -1.722223 -0.774650\n5      5  0.803627  0.221665\n6      6  0.584637  0.147264\n7      7  1.057825 -0.284136\n8      8  0.912395  1.552808\n9      9  0.189376 -0.109830\n  Specifying a chunksize yields a StataReader instance that can be used to read chunksize lines from the file at a time. The StataReader object can be used as an iterator. \nIn [601]: with pd.read_stata(\"stata.dta\", chunksize=3) as reader:\n   .....:     for df in reader:\n   .....:         print(df.shape)\n   .....: \n(3, 3)\n(3, 3)\n(3, 3)\n(1, 3)\n  For more fine-grained control, use iterator=True and specify chunksize with each call to read(). \nIn [602]: with pd.read_stata(\"stata.dta\", iterator=True) as reader:\n   .....:     chunk1 = reader.read(5)\n   .....:     chunk2 = reader.read(5)\n   .....: \n  Currently the index is retrieved as a column. The parameter convert_categoricals indicates whether value labels should be read and used to create a Categorical variable from them. Value labels can also be retrieved by the function value_labels, which requires read() to be called before use. The parameter convert_missing indicates whether missing value representations in Stata should be preserved. If False (the default), missing values are represented as np.nan. If True, missing values are represented using StataMissingValue objects, and columns containing missing values will have object data type.  Note read_stata() and StataReader support .dta formats 113-115 (Stata 10-12), 117 (Stata 13), and 118 (Stata 14).   Note Setting preserve_dtypes=False will upcast to the standard pandas data types: int64 for all integer types and float64 for floating point data. By default, the Stata data types are preserved when importing.   Categorical data Categorical data can be exported to Stata data files as value labeled data. The exported data consists of the underlying category codes as integer data values and the categories as value labels. Stata does not have an explicit equivalent to a Categorical and information about whether the variable is ordered is lost when exporting.  Warning Stata only supports string value labels, and so str is called on the categories when exporting data. Exporting Categorical variables with non-string categories produces a warning, and can result a loss of information if the str representations of the categories are not unique.  Labeled data can similarly be imported from Stata data files as Categorical variables using the keyword argument convert_categoricals (True by default). The keyword argument order_categoricals (True by default) determines whether imported Categorical variables are ordered.  Note When importing categorical data, the values of the variables in the Stata data file are not preserved since Categorical variables always use integer data types between -1 and n-1 where n is the number of categories. If the original values in the Stata data file are required, these can be imported by setting convert_categoricals=False, which will import original data (but not the variable labels). The original values can be matched to the imported categorical data since there is a simple mapping between the original Stata data values and the category codes of imported Categorical variables: missing values are assigned code -1, and the smallest original value is assigned 0, the second smallest is assigned 1 and so on until the largest original value is assigned the code n-1.   Note Stata supports partially labeled series. These series have value labels for some but not all data values. Importing a partially labeled series will produce a Categorical with string categories for the values that are labeled and numeric categories for values with no label.      SAS formats The top-level function read_sas() can read (but not write) SAS XPORT (.xpt) and (since v0.18.0) SAS7BDAT (.sas7bdat) format files. SAS files only contain two value types: ASCII text and floating point values (usually 8 bytes but sometimes truncated). For xport files, there is no automatic type conversion to integers, dates, or categoricals. For SAS7BDAT files, the format codes may allow date variables to be automatically converted to dates. By default the whole file is read and returned as a DataFrame. Specify a chunksize or use iterator=True to obtain reader objects (XportReader or SAS7BDATReader) for incrementally reading the file. The reader objects also have attributes that contain additional information about the file and its variables. Read a SAS7BDAT file: \ndf = pd.read_sas(\"sas_data.sas7bdat\")\n  Obtain an iterator and read an XPORT file 100,000 lines at a time: \ndef do_something(chunk):\n    pass\n\n\nwith pd.read_sas(\"sas_xport.xpt\", chunk=100000) as rdr:\n    for chunk in rdr:\n        do_something(chunk)\n  The specification for the xport file format is available from the SAS web site. No official documentation is available for the SAS7BDAT format.   SPSS formats  New in version 0.25.0.  The top-level function read_spss() can read (but not write) SPSS SAV (.sav) and ZSAV (.zsav) format files. SPSS files contain column names. By default the whole file is read, categorical columns are converted into pd.Categorical, and a DataFrame with all columns is returned. Specify the usecols parameter to obtain a subset of columns. Specify convert_categoricals=False to avoid converting categorical columns into pd.Categorical. Read an SPSS file: \ndf = pd.read_spss(\"spss_data.sav\")\n  Extract a subset of columns contained in usecols from an SPSS file and avoid converting categorical columns into pd.Categorical: \ndf = pd.read_spss(\n    \"spss_data.sav\",\n    usecols=[\"foo\", \"bar\"],\n    convert_categoricals=False,\n)\n  More information about the SAV and ZSAV file formats is available here.   Other file formats pandas itself only supports IO with a limited set of file formats that map cleanly to its tabular data model. For reading and writing other file formats into and from pandas, we recommend these packages from the broader community.  netCDF xarray provides data structures inspired by the pandas DataFrame for working with multi-dimensional datasets, with a focus on the netCDF file format and easy conversion to and from pandas.    Performance considerations This is an informal comparison of various IO methods, using pandas 0.24.2. Timings are machine dependent and small differences should be ignored. \nIn [1]: sz = 1000000\nIn [2]: df = pd.DataFrame({'A': np.random.randn(sz), 'B': [1] * sz})\n\nIn [3]: df.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 2 columns):\nA    1000000 non-null float64\nB    1000000 non-null int64\ndtypes: float64(1), int64(1)\nmemory usage: 15.3 MB\n  The following test functions will be used below to compare the performance of several IO methods: \nimport numpy as np\n\nimport os\n\nsz = 1000000\ndf = pd.DataFrame({\"A\": np.random.randn(sz), \"B\": [1] * sz})\n\nsz = 1000000\nnp.random.seed(42)\ndf = pd.DataFrame({\"A\": np.random.randn(sz), \"B\": [1] * sz})\n\n\ndef test_sql_write(df):\n    if os.path.exists(\"test.sql\"):\n        os.remove(\"test.sql\")\n    sql_db = sqlite3.connect(\"test.sql\")\n    df.to_sql(name=\"test_table\", con=sql_db)\n    sql_db.close()\n\n\ndef test_sql_read():\n    sql_db = sqlite3.connect(\"test.sql\")\n    pd.read_sql_query(\"select * from test_table\", sql_db)\n    sql_db.close()\n\n\ndef test_hdf_fixed_write(df):\n    df.to_hdf(\"test_fixed.hdf\", \"test\", mode=\"w\")\n\n\ndef test_hdf_fixed_read():\n    pd.read_hdf(\"test_fixed.hdf\", \"test\")\n\n\ndef test_hdf_fixed_write_compress(df):\n    df.to_hdf(\"test_fixed_compress.hdf\", \"test\", mode=\"w\", complib=\"blosc\")\n\n\ndef test_hdf_fixed_read_compress():\n    pd.read_hdf(\"test_fixed_compress.hdf\", \"test\")\n\n\ndef test_hdf_table_write(df):\n    df.to_hdf(\"test_table.hdf\", \"test\", mode=\"w\", format=\"table\")\n\n\ndef test_hdf_table_read():\n    pd.read_hdf(\"test_table.hdf\", \"test\")\n\n\ndef test_hdf_table_write_compress(df):\n    df.to_hdf(\n        \"test_table_compress.hdf\", \"test\", mode=\"w\", complib=\"blosc\", format=\"table\"\n    )\n\n\ndef test_hdf_table_read_compress():\n    pd.read_hdf(\"test_table_compress.hdf\", \"test\")\n\n\ndef test_csv_write(df):\n    df.to_csv(\"test.csv\", mode=\"w\")\n\n\ndef test_csv_read():\n    pd.read_csv(\"test.csv\", index_col=0)\n\n\ndef test_feather_write(df):\n    df.to_feather(\"test.feather\")\n\n\ndef test_feather_read():\n    pd.read_feather(\"test.feather\")\n\n\ndef test_pickle_write(df):\n    df.to_pickle(\"test.pkl\")\n\n\ndef test_pickle_read():\n    pd.read_pickle(\"test.pkl\")\n\n\ndef test_pickle_write_compress(df):\n    df.to_pickle(\"test.pkl.compress\", compression=\"xz\")\n\n\ndef test_pickle_read_compress():\n    pd.read_pickle(\"test.pkl.compress\", compression=\"xz\")\n\n\ndef test_parquet_write(df):\n    df.to_parquet(\"test.parquet\")\n\n\ndef test_parquet_read():\n    pd.read_parquet(\"test.parquet\")\n  When writing, the top three functions in terms of speed are test_feather_write, test_hdf_fixed_write and test_hdf_fixed_write_compress. \nIn [4]: %timeit test_sql_write(df)\n3.29 s \u00b1 43.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [5]: %timeit test_hdf_fixed_write(df)\n19.4 ms \u00b1 560 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [6]: %timeit test_hdf_fixed_write_compress(df)\n19.6 ms \u00b1 308 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [7]: %timeit test_hdf_table_write(df)\n449 ms \u00b1 5.61 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [8]: %timeit test_hdf_table_write_compress(df)\n448 ms \u00b1 11.9 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [9]: %timeit test_csv_write(df)\n3.66 s \u00b1 26.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [10]: %timeit test_feather_write(df)\n9.75 ms \u00b1 117 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [11]: %timeit test_pickle_write(df)\n30.1 ms \u00b1 229 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [12]: %timeit test_pickle_write_compress(df)\n4.29 s \u00b1 15.9 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [13]: %timeit test_parquet_write(df)\n67.6 ms \u00b1 706 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n  When reading, the top three functions in terms of speed are test_feather_read, test_pickle_read and test_hdf_fixed_read. \nIn [14]: %timeit test_sql_read()\n1.77 s \u00b1 17.7 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [15]: %timeit test_hdf_fixed_read()\n19.4 ms \u00b1 436 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [16]: %timeit test_hdf_fixed_read_compress()\n19.5 ms \u00b1 222 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [17]: %timeit test_hdf_table_read()\n38.6 ms \u00b1 857 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [18]: %timeit test_hdf_table_read_compress()\n38.8 ms \u00b1 1.49 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [19]: %timeit test_csv_read()\n452 ms \u00b1 9.04 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [20]: %timeit test_feather_read()\n12.4 ms \u00b1 99.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [21]: %timeit test_pickle_read()\n18.4 ms \u00b1 191 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [22]: %timeit test_pickle_read_compress()\n915 ms \u00b1 7.48 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [23]: %timeit test_parquet_read()\n24.4 ms \u00b1 146 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n  The files test.pkl.compress, test.parquet and test.feather took the least space on disk (in bytes). \n29519500 Oct 10 06:45 test.csv\n16000248 Oct 10 06:45 test.feather\n8281983  Oct 10 06:49 test.parquet\n16000857 Oct 10 06:47 test.pkl\n7552144  Oct 10 06:48 test.pkl.compress\n34816000 Oct 10 06:42 test.sql\n24009288 Oct 10 06:43 test_fixed.hdf\n24009288 Oct 10 06:43 test_fixed_compress.hdf\n24458940 Oct 10 06:44 test_table.hdf\n24458940 Oct 10 06:44 test_table_compress.hdf\n  \n"}, {"name": "Merge, join, concatenate and compare", "path": "user_guide/merging", "type": "Manual", "text": "Merge, join, concatenate and compare pandas provides various facilities for easily combining together Series or DataFrame with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations. In addition, pandas also provides utilities to compare two Series or DataFrame and summarize their differences.  Concatenating objects The concat() function (in the main pandas namespace) does all of the heavy lifting of performing concatenation operations along an axis while performing optional set logic (union or intersection) of the indexes (if any) on the other axes. Note that I say \u201cif any\u201d because there is only a single possible axis of concatenation for Series. Before diving into all of the details of concat and what it can do, here is a simple example: \nIn [1]: df1 = pd.DataFrame(\n   ...:     {\n   ...:         \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n   ...:         \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n   ...:         \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n   ...:         \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n   ...:     },\n   ...:     index=[0, 1, 2, 3],\n   ...: )\n   ...: \n\nIn [2]: df2 = pd.DataFrame(\n   ...:     {\n   ...:         \"A\": [\"A4\", \"A5\", \"A6\", \"A7\"],\n   ...:         \"B\": [\"B4\", \"B5\", \"B6\", \"B7\"],\n   ...:         \"C\": [\"C4\", \"C5\", \"C6\", \"C7\"],\n   ...:         \"D\": [\"D4\", \"D5\", \"D6\", \"D7\"],\n   ...:     },\n   ...:     index=[4, 5, 6, 7],\n   ...: )\n   ...: \n\nIn [3]: df3 = pd.DataFrame(\n   ...:     {\n   ...:         \"A\": [\"A8\", \"A9\", \"A10\", \"A11\"],\n   ...:         \"B\": [\"B8\", \"B9\", \"B10\", \"B11\"],\n   ...:         \"C\": [\"C8\", \"C9\", \"C10\", \"C11\"],\n   ...:         \"D\": [\"D8\", \"D9\", \"D10\", \"D11\"],\n   ...:     },\n   ...:     index=[8, 9, 10, 11],\n   ...: )\n   ...: \n\nIn [4]: frames = [df1, df2, df3]\n\nIn [5]: result = pd.concat(frames)\n   Like its sibling function on ndarrays, numpy.concatenate, pandas.concat takes a list or dict of homogeneously-typed objects and concatenates them with some configurable handling of \u201cwhat to do with the other axes\u201d: \npd.concat(\n    objs,\n    axis=0,\n    join=\"outer\",\n    ignore_index=False,\n    keys=None,\n    levels=None,\n    names=None,\n    verify_integrity=False,\n    copy=True,\n)\n   objs : a sequence or mapping of Series or DataFrame objects. If a dict is passed, the sorted keys will be used as the keys argument, unless it is passed, in which case the values will be selected (see below). Any None objects will be dropped silently unless they are all None in which case a ValueError will be raised. axis : {0, 1, \u2026}, default 0. The axis to concatenate along. join : {\u2018inner\u2019, \u2018outer\u2019}, default \u2018outer\u2019. How to handle indexes on other axis(es). Outer for union and inner for intersection. ignore_index : boolean, default False. If True, do not use the index values on the concatenation axis. The resulting axis will be labeled 0, \u2026, n - 1. This is useful if you are concatenating objects where the concatenation axis does not have meaningful indexing information. Note the index values on the other axes are still respected in the join. keys : sequence, default None. Construct hierarchical index using the passed keys as the outermost level. If multiple levels passed, should contain tuples. levels : list of sequences, default None. Specific levels (unique values) to use for constructing a MultiIndex. Otherwise they will be inferred from the keys. names : list, default None. Names for the levels in the resulting hierarchical index. verify_integrity : boolean, default False. Check whether the new concatenated axis contains duplicates. This can be very expensive relative to the actual data concatenation. copy : boolean, default True. If False, do not copy data unnecessarily.  Without a little bit of context many of these arguments don\u2019t make much sense. Let\u2019s revisit the above example. Suppose we wanted to associate specific keys with each of the pieces of the chopped up DataFrame. We can do this using the keys argument: \nIn [6]: result = pd.concat(frames, keys=[\"x\", \"y\", \"z\"])\n   As you can see (if you\u2019ve read the rest of the documentation), the resulting object\u2019s index has a hierarchical index. This means that we can now select out each chunk by key: \nIn [7]: result.loc[\"y\"]\nOut[7]: \n    A   B   C   D\n4  A4  B4  C4  D4\n5  A5  B5  C5  D5\n6  A6  B6  C6  D6\n7  A7  B7  C7  D7\n  It\u2019s not a stretch to see how this can be very useful. More detail on this functionality below.  Note It is worth noting that concat() (and therefore append()) makes a full copy of the data, and that constantly reusing this function can create a significant performance hit. If you need to use the operation over several datasets, use a list comprehension.  \nframes = [ process_your_file(f) for f in files ]\nresult = pd.concat(frames)\n   Note When concatenating DataFrames with named axes, pandas will attempt to preserve these index/column names whenever possible. In the case where all inputs share a common name, this name will be assigned to the result. When the input names do not all agree, the result will be unnamed. The same is true for MultiIndex, but the logic is applied separately on a level-by-level basis.   Set logic on the other axes When gluing together multiple DataFrames, you have a choice of how to handle the other axes (other than the one being concatenated). This can be done in the following two ways:  Take the union of them all, join='outer'. This is the default option as it results in zero information loss. Take the intersection, join='inner'.  Here is an example of each of these methods. First, the default join='outer' behavior: \nIn [8]: df4 = pd.DataFrame(\n   ...:     {\n   ...:         \"B\": [\"B2\", \"B3\", \"B6\", \"B7\"],\n   ...:         \"D\": [\"D2\", \"D3\", \"D6\", \"D7\"],\n   ...:         \"F\": [\"F2\", \"F3\", \"F6\", \"F7\"],\n   ...:     },\n   ...:     index=[2, 3, 6, 7],\n   ...: )\n   ...: \n\nIn [9]: result = pd.concat([df1, df4], axis=1)\n   Here is the same thing with join='inner': \nIn [10]: result = pd.concat([df1, df4], axis=1, join=\"inner\")\n   Lastly, suppose we just wanted to reuse the exact index from the original DataFrame: \nIn [11]: result = pd.concat([df1, df4], axis=1).reindex(df1.index)\n  Similarly, we could index before the concatenation: \nIn [12]: pd.concat([df1, df4.reindex(df1.index)], axis=1)\nOut[12]: \n    A   B   C   D    B    D    F\n0  A0  B0  C0  D0  NaN  NaN  NaN\n1  A1  B1  C1  D1  NaN  NaN  NaN\n2  A2  B2  C2  D2   B2   D2   F2\n3  A3  B3  C3  D3   B3   D3   F3\n     Ignoring indexes on the concatenation axis For DataFrame objects which don\u2019t have a meaningful index, you may wish to append them and ignore the fact that they may have overlapping indexes. To do this, use the ignore_index argument: \nIn [13]: result = pd.concat([df1, df4], ignore_index=True, sort=False)\n     Concatenating with mixed ndims You can concatenate a mix of Series and DataFrame objects. The Series will be transformed to DataFrame with the column name as the name of the Series. \nIn [14]: s1 = pd.Series([\"X0\", \"X1\", \"X2\", \"X3\"], name=\"X\")\n\nIn [15]: result = pd.concat([df1, s1], axis=1)\n    Note Since we\u2019re concatenating a Series to a DataFrame, we could have achieved the same result with DataFrame.assign(). To concatenate an arbitrary number of pandas objects (DataFrame or Series), use concat.  If unnamed Series are passed they will be numbered consecutively. \nIn [16]: s2 = pd.Series([\"_0\", \"_1\", \"_2\", \"_3\"])\n\nIn [17]: result = pd.concat([df1, s2, s2, s2], axis=1)\n   Passing ignore_index=True will drop all name references. \nIn [18]: result = pd.concat([df1, s1], axis=1, ignore_index=True)\n     More concatenating with group keys A fairly common use of the keys argument is to override the column names when creating a new DataFrame based on existing Series. Notice how the default behaviour consists on letting the resulting DataFrame inherit the parent Series\u2019 name, when these existed. \nIn [19]: s3 = pd.Series([0, 1, 2, 3], name=\"foo\")\n\nIn [20]: s4 = pd.Series([0, 1, 2, 3])\n\nIn [21]: s5 = pd.Series([0, 1, 4, 5])\n\nIn [22]: pd.concat([s3, s4, s5], axis=1)\nOut[22]: \n   foo  0  1\n0    0  0  0\n1    1  1  1\n2    2  2  4\n3    3  3  5\n  Through the keys argument we can override the existing column names. \nIn [23]: pd.concat([s3, s4, s5], axis=1, keys=[\"red\", \"blue\", \"yellow\"])\nOut[23]: \n   red  blue  yellow\n0    0     0       0\n1    1     1       1\n2    2     2       4\n3    3     3       5\n  Let\u2019s consider a variation of the very first example presented: \nIn [24]: result = pd.concat(frames, keys=[\"x\", \"y\", \"z\"])\n   You can also pass a dict to concat in which case the dict keys will be used for the keys argument (unless other keys are specified): \nIn [25]: pieces = {\"x\": df1, \"y\": df2, \"z\": df3}\n\nIn [26]: result = pd.concat(pieces)\n   \nIn [27]: result = pd.concat(pieces, keys=[\"z\", \"y\"])\n   The MultiIndex created has levels that are constructed from the passed keys and the index of the DataFrame pieces: \nIn [28]: result.index.levels\nOut[28]: FrozenList([['z', 'y'], [4, 5, 6, 7, 8, 9, 10, 11]])\n  If you wish to specify other levels (as will occasionally be the case), you can do so using the levels argument: \nIn [29]: result = pd.concat(\n   ....:     pieces, keys=[\"x\", \"y\", \"z\"], levels=[[\"z\", \"y\", \"x\", \"w\"]], names=[\"group_key\"]\n   ....: )\n   ....: \n   \nIn [30]: result.index.levels\nOut[30]: FrozenList([['z', 'y', 'x', 'w'], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]])\n  This is fairly esoteric, but it is actually necessary for implementing things like GroupBy where the order of a categorical variable is meaningful.   Appending rows to a DataFrame If you have a series that you want to append as a single row to a DataFrame, you can convert the row into a DataFrame and use concat \nIn [31]: s2 = pd.Series([\"X0\", \"X1\", \"X2\", \"X3\"], index=[\"A\", \"B\", \"C\", \"D\"])\n\nIn [32]: result = pd.concat([df1, s2.to_frame().T], ignore_index=True)\n   You should use ignore_index with this method to instruct DataFrame to discard its index. If you wish to preserve the index, you should construct an appropriately-indexed DataFrame and append or concatenate those objects.    Database-style DataFrame or named Series joining/merging pandas has full-featured, high performance in-memory join operations idiomatically very similar to relational databases like SQL. These methods perform significantly better (in some cases well over an order of magnitude better) than other open source implementations (like base::merge.data.frame in R). The reason for this is careful algorithmic design and the internal layout of the data in DataFrame. See the cookbook for some advanced strategies. Users who are familiar with SQL but new to pandas might be interested in a comparison with SQL. pandas provides a single function, merge(), as the entry point for all standard database join operations between DataFrame or named Series objects: \npd.merge(\n    left,\n    right,\n    how=\"inner\",\n    on=None,\n    left_on=None,\n    right_on=None,\n    left_index=False,\n    right_index=False,\n    sort=True,\n    suffixes=(\"_x\", \"_y\"),\n    copy=True,\n    indicator=False,\n    validate=None,\n)\n   left: A DataFrame or named Series object. right: Another DataFrame or named Series object. on: Column or index level names to join on. Must be found in both the left and right DataFrame and/or Series objects. If not passed and left_index and right_index are False, the intersection of the columns in the DataFrames and/or Series will be inferred to be the join keys. left_on: Columns or index levels from the left DataFrame or Series to use as keys. Can either be column names, index level names, or arrays with length equal to the length of the DataFrame or Series. right_on: Columns or index levels from the right DataFrame or Series to use as keys. Can either be column names, index level names, or arrays with length equal to the length of the DataFrame or Series. left_index: If True, use the index (row labels) from the left DataFrame or Series as its join key(s). In the case of a DataFrame or Series with a MultiIndex (hierarchical), the number of levels must match the number of join keys from the right DataFrame or Series. right_index: Same usage as left_index for the right DataFrame or Series how: One of 'left', 'right', 'outer', 'inner', 'cross'. Defaults to inner. See below for more detailed description of each method. sort: Sort the result DataFrame by the join keys in lexicographical order. Defaults to True, setting to False will improve performance substantially in many cases. suffixes: A tuple of string suffixes to apply to overlapping columns. Defaults to ('_x', '_y'). copy: Always copy data (default True) from the passed DataFrame or named Series objects, even when reindexing is not necessary. Cannot be avoided in many cases but may improve performance / memory usage. The cases where copying can be avoided are somewhat pathological but this option is provided nonetheless. indicator: Add a column to the output DataFrame called _merge with information on the source of each row. _merge is Categorical-type and takes on a value of left_only for observations whose merge key only appears in 'left' DataFrame or Series, right_only for observations whose merge key only appears in 'right' DataFrame or Series, and both if the observation\u2019s merge key is found in both. \nvalidate : string, default None. If specified, checks if merge is of specified type.  \n \u201cone_to_one\u201d or \u201c1:1\u201d: checks if merge keys are unique in both left and right datasets. \u201cone_to_many\u201d or \u201c1:m\u201d: checks if merge keys are unique in left dataset. \u201cmany_to_one\u201d or \u201cm:1\u201d: checks if merge keys are unique in right dataset. \u201cmany_to_many\u201d or \u201cm:m\u201d: allowed, but does not result in checks.  \n    Note Support for specifying index levels as the on, left_on, and right_on parameters was added in version 0.23.0. Support for merging named Series objects was added in version 0.24.0.  The return type will be the same as left. If left is a DataFrame or named Series and right is a subclass of DataFrame, the return type will still be DataFrame. merge is a function in the pandas namespace, and it is also available as a DataFrame instance method merge(), with the calling DataFrame being implicitly considered the left object in the join. The related join() method, uses merge internally for the index-on-index (by default) and column(s)-on-index join. If you are joining on index only, you may wish to use DataFrame.join to save yourself some typing.  Brief primer on merge methods (relational algebra) Experienced users of relational databases like SQL will be familiar with the terminology used to describe join operations between two SQL-table like structures (DataFrame objects). There are several cases to consider which are very important to understand:  one-to-one joins: for example when joining two DataFrame objects on their indexes (which must contain unique values). many-to-one joins: for example when joining an index (unique) to one or more columns in a different DataFrame. many-to-many joins: joining columns on columns.   Note When joining columns on columns (potentially a many-to-many join), any indexes on the passed DataFrame objects will be discarded.  It is worth spending some time understanding the result of the many-to-many join case. In SQL / standard relational algebra, if a key combination appears more than once in both tables, the resulting table will have the Cartesian product of the associated data. Here is a very basic example with one unique key combination: \nIn [33]: left = pd.DataFrame(\n   ....:     {\n   ....:         \"key\": [\"K0\", \"K1\", \"K2\", \"K3\"],\n   ....:         \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n   ....:         \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n   ....:     }\n   ....: )\n   ....: \n\nIn [34]: right = pd.DataFrame(\n   ....:     {\n   ....:         \"key\": [\"K0\", \"K1\", \"K2\", \"K3\"],\n   ....:         \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n   ....:         \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n   ....:     }\n   ....: )\n   ....: \n\nIn [35]: result = pd.merge(left, right, on=\"key\")\n   Here is a more complicated example with multiple join keys. Only the keys appearing in left and right are present (the intersection), since how='inner' by default. \nIn [36]: left = pd.DataFrame(\n   ....:     {\n   ....:         \"key1\": [\"K0\", \"K0\", \"K1\", \"K2\"],\n   ....:         \"key2\": [\"K0\", \"K1\", \"K0\", \"K1\"],\n   ....:         \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n   ....:         \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n   ....:     }\n   ....: )\n   ....: \n\nIn [37]: right = pd.DataFrame(\n   ....:     {\n   ....:         \"key1\": [\"K0\", \"K1\", \"K1\", \"K2\"],\n   ....:         \"key2\": [\"K0\", \"K0\", \"K0\", \"K0\"],\n   ....:         \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n   ....:         \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n   ....:     }\n   ....: )\n   ....: \n\nIn [38]: result = pd.merge(left, right, on=[\"key1\", \"key2\"])\n   The how argument to merge specifies how to determine which keys are to be included in the resulting table. If a key combination does not appear in either the left or right tables, the values in the joined table will be NA. Here is a summary of the how options and their SQL equivalent names:        \nMerge method SQL Join Name Description    \nleft LEFT OUTER JOIN Use keys from left frame only  \nright RIGHT OUTER JOIN Use keys from right frame only  \nouter FULL OUTER JOIN Use union of keys from both frames  \ninner INNER JOIN Use intersection of keys from both frames  \ncross CROSS JOIN Create the cartesian product of rows of both frames    \nIn [39]: result = pd.merge(left, right, how=\"left\", on=[\"key1\", \"key2\"])\n   \nIn [40]: result = pd.merge(left, right, how=\"right\", on=[\"key1\", \"key2\"])\n   \nIn [41]: result = pd.merge(left, right, how=\"outer\", on=[\"key1\", \"key2\"])\n   \nIn [42]: result = pd.merge(left, right, how=\"inner\", on=[\"key1\", \"key2\"])\n   \nIn [43]: result = pd.merge(left, right, how=\"cross\")\n   You can merge a mult-indexed Series and a DataFrame, if the names of the MultiIndex correspond to the columns from the DataFrame. Transform the Series to a DataFrame using Series.reset_index() before merging, as shown in the following example. \nIn [44]: df = pd.DataFrame({\"Let\": [\"A\", \"B\", \"C\"], \"Num\": [1, 2, 3]})\n\nIn [45]: df\nOut[45]: \n  Let  Num\n0   A    1\n1   B    2\n2   C    3\n\nIn [46]: ser = pd.Series(\n   ....:     [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"],\n   ....:     index=pd.MultiIndex.from_arrays(\n   ....:         [[\"A\", \"B\", \"C\"] * 2, [1, 2, 3, 4, 5, 6]], names=[\"Let\", \"Num\"]\n   ....:     ),\n   ....: )\n   ....: \n\nIn [47]: ser\nOut[47]: \nLet  Num\nA    1      a\nB    2      b\nC    3      c\nA    4      d\nB    5      e\nC    6      f\ndtype: object\n\nIn [48]: pd.merge(df, ser.reset_index(), on=[\"Let\", \"Num\"])\nOut[48]: \n  Let  Num  0\n0   A    1  a\n1   B    2  b\n2   C    3  c\n  Here is another example with duplicate join keys in DataFrames: \nIn [49]: left = pd.DataFrame({\"A\": [1, 2], \"B\": [2, 2]})\n\nIn [50]: right = pd.DataFrame({\"A\": [4, 5, 6], \"B\": [2, 2, 2]})\n\nIn [51]: result = pd.merge(left, right, on=\"B\", how=\"outer\")\n    Warning Joining / merging on duplicate keys can cause a returned frame that is the multiplication of the row dimensions, which may result in memory overflow. It is the user\u2019 s responsibility to manage duplicate values in keys before joining large DataFrames.    Checking for duplicate keys Users can use the validate argument to automatically check whether there are unexpected duplicates in their merge keys. Key uniqueness is checked before merge operations and so should protect against memory overflows. Checking key uniqueness is also a good way to ensure user data structures are as expected. In the following example, there are duplicate values of B in the right DataFrame. As this is not a one-to-one merge \u2013 as specified in the validate argument \u2013 an exception will be raised. \nIn [52]: left = pd.DataFrame({\"A\": [1, 2], \"B\": [1, 2]})\n\nIn [53]: right = pd.DataFrame({\"A\": [4, 5, 6], \"B\": [2, 2, 2]})\n  \nIn [53]: result = pd.merge(left, right, on=\"B\", how=\"outer\", validate=\"one_to_one\")\n...\nMergeError: Merge keys are not unique in right dataset; not a one-to-one merge\n  If the user is aware of the duplicates in the right DataFrame but wants to ensure there are no duplicates in the left DataFrame, one can use the validate='one_to_many' argument instead, which will not raise an exception. \nIn [54]: pd.merge(left, right, on=\"B\", how=\"outer\", validate=\"one_to_many\")\nOut[54]: \n   A_x  B  A_y\n0    1  1  NaN\n1    2  2  4.0\n2    2  2  5.0\n3    2  2  6.0\n    The merge indicator merge() accepts the argument indicator. If True, a Categorical-type column called _merge will be added to the output object that takes on values:  \n      \nObservation Origin _merge value    \nMerge key only in 'left' frame left_only  \nMerge key only in 'right' frame right_only  \nMerge key in both frames both    \n \nIn [55]: df1 = pd.DataFrame({\"col1\": [0, 1], \"col_left\": [\"a\", \"b\"]})\n\nIn [56]: df2 = pd.DataFrame({\"col1\": [1, 2, 2], \"col_right\": [2, 2, 2]})\n\nIn [57]: pd.merge(df1, df2, on=\"col1\", how=\"outer\", indicator=True)\nOut[57]: \n   col1 col_left  col_right      _merge\n0     0        a        NaN   left_only\n1     1        b        2.0        both\n2     2      NaN        2.0  right_only\n3     2      NaN        2.0  right_only\n  The indicator argument will also accept string arguments, in which case the indicator function will use the value of the passed string as the name for the indicator column. \nIn [58]: pd.merge(df1, df2, on=\"col1\", how=\"outer\", indicator=\"indicator_column\")\nOut[58]: \n   col1 col_left  col_right indicator_column\n0     0        a        NaN        left_only\n1     1        b        2.0             both\n2     2      NaN        2.0       right_only\n3     2      NaN        2.0       right_only\n    Merge dtypes Merging will preserve the dtype of the join keys. \nIn [59]: left = pd.DataFrame({\"key\": [1], \"v1\": [10]})\n\nIn [60]: left\nOut[60]: \n   key  v1\n0    1  10\n\nIn [61]: right = pd.DataFrame({\"key\": [1, 2], \"v1\": [20, 30]})\n\nIn [62]: right\nOut[62]: \n   key  v1\n0    1  20\n1    2  30\n  We are able to preserve the join keys: \nIn [63]: pd.merge(left, right, how=\"outer\")\nOut[63]: \n   key  v1\n0    1  10\n1    1  20\n2    2  30\n\nIn [64]: pd.merge(left, right, how=\"outer\").dtypes\nOut[64]: \nkey    int64\nv1     int64\ndtype: object\n  Of course if you have missing values that are introduced, then the resulting dtype will be upcast. \nIn [65]: pd.merge(left, right, how=\"outer\", on=\"key\")\nOut[65]: \n   key  v1_x  v1_y\n0    1  10.0    20\n1    2   NaN    30\n\nIn [66]: pd.merge(left, right, how=\"outer\", on=\"key\").dtypes\nOut[66]: \nkey       int64\nv1_x    float64\nv1_y      int64\ndtype: object\n  Merging will preserve category dtypes of the mergands. See also the section on categoricals. The left frame. \nIn [67]: from pandas.api.types import CategoricalDtype\n\nIn [68]: X = pd.Series(np.random.choice([\"foo\", \"bar\"], size=(10,)))\n\nIn [69]: X = X.astype(CategoricalDtype(categories=[\"foo\", \"bar\"]))\n\nIn [70]: left = pd.DataFrame(\n   ....:     {\"X\": X, \"Y\": np.random.choice([\"one\", \"two\", \"three\"], size=(10,))}\n   ....: )\n   ....: \n\nIn [71]: left\nOut[71]: \n     X      Y\n0  bar    one\n1  foo    one\n2  foo  three\n3  bar  three\n4  foo    one\n5  bar    one\n6  bar  three\n7  bar  three\n8  bar  three\n9  foo  three\n\nIn [72]: left.dtypes\nOut[72]: \nX    category\nY      object\ndtype: object\n  The right frame. \nIn [73]: right = pd.DataFrame(\n   ....:     {\n   ....:         \"X\": pd.Series([\"foo\", \"bar\"], dtype=CategoricalDtype([\"foo\", \"bar\"])),\n   ....:         \"Z\": [1, 2],\n   ....:     }\n   ....: )\n   ....: \n\nIn [74]: right\nOut[74]: \n     X  Z\n0  foo  1\n1  bar  2\n\nIn [75]: right.dtypes\nOut[75]: \nX    category\nZ       int64\ndtype: object\n  The merged result: \nIn [76]: result = pd.merge(left, right, how=\"outer\")\n\nIn [77]: result\nOut[77]: \n     X      Y  Z\n0  bar    one  2\n1  bar  three  2\n2  bar    one  2\n3  bar  three  2\n4  bar  three  2\n5  bar  three  2\n6  foo    one  1\n7  foo  three  1\n8  foo    one  1\n9  foo  three  1\n\nIn [78]: result.dtypes\nOut[78]: \nX    category\nY      object\nZ       int64\ndtype: object\n   Note The category dtypes must be exactly the same, meaning the same categories and the ordered attribute. Otherwise the result will coerce to the categories\u2019 dtype.   Note Merging on category dtypes that are the same can be quite performant compared to object dtype merging.    Joining on index DataFrame.join() is a convenient method for combining the columns of two potentially differently-indexed DataFrames into a single result DataFrame. Here is a very basic example: \nIn [79]: left = pd.DataFrame(\n   ....:     {\"A\": [\"A0\", \"A1\", \"A2\"], \"B\": [\"B0\", \"B1\", \"B2\"]}, index=[\"K0\", \"K1\", \"K2\"]\n   ....: )\n   ....: \n\nIn [80]: right = pd.DataFrame(\n   ....:     {\"C\": [\"C0\", \"C2\", \"C3\"], \"D\": [\"D0\", \"D2\", \"D3\"]}, index=[\"K0\", \"K2\", \"K3\"]\n   ....: )\n   ....: \n\nIn [81]: result = left.join(right)\n   \nIn [82]: result = left.join(right, how=\"outer\")\n   The same as above, but with how='inner'. \nIn [83]: result = left.join(right, how=\"inner\")\n   The data alignment here is on the indexes (row labels). This same behavior can be achieved using merge plus additional arguments instructing it to use the indexes: \nIn [84]: result = pd.merge(left, right, left_index=True, right_index=True, how=\"outer\")\n   \nIn [85]: result = pd.merge(left, right, left_index=True, right_index=True, how=\"inner\")\n     Joining key columns on an index join() takes an optional on argument which may be a column or multiple column names, which specifies that the passed DataFrame is to be aligned on that column in the DataFrame. These two function calls are completely equivalent: \nleft.join(right, on=key_or_keys)\npd.merge(\n    left, right, left_on=key_or_keys, right_index=True, how=\"left\", sort=False\n)\n  Obviously you can choose whichever form you find more convenient. For many-to-one joins (where one of the DataFrame\u2019s is already indexed by the join key), using join may be more convenient. Here is a simple example: \nIn [86]: left = pd.DataFrame(\n   ....:     {\n   ....:         \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n   ....:         \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n   ....:         \"key\": [\"K0\", \"K1\", \"K0\", \"K1\"],\n   ....:     }\n   ....: )\n   ....: \n\nIn [87]: right = pd.DataFrame({\"C\": [\"C0\", \"C1\"], \"D\": [\"D0\", \"D1\"]}, index=[\"K0\", \"K1\"])\n\nIn [88]: result = left.join(right, on=\"key\")\n   \nIn [89]: result = pd.merge(\n   ....:     left, right, left_on=\"key\", right_index=True, how=\"left\", sort=False\n   ....: )\n   ....: \n   To join on multiple keys, the passed DataFrame must have a MultiIndex: \nIn [90]: left = pd.DataFrame(\n   ....:     {\n   ....:         \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n   ....:         \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n   ....:         \"key1\": [\"K0\", \"K0\", \"K1\", \"K2\"],\n   ....:         \"key2\": [\"K0\", \"K1\", \"K0\", \"K1\"],\n   ....:     }\n   ....: )\n   ....: \n\nIn [91]: index = pd.MultiIndex.from_tuples(\n   ....:     [(\"K0\", \"K0\"), (\"K1\", \"K0\"), (\"K2\", \"K0\"), (\"K2\", \"K1\")]\n   ....: )\n   ....: \n\nIn [92]: right = pd.DataFrame(\n   ....:     {\"C\": [\"C0\", \"C1\", \"C2\", \"C3\"], \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"]}, index=index\n   ....: )\n   ....: \n  Now this can be joined by passing the two key column names: \nIn [93]: result = left.join(right, on=[\"key1\", \"key2\"])\n   The default for DataFrame.join is to perform a left join (essentially a \u201cVLOOKUP\u201d operation, for Excel users), which uses only the keys found in the calling DataFrame. Other join types, for example inner join, can be just as easily performed: \nIn [94]: result = left.join(right, on=[\"key1\", \"key2\"], how=\"inner\")\n   As you can see, this drops any rows where there was no match.   Joining a single Index to a MultiIndex You can join a singly-indexed DataFrame with a level of a MultiIndexed DataFrame. The level will match on the name of the index of the singly-indexed frame against a level name of the MultiIndexed frame. \nIn [95]: left = pd.DataFrame(\n   ....:     {\"A\": [\"A0\", \"A1\", \"A2\"], \"B\": [\"B0\", \"B1\", \"B2\"]},\n   ....:     index=pd.Index([\"K0\", \"K1\", \"K2\"], name=\"key\"),\n   ....: )\n   ....: \n\nIn [96]: index = pd.MultiIndex.from_tuples(\n   ....:     [(\"K0\", \"Y0\"), (\"K1\", \"Y1\"), (\"K2\", \"Y2\"), (\"K2\", \"Y3\")],\n   ....:     names=[\"key\", \"Y\"],\n   ....: )\n   ....: \n\nIn [97]: right = pd.DataFrame(\n   ....:     {\"C\": [\"C0\", \"C1\", \"C2\", \"C3\"], \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"]},\n   ....:     index=index,\n   ....: )\n   ....: \n\nIn [98]: result = left.join(right, how=\"inner\")\n   This is equivalent but less verbose and more memory efficient / faster than this. \nIn [99]: result = pd.merge(\n   ....:     left.reset_index(), right.reset_index(), on=[\"key\"], how=\"inner\"\n   ....: ).set_index([\"key\",\"Y\"])\n   ....: \n     Joining with two MultiIndexes This is supported in a limited way, provided that the index for the right argument is completely used in the join, and is a subset of the indices in the left argument, as in this example: \nIn [100]: leftindex = pd.MultiIndex.from_product(\n   .....:     [list(\"abc\"), list(\"xy\"), [1, 2]], names=[\"abc\", \"xy\", \"num\"]\n   .....: )\n   .....: \n\nIn [101]: left = pd.DataFrame({\"v1\": range(12)}, index=leftindex)\n\nIn [102]: left\nOut[102]: \n            v1\nabc xy num    \na   x  1     0\n       2     1\n    y  1     2\n       2     3\nb   x  1     4\n       2     5\n    y  1     6\n       2     7\nc   x  1     8\n       2     9\n    y  1    10\n       2    11\n\nIn [103]: rightindex = pd.MultiIndex.from_product(\n   .....:     [list(\"abc\"), list(\"xy\")], names=[\"abc\", \"xy\"]\n   .....: )\n   .....: \n\nIn [104]: right = pd.DataFrame({\"v2\": [100 * i for i in range(1, 7)]}, index=rightindex)\n\nIn [105]: right\nOut[105]: \n         v2\nabc xy     \na   x   100\n    y   200\nb   x   300\n    y   400\nc   x   500\n    y   600\n\nIn [106]: left.join(right, on=[\"abc\", \"xy\"], how=\"inner\")\nOut[106]: \n            v1   v2\nabc xy num         \na   x  1     0  100\n       2     1  100\n    y  1     2  200\n       2     3  200\nb   x  1     4  300\n       2     5  300\n    y  1     6  400\n       2     7  400\nc   x  1     8  500\n       2     9  500\n    y  1    10  600\n       2    11  600\n  If that condition is not satisfied, a join with two multi-indexes can be done using the following code. \nIn [107]: leftindex = pd.MultiIndex.from_tuples(\n   .....:     [(\"K0\", \"X0\"), (\"K0\", \"X1\"), (\"K1\", \"X2\")], names=[\"key\", \"X\"]\n   .....: )\n   .....: \n\nIn [108]: left = pd.DataFrame(\n   .....:     {\"A\": [\"A0\", \"A1\", \"A2\"], \"B\": [\"B0\", \"B1\", \"B2\"]}, index=leftindex\n   .....: )\n   .....: \n\nIn [109]: rightindex = pd.MultiIndex.from_tuples(\n   .....:     [(\"K0\", \"Y0\"), (\"K1\", \"Y1\"), (\"K2\", \"Y2\"), (\"K2\", \"Y3\")], names=[\"key\", \"Y\"]\n   .....: )\n   .....: \n\nIn [110]: right = pd.DataFrame(\n   .....:     {\"C\": [\"C0\", \"C1\", \"C2\", \"C3\"], \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"]}, index=rightindex\n   .....: )\n   .....: \n\nIn [111]: result = pd.merge(\n   .....:     left.reset_index(), right.reset_index(), on=[\"key\"], how=\"inner\"\n   .....: ).set_index([\"key\", \"X\", \"Y\"])\n   .....: \n     Merging on a combination of columns and index levels Strings passed as the on, left_on, and right_on parameters may refer to either column names or index level names. This enables merging DataFrame instances on a combination of index levels and columns without resetting indexes. \nIn [112]: left_index = pd.Index([\"K0\", \"K0\", \"K1\", \"K2\"], name=\"key1\")\n\nIn [113]: left = pd.DataFrame(\n   .....:     {\n   .....:         \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n   .....:         \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n   .....:         \"key2\": [\"K0\", \"K1\", \"K0\", \"K1\"],\n   .....:     },\n   .....:     index=left_index,\n   .....: )\n   .....: \n\nIn [114]: right_index = pd.Index([\"K0\", \"K1\", \"K2\", \"K2\"], name=\"key1\")\n\nIn [115]: right = pd.DataFrame(\n   .....:     {\n   .....:         \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n   .....:         \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n   .....:         \"key2\": [\"K0\", \"K0\", \"K0\", \"K1\"],\n   .....:     },\n   .....:     index=right_index,\n   .....: )\n   .....: \n\nIn [116]: result = left.merge(right, on=[\"key1\", \"key2\"])\n    Note When DataFrames are merged on a string that matches an index level in both frames, the index level is preserved as an index level in the resulting DataFrame.   Note When DataFrames are merged using only some of the levels of a MultiIndex, the extra levels will be dropped from the resulting merge. In order to preserve those levels, use reset_index on those level names to move those levels to columns prior to doing the merge.   Note If a string matches both a column name and an index level name, then a warning is issued and the column takes precedence. This will result in an ambiguity error in a future version.    Overlapping value columns The merge suffixes argument takes a tuple of list of strings to append to overlapping column names in the input DataFrames to disambiguate the result columns: \nIn [117]: left = pd.DataFrame({\"k\": [\"K0\", \"K1\", \"K2\"], \"v\": [1, 2, 3]})\n\nIn [118]: right = pd.DataFrame({\"k\": [\"K0\", \"K0\", \"K3\"], \"v\": [4, 5, 6]})\n\nIn [119]: result = pd.merge(left, right, on=\"k\")\n   \nIn [120]: result = pd.merge(left, right, on=\"k\", suffixes=(\"_l\", \"_r\"))\n   DataFrame.join() has lsuffix and rsuffix arguments which behave similarly. \nIn [121]: left = left.set_index(\"k\")\n\nIn [122]: right = right.set_index(\"k\")\n\nIn [123]: result = left.join(right, lsuffix=\"_l\", rsuffix=\"_r\")\n     Joining multiple DataFrames A list or tuple of DataFrames can also be passed to join() to join them together on their indexes. \nIn [124]: right2 = pd.DataFrame({\"v\": [7, 8, 9]}, index=[\"K1\", \"K1\", \"K2\"])\n\nIn [125]: result = left.join([right, right2])\n     Merging together values within Series or DataFrame columns Another fairly common situation is to have two like-indexed (or similarly indexed) Series or DataFrame objects and wanting to \u201cpatch\u201d values in one object from values for matching indices in the other. Here is an example: \nIn [126]: df1 = pd.DataFrame(\n   .....:     [[np.nan, 3.0, 5.0], [-4.6, np.nan, np.nan], [np.nan, 7.0, np.nan]]\n   .....: )\n   .....: \n\nIn [127]: df2 = pd.DataFrame([[-42.6, np.nan, -8.2], [-5.0, 1.6, 4]], index=[1, 2])\n  For this, use the combine_first() method: \nIn [128]: result = df1.combine_first(df2)\n   Note that this method only takes values from the right DataFrame if they are missing in the left DataFrame. A related method, update(), alters non-NA values in place: \nIn [129]: df1.update(df2)\n      Timeseries friendly merging  Merging ordered data A merge_ordered() function allows combining time series and other ordered data. In particular it has an optional fill_method keyword to fill/interpolate missing data: \nIn [130]: left = pd.DataFrame(\n   .....:     {\"k\": [\"K0\", \"K1\", \"K1\", \"K2\"], \"lv\": [1, 2, 3, 4], \"s\": [\"a\", \"b\", \"c\", \"d\"]}\n   .....: )\n   .....: \n\nIn [131]: right = pd.DataFrame({\"k\": [\"K1\", \"K2\", \"K4\"], \"rv\": [1, 2, 3]})\n\nIn [132]: pd.merge_ordered(left, right, fill_method=\"ffill\", left_by=\"s\")\nOut[132]: \n     k   lv  s   rv\n0   K0  1.0  a  NaN\n1   K1  1.0  a  1.0\n2   K2  1.0  a  2.0\n3   K4  1.0  a  3.0\n4   K1  2.0  b  1.0\n5   K2  2.0  b  2.0\n6   K4  2.0  b  3.0\n7   K1  3.0  c  1.0\n8   K2  3.0  c  2.0\n9   K4  3.0  c  3.0\n10  K1  NaN  d  1.0\n11  K2  4.0  d  2.0\n12  K4  4.0  d  3.0\n    Merging asof A merge_asof() is similar to an ordered left-join except that we match on nearest key rather than equal keys. For each row in the left DataFrame, we select the last row in the right DataFrame whose on key is less than the left\u2019s key. Both DataFrames must be sorted by the key. Optionally an asof merge can perform a group-wise merge. This matches the by key equally, in addition to the nearest match on the on key. For example; we might have trades and quotes and we want to asof merge them. \nIn [133]: trades = pd.DataFrame(\n   .....:     {\n   .....:         \"time\": pd.to_datetime(\n   .....:             [\n   .....:                 \"20160525 13:30:00.023\",\n   .....:                 \"20160525 13:30:00.038\",\n   .....:                 \"20160525 13:30:00.048\",\n   .....:                 \"20160525 13:30:00.048\",\n   .....:                 \"20160525 13:30:00.048\",\n   .....:             ]\n   .....:         ),\n   .....:         \"ticker\": [\"MSFT\", \"MSFT\", \"GOOG\", \"GOOG\", \"AAPL\"],\n   .....:         \"price\": [51.95, 51.95, 720.77, 720.92, 98.00],\n   .....:         \"quantity\": [75, 155, 100, 100, 100],\n   .....:     },\n   .....:     columns=[\"time\", \"ticker\", \"price\", \"quantity\"],\n   .....: )\n   .....: \n\nIn [134]: quotes = pd.DataFrame(\n   .....:     {\n   .....:         \"time\": pd.to_datetime(\n   .....:             [\n   .....:                 \"20160525 13:30:00.023\",\n   .....:                 \"20160525 13:30:00.023\",\n   .....:                 \"20160525 13:30:00.030\",\n   .....:                 \"20160525 13:30:00.041\",\n   .....:                 \"20160525 13:30:00.048\",\n   .....:                 \"20160525 13:30:00.049\",\n   .....:                 \"20160525 13:30:00.072\",\n   .....:                 \"20160525 13:30:00.075\",\n   .....:             ]\n   .....:         ),\n   .....:         \"ticker\": [\"GOOG\", \"MSFT\", \"MSFT\", \"MSFT\", \"GOOG\", \"AAPL\", \"GOOG\", \"MSFT\"],\n   .....:         \"bid\": [720.50, 51.95, 51.97, 51.99, 720.50, 97.99, 720.50, 52.01],\n   .....:         \"ask\": [720.93, 51.96, 51.98, 52.00, 720.93, 98.01, 720.88, 52.03],\n   .....:     },\n   .....:     columns=[\"time\", \"ticker\", \"bid\", \"ask\"],\n   .....: )\n   .....: \n  \nIn [135]: trades\nOut[135]: \n                     time ticker   price  quantity\n0 2016-05-25 13:30:00.023   MSFT   51.95        75\n1 2016-05-25 13:30:00.038   MSFT   51.95       155\n2 2016-05-25 13:30:00.048   GOOG  720.77       100\n3 2016-05-25 13:30:00.048   GOOG  720.92       100\n4 2016-05-25 13:30:00.048   AAPL   98.00       100\n\nIn [136]: quotes\nOut[136]: \n                     time ticker     bid     ask\n0 2016-05-25 13:30:00.023   GOOG  720.50  720.93\n1 2016-05-25 13:30:00.023   MSFT   51.95   51.96\n2 2016-05-25 13:30:00.030   MSFT   51.97   51.98\n3 2016-05-25 13:30:00.041   MSFT   51.99   52.00\n4 2016-05-25 13:30:00.048   GOOG  720.50  720.93\n5 2016-05-25 13:30:00.049   AAPL   97.99   98.01\n6 2016-05-25 13:30:00.072   GOOG  720.50  720.88\n7 2016-05-25 13:30:00.075   MSFT   52.01   52.03\n  By default we are taking the asof of the quotes. \nIn [137]: pd.merge_asof(trades, quotes, on=\"time\", by=\"ticker\")\nOut[137]: \n                     time ticker   price  quantity     bid     ask\n0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96\n1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.98\n2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93\n3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93\n4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\n  We only asof within 2ms between the quote time and the trade time. \nIn [138]: pd.merge_asof(trades, quotes, on=\"time\", by=\"ticker\", tolerance=pd.Timedelta(\"2ms\"))\nOut[138]: \n                     time ticker   price  quantity     bid     ask\n0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96\n1 2016-05-25 13:30:00.038   MSFT   51.95       155     NaN     NaN\n2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93\n3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93\n4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\n  We only asof within 10ms between the quote time and the trade time and we exclude exact matches on time. Note that though we exclude the exact matches (of the quotes), prior quotes do propagate to that point in time. \nIn [139]: pd.merge_asof(\n   .....:     trades,\n   .....:     quotes,\n   .....:     on=\"time\",\n   .....:     by=\"ticker\",\n   .....:     tolerance=pd.Timedelta(\"10ms\"),\n   .....:     allow_exact_matches=False,\n   .....: )\n   .....: \nOut[139]: \n                     time ticker   price  quantity    bid    ask\n0 2016-05-25 13:30:00.023   MSFT   51.95        75    NaN    NaN\n1 2016-05-25 13:30:00.038   MSFT   51.95       155  51.97  51.98\n2 2016-05-25 13:30:00.048   GOOG  720.77       100    NaN    NaN\n3 2016-05-25 13:30:00.048   GOOG  720.92       100    NaN    NaN\n4 2016-05-25 13:30:00.048   AAPL   98.00       100    NaN    NaN\n     Comparing objects The compare() and compare() methods allow you to compare two DataFrame or Series, respectively, and summarize their differences. This feature was added in V1.1.0. For example, you might want to compare two DataFrame and stack their differences side by side. \nIn [140]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"col1\": [\"a\", \"a\", \"b\", \"b\", \"a\"],\n   .....:         \"col2\": [1.0, 2.0, 3.0, np.nan, 5.0],\n   .....:         \"col3\": [1.0, 2.0, 3.0, 4.0, 5.0],\n   .....:     },\n   .....:     columns=[\"col1\", \"col2\", \"col3\"],\n   .....: )\n   .....: \n\nIn [141]: df\nOut[141]: \n  col1  col2  col3\n0    a   1.0   1.0\n1    a   2.0   2.0\n2    b   3.0   3.0\n3    b   NaN   4.0\n4    a   5.0   5.0\n  \nIn [142]: df2 = df.copy()\n\nIn [143]: df2.loc[0, \"col1\"] = \"c\"\n\nIn [144]: df2.loc[2, \"col3\"] = 4.0\n\nIn [145]: df2\nOut[145]: \n  col1  col2  col3\n0    c   1.0   1.0\n1    a   2.0   2.0\n2    b   3.0   4.0\n3    b   NaN   4.0\n4    a   5.0   5.0\n  \nIn [146]: df.compare(df2)\nOut[146]: \n  col1       col3      \n  self other self other\n0    a     c  NaN   NaN\n2  NaN   NaN  3.0   4.0\n  By default, if two corresponding values are equal, they will be shown as NaN. Furthermore, if all values in an entire row / column, the row / column will be omitted from the result. The remaining differences will be aligned on columns. If you wish, you may choose to stack the differences on rows. \nIn [147]: df.compare(df2, align_axis=0)\nOut[147]: \n        col1  col3\n0 self     a   NaN\n  other    c   NaN\n2 self   NaN   3.0\n  other  NaN   4.0\n  If you wish to keep all original rows and columns, set keep_shape argument to True. \nIn [148]: df.compare(df2, keep_shape=True)\nOut[148]: \n  col1       col2       col3      \n  self other self other self other\n0    a     c  NaN   NaN  NaN   NaN\n1  NaN   NaN  NaN   NaN  NaN   NaN\n2  NaN   NaN  NaN   NaN  3.0   4.0\n3  NaN   NaN  NaN   NaN  NaN   NaN\n4  NaN   NaN  NaN   NaN  NaN   NaN\n  You may also keep all the original values even if they are equal. \nIn [149]: df.compare(df2, keep_shape=True, keep_equal=True)\nOut[149]: \n  col1       col2       col3      \n  self other self other self other\n0    a     c  1.0   1.0  1.0   1.0\n1    a     a  2.0   2.0  2.0   2.0\n2    b     b  3.0   3.0  3.0   4.0\n3    b     b  NaN   NaN  4.0   4.0\n4    a     a  5.0   5.0  5.0   5.0\n  \n"}, {"name": "MultiIndex / advanced indexing", "path": "user_guide/advanced", "type": "Manual", "text": "MultiIndex / advanced indexing This section covers indexing with a MultiIndex and other advanced indexing features. See the Indexing and Selecting Data for general indexing documentation.  Warning Whether a copy or a reference is returned for a setting operation may depend on the context. This is sometimes called chained assignment and should be avoided. See Returning a View versus Copy.  See the cookbook for some advanced strategies.  Hierarchical indexing (MultiIndex) Hierarchical / Multi-level indexing is very exciting as it opens the door to some quite sophisticated data analysis and manipulation, especially for working with higher dimensional data. In essence, it enables you to store and manipulate data with an arbitrary number of dimensions in lower dimensional data structures like Series (1d) and DataFrame (2d). In this section, we will show what exactly we mean by \u201chierarchical\u201d indexing and how it integrates with all of the pandas indexing functionality described above and in prior sections. Later, when discussing group by and pivoting and reshaping data, we\u2019ll show non-trivial applications to illustrate how it aids in structuring data for analysis. See the cookbook for some advanced strategies.  Creating a MultiIndex (hierarchical index) object The MultiIndex object is the hierarchical analogue of the standard Index object which typically stores the axis labels in pandas objects. You can think of MultiIndex as an array of tuples where each tuple is unique. A MultiIndex can be created from a list of arrays (using MultiIndex.from_arrays()), an array of tuples (using MultiIndex.from_tuples()), a crossed set of iterables (using MultiIndex.from_product()), or a DataFrame (using MultiIndex.from_frame()). The Index constructor will attempt to return a MultiIndex when it is passed a list of tuples. The following examples demonstrate different ways to initialize MultiIndexes. \nIn [1]: arrays = [\n   ...:     [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"],\n   ...:     [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"],\n   ...: ]\n   ...: \n\nIn [2]: tuples = list(zip(*arrays))\n\nIn [3]: tuples\nOut[3]: \n[('bar', 'one'),\n ('bar', 'two'),\n ('baz', 'one'),\n ('baz', 'two'),\n ('foo', 'one'),\n ('foo', 'two'),\n ('qux', 'one'),\n ('qux', 'two')]\n\nIn [4]: index = pd.MultiIndex.from_tuples(tuples, names=[\"first\", \"second\"])\n\nIn [5]: index\nOut[5]: \nMultiIndex([('bar', 'one'),\n            ('bar', 'two'),\n            ('baz', 'one'),\n            ('baz', 'two'),\n            ('foo', 'one'),\n            ('foo', 'two'),\n            ('qux', 'one'),\n            ('qux', 'two')],\n           names=['first', 'second'])\n\nIn [6]: s = pd.Series(np.random.randn(8), index=index)\n\nIn [7]: s\nOut[7]: \nfirst  second\nbar    one       0.469112\n       two      -0.282863\nbaz    one      -1.509059\n       two      -1.135632\nfoo    one       1.212112\n       two      -0.173215\nqux    one       0.119209\n       two      -1.044236\ndtype: float64\n  When you want every pairing of the elements in two iterables, it can be easier to use the MultiIndex.from_product() method: \nIn [8]: iterables = [[\"bar\", \"baz\", \"foo\", \"qux\"], [\"one\", \"two\"]]\n\nIn [9]: pd.MultiIndex.from_product(iterables, names=[\"first\", \"second\"])\nOut[9]: \nMultiIndex([('bar', 'one'),\n            ('bar', 'two'),\n            ('baz', 'one'),\n            ('baz', 'two'),\n            ('foo', 'one'),\n            ('foo', 'two'),\n            ('qux', 'one'),\n            ('qux', 'two')],\n           names=['first', 'second'])\n  You can also construct a MultiIndex from a DataFrame directly, using the method MultiIndex.from_frame(). This is a complementary method to MultiIndex.to_frame(). \nIn [10]: df = pd.DataFrame(\n   ....:     [[\"bar\", \"one\"], [\"bar\", \"two\"], [\"foo\", \"one\"], [\"foo\", \"two\"]],\n   ....:     columns=[\"first\", \"second\"],\n   ....: )\n   ....: \n\nIn [11]: pd.MultiIndex.from_frame(df)\nOut[11]: \nMultiIndex([('bar', 'one'),\n            ('bar', 'two'),\n            ('foo', 'one'),\n            ('foo', 'two')],\n           names=['first', 'second'])\n  As a convenience, you can pass a list of arrays directly into Series or DataFrame to construct a MultiIndex automatically: \nIn [12]: arrays = [\n   ....:     np.array([\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"]),\n   ....:     np.array([\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"]),\n   ....: ]\n   ....: \n\nIn [13]: s = pd.Series(np.random.randn(8), index=arrays)\n\nIn [14]: s\nOut[14]: \nbar  one   -0.861849\n     two   -2.104569\nbaz  one   -0.494929\n     two    1.071804\nfoo  one    0.721555\n     two   -0.706771\nqux  one   -1.039575\n     two    0.271860\ndtype: float64\n\nIn [15]: df = pd.DataFrame(np.random.randn(8, 4), index=arrays)\n\nIn [16]: df\nOut[16]: \n                0         1         2         3\nbar one -0.424972  0.567020  0.276232 -1.087401\n    two -0.673690  0.113648 -1.478427  0.524988\nbaz one  0.404705  0.577046 -1.715002 -1.039268\n    two -0.370647 -1.157892 -1.344312  0.844885\nfoo one  1.075770 -0.109050  1.643563 -1.469388\n    two  0.357021 -0.674600 -1.776904 -0.968914\nqux one -1.294524  0.413738  0.276662 -0.472035\n    two -0.013960 -0.362543 -0.006154 -0.923061\n  All of the MultiIndex constructors accept a names argument which stores string names for the levels themselves. If no names are provided, None will be assigned: \nIn [17]: df.index.names\nOut[17]: FrozenList([None, None])\n  This index can back any axis of a pandas object, and the number of levels of the index is up to you: \nIn [18]: df = pd.DataFrame(np.random.randn(3, 8), index=[\"A\", \"B\", \"C\"], columns=index)\n\nIn [19]: df\nOut[19]: \nfirst        bar                 baz                 foo                 qux          \nsecond       one       two       one       two       one       two       one       two\nA       0.895717  0.805244 -1.206412  2.565646  1.431256  1.340309 -1.170299 -0.226169\nB       0.410835  0.813850  0.132003 -0.827317 -0.076467 -1.187678  1.130127 -1.436737\nC      -1.413681  1.607920  1.024180  0.569605  0.875906 -2.211372  0.974466 -2.006747\n\nIn [20]: pd.DataFrame(np.random.randn(6, 6), index=index[:6], columns=index[:6])\nOut[20]: \nfirst              bar                 baz                 foo          \nsecond             one       two       one       two       one       two\nfirst second                                                            \nbar   one    -0.410001 -0.078638  0.545952 -1.219217 -1.226825  0.769804\n      two    -1.281247 -0.727707 -0.121306 -0.097883  0.695775  0.341734\nbaz   one     0.959726 -1.110336 -0.619976  0.149748 -0.732339  0.687738\n      two     0.176444  0.403310 -0.154951  0.301624 -2.179861 -1.369849\nfoo   one    -0.954208  1.462696 -1.743161 -0.826591 -0.345352  1.314232\n      two     0.690579  0.995761  2.396780  0.014871  3.357427 -0.317441\n  We\u2019ve \u201csparsified\u201d the higher levels of the indexes to make the console output a bit easier on the eyes. Note that how the index is displayed can be controlled using the multi_sparse option in pandas.set_options(): \nIn [21]: with pd.option_context(\"display.multi_sparse\", False):\n   ....:     df\n   ....: \n  It\u2019s worth keeping in mind that there\u2019s nothing preventing you from using tuples as atomic labels on an axis: \nIn [22]: pd.Series(np.random.randn(8), index=tuples)\nOut[22]: \n(bar, one)   -1.236269\n(bar, two)    0.896171\n(baz, one)   -0.487602\n(baz, two)   -0.082240\n(foo, one)   -2.182937\n(foo, two)    0.380396\n(qux, one)    0.084844\n(qux, two)    0.432390\ndtype: float64\n  The reason that the MultiIndex matters is that it can allow you to do grouping, selection, and reshaping operations as we will describe below and in subsequent areas of the documentation. As you will see in later sections, you can find yourself working with hierarchically-indexed data without creating a MultiIndex explicitly yourself. However, when loading data from a file, you may wish to generate your own MultiIndex when preparing the data set.   Reconstructing the level labels The method get_level_values() will return a vector of the labels for each location at a particular level: \nIn [23]: index.get_level_values(0)\nOut[23]: Index(['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], dtype='object', name='first')\n\nIn [24]: index.get_level_values(\"second\")\nOut[24]: Index(['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two'], dtype='object', name='second')\n    Basic indexing on axis with MultiIndex One of the important features of hierarchical indexing is that you can select data by a \u201cpartial\u201d label identifying a subgroup in the data. Partial selection \u201cdrops\u201d levels of the hierarchical index in the result in a completely analogous way to selecting a column in a regular DataFrame: \nIn [25]: df[\"bar\"]\nOut[25]: \nsecond       one       two\nA       0.895717  0.805244\nB       0.410835  0.813850\nC      -1.413681  1.607920\n\nIn [26]: df[\"bar\", \"one\"]\nOut[26]: \nA    0.895717\nB    0.410835\nC   -1.413681\nName: (bar, one), dtype: float64\n\nIn [27]: df[\"bar\"][\"one\"]\nOut[27]: \nA    0.895717\nB    0.410835\nC   -1.413681\nName: one, dtype: float64\n\nIn [28]: s[\"qux\"]\nOut[28]: \none   -1.039575\ntwo    0.271860\ndtype: float64\n  See Cross-section with hierarchical index for how to select on a deeper level.   Defined levels The MultiIndex keeps all the defined levels of an index, even if they are not actually used. When slicing an index, you may notice this. For example: \nIn [29]: df.columns.levels  # original MultiIndex\nOut[29]: FrozenList([['bar', 'baz', 'foo', 'qux'], ['one', 'two']])\n\nIn [30]: df[[\"foo\",\"qux\"]].columns.levels  # sliced\nOut[30]: FrozenList([['bar', 'baz', 'foo', 'qux'], ['one', 'two']])\n  This is done to avoid a recomputation of the levels in order to make slicing highly performant. If you want to see only the used levels, you can use the get_level_values() method. \nIn [31]: df[[\"foo\", \"qux\"]].columns.to_numpy()\nOut[31]: \narray([('foo', 'one'), ('foo', 'two'), ('qux', 'one'), ('qux', 'two')],\n      dtype=object)\n\n# for a specific level\nIn [32]: df[[\"foo\", \"qux\"]].columns.get_level_values(0)\nOut[32]: Index(['foo', 'foo', 'qux', 'qux'], dtype='object', name='first')\n  To reconstruct the MultiIndex with only the used levels, the remove_unused_levels() method may be used. \nIn [33]: new_mi = df[[\"foo\", \"qux\"]].columns.remove_unused_levels()\n\nIn [34]: new_mi.levels\nOut[34]: FrozenList([['foo', 'qux'], ['one', 'two']])\n    Data alignment and using reindex\n Operations between differently-indexed objects having MultiIndex on the axes will work as you expect; data alignment will work the same as an Index of tuples: \nIn [35]: s + s[:-2]\nOut[35]: \nbar  one   -1.723698\n     two   -4.209138\nbaz  one   -0.989859\n     two    2.143608\nfoo  one    1.443110\n     two   -1.413542\nqux  one         NaN\n     two         NaN\ndtype: float64\n\nIn [36]: s + s[::2]\nOut[36]: \nbar  one   -1.723698\n     two         NaN\nbaz  one   -0.989859\n     two         NaN\nfoo  one    1.443110\n     two         NaN\nqux  one   -2.079150\n     two         NaN\ndtype: float64\n  The reindex() method of Series/DataFrames can be called with another MultiIndex, or even a list or array of tuples: \nIn [37]: s.reindex(index[:3])\nOut[37]: \nfirst  second\nbar    one      -0.861849\n       two      -2.104569\nbaz    one      -0.494929\ndtype: float64\n\nIn [38]: s.reindex([(\"foo\", \"two\"), (\"bar\", \"one\"), (\"qux\", \"one\"), (\"baz\", \"one\")])\nOut[38]: \nfoo  two   -0.706771\nbar  one   -0.861849\nqux  one   -1.039575\nbaz  one   -0.494929\ndtype: float64\n     Advanced indexing with hierarchical index Syntactically integrating MultiIndex in advanced indexing with .loc is a bit challenging, but we\u2019ve made every effort to do so. In general, MultiIndex keys take the form of tuples. For example, the following works as you would expect: \nIn [39]: df = df.T\n\nIn [40]: df\nOut[40]: \n                     A         B         C\nfirst second                              \nbar   one     0.895717  0.410835 -1.413681\n      two     0.805244  0.813850  1.607920\nbaz   one    -1.206412  0.132003  1.024180\n      two     2.565646 -0.827317  0.569605\nfoo   one     1.431256 -0.076467  0.875906\n      two     1.340309 -1.187678 -2.211372\nqux   one    -1.170299  1.130127  0.974466\n      two    -0.226169 -1.436737 -2.006747\n\nIn [41]: df.loc[(\"bar\", \"two\")]\nOut[41]: \nA    0.805244\nB    0.813850\nC    1.607920\nName: (bar, two), dtype: float64\n  Note that df.loc['bar', 'two'] would also work in this example, but this shorthand notation can lead to ambiguity in general. If you also want to index a specific column with .loc, you must use a tuple like this: \nIn [42]: df.loc[(\"bar\", \"two\"), \"A\"]\nOut[42]: 0.8052440253863785\n  You don\u2019t have to specify all levels of the MultiIndex by passing only the first elements of the tuple. For example, you can use \u201cpartial\u201d indexing to get all elements with bar in the first level as follows: \nIn [43]: df.loc[\"bar\"]\nOut[43]: \n               A         B         C\nsecond                              \none     0.895717  0.410835 -1.413681\ntwo     0.805244  0.813850  1.607920\n  This is a shortcut for the slightly more verbose notation df.loc[('bar',),] (equivalent to df.loc['bar',] in this example). \u201cPartial\u201d slicing also works quite nicely. \nIn [44]: df.loc[\"baz\":\"foo\"]\nOut[44]: \n                     A         B         C\nfirst second                              \nbaz   one    -1.206412  0.132003  1.024180\n      two     2.565646 -0.827317  0.569605\nfoo   one     1.431256 -0.076467  0.875906\n      two     1.340309 -1.187678 -2.211372\n  You can slice with a \u2018range\u2019 of values, by providing a slice of tuples. \nIn [45]: df.loc[(\"baz\", \"two\"):(\"qux\", \"one\")]\nOut[45]: \n                     A         B         C\nfirst second                              \nbaz   two     2.565646 -0.827317  0.569605\nfoo   one     1.431256 -0.076467  0.875906\n      two     1.340309 -1.187678 -2.211372\nqux   one    -1.170299  1.130127  0.974466\n\nIn [46]: df.loc[(\"baz\", \"two\"):\"foo\"]\nOut[46]: \n                     A         B         C\nfirst second                              \nbaz   two     2.565646 -0.827317  0.569605\nfoo   one     1.431256 -0.076467  0.875906\n      two     1.340309 -1.187678 -2.211372\n  Passing a list of labels or tuples works similar to reindexing: \nIn [47]: df.loc[[(\"bar\", \"two\"), (\"qux\", \"one\")]]\nOut[47]: \n                     A         B         C\nfirst second                              \nbar   two     0.805244  0.813850  1.607920\nqux   one    -1.170299  1.130127  0.974466\n   Note It is important to note that tuples and lists are not treated identically in pandas when it comes to indexing. Whereas a tuple is interpreted as one multi-level key, a list is used to specify several keys. Or in other words, tuples go horizontally (traversing levels), lists go vertically (scanning levels).  Importantly, a list of tuples indexes several complete MultiIndex keys, whereas a tuple of lists refer to several values within a level: \nIn [48]: s = pd.Series(\n   ....:     [1, 2, 3, 4, 5, 6],\n   ....:     index=pd.MultiIndex.from_product([[\"A\", \"B\"], [\"c\", \"d\", \"e\"]]),\n   ....: )\n   ....: \n\nIn [49]: s.loc[[(\"A\", \"c\"), (\"B\", \"d\")]]  # list of tuples\nOut[49]: \nA  c    1\nB  d    5\ndtype: int64\n\nIn [50]: s.loc[([\"A\", \"B\"], [\"c\", \"d\"])]  # tuple of lists\nOut[50]: \nA  c    1\n   d    2\nB  c    4\n   d    5\ndtype: int64\n   Using slicers You can slice a MultiIndex by providing multiple indexers. You can provide any of the selectors as if you are indexing by label, see Selection by Label, including slices, lists of labels, labels, and boolean indexers. You can use slice(None) to select all the contents of that level. You do not need to specify all the deeper levels, they will be implied as slice(None). As usual, both sides of the slicers are included as this is label indexing.  Warning You should specify all axes in the .loc specifier, meaning the indexer for the index and for the columns. There are some ambiguous cases where the passed indexer could be mis-interpreted as indexing both axes, rather than into say the MultiIndex for the rows. You should do this: \ndf.loc[(slice(\"A1\", \"A3\"), ...), :]  # noqa: E999\n  You should not do this: \ndf.loc[(slice(\"A1\", \"A3\"), ...)]  # noqa: E999\n   \nIn [51]: def mklbl(prefix, n):\n   ....:     return [\"%s%s\" % (prefix, i) for i in range(n)]\n   ....: \n\nIn [52]: miindex = pd.MultiIndex.from_product(\n   ....:     [mklbl(\"A\", 4), mklbl(\"B\", 2), mklbl(\"C\", 4), mklbl(\"D\", 2)]\n   ....: )\n   ....: \n\nIn [53]: micolumns = pd.MultiIndex.from_tuples(\n   ....:     [(\"a\", \"foo\"), (\"a\", \"bar\"), (\"b\", \"foo\"), (\"b\", \"bah\")], names=[\"lvl0\", \"lvl1\"]\n   ....: )\n   ....: \n\nIn [54]: dfmi = (\n   ....:     pd.DataFrame(\n   ....:         np.arange(len(miindex) * len(micolumns)).reshape(\n   ....:             (len(miindex), len(micolumns))\n   ....:         ),\n   ....:         index=miindex,\n   ....:         columns=micolumns,\n   ....:     )\n   ....:     .sort_index()\n   ....:     .sort_index(axis=1)\n   ....: )\n   ....: \n\nIn [55]: dfmi\nOut[55]: \nlvl0           a         b     \nlvl1         bar  foo  bah  foo\nA0 B0 C0 D0    1    0    3    2\n         D1    5    4    7    6\n      C1 D0    9    8   11   10\n         D1   13   12   15   14\n      C2 D0   17   16   19   18\n...          ...  ...  ...  ...\nA3 B1 C1 D1  237  236  239  238\n      C2 D0  241  240  243  242\n         D1  245  244  247  246\n      C3 D0  249  248  251  250\n         D1  253  252  255  254\n\n[64 rows x 4 columns]\n  Basic MultiIndex slicing using slices, lists, and labels. \nIn [56]: dfmi.loc[(slice(\"A1\", \"A3\"), slice(None), [\"C1\", \"C3\"]), :]\nOut[56]: \nlvl0           a         b     \nlvl1         bar  foo  bah  foo\nA1 B0 C1 D0   73   72   75   74\n         D1   77   76   79   78\n      C3 D0   89   88   91   90\n         D1   93   92   95   94\n   B1 C1 D0  105  104  107  106\n...          ...  ...  ...  ...\nA3 B0 C3 D1  221  220  223  222\n   B1 C1 D0  233  232  235  234\n         D1  237  236  239  238\n      C3 D0  249  248  251  250\n         D1  253  252  255  254\n\n[24 rows x 4 columns]\n  You can use pandas.IndexSlice to facilitate a more natural syntax using :, rather than using slice(None). \nIn [57]: idx = pd.IndexSlice\n\nIn [58]: dfmi.loc[idx[:, :, [\"C1\", \"C3\"]], idx[:, \"foo\"]]\nOut[58]: \nlvl0           a    b\nlvl1         foo  foo\nA0 B0 C1 D0    8   10\n         D1   12   14\n      C3 D0   24   26\n         D1   28   30\n   B1 C1 D0   40   42\n...          ...  ...\nA3 B0 C3 D1  220  222\n   B1 C1 D0  232  234\n         D1  236  238\n      C3 D0  248  250\n         D1  252  254\n\n[32 rows x 2 columns]\n  It is possible to perform quite complicated selections using this method on multiple axes at the same time. \nIn [59]: dfmi.loc[\"A1\", (slice(None), \"foo\")]\nOut[59]: \nlvl0        a    b\nlvl1      foo  foo\nB0 C0 D0   64   66\n      D1   68   70\n   C1 D0   72   74\n      D1   76   78\n   C2 D0   80   82\n...       ...  ...\nB1 C1 D1  108  110\n   C2 D0  112  114\n      D1  116  118\n   C3 D0  120  122\n      D1  124  126\n\n[16 rows x 2 columns]\n\nIn [60]: dfmi.loc[idx[:, :, [\"C1\", \"C3\"]], idx[:, \"foo\"]]\nOut[60]: \nlvl0           a    b\nlvl1         foo  foo\nA0 B0 C1 D0    8   10\n         D1   12   14\n      C3 D0   24   26\n         D1   28   30\n   B1 C1 D0   40   42\n...          ...  ...\nA3 B0 C3 D1  220  222\n   B1 C1 D0  232  234\n         D1  236  238\n      C3 D0  248  250\n         D1  252  254\n\n[32 rows x 2 columns]\n  Using a boolean indexer you can provide selection related to the values. \nIn [61]: mask = dfmi[(\"a\", \"foo\")] > 200\n\nIn [62]: dfmi.loc[idx[mask, :, [\"C1\", \"C3\"]], idx[:, \"foo\"]]\nOut[62]: \nlvl0           a    b\nlvl1         foo  foo\nA3 B0 C1 D1  204  206\n      C3 D0  216  218\n         D1  220  222\n   B1 C1 D0  232  234\n         D1  236  238\n      C3 D0  248  250\n         D1  252  254\n  You can also specify the axis argument to .loc to interpret the passed slicers on a single axis. \nIn [63]: dfmi.loc(axis=0)[:, :, [\"C1\", \"C3\"]]\nOut[63]: \nlvl0           a         b     \nlvl1         bar  foo  bah  foo\nA0 B0 C1 D0    9    8   11   10\n         D1   13   12   15   14\n      C3 D0   25   24   27   26\n         D1   29   28   31   30\n   B1 C1 D0   41   40   43   42\n...          ...  ...  ...  ...\nA3 B0 C3 D1  221  220  223  222\n   B1 C1 D0  233  232  235  234\n         D1  237  236  239  238\n      C3 D0  249  248  251  250\n         D1  253  252  255  254\n\n[32 rows x 4 columns]\n  Furthermore, you can set the values using the following methods. \nIn [64]: df2 = dfmi.copy()\n\nIn [65]: df2.loc(axis=0)[:, :, [\"C1\", \"C3\"]] = -10\n\nIn [66]: df2\nOut[66]: \nlvl0           a         b     \nlvl1         bar  foo  bah  foo\nA0 B0 C0 D0    1    0    3    2\n         D1    5    4    7    6\n      C1 D0  -10  -10  -10  -10\n         D1  -10  -10  -10  -10\n      C2 D0   17   16   19   18\n...          ...  ...  ...  ...\nA3 B1 C1 D1  -10  -10  -10  -10\n      C2 D0  241  240  243  242\n         D1  245  244  247  246\n      C3 D0  -10  -10  -10  -10\n         D1  -10  -10  -10  -10\n\n[64 rows x 4 columns]\n  You can use a right-hand-side of an alignable object as well. \nIn [67]: df2 = dfmi.copy()\n\nIn [68]: df2.loc[idx[:, :, [\"C1\", \"C3\"]], :] = df2 * 1000\n\nIn [69]: df2\nOut[69]: \nlvl0              a               b        \nlvl1            bar     foo     bah     foo\nA0 B0 C0 D0       1       0       3       2\n         D1       5       4       7       6\n      C1 D0    9000    8000   11000   10000\n         D1   13000   12000   15000   14000\n      C2 D0      17      16      19      18\n...             ...     ...     ...     ...\nA3 B1 C1 D1  237000  236000  239000  238000\n      C2 D0     241     240     243     242\n         D1     245     244     247     246\n      C3 D0  249000  248000  251000  250000\n         D1  253000  252000  255000  254000\n\n[64 rows x 4 columns]\n    Cross-section The xs() method of DataFrame additionally takes a level argument to make selecting data at a particular level of a MultiIndex easier. \nIn [70]: df\nOut[70]: \n                     A         B         C\nfirst second                              \nbar   one     0.895717  0.410835 -1.413681\n      two     0.805244  0.813850  1.607920\nbaz   one    -1.206412  0.132003  1.024180\n      two     2.565646 -0.827317  0.569605\nfoo   one     1.431256 -0.076467  0.875906\n      two     1.340309 -1.187678 -2.211372\nqux   one    -1.170299  1.130127  0.974466\n      two    -0.226169 -1.436737 -2.006747\n\nIn [71]: df.xs(\"one\", level=\"second\")\nOut[71]: \n              A         B         C\nfirst                              \nbar    0.895717  0.410835 -1.413681\nbaz   -1.206412  0.132003  1.024180\nfoo    1.431256 -0.076467  0.875906\nqux   -1.170299  1.130127  0.974466\n  \n# using the slicers\nIn [72]: df.loc[(slice(None), \"one\"), :]\nOut[72]: \n                     A         B         C\nfirst second                              \nbar   one     0.895717  0.410835 -1.413681\nbaz   one    -1.206412  0.132003  1.024180\nfoo   one     1.431256 -0.076467  0.875906\nqux   one    -1.170299  1.130127  0.974466\n  You can also select on the columns with xs, by providing the axis argument. \nIn [73]: df = df.T\n\nIn [74]: df.xs(\"one\", level=\"second\", axis=1)\nOut[74]: \nfirst       bar       baz       foo       qux\nA      0.895717 -1.206412  1.431256 -1.170299\nB      0.410835  0.132003 -0.076467  1.130127\nC     -1.413681  1.024180  0.875906  0.974466\n  \n# using the slicers\nIn [75]: df.loc[:, (slice(None), \"one\")]\nOut[75]: \nfirst        bar       baz       foo       qux\nsecond       one       one       one       one\nA       0.895717 -1.206412  1.431256 -1.170299\nB       0.410835  0.132003 -0.076467  1.130127\nC      -1.413681  1.024180  0.875906  0.974466\n  xs also allows selection with multiple keys. \nIn [76]: df.xs((\"one\", \"bar\"), level=(\"second\", \"first\"), axis=1)\nOut[76]: \nfirst        bar\nsecond       one\nA       0.895717\nB       0.410835\nC      -1.413681\n  \n# using the slicers\nIn [77]: df.loc[:, (\"bar\", \"one\")]\nOut[77]: \nA    0.895717\nB    0.410835\nC   -1.413681\nName: (bar, one), dtype: float64\n  You can pass drop_level=False to xs to retain the level that was selected. \nIn [78]: df.xs(\"one\", level=\"second\", axis=1, drop_level=False)\nOut[78]: \nfirst        bar       baz       foo       qux\nsecond       one       one       one       one\nA       0.895717 -1.206412  1.431256 -1.170299\nB       0.410835  0.132003 -0.076467  1.130127\nC      -1.413681  1.024180  0.875906  0.974466\n  Compare the above with the result using drop_level=True (the default value). \nIn [79]: df.xs(\"one\", level=\"second\", axis=1, drop_level=True)\nOut[79]: \nfirst       bar       baz       foo       qux\nA      0.895717 -1.206412  1.431256 -1.170299\nB      0.410835  0.132003 -0.076467  1.130127\nC     -1.413681  1.024180  0.875906  0.974466\n    Advanced reindexing and alignment Using the parameter level in the reindex() and align() methods of pandas objects is useful to broadcast values across a level. For instance: \nIn [80]: midx = pd.MultiIndex(\n   ....:     levels=[[\"zero\", \"one\"], [\"x\", \"y\"]], codes=[[1, 1, 0, 0], [1, 0, 1, 0]]\n   ....: )\n   ....: \n\nIn [81]: df = pd.DataFrame(np.random.randn(4, 2), index=midx)\n\nIn [82]: df\nOut[82]: \n               0         1\none  y  1.519970 -0.493662\n     x  0.600178  0.274230\nzero y  0.132885 -0.023688\n     x  2.410179  1.450520\n\nIn [83]: df2 = df.groupby(level=0).mean()\n\nIn [84]: df2\nOut[84]: \n             0         1\none   1.060074 -0.109716\nzero  1.271532  0.713416\n\nIn [85]: df2.reindex(df.index, level=0)\nOut[85]: \n               0         1\none  y  1.060074 -0.109716\n     x  1.060074 -0.109716\nzero y  1.271532  0.713416\n     x  1.271532  0.713416\n\n# aligning\nIn [86]: df_aligned, df2_aligned = df.align(df2, level=0)\n\nIn [87]: df_aligned\nOut[87]: \n               0         1\none  y  1.519970 -0.493662\n     x  0.600178  0.274230\nzero y  0.132885 -0.023688\n     x  2.410179  1.450520\n\nIn [88]: df2_aligned\nOut[88]: \n               0         1\none  y  1.060074 -0.109716\n     x  1.060074 -0.109716\nzero y  1.271532  0.713416\n     x  1.271532  0.713416\n    Swapping levels with swaplevel\n The swaplevel() method can switch the order of two levels: \nIn [89]: df[:5]\nOut[89]: \n               0         1\none  y  1.519970 -0.493662\n     x  0.600178  0.274230\nzero y  0.132885 -0.023688\n     x  2.410179  1.450520\n\nIn [90]: df[:5].swaplevel(0, 1, axis=0)\nOut[90]: \n               0         1\ny one   1.519970 -0.493662\nx one   0.600178  0.274230\ny zero  0.132885 -0.023688\nx zero  2.410179  1.450520\n    Reordering levels with reorder_levels\n The reorder_levels() method generalizes the swaplevel method, allowing you to permute the hierarchical index levels in one step: \nIn [91]: df[:5].reorder_levels([1, 0], axis=0)\nOut[91]: \n               0         1\ny one   1.519970 -0.493662\nx one   0.600178  0.274230\ny zero  0.132885 -0.023688\nx zero  2.410179  1.450520\n    Renaming names of an Index or MultiIndex\n The rename() method is used to rename the labels of a MultiIndex, and is typically used to rename the columns of a DataFrame. The columns argument of rename allows a dictionary to be specified that includes only the columns you wish to rename. \nIn [92]: df.rename(columns={0: \"col0\", 1: \"col1\"})\nOut[92]: \n            col0      col1\none  y  1.519970 -0.493662\n     x  0.600178  0.274230\nzero y  0.132885 -0.023688\n     x  2.410179  1.450520\n  This method can also be used to rename specific labels of the main index of the DataFrame. \nIn [93]: df.rename(index={\"one\": \"two\", \"y\": \"z\"})\nOut[93]: \n               0         1\ntwo  z  1.519970 -0.493662\n     x  0.600178  0.274230\nzero z  0.132885 -0.023688\n     x  2.410179  1.450520\n  The rename_axis() method is used to rename the name of a Index or MultiIndex. In particular, the names of the levels of a MultiIndex can be specified, which is useful if reset_index() is later used to move the values from the MultiIndex to a column. \nIn [94]: df.rename_axis(index=[\"abc\", \"def\"])\nOut[94]: \n                 0         1\nabc  def                    \none  y    1.519970 -0.493662\n     x    0.600178  0.274230\nzero y    0.132885 -0.023688\n     x    2.410179  1.450520\n  Note that the columns of a DataFrame are an index, so that using rename_axis with the columns argument will change the name of that index. \nIn [95]: df.rename_axis(columns=\"Cols\").columns\nOut[95]: RangeIndex(start=0, stop=2, step=1, name='Cols')\n  Both rename and rename_axis support specifying a dictionary, Series or a mapping function to map labels/names to new values. When working with an Index object directly, rather than via a DataFrame, Index.set_names() can be used to change the names. \nIn [96]: mi = pd.MultiIndex.from_product([[1, 2], [\"a\", \"b\"]], names=[\"x\", \"y\"])\n\nIn [97]: mi.names\nOut[97]: FrozenList(['x', 'y'])\n\nIn [98]: mi2 = mi.rename(\"new name\", level=0)\n\nIn [99]: mi2\nOut[99]: \nMultiIndex([(1, 'a'),\n            (1, 'b'),\n            (2, 'a'),\n            (2, 'b')],\n           names=['new name', 'y'])\n  You cannot set the names of the MultiIndex via a level. \nIn [100]: mi.levels[0].name = \"name via level\"\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [100], in <module>\n----> 1 mi.levels[0].name = \"name via level\"\n\nFile /pandas/pandas/core/indexes/base.py:1661, in Index.name(self, value)\n   1657 @name.setter\n   1658 def name(self, value: Hashable):\n   1659     if self._no_setting_name:\n   1660         # Used in MultiIndex.levels to avoid silently ignoring name updates.\n-> 1661         raise RuntimeError(\n   1662             \"Cannot set name on a level of a MultiIndex. Use \"\n   1663             \"'MultiIndex.set_names' instead.\"\n   1664         )\n   1665     maybe_extract_name(value, None, type(self))\n   1666     self._name = value\n\nRuntimeError: Cannot set name on a level of a MultiIndex. Use 'MultiIndex.set_names' instead.\n  Use Index.set_names() instead.    Sorting a MultiIndex\n For MultiIndex-ed objects to be indexed and sliced effectively, they need to be sorted. As with any index, you can use sort_index(). \nIn [101]: import random\n\nIn [102]: random.shuffle(tuples)\n\nIn [103]: s = pd.Series(np.random.randn(8), index=pd.MultiIndex.from_tuples(tuples))\n\nIn [104]: s\nOut[104]: \nbaz  one    0.206053\nbar  two   -0.251905\n     one   -2.213588\nqux  two    1.063327\nbaz  two    1.266143\nqux  one    0.299368\nfoo  two   -0.863838\n     one    0.408204\ndtype: float64\n\nIn [105]: s.sort_index()\nOut[105]: \nbar  one   -2.213588\n     two   -0.251905\nbaz  one    0.206053\n     two    1.266143\nfoo  one    0.408204\n     two   -0.863838\nqux  one    0.299368\n     two    1.063327\ndtype: float64\n\nIn [106]: s.sort_index(level=0)\nOut[106]: \nbar  one   -2.213588\n     two   -0.251905\nbaz  one    0.206053\n     two    1.266143\nfoo  one    0.408204\n     two   -0.863838\nqux  one    0.299368\n     two    1.063327\ndtype: float64\n\nIn [107]: s.sort_index(level=1)\nOut[107]: \nbar  one   -2.213588\nbaz  one    0.206053\nfoo  one    0.408204\nqux  one    0.299368\nbar  two   -0.251905\nbaz  two    1.266143\nfoo  two   -0.863838\nqux  two    1.063327\ndtype: float64\n  You may also pass a level name to sort_index if the MultiIndex levels are named. \nIn [108]: s.index.set_names([\"L1\", \"L2\"], inplace=True)\n\nIn [109]: s.sort_index(level=\"L1\")\nOut[109]: \nL1   L2 \nbar  one   -2.213588\n     two   -0.251905\nbaz  one    0.206053\n     two    1.266143\nfoo  one    0.408204\n     two   -0.863838\nqux  one    0.299368\n     two    1.063327\ndtype: float64\n\nIn [110]: s.sort_index(level=\"L2\")\nOut[110]: \nL1   L2 \nbar  one   -2.213588\nbaz  one    0.206053\nfoo  one    0.408204\nqux  one    0.299368\nbar  two   -0.251905\nbaz  two    1.266143\nfoo  two   -0.863838\nqux  two    1.063327\ndtype: float64\n  On higher dimensional objects, you can sort any of the other axes by level if they have a MultiIndex: \nIn [111]: df.T.sort_index(level=1, axis=1)\nOut[111]: \n        one      zero       one      zero\n          x         x         y         y\n0  0.600178  2.410179  1.519970  0.132885\n1  0.274230  1.450520 -0.493662 -0.023688\n  Indexing will work even if the data are not sorted, but will be rather inefficient (and show a PerformanceWarning). It will also return a copy of the data rather than a view: \nIn [112]: dfm = pd.DataFrame(\n   .....:     {\"jim\": [0, 0, 1, 1], \"joe\": [\"x\", \"x\", \"z\", \"y\"], \"jolie\": np.random.rand(4)}\n   .....: )\n   .....: \n\nIn [113]: dfm = dfm.set_index([\"jim\", \"joe\"])\n\nIn [114]: dfm\nOut[114]: \n            jolie\njim joe          \n0   x    0.490671\n    x    0.120248\n1   z    0.537020\n    y    0.110968\n  \nIn [4]: dfm.loc[(1, 'z')]\nPerformanceWarning: indexing past lexsort depth may impact performance.\n\nOut[4]:\n           jolie\njim joe\n1   z    0.64094\n  Furthermore, if you try to index something that is not fully lexsorted, this can raise: \nIn [5]: dfm.loc[(0, 'y'):(1, 'z')]\nUnsortedIndexError: 'Key length (2) was greater than MultiIndex lexsort depth (1)'\n  The is_monotonic_increasing() method on a MultiIndex shows if the index is sorted: \nIn [115]: dfm.index.is_monotonic_increasing\nOut[115]: False\n  \nIn [116]: dfm = dfm.sort_index()\n\nIn [117]: dfm\nOut[117]: \n            jolie\njim joe          \n0   x    0.490671\n    x    0.120248\n1   y    0.110968\n    z    0.537020\n\nIn [118]: dfm.index.is_monotonic_increasing\nOut[118]: True\n  And now selection works as expected. \nIn [119]: dfm.loc[(0, \"y\"):(1, \"z\")]\nOut[119]: \n            jolie\njim joe          \n1   y    0.110968\n    z    0.537020\n    Take methods Similar to NumPy ndarrays, pandas Index, Series, and DataFrame also provides the take() method that retrieves elements along a given axis at the given indices. The given indices must be either a list or an ndarray of integer index positions. take will also accept negative integers as relative positions to the end of the object. \nIn [120]: index = pd.Index(np.random.randint(0, 1000, 10))\n\nIn [121]: index\nOut[121]: Int64Index([214, 502, 712, 567, 786, 175, 993, 133, 758, 329], dtype='int64')\n\nIn [122]: positions = [0, 9, 3]\n\nIn [123]: index[positions]\nOut[123]: Int64Index([214, 329, 567], dtype='int64')\n\nIn [124]: index.take(positions)\nOut[124]: Int64Index([214, 329, 567], dtype='int64')\n\nIn [125]: ser = pd.Series(np.random.randn(10))\n\nIn [126]: ser.iloc[positions]\nOut[126]: \n0   -0.179666\n9    1.824375\n3    0.392149\ndtype: float64\n\nIn [127]: ser.take(positions)\nOut[127]: \n0   -0.179666\n9    1.824375\n3    0.392149\ndtype: float64\n  For DataFrames, the given indices should be a 1d list or ndarray that specifies row or column positions. \nIn [128]: frm = pd.DataFrame(np.random.randn(5, 3))\n\nIn [129]: frm.take([1, 4, 3])\nOut[129]: \n          0         1         2\n1 -1.237881  0.106854 -1.276829\n4  0.629675 -1.425966  1.857704\n3  0.979542 -1.633678  0.615855\n\nIn [130]: frm.take([0, 2], axis=1)\nOut[130]: \n          0         2\n0  0.595974  0.601544\n1 -1.237881 -1.276829\n2 -0.767101  1.499591\n3  0.979542  0.615855\n4  0.629675  1.857704\n  It is important to note that the take method on pandas objects are not intended to work on boolean indices and may return unexpected results. \nIn [131]: arr = np.random.randn(10)\n\nIn [132]: arr.take([False, False, True, True])\nOut[132]: array([-1.1935, -1.1935,  0.6775,  0.6775])\n\nIn [133]: arr[[0, 1]]\nOut[133]: array([-1.1935,  0.6775])\n\nIn [134]: ser = pd.Series(np.random.randn(10))\n\nIn [135]: ser.take([False, False, True, True])\nOut[135]: \n0    0.233141\n0    0.233141\n1   -0.223540\n1   -0.223540\ndtype: float64\n\nIn [136]: ser.iloc[[0, 1]]\nOut[136]: \n0    0.233141\n1   -0.223540\ndtype: float64\n  Finally, as a small note on performance, because the take method handles a narrower range of inputs, it can offer performance that is a good deal faster than fancy indexing. \nIn [137]: arr = np.random.randn(10000, 5)\n\nIn [138]: indexer = np.arange(10000)\n\nIn [139]: random.shuffle(indexer)\n\nIn [140]: %timeit arr[indexer]\n   .....: %timeit arr.take(indexer, axis=0)\n   .....: \n183 us +- 14.4 us per loop (mean +- std. dev. of 7 runs, 10,000 loops each)\n50.7 us +- 4.88 us per loop (mean +- std. dev. of 7 runs, 10,000 loops each)\n  \nIn [141]: ser = pd.Series(arr[:, 0])\n\nIn [142]: %timeit ser.iloc[indexer]\n   .....: %timeit ser.take(indexer)\n   .....: \n85.9 us +- 5.47 us per loop (mean +- std. dev. of 7 runs, 10,000 loops each)\n71 us +- 7.64 us per loop (mean +- std. dev. of 7 runs, 10,000 loops each)\n    Index types We have discussed MultiIndex in the previous sections pretty extensively. Documentation about DatetimeIndex and PeriodIndex are shown here, and documentation about TimedeltaIndex is found here. In the following sub-sections we will highlight some other index types.  CategoricalIndex CategoricalIndex is a type of index that is useful for supporting indexing with duplicates. This is a container around a Categorical and allows efficient indexing and storage of an index with a large number of duplicated elements. \nIn [143]: from pandas.api.types import CategoricalDtype\n\nIn [144]: df = pd.DataFrame({\"A\": np.arange(6), \"B\": list(\"aabbca\")})\n\nIn [145]: df[\"B\"] = df[\"B\"].astype(CategoricalDtype(list(\"cab\")))\n\nIn [146]: df\nOut[146]: \n   A  B\n0  0  a\n1  1  a\n2  2  b\n3  3  b\n4  4  c\n5  5  a\n\nIn [147]: df.dtypes\nOut[147]: \nA       int64\nB    category\ndtype: object\n\nIn [148]: df[\"B\"].cat.categories\nOut[148]: Index(['c', 'a', 'b'], dtype='object')\n  Setting the index will create a CategoricalIndex. \nIn [149]: df2 = df.set_index(\"B\")\n\nIn [150]: df2.index\nOut[150]: CategoricalIndex(['a', 'a', 'b', 'b', 'c', 'a'], categories=['c', 'a', 'b'], ordered=False, dtype='category', name='B')\n  Indexing with __getitem__/.iloc/.loc works similarly to an Index with duplicates. The indexers must be in the category or the operation will raise a KeyError. \nIn [151]: df2.loc[\"a\"]\nOut[151]: \n   A\nB   \na  0\na  1\na  5\n  The CategoricalIndex is preserved after indexing: \nIn [152]: df2.loc[\"a\"].index\nOut[152]: CategoricalIndex(['a', 'a', 'a'], categories=['c', 'a', 'b'], ordered=False, dtype='category', name='B')\n  Sorting the index will sort by the order of the categories (recall that we created the index with CategoricalDtype(list('cab')), so the sorted order is cab). \nIn [153]: df2.sort_index()\nOut[153]: \n   A\nB   \nc  4\na  0\na  1\na  5\nb  2\nb  3\n  Groupby operations on the index will preserve the index nature as well. \nIn [154]: df2.groupby(level=0).sum()\nOut[154]: \n   A\nB   \nc  4\na  6\nb  5\n\nIn [155]: df2.groupby(level=0).sum().index\nOut[155]: CategoricalIndex(['c', 'a', 'b'], categories=['c', 'a', 'b'], ordered=False, dtype='category', name='B')\n  Reindexing operations will return a resulting index based on the type of the passed indexer. Passing a list will return a plain-old Index; indexing with a Categorical will return a CategoricalIndex, indexed according to the categories of the passed Categorical dtype. This allows one to arbitrarily index these even with values not in the categories, similarly to how you can reindex any pandas index. \nIn [156]: df3 = pd.DataFrame(\n   .....:     {\"A\": np.arange(3), \"B\": pd.Series(list(\"abc\")).astype(\"category\")}\n   .....: )\n   .....: \n\nIn [157]: df3 = df3.set_index(\"B\")\n\nIn [158]: df3\nOut[158]: \n   A\nB   \na  0\nb  1\nc  2\n  \nIn [159]: df3.reindex([\"a\", \"e\"])\nOut[159]: \n     A\nB     \na  0.0\ne  NaN\n\nIn [160]: df3.reindex([\"a\", \"e\"]).index\nOut[160]: Index(['a', 'e'], dtype='object', name='B')\n\nIn [161]: df3.reindex(pd.Categorical([\"a\", \"e\"], categories=list(\"abe\")))\nOut[161]: \n     A\nB     \na  0.0\ne  NaN\n\nIn [162]: df3.reindex(pd.Categorical([\"a\", \"e\"], categories=list(\"abe\"))).index\nOut[162]: CategoricalIndex(['a', 'e'], categories=['a', 'b', 'e'], ordered=False, dtype='category', name='B')\n   Warning Reshaping and Comparison operations on a CategoricalIndex must have the same categories or a TypeError will be raised. \nIn [163]: df4 = pd.DataFrame({\"A\": np.arange(2), \"B\": list(\"ba\")})\n\nIn [164]: df4[\"B\"] = df4[\"B\"].astype(CategoricalDtype(list(\"ab\")))\n\nIn [165]: df4 = df4.set_index(\"B\")\n\nIn [166]: df4.index\nOut[166]: CategoricalIndex(['b', 'a'], categories=['a', 'b'], ordered=False, dtype='category', name='B')\n\nIn [167]: df5 = pd.DataFrame({\"A\": np.arange(2), \"B\": list(\"bc\")})\n\nIn [168]: df5[\"B\"] = df5[\"B\"].astype(CategoricalDtype(list(\"bc\")))\n\nIn [169]: df5 = df5.set_index(\"B\")\n\nIn [170]: df5.index\nOut[170]: CategoricalIndex(['b', 'c'], categories=['b', 'c'], ordered=False, dtype='category', name='B')\n  \nIn [1]: pd.concat([df4, df5])\nTypeError: categories must match existing categories when appending\n     Int64Index and RangeIndex  Deprecated since version 1.4.0: In pandas 2.0, Index will become the default index type for numeric types instead of Int64Index, Float64Index and UInt64Index and those index types are therefore deprecated and will be removed in a futire version. RangeIndex will not be removed, as it represents an optimized version of an integer index.  Int64Index is a fundamental basic index in pandas. This is an immutable array implementing an ordered, sliceable set. RangeIndex is a sub-class of Int64Index that provides the default index for all NDFrame objects. RangeIndex is an optimized version of Int64Index that can represent a monotonic ordered set. These are analogous to Python range types.   Float64Index  Deprecated since version 1.4.0: Index will become the default index type for numeric types in the future instead of Int64Index, Float64Index and UInt64Index and those index types are therefore deprecated and will be removed in a future version of Pandas. RangeIndex will not be removed as it represents an optimized version of an integer index.  By default a Float64Index will be automatically created when passing floating, or mixed-integer-floating values in index creation. This enables a pure label-based slicing paradigm that makes [],ix,loc for scalar indexing and slicing work exactly the same. \nIn [171]: indexf = pd.Index([1.5, 2, 3, 4.5, 5])\n\nIn [172]: indexf\nOut[172]: Float64Index([1.5, 2.0, 3.0, 4.5, 5.0], dtype='float64')\n\nIn [173]: sf = pd.Series(range(5), index=indexf)\n\nIn [174]: sf\nOut[174]: \n1.5    0\n2.0    1\n3.0    2\n4.5    3\n5.0    4\ndtype: int64\n  Scalar selection for [],.loc will always be label based. An integer will match an equal float index (e.g. 3 is equivalent to 3.0). \nIn [175]: sf[3]\nOut[175]: 2\n\nIn [176]: sf[3.0]\nOut[176]: 2\n\nIn [177]: sf.loc[3]\nOut[177]: 2\n\nIn [178]: sf.loc[3.0]\nOut[178]: 2\n  The only positional indexing is via iloc. \nIn [179]: sf.iloc[3]\nOut[179]: 3\n  A scalar index that is not found will raise a KeyError. Slicing is primarily on the values of the index when using [],ix,loc, and always positional when using iloc. The exception is when the slice is boolean, in which case it will always be positional. \nIn [180]: sf[2:4]\nOut[180]: \n2.0    1\n3.0    2\ndtype: int64\n\nIn [181]: sf.loc[2:4]\nOut[181]: \n2.0    1\n3.0    2\ndtype: int64\n\nIn [182]: sf.iloc[2:4]\nOut[182]: \n3.0    2\n4.5    3\ndtype: int64\n  In float indexes, slicing using floats is allowed. \nIn [183]: sf[2.1:4.6]\nOut[183]: \n3.0    2\n4.5    3\ndtype: int64\n\nIn [184]: sf.loc[2.1:4.6]\nOut[184]: \n3.0    2\n4.5    3\ndtype: int64\n  In non-float indexes, slicing using floats will raise a TypeError. \nIn [1]: pd.Series(range(5))[3.5]\nTypeError: the label [3.5] is not a proper indexer for this index type (Int64Index)\n\nIn [1]: pd.Series(range(5))[3.5:4.5]\nTypeError: the slice start [3.5] is not a proper indexer for this index type (Int64Index)\n  Here is a typical use-case for using this type of indexing. Imagine that you have a somewhat irregular timedelta-like indexing scheme, but the data is recorded as floats. This could, for example, be millisecond offsets. \nIn [185]: dfir = pd.concat(\n   .....:     [\n   .....:         pd.DataFrame(\n   .....:             np.random.randn(5, 2), index=np.arange(5) * 250.0, columns=list(\"AB\")\n   .....:         ),\n   .....:         pd.DataFrame(\n   .....:             np.random.randn(6, 2),\n   .....:             index=np.arange(4, 10) * 250.1,\n   .....:             columns=list(\"AB\"),\n   .....:         ),\n   .....:     ]\n   .....: )\n   .....: \n\nIn [186]: dfir\nOut[186]: \n               A         B\n0.0    -0.435772 -1.188928\n250.0  -0.808286 -0.284634\n500.0  -1.815703  1.347213\n750.0  -0.243487  0.514704\n1000.0  1.162969 -0.287725\n1000.4 -0.179734  0.993962\n1250.5 -0.212673  0.909872\n1500.6 -0.733333 -0.349893\n1750.7  0.456434 -0.306735\n2000.8  0.553396  0.166221\n2250.9 -0.101684 -0.734907\n  Selection operations then will always work on a value basis, for all selection operators. \nIn [187]: dfir[0:1000.4]\nOut[187]: \n               A         B\n0.0    -0.435772 -1.188928\n250.0  -0.808286 -0.284634\n500.0  -1.815703  1.347213\n750.0  -0.243487  0.514704\n1000.0  1.162969 -0.287725\n1000.4 -0.179734  0.993962\n\nIn [188]: dfir.loc[0:1001, \"A\"]\nOut[188]: \n0.0      -0.435772\n250.0    -0.808286\n500.0    -1.815703\n750.0    -0.243487\n1000.0    1.162969\n1000.4   -0.179734\nName: A, dtype: float64\n\nIn [189]: dfir.loc[1000.4]\nOut[189]: \nA   -0.179734\nB    0.993962\nName: 1000.4, dtype: float64\n  You could retrieve the first 1 second (1000 ms) of data as such: \nIn [190]: dfir[0:1000]\nOut[190]: \n               A         B\n0.0    -0.435772 -1.188928\n250.0  -0.808286 -0.284634\n500.0  -1.815703  1.347213\n750.0  -0.243487  0.514704\n1000.0  1.162969 -0.287725\n  If you need integer based selection, you should use iloc: \nIn [191]: dfir.iloc[0:5]\nOut[191]: \n               A         B\n0.0    -0.435772 -1.188928\n250.0  -0.808286 -0.284634\n500.0  -1.815703  1.347213\n750.0  -0.243487  0.514704\n1000.0  1.162969 -0.287725\n    IntervalIndex IntervalIndex together with its own dtype, IntervalDtype as well as the Interval scalar type, allow first-class support in pandas for interval notation. The IntervalIndex allows some unique indexing and is also used as a return type for the categories in cut() and qcut().  Indexing with an IntervalIndex\n An IntervalIndex can be used in Series and in DataFrame as the index. \nIn [192]: df = pd.DataFrame(\n   .....:     {\"A\": [1, 2, 3, 4]}, index=pd.IntervalIndex.from_breaks([0, 1, 2, 3, 4])\n   .....: )\n   .....: \n\nIn [193]: df\nOut[193]: \n        A\n(0, 1]  1\n(1, 2]  2\n(2, 3]  3\n(3, 4]  4\n  Label based indexing via .loc along the edges of an interval works as you would expect, selecting that particular interval. \nIn [194]: df.loc[2]\nOut[194]: \nA    2\nName: (1, 2], dtype: int64\n\nIn [195]: df.loc[[2, 3]]\nOut[195]: \n        A\n(1, 2]  2\n(2, 3]  3\n  If you select a label contained within an interval, this will also select the interval. \nIn [196]: df.loc[2.5]\nOut[196]: \nA    3\nName: (2, 3], dtype: int64\n\nIn [197]: df.loc[[2.5, 3.5]]\nOut[197]: \n        A\n(2, 3]  3\n(3, 4]  4\n  Selecting using an Interval will only return exact matches (starting from pandas 0.25.0). \nIn [198]: df.loc[pd.Interval(1, 2)]\nOut[198]: \nA    2\nName: (1, 2], dtype: int64\n  Trying to select an Interval that is not exactly contained in the IntervalIndex will raise a KeyError. \nIn [7]: df.loc[pd.Interval(0.5, 2.5)]\n---------------------------------------------------------------------------\nKeyError: Interval(0.5, 2.5, closed='right')\n  Selecting all Intervals that overlap a given Interval can be performed using the overlaps() method to create a boolean indexer. \nIn [199]: idxr = df.index.overlaps(pd.Interval(0.5, 2.5))\n\nIn [200]: idxr\nOut[200]: array([ True,  True,  True, False])\n\nIn [201]: df[idxr]\nOut[201]: \n        A\n(0, 1]  1\n(1, 2]  2\n(2, 3]  3\n    Binning data with cut and qcut\n cut() and qcut() both return a Categorical object, and the bins they create are stored as an IntervalIndex in its .categories attribute. \nIn [202]: c = pd.cut(range(4), bins=2)\n\nIn [203]: c\nOut[203]: \n[(-0.003, 1.5], (-0.003, 1.5], (1.5, 3.0], (1.5, 3.0]]\nCategories (2, interval[float64, right]): [(-0.003, 1.5] < (1.5, 3.0]]\n\nIn [204]: c.categories\nOut[204]: IntervalIndex([(-0.003, 1.5], (1.5, 3.0]], dtype='interval[float64, right]')\n  cut() also accepts an IntervalIndex for its bins argument, which enables a useful pandas idiom. First, We call cut() with some data and bins set to a fixed number, to generate the bins. Then, we pass the values of .categories as the bins argument in subsequent calls to cut(), supplying new data which will be binned into the same bins. \nIn [205]: pd.cut([0, 3, 5, 1], bins=c.categories)\nOut[205]: \n[(-0.003, 1.5], (1.5, 3.0], NaN, (-0.003, 1.5]]\nCategories (2, interval[float64, right]): [(-0.003, 1.5] < (1.5, 3.0]]\n  Any value which falls outside all bins will be assigned a NaN value.   Generating ranges of intervals If we need intervals on a regular frequency, we can use the interval_range() function to create an IntervalIndex using various combinations of start, end, and periods. The default frequency for interval_range is a 1 for numeric intervals, and calendar day for datetime-like intervals: \nIn [206]: pd.interval_range(start=0, end=5)\nOut[206]: IntervalIndex([(0, 1], (1, 2], (2, 3], (3, 4], (4, 5]], dtype='interval[int64, right]')\n\nIn [207]: pd.interval_range(start=pd.Timestamp(\"2017-01-01\"), periods=4)\nOut[207]: IntervalIndex([(2017-01-01, 2017-01-02], (2017-01-02, 2017-01-03], (2017-01-03, 2017-01-04], (2017-01-04, 2017-01-05]], dtype='interval[datetime64[ns], right]')\n\nIn [208]: pd.interval_range(end=pd.Timedelta(\"3 days\"), periods=3)\nOut[208]: IntervalIndex([(0 days 00:00:00, 1 days 00:00:00], (1 days 00:00:00, 2 days 00:00:00], (2 days 00:00:00, 3 days 00:00:00]], dtype='interval[timedelta64[ns], right]')\n  The freq parameter can used to specify non-default frequencies, and can utilize a variety of frequency aliases with datetime-like intervals: \nIn [209]: pd.interval_range(start=0, periods=5, freq=1.5)\nOut[209]: IntervalIndex([(0.0, 1.5], (1.5, 3.0], (3.0, 4.5], (4.5, 6.0], (6.0, 7.5]], dtype='interval[float64, right]')\n\nIn [210]: pd.interval_range(start=pd.Timestamp(\"2017-01-01\"), periods=4, freq=\"W\")\nOut[210]: IntervalIndex([(2017-01-01, 2017-01-08], (2017-01-08, 2017-01-15], (2017-01-15, 2017-01-22], (2017-01-22, 2017-01-29]], dtype='interval[datetime64[ns], right]')\n\nIn [211]: pd.interval_range(start=pd.Timedelta(\"0 days\"), periods=3, freq=\"9H\")\nOut[211]: IntervalIndex([(0 days 00:00:00, 0 days 09:00:00], (0 days 09:00:00, 0 days 18:00:00], (0 days 18:00:00, 1 days 03:00:00]], dtype='interval[timedelta64[ns], right]')\n  Additionally, the closed parameter can be used to specify which side(s) the intervals are closed on. Intervals are closed on the right side by default. \nIn [212]: pd.interval_range(start=0, end=4, closed=\"both\")\nOut[212]: IntervalIndex([[0, 1], [1, 2], [2, 3], [3, 4]], dtype='interval[int64, both]')\n\nIn [213]: pd.interval_range(start=0, end=4, closed=\"neither\")\nOut[213]: IntervalIndex([(0, 1), (1, 2), (2, 3), (3, 4)], dtype='interval[int64, neither]')\n  Specifying start, end, and periods will generate a range of evenly spaced intervals from start to end inclusively, with periods number of elements in the resulting IntervalIndex: \nIn [214]: pd.interval_range(start=0, end=6, periods=4)\nOut[214]: IntervalIndex([(0.0, 1.5], (1.5, 3.0], (3.0, 4.5], (4.5, 6.0]], dtype='interval[float64, right]')\n\nIn [215]: pd.interval_range(pd.Timestamp(\"2018-01-01\"), pd.Timestamp(\"2018-02-28\"), periods=3)\nOut[215]: IntervalIndex([(2018-01-01, 2018-01-20 08:00:00], (2018-01-20 08:00:00, 2018-02-08 16:00:00], (2018-02-08 16:00:00, 2018-02-28]], dtype='interval[datetime64[ns], right]')\n      Miscellaneous indexing FAQ  Integer indexing Label-based indexing with integer axis labels is a thorny topic. It has been discussed heavily on mailing lists and among various members of the scientific Python community. In pandas, our general viewpoint is that labels matter more than integer locations. Therefore, with an integer axis index only label-based indexing is possible with the standard tools like .loc. The following code will generate exceptions: \nIn [216]: s = pd.Series(range(5))\n\nIn [217]: s[-1]\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nFile /pandas/pandas/core/indexes/range.py:385, in RangeIndex.get_loc(self, key, method, tolerance)\n    384 try:\n--> 385     return self._range.index(new_key)\n    386 except ValueError as err:\n\nValueError: -1 is not in range\n\nThe above exception was the direct cause of the following exception:\n\nKeyError                                  Traceback (most recent call last)\nInput In [217], in <module>\n----> 1 s[-1]\n\nFile /pandas/pandas/core/series.py:959, in Series.__getitem__(self, key)\n    956     return self._values[key]\n    958 elif key_is_scalar:\n--> 959     return self._get_value(key)\n    961 if is_hashable(key):\n    962     # Otherwise index.get_value will raise InvalidIndexError\n    963     try:\n    964         # For labels that don't resolve as scalars like tuples and frozensets\n\nFile /pandas/pandas/core/series.py:1070, in Series._get_value(self, label, takeable)\n   1067     return self._values[label]\n   1069 # Similar to Index.get_value, but we do not fall back to positional\n-> 1070 loc = self.index.get_loc(label)\n   1071 return self.index._get_values_for_loc(self, loc, label)\n\nFile /pandas/pandas/core/indexes/range.py:387, in RangeIndex.get_loc(self, key, method, tolerance)\n    385         return self._range.index(new_key)\n    386     except ValueError as err:\n--> 387         raise KeyError(key) from err\n    388 self._check_indexing_error(key)\n    389 raise KeyError(key)\n\nKeyError: -1\n\nIn [218]: df = pd.DataFrame(np.random.randn(5, 4))\n\nIn [219]: df\nOut[219]: \n          0         1         2         3\n0 -0.130121 -0.476046  0.759104  0.213379\n1 -0.082641  0.448008  0.656420 -1.051443\n2  0.594956 -0.151360 -0.069303  1.221431\n3 -0.182832  0.791235  0.042745  2.069775\n4  1.446552  0.019814 -1.389212 -0.702312\n\nIn [220]: df.loc[-2:]\nOut[220]: \n          0         1         2         3\n0 -0.130121 -0.476046  0.759104  0.213379\n1 -0.082641  0.448008  0.656420 -1.051443\n2  0.594956 -0.151360 -0.069303  1.221431\n3 -0.182832  0.791235  0.042745  2.069775\n4  1.446552  0.019814 -1.389212 -0.702312\n  This deliberate decision was made to prevent ambiguities and subtle bugs (many users reported finding bugs when the API change was made to stop \u201cfalling back\u201d on position-based indexing).   Non-monotonic indexes require exact matches If the index of a Series or DataFrame is monotonically increasing or decreasing, then the bounds of a label-based slice can be outside the range of the index, much like slice indexing a normal Python list. Monotonicity of an index can be tested with the is_monotonic_increasing() and is_monotonic_decreasing() attributes. \nIn [221]: df = pd.DataFrame(index=[2, 3, 3, 4, 5], columns=[\"data\"], data=list(range(5)))\n\nIn [222]: df.index.is_monotonic_increasing\nOut[222]: True\n\n# no rows 0 or 1, but still returns rows 2, 3 (both of them), and 4:\nIn [223]: df.loc[0:4, :]\nOut[223]: \n   data\n2     0\n3     1\n3     2\n4     3\n\n# slice is are outside the index, so empty DataFrame is returned\nIn [224]: df.loc[13:15, :]\nOut[224]: \nEmpty DataFrame\nColumns: [data]\nIndex: []\n  On the other hand, if the index is not monotonic, then both slice bounds must be unique members of the index. \nIn [225]: df = pd.DataFrame(index=[2, 3, 1, 4, 3, 5], columns=[\"data\"], data=list(range(6)))\n\nIn [226]: df.index.is_monotonic_increasing\nOut[226]: False\n\n# OK because 2 and 4 are in the index\nIn [227]: df.loc[2:4, :]\nOut[227]: \n   data\n2     0\n3     1\n1     2\n4     3\n  \n# 0 is not in the index\nIn [9]: df.loc[0:4, :]\nKeyError: 0\n\n# 3 is not a unique label\nIn [11]: df.loc[2:3, :]\nKeyError: 'Cannot get right slice bound for non-unique label: 3'\n  Index.is_monotonic_increasing and Index.is_monotonic_decreasing only check that an index is weakly monotonic. To check for strict monotonicity, you can combine one of those with the is_unique() attribute. \nIn [228]: weakly_monotonic = pd.Index([\"a\", \"b\", \"c\", \"c\"])\n\nIn [229]: weakly_monotonic\nOut[229]: Index(['a', 'b', 'c', 'c'], dtype='object')\n\nIn [230]: weakly_monotonic.is_monotonic_increasing\nOut[230]: True\n\nIn [231]: weakly_monotonic.is_monotonic_increasing & weakly_monotonic.is_unique\nOut[231]: False\n    Endpoints are inclusive Compared with standard Python sequence slicing in which the slice endpoint is not inclusive, label-based slicing in pandas is inclusive. The primary reason for this is that it is often not possible to easily determine the \u201csuccessor\u201d or next element after a particular label in an index. For example, consider the following Series: \nIn [232]: s = pd.Series(np.random.randn(6), index=list(\"abcdef\"))\n\nIn [233]: s\nOut[233]: \na    0.301379\nb    1.240445\nc   -0.846068\nd   -0.043312\ne   -1.658747\nf   -0.819549\ndtype: float64\n  Suppose we wished to slice from c to e, using integers this would be accomplished as such: \nIn [234]: s[2:5]\nOut[234]: \nc   -0.846068\nd   -0.043312\ne   -1.658747\ndtype: float64\n  However, if you only had c and e, determining the next element in the index can be somewhat complicated. For example, the following does not work: \ns.loc['c':'e' + 1]\n  A very common use case is to limit a time series to start and end at two specific dates. To enable this, we made the design choice to make label-based slicing include both endpoints: \nIn [235]: s.loc[\"c\":\"e\"]\nOut[235]: \nc   -0.846068\nd   -0.043312\ne   -1.658747\ndtype: float64\n  This is most definitely a \u201cpracticality beats purity\u201d sort of thing, but it is something to watch out for if you expect label-based slicing to behave exactly in the way that standard Python integer slicing works.   Indexing potentially changes underlying Series dtype The different indexing operation can potentially change the dtype of a Series. \nIn [236]: series1 = pd.Series([1, 2, 3])\n\nIn [237]: series1.dtype\nOut[237]: dtype('int64')\n\nIn [238]: res = series1.reindex([0, 4])\n\nIn [239]: res.dtype\nOut[239]: dtype('float64')\n\nIn [240]: res\nOut[240]: \n0    1.0\n4    NaN\ndtype: float64\n  \nIn [241]: series2 = pd.Series([True])\n\nIn [242]: series2.dtype\nOut[242]: dtype('bool')\n\nIn [243]: res = series2.reindex_like(series1)\n\nIn [244]: res.dtype\nOut[244]: dtype('O')\n\nIn [245]: res\nOut[245]: \n0    True\n1     NaN\n2     NaN\ndtype: object\n  This is because the (re)indexing operations above silently inserts NaNs and the dtype changes accordingly. This can cause some issues when using numpy ufuncs such as numpy.logical_and. See the GH2388 for a more detailed discussion.  \n"}, {"name": "Nullable Boolean data type", "path": "user_guide/boolean", "type": "Manual", "text": "Nullable Boolean data type  Note BooleanArray is currently experimental. Its API or implementation may change without warning.   New in version 1.0.0.   Indexing with NA values pandas allows indexing with NA values in a boolean array, which are treated as False.  Changed in version 1.0.2.  \nIn [1]: s = pd.Series([1, 2, 3])\n\nIn [2]: mask = pd.array([True, False, pd.NA], dtype=\"boolean\")\n\nIn [3]: s[mask]\nOut[3]: \n0    1\ndtype: int64\n  If you would prefer to keep the NA values you can manually fill them with fillna(True). \nIn [4]: s[mask.fillna(True)]\nOut[4]: \n0    1\n2    3\ndtype: int64\n    Kleene logical operations arrays.BooleanArray implements Kleene Logic (sometimes called three-value logic) for logical operations like & (and), | (or) and ^ (exclusive-or). This table demonstrates the results for every combination. These operations are symmetrical, so flipping the left- and right-hand side makes no difference in the result.       \nExpression Result    \nTrue & True True  \nTrue & False False  \nTrue & NA NA  \nFalse & False False  \nFalse & NA False  \nNA & NA NA  \nTrue | True True  \nTrue | False True  \nTrue | NA True  \nFalse | False False  \nFalse | NA NA  \nNA | NA NA  \nTrue ^ True False  \nTrue ^ False True  \nTrue ^ NA NA  \nFalse ^ False False  \nFalse ^ NA NA  \nNA ^ NA NA    When an NA is present in an operation, the output value is NA only if the result cannot be determined solely based on the other input. For example, True | NA is True, because both True | True and True | False are True. In that case, we don\u2019t actually need to consider the value of the NA. On the other hand, True & NA is NA. The result depends on whether the NA really is True or False, since True & True is True, but True & False is False, so we can\u2019t determine the output. This differs from how np.nan behaves in logical operations. pandas treated np.nan is always false in the output. In or \nIn [5]: pd.Series([True, False, np.nan], dtype=\"object\") | True\nOut[5]: \n0     True\n1     True\n2    False\ndtype: bool\n\nIn [6]: pd.Series([True, False, np.nan], dtype=\"boolean\") | True\nOut[6]: \n0    True\n1    True\n2    True\ndtype: boolean\n  In and \nIn [7]: pd.Series([True, False, np.nan], dtype=\"object\") & True\nOut[7]: \n0     True\n1    False\n2    False\ndtype: bool\n\nIn [8]: pd.Series([True, False, np.nan], dtype=\"boolean\") & True\nOut[8]: \n0     True\n1    False\n2     <NA>\ndtype: boolean\n  \n"}, {"name": "Nullable integer data type", "path": "user_guide/integer_na", "type": "Manual", "text": "Nullable integer data type  Note IntegerArray is currently experimental. Its API or implementation may change without warning.   Changed in version 1.0.0: Now uses pandas.NA as the missing value rather than numpy.nan.  In Working with missing data, we saw that pandas primarily uses NaN to represent missing data. Because NaN is a float, this forces an array of integers with any missing values to become floating point. In some cases, this may not matter much. But if your integer column is, say, an identifier, casting to float can be problematic. Some integers cannot even be represented as floating point numbers.  Construction pandas can represent integer data with possibly missing values using arrays.IntegerArray. This is an extension types implemented within pandas. \nIn [1]: arr = pd.array([1, 2, None], dtype=pd.Int64Dtype())\n\nIn [2]: arr\nOut[2]: \n<IntegerArray>\n[1, 2, <NA>]\nLength: 3, dtype: Int64\n  Or the string alias \"Int64\" (note the capital \"I\", to differentiate from NumPy\u2019s 'int64' dtype: \nIn [3]: pd.array([1, 2, np.nan], dtype=\"Int64\")\nOut[3]: \n<IntegerArray>\n[1, 2, <NA>]\nLength: 3, dtype: Int64\n  All NA-like values are replaced with pandas.NA. \nIn [4]: pd.array([1, 2, np.nan, None, pd.NA], dtype=\"Int64\")\nOut[4]: \n<IntegerArray>\n[1, 2, <NA>, <NA>, <NA>]\nLength: 5, dtype: Int64\n  This array can be stored in a DataFrame or Series like any NumPy array. \nIn [5]: pd.Series(arr)\nOut[5]: \n0       1\n1       2\n2    <NA>\ndtype: Int64\n  You can also pass the list-like object to the Series constructor with the dtype.  Warning Currently pandas.array() and pandas.Series() use different rules for dtype inference. pandas.array() will infer a nullable- integer dtype \nIn [6]: pd.array([1, None])\nOut[6]: \n<IntegerArray>\n[1, <NA>]\nLength: 2, dtype: Int64\n\nIn [7]: pd.array([1, 2])\nOut[7]: \n<IntegerArray>\n[1, 2]\nLength: 2, dtype: Int64\n  For backwards-compatibility, Series infers these as either integer or float dtype \nIn [8]: pd.Series([1, None])\nOut[8]: \n0    1.0\n1    NaN\ndtype: float64\n\nIn [9]: pd.Series([1, 2])\nOut[9]: \n0    1\n1    2\ndtype: int64\n  We recommend explicitly providing the dtype to avoid confusion. \nIn [10]: pd.array([1, None], dtype=\"Int64\")\nOut[10]: \n<IntegerArray>\n[1, <NA>]\nLength: 2, dtype: Int64\n\nIn [11]: pd.Series([1, None], dtype=\"Int64\")\nOut[11]: \n0       1\n1    <NA>\ndtype: Int64\n  In the future, we may provide an option for Series to infer a nullable-integer dtype.    Operations Operations involving an integer array will behave similar to NumPy arrays. Missing values will be propagated, and the data will be coerced to another dtype if needed. \nIn [12]: s = pd.Series([1, 2, None], dtype=\"Int64\")\n\n# arithmetic\nIn [13]: s + 1\nOut[13]: \n0       2\n1       3\n2    <NA>\ndtype: Int64\n\n# comparison\nIn [14]: s == 1\nOut[14]: \n0     True\n1    False\n2     <NA>\ndtype: boolean\n\n# indexing\nIn [15]: s.iloc[1:3]\nOut[15]: \n1       2\n2    <NA>\ndtype: Int64\n\n# operate with other dtypes\nIn [16]: s + s.iloc[1:3].astype(\"Int8\")\nOut[16]: \n0    <NA>\n1       4\n2    <NA>\ndtype: Int64\n\n# coerce when needed\nIn [17]: s + 0.01\nOut[17]: \n0    1.01\n1    2.01\n2    <NA>\ndtype: Float64\n  These dtypes can operate as part of DataFrame. \nIn [18]: df = pd.DataFrame({\"A\": s, \"B\": [1, 1, 3], \"C\": list(\"aab\")})\n\nIn [19]: df\nOut[19]: \n      A  B  C\n0     1  1  a\n1     2  1  a\n2  <NA>  3  b\n\nIn [20]: df.dtypes\nOut[20]: \nA     Int64\nB     int64\nC    object\ndtype: object\n  These dtypes can be merged & reshaped & casted. \nIn [21]: pd.concat([df[[\"A\"]], df[[\"B\", \"C\"]]], axis=1).dtypes\nOut[21]: \nA     Int64\nB     int64\nC    object\ndtype: object\n\nIn [22]: df[\"A\"].astype(float)\nOut[22]: \n0    1.0\n1    2.0\n2    NaN\nName: A, dtype: float64\n  Reduction and groupby operations such as \u2018sum\u2019 work as well. \nIn [23]: df.sum()\nOut[23]: \nA      3\nB      5\nC    aab\ndtype: object\n\nIn [24]: df.groupby(\"B\").A.sum()\nOut[24]: \nB\n1    3\n3    0\nName: A, dtype: Int64\n    Scalar NA Value arrays.IntegerArray uses pandas.NA as its scalar missing value. Slicing a single element that\u2019s missing will return pandas.NA \nIn [25]: a = pd.array([1, None], dtype=\"Int64\")\n\nIn [26]: a[1]\nOut[26]: <NA>\n  \n"}, {"name": "Options and settings", "path": "user_guide/options", "type": "Manual", "text": "Options and settings  Overview pandas has an options system that lets you customize some aspects of its behaviour, display-related options being those the user is most likely to adjust. Options have a full \u201cdotted-style\u201d, case-insensitive name (e.g. display.max_rows). You can get/set options directly as attributes of the top-level options attribute: \nIn [1]: import pandas as pd\n\nIn [2]: pd.options.display.max_rows\nOut[2]: 15\n\nIn [3]: pd.options.display.max_rows = 999\n\nIn [4]: pd.options.display.max_rows\nOut[4]: 999\n  The API is composed of 5 relevant functions, available directly from the pandas namespace:  get_option() / set_option() - get/set the value of a single option. reset_option() - reset one or more options to their default value. describe_option() - print the descriptions of one or more options. option_context() - execute a codeblock with a set of options that revert to prior settings after execution.  Note: Developers can check out pandas/core/config_init.py for more information. All of the functions above accept a regexp pattern (re.search style) as an argument, and so passing in a substring will work - as long as it is unambiguous: \nIn [5]: pd.get_option(\"display.chop_threshold\")\n\nIn [6]: pd.set_option(\"display.chop_threshold\", 2)\n\nIn [7]: pd.get_option(\"display.chop_threshold\")\nOut[7]: 2\n\nIn [8]: pd.set_option(\"chop\", 4)\n\nIn [9]: pd.get_option(\"display.chop_threshold\")\nOut[9]: 4\n  The following will not work because it matches multiple option names, e.g. display.max_colwidth, display.max_rows, display.max_columns: \nIn [10]: try:\n   ....:     pd.get_option(\"max\")\n   ....: except KeyError as e:\n   ....:     print(e)\n   ....: \n'Pattern matched multiple keys'\n  Note: Using this form of shorthand may cause your code to break if new options with similar names are added in future versions. You can get a list of available options and their descriptions with describe_option. When called with no argument describe_option will print out the descriptions for all available options.   Getting and setting options As described above, get_option() and set_option() are available from the pandas namespace. To change an option, call set_option('option regex', new_value). \nIn [11]: pd.get_option(\"mode.sim_interactive\")\nOut[11]: False\n\nIn [12]: pd.set_option(\"mode.sim_interactive\", True)\n\nIn [13]: pd.get_option(\"mode.sim_interactive\")\nOut[13]: True\n  Note: The option \u2018mode.sim_interactive\u2019 is mostly used for debugging purposes. All options also have a default value, and you can use reset_option to do just that: \nIn [14]: pd.get_option(\"display.max_rows\")\nOut[14]: 60\n\nIn [15]: pd.set_option(\"display.max_rows\", 999)\n\nIn [16]: pd.get_option(\"display.max_rows\")\nOut[16]: 999\n\nIn [17]: pd.reset_option(\"display.max_rows\")\n\nIn [18]: pd.get_option(\"display.max_rows\")\nOut[18]: 60\n  It\u2019s also possible to reset multiple options at once (using a regex): \nIn [19]: pd.reset_option(\"^display\")\n  option_context context manager has been exposed through the top-level API, allowing you to execute code with given option values. Option values are restored automatically when you exit the with block: \nIn [20]: with pd.option_context(\"display.max_rows\", 10, \"display.max_columns\", 5):\n   ....:     print(pd.get_option(\"display.max_rows\"))\n   ....:     print(pd.get_option(\"display.max_columns\"))\n   ....: \n10\n5\n\nIn [21]: print(pd.get_option(\"display.max_rows\"))\n60\n\nIn [22]: print(pd.get_option(\"display.max_columns\"))\n0\n    Setting startup options in Python/IPython environment Using startup scripts for the Python/IPython environment to import pandas and set options makes working with pandas more efficient. To do this, create a .py or .ipy script in the startup directory of the desired profile. An example where the startup folder is in a default IPython profile can be found at: \n$IPYTHONDIR/profile_default/startup\n  More information can be found in the IPython documentation. An example startup script for pandas is displayed below: \nimport pandas as pd\n\npd.set_option(\"display.max_rows\", 999)\npd.set_option(\"display.precision\", 5)\n    Frequently used options The following is a walk-through of the more frequently used display options. display.max_rows and display.max_columns sets the maximum number of rows and columns displayed when a frame is pretty-printed. Truncated lines are replaced by an ellipsis. \nIn [23]: df = pd.DataFrame(np.random.randn(7, 2))\n\nIn [24]: pd.set_option(\"display.max_rows\", 7)\n\nIn [25]: df\nOut[25]: \n          0         1\n0  0.469112 -0.282863\n1 -1.509059 -1.135632\n2  1.212112 -0.173215\n3  0.119209 -1.044236\n4 -0.861849 -2.104569\n5 -0.494929  1.071804\n6  0.721555 -0.706771\n\nIn [26]: pd.set_option(\"display.max_rows\", 5)\n\nIn [27]: df\nOut[27]: \n           0         1\n0   0.469112 -0.282863\n1  -1.509059 -1.135632\n..       ...       ...\n5  -0.494929  1.071804\n6   0.721555 -0.706771\n\n[7 rows x 2 columns]\n\nIn [28]: pd.reset_option(\"display.max_rows\")\n  Once the display.max_rows is exceeded, the display.min_rows options determines how many rows are shown in the truncated repr. \nIn [29]: pd.set_option(\"display.max_rows\", 8)\n\nIn [30]: pd.set_option(\"display.min_rows\", 4)\n\n# below max_rows -> all rows shown\nIn [31]: df = pd.DataFrame(np.random.randn(7, 2))\n\nIn [32]: df\nOut[32]: \n          0         1\n0 -1.039575  0.271860\n1 -0.424972  0.567020\n2  0.276232 -1.087401\n3 -0.673690  0.113648\n4 -1.478427  0.524988\n5  0.404705  0.577046\n6 -1.715002 -1.039268\n\n# above max_rows -> only min_rows (4) rows shown\nIn [33]: df = pd.DataFrame(np.random.randn(9, 2))\n\nIn [34]: df\nOut[34]: \n           0         1\n0  -0.370647 -1.157892\n1  -1.344312  0.844885\n..       ...       ...\n7   0.276662 -0.472035\n8  -0.013960 -0.362543\n\n[9 rows x 2 columns]\n\nIn [35]: pd.reset_option(\"display.max_rows\")\n\nIn [36]: pd.reset_option(\"display.min_rows\")\n  display.expand_frame_repr allows for the representation of dataframes to stretch across pages, wrapped over the full column vs row-wise. \nIn [37]: df = pd.DataFrame(np.random.randn(5, 10))\n\nIn [38]: pd.set_option(\"expand_frame_repr\", True)\n\nIn [39]: df\nOut[39]: \n          0         1         2         3         4         5         6         7         8         9\n0 -0.006154 -0.923061  0.895717  0.805244 -1.206412  2.565646  1.431256  1.340309 -1.170299 -0.226169\n1  0.410835  0.813850  0.132003 -0.827317 -0.076467 -1.187678  1.130127 -1.436737 -1.413681  1.607920\n2  1.024180  0.569605  0.875906 -2.211372  0.974466 -2.006747 -0.410001 -0.078638  0.545952 -1.219217\n3 -1.226825  0.769804 -1.281247 -0.727707 -0.121306 -0.097883  0.695775  0.341734  0.959726 -1.110336\n4 -0.619976  0.149748 -0.732339  0.687738  0.176444  0.403310 -0.154951  0.301624 -2.179861 -1.369849\n\nIn [40]: pd.set_option(\"expand_frame_repr\", False)\n\nIn [41]: df\nOut[41]: \n          0         1         2         3         4         5         6         7         8         9\n0 -0.006154 -0.923061  0.895717  0.805244 -1.206412  2.565646  1.431256  1.340309 -1.170299 -0.226169\n1  0.410835  0.813850  0.132003 -0.827317 -0.076467 -1.187678  1.130127 -1.436737 -1.413681  1.607920\n2  1.024180  0.569605  0.875906 -2.211372  0.974466 -2.006747 -0.410001 -0.078638  0.545952 -1.219217\n3 -1.226825  0.769804 -1.281247 -0.727707 -0.121306 -0.097883  0.695775  0.341734  0.959726 -1.110336\n4 -0.619976  0.149748 -0.732339  0.687738  0.176444  0.403310 -0.154951  0.301624 -2.179861 -1.369849\n\nIn [42]: pd.reset_option(\"expand_frame_repr\")\n  display.large_repr lets you select whether to display dataframes that exceed max_columns or max_rows as a truncated frame, or as a summary. \nIn [43]: df = pd.DataFrame(np.random.randn(10, 10))\n\nIn [44]: pd.set_option(\"display.max_rows\", 5)\n\nIn [45]: pd.set_option(\"large_repr\", \"truncate\")\n\nIn [46]: df\nOut[46]: \n           0         1         2         3         4         5         6         7         8         9\n0  -0.954208  1.462696 -1.743161 -0.826591 -0.345352  1.314232  0.690579  0.995761  2.396780  0.014871\n1   3.357427 -0.317441 -1.236269  0.896171 -0.487602 -0.082240 -2.182937  0.380396  0.084844  0.432390\n..       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...\n8  -0.303421 -0.858447  0.306996 -0.028665  0.384316  1.574159  1.588931  0.476720  0.473424 -0.242861\n9  -0.014805 -0.284319  0.650776 -1.461665 -1.137707 -0.891060 -0.693921  1.613616  0.464000  0.227371\n\n[10 rows x 10 columns]\n\nIn [47]: pd.set_option(\"large_repr\", \"info\")\n\nIn [48]: df\nOut[48]: \n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10 entries, 0 to 9\nData columns (total 10 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   0       10 non-null     float64\n 1   1       10 non-null     float64\n 2   2       10 non-null     float64\n 3   3       10 non-null     float64\n 4   4       10 non-null     float64\n 5   5       10 non-null     float64\n 6   6       10 non-null     float64\n 7   7       10 non-null     float64\n 8   8       10 non-null     float64\n 9   9       10 non-null     float64\ndtypes: float64(10)\nmemory usage: 928.0 bytes\n\nIn [49]: pd.reset_option(\"large_repr\")\n\nIn [50]: pd.reset_option(\"display.max_rows\")\n  display.max_colwidth sets the maximum width of columns. Cells of this length or longer will be truncated with an ellipsis. \nIn [51]: df = pd.DataFrame(\n   ....:     np.array(\n   ....:         [\n   ....:             [\"foo\", \"bar\", \"bim\", \"uncomfortably long string\"],\n   ....:             [\"horse\", \"cow\", \"banana\", \"apple\"],\n   ....:         ]\n   ....:     )\n   ....: )\n   ....: \n\nIn [52]: pd.set_option(\"max_colwidth\", 40)\n\nIn [53]: df\nOut[53]: \n       0    1       2                          3\n0    foo  bar     bim  uncomfortably long string\n1  horse  cow  banana                      apple\n\nIn [54]: pd.set_option(\"max_colwidth\", 6)\n\nIn [55]: df\nOut[55]: \n       0    1      2      3\n0    foo  bar    bim  un...\n1  horse  cow  ba...  apple\n\nIn [56]: pd.reset_option(\"max_colwidth\")\n  display.max_info_columns sets a threshold for when by-column info will be given. \nIn [57]: df = pd.DataFrame(np.random.randn(10, 10))\n\nIn [58]: pd.set_option(\"max_info_columns\", 11)\n\nIn [59]: df.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10 entries, 0 to 9\nData columns (total 10 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   0       10 non-null     float64\n 1   1       10 non-null     float64\n 2   2       10 non-null     float64\n 3   3       10 non-null     float64\n 4   4       10 non-null     float64\n 5   5       10 non-null     float64\n 6   6       10 non-null     float64\n 7   7       10 non-null     float64\n 8   8       10 non-null     float64\n 9   9       10 non-null     float64\ndtypes: float64(10)\nmemory usage: 928.0 bytes\n\nIn [60]: pd.set_option(\"max_info_columns\", 5)\n\nIn [61]: df.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10 entries, 0 to 9\nColumns: 10 entries, 0 to 9\ndtypes: float64(10)\nmemory usage: 928.0 bytes\n\nIn [62]: pd.reset_option(\"max_info_columns\")\n  display.max_info_rows: df.info() will usually show null-counts for each column. For large frames this can be quite slow. max_info_rows and max_info_cols limit this null check only to frames with smaller dimensions then specified. Note that you can specify the option df.info(null_counts=True) to override on showing a particular frame. \nIn [63]: df = pd.DataFrame(np.random.choice([0, 1, np.nan], size=(10, 10)))\n\nIn [64]: df\nOut[64]: \n     0    1    2    3    4    5    6    7    8    9\n0  0.0  NaN  1.0  NaN  NaN  0.0  NaN  0.0  NaN  1.0\n1  1.0  NaN  1.0  1.0  1.0  1.0  NaN  0.0  0.0  NaN\n2  0.0  NaN  1.0  0.0  0.0  NaN  NaN  NaN  NaN  0.0\n3  NaN  NaN  NaN  0.0  1.0  1.0  NaN  1.0  NaN  1.0\n4  0.0  NaN  NaN  NaN  0.0  NaN  NaN  NaN  1.0  0.0\n5  0.0  1.0  1.0  1.0  1.0  0.0  NaN  NaN  1.0  0.0\n6  1.0  1.0  1.0  NaN  1.0  NaN  1.0  0.0  NaN  NaN\n7  0.0  0.0  1.0  0.0  1.0  0.0  1.0  1.0  0.0  NaN\n8  NaN  NaN  NaN  0.0  NaN  NaN  NaN  NaN  1.0  NaN\n9  0.0  NaN  0.0  NaN  NaN  0.0  NaN  1.0  1.0  0.0\n\nIn [65]: pd.set_option(\"max_info_rows\", 11)\n\nIn [66]: df.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10 entries, 0 to 9\nData columns (total 10 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   0       8 non-null      float64\n 1   1       3 non-null      float64\n 2   2       7 non-null      float64\n 3   3       6 non-null      float64\n 4   4       7 non-null      float64\n 5   5       6 non-null      float64\n 6   6       2 non-null      float64\n 7   7       6 non-null      float64\n 8   8       6 non-null      float64\n 9   9       6 non-null      float64\ndtypes: float64(10)\nmemory usage: 928.0 bytes\n\nIn [67]: pd.set_option(\"max_info_rows\", 5)\n\nIn [68]: df.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10 entries, 0 to 9\nData columns (total 10 columns):\n #   Column  Dtype  \n---  ------  -----  \n 0   0       float64\n 1   1       float64\n 2   2       float64\n 3   3       float64\n 4   4       float64\n 5   5       float64\n 6   6       float64\n 7   7       float64\n 8   8       float64\n 9   9       float64\ndtypes: float64(10)\nmemory usage: 928.0 bytes\n\nIn [69]: pd.reset_option(\"max_info_rows\")\n  display.precision sets the output display precision in terms of decimal places. This is only a suggestion. \nIn [70]: df = pd.DataFrame(np.random.randn(5, 5))\n\nIn [71]: pd.set_option(\"display.precision\", 7)\n\nIn [72]: df\nOut[72]: \n           0          1          2          3          4\n0 -1.1506406 -0.7983341 -0.5576966  0.3813531  1.3371217\n1 -1.5310949  1.3314582 -0.5713290 -0.0266708 -1.0856630\n2 -1.1147378 -0.0582158 -0.4867681  1.6851483  0.1125723\n3 -1.4953086  0.8984347 -0.1482168 -1.5960698  0.1596530\n4  0.2621358  0.0362196  0.1847350 -0.2550694 -0.2710197\n\nIn [73]: pd.set_option(\"display.precision\", 4)\n\nIn [74]: df\nOut[74]: \n        0       1       2       3       4\n0 -1.1506 -0.7983 -0.5577  0.3814  1.3371\n1 -1.5311  1.3315 -0.5713 -0.0267 -1.0857\n2 -1.1147 -0.0582 -0.4868  1.6851  0.1126\n3 -1.4953  0.8984 -0.1482 -1.5961  0.1597\n4  0.2621  0.0362  0.1847 -0.2551 -0.2710\n  display.chop_threshold sets at what level pandas rounds to zero when it displays a Series of DataFrame. This setting does not change the precision at which the number is stored. \nIn [75]: df = pd.DataFrame(np.random.randn(6, 6))\n\nIn [76]: pd.set_option(\"chop_threshold\", 0)\n\nIn [77]: df\nOut[77]: \n        0       1       2       3       4       5\n0  1.2884  0.2946 -1.1658  0.8470 -0.6856  0.6091\n1 -0.3040  0.6256 -0.0593  0.2497  1.1039 -1.0875\n2  1.9980 -0.2445  0.1362  0.8863 -1.3507 -0.8863\n3 -1.0133  1.9209 -0.3882 -2.3144  0.6655  0.4026\n4  0.3996 -1.7660  0.8504  0.3881  0.9923  0.7441\n5 -0.7398 -1.0549 -0.1796  0.6396  1.5850  1.9067\n\nIn [78]: pd.set_option(\"chop_threshold\", 0.5)\n\nIn [79]: df\nOut[79]: \n        0       1       2       3       4       5\n0  1.2884  0.0000 -1.1658  0.8470 -0.6856  0.6091\n1  0.0000  0.6256  0.0000  0.0000  1.1039 -1.0875\n2  1.9980  0.0000  0.0000  0.8863 -1.3507 -0.8863\n3 -1.0133  1.9209  0.0000 -2.3144  0.6655  0.0000\n4  0.0000 -1.7660  0.8504  0.0000  0.9923  0.7441\n5 -0.7398 -1.0549  0.0000  0.6396  1.5850  1.9067\n\nIn [80]: pd.reset_option(\"chop_threshold\")\n  display.colheader_justify controls the justification of the headers. The options are \u2018right\u2019, and \u2018left\u2019. \nIn [81]: df = pd.DataFrame(\n   ....:     np.array([np.random.randn(6), np.random.randint(1, 9, 6) * 0.1, np.zeros(6)]).T,\n   ....:     columns=[\"A\", \"B\", \"C\"],\n   ....:     dtype=\"float\",\n   ....: )\n   ....: \n\nIn [82]: pd.set_option(\"colheader_justify\", \"right\")\n\nIn [83]: df\nOut[83]: \n        A    B    C\n0  0.1040  0.1  0.0\n1  0.1741  0.5  0.0\n2 -0.4395  0.4  0.0\n3 -0.7413  0.8  0.0\n4 -0.0797  0.4  0.0\n5 -0.9229  0.3  0.0\n\nIn [84]: pd.set_option(\"colheader_justify\", \"left\")\n\nIn [85]: df\nOut[85]: \n   A       B    C  \n0  0.1040  0.1  0.0\n1  0.1741  0.5  0.0\n2 -0.4395  0.4  0.0\n3 -0.7413  0.8  0.0\n4 -0.0797  0.4  0.0\n5 -0.9229  0.3  0.0\n\nIn [86]: pd.reset_option(\"colheader_justify\")\n    Available options        \nOption Default Function    \ndisplay.chop_threshold None If set to a float value, all float values smaller then the given threshold will be displayed as exactly 0 by repr and friends.  \ndisplay.colheader_justify right Controls the justification of column headers. used by DataFrameFormatter.  \ndisplay.column_space 12 No description available.  \ndisplay.date_dayfirst False When True, prints and parses dates with the day first, eg 20/01/2005  \ndisplay.date_yearfirst False When True, prints and parses dates with the year first, eg 2005/01/20  \ndisplay.encoding UTF-8 Defaults to the detected encoding of the console. Specifies the encoding to be used for strings returned by to_string, these are generally strings meant to be displayed on the console.  \ndisplay.expand_frame_repr True Whether to print out the full DataFrame repr for wide DataFrames across multiple lines, max_columns is still respected, but the output will wrap-around across multiple \u201cpages\u201d if its width exceeds display.width.  \ndisplay.float_format None The callable should accept a floating point number and return a string with the desired format of the number. This is used in some places like SeriesFormatter. See core.format.EngFormatter for an example.  \ndisplay.large_repr truncate For DataFrames exceeding max_rows/max_cols, the repr (and HTML repr) can show a truncated table (the default), or switch to the view from df.info() (the behaviour in earlier versions of pandas). allowable settings, [\u2018truncate\u2019, \u2018info\u2019]  \ndisplay.latex.repr False Whether to produce a latex DataFrame representation for Jupyter frontends that support it.  \ndisplay.latex.escape True Escapes special characters in DataFrames, when using the to_latex method.  \ndisplay.latex.longtable False Specifies if the to_latex method of a DataFrame uses the longtable format.  \ndisplay.latex.multicolumn True Combines columns when using a MultiIndex  \ndisplay.latex.multicolumn_format \u2018l\u2019 Alignment of multicolumn labels  \ndisplay.latex.multirow False Combines rows when using a MultiIndex. Centered instead of top-aligned, separated by clines.  \ndisplay.max_columns 0 or 20 max_rows and max_columns are used in __repr__() methods to decide if to_string() or info() is used to render an object to a string. In case Python/IPython is running in a terminal this is set to 0 by default and pandas will correctly auto-detect the width of the terminal and switch to a smaller format in case all columns would not fit vertically. The IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to do correct auto-detection, in which case the default is set to 20. \u2018None\u2019 value means unlimited.  \ndisplay.max_colwidth 50 The maximum width in characters of a column in the repr of a pandas data structure. When the column overflows, a \u201c\u2026\u201d placeholder is embedded in the output. \u2018None\u2019 value means unlimited.  \ndisplay.max_info_columns 100 max_info_columns is used in DataFrame.info method to decide if per column information will be printed.  \ndisplay.max_info_rows 1690785 df.info() will usually show null-counts for each column. For large frames this can be quite slow. max_info_rows and max_info_cols limit this null check only to frames with smaller dimensions then specified.  \ndisplay.max_rows 60 This sets the maximum number of rows pandas should output when printing out various output. For example, this value determines whether the repr() for a dataframe prints out fully or just a truncated or summary repr. \u2018None\u2019 value means unlimited.  \ndisplay.min_rows 10 The numbers of rows to show in a truncated repr (when max_rows is exceeded). Ignored when max_rows is set to None or 0. When set to None, follows the value of max_rows.  \ndisplay.max_seq_items 100 when pretty-printing a long sequence, no more then max_seq_items will be printed. If items are omitted, they will be denoted by the addition of \u201c\u2026\u201d to the resulting string. If set to None, the number of items to be printed is unlimited.  \ndisplay.memory_usage True This specifies if the memory usage of a DataFrame should be displayed when the df.info() method is invoked.  \ndisplay.multi_sparse True \u201cSparsify\u201d MultiIndex display (don\u2019t display repeated elements in outer levels within groups)  \ndisplay.notebook_repr_html True When True, IPython notebook will use html representation for pandas objects (if it is available).  \ndisplay.pprint_nest_depth 3 Controls the number of nested levels to process when pretty-printing  \ndisplay.precision 6 Floating point output precision in terms of number of places after the decimal, for regular formatting as well as scientific notation. Similar to numpy\u2019s precision print option  \ndisplay.show_dimensions truncate Whether to print out dimensions at the end of DataFrame repr. If \u2018truncate\u2019 is specified, only print out the dimensions if the frame is truncated (e.g. not display all rows and/or columns)  \ndisplay.width 80 Width of the display in characters. In case Python/IPython is running in a terminal this can be set to None and pandas will correctly auto-detect the width. Note that the IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to correctly detect the width.  \ndisplay.html.table_schema False Whether to publish a Table Schema representation for frontends that support it.  \ndisplay.html.border 1 A border=value attribute is inserted in the <table> tag for the DataFrame HTML repr.  \ndisplay.html.use_mathjax True When True, Jupyter notebook will process table contents using MathJax, rendering mathematical expressions enclosed by the dollar symbol.  \ndisplay.max_dir_items 100 The number of columns from a dataframe that are added to dir. These columns can then be suggested by tab completion. \u2018None\u2019 value means unlimited.  \nio.excel.xls.writer xlwt \nThe default Excel writer engine for \u2018xls\u2019 files.  Deprecated since version 1.2.0: As xlwt package is no longer maintained, the xlwt engine will be removed in a future version of pandas. Since this is the only engine in pandas that supports writing to .xls files, this option will also be removed.    \nio.excel.xlsm.writer openpyxl The default Excel writer engine for \u2018xlsm\u2019 files. Available options: \u2018openpyxl\u2019 (the default).  \nio.excel.xlsx.writer openpyxl The default Excel writer engine for \u2018xlsx\u2019 files.  \nio.hdf.default_format None default format writing format, if None, then put will default to \u2018fixed\u2019 and append will default to \u2018table\u2019  \nio.hdf.dropna_table True drop ALL nan rows when appending to a table  \nio.parquet.engine None The engine to use as a default for parquet reading and writing. If None then try \u2018pyarrow\u2019 and \u2018fastparquet\u2019  \nio.sql.engine None The engine to use as a default for sql reading and writing, with SQLAlchemy as a higher level interface. If None then try \u2018sqlalchemy\u2019  \nmode.chained_assignment warn Controls SettingWithCopyWarning: \u2018raise\u2019, \u2018warn\u2019, or None. Raise an exception, warn, or no action if trying to use chained assignment.  \nmode.sim_interactive False Whether to simulate interactive mode for purposes of testing.  \nmode.use_inf_as_na False True means treat None, NaN, -INF, INF as NA (old way), False means None and NaN are null, but INF, -INF are not NA (new way).  \ncompute.use_bottleneck True Use the bottleneck library to accelerate computation if it is installed.  \ncompute.use_numexpr True Use the numexpr library to accelerate computation if it is installed.  \nplotting.backend matplotlib Change the plotting backend to a different backend than the current matplotlib one. Backends can be implemented as third-party libraries implementing the pandas plotting API. They can use other plotting libraries like Bokeh, Altair, etc.  \nplotting.matplotlib.register_converters True Register custom converters with matplotlib. Set to False to de-register.  \nstyler.sparse.index True \u201cSparsify\u201d MultiIndex display for rows in Styler output (don\u2019t display repeated elements in outer levels within groups).  \nstyler.sparse.columns True \u201cSparsify\u201d MultiIndex display for columns in Styler output.  \nstyler.render.repr html Standard output format for Styler rendered in Jupyter Notebook. Should be one of \u201chtml\u201d or \u201clatex\u201d.  \nstyler.render.max_elements 262144 Maximum number of datapoints that Styler will render trimming either rows, columns or both to fit.  \nstyler.render.max_rows None Maximum number of rows that Styler will render. By default this is dynamic based on max_elements.  \nstyler.render.max_columns None Maximum number of columns that Styler will render. By default this is dynamic based on max_elements.  \nstyler.render.encoding utf-8 Default encoding for output HTML or LaTeX files.  \nstyler.format.formatter None Object to specify formatting functions to Styler.format.  \nstyler.format.na_rep None String representation for missing data.  \nstyler.format.precision 6 Precision to display floating point and complex numbers.  \nstyler.format.decimal . String representation for decimal point separator for floating point and complex numbers.  \nstyler.format.thousands None String representation for thousands separator for integers, and floating point and complex numbers.  \nstyler.format.escape None Whether to escape \u201chtml\u201d or \u201clatex\u201d special characters in the display representation.  \nstyler.html.mathjax True If set to False will render specific CSS classes to table attributes that will prevent Mathjax from rendering in Jupyter Notebook.  \nstyler.latex.multicol_align r Alignment of headers in a merged column due to sparsification. Can be in {\u201cr\u201d, \u201cc\u201d, \u201cl\u201d}.  \nstyler.latex.multirow_align c Alignment of index labels in a merged row due to sparsification. Can be in {\u201cc\u201d, \u201ct\u201d, \u201cb\u201d}.  \nstyler.latex.environment None If given will replace the default \\\\begin{table} environment. If \u201clongtable\u201d is specified this will render with a specific \u201clongtable\u201d template with longtable features.  \nstyler.latex.hrules False If set to True will render \\\\toprule, \\\\midrule, and \\bottomrule by default.      Number formatting pandas also allows you to set how numbers are displayed in the console. This option is not set through the set_options API. Use the set_eng_float_format function to alter the floating-point formatting of pandas objects to produce a particular format. For instance: \nIn [87]: import numpy as np\n\nIn [88]: pd.set_eng_float_format(accuracy=3, use_eng_prefix=True)\n\nIn [89]: s = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n\nIn [90]: s / 1.0e3\nOut[90]: \na    303.638u\nb   -721.084u\nc   -622.696u\nd    648.250u\ne     -1.945m\ndtype: float64\n\nIn [91]: s / 1.0e6\nOut[91]: \na    303.638n\nb   -721.084n\nc   -622.696n\nd    648.250n\ne     -1.945u\ndtype: float64\n  To round floats on a case-by-case basis, you can also use round() and round().   Unicode formatting  Warning Enabling this option will affect the performance for printing of DataFrame and Series (about 2 times slower). Use only when it is actually required.  Some East Asian countries use Unicode characters whose width corresponds to two Latin characters. If a DataFrame or Series contains these characters, the default output mode may not align them properly.  Note Screen captures are attached for each output to show the actual results.  \nIn [92]: df = pd.DataFrame({\"\u56fd\u7c4d\": [\"UK\", \"\u65e5\u672c\"], \"\u540d\u524d\": [\"Alice\", \"\u3057\u306e\u3076\"]})\n\nIn [93]: df\nOut[93]: \n   \u56fd\u7c4d     \u540d\u524d\n0  UK  Alice\n1  \u65e5\u672c    \u3057\u306e\u3076\n   Enabling display.unicode.east_asian_width allows pandas to check each character\u2019s \u201cEast Asian Width\u201d property. These characters can be aligned properly by setting this option to True. However, this will result in longer render times than the standard len function. \nIn [94]: pd.set_option(\"display.unicode.east_asian_width\", True)\n\nIn [95]: df\nOut[95]: \n   \u56fd\u7c4d    \u540d\u524d\n0    UK   Alice\n1  \u65e5\u672c  \u3057\u306e\u3076\n   In addition, Unicode characters whose width is \u201cAmbiguous\u201d can either be 1 or 2 characters wide depending on the terminal setting or encoding. The option display.unicode.ambiguous_as_wide can be used to handle the ambiguity. By default, an \u201cAmbiguous\u201d character\u2019s width, such as \u201c\u00a1\u201d (inverted exclamation) in the example below, is taken to be 1. \nIn [96]: df = pd.DataFrame({\"a\": [\"xxx\", \"\u00a1\u00a1\"], \"b\": [\"yyy\", \"\u00a1\u00a1\"]})\n\nIn [97]: df\nOut[97]: \n     a    b\n0  xxx  yyy\n1   \u00a1\u00a1   \u00a1\u00a1\n   Enabling display.unicode.ambiguous_as_wide makes pandas interpret these characters\u2019 widths to be 2. (Note that this option will only be effective when display.unicode.east_asian_width is enabled.) However, setting this option incorrectly for your terminal will cause these characters to be aligned incorrectly: \nIn [98]: pd.set_option(\"display.unicode.ambiguous_as_wide\", True)\n\nIn [99]: df\nOut[99]: \n      a     b\n0   xxx   yyy\n1  \u00a1\u00a1  \u00a1\u00a1\n     Table schema display DataFrame and Series will publish a Table Schema representation by default. False by default, this can be enabled globally with the display.html.table_schema option: \nIn [100]: pd.set_option(\"display.html.table_schema\", True)\n  Only 'display.max_rows' are serialized and published. \n"}, {"name": "pandas arrays, scalars, and data types", "path": "reference/arrays", "type": "Pandas arrays", "text": "pandas arrays, scalars, and data types For most data types, pandas uses NumPy arrays as the concrete objects contained with a Index, Series, or DataFrame. For some data types, pandas extends NumPy\u2019s type system. String aliases for these types can be found at dtypes.         \nKind of Data pandas Data Type Scalar Array    \nTZ-aware datetime DatetimeTZDtype Timestamp Datetime data  \nTimedeltas (none) Timedelta Timedelta data  \nPeriod (time spans) PeriodDtype Period Timespan data  \nIntervals IntervalDtype Interval Interval data  \nNullable Integer Int64Dtype, \u2026 (none) Nullable integer  \nCategorical CategoricalDtype (none) Categorical data  \nSparse SparseDtype (none) Sparse data  \nStrings StringDtype str Text data  \nBoolean (with NA) BooleanDtype bool Boolean data with missing values    pandas and third-party libraries can extend NumPy\u2019s type system (see Extension types). The top-level array() method can be used to create a new array, which may be stored in a Series, Index, or as a column in a DataFrame.       \narray(data[, dtype, copy]) Create an array.     Datetime data NumPy cannot natively represent timezone-aware datetimes. pandas supports this with the arrays.DatetimeArray extension array, which can hold timezone-naive or timezone-aware values. Timestamp, a subclass of datetime.datetime, is pandas\u2019 scalar type for timezone-naive or timezone-aware datetime data.       \nTimestamp([ts_input, freq, tz, unit, year, ...]) Pandas replacement for python datetime.datetime object.     Properties       \nTimestamp.asm8 Return numpy datetime64 format in nanoseconds.  \nTimestamp.day   \nTimestamp.dayofweek Return day of the week.  \nTimestamp.day_of_week Return day of the week.  \nTimestamp.dayofyear Return the day of the year.  \nTimestamp.day_of_year Return the day of the year.  \nTimestamp.days_in_month Return the number of days in the month.  \nTimestamp.daysinmonth Return the number of days in the month.  \nTimestamp.fold   \nTimestamp.hour   \nTimestamp.is_leap_year Return True if year is a leap year.  \nTimestamp.is_month_end Return True if date is last day of month.  \nTimestamp.is_month_start Return True if date is first day of month.  \nTimestamp.is_quarter_end Return True if date is last day of the quarter.  \nTimestamp.is_quarter_start Return True if date is first day of the quarter.  \nTimestamp.is_year_end Return True if date is last day of the year.  \nTimestamp.is_year_start Return True if date is first day of the year.  \nTimestamp.max   \nTimestamp.microsecond   \nTimestamp.min   \nTimestamp.minute   \nTimestamp.month   \nTimestamp.nanosecond   \nTimestamp.quarter Return the quarter of the year.  \nTimestamp.resolution   \nTimestamp.second   \nTimestamp.tz Alias for tzinfo.  \nTimestamp.tzinfo   \nTimestamp.value   \nTimestamp.week Return the week number of the year.  \nTimestamp.weekofyear Return the week number of the year.  \nTimestamp.year       Methods       \nTimestamp.astimezone(tz) Convert timezone-aware Timestamp to another time zone.  \nTimestamp.ceil(freq[, ambiguous, nonexistent]) Return a new Timestamp ceiled to this resolution.  \nTimestamp.combine(date, time) Combine date, time into datetime with same date and time fields.  \nTimestamp.ctime Return ctime() style string.  \nTimestamp.date Return date object with same year, month and day.  \nTimestamp.day_name Return the day name of the Timestamp with specified locale.  \nTimestamp.dst Return self.tzinfo.dst(self).  \nTimestamp.floor(freq[, ambiguous, nonexistent]) Return a new Timestamp floored to this resolution.  \nTimestamp.freq   \nTimestamp.freqstr Return the total number of days in the month.  \nTimestamp.fromordinal(ordinal[, freq, tz]) Passed an ordinal, translate and convert to a ts.  \nTimestamp.fromtimestamp(ts) Transform timestamp[, tz] to tz's local time from POSIX timestamp.  \nTimestamp.isocalendar Return a 3-tuple containing ISO year, week number, and weekday.  \nTimestamp.isoformat Return the time formatted according to ISO 8610.  \nTimestamp.isoweekday() Return the day of the week represented by the date.  \nTimestamp.month_name Return the month name of the Timestamp with specified locale.  \nTimestamp.normalize Normalize Timestamp to midnight, preserving tz information.  \nTimestamp.now([tz]) Return new Timestamp object representing current time local to tz.  \nTimestamp.replace([year, month, day, hour, ...]) Implements datetime.replace, handles nanoseconds.  \nTimestamp.round(freq[, ambiguous, nonexistent]) Round the Timestamp to the specified resolution.  \nTimestamp.strftime(format) Return a string representing the given POSIX timestamp controlled by an explicit format string.  \nTimestamp.strptime(string, format) Function is not implemented.  \nTimestamp.time Return time object with same time but with tzinfo=None.  \nTimestamp.timestamp Return POSIX timestamp as float.  \nTimestamp.timetuple Return time tuple, compatible with time.localtime().  \nTimestamp.timetz Return time object with same time and tzinfo.  \nTimestamp.to_datetime64 Return a numpy.datetime64 object with 'ns' precision.  \nTimestamp.to_numpy Convert the Timestamp to a NumPy datetime64.  \nTimestamp.to_julian_date() Convert TimeStamp to a Julian Date.  \nTimestamp.to_period Return an period of which this timestamp is an observation.  \nTimestamp.to_pydatetime Convert a Timestamp object to a native Python datetime object.  \nTimestamp.today(cls[, tz]) Return the current time in the local timezone.  \nTimestamp.toordinal Return proleptic Gregorian ordinal.  \nTimestamp.tz_convert(tz) Convert timezone-aware Timestamp to another time zone.  \nTimestamp.tz_localize(tz[, ambiguous, ...]) Convert naive Timestamp to local time zone, or remove timezone from timezone-aware Timestamp.  \nTimestamp.tzname Return self.tzinfo.tzname(self).  \nTimestamp.utcfromtimestamp(ts) Construct a naive UTC datetime from a POSIX timestamp.  \nTimestamp.utcnow() Return a new Timestamp representing UTC day and time.  \nTimestamp.utcoffset Return self.tzinfo.utcoffset(self).  \nTimestamp.utctimetuple Return UTC time tuple, compatible with time.localtime().  \nTimestamp.weekday() Return the day of the week represented by the date.    A collection of timestamps may be stored in a arrays.DatetimeArray. For timezone-aware data, the .dtype of a arrays.DatetimeArray is a DatetimeTZDtype. For timezone-naive data, np.dtype(\"datetime64[ns]\") is used. If the data are timezone-aware, then every value in the array must have the same timezone.       \narrays.DatetimeArray(values[, dtype, freq, copy]) Pandas ExtensionArray for tz-naive or tz-aware datetime data.          \nDatetimeTZDtype([unit, tz]) An ExtensionDtype for timezone-aware datetime data.       Timedelta data NumPy can natively represent timedeltas. pandas provides Timedelta for symmetry with Timestamp.       \nTimedelta([value, unit]) Represents a duration, the difference between two dates or times.     Properties       \nTimedelta.asm8 Return a numpy timedelta64 array scalar view.  \nTimedelta.components Return a components namedtuple-like.  \nTimedelta.days Number of days.  \nTimedelta.delta Return the timedelta in nanoseconds (ns), for internal compatibility.  \nTimedelta.freq   \nTimedelta.is_populated   \nTimedelta.max   \nTimedelta.microseconds Number of microseconds (>= 0 and less than 1 second).  \nTimedelta.min   \nTimedelta.nanoseconds Return the number of nanoseconds (n), where 0 <= n < 1 microsecond.  \nTimedelta.resolution   \nTimedelta.seconds Number of seconds (>= 0 and less than 1 day).  \nTimedelta.value   \nTimedelta.view Array view compatibility.      Methods       \nTimedelta.ceil(freq) Return a new Timedelta ceiled to this resolution.  \nTimedelta.floor(freq) Return a new Timedelta floored to this resolution.  \nTimedelta.isoformat Format Timedelta as ISO 8601 Duration like P[n]Y[n]M[n]DT[n]H[n]M[n]S, where the [n] s are replaced by the values.  \nTimedelta.round(freq) Round the Timedelta to the specified resolution.  \nTimedelta.to_pytimedelta Convert a pandas Timedelta object into a python datetime.timedelta object.  \nTimedelta.to_timedelta64 Return a numpy.timedelta64 object with 'ns' precision.  \nTimedelta.to_numpy Convert the Timedelta to a NumPy timedelta64.  \nTimedelta.total_seconds Total seconds in the duration.    A collection of Timedelta may be stored in a TimedeltaArray.       \narrays.TimedeltaArray(values[, dtype, freq, ...]) Pandas ExtensionArray for timedelta data.       Timespan data pandas represents spans of times as Period objects.   Period       \nPeriod([value, freq, ordinal, year, month, ...]) Represents a period of time.     Properties       \nPeriod.day Get day of the month that a Period falls on.  \nPeriod.dayofweek Day of the week the period lies in, with Monday=0 and Sunday=6.  \nPeriod.day_of_week Day of the week the period lies in, with Monday=0 and Sunday=6.  \nPeriod.dayofyear Return the day of the year.  \nPeriod.day_of_year Return the day of the year.  \nPeriod.days_in_month Get the total number of days in the month that this period falls on.  \nPeriod.daysinmonth Get the total number of days of the month that the Period falls in.  \nPeriod.end_time Get the Timestamp for the end of the period.  \nPeriod.freq   \nPeriod.freqstr Return a string representation of 