{"index":"<h1 id=\"pytorch-documentation\">PyTorch documentation</h1> <p>PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.</p> <p>Features described in this documentation are classified by release status:</p>  <p><em>Stable:</em> These features will be maintained long-term and there should generally be no major performance limitations or gaps in documentation. We also expect to maintain backwards compatibility (although breaking changes can happen and notice will be given one release ahead of time).</p> <p><em>Beta:</em> These features are tagged as Beta because the API may change based on user feedback, because the performance needs to improve, or because coverage across operators is not yet complete. For Beta features, we are committing to seeing the feature through to the Stable classification. We are not, however, committing to backwards compatibility.</p> <p><em>Prototype:</em> These features are typically not available as part of binary distributions like PyPI or Conda, except sometimes behind run-time flags, and are at an early stage for feedback and testing.</p>   <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Community</span></p> <ul> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/community/build_ci_governance.html\">PyTorch Governance | Build + CI</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/community/contribution_guide.html\">PyTorch Contribution Guide</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/community/design.html\">PyTorch Design Philosophy</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/community/governance.html\">PyTorch Governance | Mechanics</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/community/persons_of_interest.html\">PyTorch Governance | Maintainers</a></li> </ul>   <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Developer Notes</span></p> <ul> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/amp_examples.html\">CUDA Automatic Mixed Precision examples</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/autograd.html\">Autograd mechanics</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/broadcasting.html\">Broadcasting semantics</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/cpu_threading_torchscript_inference.html\">CPU threading and TorchScript inference</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/cuda.html\">CUDA semantics</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/ddp.html\">Distributed Data Parallel</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/extending.html\">Extending PyTorch</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/extending.func.html\">Extending torch.func with autograd.Function</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/faq.html\">Frequently Asked Questions</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/gradcheck.html\">Gradcheck mechanics</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/hip.html\">HIP (ROCm) semantics</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/large_scale_deployments.html\">Features for large-scale deployments</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/modules.html\">Modules</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/mps.html\">MPS backend</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/multiprocessing.html\">Multiprocessing best practices</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/numerical_accuracy.html\">Numerical accuracy</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/randomness.html\">Reproducibility</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/serialization.html\">Serialization semantics</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/windows.html\">Windows FAQ</a></li> </ul>   <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Language Bindings</span></p> <ul> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/cpp_index.html\">C++</a></li> <li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://pytorch.org/javadoc/\">Javadoc</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/deploy.html\">torch::deploy</a></li> </ul>   <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p> <ul> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"torch\">torch</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"torch#tensors\">Tensors</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"torch#generators\">Generators</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"torch#random-sampling\">Random sampling</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"torch#serialization\">Serialization</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"torch#parallelism\">Parallelism</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"torch#locally-disabling-gradient-computation\">Locally disabling gradient computation</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"torch#math-operations\">Math operations</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"torch#utilities\">Utilities</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"torch#symbolic-numbers\">Symbolic Numbers</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"torch#export-path\">Export Path</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"torch#optimizations\">Optimizations</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"torch#operator-tags\">Operator Tags</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"nn\">torch.nn</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.nn.parameter.parameter\">Parameter</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.nn.parameter.uninitializedparameter\">UninitializedParameter</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.nn.parameter.uninitializedbuffer\">UninitializedBuffer</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn#containers\">Containers</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn#convolution-layers\">Convolution Layers</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn#pooling-layers\">Pooling layers</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn#padding-layers\">Padding Layers</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn#non-linear-activations-weighted-sum-nonlinearity\">Non-linear Activations (weighted sum, nonlinearity)</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn#non-linear-activations-other\">Non-linear Activations (other)</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn#normalization-layers\">Normalization Layers</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn#recurrent-layers\">Recurrent Layers</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn#transformer-layers\">Transformer Layers</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn#linear-layers\">Linear Layers</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn#dropout-layers\">Dropout Layers</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn#sparse-layers\">Sparse Layers</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn#distance-functions\">Distance Functions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn#loss-functions\">Loss Functions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn#vision-layers\">Vision Layers</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn#shuffle-layers\">Shuffle Layers</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn#module-torch.nn.parallel\">DataParallel Layers (multi-GPU, distributed)</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn#module-torch.nn.utils\">Utilities</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn#quantized-functions\">Quantized Functions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn#lazy-modules-initialization\">Lazy Modules Initialization</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"nn.functional\">torch.nn.functional</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.functional#convolution-functions\">Convolution functions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.functional#pooling-functions\">Pooling functions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.functional#attention-mechanisms\">Attention Mechanisms</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.functional#non-linear-activation-functions\">Non-linear activation functions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.functional#linear-functions\">Linear functions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.functional#dropout-functions\">Dropout functions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.functional#sparse-functions\">Sparse functions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.functional#distance-functions\">Distance functions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.functional#loss-functions\">Loss functions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.functional#vision-functions\">Vision functions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.functional#dataparallel-functions-multi-gpu-distributed\">DataParallel functions (multi-GPU, distributed)</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"tensors\">torch.Tensor</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"tensors#data-types\">Data types</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"tensors#initializing-and-basic-operations\">Initializing and basic operations</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"tensors#tensor-class-reference\">Tensor class reference</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"tensor_attributes\">Tensor Attributes</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"tensor_attributes#torch-dtype\">torch.dtype</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"tensor_attributes#torch-device\">torch.device</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"tensor_attributes#torch-layout\">torch.layout</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"tensor_attributes#torch-memory-format\">torch.memory_format</a></li> </ul> </li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tensor_view\">Tensor Views</a></li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"amp\">torch.amp</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"amp#autocasting\">Autocasting</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"amp#gradient-scaling\">Gradient Scaling</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"amp#autocast-op-reference\">Autocast Op Reference</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"autograd\">torch.autograd</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.autograd.backward\">torch.autograd.backward</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.autograd.grad\">torch.autograd.grad</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"autograd#forward-mode-automatic-differentiation\">Forward-mode Automatic Differentiation</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"autograd#functional-higher-level-api\">Functional higher level API</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"autograd#locally-disabling-gradient-computation\">Locally disabling gradient computation</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"autograd#default-gradient-layouts\">Default gradient layouts</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"autograd#in-place-operations-on-tensors\">In-place operations on Tensors</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"autograd#variable-deprecated\">Variable (deprecated)</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"autograd#tensor-autograd-functions\">Tensor autograd functions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"autograd#function\"><span class=\"hidden-section\">Function</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"autograd#context-method-mixins\">Context method mixins</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"autograd#numerical-gradient-checking\">Numerical gradient checking</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"autograd#profiler\">Profiler</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"autograd#anomaly-detection\">Anomaly detection</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"autograd#autograd-graph\">Autograd graph</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"library\">torch.library</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"library#torch.library.Library\"><code>Library</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"library#torch.library.fallthrough_kernel\"><code>fallthrough_kernel()</code></a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"cpu\">torch.cpu</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cpu.current_stream\">torch.cpu.current_stream</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cpu.is_available\">torch.cpu.is_available</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cpu.synchronize\">torch.cpu.synchronize</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cpu.stream\">torch.cpu.stream</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cpu.device_count\">torch.cpu.device_count</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cpu.streamcontext\">StreamContext</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"cpu#streams-and-events\">Streams and events</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"cuda\">torch.cuda</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.streamcontext\">StreamContext</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.can_device_access_peer\">torch.cuda.can_device_access_peer</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.current_blas_handle\">torch.cuda.current_blas_handle</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.current_device\">torch.cuda.current_device</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.current_stream\">torch.cuda.current_stream</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.default_stream\">torch.cuda.default_stream</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.device\">device</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.device_count\">torch.cuda.device_count</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.device_of\">device_of</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.get_arch_list\">torch.cuda.get_arch_list</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.get_device_capability\">torch.cuda.get_device_capability</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.get_device_name\">torch.cuda.get_device_name</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.get_device_properties\">torch.cuda.get_device_properties</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.get_gencode_flags\">torch.cuda.get_gencode_flags</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.get_sync_debug_mode\">torch.cuda.get_sync_debug_mode</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.init\">torch.cuda.init</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.ipc_collect\">torch.cuda.ipc_collect</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.is_available\">torch.cuda.is_available</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.is_initialized\">torch.cuda.is_initialized</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.memory_usage\">torch.cuda.memory_usage</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.set_device\">torch.cuda.set_device</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.set_stream\">torch.cuda.set_stream</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.set_sync_debug_mode\">torch.cuda.set_sync_debug_mode</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.stream\">torch.cuda.stream</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.synchronize\">torch.cuda.synchronize</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.utilization\">torch.cuda.utilization</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.temperature\">torch.cuda.temperature</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.power_draw\">torch.cuda.power_draw</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.clock_rate\">torch.cuda.clock_rate</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.cuda.outofmemoryerror\">torch.cuda.OutOfMemoryError</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"cuda#random-number-generator\">Random Number Generator</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"cuda#communication-collectives\">Communication collectives</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"cuda#streams-and-events\">Streams and events</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"cuda#graphs-beta\">Graphs (beta)</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"cuda#memory-management\">Memory management</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"cuda#nvidia-tools-extension-nvtx\">NVIDIA Tools Extension (NVTX)</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"cuda#jiterator-beta\">Jiterator (beta)</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"cuda#stream-sanitizer-prototype\">Stream Sanitizer (prototype)</a></li> </ul> </li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"torch_cuda_memory\">Understanding CUDA Memory Usage</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"torch_cuda_memory#generating-a-snapshot\">Generating a Snapshot</a></li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"torch_cuda_memory#using-the-visualizer\">Using the visualizer</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"torch_cuda_memory#active-memory-timeline\">Active Memory Timeline</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"torch_cuda_memory#allocator-state-history\">Allocator State History</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"torch_cuda_memory#snapshot-api-reference\">Snapshot API Reference</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"torch_cuda_memory#torch.cuda.memory._record_memory_history\"><code>_record_memory_history()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"torch_cuda_memory#torch.cuda.memory._snapshot\"><code>_snapshot()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"torch_cuda_memory#torch.cuda.memory._dump_snapshot\"><code>_dump_snapshot()</code></a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"mps\">torch.mps</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.mps.synchronize\">torch.mps.synchronize</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.mps.get_rng_state\">torch.mps.get_rng_state</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.mps.set_rng_state\">torch.mps.set_rng_state</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.mps.manual_seed\">torch.mps.manual_seed</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.mps.seed\">torch.mps.seed</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.mps.empty_cache\">torch.mps.empty_cache</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.mps.set_per_process_memory_fraction\">torch.mps.set_per_process_memory_fraction</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.mps.current_allocated_memory\">torch.mps.current_allocated_memory</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.mps.driver_allocated_memory\">torch.mps.driver_allocated_memory</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"mps#mps-profiler\">MPS Profiler</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"mps#mps-event\">MPS Event</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"backends\">torch.backends</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"backends#module-torch.backends.cpu\">torch.backends.cpu</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"backends#module-torch.backends.cuda\">torch.backends.cuda</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"backends#module-torch.backends.cudnn\">torch.backends.cudnn</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"backends#module-torch.backends.mps\">torch.backends.mps</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"backends#module-torch.backends.mkl\">torch.backends.mkl</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"backends#module-torch.backends.mkldnn\">torch.backends.mkldnn</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"backends#module-torch.backends.openmp\">torch.backends.openmp</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"backends#module-torch.backends.opt_einsum\">torch.backends.opt_einsum</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"backends#module-torch.backends.xeon\">torch.backends.xeon</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"export\">torch.export</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"export#overview\">Overview</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"export#exporting-a-pytorch-model\">Exporting a PyTorch Model</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"export#limitations-of-torch-export\">Limitations of torch.export</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"export#read-more\">Read More</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"export#module-torch.export\">API Reference</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"distributed\">torch.distributed</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed#backends\">Backends</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed#basics\">Basics</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed#initialization\">Initialization</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed#post-initialization\">Post-Initialization</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed#distributed-key-value-store\">Distributed Key-Value Store</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed#groups\">Groups</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed#point-to-point-communication\">Point-to-point communication</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed#synchronous-and-asynchronous-collective-operations\">Synchronous and asynchronous collective operations</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed#collective-functions\">Collective functions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed#profiling-collective-communication\">Profiling Collective Communication</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed#multi-gpu-collective-functions\">Multi-GPU collective functions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed#third-party-backends\">Third-party backends</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed#launch-utility\">Launch utility</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed#spawn-utility\">Spawn utility</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed#debugging-torch-distributed-applications\">Debugging <code>torch.distributed</code> applications</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed#logging\">Logging</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"distributed.algorithms.join\">torch.distributed.algorithms.join</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.algorithms.join#torch.distributed.algorithms.Join\"><code>Join</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.algorithms.join#torch.distributed.algorithms.Joinable\"><code>Joinable</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.algorithms.join#torch.distributed.algorithms.JoinHook\"><code>JoinHook</code></a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"distributed.elastic\">torch.distributed.elastic</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.elastic#get-started\">Get Started</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.elastic#documentation\">Documentation</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"fsdp\">torch.distributed.fsdp</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"fsdp#torch.distributed.fsdp.FullyShardedDataParallel\"><code>FullyShardedDataParallel</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"fsdp#torch.distributed.fsdp.BackwardPrefetch\"><code>BackwardPrefetch</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"fsdp#torch.distributed.fsdp.ShardingStrategy\"><code>ShardingStrategy</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"fsdp#torch.distributed.fsdp.MixedPrecision\"><code>MixedPrecision</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"fsdp#torch.distributed.fsdp.CPUOffload\"><code>CPUOffload</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"fsdp#torch.distributed.fsdp.StateDictConfig\"><code>StateDictConfig</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"fsdp#torch.distributed.fsdp.FullStateDictConfig\"><code>FullStateDictConfig</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"fsdp#torch.distributed.fsdp.ShardedStateDictConfig\"><code>ShardedStateDictConfig</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"fsdp#torch.distributed.fsdp.LocalStateDictConfig\"><code>LocalStateDictConfig</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"fsdp#torch.distributed.fsdp.OptimStateDictConfig\"><code>OptimStateDictConfig</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"fsdp#torch.distributed.fsdp.FullOptimStateDictConfig\"><code>FullOptimStateDictConfig</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"fsdp#torch.distributed.fsdp.ShardedOptimStateDictConfig\"><code>ShardedOptimStateDictConfig</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"fsdp#torch.distributed.fsdp.LocalOptimStateDictConfig\"><code>LocalOptimStateDictConfig</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"fsdp#torch.distributed.fsdp.StateDictSettings\"><code>StateDictSettings</code></a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"distributed.optim\">torch.distributed.optim</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.optim#torch.distributed.optim.DistributedOptimizer\"><code>DistributedOptimizer</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.optim#torch.distributed.optim.PostLocalSGDOptimizer\"><code>PostLocalSGDOptimizer</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.optim#torch.distributed.optim.ZeroRedundancyOptimizer\"><code>ZeroRedundancyOptimizer</code></a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"distributed.tensor.parallel\">torch.distributed.tensor.parallel</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.tensor.parallel#torch.distributed.tensor.parallel.parallelize_module\"><code>parallelize_module()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.tensor.parallel#torch.distributed.tensor.parallel.style.RowwiseParallel\"><code>RowwiseParallel</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.tensor.parallel#torch.distributed.tensor.parallel.style.ColwiseParallel\"><code>ColwiseParallel</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.tensor.parallel#torch.distributed.tensor.parallel.style.PairwiseParallel\"><code>PairwiseParallel</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.tensor.parallel#torch.distributed.tensor.parallel.style.SequenceParallel\"><code>SequenceParallel</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_input_replicate_1d\"><code>make_input_replicate_1d()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_input_reshard_replicate\"><code>make_input_reshard_replicate()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_input_shard_1d\"><code>make_input_shard_1d()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_input_shard_1d_last_dim\"><code>make_input_shard_1d_last_dim()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_output_replicate_1d\"><code>make_output_replicate_1d()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_output_reshard_tensor\"><code>make_output_reshard_tensor()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_output_shard_1d\"><code>make_output_shard_1d()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_output_tensor\"><code>make_output_tensor()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.tensor.parallel#torch.distributed.tensor.parallel.fsdp.enable_2d_with_fsdp\"><code>enable_2d_with_fsdp()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.tensor.parallel#torch.distributed.tensor.parallel.ddp.pre_dp_module_transform\"><code>pre_dp_module_transform()</code></a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"distributed.checkpoint\">torch.distributed.checkpoint</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.checkpoint#torch.distributed.checkpoint.load_state_dict\"><code>load_state_dict()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.checkpoint#torch.distributed.checkpoint.save_state_dict\"><code>save_state_dict()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.checkpoint#torch.distributed.checkpoint.StorageReader\"><code>StorageReader</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.checkpoint#torch.distributed.checkpoint.StorageWriter\"><code>StorageWriter</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner\"><code>LoadPlanner</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.checkpoint#torch.distributed.checkpoint.LoadPlan\"><code>LoadPlan</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.checkpoint#torch.distributed.checkpoint.ReadItem\"><code>ReadItem</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.checkpoint#torch.distributed.checkpoint.SavePlanner\"><code>SavePlanner</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.checkpoint#torch.distributed.checkpoint.SavePlan\"><code>SavePlan</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.checkpoint#torch.distributed.checkpoint.WriteItem\"><code>WriteItem</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.checkpoint#torch.distributed.checkpoint.FileSystemReader\"><code>FileSystemReader</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.checkpoint#torch.distributed.checkpoint.FileSystemWriter\"><code>FileSystemWriter</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.checkpoint#torch.distributed.checkpoint.DefaultSavePlanner\"><code>DefaultSavePlanner</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributed.checkpoint#torch.distributed.checkpoint.DefaultLoadPlanner\"><code>DefaultLoadPlanner</code></a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"distributions\">torch.distributions</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#score-function\">Score function</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#pathwise-derivative\">Pathwise derivative</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#distribution\"><span class=\"hidden-section\">Distribution</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#exponentialfamily\"><span class=\"hidden-section\">ExponentialFamily</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#bernoulli\"><span class=\"hidden-section\">Bernoulli</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#beta\"><span class=\"hidden-section\">Beta</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#binomial\"><span class=\"hidden-section\">Binomial</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#categorical\"><span class=\"hidden-section\">Categorical</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#cauchy\"><span class=\"hidden-section\">Cauchy</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#chi2\"><span class=\"hidden-section\">Chi2</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#continuousbernoulli\"><span class=\"hidden-section\">ContinuousBernoulli</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#dirichlet\"><span class=\"hidden-section\">Dirichlet</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#exponential\"><span class=\"hidden-section\">Exponential</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#fishersnedecor\"><span class=\"hidden-section\">FisherSnedecor</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#gamma\"><span class=\"hidden-section\">Gamma</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#geometric\"><span class=\"hidden-section\">Geometric</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#gumbel\"><span class=\"hidden-section\">Gumbel</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#halfcauchy\"><span class=\"hidden-section\">HalfCauchy</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#halfnormal\"><span class=\"hidden-section\">HalfNormal</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#independent\"><span class=\"hidden-section\">Independent</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#kumaraswamy\"><span class=\"hidden-section\">Kumaraswamy</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#lkjcholesky\"><span class=\"hidden-section\">LKJCholesky</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#laplace\"><span class=\"hidden-section\">Laplace</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#lognormal\"><span class=\"hidden-section\">LogNormal</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#lowrankmultivariatenormal\"><span class=\"hidden-section\">LowRankMultivariateNormal</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#mixturesamefamily\"><span class=\"hidden-section\">MixtureSameFamily</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#multinomial\"><span class=\"hidden-section\">Multinomial</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#multivariatenormal\"><span class=\"hidden-section\">MultivariateNormal</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#negativebinomial\"><span class=\"hidden-section\">NegativeBinomial</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#normal\"><span class=\"hidden-section\">Normal</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#onehotcategorical\"><span class=\"hidden-section\">OneHotCategorical</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#pareto\"><span class=\"hidden-section\">Pareto</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#poisson\"><span class=\"hidden-section\">Poisson</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#relaxedbernoulli\"><span class=\"hidden-section\">RelaxedBernoulli</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#logitrelaxedbernoulli\"><span class=\"hidden-section\">LogitRelaxedBernoulli</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#relaxedonehotcategorical\"><span class=\"hidden-section\">RelaxedOneHotCategorical</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#studentt\"><span class=\"hidden-section\">StudentT</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#transformeddistribution\"><span class=\"hidden-section\">TransformedDistribution</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#uniform\"><span class=\"hidden-section\">Uniform</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#vonmises\"><span class=\"hidden-section\">VonMises</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#weibull\"><span class=\"hidden-section\">Weibull</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#wishart\"><span class=\"hidden-section\">Wishart</span></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#module-torch.distributions.kl\"><code>KL Divergence</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#module-torch.distributions.transforms\"><code>Transforms</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#module-torch.distributions.constraints\"><code>Constraints</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"distributions#module-torch.distributions.constraint_registry\"><code>Constraint Registry</code></a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"torch.compiler\">torch.compiler</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"torch.compiler#read-more\">Read More</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"fft\">torch.fft</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"fft#fast-fourier-transforms\">Fast Fourier Transforms</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"fft#helper-functions\">Helper Functions</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"func\">torch.func</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"func#what-are-composable-function-transforms\">What are composable function transforms?</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"func#why-composable-function-transforms\">Why composable function transforms?</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"func#read-more\">Read More</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"futures\">torch.futures</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"futures#torch.futures.Future\"><code>Future</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"futures#torch.futures.collect_all\"><code>collect_all()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"futures#torch.futures.wait_all\"><code>wait_all()</code></a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"fx\">torch.fx</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"fx#module-torch.fx\">Overview</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"fx#writing-transformations\">Writing Transformations</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"fx#debugging\">Debugging</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"fx#limitations-of-symbolic-tracing\">Limitations of Symbolic Tracing</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"fx#api-reference\">API Reference</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"hub\">torch.hub</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"hub#publishing-models\">Publishing models</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"hub#loading-models-from-hub\">Loading models from Hub</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"jit\">torch.jit</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"jit_language_reference_v2\">TorchScript Language Reference</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"jit#creating-torchscript-code\">Creating TorchScript Code</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"jit#mixing-tracing-and-scripting\">Mixing Tracing and Scripting</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"jit#torchscript-language\">TorchScript Language</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"jit#built-in-functions-and-modules\">Built-in Functions and Modules</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"jit#debugging\">Debugging</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"jit#frequently-asked-questions\">Frequently Asked Questions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"jit#known-issues\">Known Issues</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"jit#appendix\">Appendix</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"linalg\">torch.linalg</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"linalg#matrix-properties\">Matrix Properties</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"linalg#decompositions\">Decompositions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"linalg#solvers\">Solvers</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"linalg#inverses\">Inverses</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"linalg#matrix-functions\">Matrix Functions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"linalg#matrix-products\">Matrix Products</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"linalg#tensor-operations\">Tensor Operations</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"linalg#misc\">Misc</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"linalg#experimental-functions\">Experimental Functions</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"monitor\">torch.monitor</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"monitor#module-torch.monitor\">API Reference</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"signal\">torch.signal</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"signal#module-torch.signal.windows\">torch.signal.windows</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"special\">torch.special</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"special#functions\">Functions</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"torch.overrides\">torch.overrides</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"torch.overrides#functions\">Functions</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"package\">torch.package</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"package#tutorials\">Tutorials</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"package#how-do-i\">How do I</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"package#explanation\">Explanation</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"package#api-reference\">API Reference</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"profiler\">torch.profiler</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"profiler#module-torch.profiler\">Overview</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"profiler#api-reference\">API Reference</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"profiler#intel-instrumentation-and-tracing-technology-apis\">Intel Instrumentation and Tracing Technology APIs</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"nn.init\">torch.nn.init</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.init#torch.nn.init.calculate_gain\"><code>calculate_gain()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.init#torch.nn.init.uniform_\"><code>uniform_()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.init#torch.nn.init.normal_\"><code>normal_()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.init#torch.nn.init.constant_\"><code>constant_()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.init#torch.nn.init.ones_\"><code>ones_()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.init#torch.nn.init.zeros_\"><code>zeros_()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.init#torch.nn.init.eye_\"><code>eye_()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.init#torch.nn.init.dirac_\"><code>dirac_()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.init#torch.nn.init.xavier_uniform_\"><code>xavier_uniform_()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.init#torch.nn.init.xavier_normal_\"><code>xavier_normal_()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.init#torch.nn.init.kaiming_uniform_\"><code>kaiming_uniform_()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.init#torch.nn.init.kaiming_normal_\"><code>kaiming_normal_()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.init#torch.nn.init.trunc_normal_\"><code>trunc_normal_()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.init#torch.nn.init.orthogonal_\"><code>orthogonal_()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nn.init#torch.nn.init.sparse_\"><code>sparse_()</code></a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"onnx\">torch.onnx</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"onnx#overview\">Overview</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"onnx#torchdynamo-based-onnx-exporter\">TorchDynamo-based ONNX Exporter</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"onnx#torchscript-based-onnx-exporter\">TorchScript-based ONNX Exporter</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"onnx#contributing-developing\">Contributing / Developing</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"optim\">torch.optim</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"optim#how-to-use-an-optimizer\">How to use an optimizer</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"optim#base-class\">Base class</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"optim#algorithms\">Algorithms</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"optim#how-to-adjust-learning-rate\">How to adjust learning rate</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"optim#weight-averaging-swa-and-ema\">Weight Averaging (SWA and EMA)</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"complex_numbers\">Complex Numbers</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"complex_numbers#creating-complex-tensors\">Creating Complex Tensors</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"complex_numbers#transition-from-the-old-representation\">Transition from the old representation</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"complex_numbers#accessing-real-and-imag\">Accessing real and imag</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"complex_numbers#angle-and-abs\">Angle and abs</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"complex_numbers#linear-algebra\">Linear Algebra</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"complex_numbers#serialization\">Serialization</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"complex_numbers#autograd\">Autograd</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"ddp_comm_hooks\">DDP Communication Hooks</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"ddp_comm_hooks#how-to-use-a-communication-hook\">How to Use a Communication Hook?</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"ddp_comm_hooks#what-does-a-communication-hook-operate-on\">What Does a Communication Hook Operate On?</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"ddp_comm_hooks#default-communication-hooks\">Default Communication Hooks</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"ddp_comm_hooks#powersgd-communication-hook\">PowerSGD Communication Hook</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"ddp_comm_hooks#debugging-communication-hooks\">Debugging Communication Hooks</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"ddp_comm_hooks#checkpointing-of-communication-hooks\">Checkpointing of Communication Hooks</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"ddp_comm_hooks#acknowledgements\">Acknowledgements</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"pipeline\">Pipeline Parallelism</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"pipeline#model-parallelism-using-multiple-gpus\">Model Parallelism using multiple GPUs</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"pipeline#pipelined-execution\">Pipelined Execution</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"pipeline#pipe-apis-in-pytorch\">Pipe APIs in PyTorch</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"pipeline#tutorials\">Tutorials</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"pipeline#acknowledgements\">Acknowledgements</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"quantization\">Quantization</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"quantization#introduction-to-quantization\">Introduction to Quantization</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"quantization#quantization-api-summary\">Quantization API Summary</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"quantization#quantization-stack\">Quantization Stack</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"quantization#quantization-support-matrix\">Quantization Support Matrix</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"quantization#quantization-api-reference\">Quantization API Reference</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"quantization#quantization-backend-configuration\">Quantization Backend Configuration</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"quantization#quantization-accuracy-debugging\">Quantization Accuracy Debugging</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"quantization#quantization-customizations\">Quantization Customizations</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"quantization#best-practices\">Best Practices</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"quantization#frequently-asked-questions\">Frequently Asked Questions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"quantization#common-errors\">Common Errors</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"rpc\">Distributed RPC Framework</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"rpc#basics\">Basics</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"rpc#rpc\">RPC</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"rpc#rref\">RRef</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"rpc#remotemodule\">RemoteModule</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"rpc#distributed-autograd-framework\">Distributed Autograd Framework</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"rpc#distributed-optimizer\">Distributed Optimizer</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"rpc#design-notes\">Design Notes</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"rpc#tutorials\">Tutorials</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"random\">torch.random</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"random#torch.random.fork_rng\"><code>fork_rng()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"random#torch.random.get_rng_state\"><code>get_rng_state()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"random#torch.random.initial_seed\"><code>initial_seed()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"random#torch.random.manual_seed\"><code>manual_seed()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"random#torch.random.seed\"><code>seed()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"random#torch.random.set_rng_state\"><code>set_rng_state()</code></a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"masked\">torch.masked</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"masked#introduction\">Introduction</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"masked#supported-operators\">Supported Operators</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"nested\">torch.nested</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nested#introduction\">Introduction</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nested#construction\">Construction</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nested#size\">size</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nested#unbind\">unbind</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nested#nested-tensor-constructor-and-conversion-functions\">Nested tensor constructor and conversion functions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"nested#supported-operations\">Supported operations</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"sparse\">torch.sparse</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"sparse#why-and-when-to-use-sparsity\">Why and when to use sparsity</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"sparse#functionality-overview\">Functionality overview</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"sparse#operator-overview\">Operator overview</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"sparse#sparse-semi-structured-tensors\">Sparse Semi-Structured Tensors</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"sparse#sparse-coo-tensors\">Sparse COO tensors</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"sparse#sparse-compressed-tensors\">Sparse Compressed Tensors</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"sparse#supported-operations\">Supported operations</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"storage\">torch.Storage</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"storage#torch.TypedStorage\"><code>TypedStorage</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"storage#torch.UntypedStorage\"><code>UntypedStorage</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"storage#torch.DoubleStorage\"><code>DoubleStorage</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"storage#torch.FloatStorage\"><code>FloatStorage</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"storage#torch.HalfStorage\"><code>HalfStorage</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"storage#torch.LongStorage\"><code>LongStorage</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"storage#torch.IntStorage\"><code>IntStorage</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"storage#torch.ShortStorage\"><code>ShortStorage</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"storage#torch.CharStorage\"><code>CharStorage</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"storage#torch.ByteStorage\"><code>ByteStorage</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"storage#torch.BoolStorage\"><code>BoolStorage</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"storage#torch.BFloat16Storage\"><code>BFloat16Storage</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"storage#torch.ComplexDoubleStorage\"><code>ComplexDoubleStorage</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"storage#torch.ComplexFloatStorage\"><code>ComplexFloatStorage</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"storage#torch.QUInt8Storage\"><code>QUInt8Storage</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"storage#torch.QInt8Storage\"><code>QInt8Storage</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"storage#torch.QInt32Storage\"><code>QInt32Storage</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"storage#torch.QUInt4x2Storage\"><code>QUInt4x2Storage</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"storage#torch.QUInt2x4Storage\"><code>QUInt2x4Storage</code></a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"testing\">torch.testing</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"testing#torch.testing.assert_close\"><code>assert_close()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"testing#torch.testing.make_tensor\"><code>make_tensor()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"testing#torch.testing.assert_allclose\"><code>assert_allclose()</code></a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"utils\">torch.utils</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.utils.rename_privateuse1_backend\">torch.utils.rename_privateuse1_backend</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.utils.generate_methods_for_privateuse1_backend\">torch.utils.generate_methods_for_privateuse1_backend</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.utils.get_cpp_backtrace\">torch.utils.get_cpp_backtrace</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch.utils.set_module\">torch.utils.set_module</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"benchmark_utils\">torch.utils.benchmark</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"benchmark_utils#torch.utils.benchmark.Timer\"><code>Timer</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"benchmark_utils#torch.utils.benchmark.Measurement\"><code>Measurement</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"benchmark_utils#torch.utils.benchmark.CallgrindStats\"><code>CallgrindStats</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"benchmark_utils#torch.utils.benchmark.FunctionCounts\"><code>FunctionCounts</code></a></li> </ul> </li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"bottleneck\">torch.utils.bottleneck</a></li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"checkpoint\">torch.utils.checkpoint</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"checkpoint#torch.utils.checkpoint.checkpoint\"><code>checkpoint()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"checkpoint#torch.utils.checkpoint.checkpoint_sequential\"><code>checkpoint_sequential()</code></a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"cpp_extension\">torch.utils.cpp_extension</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"cpp_extension#torch.utils.cpp_extension.CppExtension\"><code>CppExtension()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"cpp_extension#torch.utils.cpp_extension.CUDAExtension\"><code>CUDAExtension()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"cpp_extension#torch.utils.cpp_extension.BuildExtension\"><code>BuildExtension()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"cpp_extension#torch.utils.cpp_extension.load\"><code>load()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"cpp_extension#torch.utils.cpp_extension.load_inline\"><code>load_inline()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"cpp_extension#torch.utils.cpp_extension.include_paths\"><code>include_paths()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"cpp_extension#torch.utils.cpp_extension.get_compiler_abi_compatibility_and_version\"><code>get_compiler_abi_compatibility_and_version()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"cpp_extension#torch.utils.cpp_extension.verify_ninja_availability\"><code>verify_ninja_availability()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"cpp_extension#torch.utils.cpp_extension.is_ninja_available\"><code>is_ninja_available()</code></a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"data\">torch.utils.data</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"data#dataset-types\">Dataset Types</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"data#data-loading-order-and-sampler\">Data Loading Order and <code>Sampler</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"data#loading-batched-and-non-batched-data\">Loading Batched and Non-Batched Data</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"data#single-and-multi-process-data-loading\">Single- and Multi-process Data Loading</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"data#memory-pinning\">Memory Pinning</a></li> </ul> </li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"jit_utils\">torch.utils.jit</a></li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"dlpack\">torch.utils.dlpack</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"dlpack#torch.utils.dlpack.from_dlpack\"><code>from_dlpack()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"dlpack#torch.utils.dlpack.to_dlpack\"><code>to_dlpack()</code></a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"mobile_optimizer\">torch.utils.mobile_optimizer</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"mobile_optimizer#torch.utils.mobile_optimizer.optimize_for_mobile\"><code>optimize_for_mobile()</code></a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"model_zoo\">torch.utils.model_zoo</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"model_zoo#torch.utils.model_zoo.load_url\"><code>load_url()</code></a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"tensorboard\">torch.utils.tensorboard</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"tensorboard#torch.utils.tensorboard.writer.SummaryWriter\"><code>SummaryWriter</code></a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"type_info\">Type Info</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"type_info#torch-finfo\">torch.finfo</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"type_info#torch-iinfo\">torch.iinfo</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"named_tensor\">Named Tensors</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"named_tensor#creating-named-tensors\">Creating named tensors</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"named_tensor#named-dimensions\">Named dimensions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"named_tensor#name-propagation-semantics\">Name propagation semantics</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"named_tensor#explicit-alignment-by-names\">Explicit alignment by names</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"named_tensor#manipulating-dimensions\">Manipulating dimensions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"named_tensor#autograd-support\">Autograd support</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"named_tensor#currently-supported-operations-and-subsystems\">Currently supported operations and subsystems</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"named_tensor#named-tensor-api-reference\">Named tensor API reference</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"name_inference\">Named Tensors operator coverage</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"name_inference#keeps-input-names\">Keeps input names</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"name_inference#removes-dimensions\">Removes dimensions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"name_inference#unifies-names-from-inputs\">Unifies names from inputs</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"name_inference#permutes-dimensions\">Permutes dimensions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"name_inference#contracts-away-dims\">Contracts away dims</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"name_inference#factory-functions\">Factory functions</a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"name_inference#out-function-and-in-place-variants\">out function and in-place variants</a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"config_mod\">torch.__config__</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"config_mod#torch.__config__.show\"><code>show()</code></a></li> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"config_mod#torch.__config__.parallel_info\"><code>parallel_info()</code></a></li> </ul> </li> <li class=\"toctree-l1\">\n<a class=\"reference internal\" href=\"logging\">torch._logging</a><ul> <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"generated/torch._logging.set_logs\">torch._logging.set_logs</a></li> </ul> </li> </ul>   <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Libraries</span></p> <ul> <li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://pytorch.org/audio/stable\">torchaudio</a></li> <li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://pytorch.org/data\">TorchData</a></li> <li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://pytorch.org/torchrec\">TorchRec</a></li> <li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://pytorch.org/serve\">TorchServe</a></li> <li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://pytorch.org/text/stable\">torchtext</a></li> <li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://pytorch.org/vision/stable\">torchvision</a></li> <li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://pytorch.org/xla/\">PyTorch on XLA Devices</a></li> </ul>    <h1 id=\"indices-and-tables\">Indices and tables</h1> <ul class=\"simple\"> <li><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/genindex.html\"><span class=\"std std-ref\">Index</span></a></li> <li><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/py-modindex.html\"><span class=\"std std-ref\">Module Index</span></a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href=\"https://github.com/pytorch/pytorch/blob/main/LICENSE\">LICENSE</a> file.<br>\n    <a href=\"https://pytorch.org/docs/2.1/\" class=\"_attribution-link\">https://pytorch.org/docs/2.1/</a>\n  </p>\n</div>\n","torch":"<h1 id=\"torch\">torch</h1> <p id=\"module-torch\">The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.</p> <p>It has a CUDA counterpart, that enables you to run your tensor computations on an NVIDIA GPU with compute capability &gt;= 3.0.</p>  <h2 id=\"tensors\">Tensors</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.is_tensor#torch.is_tensor\" title=\"torch.is_tensor\"><code>is_tensor</code></a>\n</td> <td><p>Returns True if <code>obj</code> is a PyTorch tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.is_storage#torch.is_storage\" title=\"torch.is_storage\"><code>is_storage</code></a>\n</td> <td><p>Returns True if <code>obj</code> is a PyTorch storage object.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.is_complex#torch.is_complex\" title=\"torch.is_complex\"><code>is_complex</code></a>\n</td> <td><p>Returns True if the data type of <code>input</code> is a complex data type i.e., one of <code>torch.complex64</code>, and <code>torch.complex128</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.is_conj#torch.is_conj\" title=\"torch.is_conj\"><code>is_conj</code></a>\n</td> <td><p>Returns True if the <code>input</code> is a conjugated tensor, i.e. its conjugate bit is set to <code>True</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.is_floating_point#torch.is_floating_point\" title=\"torch.is_floating_point\"><code>is_floating_point</code></a>\n</td> <td><p>Returns True if the data type of <code>input</code> is a floating point data type i.e., one of <code>torch.float64</code>, <code>torch.float32</code>, <code>torch.float16</code>, and <code>torch.bfloat16</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.is_nonzero#torch.is_nonzero\" title=\"torch.is_nonzero\"><code>is_nonzero</code></a>\n</td> <td><p>Returns True if the <code>input</code> is a single element tensor which is not equal to zero after type conversions.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.set_default_dtype#torch.set_default_dtype\" title=\"torch.set_default_dtype\"><code>set_default_dtype</code></a>\n</td> <td><p>Sets the default floating point dtype to <code>d</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.get_default_dtype#torch.get_default_dtype\" title=\"torch.get_default_dtype\"><code>get_default_dtype</code></a>\n</td> <td><p>Get the current default floating point <a class=\"reference internal\" href=\"tensor_attributes#torch.dtype\" title=\"torch.dtype\"><code>torch.dtype</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.set_default_device#torch.set_default_device\" title=\"torch.set_default_device\"><code>set_default_device</code></a>\n</td> <td><p>Sets the default <code>torch.Tensor</code> to be allocated on <code>device</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.set_default_tensor_type#torch.set_default_tensor_type\" title=\"torch.set_default_tensor_type\"><code>set_default_tensor_type</code></a>\n</td> <td><p>Sets the default <code>torch.Tensor</code> type to floating point tensor type <code>t</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.numel#torch.numel\" title=\"torch.numel\"><code>numel</code></a>\n</td> <td><p>Returns the total number of elements in the <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.set_printoptions#torch.set_printoptions\" title=\"torch.set_printoptions\"><code>set_printoptions</code></a>\n</td> <td><p>Set options for printing.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.set_flush_denormal#torch.set_flush_denormal\" title=\"torch.set_flush_denormal\"><code>set_flush_denormal</code></a>\n</td> <td><p>Disables denormal floating numbers on CPU.</p></td> </tr>  </table>  <h3 id=\"tensor-creation-ops\">Creation Ops</h3> <div class=\"admonition note\" id=\"creation-ops\"> <p class=\"admonition-title\">Note</p> <p>Random sampling creation ops are listed under <a class=\"reference internal\" href=\"#random-sampling\"><span class=\"std std-ref\">Random sampling</span></a> and include: <a class=\"reference internal\" href=\"generated/torch.rand#torch.rand\" title=\"torch.rand\"><code>torch.rand()</code></a> <a class=\"reference internal\" href=\"generated/torch.rand_like#torch.rand_like\" title=\"torch.rand_like\"><code>torch.rand_like()</code></a> <a class=\"reference internal\" href=\"generated/torch.randn#torch.randn\" title=\"torch.randn\"><code>torch.randn()</code></a> <a class=\"reference internal\" href=\"generated/torch.randn_like#torch.randn_like\" title=\"torch.randn_like\"><code>torch.randn_like()</code></a> <a class=\"reference internal\" href=\"generated/torch.randint#torch.randint\" title=\"torch.randint\"><code>torch.randint()</code></a> <a class=\"reference internal\" href=\"generated/torch.randint_like#torch.randint_like\" title=\"torch.randint_like\"><code>torch.randint_like()</code></a> <a class=\"reference internal\" href=\"generated/torch.randperm#torch.randperm\" title=\"torch.randperm\"><code>torch.randperm()</code></a> You may also use <a class=\"reference internal\" href=\"generated/torch.empty#torch.empty\" title=\"torch.empty\"><code>torch.empty()</code></a> with the <a class=\"reference internal\" href=\"#inplace-random-sampling\"><span class=\"std std-ref\">In-place random sampling</span></a> methods to create <a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a> s with values sampled from a broader range of distributions.</p> </div> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>tensor</code></a>\n</td> <td><p>Constructs a tensor with no autograd history (also known as a \"leaf tensor\", see <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/autograd.html\"><span class=\"doc\">Autograd mechanics</span></a>) by copying <code>data</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sparse_coo_tensor#torch.sparse_coo_tensor\" title=\"torch.sparse_coo_tensor\"><code>sparse_coo_tensor</code></a>\n</td> <td><p>Constructs a <a class=\"reference internal\" href=\"sparse#sparse-coo-docs\"><span class=\"std std-ref\">sparse tensor in COO(rdinate) format</span></a> with specified values at the given <code>indices</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sparse_csr_tensor#torch.sparse_csr_tensor\" title=\"torch.sparse_csr_tensor\"><code>sparse_csr_tensor</code></a>\n</td> <td><p>Constructs a <a class=\"reference internal\" href=\"sparse#sparse-csr-docs\"><span class=\"std std-ref\">sparse tensor in CSR (Compressed Sparse Row)</span></a> with specified values at the given <code>crow_indices</code> and <code>col_indices</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sparse_csc_tensor#torch.sparse_csc_tensor\" title=\"torch.sparse_csc_tensor\"><code>sparse_csc_tensor</code></a>\n</td> <td><p>Constructs a <a class=\"reference internal\" href=\"sparse#sparse-csc-docs\"><span class=\"std std-ref\">sparse tensor in CSC (Compressed Sparse Column)</span></a> with specified values at the given <code>ccol_indices</code> and <code>row_indices</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sparse_bsr_tensor#torch.sparse_bsr_tensor\" title=\"torch.sparse_bsr_tensor\"><code>sparse_bsr_tensor</code></a>\n</td> <td><p>Constructs a <a class=\"reference internal\" href=\"sparse#sparse-bsr-docs\"><span class=\"std std-ref\">sparse tensor in BSR (Block Compressed Sparse Row))</span></a> with specified 2-dimensional blocks at the given <code>crow_indices</code> and <code>col_indices</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sparse_bsc_tensor#torch.sparse_bsc_tensor\" title=\"torch.sparse_bsc_tensor\"><code>sparse_bsc_tensor</code></a>\n</td> <td><p>Constructs a <a class=\"reference internal\" href=\"sparse#sparse-bsc-docs\"><span class=\"std std-ref\">sparse tensor in BSC (Block Compressed Sparse Column))</span></a> with specified 2-dimensional blocks at the given <code>ccol_indices</code> and <code>row_indices</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.asarray#torch.asarray\" title=\"torch.asarray\"><code>asarray</code></a>\n</td> <td><p>Converts <code>obj</code> to a tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.as_tensor#torch.as_tensor\" title=\"torch.as_tensor\"><code>as_tensor</code></a>\n</td> <td><p>Converts <code>data</code> into a tensor, sharing data and preserving autograd history if possible.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.as_strided#torch.as_strided\" title=\"torch.as_strided\"><code>as_strided</code></a>\n</td> <td><p>Create a view of an existing <code>torch.Tensor</code> <code>input</code> with specified <code>size</code>, <code>stride</code> and <code>storage_offset</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.from_numpy#torch.from_numpy\" title=\"torch.from_numpy\"><code>from_numpy</code></a>\n</td> <td><p>Creates a <a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\"><code>Tensor</code></a> from a <a class=\"reference external\" href=\"https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray\" title=\"(in NumPy v1.26)\"><code>numpy.ndarray</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.from_dlpack#torch.from_dlpack\" title=\"torch.from_dlpack\"><code>from_dlpack</code></a>\n</td> <td><p>Converts a tensor from an external library into a <code>torch.Tensor</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.frombuffer#torch.frombuffer\" title=\"torch.frombuffer\"><code>frombuffer</code></a>\n</td> <td><p>Creates a 1-dimensional <a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\"><code>Tensor</code></a> from an object that implements the Python buffer protocol.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.zeros#torch.zeros\" title=\"torch.zeros\"><code>zeros</code></a>\n</td> <td><p>Returns a tensor filled with the scalar value <code>0</code>, with the shape defined by the variable argument <code>size</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.zeros_like#torch.zeros_like\" title=\"torch.zeros_like\"><code>zeros_like</code></a>\n</td> <td><p>Returns a tensor filled with the scalar value <code>0</code>, with the same size as <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.ones#torch.ones\" title=\"torch.ones\"><code>ones</code></a>\n</td> <td><p>Returns a tensor filled with the scalar value <code>1</code>, with the shape defined by the variable argument <code>size</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.ones_like#torch.ones_like\" title=\"torch.ones_like\"><code>ones_like</code></a>\n</td> <td><p>Returns a tensor filled with the scalar value <code>1</code>, with the same size as <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.arange#torch.arange\" title=\"torch.arange\"><code>arange</code></a>\n</td> <td><p>Returns a 1-D tensor of size <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo fence=\"true\"></mo><mfrac><mrow><mtext>end</mtext><mo></mo><mtext>start</mtext></mrow><mtext>step</mtext></mfrac><mo fence=\"true\"></mo></mrow><annotation encoding=\"application/x-tex\">\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil</annotation></semantics></math></span></span></span> with values from the interval <code>[start, end)</code> taken with common difference <code>step</code> beginning from <code>start</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.range#torch.range\" title=\"torch.range\"><code>range</code></a>\n</td> <td><p>Returns a 1-D tensor of size <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mrow><mo fence=\"true\"></mo><mfrac><mrow><mtext>end</mtext><mo></mo><mtext>start</mtext></mrow><mtext>step</mtext></mfrac><mo fence=\"true\"></mo></mrow><mo>+</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1</annotation></semantics></math></span></span></span> with values from <code>start</code> to <code>end</code> with step <code>step</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.linspace#torch.linspace\" title=\"torch.linspace\"><code>linspace</code></a>\n</td> <td><p>Creates a one-dimensional tensor of size <code>steps</code> whose values are evenly spaced from <code>start</code> to <code>end</code>, inclusive.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.logspace#torch.logspace\" title=\"torch.logspace\"><code>logspace</code></a>\n</td> <td><p>Creates a one-dimensional tensor of size <code>steps</code> whose values are evenly spaced from <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mtext>base</mtext><mtext>start</mtext></msup></mrow><annotation encoding=\"application/x-tex\">{{\\text{{base}}}}^{{\\text{{start}}}}</annotation></semantics></math></span></span></span> to <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mtext>base</mtext><mtext>end</mtext></msup></mrow><annotation encoding=\"application/x-tex\">{{\\text{{base}}}}^{{\\text{{end}}}}</annotation></semantics></math></span></span></span>, inclusive, on a logarithmic scale with base <code>base</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.eye#torch.eye\" title=\"torch.eye\"><code>eye</code></a>\n</td> <td><p>Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.empty#torch.empty\" title=\"torch.empty\"><code>empty</code></a>\n</td> <td><p>Returns a tensor filled with uninitialized data.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.empty_like#torch.empty_like\" title=\"torch.empty_like\"><code>empty_like</code></a>\n</td> <td><p>Returns an uninitialized tensor with the same size as <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.empty_strided#torch.empty_strided\" title=\"torch.empty_strided\"><code>empty_strided</code></a>\n</td> <td><p>Creates a tensor with the specified <code>size</code> and <code>stride</code> and filled with undefined data.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.full#torch.full\" title=\"torch.full\"><code>full</code></a>\n</td> <td><p>Creates a tensor of size <code>size</code> filled with <code>fill_value</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.full_like#torch.full_like\" title=\"torch.full_like\"><code>full_like</code></a>\n</td> <td><p>Returns a tensor with the same size as <code>input</code> filled with <code>fill_value</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.quantize_per_tensor#torch.quantize_per_tensor\" title=\"torch.quantize_per_tensor\"><code>quantize_per_tensor</code></a>\n</td> <td><p>Converts a float tensor to a quantized tensor with given scale and zero point.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.quantize_per_channel#torch.quantize_per_channel\" title=\"torch.quantize_per_channel\"><code>quantize_per_channel</code></a>\n</td> <td><p>Converts a float tensor to a per-channel quantized tensor with given scales and zero points.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.dequantize#torch.dequantize\" title=\"torch.dequantize\"><code>dequantize</code></a>\n</td> <td><p>Returns an fp32 Tensor by dequantizing a quantized Tensor</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.complex#torch.complex\" title=\"torch.complex\"><code>complex</code></a>\n</td> <td><p>Constructs a complex tensor with its real part equal to <a class=\"reference internal\" href=\"generated/torch.real#torch.real\" title=\"torch.real\"><code>real</code></a> and its imaginary part equal to <a class=\"reference internal\" href=\"generated/torch.imag#torch.imag\" title=\"torch.imag\"><code>imag</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.polar#torch.polar\" title=\"torch.polar\"><code>polar</code></a>\n</td> <td><p>Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value <a class=\"reference internal\" href=\"generated/torch.abs#torch.abs\" title=\"torch.abs\"><code>abs</code></a> and angle <a class=\"reference internal\" href=\"generated/torch.angle#torch.angle\" title=\"torch.angle\"><code>angle</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.heaviside#torch.heaviside\" title=\"torch.heaviside\"><code>heaviside</code></a>\n</td> <td><p>Computes the Heaviside step function for each element in <code>input</code>.</p></td> </tr>  </table>   <h3 id=\"indexing-slicing-joining\">Indexing, Slicing, Joining, Mutating Ops</h3> <table class=\"autosummary longtable docutils colwidths-auto align-default\" id=\"indexing-slicing-joining-mutating-ops\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.adjoint#torch.adjoint\" title=\"torch.adjoint\"><code>adjoint</code></a>\n</td> <td><p>Returns a view of the tensor conjugated and with the last two dimensions transposed.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.argwhere#torch.argwhere\" title=\"torch.argwhere\"><code>argwhere</code></a>\n</td> <td><p>Returns a tensor containing the indices of all non-zero elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cat#torch.cat\" title=\"torch.cat\"><code>cat</code></a>\n</td> <td><p>Concatenates the given sequence of <code>seq</code> tensors in the given dimension.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.concat#torch.concat\" title=\"torch.concat\"><code>concat</code></a>\n</td> <td><p>Alias of <a class=\"reference internal\" href=\"generated/torch.cat#torch.cat\" title=\"torch.cat\"><code>torch.cat()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.concatenate#torch.concatenate\" title=\"torch.concatenate\"><code>concatenate</code></a>\n</td> <td><p>Alias of <a class=\"reference internal\" href=\"generated/torch.cat#torch.cat\" title=\"torch.cat\"><code>torch.cat()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.conj#torch.conj\" title=\"torch.conj\"><code>conj</code></a>\n</td> <td><p>Returns a view of <code>input</code> with a flipped conjugate bit.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.chunk#torch.chunk\" title=\"torch.chunk\"><code>chunk</code></a>\n</td> <td><p>Attempts to split a tensor into the specified number of chunks.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.dsplit#torch.dsplit\" title=\"torch.dsplit\"><code>dsplit</code></a>\n</td> <td><p>Splits <code>input</code>, a tensor with three or more dimensions, into multiple tensors depthwise according to <code>indices_or_sections</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.column_stack#torch.column_stack\" title=\"torch.column_stack\"><code>column_stack</code></a>\n</td> <td><p>Creates a new tensor by horizontally stacking the tensors in <code>tensors</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.dstack#torch.dstack\" title=\"torch.dstack\"><code>dstack</code></a>\n</td> <td><p>Stack tensors in sequence depthwise (along third axis).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.gather#torch.gather\" title=\"torch.gather\"><code>gather</code></a>\n</td> <td><p>Gathers values along an axis specified by <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.hsplit#torch.hsplit\" title=\"torch.hsplit\"><code>hsplit</code></a>\n</td> <td><p>Splits <code>input</code>, a tensor with one or more dimensions, into multiple tensors horizontally according to <code>indices_or_sections</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.hstack#torch.hstack\" title=\"torch.hstack\"><code>hstack</code></a>\n</td> <td><p>Stack tensors in sequence horizontally (column wise).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.index_add#torch.index_add\" title=\"torch.index_add\"><code>index_add</code></a>\n</td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.tensor.index_add_#torch.Tensor.index_add_\" title=\"torch.Tensor.index_add_\"><code>index_add_()</code></a> for function description.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.index_copy#torch.index_copy\" title=\"torch.index_copy\"><code>index_copy</code></a>\n</td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.tensor.index_add_#torch.Tensor.index_add_\" title=\"torch.Tensor.index_add_\"><code>index_add_()</code></a> for function description.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.index_reduce#torch.index_reduce\" title=\"torch.index_reduce\"><code>index_reduce</code></a>\n</td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.tensor.index_reduce_#torch.Tensor.index_reduce_\" title=\"torch.Tensor.index_reduce_\"><code>index_reduce_()</code></a> for function description.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.index_select#torch.index_select\" title=\"torch.index_select\"><code>index_select</code></a>\n</td> <td><p>Returns a new tensor which indexes the <code>input</code> tensor along dimension <code>dim</code> using the entries in <code>index</code> which is a <code>LongTensor</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.masked_select#torch.masked_select\" title=\"torch.masked_select\"><code>masked_select</code></a>\n</td> <td><p>Returns a new 1-D tensor which indexes the <code>input</code> tensor according to the boolean mask <code>mask</code> which is a <code>BoolTensor</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.movedim#torch.movedim\" title=\"torch.movedim\"><code>movedim</code></a>\n</td> <td><p>Moves the dimension(s) of <code>input</code> at the position(s) in <code>source</code> to the position(s) in <code>destination</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.moveaxis#torch.moveaxis\" title=\"torch.moveaxis\"><code>moveaxis</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.movedim#torch.movedim\" title=\"torch.movedim\"><code>torch.movedim()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.narrow#torch.narrow\" title=\"torch.narrow\"><code>narrow</code></a>\n</td> <td><p>Returns a new tensor that is a narrowed version of <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.narrow_copy#torch.narrow_copy\" title=\"torch.narrow_copy\"><code>narrow_copy</code></a>\n</td> <td><p>Same as <a class=\"reference internal\" href=\"generated/torch.tensor.narrow#torch.Tensor.narrow\" title=\"torch.Tensor.narrow\"><code>Tensor.narrow()</code></a> except this returns a copy rather than shared storage.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nonzero#torch.nonzero\" title=\"torch.nonzero\"><code>nonzero</code></a>\n</td> <td></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.permute#torch.permute\" title=\"torch.permute\"><code>permute</code></a>\n</td> <td><p>Returns a view of the original tensor <code>input</code> with its dimensions permuted.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.reshape#torch.reshape\" title=\"torch.reshape\"><code>reshape</code></a>\n</td> <td><p>Returns a tensor with the same data and number of elements as <code>input</code>, but with the specified shape.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.row_stack#torch.row_stack\" title=\"torch.row_stack\"><code>row_stack</code></a>\n</td> <td><p>Alias of <a class=\"reference internal\" href=\"generated/torch.vstack#torch.vstack\" title=\"torch.vstack\"><code>torch.vstack()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.select#torch.select\" title=\"torch.select\"><code>select</code></a>\n</td> <td><p>Slices the <code>input</code> tensor along the selected dimension at the given index.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.scatter#torch.scatter\" title=\"torch.scatter\"><code>scatter</code></a>\n</td> <td><p>Out-of-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.scatter_#torch.Tensor.scatter_\" title=\"torch.Tensor.scatter_\"><code>torch.Tensor.scatter_()</code></a></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.diagonal_scatter#torch.diagonal_scatter\" title=\"torch.diagonal_scatter\"><code>diagonal_scatter</code></a>\n</td> <td><p>Embeds the values of the <code>src</code> tensor into <code>input</code> along the diagonal elements of <code>input</code>, with respect to <code>dim1</code> and <code>dim2</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.select_scatter#torch.select_scatter\" title=\"torch.select_scatter\"><code>select_scatter</code></a>\n</td> <td><p>Embeds the values of the <code>src</code> tensor into <code>input</code> at the given index.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.slice_scatter#torch.slice_scatter\" title=\"torch.slice_scatter\"><code>slice_scatter</code></a>\n</td> <td><p>Embeds the values of the <code>src</code> tensor into <code>input</code> at the given dimension.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.scatter_add#torch.scatter_add\" title=\"torch.scatter_add\"><code>scatter_add</code></a>\n</td> <td><p>Out-of-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.scatter_add_#torch.Tensor.scatter_add_\" title=\"torch.Tensor.scatter_add_\"><code>torch.Tensor.scatter_add_()</code></a></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.scatter_reduce#torch.scatter_reduce\" title=\"torch.scatter_reduce\"><code>scatter_reduce</code></a>\n</td> <td><p>Out-of-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.scatter_reduce_#torch.Tensor.scatter_reduce_\" title=\"torch.Tensor.scatter_reduce_\"><code>torch.Tensor.scatter_reduce_()</code></a></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.split#torch.split\" title=\"torch.split\"><code>split</code></a>\n</td> <td><p>Splits the tensor into chunks.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.squeeze#torch.squeeze\" title=\"torch.squeeze\"><code>squeeze</code></a>\n</td> <td><p>Returns a tensor with all specified dimensions of <code>input</code> of size <code>1</code> removed.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.stack#torch.stack\" title=\"torch.stack\"><code>stack</code></a>\n</td> <td><p>Concatenates a sequence of tensors along a new dimension.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.swapaxes#torch.swapaxes\" title=\"torch.swapaxes\"><code>swapaxes</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.transpose#torch.transpose\" title=\"torch.transpose\"><code>torch.transpose()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.swapdims#torch.swapdims\" title=\"torch.swapdims\"><code>swapdims</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.transpose#torch.transpose\" title=\"torch.transpose\"><code>torch.transpose()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.t#torch.t\" title=\"torch.t\"><code>t</code></a>\n</td> <td><p>Expects <code>input</code> to be &lt;= 2-D tensor and transposes dimensions 0 and 1.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.take#torch.take\" title=\"torch.take\"><code>take</code></a>\n</td> <td><p>Returns a new tensor with the elements of <code>input</code> at the given indices.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.take_along_dim#torch.take_along_dim\" title=\"torch.take_along_dim\"><code>take_along_dim</code></a>\n</td> <td><p>Selects values from <code>input</code> at the 1-dimensional indices from <code>indices</code> along the given <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.tensor_split#torch.tensor_split\" title=\"torch.tensor_split\"><code>tensor_split</code></a>\n</td> <td><p>Splits a tensor into multiple sub-tensors, all of which are views of <code>input</code>, along dimension <code>dim</code> according to the indices or number of sections specified by <code>indices_or_sections</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.tile#torch.tile\" title=\"torch.tile\"><code>tile</code></a>\n</td> <td><p>Constructs a tensor by repeating the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.transpose#torch.transpose\" title=\"torch.transpose\"><code>transpose</code></a>\n</td> <td><p>Returns a tensor that is a transposed version of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.unbind#torch.unbind\" title=\"torch.unbind\"><code>unbind</code></a>\n</td> <td><p>Removes a tensor dimension.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.unsqueeze#torch.unsqueeze\" title=\"torch.unsqueeze\"><code>unsqueeze</code></a>\n</td> <td><p>Returns a new tensor with a dimension of size one inserted at the specified position.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.vsplit#torch.vsplit\" title=\"torch.vsplit\"><code>vsplit</code></a>\n</td> <td><p>Splits <code>input</code>, a tensor with two or more dimensions, into multiple tensors vertically according to <code>indices_or_sections</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.vstack#torch.vstack\" title=\"torch.vstack\"><code>vstack</code></a>\n</td> <td><p>Stack tensors in sequence vertically (row wise).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.where#torch.where\" title=\"torch.where\"><code>where</code></a>\n</td> <td><p>Return a tensor of elements selected from either <code>input</code> or <code>other</code>, depending on <code>condition</code>.</p></td> </tr>  </table>    <h2 id=\"id1\">Generators</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\" id=\"generators\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.generator#torch.Generator\" title=\"torch.Generator\"><code>Generator</code></a>\n</td> <td><p>Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers.</p></td> </tr>  </table>   <h2 id=\"id2\">Random sampling</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\" id=\"random-sampling\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.seed#torch.seed\" title=\"torch.seed\"><code>seed</code></a>\n</td> <td><p>Sets the seed for generating random numbers to a non-deterministic random number.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.manual_seed#torch.manual_seed\" title=\"torch.manual_seed\"><code>manual_seed</code></a>\n</td> <td><p>Sets the seed for generating random numbers.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.initial_seed#torch.initial_seed\" title=\"torch.initial_seed\"><code>initial_seed</code></a>\n</td> <td><p>Returns the initial seed for generating random numbers as a Python <code>long</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.get_rng_state#torch.get_rng_state\" title=\"torch.get_rng_state\"><code>get_rng_state</code></a>\n</td> <td><p>Returns the random number generator state as a <code>torch.ByteTensor</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.set_rng_state#torch.set_rng_state\" title=\"torch.set_rng_state\"><code>set_rng_state</code></a>\n</td> <td><p>Sets the random number generator state.</p></td> </tr>  </table> <dl class=\"py attribute\"> <dt class=\"sig sig-object py\" id=\"torch.torch.default_generator\">\n<code>torch.default_generator Returns the default CPU torch.Generator</code> </dt> \n</dl> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.bernoulli#torch.bernoulli\" title=\"torch.bernoulli\"><code>bernoulli</code></a>\n</td> <td><p>Draws binary random numbers (0 or 1) from a Bernoulli distribution.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.multinomial#torch.multinomial\" title=\"torch.multinomial\"><code>multinomial</code></a>\n</td> <td><p>Returns a tensor where each row contains <code>num_samples</code> indices sampled from the multinomial probability distribution located in the corresponding row of tensor <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.normal#torch.normal\" title=\"torch.normal\"><code>normal</code></a>\n</td> <td><p>Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.poisson#torch.poisson\" title=\"torch.poisson\"><code>poisson</code></a>\n</td> <td><p>Returns a tensor of the same size as <code>input</code> with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in <code>input</code> i.e.,</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.rand#torch.rand\" title=\"torch.rand\"><code>rand</code></a>\n</td> <td><p>Returns a tensor filled with random numbers from a uniform distribution on the interval <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo separator=\"true\">,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">[0, 1)</annotation></semantics></math></span></span></span></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.rand_like#torch.rand_like\" title=\"torch.rand_like\"><code>rand_like</code></a>\n</td> <td><p>Returns a tensor with the same size as <code>input</code> that is filled with random numbers from a uniform distribution on the interval <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo separator=\"true\">,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">[0, 1)</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.randint#torch.randint\" title=\"torch.randint\"><code>randint</code></a>\n</td> <td><p>Returns a tensor filled with random integers generated uniformly between <code>low</code> (inclusive) and <code>high</code> (exclusive).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.randint_like#torch.randint_like\" title=\"torch.randint_like\"><code>randint_like</code></a>\n</td> <td><p>Returns a tensor with the same shape as Tensor <code>input</code> filled with random integers generated uniformly between <code>low</code> (inclusive) and <code>high</code> (exclusive).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.randn#torch.randn\" title=\"torch.randn\"><code>randn</code></a>\n</td> <td><p>Returns a tensor filled with random numbers from a normal distribution with mean <code>0</code> and variance <code>1</code> (also called the standard normal distribution).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.randn_like#torch.randn_like\" title=\"torch.randn_like\"><code>randn_like</code></a>\n</td> <td><p>Returns a tensor with the same size as <code>input</code> that is filled with random numbers from a normal distribution with mean 0 and variance 1.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.randperm#torch.randperm\" title=\"torch.randperm\"><code>randperm</code></a>\n</td> <td><p>Returns a random permutation of integers from <code>0</code> to <code>n - 1</code>.</p></td> </tr>  </table>  <h3 id=\"inplace-random-sampling\">In-place random sampling</h3> <p id=\"in-place-random-sampling\">There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:</p> <ul class=\"simple\"> <li>\n<a class=\"reference internal\" href=\"generated/torch.tensor.bernoulli_#torch.Tensor.bernoulli_\" title=\"torch.Tensor.bernoulli_\"><code>torch.Tensor.bernoulli_()</code></a> - in-place version of <a class=\"reference internal\" href=\"generated/torch.bernoulli#torch.bernoulli\" title=\"torch.bernoulli\"><code>torch.bernoulli()</code></a>\n</li> <li>\n<a class=\"reference internal\" href=\"generated/torch.tensor.cauchy_#torch.Tensor.cauchy_\" title=\"torch.Tensor.cauchy_\"><code>torch.Tensor.cauchy_()</code></a> - numbers drawn from the Cauchy distribution</li> <li>\n<a class=\"reference internal\" href=\"generated/torch.tensor.exponential_#torch.Tensor.exponential_\" title=\"torch.Tensor.exponential_\"><code>torch.Tensor.exponential_()</code></a> - numbers drawn from the exponential distribution</li> <li>\n<a class=\"reference internal\" href=\"generated/torch.tensor.geometric_#torch.Tensor.geometric_\" title=\"torch.Tensor.geometric_\"><code>torch.Tensor.geometric_()</code></a> - elements drawn from the geometric distribution</li> <li>\n<a class=\"reference internal\" href=\"generated/torch.tensor.log_normal_#torch.Tensor.log_normal_\" title=\"torch.Tensor.log_normal_\"><code>torch.Tensor.log_normal_()</code></a> - samples from the log-normal distribution</li> <li>\n<a class=\"reference internal\" href=\"generated/torch.tensor.normal_#torch.Tensor.normal_\" title=\"torch.Tensor.normal_\"><code>torch.Tensor.normal_()</code></a> - in-place version of <a class=\"reference internal\" href=\"generated/torch.normal#torch.normal\" title=\"torch.normal\"><code>torch.normal()</code></a>\n</li> <li>\n<a class=\"reference internal\" href=\"generated/torch.tensor.random_#torch.Tensor.random_\" title=\"torch.Tensor.random_\"><code>torch.Tensor.random_()</code></a> - numbers sampled from the discrete uniform distribution</li> <li>\n<a class=\"reference internal\" href=\"generated/torch.tensor.uniform_#torch.Tensor.uniform_\" title=\"torch.Tensor.uniform_\"><code>torch.Tensor.uniform_()</code></a> - numbers sampled from the continuous uniform distribution</li> </ul>   <h3 id=\"quasi-random-sampling\">Quasi-random sampling</h3> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine\" title=\"torch.quasirandom.SobolEngine\"><code>quasirandom.SobolEngine</code></a></p></td> <td><p>The <a class=\"reference internal\" href=\"generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine\" title=\"torch.quasirandom.SobolEngine\"><code>torch.quasirandom.SobolEngine</code></a> is an engine for generating (scrambled) Sobol sequences.</p></td> </tr>  </table>    <h2 id=\"serialization\">Serialization</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.save#torch.save\" title=\"torch.save\"><code>save</code></a>\n</td> <td><p>Saves an object to a disk file.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.load#torch.load\" title=\"torch.load\"><code>load</code></a>\n</td> <td><p>Loads an object saved with <a class=\"reference internal\" href=\"generated/torch.save#torch.save\" title=\"torch.save\"><code>torch.save()</code></a> from a file.</p></td> </tr>  </table>   <h2 id=\"parallelism\">Parallelism</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.get_num_threads#torch.get_num_threads\" title=\"torch.get_num_threads\"><code>get_num_threads</code></a>\n</td> <td><p>Returns the number of threads used for parallelizing CPU operations</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.set_num_threads#torch.set_num_threads\" title=\"torch.set_num_threads\"><code>set_num_threads</code></a>\n</td> <td><p>Sets the number of threads used for intraop parallelism on CPU.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.get_num_interop_threads#torch.get_num_interop_threads\" title=\"torch.get_num_interop_threads\"><code>get_num_interop_threads</code></a>\n</td> <td><p>Returns the number of threads used for inter-op parallelism on CPU (e.g.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.set_num_interop_threads#torch.set_num_interop_threads\" title=\"torch.set_num_interop_threads\"><code>set_num_interop_threads</code></a>\n</td> <td><p>Sets the number of threads used for interop parallelism (e.g.</p></td> </tr>  </table>   <h2 id=\"torch-rst-local-disable-grad\">Locally disabling gradient computation</h2> <p id=\"locally-disabling-gradient-computation\">The context managers <a class=\"reference internal\" href=\"generated/torch.no_grad#torch.no_grad\" title=\"torch.no_grad\"><code>torch.no_grad()</code></a>, <a class=\"reference internal\" href=\"generated/torch.enable_grad#torch.enable_grad\" title=\"torch.enable_grad\"><code>torch.enable_grad()</code></a>, and <a class=\"reference internal\" href=\"generated/torch.set_grad_enabled#torch.set_grad_enabled\" title=\"torch.set_grad_enabled\"><code>torch.set_grad_enabled()</code></a> are helpful for locally disabling and enabling gradient computation. See <a class=\"reference internal\" href=\"autograd#locally-disable-grad\"><span class=\"std std-ref\">Locally disabling gradient computation</span></a> for more details on their usage. These context managers are thread local, so they wont work if you send work to another thread using the <code>threading</code> module, etc.</p> <p>Examples:</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.zeros(1, requires_grad=True)\n&gt;&gt;&gt; with torch.no_grad():\n...     y = x * 2\n&gt;&gt;&gt; y.requires_grad\nFalse\n\n&gt;&gt;&gt; is_train = False\n&gt;&gt;&gt; with torch.set_grad_enabled(is_train):\n...     y = x * 2\n&gt;&gt;&gt; y.requires_grad\nFalse\n\n&gt;&gt;&gt; torch.set_grad_enabled(True)  # this can also be used as a function\n&gt;&gt;&gt; y = x * 2\n&gt;&gt;&gt; y.requires_grad\nTrue\n\n&gt;&gt;&gt; torch.set_grad_enabled(False)\n&gt;&gt;&gt; y = x * 2\n&gt;&gt;&gt; y.requires_grad\nFalse\n</pre> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.no_grad#torch.no_grad\" title=\"torch.no_grad\"><code>no_grad</code></a>\n</td> <td><p>Context-manager that disables gradient calculation.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.enable_grad#torch.enable_grad\" title=\"torch.enable_grad\"><code>enable_grad</code></a>\n</td> <td><p>Context-manager that enables gradient calculation.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.set_grad_enabled#torch.set_grad_enabled\" title=\"torch.set_grad_enabled\"><code>set_grad_enabled</code></a>\n</td> <td><p>Context-manager that sets gradient calculation on or off.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.is_grad_enabled#torch.is_grad_enabled\" title=\"torch.is_grad_enabled\"><code>is_grad_enabled</code></a>\n</td> <td><p>Returns True if grad mode is currently enabled.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.inference_mode#torch.inference_mode\" title=\"torch.inference_mode\"><code>inference_mode</code></a>\n</td> <td><p>Context-manager that enables or disables inference mode</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.is_inference_mode_enabled#torch.is_inference_mode_enabled\" title=\"torch.is_inference_mode_enabled\"><code>is_inference_mode_enabled</code></a>\n</td> <td><p>Returns True if inference mode is currently enabled.</p></td> </tr>  </table>   <h2 id=\"math-operations\">Math operations</h2>  <h3 id=\"pointwise-ops\">Pointwise Ops</h3> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.abs#torch.abs\" title=\"torch.abs\"><code>abs</code></a>\n</td> <td><p>Computes the absolute value of each element in <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.absolute#torch.absolute\" title=\"torch.absolute\"><code>absolute</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.abs#torch.abs\" title=\"torch.abs\"><code>torch.abs()</code></a></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.acos#torch.acos\" title=\"torch.acos\"><code>acos</code></a>\n</td> <td><p>Computes the inverse cosine of each element in <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.arccos#torch.arccos\" title=\"torch.arccos\"><code>arccos</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.acos#torch.acos\" title=\"torch.acos\"><code>torch.acos()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.acosh#torch.acosh\" title=\"torch.acosh\"><code>acosh</code></a>\n</td> <td><p>Returns a new tensor with the inverse hyperbolic cosine of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.arccosh#torch.arccosh\" title=\"torch.arccosh\"><code>arccosh</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.acosh#torch.acosh\" title=\"torch.acosh\"><code>torch.acosh()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.add#torch.add\" title=\"torch.add\"><code>add</code></a>\n</td> <td><p>Adds <code>other</code>, scaled by <code>alpha</code>, to <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.addcdiv#torch.addcdiv\" title=\"torch.addcdiv\"><code>addcdiv</code></a>\n</td> <td><p>Performs the element-wise division of <code>tensor1</code> by <code>tensor2</code>, multiplies the result by the scalar <code>value</code> and adds it to <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.addcmul#torch.addcmul\" title=\"torch.addcmul\"><code>addcmul</code></a>\n</td> <td><p>Performs the element-wise multiplication of <code>tensor1</code> by <code>tensor2</code>, multiplies the result by the scalar <code>value</code> and adds it to <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.angle#torch.angle\" title=\"torch.angle\"><code>angle</code></a>\n</td> <td><p>Computes the element-wise angle (in radians) of the given <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.asin#torch.asin\" title=\"torch.asin\"><code>asin</code></a>\n</td> <td><p>Returns a new tensor with the arcsine of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.arcsin#torch.arcsin\" title=\"torch.arcsin\"><code>arcsin</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.asin#torch.asin\" title=\"torch.asin\"><code>torch.asin()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.asinh#torch.asinh\" title=\"torch.asinh\"><code>asinh</code></a>\n</td> <td><p>Returns a new tensor with the inverse hyperbolic sine of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.arcsinh#torch.arcsinh\" title=\"torch.arcsinh\"><code>arcsinh</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.asinh#torch.asinh\" title=\"torch.asinh\"><code>torch.asinh()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.atan#torch.atan\" title=\"torch.atan\"><code>atan</code></a>\n</td> <td><p>Returns a new tensor with the arctangent of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.arctan#torch.arctan\" title=\"torch.arctan\"><code>arctan</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.atan#torch.atan\" title=\"torch.atan\"><code>torch.atan()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.atanh#torch.atanh\" title=\"torch.atanh\"><code>atanh</code></a>\n</td> <td><p>Returns a new tensor with the inverse hyperbolic tangent of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.arctanh#torch.arctanh\" title=\"torch.arctanh\"><code>arctanh</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.atanh#torch.atanh\" title=\"torch.atanh\"><code>torch.atanh()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.atan2#torch.atan2\" title=\"torch.atan2\"><code>atan2</code></a>\n</td> <td><p>Element-wise arctangent of <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mtext>input</mtext><mi>i</mi></msub><mi mathvariant=\"normal\">/</mi><msub><mtext>other</mtext><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\text{input}_{i} / \\text{other}_{i}</annotation></semantics></math></span></span></span> with consideration of the quadrant.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.arctan2#torch.arctan2\" title=\"torch.arctan2\"><code>arctan2</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.atan2#torch.atan2\" title=\"torch.atan2\"><code>torch.atan2()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.bitwise_not#torch.bitwise_not\" title=\"torch.bitwise_not\"><code>bitwise_not</code></a>\n</td> <td><p>Computes the bitwise NOT of the given input tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.bitwise_and#torch.bitwise_and\" title=\"torch.bitwise_and\"><code>bitwise_and</code></a>\n</td> <td><p>Computes the bitwise AND of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.bitwise_or#torch.bitwise_or\" title=\"torch.bitwise_or\"><code>bitwise_or</code></a>\n</td> <td><p>Computes the bitwise OR of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.bitwise_xor#torch.bitwise_xor\" title=\"torch.bitwise_xor\"><code>bitwise_xor</code></a>\n</td> <td><p>Computes the bitwise XOR of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.bitwise_left_shift#torch.bitwise_left_shift\" title=\"torch.bitwise_left_shift\"><code>bitwise_left_shift</code></a>\n</td> <td><p>Computes the left arithmetic shift of <code>input</code> by <code>other</code> bits.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.bitwise_right_shift#torch.bitwise_right_shift\" title=\"torch.bitwise_right_shift\"><code>bitwise_right_shift</code></a>\n</td> <td><p>Computes the right arithmetic shift of <code>input</code> by <code>other</code> bits.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.ceil#torch.ceil\" title=\"torch.ceil\"><code>ceil</code></a>\n</td> <td><p>Returns a new tensor with the ceil of the elements of <code>input</code>, the smallest integer greater than or equal to each element.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.clamp#torch.clamp\" title=\"torch.clamp\"><code>clamp</code></a>\n</td> <td><p>Clamps all elements in <code>input</code> into the range <code>[</code> <a class=\"reference internal\" href=\"generated/torch.min#torch.min\" title=\"torch.min\"><code>min</code></a>, <a class=\"reference internal\" href=\"generated/torch.max#torch.max\" title=\"torch.max\"><code>max</code></a> <code>]</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.clip#torch.clip\" title=\"torch.clip\"><code>clip</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.clamp#torch.clamp\" title=\"torch.clamp\"><code>torch.clamp()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.conj_physical#torch.conj_physical\" title=\"torch.conj_physical\"><code>conj_physical</code></a>\n</td> <td><p>Computes the element-wise conjugate of the given <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.copysign#torch.copysign\" title=\"torch.copysign\"><code>copysign</code></a>\n</td> <td><p>Create a new floating-point tensor with the magnitude of <code>input</code> and the sign of <code>other</code>, elementwise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cos#torch.cos\" title=\"torch.cos\"><code>cos</code></a>\n</td> <td><p>Returns a new tensor with the cosine of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cosh#torch.cosh\" title=\"torch.cosh\"><code>cosh</code></a>\n</td> <td><p>Returns a new tensor with the hyperbolic cosine of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.deg2rad#torch.deg2rad\" title=\"torch.deg2rad\"><code>deg2rad</code></a>\n</td> <td><p>Returns a new tensor with each of the elements of <code>input</code> converted from angles in degrees to radians.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.div#torch.div\" title=\"torch.div\"><code>div</code></a>\n</td> <td><p>Divides each element of the input <code>input</code> by the corresponding element of <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.divide#torch.divide\" title=\"torch.divide\"><code>divide</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.div#torch.div\" title=\"torch.div\"><code>torch.div()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.digamma#torch.digamma\" title=\"torch.digamma\"><code>digamma</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"special#torch.special.digamma\" title=\"torch.special.digamma\"><code>torch.special.digamma()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.erf#torch.erf\" title=\"torch.erf\"><code>erf</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"special#torch.special.erf\" title=\"torch.special.erf\"><code>torch.special.erf()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.erfc#torch.erfc\" title=\"torch.erfc\"><code>erfc</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"special#torch.special.erfc\" title=\"torch.special.erfc\"><code>torch.special.erfc()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.erfinv#torch.erfinv\" title=\"torch.erfinv\"><code>erfinv</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"special#torch.special.erfinv\" title=\"torch.special.erfinv\"><code>torch.special.erfinv()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.exp#torch.exp\" title=\"torch.exp\"><code>exp</code></a>\n</td> <td><p>Returns a new tensor with the exponential of the elements of the input tensor <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.exp2#torch.exp2\" title=\"torch.exp2\"><code>exp2</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"special#torch.special.exp2\" title=\"torch.special.exp2\"><code>torch.special.exp2()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.expm1#torch.expm1\" title=\"torch.expm1\"><code>expm1</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"special#torch.special.expm1\" title=\"torch.special.expm1\"><code>torch.special.expm1()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.fake_quantize_per_channel_affine#torch.fake_quantize_per_channel_affine\" title=\"torch.fake_quantize_per_channel_affine\"><code>fake_quantize_per_channel_affine</code></a>\n</td> <td><p>Returns a new tensor with the data in <code>input</code> fake quantized per channel using <code>scale</code>, <code>zero_point</code>, <code>quant_min</code> and <code>quant_max</code>, across the channel specified by <code>axis</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.fake_quantize_per_tensor_affine#torch.fake_quantize_per_tensor_affine\" title=\"torch.fake_quantize_per_tensor_affine\"><code>fake_quantize_per_tensor_affine</code></a>\n</td> <td><p>Returns a new tensor with the data in <code>input</code> fake quantized using <code>scale</code>, <code>zero_point</code>, <code>quant_min</code> and <code>quant_max</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.fix#torch.fix\" title=\"torch.fix\"><code>fix</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.trunc#torch.trunc\" title=\"torch.trunc\"><code>torch.trunc()</code></a></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.float_power#torch.float_power\" title=\"torch.float_power\"><code>float_power</code></a>\n</td> <td><p>Raises <code>input</code> to the power of <code>exponent</code>, elementwise, in double precision.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.floor#torch.floor\" title=\"torch.floor\"><code>floor</code></a>\n</td> <td><p>Returns a new tensor with the floor of the elements of <code>input</code>, the largest integer less than or equal to each element.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.floor_divide#torch.floor_divide\" title=\"torch.floor_divide\"><code>floor_divide</code></a>\n</td> <td></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.fmod#torch.fmod\" title=\"torch.fmod\"><code>fmod</code></a>\n</td> <td><p>Applies C++'s <a class=\"reference external\" href=\"https://en.cppreference.com/w/cpp/numeric/math/fmod\">std::fmod</a> entrywise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.frac#torch.frac\" title=\"torch.frac\"><code>frac</code></a>\n</td> <td><p>Computes the fractional portion of each element in <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.frexp#torch.frexp\" title=\"torch.frexp\"><code>frexp</code></a>\n</td> <td><p>Decomposes <code>input</code> into mantissa and exponent tensors such that <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo>=</mo><mtext>mantissa</mtext><mo></mo><msup><mn>2</mn><mtext>exponent</mtext></msup></mrow><annotation encoding=\"application/x-tex\">\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.gradient#torch.gradient\" title=\"torch.gradient\"><code>gradient</code></a>\n</td> <td><p>Estimates the gradient of a function <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>g</mi><mo>:</mo><msup><mi mathvariant=\"double-struck\">R</mi><mi>n</mi></msup><mo></mo><mi mathvariant=\"double-struck\">R</mi></mrow><annotation encoding=\"application/x-tex\">g : \\mathbb{R}^n \\rightarrow \\mathbb{R}</annotation></semantics></math></span></span></span> in one or more dimensions using the <a class=\"reference external\" href=\"https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf\">second-order accurate central differences method</a> and either first or second order estimates at the boundaries.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.imag#torch.imag\" title=\"torch.imag\"><code>imag</code></a>\n</td> <td><p>Returns a new tensor containing imaginary values of the <code>self</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.ldexp#torch.ldexp\" title=\"torch.ldexp\"><code>ldexp</code></a>\n</td> <td><p>Multiplies <code>input</code> by 2 ** <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.lerp#torch.lerp\" title=\"torch.lerp\"><code>lerp</code></a>\n</td> <td><p>Does a linear interpolation of two tensors <code>start</code> (given by <code>input</code>) and <code>end</code> based on a scalar or tensor <code>weight</code> and returns the resulting <code>out</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.lgamma#torch.lgamma\" title=\"torch.lgamma\"><code>lgamma</code></a>\n</td> <td><p>Computes the natural logarithm of the absolute value of the gamma function on <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.log#torch.log\" title=\"torch.log\"><code>log</code></a>\n</td> <td><p>Returns a new tensor with the natural logarithm of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.log10#torch.log10\" title=\"torch.log10\"><code>log10</code></a>\n</td> <td><p>Returns a new tensor with the logarithm to the base 10 of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.log1p#torch.log1p\" title=\"torch.log1p\"><code>log1p</code></a>\n</td> <td><p>Returns a new tensor with the natural logarithm of (1 + <code>input</code>).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.log2#torch.log2\" title=\"torch.log2\"><code>log2</code></a>\n</td> <td><p>Returns a new tensor with the logarithm to the base 2 of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.logaddexp#torch.logaddexp\" title=\"torch.logaddexp\"><code>logaddexp</code></a>\n</td> <td><p>Logarithm of the sum of exponentiations of the inputs.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.logaddexp2#torch.logaddexp2\" title=\"torch.logaddexp2\"><code>logaddexp2</code></a>\n</td> <td><p>Logarithm of the sum of exponentiations of the inputs in base-2.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.logical_and#torch.logical_and\" title=\"torch.logical_and\"><code>logical_and</code></a>\n</td> <td><p>Computes the element-wise logical AND of the given input tensors.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.logical_not#torch.logical_not\" title=\"torch.logical_not\"><code>logical_not</code></a>\n</td> <td><p>Computes the element-wise logical NOT of the given input tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.logical_or#torch.logical_or\" title=\"torch.logical_or\"><code>logical_or</code></a>\n</td> <td><p>Computes the element-wise logical OR of the given input tensors.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.logical_xor#torch.logical_xor\" title=\"torch.logical_xor\"><code>logical_xor</code></a>\n</td> <td><p>Computes the element-wise logical XOR of the given input tensors.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.logit#torch.logit\" title=\"torch.logit\"><code>logit</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"special#torch.special.logit\" title=\"torch.special.logit\"><code>torch.special.logit()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.hypot#torch.hypot\" title=\"torch.hypot\"><code>hypot</code></a>\n</td> <td><p>Given the legs of a right triangle, return its hypotenuse.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.i0#torch.i0\" title=\"torch.i0\"><code>i0</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"special#torch.special.i0\" title=\"torch.special.i0\"><code>torch.special.i0()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.igamma#torch.igamma\" title=\"torch.igamma\"><code>igamma</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"special#torch.special.gammainc\" title=\"torch.special.gammainc\"><code>torch.special.gammainc()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.igammac#torch.igammac\" title=\"torch.igammac\"><code>igammac</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"special#torch.special.gammaincc\" title=\"torch.special.gammaincc\"><code>torch.special.gammaincc()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.mul#torch.mul\" title=\"torch.mul\"><code>mul</code></a>\n</td> <td><p>Multiplies <code>input</code> by <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.multiply#torch.multiply\" title=\"torch.multiply\"><code>multiply</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.mul#torch.mul\" title=\"torch.mul\"><code>torch.mul()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.mvlgamma#torch.mvlgamma\" title=\"torch.mvlgamma\"><code>mvlgamma</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"special#torch.special.multigammaln\" title=\"torch.special.multigammaln\"><code>torch.special.multigammaln()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nan_to_num#torch.nan_to_num\" title=\"torch.nan_to_num\"><code>nan_to_num</code></a>\n</td> <td><p>Replaces <code>NaN</code>, positive infinity, and negative infinity values in <code>input</code> with the values specified by <code>nan</code>, <code>posinf</code>, and <code>neginf</code>, respectively.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.neg#torch.neg\" title=\"torch.neg\"><code>neg</code></a>\n</td> <td><p>Returns a new tensor with the negative of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.negative#torch.negative\" title=\"torch.negative\"><code>negative</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.neg#torch.neg\" title=\"torch.neg\"><code>torch.neg()</code></a></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nextafter#torch.nextafter\" title=\"torch.nextafter\"><code>nextafter</code></a>\n</td> <td><p>Return the next floating-point value after <code>input</code> towards <code>other</code>, elementwise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.polygamma#torch.polygamma\" title=\"torch.polygamma\"><code>polygamma</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"special#torch.special.polygamma\" title=\"torch.special.polygamma\"><code>torch.special.polygamma()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.positive#torch.positive\" title=\"torch.positive\"><code>positive</code></a>\n</td> <td><p>Returns <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.pow#torch.pow\" title=\"torch.pow\"><code>pow</code></a>\n</td> <td><p>Takes the power of each element in <code>input</code> with <code>exponent</code> and returns a tensor with the result.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.quantized_batch_norm#torch.quantized_batch_norm\" title=\"torch.quantized_batch_norm\"><code>quantized_batch_norm</code></a>\n</td> <td><p>Applies batch normalization on a 4D (NCHW) quantized tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.quantized_max_pool1d#torch.quantized_max_pool1d\" title=\"torch.quantized_max_pool1d\"><code>quantized_max_pool1d</code></a>\n</td> <td><p>Applies a 1D max pooling over an input quantized tensor composed of several input planes.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.quantized_max_pool2d#torch.quantized_max_pool2d\" title=\"torch.quantized_max_pool2d\"><code>quantized_max_pool2d</code></a>\n</td> <td><p>Applies a 2D max pooling over an input quantized tensor composed of several input planes.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.rad2deg#torch.rad2deg\" title=\"torch.rad2deg\"><code>rad2deg</code></a>\n</td> <td><p>Returns a new tensor with each of the elements of <code>input</code> converted from angles in radians to degrees.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.real#torch.real\" title=\"torch.real\"><code>real</code></a>\n</td> <td><p>Returns a new tensor containing real values of the <code>self</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.reciprocal#torch.reciprocal\" title=\"torch.reciprocal\"><code>reciprocal</code></a>\n</td> <td><p>Returns a new tensor with the reciprocal of the elements of <code>input</code></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.remainder#torch.remainder\" title=\"torch.remainder\"><code>remainder</code></a>\n</td> <td><p>Computes <a class=\"reference external\" href=\"https://docs.python.org/3/reference/expressions.html#binary-arithmetic-operations\">Python's modulus operation</a> entrywise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.round#torch.round\" title=\"torch.round\"><code>round</code></a>\n</td> <td><p>Rounds elements of <code>input</code> to the nearest integer.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.rsqrt#torch.rsqrt\" title=\"torch.rsqrt\"><code>rsqrt</code></a>\n</td> <td><p>Returns a new tensor with the reciprocal of the square-root of each of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sigmoid#torch.sigmoid\" title=\"torch.sigmoid\"><code>sigmoid</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"special#torch.special.expit\" title=\"torch.special.expit\"><code>torch.special.expit()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sign#torch.sign\" title=\"torch.sign\"><code>sign</code></a>\n</td> <td><p>Returns a new tensor with the signs of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sgn#torch.sgn\" title=\"torch.sgn\"><code>sgn</code></a>\n</td> <td><p>This function is an extension of torch.sign() to complex tensors.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.signbit#torch.signbit\" title=\"torch.signbit\"><code>signbit</code></a>\n</td> <td><p>Tests if each element of <code>input</code> has its sign bit set or not.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sin#torch.sin\" title=\"torch.sin\"><code>sin</code></a>\n</td> <td><p>Returns a new tensor with the sine of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sinc#torch.sinc\" title=\"torch.sinc\"><code>sinc</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"special#torch.special.sinc\" title=\"torch.special.sinc\"><code>torch.special.sinc()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sinh#torch.sinh\" title=\"torch.sinh\"><code>sinh</code></a>\n</td> <td><p>Returns a new tensor with the hyperbolic sine of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.softmax#torch.softmax\" title=\"torch.softmax\"><code>softmax</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.nn.functional.softmax#torch.nn.functional.softmax\" title=\"torch.nn.functional.softmax\"><code>torch.nn.functional.softmax()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sqrt#torch.sqrt\" title=\"torch.sqrt\"><code>sqrt</code></a>\n</td> <td><p>Returns a new tensor with the square-root of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.square#torch.square\" title=\"torch.square\"><code>square</code></a>\n</td> <td><p>Returns a new tensor with the square of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sub#torch.sub\" title=\"torch.sub\"><code>sub</code></a>\n</td> <td><p>Subtracts <code>other</code>, scaled by <code>alpha</code>, from <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.subtract#torch.subtract\" title=\"torch.subtract\"><code>subtract</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.sub#torch.sub\" title=\"torch.sub\"><code>torch.sub()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.tan#torch.tan\" title=\"torch.tan\"><code>tan</code></a>\n</td> <td><p>Returns a new tensor with the tangent of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.tanh#torch.tanh\" title=\"torch.tanh\"><code>tanh</code></a>\n</td> <td><p>Returns a new tensor with the hyperbolic tangent of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.true_divide#torch.true_divide\" title=\"torch.true_divide\"><code>true_divide</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.div#torch.div\" title=\"torch.div\"><code>torch.div()</code></a> with <code>rounding_mode=None</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.trunc#torch.trunc\" title=\"torch.trunc\"><code>trunc</code></a>\n</td> <td><p>Returns a new tensor with the truncated integer values of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.xlogy#torch.xlogy\" title=\"torch.xlogy\"><code>xlogy</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"special#torch.special.xlogy\" title=\"torch.special.xlogy\"><code>torch.special.xlogy()</code></a>.</p></td> </tr>  </table>   <h3 id=\"reduction-ops\">Reduction Ops</h3> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.argmax#torch.argmax\" title=\"torch.argmax\"><code>argmax</code></a>\n</td> <td><p>Returns the indices of the maximum value of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.argmin#torch.argmin\" title=\"torch.argmin\"><code>argmin</code></a>\n</td> <td><p>Returns the indices of the minimum value(s) of the flattened tensor or along a dimension</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.amax#torch.amax\" title=\"torch.amax\"><code>amax</code></a>\n</td> <td><p>Returns the maximum value of each slice of the <code>input</code> tensor in the given dimension(s) <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.amin#torch.amin\" title=\"torch.amin\"><code>amin</code></a>\n</td> <td><p>Returns the minimum value of each slice of the <code>input</code> tensor in the given dimension(s) <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.aminmax#torch.aminmax\" title=\"torch.aminmax\"><code>aminmax</code></a>\n</td> <td><p>Computes the minimum and maximum values of the <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.all#torch.all\" title=\"torch.all\"><code>all</code></a>\n</td> <td><p>Tests if all elements in <code>input</code> evaluate to <code>True</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.any#torch.any\" title=\"torch.any\"><code>any</code></a>\n</td> <td><p>Tests if any element in <code>input</code> evaluates to <code>True</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.max#torch.max\" title=\"torch.max\"><code>max</code></a>\n</td> <td><p>Returns the maximum value of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.min#torch.min\" title=\"torch.min\"><code>min</code></a>\n</td> <td><p>Returns the minimum value of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.dist#torch.dist\" title=\"torch.dist\"><code>dist</code></a>\n</td> <td><p>Returns the p-norm of (<code>input</code> - <code>other</code>)</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.logsumexp#torch.logsumexp\" title=\"torch.logsumexp\"><code>logsumexp</code></a>\n</td> <td><p>Returns the log of summed exponentials of each row of the <code>input</code> tensor in the given dimension <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.mean#torch.mean\" title=\"torch.mean\"><code>mean</code></a>\n</td> <td><p>Returns the mean value of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nanmean#torch.nanmean\" title=\"torch.nanmean\"><code>nanmean</code></a>\n</td> <td><p>Computes the mean of all <code>non-NaN</code> elements along the specified dimensions.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.median#torch.median\" title=\"torch.median\"><code>median</code></a>\n</td> <td><p>Returns the median of the values in <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nanmedian#torch.nanmedian\" title=\"torch.nanmedian\"><code>nanmedian</code></a>\n</td> <td><p>Returns the median of the values in <code>input</code>, ignoring <code>NaN</code> values.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.mode#torch.mode\" title=\"torch.mode\"><code>mode</code></a>\n</td> <td><p>Returns a namedtuple <code>(values, indices)</code> where <code>values</code> is the mode value of each row of the <code>input</code> tensor in the given dimension <code>dim</code>, i.e. a value which appears most often in that row, and <code>indices</code> is the index location of each mode value found.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.norm#torch.norm\" title=\"torch.norm\"><code>norm</code></a>\n</td> <td><p>Returns the matrix norm or vector norm of a given tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nansum#torch.nansum\" title=\"torch.nansum\"><code>nansum</code></a>\n</td> <td><p>Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.prod#torch.prod\" title=\"torch.prod\"><code>prod</code></a>\n</td> <td><p>Returns the product of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.quantile#torch.quantile\" title=\"torch.quantile\"><code>quantile</code></a>\n</td> <td><p>Computes the q-th quantiles of each row of the <code>input</code> tensor along the dimension <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nanquantile#torch.nanquantile\" title=\"torch.nanquantile\"><code>nanquantile</code></a>\n</td> <td><p>This is a variant of <a class=\"reference internal\" href=\"generated/torch.quantile#torch.quantile\" title=\"torch.quantile\"><code>torch.quantile()</code></a> that \"ignores\" <code>NaN</code> values, computing the quantiles <code>q</code> as if <code>NaN</code> values in <code>input</code> did not exist.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.std#torch.std\" title=\"torch.std\"><code>std</code></a>\n</td> <td><p>Calculates the standard deviation over the dimensions specified by <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.std_mean#torch.std_mean\" title=\"torch.std_mean\"><code>std_mean</code></a>\n</td> <td><p>Calculates the standard deviation and mean over the dimensions specified by <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sum#torch.sum\" title=\"torch.sum\"><code>sum</code></a>\n</td> <td><p>Returns the sum of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.unique#torch.unique\" title=\"torch.unique\"><code>unique</code></a>\n</td> <td><p>Returns the unique elements of the input tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.unique_consecutive#torch.unique_consecutive\" title=\"torch.unique_consecutive\"><code>unique_consecutive</code></a>\n</td> <td><p>Eliminates all but the first element from every consecutive group of equivalent elements.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.var#torch.var\" title=\"torch.var\"><code>var</code></a>\n</td> <td><p>Calculates the variance over the dimensions specified by <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.var_mean#torch.var_mean\" title=\"torch.var_mean\"><code>var_mean</code></a>\n</td> <td><p>Calculates the variance and mean over the dimensions specified by <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.count_nonzero#torch.count_nonzero\" title=\"torch.count_nonzero\"><code>count_nonzero</code></a>\n</td> <td><p>Counts the number of non-zero values in the tensor <code>input</code> along the given <code>dim</code>.</p></td> </tr>  </table>   <h3 id=\"comparison-ops\">Comparison Ops</h3> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.allclose#torch.allclose\" title=\"torch.allclose\"><code>allclose</code></a>\n</td> <td><p>This function checks if <code>input</code> and <code>other</code> satisfy the condition:</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.argsort#torch.argsort\" title=\"torch.argsort\"><code>argsort</code></a>\n</td> <td><p>Returns the indices that sort a tensor along a given dimension in ascending order by value.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.eq#torch.eq\" title=\"torch.eq\"><code>eq</code></a>\n</td> <td><p>Computes element-wise equality</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.equal#torch.equal\" title=\"torch.equal\"><code>equal</code></a>\n</td> <td><p><code>True</code> if two tensors have the same size and elements, <code>False</code> otherwise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.ge#torch.ge\" title=\"torch.ge\"><code>ge</code></a>\n</td> <td><p>Computes <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo></mo><mtext>other</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{input} \\geq \\text{other}</annotation></semantics></math></span></span></span> element-wise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.greater_equal#torch.greater_equal\" title=\"torch.greater_equal\"><code>greater_equal</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.ge#torch.ge\" title=\"torch.ge\"><code>torch.ge()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.gt#torch.gt\" title=\"torch.gt\"><code>gt</code></a>\n</td> <td><p>Computes <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo>&gt;</mo><mtext>other</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{input} &gt; \\text{other}</annotation></semantics></math></span></span></span> element-wise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.greater#torch.greater\" title=\"torch.greater\"><code>greater</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.gt#torch.gt\" title=\"torch.gt\"><code>torch.gt()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.isclose#torch.isclose\" title=\"torch.isclose\"><code>isclose</code></a>\n</td> <td><p>Returns a new tensor with boolean elements representing if each element of <code>input</code> is \"close\" to the corresponding element of <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.isfinite#torch.isfinite\" title=\"torch.isfinite\"><code>isfinite</code></a>\n</td> <td><p>Returns a new tensor with boolean elements representing if each element is <code>finite</code> or not.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.isin#torch.isin\" title=\"torch.isin\"><code>isin</code></a>\n</td> <td><p>Tests if each element of <code>elements</code> is in <code>test_elements</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.isinf#torch.isinf\" title=\"torch.isinf\"><code>isinf</code></a>\n</td> <td><p>Tests if each element of <code>input</code> is infinite (positive or negative infinity) or not.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.isposinf#torch.isposinf\" title=\"torch.isposinf\"><code>isposinf</code></a>\n</td> <td><p>Tests if each element of <code>input</code> is positive infinity or not.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.isneginf#torch.isneginf\" title=\"torch.isneginf\"><code>isneginf</code></a>\n</td> <td><p>Tests if each element of <code>input</code> is negative infinity or not.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.isnan#torch.isnan\" title=\"torch.isnan\"><code>isnan</code></a>\n</td> <td><p>Returns a new tensor with boolean elements representing if each element of <code>input</code> is NaN or not.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.isreal#torch.isreal\" title=\"torch.isreal\"><code>isreal</code></a>\n</td> <td><p>Returns a new tensor with boolean elements representing if each element of <code>input</code> is real-valued or not.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.kthvalue#torch.kthvalue\" title=\"torch.kthvalue\"><code>kthvalue</code></a>\n</td> <td><p>Returns a namedtuple <code>(values, indices)</code> where <code>values</code> is the <code>k</code> th smallest element of each row of the <code>input</code> tensor in the given dimension <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.le#torch.le\" title=\"torch.le\"><code>le</code></a>\n</td> <td><p>Computes <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo></mo><mtext>other</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{input} \\leq \\text{other}</annotation></semantics></math></span></span></span> element-wise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.less_equal#torch.less_equal\" title=\"torch.less_equal\"><code>less_equal</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.le#torch.le\" title=\"torch.le\"><code>torch.le()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.lt#torch.lt\" title=\"torch.lt\"><code>lt</code></a>\n</td> <td><p>Computes <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo>&lt;</mo><mtext>other</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{input} &lt; \\text{other}</annotation></semantics></math></span></span></span> element-wise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.less#torch.less\" title=\"torch.less\"><code>less</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.lt#torch.lt\" title=\"torch.lt\"><code>torch.lt()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.maximum#torch.maximum\" title=\"torch.maximum\"><code>maximum</code></a>\n</td> <td><p>Computes the element-wise maximum of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.minimum#torch.minimum\" title=\"torch.minimum\"><code>minimum</code></a>\n</td> <td><p>Computes the element-wise minimum of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.fmax#torch.fmax\" title=\"torch.fmax\"><code>fmax</code></a>\n</td> <td><p>Computes the element-wise maximum of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.fmin#torch.fmin\" title=\"torch.fmin\"><code>fmin</code></a>\n</td> <td><p>Computes the element-wise minimum of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.ne#torch.ne\" title=\"torch.ne\"><code>ne</code></a>\n</td> <td><p>Computes <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo mathvariant=\"normal\"></mo><mtext>other</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{input} \\neq \\text{other}</annotation></semantics></math></span></span></span> element-wise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.not_equal#torch.not_equal\" title=\"torch.not_equal\"><code>not_equal</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.ne#torch.ne\" title=\"torch.ne\"><code>torch.ne()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sort#torch.sort\" title=\"torch.sort\"><code>sort</code></a>\n</td> <td><p>Sorts the elements of the <code>input</code> tensor along a given dimension in ascending order by value.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.topk#torch.topk\" title=\"torch.topk\"><code>topk</code></a>\n</td> <td><p>Returns the <code>k</code> largest elements of the given <code>input</code> tensor along a given dimension.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.msort#torch.msort\" title=\"torch.msort\"><code>msort</code></a>\n</td> <td><p>Sorts the elements of the <code>input</code> tensor along its first dimension in ascending order by value.</p></td> </tr>  </table>   <h3 id=\"spectral-ops\">Spectral Ops</h3> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.stft#torch.stft\" title=\"torch.stft\"><code>stft</code></a>\n</td> <td><p>Short-time Fourier transform (STFT).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.istft#torch.istft\" title=\"torch.istft\"><code>istft</code></a>\n</td> <td><p>Inverse short time Fourier Transform.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.bartlett_window#torch.bartlett_window\" title=\"torch.bartlett_window\"><code>bartlett_window</code></a>\n</td> <td><p>Bartlett window function.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.blackman_window#torch.blackman_window\" title=\"torch.blackman_window\"><code>blackman_window</code></a>\n</td> <td><p>Blackman window function.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.hamming_window#torch.hamming_window\" title=\"torch.hamming_window\"><code>hamming_window</code></a>\n</td> <td><p>Hamming window function.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.hann_window#torch.hann_window\" title=\"torch.hann_window\"><code>hann_window</code></a>\n</td> <td><p>Hann window function.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.kaiser_window#torch.kaiser_window\" title=\"torch.kaiser_window\"><code>kaiser_window</code></a>\n</td> <td><p>Computes the Kaiser window with window length <code>window_length</code> and shape parameter <code>beta</code>.</p></td> </tr>  </table>   <h3 id=\"other-operations\">Other Operations</h3> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.atleast_1d#torch.atleast_1d\" title=\"torch.atleast_1d\"><code>atleast_1d</code></a>\n</td> <td><p>Returns a 1-dimensional view of each input tensor with zero dimensions.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.atleast_2d#torch.atleast_2d\" title=\"torch.atleast_2d\"><code>atleast_2d</code></a>\n</td> <td><p>Returns a 2-dimensional view of each input tensor with zero dimensions.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.atleast_3d#torch.atleast_3d\" title=\"torch.atleast_3d\"><code>atleast_3d</code></a>\n</td> <td><p>Returns a 3-dimensional view of each input tensor with zero dimensions.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.bincount#torch.bincount\" title=\"torch.bincount\"><code>bincount</code></a>\n</td> <td><p>Count the frequency of each value in an array of non-negative ints.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.block_diag#torch.block_diag\" title=\"torch.block_diag\"><code>block_diag</code></a>\n</td> <td><p>Create a block diagonal matrix from provided tensors.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.broadcast_tensors#torch.broadcast_tensors\" title=\"torch.broadcast_tensors\"><code>broadcast_tensors</code></a>\n</td> <td><p>Broadcasts the given tensors according to <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/broadcasting.html#broadcasting-semantics\"><span class=\"std std-ref\">Broadcasting semantics</span></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.broadcast_to#torch.broadcast_to\" title=\"torch.broadcast_to\"><code>broadcast_to</code></a>\n</td> <td><p>Broadcasts <code>input</code> to the shape <code>shape</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.broadcast_shapes#torch.broadcast_shapes\" title=\"torch.broadcast_shapes\"><code>broadcast_shapes</code></a>\n</td> <td><p>Similar to <a class=\"reference internal\" href=\"generated/torch.broadcast_tensors#torch.broadcast_tensors\" title=\"torch.broadcast_tensors\"><code>broadcast_tensors()</code></a> but for shapes.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.bucketize#torch.bucketize\" title=\"torch.bucketize\"><code>bucketize</code></a>\n</td> <td><p>Returns the indices of the buckets to which each value in the <code>input</code> belongs, where the boundaries of the buckets are set by <code>boundaries</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cartesian_prod#torch.cartesian_prod\" title=\"torch.cartesian_prod\"><code>cartesian_prod</code></a>\n</td> <td><p>Do cartesian product of the given sequence of tensors.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cdist#torch.cdist\" title=\"torch.cdist\"><code>cdist</code></a>\n</td> <td><p>Computes batched the p-norm distance between each pair of the two collections of row vectors.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.clone#torch.clone\" title=\"torch.clone\"><code>clone</code></a>\n</td> <td><p>Returns a copy of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.combinations#torch.combinations\" title=\"torch.combinations\"><code>combinations</code></a>\n</td> <td><p>Compute combinations of length <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi></mrow><annotation encoding=\"application/x-tex\">r</annotation></semantics></math></span></span></span> of the given tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.corrcoef#torch.corrcoef\" title=\"torch.corrcoef\"><code>corrcoef</code></a>\n</td> <td><p>Estimates the Pearson product-moment correlation coefficient matrix of the variables given by the <code>input</code> matrix, where rows are the variables and columns are the observations.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cov#torch.cov\" title=\"torch.cov\"><code>cov</code></a>\n</td> <td><p>Estimates the covariance matrix of the variables given by the <code>input</code> matrix, where rows are the variables and columns are the observations.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cross#torch.cross\" title=\"torch.cross\"><code>cross</code></a>\n</td> <td><p>Returns the cross product of vectors in dimension <code>dim</code> of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cummax#torch.cummax\" title=\"torch.cummax\"><code>cummax</code></a>\n</td> <td><p>Returns a namedtuple <code>(values, indices)</code> where <code>values</code> is the cumulative maximum of elements of <code>input</code> in the dimension <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cummin#torch.cummin\" title=\"torch.cummin\"><code>cummin</code></a>\n</td> <td><p>Returns a namedtuple <code>(values, indices)</code> where <code>values</code> is the cumulative minimum of elements of <code>input</code> in the dimension <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cumprod#torch.cumprod\" title=\"torch.cumprod\"><code>cumprod</code></a>\n</td> <td><p>Returns the cumulative product of elements of <code>input</code> in the dimension <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cumsum#torch.cumsum\" title=\"torch.cumsum\"><code>cumsum</code></a>\n</td> <td><p>Returns the cumulative sum of elements of <code>input</code> in the dimension <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.diag#torch.diag\" title=\"torch.diag\"><code>diag</code></a>\n</td> <td>\n\n<ul class=\"simple\"> <li>If <code>input</code> is a vector (1-D tensor), then returns a 2-D square tensor</li> </ul> </td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.diag_embed#torch.diag_embed\" title=\"torch.diag_embed\"><code>diag_embed</code></a>\n</td> <td><p>Creates a tensor whose diagonals of certain 2D planes (specified by <code>dim1</code> and <code>dim2</code>) are filled by <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.diagflat#torch.diagflat\" title=\"torch.diagflat\"><code>diagflat</code></a>\n</td> <td>\n\n<ul class=\"simple\"> <li>If <code>input</code> is a vector (1-D tensor), then returns a 2-D square tensor</li> </ul> </td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.diagonal#torch.diagonal\" title=\"torch.diagonal\"><code>diagonal</code></a>\n</td> <td><p>Returns a partial view of <code>input</code> with the its diagonal elements with respect to <code>dim1</code> and <code>dim2</code> appended as a dimension at the end of the shape.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.diff#torch.diff\" title=\"torch.diff\"><code>diff</code></a>\n</td> <td><p>Computes the n-th forward difference along the given dimension.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.einsum#torch.einsum\" title=\"torch.einsum\"><code>einsum</code></a>\n</td> <td><p>Sums the product of the elements of the input <code>operands</code> along dimensions specified using a notation based on the Einstein summation convention.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.flatten#torch.flatten\" title=\"torch.flatten\"><code>flatten</code></a>\n</td> <td><p>Flattens <code>input</code> by reshaping it into a one-dimensional tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.flip#torch.flip\" title=\"torch.flip\"><code>flip</code></a>\n</td> <td><p>Reverse the order of an n-D tensor along given axis in dims.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.fliplr#torch.fliplr\" title=\"torch.fliplr\"><code>fliplr</code></a>\n</td> <td><p>Flip tensor in the left/right direction, returning a new tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.flipud#torch.flipud\" title=\"torch.flipud\"><code>flipud</code></a>\n</td> <td><p>Flip tensor in the up/down direction, returning a new tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.kron#torch.kron\" title=\"torch.kron\"><code>kron</code></a>\n</td> <td><p>Computes the Kronecker product, denoted by <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo></mo></mrow><annotation encoding=\"application/x-tex\">\\otimes</annotation></semantics></math></span></span></span>, of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.rot90#torch.rot90\" title=\"torch.rot90\"><code>rot90</code></a>\n</td> <td><p>Rotate an n-D tensor by 90 degrees in the plane specified by dims axis.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.gcd#torch.gcd\" title=\"torch.gcd\"><code>gcd</code></a>\n</td> <td><p>Computes the element-wise greatest common divisor (GCD) of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.histc#torch.histc\" title=\"torch.histc\"><code>histc</code></a>\n</td> <td><p>Computes the histogram of a tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.histogram#torch.histogram\" title=\"torch.histogram\"><code>histogram</code></a>\n</td> <td><p>Computes a histogram of the values in a tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.histogramdd#torch.histogramdd\" title=\"torch.histogramdd\"><code>histogramdd</code></a>\n</td> <td><p>Computes a multi-dimensional histogram of the values in a tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.meshgrid#torch.meshgrid\" title=\"torch.meshgrid\"><code>meshgrid</code></a>\n</td> <td><p>Creates grids of coordinates specified by the 1D inputs in <code>attr</code>:tensors.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.lcm#torch.lcm\" title=\"torch.lcm\"><code>lcm</code></a>\n</td> <td><p>Computes the element-wise least common multiple (LCM) of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.logcumsumexp#torch.logcumsumexp\" title=\"torch.logcumsumexp\"><code>logcumsumexp</code></a>\n</td> <td><p>Returns the logarithm of the cumulative summation of the exponentiation of elements of <code>input</code> in the dimension <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.ravel#torch.ravel\" title=\"torch.ravel\"><code>ravel</code></a>\n</td> <td><p>Return a contiguous flattened tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.renorm#torch.renorm\" title=\"torch.renorm\"><code>renorm</code></a>\n</td> <td><p>Returns a tensor where each sub-tensor of <code>input</code> along dimension <code>dim</code> is normalized such that the <code>p</code>-norm of the sub-tensor is lower than the value <code>maxnorm</code></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.repeat_interleave#torch.repeat_interleave\" title=\"torch.repeat_interleave\"><code>repeat_interleave</code></a>\n</td> <td><p>Repeat elements of a tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.roll#torch.roll\" title=\"torch.roll\"><code>roll</code></a>\n</td> <td><p>Roll the tensor <code>input</code> along the given dimension(s).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.searchsorted#torch.searchsorted\" title=\"torch.searchsorted\"><code>searchsorted</code></a>\n</td> <td><p>Find the indices from the <em>innermost</em> dimension of <code>sorted_sequence</code> such that, if the corresponding values in <code>values</code> were inserted before the indices, when sorted, the order of the corresponding <em>innermost</em> dimension within <code>sorted_sequence</code> would be preserved.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.tensordot#torch.tensordot\" title=\"torch.tensordot\"><code>tensordot</code></a>\n</td> <td><p>Returns a contraction of a and b over multiple dimensions.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.trace#torch.trace\" title=\"torch.trace\"><code>trace</code></a>\n</td> <td><p>Returns the sum of the elements of the diagonal of the input 2-D matrix.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.tril#torch.tril\" title=\"torch.tril\"><code>tril</code></a>\n</td> <td><p>Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices <code>input</code>, the other elements of the result tensor <code>out</code> are set to 0.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.tril_indices#torch.tril_indices\" title=\"torch.tril_indices\"><code>tril_indices</code></a>\n</td> <td><p>Returns the indices of the lower triangular part of a <code>row</code>-by- <code>col</code> matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.triu#torch.triu\" title=\"torch.triu\"><code>triu</code></a>\n</td> <td><p>Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices <code>input</code>, the other elements of the result tensor <code>out</code> are set to 0.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.triu_indices#torch.triu_indices\" title=\"torch.triu_indices\"><code>triu_indices</code></a>\n</td> <td><p>Returns the indices of the upper triangular part of a <code>row</code> by <code>col</code> matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.unflatten#torch.unflatten\" title=\"torch.unflatten\"><code>unflatten</code></a>\n</td> <td><p>Expands a dimension of the input tensor over multiple dimensions.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.vander#torch.vander\" title=\"torch.vander\"><code>vander</code></a>\n</td> <td><p>Generates a Vandermonde matrix.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.view_as_real#torch.view_as_real\" title=\"torch.view_as_real\"><code>view_as_real</code></a>\n</td> <td><p>Returns a view of <code>input</code> as a real tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.view_as_complex#torch.view_as_complex\" title=\"torch.view_as_complex\"><code>view_as_complex</code></a>\n</td> <td><p>Returns a view of <code>input</code> as a complex tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.resolve_conj#torch.resolve_conj\" title=\"torch.resolve_conj\"><code>resolve_conj</code></a>\n</td> <td><p>Returns a new tensor with materialized conjugation if <code>input</code>'s conjugate bit is set to <code>True</code>, else returns <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.resolve_neg#torch.resolve_neg\" title=\"torch.resolve_neg\"><code>resolve_neg</code></a>\n</td> <td><p>Returns a new tensor with materialized negation if <code>input</code>'s negative bit is set to <code>True</code>, else returns <code>input</code>.</p></td> </tr>  </table>   <h3 id=\"blas-and-lapack-operations\">BLAS and LAPACK Operations</h3> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.addbmm#torch.addbmm\" title=\"torch.addbmm\"><code>addbmm</code></a>\n</td> <td><p>Performs a batch matrix-matrix product of matrices stored in <code>batch1</code> and <code>batch2</code>, with a reduced add step (all matrix multiplications get accumulated along the first dimension).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.addmm#torch.addmm\" title=\"torch.addmm\"><code>addmm</code></a>\n</td> <td><p>Performs a matrix multiplication of the matrices <code>mat1</code> and <code>mat2</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.addmv#torch.addmv\" title=\"torch.addmv\"><code>addmv</code></a>\n</td> <td><p>Performs a matrix-vector product of the matrix <code>mat</code> and the vector <code>vec</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.addr#torch.addr\" title=\"torch.addr\"><code>addr</code></a>\n</td> <td><p>Performs the outer-product of vectors <code>vec1</code> and <code>vec2</code> and adds it to the matrix <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.baddbmm#torch.baddbmm\" title=\"torch.baddbmm\"><code>baddbmm</code></a>\n</td> <td><p>Performs a batch matrix-matrix product of matrices in <code>batch1</code> and <code>batch2</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.bmm#torch.bmm\" title=\"torch.bmm\"><code>bmm</code></a>\n</td> <td><p>Performs a batch matrix-matrix product of matrices stored in <code>input</code> and <code>mat2</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.chain_matmul#torch.chain_matmul\" title=\"torch.chain_matmul\"><code>chain_matmul</code></a>\n</td> <td><p>Returns the matrix product of the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span></span></span> 2-D tensors.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cholesky#torch.cholesky\" title=\"torch.cholesky\"><code>cholesky</code></a>\n</td> <td><p>Computes the Cholesky decomposition of a symmetric positive-definite matrix <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span></span></span> or for batches of symmetric positive-definite matrices.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cholesky_inverse#torch.cholesky_inverse\" title=\"torch.cholesky_inverse\"><code>cholesky_inverse</code></a>\n</td> <td><p>Computes the inverse of a symmetric positive-definite matrix <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span></span></span> using its Cholesky factor <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>u</mi></mrow><annotation encoding=\"application/x-tex\">u</annotation></semantics></math></span></span></span>: returns matrix <code>inv</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cholesky_solve#torch.cholesky_solve\" title=\"torch.cholesky_solve\"><code>cholesky_solve</code></a>\n</td> <td><p>Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>u</mi></mrow><annotation encoding=\"application/x-tex\">u</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.dot#torch.dot\" title=\"torch.dot\"><code>dot</code></a>\n</td> <td><p>Computes the dot product of two 1D tensors.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.geqrf#torch.geqrf\" title=\"torch.geqrf\"><code>geqrf</code></a>\n</td> <td><p>This is a low-level function for calling LAPACK's geqrf directly.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.ger#torch.ger\" title=\"torch.ger\"><code>ger</code></a>\n</td> <td><p>Alias of <a class=\"reference internal\" href=\"generated/torch.outer#torch.outer\" title=\"torch.outer\"><code>torch.outer()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.inner#torch.inner\" title=\"torch.inner\"><code>inner</code></a>\n</td> <td><p>Computes the dot product for 1D tensors.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.inverse#torch.inverse\" title=\"torch.inverse\"><code>inverse</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.linalg.inv#torch.linalg.inv\" title=\"torch.linalg.inv\"><code>torch.linalg.inv()</code></a></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.det#torch.det\" title=\"torch.det\"><code>det</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.linalg.det#torch.linalg.det\" title=\"torch.linalg.det\"><code>torch.linalg.det()</code></a></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.logdet#torch.logdet\" title=\"torch.logdet\"><code>logdet</code></a>\n</td> <td><p>Calculates log determinant of a square matrix or batches of square matrices.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.slogdet#torch.slogdet\" title=\"torch.slogdet\"><code>slogdet</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.linalg.slogdet#torch.linalg.slogdet\" title=\"torch.linalg.slogdet\"><code>torch.linalg.slogdet()</code></a></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.lu#torch.lu\" title=\"torch.lu\"><code>lu</code></a>\n</td> <td><p>Computes the LU factorization of a matrix or batches of matrices <code>A</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.lu_solve#torch.lu_solve\" title=\"torch.lu_solve\"><code>lu_solve</code></a>\n</td> <td><p>Returns the LU solve of the linear system <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi><mi>x</mi><mo>=</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">Ax = b</annotation></semantics></math></span></span></span> using the partially pivoted LU factorization of A from <a class=\"reference internal\" href=\"generated/torch.linalg.lu_factor#torch.linalg.lu_factor\" title=\"torch.linalg.lu_factor\"><code>lu_factor()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.lu_unpack#torch.lu_unpack\" title=\"torch.lu_unpack\"><code>lu_unpack</code></a>\n</td> <td><p>Unpacks the LU decomposition returned by <a class=\"reference internal\" href=\"generated/torch.linalg.lu_factor#torch.linalg.lu_factor\" title=\"torch.linalg.lu_factor\"><code>lu_factor()</code></a> into the <code>P, L, U</code> matrices.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.matmul#torch.matmul\" title=\"torch.matmul\"><code>matmul</code></a>\n</td> <td><p>Matrix product of two tensors.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.matrix_power#torch.matrix_power\" title=\"torch.matrix_power\"><code>matrix_power</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.linalg.matrix_power#torch.linalg.matrix_power\" title=\"torch.linalg.matrix_power\"><code>torch.linalg.matrix_power()</code></a></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.matrix_exp#torch.matrix_exp\" title=\"torch.matrix_exp\"><code>matrix_exp</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.linalg.matrix_exp#torch.linalg.matrix_exp\" title=\"torch.linalg.matrix_exp\"><code>torch.linalg.matrix_exp()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.mm#torch.mm\" title=\"torch.mm\"><code>mm</code></a>\n</td> <td><p>Performs a matrix multiplication of the matrices <code>input</code> and <code>mat2</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.mv#torch.mv\" title=\"torch.mv\"><code>mv</code></a>\n</td> <td><p>Performs a matrix-vector product of the matrix <code>input</code> and the vector <code>vec</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.orgqr#torch.orgqr\" title=\"torch.orgqr\"><code>orgqr</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.linalg.householder_product#torch.linalg.householder_product\" title=\"torch.linalg.householder_product\"><code>torch.linalg.householder_product()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.ormqr#torch.ormqr\" title=\"torch.ormqr\"><code>ormqr</code></a>\n</td> <td><p>Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.outer#torch.outer\" title=\"torch.outer\"><code>outer</code></a>\n</td> <td><p>Outer product of <code>input</code> and <code>vec2</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.pinverse#torch.pinverse\" title=\"torch.pinverse\"><code>pinverse</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.linalg.pinv#torch.linalg.pinv\" title=\"torch.linalg.pinv\"><code>torch.linalg.pinv()</code></a></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.qr#torch.qr\" title=\"torch.qr\"><code>qr</code></a>\n</td> <td><p>Computes the QR decomposition of a matrix or a batch of matrices <code>input</code>, and returns a namedtuple (Q, R) of tensors such that <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo>=</mo><mi>Q</mi><mi>R</mi></mrow><annotation encoding=\"application/x-tex\">\\text{input} = Q R</annotation></semantics></math></span></span></span> with <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span></span></span> being an orthogonal matrix or batch of orthogonal matrices and <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>R</mi></mrow><annotation encoding=\"application/x-tex\">R</annotation></semantics></math></span></span></span> being an upper triangular matrix or batch of upper triangular matrices.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.svd#torch.svd\" title=\"torch.svd\"><code>svd</code></a>\n</td> <td><p>Computes the singular value decomposition of either a matrix or batch of matrices <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.svd_lowrank#torch.svd_lowrank\" title=\"torch.svd_lowrank\"><code>svd_lowrank</code></a>\n</td> <td><p>Return the singular value decomposition <code>(U, S, V)</code> of a matrix, batches of matrices, or a sparse matrix <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span></span></span> such that <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi><mo></mo><mi>U</mi><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo><msup><mi>V</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">A \\approx U diag(S) V^T</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.pca_lowrank#torch.pca_lowrank\" title=\"torch.pca_lowrank\"><code>pca_lowrank</code></a>\n</td> <td><p>Performs linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.lobpcg#torch.lobpcg\" title=\"torch.lobpcg\"><code>lobpcg</code></a>\n</td> <td><p>Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive definite generalized eigenvalue problem using matrix-free LOBPCG methods.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.trapz#torch.trapz\" title=\"torch.trapz\"><code>trapz</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.trapezoid#torch.trapezoid\" title=\"torch.trapezoid\"><code>torch.trapezoid()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.trapezoid#torch.trapezoid\" title=\"torch.trapezoid\"><code>trapezoid</code></a>\n</td> <td><p>Computes the <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Trapezoidal_rule\">trapezoidal rule</a> along <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cumulative_trapezoid#torch.cumulative_trapezoid\" title=\"torch.cumulative_trapezoid\"><code>cumulative_trapezoid</code></a>\n</td> <td>\n\n<p>Cumulatively computes the <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Trapezoidal_rule\">trapezoidal rule</a> along <code>dim</code>.</p> </td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.triangular_solve#torch.triangular_solve\" title=\"torch.triangular_solve\"><code>triangular_solve</code></a>\n</td> <td><p>Solves a system of equations with a square upper or lower triangular invertible matrix <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span></span></span> and multiple right-hand sides <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">b</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.vdot#torch.vdot\" title=\"torch.vdot\"><code>vdot</code></a>\n</td> <td><p>Computes the dot product of two 1D vectors along a dimension.</p></td> </tr>  </table>   <h3 id=\"foreach-operations\">Foreach Operations</h3> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This API is in beta and subject to future changes. Forward-mode AD is not supported.</p> </div> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_abs#torch._foreach_abs\" title=\"torch._foreach_abs\"><code>_foreach_abs</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.abs#torch.abs\" title=\"torch.abs\"><code>torch.abs()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_abs_#torch._foreach_abs_\" title=\"torch._foreach_abs_\"><code>_foreach_abs_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.abs#torch.abs\" title=\"torch.abs\"><code>torch.abs()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_acos#torch._foreach_acos\" title=\"torch._foreach_acos\"><code>_foreach_acos</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.acos#torch.acos\" title=\"torch.acos\"><code>torch.acos()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_acos_#torch._foreach_acos_\" title=\"torch._foreach_acos_\"><code>_foreach_acos_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.acos#torch.acos\" title=\"torch.acos\"><code>torch.acos()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_asin#torch._foreach_asin\" title=\"torch._foreach_asin\"><code>_foreach_asin</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.asin#torch.asin\" title=\"torch.asin\"><code>torch.asin()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_asin_#torch._foreach_asin_\" title=\"torch._foreach_asin_\"><code>_foreach_asin_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.asin#torch.asin\" title=\"torch.asin\"><code>torch.asin()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_atan#torch._foreach_atan\" title=\"torch._foreach_atan\"><code>_foreach_atan</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.atan#torch.atan\" title=\"torch.atan\"><code>torch.atan()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_atan_#torch._foreach_atan_\" title=\"torch._foreach_atan_\"><code>_foreach_atan_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.atan#torch.atan\" title=\"torch.atan\"><code>torch.atan()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_ceil#torch._foreach_ceil\" title=\"torch._foreach_ceil\"><code>_foreach_ceil</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.ceil#torch.ceil\" title=\"torch.ceil\"><code>torch.ceil()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_ceil_#torch._foreach_ceil_\" title=\"torch._foreach_ceil_\"><code>_foreach_ceil_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.ceil#torch.ceil\" title=\"torch.ceil\"><code>torch.ceil()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_cos#torch._foreach_cos\" title=\"torch._foreach_cos\"><code>_foreach_cos</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.cos#torch.cos\" title=\"torch.cos\"><code>torch.cos()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_cos_#torch._foreach_cos_\" title=\"torch._foreach_cos_\"><code>_foreach_cos_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.cos#torch.cos\" title=\"torch.cos\"><code>torch.cos()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_cosh#torch._foreach_cosh\" title=\"torch._foreach_cosh\"><code>_foreach_cosh</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.cosh#torch.cosh\" title=\"torch.cosh\"><code>torch.cosh()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_cosh_#torch._foreach_cosh_\" title=\"torch._foreach_cosh_\"><code>_foreach_cosh_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.cosh#torch.cosh\" title=\"torch.cosh\"><code>torch.cosh()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_erf#torch._foreach_erf\" title=\"torch._foreach_erf\"><code>_foreach_erf</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.erf#torch.erf\" title=\"torch.erf\"><code>torch.erf()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_erf_#torch._foreach_erf_\" title=\"torch._foreach_erf_\"><code>_foreach_erf_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.erf#torch.erf\" title=\"torch.erf\"><code>torch.erf()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_erfc#torch._foreach_erfc\" title=\"torch._foreach_erfc\"><code>_foreach_erfc</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.erfc#torch.erfc\" title=\"torch.erfc\"><code>torch.erfc()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_erfc_#torch._foreach_erfc_\" title=\"torch._foreach_erfc_\"><code>_foreach_erfc_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.erfc#torch.erfc\" title=\"torch.erfc\"><code>torch.erfc()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_exp#torch._foreach_exp\" title=\"torch._foreach_exp\"><code>_foreach_exp</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.exp#torch.exp\" title=\"torch.exp\"><code>torch.exp()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_exp_#torch._foreach_exp_\" title=\"torch._foreach_exp_\"><code>_foreach_exp_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.exp#torch.exp\" title=\"torch.exp\"><code>torch.exp()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_expm1#torch._foreach_expm1\" title=\"torch._foreach_expm1\"><code>_foreach_expm1</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.expm1#torch.expm1\" title=\"torch.expm1\"><code>torch.expm1()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_expm1_#torch._foreach_expm1_\" title=\"torch._foreach_expm1_\"><code>_foreach_expm1_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.expm1#torch.expm1\" title=\"torch.expm1\"><code>torch.expm1()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_floor#torch._foreach_floor\" title=\"torch._foreach_floor\"><code>_foreach_floor</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.floor#torch.floor\" title=\"torch.floor\"><code>torch.floor()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_floor_#torch._foreach_floor_\" title=\"torch._foreach_floor_\"><code>_foreach_floor_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.floor#torch.floor\" title=\"torch.floor\"><code>torch.floor()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_log#torch._foreach_log\" title=\"torch._foreach_log\"><code>_foreach_log</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.log#torch.log\" title=\"torch.log\"><code>torch.log()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_log_#torch._foreach_log_\" title=\"torch._foreach_log_\"><code>_foreach_log_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.log#torch.log\" title=\"torch.log\"><code>torch.log()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_log10#torch._foreach_log10\" title=\"torch._foreach_log10\"><code>_foreach_log10</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.log10#torch.log10\" title=\"torch.log10\"><code>torch.log10()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_log10_#torch._foreach_log10_\" title=\"torch._foreach_log10_\"><code>_foreach_log10_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.log10#torch.log10\" title=\"torch.log10\"><code>torch.log10()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_log1p#torch._foreach_log1p\" title=\"torch._foreach_log1p\"><code>_foreach_log1p</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.log1p#torch.log1p\" title=\"torch.log1p\"><code>torch.log1p()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_log1p_#torch._foreach_log1p_\" title=\"torch._foreach_log1p_\"><code>_foreach_log1p_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.log1p#torch.log1p\" title=\"torch.log1p\"><code>torch.log1p()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_log2#torch._foreach_log2\" title=\"torch._foreach_log2\"><code>_foreach_log2</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.log2#torch.log2\" title=\"torch.log2\"><code>torch.log2()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_log2_#torch._foreach_log2_\" title=\"torch._foreach_log2_\"><code>_foreach_log2_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.log2#torch.log2\" title=\"torch.log2\"><code>torch.log2()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_neg#torch._foreach_neg\" title=\"torch._foreach_neg\"><code>_foreach_neg</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.neg#torch.neg\" title=\"torch.neg\"><code>torch.neg()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_neg_#torch._foreach_neg_\" title=\"torch._foreach_neg_\"><code>_foreach_neg_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.neg#torch.neg\" title=\"torch.neg\"><code>torch.neg()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_tan#torch._foreach_tan\" title=\"torch._foreach_tan\"><code>_foreach_tan</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.tan#torch.tan\" title=\"torch.tan\"><code>torch.tan()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_tan_#torch._foreach_tan_\" title=\"torch._foreach_tan_\"><code>_foreach_tan_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.tan#torch.tan\" title=\"torch.tan\"><code>torch.tan()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_sin#torch._foreach_sin\" title=\"torch._foreach_sin\"><code>_foreach_sin</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.sin#torch.sin\" title=\"torch.sin\"><code>torch.sin()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_sin_#torch._foreach_sin_\" title=\"torch._foreach_sin_\"><code>_foreach_sin_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.sin#torch.sin\" title=\"torch.sin\"><code>torch.sin()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_sinh#torch._foreach_sinh\" title=\"torch._foreach_sinh\"><code>_foreach_sinh</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.sinh#torch.sinh\" title=\"torch.sinh\"><code>torch.sinh()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_sinh_#torch._foreach_sinh_\" title=\"torch._foreach_sinh_\"><code>_foreach_sinh_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.sinh#torch.sinh\" title=\"torch.sinh\"><code>torch.sinh()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_round#torch._foreach_round\" title=\"torch._foreach_round\"><code>_foreach_round</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.round#torch.round\" title=\"torch.round\"><code>torch.round()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_round_#torch._foreach_round_\" title=\"torch._foreach_round_\"><code>_foreach_round_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.round#torch.round\" title=\"torch.round\"><code>torch.round()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_sqrt#torch._foreach_sqrt\" title=\"torch._foreach_sqrt\"><code>_foreach_sqrt</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.sqrt#torch.sqrt\" title=\"torch.sqrt\"><code>torch.sqrt()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_sqrt_#torch._foreach_sqrt_\" title=\"torch._foreach_sqrt_\"><code>_foreach_sqrt_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.sqrt#torch.sqrt\" title=\"torch.sqrt\"><code>torch.sqrt()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_lgamma#torch._foreach_lgamma\" title=\"torch._foreach_lgamma\"><code>_foreach_lgamma</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.lgamma#torch.lgamma\" title=\"torch.lgamma\"><code>torch.lgamma()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_lgamma_#torch._foreach_lgamma_\" title=\"torch._foreach_lgamma_\"><code>_foreach_lgamma_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.lgamma#torch.lgamma\" title=\"torch.lgamma\"><code>torch.lgamma()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_frac#torch._foreach_frac\" title=\"torch._foreach_frac\"><code>_foreach_frac</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.frac#torch.frac\" title=\"torch.frac\"><code>torch.frac()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_frac_#torch._foreach_frac_\" title=\"torch._foreach_frac_\"><code>_foreach_frac_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.frac#torch.frac\" title=\"torch.frac\"><code>torch.frac()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_reciprocal#torch._foreach_reciprocal\" title=\"torch._foreach_reciprocal\"><code>_foreach_reciprocal</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.reciprocal#torch.reciprocal\" title=\"torch.reciprocal\"><code>torch.reciprocal()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_reciprocal_#torch._foreach_reciprocal_\" title=\"torch._foreach_reciprocal_\"><code>_foreach_reciprocal_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.reciprocal#torch.reciprocal\" title=\"torch.reciprocal\"><code>torch.reciprocal()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_sigmoid#torch._foreach_sigmoid\" title=\"torch._foreach_sigmoid\"><code>_foreach_sigmoid</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.sigmoid#torch.sigmoid\" title=\"torch.sigmoid\"><code>torch.sigmoid()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_sigmoid_#torch._foreach_sigmoid_\" title=\"torch._foreach_sigmoid_\"><code>_foreach_sigmoid_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.sigmoid#torch.sigmoid\" title=\"torch.sigmoid\"><code>torch.sigmoid()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_trunc#torch._foreach_trunc\" title=\"torch._foreach_trunc\"><code>_foreach_trunc</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.trunc#torch.trunc\" title=\"torch.trunc\"><code>torch.trunc()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_trunc_#torch._foreach_trunc_\" title=\"torch._foreach_trunc_\"><code>_foreach_trunc_</code></a>\n</td> <td><p>Apply <a class=\"reference internal\" href=\"generated/torch.trunc#torch.trunc\" title=\"torch.trunc\"><code>torch.trunc()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._foreach_zero_#torch._foreach_zero_\" title=\"torch._foreach_zero_\"><code>_foreach_zero_</code></a>\n</td> <td><p>Apply <code>torch.zero()</code> to each Tensor of the input list.</p></td> </tr>  </table>    <h2 id=\"utilities\">Utilities</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.compiled_with_cxx11_abi#torch.compiled_with_cxx11_abi\" title=\"torch.compiled_with_cxx11_abi\"><code>compiled_with_cxx11_abi</code></a>\n</td> <td><p>Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.result_type#torch.result_type\" title=\"torch.result_type\"><code>result_type</code></a>\n</td> <td><p>Returns the <a class=\"reference internal\" href=\"tensor_attributes#torch.dtype\" title=\"torch.dtype\"><code>torch.dtype</code></a> that would result from performing an arithmetic operation on the provided input tensors.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.can_cast#torch.can_cast\" title=\"torch.can_cast\"><code>can_cast</code></a>\n</td> <td><p>Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion <a class=\"reference internal\" href=\"tensor_attributes#type-promotion-doc\"><span class=\"std std-ref\">documentation</span></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.promote_types#torch.promote_types\" title=\"torch.promote_types\"><code>promote_types</code></a>\n</td> <td><p>Returns the <a class=\"reference internal\" href=\"tensor_attributes#torch.dtype\" title=\"torch.dtype\"><code>torch.dtype</code></a> with the smallest size and scalar kind that is not smaller nor of lower kind than either <code>type1</code> or <code>type2</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.use_deterministic_algorithms#torch.use_deterministic_algorithms\" title=\"torch.use_deterministic_algorithms\"><code>use_deterministic_algorithms</code></a>\n</td> <td><p>Sets whether PyTorch operations must use \"deterministic\" algorithms.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.are_deterministic_algorithms_enabled#torch.are_deterministic_algorithms_enabled\" title=\"torch.are_deterministic_algorithms_enabled\"><code>are_deterministic_algorithms_enabled</code></a>\n</td> <td><p>Returns True if the global deterministic flag is turned on.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.is_deterministic_algorithms_warn_only_enabled#torch.is_deterministic_algorithms_warn_only_enabled\" title=\"torch.is_deterministic_algorithms_warn_only_enabled\"><code>is_deterministic_algorithms_warn_only_enabled</code></a>\n</td> <td><p>Returns True if the global deterministic flag is set to warn only.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.set_deterministic_debug_mode#torch.set_deterministic_debug_mode\" title=\"torch.set_deterministic_debug_mode\"><code>set_deterministic_debug_mode</code></a>\n</td> <td><p>Sets the debug mode for deterministic operations.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.get_deterministic_debug_mode#torch.get_deterministic_debug_mode\" title=\"torch.get_deterministic_debug_mode\"><code>get_deterministic_debug_mode</code></a>\n</td> <td><p>Returns the current value of the debug mode for deterministic operations.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.set_float32_matmul_precision#torch.set_float32_matmul_precision\" title=\"torch.set_float32_matmul_precision\"><code>set_float32_matmul_precision</code></a>\n</td> <td><p>Sets the internal precision of float32 matrix multiplications.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.get_float32_matmul_precision#torch.get_float32_matmul_precision\" title=\"torch.get_float32_matmul_precision\"><code>get_float32_matmul_precision</code></a>\n</td> <td><p>Returns the current value of float32 matrix multiplication precision.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.set_warn_always#torch.set_warn_always\" title=\"torch.set_warn_always\"><code>set_warn_always</code></a>\n</td> <td><p>When this flag is False (default) then some PyTorch warnings may only appear once per process.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.is_warn_always_enabled#torch.is_warn_always_enabled\" title=\"torch.is_warn_always_enabled\"><code>is_warn_always_enabled</code></a>\n</td> <td><p>Returns True if the global warn_always flag is turned on.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.vmap#torch.vmap\" title=\"torch.vmap\"><code>vmap</code></a>\n</td> <td><p>vmap is the vectorizing map; <code>vmap(func)</code> returns a new function that maps <code>func</code> over some dimension of the inputs.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._assert#torch._assert\" title=\"torch._assert\"><code>_assert</code></a>\n</td> <td><p>A wrapper around Python's assert which is symbolically traceable.</p></td> </tr>  </table>   <h2 id=\"symbolic-numbers\">Symbolic Numbers</h2> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.SymInt\">\n<code>class torch.SymInt(node)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch.html#SymInt\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Like an int (including magic methods), but redirects all operations on the wrapped node. This is used in particular to symbolically record operations in the symbolic shape workflow.</p> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.SymFloat\">\n<code>class torch.SymFloat(node)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch.html#SymFloat\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Like an float (including magic methods), but redirects all operations on the wrapped node. This is used in particular to symbolically record operations in the symbolic shape workflow.</p> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.SymBool\">\n<code>class torch.SymBool(node)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch.html#SymBool\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Like an bool (including magic methods), but redirects all operations on the wrapped node. This is used in particular to symbolically record operations in the symbolic shape workflow.</p> <p>Unlike regular bools, regular boolean operators will force extra guards instead of symbolically evaluate. Use the bitwise operators instead to handle this.</p> </dd>\n</dl> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sym_float#torch.sym_float\" title=\"torch.sym_float\"><code>sym_float</code></a>\n</td> <td><p>SymInt-aware utility for float casting.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sym_int#torch.sym_int\" title=\"torch.sym_int\"><code>sym_int</code></a>\n</td> <td><p>SymInt-aware utility for int casting.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sym_max#torch.sym_max\" title=\"torch.sym_max\"><code>sym_max</code></a>\n</td> <td><p>SymInt-aware utility for max().</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sym_min#torch.sym_min\" title=\"torch.sym_min\"><code>sym_min</code></a>\n</td> <td><p>SymInt-aware utility for max().</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sym_not#torch.sym_not\" title=\"torch.sym_not\"><code>sym_not</code></a>\n</td> <td><p>SymInt-aware utility for logical negation.</p></td> </tr>  </table>   <h2 id=\"export-path\">Export Path</h2>  <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This feature is a prototype and may have compatibility breaking changes in the future.</p> <p>export generated/exportdb/index</p> </div>   <h2 id=\"optimizations\">Optimizations</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.compile#torch.compile\" title=\"torch.compile\"><code>compile</code></a>\n</td> <td><p>Optimizes given model/function using TorchDynamo and specified backend.</p></td> </tr>  </table> <p><a class=\"reference external\" href=\"https://pytorch.org/docs/main/compile/index.html\">torch.compile documentation</a></p>   <h2 id=\"operator-tags\">Operator Tags</h2> <dl class=\"py class\" id=\"module-torch.utils.viz\"> <dt class=\"sig sig-object py\" id=\"torch.Tag\">\n<code>class torch.Tag</code> </dt> <dd>\n<p>Members:</p> <p>core</p> <p>data_dependent_output</p> <p>dynamic_output_shape</p> <p>generated</p> <p>inplace_view</p> <p>nondeterministic_bitwise</p> <p>nondeterministic_seeded</p> <p>pointwise</p> <p>view_copy</p> <dl class=\"py property\"> <dt class=\"sig sig-object py\" id=\"torch.Tag.name\">\n<code>property name</code> </dt> \n</dl> </dd>\n</dl><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href=\"https://github.com/pytorch/pytorch/blob/main/LICENSE\">LICENSE</a> file.<br>\n    <a href=\"https://pytorch.org/docs/2.1/torch.html\" class=\"_attribution-link\">https://pytorch.org/docs/2.1/torch.html</a>\n  </p>\n</div>\n","nn":"<h1 id=\"torch-nn\">torch.nn</h1> <p id=\"module-torch.nn\">These are the basic building blocks for graphs:</p>  <p class=\"topic-title\">torch.nn</p> <ul class=\"simple\"> <li><a class=\"reference internal\" href=\"#containers\" id=\"id2\">Containers</a></li> <li><a class=\"reference internal\" href=\"#convolution-layers\" id=\"id3\">Convolution Layers</a></li> <li><a class=\"reference internal\" href=\"#pooling-layers\" id=\"id4\">Pooling layers</a></li> <li><a class=\"reference internal\" href=\"#padding-layers\" id=\"id5\">Padding Layers</a></li> <li><a class=\"reference internal\" href=\"#non-linear-activations-weighted-sum-nonlinearity\" id=\"id6\">Non-linear Activations (weighted sum, nonlinearity)</a></li> <li><a class=\"reference internal\" href=\"#non-linear-activations-other\" id=\"id7\">Non-linear Activations (other)</a></li> <li><a class=\"reference internal\" href=\"#normalization-layers\" id=\"id8\">Normalization Layers</a></li> <li><a class=\"reference internal\" href=\"#recurrent-layers\" id=\"id9\">Recurrent Layers</a></li> <li><a class=\"reference internal\" href=\"#transformer-layers\" id=\"id10\">Transformer Layers</a></li> <li><a class=\"reference internal\" href=\"#linear-layers\" id=\"id11\">Linear Layers</a></li> <li><a class=\"reference internal\" href=\"#dropout-layers\" id=\"id12\">Dropout Layers</a></li> <li><a class=\"reference internal\" href=\"#sparse-layers\" id=\"id13\">Sparse Layers</a></li> <li><a class=\"reference internal\" href=\"#distance-functions\" id=\"id14\">Distance Functions</a></li> <li><a class=\"reference internal\" href=\"#loss-functions\" id=\"id15\">Loss Functions</a></li> <li><a class=\"reference internal\" href=\"#vision-layers\" id=\"id16\">Vision Layers</a></li> <li><a class=\"reference internal\" href=\"#shuffle-layers\" id=\"id17\">Shuffle Layers</a></li> <li><a class=\"reference internal\" href=\"#module-torch.nn.parallel\" id=\"id18\">DataParallel Layers (multi-GPU, distributed)</a></li> <li><a class=\"reference internal\" href=\"#module-torch.nn.utils\" id=\"id19\">Utilities</a></li> <li><a class=\"reference internal\" href=\"#quantized-functions\" id=\"id20\">Quantized Functions</a></li> <li><a class=\"reference internal\" href=\"#lazy-modules-initialization\" id=\"id21\">Lazy Modules Initialization</a></li> </ul>  <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.parameter.parameter#torch.nn.parameter.Parameter\" title=\"torch.nn.parameter.Parameter\"><code>Parameter</code></a>\n</td> <td><p>A kind of Tensor that is to be considered a module parameter.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.parameter.uninitializedparameter#torch.nn.parameter.UninitializedParameter\" title=\"torch.nn.parameter.UninitializedParameter\"><code>UninitializedParameter</code></a>\n</td> <td><p>A parameter that is not initialized.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.parameter.uninitializedbuffer#torch.nn.parameter.UninitializedBuffer\" title=\"torch.nn.parameter.UninitializedBuffer\"><code>UninitializedBuffer</code></a>\n</td> <td><p>A buffer that is not initialized.</p></td> </tr>  </table>  <h2 id=\"containers\">Containers</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.module#torch.nn.Module\" title=\"torch.nn.Module\"><code>Module</code></a>\n</td> <td><p>Base class for all neural network modules.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.sequential#torch.nn.Sequential\" title=\"torch.nn.Sequential\"><code>Sequential</code></a>\n</td> <td><p>A sequential container.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.modulelist#torch.nn.ModuleList\" title=\"torch.nn.ModuleList\"><code>ModuleList</code></a>\n</td> <td><p>Holds submodules in a list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.moduledict#torch.nn.ModuleDict\" title=\"torch.nn.ModuleDict\"><code>ModuleDict</code></a>\n</td> <td><p>Holds submodules in a dictionary.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.parameterlist#torch.nn.ParameterList\" title=\"torch.nn.ParameterList\"><code>ParameterList</code></a>\n</td> <td><p>Holds parameters in a list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.parameterdict#torch.nn.ParameterDict\" title=\"torch.nn.ParameterDict\"><code>ParameterDict</code></a>\n</td> <td><p>Holds parameters in a dictionary.</p></td> </tr>  </table> <p>Global Hooks For Module</p> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.modules.module.register_module_forward_pre_hook#torch.nn.modules.module.register_module_forward_pre_hook\" title=\"torch.nn.modules.module.register_module_forward_pre_hook\"><code>register_module_forward_pre_hook</code></a>\n</td> <td><p>Registers a forward pre-hook common to all modules.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.modules.module.register_module_forward_hook#torch.nn.modules.module.register_module_forward_hook\" title=\"torch.nn.modules.module.register_module_forward_hook\"><code>register_module_forward_hook</code></a>\n</td> <td><p>Registers a global forward hook for all the modules</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.modules.module.register_module_backward_hook#torch.nn.modules.module.register_module_backward_hook\" title=\"torch.nn.modules.module.register_module_backward_hook\"><code>register_module_backward_hook</code></a>\n</td> <td><p>Registers a backward hook common to all the modules.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.modules.module.register_module_full_backward_pre_hook#torch.nn.modules.module.register_module_full_backward_pre_hook\" title=\"torch.nn.modules.module.register_module_full_backward_pre_hook\"><code>register_module_full_backward_pre_hook</code></a>\n</td> <td><p>Registers a backward pre-hook common to all the modules.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.modules.module.register_module_full_backward_hook#torch.nn.modules.module.register_module_full_backward_hook\" title=\"torch.nn.modules.module.register_module_full_backward_hook\"><code>register_module_full_backward_hook</code></a>\n</td> <td><p>Registers a backward hook common to all the modules.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.modules.module.register_module_buffer_registration_hook#torch.nn.modules.module.register_module_buffer_registration_hook\" title=\"torch.nn.modules.module.register_module_buffer_registration_hook\"><code>register_module_buffer_registration_hook</code></a>\n</td> <td><p>Registers a buffer registration hook common to all modules.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.modules.module.register_module_module_registration_hook#torch.nn.modules.module.register_module_module_registration_hook\" title=\"torch.nn.modules.module.register_module_module_registration_hook\"><code>register_module_module_registration_hook</code></a>\n</td> <td><p>Registers a module registration hook common to all modules.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.modules.module.register_module_parameter_registration_hook#torch.nn.modules.module.register_module_parameter_registration_hook\" title=\"torch.nn.modules.module.register_module_parameter_registration_hook\"><code>register_module_parameter_registration_hook</code></a>\n</td> <td><p>Registers a parameter registration hook common to all modules.</p></td> </tr>  </table>   <h2 id=\"convolution-layers\">Convolution Layers</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.conv1d#torch.nn.Conv1d\" title=\"torch.nn.Conv1d\"><code>nn.Conv1d</code></a></p></td> <td><p>Applies a 1D convolution over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.conv2d#torch.nn.Conv2d\" title=\"torch.nn.Conv2d\"><code>nn.Conv2d</code></a></p></td> <td><p>Applies a 2D convolution over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.conv3d#torch.nn.Conv3d\" title=\"torch.nn.Conv3d\"><code>nn.Conv3d</code></a></p></td> <td><p>Applies a 3D convolution over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.convtranspose1d#torch.nn.ConvTranspose1d\" title=\"torch.nn.ConvTranspose1d\"><code>nn.ConvTranspose1d</code></a></p></td> <td><p>Applies a 1D transposed convolution operator over an input image composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.convtranspose2d#torch.nn.ConvTranspose2d\" title=\"torch.nn.ConvTranspose2d\"><code>nn.ConvTranspose2d</code></a></p></td> <td><p>Applies a 2D transposed convolution operator over an input image composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.convtranspose3d#torch.nn.ConvTranspose3d\" title=\"torch.nn.ConvTranspose3d\"><code>nn.ConvTranspose3d</code></a></p></td> <td><p>Applies a 3D transposed convolution operator over an input image composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lazyconv1d#torch.nn.LazyConv1d\" title=\"torch.nn.LazyConv1d\"><code>nn.LazyConv1d</code></a></p></td> <td><p>A <a class=\"reference internal\" href=\"generated/torch.nn.conv1d#torch.nn.Conv1d\" title=\"torch.nn.Conv1d\"><code>torch.nn.Conv1d</code></a> module with lazy initialization of the <code>in_channels</code> argument of the <code>Conv1d</code> that is inferred from the <code>input.size(1)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lazyconv2d#torch.nn.LazyConv2d\" title=\"torch.nn.LazyConv2d\"><code>nn.LazyConv2d</code></a></p></td> <td><p>A <a class=\"reference internal\" href=\"generated/torch.nn.conv2d#torch.nn.Conv2d\" title=\"torch.nn.Conv2d\"><code>torch.nn.Conv2d</code></a> module with lazy initialization of the <code>in_channels</code> argument of the <code>Conv2d</code> that is inferred from the <code>input.size(1)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lazyconv3d#torch.nn.LazyConv3d\" title=\"torch.nn.LazyConv3d\"><code>nn.LazyConv3d</code></a></p></td> <td><p>A <a class=\"reference internal\" href=\"generated/torch.nn.conv3d#torch.nn.Conv3d\" title=\"torch.nn.Conv3d\"><code>torch.nn.Conv3d</code></a> module with lazy initialization of the <code>in_channels</code> argument of the <code>Conv3d</code> that is inferred from the <code>input.size(1)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lazyconvtranspose1d#torch.nn.LazyConvTranspose1d\" title=\"torch.nn.LazyConvTranspose1d\"><code>nn.LazyConvTranspose1d</code></a></p></td> <td><p>A <a class=\"reference internal\" href=\"generated/torch.nn.convtranspose1d#torch.nn.ConvTranspose1d\" title=\"torch.nn.ConvTranspose1d\"><code>torch.nn.ConvTranspose1d</code></a> module with lazy initialization of the <code>in_channels</code> argument of the <code>ConvTranspose1d</code> that is inferred from the <code>input.size(1)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lazyconvtranspose2d#torch.nn.LazyConvTranspose2d\" title=\"torch.nn.LazyConvTranspose2d\"><code>nn.LazyConvTranspose2d</code></a></p></td> <td><p>A <a class=\"reference internal\" href=\"generated/torch.nn.convtranspose2d#torch.nn.ConvTranspose2d\" title=\"torch.nn.ConvTranspose2d\"><code>torch.nn.ConvTranspose2d</code></a> module with lazy initialization of the <code>in_channels</code> argument of the <code>ConvTranspose2d</code> that is inferred from the <code>input.size(1)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lazyconvtranspose3d#torch.nn.LazyConvTranspose3d\" title=\"torch.nn.LazyConvTranspose3d\"><code>nn.LazyConvTranspose3d</code></a></p></td> <td><p>A <a class=\"reference internal\" href=\"generated/torch.nn.convtranspose3d#torch.nn.ConvTranspose3d\" title=\"torch.nn.ConvTranspose3d\"><code>torch.nn.ConvTranspose3d</code></a> module with lazy initialization of the <code>in_channels</code> argument of the <code>ConvTranspose3d</code> that is inferred from the <code>input.size(1)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.unfold#torch.nn.Unfold\" title=\"torch.nn.Unfold\"><code>nn.Unfold</code></a></p></td> <td><p>Extracts sliding local blocks from a batched input tensor.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.fold#torch.nn.Fold\" title=\"torch.nn.Fold\"><code>nn.Fold</code></a></p></td> <td><p>Combines an array of sliding local blocks into a large containing tensor.</p></td> </tr>  </table>   <h2 id=\"pooling-layers\">Pooling layers</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.maxpool1d#torch.nn.MaxPool1d\" title=\"torch.nn.MaxPool1d\"><code>nn.MaxPool1d</code></a></p></td> <td><p>Applies a 1D max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.maxpool2d#torch.nn.MaxPool2d\" title=\"torch.nn.MaxPool2d\"><code>nn.MaxPool2d</code></a></p></td> <td><p>Applies a 2D max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.maxpool3d#torch.nn.MaxPool3d\" title=\"torch.nn.MaxPool3d\"><code>nn.MaxPool3d</code></a></p></td> <td><p>Applies a 3D max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.maxunpool1d#torch.nn.MaxUnpool1d\" title=\"torch.nn.MaxUnpool1d\"><code>nn.MaxUnpool1d</code></a></p></td> <td><p>Computes a partial inverse of <code>MaxPool1d</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.maxunpool2d#torch.nn.MaxUnpool2d\" title=\"torch.nn.MaxUnpool2d\"><code>nn.MaxUnpool2d</code></a></p></td> <td><p>Computes a partial inverse of <code>MaxPool2d</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.maxunpool3d#torch.nn.MaxUnpool3d\" title=\"torch.nn.MaxUnpool3d\"><code>nn.MaxUnpool3d</code></a></p></td> <td><p>Computes a partial inverse of <code>MaxPool3d</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.avgpool1d#torch.nn.AvgPool1d\" title=\"torch.nn.AvgPool1d\"><code>nn.AvgPool1d</code></a></p></td> <td><p>Applies a 1D average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.avgpool2d#torch.nn.AvgPool2d\" title=\"torch.nn.AvgPool2d\"><code>nn.AvgPool2d</code></a></p></td> <td><p>Applies a 2D average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.avgpool3d#torch.nn.AvgPool3d\" title=\"torch.nn.AvgPool3d\"><code>nn.AvgPool3d</code></a></p></td> <td><p>Applies a 3D average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.fractionalmaxpool2d#torch.nn.FractionalMaxPool2d\" title=\"torch.nn.FractionalMaxPool2d\"><code>nn.FractionalMaxPool2d</code></a></p></td> <td><p>Applies a 2D fractional max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.fractionalmaxpool3d#torch.nn.FractionalMaxPool3d\" title=\"torch.nn.FractionalMaxPool3d\"><code>nn.FractionalMaxPool3d</code></a></p></td> <td><p>Applies a 3D fractional max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lppool1d#torch.nn.LPPool1d\" title=\"torch.nn.LPPool1d\"><code>nn.LPPool1d</code></a></p></td> <td><p>Applies a 1D power-average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lppool2d#torch.nn.LPPool2d\" title=\"torch.nn.LPPool2d\"><code>nn.LPPool2d</code></a></p></td> <td><p>Applies a 2D power-average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.adaptivemaxpool1d#torch.nn.AdaptiveMaxPool1d\" title=\"torch.nn.AdaptiveMaxPool1d\"><code>nn.AdaptiveMaxPool1d</code></a></p></td> <td><p>Applies a 1D adaptive max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.adaptivemaxpool2d#torch.nn.AdaptiveMaxPool2d\" title=\"torch.nn.AdaptiveMaxPool2d\"><code>nn.AdaptiveMaxPool2d</code></a></p></td> <td><p>Applies a 2D adaptive max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.adaptivemaxpool3d#torch.nn.AdaptiveMaxPool3d\" title=\"torch.nn.AdaptiveMaxPool3d\"><code>nn.AdaptiveMaxPool3d</code></a></p></td> <td><p>Applies a 3D adaptive max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.adaptiveavgpool1d#torch.nn.AdaptiveAvgPool1d\" title=\"torch.nn.AdaptiveAvgPool1d\"><code>nn.AdaptiveAvgPool1d</code></a></p></td> <td><p>Applies a 1D adaptive average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.adaptiveavgpool2d#torch.nn.AdaptiveAvgPool2d\" title=\"torch.nn.AdaptiveAvgPool2d\"><code>nn.AdaptiveAvgPool2d</code></a></p></td> <td><p>Applies a 2D adaptive average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.adaptiveavgpool3d#torch.nn.AdaptiveAvgPool3d\" title=\"torch.nn.AdaptiveAvgPool3d\"><code>nn.AdaptiveAvgPool3d</code></a></p></td> <td><p>Applies a 3D adaptive average pooling over an input signal composed of several input planes.</p></td> </tr>  </table>   <h2 id=\"padding-layers\">Padding Layers</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.reflectionpad1d#torch.nn.ReflectionPad1d\" title=\"torch.nn.ReflectionPad1d\"><code>nn.ReflectionPad1d</code></a></p></td> <td><p>Pads the input tensor using the reflection of the input boundary.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.reflectionpad2d#torch.nn.ReflectionPad2d\" title=\"torch.nn.ReflectionPad2d\"><code>nn.ReflectionPad2d</code></a></p></td> <td><p>Pads the input tensor using the reflection of the input boundary.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.reflectionpad3d#torch.nn.ReflectionPad3d\" title=\"torch.nn.ReflectionPad3d\"><code>nn.ReflectionPad3d</code></a></p></td> <td><p>Pads the input tensor using the reflection of the input boundary.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.replicationpad1d#torch.nn.ReplicationPad1d\" title=\"torch.nn.ReplicationPad1d\"><code>nn.ReplicationPad1d</code></a></p></td> <td><p>Pads the input tensor using replication of the input boundary.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.replicationpad2d#torch.nn.ReplicationPad2d\" title=\"torch.nn.ReplicationPad2d\"><code>nn.ReplicationPad2d</code></a></p></td> <td><p>Pads the input tensor using replication of the input boundary.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.replicationpad3d#torch.nn.ReplicationPad3d\" title=\"torch.nn.ReplicationPad3d\"><code>nn.ReplicationPad3d</code></a></p></td> <td><p>Pads the input tensor using replication of the input boundary.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.zeropad1d#torch.nn.ZeroPad1d\" title=\"torch.nn.ZeroPad1d\"><code>nn.ZeroPad1d</code></a></p></td> <td><p>Pads the input tensor boundaries with zero.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.zeropad2d#torch.nn.ZeroPad2d\" title=\"torch.nn.ZeroPad2d\"><code>nn.ZeroPad2d</code></a></p></td> <td><p>Pads the input tensor boundaries with zero.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.zeropad3d#torch.nn.ZeroPad3d\" title=\"torch.nn.ZeroPad3d\"><code>nn.ZeroPad3d</code></a></p></td> <td><p>Pads the input tensor boundaries with zero.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.constantpad1d#torch.nn.ConstantPad1d\" title=\"torch.nn.ConstantPad1d\"><code>nn.ConstantPad1d</code></a></p></td> <td><p>Pads the input tensor boundaries with a constant value.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.constantpad2d#torch.nn.ConstantPad2d\" title=\"torch.nn.ConstantPad2d\"><code>nn.ConstantPad2d</code></a></p></td> <td><p>Pads the input tensor boundaries with a constant value.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.constantpad3d#torch.nn.ConstantPad3d\" title=\"torch.nn.ConstantPad3d\"><code>nn.ConstantPad3d</code></a></p></td> <td><p>Pads the input tensor boundaries with a constant value.</p></td> </tr>  </table>   <h2 id=\"non-linear-activations-weighted-sum-nonlinearity\">Non-linear Activations (weighted sum, nonlinearity)</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.elu#torch.nn.ELU\" title=\"torch.nn.ELU\"><code>nn.ELU</code></a></p></td> <td><p>Applies the Exponential Linear Unit (ELU) function, element-wise, as described in the paper: <a class=\"reference external\" href=\"https://arxiv.org/abs/1511.07289\">Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)</a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.hardshrink#torch.nn.Hardshrink\" title=\"torch.nn.Hardshrink\"><code>nn.Hardshrink</code></a></p></td> <td><p>Applies the Hard Shrinkage (Hardshrink) function element-wise.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.hardsigmoid#torch.nn.Hardsigmoid\" title=\"torch.nn.Hardsigmoid\"><code>nn.Hardsigmoid</code></a></p></td> <td><p>Applies the Hardsigmoid function element-wise.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.hardtanh#torch.nn.Hardtanh\" title=\"torch.nn.Hardtanh\"><code>nn.Hardtanh</code></a></p></td> <td><p>Applies the HardTanh function element-wise.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.hardswish#torch.nn.Hardswish\" title=\"torch.nn.Hardswish\"><code>nn.Hardswish</code></a></p></td> <td><p>Applies the Hardswish function, element-wise, as described in the paper: <a class=\"reference external\" href=\"https://arxiv.org/abs/1905.02244\">Searching for MobileNetV3</a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.leakyrelu#torch.nn.LeakyReLU\" title=\"torch.nn.LeakyReLU\"><code>nn.LeakyReLU</code></a></p></td> <td><p>Applies the element-wise function:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.logsigmoid#torch.nn.LogSigmoid\" title=\"torch.nn.LogSigmoid\"><code>nn.LogSigmoid</code></a></p></td> <td><p>Applies the element-wise function:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.multiheadattention#torch.nn.MultiheadAttention\" title=\"torch.nn.MultiheadAttention\"><code>nn.MultiheadAttention</code></a></p></td> <td><p>Allows the model to jointly attend to information from different representation subspaces as described in the paper: <a class=\"reference external\" href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.prelu#torch.nn.PReLU\" title=\"torch.nn.PReLU\"><code>nn.PReLU</code></a></p></td> <td><p>Applies the element-wise function:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.relu#torch.nn.ReLU\" title=\"torch.nn.ReLU\"><code>nn.ReLU</code></a></p></td> <td><p>Applies the rectified linear unit function element-wise:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.relu6#torch.nn.ReLU6\" title=\"torch.nn.ReLU6\"><code>nn.ReLU6</code></a></p></td> <td><p>Applies the element-wise function:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.rrelu#torch.nn.RReLU\" title=\"torch.nn.RReLU\"><code>nn.RReLU</code></a></p></td> <td><p>Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.selu#torch.nn.SELU\" title=\"torch.nn.SELU\"><code>nn.SELU</code></a></p></td> <td><p>Applied element-wise, as:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.celu#torch.nn.CELU\" title=\"torch.nn.CELU\"><code>nn.CELU</code></a></p></td> <td><p>Applies the element-wise function:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.gelu#torch.nn.GELU\" title=\"torch.nn.GELU\"><code>nn.GELU</code></a></p></td> <td><p>Applies the Gaussian Error Linear Units function:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.sigmoid#torch.nn.Sigmoid\" title=\"torch.nn.Sigmoid\"><code>nn.Sigmoid</code></a></p></td> <td><p>Applies the element-wise function:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.silu#torch.nn.SiLU\" title=\"torch.nn.SiLU\"><code>nn.SiLU</code></a></p></td> <td><p>Applies the Sigmoid Linear Unit (SiLU) function, element-wise.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.mish#torch.nn.Mish\" title=\"torch.nn.Mish\"><code>nn.Mish</code></a></p></td> <td><p>Applies the Mish function, element-wise.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.softplus#torch.nn.Softplus\" title=\"torch.nn.Softplus\"><code>nn.Softplus</code></a></p></td> <td><p>Applies the Softplus function <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>Softplus</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mn>1</mn><mi></mi></mfrac><mo></mo><mi>log</mi><mo></mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>+</mo><mi>exp</mi><mo></mo><mo stretchy=\"false\">(</mo><mi></mi><mo></mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))</annotation></semantics></math></span></span></span> element-wise.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.softshrink#torch.nn.Softshrink\" title=\"torch.nn.Softshrink\"><code>nn.Softshrink</code></a></p></td> <td><p>Applies the soft shrinkage function elementwise:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.softsign#torch.nn.Softsign\" title=\"torch.nn.Softsign\"><code>nn.Softsign</code></a></p></td> <td><p>Applies the element-wise function:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.tanh#torch.nn.Tanh\" title=\"torch.nn.Tanh\"><code>nn.Tanh</code></a></p></td> <td><p>Applies the Hyperbolic Tangent (Tanh) function element-wise.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.tanhshrink#torch.nn.Tanhshrink\" title=\"torch.nn.Tanhshrink\"><code>nn.Tanhshrink</code></a></p></td> <td><p>Applies the element-wise function:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.threshold#torch.nn.Threshold\" title=\"torch.nn.Threshold\"><code>nn.Threshold</code></a></p></td> <td><p>Thresholds each element of the input Tensor.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.glu#torch.nn.GLU\" title=\"torch.nn.GLU\"><code>nn.GLU</code></a></p></td> <td><p>Applies the gated linear unit function <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mrow><mi>G</mi><mi>L</mi><mi>U</mi></mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo separator=\"true\">,</mo><mi>b</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>a</mi><mo></mo><mi></mi><mo stretchy=\"false\">(</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">{GLU}(a, b)= a \\otimes \\sigma(b)</annotation></semantics></math></span></span></span> where <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>a</mi></mrow><annotation encoding=\"application/x-tex\">a</annotation></semantics></math></span></span></span> is the first half of the input matrices and <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">b</annotation></semantics></math></span></span></span> is the second half.</p></td> </tr>  </table>   <h2 id=\"non-linear-activations-other\">Non-linear Activations (other)</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.softmin#torch.nn.Softmin\" title=\"torch.nn.Softmin\"><code>nn.Softmin</code></a></p></td> <td><p>Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range <code>[0, 1]</code> and sum to 1.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.softmax#torch.nn.Softmax\" title=\"torch.nn.Softmax\"><code>nn.Softmax</code></a></p></td> <td><p>Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.softmax2d#torch.nn.Softmax2d\" title=\"torch.nn.Softmax2d\"><code>nn.Softmax2d</code></a></p></td> <td><p>Applies SoftMax over features to each spatial location.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.logsoftmax#torch.nn.LogSoftmax\" title=\"torch.nn.LogSoftmax\"><code>nn.LogSoftmax</code></a></p></td> <td><p>Applies the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>log</mi><mo></mo><mo stretchy=\"false\">(</mo><mtext>Softmax</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\log(\\text{Softmax}(x))</annotation></semantics></math></span></span></span> function to an n-dimensional input Tensor.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.adaptivelogsoftmaxwithloss#torch.nn.AdaptiveLogSoftmaxWithLoss\" title=\"torch.nn.AdaptiveLogSoftmaxWithLoss\"><code>nn.AdaptiveLogSoftmaxWithLoss</code></a></p></td> <td><p>Efficient softmax approximation as described in <a class=\"reference external\" href=\"https://arxiv.org/abs/1609.04309\">Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss, David Grangier, and Herv Jgou</a>.</p></td> </tr>  </table>   <h2 id=\"normalization-layers\">Normalization Layers</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.batchnorm1d#torch.nn.BatchNorm1d\" title=\"torch.nn.BatchNorm1d\"><code>nn.BatchNorm1d</code></a></p></td> <td><p>Applies Batch Normalization over a 2D or 3D input as described in the paper <a class=\"reference external\" href=\"https://arxiv.org/abs/1502.03167\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> .</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d\" title=\"torch.nn.BatchNorm2d\"><code>nn.BatchNorm2d</code></a></p></td> <td><p>Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper <a class=\"reference external\" href=\"https://arxiv.org/abs/1502.03167\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> .</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.batchnorm3d#torch.nn.BatchNorm3d\" title=\"torch.nn.BatchNorm3d\"><code>nn.BatchNorm3d</code></a></p></td> <td><p>Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper <a class=\"reference external\" href=\"https://arxiv.org/abs/1502.03167\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> .</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lazybatchnorm1d#torch.nn.LazyBatchNorm1d\" title=\"torch.nn.LazyBatchNorm1d\"><code>nn.LazyBatchNorm1d</code></a></p></td> <td><p>A <a class=\"reference internal\" href=\"generated/torch.nn.batchnorm1d#torch.nn.BatchNorm1d\" title=\"torch.nn.BatchNorm1d\"><code>torch.nn.BatchNorm1d</code></a> module with lazy initialization of the <code>num_features</code> argument of the <code>BatchNorm1d</code> that is inferred from the <code>input.size(1)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lazybatchnorm2d#torch.nn.LazyBatchNorm2d\" title=\"torch.nn.LazyBatchNorm2d\"><code>nn.LazyBatchNorm2d</code></a></p></td> <td><p>A <a class=\"reference internal\" href=\"generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d\" title=\"torch.nn.BatchNorm2d\"><code>torch.nn.BatchNorm2d</code></a> module with lazy initialization of the <code>num_features</code> argument of the <code>BatchNorm2d</code> that is inferred from the <code>input.size(1)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lazybatchnorm3d#torch.nn.LazyBatchNorm3d\" title=\"torch.nn.LazyBatchNorm3d\"><code>nn.LazyBatchNorm3d</code></a></p></td> <td><p>A <a class=\"reference internal\" href=\"generated/torch.nn.batchnorm3d#torch.nn.BatchNorm3d\" title=\"torch.nn.BatchNorm3d\"><code>torch.nn.BatchNorm3d</code></a> module with lazy initialization of the <code>num_features</code> argument of the <code>BatchNorm3d</code> that is inferred from the <code>input.size(1)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.groupnorm#torch.nn.GroupNorm\" title=\"torch.nn.GroupNorm\"><code>nn.GroupNorm</code></a></p></td> <td><p>Applies Group Normalization over a mini-batch of inputs as described in the paper <a class=\"reference external\" href=\"https://arxiv.org/abs/1803.08494\">Group Normalization</a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.syncbatchnorm#torch.nn.SyncBatchNorm\" title=\"torch.nn.SyncBatchNorm\"><code>nn.SyncBatchNorm</code></a></p></td> <td><p>Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper <a class=\"reference external\" href=\"https://arxiv.org/abs/1502.03167\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> .</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.instancenorm1d#torch.nn.InstanceNorm1d\" title=\"torch.nn.InstanceNorm1d\"><code>nn.InstanceNorm1d</code></a></p></td> <td><p>Applies Instance Normalization over a 2D (unbatched) or 3D (batched) input as described in the paper <a class=\"reference external\" href=\"https://arxiv.org/abs/1607.08022\">Instance Normalization: The Missing Ingredient for Fast Stylization</a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.instancenorm2d#torch.nn.InstanceNorm2d\" title=\"torch.nn.InstanceNorm2d\"><code>nn.InstanceNorm2d</code></a></p></td> <td><p>Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper <a class=\"reference external\" href=\"https://arxiv.org/abs/1607.08022\">Instance Normalization: The Missing Ingredient for Fast Stylization</a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.instancenorm3d#torch.nn.InstanceNorm3d\" title=\"torch.nn.InstanceNorm3d\"><code>nn.InstanceNorm3d</code></a></p></td> <td><p>Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper <a class=\"reference external\" href=\"https://arxiv.org/abs/1607.08022\">Instance Normalization: The Missing Ingredient for Fast Stylization</a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lazyinstancenorm1d#torch.nn.LazyInstanceNorm1d\" title=\"torch.nn.LazyInstanceNorm1d\"><code>nn.LazyInstanceNorm1d</code></a></p></td> <td><p>A <a class=\"reference internal\" href=\"generated/torch.nn.instancenorm1d#torch.nn.InstanceNorm1d\" title=\"torch.nn.InstanceNorm1d\"><code>torch.nn.InstanceNorm1d</code></a> module with lazy initialization of the <code>num_features</code> argument of the <code>InstanceNorm1d</code> that is inferred from the <code>input.size(1)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lazyinstancenorm2d#torch.nn.LazyInstanceNorm2d\" title=\"torch.nn.LazyInstanceNorm2d\"><code>nn.LazyInstanceNorm2d</code></a></p></td> <td><p>A <a class=\"reference internal\" href=\"generated/torch.nn.instancenorm2d#torch.nn.InstanceNorm2d\" title=\"torch.nn.InstanceNorm2d\"><code>torch.nn.InstanceNorm2d</code></a> module with lazy initialization of the <code>num_features</code> argument of the <code>InstanceNorm2d</code> that is inferred from the <code>input.size(1)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lazyinstancenorm3d#torch.nn.LazyInstanceNorm3d\" title=\"torch.nn.LazyInstanceNorm3d\"><code>nn.LazyInstanceNorm3d</code></a></p></td> <td><p>A <a class=\"reference internal\" href=\"generated/torch.nn.instancenorm3d#torch.nn.InstanceNorm3d\" title=\"torch.nn.InstanceNorm3d\"><code>torch.nn.InstanceNorm3d</code></a> module with lazy initialization of the <code>num_features</code> argument of the <code>InstanceNorm3d</code> that is inferred from the <code>input.size(1)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.layernorm#torch.nn.LayerNorm\" title=\"torch.nn.LayerNorm\"><code>nn.LayerNorm</code></a></p></td> <td><p>Applies Layer Normalization over a mini-batch of inputs as described in the paper <a class=\"reference external\" href=\"https://arxiv.org/abs/1607.06450\">Layer Normalization</a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.localresponsenorm#torch.nn.LocalResponseNorm\" title=\"torch.nn.LocalResponseNorm\"><code>nn.LocalResponseNorm</code></a></p></td> <td><p>Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.</p></td> </tr>  </table>   <h2 id=\"recurrent-layers\">Recurrent Layers</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.rnnbase#torch.nn.RNNBase\" title=\"torch.nn.RNNBase\"><code>nn.RNNBase</code></a></p></td> <td><p>Base class for RNN modules (RNN, LSTM, GRU).</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.rnn#torch.nn.RNN\" title=\"torch.nn.RNN\"><code>nn.RNN</code></a></p></td> <td><p>Applies a multi-layer Elman RNN with <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>tanh</mi><mo></mo></mrow><annotation encoding=\"application/x-tex\">\\tanh</annotation></semantics></math></span></span></span> or <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>ReLU</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{ReLU}</annotation></semantics></math></span></span></span> non-linearity to an input sequence.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lstm#torch.nn.LSTM\" title=\"torch.nn.LSTM\"><code>nn.LSTM</code></a></p></td> <td><p>Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.gru#torch.nn.GRU\" title=\"torch.nn.GRU\"><code>nn.GRU</code></a></p></td> <td><p>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.rnncell#torch.nn.RNNCell\" title=\"torch.nn.RNNCell\"><code>nn.RNNCell</code></a></p></td> <td><p>An Elman RNN cell with tanh or ReLU non-linearity.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lstmcell#torch.nn.LSTMCell\" title=\"torch.nn.LSTMCell\"><code>nn.LSTMCell</code></a></p></td> <td><p>A long short-term memory (LSTM) cell.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.grucell#torch.nn.GRUCell\" title=\"torch.nn.GRUCell\"><code>nn.GRUCell</code></a></p></td> <td><p>A gated recurrent unit (GRU) cell</p></td> </tr>  </table>   <h2 id=\"transformer-layers\">Transformer Layers</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.transformer#torch.nn.Transformer\" title=\"torch.nn.Transformer\"><code>nn.Transformer</code></a></p></td> <td><p>A transformer model.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.transformerencoder#torch.nn.TransformerEncoder\" title=\"torch.nn.TransformerEncoder\"><code>nn.TransformerEncoder</code></a></p></td> <td><p>TransformerEncoder is a stack of N encoder layers.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.transformerdecoder#torch.nn.TransformerDecoder\" title=\"torch.nn.TransformerDecoder\"><code>nn.TransformerDecoder</code></a></p></td> <td><p>TransformerDecoder is a stack of N decoder layers</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.transformerencoderlayer#torch.nn.TransformerEncoderLayer\" title=\"torch.nn.TransformerEncoderLayer\"><code>nn.TransformerEncoderLayer</code></a></p></td> <td><p>TransformerEncoderLayer is made up of self-attn and feedforward network.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.transformerdecoderlayer#torch.nn.TransformerDecoderLayer\" title=\"torch.nn.TransformerDecoderLayer\"><code>nn.TransformerDecoderLayer</code></a></p></td> <td><p>TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.</p></td> </tr>  </table>   <h2 id=\"linear-layers\">Linear Layers</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.identity#torch.nn.Identity\" title=\"torch.nn.Identity\"><code>nn.Identity</code></a></p></td> <td><p>A placeholder identity operator that is argument-insensitive.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.linear#torch.nn.Linear\" title=\"torch.nn.Linear\"><code>nn.Linear</code></a></p></td> <td><p>Applies a linear transformation to the incoming data: <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi><mo>=</mo><mi>x</mi><msup><mi>A</mi><mi>T</mi></msup><mo>+</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">y = xA^T + b</annotation></semantics></math></span></span></span></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.bilinear#torch.nn.Bilinear\" title=\"torch.nn.Bilinear\"><code>nn.Bilinear</code></a></p></td> <td><p>Applies a bilinear transformation to the incoming data: <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi><mo>=</mo><msubsup><mi>x</mi><mn>1</mn><mi>T</mi></msubsup><mi>A</mi><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">y = x_1^T A x_2 + b</annotation></semantics></math></span></span></span></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lazylinear#torch.nn.LazyLinear\" title=\"torch.nn.LazyLinear\"><code>nn.LazyLinear</code></a></p></td> <td><p>A <a class=\"reference internal\" href=\"generated/torch.nn.linear#torch.nn.Linear\" title=\"torch.nn.Linear\"><code>torch.nn.Linear</code></a> module where <code>in_features</code> is inferred.</p></td> </tr>  </table>   <h2 id=\"dropout-layers\">Dropout Layers</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.dropout#torch.nn.Dropout\" title=\"torch.nn.Dropout\"><code>nn.Dropout</code></a></p></td> <td><p>During training, randomly zeroes some of the elements of the input tensor with probability <code>p</code> using samples from a Bernoulli distribution.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.dropout1d#torch.nn.Dropout1d\" title=\"torch.nn.Dropout1d\"><code>nn.Dropout1d</code></a></p></td> <td><p>Randomly zero out entire channels (a channel is a 1D feature map, e.g., the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span></span></span>-th channel of the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span></span></span>-th sample in the batched input is a 1D tensor <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo stretchy=\"false\">[</mo><mi>i</mi><mo separator=\"true\">,</mo><mi>j</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\text{input}[i, j]</annotation></semantics></math></span></span></span>).</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.dropout2d#torch.nn.Dropout2d\" title=\"torch.nn.Dropout2d\"><code>nn.Dropout2d</code></a></p></td> <td><p>Randomly zero out entire channels (a channel is a 2D feature map, e.g., the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span></span></span>-th channel of the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span></span></span>-th sample in the batched input is a 2D tensor <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo stretchy=\"false\">[</mo><mi>i</mi><mo separator=\"true\">,</mo><mi>j</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\text{input}[i, j]</annotation></semantics></math></span></span></span>).</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.dropout3d#torch.nn.Dropout3d\" title=\"torch.nn.Dropout3d\"><code>nn.Dropout3d</code></a></p></td> <td><p>Randomly zero out entire channels (a channel is a 3D feature map, e.g., the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span></span></span>-th channel of the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span></span></span>-th sample in the batched input is a 3D tensor <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo stretchy=\"false\">[</mo><mi>i</mi><mo separator=\"true\">,</mo><mi>j</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\text{input}[i, j]</annotation></semantics></math></span></span></span>).</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.alphadropout#torch.nn.AlphaDropout\" title=\"torch.nn.AlphaDropout\"><code>nn.AlphaDropout</code></a></p></td> <td><p>Applies Alpha Dropout over the input.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.featurealphadropout#torch.nn.FeatureAlphaDropout\" title=\"torch.nn.FeatureAlphaDropout\"><code>nn.FeatureAlphaDropout</code></a></p></td> <td><p>Randomly masks out entire channels (a channel is a feature map, e.g.</p></td> </tr>  </table>   <h2 id=\"sparse-layers\">Sparse Layers</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.embedding#torch.nn.Embedding\" title=\"torch.nn.Embedding\"><code>nn.Embedding</code></a></p></td> <td><p>A simple lookup table that stores embeddings of a fixed dictionary and size.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.embeddingbag#torch.nn.EmbeddingBag\" title=\"torch.nn.EmbeddingBag\"><code>nn.EmbeddingBag</code></a></p></td> <td><p>Computes sums or means of 'bags' of embeddings, without instantiating the intermediate embeddings.</p></td> </tr>  </table>   <h2 id=\"distance-functions\">Distance Functions</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.cosinesimilarity#torch.nn.CosineSimilarity\" title=\"torch.nn.CosineSimilarity\"><code>nn.CosineSimilarity</code></a></p></td> <td><p>Returns cosine similarity between <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">x_1</annotation></semantics></math></span></span></span> and <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>x</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">x_2</annotation></semantics></math></span></span></span>, computed along <code>dim</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.pairwisedistance#torch.nn.PairwiseDistance\" title=\"torch.nn.PairwiseDistance\"><code>nn.PairwiseDistance</code></a></p></td> <td><p>Computes the pairwise distance between input vectors, or between columns of input matrices.</p></td> </tr>  </table>   <h2 id=\"loss-functions\">Loss Functions</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.l1loss#torch.nn.L1Loss\" title=\"torch.nn.L1Loss\"><code>nn.L1Loss</code></a></p></td> <td><p>Creates a criterion that measures the mean absolute error (MAE) between each element in the input <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span></span></span> and target <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.mseloss#torch.nn.MSELoss\" title=\"torch.nn.MSELoss\"><code>nn.MSELoss</code></a></p></td> <td><p>Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span></span></span> and target <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.crossentropyloss#torch.nn.CrossEntropyLoss\" title=\"torch.nn.CrossEntropyLoss\"><code>nn.CrossEntropyLoss</code></a></p></td> <td><p>This criterion computes the cross entropy loss between input logits and target.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.ctcloss#torch.nn.CTCLoss\" title=\"torch.nn.CTCLoss\"><code>nn.CTCLoss</code></a></p></td> <td><p>The Connectionist Temporal Classification loss.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.nllloss#torch.nn.NLLLoss\" title=\"torch.nn.NLLLoss\"><code>nn.NLLLoss</code></a></p></td> <td><p>The negative log likelihood loss.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.poissonnllloss#torch.nn.PoissonNLLLoss\" title=\"torch.nn.PoissonNLLLoss\"><code>nn.PoissonNLLLoss</code></a></p></td> <td><p>Negative log likelihood loss with Poisson distribution of target.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.gaussiannllloss#torch.nn.GaussianNLLLoss\" title=\"torch.nn.GaussianNLLLoss\"><code>nn.GaussianNLLLoss</code></a></p></td> <td><p>Gaussian negative log likelihood loss.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.kldivloss#torch.nn.KLDivLoss\" title=\"torch.nn.KLDivLoss\"><code>nn.KLDivLoss</code></a></p></td> <td><p>The Kullback-Leibler divergence loss.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.bceloss#torch.nn.BCELoss\" title=\"torch.nn.BCELoss\"><code>nn.BCELoss</code></a></p></td> <td><p>Creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.bcewithlogitsloss#torch.nn.BCEWithLogitsLoss\" title=\"torch.nn.BCEWithLogitsLoss\"><code>nn.BCEWithLogitsLoss</code></a></p></td> <td><p>This loss combines a <code>Sigmoid</code> layer and the <code>BCELoss</code> in one single class.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.marginrankingloss#torch.nn.MarginRankingLoss\" title=\"torch.nn.MarginRankingLoss\"><code>nn.MarginRankingLoss</code></a></p></td> <td><p>Creates a criterion that measures the loss given inputs <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">x1</annotation></semantics></math></span></span></span>, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">x2</annotation></semantics></math></span></span></span>, two 1D mini-batch or 0D <code>Tensors</code>, and a label 1D mini-batch or 0D <code>Tensor</code> <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span></span></span> (containing 1 or -1).</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.hingeembeddingloss#torch.nn.HingeEmbeddingLoss\" title=\"torch.nn.HingeEmbeddingLoss\"><code>nn.HingeEmbeddingLoss</code></a></p></td> <td><p>Measures the loss given an input tensor <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span></span></span> and a labels tensor <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span></span></span> (containing 1 or -1).</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.multilabelmarginloss#torch.nn.MultiLabelMarginLoss\" title=\"torch.nn.MultiLabelMarginLoss\"><code>nn.MultiLabelMarginLoss</code></a></p></td> <td><p>Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span></span></span> (a 2D mini-batch <code>Tensor</code>) and output <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span></span></span> (which is a 2D <code>Tensor</code> of target class indices).</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.huberloss#torch.nn.HuberLoss\" title=\"torch.nn.HuberLoss\"><code>nn.HuberLoss</code></a></p></td> <td><p>Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.smoothl1loss#torch.nn.SmoothL1Loss\" title=\"torch.nn.SmoothL1Loss\"><code>nn.SmoothL1Loss</code></a></p></td> <td><p>Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.softmarginloss#torch.nn.SoftMarginLoss\" title=\"torch.nn.SoftMarginLoss\"><code>nn.SoftMarginLoss</code></a></p></td> <td><p>Creates a criterion that optimizes a two-class classification logistic loss between input tensor <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span></span></span> and target tensor <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span></span></span> (containing 1 or -1).</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.multilabelsoftmarginloss#torch.nn.MultiLabelSoftMarginLoss\" title=\"torch.nn.MultiLabelSoftMarginLoss\"><code>nn.MultiLabelSoftMarginLoss</code></a></p></td> <td><p>Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span></span></span> and target <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span></span></span> of size <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, C)</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.cosineembeddingloss#torch.nn.CosineEmbeddingLoss\" title=\"torch.nn.CosineEmbeddingLoss\"><code>nn.CosineEmbeddingLoss</code></a></p></td> <td><p>Creates a criterion that measures the loss given input tensors <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">x_1</annotation></semantics></math></span></span></span>, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>x</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">x_2</annotation></semantics></math></span></span></span> and a <code>Tensor</code> label <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span></span></span> with values 1 or -1.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.multimarginloss#torch.nn.MultiMarginLoss\" title=\"torch.nn.MultiMarginLoss\"><code>nn.MultiMarginLoss</code></a></p></td> <td><p>Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span></span></span> (a 2D mini-batch <code>Tensor</code>) and output <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span></span></span> (which is a 1D tensor of target class indices, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn><mo></mo><mi>y</mi><mo></mo><mtext>x.size</mtext><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo><mo></mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">0 \\leq y \\leq \\text{x.size}(1)-1</annotation></semantics></math></span></span></span>):</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.tripletmarginloss#torch.nn.TripletMarginLoss\" title=\"torch.nn.TripletMarginLoss\"><code>nn.TripletMarginLoss</code></a></p></td> <td><p>Creates a criterion that measures the triplet loss given an input tensors <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">x1</annotation></semantics></math></span></span></span>, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">x2</annotation></semantics></math></span></span></span>, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">x3</annotation></semantics></math></span></span></span> and a margin with a value greater than <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">0</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.tripletmarginwithdistanceloss#torch.nn.TripletMarginWithDistanceLoss\" title=\"torch.nn.TripletMarginWithDistanceLoss\"><code>nn.TripletMarginWithDistanceLoss</code></a></p></td> <td><p>Creates a criterion that measures the triplet loss given input tensors <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>a</mi></mrow><annotation encoding=\"application/x-tex\">a</annotation></semantics></math></span></span></span>, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span></span></span>, and <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span></span></span> (representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\"distance function\") used to compute the relationship between the anchor and positive example (\"positive distance\") and the anchor and negative example (\"negative distance\").</p></td> </tr>  </table>   <h2 id=\"vision-layers\">Vision Layers</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.pixelshuffle#torch.nn.PixelShuffle\" title=\"torch.nn.PixelShuffle\"><code>nn.PixelShuffle</code></a></p></td> <td><p>Rearranges elements in a tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo></mo><mo separator=\"true\">,</mo><mi>C</mi><mo></mo><msup><mi>r</mi><mn>2</mn></msup><mo separator=\"true\">,</mo><mi>H</mi><mo separator=\"true\">,</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(*, C \\times r^2, H, W)</annotation></semantics></math></span></span></span> to a tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo></mo><mo separator=\"true\">,</mo><mi>C</mi><mo separator=\"true\">,</mo><mi>H</mi><mo></mo><mi>r</mi><mo separator=\"true\">,</mo><mi>W</mi><mo></mo><mi>r</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(*, C, H \\times r, W \\times r)</annotation></semantics></math></span></span></span>, where r is an upscale factor.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.pixelunshuffle#torch.nn.PixelUnshuffle\" title=\"torch.nn.PixelUnshuffle\"><code>nn.PixelUnshuffle</code></a></p></td> <td><p>Reverses the <a class=\"reference internal\" href=\"generated/torch.nn.pixelshuffle#torch.nn.PixelShuffle\" title=\"torch.nn.PixelShuffle\"><code>PixelShuffle</code></a> operation by rearranging elements in a tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo></mo><mo separator=\"true\">,</mo><mi>C</mi><mo separator=\"true\">,</mo><mi>H</mi><mo></mo><mi>r</mi><mo separator=\"true\">,</mo><mi>W</mi><mo></mo><mi>r</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(*, C, H \\times r, W \\times r)</annotation></semantics></math></span></span></span> to a tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo></mo><mo separator=\"true\">,</mo><mi>C</mi><mo></mo><msup><mi>r</mi><mn>2</mn></msup><mo separator=\"true\">,</mo><mi>H</mi><mo separator=\"true\">,</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(*, C \\times r^2, H, W)</annotation></semantics></math></span></span></span>, where r is a downscale factor.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.upsample#torch.nn.Upsample\" title=\"torch.nn.Upsample\"><code>nn.Upsample</code></a></p></td> <td><p>Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.upsamplingnearest2d#torch.nn.UpsamplingNearest2d\" title=\"torch.nn.UpsamplingNearest2d\"><code>nn.UpsamplingNearest2d</code></a></p></td> <td><p>Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.upsamplingbilinear2d#torch.nn.UpsamplingBilinear2d\" title=\"torch.nn.UpsamplingBilinear2d\"><code>nn.UpsamplingBilinear2d</code></a></p></td> <td><p>Applies a 2D bilinear upsampling to an input signal composed of several input channels.</p></td> </tr>  </table>   <h2 id=\"shuffle-layers\">Shuffle Layers</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.channelshuffle#torch.nn.ChannelShuffle\" title=\"torch.nn.ChannelShuffle\"><code>nn.ChannelShuffle</code></a></p></td> <td><p>Divide the channels in a tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo></mo><mo separator=\"true\">,</mo><mi>C</mi><mo separator=\"true\">,</mo><mi>H</mi><mo separator=\"true\">,</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(*, C , H, W)</annotation></semantics></math></span></span></span> into g groups and rearrange them as <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo></mo><mo separator=\"true\">,</mo><mi>C</mi><mfrac><mi>g</mi><mo separator=\"true\" lspace=\"0em\" rspace=\"0em\">,</mo></mfrac><mi>g</mi><mo separator=\"true\">,</mo><mi>H</mi><mo separator=\"true\">,</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(*, C \\frac g, g, H, W)</annotation></semantics></math></span></span></span>, while keeping the original tensor shape.</p></td> </tr>  </table>   <h2 id=\"dataparallel-layers-multi-gpu-distributed\">DataParallel Layers (multi-GPU, distributed)</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\" id=\"module-torch.nn.parallel\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.dataparallel#torch.nn.DataParallel\" title=\"torch.nn.DataParallel\"><code>nn.DataParallel</code></a></p></td> <td><p>Implements data parallelism at the module level.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel\" title=\"torch.nn.parallel.DistributedDataParallel\"><code>nn.parallel.DistributedDataParallel</code></a></p></td> <td><p>Implements distributed data parallelism that is based on <code>torch.distributed</code> package at the module level.</p></td> </tr>  </table>   <h2 id=\"utilities\">Utilities</h2> <p id=\"module-torch.nn.utils\">From the <code>torch.nn.utils</code> module</p> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.utils.clip_grad_norm_#torch.nn.utils.clip_grad_norm_\" title=\"torch.nn.utils.clip_grad_norm_\"><code>clip_grad_norm_</code></a>\n</td> <td><p>Clips gradient norm of an iterable of parameters.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.utils.clip_grad_value_#torch.nn.utils.clip_grad_value_\" title=\"torch.nn.utils.clip_grad_value_\"><code>clip_grad_value_</code></a>\n</td> <td><p>Clips gradient of an iterable of parameters at specified value.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.utils.parameters_to_vector#torch.nn.utils.parameters_to_vector\" title=\"torch.nn.utils.parameters_to_vector\"><code>parameters_to_vector</code></a>\n</td> <td><p>Convert parameters to one vector</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.utils.vector_to_parameters#torch.nn.utils.vector_to_parameters\" title=\"torch.nn.utils.vector_to_parameters\"><code>vector_to_parameters</code></a>\n</td> <td><p>Convert one vector to the parameters</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod\" title=\"torch.nn.utils.prune.BasePruningMethod\"><code>prune.BasePruningMethod</code></a></p></td> <td><p>Abstract base class for creation of new pruning techniques.</p></td> </tr>  </table> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer\" title=\"torch.nn.utils.prune.PruningContainer\"><code>prune.PruningContainer</code></a></p></td> <td><p>Container holding a sequence of pruning methods for iterative pruning.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity\" title=\"torch.nn.utils.prune.Identity\"><code>prune.Identity</code></a></p></td> <td><p>Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured\" title=\"torch.nn.utils.prune.RandomUnstructured\"><code>prune.RandomUnstructured</code></a></p></td> <td><p>Prune (currently unpruned) units in a tensor at random.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured\" title=\"torch.nn.utils.prune.L1Unstructured\"><code>prune.L1Unstructured</code></a></p></td> <td><p>Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured\" title=\"torch.nn.utils.prune.RandomStructured\"><code>prune.RandomStructured</code></a></p></td> <td><p>Prune entire (currently unpruned) channels in a tensor at random.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured\" title=\"torch.nn.utils.prune.LnStructured\"><code>prune.LnStructured</code></a></p></td> <td><p>Prune entire (currently unpruned) channels in a tensor based on their L<code>n</code>-norm.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask\" title=\"torch.nn.utils.prune.CustomFromMask\"><code>prune.CustomFromMask</code></a></p></td> <td></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.identity\" title=\"torch.nn.utils.prune.identity\"><code>prune.identity</code></a></p></td> <td><p>Applies pruning reparametrization to the tensor corresponding to the parameter called <code>name</code> in <code>module</code> without actually pruning any units.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.random_unstructured#torch.nn.utils.prune.random_unstructured\" title=\"torch.nn.utils.prune.random_unstructured\"><code>prune.random_unstructured</code></a></p></td> <td><p>Prunes tensor corresponding to parameter called <code>name</code> in <code>module</code> by removing the specified <code>amount</code> of (currently unpruned) units selected at random.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.l1_unstructured#torch.nn.utils.prune.l1_unstructured\" title=\"torch.nn.utils.prune.l1_unstructured\"><code>prune.l1_unstructured</code></a></p></td> <td><p>Prunes tensor corresponding to parameter called <code>name</code> in <code>module</code> by removing the specified <code>amount</code> of (currently unpruned) units with the lowest L1-norm.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.random_structured#torch.nn.utils.prune.random_structured\" title=\"torch.nn.utils.prune.random_structured\"><code>prune.random_structured</code></a></p></td> <td><p>Prunes tensor corresponding to parameter called <code>name</code> in <code>module</code> by removing the specified <code>amount</code> of (currently unpruned) channels along the specified <code>dim</code> selected at random.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.ln_structured#torch.nn.utils.prune.ln_structured\" title=\"torch.nn.utils.prune.ln_structured\"><code>prune.ln_structured</code></a></p></td> <td><p>Prunes tensor corresponding to parameter called <code>name</code> in <code>module</code> by removing the specified <code>amount</code> of (currently unpruned) channels along the specified <code>dim</code> with the lowest L<code>n</code>-norm.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.global_unstructured#torch.nn.utils.prune.global_unstructured\" title=\"torch.nn.utils.prune.global_unstructured\"><code>prune.global_unstructured</code></a></p></td> <td><p>Globally prunes tensors corresponding to all parameters in <code>parameters</code> by applying the specified <code>pruning_method</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.custom_from_mask#torch.nn.utils.prune.custom_from_mask\" title=\"torch.nn.utils.prune.custom_from_mask\"><code>prune.custom_from_mask</code></a></p></td> <td><p>Prunes tensor corresponding to parameter called <code>name</code> in <code>module</code> by applying the pre-computed mask in <code>mask</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.remove#torch.nn.utils.prune.remove\" title=\"torch.nn.utils.prune.remove\"><code>prune.remove</code></a></p></td> <td><p>Removes the pruning reparameterization from a module and the pruning method from the forward hook.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.is_pruned#torch.nn.utils.prune.is_pruned\" title=\"torch.nn.utils.prune.is_pruned\"><code>prune.is_pruned</code></a></p></td> <td><p>Check whether <code>module</code> is pruned by looking for <code>forward_pre_hooks</code> in its modules that inherit from the <code>BasePruningMethod</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.utils.weight_norm#torch.nn.utils.weight_norm\" title=\"torch.nn.utils.weight_norm\"><code>weight_norm</code></a>\n</td> <td><p>Applies weight normalization to a parameter in the given module.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.utils.remove_weight_norm#torch.nn.utils.remove_weight_norm\" title=\"torch.nn.utils.remove_weight_norm\"><code>remove_weight_norm</code></a>\n</td> <td><p>Removes the weight normalization reparameterization from a module.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.utils.spectral_norm#torch.nn.utils.spectral_norm\" title=\"torch.nn.utils.spectral_norm\"><code>spectral_norm</code></a>\n</td> <td><p>Applies spectral normalization to a parameter in the given module.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.utils.remove_spectral_norm#torch.nn.utils.remove_spectral_norm\" title=\"torch.nn.utils.remove_spectral_norm\"><code>remove_spectral_norm</code></a>\n</td> <td><p>Removes the spectral normalization reparameterization from a module.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.utils.skip_init#torch.nn.utils.skip_init\" title=\"torch.nn.utils.skip_init\"><code>skip_init</code></a>\n</td> <td><p>Given a module class object and args / kwargs, instantiates the module without initializing parameters / buffers.</p></td> </tr>  </table> <p>Parametrizations implemented using the new parametrization functionality in <code>torch.nn.utils.parameterize.register_parametrization()</code>.</p> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.parametrizations.orthogonal#torch.nn.utils.parametrizations.orthogonal\" title=\"torch.nn.utils.parametrizations.orthogonal\"><code>parametrizations.orthogonal</code></a></p></td> <td><p>Applies an orthogonal or unitary parametrization to a matrix or a batch of matrices.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.parametrizations.spectral_norm#torch.nn.utils.parametrizations.spectral_norm\" title=\"torch.nn.utils.parametrizations.spectral_norm\"><code>parametrizations.spectral_norm</code></a></p></td> <td><p>Applies spectral normalization to a parameter in the given module.</p></td> </tr>  </table> <p>Utility functions to parametrize Tensors on existing Modules. Note that these functions can be used to parametrize a given Parameter or Buffer given a specific function that maps from an input space to the parametrized space. They are not parameterizations that would transform an object into a parameter. See the <a class=\"reference external\" href=\"https://pytorch.org/tutorials/intermediate/parametrizations.html\">Parametrizations tutorial</a> for more information on how to implement your own parametrizations.</p> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.parametrize.register_parametrization#torch.nn.utils.parametrize.register_parametrization\" title=\"torch.nn.utils.parametrize.register_parametrization\"><code>parametrize.register_parametrization</code></a></p></td> <td><p>Adds a parametrization to a tensor in a module.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.parametrize.remove_parametrizations#torch.nn.utils.parametrize.remove_parametrizations\" title=\"torch.nn.utils.parametrize.remove_parametrizations\"><code>parametrize.remove_parametrizations</code></a></p></td> <td><p>Removes the parametrizations on a tensor in a module.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.parametrize.cached#torch.nn.utils.parametrize.cached\" title=\"torch.nn.utils.parametrize.cached\"><code>parametrize.cached</code></a></p></td> <td><p>Context manager that enables the caching system within parametrizations registered with <code>register_parametrization()</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.parametrize.is_parametrized#torch.nn.utils.parametrize.is_parametrized\" title=\"torch.nn.utils.parametrize.is_parametrized\"><code>parametrize.is_parametrized</code></a></p></td> <td><p>Returns <code>True</code> if module has an active parametrization.</p></td> </tr>  </table> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.parametrize.parametrizationlist#torch.nn.utils.parametrize.ParametrizationList\" title=\"torch.nn.utils.parametrize.ParametrizationList\"><code>parametrize.ParametrizationList</code></a></p></td> <td><p>A sequential container that holds and manages the <code>original</code> or <code>original0</code>, <code>original1</code>, .</p></td> </tr>  </table> <p>Utility functions to calls a given Module in a stateless manner.</p> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.stateless.functional_call#torch.nn.utils.stateless.functional_call\" title=\"torch.nn.utils.stateless.functional_call\"><code>stateless.functional_call</code></a></p></td> <td><p>Performs a functional call on the module by replacing the module parameters and buffers with the provided ones.</p></td> </tr>  </table> <p>Utility functions in other modules</p> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence\" title=\"torch.nn.utils.rnn.PackedSequence\"><code>nn.utils.rnn.PackedSequence</code></a></p></td> <td><p>Holds the data and list of <code>batch_sizes</code> of a packed sequence.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence\" title=\"torch.nn.utils.rnn.pack_padded_sequence\"><code>nn.utils.rnn.pack_padded_sequence</code></a></p></td> <td><p>Packs a Tensor containing padded sequences of variable length.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.rnn.pad_packed_sequence#torch.nn.utils.rnn.pad_packed_sequence\" title=\"torch.nn.utils.rnn.pad_packed_sequence\"><code>nn.utils.rnn.pad_packed_sequence</code></a></p></td> <td><p>Pads a packed batch of variable length sequences.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.rnn.pad_sequence#torch.nn.utils.rnn.pad_sequence\" title=\"torch.nn.utils.rnn.pad_sequence\"><code>nn.utils.rnn.pad_sequence</code></a></p></td> <td><p>Pad a list of variable length Tensors with <code>padding_value</code></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.rnn.pack_sequence#torch.nn.utils.rnn.pack_sequence\" title=\"torch.nn.utils.rnn.pack_sequence\"><code>nn.utils.rnn.pack_sequence</code></a></p></td> <td><p>Packs a list of variable length Tensors</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.rnn.unpack_sequence#torch.nn.utils.rnn.unpack_sequence\" title=\"torch.nn.utils.rnn.unpack_sequence\"><code>nn.utils.rnn.unpack_sequence</code></a></p></td> <td><p>Unpacks PackedSequence into a list of variable length Tensors</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.rnn.unpad_sequence#torch.nn.utils.rnn.unpad_sequence\" title=\"torch.nn.utils.rnn.unpad_sequence\"><code>nn.utils.rnn.unpad_sequence</code></a></p></td> <td><p>Unpad padded Tensor into a list of variable length Tensors</p></td> </tr>  </table> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.flatten#torch.nn.Flatten\" title=\"torch.nn.Flatten\"><code>nn.Flatten</code></a></p></td> <td><p>Flattens a contiguous range of dims into a tensor.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.unflatten#torch.nn.Unflatten\" title=\"torch.nn.Unflatten\"><code>nn.Unflatten</code></a></p></td> <td><p>Unflattens a tensor dim expanding it to a desired shape.</p></td> </tr>  </table>   <h2 id=\"quantized-functions\">Quantized Functions</h2> <p>Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than floating point precision. PyTorch supports both per tensor and per channel asymmetric linear quantization. To learn more how to use quantized functions in PyTorch, please refer to the <a class=\"reference internal\" href=\"quantization#quantization-doc\"><span class=\"std std-ref\">Quantization</span></a> documentation.</p>   <h2 id=\"lazy-modules-initialization\">Lazy Modules Initialization</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\" id=\"module-torch.nn.utils.stateless\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.modules.lazy.lazymodulemixin#torch.nn.modules.lazy.LazyModuleMixin\" title=\"torch.nn.modules.lazy.LazyModuleMixin\"><code>nn.modules.lazy.LazyModuleMixin</code></a></p></td> <td><p>A mixin for modules that lazily initialize parameters, also known as \"lazy modules.\"</p></td> </tr>  </table><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href=\"https://github.com/pytorch/pytorch/blob/main/LICENSE\">LICENSE</a> file.<br>\n    <a href=\"https://pytorch.org/docs/2.1/nn.html\" class=\"_attribution-link\">https://pytorch.org/docs/2.1/nn.html</a>\n  </p>\n</div>\n","nn.functional":"<h1 id=\"torch-nn-functional\">torch.nn.functional</h1>  <h2 id=\"convolution-functions\">Convolution functions</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.conv1d#torch.nn.functional.conv1d\" title=\"torch.nn.functional.conv1d\"><code>conv1d</code></a>\n</td> <td><p>Applies a 1D convolution over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.conv2d#torch.nn.functional.conv2d\" title=\"torch.nn.functional.conv2d\"><code>conv2d</code></a>\n</td> <td><p>Applies a 2D convolution over an input image composed of several input planes.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.conv3d#torch.nn.functional.conv3d\" title=\"torch.nn.functional.conv3d\"><code>conv3d</code></a>\n</td> <td><p>Applies a 3D convolution over an input image composed of several input planes.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.conv_transpose1d#torch.nn.functional.conv_transpose1d\" title=\"torch.nn.functional.conv_transpose1d\"><code>conv_transpose1d</code></a>\n</td> <td><p>Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \"deconvolution\".</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.conv_transpose2d#torch.nn.functional.conv_transpose2d\" title=\"torch.nn.functional.conv_transpose2d\"><code>conv_transpose2d</code></a>\n</td> <td><p>Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \"deconvolution\".</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.conv_transpose3d#torch.nn.functional.conv_transpose3d\" title=\"torch.nn.functional.conv_transpose3d\"><code>conv_transpose3d</code></a>\n</td> <td><p>Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \"deconvolution\"</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.unfold#torch.nn.functional.unfold\" title=\"torch.nn.functional.unfold\"><code>unfold</code></a>\n</td> <td><p>Extracts sliding local blocks from a batched input tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.fold#torch.nn.functional.fold\" title=\"torch.nn.functional.fold\"><code>fold</code></a>\n</td> <td><p>Combines an array of sliding local blocks into a large containing tensor.</p></td> </tr>  </table>   <h2 id=\"pooling-functions\">Pooling functions</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.avg_pool1d#torch.nn.functional.avg_pool1d\" title=\"torch.nn.functional.avg_pool1d\"><code>avg_pool1d</code></a>\n</td> <td><p>Applies a 1D average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.avg_pool2d#torch.nn.functional.avg_pool2d\" title=\"torch.nn.functional.avg_pool2d\"><code>avg_pool2d</code></a>\n</td> <td><p>Applies 2D average-pooling operation in <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>k</mi><mi>H</mi><mo></mo><mi>k</mi><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">kH \\times kW</annotation></semantics></math></span></span></span> regions by step size <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi><mi>H</mi><mo></mo><mi>s</mi><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">sH \\times sW</annotation></semantics></math></span></span></span> steps.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.avg_pool3d#torch.nn.functional.avg_pool3d\" title=\"torch.nn.functional.avg_pool3d\"><code>avg_pool3d</code></a>\n</td> <td><p>Applies 3D average-pooling operation in <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>k</mi><mi>T</mi><mo></mo><mi>k</mi><mi>H</mi><mo></mo><mi>k</mi><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">kT \\times kH \\times kW</annotation></semantics></math></span></span></span> regions by step size <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi><mi>T</mi><mo></mo><mi>s</mi><mi>H</mi><mo></mo><mi>s</mi><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">sT \\times sH \\times sW</annotation></semantics></math></span></span></span> steps.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.max_pool1d#torch.nn.functional.max_pool1d\" title=\"torch.nn.functional.max_pool1d\"><code>max_pool1d</code></a>\n</td> <td><p>Applies a 1D max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.max_pool2d#torch.nn.functional.max_pool2d\" title=\"torch.nn.functional.max_pool2d\"><code>max_pool2d</code></a>\n</td> <td><p>Applies a 2D max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.max_pool3d#torch.nn.functional.max_pool3d\" title=\"torch.nn.functional.max_pool3d\"><code>max_pool3d</code></a>\n</td> <td><p>Applies a 3D max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.max_unpool1d#torch.nn.functional.max_unpool1d\" title=\"torch.nn.functional.max_unpool1d\"><code>max_unpool1d</code></a>\n</td> <td><p>Computes a partial inverse of <code>MaxPool1d</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.max_unpool2d#torch.nn.functional.max_unpool2d\" title=\"torch.nn.functional.max_unpool2d\"><code>max_unpool2d</code></a>\n</td> <td><p>Computes a partial inverse of <code>MaxPool2d</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.max_unpool3d#torch.nn.functional.max_unpool3d\" title=\"torch.nn.functional.max_unpool3d\"><code>max_unpool3d</code></a>\n</td> <td><p>Computes a partial inverse of <code>MaxPool3d</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.lp_pool1d#torch.nn.functional.lp_pool1d\" title=\"torch.nn.functional.lp_pool1d\"><code>lp_pool1d</code></a>\n</td> <td><p>Applies a 1D power-average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.lp_pool2d#torch.nn.functional.lp_pool2d\" title=\"torch.nn.functional.lp_pool2d\"><code>lp_pool2d</code></a>\n</td> <td><p>Applies a 2D power-average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.adaptive_max_pool1d#torch.nn.functional.adaptive_max_pool1d\" title=\"torch.nn.functional.adaptive_max_pool1d\"><code>adaptive_max_pool1d</code></a>\n</td> <td><p>Applies a 1D adaptive max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.adaptive_max_pool2d#torch.nn.functional.adaptive_max_pool2d\" title=\"torch.nn.functional.adaptive_max_pool2d\"><code>adaptive_max_pool2d</code></a>\n</td> <td><p>Applies a 2D adaptive max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.adaptive_max_pool3d#torch.nn.functional.adaptive_max_pool3d\" title=\"torch.nn.functional.adaptive_max_pool3d\"><code>adaptive_max_pool3d</code></a>\n</td> <td><p>Applies a 3D adaptive max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.adaptive_avg_pool1d#torch.nn.functional.adaptive_avg_pool1d\" title=\"torch.nn.functional.adaptive_avg_pool1d\"><code>adaptive_avg_pool1d</code></a>\n</td> <td><p>Applies a 1D adaptive average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.adaptive_avg_pool2d#torch.nn.functional.adaptive_avg_pool2d\" title=\"torch.nn.functional.adaptive_avg_pool2d\"><code>adaptive_avg_pool2d</code></a>\n</td> <td><p>Applies a 2D adaptive average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.adaptive_avg_pool3d#torch.nn.functional.adaptive_avg_pool3d\" title=\"torch.nn.functional.adaptive_avg_pool3d\"><code>adaptive_avg_pool3d</code></a>\n</td> <td><p>Applies a 3D adaptive average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.fractional_max_pool2d#torch.nn.functional.fractional_max_pool2d\" title=\"torch.nn.functional.fractional_max_pool2d\"><code>fractional_max_pool2d</code></a>\n</td> <td><p>Applies 2D fractional max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.fractional_max_pool3d#torch.nn.functional.fractional_max_pool3d\" title=\"torch.nn.functional.fractional_max_pool3d\"><code>fractional_max_pool3d</code></a>\n</td> <td><p>Applies 3D fractional max pooling over an input signal composed of several input planes.</p></td> </tr>  </table>   <h2 id=\"attention-mechanisms\">Attention Mechanisms</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.scaled_dot_product_attention#torch.nn.functional.scaled_dot_product_attention\" title=\"torch.nn.functional.scaled_dot_product_attention\"><code>scaled_dot_product_attention</code></a>\n</td> <td><p>Computes scaled dot product attention on query, key and value tensors, using an optional attention mask if passed, and applying dropout if a probability greater than 0.0 is specified.</p></td> </tr>  </table>   <h2 id=\"non-linear-activation-functions\">Non-linear activation functions</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.threshold#torch.nn.functional.threshold\" title=\"torch.nn.functional.threshold\"><code>threshold</code></a>\n</td> <td><p>Thresholds each element of the input Tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.threshold_#torch.nn.functional.threshold_\" title=\"torch.nn.functional.threshold_\"><code>threshold_</code></a>\n</td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.nn.functional.threshold#torch.nn.functional.threshold\" title=\"torch.nn.functional.threshold\"><code>threshold()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.relu#torch.nn.functional.relu\" title=\"torch.nn.functional.relu\"><code>relu</code></a>\n</td> <td><p>Applies the rectified linear unit function element-wise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.relu_#torch.nn.functional.relu_\" title=\"torch.nn.functional.relu_\"><code>relu_</code></a>\n</td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.nn.functional.relu#torch.nn.functional.relu\" title=\"torch.nn.functional.relu\"><code>relu()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.hardtanh#torch.nn.functional.hardtanh\" title=\"torch.nn.functional.hardtanh\"><code>hardtanh</code></a>\n</td> <td><p>Applies the HardTanh function element-wise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.hardtanh_#torch.nn.functional.hardtanh_\" title=\"torch.nn.functional.hardtanh_\"><code>hardtanh_</code></a>\n</td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.nn.functional.hardtanh#torch.nn.functional.hardtanh\" title=\"torch.nn.functional.hardtanh\"><code>hardtanh()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.hardswish#torch.nn.functional.hardswish\" title=\"torch.nn.functional.hardswish\"><code>hardswish</code></a>\n</td> <td><p>Applies the hardswish function, element-wise, as described in the paper:</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.relu6#torch.nn.functional.relu6\" title=\"torch.nn.functional.relu6\"><code>relu6</code></a>\n</td> <td><p>Applies the element-wise function <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>ReLU6</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>min</mi><mo></mo><mo stretchy=\"false\">(</mo><mi>max</mi><mo></mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo><mn>6</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{ReLU6}(x) = \\min(\\max(0,x), 6)</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.elu#torch.nn.functional.elu\" title=\"torch.nn.functional.elu\"><code>elu</code></a>\n</td> <td><p>Applies the Exponential Linear Unit (ELU) function element-wise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.elu_#torch.nn.functional.elu_\" title=\"torch.nn.functional.elu_\"><code>elu_</code></a>\n</td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.nn.functional.elu#torch.nn.functional.elu\" title=\"torch.nn.functional.elu\"><code>elu()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.selu#torch.nn.functional.selu\" title=\"torch.nn.functional.selu\"><code>selu</code></a>\n</td> <td><p>Applies element-wise, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>SELU</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mo></mo><mo stretchy=\"false\">(</mo><mi>max</mi><mo></mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mi>min</mi><mo></mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi></mi><mo></mo><mo stretchy=\"false\">(</mo><mi>exp</mi><mo></mo><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo></mo><mn>1</mn><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))</annotation></semantics></math></span></span></span>, with <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi></mi><mo>=</mo><mn>1.6732632423543772848170429916717</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=1.6732632423543772848170429916717</annotation></semantics></math></span></span></span> and <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mo>=</mo><mn>1.0507009873554804934193349852946</mn></mrow><annotation encoding=\"application/x-tex\">scale=1.0507009873554804934193349852946</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.celu#torch.nn.functional.celu\" title=\"torch.nn.functional.celu\"><code>celu</code></a>\n</td> <td><p>Applies element-wise, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>CELU</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>max</mi><mo></mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mi>min</mi><mo></mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi></mi><mo></mo><mo stretchy=\"false\">(</mo><mi>exp</mi><mo></mo><mo stretchy=\"false\">(</mo><mi>x</mi><mi mathvariant=\"normal\">/</mi><mi></mi><mo stretchy=\"false\">)</mo><mo></mo><mn>1</mn><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.leaky_relu#torch.nn.functional.leaky_relu\" title=\"torch.nn.functional.leaky_relu\"><code>leaky_relu</code></a>\n</td> <td><p>Applies element-wise, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>LeakyReLU</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>max</mi><mo></mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mtext>negative_slope</mtext><mo></mo><mi>min</mi><mo></mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)</annotation></semantics></math></span></span></span></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.leaky_relu_#torch.nn.functional.leaky_relu_\" title=\"torch.nn.functional.leaky_relu_\"><code>leaky_relu_</code></a>\n</td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.nn.functional.leaky_relu#torch.nn.functional.leaky_relu\" title=\"torch.nn.functional.leaky_relu\"><code>leaky_relu()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.prelu#torch.nn.functional.prelu\" title=\"torch.nn.functional.prelu\"><code>prelu</code></a>\n</td> <td><p>Applies element-wise the function <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>PReLU</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>max</mi><mo></mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mtext>weight</mtext><mo></mo><mi>min</mi><mo></mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)</annotation></semantics></math></span></span></span> where weight is a learnable parameter.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.rrelu#torch.nn.functional.rrelu\" title=\"torch.nn.functional.rrelu\"><code>rrelu</code></a>\n</td> <td><p>Randomized leaky ReLU.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.rrelu_#torch.nn.functional.rrelu_\" title=\"torch.nn.functional.rrelu_\"><code>rrelu_</code></a>\n</td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.nn.functional.rrelu#torch.nn.functional.rrelu\" title=\"torch.nn.functional.rrelu\"><code>rrelu()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.glu#torch.nn.functional.glu\" title=\"torch.nn.functional.glu\"><code>glu</code></a>\n</td> <td><p>The gated linear unit.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.gelu#torch.nn.functional.gelu\" title=\"torch.nn.functional.gelu\"><code>gelu</code></a>\n</td> <td><p>When the approximate argument is 'none', it applies element-wise the function <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>GELU</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>x</mi><mo></mo><mi mathvariant=\"normal\"></mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{GELU}(x) = x * \\Phi(x)</annotation></semantics></math></span></span></span></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.logsigmoid#torch.nn.functional.logsigmoid\" title=\"torch.nn.functional.logsigmoid\"><code>logsigmoid</code></a>\n</td> <td><p>Applies element-wise <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>LogSigmoid</mtext><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mi>log</mi><mo></mo><mrow><mo fence=\"true\">(</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mo></mo><mo stretchy=\"false\">(</mo><mo></mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mfrac><mo fence=\"true\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)</annotation></semantics></math></span></span></span></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.hardshrink#torch.nn.functional.hardshrink\" title=\"torch.nn.functional.hardshrink\"><code>hardshrink</code></a>\n</td> <td><p>Applies the hard shrinkage function element-wise</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.tanhshrink#torch.nn.functional.tanhshrink\" title=\"torch.nn.functional.tanhshrink\"><code>tanhshrink</code></a>\n</td> <td><p>Applies element-wise, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>Tanhshrink</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>x</mi><mo></mo><mtext>Tanh</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)</annotation></semantics></math></span></span></span></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.softsign#torch.nn.functional.softsign\" title=\"torch.nn.functional.softsign\"><code>softsign</code></a>\n</td> <td><p>Applies element-wise, the function <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>SoftSign</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mi>x</mi><mrow><mn>1</mn><mo>+</mo><mi mathvariant=\"normal\"></mi><mi>x</mi><mi mathvariant=\"normal\"></mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\text{SoftSign}(x) = \\frac{x}{1 + |x|}</annotation></semantics></math></span></span></span></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.softplus#torch.nn.functional.softplus\" title=\"torch.nn.functional.softplus\"><code>softplus</code></a>\n</td> <td><p>Applies element-wise, the function <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>Softplus</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mn>1</mn><mi></mi></mfrac><mo></mo><mi>log</mi><mo></mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>+</mo><mi>exp</mi><mo></mo><mo stretchy=\"false\">(</mo><mi></mi><mo></mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.softmin#torch.nn.functional.softmin\" title=\"torch.nn.functional.softmin\"><code>softmin</code></a>\n</td> <td><p>Applies a softmin function.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.softmax#torch.nn.functional.softmax\" title=\"torch.nn.functional.softmax\"><code>softmax</code></a>\n</td> <td><p>Applies a softmax function.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.softshrink#torch.nn.functional.softshrink\" title=\"torch.nn.functional.softshrink\"><code>softshrink</code></a>\n</td> <td><p>Applies the soft shrinkage function elementwise</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.gumbel_softmax#torch.nn.functional.gumbel_softmax\" title=\"torch.nn.functional.gumbel_softmax\"><code>gumbel_softmax</code></a>\n</td> <td><p>Samples from the Gumbel-Softmax distribution (<a class=\"reference external\" href=\"https://arxiv.org/abs/1611.00712\">Link 1</a> <a class=\"reference external\" href=\"https://arxiv.org/abs/1611.01144\">Link 2</a>) and optionally discretizes.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.log_softmax#torch.nn.functional.log_softmax\" title=\"torch.nn.functional.log_softmax\"><code>log_softmax</code></a>\n</td> <td><p>Applies a softmax followed by a logarithm.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.tanh#torch.nn.functional.tanh\" title=\"torch.nn.functional.tanh\"><code>tanh</code></a>\n</td> <td><p>Applies element-wise, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>Tanh</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>tanh</mi><mo></mo><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo></mo><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo></mo><mi>exp</mi><mo></mo><mo stretchy=\"false\">(</mo><mo></mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mi>exp</mi><mo></mo><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mi>exp</mi><mo></mo><mo stretchy=\"false\">(</mo><mo></mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}</annotation></semantics></math></span></span></span></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.sigmoid#torch.nn.functional.sigmoid\" title=\"torch.nn.functional.sigmoid\"><code>sigmoid</code></a>\n</td> <td><p>Applies the element-wise function <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>Sigmoid</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mo></mo><mo stretchy=\"false\">(</mo><mo></mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}</annotation></semantics></math></span></span></span></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.hardsigmoid#torch.nn.functional.hardsigmoid\" title=\"torch.nn.functional.hardsigmoid\"><code>hardsigmoid</code></a>\n</td> <td><p>Applies the element-wise function</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.silu#torch.nn.functional.silu\" title=\"torch.nn.functional.silu\"><code>silu</code></a>\n</td> <td><p>Applies the Sigmoid Linear Unit (SiLU) function, element-wise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.mish#torch.nn.functional.mish\" title=\"torch.nn.functional.mish\"><code>mish</code></a>\n</td> <td><p>Applies the Mish function, element-wise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.batch_norm#torch.nn.functional.batch_norm\" title=\"torch.nn.functional.batch_norm\"><code>batch_norm</code></a>\n</td> <td><p>Applies Batch Normalization for each channel across a batch of data.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.group_norm#torch.nn.functional.group_norm\" title=\"torch.nn.functional.group_norm\"><code>group_norm</code></a>\n</td> <td><p>Applies Group Normalization for last certain number of dimensions.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.instance_norm#torch.nn.functional.instance_norm\" title=\"torch.nn.functional.instance_norm\"><code>instance_norm</code></a>\n</td> <td><p>Applies Instance Normalization for each channel in each data sample in a batch.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.layer_norm#torch.nn.functional.layer_norm\" title=\"torch.nn.functional.layer_norm\"><code>layer_norm</code></a>\n</td> <td><p>Applies Layer Normalization for last certain number of dimensions.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.local_response_norm#torch.nn.functional.local_response_norm\" title=\"torch.nn.functional.local_response_norm\"><code>local_response_norm</code></a>\n</td> <td><p>Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.normalize#torch.nn.functional.normalize\" title=\"torch.nn.functional.normalize\"><code>normalize</code></a>\n</td> <td><p>Performs <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>L</mi><mi>p</mi></msub></mrow><annotation encoding=\"application/x-tex\">L_p</annotation></semantics></math></span></span></span> normalization of inputs over specified dimension.</p></td> </tr>  </table>   <h2 id=\"linear-functions\">Linear functions</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.linear#torch.nn.functional.linear\" title=\"torch.nn.functional.linear\"><code>linear</code></a>\n</td> <td><p>Applies a linear transformation to the incoming data: <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi><mo>=</mo><mi>x</mi><msup><mi>A</mi><mi>T</mi></msup><mo>+</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">y = xA^T + b</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.bilinear#torch.nn.functional.bilinear\" title=\"torch.nn.functional.bilinear\"><code>bilinear</code></a>\n</td> <td><p>Applies a bilinear transformation to the incoming data: <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi><mo>=</mo><msubsup><mi>x</mi><mn>1</mn><mi>T</mi></msubsup><mi>A</mi><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">y = x_1^T A x_2 + b</annotation></semantics></math></span></span></span></p></td> </tr>  </table>   <h2 id=\"dropout-functions\">Dropout functions</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.dropout#torch.nn.functional.dropout\" title=\"torch.nn.functional.dropout\"><code>dropout</code></a>\n</td> <td><p>During training, randomly zeroes some of the elements of the input tensor with probability <code>p</code> using samples from a Bernoulli distribution.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.alpha_dropout#torch.nn.functional.alpha_dropout\" title=\"torch.nn.functional.alpha_dropout\"><code>alpha_dropout</code></a>\n</td> <td><p>Applies alpha dropout to the input.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.feature_alpha_dropout#torch.nn.functional.feature_alpha_dropout\" title=\"torch.nn.functional.feature_alpha_dropout\"><code>feature_alpha_dropout</code></a>\n</td> <td><p>Randomly masks out entire channels (a channel is a feature map, e.g.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.dropout1d#torch.nn.functional.dropout1d\" title=\"torch.nn.functional.dropout1d\"><code>dropout1d</code></a>\n</td> <td><p>Randomly zero out entire channels (a channel is a 1D feature map, e.g., the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span></span></span>-th channel of the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span></span></span>-th sample in the batched input is a 1D tensor <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo stretchy=\"false\">[</mo><mi>i</mi><mo separator=\"true\">,</mo><mi>j</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\text{input}[i, j]</annotation></semantics></math></span></span></span>) of the input tensor).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.dropout2d#torch.nn.functional.dropout2d\" title=\"torch.nn.functional.dropout2d\"><code>dropout2d</code></a>\n</td> <td><p>Randomly zero out entire channels (a channel is a 2D feature map, e.g., the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span></span></span>-th channel of the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span></span></span>-th sample in the batched input is a 2D tensor <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo stretchy=\"false\">[</mo><mi>i</mi><mo separator=\"true\">,</mo><mi>j</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\text{input}[i, j]</annotation></semantics></math></span></span></span>) of the input tensor).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.dropout3d#torch.nn.functional.dropout3d\" title=\"torch.nn.functional.dropout3d\"><code>dropout3d</code></a>\n</td> <td><p>Randomly zero out entire channels (a channel is a 3D feature map, e.g., the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span></span></span>-th channel of the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span></span></span>-th sample in the batched input is a 3D tensor <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo stretchy=\"false\">[</mo><mi>i</mi><mo separator=\"true\">,</mo><mi>j</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\text{input}[i, j]</annotation></semantics></math></span></span></span>) of the input tensor).</p></td> </tr>  </table>   <h2 id=\"sparse-functions\">Sparse functions</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.embedding#torch.nn.functional.embedding\" title=\"torch.nn.functional.embedding\"><code>embedding</code></a>\n</td> <td><p>A simple lookup table that looks up embeddings in a fixed dictionary and size.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.embedding_bag#torch.nn.functional.embedding_bag\" title=\"torch.nn.functional.embedding_bag\"><code>embedding_bag</code></a>\n</td> <td><p>Computes sums, means or maxes of <code>bags</code> of embeddings, without instantiating the intermediate embeddings.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.one_hot#torch.nn.functional.one_hot\" title=\"torch.nn.functional.one_hot\"><code>one_hot</code></a>\n</td> <td><p>Takes LongTensor with index values of shape <code>(*)</code> and returns a tensor of shape <code>(*, num_classes)</code> that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1.</p></td> </tr>  </table>   <h2 id=\"distance-functions\">Distance functions</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.pairwise_distance#torch.nn.functional.pairwise_distance\" title=\"torch.nn.functional.pairwise_distance\"><code>pairwise_distance</code></a>\n</td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.nn.pairwisedistance#torch.nn.PairwiseDistance\" title=\"torch.nn.PairwiseDistance\"><code>torch.nn.PairwiseDistance</code></a> for details</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.cosine_similarity#torch.nn.functional.cosine_similarity\" title=\"torch.nn.functional.cosine_similarity\"><code>cosine_similarity</code></a>\n</td> <td><p>Returns cosine similarity between <code>x1</code> and <code>x2</code>, computed along dim.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.pdist#torch.nn.functional.pdist\" title=\"torch.nn.functional.pdist\"><code>pdist</code></a>\n</td> <td><p>Computes the p-norm distance between every pair of row vectors in the input.</p></td> </tr>  </table>   <h2 id=\"loss-functions\">Loss functions</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.binary_cross_entropy#torch.nn.functional.binary_cross_entropy\" title=\"torch.nn.functional.binary_cross_entropy\"><code>binary_cross_entropy</code></a>\n</td> <td><p>Function that measures the Binary Cross Entropy between the target and input probabilities.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.binary_cross_entropy_with_logits#torch.nn.functional.binary_cross_entropy_with_logits\" title=\"torch.nn.functional.binary_cross_entropy_with_logits\"><code>binary_cross_entropy_with_logits</code></a>\n</td> <td><p>Function that measures Binary Cross Entropy between target and input logits.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.poisson_nll_loss#torch.nn.functional.poisson_nll_loss\" title=\"torch.nn.functional.poisson_nll_loss\"><code>poisson_nll_loss</code></a>\n</td> <td><p>Poisson negative log likelihood loss.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.cosine_embedding_loss#torch.nn.functional.cosine_embedding_loss\" title=\"torch.nn.functional.cosine_embedding_loss\"><code>cosine_embedding_loss</code></a>\n</td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.nn.cosineembeddingloss#torch.nn.CosineEmbeddingLoss\" title=\"torch.nn.CosineEmbeddingLoss\"><code>CosineEmbeddingLoss</code></a> for details.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.cross_entropy#torch.nn.functional.cross_entropy\" title=\"torch.nn.functional.cross_entropy\"><code>cross_entropy</code></a>\n</td> <td><p>This criterion computes the cross entropy loss between input logits and target.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.ctc_loss#torch.nn.functional.ctc_loss\" title=\"torch.nn.functional.ctc_loss\"><code>ctc_loss</code></a>\n</td> <td><p>The Connectionist Temporal Classification loss.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.gaussian_nll_loss#torch.nn.functional.gaussian_nll_loss\" title=\"torch.nn.functional.gaussian_nll_loss\"><code>gaussian_nll_loss</code></a>\n</td> <td><p>Gaussian negative log likelihood loss.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.hinge_embedding_loss#torch.nn.functional.hinge_embedding_loss\" title=\"torch.nn.functional.hinge_embedding_loss\"><code>hinge_embedding_loss</code></a>\n</td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.nn.hingeembeddingloss#torch.nn.HingeEmbeddingLoss\" title=\"torch.nn.HingeEmbeddingLoss\"><code>HingeEmbeddingLoss</code></a> for details.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.kl_div#torch.nn.functional.kl_div\" title=\"torch.nn.functional.kl_div\"><code>kl_div</code></a>\n</td> <td><p>The <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Kullback-Leibler_divergence\">Kullback-Leibler divergence Loss</a></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.l1_loss#torch.nn.functional.l1_loss\" title=\"torch.nn.functional.l1_loss\"><code>l1_loss</code></a>\n</td> <td><p>Function that takes the mean element-wise absolute value difference.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.mse_loss#torch.nn.functional.mse_loss\" title=\"torch.nn.functional.mse_loss\"><code>mse_loss</code></a>\n</td> <td><p>Measures the element-wise mean squared error.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.margin_ranking_loss#torch.nn.functional.margin_ranking_loss\" title=\"torch.nn.functional.margin_ranking_loss\"><code>margin_ranking_loss</code></a>\n</td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.nn.marginrankingloss#torch.nn.MarginRankingLoss\" title=\"torch.nn.MarginRankingLoss\"><code>MarginRankingLoss</code></a> for details.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.multilabel_margin_loss#torch.nn.functional.multilabel_margin_loss\" title=\"torch.nn.functional.multilabel_margin_loss\"><code>multilabel_margin_loss</code></a>\n</td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.nn.multilabelmarginloss#torch.nn.MultiLabelMarginLoss\" title=\"torch.nn.MultiLabelMarginLoss\"><code>MultiLabelMarginLoss</code></a> for details.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.multilabel_soft_margin_loss#torch.nn.functional.multilabel_soft_margin_loss\" title=\"torch.nn.functional.multilabel_soft_margin_loss\"><code>multilabel_soft_margin_loss</code></a>\n</td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.nn.multilabelsoftmarginloss#torch.nn.MultiLabelSoftMarginLoss\" title=\"torch.nn.MultiLabelSoftMarginLoss\"><code>MultiLabelSoftMarginLoss</code></a> for details.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.multi_margin_loss#torch.nn.functional.multi_margin_loss\" title=\"torch.nn.functional.multi_margin_loss\"><code>multi_margin_loss</code></a>\n</td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.nn.multimarginloss#torch.nn.MultiMarginLoss\" title=\"torch.nn.MultiMarginLoss\"><code>MultiMarginLoss</code></a> for details.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.nll_loss#torch.nn.functional.nll_loss\" title=\"torch.nn.functional.nll_loss\"><code>nll_loss</code></a>\n</td> <td><p>The negative log likelihood loss.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.huber_loss#torch.nn.functional.huber_loss\" title=\"torch.nn.functional.huber_loss\"><code>huber_loss</code></a>\n</td> <td><p>Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.smooth_l1_loss#torch.nn.functional.smooth_l1_loss\" title=\"torch.nn.functional.smooth_l1_loss\"><code>smooth_l1_loss</code></a>\n</td> <td><p>Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.soft_margin_loss#torch.nn.functional.soft_margin_loss\" title=\"torch.nn.functional.soft_margin_loss\"><code>soft_margin_loss</code></a>\n</td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.nn.softmarginloss#torch.nn.SoftMarginLoss\" title=\"torch.nn.SoftMarginLoss\"><code>SoftMarginLoss</code></a> for details.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.triplet_margin_loss#torch.nn.functional.triplet_margin_loss\" title=\"torch.nn.functional.triplet_margin_loss\"><code>triplet_margin_loss</code></a>\n</td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.nn.tripletmarginloss#torch.nn.TripletMarginLoss\" title=\"torch.nn.TripletMarginLoss\"><code>TripletMarginLoss</code></a> for details</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.triplet_margin_with_distance_loss#torch.nn.functional.triplet_margin_with_distance_loss\" title=\"torch.nn.functional.triplet_margin_with_distance_loss\"><code>triplet_margin_with_distance_loss</code></a>\n</td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.nn.tripletmarginwithdistanceloss#torch.nn.TripletMarginWithDistanceLoss\" title=\"torch.nn.TripletMarginWithDistanceLoss\"><code>TripletMarginWithDistanceLoss</code></a> for details.</p></td> </tr>  </table>   <h2 id=\"vision-functions\">Vision functions</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.pixel_shuffle#torch.nn.functional.pixel_shuffle\" title=\"torch.nn.functional.pixel_shuffle\"><code>pixel_shuffle</code></a>\n</td> <td><p>Rearranges elements in a tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo></mo><mo separator=\"true\">,</mo><mi>C</mi><mo></mo><msup><mi>r</mi><mn>2</mn></msup><mo separator=\"true\">,</mo><mi>H</mi><mo separator=\"true\">,</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(*, C \\times r^2, H, W)</annotation></semantics></math></span></span></span> to a tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo></mo><mo separator=\"true\">,</mo><mi>C</mi><mo separator=\"true\">,</mo><mi>H</mi><mo></mo><mi>r</mi><mo separator=\"true\">,</mo><mi>W</mi><mo></mo><mi>r</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(*, C, H \\times r, W \\times r)</annotation></semantics></math></span></span></span>, where r is the <code>upscale_factor</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.pixel_unshuffle#torch.nn.functional.pixel_unshuffle\" title=\"torch.nn.functional.pixel_unshuffle\"><code>pixel_unshuffle</code></a>\n</td> <td><p>Reverses the <a class=\"reference internal\" href=\"generated/torch.nn.pixelshuffle#torch.nn.PixelShuffle\" title=\"torch.nn.PixelShuffle\"><code>PixelShuffle</code></a> operation by rearranging elements in a tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo></mo><mo separator=\"true\">,</mo><mi>C</mi><mo separator=\"true\">,</mo><mi>H</mi><mo></mo><mi>r</mi><mo separator=\"true\">,</mo><mi>W</mi><mo></mo><mi>r</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(*, C, H \\times r, W \\times r)</annotation></semantics></math></span></span></span> to a tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo></mo><mo separator=\"true\">,</mo><mi>C</mi><mo></mo><msup><mi>r</mi><mn>2</mn></msup><mo separator=\"true\">,</mo><mi>H</mi><mo separator=\"true\">,</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(*, C \\times r^2, H, W)</annotation></semantics></math></span></span></span>, where r is the <code>downscale_factor</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.pad#torch.nn.functional.pad\" title=\"torch.nn.functional.pad\"><code>pad</code></a>\n</td> <td><p>Pads tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.interpolate#torch.nn.functional.interpolate\" title=\"torch.nn.functional.interpolate\"><code>interpolate</code></a>\n</td> <td><p>Down/up samples the input to either the given <code>size</code> or the given <code>scale_factor</code></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.upsample#torch.nn.functional.upsample\" title=\"torch.nn.functional.upsample\"><code>upsample</code></a>\n</td> <td><p>Upsamples the input to either the given <code>size</code> or the given <code>scale_factor</code></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.upsample_nearest#torch.nn.functional.upsample_nearest\" title=\"torch.nn.functional.upsample_nearest\"><code>upsample_nearest</code></a>\n</td> <td><p>Upsamples the input, using nearest neighbours' pixel values.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.upsample_bilinear#torch.nn.functional.upsample_bilinear\" title=\"torch.nn.functional.upsample_bilinear\"><code>upsample_bilinear</code></a>\n</td> <td><p>Upsamples the input, using bilinear upsampling.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.grid_sample#torch.nn.functional.grid_sample\" title=\"torch.nn.functional.grid_sample\"><code>grid_sample</code></a>\n</td> <td><p>Given an <code>input</code> and a flow-field <code>grid</code>, computes the <code>output</code> using <code>input</code> values and pixel locations from <code>grid</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.functional.affine_grid#torch.nn.functional.affine_grid\" title=\"torch.nn.functional.affine_grid\"><code>affine_grid</code></a>\n</td> <td><p>Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices <code>theta</code>.</p></td> </tr>  </table>   <h2 id=\"dataparallel-functions-multi-gpu-distributed\">DataParallel functions (multi-GPU, distributed)</h2>  <h3 id=\"data-parallel\"><span class=\"hidden-section\">data_parallel</span></h3> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><code>torch.nn.parallel.data_parallel</code></p></td> <td><p>Evaluates module(input) in parallel across the GPUs given in device_ids.</p></td> </tr>  </table><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href=\"https://github.com/pytorch/pytorch/blob/main/LICENSE\">LICENSE</a> file.<br>\n    <a href=\"https://pytorch.org/docs/2.1/nn.functional.html\" class=\"_attribution-link\">https://pytorch.org/docs/2.1/nn.functional.html</a>\n  </p>\n</div>\n","tensor_attributes":"<h1 id=\"tensor-attributes-doc\">Tensor Attributes</h1> <p id=\"tensor-attributes\">Each <code>torch.Tensor</code> has a <a class=\"reference internal\" href=\"#torch.dtype\" title=\"torch.dtype\"><code>torch.dtype</code></a>, <a class=\"reference internal\" href=\"#torch.device\" title=\"torch.device\"><code>torch.device</code></a>, and <a class=\"reference internal\" href=\"#torch.layout\" title=\"torch.layout\"><code>torch.layout</code></a>.</p>  <h2 id=\"dtype-doc\">torch.dtype</h2> <dl class=\"py class\" id=\"torch-dtype\"> <dt class=\"sig sig-object py\" id=\"torch.dtype\">\n<code>class torch.dtype</code> </dt> \n</dl> <p>A <a class=\"reference internal\" href=\"#torch.dtype\" title=\"torch.dtype\"><code>torch.dtype</code></a> is an object that represents the data type of a <a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a>. PyTorch has twelve different data types:</p> <table class=\"docutils colwidths-auto align-default\"> <thead> <tr>\n<th class=\"head\"><p>Data type</p></th> <th class=\"head\"><p>dtype</p></th> <th class=\"head\"><p>Legacy Constructors</p></th> </tr> </thead>  <tr>\n<td><p>32-bit floating point</p></td> <td><p><code>torch.float32</code> or <code>torch.float</code></p></td> <td><p><code>torch.*.FloatTensor</code></p></td> </tr> <tr>\n<td><p>64-bit floating point</p></td> <td><p><code>torch.float64</code> or <code>torch.double</code></p></td> <td><p><code>torch.*.DoubleTensor</code></p></td> </tr> <tr>\n<td><p>64-bit complex</p></td> <td><p><code>torch.complex64</code> or <code>torch.cfloat</code></p></td> <td></td> </tr> <tr>\n<td><p>128-bit complex</p></td> <td><p><code>torch.complex128</code> or <code>torch.cdouble</code></p></td> <td></td> </tr> <tr>\n<td><p>16-bit floating point <a class=\"footnote-reference brackets\" href=\"#id3\" id=\"id1\">1</a></p></td> <td><p><code>torch.float16</code> or <code>torch.half</code></p></td> <td><p><code>torch.*.HalfTensor</code></p></td> </tr> <tr>\n<td><p>16-bit floating point <a class=\"footnote-reference brackets\" href=\"#id4\" id=\"id2\">2</a></p></td> <td><p><code>torch.bfloat16</code></p></td> <td><p><code>torch.*.BFloat16Tensor</code></p></td> </tr> <tr>\n<td><p>8-bit integer (unsigned)</p></td> <td><p><code>torch.uint8</code></p></td> <td><p><code>torch.*.ByteTensor</code></p></td> </tr> <tr>\n<td><p>8-bit integer (signed)</p></td> <td><p><code>torch.int8</code></p></td> <td><p><code>torch.*.CharTensor</code></p></td> </tr> <tr>\n<td><p>16-bit integer (signed)</p></td> <td><p><code>torch.int16</code> or <code>torch.short</code></p></td> <td><p><code>torch.*.ShortTensor</code></p></td> </tr> <tr>\n<td><p>32-bit integer (signed)</p></td> <td><p><code>torch.int32</code> or <code>torch.int</code></p></td> <td><p><code>torch.*.IntTensor</code></p></td> </tr> <tr>\n<td><p>64-bit integer (signed)</p></td> <td><p><code>torch.int64</code> or <code>torch.long</code></p></td> <td><p><code>torch.*.LongTensor</code></p></td> </tr> <tr>\n<td><p>Boolean</p></td> <td><p><code>torch.bool</code></p></td> <td><p><code>torch.*.BoolTensor</code></p></td> </tr>  </table> <dl class=\"footnote brackets\"> <dt class=\"label\" id=\"id3\">\n<code>1</code> </dt> <dd>\n<p>Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10 significand bits. Useful when precision is important.</p> </dd> <dt class=\"label\" id=\"id4\">\n<code>2</code> </dt> <dd>\n<p>Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7 significand bits. Useful when range is important, since it has the same number of exponent bits as <code>float32</code></p> </dd> </dl> <p>To find out if a <a class=\"reference internal\" href=\"#torch.dtype\" title=\"torch.dtype\"><code>torch.dtype</code></a> is a floating point data type, the property <a class=\"reference internal\" href=\"generated/torch.is_floating_point#torch.is_floating_point\" title=\"torch.is_floating_point\"><code>is_floating_point</code></a> can be used, which returns <code>True</code> if the data type is a floating point data type.</p> <p>To find out if a <a class=\"reference internal\" href=\"#torch.dtype\" title=\"torch.dtype\"><code>torch.dtype</code></a> is a complex data type, the property <a class=\"reference internal\" href=\"generated/torch.is_complex#torch.is_complex\" title=\"torch.is_complex\"><code>is_complex</code></a> can be used, which returns <code>True</code> if the data type is a complex data type.</p> <p id=\"type-promotion-doc\">When the dtypes of inputs to an arithmetic operation (<code>add</code>, <code>sub</code>, <code>div</code>, <code>mul</code>) differ, we promote by finding the minimum dtype that satisfies the following rules:</p> <ul class=\"simple\"> <li>If the type of a scalar operand is of a higher category than tensor operands (where complex &gt; floating &gt; integral &gt; boolean), we promote to a type with sufficient size to hold all scalar operands of that category.</li> <li>If a zero-dimension tensor operand has a higher category than dimensioned operands, we promote to a type with sufficient size and category to hold all zero-dim tensor operands of that category.</li> <li>If there are no higher-category zero-dim operands, we promote to a type with sufficient size and category to hold all dimensioned operands.</li> </ul> <p>A floating point scalar operand has dtype <code>torch.get_default_dtype()</code> and an integral non-boolean scalar operand has dtype <code>torch.int64</code>. Unlike numpy, we do not inspect values when determining the minimum <code>dtypes</code> of an operand. Quantized and complex types are not yet supported.</p> <p>Promotion Examples:</p> <pre data-language=\"python\">&gt;&gt;&gt; float_tensor = torch.ones(1, dtype=torch.float)\n&gt;&gt;&gt; double_tensor = torch.ones(1, dtype=torch.double)\n&gt;&gt;&gt; complex_float_tensor = torch.ones(1, dtype=torch.complex64)\n&gt;&gt;&gt; complex_double_tensor = torch.ones(1, dtype=torch.complex128)\n&gt;&gt;&gt; int_tensor = torch.ones(1, dtype=torch.int)\n&gt;&gt;&gt; long_tensor = torch.ones(1, dtype=torch.long)\n&gt;&gt;&gt; uint_tensor = torch.ones(1, dtype=torch.uint8)\n&gt;&gt;&gt; double_tensor = torch.ones(1, dtype=torch.double)\n&gt;&gt;&gt; bool_tensor = torch.ones(1, dtype=torch.bool)\n# zero-dim tensors\n&gt;&gt;&gt; long_zerodim = torch.tensor(1, dtype=torch.long)\n&gt;&gt;&gt; int_zerodim = torch.tensor(1, dtype=torch.int)\n\n&gt;&gt;&gt; torch.add(5, 5).dtype\ntorch.int64\n# 5 is an int64, but does not have higher category than int_tensor so is not considered.\n&gt;&gt;&gt; (int_tensor + 5).dtype\ntorch.int32\n&gt;&gt;&gt; (int_tensor + long_zerodim).dtype\ntorch.int32\n&gt;&gt;&gt; (long_tensor + int_tensor).dtype\ntorch.int64\n&gt;&gt;&gt; (bool_tensor + long_tensor).dtype\ntorch.int64\n&gt;&gt;&gt; (bool_tensor + uint_tensor).dtype\ntorch.uint8\n&gt;&gt;&gt; (float_tensor + double_tensor).dtype\ntorch.float64\n&gt;&gt;&gt; (complex_float_tensor + complex_double_tensor).dtype\ntorch.complex128\n&gt;&gt;&gt; (bool_tensor + int_tensor).dtype\ntorch.int32\n# Since long is a different kind than float, result dtype only needs to be large enough\n# to hold the float.\n&gt;&gt;&gt; torch.add(long_tensor, float_tensor).dtype\ntorch.float32\n</pre> <dl class=\"simple\"> <dt>\n<code>When the output tensor of an arithmetic operation is specified, we allow casting to its dtype except that:</code> </dt>\n<dd>\n<ul class=\"simple\"> <li>An integral output tensor cannot accept a floating point tensor.</li> <li>A boolean output tensor cannot accept a non-boolean tensor.</li> <li>A non-complex output tensor cannot accept a complex tensor</li> </ul> </dd> </dl> <p>Casting Examples:</p> <pre data-language=\"python\"># allowed:\n&gt;&gt;&gt; float_tensor *= float_tensor\n&gt;&gt;&gt; float_tensor *= int_tensor\n&gt;&gt;&gt; float_tensor *= uint_tensor\n&gt;&gt;&gt; float_tensor *= bool_tensor\n&gt;&gt;&gt; float_tensor *= double_tensor\n&gt;&gt;&gt; int_tensor *= long_tensor\n&gt;&gt;&gt; int_tensor *= uint_tensor\n&gt;&gt;&gt; uint_tensor *= int_tensor\n\n# disallowed (RuntimeError: result type can't be cast to the desired output type):\n&gt;&gt;&gt; int_tensor *= float_tensor\n&gt;&gt;&gt; bool_tensor *= int_tensor\n&gt;&gt;&gt; bool_tensor *= uint_tensor\n&gt;&gt;&gt; float_tensor *= complex_float_tensor\n</pre>   <h2 id=\"device-doc\">torch.device</h2> <dl class=\"py class\" id=\"torch-device\"> <dt class=\"sig sig-object py\" id=\"torch.device\">\n<code>class torch.device</code> </dt> \n</dl> <p>A <a class=\"reference internal\" href=\"#torch.device\" title=\"torch.device\"><code>torch.device</code></a> is an object representing the device on which a <a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a> is or will be allocated.</p> <p>The <a class=\"reference internal\" href=\"#torch.device\" title=\"torch.device\"><code>torch.device</code></a> contains a device type (<code>'cpu'</code>, <code>'cuda'</code> or <code>'mps'</code>) and optional device ordinal for the device type. If the device ordinal is not present, this object will always represent the current device for the device type, even after <a class=\"reference internal\" href=\"generated/torch.cuda.set_device#torch.cuda.set_device\" title=\"torch.cuda.set_device\"><code>torch.cuda.set_device()</code></a> is called; e.g., a <a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a> constructed with device <code>'cuda'</code> is equivalent to <code>'cuda:X'</code> where X is the result of <a class=\"reference internal\" href=\"generated/torch.cuda.current_device#torch.cuda.current_device\" title=\"torch.cuda.current_device\"><code>torch.cuda.current_device()</code></a>.</p> <p>A <a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a>s device can be accessed via the <a class=\"reference internal\" href=\"generated/torch.tensor.device#torch.Tensor.device\" title=\"torch.Tensor.device\"><code>Tensor.device</code></a> property.</p> <p>A <a class=\"reference internal\" href=\"#torch.device\" title=\"torch.device\"><code>torch.device</code></a> can be constructed via a string or via a string and device ordinal</p> <p>Via a string:</p> <pre data-language=\"python\">&gt;&gt;&gt; torch.device('cuda:0')\ndevice(type='cuda', index=0)\n\n&gt;&gt;&gt; torch.device('cpu')\ndevice(type='cpu')\n\n&gt;&gt;&gt; torch.device('mps')\ndevice(type='mps')\n\n&gt;&gt;&gt; torch.device('cuda')  # current cuda device\ndevice(type='cuda')\n</pre> <p>Via a string and device ordinal:</p> <pre data-language=\"python\">&gt;&gt;&gt; torch.device('cuda', 0)\ndevice(type='cuda', index=0)\n\n&gt;&gt;&gt; torch.device('mps', 0)\ndevice(type='mps', index=0)\n\n&gt;&gt;&gt; torch.device('cpu', 0)\ndevice(type='cpu', index=0)\n</pre> <p>The device object can also be used as a context manager to change the default device tensors are allocated on:</p> <pre data-language=\"python\">&gt;&gt;&gt; with torch.device('cuda:1'):\n...     r = torch.randn(2, 3)\n&gt;&gt;&gt; r.device\ndevice(type='cuda', index=1)\n</pre> <p>This context manager has no effect if a factory function is passed an explicit, non-None device argument. To globally change the default device, see also <a class=\"reference internal\" href=\"generated/torch.set_default_device#torch.set_default_device\" title=\"torch.set_default_device\"><code>torch.set_default_device()</code></a>.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This function imposes a slight performance cost on every Python call to the torch API (not just factory functions). If this is causing problems for you, please comment on <a class=\"reference external\" href=\"https://github.com/pytorch/pytorch/issues/92701\">https://github.com/pytorch/pytorch/issues/92701</a></p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The <a class=\"reference internal\" href=\"#torch.device\" title=\"torch.device\"><code>torch.device</code></a> argument in functions can generally be substituted with a string. This allows for fast prototyping of code.</p> <pre data-language=\"python\">&gt;&gt;&gt; # Example of a function that takes in a torch.device\n&gt;&gt;&gt; cuda1 = torch.device('cuda:1')\n&gt;&gt;&gt; torch.randn((2,3), device=cuda1)\n</pre> <pre data-language=\"python\">&gt;&gt;&gt; # You can substitute the torch.device with a string\n&gt;&gt;&gt; torch.randn((2,3), device='cuda:1')\n</pre> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>For legacy reasons, a device can be constructed via a single device ordinal, which is treated as a cuda device. This matches <a class=\"reference internal\" href=\"generated/torch.tensor.get_device#torch.Tensor.get_device\" title=\"torch.Tensor.get_device\"><code>Tensor.get_device()</code></a>, which returns an ordinal for cuda tensors and is not supported for cpu tensors.</p> <pre data-language=\"python\">&gt;&gt;&gt; torch.device(1)\ndevice(type='cuda', index=1)\n</pre> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Methods which take a device will generally accept a (properly formatted) string or (legacy) integer device ordinal, i.e. the following are all equivalent:</p> <pre data-language=\"python\">&gt;&gt;&gt; torch.randn((2,3), device=torch.device('cuda:1'))\n&gt;&gt;&gt; torch.randn((2,3), device='cuda:1')\n&gt;&gt;&gt; torch.randn((2,3), device=1)  # legacy\n</pre> </div>   <h2 id=\"layout-doc\">torch.layout</h2> <dl class=\"py class\" id=\"torch-layout\"> <dt class=\"sig sig-object py\" id=\"torch.layout\">\n<code>class torch.layout</code> </dt> \n</dl> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>The <code>torch.layout</code> class is in beta and subject to change.</p> </div> <p>A <a class=\"reference internal\" href=\"#torch.layout\" title=\"torch.layout\"><code>torch.layout</code></a> is an object that represents the memory layout of a <a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a>. Currently, we support <code>torch.strided</code> (dense Tensors) and have beta support for <code>torch.sparse_coo</code> (sparse COO Tensors).</p> <p><code>torch.strided</code> represents dense Tensors and is the memory layout that is most commonly used. Each strided tensor has an associated <code>torch.Storage</code>, which holds its data. These tensors provide multi-dimensional, <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Stride_of_an_array\">strided</a> view of a storage. Strides are a list of integers: the k-th stride represents the jump in the memory necessary to go from one element to the next one in the k-th dimension of the Tensor. This concept makes it possible to perform many tensor operations efficiently.</p> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n&gt;&gt;&gt; x.stride()\n(5, 1)\n\n&gt;&gt;&gt; x.t().stride()\n(1, 5)\n</pre> <p>For more information on <code>torch.sparse_coo</code> tensors, see <a class=\"reference internal\" href=\"sparse#sparse-docs\"><span class=\"std std-ref\">torch.sparse</span></a>.</p>   <h2 id=\"torch-memory-format\">torch.memory_format</h2> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.memory_format\">\n<code>class torch.memory_format</code> </dt> \n</dl> <p>A <a class=\"reference internal\" href=\"#torch.memory_format\" title=\"torch.memory_format\"><code>torch.memory_format</code></a> is an object representing the memory format on which a <a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a> is or will be allocated.</p> <p>Possible values are:</p> <ul class=\"simple\"> <li>\n<code>torch.contiguous_format</code>: Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in decreasing order.</li> <li>\n<code>torch.channels_last</code>: Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in <code>strides[0] &gt; strides[2] &gt; strides[3] &gt; strides[1] == 1</code> aka NHWC order.</li> <li>\n<code>torch.channels_last_3d</code>: Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in <code>strides[0] &gt; strides[2] &gt; strides[3] &gt; strides[4] &gt; strides[1] == 1</code> aka NDHWC order.</li> <li>\n<code>torch.preserve_format</code>: Used in functions like <code>clone</code> to preserve the memory format of the input tensor. If input tensor is allocated in dense non-overlapping memory, the output tensor strides will be copied from the input. Otherwise output strides will follow <code>torch.contiguous_format</code>\n</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href=\"https://github.com/pytorch/pytorch/blob/main/LICENSE\">LICENSE</a> file.<br>\n    <a href=\"https://pytorch.org/docs/2.1/tensor_attributes.html\" class=\"_attribution-link\">https://pytorch.org/docs/2.1/tensor_attributes.html</a>\n  </p>\n</div>\n","amp":"<h1 id=\"automatic-mixed-precision-package-torch-amp\">Automatic Mixed Precision package - torch.amp</h1> <p id=\"module-torch.amp\"><a class=\"reference internal\" href=\"#module-torch.amp\" title=\"torch.amp\"><code>torch.amp</code></a> provides convenience methods for mixed precision, where some operations use the <code>torch.float32</code> (<code>float</code>) datatype and other operations use lower precision floating point datatype (<code>lower_precision_fp</code>): <code>torch.float16</code> (<code>half</code>) or <code>torch.bfloat16</code>. Some ops, like linear layers and convolutions, are much faster in <code>lower_precision_fp</code>. Other ops, like reductions, often require the dynamic range of <code>float32</code>. Mixed precision tries to match each op to its appropriate datatype.</p> <p>Ordinarily, automatic mixed precision training with datatype of <code>torch.float16</code> uses <a class=\"reference internal\" href=\"#torch.autocast\" title=\"torch.autocast\"><code>torch.autocast</code></a> and <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler\" title=\"torch.cuda.amp.GradScaler\"><code>torch.cuda.amp.GradScaler</code></a> together, as shown in the <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/amp_examples.html#amp-examples\"><span class=\"std std-ref\">CUDA Automatic Mixed Precision examples</span></a> and <a class=\"reference external\" href=\"https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html\">CUDA Automatic Mixed Precision recipe</a>. However, <a class=\"reference internal\" href=\"#torch.autocast\" title=\"torch.autocast\"><code>torch.autocast</code></a> and <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler\" title=\"torch.cuda.amp.GradScaler\"><code>torch.cuda.amp.GradScaler</code></a> are modular, and may be used separately if desired. As shown in the CPU example section of <a class=\"reference internal\" href=\"#torch.autocast\" title=\"torch.autocast\"><code>torch.autocast</code></a>, automatic mixed precision training/inference on CPU with datatype of <code>torch.bfloat16</code> only uses <a class=\"reference internal\" href=\"#torch.autocast\" title=\"torch.autocast\"><code>torch.autocast</code></a>.</p> <p>For CUDA and CPU, APIs are also provided separately:</p> <ul class=\"simple\"> <li>\n<code>torch.autocast(\"cuda\", args...)</code> is equivalent to <code>torch.cuda.amp.autocast(args...)</code>.</li> <li>\n<code>torch.autocast(\"cpu\", args...)</code> is equivalent to <code>torch.cpu.amp.autocast(args...)</code>. For CPU, only lower precision floating point datatype of <code>torch.bfloat16</code> is supported for now.</li> </ul> <p><a class=\"reference internal\" href=\"#torch.autocast\" title=\"torch.autocast\"><code>torch.autocast</code></a> and <a class=\"reference internal\" href=\"#torch.cpu.amp.autocast\" title=\"torch.cpu.amp.autocast\"><code>torch.cpu.amp.autocast</code></a> are new in version <code>1.10</code>.</p>  <ul class=\"simple\"> <li><a class=\"reference internal\" href=\"#autocasting\" id=\"id4\">Autocasting</a></li> <li><a class=\"reference internal\" href=\"#gradient-scaling\" id=\"id5\">Gradient Scaling</a></li> <li>\n<p><a class=\"reference internal\" href=\"#autocast-op-reference\" id=\"id6\">Autocast Op Reference</a></p> <ul> <li><a class=\"reference internal\" href=\"#op-eligibility\" id=\"id7\">Op Eligibility</a></li> <li>\n<p><a class=\"reference internal\" href=\"#cuda-op-specific-behavior\" id=\"id8\">CUDA Op-Specific Behavior</a></p> <ul> <li><a class=\"reference internal\" href=\"#cuda-ops-that-can-autocast-to-float16\" id=\"id9\">CUDA Ops that can autocast to <code>float16</code></a></li> <li><a class=\"reference internal\" href=\"#cuda-ops-that-can-autocast-to-float32\" id=\"id10\">CUDA Ops that can autocast to <code>float32</code></a></li> <li><a class=\"reference internal\" href=\"#cuda-ops-that-promote-to-the-widest-input-type\" id=\"id11\">CUDA Ops that promote to the widest input type</a></li> <li><a class=\"reference internal\" href=\"#prefer-binary-cross-entropy-with-logits-over-binary-cross-entropy\" id=\"id12\">Prefer <code>binary_cross_entropy_with_logits</code> over <code>binary_cross_entropy</code></a></li> </ul> </li> <li>\n<p><a class=\"reference internal\" href=\"#cpu-op-specific-behavior\" id=\"id13\">CPU Op-Specific Behavior</a></p> <ul> <li><a class=\"reference internal\" href=\"#cpu-ops-that-can-autocast-to-bfloat16\" id=\"id14\">CPU Ops that can autocast to <code>bfloat16</code></a></li> <li><a class=\"reference internal\" href=\"#cpu-ops-that-can-autocast-to-float32\" id=\"id15\">CPU Ops that can autocast to <code>float32</code></a></li> <li><a class=\"reference internal\" href=\"#cpu-ops-that-promote-to-the-widest-input-type\" id=\"id16\">CPU Ops that promote to the widest input type</a></li> </ul> </li> </ul> </li> </ul>   <h2 id=\"id1\">Autocasting</h2> <dl class=\"py class\" id=\"autocasting\"> <dt class=\"sig sig-object py\" id=\"torch.autocast\">\n<code>class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/amp/autocast_mode.html#autocast\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Instances of <a class=\"reference internal\" href=\"#torch.autocast\" title=\"torch.autocast\"><code>autocast</code></a> serve as context managers or decorators that allow regions of your script to run in mixed precision.</p> <p>In these regions, ops run in an op-specific dtype chosen by autocast to improve performance while maintaining accuracy. See the <a class=\"reference internal\" href=\"#autocast-op-reference\"><span class=\"std std-ref\">Autocast Op Reference</span></a> for details.</p> <p>When entering an autocast-enabled region, Tensors may be any type. You should not call <code>half()</code> or <code>bfloat16()</code> on your model(s) or inputs when using autocasting.</p> <p><a class=\"reference internal\" href=\"#torch.autocast\" title=\"torch.autocast\"><code>autocast</code></a> should wrap only the forward pass(es) of your network, including the loss computation(s). Backward passes under autocast are not recommended. Backward ops run in the same type that autocast used for corresponding forward ops.</p> <p>Example for CUDA Devices:</p> <pre data-language=\"python\"># Creates model and optimizer in default precision\nmodel = Net().cuda()\noptimizer = optim.SGD(model.parameters(), ...)\n\nfor input, target in data:\n    optimizer.zero_grad()\n\n    # Enables autocasting for the forward pass (model + loss)\n    with torch.autocast(device_type=\"cuda\"):\n        output = model(input)\n        loss = loss_fn(output, target)\n\n    # Exits the context manager before backward()\n    loss.backward()\n    optimizer.step()\n</pre> <p>See the <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/amp_examples.html#amp-examples\"><span class=\"std std-ref\">CUDA Automatic Mixed Precision examples</span></a> for usage (along with gradient scaling) in more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions).</p> <p><a class=\"reference internal\" href=\"#torch.autocast\" title=\"torch.autocast\"><code>autocast</code></a> can also be used as a decorator, e.g., on the <code>forward</code> method of your model:</p> <pre data-language=\"python\">class AutocastModel(nn.Module):\n    ...\n    @torch.autocast(device_type=\"cuda\")\n    def forward(self, input):\n        ...\n</pre> <p>Floating-point Tensors produced in an autocast-enabled region may be <code>float16</code>. After returning to an autocast-disabled region, using them with floating-point Tensors of different dtypes may cause type mismatch errors. If so, cast the Tensor(s) produced in the autocast region back to <code>float32</code> (or other dtype if desired). If a Tensor from the autocast region is already <code>float32</code>, the cast is a no-op, and incurs no additional overhead. CUDA Example:</p> <pre data-language=\"python\"># Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith torch.autocast(device_type=\"cuda\"):\n    # torch.mm is on autocast's list of ops that should run in float16.\n    # Inputs are float32, but the op runs in float16 and produces float16 output.\n    # No manual casts are required.\n    e_float16 = torch.mm(a_float32, b_float32)\n    # Also handles mixed input types\n    f_float16 = torch.mm(d_float32, e_float16)\n\n# After exiting autocast, calls f_float16.float() to use with d_float32\ng_float32 = torch.mm(d_float32, f_float16.float())\n</pre> <p>CPU Training Example:</p> <pre data-language=\"python\"># Creates model and optimizer in default precision\nmodel = Net()\noptimizer = optim.SGD(model.parameters(), ...)\n\nfor epoch in epochs:\n    for input, target in data:\n        optimizer.zero_grad()\n\n        # Runs the forward pass with autocasting.\n        with torch.autocast(device_type=\"cpu\", dtype=torch.bfloat16):\n            output = model(input)\n            loss = loss_fn(output, target)\n\n        loss.backward()\n        optimizer.step()\n</pre> <p>CPU Inference Example:</p> <pre data-language=\"python\"># Creates model in default precision\nmodel = Net().eval()\n\nwith torch.autocast(device_type=\"cpu\", dtype=torch.bfloat16):\n    for input in data:\n        # Runs the forward pass with autocasting.\n        output = model(input)\n</pre> <p>CPU Inference Example with Jit Trace:</p> <pre data-language=\"python\">class TestModel(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, num_classes)\n    def forward(self, x):\n        return self.fc1(x)\n\ninput_size = 2\nnum_classes = 2\nmodel = TestModel(input_size, num_classes).eval()\n\n# For now, we suggest to disable the Jit Autocast Pass,\n# As the issue: https://github.com/pytorch/pytorch/issues/75956\ntorch._C._jit_set_autocast_mode(False)\n\nwith torch.cpu.amp.autocast(cache_enabled=False):\n    model = torch.jit.trace(model, torch.randn(1, input_size))\nmodel = torch.jit.freeze(model)\n# Models Run\nfor _ in range(3):\n    model(torch.randn(1, input_size))\n</pre> <p>Type mismatch errors <em>in</em> an autocast-enabled region are a bug; if this is what you observe, please file an issue.</p> <p><code>autocast(enabled=False)</code> subregions can be nested in autocast-enabled regions. Locally disabling autocast can be useful, for example, if you want to force a subregion to run in a particular <code>dtype</code>. Disabling autocast gives you explicit control over the execution type. In the subregion, inputs from the surrounding region should be cast to <code>dtype</code> before use:</p> <pre data-language=\"python\"># Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith torch.autocast(device_type=\"cuda\"):\n    e_float16 = torch.mm(a_float32, b_float32)\n    with torch.autocast(device_type=\"cuda\", enabled=False):\n        # Calls e_float16.float() to ensure float32 execution\n        # (necessary because e_float16 was created in an autocasted region)\n        f_float32 = torch.mm(c_float32, e_float16.float())\n\n    # No manual casts are required when re-entering the autocast-enabled region.\n    # torch.mm again runs in float16 and produces float16 output, regardless of input types.\n    g_float16 = torch.mm(d_float32, f_float32)\n</pre> <p>The autocast state is thread-local. If you want it enabled in a new thread, the context manager or decorator must be invoked in that thread. This affects <a class=\"reference internal\" href=\"generated/torch.nn.dataparallel#torch.nn.DataParallel\" title=\"torch.nn.DataParallel\"><code>torch.nn.DataParallel</code></a> and <a class=\"reference internal\" href=\"generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel\" title=\"torch.nn.parallel.DistributedDataParallel\"><code>torch.nn.parallel.DistributedDataParallel</code></a> when used with more than one GPU per process (see <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/amp_examples.html#amp-multigpu\"><span class=\"std std-ref\">Working with Multiple GPUs</span></a>).</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>device_type</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a><em>, </em><em>required</em>)  Device type to use. Possible values are: cuda, cpu, xpu and hpu. The type is the same as the <code>type</code> attribute of a <a class=\"reference internal\" href=\"tensor_attributes#torch.device\" title=\"torch.device\"><code>torch.device</code></a>. Thus, you may obtain the device type of a tensor using <code>Tensor.device.type</code>.</li> <li>\n<strong>enabled</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Whether autocasting should be enabled in the region. Default: <code>True</code>\n</li> <li>\n<strong>dtype</strong> (<em>torch_dtype</em><em>, </em><em>optional</em>)  Whether to use torch.float16 or torch.bfloat16.</li> <li>\n<strong>cache_enabled</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Whether the weight cache inside autocast should be enabled. Default: <code>True</code>\n</li> </ul> </dd> </dl> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.cuda.amp.autocast\">\n<code>class torch.cuda.amp.autocast(enabled=True, dtype=torch.float16, cache_enabled=True)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/cuda/amp/autocast_mode.html#autocast\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>See <a class=\"reference internal\" href=\"#torch.autocast\" title=\"torch.autocast\"><code>torch.autocast</code></a>. <code>torch.cuda.amp.autocast(args...)</code> is equivalent to <code>torch.autocast(\"cuda\", args...)</code></p>  </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.cuda.amp.custom_fwd\">\n<code>torch.cuda.amp.custom_fwd(fwd=None, *, cast_inputs=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/cuda/amp/autocast_mode.html#custom_fwd\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Helper decorator for <code>forward</code> methods of custom autograd functions (subclasses of <a class=\"reference internal\" href=\"autograd#torch.autograd.Function\" title=\"torch.autograd.Function\"><code>torch.autograd.Function</code></a>). See the <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/amp_examples.html#amp-custom-examples\"><span class=\"std std-ref\">example page</span></a> for more detail.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>cast_inputs</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.dtype\" title=\"torch.dtype\"><code>torch.dtype</code></a> or None, optional, default=None)  If not <code>None</code>, when <code>forward</code> runs in an autocast-enabled region, casts incoming floating-point CUDA Tensors to the target dtype (non-floating-point Tensors are not affected), then executes <code>forward</code> with autocast disabled. If <code>None</code>, <code>forward</code>s internal ops execute with the current autocast state.</p> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>If the decorated <code>forward</code> is called outside an autocast-enabled region, <a class=\"reference internal\" href=\"#torch.cuda.amp.custom_fwd\" title=\"torch.cuda.amp.custom_fwd\"><code>custom_fwd</code></a> is a no-op and <code>cast_inputs</code> has no effect.</p> </div> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.cuda.amp.custom_bwd\">\n<code>torch.cuda.amp.custom_bwd(bwd)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/cuda/amp/autocast_mode.html#custom_bwd\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Helper decorator for backward methods of custom autograd functions (subclasses of <a class=\"reference internal\" href=\"autograd#torch.autograd.Function\" title=\"torch.autograd.Function\"><code>torch.autograd.Function</code></a>). Ensures that <code>backward</code> executes with the same autocast state as <code>forward</code>. See the <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/amp_examples.html#amp-custom-examples\"><span class=\"std std-ref\">example page</span></a> for more detail.</p> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.cpu.amp.autocast\">\n<code>class torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16, cache_enabled=True)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/cpu/amp/autocast_mode.html#autocast\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>See <a class=\"reference internal\" href=\"#torch.autocast\" title=\"torch.autocast\"><code>torch.autocast</code></a>. <code>torch.cpu.amp.autocast(args...)</code> is equivalent to <code>torch.autocast(\"cpu\", args...)</code></p>  </dd>\n</dl>   <h2 id=\"id2\">Gradient Scaling</h2> <p id=\"gradient-scaling\">If the forward pass for a particular op has <code>float16</code> inputs, the backward pass for that op will produce <code>float16</code> gradients. Gradient values with small magnitudes may not be representable in <code>float16</code>. These values will flush to zero (underflow), so the update for the corresponding parameters will be lost.</p> <p>To prevent underflow, gradient scaling multiplies the networks loss(es) by a scale factor and invokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are then scaled by the same factor. In other words, gradient values have a larger magnitude, so they dont flush to zero.</p> <p>Each parameters gradient (<code>.grad</code> attribute) should be unscaled before the optimizer updates the parameters, so the scale factor does not interfere with the learning rate.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>AMP/fp16 may not work for every model! For example, most bf16-pretrained models cannot operate in the fp16 numerical range of max 65504 and will cause gradients to overflow instead of underflow. In this case, the scale factor may decrease under 1 as an attempt to bring gradients to a number representable in the fp16 dynamic range. While one may expect the scale to always be above 1, our GradScaler does NOT make this guarantee to maintain performance. If you encounter NaNs in your loss or gradients when running with AMP/fp16, verify your model is compatible.</p> </div> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.cuda.amp.GradScaler\">\n<code>class torch.cuda.amp.GradScaler(init_scale=65536.0, growth_factor=2.0, backoff_factor=0.5, growth_interval=2000, enabled=True)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/cuda/amp/grad_scaler.html#GradScaler\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.cuda.amp.GradScaler.get_backoff_factor\">\n<code>get_backoff_factor()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.get_backoff_factor\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns a Python float containing the scale backoff factor.</p> </dd>\n</dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.cuda.amp.GradScaler.get_growth_factor\">\n<code>get_growth_factor()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.get_growth_factor\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns a Python float containing the scale growth factor.</p> </dd>\n</dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.cuda.amp.GradScaler.get_growth_interval\">\n<code>get_growth_interval()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.get_growth_interval\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns a Python int containing the growth interval.</p> </dd>\n</dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.cuda.amp.GradScaler.get_scale\">\n<code>get_scale()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.get_scale\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns a Python float containing the current scale, or 1.0 if scaling is disabled.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.get_scale\" title=\"torch.cuda.amp.GradScaler.get_scale\"><code>get_scale()</code></a> incurs a CPU-GPU sync.</p> </div> </dd>\n</dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.cuda.amp.GradScaler.is_enabled\">\n<code>is_enabled()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.is_enabled\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns a bool indicating whether this instance is enabled.</p> </dd>\n</dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.cuda.amp.GradScaler.load_state_dict\">\n<code>load_state_dict(state_dict)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.load_state_dict\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Loads the scaler state. If this instance is disabled, <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.load_state_dict\" title=\"torch.cuda.amp.GradScaler.load_state_dict\"><code>load_state_dict()</code></a> is a no-op.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>state_dict</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#dict\" title=\"(in Python v3.12)\">dict</a>)  scaler state. Should be an object returned from a call to <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.state_dict\" title=\"torch.cuda.amp.GradScaler.state_dict\"><code>state_dict()</code></a>.</p> </dd> </dl> </dd>\n</dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.cuda.amp.GradScaler.scale\">\n<code>scale(outputs)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.scale\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Multiplies (scales) a tensor or list of tensors by the scale factor.</p> <p>Returns scaled outputs. If this instance of <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler\" title=\"torch.cuda.amp.GradScaler\"><code>GradScaler</code></a> is not enabled, outputs are returned unmodified.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>outputs</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em> or </em><em>iterable</em><em> of </em><em>Tensors</em>)  Outputs to scale.</p> </dd> </dl> </dd>\n</dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.cuda.amp.GradScaler.set_backoff_factor\">\n<code>set_backoff_factor(new_factor)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.set_backoff_factor\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>new_scale</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.12)\">float</a>)  Value to use as the new scale backoff factor.</p> </dd> </dl> </dd>\n</dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.cuda.amp.GradScaler.set_growth_factor\">\n<code>set_growth_factor(new_factor)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.set_growth_factor\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>new_scale</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.12)\">float</a>)  Value to use as the new scale growth factor.</p> </dd> </dl> </dd>\n</dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.cuda.amp.GradScaler.set_growth_interval\">\n<code>set_growth_interval(new_interval)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.set_growth_interval\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>new_interval</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a>)  Value to use as the new growth interval.</p> </dd> </dl> </dd>\n</dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.cuda.amp.GradScaler.state_dict\">\n<code>state_dict()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.state_dict\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the state of the scaler as a <a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#dict\" title=\"(in Python v3.12)\"><code>dict</code></a>. It contains five entries:</p> <ul class=\"simple\"> <li>\n<code>\"scale\"</code> - a Python float containing the current scale</li> <li>\n<code>\"growth_factor\"</code> - a Python float containing the current growth factor</li> <li>\n<code>\"backoff_factor\"</code> - a Python float containing the current backoff factor</li> <li>\n<code>\"growth_interval\"</code> - a Python int containing the current growth interval</li> <li>\n<code>\"_growth_tracker\"</code> - a Python int containing the number of recent consecutive unskipped steps.</li> </ul> <p>If this instance is not enabled, returns an empty dict.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>If you wish to checkpoint the scalers state after a particular iteration, <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.state_dict\" title=\"torch.cuda.amp.GradScaler.state_dict\"><code>state_dict()</code></a> should be called after <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.update\" title=\"torch.cuda.amp.GradScaler.update\"><code>update()</code></a>.</p> </div> </dd>\n</dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.cuda.amp.GradScaler.step\">\n<code>step(optimizer, *args, **kwargs)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.step\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p><a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.step\" title=\"torch.cuda.amp.GradScaler.step\"><code>step()</code></a> carries out the following two operations:</p> <ol class=\"arabic simple\"> <li>Internally invokes <code>unscale_(optimizer)</code> (unless <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.unscale_\" title=\"torch.cuda.amp.GradScaler.unscale_\"><code>unscale_()</code></a> was explicitly called for <code>optimizer</code> earlier in the iteration). As part of the <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.unscale_\" title=\"torch.cuda.amp.GradScaler.unscale_\"><code>unscale_()</code></a>, gradients are checked for infs/NaNs.</li> <li>If no inf/NaN gradients are found, invokes <code>optimizer.step()</code> using the unscaled gradients. Otherwise, <code>optimizer.step()</code> is skipped to avoid corrupting the params.</li> </ol> <p><code>*args</code> and <code>**kwargs</code> are forwarded to <code>optimizer.step()</code>.</p> <p>Returns the return value of <code>optimizer.step(*args, **kwargs)</code>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>optimizer</strong> (<a class=\"reference internal\" href=\"optim#torch.optim.Optimizer\" title=\"torch.optim.Optimizer\">torch.optim.Optimizer</a>)  Optimizer that applies the gradients.</li> <li>\n<strong>args</strong>  Any arguments.</li> <li>\n<strong>kwargs</strong>  Any keyword arguments.</li> </ul> </dd> </dl> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Closure use is not currently supported.</p> </div> </dd>\n</dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.cuda.amp.GradScaler.unscale_\">\n<code>unscale_(optimizer)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.unscale_\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Divides (unscales) the optimizers gradient tensors by the scale factor.</p> <p><a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.unscale_\" title=\"torch.cuda.amp.GradScaler.unscale_\"><code>unscale_()</code></a> is optional, serving cases where you need to <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/amp_examples.html#working-with-unscaled-gradients\"><span class=\"std std-ref\">modify or inspect gradients</span></a> between the backward pass(es) and <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.step\" title=\"torch.cuda.amp.GradScaler.step\"><code>step()</code></a>. If <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.unscale_\" title=\"torch.cuda.amp.GradScaler.unscale_\"><code>unscale_()</code></a> is not called explicitly, gradients will be unscaled automatically during <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.step\" title=\"torch.cuda.amp.GradScaler.step\"><code>step()</code></a>.</p> <p>Simple example, using <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.unscale_\" title=\"torch.cuda.amp.GradScaler.unscale_\"><code>unscale_()</code></a> to enable clipping of unscaled gradients:</p> <pre data-language=\"python\">...\nscaler.scale(loss).backward()\nscaler.unscale_(optimizer)\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\nscaler.step(optimizer)\nscaler.update()\n</pre> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>optimizer</strong> (<a class=\"reference internal\" href=\"optim#torch.optim.Optimizer\" title=\"torch.optim.Optimizer\">torch.optim.Optimizer</a>)  Optimizer that owns the gradients to be unscaled.</p> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p><a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.unscale_\" title=\"torch.cuda.amp.GradScaler.unscale_\"><code>unscale_()</code></a> does not incur a CPU-GPU sync.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.unscale_\" title=\"torch.cuda.amp.GradScaler.unscale_\"><code>unscale_()</code></a> should only be called once per optimizer per <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.step\" title=\"torch.cuda.amp.GradScaler.step\"><code>step()</code></a> call, and only after all gradients for that optimizers assigned parameters have been accumulated. Calling <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.unscale_\" title=\"torch.cuda.amp.GradScaler.unscale_\"><code>unscale_()</code></a> twice for a given optimizer between each <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.step\" title=\"torch.cuda.amp.GradScaler.step\"><code>step()</code></a> triggers a RuntimeError.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.unscale_\" title=\"torch.cuda.amp.GradScaler.unscale_\"><code>unscale_()</code></a> may unscale sparse gradients out of place, replacing the <code>.grad</code> attribute.</p> </div> </dd>\n</dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.cuda.amp.GradScaler.update\">\n<code>update(new_scale=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.update\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Updates the scale factor.</p> <p>If any optimizer steps were skipped the scale is multiplied by <code>backoff_factor</code> to reduce it. If <code>growth_interval</code> unskipped iterations occurred consecutively, the scale is multiplied by <code>growth_factor</code> to increase it.</p> <p>Passing <code>new_scale</code> sets the new scale value manually. (<code>new_scale</code> is not used directly, its used to fill GradScalers internal scale tensor. So if <code>new_scale</code> was a tensor, later in-place changes to that tensor will not further affect the scale GradScaler uses internally.)</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>new_scale</strong> (float or <code>torch.cuda.FloatTensor</code>, optional, default=None)  New scale factor.</p> </dd> </dl> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.update\" title=\"torch.cuda.amp.GradScaler.update\"><code>update()</code></a> should only be called at the end of the iteration, after <code>scaler.step(optimizer)</code> has been invoked for all optimizers used this iteration.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>For performance reasons, we do not check the scale factor value to avoid synchronizations, so the scale factor is not guaranteed to be above 1. If the scale falls below 1 and/or you are seeing NaNs in your gradients or loss, something is likely wrong. For example, bf16-pretrained models are often incompatible with AMP/fp16 due to differing dynamic ranges.</p> </div> </dd>\n</dl> </dd>\n</dl>   <h2 id=\"id3\">Autocast Op Reference</h2>  <h3 id=\"autocast-eligibility\">Op Eligibility</h3> <p id=\"autocast-op-reference\">Ops that run in <code>float64</code> or non-floating-point dtypes are not eligible, and will run in these types whether or not autocast is enabled.</p> <p>Only out-of-place ops and Tensor methods are eligible. In-place variants and calls that explicitly supply an <code>out=...</code> Tensor are allowed in autocast-enabled regions, but wont go through autocasting. For example, in an autocast-enabled region <code>a.addmm(b, c)</code> can autocast, but <code>a.addmm_(b, c)</code> and <code>a.addmm(b, c, out=d)</code> cannot. For best performance and stability, prefer out-of-place ops in autocast-enabled regions.</p> <p>Ops called with an explicit <code>dtype=...</code> argument are not eligible, and will produce output that respects the <code>dtype</code> argument.</p>   <h3 id=\"autocast-cuda-op-reference\">CUDA Op-Specific Behavior</h3> <p id=\"cuda-op-specific-behavior\">The following lists describe the behavior of eligible ops in autocast-enabled regions. These ops always go through autocasting whether they are invoked as part of a <a class=\"reference internal\" href=\"generated/torch.nn.module#torch.nn.Module\" title=\"torch.nn.Module\"><code>torch.nn.Module</code></a>, as a function, or as a <a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a> method. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace.</p> <p>Ops not listed below do not go through autocasting. They run in the type defined by their inputs. However, autocasting may still change the type in which unlisted ops run if theyre downstream from autocasted ops.</p> <p>If an op is unlisted, we assume its numerically stable in <code>float16</code>. If you believe an unlisted op is numerically unstable in <code>float16</code>, please file an issue.</p>  <h4 id=\"cuda-ops-that-can-autocast-to-float16\">CUDA Ops that can autocast to <code>float16</code>\n</h4> <p><code>__matmul__</code>, <code>addbmm</code>, <code>addmm</code>, <code>addmv</code>, <code>addr</code>, <code>baddbmm</code>, <code>bmm</code>, <code>chain_matmul</code>, <code>multi_dot</code>, <code>conv1d</code>, <code>conv2d</code>, <code>conv3d</code>, <code>conv_transpose1d</code>, <code>conv_transpose2d</code>, <code>conv_transpose3d</code>, <code>GRUCell</code>, <code>linear</code>, <code>LSTMCell</code>, <code>matmul</code>, <code>mm</code>, <code>mv</code>, <code>prelu</code>, <code>RNNCell</code></p>   <h4 id=\"cuda-ops-that-can-autocast-to-float32\">CUDA Ops that can autocast to <code>float32</code>\n</h4> <p><code>__pow__</code>, <code>__rdiv__</code>, <code>__rpow__</code>, <code>__rtruediv__</code>, <code>acos</code>, <code>asin</code>, <code>binary_cross_entropy_with_logits</code>, <code>cosh</code>, <code>cosine_embedding_loss</code>, <code>cdist</code>, <code>cosine_similarity</code>, <code>cross_entropy</code>, <code>cumprod</code>, <code>cumsum</code>, <code>dist</code>, <code>erfinv</code>, <code>exp</code>, <code>expm1</code>, <code>group_norm</code>, <code>hinge_embedding_loss</code>, <code>kl_div</code>, <code>l1_loss</code>, <code>layer_norm</code>, <code>log</code>, <code>log_softmax</code>, <code>log10</code>, <code>log1p</code>, <code>log2</code>, <code>margin_ranking_loss</code>, <code>mse_loss</code>, <code>multilabel_margin_loss</code>, <code>multi_margin_loss</code>, <code>nll_loss</code>, <code>norm</code>, <code>normalize</code>, <code>pdist</code>, <code>poisson_nll_loss</code>, <code>pow</code>, <code>prod</code>, <code>reciprocal</code>, <code>rsqrt</code>, <code>sinh</code>, <code>smooth_l1_loss</code>, <code>soft_margin_loss</code>, <code>softmax</code>, <code>softmin</code>, <code>softplus</code>, <code>sum</code>, <code>renorm</code>, <code>tan</code>, <code>triplet_margin_loss</code></p>   <h4 id=\"cuda-ops-that-promote-to-the-widest-input-type\">CUDA Ops that promote to the widest input type</h4> <p>These ops dont require a particular dtype for stability, but take multiple inputs and require that the inputs dtypes match. If all of the inputs are <code>float16</code>, the op runs in <code>float16</code>. If any of the inputs is <code>float32</code>, autocast casts all inputs to <code>float32</code> and runs the op in <code>float32</code>.</p> <p><code>addcdiv</code>, <code>addcmul</code>, <code>atan2</code>, <code>bilinear</code>, <code>cross</code>, <code>dot</code>, <code>grid_sample</code>, <code>index_put</code>, <code>scatter_add</code>, <code>tensordot</code></p> <p>Some ops not listed here (e.g., binary ops like <code>add</code>) natively promote inputs without autocastings intervention. If inputs are a mixture of <code>float16</code> and <code>float32</code>, these ops run in <code>float32</code> and produce <code>float32</code> output, regardless of whether autocast is enabled.</p>   <h4 id=\"prefer-binary-cross-entropy-with-logits-over-binary-cross-entropy\">Prefer <code>binary_cross_entropy_with_logits</code> over <code>binary_cross_entropy</code>\n</h4> <p>The backward passes of <a class=\"reference internal\" href=\"generated/torch.nn.functional.binary_cross_entropy#torch.nn.functional.binary_cross_entropy\" title=\"torch.nn.functional.binary_cross_entropy\"><code>torch.nn.functional.binary_cross_entropy()</code></a> (and <a class=\"reference internal\" href=\"generated/torch.nn.bceloss#torch.nn.BCELoss\" title=\"torch.nn.BCELoss\"><code>torch.nn.BCELoss</code></a>, which wraps it) can produce gradients that arent representable in <code>float16</code>. In autocast-enabled regions, the forward input may be <code>float16</code>, which means the backward gradient must be representable in <code>float16</code> (autocasting <code>float16</code> forward inputs to <code>float32</code> doesnt help, because that cast must be reversed in backward). Therefore, <code>binary_cross_entropy</code> and <code>BCELoss</code> raise an error in autocast-enabled regions.</p> <p>Many models use a sigmoid layer right before the binary cross entropy layer. In this case, combine the two layers using <a class=\"reference internal\" href=\"generated/torch.nn.functional.binary_cross_entropy_with_logits#torch.nn.functional.binary_cross_entropy_with_logits\" title=\"torch.nn.functional.binary_cross_entropy_with_logits\"><code>torch.nn.functional.binary_cross_entropy_with_logits()</code></a> or <a class=\"reference internal\" href=\"generated/torch.nn.bcewithlogitsloss#torch.nn.BCEWithLogitsLoss\" title=\"torch.nn.BCEWithLogitsLoss\"><code>torch.nn.BCEWithLogitsLoss</code></a>. <code>binary_cross_entropy_with_logits</code> and <code>BCEWithLogits</code> are safe to autocast.</p>    <h3 id=\"autocast-cpu-op-reference\">CPU Op-Specific Behavior</h3> <p id=\"cpu-op-specific-behavior\">The following lists describe the behavior of eligible ops in autocast-enabled regions. These ops always go through autocasting whether they are invoked as part of a <a class=\"reference internal\" href=\"generated/torch.nn.module#torch.nn.Module\" title=\"torch.nn.Module\"><code>torch.nn.Module</code></a>, as a function, or as a <a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a> method. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace.</p> <p>Ops not listed below do not go through autocasting. They run in the type defined by their inputs. However, autocasting may still change the type in which unlisted ops run if theyre downstream from autocasted ops.</p> <p>If an op is unlisted, we assume its numerically stable in <code>bfloat16</code>. If you believe an unlisted op is numerically unstable in <code>bfloat16</code>, please file an issue.</p>  <h4 id=\"cpu-ops-that-can-autocast-to-bfloat16\">CPU Ops that can autocast to <code>bfloat16</code>\n</h4> <p><code>conv1d</code>, <code>conv2d</code>, <code>conv3d</code>, <code>bmm</code>, <code>mm</code>, <code>baddbmm</code>, <code>addmm</code>, <code>addbmm</code>, <code>linear</code>, <code>matmul</code>, <code>_convolution</code></p>   <h4 id=\"cpu-ops-that-can-autocast-to-float32\">CPU Ops that can autocast to <code>float32</code>\n</h4> <p><code>conv_transpose1d</code>, <code>conv_transpose2d</code>, <code>conv_transpose3d</code>, <code>avg_pool3d</code>, <code>binary_cross_entropy</code>, <code>grid_sampler</code>, <code>grid_sampler_2d</code>, <code>_grid_sampler_2d_cpu_fallback</code>, <code>grid_sampler_3d</code>, <code>polar</code>, <code>prod</code>, <code>quantile</code>, <code>nanquantile</code>, <code>stft</code>, <code>cdist</code>, <code>trace</code>, <code>view_as_complex</code>, <code>cholesky</code>, <code>cholesky_inverse</code>, <code>cholesky_solve</code>, <code>inverse</code>, <code>lu_solve</code>, <code>orgqr</code>, <code>inverse</code>, <code>ormqr</code>, <code>pinverse</code>, <code>max_pool3d</code>, <code>max_unpool2d</code>, <code>max_unpool3d</code>, <code>adaptive_avg_pool3d</code>, <code>reflection_pad1d</code>, <code>reflection_pad2d</code>, <code>replication_pad1d</code>, <code>replication_pad2d</code>, <code>replication_pad3d</code>, <code>mse_loss</code>, <code>ctc_loss</code>, <code>kl_div</code>, <code>multilabel_margin_loss</code>, <code>fft_fft</code>, <code>fft_ifft</code>, <code>fft_fft2</code>, <code>fft_ifft2</code>, <code>fft_fftn</code>, <code>fft_ifftn</code>, <code>fft_rfft</code>, <code>fft_irfft</code>, <code>fft_rfft2</code>, <code>fft_irfft2</code>, <code>fft_rfftn</code>, <code>fft_irfftn</code>, <code>fft_hfft</code>, <code>fft_ihfft</code>, <code>linalg_matrix_norm</code>, <code>linalg_cond</code>, <code>linalg_matrix_rank</code>, <code>linalg_solve</code>, <code>linalg_cholesky</code>, <code>linalg_svdvals</code>, <code>linalg_eigvals</code>, <code>linalg_eigvalsh</code>, <code>linalg_inv</code>, <code>linalg_householder_product</code>, <code>linalg_tensorinv</code>, <code>linalg_tensorsolve</code>, <code>fake_quantize_per_tensor_affine</code>, <code>eig</code>, <code>geqrf</code>, <code>lstsq</code>, <code>_lu_with_info</code>, <code>qr</code>, <code>solve</code>, <code>svd</code>, <code>symeig</code>, <code>triangular_solve</code>, <code>fractional_max_pool2d</code>, <code>fractional_max_pool3d</code>, <code>adaptive_max_pool3d</code>, <code>multilabel_margin_loss_forward</code>, <code>linalg_qr</code>, <code>linalg_cholesky_ex</code>, <code>linalg_svd</code>, <code>linalg_eig</code>, <code>linalg_eigh</code>, <code>linalg_lstsq</code>, <code>linalg_inv_ex</code></p>   <h4 id=\"cpu-ops-that-promote-to-the-widest-input-type\">CPU Ops that promote to the widest input type</h4> <p>These ops dont require a particular dtype for stability, but take multiple inputs and require that the inputs dtypes match. If all of the inputs are <code>bfloat16</code>, the op runs in <code>bfloat16</code>. If any of the inputs is <code>float32</code>, autocast casts all inputs to <code>float32</code> and runs the op in <code>float32</code>.</p> <p><code>cat</code>, <code>stack</code>, <code>index_copy</code></p> <p>Some ops not listed here (e.g., binary ops like <code>add</code>) natively promote inputs without autocastings intervention. If inputs are a mixture of <code>bfloat16</code> and <code>float32</code>, these ops run in <code>float32</code> and produce <code>float32</code> output, regardless of whether autocast is enabled.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href=\"https://github.com/pytorch/pytorch/blob/main/LICENSE\">LICENSE</a> file.<br>\n    <a href=\"https://pytorch.org/docs/2.1/amp.html\" class=\"_attribution-link\">https://pytorch.org/docs/2.1/amp.html</a>\n  </p>\n</div>\n","autograd":"<h1 id=\"automatic-differentiation-package-torch-autograd\">Automatic differentiation package - torch.autograd</h1> <p id=\"module-torch.autograd\"><code>torch.autograd</code> provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions. It requires minimal changes to the existing code - you only need to declare <code>Tensor</code> s for which gradients should be computed with the <code>requires_grad=True</code> keyword. As of now, we only support autograd for floating point <code>Tensor</code> types ( half, float, double and bfloat16) and complex <code>Tensor</code> types (cfloat, cdouble).</p> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.autograd.backward#torch.autograd.backward\" title=\"torch.autograd.backward\"><code>backward</code></a>\n</td> <td><p>Computes the sum of gradients of given tensors with respect to graph leaves.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.autograd.grad#torch.autograd.grad\" title=\"torch.autograd.grad\"><code>grad</code></a>\n</td> <td><p>Computes and returns the sum of gradients of outputs with respect to the inputs.</p></td> </tr>  </table>  <h2 id=\"forward-mode-ad\">Forward-mode Automatic Differentiation</h2> <div class=\"admonition warning\" id=\"forward-mode-automatic-differentiation\"> <p class=\"admonition-title\">Warning</p> <p>This API is in beta. Even though the function signatures are very unlikely to change, improved operator coverage is planned before we consider this stable.</p> </div> <p>Please see the <a class=\"reference external\" href=\"https://pytorch.org/tutorials/intermediate/forward_ad_usage.html\">forward-mode AD tutorial</a> for detailed steps on how to use this API.</p> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.forward_ad.dual_level#torch.autograd.forward_ad.dual_level\" title=\"torch.autograd.forward_ad.dual_level\"><code>forward_ad.dual_level</code></a></p></td> <td><p>Context-manager that enables forward AD.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.forward_ad.make_dual#torch.autograd.forward_ad.make_dual\" title=\"torch.autograd.forward_ad.make_dual\"><code>forward_ad.make_dual</code></a></p></td> <td><p>Associates a tensor value with a forward gradient, the tangent, to create a \"dual tensor\", which is used to compute forward AD gradients.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.forward_ad.unpack_dual#torch.autograd.forward_ad.unpack_dual\" title=\"torch.autograd.forward_ad.unpack_dual\"><code>forward_ad.unpack_dual</code></a></p></td> <td><p>Unpacks a \"dual tensor\" to get both its Tensor value and its forward AD gradient.</p></td> </tr>  </table>   <h2 id=\"functional-api\">Functional higher level API</h2> <div class=\"admonition warning\" id=\"functional-higher-level-api\"> <p class=\"admonition-title\">Warning</p> <p>This API is in beta. Even though the function signatures are very unlikely to change, major improvements to performances are planned before we consider this stable.</p> </div> <p>This section contains the higher level API for the autograd that builds on the basic API above and allows you to compute jacobians, hessians, etc.</p> <p>This API works with user-provided functions that take only Tensors as input and return only Tensors. If your function takes other arguments that are not Tensors or Tensors that dont have requires_grad set, you can use a lambda to capture them. For example, for a function <code>f</code> that takes three inputs, a Tensor for which we want the jacobian, another tensor that should be considered constant and a boolean flag as <code>f(input, constant, flag=flag)</code> you can use it as <code>functional.jacobian(lambda x: f(x, constant, flag=flag), input)</code>.</p> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.functional.jacobian#torch.autograd.functional.jacobian\" title=\"torch.autograd.functional.jacobian\"><code>functional.jacobian</code></a></p></td> <td><p>Function that computes the Jacobian of a given function.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.functional.hessian#torch.autograd.functional.hessian\" title=\"torch.autograd.functional.hessian\"><code>functional.hessian</code></a></p></td> <td><p>Function that computes the Hessian of a given scalar function.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.functional.vjp#torch.autograd.functional.vjp\" title=\"torch.autograd.functional.vjp\"><code>functional.vjp</code></a></p></td> <td><p>Function that computes the dot product between a vector <code>v</code> and the Jacobian of the given function at the point given by the inputs.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.functional.jvp#torch.autograd.functional.jvp\" title=\"torch.autograd.functional.jvp\"><code>functional.jvp</code></a></p></td> <td><p>Function that computes the dot product between the Jacobian of the given function at the point given by the inputs and a vector <code>v</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.functional.vhp#torch.autograd.functional.vhp\" title=\"torch.autograd.functional.vhp\"><code>functional.vhp</code></a></p></td> <td><p>Function that computes the dot product between a vector <code>v</code> and the Hessian of a given scalar function at the point given by the inputs.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.functional.hvp#torch.autograd.functional.hvp\" title=\"torch.autograd.functional.hvp\"><code>functional.hvp</code></a></p></td> <td><p>Function that computes the dot product between the Hessian of a given scalar function and a vector <code>v</code> at the point given by the inputs.</p></td> </tr>  </table>   <h2 id=\"locally-disable-grad\">Locally disabling gradient computation</h2> <p id=\"locally-disabling-gradient-computation\">See <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/autograd.html#locally-disable-grad-doc\"><span class=\"std std-ref\">Locally disabling gradient computation</span></a> for more information on the differences between no-grad and inference mode as well as other related mechanisms that may be confused with the two. Also see <a class=\"reference internal\" href=\"torch#torch-rst-local-disable-grad\"><span class=\"std std-ref\">Locally disabling gradient computation</span></a> for a list of functions that can be used to locally disable gradients.</p>   <h2 id=\"default-grad-layouts\">Default gradient layouts</h2> <p id=\"default-gradient-layouts\">When a non-sparse <code>param</code> receives a non-sparse gradient during <a class=\"reference internal\" href=\"generated/torch.autograd.backward#torch.autograd.backward\" title=\"torch.autograd.backward\"><code>torch.autograd.backward()</code></a> or <a class=\"reference internal\" href=\"generated/torch.tensor.backward#torch.Tensor.backward\" title=\"torch.Tensor.backward\"><code>torch.Tensor.backward()</code></a> <code>param.grad</code> is accumulated as follows.</p> <p>If <code>param.grad</code> is initially <code>None</code>:</p> <ol class=\"arabic simple\"> <li>If <code>param</code>s memory is non-overlapping and dense, <code>.grad</code> is created with strides matching <code>param</code> (thus matching <code>param</code>s layout).</li> <li>Otherwise, <code>.grad</code> is created with rowmajor-contiguous strides.</li> </ol> <p>If <code>param</code> already has a non-sparse <code>.grad</code> attribute:</p> <ol class=\"arabic simple\" start=\"3\"> <li>If <code>create_graph=False</code>, <code>backward()</code> accumulates into <code>.grad</code> in-place, which preserves its strides.</li> <li>If <code>create_graph=True</code>, <code>backward()</code> replaces <code>.grad</code> with a new tensor <code>.grad + new grad</code>, which attempts (but does not guarantee) matching the preexisting <code>.grad</code>s strides.</li> </ol> <p>The default behavior (letting <code>.grad</code>s be <code>None</code> before the first <code>backward()</code>, such that their layout is created according to 1 or 2, and retained over time according to 3 or 4) is recommended for best performance. Calls to <code>model.zero_grad()</code> or <code>optimizer.zero_grad()</code> will not affect <code>.grad</code> layouts.</p> <p>In fact, resetting all <code>.grad</code>s to <code>None</code> before each accumulation phase, e.g.:</p> <pre data-language=\"python\">for iterations...\n    ...\n    for param in model.parameters():\n        param.grad = None\n    loss.backward()\n</pre> <p>such that theyre recreated according to 1 or 2 every time, is a valid alternative to <code>model.zero_grad()</code> or <code>optimizer.zero_grad()</code> that may improve performance for some networks.</p>  <h3 id=\"manual-gradient-layouts\">Manual gradient layouts</h3> <p>If you need manual control over <code>.grad</code>s strides, assign <code>param.grad =</code> a zeroed tensor with desired strides before the first <code>backward()</code>, and never reset it to <code>None</code>. 3 guarantees your layout is preserved as long as <code>create_graph=False</code>. 4 indicates your layout is <em>likely</em> preserved even if <code>create_graph=True</code>.</p>    <h2 id=\"in-place-operations-on-tensors\">In-place operations on Tensors</h2> <p>Supporting in-place operations in autograd is a hard matter, and we discourage their use in most cases. Autograds aggressive buffer freeing and reuse makes it very efficient and there are very few occasions when in-place operations actually lower memory usage by any significant amount. Unless youre operating under heavy memory pressure, you might never need to use them.</p>  <h3 id=\"in-place-correctness-checks\">In-place correctness checks</h3> <p>All <code>Tensor</code> s keep track of in-place operations applied to them, and if the implementation detects that a tensor was saved for backward in one of the functions, but it was modified in-place afterwards, an error will be raised once backward pass is started. This ensures that if youre using in-place functions and not seeing any errors, you can be sure that the computed gradients are correct.</p>    <h2 id=\"variable-deprecated\">Variable (deprecated)</h2> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>The Variable API has been deprecated: Variables are no longer necessary to use autograd with tensors. Autograd automatically supports Tensors with <code>requires_grad</code> set to <code>True</code>. Below please find a quick guide on what has changed:</p> <ul class=\"simple\"> <li>\n<code>Variable(tensor)</code> and <code>Variable(tensor, requires_grad)</code> still work as expected, but they return Tensors instead of Variables.</li> <li>\n<code>var.data</code> is the same thing as <code>tensor.data</code>.</li> <li>Methods such as <code>var.backward(), var.detach(), var.register_hook()</code> now work on tensors with the same method names.</li> </ul> <p>In addition, one can now create tensors with <code>requires_grad=True</code> using factory methods such as <a class=\"reference internal\" href=\"generated/torch.randn#torch.randn\" title=\"torch.randn\"><code>torch.randn()</code></a>, <a class=\"reference internal\" href=\"generated/torch.zeros#torch.zeros\" title=\"torch.zeros\"><code>torch.zeros()</code></a>, <a class=\"reference internal\" href=\"generated/torch.ones#torch.ones\" title=\"torch.ones\"><code>torch.ones()</code></a>, and others like the following:</p> <p><code>autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)</code></p> </div>   <h2 id=\"tensor-autograd-functions\">Tensor autograd functions</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><code>torch.Tensor.grad</code></p></td> <td><p>This attribute is <code>None</code> by default and becomes a Tensor the first time a call to <a class=\"reference internal\" href=\"generated/torch.autograd.backward#torch.autograd.backward\" title=\"torch.autograd.backward\"><code>backward()</code></a> computes gradients for <code>self</code>.</p></td> </tr> <tr>\n<td><p><code>torch.Tensor.requires_grad</code></p></td> <td><p>Is <code>True</code> if gradients need to be computed for this Tensor, <code>False</code> otherwise.</p></td> </tr> <tr>\n<td><p><code>torch.Tensor.is_leaf</code></p></td> <td><p>All Tensors that have <code>requires_grad</code> which is <code>False</code> will be leaf Tensors by convention.</p></td> </tr> <tr>\n<td><p><code>torch.Tensor.backward</code>([gradient, ...])</p></td> <td><p>Computes the gradient of current tensor wrt graph leaves.</p></td> </tr> <tr>\n<td><p><code>torch.Tensor.detach</code></p></td> <td><p>Returns a new Tensor, detached from the current graph.</p></td> </tr> <tr>\n<td><p><code>torch.Tensor.detach_</code></p></td> <td><p>Detaches the Tensor from the graph that created it, making it a leaf.</p></td> </tr> <tr>\n<td><p><code>torch.Tensor.register_hook</code>(hook)</p></td> <td><p>Registers a backward hook.</p></td> </tr> <tr>\n<td><p><code>torch.Tensor.register_post_accumulate_grad_hook</code>(hook)</p></td> <td><p>Registers a backward hook that runs after grad accumulation.</p></td> </tr> <tr>\n<td><p><code>torch.Tensor.retain_grad</code>()</p></td> <td><p>Enables this Tensor to have their <a class=\"reference internal\" href=\"generated/torch.autograd.grad#torch.autograd.grad\" title=\"torch.autograd.grad\"><code>grad</code></a> populated during <a class=\"reference internal\" href=\"generated/torch.autograd.backward#torch.autograd.backward\" title=\"torch.autograd.backward\"><code>backward()</code></a>.</p></td> </tr>  </table>   <h2 id=\"function\"><span class=\"hidden-section\">Function</span></h2> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.autograd.Function\">\n<code>class torch.autograd.Function(*args, **kwargs)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/autograd/function.html#Function\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Base class to create custom <code>autograd.Function</code></p> <p>To create a custom <code>autograd.Function</code>, subclass this class and implement the <a class=\"reference internal\" href=\"generated/torch.autograd.function.forward#torch.autograd.Function.forward\" title=\"torch.autograd.Function.forward\"><code>forward()</code></a> and <a class=\"reference internal\" href=\"generated/torch.autograd.backward#torch.autograd.backward\" title=\"torch.autograd.backward\"><code>backward()</code></a> static methods. Then, to use your custom op in the forward pass, call the class method <code>apply</code>. Do not call <a class=\"reference internal\" href=\"generated/torch.autograd.function.forward#torch.autograd.Function.forward\" title=\"torch.autograd.Function.forward\"><code>forward()</code></a> directly.</p> <p>To ensure correctness and best performance, make sure you are calling the correct methods on <code>ctx</code> and validating your backward function using <a class=\"reference internal\" href=\"generated/torch.autograd.gradcheck#torch.autograd.gradcheck\" title=\"torch.autograd.gradcheck\"><code>torch.autograd.gradcheck()</code></a>.</p> <p>See <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/extending.html#extending-autograd\"><span class=\"std std-ref\">Extending torch.autograd</span></a> for more details on how to use this class.</p> <p>Examples:</p> <pre data-language=\"python\">&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use it by calling the apply method:\n&gt;&gt;&gt; output = Exp.apply(input)\n</pre> </dd>\n</dl> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.function.forward#torch.autograd.Function.forward\" title=\"torch.autograd.Function.forward\"><code>Function.forward</code></a></p></td> <td><p>This function is to be overridden by all subclasses.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.function.backward#torch.autograd.Function.backward\" title=\"torch.autograd.Function.backward\"><code>Function.backward</code></a></p></td> <td><p>Defines a formula for differentiating the operation with backward mode automatic differentiation (alias to the vjp function).</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.function.jvp#torch.autograd.Function.jvp\" title=\"torch.autograd.Function.jvp\"><code>Function.jvp</code></a></p></td> <td><p>Defines a formula for differentiating the operation with forward mode automatic differentiation.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.function.vmap#torch.autograd.Function.vmap\" title=\"torch.autograd.Function.vmap\"><code>Function.vmap</code></a></p></td> <td><p>Defines a rule for the behavior of this autograd.Function underneath <a class=\"reference internal\" href=\"generated/torch.vmap#torch.vmap\" title=\"torch.vmap\"><code>torch.vmap()</code></a>.</p></td> </tr>  </table>   <h2 id=\"context-method-mixins\">Context method mixins</h2> <p>When creating a new <a class=\"reference internal\" href=\"#torch.autograd.Function\" title=\"torch.autograd.Function\"><code>Function</code></a>, the following methods are available to <code>ctx</code>.</p> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.function.functionctx.mark_dirty#torch.autograd.function.FunctionCtx.mark_dirty\" title=\"torch.autograd.function.FunctionCtx.mark_dirty\"><code>function.FunctionCtx.mark_dirty</code></a></p></td> <td><p>Marks given tensors as modified in an in-place operation.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.function.functionctx.mark_non_differentiable#torch.autograd.function.FunctionCtx.mark_non_differentiable\" title=\"torch.autograd.function.FunctionCtx.mark_non_differentiable\"><code>function.FunctionCtx.mark_non_differentiable</code></a></p></td> <td><p>Marks outputs as non-differentiable.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.function.functionctx.save_for_backward#torch.autograd.function.FunctionCtx.save_for_backward\" title=\"torch.autograd.function.FunctionCtx.save_for_backward\"><code>function.FunctionCtx.save_for_backward</code></a></p></td> <td><p>Saves given tensors for a future call to <a class=\"reference internal\" href=\"generated/torch.autograd.function.backward#torch.autograd.Function.backward\" title=\"torch.autograd.Function.backward\"><code>backward()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.function.functionctx.set_materialize_grads#torch.autograd.function.FunctionCtx.set_materialize_grads\" title=\"torch.autograd.function.FunctionCtx.set_materialize_grads\"><code>function.FunctionCtx.set_materialize_grads</code></a></p></td> <td><p>Sets whether to materialize grad tensors.</p></td> </tr>  </table>   <h2 id=\"grad-check\">Numerical gradient checking</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\" id=\"numerical-gradient-checking\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.autograd.gradcheck#torch.autograd.gradcheck\" title=\"torch.autograd.gradcheck\"><code>gradcheck</code></a>\n</td> <td><p>Check gradients computed via small finite differences against analytical gradients wrt tensors in <code>inputs</code> that are of floating point or complex type and with <code>requires_grad=True</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.autograd.gradgradcheck#torch.autograd.gradgradcheck\" title=\"torch.autograd.gradgradcheck\"><code>gradgradcheck</code></a>\n</td> <td><p>Check gradients of gradients computed via small finite differences against analytical gradients wrt tensors in <code>inputs</code> and <code>grad_outputs</code> that are of floating point or complex type and with <code>requires_grad=True</code>.</p></td> </tr>  </table>   <h2 id=\"profiler\">Profiler</h2> <p>Autograd includes a profiler that lets you inspect the cost of different operators inside your model - both on the CPU and GPU. There are three modes implemented at the moment - CPU-only using <a class=\"reference internal\" href=\"#torch.autograd.profiler.profile\" title=\"torch.autograd.profiler.profile\"><code>profile</code></a>. nvprof based (registers both CPU and GPU activity) using <a class=\"reference internal\" href=\"#torch.autograd.profiler.emit_nvtx\" title=\"torch.autograd.profiler.emit_nvtx\"><code>emit_nvtx</code></a>. and vtune profiler based using <a class=\"reference internal\" href=\"#torch.autograd.profiler.emit_itt\" title=\"torch.autograd.profiler.emit_itt\"><code>emit_itt</code></a>.</p> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.autograd.profiler.profile\">\n<code>class torch.autograd.profiler.profile(enabled=True, *, use_cuda=False, use_device=None, record_shapes=False, with_flops=False, profile_memory=False, with_stack=False, with_modules=False, use_kineto=False, use_cpu=True, use_mtia=False, experimental_config=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/autograd/profiler.html#profile\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Context manager that manages autograd profiler state and holds a summary of results. Under the hood it just records events of functions being executed in C++ and exposes those events to Python. You can wrap any code into it and it will only report runtime of PyTorch functions. Note: profiler is thread local and is automatically propagated into the async tasks</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>enabled</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Setting this to False makes this context manager a no-op.</li> <li>\n<strong>use_cuda</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Enables timing of CUDA events as well using the cudaEvent API. Adds approximately 4us of overhead to each tensor operation.</li> <li>\n<strong>record_shapes</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  If shapes recording is set, information about input dimensions will be collected. This allows one to see which dimensions have been used under the hood and further group by them using prof.key_averages(group_by_input_shape=True). Please note that shape recording might skew your profiling data. It is recommended to use separate runs with and without shape recording to validate the timing. Most likely the skew will be negligible for bottom most events (in a case of nested function calls). But for higher level functions the total self cpu time might be artificially increased because of the shape collection.</li> <li>\n<strong>with_flops</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  If with_flops is set, the profiler will estimate the FLOPs (floating point operations) value using the operators input shape. This allows one to estimate the hardware performance. Currently, this option only works for the matrix multiplication and 2D convolution operators.</li> <li>\n<strong>profile_memory</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  track tensor memory allocation/deallocation.</li> <li>\n<strong>with_stack</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  record source information (file and line number) for the ops.</li> <li>\n<strong>with_modules</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a>)  record module hierarchy (including function names) corresponding to the callstack of the op. e.g. If module As forward calls module Bs forward which contains an aten::add op, then aten::adds module hierarchy is A.B Note that this support exist, at the moment, only for TorchScript models and not eager mode models.</li> <li>\n<strong>use_kineto</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  experimental, enable profiling with Kineto profiler.</li> <li>\n<strong>use_cpu</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  profile CPU events; setting to <code>False</code> requires <code>use_kineto=True</code> and can be used to lower the overhead for GPU-only profiling.</li> <li>\n<strong>experimental_config</strong> (<em>_ExperimentalConfig</em>)  A set of experimental options used by profiler libraries like Kineto. Note, backward compatibility is not guaranteed.</li> </ul> </dd> </dl> <h4 class=\"rubric\">Example</h4> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.randn((1, 1), requires_grad=True)\n&gt;&gt;&gt; with torch.autograd.profiler.profile() as prof:\n&gt;&gt;&gt;     for _ in range(100):  # any normal python code, really!\n&gt;&gt;&gt;         y = x ** 2\n&gt;&gt;&gt;         y.backward()\n&gt;&gt;&gt; # NOTE: some columns were removed for brevity\n&gt;&gt;&gt; print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n-----------------------------------  ---------------  ---------------  ---------------\nName                                 Self CPU total   CPU time avg     Number of Calls\n-----------------------------------  ---------------  ---------------  ---------------\nmul                                  32.048ms         32.048ms         200\npow                                  27.041ms         27.041ms         200\nPowBackward0                         9.727ms          55.483ms         100\ntorch::autograd::AccumulateGrad      9.148ms          9.148ms          100\ntorch::autograd::GraphRoot           691.816us        691.816us        100\n-----------------------------------  ---------------  ---------------  ---------------\n</pre> </dd>\n</dl> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.profiler.profile.export_chrome_trace#torch.autograd.profiler.profile.export_chrome_trace\" title=\"torch.autograd.profiler.profile.export_chrome_trace\"><code>profiler.profile.export_chrome_trace</code></a></p></td> <td><p>Exports an EventList as a Chrome tracing tools file.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.profiler.profile.key_averages#torch.autograd.profiler.profile.key_averages\" title=\"torch.autograd.profiler.profile.key_averages\"><code>profiler.profile.key_averages</code></a></p></td> <td><p>Averages all function events over their keys.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.profiler.profile.self_cpu_time_total#torch.autograd.profiler.profile.self_cpu_time_total\" title=\"torch.autograd.profiler.profile.self_cpu_time_total\"><code>profiler.profile.self_cpu_time_total</code></a></p></td> <td><p>Returns total time spent on CPU obtained as a sum of all self times across all the events.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.profiler.profile.total_average#torch.autograd.profiler.profile.total_average\" title=\"torch.autograd.profiler.profile.total_average\"><code>profiler.profile.total_average</code></a></p></td> <td><p>Averages all events.</p></td> </tr>  </table> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.autograd.profiler.emit_nvtx\">\n<code>class torch.autograd.profiler.emit_nvtx(enabled=True, record_shapes=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/autograd/profiler.html#emit_nvtx\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Context manager that makes every autograd operation emit an NVTX range.</p> <p>It is useful when running the program under nvprof:</p> <pre data-language=\"python\">nvprof --profile-from-start off -o trace_name.prof -- &lt;regular command here&gt;\n</pre> <p>Unfortunately, theres no way to force nvprof to flush the data it collected to disk, so for CUDA profiling one has to use this context manager to annotate nvprof traces and wait for the process to exit before inspecting them. Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or <a class=\"reference internal\" href=\"generated/torch.autograd.profiler.load_nvprof#torch.autograd.profiler.load_nvprof\" title=\"torch.autograd.profiler.load_nvprof\"><code>torch.autograd.profiler.load_nvprof()</code></a> can load the results for inspection e.g. in Python REPL.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>enabled</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Setting <code>enabled=False</code> makes this context manager a no-op. Default: <code>True</code>.</li> <li>\n<strong>record_shapes</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  If <code>record_shapes=True</code>, the nvtx range wrapping each autograd op will append information about the sizes of Tensor arguments received by that op, in the following format: <code>[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]</code> Non-tensor arguments will be represented by <code>[]</code>. Arguments will be listed in the order they are received by the backend op. Please note that this order may not match the order in which those arguments were passed on the Python side. Also note that shape recording may increase the overhead of nvtx range creation. Default: <code>False</code>\n</li> </ul> </dd> </dl> <h4 class=\"rubric\">Example</h4> <pre data-language=\"python\">&gt;&gt;&gt; with torch.cuda.profiler.profile():\n...     model(x)  # Warmup CUDA memory allocator and profiler\n...     with torch.autograd.profiler.emit_nvtx():\n...         model(x)\n</pre> <p><strong>Forward-backward correlation</strong></p> <p>When viewing a profile created using <a class=\"reference internal\" href=\"#torch.autograd.profiler.emit_nvtx\" title=\"torch.autograd.profiler.emit_nvtx\"><code>emit_nvtx</code></a> in the Nvidia Visual Profiler, correlating each backward-pass op with the corresponding forward-pass op can be difficult. To ease this task, <a class=\"reference internal\" href=\"#torch.autograd.profiler.emit_nvtx\" title=\"torch.autograd.profiler.emit_nvtx\"><code>emit_nvtx</code></a> appends sequence number information to the ranges it generates.</p> <p>During the forward pass, each function range is decorated with <code>seq=&lt;N&gt;</code>. <code>seq</code> is a running counter, incremented each time a new backward Function object is created and stashed for backward. Thus, the <code>seq=&lt;N&gt;</code> annotation associated with each forward function range tells you that if a backward Function object is created by this forward function, the backward object will receive sequence number N. During the backward pass, the top-level range wrapping each C++ backward Functions <code>apply()</code> call is decorated with <code>stashed seq=&lt;M&gt;</code>. <code>M</code> is the sequence number that the backward object was created with. By comparing <code>stashed seq</code> numbers in backward with <code>seq</code> numbers in forward, you can track down which forward op created each backward Function.</p> <p>Any functions executed during the backward pass are also decorated with <code>seq=&lt;N&gt;</code>. During default backward (with <code>create_graph=False</code>) this information is irrelevant, and in fact, <code>N</code> may simply be 0 for all such functions. Only the top-level ranges associated with backward Function objects <code>apply()</code> methods are useful, as a way to correlate these Function objects with the earlier forward pass.</p> <p><strong>Double-backward</strong></p> <p>If, on the other hand, a backward pass with <code>create_graph=True</code> is underway (in other words, if you are setting up for a double-backward), each functions execution during backward is given a nonzero, useful <code>seq=&lt;N&gt;</code>. Those functions may themselves create Function objects to be executed later during double-backward, just as the original functions in the forward pass did. The relationship between backward and double-backward is conceptually the same as the relationship between forward and backward: The functions still emit current-sequence-number-tagged ranges, the Function objects they create still stash those sequence numbers, and during the eventual double-backward, the Function objects <code>apply()</code> ranges are still tagged with <code>stashed seq</code> numbers, which can be compared to <code>seq</code> numbers from the backward pass.</p> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.autograd.profiler.emit_itt\">\n<code>class torch.autograd.profiler.emit_itt(enabled=True, record_shapes=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/autograd/profiler.html#emit_itt\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Context manager that makes every autograd operation emit an ITT range.</p> <p>It is useful when running the program under Intel(R) VTune Profiler:</p> <pre data-language=\"python\">vtune &lt;--vtune-flags&gt; &lt;regular command here&gt;\n</pre> <p>The Instrumentation and Tracing Technology (ITT) API enables your application to generate and control the collection of trace data during its execution across different Intel tools. This context manager is to annotate Intel(R) VTune Profiling trace. With help of this context manager, you will be able to see labled ranges in Intel(R) VTune Profiler GUI.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>enabled</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Setting <code>enabled=False</code> makes this context manager a no-op. Default: <code>True</code>.</li> <li>\n<strong>record_shapes</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  If <code>record_shapes=True</code>, the itt range wrapping each autograd op will append information about the sizes of Tensor arguments received by that op, in the following format: <code>[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]</code> Non-tensor arguments will be represented by <code>[]</code>. Arguments will be listed in the order they are received by the backend op. Please note that this order may not match the order in which those arguments were passed on the Python side. Also note that shape recording may increase the overhead of itt range creation. Default: <code>False</code>\n</li> </ul> </dd> </dl> <h4 class=\"rubric\">Example</h4> <pre data-language=\"python\">&gt;&gt;&gt; with torch.autograd.profiler.emit_itt():\n...     model(x)\n</pre> </dd>\n</dl> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.profiler.load_nvprof#torch.autograd.profiler.load_nvprof\" title=\"torch.autograd.profiler.load_nvprof\"><code>profiler.load_nvprof</code></a></p></td> <td><p>Opens an nvprof trace file and parses autograd annotations.</p></td> </tr>  </table>   <h2 id=\"anomaly-detection\">Anomaly detection</h2> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.autograd.detect_anomaly\">\n<code>class torch.autograd.detect_anomaly(check_nan=True)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/autograd/anomaly_mode.html#detect_anomaly\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Context-manager that enable anomaly detection for the autograd engine.</p> <p>This does two things:</p> <ul class=\"simple\"> <li>Running the forward pass with detection enabled will allow the backward pass to print the traceback of the forward operation that created the failing backward function.</li> <li>If <code>check_nan</code> is <code>True</code>, any backward computation that generate nan value will raise an error. Default <code>True</code>.</li> </ul> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This mode should be enabled only for debugging as the different tests will slow down your program execution.</p> </div> <h4 class=\"rubric\">Example</h4> <pre data-language=\"python\">&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch import autograd\n&gt;&gt;&gt; class MyFunc(autograd.Function):\n...     @staticmethod\n...     def forward(ctx, inp):\n...         return inp.clone()\n...     @staticmethod\n...     def backward(ctx, gO):\n...         # Error during the backward pass\n...         raise RuntimeError(\"Some error in backward\")\n...         return gO.clone()\n&gt;&gt;&gt; def run_fn(a):\n...     out = MyFunc.apply(a)\n...     return out.sum()\n&gt;&gt;&gt; inp = torch.rand(10, 10, requires_grad=True)\n&gt;&gt;&gt; out = run_fn(inp)\n&gt;&gt;&gt; out.backward()\n    Traceback (most recent call last):\n      File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"&lt;stdin&gt;\", line 8, in backward\n    RuntimeError: Some error in backward\n&gt;&gt;&gt; with autograd.detect_anomaly():\n...     inp = torch.rand(10, 10, requires_grad=True)\n...     out = run_fn(inp)\n...     out.backward()\n    Traceback of forward call that caused the error:\n      File \"tmp.py\", line 53, in &lt;module&gt;\n        out = run_fn(inp)\n      File \"tmp.py\", line 44, in run_fn\n        out = MyFunc.apply(a)\n    Traceback (most recent call last):\n      File \"&lt;stdin&gt;\", line 4, in &lt;module&gt;\n      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"&lt;stdin&gt;\", line 8, in backward\n    RuntimeError: Some error in backward\n</pre>  </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.autograd.set_detect_anomaly\">\n<code>class torch.autograd.set_detect_anomaly(mode, check_nan=True)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/autograd/anomaly_mode.html#set_detect_anomaly\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Context-manager that sets the anomaly detection for the autograd engine on or off.</p> <p><code>set_detect_anomaly</code> will enable or disable the autograd anomaly detection based on its argument <code>mode</code>. It can be used as a context-manager or as a function.</p> <p>See <code>detect_anomaly</code> above for details of the anomaly detection behaviour.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>mode</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a>)  Flag whether to enable anomaly detection (<code>True</code>), or disable (<code>False</code>).</li> <li>\n<strong>check_nan</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a>)  Flag whether to raise an error when the backward generate nan</li> </ul> </dd> </dl> </dd>\n</dl>   <h2 id=\"autograd-graph\">Autograd graph</h2> <p>Autograd exposes methods that allow one to inspect the graph and interpose behavior during the backward pass.</p> <p>The <code>grad_fn</code> attribute of a <a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a> holds a <code>torch.autograd.graph.Node</code> if the tensor is the output of a operation that was recorded by autograd (i.e., grad_mode is enabled and at least one of the inputs required gradients), or <code>None</code> otherwise.</p> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.graph.node.name#torch.autograd.graph.Node.name\" title=\"torch.autograd.graph.Node.name\"><code>graph.Node.name</code></a></p></td> <td><p>Returns the name.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.graph.node.metadata#torch.autograd.graph.Node.metadata\" title=\"torch.autograd.graph.Node.metadata\"><code>graph.Node.metadata</code></a></p></td> <td><p>Returns the metadata.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.graph.node.next_functions#torch.autograd.graph.Node.next_functions\" title=\"torch.autograd.graph.Node.next_functions\"><code>graph.Node.next_functions</code></a></p></td> <td></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.graph.node.register_hook#torch.autograd.graph.Node.register_hook\" title=\"torch.autograd.graph.Node.register_hook\"><code>graph.Node.register_hook</code></a></p></td> <td><p>Registers a backward hook.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.autograd.graph.node.register_prehook#torch.autograd.graph.Node.register_prehook\" title=\"torch.autograd.graph.Node.register_prehook\"><code>graph.Node.register_prehook</code></a></p></td> <td><p>Registers a backward pre-hook.</p></td> </tr>  </table> <p>Some operations need intermediary results to be saved during the forward pass in order to execute the backward pass. These intermediary results are saved as attributes on the <code>grad_fn</code> and can be accessed. For example:</p> <pre data-language=\"python\">&gt;&gt;&gt; a = torch.tensor([0., 0., 0.], requires_grad=True)\n&gt;&gt;&gt; b = a.exp()\n&gt;&gt;&gt; print(isinstance(b.grad_fn, torch.autograd.graph.Node))\nTrue\n&gt;&gt;&gt; print(dir(b.grad_fn))\n['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_raw_saved_result', '_register_hook_dict', '_saved_result', 'metadata', 'name', 'next_functions', 'register_hook', 'register_prehook', 'requires_grad']\n&gt;&gt;&gt; print(torch.allclose(b.grad_fn._saved_result, b))\nTrue\n</pre> <p>You can also define how these saved tensors should be packed / unpacked using hooks. A common application is to trade compute for memory by saving those intermediary results to disk or to CPU instead of leaving them on the GPU. This is especially useful if you notice your model fits on GPU during evaluation, but not training. Also see <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/autograd.html#saved-tensors-hooks-doc\"><span class=\"std std-ref\">Hooks for saved tensors</span></a>.</p> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.autograd.graph.saved_tensors_hooks\">\n<code>class torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/autograd/graph.html#saved_tensors_hooks\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Context-manager that sets a pair of pack / unpack hooks for saved tensors.</p> <p>Use this context-manager to define how intermediary results of an operation should be packed before saving, and unpacked on retrieval.</p> <p>In that context, the <code>pack_hook</code> function will be called everytime an operation saves a tensor for backward (this includes intermediary results saved using <code>save_for_backward()</code> but also those recorded by a PyTorch-defined operation). The output of <code>pack_hook</code> is then stored in the computation graph instead of the original tensor.</p> <p>The <code>unpack_hook</code> is called when the saved tensor needs to be accessed, namely when executing <a class=\"reference internal\" href=\"generated/torch.tensor.backward#torch.Tensor.backward\" title=\"torch.Tensor.backward\"><code>torch.Tensor.backward()</code></a> or <a class=\"reference internal\" href=\"generated/torch.autograd.grad#torch.autograd.grad\" title=\"torch.autograd.grad\"><code>torch.autograd.grad()</code></a>. It takes as argument the <em>packed</em> object returned by <code>pack_hook</code> and should return a tensor which has the same content as the original tensor (passed as input to the corresponding <code>pack_hook</code>).</p> <p>The hooks should have the following signatures:</p>  <p>pack_hook(tensor: Tensor) -&gt; Any</p> <p>unpack_hook(Any) -&gt; Tensor</p>  <p>where the return value of <code>pack_hook</code> is a valid input to <code>unpack_hook</code>.</p> <p>In general, you want <code>unpack_hook(pack_hook(t))</code> to be equal to <code>t</code> in terms of value, size, dtype and device.</p> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; def pack_hook(x):\n...     print(\"Packing\", x)\n...     return x\n&gt;&gt;&gt;\n&gt;&gt;&gt; def unpack_hook(x):\n...     print(\"Unpacking\", x)\n...     return x\n&gt;&gt;&gt;\n&gt;&gt;&gt; a = torch.ones(5, requires_grad=True)\n&gt;&gt;&gt; b = torch.ones(5, requires_grad=True) * 2\n&gt;&gt;&gt; with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n...     y = a * b\nPacking tensor([1., 1., 1., 1., 1.], requires_grad=True)\nPacking tensor([2., 2., 2., 2., 2.], grad_fn=&lt;MulBackward0&gt;)\n&gt;&gt;&gt; y.sum().backward()\nUnpacking tensor([1., 1., 1., 1., 1.], requires_grad=True)\nUnpacking tensor([2., 2., 2., 2., 2.], grad_fn=&lt;MulBackward0&gt;)\n</pre> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Performing an inplace operation on the input to either hooks may lead to undefined behavior.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Only one pair of hooks is allowed at a time. When recursively nesting this context-manager, only the inner-most pair of hooks will be applied.</p> </div>  </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.autograd.graph.save_on_cpu\">\n<code>class torch.autograd.graph.save_on_cpu(pin_memory=False, device_type='cuda')</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/autograd/graph.html#save_on_cpu\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Context-manager under which tensors saved by the forward pass will be stored on cpu, then retrieved for backward.</p> <p>When performing operations within this context manager, intermediary results saved in the graph during the forward pass will be moved to CPU, then copied back to the original device when needed for the backward pass. If the graph was already on CPU, no tensor copy is performed.</p> <p>Use this context-manager to trade compute for GPU memory usage (e.g. when your model doesnt fit in GPU memory during training).</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>pin_memory</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a>)  If <code>True</code> tensors will be saved to CPU pinned memory during packing and copied to GPU asynchronously during unpacking. Defaults to <code>False</code>. Also see <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/cuda.html#cuda-memory-pinning\"><span class=\"std std-ref\">Use pinned memory buffers</span></a>.</p> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; a = torch.randn(5, requires_grad=True, device=\"cuda\")\n&gt;&gt;&gt; b = torch.randn(5, requires_grad=True, device=\"cuda\")\n&gt;&gt;&gt; c = torch.randn(5, requires_grad=True, device=\"cuda\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; def f(a, b, c):\n...     prod_1 = a * b           # a and b are saved on GPU\n...     with torch.autograd.graph.save_on_cpu():\n...         prod_2 = prod_1 * c  # prod_1 and c are saved on CPU\n...     y = prod_2 * a           # prod_2 and a are saved on GPU\n...     return y\n&gt;&gt;&gt;\n&gt;&gt;&gt; y = f(a, b, c)\n&gt;&gt;&gt; del a, b, c  # for illustration only\n&gt;&gt;&gt; # the content of a, b, and prod_2 are still alive on GPU\n&gt;&gt;&gt; # the content of prod_1 and c only live on CPU\n&gt;&gt;&gt; y.sum().backward()  # all CPU tensors are moved back to GPU, for backward\n&gt;&gt;&gt; # all intermediary tensors are released (deleted) after the call to backward\n</pre> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.autograd.graph.disable_saved_tensors_hooks\">\n<code>class torch.autograd.graph.disable_saved_tensors_hooks(error_message)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/autograd/graph.html#disable_saved_tensors_hooks\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Context-manager that disables the saved tensors default hooks feature.</p> <p>Useful for if you are creating a feature that does not work with saved tensors default hooks.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>error_message</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a>)  When saved tensors default hooks are used when they have been are disabled, a RuntimeError with this error message gets raised.</p> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; message = \"saved tensors default hooks are disabled\"\n&gt;&gt;&gt; with torch.autograd.graph.disable_saved_tensors_hooks(message):\n...     # Raises RuntimeError: saved tensors default hooks are disabled\n...     with torch.autograd.graph.save_on_cpu():\n...         pass\n</pre> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.autograd.graph.register_multi_grad_hook\">\n<code>class torch.autograd.graph.register_multi_grad_hook(tensors, fn)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/autograd/graph.html#register_multi_grad_hook\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Registers a multi-grad backward hook.</p> <p>The hook will be called after gradients with respect to every tensor in <code>tensors</code> have been computed. If a tensor is in <code>tensors</code> but is not part of the graph, or if a tensor is not needed to compute the gradients for any <code>inputs</code> specified for the current <code>.backward()</code> or <code>.grad()</code> call, this tensor will be ignored and the hook will not wait for its gradient to be computed.</p> <p>After every non-ignored tensors gradient has been computed, <code>fn</code> will be called with those gradients. <code>None</code> will be passed for tensors that did not have their gradients computed.</p> <p>The hook should not modify its arguments.</p> <p>This function returns a handle with a method <code>handle.remove()</code> that removes the hook.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>See <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/autograd.html#backward-hooks-execution\"><span class=\"std std-ref\">Backward Hooks execution</span></a> for more information on how when this hook is executed, and how its execution is ordered relative to other hooks.</p> </div> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; import torch\n&gt;&gt;&gt;\n&gt;&gt;&gt; a = torch.rand(2, 3, requires_grad=True)\n&gt;&gt;&gt; b = torch.rand(2, 3, requires_grad=True)\n&gt;&gt;&gt; c = a * b\n&gt;&gt;&gt; d = a * b\n&gt;&gt;&gt;\n&gt;&gt;&gt; def fn(grads):\n...     print([g is not None for g in grads])\n...\n&gt;&gt;&gt; torch.autograd.graph.register_multi_grad_hook((a, b, c, d), fn)\n&gt;&gt;&gt;\n&gt;&gt;&gt; c.sum().backward(retain_graph=True)\n[True, True, True, False]\n&gt;&gt;&gt; c.sum().backward(inputs=(a,), retain_graph=True)\n[True, False, True, False]\n&gt;&gt;&gt;\n</pre>  </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.autograd.graph.allow_mutation_on_saved_tensors\">\n<code>class torch.autograd.graph.allow_mutation_on_saved_tensors</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/autograd/graph.html#allow_mutation_on_saved_tensors\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Context manager under which mutating tensors saved for backward is allowed</p> <p>Under this context manager, tensors saved for backward are cloned on mutation, so the original version can still be used during backward. Normally, mutating a tensor saved for backward will result in an error raised when its used during backward.</p> <p>To ensure the correct behavior, both the forward and backward should be run under the same context manager.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Returns</dt> <dd class=\"field-odd\">\n<p>An _AllowMutationOnSavedContext object storing the state managed by this context manager. This object can be useful for debugging purposes. The state managed by the context manager is automatically cleared upon exiting.</p> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; import torch\n&gt;&gt;&gt; with torch.autograd.graph.allow_mutation_on_saved_tensors():\n...     # forward\n...     a = torch.ones(2, 3, requires_grad=True)\n...     b = a.clone()\n...     out = (b**2).sum()\n...     b.sin_()\n...     # backward\n...     out.sum().backward()\n...\ntensor([[0.8415, 0.8415, 0.8415],\n        [0.8415, 0.8415, 0.8415]], grad_fn=&lt;SinBackward0&gt;)\n</pre> </dd>\n</dl><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href=\"https://github.com/pytorch/pytorch/blob/main/LICENSE\">LICENSE</a> file.<br>\n    <a href=\"https://pytorch.org/docs/2.1/autograd.html\" class=\"_attribution-link\">https://pytorch.org/docs/2.1/autograd.html</a>\n  </p>\n</div>\n","library":"<h1 id=\"torch-library\">torch.library</h1> <p>Python operator registration API provides capabilities for extending PyTorchs core library of operators with user defined operators. Currently, this can be done in two ways:</p> <ol class=\"arabic\"> <li>\n<p>Creating new libraries</p> <ul> <li>\n<p>Lets you to register <strong>new operators</strong> and kernels for various backends and functionalities by specifying the appropriate dispatch keys. For example,</p>  <ul class=\"simple\"> <li>Consider registering a new operator <code>add</code> in your newly created namespace <code>foo</code>. You can access this operator using the <code>torch.ops</code> API and calling into by calling <code>torch.ops.foo.add</code>. You can also access specific registered overloads by calling <code>torch.ops.foo.add.{overload_name}</code>.</li> <li>If you registered a new kernel for the <code>CUDA</code> dispatch key for this operator, then your custom defined function will be called for CUDA tensor inputs.</li> </ul>  </li> <li>This can be done by creating Library class objects of <code>\"DEF\"</code> kind.</li> </ul> </li> <li>\n<p>Extending existing C++ libraries (e.g., aten)</p> <ul> <li>Lets you register kernels for <strong>existing operators</strong> corresponding to various backends and functionalities by specifying the appropriate dispatch keys.</li> <li>\n<p>This may come in handy to fill up spotty operator support for a feature implemented through a dispatch key. For example.,</p>  <ul class=\"simple\"> <li>You can add operator support for Meta Tensors (by registering function to the <code>Meta</code> dispatch key).</li> </ul>  </li> <li>This can be done by creating Library class objects of <code>\"IMPL\"</code> kind.</li> </ul> </li> </ol> <p>A tutorial that walks you through some examples on how to use this API is available on <a class=\"reference external\" href=\"https://colab.research.google.com/drive/1RRhSfk7So3Cn02itzLWE9K4Fam-8U011?usp=sharing\">Google Colab</a>.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Dispatcher is a complicated PyTorch concept and having a sound understanding of Dispatcher is crucial to be able to do anything advanced with this API. <a class=\"reference external\" href=\"http://blog.ezyang.com/2020/09/lets-talk-about-the-pytorch-dispatcher/\">This blog post</a> is a good starting point to learn about Dispatcher.</p> </div> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.library.Library\">\n<code>class torch.library.Library(ns, kind, dispatch_key='')</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/library.html#Library\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>A class to create libraries that can be used to register new operators or override operators in existing libraries from Python. A user can optionally pass in a dispatch keyname if they only want to register kernels corresponding to only one specific dispatch key.</p> <p>To create a library to override operators in an existing library (with name ns), set the kind to IMPL. To create a new library (with name ns) to register new operators, set the kind to DEF. To create a fragment of a possibly existing library to register operators (and bypass the limitation that there is only one library for a given namespace), set the kind to FRAGMENT.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>ns</strong>  library name</li> <li>\n<strong>kind</strong>  DEF, IMPL (default: IMPL), FRAGMENT</li> <li>\n<strong>dispatch_key</strong>  PyTorch dispatch key (default: )</li> </ul> </dd> </dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.library.Library.define\">\n<code>define(schema, alias_analysis='')</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/library.html#Library.define\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Defines a new operator and its semantics in the ns namespace.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>schema</strong>  function schema to define a new operator.</li> <li>\n<strong>alias_analysis</strong> (<em>optional</em>)  Indicates if the aliasing properties of the operator arguments can be inferred from the schema (default behavior) or not (CONSERVATIVE).</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>name of the operator as inferred from the schema.</p> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; my_lib = Library(\"foo\", \"DEF\")\n&gt;&gt;&gt; my_lib.define(\"sum(Tensor self) -&gt; Tensor\")\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.library.Library.impl\">\n<code>impl(op_name, fn, dispatch_key='')</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/library.html#Library.impl\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Registers the function implementation for an operator defined in the library.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>op_name</strong>  operator name (along with the overload) or OpOverload object.</li> <li>\n<strong>fn</strong>  function thats the operator implementation for the input dispatch key or <a class=\"reference internal\" href=\"#torch.library.fallthrough_kernel\" title=\"torch.library.fallthrough_kernel\"><code>fallthrough_kernel()</code></a> to register a fallthrough.</li> <li>\n<strong>dispatch_key</strong>  dispatch key that the input function should be registered for. By default, it uses the dispatch key that the library was created with.</li> </ul> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; my_lib = Library(\"aten\", \"IMPL\")\n&gt;&gt;&gt; def div_cpu(self, other):\n&gt;&gt;&gt;     return self * (1 / other)\n&gt;&gt;&gt; my_lib.impl(\"div.Tensor\", div_cpu, \"CPU\")\n</pre> </dd> </dl> </dd>\n</dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.library.fallthrough_kernel\">\n<code>torch.library.fallthrough_kernel()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/library.html#fallthrough_kernel\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>A dummy function to pass to <code>Library.impl</code> in order to register a fallthrough.</p> </dd>\n</dl> <p>We have also added some function decorators to make it convenient to register functions for operators:</p> <ul class=\"simple\"> <li><code>torch.library.impl()</code></li> <li><code>torch.library.define()</code></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href=\"https://github.com/pytorch/pytorch/blob/main/LICENSE\">LICENSE</a> file.<br>\n    <a href=\"https://pytorch.org/docs/2.1/library.html\" class=\"_attribution-link\">https://pytorch.org/docs/2.1/library.html</a>\n  </p>\n</div>\n","cpu":"<h1 id=\"torch-cpu\">torch.cpu</h1> <p id=\"module-torch.cpu\">This package implements abstractions found in <code>torch.cuda</code> to facilitate writing device-agnostic code.</p> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cpu.current_stream#torch.cpu.current_stream\" title=\"torch.cpu.current_stream\"><code>current_stream</code></a>\n</td> <td><p>Returns the currently selected <a class=\"reference internal\" href=\"generated/torch.cpu.stream#torch.cpu.Stream\" title=\"torch.cpu.Stream\"><code>Stream</code></a> for a given device.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cpu.is_available#torch.cpu.is_available\" title=\"torch.cpu.is_available\"><code>is_available</code></a>\n</td> <td><p>Returns a bool indicating if CPU is currently available.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cpu.synchronize#torch.cpu.synchronize\" title=\"torch.cpu.synchronize\"><code>synchronize</code></a>\n</td> <td><p>Waits for all kernels in all streams on the CPU device to complete.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cpu.stream#torch.cpu.stream\" title=\"torch.cpu.stream\"><code>stream</code></a>\n</td> <td><p>Wrapper around the Context-manager StreamContext that selects a given stream.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cpu.device_count#torch.cpu.device_count\" title=\"torch.cpu.device_count\"><code>device_count</code></a>\n</td> <td><p>Returns number of CPU devices (not cores).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cpu.streamcontext#torch.cpu.StreamContext\" title=\"torch.cpu.StreamContext\"><code>StreamContext</code></a>\n</td> <td><p>Context-manager that selects a given stream.</p></td> </tr>  </table>  <h2 id=\"streams-and-events\">Streams and events</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cpu.stream#torch.cpu.Stream\" title=\"torch.cpu.Stream\"><code>Stream</code></a>\n</td> <td><p>N.B.</p></td> </tr>  </table><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href=\"https://github.com/pytorch/pytorch/blob/main/LICENSE\">LICENSE</a> file.<br>\n    <a href=\"https://pytorch.org/docs/2.1/cpu.html\" class=\"_attribution-link\">https://pytorch.org/docs/2.1/cpu.html</a>\n  </p>\n</div>\n","cuda":"<h1 id=\"torch-cuda\">torch.cuda</h1> <p id=\"module-torch.cuda\">This package adds support for CUDA tensor types, that implement the same function as CPU tensors, but they utilize GPUs for computation.</p> <p>It is lazily initialized, so you can always import it, and use <a class=\"reference internal\" href=\"generated/torch.cuda.is_available#torch.cuda.is_available\" title=\"torch.cuda.is_available\"><code>is_available()</code></a> to determine if your system supports CUDA.</p> <p><a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/cuda.html#cuda-semantics\"><span class=\"std std-ref\">CUDA semantics</span></a> has more details about working with CUDA.</p> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.streamcontext#torch.cuda.StreamContext\" title=\"torch.cuda.StreamContext\"><code>StreamContext</code></a>\n</td> <td><p>Context-manager that selects a given stream.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.can_device_access_peer#torch.cuda.can_device_access_peer\" title=\"torch.cuda.can_device_access_peer\"><code>can_device_access_peer</code></a>\n</td> <td><p>Checks if peer access between two devices is possible.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.current_blas_handle#torch.cuda.current_blas_handle\" title=\"torch.cuda.current_blas_handle\"><code>current_blas_handle</code></a>\n</td> <td><p>Returns cublasHandle_t pointer to current cuBLAS handle</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.current_device#torch.cuda.current_device\" title=\"torch.cuda.current_device\"><code>current_device</code></a>\n</td> <td><p>Returns the index of a currently selected device.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.current_stream#torch.cuda.current_stream\" title=\"torch.cuda.current_stream\"><code>current_stream</code></a>\n</td> <td><p>Returns the currently selected <a class=\"reference internal\" href=\"generated/torch.cuda.stream#torch.cuda.Stream\" title=\"torch.cuda.Stream\"><code>Stream</code></a> for a given device.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.default_stream#torch.cuda.default_stream\" title=\"torch.cuda.default_stream\"><code>default_stream</code></a>\n</td> <td><p>Returns the default <a class=\"reference internal\" href=\"generated/torch.cuda.stream#torch.cuda.Stream\" title=\"torch.cuda.Stream\"><code>Stream</code></a> for a given device.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.device#torch.cuda.device\" title=\"torch.cuda.device\"><code>device</code></a>\n</td> <td><p>Context-manager that changes the selected device.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.device_count#torch.cuda.device_count\" title=\"torch.cuda.device_count\"><code>device_count</code></a>\n</td> <td><p>Returns the number of GPUs available.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.device_of#torch.cuda.device_of\" title=\"torch.cuda.device_of\"><code>device_of</code></a>\n</td> <td><p>Context-manager that changes the current device to that of given object.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.get_arch_list#torch.cuda.get_arch_list\" title=\"torch.cuda.get_arch_list\"><code>get_arch_list</code></a>\n</td> <td><p>Returns list CUDA architectures this library was compiled for.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.get_device_capability#torch.cuda.get_device_capability\" title=\"torch.cuda.get_device_capability\"><code>get_device_capability</code></a>\n</td> <td><p>Gets the cuda capability of a device.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.get_device_name#torch.cuda.get_device_name\" title=\"torch.cuda.get_device_name\"><code>get_device_name</code></a>\n</td> <td><p>Gets the name of a device.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.get_device_properties#torch.cuda.get_device_properties\" title=\"torch.cuda.get_device_properties\"><code>get_device_properties</code></a>\n</td> <td><p>Gets the properties of a device.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.get_gencode_flags#torch.cuda.get_gencode_flags\" title=\"torch.cuda.get_gencode_flags\"><code>get_gencode_flags</code></a>\n</td> <td><p>Returns NVCC gencode flags this library was compiled with.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.get_sync_debug_mode#torch.cuda.get_sync_debug_mode\" title=\"torch.cuda.get_sync_debug_mode\"><code>get_sync_debug_mode</code></a>\n</td> <td><p>Returns current value of debug mode for cuda synchronizing operations.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.init#torch.cuda.init\" title=\"torch.cuda.init\"><code>init</code></a>\n</td> <td><p>Initialize PyTorch's CUDA state.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.ipc_collect#torch.cuda.ipc_collect\" title=\"torch.cuda.ipc_collect\"><code>ipc_collect</code></a>\n</td> <td><p>Force collects GPU memory after it has been released by CUDA IPC.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.is_available#torch.cuda.is_available\" title=\"torch.cuda.is_available\"><code>is_available</code></a>\n</td> <td><p>Returns a bool indicating if CUDA is currently available.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.is_initialized#torch.cuda.is_initialized\" title=\"torch.cuda.is_initialized\"><code>is_initialized</code></a>\n</td> <td><p>Returns whether PyTorch's CUDA state has been initialized.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.memory_usage#torch.cuda.memory_usage\" title=\"torch.cuda.memory_usage\"><code>memory_usage</code></a>\n</td> <td><p>Returns the percent of time over the past sample period during which global (device) memory was being read or written.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.set_device#torch.cuda.set_device\" title=\"torch.cuda.set_device\"><code>set_device</code></a>\n</td> <td><p>Sets the current device.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.set_stream#torch.cuda.set_stream\" title=\"torch.cuda.set_stream\"><code>set_stream</code></a>\n</td> <td><p>Sets the current stream.This is a wrapper API to set the stream.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.set_sync_debug_mode#torch.cuda.set_sync_debug_mode\" title=\"torch.cuda.set_sync_debug_mode\"><code>set_sync_debug_mode</code></a>\n</td> <td><p>Sets the debug mode for cuda synchronizing operations.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.stream#torch.cuda.stream\" title=\"torch.cuda.stream\"><code>stream</code></a>\n</td> <td><p>Wrapper around the Context-manager StreamContext that selects a given stream.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.synchronize#torch.cuda.synchronize\" title=\"torch.cuda.synchronize\"><code>synchronize</code></a>\n</td> <td><p>Waits for all kernels in all streams on a CUDA device to complete.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.utilization#torch.cuda.utilization\" title=\"torch.cuda.utilization\"><code>utilization</code></a>\n</td> <td><p>Returns the percent of time over the past sample period during which one or more kernels was executing on the GPU as given by <code>nvidia-smi</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.temperature#torch.cuda.temperature\" title=\"torch.cuda.temperature\"><code>temperature</code></a>\n</td> <td><p>Returns the average temperature of the GPU sensor in Degrees C (Centigrades)</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.power_draw#torch.cuda.power_draw\" title=\"torch.cuda.power_draw\"><code>power_draw</code></a>\n</td> <td><p>Returns the average power draw of the GPU sensor in mW (MilliWatts)</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.clock_rate#torch.cuda.clock_rate\" title=\"torch.cuda.clock_rate\"><code>clock_rate</code></a>\n</td> <td><p>Returns the clock speed of the GPU SM in Hz Hertz over the past sample period as given by <code>nvidia-smi</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.outofmemoryerror#torch.cuda.OutOfMemoryError\" title=\"torch.cuda.OutOfMemoryError\"><code>OutOfMemoryError</code></a>\n</td> <td><p>Exception raised when CUDA is out of memory</p></td> </tr>  </table>  <h2 id=\"random-number-generator\">Random Number Generator</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.get_rng_state#torch.cuda.get_rng_state\" title=\"torch.cuda.get_rng_state\"><code>get_rng_state</code></a>\n</td> <td><p>Returns the random number generator state of the specified GPU as a ByteTensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.get_rng_state_all#torch.cuda.get_rng_state_all\" title=\"torch.cuda.get_rng_state_all\"><code>get_rng_state_all</code></a>\n</td> <td><p>Returns a list of ByteTensor representing the random number states of all devices.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.set_rng_state#torch.cuda.set_rng_state\" title=\"torch.cuda.set_rng_state\"><code>set_rng_state</code></a>\n</td> <td><p>Sets the random number generator state of the specified GPU.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.set_rng_state_all#torch.cuda.set_rng_state_all\" title=\"torch.cuda.set_rng_state_all\"><code>set_rng_state_all</code></a>\n</td> <td><p>Sets the random number generator state of all devices.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.manual_seed#torch.cuda.manual_seed\" title=\"torch.cuda.manual_seed\"><code>manual_seed</code></a>\n</td> <td><p>Sets the seed for generating random numbers for the current GPU.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.manual_seed_all#torch.cuda.manual_seed_all\" title=\"torch.cuda.manual_seed_all\"><code>manual_seed_all</code></a>\n</td> <td><p>Sets the seed for generating random numbers on all GPUs.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.seed#torch.cuda.seed\" title=\"torch.cuda.seed\"><code>seed</code></a>\n</td> <td><p>Sets the seed for generating random numbers to a random number for the current GPU.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.seed_all#torch.cuda.seed_all\" title=\"torch.cuda.seed_all\"><code>seed_all</code></a>\n</td> <td><p>Sets the seed for generating random numbers to a random number on all GPUs.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.initial_seed#torch.cuda.initial_seed\" title=\"torch.cuda.initial_seed\"><code>initial_seed</code></a>\n</td> <td><p>Returns the current random seed of the current GPU.</p></td> </tr>  </table>   <h2 id=\"communication-collectives\">Communication collectives</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.cuda.comm.broadcast#torch.cuda.comm.broadcast\" title=\"torch.cuda.comm.broadcast\"><code>comm.broadcast</code></a></p></td> <td><p>Broadcasts a tensor to specified GPU devices.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.cuda.comm.broadcast_coalesced#torch.cuda.comm.broadcast_coalesced\" title=\"torch.cuda.comm.broadcast_coalesced\"><code>comm.broadcast_coalesced</code></a></p></td> <td><p>Broadcasts a sequence tensors to the specified GPUs.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.cuda.comm.reduce_add#torch.cuda.comm.reduce_add\" title=\"torch.cuda.comm.reduce_add\"><code>comm.reduce_add</code></a></p></td> <td><p>Sums tensors from multiple GPUs.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.cuda.comm.scatter#torch.cuda.comm.scatter\" title=\"torch.cuda.comm.scatter\"><code>comm.scatter</code></a></p></td> <td><p>Scatters tensor across multiple GPUs.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.cuda.comm.gather#torch.cuda.comm.gather\" title=\"torch.cuda.comm.gather\"><code>comm.gather</code></a></p></td> <td><p>Gathers tensors from multiple GPU devices.</p></td> </tr>  </table>   <h2 id=\"streams-and-events\">Streams and events</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.stream#torch.cuda.Stream\" title=\"torch.cuda.Stream\"><code>Stream</code></a>\n</td> <td><p>Wrapper around a CUDA stream.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.externalstream#torch.cuda.ExternalStream\" title=\"torch.cuda.ExternalStream\"><code>ExternalStream</code></a>\n</td> <td><p>Wrapper around an externally allocated CUDA stream.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.event#torch.cuda.Event\" title=\"torch.cuda.Event\"><code>Event</code></a>\n</td> <td><p>Wrapper around a CUDA event.</p></td> </tr>  </table>   <h2 id=\"graphs-beta\">Graphs (beta)</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.is_current_stream_capturing#torch.cuda.is_current_stream_capturing\" title=\"torch.cuda.is_current_stream_capturing\"><code>is_current_stream_capturing</code></a>\n</td> <td><p>Returns True if CUDA graph capture is underway on the current CUDA stream, False otherwise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.graph_pool_handle#torch.cuda.graph_pool_handle\" title=\"torch.cuda.graph_pool_handle\"><code>graph_pool_handle</code></a>\n</td> <td><p>Returns an opaque token representing the id of a graph memory pool.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph\" title=\"torch.cuda.CUDAGraph\"><code>CUDAGraph</code></a>\n</td> <td><p>Wrapper around a CUDA graph.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.graph#torch.cuda.graph\" title=\"torch.cuda.graph\"><code>graph</code></a>\n</td> <td><p>Context-manager that captures CUDA work into a <a class=\"reference internal\" href=\"generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph\" title=\"torch.cuda.CUDAGraph\"><code>torch.cuda.CUDAGraph</code></a> object for later replay.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.make_graphed_callables#torch.cuda.make_graphed_callables\" title=\"torch.cuda.make_graphed_callables\"><code>make_graphed_callables</code></a>\n</td> <td><p>Accepts callables (functions or <a class=\"reference internal\" href=\"generated/torch.nn.module#torch.nn.Module\" title=\"torch.nn.Module\"><code>nn.Module</code></a>s) and returns graphed versions.</p></td> </tr>  </table>   <h2 id=\"cuda-memory-management-api\">Memory management</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\" id=\"memory-management\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.empty_cache#torch.cuda.empty_cache\" title=\"torch.cuda.empty_cache\"><code>empty_cache</code></a>\n</td> <td><p>Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in <code>nvidia-smi</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.list_gpu_processes#torch.cuda.list_gpu_processes\" title=\"torch.cuda.list_gpu_processes\"><code>list_gpu_processes</code></a>\n</td> <td><p>Returns a human-readable printout of the running processes and their GPU memory use for a given device.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.mem_get_info#torch.cuda.mem_get_info\" title=\"torch.cuda.mem_get_info\"><code>mem_get_info</code></a>\n</td> <td><p>Returns the global free and total GPU memory for a given device using cudaMemGetInfo.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.memory_stats#torch.cuda.memory_stats\" title=\"torch.cuda.memory_stats\"><code>memory_stats</code></a>\n</td> <td><p>Returns a dictionary of CUDA memory allocator statistics for a given device.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.memory_summary#torch.cuda.memory_summary\" title=\"torch.cuda.memory_summary\"><code>memory_summary</code></a>\n</td> <td><p>Returns a human-readable printout of the current memory allocator statistics for a given device.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.memory_snapshot#torch.cuda.memory_snapshot\" title=\"torch.cuda.memory_snapshot\"><code>memory_snapshot</code></a>\n</td> <td><p>Returns a snapshot of the CUDA memory allocator state across all devices.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.memory_allocated#torch.cuda.memory_allocated\" title=\"torch.cuda.memory_allocated\"><code>memory_allocated</code></a>\n</td> <td><p>Returns the current GPU memory occupied by tensors in bytes for a given device.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.max_memory_allocated#torch.cuda.max_memory_allocated\" title=\"torch.cuda.max_memory_allocated\"><code>max_memory_allocated</code></a>\n</td> <td><p>Returns the maximum GPU memory occupied by tensors in bytes for a given device.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.reset_max_memory_allocated#torch.cuda.reset_max_memory_allocated\" title=\"torch.cuda.reset_max_memory_allocated\"><code>reset_max_memory_allocated</code></a>\n</td> <td><p>Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.memory_reserved#torch.cuda.memory_reserved\" title=\"torch.cuda.memory_reserved\"><code>memory_reserved</code></a>\n</td> <td><p>Returns the current GPU memory managed by the caching allocator in bytes for a given device.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.max_memory_reserved#torch.cuda.max_memory_reserved\" title=\"torch.cuda.max_memory_reserved\"><code>max_memory_reserved</code></a>\n</td> <td><p>Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.set_per_process_memory_fraction#torch.cuda.set_per_process_memory_fraction\" title=\"torch.cuda.set_per_process_memory_fraction\"><code>set_per_process_memory_fraction</code></a>\n</td> <td><p>Set memory fraction for a process.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.memory_cached#torch.cuda.memory_cached\" title=\"torch.cuda.memory_cached\"><code>memory_cached</code></a>\n</td> <td><p>Deprecated; see <a class=\"reference internal\" href=\"generated/torch.cuda.memory_reserved#torch.cuda.memory_reserved\" title=\"torch.cuda.memory_reserved\"><code>memory_reserved()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.max_memory_cached#torch.cuda.max_memory_cached\" title=\"torch.cuda.max_memory_cached\"><code>max_memory_cached</code></a>\n</td> <td><p>Deprecated; see <a class=\"reference internal\" href=\"generated/torch.cuda.max_memory_reserved#torch.cuda.max_memory_reserved\" title=\"torch.cuda.max_memory_reserved\"><code>max_memory_reserved()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.reset_max_memory_cached#torch.cuda.reset_max_memory_cached\" title=\"torch.cuda.reset_max_memory_cached\"><code>reset_max_memory_cached</code></a>\n</td> <td><p>Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.reset_peak_memory_stats#torch.cuda.reset_peak_memory_stats\" title=\"torch.cuda.reset_peak_memory_stats\"><code>reset_peak_memory_stats</code></a>\n</td> <td><p>Resets the \"peak\" stats tracked by the CUDA memory allocator.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.caching_allocator_alloc#torch.cuda.caching_allocator_alloc\" title=\"torch.cuda.caching_allocator_alloc\"><code>caching_allocator_alloc</code></a>\n</td> <td><p>Performs a memory allocation using the CUDA memory allocator.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.caching_allocator_delete#torch.cuda.caching_allocator_delete\" title=\"torch.cuda.caching_allocator_delete\"><code>caching_allocator_delete</code></a>\n</td> <td><p>Deletes memory allocated using the CUDA memory allocator.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.get_allocator_backend#torch.cuda.get_allocator_backend\" title=\"torch.cuda.get_allocator_backend\"><code>get_allocator_backend</code></a>\n</td> <td><p>Returns a string describing the active allocator backend as set by <code>PYTORCH_CUDA_ALLOC_CONF</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.cudapluggableallocator#torch.cuda.CUDAPluggableAllocator\" title=\"torch.cuda.CUDAPluggableAllocator\"><code>CUDAPluggableAllocator</code></a>\n</td> <td><p>CUDA memory allocator loaded from a so file.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cuda.change_current_allocator#torch.cuda.change_current_allocator\" title=\"torch.cuda.change_current_allocator\"><code>change_current_allocator</code></a>\n</td> <td><p>Changes the currently used memory allocator to be the one provided.</p></td> </tr>  </table>   <h2 id=\"nvidia-tools-extension-nvtx\">NVIDIA Tools Extension (NVTX)</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.cuda.nvtx.mark#torch.cuda.nvtx.mark\" title=\"torch.cuda.nvtx.mark\"><code>nvtx.mark</code></a></p></td> <td><p>Describe an instantaneous event that occurred at some point.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.cuda.nvtx.range_push#torch.cuda.nvtx.range_push\" title=\"torch.cuda.nvtx.range_push\"><code>nvtx.range_push</code></a></p></td> <td><p>Pushes a range onto a stack of nested range span.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.cuda.nvtx.range_pop#torch.cuda.nvtx.range_pop\" title=\"torch.cuda.nvtx.range_pop\"><code>nvtx.range_pop</code></a></p></td> <td><p>Pops a range off of a stack of nested range spans.</p></td> </tr>  </table>   <h2 id=\"jiterator-beta\">Jiterator (beta)</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.cuda.jiterator._create_jit_fn#torch.cuda.jiterator._create_jit_fn\" title=\"torch.cuda.jiterator._create_jit_fn\"><code>jiterator._create_jit_fn</code></a></p></td> <td><p>Create a jiterator-generated cuda kernel for an elementwise op.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.cuda.jiterator._create_multi_output_jit_fn#torch.cuda.jiterator._create_multi_output_jit_fn\" title=\"torch.cuda.jiterator._create_multi_output_jit_fn\"><code>jiterator._create_multi_output_jit_fn</code></a></p></td> <td><p>Create a jiterator-generated cuda kernel for an elementwise op that supports returning one or more outputs.</p></td> </tr>  </table>   <h2 id=\"stream-sanitizer-prototype\">Stream Sanitizer (prototype)</h2> <p>CUDA Sanitizer is a prototype tool for detecting synchronization errors between streams in PyTorch. See the <a class=\"reference internal\" href=\"cuda._sanitizer\"><span class=\"doc\">documentation</span></a> for information on how to use it.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href=\"https://github.com/pytorch/pytorch/blob/main/LICENSE\">LICENSE</a> file.<br>\n    <a href=\"https://pytorch.org/docs/2.1/cuda.html\" class=\"_attribution-link\">https://pytorch.org/docs/2.1/cuda.html</a>\n  </p>\n</div>\n","torch_cuda_memory":"<h1 id=\"torch-cuda-memory\">Understanding CUDA Memory Usage</h1> <p id=\"understanding-cuda-memory-usage\">To debug CUDA memory use, PyTorch provides a way to generate memory snapshots that record the state of allocated CUDA memory at any point in time, and optionally record the history of allocation events that led up to that snapshot.</p> <p>The generated snapshots can then be drag and dropped onto the interactiver viewer hosted at <a class=\"reference external\" href=\"https://pytorch.org/memory_viz\">pytorch.org/memory_viz</a> which can be used to explore the snapshot.</p>   <h1 id=\"generating-a-snapshot\">Generating a Snapshot</h1> <p>The common pattern for recording a snapshot is to enable memory history, run the code to be observed, and then save a file with a pickled snapshot:</p> <pre data-language=\"python\"># enable memory history, which will\n# add tracebacks and event history to snapshots\ntorch.cuda.memory._record_memory_history()\n\nrun_your_code()\ntorch.cuda.memory._dump_snapshot(\"my_snapshot.pickle\")\n</pre>   <h1 id=\"using-the-visualizer\">Using the visualizer</h1> <p>Open <a class=\"reference external\" href=\"https://pytorch.org/memory_viz\">pytorch.org/memory_viz</a> and drag/drop the pickled snapshot file into the visualizer. The visualizer is a javascript application that runs locally on your computer. It does not upload any snapshot data.</p>  <h2 id=\"active-memory-timeline\">Active Memory Timeline</h2> <p>The Active Memory Timeline shows all the live tensors over time in the snapshot on a particular GPU. Pan/Zoom over the plot to look at smaller allocations. Mouse over allocated blocks to see a stack trace for when that block was allocated, and details like its address. The detail slider can be adjusted to render fewer allocations and improve performance when there is a lot of data.</p> <img alt=\"_images/active_memory_timeline.png\" src=\"https://pytorch.org/docs/2.1/_images/active_memory_timeline.png\">   <h2 id=\"allocator-state-history\">Allocator State History</h2> <p>The Allocator State History shows individual allocator events in a timeline on the left. Select an event in the timeline to see a visual summary of the allocator state at that event. This summary shows each individual segment returned from cudaMalloc and how it is split up into blocks of individual allocations or free space. Mouse over segments and blocks to see the stack trace when the memory was allocated. Mouse over events to see the stack trace when the event occured, such as when a tensor was freed. Out of memory errors are reported as OOM events. Looking at the state of memory during an OOM may provide insight into why an allocation failed even though reserved memory still exists.</p> <img alt=\"_images/allocator_state_history.png\" src=\"https://pytorch.org/docs/2.1/_images/allocator_state_history.png\"> <p>The stack trace information also reports the address at which an allocation occured. The address b7f064c000000_0 refers to the (b)lock at address 7f064c000000 which is the _0th time this address was allocated. This unique string can be looked up in the Active Memory Timeline and searched in the Active State History to examine the memory state when a tensor was allocated or freed.</p>    <h1 id=\"snapshot-api-reference\">Snapshot API Reference</h1> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.cuda.memory._record_memory_history\">\n<code>torch.cuda.memory._record_memory_history(enabled='all', context='all', stacks='all', max_entries=9223372036854775807, device=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/cuda/memory.html#_record_memory_history\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Enables recording of stack traces associated with memory allocations, so you can tell what allocated any piece of memory in <a class=\"reference internal\" href=\"#torch.cuda.memory._snapshot\" title=\"torch.cuda.memory._snapshot\"><code>torch.cuda.memory._snapshot()</code></a>.</p> <p>In addition too keeping stack traces with each current allocation and free, this will also enable recording of a history of all alloc/free events.</p> <p>Use <a class=\"reference internal\" href=\"#torch.cuda.memory._snapshot\" title=\"torch.cuda.memory._snapshot\"><code>torch.cuda.memory._snapshot()</code></a> to retrieve this information, and the tools in <code>_memory_viz.py</code> to visualize snapshots.</p> <p>The Python trace collection is fast (2us per trace), so you may consider enabling this on production jobs if you anticipate ever having to debug memory issues.</p> <p>C++ trace collection is also fast (~50ns/frame), which for many typical programs works out to ~2us per trace, but can vary depending on stack depth.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>enabled</strong> (<em>Literal</em><em>[</em><em>None</em><em>, </em><em>\"state\"</em><em>, </em><em>\"all\"</em><em>]</em><em>, </em><em>optional</em>)  <code>None</code>, disable recording memory history. <code>state</code>, keep information for currenly allocated memory. <code>all</code>, additionally keep a history of all alloc/free calls. Defaults to all.</li> <li>\n<strong>context</strong> (<em>Literal</em><em>[</em><em>None</em><em>, </em><em>\"state\"</em><em>, </em><em>\"alloc\"</em><em>, </em><em>\"all\"</em><em>]</em><em>, </em><em>optional</em>)  <code>None</code>, Do not record any tracebacks. <code>state</code>, Record tracebacks for currently allocated memory. <code>alloc</code>, additionally keep tracebacks for alloc calls. <code>all</code>, additionally keep tracebacks for free calls. Defaults to all.</li> <li>\n<strong>stacks</strong> (<em>Literal</em><em>[</em><em>\"python\"</em><em>, </em><em>\"all\"</em><em>]</em><em>, </em><em>optional</em>)  <code>python</code>, include Python, TorchScript, and inductor frames in tracebacks <code>all</code>, additionally include C++ frames Defaults to all.</li> <li>\n<strong>max_entries</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>, </em><em>optional</em>)  Keep a maximum of <code>max_entries</code> alloc/free events in the recorded history recorded.</li> </ul> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.cuda.memory._snapshot\">\n<code>torch.cuda.memory._snapshot(device=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/cuda/memory.html#_snapshot\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Saves a snapshot of CUDA memory state at the time it was called. The state is represented as a dictionary with the following structure.</p> <pre data-language=\"python\">class Snapshot(TypedDict):\n    segments : List[Segment]\n    device_traces: List[List[TraceEntry]]\n\nclass Segment(TypedDict):\n    # Segments are memory returned from a cudaMalloc call.\n    # The size of reserved memory is the sum of all Segments.\n    # Segments are cached and reused for future allocations.\n    # If the reuse is smaller than the segment, the segment\n    # is split into more then one Block.\n    # empty_cache() frees Segments that are entirely inactive.\n    address: int\n    total_size: int #  cudaMalloc'd size of segment\n    stream: int\n    segment_type: Literal['small', 'large'] # 'large' (&gt;1MB)\n    allocated_size: int # size of memory in use\n    active_size: int # size of memory in use or in active_awaiting_free state\n    blocks : List[Block]\n\nclass Block(TypedDict):\n    # A piece of memory returned from the allocator, or\n    # current cached but inactive.\n    size: int\n    requested_size: int # size requested during malloc, may be smaller than\n                        # size due to rounding\n    address: int\n    state: Literal['active_allocated', # used by a tensor\n                'active_awaiting_free', # waiting for another stream to finish using\n                                        # this, then it will become free\n                'inactive',] # free for reuse\n    frames: List[Frame] # stack trace from where the allocation occurred\n\nclass Frame(TypedDict):\n        filename: str\n        line: int\n        name: str\n\nclass TraceEntry(TypedDict):\n    # When `torch.cuda.memory._record_memory_history()` is enabled,\n    # the snapshot will contain TraceEntry objects that record each\n    # action the allocator took.\n    action: Literal[\n    'alloc'  # memory allocated\n    'free_requested', # the allocated received a call to free memory\n    'free_completed', # the memory that was requested to be freed is now\n                    # able to be used in future allocation calls\n    'segment_alloc', # the caching allocator ask cudaMalloc for more memory\n                    # and added it as a segment in its cache\n    'segment_free',  # the caching allocator called cudaFree to return memory\n                    # to cuda possibly trying free up memory to\n                    # allocate more segments or because empty_caches was called\n    'oom',          # the allocator threw an OOM exception. 'size' is\n                    # the requested number of bytes that did not succeed\n    'snapshot'      # the allocator generated a memory snapshot\n                    # useful to coorelate a previously taken\n                    # snapshot with this trace\n    ]\n    addr: int # not present for OOM\n    frames: List[Frame]\n    size: int\n    stream: int\n    device_free: int # only present for OOM, the amount of\n                    # memory cuda still reports to be free\n</pre> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Returns</dt> <dd class=\"field-odd\">\n<p>The Snapshot dictionary object</p> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.cuda.memory._dump_snapshot\">\n<code>torch.cuda.memory._dump_snapshot(filename='dump_snapshot.pickle')</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/cuda/memory.html#_dump_snapshot\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Saves a pickled version of the <code>torch.memory._snapshot()</code> dictionary to a file. This file can be opened by the interactive snapshot viewer at pytorch.org/memory_viz</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>filename</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a><em>, </em><em>optional</em>)  Name of the file to create. Defaults to dump_snapshot.pickle.</p> </dd> </dl> </dd>\n</dl><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href=\"https://github.com/pytorch/pytorch/blob/main/LICENSE\">LICENSE</a> file.<br>\n    <a href=\"https://pytorch.org/docs/2.1/torch_cuda_memory.html\" class=\"_attribution-link\">https://pytorch.org/docs/2.1/torch_cuda_memory.html</a>\n  </p>\n</div>\n","mps":"<h1 id=\"torch-mps\">torch.mps</h1> <p id=\"module-torch.mps\">This package enables an interface for accessing MPS (Metal Performance Shaders) backend in Python. Metal is Apples API for programming metal GPU (graphics processor unit). Using MPS means that increased performance can be achieved, by running work on the metal GPU(s). See <a class=\"reference external\" href=\"https://developer.apple.com/documentation/metalperformanceshaders\">https://developer.apple.com/documentation/metalperformanceshaders</a> for more details.</p> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.mps.synchronize#torch.mps.synchronize\" title=\"torch.mps.synchronize\"><code>synchronize</code></a>\n</td> <td><p>Waits for all kernels in all streams on a MPS device to complete.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.mps.get_rng_state#torch.mps.get_rng_state\" title=\"torch.mps.get_rng_state\"><code>get_rng_state</code></a>\n</td> <td><p>Returns the random number generator state as a ByteTensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.mps.set_rng_state#torch.mps.set_rng_state\" title=\"torch.mps.set_rng_state\"><code>set_rng_state</code></a>\n</td> <td><p>Sets the random number generator state.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.mps.manual_seed#torch.mps.manual_seed\" title=\"torch.mps.manual_seed\"><code>manual_seed</code></a>\n</td> <td><p>Sets the seed for generating random numbers.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.mps.seed#torch.mps.seed\" title=\"torch.mps.seed\"><code>seed</code></a>\n</td> <td><p>Sets the seed for generating random numbers to a random number.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.mps.empty_cache#torch.mps.empty_cache\" title=\"torch.mps.empty_cache\"><code>empty_cache</code></a>\n</td> <td><p>Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU applications.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.mps.set_per_process_memory_fraction#torch.mps.set_per_process_memory_fraction\" title=\"torch.mps.set_per_process_memory_fraction\"><code>set_per_process_memory_fraction</code></a>\n</td> <td><p>Set memory fraction for limiting process's memory allocation on MPS device.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.mps.current_allocated_memory#torch.mps.current_allocated_memory\" title=\"torch.mps.current_allocated_memory\"><code>current_allocated_memory</code></a>\n</td> <td><p>Returns the current GPU memory occupied by tensors in bytes.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.mps.driver_allocated_memory#torch.mps.driver_allocated_memory\" title=\"torch.mps.driver_allocated_memory\"><code>driver_allocated_memory</code></a>\n</td> <td><p>Returns total GPU memory allocated by Metal driver for the process in bytes.</p></td> </tr>  </table>  <h2 id=\"mps-profiler\">MPS Profiler</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.mps.profiler.start#torch.mps.profiler.start\" title=\"torch.mps.profiler.start\"><code>profiler.start</code></a></p></td> <td><p>Start OS Signpost tracing from MPS backend.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.mps.profiler.stop#torch.mps.profiler.stop\" title=\"torch.mps.profiler.stop\"><code>profiler.stop</code></a></p></td> <td><p>Stops generating OS Signpost tracing from MPS backend.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.mps.profiler.profile#torch.mps.profiler.profile\" title=\"torch.mps.profiler.profile\"><code>profiler.profile</code></a></p></td> <td><p>Context Manager to enabling generating OS Signpost tracing from MPS backend.</p></td> </tr>  </table>   <h2 id=\"mps-event\">MPS Event</h2> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.mps.event.event#torch.mps.event.Event\" title=\"torch.mps.event.Event\"><code>event.Event</code></a></p></td> <td><p>Wrapper around an MPS event.</p></td> </tr>  </table><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href=\"https://github.com/pytorch/pytorch/blob/main/LICENSE\">LICENSE</a> file.<br>\n    <a href=\"https://pytorch.org/docs/2.1/mps.html\" class=\"_attribution-link\">https://pytorch.org/docs/2.1/mps.html</a>\n  </p>\n</div>\n","backends":"<h1 id=\"torch-backends\">torch.backends</h1> <p id=\"module-torch.backends\"><code>torch.backends</code> controls the behavior of various backends that PyTorch supports.</p> <p>These backends include:</p> <ul class=\"simple\"> <li><code>torch.backends.cpu</code></li> <li><code>torch.backends.cuda</code></li> <li><code>torch.backends.cudnn</code></li> <li><code>torch.backends.mps</code></li> <li><code>torch.backends.mkl</code></li> <li><code>torch.backends.mkldnn</code></li> <li><code>torch.backends.openmp</code></li> <li><code>torch.backends.opt_einsum</code></li> <li><code>torch.backends.xeon</code></li> </ul>  <h2 id=\"torch-backends-cpu\">torch.backends.cpu</h2> <dl class=\"py function\" id=\"module-torch.backends.cpu\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cpu.get_cpu_capability\">\n<code>torch.backends.cpu.get_cpu_capability()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/backends/cpu.html#get_cpu_capability\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns cpu capability as a string value.</p> <p>Possible values: - DEFAULT - VSX - Z VECTOR - NO AVX - AVX2 - AVX512</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a></p> </dd> </dl> </dd>\n</dl>   <h2 id=\"torch-backends-cuda\">torch.backends.cuda</h2> <dl class=\"py function\" id=\"module-torch.backends.cuda\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cuda.is_built\">\n<code>torch.backends.cuda.is_built()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/backends/cuda.html#is_built\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns whether PyTorch is built with CUDA support. Note that this doesnt necessarily mean CUDA is available; just that if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it.</p> </dd>\n</dl> <dl class=\"py attribute\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cuda.matmul.allow_tf32\">\n<code>torch.backends.cuda.matmul.allow_tf32</code> </dt> <dd>\n<p>A <a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\"><code>bool</code></a> that controls whether TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere or newer GPUs. See <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/cuda.html#tf32-on-ampere\"><span class=\"std std-ref\">TensorFloat-32(TF32) on Ampere devices</span></a>.</p> </dd>\n</dl> <dl class=\"py attribute\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction\">\n<code>torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction</code> </dt> <dd>\n<p>A <a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\"><code>bool</code></a> that controls whether reduced precision reductions (e.g., with fp16 accumulation type) are allowed with fp16 GEMMs.</p> </dd>\n</dl> <dl class=\"py attribute\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction\">\n<code>torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction</code> </dt> <dd>\n<p>A <a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\"><code>bool</code></a> that controls whether reduced precision reductions are allowed with bf16 GEMMs.</p> </dd>\n</dl> <dl class=\"py attribute\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cuda.cufft_plan_cache\">\n<code>torch.backends.cuda.cufft_plan_cache</code> </dt> <dd>\n<p><code>cufft_plan_cache</code> contains the cuFFT plan caches for each CUDA device. Query a specific device <code>i</code>s cache via <code>torch.backends.cuda.cufft_plan_cache[i]</code>.</p> <dl class=\"py attribute\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cuda.cufft_plan_cache.size\">\n<code>torch.backends.cuda.cufft_plan_cache.size</code> </dt> <dd>\n<p>A readonly <a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\"><code>int</code></a> that shows the number of plans currently in a cuFFT plan cache.</p> </dd>\n</dl> <dl class=\"py attribute\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cuda.cufft_plan_cache.max_size\">\n<code>torch.backends.cuda.cufft_plan_cache.max_size</code> </dt> <dd>\n<p>A <a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\"><code>int</code></a> that controls the capacity of a cuFFT plan cache.</p> </dd>\n</dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cuda.cufft_plan_cache.clear\">\n<code>torch.backends.cuda.cufft_plan_cache.clear()</code> </dt> <dd>\n<p>Clears a cuFFT plan cache.</p> </dd>\n</dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cuda.preferred_linalg_library\">\n<code>torch.backends.cuda.preferred_linalg_library(backend=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/backends/cuda.html#preferred_linalg_library\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This flag is experimental and subject to change.</p> </div> <p>When PyTorch runs a CUDA linear algebra operation it often uses the cuSOLVER or MAGMA libraries, and if both are available it decides which to use with a heuristic. This flag (a <a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\"><code>str</code></a>) allows overriding those heuristics.</p> <ul class=\"simple\"> <li>If <code>cusolver</code> is set then cuSOLVER will be used wherever possible.</li> <li>If <code>magma</code> is set then MAGMA will be used wherever possible.</li> <li>If <code>default</code> (the default) is set then heuristics will be used to pick between cuSOLVER and MAGMA if both are available.</li> <li>When no input is given, this function returns the currently preferred library.</li> <li>User may use the environment variable TORCH_LINALG_PREFER_CUSOLVER=1 to set the preferred library to cuSOLVER globally. This flag only sets the initial value of the preferred library and the preferred library may still be overridden by this function call later in your script.</li> </ul> <p>Note: When a library is preferred other libraries may still be used if the preferred library doesnt implement the operation(s) called. This flag may achieve better performance if PyTorchs heuristic library selection is incorrect for your applications inputs.</p> <p>Currently supported linalg operators:</p> <ul class=\"simple\"> <li><a class=\"reference internal\" href=\"generated/torch.linalg.inv#torch.linalg.inv\" title=\"torch.linalg.inv\"><code>torch.linalg.inv()</code></a></li> <li><a class=\"reference internal\" href=\"generated/torch.linalg.inv_ex#torch.linalg.inv_ex\" title=\"torch.linalg.inv_ex\"><code>torch.linalg.inv_ex()</code></a></li> <li><a class=\"reference internal\" href=\"generated/torch.linalg.cholesky#torch.linalg.cholesky\" title=\"torch.linalg.cholesky\"><code>torch.linalg.cholesky()</code></a></li> <li><a class=\"reference internal\" href=\"generated/torch.linalg.cholesky_ex#torch.linalg.cholesky_ex\" title=\"torch.linalg.cholesky_ex\"><code>torch.linalg.cholesky_ex()</code></a></li> <li><a class=\"reference internal\" href=\"generated/torch.cholesky_solve#torch.cholesky_solve\" title=\"torch.cholesky_solve\"><code>torch.cholesky_solve()</code></a></li> <li><a class=\"reference internal\" href=\"generated/torch.cholesky_inverse#torch.cholesky_inverse\" title=\"torch.cholesky_inverse\"><code>torch.cholesky_inverse()</code></a></li> <li><a class=\"reference internal\" href=\"generated/torch.linalg.lu_factor#torch.linalg.lu_factor\" title=\"torch.linalg.lu_factor\"><code>torch.linalg.lu_factor()</code></a></li> <li><a class=\"reference internal\" href=\"generated/torch.linalg.lu#torch.linalg.lu\" title=\"torch.linalg.lu\"><code>torch.linalg.lu()</code></a></li> <li><a class=\"reference internal\" href=\"generated/torch.linalg.lu_solve#torch.linalg.lu_solve\" title=\"torch.linalg.lu_solve\"><code>torch.linalg.lu_solve()</code></a></li> <li><a class=\"reference internal\" href=\"generated/torch.linalg.qr#torch.linalg.qr\" title=\"torch.linalg.qr\"><code>torch.linalg.qr()</code></a></li> <li><a class=\"reference internal\" href=\"generated/torch.linalg.eigh#torch.linalg.eigh\" title=\"torch.linalg.eigh\"><code>torch.linalg.eigh()</code></a></li> <li><code>torch.linalg.eighvals()</code></li> <li><a class=\"reference internal\" href=\"generated/torch.linalg.svd#torch.linalg.svd\" title=\"torch.linalg.svd\"><code>torch.linalg.svd()</code></a></li> <li><a class=\"reference internal\" href=\"generated/torch.linalg.svdvals#torch.linalg.svdvals\" title=\"torch.linalg.svdvals\"><code>torch.linalg.svdvals()</code></a></li> </ul> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><em>_LinalgBackend</em></p> </dd> </dl> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cuda.SDPBackend\">\n<code>class torch.backends.cuda.SDPBackend(value)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/backends/cuda.html#SDPBackend\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Enum class for the scaled dot product attention backends.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This class is in beta and subject to change.</p> </div> <p>This class needs to stay aligned with the enum defined in: pytorch/aten/src/ATen/native/transformers/sdp_utils_cpp.h</p> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cuda.flash_sdp_enabled\">\n<code>torch.backends.cuda.flash_sdp_enabled()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/backends/cuda.html#flash_sdp_enabled\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This flag is beta and subject to change.</p> </div> <p>Returns whether flash scaled dot product attention is enabled or not.</p> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cuda.enable_mem_efficient_sdp\">\n<code>torch.backends.cuda.enable_mem_efficient_sdp(enabled)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/backends/cuda.html#enable_mem_efficient_sdp\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This flag is beta and subject to change.</p> </div> <p>Enables or disables memory efficient scaled dot product attention.</p>  </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cuda.mem_efficient_sdp_enabled\">\n<code>torch.backends.cuda.mem_efficient_sdp_enabled()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/backends/cuda.html#mem_efficient_sdp_enabled\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This flag is beta and subject to change.</p> </div> <p>Returns whether memory efficient scaled dot product attention is enabled or not.</p> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cuda.enable_flash_sdp\">\n<code>torch.backends.cuda.enable_flash_sdp(enabled)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/backends/cuda.html#enable_flash_sdp\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This flag is beta and subject to change.</p> </div> <p>Enables or disables flash scaled dot product attention.</p>  </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cuda.math_sdp_enabled\">\n<code>torch.backends.cuda.math_sdp_enabled()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/backends/cuda.html#math_sdp_enabled\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This flag is beta and subject to change.</p> </div> <p>Returns whether math scaled dot product attention is enabled or not.</p> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cuda.enable_math_sdp\">\n<code>torch.backends.cuda.enable_math_sdp(enabled)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/backends/cuda.html#enable_math_sdp\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This flag is beta and subject to change.</p> </div> <p>Enables or disables math scaled dot product attention.</p>  </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cuda.sdp_kernel\">\n<code>torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=True, enable_mem_efficient=True)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/backends/cuda.html#sdp_kernel\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This flag is beta and subject to change.</p> </div> <p>This context manager can be used to temporarily enable or disable any of the three backends for scaled dot product attention. Upon exiting the context manager, the previous state of the flags will be restored.</p>  </dd>\n</dl>   <h2 id=\"torch-backends-cudnn\">torch.backends.cudnn</h2> <dl class=\"py function\" id=\"module-torch.backends.cudnn\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cudnn.version\">\n<code>torch.backends.cudnn.version()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/backends/cudnn.html#version\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the version of cuDNN</p> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cudnn.is_available\">\n<code>torch.backends.cudnn.is_available()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/backends/cudnn.html#is_available\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns a bool indicating if CUDNN is currently available.</p> </dd>\n</dl> <dl class=\"py attribute\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cudnn.enabled\">\n<code>torch.backends.cudnn.enabled</code> </dt> <dd>\n<p>A <a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\"><code>bool</code></a> that controls whether cuDNN is enabled.</p> </dd>\n</dl> <dl class=\"py attribute\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cudnn.allow_tf32\">\n<code>torch.backends.cudnn.allow_tf32</code> </dt> <dd>\n<p>A <a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\"><code>bool</code></a> that controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on Ampere or newer GPUs. See <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/notes/cuda.html#tf32-on-ampere\"><span class=\"std std-ref\">TensorFloat-32(TF32) on Ampere devices</span></a>.</p> </dd>\n</dl> <dl class=\"py attribute\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cudnn.deterministic\">\n<code>torch.backends.cudnn.deterministic</code> </dt> <dd>\n<p>A <a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\"><code>bool</code></a> that, if True, causes cuDNN to only use deterministic convolution algorithms. See also <a class=\"reference internal\" href=\"generated/torch.are_deterministic_algorithms_enabled#torch.are_deterministic_algorithms_enabled\" title=\"torch.are_deterministic_algorithms_enabled\"><code>torch.are_deterministic_algorithms_enabled()</code></a> and <a class=\"reference internal\" href=\"generated/torch.use_deterministic_algorithms#torch.use_deterministic_algorithms\" title=\"torch.use_deterministic_algorithms\"><code>torch.use_deterministic_algorithms()</code></a>.</p> </dd>\n</dl> <dl class=\"py attribute\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cudnn.benchmark\">\n<code>torch.backends.cudnn.benchmark</code> </dt> <dd>\n<p>A <a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\"><code>bool</code></a> that, if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest.</p> </dd>\n</dl> <dl class=\"py attribute\"> <dt class=\"sig sig-object py\" id=\"torch.backends.cudnn.benchmark_limit\">\n<code>torch.backends.cudnn.benchmark_limit</code> </dt> <dd>\n<p>A <a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\"><code>int</code></a> that specifies the maximum number of cuDNN convolution algorithms to try when <code>torch.backends.cudnn.benchmark</code> is True. Set <code>benchmark_limit</code> to zero to try every available algorithm. Note that this setting only affects convolutions dispatched via the cuDNN v8 API.</p> </dd>\n</dl>   <h2 id=\"torch-backends-mps\">torch.backends.mps</h2> <dl class=\"py function\" id=\"module-torch.backends.mps\"> <dt class=\"sig sig-object py\" id=\"torch.backends.mps.is_available\">\n<code>torch.backends.mps.is_available()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/backends/mps.html#is_available\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns a bool indicating if MPS is currently available.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a></p> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.backends.mps.is_built\">\n<code>torch.backends.mps.is_built()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/backends/mps.html#is_built\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns whether PyTorch is built with MPS support. Note that this doesnt necessarily mean MPS is available; just that if this PyTorch binary were run a machine with working MPS drivers and devices, we would be able to use it.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a></p> </dd> </dl> </dd>\n</dl>   <h2 id=\"torch-backends-mkl\">torch.backends.mkl</h2> <dl class=\"py function\" id=\"module-torch.backends.mkl\"> <dt class=\"sig sig-object py\" id=\"torch.backends.mkl.is_available\">\n<code>torch.backends.mkl.is_available()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/backends/mkl.html#is_available\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns whether PyTorch is built with MKL support.</p> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.backends.mkl.verbose\">\n<code>class torch.backends.mkl.verbose(enable)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/backends/mkl.html#verbose\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>On-demand oneMKL verbosing functionality To make it easier to debug performance issues, oneMKL can dump verbose messages containing execution information like duration while executing the kernel. The verbosing functionality can be invoked via an environment variable named <code>MKL_VERBOSE</code>. However, this methodology dumps messages in all steps. Those are a large amount of verbose messages. Moreover, for investigating the performance issues, generally taking verbose messages for one single iteration is enough. This on-demand verbosing functionality makes it possible to control scope for verbose message dumping. In the following example, verbose messages will be dumped out for the second inference only.</p> <pre data-language=\"python\">import torch\nmodel(data)\nwith torch.backends.mkl.verbose(torch.backends.mkl.VERBOSE_ON):\n    model(data)\n</pre> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>level</strong>  Verbose level - <code>VERBOSE_OFF</code>: Disable verbosing - <code>VERBOSE_ON</code>: Enable verbosing</p> </dd> </dl> </dd>\n</dl>   <h2 id=\"torch-backends-mkldnn\">torch.backends.mkldnn</h2> <dl class=\"py function\" id=\"module-torch.backends.mkldnn\"> <dt class=\"sig sig-object py\" id=\"torch.backends.mkldnn.is_available\">\n<code>torch.backends.mkldnn.is_available()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/backends/mkldnn.html#is_available\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns whether PyTorch is built with MKL-DNN support.</p> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.backends.mkldnn.verbose\">\n<code>class torch.backends.mkldnn.verbose(level)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/backends/mkldnn.html#verbose\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>On-demand oneDNN (former MKL-DNN) verbosing functionality To make it easier to debug performance issues, oneDNN can dump verbose messages containing information like kernel size, input data size and execution duration while executing the kernel. The verbosing functionality can be invoked via an environment variable named <code>DNNL_VERBOSE</code>. However, this methodology dumps messages in all steps. Those are a large amount of verbose messages. Moreover, for investigating the performance issues, generally taking verbose messages for one single iteration is enough. This on-demand verbosing functionality makes it possible to control scope for verbose message dumping. In the following example, verbose messages will be dumped out for the second inference only.</p> <pre data-language=\"python\">import torch\nmodel(data)\nwith torch.backends.mkldnn.verbose(torch.backends.mkldnn.VERBOSE_ON):\n    model(data)\n</pre> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>level</strong>  Verbose level - <code>VERBOSE_OFF</code>: Disable verbosing - <code>VERBOSE_ON</code>: Enable verbosing - <code>VERBOSE_ON_CREATION</code>: Enable verbosing, including oneDNN kernel creation</p> </dd> </dl> </dd>\n</dl>   <h2 id=\"torch-backends-openmp\">torch.backends.openmp</h2> <dl class=\"py function\" id=\"module-torch.backends.openmp\"> <dt class=\"sig sig-object py\" id=\"torch.backends.openmp.is_available\">\n<code>torch.backends.openmp.is_available()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/backends/openmp.html#is_available\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns whether PyTorch is built with OpenMP support.</p> </dd>\n</dl>   <h2 id=\"torch-backends-opt-einsum\">torch.backends.opt_einsum</h2> <dl class=\"py function\" id=\"module-torch.backends.opt_einsum\"> <dt class=\"sig sig-object py\" id=\"torch.backends.opt_einsum.is_available\">\n<code>torch.backends.opt_einsum.is_available()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/backends/opt_einsum.html#is_available\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns a bool indicating if opt_einsum is currently available.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a></p> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.backends.opt_einsum.get_opt_einsum\">\n<code>torch.backends.opt_einsum.get_opt_einsum()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/backends/opt_einsum.html#get_opt_einsum\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the opt_einsum package if opt_einsum is currently available, else None.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Any\" title=\"(in Python v3.12)\">Any</a></p> </dd> </dl> </dd>\n</dl> <dl class=\"py attribute\"> <dt class=\"sig sig-object py\" id=\"torch.backends.opt_einsum.enabled\">\n<code>torch.backends.opt_einsum.enabled</code> </dt> <dd>\n<p>A :class:<code>bool</code> that controls whether opt_einsum is enabled (<code>True</code> by default). If so, torch.einsum will use opt_einsum (<a class=\"reference external\" href=\"https://optimized-einsum.readthedocs.io/en/stable/path_finding.html\">https://optimized-einsum.readthedocs.io/en/stable/path_finding.html</a>) if available to calculate an optimal path of contraction for faster performance.</p> <p>If opt_einsum is not available, torch.einsum will fall back to the default contraction path of left to right.</p> </dd>\n</dl> <dl class=\"py attribute\"> <dt class=\"sig sig-object py\" id=\"torch.backends.opt_einsum.strategy\">\n<code>torch.backends.opt_einsum.strategy</code> </dt> <dd>\n<p>A :class:<code>str</code> that specifies which strategies to try when <code>torch.backends.opt_einsum.enabled</code> is <code>True</code>. By default, torch.einsum will try the auto strategy, but the greedy and optimal strategies are also supported. Note that the optimal strategy is factorial on the number of inputs as it tries all possible paths. See more details in opt_einsums docs (<a class=\"reference external\" href=\"https://optimized-einsum.readthedocs.io/en/stable/path_finding.html\">https://optimized-einsum.readthedocs.io/en/stable/path_finding.html</a>).</p> </dd>\n</dl>   <h2 id=\"torch-backends-xeon\">torch.backends.xeon</h2><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href=\"https://github.com/pytorch/pytorch/blob/main/LICENSE\">LICENSE</a> file.<br>\n    <a href=\"https://pytorch.org/docs/2.1/backends.html\" class=\"_attribution-link\">https://pytorch.org/docs/2.1/backends.html</a>\n  </p>\n</div>\n","export":"<h1 id=\"id1\">torch.export</h1> <div class=\"admonition warning\" id=\"torch-export\"> <p class=\"admonition-title\">Warning</p> <p>This feature is a prototype under active development and there WILL BE BREAKING CHANGES in the future.</p> </div>  <h2 id=\"overview\">Overview</h2> <p><a class=\"reference internal\" href=\"#torch.export.export\" title=\"torch.export.export\"><code>torch.export.export()</code></a> takes an arbitrary Python callable (a <a class=\"reference internal\" href=\"generated/torch.nn.module#torch.nn.Module\" title=\"torch.nn.Module\"><code>torch.nn.Module</code></a>, a function or a method) and produces a traced graph representing only the Tensor computation of the function in an Ahead-of-Time (AOT) fashion, which can subsequently be executed with different outputs or serialized.</p> <pre data-language=\"python\">import torch\nfrom torch.export import export\n\ndef f(x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n    a = torch.sin(x)\n    b = torch.cos(y)\n    return a + b\n\nexample_args = (torch.randn(10, 10), torch.randn(10, 10))\n\nexported_program: torch.export.ExportedProgram = export(\n    f, args=example_args\n)\nprint(exported_program)\n</pre> <pre data-language=\"python\">ExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, arg0_1: f32[10, 10], arg1_1: f32[10, 10]):\n            # code: a = torch.sin(x)\n            sin: f32[10, 10] = torch.ops.aten.sin.default(arg0_1);\n\n            # code: b = torch.cos(y)\n            cos: f32[10, 10] = torch.ops.aten.cos.default(arg1_1);\n\n            # code: return a + b\n            add: f32[10, 10] = torch.ops.aten.add.Tensor(sin, cos);\n            return (add,)\n\n    Graph signature: ExportGraphSignature(\n        parameters=[],\n        buffers=[],\n        user_inputs=['arg0_1', 'arg1_1'],\n        user_outputs=['add'],\n        inputs_to_parameters={},\n        inputs_to_buffers={},\n        buffers_to_mutate={},\n        backward_signature=None,\n        assertion_dep_token=None,\n    )\n    Range constraints: {}\n    Equality constraints: []\n</pre> <p><code>torch.export</code> produces a clean intermediate representation (IR) with the following invariants. More specifications about the IR can be found here (coming soon!).</p> <ul class=\"simple\"> <li>\n<strong>Soundness</strong>: It is guaranteed to be a sound representation of the original program, and maintains the same calling conventions of the original program.</li> <li>\n<strong>Normalized</strong>: There are no Python semantics within the graph. Submodules from the original programs are inlined to form one fully flattened computational graph.</li> <li>\n<strong>Defined Operator Set</strong>: The graph produced contains only a small defined <a class=\"reference internal\" href=\"torch.compiler_ir#torch-compiler-ir\"><span class=\"std std-ref\">Core ATen IR</span></a> opset and registered custom operators.</li> <li>\n<strong>Graph properties</strong>: The graph is purely functional, meaning it does not contain operations with side effects such as mutations or aliasing. It does not mutate any intermediate values, parameters, or buffers.</li> <li>\n<strong>Metadata</strong>: The graph contains metadata captured during tracing, such as a stacktrace from users code.</li> </ul> <p>Under the hood, <code>torch.export</code> leverages the following latest technologies:</p> <ul class=\"simple\"> <li>\n<strong>TorchDynamo (torch._dynamo)</strong> is an internal API that uses a CPython feature called the Frame Evaluation API to safely trace PyTorch graphs. This provides a massively improved graph capturing experience, with much fewer rewrites needed in order to fully trace the PyTorch code.</li> <li>\n<strong>AOT Autograd</strong> provides a functionalized PyTorch graph and ensures the graph is decomposed/lowered to the small defined Core ATen operator set.</li> <li>\n<strong>Torch FX (torch.fx)</strong> is the underlying representation of the graph, allowing flexible Python-based transformations.</li> </ul>  <h3 id=\"existing-frameworks\">Existing frameworks</h3> <p><a class=\"reference internal\" href=\"generated/torch.compile#torch.compile\" title=\"torch.compile\"><code>torch.compile()</code></a> also utilizes the same PT2 stack as <code>torch.export</code>, but is slightly different:</p> <ul class=\"simple\"> <li>\n<strong>JIT vs. AOT</strong>: <a class=\"reference internal\" href=\"generated/torch.compile#torch.compile\" title=\"torch.compile\"><code>torch.compile()</code></a> is a JIT compiler whereas which is not intended to be used to produce compiled artifacts outside of deployment.</li> <li>\n<strong>Partial vs. Full Graph Capture</strong>: When <a class=\"reference internal\" href=\"generated/torch.compile#torch.compile\" title=\"torch.compile\"><code>torch.compile()</code></a> runs into an untraceable part of a model, it will graph break and fall back to running the program in the eager Python runtime. In comparison, <code>torch.export</code> aims to get a full graph representation of a PyTorch model, so it will error out when something untraceable is reached. Since <code>torch.export</code> produces a full graph disjoint from any Python features or runtime, this graph can then be saved, loaded, and run in different environments and languages.</li> <li>\n<strong>Usability tradeoff</strong>: Since <a class=\"reference internal\" href=\"generated/torch.compile#torch.compile\" title=\"torch.compile\"><code>torch.compile()</code></a> is able to fallback to the Python runtime whenever it reaches something untraceable, it is a lot more flexible. <code>torch.export</code> will instead require users to provide more information or rewrite their code to make it traceable.</li> </ul> <p>Compared to <a class=\"reference internal\" href=\"fx#torch.fx.symbolic_trace\" title=\"torch.fx.symbolic_trace\"><code>torch.fx.symbolic_trace()</code></a>, <code>torch.export</code> traces using TorchDynamo which operates at the Python bytecode level, giving it the ability to trace arbitrary Python constructs not limited by what Python operator overloading supports. Additionally, <code>torch.export</code> keeps fine-grained track of tensor metadata, so that conditionals on things like tensor shapes do not fail tracing. In general, <code>torch.export</code> is expected to work on more user programs, and produce lower-level graphs (at the <code>torch.ops.aten</code> operator level). Note that users can still use <a class=\"reference internal\" href=\"fx#torch.fx.symbolic_trace\" title=\"torch.fx.symbolic_trace\"><code>torch.fx.symbolic_trace()</code></a> as a preprocessing step before <code>torch.export</code>.</p> <p>Compared to <a class=\"reference internal\" href=\"generated/torch.jit.script#torch.jit.script\" title=\"torch.jit.script\"><code>torch.jit.script()</code></a>, <code>torch.export</code> does not capture Python control flow or data structures, but it supports more Python language features than TorchScript (as it is easier to have comprehensive coverage over Python bytecodes). The resulting graphs are simpler and only have straight line control flow (except for explicit control flow operators).</p> <p>Compared to <a class=\"reference internal\" href=\"generated/torch.jit.trace#torch.jit.trace\" title=\"torch.jit.trace\"><code>torch.jit.trace()</code></a>, <code>torch.export</code> is sound: it is able to trace code that performs integer computation on sizes and records all of the side-conditions necessary to show that a particular trace is valid for other inputs.</p>    <h2 id=\"exporting-a-pytorch-model\">Exporting a PyTorch Model</h2>  <h3 id=\"an-example\">An Example</h3> <p>The main entrypoint is through <a class=\"reference internal\" href=\"#torch.export.export\" title=\"torch.export.export\"><code>torch.export.export()</code></a>, which takes a callable (<a class=\"reference internal\" href=\"generated/torch.nn.module#torch.nn.Module\" title=\"torch.nn.Module\"><code>torch.nn.Module</code></a>, function, or method) and sample inputs, and captures the computation graph into an <a class=\"reference internal\" href=\"#torch.export.ExportedProgram\" title=\"torch.export.ExportedProgram\"><code>torch.export.ExportedProgram</code></a>. An example:</p> <pre data-language=\"python\">import torch\nfrom torch.export import export\n\n# Simple module for demonstration\nclass M(torch.nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv = torch.nn.Conv2d(\n            in_channels=3, out_channels=16, kernel_size=3, padding=1\n        )\n        self.relu = torch.nn.ReLU()\n        self.maxpool = torch.nn.MaxPool2d(kernel_size=3)\n\n    def forward(self, x: torch.Tensor, *, constant=None) -&gt; torch.Tensor:\n        a = self.conv(x)\n        a.add_(constant)\n        return self.maxpool(self.relu(a))\n\nexample_args = (torch.randn(1, 3, 256, 256),)\nexample_kwargs = {\"constant\": torch.ones(1, 16, 256, 256)}\n\nexported_program: torch.export.ExportedProgram = export(\n    M(), args=example_args, kwargs=example_kwargs\n)\nprint(exported_program)\n</pre> <pre data-language=\"python\">ExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, arg0_1: f32[16, 3, 3, 3], arg1_1: f32[16], arg2_1: f32[1, 3, 256, 256], arg3_1: f32[1, 16, 256, 256]):\n\n            # code: a = self.conv(x)\n            convolution: f32[1, 16, 256, 256] = torch.ops.aten.convolution.default(\n                arg2_1, arg0_1, arg1_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1\n            );\n\n            # code: a.add_(constant)\n            add: f32[1, 16, 256, 256] = torch.ops.aten.add.Tensor(convolution, arg3_1);\n\n            # code: return self.maxpool(self.relu(a))\n            relu: f32[1, 16, 256, 256] = torch.ops.aten.relu.default(add);\n            max_pool2d_with_indices = torch.ops.aten.max_pool2d_with_indices.default(\n                relu, [3, 3], [3, 3]\n            );\n            getitem: f32[1, 16, 85, 85] = max_pool2d_with_indices[0];\n            return (getitem,)\n\n    Graph signature: ExportGraphSignature(\n        parameters=['L__self___conv.weight', 'L__self___conv.bias'],\n        buffers=[],\n        user_inputs=['arg2_1', 'arg3_1'],\n        user_outputs=['getitem'],\n        inputs_to_parameters={\n            'arg0_1': 'L__self___conv.weight',\n            'arg1_1': 'L__self___conv.bias',\n        },\n        inputs_to_buffers={},\n        buffers_to_mutate={},\n        backward_signature=None,\n        assertion_dep_token=None,\n    )\n    Range constraints: {}\n    Equality constraints: []\n</pre> <p>Inspecting the <code>ExportedProgram</code>, we can note the following:</p> <ul class=\"simple\"> <li>The <a class=\"reference internal\" href=\"fx#torch.fx.Graph\" title=\"torch.fx.Graph\"><code>torch.fx.Graph</code></a> contains the computation graph of the original program, along with records of the original code for easy debugging.</li> <li>The graph contains only <code>torch.ops.aten</code> operators found in the <a class=\"reference internal\" href=\"torch.compiler_ir#torch-compiler-ir\"><span class=\"std std-ref\">Core ATen IR</span></a> opset and custom operators, and is fully functional, without any inplace operators such as <code>torch.add_</code>.</li> <li>The parameters (weight and bias to conv) are lifted as inputs to the graph, resulting in no <code>get_attr</code> nodes in the graph, which previously existed in the result of <a class=\"reference internal\" href=\"fx#torch.fx.symbolic_trace\" title=\"torch.fx.symbolic_trace\"><code>torch.fx.symbolic_trace()</code></a>.</li> <li>The <a class=\"reference internal\" href=\"#torch.export.ExportGraphSignature\" title=\"torch.export.ExportGraphSignature\"><code>torch.export.ExportGraphSignature</code></a> models the input and output signature, along with specifying which inputs are parameters.</li> <li>The resulting shape and dtype of tensors produced by each node in the graph is noted. For example, the <code>convolution</code> node will result in a tensor of dtype <code>torch.float32</code> and shape (1, 16, 256, 256).</li> </ul>   <h3 id=\"expressing-dynamism\">Expressing Dynamism</h3> <p>By default <code>torch.export</code> will trace the program assuming all input shapes are <strong>static</strong>, and specializing the exported program to those dimensions. However, some dimensions, such as a batch dimension, can be dynamic and vary from run to run. Such dimensions must be marked dynamic using the <a class=\"reference internal\" href=\"#torch.export.dynamic_dim\" title=\"torch.export.dynamic_dim\"><code>torch.export.dynamic_dim()</code></a> API, and passed into <a class=\"reference internal\" href=\"#torch.export.export\" title=\"torch.export.export\"><code>torch.export.export()</code></a> through the <code>constraints</code> argument. An example:</p> <pre data-language=\"python\">import torch\nfrom torch.export import export, dynamic_dim\n\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.branch1 = torch.nn.Sequential(\n            torch.nn.Linear(64, 32), torch.nn.ReLU()\n        )\n        self.branch2 = torch.nn.Sequential(\n            torch.nn.Linear(128, 64), torch.nn.ReLU()\n        )\n        self.buffer = torch.ones(32)\n\n    def forward(self, x1, x2):\n        out1 = self.branch1(x1)\n        out2 = self.branch2(x2)\n        return (out1 + self.buffer, out2)\n\nexample_args = (torch.randn(32, 64), torch.randn(32, 128))\nconstraints = [\n    # First dimension of each input is a dynamic batch size\n    dynamic_dim(example_args[0], 0),\n    dynamic_dim(example_args[1], 0),\n    # The dynamic batch size between the inputs are equal\n    dynamic_dim(example_args[0], 0) == dynamic_dim(example_args[1], 0),\n]\n\nexported_program: torch.export.ExportedProgram = export(\n  M(), args=example_args, constraints=constraints\n)\nprint(exported_program)\n</pre> <pre data-language=\"python\">ExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, arg0_1: f32[32, 64], arg1_1: f32[32], arg2_1: f32[64, 128], arg3_1: f32[64], arg4_1: f32[32], arg5_1: f32[s0, 64], arg6_1: f32[s0, 128]):\n\n            # code: out1 = self.branch1(x1)\n            permute: f32[64, 32] = torch.ops.aten.permute.default(arg0_1, [1, 0]);\n            addmm: f32[s0, 32] = torch.ops.aten.addmm.default(arg1_1, arg5_1, permute);\n            relu: f32[s0, 32] = torch.ops.aten.relu.default(addmm);\n\n            # code: out2 = self.branch2(x2)\n            permute_1: f32[128, 64] = torch.ops.aten.permute.default(arg2_1, [1, 0]);\n            addmm_1: f32[s0, 64] = torch.ops.aten.addmm.default(arg3_1, arg6_1, permute_1);\n            relu_1: f32[s0, 64] = torch.ops.aten.relu.default(addmm_1);  addmm_1 = None\n\n            # code: return (out1 + self.buffer, out2)\n            add: f32[s0, 32] = torch.ops.aten.add.Tensor(relu, arg4_1);\n            return (add, relu_1)\n\n    Graph signature: ExportGraphSignature(\n        parameters=[\n            'branch1.0.weight',\n            'branch1.0.bias',\n            'branch2.0.weight',\n            'branch2.0.bias',\n        ],\n        buffers=['L__self___buffer'],\n        user_inputs=['arg5_1', 'arg6_1'],\n        user_outputs=['add', 'relu_1'],\n        inputs_to_parameters={\n            'arg0_1': 'branch1.0.weight',\n            'arg1_1': 'branch1.0.bias',\n            'arg2_1': 'branch2.0.weight',\n            'arg3_1': 'branch2.0.bias',\n        },\n        inputs_to_buffers={'arg4_1': 'L__self___buffer'},\n        buffers_to_mutate={},\n        backward_signature=None,\n        assertion_dep_token=None,\n    )\n    Range constraints: {s0: RangeConstraint(min_val=2, max_val=9223372036854775806)}\n    Equality constraints: [(InputDim(input_name='arg5_1', dim=0), InputDim(input_name='arg6_1', dim=0))]\n</pre> <p>Some additional things to note:</p> <ul class=\"simple\"> <li>Through the <a class=\"reference internal\" href=\"#torch.export.dynamic_dim\" title=\"torch.export.dynamic_dim\"><code>torch.export.dynamic_dim()</code></a> API, we specified the first dimension of each input to be dynamic. Looking at the inputs <code>arg5_1</code> and <code>arg6_1</code>, they have a symbolic shape of (s0, 64) and (s0, 128), instead of the (32, 64) and (32, 128) shaped tensors that we passed in as example inputs. <code>s0</code> is a symbol representing that this dimension can be a range of values.</li> <li>\n<code>exported_program.range_constraints</code> describes the ranges of each symbol appearing in the graph. In this case, we see that <code>s0</code> has the range [2, inf]. For technical reasons that are difficult to explain here, they are assumed to be not 0 or 1. This is not a bug, and does not necessarily mean that the exported program will not work for dimensions 0 or 1. See <a class=\"reference external\" href=\"https://docs.google.com/document/d/16VPOa3d-Liikf48teAOmxLc92rgvJdfosIy-yoT38Io/edit?fbclid=IwAR3HNwmmexcitV0pbZm_x1a4ykdXZ9th_eJWK-3hBtVgKnrkmemz6Pm5jRQ#heading=h.ez923tomjvyk\">The 0/1 Specialization Problem</a> for an in-depth discussion of this topic.</li> <li>\n<code>exported_program.equality_constraints</code> describes which dimensions are required to be equal. Since we specified in the constraints that the first dimension of each argument is equivalent, (<code>dynamic_dim(example_args[0], 0) == dynamic_dim(example_args[1], 0)</code>), we see in the equality constraints the tuple specifying that <code>arg5_1</code> dimension 0 and <code>arg6_1</code> dimension 0 are equal.</li> </ul>   <h3 id=\"serialization\">Serialization</h3> <p>To save the <code>ExportedProgram</code>, users can use the <a class=\"reference internal\" href=\"#torch.export.save\" title=\"torch.export.save\"><code>torch.export.save()</code></a> and <a class=\"reference internal\" href=\"#torch.export.load\" title=\"torch.export.load\"><code>torch.export.load()</code></a> APIs. A convention is to save the <code>ExportedProgram</code> using a <code>.pt2</code> file extension.</p> <p>An example:</p> <pre data-language=\"python\">import torch\nimport io\n\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return x + 10\n\nexported_program = torch.export.export(MyModule(), torch.randn(5))\n\ntorch.export.save(exported_program, 'exported_program.pt2')\nsaved_exported_program = torch.export.load('exported_program.pt2')\n</pre>   <h3 id=\"specialization\">Specialization</h3>  <h4 id=\"input-shapes\">Input shapes</h4> <p>As mentioned before, by default, <code>torch.export</code> will trace the program specializing on the input tensors shapes, unless a dimension is specified as dynamic via the <a class=\"reference internal\" href=\"#torch.export.dynamic_dim\" title=\"torch.export.dynamic_dim\"><code>torch.export.dynamic_dim()</code></a> API. This means that if there exists shape-dependent control flow, <code>torch.export</code> will specialize on the branch that is being taken with the given sample inputs. For example:</p> <pre data-language=\"python\">import torch\nfrom torch.export import export\n\ndef fn(x):\n    if x.shape[0] &gt; 5:\n        return x + 1\n    else:\n        return x - 1\n\nexample_inputs = (torch.rand(10, 2),)\nexported_program = export(fn, example_inputs)\nprint(exported_program)\n</pre> <pre data-language=\"python\">ExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, arg0_1: f32[10, 2]):\n            add: f32[10, 2] = torch.ops.aten.add.Tensor(arg0_1, 1);\n            return (add,)\n</pre> <p>The conditional of (<code>x.shape[0] &gt; 5</code>) does not appear in the <code>ExportedProgram</code> because the example inputs have the static shape of (10, 2). Since <code>torch.export</code> specializes on the inputs static shapes, the else branch (<code>x - 1</code>) will never be reached. To preserve the dynamic branching behavior based on the shape of a tensor in the traced graph, <a class=\"reference internal\" href=\"#torch.export.dynamic_dim\" title=\"torch.export.dynamic_dim\"><code>torch.export.dynamic_dim()</code></a> will need to be used to specify the dimension of the input tensor (<code>x.shape[0]</code>) to be dynamic, and the source code will need to be <a class=\"reference internal\" href=\"#data-shape-dependent-control-flow\"><span class=\"std std-ref\">rewritten</span></a>.</p>   <h4 id=\"non-tensor-inputs\">Non-tensor inputs</h4> <p><code>torch.export</code> also specializes the traced graph based on the values of inputs that are not <code>torch.Tensor</code>, such as <code>int</code>, <code>float</code>, <code>bool</code>, and <code>str</code>. However, we will likely change this in the near future to not specialize on inputs of primitive types.</p> <p>For example:</p> <pre data-language=\"python\">import torch\nfrom torch.export import export\n\ndef fn(x: torch.Tensor, const: int, times: int):\n    for i in range(times):\n        x = x + const\n    return x\n\nexample_inputs = (torch.rand(2, 2), 1, 3)\nexported_program = export(fn, example_inputs)\nprint(exported_program)\n</pre> <pre data-language=\"python\">ExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, arg0_1: f32[2, 2], arg1_1, arg2_1):\n            add: f32[2, 2] = torch.ops.aten.add.Tensor(arg0_1, 1);\n            add_1: f32[2, 2] = torch.ops.aten.add.Tensor(add, 1);\n            add_2: f32[2, 2] = torch.ops.aten.add.Tensor(add_1, 1);\n            return (add_2,)\n</pre> <p>Because integers are specialized, the <code>torch.ops.aten.add.Tensor</code> operations are all computed with the inlined constant <code>1</code>, rather than <code>arg1_1</code>. Additionally, the <code>times</code> iterator used in the <code>for</code> loop is also inlined in the graph through the 3 repeated <code>torch.ops.aten.add.Tensor</code> calls, and the input <code>arg2_1</code> is never used.</p>     <h2 id=\"limitations-of-torch-export\">Limitations of torch.export</h2>  <h3 id=\"graph-breaks\">Graph Breaks</h3> <p>As <code>torch.export</code> is a one-shot process for capturing a computation graph from a PyTorch program, it might ultimately run into untraceable parts of programs as it is nearly impossible to support tracing all PyTorch and Python features. In the case of <code>torch.compile</code>, an unsupported operation will cause a graph break and the unsupported operation will be run with default Python evaluation. In contrast, <code>torch.export</code> will require users to provide additional information or rewrite parts of their code to make it traceable. As the tracing is based on TorchDynamo, which evaluates at the Python bytecode level, there will be significantly fewer rewrites required compared to previous tracing frameworks.</p> <p>When a graph break is encountered, <a class=\"reference internal\" href=\"generated/exportdb/index#torch-export-db\"><span class=\"std std-ref\">ExportDB</span></a> is a great resource for learning about the kinds of programs that are supported and unsupported, along with ways to rewrite programs to make them traceable.</p>   <h3 id=\"id2\">Data/Shape-Dependent Control Flow</h3> <p id=\"data-shape-dependent-control-flow\">Graph breaks can also be encountered on data-dependent control flow (<code>if\nx.shape[0] &gt; 2</code>) when shapes are not being specialized, as a tracing compiler cannot possibly deal with without generating code for a combinatorially exploding number of paths. In such cases, users will need to rewrite their code using special control flow operators (coming soon!).</p>   <h3 id=\"data-dependent-accesses\">Data-Dependent Accesses</h3> <p>Data dependent behavior such as using the value inside of a tensor to construct another tensor, or using the value of a tensor to slice into another tensor, is also something the tracer cannot fully determine. Users will need to rewrite their code using the inline constraint APIs <a class=\"reference internal\" href=\"#torch.export.constrain_as_size\" title=\"torch.export.constrain_as_size\"><code>torch.export.constrain_as_size()</code></a> and <a class=\"reference internal\" href=\"#torch.export.constrain_as_value\" title=\"torch.export.constrain_as_value\"><code>torch.export.constrain_as_value()</code></a>.</p>   <h3 id=\"missing-meta-kernels-for-operators\">Missing Meta Kernels for Operators</h3> <p>When tracing, a META implementation (or meta kernel) is required for all operators. This is used to reason about the input/output shapes for this operator.</p> <p>Note that the official API for registering custom meta kernels for custom ops is currently undergoing development. While the final API is being refined, you can refer to the documentation <a class=\"reference external\" href=\"https://docs.google.com/document/d/1GgvOe7C8_NVOMLOCwDaYV1mXXyHMXY7ExoewHqooxrs/edit#heading=h.64r4npvq0w0\">here</a>.</p> <p>In the unfortunate case where your model uses an ATen operator that is does not have a meta kernel implementation yet, please file an issue.</p>    <h2 id=\"read-more\">Read More</h2>  <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Additional Links for Export Users</span></p> <ul> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"torch.compiler_transformations\">Writing Graph Transformations on ATen IR</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"torch.compiler_ir\">IRs</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"generated/exportdb/index\">ExportDB</a></li> </ul>   <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Deep Dive for PyTorch Developers</span></p> <ul> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"torch.compiler_deepdive\">TorchDynamo Deep Dive</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"torch.compiler_dynamic_shapes\">Dynamic shapes</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"torch.compiler_fake_tensor\">Fake tensor</a></li> </ul>    <h2 id=\"api-reference\">API Reference</h2> <dl class=\"py function\" id=\"module-torch.export\"> <dt class=\"sig sig-object py\" id=\"torch.export.export\">\n<code>torch.export.export(f, args, kwargs=None, *, constraints=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/export.html#export\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p><a class=\"reference internal\" href=\"#torch.export.export\" title=\"torch.export.export\"><code>export()</code></a> takes an arbitrary Python callable (an nn.Module, a function or a method) and produces a traced graph representing only the Tensor computation of the function in an Ahead-of-Time (AOT) fashion, which can subsequently be executed with different outputs or serialized. The traced graph (1) produces a normalized operator set consisting only of functional <a class=\"reference external\" href=\"https://pytorch.org/docs/stable/ir.html\">Core ATen Operator Set</a> and user specified custom operators, (2) has eliminated all Python control flow and data structures (except for certain conditions), and (3) has the set of shape constraints needed to show that this normalization and control flow elimination is sound for a future input.</p> <p><strong>Soundness Guarantee</strong></p> <p>While tracing, <a class=\"reference internal\" href=\"#torch.export.export\" title=\"torch.export.export\"><code>export()</code></a> takes note of shape-related assumptions made by the user program and the underlying PyTorch operator kernels. The output <a class=\"reference internal\" href=\"#torch.export.ExportedProgram\" title=\"torch.export.ExportedProgram\"><code>ExportedProgram</code></a> is considered valid only when these assumptions hold true.</p> <p>There are 2 types of assumptions made during tracing</p> <ul class=\"simple\"> <li>Shapes (not values) of input tensors.</li> <li>Ranges (lower and upper bound) of values extracted from intermediate tensors via <code>.item()</code> or direct indexing.</li> </ul> <p>All assumptions must be validated at graph capture time for <a class=\"reference internal\" href=\"#torch.export.export\" title=\"torch.export.export\"><code>export()</code></a> to succeed. Specifically:</p> <ul class=\"simple\"> <li>Assumptions on static shapes of input tensors are automatically validated without additional effort.</li> <li>Assumptions on dynamic shape of input tensors require explicit <code>Input Constraint</code> constructed with <a class=\"reference internal\" href=\"#torch.export.dynamic_dim\" title=\"torch.export.dynamic_dim\"><code>dynamic_dim()</code></a> APIs</li> <li>Assumptions on range of intermediate values require explicit <code>Inline Constraint</code>, constructed use <a class=\"reference internal\" href=\"#torch.export.constrain_as_size\" title=\"torch.export.constrain_as_size\"><code>constrain_as_size()</code></a> and <code>constraint_as_value()</code> APIs.</li> </ul> <p>If any assumption can not be validated, a fatal error will be raised. When that happens, the error message will include suggested code needed to construct necessary constraints to validate the assumptions, for example <a class=\"reference internal\" href=\"#torch.export.export\" title=\"torch.export.export\"><code>export()</code></a> would suggest following code for input constraints:</p> <pre data-language=\"python\">def specify_constraints(x):\n    return [\n        # x:\n        dynamic_dim(x, 0) &lt;= 5,\n    ]\n</pre> <p>This example means the program requires the dim 0 of input <code>x</code> to be less than or equal to 5 to be valid. You can inspect the constraints needed and then copy this exact function into your code to generated needed constraints to be passed into <code>constraints</code> argument.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>f</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Callable\" title=\"(in Python v3.12)\">Callable</a>)  The callable to trace.</li> <li>\n<strong>args</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Tuple\" title=\"(in Python v3.12)\">Tuple</a><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Any\" title=\"(in Python v3.12)\">Any</a><em>, </em><em>...</em><em>]</em>)  Example positional inputs.</li> <li>\n<strong>kwargs</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Optional\" title=\"(in Python v3.12)\">Optional</a><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Dict\" title=\"(in Python v3.12)\">Dict</a><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a><em>, </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Any\" title=\"(in Python v3.12)\">Any</a><em>]</em><em>]</em>)  Optional example keyword inputs.</li> <li>\n<strong>constraints</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Optional\" title=\"(in Python v3.12)\">Optional</a><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.List\" title=\"(in Python v3.12)\">List</a><em>[</em><a class=\"reference internal\" href=\"#torch.export.Constraint\" title=\"torch.export.Constraint\">Constraint</a><em>]</em><em>]</em>)  An optional list of constraints on the dynamic arguments that specify their possible range of shapes. By default, shapes of input torch.Tensors are assumed to be static. If an input torch.Tensor is expected to have dynamic shapes, please use <a class=\"reference internal\" href=\"#torch.export.dynamic_dim\" title=\"torch.export.dynamic_dim\"><code>dynamic_dim()</code></a> to define <a class=\"reference internal\" href=\"#torch.export.Constraint\" title=\"torch.export.Constraint\"><code>Constraint</code></a> objects that specify the dynamics and the possible range of shapes. See <a class=\"reference internal\" href=\"#torch.export.dynamic_dim\" title=\"torch.export.dynamic_dim\"><code>dynamic_dim()</code></a> docstring for examples on how to use it.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>An <a class=\"reference internal\" href=\"#torch.export.ExportedProgram\" title=\"torch.export.ExportedProgram\"><code>ExportedProgram</code></a> containing the traced callable.</p> </dd> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><a class=\"reference internal\" href=\"#torch.export.ExportedProgram\" title=\"torch.export.ExportedProgram\">ExportedProgram</a></p> </dd> </dl> <p><strong>Acceptable input/output types</strong></p> <p>Acceptable types of inputs (for <code>args</code> and <code>kwargs</code>) and outputs include:</p> <ul class=\"simple\"> <li>Primitive types, i.e. <code>torch.Tensor</code>, <code>int</code>, <code>float</code>, <code>bool</code> and <code>str</code>.</li> <li>(Nested) Data structures comprising of <code>dict</code>, <code>list</code>, <code>tuple</code>, <code>namedtuple</code> and <code>OrderedDict</code> containing all above types.</li> </ul> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.export.dynamic_dim\">\n<code>torch.export.dynamic_dim(t, index)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/export.html#dynamic_dim\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p><a class=\"reference internal\" href=\"#torch.export.dynamic_dim\" title=\"torch.export.dynamic_dim\"><code>dynamic_dim()</code></a> constructs a <a class=\"reference internal\" href=\"#torch.export.Constraint\" title=\"torch.export.Constraint\"><code>Constraint</code></a> object that describes the dynamism of a dimension <code>index</code> of tensor <code>t</code>. <a class=\"reference internal\" href=\"#torch.export.Constraint\" title=\"torch.export.Constraint\"><code>Constraint</code></a> objects should be passed to <code>constraints</code> argument of <a class=\"reference internal\" href=\"#torch.export.export\" title=\"torch.export.export\"><code>export()</code></a>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>t</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">torch.Tensor</a>)  Example input tensor that have dynamic dimension size(s)</li> <li>\n<strong>index</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a>)  Index of dynamic dimension</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>A <a class=\"reference internal\" href=\"#torch.export.Constraint\" title=\"torch.export.Constraint\"><code>Constraint</code></a> object that describes shape dynamism. It can be passed to <a class=\"reference internal\" href=\"#torch.export.export\" title=\"torch.export.export\"><code>export()</code></a> so that <a class=\"reference internal\" href=\"#torch.export.export\" title=\"torch.export.export\"><code>export()</code></a> does not assume static size of specified tensor, i.e. keeping it dynamic as a symbolic size rather than specializing according to size of example tracing input.</p> </dd> </dl> <p>Specifically <a class=\"reference internal\" href=\"#torch.export.dynamic_dim\" title=\"torch.export.dynamic_dim\"><code>dynamic_dim()</code></a> can be used to express following types of dynamism.</p> <ul> <li>\n<p>Size of a dimension is dynamic and unbounded:</p> <pre data-language=\"python\">t0 = torch.rand(2, 3)\nt1 = torch.rand(3, 4)\n\n# First dimension of t0 can be dynamic size rather than always being static size 2\nconstraints = [dynamic_dim(t0, 0)]\nep = export(fn, (t0, t1), constraints=constraints)\n</pre> </li> <li>\n<p>Size of a dimension is dynamic with a lower bound:</p> <pre data-language=\"python\">t0 = torch.rand(10, 3)\nt1 = torch.rand(3, 4)\n\n# First dimension of t0 can be dynamic size with a lower bound of 5 (inclusive)\n# Second dimension of t1 can be dynamic size with a lower bound of 2 (exclusive)\nconstraints = [\n    dynamic_dim(t0, 0) &gt;= 5,\n    dynamic_dim(t1, 1) &gt; 2,\n]\nep = export(fn, (t0, t1), constraints=constraints)\n</pre> </li> <li>\n<p>Size of a dimension is dynamic with an upper bound:</p> <pre data-language=\"python\">t0 = torch.rand(10, 3)\nt1 = torch.rand(3, 4)\n\n# First dimension of t0 can be dynamic size with a upper bound of 16 (inclusive)\n# Second dimension of t1 can be dynamic size with a upper bound of 8 (exclusive)\nconstraints = [\n    dynamic_dim(t0, 0) &lt;= 16,\n    dynamic_dim(t1, 1) &lt; 8,\n]\nep = export(fn, (t0, t1), constraints=constraints)\n</pre> </li> <li>\n<p>Size of a dimension is dynamic and it is always equal to size of another dynamic dimension:</p> <pre data-language=\"python\">t0 = torch.rand(10, 3)\nt1 = torch.rand(3, 4)\n\n# Sizes of second dimension of t0 and first dimension are always equal\nconstraints = [\n    dynamic_dim(t0, 1) == dynamic_dim(t1, 0),\n]\nep = export(fn, (t0, t1), constraints=constraints)\n</pre> </li> <li>Mix and match all types above as long as they do not express conflicting requirements</li> </ul> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.export.constrain_as_size\">\n<code>torch.export.constrain_as_size(symbol, min=None, max=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/export.html#constrain_as_size\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Hint <a class=\"reference internal\" href=\"#torch.export.export\" title=\"torch.export.export\"><code>export()</code></a> about the constraint of an intermediate scalar value that represents shape of a tensor so that subsequent tensor constructors can be traced correctly because many operators need to make assumption about range of sizes.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>symbol</strong>  Intermediate scalar value (int-only now) to apply range constraint on.</li> <li>\n<strong>min</strong> (<em>Optional</em><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>]</em>)  Minimum possible value of given symbol (inclusive)</li> <li>\n<strong>max</strong> (<em>Optional</em><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>]</em>)  Maximum possible value of given symbol (inclusive)</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>None</p> </dd> </dl> <p>For example, following program can not be traced soundly wihout using <a class=\"reference internal\" href=\"#torch.export.constrain_as_size\" title=\"torch.export.constrain_as_size\"><code>constrain_as_size()</code></a> to give <a class=\"reference internal\" href=\"#torch.export.export\" title=\"torch.export.export\"><code>export()</code></a> a hint about shape ranges:</p> <pre data-language=\"python\">def fn(x):\n    d = x.max().item()\n    return torch.ones(v)\n</pre> <p><a class=\"reference internal\" href=\"#torch.export.export\" title=\"torch.export.export\"><code>export()</code></a> would give following error:</p> <pre data-language=\"python\">torch._dynamo.exc.Unsupported: guard on data-dependent symbolic int/float\n</pre> <p>Assuming the actual range of <code>d</code> can be between [3, 10], you can add a call to <a class=\"reference internal\" href=\"#torch.export.constrain_as_size\" title=\"torch.export.constrain_as_size\"><code>constrain_as_size()</code></a> in the source code like this:</p> <pre data-language=\"python\">def fn(x):\n    d = x.max().item()\n    torch.export.constrain_as_size(d, min=3, max=10)\n    return torch.ones(d)\n</pre> <p>With the additional hint, <a class=\"reference internal\" href=\"#torch.export.export\" title=\"torch.export.export\"><code>export()</code></a> would be able to trace the program correctly by taking the <code>else</code> branch, resulting in following graph:</p> <pre data-language=\"python\">graph():\n    %arg0_1 := placeholder[target=arg0_1]\n\n    # d = x.max().item()\n    %max_1 := call_function[target=torch.ops.aten.max.default](args = (%arg0_1,))\n    %_local_scalar_dense := call_function[target=torch.ops.aten._local_scalar_dense.default](args = (%max_1,))\n\n    # Asserting 3 &lt;= d &lt;= 10\n    %ge := call_function[target=operator.ge](args = (%_local_scalar_dense, 3))\n    %scalar_tensor := call_function[target=torch.ops.aten.scalar_tensor.default](args = (%ge,))\n    %_assert_async := call_function[target=torch.ops.aten._assert_async.msg](\n        args = (%scalar_tensor, _local_scalar_dense is outside of inline constraint [3, 10].))\n    %le := call_function[target=operator.le](args = (%_local_scalar_dense, 10))\n    %scalar_tensor_1 := call_function[target=torch.ops.aten.scalar_tensor.default](args = (%le,))\n    %_assert_async_1 := call_function[target=torch.ops.aten._assert_async.msg](\n        args = (%scalar_tensor_1, _local_scalar_dense is outside of inline constraint [3, 10].))\n    %sym_constrain_range_for_size := call_function[target=torch.ops.aten.sym_constrain_range_for_size.default](\n        args = (%_local_scalar_dense,), kwargs = {min: 3, max: 10})\n\n    # Constructing new tensor with d\n    %full := call_function[target=torch.ops.aten.full.default](\n        args = ([%_local_scalar_dense], 1),\n        kwargs = {dtype: torch.float32, layout: torch.strided, device: cpu, pin_memory: False})\n\n    ......\n</pre> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>if your size is intended to be dynamic, do NOT test if sizes are equal to 0 or 1, these will SILENTLY report false and be bypassed</p> </div> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.export.constrain_as_value\">\n<code>torch.export.constrain_as_value(symbol, min=None, max=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/export.html#constrain_as_value\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Hint <a class=\"reference internal\" href=\"#torch.export.export\" title=\"torch.export.export\"><code>export()</code></a> about the constraint of an intermediate scalar value so that subsequent branching behaviors that check on the range of aforementioned scalar value can be soundly traced.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>(Note that if the intermediate scalar value will be used like a size, including being passed as size arg to a tensor factory or view, call <a class=\"reference internal\" href=\"#torch.export.constrain_as_size\" title=\"torch.export.constrain_as_size\"><code>constrain_as_size()</code></a> instead.)</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>symbol</strong>  Intermediate scalar value (int-only now) to apply range constraint on.</li> <li>\n<strong>min</strong> (<em>Optional</em><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>]</em>)  Minimum possible value of given symbol (inclusive)</li> <li>\n<strong>max</strong> (<em>Optional</em><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>]</em>)  Maximum possible value of given symbol (inclusive)</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>None</p> </dd> </dl> <p>For example, following program can not be traced soundly:</p> <pre data-language=\"python\">def fn(x):\n    v = x.max().item()\n    if v &gt; 1024:\n        return x\n    else:\n        return x * 2\n</pre> <p><code>v</code> is a data-dependent value, which is assumed to have a range of (-inf, inf). <a class=\"reference internal\" href=\"#torch.export.export\" title=\"torch.export.export\"><code>export()</code></a> a hint about which branch to take would not be able to determine if the traced branching decision is correct or not. Thus <a class=\"reference internal\" href=\"#torch.export.export\" title=\"torch.export.export\"><code>export()</code></a> would give following error:</p> <pre data-language=\"python\">torch._dynamo.exc.UserError: Consider annotating your code using\ntorch.export.constrain_as_size() or torch.export().constrain_as_value() APIs.\nIt appears that you're trying to get a value out of symbolic int/float whose value\nis data-dependent (and thus we do not know the true value.)  The expression we were\ntrying to evaluate is f0 &gt; 1024 (unhinted: f0 &gt; 1024).\n</pre> <p>Assuming the actual range of <code>v</code> can be between [10, 200], you can add a call to <a class=\"reference internal\" href=\"#torch.export.constrain_as_value\" title=\"torch.export.constrain_as_value\"><code>constrain_as_value()</code></a> in the source code like this:</p> <pre data-language=\"python\">def fn(x):\n    v = x.max().item()\n\n    # Give export() a hint\n    torch.export.constrain_as_value(v, min=10, max=200)\n\n    if v &gt; 1024:\n        return x\n    else:\n        return x * 2\n</pre> <p>With the additional hint, <a class=\"reference internal\" href=\"#torch.export.export\" title=\"torch.export.export\"><code>export()</code></a> would be able to trace the program correctly by taking the <code>else</code> branch, resulting in following graph:</p> <pre data-language=\"python\">graph():\n    %arg0_1 := placeholder[target=arg0_1]\n\n    # v = x.max().item()\n    %max_1 := call_function[target=torch.ops.aten.max.default](args = (%arg0_1,))\n    %_local_scalar_dense := call_function[target=torch.ops.aten._local_scalar_dense.default](args = (%max_1,))\n\n    # Asserting 10 &lt;= v &lt;= 200\n    %ge := call_function[target=operator.ge](args = (%_local_scalar_dense, 10))\n    %scalar_tensor := call_function[target=torch.ops.aten.scalar_tensor.default](args = (%ge,))\n    %_assert_async := call_function[target=torch.ops.aten._assert_async.msg](\n        args = (%scalar_tensor, _local_scalar_dense is outside of inline constraint [10, 200].))\n    %le := call_function[target=operator.le](args = (%_local_scalar_dense, 200))\n    %scalar_tensor_1 := call_function[target=torch.ops.aten.scalar_tensor.default](args = (%le,))\n    %_assert_async_1 := call_function[target=torch.ops.aten._assert_async.msg](\n        args = (%scalar_tensor_1, _local_scalar_dense is outside of inline constraint [10, 200].))\n    %sym_constrain_range := call_function[target=torch.ops.aten.sym_constrain_range.default](\n        args = (%_local_scalar_dense,), kwargs = {min: 10, max: 200})\n\n    # Always taking `else` branch to multiply elements `x` by 2 due to hints above\n    %mul := call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, 2), kwargs = {})\n    return (mul,)\n</pre> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.export.save\">\n<code>torch.export.save(ep, f, *, extra_files=None, opset_version=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/export.html#save\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Under active development, saved files may not be usable in newer versions of PyTorch.</p> </div> <p>Saves an <a class=\"reference internal\" href=\"#torch.export.ExportedProgram\" title=\"torch.export.ExportedProgram\"><code>ExportedProgram</code></a> to a file-like object. It can then be loaded using the Python API <a class=\"reference internal\" href=\"#torch.export.load\" title=\"torch.export.load\"><code>torch.export.load</code></a>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>ep</strong> (<a class=\"reference internal\" href=\"#torch.export.ExportedProgram\" title=\"torch.export.ExportedProgram\">ExportedProgram</a>)  The exported program to save.</li> <li>\n<strong>f</strong> (<em>Union</em><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a><em>, </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/pathlib.html#pathlib.Path\" title=\"(in Python v3.12)\">pathlib.Path</a><em>, </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/io.html#io.BytesIO\" title=\"(in Python v3.12)\">io.BytesIO</a>)  A file-like object (has to implement write and flush) or a string containing a file name.</li> <li>\n<strong>extra_files</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a><em>, </em><em>Any</em><em>]</em><em>]</em>)  Map from filename to contents which will be stored as part of f.</li> <li>\n<strong>opset_version</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a><em>, </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>]</em><em>]</em>)  A map of opset names to the version of this opset</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">import torch\nimport io\n\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return x + 10\n\nep = torch.export.export(MyModule(), torch.randn(5))\n\n# Save to file\ntorch.export.save(ep, 'exported_program.pt2')\n\n# Save to io.BytesIO buffer\nbuffer = io.BytesIO()\ntorch.export.save(ep, buffer)\n\n# Save with extra files\nextra_files = {'foo.txt': b'bar'}\ntorch.export.save(ep, 'exported_program.pt2', extra_files=extra_files)\n</pre> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.export.load\">\n<code>torch.export.load(f, *, extra_files=None, expected_opset_version=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/export.html#load\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Under active development, saved files may not be usable in newer versions of PyTorch.</p> </div> <p>Loads an <a class=\"reference internal\" href=\"#torch.export.ExportedProgram\" title=\"torch.export.ExportedProgram\"><code>ExportedProgram</code></a> previously saved with <a class=\"reference internal\" href=\"#torch.export.save\" title=\"torch.export.save\"><code>torch.export.save</code></a>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>ep</strong> (<a class=\"reference internal\" href=\"#torch.export.ExportedProgram\" title=\"torch.export.ExportedProgram\">ExportedProgram</a>)  The exported program to save.</li> <li>\n<strong>f</strong> (<em>Union</em><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a><em>, </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/pathlib.html#pathlib.Path\" title=\"(in Python v3.12)\">pathlib.Path</a><em>, </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/io.html#io.BytesIO\" title=\"(in Python v3.12)\">io.BytesIO</a>)  A file-like object (has to implement write and flush) or a string containing a file name.</li> <li>\n<strong>extra_files</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a><em>, </em><em>Any</em><em>]</em><em>]</em>)  The extra filenames given in this map would be loaded and their content would be stored in the provided map.</li> <li>\n<strong>expected_opset_version</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a><em>, </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>]</em><em>]</em>)  A map of opset names to expected opset versions</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>An <a class=\"reference internal\" href=\"#torch.export.ExportedProgram\" title=\"torch.export.ExportedProgram\"><code>ExportedProgram</code></a> object</p> </dd> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><a class=\"reference internal\" href=\"#torch.export.ExportedProgram\" title=\"torch.export.ExportedProgram\">ExportedProgram</a></p> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">import torch\nimport io\n\n# Load ExportedProgram from file\nep = torch.export.load('exported_program.pt2')\n\n# Load ExportedProgram from io.BytesIO object\nwith open('exported_program.pt2', 'rb') as f:\n    buffer = io.BytesIO(f.read())\nbuffer.seek(0)\nep = torch.export.load(buffer)\n\n# Load with extra files.\nextra_files = {'foo.txt': ''}  # values will be replaced with data\nep = torch.export.load('exported_program.pt2', extra_files=extra_files)\nprint(extra_files['foo.txt'])\n</pre> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.export.Constraint\">\n<code>class torch.export.Constraint(*args, **kwargs)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/export.html#Constraint\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Do not construct <a class=\"reference internal\" href=\"#torch.export.Constraint\" title=\"torch.export.Constraint\"><code>Constraint</code></a> directly, use <a class=\"reference internal\" href=\"#torch.export.dynamic_dim\" title=\"torch.export.dynamic_dim\"><code>dynamic_dim()</code></a> instead.</p> </div> <p>This represents constraints on input tensor dimensions, e.g., requiring them to be fully polymorphic or within some range.</p> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.export.ExportedProgram\">\n<code>class torch.export.ExportedProgram(root, graph, graph_signature, call_spec, state_dict, range_constraints, equality_constraints, module_call_graph, example_inputs=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/export.html#ExportedProgram\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Package of a program from <a class=\"reference internal\" href=\"#torch.export.export\" title=\"torch.export.export\"><code>export()</code></a>. It contains an <a class=\"reference internal\" href=\"fx#torch.fx.Graph\" title=\"torch.fx.Graph\"><code>torch.fx.Graph</code></a> that represents Tensor computation, a state_dict containing tensor values of all lifted parameters and buffers, and various metadata.</p> <p>You can call an ExportedProgram like the original callable traced by <a class=\"reference internal\" href=\"#torch.export.export\" title=\"torch.export.export\"><code>export()</code></a> with the same calling convention.</p> <p>To perform transformations on the graph, use <code>.module</code> property to access an <a class=\"reference internal\" href=\"fx#torch.fx.GraphModule\" title=\"torch.fx.GraphModule\"><code>torch.fx.GraphModule</code></a>. You can then use <a class=\"reference external\" href=\"https://pytorch.org/docs/stable/fx.html#writing-transformations\">FX transformation</a> to rewrite the graph. Afterwards, you can simply use <a class=\"reference internal\" href=\"#torch.export.export\" title=\"torch.export.export\"><code>export()</code></a> again to construct a correct ExportedProgram.</p>  <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.export.ExportedProgram.module\">\n<code>module()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/export.html#ExportedProgram.module\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns a self contained GraphModule with all the parameters/buffers inlined.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.module#torch.nn.Module\" title=\"torch.nn.modules.module.Module\">Module</a> </dd> </dl> </dd>\n</dl> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.export.ExportBackwardSignature\">\n<code>class torch.export.ExportBackwardSignature(gradients_to_parameters: Dict[str, str], gradients_to_user_inputs: Dict[str, str], loss_output: str)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/export.html#ExportBackwardSignature\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> \n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.export.ExportGraphSignature\">\n<code>class torch.export.ExportGraphSignature(parameters, buffers, user_inputs, user_outputs, inputs_to_parameters, inputs_to_buffers, buffers_to_mutate, backward_signature, assertion_dep_token=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/export.html#ExportGraphSignature\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p><a class=\"reference internal\" href=\"#torch.export.ExportGraphSignature\" title=\"torch.export.ExportGraphSignature\"><code>ExportGraphSignature</code></a> models the input/output signature of Export Graph, which is a fx.Graph with stronger invariants gurantees.</p> <p>Export Graph is functional and does not access states like parameters or buffers within the graph via <code>getattr</code> nodes. Instead, <a class=\"reference internal\" href=\"#torch.export.export\" title=\"torch.export.export\"><code>export()</code></a> gurantees that parameters and buffers are lifted out of the graph as inputs. Similarly, any mutations to buffers are not included in the graph either, instead the updated values of mutated buffers are modeled as additional outputs of Export Graph.</p> <p>The ordering of all inputs and outputs are:</p> <pre data-language=\"python\">Inputs = [*parameters_buffers, *flattened_user_inputs]\nOutputs = [*mutated_inputs, *flattened_user_outputs]\n</pre> <p>e.g. If following module is exported:</p> <pre data-language=\"python\">class CustomModule(nn.Module):\n    def __init__(self):\n        super(CustomModule, self).__init__()\n\n        # Define a parameter\n        self.my_parameter = nn.Parameter(torch.tensor(2.0))\n\n        # Define two buffers\n        self.register_buffer('my_buffer1', torch.tensor(3.0))\n        self.register_buffer('my_buffer2', torch.tensor(4.0))\n\n    def forward(self, x1, x2):\n        # Use the parameter, buffers, and both inputs in the forward method\n        output = (x1 + self.my_parameter) * self.my_buffer1 + x2 * self.my_buffer2\n\n        # Mutate one of the buffers (e.g., increment it by 1)\n        self.my_buffer2.add_(1.0) # In-place addition\n\n        return output\n</pre> <p>Resulting Graph would be:</p> <pre data-language=\"python\">graph():\n    %arg0_1 := placeholder[target=arg0_1]\n    %arg1_1 := placeholder[target=arg1_1]\n    %arg2_1 := placeholder[target=arg2_1]\n    %arg3_1 := placeholder[target=arg3_1]\n    %arg4_1 := placeholder[target=arg4_1]\n    %add_tensor := call_function[target=torch.ops.aten.add.Tensor](args = (%arg3_1, %arg0_1), kwargs = {})\n    %mul_tensor := call_function[target=torch.ops.aten.mul.Tensor](args = (%add_tensor, %arg1_1), kwargs = {})\n    %mul_tensor_1 := call_function[target=torch.ops.aten.mul.Tensor](args = (%arg4_1, %arg2_1), kwargs = {})\n    %add_tensor_1 := call_function[target=torch.ops.aten.add.Tensor](args = (%mul_tensor, %mul_tensor_1), kwargs = {})\n    %add_tensor_2 := call_function[target=torch.ops.aten.add.Tensor](args = (%arg2_1, 1.0), kwargs = {})\n    return (add_tensor_2, add_tensor_1)\n</pre> <p>Resulting ExportGraphSignature would be:</p> <pre data-language=\"python\">ExportGraphSignature(\n    # Indicates that there is one parameter named `my_parameter`\n    parameters=['L__self___my_parameter'],\n\n    # Indicates that there are two buffers, `my_buffer1` and `my_buffer2`\n    buffers=['L__self___my_buffer1', 'L__self___my_buffer2'],\n\n    # Indicates that the nodes `arg3_1` and `arg4_1` in produced graph map to\n    # original user inputs, ie. x1 and x2\n    user_inputs=['arg3_1', 'arg4_1'],\n\n    # Indicates that the node `add_tensor_1` maps to output of original program\n    user_outputs=['add_tensor_1'],\n\n    # Indicates that there is one parameter (self.my_parameter) captured,\n    # its name is now mangled to be `L__self___my_parameter`, which is now\n    # represented by node `arg0_1` in the graph.\n    inputs_to_parameters={'arg0_1': 'L__self___my_parameter'},\n\n    # Indicates that there are two buffers (self.my_buffer1, self.my_buffer2) captured,\n    # their name are now mangled to be `L__self___my_my_buffer1` and `L__self___my_buffer2`.\n    # They are now represented by nodes `arg1_1` and `arg2_1` in the graph.\n    inputs_to_buffers={'arg1_1': 'L__self___my_buffer1', 'arg2_1': 'L__self___my_buffer2'},\n\n    # Indicates that one buffer named `L__self___my_buffer2` is mutated during execution,\n    # its new value is output from the graph represented by the node named `add_tensor_2`\n    buffers_to_mutate={'add_tensor_2': 'L__self___my_buffer2'},\n\n    # Backward graph not captured\n    backward_signature=None,\n\n    # Work in progress feature, please ignore now.\n    assertion_dep_token=None\n)\n</pre>  </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.export.ArgumentKind\">\n<code>class torch.export.ArgumentKind(value)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/export.html#ArgumentKind\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>An enumeration.</p> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.export.ArgumentSpec\">\n<code>class torch.export.ArgumentSpec(kind: torch.export.ArgumentKind, value: Any)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/export.html#ArgumentSpec\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> \n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.export.ModuleCallSignature\">\n<code>class torch.export.ModuleCallSignature(inputs: List[torch.export.ArgumentSpec], outputs: List[torch.export.ArgumentSpec], in_spec: torch.utils._pytree.TreeSpec, out_spec: torch.utils._pytree.TreeSpec)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/export.html#ModuleCallSignature\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> \n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.export.ModuleCallEntry\">\n<code>class torch.export.ModuleCallEntry(fqn: str, signature: Union[torch.export.ModuleCallSignature, NoneType] = None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/export.html#ModuleCallEntry\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> \n</dl><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href=\"https://github.com/pytorch/pytorch/blob/main/LICENSE\">LICENSE</a> file.<br>\n    <a href=\"https://pytorch.org/docs/2.1/export.html\" class=\"_attribution-link\">https://pytorch.org/docs/2.1/export.html</a>\n  </p>\n</div>\n","distributed":"<h1 id=\"distributed-communication-package-torch-distributed\">Distributed communication package - torch.distributed</h1> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Please refer to <a class=\"reference external\" href=\"https://pytorch.org/tutorials/beginner/dist_overview.html\">PyTorch Distributed Overview</a> for a brief introduction to all features related to distributed training.</p> </div>  <h2 id=\"backends\">Backends</h2> <p><code>torch.distributed</code> supports three built-in backends, each with different capabilities. The table below shows which functions are available for use with CPU / CUDA tensors. MPI supports CUDA only if the implementation used to build PyTorch supports it.</p> <table class=\"docutils colwidths-auto align-default\"> <thead> <tr>\n<th class=\"head\"><p>Backend</p></th> <th class=\"head\" colspan=\"2\"><p><code>gloo</code></p></th> <th class=\"head\" colspan=\"2\"><p><code>mpi</code></p></th> <th class=\"head\" colspan=\"2\"><p><code>nccl</code></p></th> </tr> <tr>\n<th class=\"head\"><p>Device</p></th> <th class=\"head\"><p>CPU</p></th> <th class=\"head\"><p>GPU</p></th> <th class=\"head\"><p>CPU</p></th> <th class=\"head\"><p>GPU</p></th> <th class=\"head\"><p>CPU</p></th> <th class=\"head\"><p>GPU</p></th> </tr> </thead>  <tr>\n<td><p>send</p></td> <td><p></p></td> <td><p></p></td> <td><p></p></td> <td><p>?</p></td> <td><p></p></td> <td><p></p></td> </tr> <tr>\n<td><p>recv</p></td> <td><p></p></td> <td><p></p></td> <td><p></p></td> <td><p>?</p></td> <td><p></p></td> <td><p></p></td> </tr> <tr>\n<td><p>broadcast</p></td> <td><p></p></td> <td><p></p></td> <td><p></p></td> <td><p>?</p></td> <td><p></p></td> <td><p></p></td> </tr> <tr>\n<td><p>all_reduce</p></td> <td><p></p></td> <td><p></p></td> <td><p></p></td> <td><p>?</p></td> <td><p></p></td> <td><p></p></td> </tr> <tr>\n<td><p>reduce</p></td> <td><p></p></td> <td><p></p></td> <td><p></p></td> <td><p>?</p></td> <td><p></p></td> <td><p></p></td> </tr> <tr>\n<td><p>all_gather</p></td> <td><p></p></td> <td><p></p></td> <td><p></p></td> <td><p>?</p></td> <td><p></p></td> <td><p></p></td> </tr> <tr>\n<td><p>gather</p></td> <td><p></p></td> <td><p></p></td> <td><p></p></td> <td><p>?</p></td> <td><p></p></td> <td><p></p></td> </tr> <tr>\n<td><p>scatter</p></td> <td><p></p></td> <td><p></p></td> <td><p></p></td> <td><p>?</p></td> <td><p></p></td> <td><p></p></td> </tr> <tr>\n<td><p>reduce_scatter</p></td> <td><p></p></td> <td><p></p></td> <td><p></p></td> <td><p></p></td> <td><p></p></td> <td><p></p></td> </tr> <tr>\n<td><p>all_to_all</p></td> <td><p></p></td> <td><p></p></td> <td><p></p></td> <td><p>?</p></td> <td><p></p></td> <td><p></p></td> </tr> <tr>\n<td><p>barrier</p></td> <td><p></p></td> <td><p></p></td> <td><p></p></td> <td><p>?</p></td> <td><p></p></td> <td><p></p></td> </tr>  </table>  <h3 id=\"backends-that-come-with-pytorch\">Backends that come with PyTorch</h3> <p>PyTorch distributed package supports Linux (stable), MacOS (stable), and Windows (prototype). By default for Linux, the Gloo and NCCL backends are built and included in PyTorch distributed (NCCL only when building with CUDA). MPI is an optional backend that can only be included if you build PyTorch from source. (e.g. building PyTorch on a host that has MPI installed.)</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>As of PyTorch v1.8, Windows supports all collective communications backend but NCCL, If the <code>init_method</code> argument of <a class=\"reference internal\" href=\"#torch.distributed.init_process_group\" title=\"torch.distributed.init_process_group\"><code>init_process_group()</code></a> points to a file it must adhere to the following schema:</p> <ul class=\"simple\"> <li>Local file system, <code>init_method=\"file:///d:/tmp/some_file\"</code>\n</li> <li>Shared file system, <code>init_method=\"file://////{machine_name}/{share_folder_name}/some_file\"</code>\n</li> </ul> <p>Same as on Linux platform, you can enable TcpStore by setting environment variables, MASTER_ADDR and MASTER_PORT.</p> </div>   <h3 id=\"which-backend-to-use\">Which backend to use?</h3> <p>In the past, we were often asked: which backend should I use?.</p> <ul class=\"simple\"> <li>\n<p>Rule of thumb</p> <ul> <li>Use the NCCL backend for distributed <strong>GPU</strong> training</li> <li>Use the Gloo backend for distributed <strong>CPU</strong> training.</li> </ul> </li> <li>\n<p>GPU hosts with InfiniBand interconnect</p> <ul> <li>Use NCCL, since its the only backend that currently supports InfiniBand and GPUDirect.</li> </ul> </li> <li>\n<p>GPU hosts with Ethernet interconnect</p> <ul> <li>Use NCCL, since it currently provides the best distributed GPU training performance, especially for multiprocess single-node or multi-node distributed training. If you encounter any problem with NCCL, use Gloo as the fallback option. (Note that Gloo currently runs slower than NCCL for GPUs.)</li> </ul> </li> <li>\n<p>CPU hosts with InfiniBand interconnect</p> <ul> <li>If your InfiniBand has enabled IP over IB, use Gloo, otherwise, use MPI instead. We are planning on adding InfiniBand support for Gloo in the upcoming releases.</li> </ul> </li> <li>\n<p>CPU hosts with Ethernet interconnect</p> <ul> <li>Use Gloo, unless you have specific reasons to use MPI.</li> </ul> </li> </ul>   <h3 id=\"common-environment-variables\">Common environment variables</h3>  <h4 id=\"choosing-the-network-interface-to-use\">Choosing the network interface to use</h4> <p>By default, both the NCCL and Gloo backends will try to find the right network interface to use. If the automatically detected interface is not correct, you can override it using the following environment variables (applicable to the respective backend):</p> <ul class=\"simple\"> <li>\n<strong>NCCL_SOCKET_IFNAME</strong>, for example <code>export NCCL_SOCKET_IFNAME=eth0</code>\n</li> <li>\n<strong>GLOO_SOCKET_IFNAME</strong>, for example <code>export GLOO_SOCKET_IFNAME=eth0</code>\n</li> </ul> <p>If youre using the Gloo backend, you can specify multiple interfaces by separating them by a comma, like this: <code>export GLOO_SOCKET_IFNAME=eth0,eth1,eth2,eth3</code>. The backend will dispatch operations in a round-robin fashion across these interfaces. It is imperative that all processes specify the same number of interfaces in this variable.</p>   <h4 id=\"other-nccl-environment-variables\">Other NCCL environment variables</h4> <p><strong>Debugging</strong> - in case of NCCL failure, you can set <code>NCCL_DEBUG=INFO</code> to print an explicit warning message as well as basic NCCL initialization information.</p> <p>You may also use <code>NCCL_DEBUG_SUBSYS</code> to get more details about a specific aspect of NCCL. For example, <code>NCCL_DEBUG_SUBSYS=COLL</code> would print logs of collective calls, which may be helpful when debugging hangs, especially those caused by collective type or message size mismatch. In case of topology detection failure, it would be helpful to set <code>NCCL_DEBUG_SUBSYS=GRAPH</code> to inspect the detailed detection result and save as reference if further help from NCCL team is needed.</p> <p><strong>Performance tuning</strong> - NCCL performs automatic tuning based on its topology detection to save users tuning effort. On some socket-based systems, users may still try tuning <code>NCCL_SOCKET_NTHREADS</code> and <code>NCCL_NSOCKS_PERTHREAD</code> to increase socket network bandwidth. These two environment variables have been pre-tuned by NCCL for some cloud providers, such as AWS or GCP.</p> <p>For a full list of NCCL environment variables, please refer to <a class=\"reference external\" href=\"https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/env.html\">NVIDIA NCCLs official documentation</a></p>     <h2 id=\"distributed-basics\">Basics</h2> <p id=\"basics\">The <code>torch.distributed</code> package provides PyTorch support and communication primitives for multiprocess parallelism across several computation nodes running on one or more machines. The class <a class=\"reference internal\" href=\"generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel\" title=\"torch.nn.parallel.DistributedDataParallel\"><code>torch.nn.parallel.DistributedDataParallel()</code></a> builds on this functionality to provide synchronous distributed training as a wrapper around any PyTorch model. This differs from the kinds of parallelism provided by <a class=\"reference internal\" href=\"multiprocessing\"><span class=\"doc\">Multiprocessing package - torch.multiprocessing</span></a> and <a class=\"reference internal\" href=\"generated/torch.nn.dataparallel#torch.nn.DataParallel\" title=\"torch.nn.DataParallel\"><code>torch.nn.DataParallel()</code></a> in that it supports multiple network-connected machines and in that the user must explicitly launch a separate copy of the main training script for each process.</p> <p>In the single-machine synchronous case, <code>torch.distributed</code> or the <a class=\"reference internal\" href=\"generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel\" title=\"torch.nn.parallel.DistributedDataParallel\"><code>torch.nn.parallel.DistributedDataParallel()</code></a> wrapper may still have advantages over other approaches to data-parallelism, including <a class=\"reference internal\" href=\"generated/torch.nn.dataparallel#torch.nn.DataParallel\" title=\"torch.nn.DataParallel\"><code>torch.nn.DataParallel()</code></a>:</p> <ul class=\"simple\"> <li>Each process maintains its own optimizer and performs a complete optimization step with each iteration. While this may appear redundant, since the gradients have already been gathered together and averaged across processes and are thus the same for every process, this means that no parameter broadcast step is needed, reducing time spent transferring tensors between nodes.</li> <li>Each process contains an independent Python interpreter, eliminating the extra interpreter overhead and GIL-thrashing that comes from driving several execution threads, model replicas, or GPUs from a single Python process. This is especially important for models that make heavy use of the Python runtime, including models with recurrent layers or many small components.</li> </ul>   <h2 id=\"initialization\">Initialization</h2> <p>The package needs to be initialized using the <a class=\"reference internal\" href=\"#torch.distributed.init_process_group\" title=\"torch.distributed.init_process_group\"><code>torch.distributed.init_process_group()</code></a> function before calling any other methods. This blocks until all processes have joined.</p> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.is_available\">\n<code>torch.distributed.is_available()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed.html#is_available\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns <code>True</code> if the distributed package is available. Otherwise, <code>torch.distributed</code> does not expose any other APIs. Currently, <code>torch.distributed</code> is available on Linux, MacOS and Windows. Set <code>USE_DISTRIBUTED=1</code> to enable it when building PyTorch from source. Currently, the default value is <code>USE_DISTRIBUTED=1</code> for Linux and Windows, <code>USE_DISTRIBUTED=0</code> for MacOS.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a></p> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.init_process_group\">\n<code>torch.distributed.init_process_group(backend=None, init_method=None, timeout=datetime.timedelta(seconds=1800), world_size=-1, rank=-1, store=None, group_name='', pg_options=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#init_process_group\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Initializes the default distributed process group, and this will also initialize the distributed package.</p> <dl class=\"simple\"> <dt>There are 2 main ways to initialize a process group:</dt>\n<dd>\n<ol class=\"arabic simple\"> <li>Specify <code>store</code>, <code>rank</code>, and <code>world_size</code> explicitly.</li> <li>Specify <code>init_method</code> (a URL string) which indicates where/how to discover peers. Optionally specify <code>rank</code> and <code>world_size</code>, or encode all required parameters in the URL and omit them.</li> </ol> </dd> </dl> <p>If neither is specified, <code>init_method</code> is assumed to be env://.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>backend</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a><em> or </em><a class=\"reference internal\" href=\"#torch.distributed.Backend\" title=\"torch.distributed.Backend\">Backend</a><em>, </em><em>optional</em>)  The backend to use. Depending on build-time configurations, valid values include <code>mpi</code>, <code>gloo</code>, <code>nccl</code>, and <code>ucc</code>. If the backend is not provided, then both a <code>gloo</code> and <code>nccl</code> backend will be created, see notes below for how multiple backends are managed. This field can be given as a lowercase string (e.g., <code>\"gloo\"</code>), which can also be accessed via <a class=\"reference internal\" href=\"#torch.distributed.Backend\" title=\"torch.distributed.Backend\"><code>Backend</code></a> attributes (e.g., <code>Backend.GLOO</code>). If using multiple processes per machine with <code>nccl</code> backend, each process must have exclusive access to every GPU it uses, as sharing GPUs between processes can result in deadlocks. <code>ucc</code> backend is experimental.</li> <li>\n<strong>init_method</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a><em>, </em><em>optional</em>)  URL specifying how to initialize the process group. Default is env:// if no <code>init_method</code> or <code>store</code> is specified. Mutually exclusive with <code>store</code>.</li> <li>\n<strong>world_size</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>, </em><em>optional</em>)  Number of processes participating in the job. Required if <code>store</code> is specified.</li> <li>\n<strong>rank</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>, </em><em>optional</em>)  Rank of the current process (it should be a number between 0 and <code>world_size</code>-1). Required if <code>store</code> is specified.</li> <li>\n<strong>store</strong> (<a class=\"reference internal\" href=\"#torch.distributed.Store\" title=\"torch.distributed.Store\">Store</a><em>, </em><em>optional</em>)  Key/value store accessible to all workers, used to exchange connection/address information. Mutually exclusive with <code>init_method</code>.</li> <li>\n<strong>timeout</strong> (<em>timedelta</em><em>, </em><em>optional</em>)  Timeout for operations executed against the process group. Default value equals 30 minutes. This is applicable for the <code>gloo</code> backend. For <code>nccl</code>, this is applicable only if the environment variable <code>NCCL_BLOCKING_WAIT</code> or <code>NCCL_ASYNC_ERROR_HANDLING</code> is set to 1. When <code>NCCL_BLOCKING_WAIT</code> is set, this is the duration for which the process will block and wait for collectives to complete before throwing an exception. When <code>NCCL_ASYNC_ERROR_HANDLING</code> is set, this is the duration after which collectives will be aborted asynchronously and the process will crash. <code>NCCL_BLOCKING_WAIT</code> will provide errors to the user which can be caught and handled, but due to its blocking nature, it has a performance overhead. On the other hand, <code>NCCL_ASYNC_ERROR_HANDLING</code> has very little performance overhead, but crashes the process on errors. This is done since CUDA execution is async and it is no longer safe to continue executing user code since failed async NCCL operations might result in subsequent CUDA operations running on corrupted data. Only one of these two environment variables should be set. For <code>ucc</code>, blocking wait is supported similar to NCCL. However, async error handling is done differently since with UCC we have progress thread and not watch-dog thread.</li> <li>\n<strong>group_name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a><em>, </em><em>optional</em><em>, </em><em>deprecated</em>)  Group name. This argument is ignored</li> <li>\n<strong>pg_options</strong> (<em>ProcessGroupOptions</em><em>, </em><em>optional</em>)  process group options specifying what additional options need to be passed in during the construction of specific process groups. As of now, the only options we support is <code>ProcessGroupNCCL.Options</code> for the <code>nccl</code> backend, <code>is_high_priority_stream</code> can be specified so that the nccl backend can pick up high priority cuda streams when therere compute kernels waiting.</li> </ul> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>To enable <code>backend == Backend.MPI</code>, PyTorch needs to be built from source on a system that supports MPI.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Support for multiple backends is experimental. Currently when no backend is specified, both <code>gloo</code> and <code>nccl</code> backends will be created. The <code>gloo</code> backend will be used for collectives with CPU tensors and the <code>nccl</code> backend will be used for collectives with CUDA tensors. A custom backend can be specified by passing in a string with format &lt;device_type&gt;:&lt;backend_name&gt;,&lt;device_type&gt;:&lt;backend_name&gt;, e.g. cpu:gloo,cuda:custom_backend.</p> </div> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.is_initialized\">\n<code>torch.distributed.is_initialized()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#is_initialized\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Checking if the default process group has been initialized</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a></p> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.is_mpi_available\">\n<code>torch.distributed.is_mpi_available()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#is_mpi_available\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Checks if the MPI backend is available.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a></p> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.is_nccl_available\">\n<code>torch.distributed.is_nccl_available()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#is_nccl_available\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Checks if the NCCL backend is available.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a></p> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.is_gloo_available\">\n<code>torch.distributed.is_gloo_available()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#is_gloo_available\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Checks if the Gloo backend is available.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a></p> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.is_torchelastic_launched\">\n<code>torch.distributed.is_torchelastic_launched()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#is_torchelastic_launched\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Checks whether this process was launched with <code>torch.distributed.elastic</code> (aka torchelastic). The existence of <code>TORCHELASTIC_RUN_ID</code> environment variable is used as a proxy to determine whether the current process was launched with torchelastic. This is a reasonable proxy since <code>TORCHELASTIC_RUN_ID</code> maps to the rendezvous id which is always a non-null value indicating the job id for peer discovery purposes..</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a></p> </dd> </dl> </dd>\n</dl>  <p>Currently three initialization methods are supported:</p>  <h3 id=\"tcp-initialization\">TCP initialization</h3> <p>There are two ways to initialize using TCP, both requiring a network address reachable from all processes and a desired <code>world_size</code>. The first way requires specifying an address that belongs to the rank 0 process. This initialization method requires that all processes have manually specified ranks.</p> <p>Note that multicast address is not supported anymore in the latest distributed package. <code>group_name</code> is deprecated as well.</p> <pre data-language=\"python\">import torch.distributed as dist\n\n# Use address of one of the machines\ndist.init_process_group(backend, init_method='tcp://10.1.1.20:23456',\n                        rank=args.rank, world_size=4)\n</pre>   <h3 id=\"shared-file-system-initialization\">Shared file-system initialization</h3> <p>Another initialization method makes use of a file system that is shared and visible from all machines in a group, along with a desired <code>world_size</code>. The URL should start with <code>file://</code> and contain a path to a non-existent file (in an existing directory) on a shared file system. File-system initialization will automatically create that file if it doesnt exist, but will not delete the file. Therefore, it is your responsibility to make sure that the file is cleaned up before the next <a class=\"reference internal\" href=\"#torch.distributed.init_process_group\" title=\"torch.distributed.init_process_group\"><code>init_process_group()</code></a> call on the same file path/name.</p> <p>Note that automatic rank assignment is not supported anymore in the latest distributed package and <code>group_name</code> is deprecated as well.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This method assumes that the file system supports locking using <code>fcntl</code> - most local systems and NFS support it.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This method will always create the file and try its best to clean up and remove the file at the end of the program. In other words, each initialization with the file init method will need a brand new empty file in order for the initialization to succeed. If the same file used by the previous initialization (which happens not to get cleaned up) is used again, this is unexpected behavior and can often cause deadlocks and failures. Therefore, even though this method will try its best to clean up the file, if the auto-delete happens to be unsuccessful, it is your responsibility to ensure that the file is removed at the end of the training to prevent the same file to be reused again during the next time. This is especially important if you plan to call <a class=\"reference internal\" href=\"#torch.distributed.init_process_group\" title=\"torch.distributed.init_process_group\"><code>init_process_group()</code></a> multiple times on the same file name. In other words, if the file is not removed/cleaned up and you call <a class=\"reference internal\" href=\"#torch.distributed.init_process_group\" title=\"torch.distributed.init_process_group\"><code>init_process_group()</code></a> again on that file, failures are expected. The rule of thumb here is that, make sure that the file is non-existent or empty every time <a class=\"reference internal\" href=\"#torch.distributed.init_process_group\" title=\"torch.distributed.init_process_group\"><code>init_process_group()</code></a> is called.</p> </div> <pre data-language=\"python\">import torch.distributed as dist\n\n# rank should always be specified\ndist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile',\n                        world_size=4, rank=args.rank)\n</pre>   <h3 id=\"environment-variable-initialization\">Environment variable initialization</h3> <p>This method will read the configuration from environment variables, allowing one to fully customize how the information is obtained. The variables to be set are:</p> <ul class=\"simple\"> <li>\n<code>MASTER_PORT</code> - required; has to be a free port on machine with rank 0</li> <li>\n<code>MASTER_ADDR</code> - required (except for rank 0); address of rank 0 node</li> <li>\n<code>WORLD_SIZE</code> - required; can be set either here, or in a call to init function</li> <li>\n<code>RANK</code> - required; can be set either here, or in a call to init function</li> </ul> <p>The machine with rank 0 will be used to set up all connections.</p> <p>This is the default method, meaning that <code>init_method</code> does not have to be specified (or can be <code>env://</code>).</p>    <h2 id=\"post-initialization\">Post-Initialization</h2> <p>Once <a class=\"reference internal\" href=\"#torch.distributed.init_process_group\" title=\"torch.distributed.init_process_group\"><code>torch.distributed.init_process_group()</code></a> was run, the following functions can be used. To check whether the process group has already been initialized use <a class=\"reference internal\" href=\"#torch.distributed.is_initialized\" title=\"torch.distributed.is_initialized\"><code>torch.distributed.is_initialized()</code></a>.</p> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.Backend\">\n<code>class torch.distributed.Backend(name)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#Backend\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>An enum-like class of available backends: GLOO, NCCL, UCC, MPI, and other registered backends.</p> <p>The values of this class are lowercase strings, e.g., <code>\"gloo\"</code>. They can be accessed as attributes, e.g., <code>Backend.NCCL</code>.</p> <p>This class can be directly called to parse the string, e.g., <code>Backend(backend_str)</code> will check if <code>backend_str</code> is valid, and return the parsed lowercase string if so. It also accepts uppercase strings, e.g., <code>Backend(\"GLOO\")</code> returns <code>\"gloo\"</code>.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The entry <code>Backend.UNDEFINED</code> is present but only used as initial value of some fields. Users should neither use it directly nor assume its existence.</p> </div>  <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.Backend.register_backend\">\n<code>classmethod register_backend(name, func, extended_api=False, devices=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#Backend.register_backend\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Registers a new backend with the given name and instantiating function.</p> <p>This class method is used by 3rd party <code>ProcessGroup</code> extension to register new backends.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a>)  Backend name of the <code>ProcessGroup</code> extension. It should match the one in <code>init_process_group()</code>.</li> <li>\n<strong>func</strong> (<em>function</em>)  Function handler that instantiates the backend. The function should be implemented in the backend extension and takes four arguments, including <code>store</code>, <code>rank</code>, <code>world_size</code>, and <code>timeout</code>.</li> <li>\n<strong>extended_api</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Whether the backend supports extended argument structure. Default: <code>False</code>. If set to <code>True</code>, the backend will get an instance of <code>c10d::DistributedBackendOptions</code>, and a process group options object as defined by the backend implementation.</li> <li>\n<strong>device</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.12)\">list</a><em> of </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a><em>, </em><em>optional</em>)  device type this backend supports, e.g. cpu, cuda, etc. If <code>None</code>, assuming both cpu and cuda</li> </ul> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>This support of 3rd party backend is experimental and subject to change.</p> </div> </dd>\n</dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.get_backend\">\n<code>torch.distributed.get_backend(group=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#get_backend\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the backend of the given process group.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. The default is the general main process group. If another specific group is specified, the calling process must be part of <code>group</code>.</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>The backend of the given process group as a lower case string.</p> </dd> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a></p> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.get_rank\">\n<code>torch.distributed.get_rank(group=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#get_rank\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the rank of the current process in the provided <code>group</code> or the default group if none was provided.</p> <p>Rank is a unique identifier assigned to each process within a distributed process group. They are always consecutive integers ranging from 0 to <code>world_size</code>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If None, the default process group will be used.</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>The rank of the process group -1, if not part of the group</p> </dd> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a></p> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.get_world_size\">\n<code>torch.distributed.get_world_size(group=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#get_world_size\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the number of processes in the current process group</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If None, the default process group will be used.</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>The world size of the process group -1, if not part of the group</p> </dd> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a></p> </dd> </dl> </dd>\n</dl>    <h2 id=\"distributed-key-value-store\">Distributed Key-Value Store</h2> <p>The distributed package comes with a distributed key-value store, which can be used to share information between processes in the group as well as to initialize the distributed package in <a class=\"reference internal\" href=\"#torch.distributed.init_process_group\" title=\"torch.distributed.init_process_group\"><code>torch.distributed.init_process_group()</code></a> (by explicitly creating the store as an alternative to specifying <code>init_method</code>.) There are 3 choices for Key-Value Stores: <a class=\"reference internal\" href=\"#torch.distributed.TCPStore\" title=\"torch.distributed.TCPStore\"><code>TCPStore</code></a>, <a class=\"reference internal\" href=\"#torch.distributed.FileStore\" title=\"torch.distributed.FileStore\"><code>FileStore</code></a>, and <a class=\"reference internal\" href=\"#torch.distributed.HashStore\" title=\"torch.distributed.HashStore\"><code>HashStore</code></a>.</p> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.Store\">\n<code>class torch.distributed.Store</code> </dt> <dd>\n<p>Base class for all store implementations, such as the 3 provided by PyTorch distributed: (<a class=\"reference internal\" href=\"#torch.distributed.TCPStore\" title=\"torch.distributed.TCPStore\"><code>TCPStore</code></a>, <a class=\"reference internal\" href=\"#torch.distributed.FileStore\" title=\"torch.distributed.FileStore\"><code>FileStore</code></a>, and <a class=\"reference internal\" href=\"#torch.distributed.HashStore\" title=\"torch.distributed.HashStore\"><code>HashStore</code></a>).</p> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.TCPStore\">\n<code>class torch.distributed.TCPStore</code> </dt> <dd>\n<p>A TCP-based distributed key-value store implementation. The server store holds the data, while the client stores can connect to the server store over TCP and perform actions such as <code>set()</code> to insert a key-value pair, <code>get()</code> to retrieve a key-value pair, etc. There should always be one server store initialized because the client store(s) will wait for the server to establish a connection.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>host_name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a>)  The hostname or IP Address the server store should run on.</li> <li>\n<strong>port</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a>)  The port on which the server store should listen for incoming requests.</li> <li>\n<strong>world_size</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>, </em><em>optional</em>)  The total number of store users (number of clients + 1 for the server). Default is None (None indicates a non-fixed number of store users).</li> <li>\n<strong>is_master</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  True when initializing the server store and False for client stores. Default is False.</li> <li>\n<strong>timeout</strong> (<em>timedelta</em><em>, </em><em>optional</em>)  Timeout used by the store during initialization and for methods such as <code>get()</code> and <code>wait()</code>. Default is timedelta(seconds=300)</li> <li>\n<strong>wait_for_worker</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Whether to wait for all the workers to connect with the server store. This is only applicable when world_size is a fixed value. Default is True.</li> <li>\n<strong>multi_tenant</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  If True, all <code>TCPStore</code> instances in the current process with the same host/port will use the same underlying <code>TCPServer</code>. Default is False.</li> <li>\n<strong>master_listen_fd</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>, </em><em>optional</em>)  If specified, the underlying <code>TCPServer</code> will listen on this file descriptor, which must be a socket already bound to <code>port</code>. Useful to avoid port assignment races in some scenarios. Default is None (meaning the server creates a new socket and attempts to bind it to <code>port</code>).</li> </ul> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; from datetime import timedelta\n&gt;&gt;&gt; # Run on process 1 (server)\n&gt;&gt;&gt; server_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, True, timedelta(seconds=30))\n&gt;&gt;&gt; # Run on process 2 (client)\n&gt;&gt;&gt; client_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, False)\n&gt;&gt;&gt; # Use any of the store methods from either the client or server after initialization\n&gt;&gt;&gt; server_store.set(\"first_key\", \"first_value\")\n&gt;&gt;&gt; client_store.get(\"first_key\")\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.HashStore\">\n<code>class torch.distributed.HashStore</code> </dt> <dd>\n<p>A thread-safe store implementation based on an underlying hashmap. This store can be used within the same process (for example, by other threads), but cannot be used across processes.</p> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; store = dist.HashStore()\n&gt;&gt;&gt; # store can be used from other threads\n&gt;&gt;&gt; # Use any of the store methods after initialization\n&gt;&gt;&gt; store.set(\"first_key\", \"first_value\")\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.FileStore\">\n<code>class torch.distributed.FileStore</code> </dt> <dd>\n<p>A store implementation that uses a file to store the underlying key-value pairs.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>file_name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a>)  path of the file in which to store the key-value pairs</li> <li>\n<strong>world_size</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>, </em><em>optional</em>)  The total number of processes using the store. Default is -1 (a negative value indicates a non-fixed number of store users).</li> </ul> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; store1 = dist.FileStore(\"/tmp/filestore\", 2)\n&gt;&gt;&gt; store2 = dist.FileStore(\"/tmp/filestore\", 2)\n&gt;&gt;&gt; # Use any of the store methods from either the client or server after initialization\n&gt;&gt;&gt; store1.set(\"first_key\", \"first_value\")\n&gt;&gt;&gt; store2.get(\"first_key\")\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.PrefixStore\">\n<code>class torch.distributed.PrefixStore</code> </dt> <dd>\n<p>A wrapper around any of the 3 key-value stores (<a class=\"reference internal\" href=\"#torch.distributed.TCPStore\" title=\"torch.distributed.TCPStore\"><code>TCPStore</code></a>, <a class=\"reference internal\" href=\"#torch.distributed.FileStore\" title=\"torch.distributed.FileStore\"><code>FileStore</code></a>, and <a class=\"reference internal\" href=\"#torch.distributed.HashStore\" title=\"torch.distributed.HashStore\"><code>HashStore</code></a>) that adds a prefix to each key inserted to the store.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>prefix</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a>)  The prefix string that is prepended to each key before being inserted into the store.</li> <li>\n<strong>store</strong> (<em>torch.distributed.store</em>)  A store object that forms the underlying key-value store.</li> </ul> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.Store.set\">\n<code>torch.distributed.Store.set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str)  None</code> </dt> <dd>\n<p>Inserts the key-value pair into the store based on the supplied <code>key</code> and <code>value</code>. If <code>key</code> already exists in the store, it will overwrite the old value with the new supplied <code>value</code>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>key</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a>)  The key to be added to the store.</li> <li>\n<strong>value</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a>)  The value associated with <code>key</code> to be added to the store.</li> </ul> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; from datetime import timedelta\n&gt;&gt;&gt; store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n&gt;&gt;&gt; store.set(\"first_key\", \"first_value\")\n&gt;&gt;&gt; # Should return \"first_value\"\n&gt;&gt;&gt; store.get(\"first_key\")\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.Store.get\">\n<code>torch.distributed.Store.get(self: torch._C._distributed_c10d.Store, arg0: str)  bytes</code> </dt> <dd>\n<p>Retrieves the value associated with the given <code>key</code> in the store. If <code>key</code> is not present in the store, the function will wait for <code>timeout</code>, which is defined when initializing the store, before throwing an exception.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>key</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a>)  The function will return the value associated with this key.</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Value associated with <code>key</code> if <code>key</code> is in the store.</p> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; from datetime import timedelta\n&gt;&gt;&gt; store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n&gt;&gt;&gt; store.set(\"first_key\", \"first_value\")\n&gt;&gt;&gt; # Should return \"first_value\"\n&gt;&gt;&gt; store.get(\"first_key\")\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.Store.add\">\n<code>torch.distributed.Store.add(self: torch._C._distributed_c10d.Store, arg0: str, arg1: int)  int</code> </dt> <dd>\n<p>The first call to add for a given <code>key</code> creates a counter associated with <code>key</code> in the store, initialized to <code>amount</code>. Subsequent calls to add with the same <code>key</code> increment the counter by the specified <code>amount</code>. Calling <code>add()</code> with a key that has already been set in the store by <code>set()</code> will result in an exception.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>key</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a>)  The key in the store whose counter will be incremented.</li> <li>\n<strong>amount</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a>)  The quantity by which the counter will be incremented.</li> </ul> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; from datetime import timedelta\n&gt;&gt;&gt; # Using TCPStore as an example, other store types can also be used\n&gt;&gt;&gt; store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n&gt;&gt;&gt; store.add(\"first_key\", 1)\n&gt;&gt;&gt; store.add(\"first_key\", 6)\n&gt;&gt;&gt; # Should return 7\n&gt;&gt;&gt; store.get(\"first_key\")\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.Store.compare_set\">\n<code>torch.distributed.Store.compare_set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str, arg2: str)  bytes</code> </dt> <dd>\n<p>Inserts the key-value pair into the store based on the supplied <code>key</code> and performs comparison between <code>expected_value</code> and <code>desired_value</code> before inserting. <code>desired_value</code> will only be set if <code>expected_value</code> for the <code>key</code> already exists in the store or if <code>expected_value</code> is an empty string.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>key</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a>)  The key to be checked in the store.</li> <li>\n<strong>expected_value</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a>)  The value associated with <code>key</code> to be checked before insertion.</li> <li>\n<strong>desired_value</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a>)  The value associated with <code>key</code> to be added to the store.</li> </ul> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; from datetime import timedelta\n&gt;&gt;&gt; store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n&gt;&gt;&gt; store.set(\"key\", \"first_value\")\n&gt;&gt;&gt; store.compare_set(\"key\", \"first_value\", \"second_value\")\n&gt;&gt;&gt; # Should return \"second_value\"\n&gt;&gt;&gt; store.get(\"key\")\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.Store.wait\">\n<code>torch.distributed.Store.wait(*args, **kwargs)</code> </dt> <dd>\n<p>Overloaded function.</p> <ol class=\"arabic simple\"> <li>wait(self: torch._C._distributed_c10d.Store, arg0: List[str]) -&gt; None</li> </ol> <p>Waits for each key in <code>keys</code> to be added to the store. If not all keys are set before the <code>timeout</code> (set during store initialization), then <code>wait</code> will throw an exception.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>keys</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.12)\">list</a>)  List of keys on which to wait until they are set in the store.</p> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; from datetime import timedelta\n&gt;&gt;&gt; # Using TCPStore as an example, other store types can also be used\n&gt;&gt;&gt; store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n&gt;&gt;&gt; # This will throw an exception after 30 seconds\n&gt;&gt;&gt; store.wait([\"bad_key\"])\n</pre> </dd> </dl> <ol class=\"arabic simple\" start=\"2\"> <li>wait(self: torch._C._distributed_c10d.Store, arg0: List[str], arg1: datetime.timedelta) -&gt; None</li> </ol> <p>Waits for each key in <code>keys</code> to be added to the store, and throws an exception if the keys have not been set by the supplied <code>timeout</code>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>keys</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.12)\">list</a>)  List of keys on which to wait until they are set in the store.</li> <li>\n<strong>timeout</strong> (<em>timedelta</em>)  Time to wait for the keys to be added before throwing an exception.</li> </ul> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; from datetime import timedelta\n&gt;&gt;&gt; # Using TCPStore as an example, other store types can also be used\n&gt;&gt;&gt; store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n&gt;&gt;&gt; # This will throw an exception after 10 seconds\n&gt;&gt;&gt; store.wait([\"bad_key\"], timedelta(seconds=10))\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.Store.num_keys\">\n<code>torch.distributed.Store.num_keys(self: torch._C._distributed_c10d.Store)  int</code> </dt> <dd>\n<p>Returns the number of keys set in the store. Note that this number will typically be one greater than the number of keys added by <code>set()</code> and <code>add()</code> since one key is used to coordinate all the workers using the store.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>When used with the <a class=\"reference internal\" href=\"#torch.distributed.TCPStore\" title=\"torch.distributed.TCPStore\"><code>TCPStore</code></a>, <code>num_keys</code> returns the number of keys written to the underlying file. If the store is destructed and another store is created with the same file, the original keys will be retained.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Returns</dt> <dd class=\"field-odd\">\n<p>The number of keys present in the store.</p> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; from datetime import timedelta\n&gt;&gt;&gt; # Using TCPStore as an example, other store types can also be used\n&gt;&gt;&gt; store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n&gt;&gt;&gt; store.set(\"first_key\", \"first_value\")\n&gt;&gt;&gt; # This should return 2\n&gt;&gt;&gt; store.num_keys()\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.Store.delete_key\">\n<code>torch.distributed.Store.delete_key(self: torch._C._distributed_c10d.Store, arg0: str)  bool</code> </dt> <dd>\n<p>Deletes the key-value pair associated with <code>key</code> from the store. Returns <code>true</code> if the key was successfully deleted, and <code>false</code> if it was not.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>The <code>delete_key</code> API is only supported by the <a class=\"reference internal\" href=\"#torch.distributed.TCPStore\" title=\"torch.distributed.TCPStore\"><code>TCPStore</code></a> and <a class=\"reference internal\" href=\"#torch.distributed.HashStore\" title=\"torch.distributed.HashStore\"><code>HashStore</code></a>. Using this API with the <a class=\"reference internal\" href=\"#torch.distributed.FileStore\" title=\"torch.distributed.FileStore\"><code>FileStore</code></a> will result in an exception.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>key</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a>)  The key to be deleted from the store</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p><code>True</code> if <code>key</code> was deleted, otherwise <code>False</code>.</p> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; from datetime import timedelta\n&gt;&gt;&gt; # Using TCPStore as an example, HashStore can also be used\n&gt;&gt;&gt; store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n&gt;&gt;&gt; store.set(\"first_key\")\n&gt;&gt;&gt; # This should return true\n&gt;&gt;&gt; store.delete_key(\"first_key\")\n&gt;&gt;&gt; # This should return false\n&gt;&gt;&gt; store.delete_key(\"bad_key\")\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.Store.set_timeout\">\n<code>torch.distributed.Store.set_timeout(self: torch._C._distributed_c10d.Store, arg0: datetime.timedelta)  None</code> </dt> <dd>\n<p>Sets the stores default timeout. This timeout is used during initialization and in <code>wait()</code> and <code>get()</code>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>timeout</strong> (<em>timedelta</em>)  timeout to be set in the store.</p> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; from datetime import timedelta\n&gt;&gt;&gt; # Using TCPStore as an example, other store types can also be used\n&gt;&gt;&gt; store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n&gt;&gt;&gt; store.set_timeout(timedelta(seconds=10))\n&gt;&gt;&gt; # This will throw an exception after 10 seconds\n&gt;&gt;&gt; store.wait([\"bad_key\"])\n</pre> </dd> </dl> </dd>\n</dl>   <h2 id=\"groups\">Groups</h2> <p>By default collectives operate on the default group (also called the world) and require all processes to enter the distributed function call. However, some workloads can benefit from more fine-grained communication. This is where distributed groups come into play. <a class=\"reference internal\" href=\"#torch.distributed.new_group\" title=\"torch.distributed.new_group\"><code>new_group()</code></a> function can be used to create new groups, with arbitrary subsets of all processes. It returns an opaque group handle that can be given as a <code>group</code> argument to all collectives (collectives are distributed functions to exchange information in certain well-known programming patterns).</p> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.new_group\">\n<code>torch.distributed.new_group(ranks=None, timeout=datetime.timedelta(seconds=1800), backend=None, pg_options=None, use_local_synchronization=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#new_group\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Creates a new distributed group.</p> <p>This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Using multiple process groups with the <code>NCCL</code> backend concurrently is not safe and the user should perform explicit synchronization in their application to ensure only one process group is used at a time. This means collectives from one process group should have completed execution on the device (not just enqueued since CUDA execution is async) before collectives from another process group are enqueued. See <a class=\"reference external\" href=\"https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/communicators.html#using-multiple-nccl-communicators-concurrently\">Using multiple NCCL communicators concurrently</a> for more details.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>ranks</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.12)\">list</a><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>]</em>)  List of ranks of group members. If <code>None</code>, will be set to all ranks. Default is <code>None</code>.</li> <li>\n<strong>timeout</strong> (<em>timedelta</em><em>, </em><em>optional</em>)  Timeout for operations executed against the process group. Default value equals 30 minutes. This is applicable for the <code>gloo</code> backend. For <code>nccl</code>, this is applicable only if the environment variable <code>NCCL_BLOCKING_WAIT</code> or <code>NCCL_ASYNC_ERROR_HANDLING</code> is set to 1. When <code>NCCL_BLOCKING_WAIT</code> is set, this is the duration for which the process will block and wait for collectives to complete before throwing an exception. When <code>NCCL_ASYNC_ERROR_HANDLING</code> is set, this is the duration after which collectives will be aborted asynchronously and the process will crash. <code>NCCL_BLOCKING_WAIT</code> will provide errors to the user which can be caught and handled, but due to its blocking nature, it has a performance overhead. On the other hand, <code>NCCL_ASYNC_ERROR_HANDLING</code> has very little performance overhead, but crashes the process on errors. This is done since CUDA execution is async and it is no longer safe to continue executing user code since failed async NCCL operations might result in subsequent CUDA operations running on corrupted data. Only one of these two environment variables should be set.</li> <li>\n<strong>backend</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a><em> or </em><a class=\"reference internal\" href=\"#torch.distributed.Backend\" title=\"torch.distributed.Backend\">Backend</a><em>, </em><em>optional</em>)  The backend to use. Depending on build-time configurations, valid values are <code>gloo</code> and <code>nccl</code>. By default uses the same backend as the global group. This field should be given as a lowercase string (e.g., <code>\"gloo\"</code>), which can also be accessed via <a class=\"reference internal\" href=\"#torch.distributed.Backend\" title=\"torch.distributed.Backend\"><code>Backend</code></a> attributes (e.g., <code>Backend.GLOO</code>). If <code>None</code> is passed in, the backend corresponding to the default process group will be used. Default is <code>None</code>.</li> <li>\n<strong>pg_options</strong> (<em>ProcessGroupOptions</em><em>, </em><em>optional</em>)  process group options specifying what additional options need to be passed in during the construction of specific process groups. i.e. for the <code>nccl</code> backend, <code>is_high_priority_stream</code> can be specified so that process group can pick up high priority cuda streams.</li> <li>\n<strong>use_local_synchronization</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  perform a group-local barrier at the end of the process group creation. This is different in that non-member ranks dont need to call into API and dont join the barrier.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>A handle of distributed group that can be given to collective calls or None if the rank is not part of <code>ranks</code>.</p> </dd> </dl> <p>N.B. use_local_synchronization doesnt work with MPI.</p> <p>N.B. While use_local_synchronization=True can be significantly faster with larger clusters and small process groups, care must be taken since it changes cluster behavior as non-member ranks dont join the group barrier().</p> <p>N.B. use_local_synchronization=True can lead to deadlocks when each rank creates multiple overlaping process groups. To avoid that, make sure all ranks follow the same global creation order.</p> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.get_group_rank\">\n<code>torch.distributed.get_group_rank(group, global_rank)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#get_group_rank\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Translate a global rank into a group rank.</p> <p><code>global_rank</code> must be part of <code>group</code> otherwise this raises RuntimeError.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>group</strong> (<em>ProcessGroup</em>)  ProcessGroup to find the relative rank.</li> <li>\n<strong>global_rank</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a>)  Global rank to query.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Group rank of <code>global_rank</code> relative to <code>group</code></p> </dd> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a></p> </dd> </dl> <p>N.B. calling this function on the default process group returns identity</p> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.get_global_rank\">\n<code>torch.distributed.get_global_rank(group, group_rank)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#get_global_rank\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Translate a group rank into a global rank.</p> <p><code>group_rank</code> must be part of <code>group</code> otherwise this raises RuntimeError.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>group</strong> (<em>ProcessGroup</em>)  ProcessGroup to find the global rank from.</li> <li>\n<strong>group_rank</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a>)  Group rank to query.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Global rank of <code>group_rank</code> relative to <code>group</code></p> </dd> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a></p> </dd> </dl> <p>N.B. calling this function on the default process group returns identity</p> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.get_process_group_ranks\">\n<code>torch.distributed.get_process_group_ranks(group)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#get_process_group_ranks\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Get all ranks associated with <code>group</code>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>group</strong> (<em>ProcessGroup</em>)  ProcessGroup to get all ranks from.</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>List of global ranks ordered by group rank.</p> </dd> </dl> </dd>\n</dl>   <h2 id=\"point-to-point-communication\">Point-to-point communication</h2> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.send\">\n<code>torch.distributed.send(tensor, dst, group=None, tag=0)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#send\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Sends a tensor synchronously.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>)  Tensor to send.</li> <li>\n<strong>dst</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a>)  Destination rank. Destination rank should not be the same</li> <li>\n<strong>process.</strong> (<em>as the rank</em><em> of </em><em>the current</em>)  </li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>tag</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>, </em><em>optional</em>)  Tag to match send with remote recv</li> </ul> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.recv\">\n<code>torch.distributed.recv(tensor, src=None, group=None, tag=0)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#recv\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Receives a tensor synchronously.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>)  Tensor to fill with received data.</li> <li>\n<strong>src</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>, </em><em>optional</em>)  Source rank. Will receive from any process if unspecified.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>tag</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>, </em><em>optional</em>)  Tag to match recv with remote send</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Sender rank -1, if not part of the group</p> </dd> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a></p> </dd> </dl> </dd>\n</dl> <p><a class=\"reference internal\" href=\"#torch.distributed.isend\" title=\"torch.distributed.isend\"><code>isend()</code></a> and <a class=\"reference internal\" href=\"#torch.distributed.irecv\" title=\"torch.distributed.irecv\"><code>irecv()</code></a> return distributed request objects when used. In general, the type of this object is unspecified as they should never be created manually, but they are guaranteed to support two methods:</p> <ul class=\"simple\"> <li>\n<code>is_completed()</code> - returns True if the operation has finished</li> <li>\n<code>wait()</code> - will block the process until the operation is finished. <code>is_completed()</code> is guaranteed to return True once it returns.</li> </ul> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.isend\">\n<code>torch.distributed.isend(tensor, dst, group=None, tag=0)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#isend\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Sends a tensor asynchronously.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Modifying <code>tensor</code> before the request completes causes undefined behavior.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><code>tag</code> is not supported with the NCCL backend.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>)  Tensor to send.</li> <li>\n<strong>dst</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a>)  Destination rank.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>tag</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>, </em><em>optional</em>)  Tag to match send with remote recv</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>A distributed request object. None, if not part of the group</p> </dd> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><em>Work</em></p> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.irecv\">\n<code>torch.distributed.irecv(tensor, src=None, group=None, tag=0)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#irecv\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Receives a tensor asynchronously.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><code>tag</code> is not supported with the NCCL backend.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>)  Tensor to fill with received data.</li> <li>\n<strong>src</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>, </em><em>optional</em>)  Source rank. Will receive from any process if unspecified.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>tag</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>, </em><em>optional</em>)  Tag to match recv with remote send</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>A distributed request object. None, if not part of the group</p> </dd> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><em>Work</em></p> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.batch_isend_irecv\">\n<code>torch.distributed.batch_isend_irecv(p2p_op_list)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#batch_isend_irecv\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Send or Receive a batch of tensors asynchronously and return a list of requests.</p> <p>Process each of the operations in <code>p2p_op_list</code> and return the corresponding requests. NCCL, Gloo, and UCC backend are currently supported.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>p2p_op_list</strong>  A list of point-to-point operations(type of each operator is <code>torch.distributed.P2POp</code>). The order of the isend/irecv in the list matters and it needs to match with corresponding isend/irecv on the remote end.</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>A list of distributed request objects returned by calling the corresponding op in the op_list.</p> </dd> </dl> <h4 class=\"rubric\">Examples</h4> <pre data-language=\"python\">&gt;&gt;&gt; send_tensor = torch.arange(2) + 2 * rank\n&gt;&gt;&gt; recv_tensor = torch.randn(2)\n&gt;&gt;&gt; send_op = dist.P2POp(dist.isend, send_tensor, (rank + 1)%world_size)\n&gt;&gt;&gt; recv_op = dist.P2POp(dist.irecv, recv_tensor, (rank - 1 + world_size)%world_size)\n&gt;&gt;&gt; reqs = batch_isend_irecv([send_op, recv_op])\n&gt;&gt;&gt; for req in reqs:\n&gt;&gt;&gt;     req.wait()\n&gt;&gt;&gt; recv_tensor\ntensor([2, 3])     # Rank 0\ntensor([0, 1])     # Rank 1\n</pre> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Note that when this API is used with the NCCL PG backend, users must set the current GPU device with <code>torch.cuda.set_device</code>, otherwise it will lead to unexpected hang issues.</p> <p>In addition, if this API is the first collective call in the <code>group</code> passed to <code>dist.P2POp</code>, all ranks of the <code>group</code> must participate in this API call; otherwise, the behavior is undefined. If this API call is not the first collective call in the <code>group</code>, batched P2P operations involving only a subset of ranks of the <code>group</code> are allowed.</p> </div> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.P2POp\">\n<code>class torch.distributed.P2POp(op, tensor, peer, group=None, tag=0)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#P2POp\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>A class to build point-to-point operations for <code>batch_isend_irecv</code>.</p> <p>This class builds the type of P2P operation, communication buffer, peer rank, Process Group, and tag. Instances of this class will be passed to <code>batch_isend_irecv</code> for point-to-point communications.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>op</strong> (<em>Callable</em>)  A function to send data to or receive data from a peer process. The type of <code>op</code> is either <code>torch.distributed.isend</code> or <code>torch.distributed.irecv</code>.</li> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>)  Tensor to send or receive.</li> <li>\n<strong>peer</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a>)  Destination or source rank.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>tag</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>, </em><em>optional</em>)  Tag to match send with recv.</li> </ul> </dd> </dl> </dd>\n</dl>   <h2 id=\"synchronous-and-asynchronous-collective-operations\">Synchronous and asynchronous collective operations</h2> <p>Every collective operation function supports the following two kinds of operations, depending on the setting of the <code>async_op</code> flag passed into the collective:</p> <p><strong>Synchronous operation</strong> - the default mode, when <code>async_op</code> is set to <code>False</code>. When the function returns, it is guaranteed that the collective operation is performed. In the case of CUDA operations, it is not guaranteed that the CUDA operation is completed, since CUDA operations are asynchronous. For CPU collectives, any further function calls utilizing the output of the collective call will behave as expected. For CUDA collectives, function calls utilizing the output on the same CUDA stream will behave as expected. Users must take care of synchronization under the scenario of running under different streams. For details on CUDA semantics such as stream synchronization, see <a class=\"reference external\" href=\"https://pytorch.org/docs/stable/notes/cuda.html\">CUDA Semantics</a>. See the below script to see examples of differences in these semantics for CPU and CUDA operations.</p> <p><strong>Asynchronous operation</strong> - when <code>async_op</code> is set to True. The collective operation function returns a distributed request object. In general, you dont need to create it manually and it is guaranteed to support two methods:</p> <ul class=\"simple\"> <li>\n<code>is_completed()</code> - in the case of CPU collectives, returns <code>True</code> if completed. In the case of CUDA operations, returns <code>True</code> if the operation has been successfully enqueued onto a CUDA stream and the output can be utilized on the default stream without further synchronization.</li> <li>\n<code>wait()</code> - in the case of CPU collectives, will block the process until the operation is completed. In the case of CUDA collectives, will block until the operation has been successfully enqueued onto a CUDA stream and the output can be utilized on the default stream without further synchronization.</li> <li>\n<code>get_future()</code> - returns <code>torch._C.Future</code> object. Supported for NCCL, also supported for most operations on GLOO and MPI, except for peer to peer operations. Note: as we continue adopting Futures and merging APIs, <code>get_future()</code> call might become redundant.</li> </ul> <p><strong>Example</strong></p> <p>The following code can serve as a reference regarding semantics for CUDA operations when using distributed collectives. It shows the explicit need to synchronize when using collective outputs on different CUDA streams:</p> <pre data-language=\"python\"># Code runs on each rank.\ndist.init_process_group(\"nccl\", rank=rank, world_size=2)\noutput = torch.tensor([rank]).cuda(rank)\ns = torch.cuda.Stream()\nhandle = dist.all_reduce(output, async_op=True)\n# Wait ensures the operation is enqueued, but not necessarily complete.\nhandle.wait()\n# Using result on non-default stream.\nwith torch.cuda.stream(s):\n    s.wait_stream(torch.cuda.default_stream())\n    output.add_(100)\nif rank == 0:\n    # if the explicit call to wait_stream was omitted, the output below will be\n    # non-deterministically 1 or 101, depending on whether the allreduce overwrote\n    # the value after the add completed.\n    print(output)\n</pre>   <h2 id=\"collective-functions\">Collective functions</h2> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.broadcast\">\n<code>torch.distributed.broadcast(tensor, src, group=None, async_op=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#broadcast\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Broadcasts the tensor to the whole group.</p> <p><code>tensor</code> must have the same number of elements in all processes participating in the collective.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>)  Data to be sent if <code>src</code> is the rank of current process, and tensor to be used to save received data otherwise.</li> <li>\n<strong>src</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a>)  Source rank.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>async_op</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Whether this op should be an async op</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group</p> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.broadcast_object_list\">\n<code>torch.distributed.broadcast_object_list(object_list, src=0, group=None, device=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#broadcast_object_list\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Broadcasts picklable objects in <code>object_list</code> to the whole group. Similar to <a class=\"reference internal\" href=\"#torch.distributed.broadcast\" title=\"torch.distributed.broadcast\"><code>broadcast()</code></a>, but Python objects can be passed in. Note that all objects in <code>object_list</code> must be picklable in order to be broadcasted.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>object_list</strong> (<em>List</em><em>[</em><em>Any</em><em>]</em>)  List of input objects to broadcast. Each object must be picklable. Only objects on the <code>src</code> rank will be broadcast, but each rank must provide lists of equal sizes.</li> <li>\n<strong>src</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a>)  Source rank from which to broadcast <code>object_list</code>.</li> <li>\n<strong>group</strong>  (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is <code>None</code>.</li> <li>\n<strong>device</strong> (<code>torch.device</code>, optional)  If not None, the objects are serialized and converted to tensors which are moved to the <code>device</code> before broadcasting. Default is <code>None</code>.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p><code>None</code>. If rank is part of the group, <code>object_list</code> will contain the broadcasted objects from <code>src</code> rank.</p> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>For NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by <code>torch.cuda.current_device()</code> and it is the users responsibility to ensure that this is set so that each rank has an individual GPU, via <code>torch.cuda.set_device()</code>.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Note that this API differs slightly from the <a class=\"reference internal\" href=\"#torch.distributed.all_gather\" title=\"torch.distributed.all_gather\"><code>all_gather()</code></a> collective since it does not provide an <code>async_op</code> handle and thus will be a blocking call.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><a class=\"reference internal\" href=\"#torch.distributed.broadcast_object_list\" title=\"torch.distributed.broadcast_object_list\"><code>broadcast_object_list()</code></a> uses <code>pickle</code> module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Calling <a class=\"reference internal\" href=\"#torch.distributed.broadcast_object_list\" title=\"torch.distributed.broadcast_object_list\"><code>broadcast_object_list()</code></a> with GPU tensors is not well supported and inefficient as it incurs GPU -&gt; CPU transfer since tensors would be pickled. Please consider using <a class=\"reference internal\" href=\"#torch.distributed.broadcast\" title=\"torch.distributed.broadcast\"><code>broadcast()</code></a> instead.</p> </div> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; # Note: Process group initialization omitted on each rank.\n&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; if dist.get_rank() == 0:\n&gt;&gt;&gt;     # Assumes world_size of 3.\n&gt;&gt;&gt;     objects = [\"foo\", 12, {1: 2}] # any picklable object\n&gt;&gt;&gt; else:\n&gt;&gt;&gt;     objects = [None, None, None]\n&gt;&gt;&gt; # Assumes backend is not NCCL\n&gt;&gt;&gt; device = torch.device(\"cpu\")\n&gt;&gt;&gt; dist.broadcast_object_list(objects, src=0, device=device)\n&gt;&gt;&gt; objects\n['foo', 12, {1: 2}]\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.all_reduce\">\n<code>torch.distributed.all_reduce(tensor, op=&lt;RedOpType.SUM: 0&gt;, group=None, async_op=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#all_reduce\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Reduces the tensor data across all machines in such a way that all get the final result.</p> <p>After the call <code>tensor</code> is going to be bitwise identical in all processes.</p> <p>Complex tensors are supported.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>)  Input and output of the collective. The function operates in-place.</li> <li>\n<strong>op</strong> (<em>optional</em>)  One of the values from <code>torch.distributed.ReduceOp</code> enum. Specifies an operation used for element-wise reductions.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>async_op</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Whether this op should be an async op</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group</p> </dd> </dl> <h4 class=\"rubric\">Examples</h4> <pre data-language=\"python\">&gt;&gt;&gt; # All tensors below are of torch.int64 type.\n&gt;&gt;&gt; # We have 2 process groups, 2 ranks.\n&gt;&gt;&gt; tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n&gt;&gt;&gt; tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n&gt;&gt;&gt; dist.all_reduce(tensor, op=ReduceOp.SUM)\n&gt;&gt;&gt; tensor\ntensor([4, 6]) # Rank 0\ntensor([4, 6]) # Rank 1\n</pre> <pre data-language=\"python\">&gt;&gt;&gt; # All tensors below are of torch.cfloat type.\n&gt;&gt;&gt; # We have 2 process groups, 2 ranks.\n&gt;&gt;&gt; tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\n&gt;&gt;&gt; tensor\ntensor([1.+1.j, 2.+2.j]) # Rank 0\ntensor([3.+3.j, 4.+4.j]) # Rank 1\n&gt;&gt;&gt; dist.all_reduce(tensor, op=ReduceOp.SUM)\n&gt;&gt;&gt; tensor\ntensor([4.+4.j, 6.+6.j]) # Rank 0\ntensor([4.+4.j, 6.+6.j]) # Rank 1\n</pre> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.reduce\">\n<code>torch.distributed.reduce(tensor, dst, op=&lt;RedOpType.SUM: 0&gt;, group=None, async_op=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#reduce\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Reduces the tensor data across all machines.</p> <p>Only the process with rank <code>dst</code> is going to receive the final result.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>)  Input and output of the collective. The function operates in-place.</li> <li>\n<strong>dst</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a>)  Destination rank</li> <li>\n<strong>op</strong> (<em>optional</em>)  One of the values from <code>torch.distributed.ReduceOp</code> enum. Specifies an operation used for element-wise reductions.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>async_op</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Whether this op should be an async op</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group</p> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.all_gather\">\n<code>torch.distributed.all_gather(tensor_list, tensor, group=None, async_op=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#all_gather\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Gathers tensors from the whole group in a list.</p> <p>Complex tensors are supported.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor_list</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.12)\">list</a><em>[</em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>]</em>)  Output list. It should contain correctly-sized tensors to be used for output of the collective.</li> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>)  Tensor to be broadcast from current process.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>async_op</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Whether this op should be an async op</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group</p> </dd> </dl> <h4 class=\"rubric\">Examples</h4> <pre data-language=\"python\">&gt;&gt;&gt; # All tensors below are of torch.int64 dtype.\n&gt;&gt;&gt; # We have 2 process groups, 2 ranks.\n&gt;&gt;&gt; tensor_list = [torch.zeros(2, dtype=torch.int64) for _ in range(2)]\n&gt;&gt;&gt; tensor_list\n[tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1\n&gt;&gt;&gt; tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n&gt;&gt;&gt; tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n&gt;&gt;&gt; dist.all_gather(tensor_list, tensor)\n&gt;&gt;&gt; tensor_list\n[tensor([1, 2]), tensor([3, 4])] # Rank 0\n[tensor([1, 2]), tensor([3, 4])] # Rank 1\n</pre> <pre data-language=\"python\">&gt;&gt;&gt; # All tensors below are of torch.cfloat dtype.\n&gt;&gt;&gt; # We have 2 process groups, 2 ranks.\n&gt;&gt;&gt; tensor_list = [torch.zeros(2, dtype=torch.cfloat) for _ in range(2)]\n&gt;&gt;&gt; tensor_list\n[tensor([0.+0.j, 0.+0.j]), tensor([0.+0.j, 0.+0.j])] # Rank 0 and 1\n&gt;&gt;&gt; tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\n&gt;&gt;&gt; tensor\ntensor([1.+1.j, 2.+2.j]) # Rank 0\ntensor([3.+3.j, 4.+4.j]) # Rank 1\n&gt;&gt;&gt; dist.all_gather(tensor_list, tensor)\n&gt;&gt;&gt; tensor_list\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 0\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 1\n</pre> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.all_gather_into_tensor\">\n<code>torch.distributed.all_gather_into_tensor(output_tensor, input_tensor, group=None, async_op=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#all_gather_into_tensor\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Gather tensors from all ranks and put them in a single output tensor.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>output_tensor</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>)  Output tensor to accommodate tensor elements from all ranks. It must be correctly sized to have one of the following forms: (i) a concatenation of all the input tensors along the primary dimension; for definition of concatenation, see <code>torch.cat()</code>; (ii) a stack of all the input tensors along the primary dimension; for definition of stack, see <code>torch.stack()</code>. Examples below may better explain the supported output forms.</li> <li>\n<strong>input_tensor</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>)  Tensor to be gathered from current rank. Different from the <code>all_gather</code> API, the input tensors in this API must have the same size across all ranks.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>async_op</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Whether this op should be an async op</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group</p> </dd> </dl> <h4 class=\"rubric\">Examples</h4> <pre data-language=\"python\">&gt;&gt;&gt; # All tensors below are of torch.int64 dtype and on CUDA devices.\n&gt;&gt;&gt; # We have two ranks.\n&gt;&gt;&gt; device = torch.device(f'cuda:{rank}')\n&gt;&gt;&gt; tensor_in = torch.arange(2, dtype=torch.int64, device=device) + 1 + 2 * rank\n&gt;&gt;&gt; tensor_in\ntensor([1, 2], device='cuda:0') # Rank 0\ntensor([3, 4], device='cuda:1') # Rank 1\n&gt;&gt;&gt; # Output in concatenation form\n&gt;&gt;&gt; tensor_out = torch.zeros(world_size * 2, dtype=torch.int64, device=device)\n&gt;&gt;&gt; dist.all_gather_into_tensor(tensor_out, tensor_in)\n&gt;&gt;&gt; tensor_out\ntensor([1, 2, 3, 4], device='cuda:0') # Rank 0\ntensor([1, 2, 3, 4], device='cuda:1') # Rank 1\n&gt;&gt;&gt; # Output in stack form\n&gt;&gt;&gt; tensor_out2 = torch.zeros(world_size, 2, dtype=torch.int64, device=device)\n&gt;&gt;&gt; dist.all_gather_into_tensor(tensor_out2, tensor_in)\n&gt;&gt;&gt; tensor_out2\ntensor([[1, 2],\n        [3, 4]], device='cuda:0') # Rank 0\ntensor([[1, 2],\n        [3, 4]], device='cuda:1') # Rank 1\n</pre> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>The Gloo backend does not support this API.</p> </div> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.all_gather_object\">\n<code>torch.distributed.all_gather_object(object_list, obj, group=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#all_gather_object\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Gathers picklable objects from the whole group into a list. Similar to <a class=\"reference internal\" href=\"#torch.distributed.all_gather\" title=\"torch.distributed.all_gather\"><code>all_gather()</code></a>, but Python objects can be passed in. Note that the object must be picklable in order to be gathered.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>object_list</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.12)\">list</a><em>[</em><em>Any</em><em>]</em>)  Output list. It should be correctly sized as the size of the group for this collective and will contain the output.</li> <li>\n<strong>obj</strong> (<em>Any</em>)  Pickable Python object to be broadcast from current process.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If None, the default process group will be used. Default is <code>None</code>.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>None. If the calling rank is part of this group, the output of the collective will be populated into the input <code>object_list</code>. If the calling rank is not part of the group, the passed in <code>object_list</code> will be unmodified.</p> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Note that this API differs slightly from the <a class=\"reference internal\" href=\"#torch.distributed.all_gather\" title=\"torch.distributed.all_gather\"><code>all_gather()</code></a> collective since it does not provide an <code>async_op</code> handle and thus will be a blocking call.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>For NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by <code>torch.cuda.current_device()</code> and it is the users responsiblity to ensure that this is set so that each rank has an individual GPU, via <code>torch.cuda.set_device()</code>.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><a class=\"reference internal\" href=\"#torch.distributed.all_gather_object\" title=\"torch.distributed.all_gather_object\"><code>all_gather_object()</code></a> uses <code>pickle</code> module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Calling <a class=\"reference internal\" href=\"#torch.distributed.all_gather_object\" title=\"torch.distributed.all_gather_object\"><code>all_gather_object()</code></a> with GPU tensors is not well supported and inefficient as it incurs GPU -&gt; CPU transfer since tensors would be pickled. Please consider using <a class=\"reference internal\" href=\"#torch.distributed.all_gather\" title=\"torch.distributed.all_gather\"><code>all_gather()</code></a> instead.</p> </div> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; # Note: Process group initialization omitted on each rank.\n&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; # Assumes world_size of 3.\n&gt;&gt;&gt; gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n&gt;&gt;&gt; output = [None for _ in gather_objects]\n&gt;&gt;&gt; dist.all_gather_object(output, gather_objects[dist.get_rank()])\n&gt;&gt;&gt; output\n['foo', 12, {1: 2}]\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.gather\">\n<code>torch.distributed.gather(tensor, gather_list=None, dst=0, group=None, async_op=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#gather\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Gathers a list of tensors in a single process.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>)  Input tensor.</li> <li>\n<strong>gather_list</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.12)\">list</a><em>[</em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>]</em><em>, </em><em>optional</em>)  List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank)</li> <li>\n<strong>dst</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>, </em><em>optional</em>)  Destination rank (default is 0)</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>async_op</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Whether this op should be an async op</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group</p> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.gather_object\">\n<code>torch.distributed.gather_object(obj, object_gather_list=None, dst=0, group=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#gather_object\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Gathers picklable objects from the whole group in a single process. Similar to <a class=\"reference internal\" href=\"#torch.distributed.gather\" title=\"torch.distributed.gather\"><code>gather()</code></a>, but Python objects can be passed in. Note that the object must be picklable in order to be gathered.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>obj</strong> (<em>Any</em>)  Input object. Must be picklable.</li> <li>\n<strong>object_gather_list</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.12)\">list</a><em>[</em><em>Any</em><em>]</em>)  Output list. On the <code>dst</code> rank, it should be correctly sized as the size of the group for this collective and will contain the output. Must be <code>None</code> on non-dst ranks. (default is <code>None</code>)</li> <li>\n<strong>dst</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>, </em><em>optional</em>)  Destination rank. (default is 0)</li> <li>\n<strong>group</strong>  (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is <code>None</code>.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>None. On the <code>dst</code> rank, <code>object_gather_list</code> will contain the output of the collective.</p> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Note that this API differs slightly from the gather collective since it does not provide an async_op handle and thus will be a blocking call.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>For NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by <code>torch.cuda.current_device()</code> and it is the users responsiblity to ensure that this is set so that each rank has an individual GPU, via <code>torch.cuda.set_device()</code>.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><a class=\"reference internal\" href=\"#torch.distributed.gather_object\" title=\"torch.distributed.gather_object\"><code>gather_object()</code></a> uses <code>pickle</code> module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Calling <a class=\"reference internal\" href=\"#torch.distributed.gather_object\" title=\"torch.distributed.gather_object\"><code>gather_object()</code></a> with GPU tensors is not well supported and inefficient as it incurs GPU -&gt; CPU transfer since tensors would be pickled. Please consider using <a class=\"reference internal\" href=\"#torch.distributed.gather\" title=\"torch.distributed.gather\"><code>gather()</code></a> instead.</p> </div> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; # Note: Process group initialization omitted on each rank.\n&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; # Assumes world_size of 3.\n&gt;&gt;&gt; gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n&gt;&gt;&gt; output = [None for _ in gather_objects]\n&gt;&gt;&gt; dist.gather_object(\n...     gather_objects[dist.get_rank()],\n...     output if dist.get_rank() == 0 else None,\n...     dst=0\n... )\n&gt;&gt;&gt; # On rank 0\n&gt;&gt;&gt; output\n['foo', 12, {1: 2}]\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.scatter\">\n<code>torch.distributed.scatter(tensor, scatter_list=None, src=0, group=None, async_op=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#scatter\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Scatters a list of tensors to all processes in a group.</p> <p>Each process will receive exactly one tensor and store its data in the <code>tensor</code> argument.</p> <p>Complex tensors are supported.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>)  Output tensor.</li> <li>\n<strong>scatter_list</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.12)\">list</a><em>[</em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>]</em>)  List of tensors to scatter (default is None, must be specified on the source rank)</li> <li>\n<strong>src</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a>)  Source rank (default is 0)</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>async_op</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Whether this op should be an async op</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group</p> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Note that all Tensors in scatter_list must have the same size.</p> </div> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; # Note: Process group initialization omitted on each rank.\n&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; tensor_size = 2\n&gt;&gt;&gt; t_ones = torch.ones(tensor_size)\n&gt;&gt;&gt; t_fives = torch.ones(tensor_size) * 5\n&gt;&gt;&gt; output_tensor = torch.zeros(tensor_size)\n&gt;&gt;&gt; if dist.get_rank() == 0:\n&gt;&gt;&gt;     # Assumes world_size of 2.\n&gt;&gt;&gt;     # Only tensors, all of which must be the same size.\n&gt;&gt;&gt;     scatter_list = [t_ones, t_fives]\n&gt;&gt;&gt; else:\n&gt;&gt;&gt;     scatter_list = None\n&gt;&gt;&gt; dist.scatter(output_tensor, scatter_list, src=0)\n&gt;&gt;&gt; # Rank i gets scatter_list[i]. For example, on rank 1:\n&gt;&gt;&gt; output_tensor\ntensor([5., 5.])\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.scatter_object_list\">\n<code>torch.distributed.scatter_object_list(scatter_object_output_list, scatter_object_input_list, src=0, group=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#scatter_object_list\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Scatters picklable objects in <code>scatter_object_input_list</code> to the whole group. Similar to <a class=\"reference internal\" href=\"#torch.distributed.scatter\" title=\"torch.distributed.scatter\"><code>scatter()</code></a>, but Python objects can be passed in. On each rank, the scattered object will be stored as the first element of <code>scatter_object_output_list</code>. Note that all objects in <code>scatter_object_input_list</code> must be picklable in order to be scattered.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>scatter_object_output_list</strong> (<em>List</em><em>[</em><em>Any</em><em>]</em>)  Non-empty list whose first element will store the object scattered to this rank.</li> <li>\n<strong>scatter_object_input_list</strong> (<em>List</em><em>[</em><em>Any</em><em>]</em>)  List of input objects to scatter. Each object must be picklable. Only objects on the <code>src</code> rank will be scattered, and the argument can be <code>None</code> for non-src ranks.</li> <li>\n<strong>src</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a>)  Source rank from which to scatter <code>scatter_object_input_list</code>.</li> <li>\n<strong>group</strong>  (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is <code>None</code>.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p><code>None</code>. If rank is part of the group, <code>scatter_object_output_list</code> will have its first element set to the scattered object for this rank.</p> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Note that this API differs slightly from the scatter collective since it does not provide an <code>async_op</code> handle and thus will be a blocking call.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><a class=\"reference internal\" href=\"#torch.distributed.scatter_object_list\" title=\"torch.distributed.scatter_object_list\"><code>scatter_object_list()</code></a> uses <code>pickle</code> module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Calling <a class=\"reference internal\" href=\"#torch.distributed.scatter_object_list\" title=\"torch.distributed.scatter_object_list\"><code>scatter_object_list()</code></a> with GPU tensors is not well supported and inefficient as it incurs GPU -&gt; CPU transfer since tensors would be pickled. Please consider using <a class=\"reference internal\" href=\"#torch.distributed.scatter\" title=\"torch.distributed.scatter\"><code>scatter()</code></a> instead.</p> </div> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; # Note: Process group initialization omitted on each rank.\n&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; if dist.get_rank() == 0:\n&gt;&gt;&gt;     # Assumes world_size of 3.\n&gt;&gt;&gt;     objects = [\"foo\", 12, {1: 2}] # any picklable object\n&gt;&gt;&gt; else:\n&gt;&gt;&gt;     # Can be any list on non-src ranks, elements are not used.\n&gt;&gt;&gt;     objects = [None, None, None]\n&gt;&gt;&gt; output_list = [None]\n&gt;&gt;&gt; dist.scatter_object_list(output_list, objects, src=0)\n&gt;&gt;&gt; # Rank i gets objects[i]. For example, on rank 2:\n&gt;&gt;&gt; output_list\n[{1: 2}]\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.reduce_scatter\">\n<code>torch.distributed.reduce_scatter(output, input_list, op=&lt;RedOpType.SUM: 0&gt;, group=None, async_op=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#reduce_scatter\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Reduces, then scatters a list of tensors to all processes in a group.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>output</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>)  Output tensor.</li> <li>\n<strong>input_list</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.12)\">list</a><em>[</em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>]</em>)  List of tensors to reduce and scatter.</li> <li>\n<strong>op</strong> (<em>optional</em>)  One of the values from <code>torch.distributed.ReduceOp</code> enum. Specifies an operation used for element-wise reductions.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>async_op</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Whether this op should be an async op.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group.</p> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.reduce_scatter_tensor\">\n<code>torch.distributed.reduce_scatter_tensor(output, input, op=&lt;RedOpType.SUM: 0&gt;, group=None, async_op=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#reduce_scatter_tensor\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Reduces, then scatters a tensor to all ranks in a group.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>output</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>)  Output tensor. It should have the same size across all ranks.</li> <li>\n<strong>input</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>)  Input tensor to be reduced and scattered. Its size should be output tensor size times the world size. The input tensor can have one of the following shapes: (i) a concatenation of the output tensors along the primary dimension, or (ii) a stack of the output tensors along the primary dimension. For definition of concatenation, see <code>torch.cat()</code>. For definition of stack, see <code>torch.stack()</code>.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>async_op</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Whether this op should be an async op.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group.</p> </dd> </dl> <h4 class=\"rubric\">Examples</h4> <pre data-language=\"python\">&gt;&gt;&gt; # All tensors below are of torch.int64 dtype and on CUDA devices.\n&gt;&gt;&gt; # We have two ranks.\n&gt;&gt;&gt; device = torch.device(f'cuda:{rank}')\n&gt;&gt;&gt; tensor_out = torch.zeros(2, dtype=torch.int64, device=device)\n&gt;&gt;&gt; # Input in concatenation form\n&gt;&gt;&gt; tensor_in = torch.arange(world_size * 2, dtype=torch.int64, device=device)\n&gt;&gt;&gt; tensor_in\ntensor([0, 1, 2, 3], device='cuda:0') # Rank 0\ntensor([0, 1, 2, 3], device='cuda:1') # Rank 1\n&gt;&gt;&gt; dist.reduce_scatter_tensor(tensor_out, tensor_in)\n&gt;&gt;&gt; tensor_out\ntensor([0, 2], device='cuda:0') # Rank 0\ntensor([4, 6], device='cuda:1') # Rank 1\n&gt;&gt;&gt; # Input in stack form\n&gt;&gt;&gt; tensor_in = torch.reshape(tensor_in, (world_size, 2))\n&gt;&gt;&gt; tensor_in\ntensor([[0, 1],\n        [2, 3]], device='cuda:0') # Rank 0\ntensor([[0, 1],\n        [2, 3]], device='cuda:1') # Rank 1\n&gt;&gt;&gt; dist.reduce_scatter_tensor(tensor_out, tensor_in)\n&gt;&gt;&gt; tensor_out\ntensor([0, 2], device='cuda:0') # Rank 0\ntensor([4, 6], device='cuda:1') # Rank 1\n</pre> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>The Gloo backend does not support this API.</p> </div> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.all_to_all_single\">\n<code>torch.distributed.all_to_all_single(output, input, output_split_sizes=None, input_split_sizes=None, group=None, async_op=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#all_to_all_single\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Each process splits input tensor and then scatters the split list to all processes in a group. Then concatenate the received tensors from all the processes in the group and return single output tensor.</p> <p>Complex tensors are supported.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>output</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>)  Gathered concatenated output tensor.</li> <li>\n<strong>input</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>)  Input tensor to scatter.</li> <li>\n<strong>output_split_sizes</strong>  (list[Int], optional): Output split sizes for dim 0 if specified None or empty, dim 0 of <code>output</code> tensor must divide equally by <code>world_size</code>.</li> <li>\n<strong>input_split_sizes</strong>  (list[Int], optional): Input split sizes for dim 0 if specified None or empty, dim 0 of <code>input</code> tensor must divide equally by <code>world_size</code>.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>async_op</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Whether this op should be an async op.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group.</p> </dd> </dl> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><code>all_to_all_single</code> is experimental and subject to change.</p> </div> <h4 class=\"rubric\">Examples</h4> <pre data-language=\"python\">&gt;&gt;&gt; input = torch.arange(4) + rank * 4\n&gt;&gt;&gt; input\ntensor([0, 1, 2, 3])     # Rank 0\ntensor([4, 5, 6, 7])     # Rank 1\ntensor([8, 9, 10, 11])   # Rank 2\ntensor([12, 13, 14, 15]) # Rank 3\n&gt;&gt;&gt; output = torch.empty([4], dtype=torch.int64)\n&gt;&gt;&gt; dist.all_to_all_single(output, input)\n&gt;&gt;&gt; output\ntensor([0, 4, 8, 12])    # Rank 0\ntensor([1, 5, 9, 13])    # Rank 1\ntensor([2, 6, 10, 14])   # Rank 2\ntensor([3, 7, 11, 15])   # Rank 3\n</pre> <pre data-language=\"python\">&gt;&gt;&gt; # Essentially, it is similar to following operation:\n&gt;&gt;&gt; scatter_list = list(input.chunk(world_size))\n&gt;&gt;&gt; gather_list  = list(output.chunk(world_size))\n&gt;&gt;&gt; for i in range(world_size):\n&gt;&gt;&gt;     dist.scatter(gather_list[i], scatter_list if i == rank else [], src = i)\n</pre> <pre data-language=\"python\">&gt;&gt;&gt; # Another example with uneven split\n&gt;&gt;&gt; input\ntensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\ntensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\ntensor([20, 21, 22, 23, 24])                                     # Rank 2\ntensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\n&gt;&gt;&gt; input_splits\n[2, 2, 1, 1]                                                     # Rank 0\n[3, 2, 2, 2]                                                     # Rank 1\n[2, 1, 1, 1]                                                     # Rank 2\n[2, 2, 2, 1]                                                     # Rank 3\n&gt;&gt;&gt; output_splits\n[2, 3, 2, 2]                                                     # Rank 0\n[2, 2, 1, 2]                                                     # Rank 1\n[1, 2, 1, 2]                                                     # Rank 2\n[1, 2, 1, 1]                                                     # Rank 3\n&gt;&gt;&gt; output = ...\n&gt;&gt;&gt; dist.all_to_all_single(output, input, output_splits, input_splits)\n&gt;&gt;&gt; output\ntensor([ 0,  1, 10, 11, 12, 20, 21, 30, 31])                     # Rank 0\ntensor([ 2,  3, 13, 14, 22, 32, 33])                             # Rank 1\ntensor([ 4, 15, 16, 23, 34, 35])                                 # Rank 2\ntensor([ 5, 17, 18, 24, 36])                                     # Rank 3\n</pre> <pre data-language=\"python\">&gt;&gt;&gt; # Another example with tensors of torch.cfloat type.\n&gt;&gt;&gt; input = torch.tensor([1+1j, 2+2j, 3+3j, 4+4j], dtype=torch.cfloat) + 4 * rank * (1+1j)\n&gt;&gt;&gt; input\ntensor([1+1j, 2+2j, 3+3j, 4+4j])                                # Rank 0\ntensor([5+5j, 6+6j, 7+7j, 8+8j])                                # Rank 1\ntensor([9+9j, 10+10j, 11+11j, 12+12j])                          # Rank 2\ntensor([13+13j, 14+14j, 15+15j, 16+16j])                        # Rank 3\n&gt;&gt;&gt; output = torch.empty([4], dtype=torch.int64)\n&gt;&gt;&gt; dist.all_to_all_single(output, input)\n&gt;&gt;&gt; output\ntensor([1+1j, 5+5j, 9+9j, 13+13j])                              # Rank 0\ntensor([2+2j, 6+6j, 10+10j, 14+14j])                            # Rank 1\ntensor([3+3j, 7+7j, 11+11j, 15+15j])                            # Rank 2\ntensor([4+4j, 8+8j, 12+12j, 16+16j])                            # Rank 3\n</pre> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.all_to_all\">\n<code>torch.distributed.all_to_all(output_tensor_list, input_tensor_list, group=None, async_op=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#all_to_all\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Each process scatters list of input tensors to all processes in a group and return gathered list of tensors in output list.</p> <p>Complex tensors are supported.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>output_tensor_list</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.12)\">list</a><em>[</em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>]</em>)  List of tensors to be gathered one per rank.</li> <li>\n<strong>input_tensor_list</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.12)\">list</a><em>[</em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>]</em>)  List of tensors to scatter one per rank.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>async_op</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Whether this op should be an async op.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group.</p> </dd> </dl> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><code>all_to_all</code> is experimental and subject to change.</p> </div> <h4 class=\"rubric\">Examples</h4> <pre data-language=\"python\">&gt;&gt;&gt; input = torch.arange(4) + rank * 4\n&gt;&gt;&gt; input = list(input.chunk(4))\n&gt;&gt;&gt; input\n[tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0\n[tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1\n[tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2\n[tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3\n&gt;&gt;&gt; output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n&gt;&gt;&gt; dist.all_to_all(output, input)\n&gt;&gt;&gt; output\n[tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0\n[tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1\n[tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2\n[tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3\n</pre> <pre data-language=\"python\">&gt;&gt;&gt; # Essentially, it is similar to following operation:\n&gt;&gt;&gt; scatter_list = input\n&gt;&gt;&gt; gather_list  = output\n&gt;&gt;&gt; for i in range(world_size):\n&gt;&gt;&gt;     dist.scatter(gather_list[i], scatter_list if i == rank else [], src=i)\n</pre> <pre data-language=\"python\">&gt;&gt;&gt; input\ntensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\ntensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\ntensor([20, 21, 22, 23, 24])                                     # Rank 2\ntensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\n&gt;&gt;&gt; input_splits\n[2, 2, 1, 1]                                                     # Rank 0\n[3, 2, 2, 2]                                                     # Rank 1\n[2, 1, 1, 1]                                                     # Rank 2\n[2, 2, 2, 1]                                                     # Rank 3\n&gt;&gt;&gt; output_splits\n[2, 3, 2, 2]                                                     # Rank 0\n[2, 2, 1, 2]                                                     # Rank 1\n[1, 2, 1, 2]                                                     # Rank 2\n[1, 2, 1, 1]                                                     # Rank 3\n&gt;&gt;&gt; input = list(input.split(input_splits))\n&gt;&gt;&gt; input\n[tensor([0, 1]), tensor([2, 3]), tensor([4]), tensor([5])]                   # Rank 0\n[tensor([10, 11, 12]), tensor([13, 14]), tensor([15, 16]), tensor([17, 18])] # Rank 1\n[tensor([20, 21]), tensor([22]), tensor([23]), tensor([24])]                 # Rank 2\n[tensor([30, 31]), tensor([32, 33]), tensor([34, 35]), tensor([36])]         # Rank 3\n&gt;&gt;&gt; output = ...\n&gt;&gt;&gt; dist.all_to_all(output, input)\n&gt;&gt;&gt; output\n[tensor([0, 1]), tensor([10, 11, 12]), tensor([20, 21]), tensor([30, 31])]   # Rank 0\n[tensor([2, 3]), tensor([13, 14]), tensor([22]), tensor([32, 33])]           # Rank 1\n[tensor([4]), tensor([15, 16]), tensor([23]), tensor([34, 35])]              # Rank 2\n[tensor([5]), tensor([17, 18]), tensor([24]), tensor([36])]                  # Rank 3\n</pre> <pre data-language=\"python\">&gt;&gt;&gt; # Another example with tensors of torch.cfloat type.\n&gt;&gt;&gt; input = torch.tensor([1+1j, 2+2j, 3+3j, 4+4j], dtype=torch.cfloat) + 4 * rank * (1+1j)\n&gt;&gt;&gt; input = list(input.chunk(4))\n&gt;&gt;&gt; input\n[tensor([1+1j]), tensor([2+2j]), tensor([3+3j]), tensor([4+4j])]            # Rank 0\n[tensor([5+5j]), tensor([6+6j]), tensor([7+7j]), tensor([8+8j])]            # Rank 1\n[tensor([9+9j]), tensor([10+10j]), tensor([11+11j]), tensor([12+12j])]      # Rank 2\n[tensor([13+13j]), tensor([14+14j]), tensor([15+15j]), tensor([16+16j])]    # Rank 3\n&gt;&gt;&gt; output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n&gt;&gt;&gt; dist.all_to_all(output, input)\n&gt;&gt;&gt; output\n[tensor([1+1j]), tensor([5+5j]), tensor([9+9j]), tensor([13+13j])]          # Rank 0\n[tensor([2+2j]), tensor([6+6j]), tensor([10+10j]), tensor([14+14j])]        # Rank 1\n[tensor([3+3j]), tensor([7+7j]), tensor([11+11j]), tensor([15+15j])]        # Rank 2\n[tensor([4+4j]), tensor([8+8j]), tensor([12+12j]), tensor([16+16j])]        # Rank 3\n</pre> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.barrier\">\n<code>torch.distributed.barrier(group=None, async_op=False, device_ids=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#barrier\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Synchronizes all processes.</p> <p>This collective blocks processes until the whole group enters this function, if async_op is False, or if async work handle is called on wait().</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>async_op</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Whether this op should be an async op</li> <li>\n<strong>device_ids</strong> (<em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>]</em><em>, </em><em>optional</em>)  List of device/GPU ids.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group</p> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.monitored_barrier\">\n<code>torch.distributed.monitored_barrier(group=None, timeout=None, wait_all_ranks=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#monitored_barrier\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Synchronizes all processes similar to <code>torch.distributed.barrier</code>, but takes a configurable timeout and is able to report ranks that did not pass this barrier within that timeout. Specifically, for non-zero ranks, will block until a send/recv is processed from rank 0. Rank 0 will block until all send /recv from other ranks are processed, and will report failures for ranks that failed to respond in time. Note that if one rank does not reach the monitored_barrier (for example due to a hang), all other ranks would fail in monitored_barrier.</p> <p>This collective will block all processes/ranks in the group, until the whole group exits the function successfully, making it useful for debugging and synchronizing. However, it can have a performance impact and should only be used for debugging or scenarios that require full synchronization points on the host-side. For debugging purposes, this barrier can be inserted before the applications collective calls to check if any ranks are desynchronized.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Note that this collective is only supported with the GLOO backend.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If <code>None</code>, the default process group will be used.</li> <li>\n<strong>timeout</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/datetime.html#datetime.timedelta\" title=\"(in Python v3.12)\">datetime.timedelta</a><em>, </em><em>optional</em>)  Timeout for monitored_barrier. If <code>None</code>, the default process group timeout will be used.</li> <li>\n<strong>wait_all_ranks</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Whether to collect all failed ranks or not. By default, this is <code>False</code> and <code>monitored_barrier</code> on rank 0 will throw on the first failed rank it encounters in order to fail fast. By setting <code>wait_all_ranks=True</code> <code>monitored_barrier</code> will collect all failed ranks and throw an error containing information about all failed ranks.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p><code>None</code>.</p> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; # Note: Process group initialization omitted on each rank.\n&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; if dist.get_rank() != 1:\n&gt;&gt;&gt;     dist.monitored_barrier() # Raises exception indicating that\n&gt;&gt;&gt; # rank 1 did not call into monitored_barrier.\n&gt;&gt;&gt; # Example with wait_all_ranks=True\n&gt;&gt;&gt; if dist.get_rank() == 0:\n&gt;&gt;&gt;     dist.monitored_barrier(wait_all_ranks=True) # Raises exception\n&gt;&gt;&gt; # indicating that ranks 1, 2, ... world_size - 1 did not call into\n&gt;&gt;&gt; # monitored_barrier.\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.ReduceOp\">\n<code>class torch.distributed.ReduceOp</code> </dt> <dd>\n<p>An enum-like class for available reduction operations: <code>SUM</code>, <code>PRODUCT</code>, <code>MIN</code>, <code>MAX</code>, <code>BAND</code>, <code>BOR</code>, <code>BXOR</code>, and <code>PREMUL_SUM</code>.</p> <p><code>BAND</code>, <code>BOR</code>, and <code>BXOR</code> reductions are not available when using the <code>NCCL</code> backend.</p> <p><code>AVG</code> divides values by the world size before summing across ranks. <code>AVG</code> is only available with the <code>NCCL</code> backend, and only for NCCL versions 2.10 or later.</p> <p><code>PREMUL_SUM</code> multiplies inputs by a given scalar locally before reduction. <code>PREMUL_SUM</code> is only available with the <code>NCCL</code> backend, and only available for NCCL versions 2.11 or later. Users are supposed to use <code>torch.distributed._make_nccl_premul_sum</code>.</p> <p>Additionally, <code>MAX</code>, <code>MIN</code> and <code>PRODUCT</code> are not supported for complex tensors.</p> <p>The values of this class can be accessed as attributes, e.g., <code>ReduceOp.SUM</code>. They are used in specifying strategies for reduction collectives, e.g., <a class=\"reference internal\" href=\"#torch.distributed.reduce\" title=\"torch.distributed.reduce\"><code>reduce()</code></a>, <a class=\"reference internal\" href=\"#torch.distributed.all_reduce_multigpu\" title=\"torch.distributed.all_reduce_multigpu\"><code>all_reduce_multigpu()</code></a>, etc.</p> <p>This class does not support <code>__members__</code> property.</p> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.reduce_op\">\n<code>class torch.distributed.reduce_op</code> </dt> <dd>\n<p>Deprecated enum-like class for reduction operations: <code>SUM</code>, <code>PRODUCT</code>, <code>MIN</code>, and <code>MAX</code>.</p> <p><a class=\"reference internal\" href=\"#torch.distributed.ReduceOp\" title=\"torch.distributed.ReduceOp\"><code>ReduceOp</code></a> is recommended to use instead.</p> </dd>\n</dl>   <h2 id=\"profiling-collective-communication\">Profiling Collective Communication</h2> <p>Note that you can use <code>torch.profiler</code> (recommended, only available after 1.8.1) or <code>torch.autograd.profiler</code> to profile collective communication and point-to-point communication APIs mentioned here. All out-of-the-box backends (<code>gloo</code>, <code>nccl</code>, <code>mpi</code>) are supported and collective communication usage will be rendered as expected in profiling output/traces. Profiling your code is the same as any regular torch operator:</p> <pre data-language=\"python\">import torch\nimport torch.distributed as dist\nwith torch.profiler():\n    tensor = torch.randn(20, 10)\n    dist.all_reduce(tensor)\n</pre> <p>Please refer to the <a class=\"reference external\" href=\"https://pytorch.org/docs/main/profiler.html\">profiler documentation</a> for a full overview of profiler features.</p>   <h2 id=\"multi-gpu-collective-functions\">Multi-GPU collective functions</h2> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>The multi-GPU functions will be deprecated. If you must use them, please revisit our documentation later.</p> </div> <p>If you have more than one GPU on each node, when using the NCCL and Gloo backend, <a class=\"reference internal\" href=\"#torch.distributed.broadcast_multigpu\" title=\"torch.distributed.broadcast_multigpu\"><code>broadcast_multigpu()</code></a> <a class=\"reference internal\" href=\"#torch.distributed.all_reduce_multigpu\" title=\"torch.distributed.all_reduce_multigpu\"><code>all_reduce_multigpu()</code></a> <a class=\"reference internal\" href=\"#torch.distributed.reduce_multigpu\" title=\"torch.distributed.reduce_multigpu\"><code>reduce_multigpu()</code></a> <a class=\"reference internal\" href=\"#torch.distributed.all_gather_multigpu\" title=\"torch.distributed.all_gather_multigpu\"><code>all_gather_multigpu()</code></a> and <a class=\"reference internal\" href=\"#torch.distributed.reduce_scatter_multigpu\" title=\"torch.distributed.reduce_scatter_multigpu\"><code>reduce_scatter_multigpu()</code></a> support distributed collective operations among multiple GPUs within each node. These functions can potentially improve the overall distributed training performance and be easily used by passing a list of tensors. Each Tensor in the passed tensor list needs to be on a separate GPU device of the host where the function is called. Note that the length of the tensor list needs to be identical among all the distributed processes. Also note that currently the multi-GPU collective functions are only supported by the NCCL backend.</p> <p>For example, if the system we use for distributed training has 2 nodes, each of which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would like to all-reduce. The following code can serve as a reference:</p> <p>Code running on Node 0</p> <pre data-language=\"python\">import torch\nimport torch.distributed as dist\n\ndist.init_process_group(backend=\"nccl\",\n                        init_method=\"file:///distributed_test\",\n                        world_size=2,\n                        rank=0)\ntensor_list = []\nfor dev_idx in range(torch.cuda.device_count()):\n    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\n\ndist.all_reduce_multigpu(tensor_list)\n</pre> <p>Code running on Node 1</p> <pre data-language=\"python\">import torch\nimport torch.distributed as dist\n\ndist.init_process_group(backend=\"nccl\",\n                        init_method=\"file:///distributed_test\",\n                        world_size=2,\n                        rank=1)\ntensor_list = []\nfor dev_idx in range(torch.cuda.device_count()):\n    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\n\ndist.all_reduce_multigpu(tensor_list)\n</pre> <p>After the call, all 16 tensors on the two nodes will have the all-reduced value of 16</p> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.broadcast_multigpu\">\n<code>torch.distributed.broadcast_multigpu(tensor_list, src, group=None, async_op=False, src_tensor=0)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#broadcast_multigpu\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Broadcasts the tensor to the whole group with multiple GPU tensors per node.</p> <p><code>tensor</code> must have the same number of elements in all the GPUs from all processes participating in the collective. each tensor in the list must be on a different GPU</p> <p>Only nccl and gloo backend are currently supported tensors should only be GPU tensors</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor_list</strong> (<em>List</em><em>[</em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>]</em>)  Tensors that participate in the collective operation. If <code>src</code> is the rank, then the specified <code>src_tensor</code> element of <code>tensor_list</code> (<code>tensor_list[src_tensor]</code>) will be broadcast to all other tensors (on different GPUs) in the src process and all tensors in <code>tensor_list</code> of other non-src processes. You also need to make sure that <code>len(tensor_list)</code> is the same for all the distributed processes calling this function.</li> <li>\n<strong>src</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a>)  Source rank.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>async_op</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Whether this op should be an async op</li> <li>\n<strong>src_tensor</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>, </em><em>optional</em>)  Source tensor rank within <code>tensor_list</code>\n</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group</p> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.all_reduce_multigpu\">\n<code>torch.distributed.all_reduce_multigpu(tensor_list, op=&lt;RedOpType.SUM: 0&gt;, group=None, async_op=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#all_reduce_multigpu\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Reduces the tensor data across all machines in such a way that all get the final result. This function reduces a number of tensors on every node, while each tensor resides on different GPUs. Therefore, the input tensor in the tensor list needs to be GPU tensors. Also, each tensor in the tensor list needs to reside on a different GPU.</p> <p>After the call, all <code>tensor</code> in <code>tensor_list</code> is going to be bitwise identical in all processes.</p> <p>Complex tensors are supported.</p> <p>Only nccl and gloo backend is currently supported tensors should only be GPU tensors</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor_list</strong> (<em>List</em><em>[</em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>]</em>)  List of input and output tensors of the collective. The function operates in-place and requires that each tensor to be a GPU tensor on different GPUs. You also need to make sure that <code>len(tensor_list)</code> is the same for all the distributed processes calling this function.</li> <li>\n<strong>op</strong> (<em>optional</em>)  One of the values from <code>torch.distributed.ReduceOp</code> enum. Specifies an operation used for element-wise reductions.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If <code>None</code>, the default process group will be used.</li> <li>\n<strong>async_op</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Whether this op should be an async op</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group</p> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.reduce_multigpu\">\n<code>torch.distributed.reduce_multigpu(tensor_list, dst, op=&lt;RedOpType.SUM: 0&gt;, group=None, async_op=False, dst_tensor=0)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#reduce_multigpu\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Reduces the tensor data on multiple GPUs across all machines. Each tensor in <code>tensor_list</code> should reside on a separate GPU</p> <p>Only the GPU of <code>tensor_list[dst_tensor]</code> on the process with rank <code>dst</code> is going to receive the final result.</p> <p>Only nccl backend is currently supported tensors should only be GPU tensors</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor_list</strong> (<em>List</em><em>[</em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>]</em>)  Input and output GPU tensors of the collective. The function operates in-place. You also need to make sure that <code>len(tensor_list)</code> is the same for all the distributed processes calling this function.</li> <li>\n<strong>dst</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a>)  Destination rank</li> <li>\n<strong>op</strong> (<em>optional</em>)  One of the values from <code>torch.distributed.ReduceOp</code> enum. Specifies an operation used for element-wise reductions.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>async_op</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Whether this op should be an async op</li> <li>\n<strong>dst_tensor</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a><em>, </em><em>optional</em>)  Destination tensor rank within <code>tensor_list</code>\n</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Async work handle, if async_op is set to True. None, otherwise</p> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.all_gather_multigpu\">\n<code>torch.distributed.all_gather_multigpu(output_tensor_lists, input_tensor_list, group=None, async_op=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#all_gather_multigpu\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Gathers tensors from the whole group in a list. Each tensor in <code>tensor_list</code> should reside on a separate GPU</p> <p>Only nccl backend is currently supported tensors should only be GPU tensors</p> <p>Complex tensors are supported.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<p><strong>output_tensor_lists</strong> (<em>List</em><em>[</em><em>List</em><em>[</em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>]</em><em>]</em>)  </p>\n<p>Output lists. It should contain correctly-sized tensors on each GPU to be used for output of the collective, e.g. <code>output_tensor_lists[i]</code> contains the all_gather result that resides on the GPU of <code>input_tensor_list[i]</code>.</p> <p>Note that each element of <code>output_tensor_lists</code> has the size of <code>world_size * len(input_tensor_list)</code>, since the function all gathers the result from every single GPU in the group. To interpret each element of <code>output_tensor_lists[i]</code>, note that <code>input_tensor_list[j]</code> of rank k will be appear in <code>output_tensor_lists[i][k * world_size + j]</code></p> <p>Also note that <code>len(output_tensor_lists)</code>, and the size of each element in <code>output_tensor_lists</code> (each element is a list, therefore <code>len(output_tensor_lists[i])</code>) need to be the same for all the distributed processes calling this function.</p> </li> <li>\n<strong>input_tensor_list</strong> (<em>List</em><em>[</em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>]</em>)  List of tensors(on different GPUs) to be broadcast from current process. Note that <code>len(input_tensor_list)</code> needs to be the same for all the distributed processes calling this function.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>async_op</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Whether this op should be an async op</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group</p> </dd> </dl> </dd>\n</dl> <dl class=\"py function\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.reduce_scatter_multigpu\">\n<code>torch.distributed.reduce_scatter_multigpu(output_tensor_list, input_tensor_lists, op=&lt;RedOpType.SUM: 0&gt;, group=None, async_op=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/distributed_c10d.html#reduce_scatter_multigpu\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Reduce and scatter a list of tensors to the whole group. Only nccl backend is currently supported.</p> <p>Each tensor in <code>output_tensor_list</code> should reside on a separate GPU, as should each list of tensors in <code>input_tensor_lists</code>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<p><strong>output_tensor_list</strong> (<em>List</em><em>[</em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>]</em>)  </p>\n<p>Output tensors (on different GPUs) to receive the result of the operation.</p> <p>Note that <code>len(output_tensor_list)</code> needs to be the same for all the distributed processes calling this function.</p> </li> <li>\n<p><strong>input_tensor_lists</strong> (<em>List</em><em>[</em><em>List</em><em>[</em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>]</em><em>]</em>)  </p>\n<p>Input lists. It should contain correctly-sized tensors on each GPU to be used for input of the collective, e.g. <code>input_tensor_lists[i]</code> contains the reduce_scatter input that resides on the GPU of <code>output_tensor_list[i]</code>.</p> <p>Note that each element of <code>input_tensor_lists</code> has the size of <code>world_size * len(output_tensor_list)</code>, since the function scatters the result from every single GPU in the group. To interpret each element of <code>input_tensor_lists[i]</code>, note that <code>output_tensor_list[j]</code> of rank k receives the reduce-scattered result from <code>input_tensor_lists[i][k * world_size + j]</code></p> <p>Also note that <code>len(input_tensor_lists)</code>, and the size of each element in <code>input_tensor_lists</code> (each element is a list, therefore <code>len(input_tensor_lists[i])</code>) need to be the same for all the distributed processes calling this function.</p> </li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>)  The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>async_op</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  Whether this op should be an async op.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group.</p> </dd> </dl> </dd>\n</dl>   <h2 id=\"distributed-launch\">Third-party backends</h2> <p id=\"third-party-backends\">Besides the builtin GLOO/MPI/NCCL backends, PyTorch distributed supports third-party backends through a run-time register mechanism. For references on how to develop a third-party backend through C++ Extension, please refer to <a class=\"reference external\" href=\"https://pytorch.org/tutorials/advanced/cpp_extension.html\">Tutorials - Custom C++ and CUDA Extensions</a> and <code>test/cpp_extensions/cpp_c10d_extension.cpp</code>. The capability of third-party backends are decided by their own implementations.</p> <p>The new backend derives from <code>c10d::ProcessGroup</code> and registers the backend name and the instantiating interface through <a class=\"reference internal\" href=\"#torch.distributed.Backend.register_backend\" title=\"torch.distributed.Backend.register_backend\"><code>torch.distributed.Backend.register_backend()</code></a> when imported.</p> <p>When manually importing this backend and invoking <a class=\"reference internal\" href=\"#torch.distributed.init_process_group\" title=\"torch.distributed.init_process_group\"><code>torch.distributed.init_process_group()</code></a> with the corresponding backend name, the <code>torch.distributed</code> package runs on the new backend.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>The support of third-party backend is experimental and subject to change.</p> </div>   <h2 id=\"launch-utility\">Launch utility</h2> <p>The <code>torch.distributed</code> package also provides a launch utility in <code>torch.distributed.launch</code>. This helper utility can be used to launch multiple processes per node for distributed training.</p> <p id=\"module-torch.distributed.launch\"><code>torch.distributed.launch</code> is a module that spawns up multiple distributed training processes on each of the training nodes.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This module is going to be deprecated in favor of <a class=\"reference internal\" href=\"elastic/run#launcher-api\"><span class=\"std std-ref\">torchrun</span></a>.</p> </div> <p>The utility can be used for single-node distributed training, in which one or more processes per node will be spawned. The utility can be used for either CPU training or GPU training. If the utility is used for GPU training, each distributed process will be operating on a single GPU. This can achieve well-improved single-node training performance. It can also be used in multi-node distributed training, by spawning up multiple processes on each node for well-improved multi-node distributed training performance as well. This will especially be beneficial for systems with multiple Infiniband interfaces that have direct-GPU support, since all of them can be utilized for aggregated communication bandwidth.</p> <p>In both cases of single-node distributed training or multi-node distributed training, this utility will launch the given number of processes per node (<code>--nproc-per-node</code>). If used for GPU training, this number needs to be less or equal to the number of GPUs on the current system (<code>nproc_per_node</code>), and each process will be operating on a single GPU from <em>GPU 0 to GPU (nproc_per_node - 1)</em>.</p> <p><strong>How to use this module:</strong></p> <ol class=\"arabic simple\"> <li>Single-Node multi-process distributed training</li> </ol> <pre data-language=\"python\">python -m torch.distributed.launch --nproc-per-node=NUM_GPUS_YOU_HAVE\n           YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other\n           arguments of your training script)\n</pre> <ol class=\"arabic simple\" start=\"2\"> <li>Multi-Node multi-process distributed training: (e.g. two nodes)</li> </ol> <p>Node 1: <em>(IP: 192.168.1.1, and has a free port: 1234)</em></p> <pre data-language=\"python\">python -m torch.distributed.launch --nproc-per-node=NUM_GPUS_YOU_HAVE\n           --nnodes=2 --node-rank=0 --master-addr=\"192.168.1.1\"\n           --master-port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n           and all other arguments of your training script)\n</pre> <p>Node 2:</p> <pre data-language=\"python\">python -m torch.distributed.launch --nproc-per-node=NUM_GPUS_YOU_HAVE\n           --nnodes=2 --node-rank=1 --master-addr=\"192.168.1.1\"\n           --master-port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n           and all other arguments of your training script)\n</pre> <ol class=\"arabic simple\" start=\"3\"> <li>To look up what optional arguments this module offers:</li> </ol> <pre data-language=\"python\">python -m torch.distributed.launch --help\n</pre> <p><strong>Important Notices:</strong></p> <p>1. This utility and multi-process distributed (single-node or multi-node) GPU training currently only achieves the best performance using the NCCL distributed backend. Thus NCCL backend is the recommended backend to use for GPU training.</p> <p>2. In your training program, you must parse the command-line argument: <code>--local-rank=LOCAL_PROCESS_RANK</code>, which will be provided by this module. If your training program uses GPUs, you should ensure that your code only runs on the GPU device of LOCAL_PROCESS_RANK. This can be done by:</p> <p>Parsing the local_rank argument</p> <pre data-language=\"python\">&gt;&gt;&gt; import argparse\n&gt;&gt;&gt; parser = argparse.ArgumentParser()\n&gt;&gt;&gt; parser.add_argument(\"--local-rank\", type=int)\n&gt;&gt;&gt; args = parser.parse_args()\n</pre> <p>Set your device to local rank using either</p> <pre data-language=\"python\">&gt;&gt;&gt; torch.cuda.set_device(args.local_rank)  # before your code runs\n</pre> <p>or</p> <pre data-language=\"python\">&gt;&gt;&gt; with torch.cuda.device(args.local_rank):\n&gt;&gt;&gt;    # your code to run\n&gt;&gt;&gt;    ...\n</pre> <p>3. In your training program, you are supposed to call the following function at the beginning to start the distributed backend. It is strongly recommended that <code>init_method=env://</code>. Other init methods (e.g. <code>tcp://</code>) may work, but <code>env://</code> is the one that is officially supported by this module.</p> <pre data-language=\"python\">&gt;&gt;&gt; torch.distributed.init_process_group(backend='YOUR BACKEND',\n&gt;&gt;&gt;                                      init_method='env://')\n</pre> <p>4. In your training program, you can either use regular distributed functions or use <a class=\"reference internal\" href=\"generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel\" title=\"torch.nn.parallel.DistributedDataParallel\"><code>torch.nn.parallel.DistributedDataParallel()</code></a> module. If your training program uses GPUs for training and you would like to use <a class=\"reference internal\" href=\"generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel\" title=\"torch.nn.parallel.DistributedDataParallel\"><code>torch.nn.parallel.DistributedDataParallel()</code></a> module, here is how to configure it.</p> <pre data-language=\"python\">&gt;&gt;&gt; model = torch.nn.parallel.DistributedDataParallel(model,\n&gt;&gt;&gt;                                                   device_ids=[args.local_rank],\n&gt;&gt;&gt;                                                   output_device=args.local_rank)\n</pre> <p>Please ensure that <code>device_ids</code> argument is set to be the only GPU device id that your code will be operating on. This is generally the local rank of the process. In other words, the <code>device_ids</code> needs to be <code>[args.local_rank]</code>, and <code>output_device</code> needs to be <code>args.local_rank</code> in order to use this utility</p> <p>5. Another way to pass <code>local_rank</code> to the subprocesses via environment variable <code>LOCAL_RANK</code>. This behavior is enabled when you launch the script with <code>--use-env=True</code>. You must adjust the subprocess example above to replace <code>args.local_rank</code> with <code>os.environ['LOCAL_RANK']</code>; the launcher will not pass <code>--local-rank</code> when you specify this flag.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><code>local_rank</code> is NOT globally unique: it is only unique per process on a machine. Thus, dont use it to decide if you should, e.g., write to a networked filesystem. See <a class=\"reference external\" href=\"https://github.com/pytorch/pytorch/issues/12042\">https://github.com/pytorch/pytorch/issues/12042</a> for an example of how things can go wrong if you dont do this correctly.</p> </div>   <h2 id=\"spawn-utility\">Spawn utility</h2> <p>The <a class=\"reference internal\" href=\"multiprocessing#multiprocessing-doc\"><span class=\"std std-ref\">Multiprocessing package - torch.multiprocessing</span></a> package also provides a <code>spawn</code> function in <a class=\"reference internal\" href=\"multiprocessing#torch.multiprocessing.spawn\" title=\"torch.multiprocessing.spawn\"><code>torch.multiprocessing.spawn()</code></a>. This helper function can be used to spawn multiple processes. It works by passing in the function that you want to run and spawns N processes to run it. This can be used for multiprocess distributed training as well.</p> <p>For references on how to use it, please refer to <a class=\"reference external\" href=\"https://github.com/pytorch/examples/tree/master/imagenet\">PyTorch example - ImageNet implementation</a></p> <p>Note that this function requires Python 3.4 or higher.</p>   <h2 id=\"debugging-torch-distributed-applications\">Debugging <code>torch.distributed</code> applications</h2> <p>Debugging distributed applications can be challenging due to hard to understand hangs, crashes, or inconsistent behavior across ranks. <code>torch.distributed</code> provides a suite of tools to help debug training applications in a self-serve fashion:</p>  <h3 id=\"monitored-barrier\">Monitored Barrier</h3> <p>As of v1.10, <a class=\"reference internal\" href=\"#torch.distributed.monitored_barrier\" title=\"torch.distributed.monitored_barrier\"><code>torch.distributed.monitored_barrier()</code></a> exists as an alternative to <a class=\"reference internal\" href=\"#torch.distributed.barrier\" title=\"torch.distributed.barrier\"><code>torch.distributed.barrier()</code></a> which fails with helpful information about which rank may be faulty when crashing, i.e. not all ranks calling into <a class=\"reference internal\" href=\"#torch.distributed.monitored_barrier\" title=\"torch.distributed.monitored_barrier\"><code>torch.distributed.monitored_barrier()</code></a> within the provided timeout. <a class=\"reference internal\" href=\"#torch.distributed.monitored_barrier\" title=\"torch.distributed.monitored_barrier\"><code>torch.distributed.monitored_barrier()</code></a> implements a host-side barrier using <code>send</code>/<code>recv</code> communication primitives in a process similar to acknowledgements, allowing rank 0 to report which rank(s) failed to acknowledge the barrier in time. As an example, consider the following function where rank 1 fails to call into <a class=\"reference internal\" href=\"#torch.distributed.monitored_barrier\" title=\"torch.distributed.monitored_barrier\"><code>torch.distributed.monitored_barrier()</code></a> (in practice this could be due to an application bug or hang in a previous collective):</p> <pre data-language=\"python\">import os\nfrom datetime import timedelta\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\n\ndef worker(rank):\n    dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n    # monitored barrier requires gloo process group to perform host-side sync.\n    group_gloo = dist.new_group(backend=\"gloo\")\n    if rank not in [1]:\n        dist.monitored_barrier(group=group_gloo, timeout=timedelta(seconds=2))\n\n\nif __name__ == \"__main__\":\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29501\"\n    mp.spawn(worker, nprocs=2, args=())\n</pre> <p>The following error message is produced on rank 0, allowing the user to determine which rank(s) may be faulty and investigate further:</p> <pre data-language=\"python\">RuntimeError: Rank 1 failed to pass monitoredBarrier in 2000 ms\n Original exception:\n[gloo/transport/tcp/pair.cc:598] Connection closed by peer [2401:db00:eef0:1100:3560:0:1c05:25d]:8594\n</pre>   <h3 id=\"torch-distributed-debug\"><code>TORCH_DISTRIBUTED_DEBUG</code></h3> <p>With <code>TORCH_CPP_LOG_LEVEL=INFO</code>, the environment variable <code>TORCH_DISTRIBUTED_DEBUG</code> can be used to trigger additional useful logging and collective synchronization checks to ensure all ranks are synchronized appropriately. <code>TORCH_DISTRIBUTED_DEBUG</code> can be set to either <code>OFF</code> (default), <code>INFO</code>, or <code>DETAIL</code> depending on the debugging level required. Please note that the most verbose option, <code>DETAIL</code> may impact the application performance and thus should only be used when debugging issues.</p> <p>Setting <code>TORCH_DISTRIBUTED_DEBUG=INFO</code> will result in additional debug logging when models trained with <a class=\"reference internal\" href=\"generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel\" title=\"torch.nn.parallel.DistributedDataParallel\"><code>torch.nn.parallel.DistributedDataParallel()</code></a> are initialized, and <code>TORCH_DISTRIBUTED_DEBUG=DETAIL</code> will additionally log runtime performance statistics a select number of iterations. These runtime statistics include data such as forward time, backward time, gradient communication time, etc. As an example, given the following application:</p> <pre data-language=\"python\">import os\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\n\nclass TwoLinLayerNet(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.a = torch.nn.Linear(10, 10, bias=False)\n        self.b = torch.nn.Linear(10, 1, bias=False)\n\n    def forward(self, x):\n        a = self.a(x)\n        b = self.b(x)\n        return (a, b)\n\n\ndef worker(rank):\n    dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n    torch.cuda.set_device(rank)\n    print(\"init model\")\n    model = TwoLinLayerNet().cuda()\n    print(\"init ddp\")\n    ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n\n    inp = torch.randn(10, 10).cuda()\n    print(\"train\")\n\n    for _ in range(20):\n        output = ddp_model(inp)\n        loss = output[0] + output[1]\n        loss.sum().backward()\n\n\nif __name__ == \"__main__\":\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29501\"\n    os.environ[\"TORCH_CPP_LOG_LEVEL\"]=\"INFO\"\n    os.environ[\n        \"TORCH_DISTRIBUTED_DEBUG\"\n    ] = \"DETAIL\"  # set to DETAIL for runtime logging.\n    mp.spawn(worker, nprocs=2, args=())\n</pre> <p>The following logs are rendered at initialization time:</p> <pre data-language=\"python\">I0607 16:10:35.739390 515217 logger.cpp:173] [Rank 0]: DDP Initialized with:\nbroadcast_buffers: 1\nbucket_cap_bytes: 26214400\nfind_unused_parameters: 0\ngradient_as_bucket_view: 0\nis_multi_device_module: 0\niteration: 0\nnum_parameter_tensors: 2\noutput_device: 0\nrank: 0\ntotal_parameter_size_bytes: 440\nworld_size: 2\nbackend_name: nccl\nbucket_sizes: 440\ncuda_visible_devices: N/A\ndevice_ids: 0\ndtypes: float\nmaster_addr: localhost\nmaster_port: 29501\nmodule_name: TwoLinLayerNet\nnccl_async_error_handling: N/A\nnccl_blocking_wait: N/A\nnccl_debug: WARN\nnccl_ib_timeout: N/A\nnccl_nthreads: N/A\nnccl_socket_ifname: N/A\ntorch_distributed_debug: INFO\n</pre> <p>The following logs are rendered during runtime (when <code>TORCH_DISTRIBUTED_DEBUG=DETAIL</code> is set):</p> <pre data-language=\"python\">I0607 16:18:58.085681 544067 logger.cpp:344] [Rank 1 / 2] Training TwoLinLayerNet unused_parameter_size=0\n Avg forward compute time: 40838608\n Avg backward compute time: 5983335\nAvg backward comm. time: 4326421\n Avg backward comm/comp overlap time: 4207652\nI0607 16:18:58.085693 544066 logger.cpp:344] [Rank 0 / 2] Training TwoLinLayerNet unused_parameter_size=0\n Avg forward compute time: 42850427\n Avg backward compute time: 3885553\nAvg backward comm. time: 2357981\n Avg backward comm/comp overlap time: 2234674\n</pre> <p>In addition, <code>TORCH_DISTRIBUTED_DEBUG=INFO</code> enhances crash logging in <a class=\"reference internal\" href=\"generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel\" title=\"torch.nn.parallel.DistributedDataParallel\"><code>torch.nn.parallel.DistributedDataParallel()</code></a> due to unused parameters in the model. Currently, <code>find_unused_parameters=True</code> must be passed into <a class=\"reference internal\" href=\"generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel\" title=\"torch.nn.parallel.DistributedDataParallel\"><code>torch.nn.parallel.DistributedDataParallel()</code></a> initialization if there are parameters that may be unused in the forward pass, and as of v1.10, all model outputs are required to be used in loss computation as <a class=\"reference internal\" href=\"generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel\" title=\"torch.nn.parallel.DistributedDataParallel\"><code>torch.nn.parallel.DistributedDataParallel()</code></a> does not support unused parameters in the backwards pass. These constraints are challenging especially for larger models, thus when crashing with an error, <a class=\"reference internal\" href=\"generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel\" title=\"torch.nn.parallel.DistributedDataParallel\"><code>torch.nn.parallel.DistributedDataParallel()</code></a> will log the fully qualified name of all parameters that went unused. For example, in the above application, if we modify <code>loss</code> to be instead computed as <code>loss = output[1]</code>, then <code>TwoLinLayerNet.a</code> does not receive a gradient in the backwards pass, and thus results in <code>DDP</code> failing. On a crash, the user is passed information about parameters which went unused, which may be challenging to manually find for large models:</p> <pre data-language=\"python\">RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing\n the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by\nmaking sure all `forward` function outputs participate in calculating loss.\nIf you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return va\nlue of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\nParameters which did not receive grad for rank 0: a.weight\nParameter indices which did not receive grad for rank 0: 0\n</pre> <p>Setting <code>TORCH_DISTRIBUTED_DEBUG=DETAIL</code> will trigger additional consistency and synchronization checks on every collective call issued by the user either directly or indirectly (such as DDP <code>allreduce</code>). This is done by creating a wrapper process group that wraps all process groups returned by <a class=\"reference internal\" href=\"#torch.distributed.init_process_group\" title=\"torch.distributed.init_process_group\"><code>torch.distributed.init_process_group()</code></a> and <a class=\"reference internal\" href=\"#torch.distributed.new_group\" title=\"torch.distributed.new_group\"><code>torch.distributed.new_group()</code></a> APIs. As a result, these APIs will return a wrapper process group that can be used exactly like a regular process group, but performs consistency checks before dispatching the collective to an underlying process group. Currently, these checks include a <a class=\"reference internal\" href=\"#torch.distributed.monitored_barrier\" title=\"torch.distributed.monitored_barrier\"><code>torch.distributed.monitored_barrier()</code></a>, which ensures all ranks complete their outstanding collective calls and reports ranks which are stuck. Next, the collective itself is checked for consistency by ensuring all collective functions match and are called with consistent tensor shapes. If this is not the case, a detailed error report is included when the application crashes, rather than a hang or uninformative error message. As an example, consider the following function which has mismatched input shapes into <a class=\"reference internal\" href=\"#torch.distributed.all_reduce\" title=\"torch.distributed.all_reduce\"><code>torch.distributed.all_reduce()</code></a>:</p> <pre data-language=\"python\">import torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\n\ndef worker(rank):\n    dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n    torch.cuda.set_device(rank)\n    tensor = torch.randn(10 if rank == 0 else 20).cuda()\n    dist.all_reduce(tensor)\n    torch.cuda.synchronize(device=rank)\n\n\nif __name__ == \"__main__\":\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29501\"\n    os.environ[\"TORCH_CPP_LOG_LEVEL\"]=\"INFO\"\n    os.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\"\n    mp.spawn(worker, nprocs=2, args=())\n</pre> <p>With the <code>NCCL</code> backend, such an application would likely result in a hang which can be challenging to root-cause in nontrivial scenarios. If the user enables <code>TORCH_DISTRIBUTED_DEBUG=DETAIL</code> and reruns the application, the following error message reveals the root cause:</p> <pre data-language=\"python\">work = default_pg.allreduce([tensor], opts)\nRuntimeError: Error when verifying shape tensors for collective ALLREDUCE on rank 0. This likely indicates that input shapes into the collective are mismatched across ranks. Got shapes:  10\n[ torch.LongTensor{1} ]\n</pre> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>For fine-grained control of the debug level during runtime the functions <code>torch.distributed.set_debug_level()</code>, <code>torch.distributed.set_debug_level_from_env()</code>, and <code>torch.distributed.get_debug_level()</code> can also be used.</p> </div> <p>In addition, <code>TORCH_DISTRIBUTED_DEBUG=DETAIL</code> can be used in conjunction with <code>TORCH_SHOW_CPP_STACKTRACES=1</code> to log the entire callstack when a collective desynchronization is detected. These collective desynchronization checks will work for all applications that use <code>c10d</code> collective calls backed by process groups created with the <a class=\"reference internal\" href=\"#torch.distributed.init_process_group\" title=\"torch.distributed.init_process_group\"><code>torch.distributed.init_process_group()</code></a> and <a class=\"reference internal\" href=\"#torch.distributed.new_group\" title=\"torch.distributed.new_group\"><code>torch.distributed.new_group()</code></a> APIs.</p>    <h2 id=\"logging\">Logging</h2> <p>In addition to explicit debugging support via <a class=\"reference internal\" href=\"#torch.distributed.monitored_barrier\" title=\"torch.distributed.monitored_barrier\"><code>torch.distributed.monitored_barrier()</code></a> and <code>TORCH_DISTRIBUTED_DEBUG</code>, the underlying C++ library of <code>torch.distributed</code> also outputs log messages at various levels. These messages can be helpful to understand the execution state of a distributed training job and to troubleshoot problems such as network connection failures. The following matrix shows how the log level can be adjusted via the combination of <code>TORCH_CPP_LOG_LEVEL</code> and <code>TORCH_DISTRIBUTED_DEBUG</code> environment variables.</p> <table class=\"docutils colwidths-auto align-default\"> <thead> <tr>\n<th class=\"head\"><p><code>TORCH_CPP_LOG_LEVEL</code></p></th> <th class=\"head\"><p><code>TORCH_DISTRIBUTED_DEBUG</code></p></th> <th class=\"head\"><p>Effective Log Level</p></th> </tr> </thead>  <tr>\n<td><p><code>ERROR</code></p></td> <td><p>ignored</p></td> <td><p>Error</p></td> </tr> <tr>\n<td><p><code>WARNING</code></p></td> <td><p>ignored</p></td> <td><p>Warning</p></td> </tr> <tr>\n<td><p><code>INFO</code></p></td> <td><p>ignored</p></td> <td><p>Info</p></td> </tr> <tr>\n<td><p><code>INFO</code></p></td> <td><p><code>INFO</code></p></td> <td><p>Debug</p></td> </tr> <tr>\n<td><p><code>INFO</code></p></td> <td><p><code>DETAIL</code></p></td> <td><p>Trace (a.k.a. All)</p></td> </tr>  </table> <p>Distributed has a custom Exception type derived from <code>RuntimeError</code> called <code>torch.distributed.DistBackendError</code>. This exception is thrown when a backend-specific error occurs. For example, if the <code>NCCL</code> backend is used and the user attempts to use a GPU that is not available to the <code>NCCL</code> library.</p> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.DistBackendError\">\n<code>class torch.distributed.DistBackendError</code> </dt> <dd>\n<p>Exception raised when a backend error occurs in distributed</p> </dd>\n</dl> <div class=\"admonition warning\" id=\"module-torch.distributed.tensor\"> <p class=\"admonition-title\">Warning</p> <p>The DistBackendError exception type is an experimental feature is subject to change.</p> </div><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href=\"https://github.com/pytorch/pytorch/blob/main/LICENSE\">LICENSE</a> file.<br>\n    <a href=\"https://pytorch.org/docs/2.1/distributed.html\" class=\"_attribution-link\">https://pytorch.org/docs/2.1/distributed.html</a>\n  </p>\n</div>\n","distributed.algorithms.join":"<h1 id=\"generic-join-context-manager\">Generic Join Context Manager</h1> <p>The generic join context manager facilitates distributed training on uneven inputs. This page outlines the API of the relevant classes: <code>Join</code>, <code>Joinable</code>, and <code>JoinHook</code>. For a tutorial, see <a class=\"reference external\" href=\"https://pytorch.org/tutorials/advanced/generic_join.html\">Distributed Training with Uneven Inputs Using the Join Context Manager</a>.</p> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.algorithms.Join\">\n<code>class torch.distributed.algorithms.Join(joinables, enable=True, throw_on_early_termination=False, **kwargs)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/algorithms/join.html#Join\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>This class defines the generic join context manager, which allows custom hooks to be called after a process joins. These hooks should shadow the collective communications of non-joined processes to prevent hanging and erroring and to ensure algorithmic correctness. Refer to <a class=\"reference internal\" href=\"#torch.distributed.algorithms.JoinHook\" title=\"torch.distributed.algorithms.JoinHook\"><code>JoinHook</code></a> for details about the hook definition.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>The context manager requires each participating <a class=\"reference internal\" href=\"#torch.distributed.algorithms.Joinable\" title=\"torch.distributed.algorithms.Joinable\"><code>Joinable</code></a> to call the method <a class=\"reference internal\" href=\"#torch.distributed.algorithms.Join.notify_join_context\" title=\"torch.distributed.algorithms.Join.notify_join_context\"><code>notify_join_context()</code></a> before its own per- iteration collective communications to ensure correctness.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>The context manager requires that all <code>process_group</code> attributes in the <a class=\"reference internal\" href=\"#torch.distributed.algorithms.JoinHook\" title=\"torch.distributed.algorithms.JoinHook\"><code>JoinHook</code></a> objects are the same. If there are multiple <a class=\"reference internal\" href=\"#torch.distributed.algorithms.JoinHook\" title=\"torch.distributed.algorithms.JoinHook\"><code>JoinHook</code></a> objects, then the <code>device</code> of the first is used. The process group and device information is used for checking for non- joined processes and for notifying processes to throw an exception if <code>throw_on_early_termination</code> is enabled, both of which using an all- reduce.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>joinables</strong> (<em>List</em><em>[</em><a class=\"reference internal\" href=\"#torch.distributed.algorithms.Joinable\" title=\"torch.distributed.algorithms.Joinable\">Joinable</a><em>]</em>)  a list of the participating <a class=\"reference internal\" href=\"#torch.distributed.algorithms.Joinable\" title=\"torch.distributed.algorithms.Joinable\"><code>Joinable</code></a> s; their hooks are iterated over in the given order.</li> <li>\n<strong>enable</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a>)  a flag enabling uneven input detection; setting to <code>False</code> disables the context managers functionality and should only be set when the user knows the inputs will not be uneven (default: <code>True</code>).</li> <li>\n<strong>throw_on_early_termination</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a>)  a flag controlling whether to throw an exception upon detecting uneven inputs (default: <code>False</code>).</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; import os\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; import torch.multiprocessing as mp\n&gt;&gt;&gt; import torch.nn.parallel.DistributedDataParallel as DDP\n&gt;&gt;&gt; import torch.distributed.optim.ZeroRedundancyOptimizer as ZeRO\n&gt;&gt;&gt; from torch.distributed.algorithms.join import Join\n&gt;&gt;&gt;\n&gt;&gt;&gt; # On each spawned worker\n&gt;&gt;&gt; def worker(rank):\n&gt;&gt;&gt;     dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n&gt;&gt;&gt;     model = DDP(torch.nn.Linear(1, 1).to(rank), device_ids=[rank])\n&gt;&gt;&gt;     optim = ZeRO(model.parameters(), torch.optim.Adam, lr=0.01)\n&gt;&gt;&gt;     # Rank 1 gets one more input than rank 0\n&gt;&gt;&gt;     inputs = [torch.tensor([1.]).to(rank) for _ in range(10 + rank)]\n&gt;&gt;&gt;     with Join([model, optim]):\n&gt;&gt;&gt;         for input in inputs:\n&gt;&gt;&gt;             loss = model(input).sum()\n&gt;&gt;&gt;             loss.backward()\n&gt;&gt;&gt;             optim.step()\n&gt;&gt;&gt;     # All ranks reach here without hanging/erroring\n</pre> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.algorithms.Join.notify_join_context\">\n<code>static notify_join_context(joinable)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/algorithms/join.html#Join.notify_join_context\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Notifies the join context manager that the calling process has not yet joined; then, if <code>throw_on_early_termination=True</code>, checks if uneven inputs have been detected (i.e. if one process has already joined) and throws an exception if so.</p> <p>This method should be called from a <a class=\"reference internal\" href=\"#torch.distributed.algorithms.Joinable\" title=\"torch.distributed.algorithms.Joinable\"><code>Joinable</code></a> object before its per-iteration collective communications. For example, this should be called at the beginning of the forward pass in <code>DistributedDataParallel</code>.</p> <p>Only the first <a class=\"reference internal\" href=\"#torch.distributed.algorithms.Joinable\" title=\"torch.distributed.algorithms.Joinable\"><code>Joinable</code></a> object passed into the context manager performs the collective communications in this method, and for the others, this method is vacuous.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>joinable</strong> (<a class=\"reference internal\" href=\"#torch.distributed.algorithms.Joinable\" title=\"torch.distributed.algorithms.Joinable\">Joinable</a>)  the <a class=\"reference internal\" href=\"#torch.distributed.algorithms.Joinable\" title=\"torch.distributed.algorithms.Joinable\"><code>Joinable</code></a> object calling this method.</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>An async work handle for the all-reduce meant to notify the context manager that the process has not yet joined if <code>joinable</code> is the first one passed into the context manager; <code>None</code> otherwise.</p> </dd> </dl> </dd>\n</dl> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.algorithms.Joinable\">\n<code>class torch.distributed.algorithms.Joinable</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/algorithms/join.html#Joinable\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>This defines an abstract base class for joinable classes. A joinable class (inheriting from <a class=\"reference internal\" href=\"#torch.distributed.algorithms.Joinable\" title=\"torch.distributed.algorithms.Joinable\"><code>Joinable</code></a>) should implement <a class=\"reference internal\" href=\"#torch.distributed.algorithms.Joinable.join_hook\" title=\"torch.distributed.algorithms.Joinable.join_hook\"><code>join_hook()</code></a>, which returns a <a class=\"reference internal\" href=\"#torch.distributed.algorithms.JoinHook\" title=\"torch.distributed.algorithms.JoinHook\"><code>JoinHook</code></a> instance, in addition to <a class=\"reference internal\" href=\"#torch.distributed.algorithms.Joinable.join_device\" title=\"torch.distributed.algorithms.Joinable.join_device\"><code>join_device()</code></a> and <a class=\"reference internal\" href=\"#torch.distributed.algorithms.Joinable.join_process_group\" title=\"torch.distributed.algorithms.Joinable.join_process_group\"><code>join_process_group()</code></a> that return device and process group information, respectively.</p> <dl class=\"py property\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.algorithms.Joinable.join_device\">\n<code>abstract property join_device: device</code> </dt> <dd>\n<p>Returns the device from which to perform collective communications needed by the join context manager implementation itself.</p> </dd>\n</dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.algorithms.Joinable.join_hook\">\n<code>abstract join_hook(**kwargs)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/algorithms/join.html#Joinable.join_hook\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns a <a class=\"reference internal\" href=\"#torch.distributed.algorithms.JoinHook\" title=\"torch.distributed.algorithms.JoinHook\"><code>JoinHook</code></a> instance for the given <a class=\"reference internal\" href=\"#torch.distributed.algorithms.Joinable\" title=\"torch.distributed.algorithms.Joinable\"><code>Joinable</code></a>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>kwargs</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#dict\" title=\"(in Python v3.12)\">dict</a>)  a <a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#dict\" title=\"(in Python v3.12)\"><code>dict</code></a> containing any keyword arguments to modify the behavior of the join hook at run time; all <a class=\"reference internal\" href=\"#torch.distributed.algorithms.Joinable\" title=\"torch.distributed.algorithms.Joinable\"><code>Joinable</code></a> instances sharing the same join context manager are forwarded the same value for <code>kwargs</code>.</p> </dd> <dt class=\"field-even\">Return type</dt> <dd class=\"field-even\">\n<p><a class=\"reference internal\" href=\"#torch.distributed.algorithms.JoinHook\" title=\"torch.distributed.algorithms.join.JoinHook\">JoinHook</a></p> </dd> </dl> </dd>\n</dl> <dl class=\"py property\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.algorithms.Joinable.join_process_group\">\n<code>abstract property join_process_group: Any</code> </dt> <dd>\n<p>Returns the process group for the collective communications needed by the join context manager itself.</p> </dd>\n</dl> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.algorithms.JoinHook\">\n<code>class torch.distributed.algorithms.JoinHook</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/algorithms/join.html#JoinHook\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>This defines a join hook, which provides two entry points in the join context manager: a main hook, which is called repeatedly while there exists a non-joined process, and a post-hook, which is called once all processes have joined.</p> <p>To implement a join hook for the generic join context manager, define a class that inherits from <a class=\"reference internal\" href=\"#torch.distributed.algorithms.JoinHook\" title=\"torch.distributed.algorithms.JoinHook\"><code>JoinHook</code></a> and override <code>main_hook()</code> and <code>post_hook()</code> as appropriate.</p> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.algorithms.JoinHook.main_hook\">\n<code>main_hook()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/algorithms/join.html#JoinHook.main_hook\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>This hook is called repeatedly while there exists a non-joined process to shadow collective communications in one training iteration (i.e. in one forward pass, backward pass, and optimizer step).</p>  </dd>\n</dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.algorithms.JoinHook.post_hook\">\n<code>post_hook(is_last_joiner)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/algorithms/join.html#JoinHook.post_hook\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>This hook is called after all processes have joined. It is passed an additional <code>bool</code> argument <code>is_last_joiner</code>, which indicates if the rank is one of the last to join.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>is_last_joiner</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a>)  <code>True</code> if the rank is one of the last to join; <code>False</code> otherwise.</p> </dd> </dl> </dd>\n</dl> </dd>\n</dl><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href=\"https://github.com/pytorch/pytorch/blob/main/LICENSE\">LICENSE</a> file.<br>\n    <a href=\"https://pytorch.org/docs/2.1/distributed.algorithms.join.html\" class=\"_attribution-link\">https://pytorch.org/docs/2.1/distributed.algorithms.join.html</a>\n  </p>\n</div>\n","distributed.optim":"<h1 id=\"distributed-optimizers\">Distributed Optimizers</h1> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Distributed optimizer is not currently supported when using CUDA tensors</p> </div> <p id=\"module-torch.distributed.optim\"><a class=\"reference internal\" href=\"#module-torch.distributed.optim\" title=\"torch.distributed.optim\"><code>torch.distributed.optim</code></a> exposes DistributedOptimizer, which takes a list of remote parameters (<code>RRef</code>) and runs the optimizer locally on the workers where the parameters live. The distributed optimizer can use any of the local optimizer <a class=\"reference internal\" href=\"optim#optimizer-algorithms\"><span class=\"std std-ref\">Base class</span></a> to apply the gradients on each worker.</p> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.optim.DistributedOptimizer\">\n<code>class torch.distributed.optim.DistributedOptimizer(optimizer_class, params_rref, *args, **kwargs)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/optim/optimizer.html#DistributedOptimizer\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>DistributedOptimizer takes remote references to parameters scattered across workers and applies the given optimizer locally for each parameter.</p> <p>This class uses <a class=\"reference internal\" href=\"rpc#torch.distributed.autograd.get_gradients\" title=\"torch.distributed.autograd.get_gradients\"><code>get_gradients()</code></a> in order to retrieve the gradients for specific parameters.</p> <p>Concurrent calls to <a class=\"reference internal\" href=\"#torch.distributed.optim.DistributedOptimizer.step\" title=\"torch.distributed.optim.DistributedOptimizer.step\"><code>step()</code></a>, either from the same or different clients, will be serialized on each worker  as each workers optimizer can only work on one set of gradients at a time. However, there is no guarantee that the full forward-backward-optimizer sequence will execute for one client at a time. This means that the gradients being applied may not correspond to the latest forward pass executed on a given worker. Also, there is no guaranteed ordering across workers.</p> <p><code>DistributedOptimizer</code> creates the local optimizer with TorchScript enabled by default, so that optimizer updates are not blocked by the Python Global Interpreter Lock (GIL) in the case of multithreaded training (e.g. Distributed Model Parallel). This feature is currently enabled for most optimizers. You can also follow <a class=\"reference external\" href=\"https://github.com/pytorch/tutorials/pull/1465\">the recipe</a> in PyTorch tutorials to enable TorchScript support for your own custom optimizers.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>optimizer_class</strong> (<a class=\"reference internal\" href=\"optim#torch.optim.Optimizer\" title=\"torch.optim.Optimizer\">optim.Optimizer</a>)  the class of optimizer to instantiate on each worker.</li> <li>\n<strong>params_rref</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.12)\">list</a><em>[</em><em>RRef</em><em>]</em>)  list of RRefs to local or remote parameters to optimize.</li> <li>\n<strong>args</strong>  arguments to pass to the optimizer constructor on each worker.</li> <li>\n<strong>kwargs</strong>  arguments to pass to the optimizer constructor on each worker.</li> </ul> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch.distributed.autograd as dist_autograd\n&gt;&gt;&gt; import torch.distributed.rpc as rpc\n&gt;&gt;&gt; from torch import optim\n&gt;&gt;&gt; from torch.distributed.optim import DistributedOptimizer\n&gt;&gt;&gt;\n&gt;&gt;&gt; with dist_autograd.context() as context_id:\n&gt;&gt;&gt;   # Forward pass.\n&gt;&gt;&gt;   rref1 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\n&gt;&gt;&gt;   rref2 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 1))\n&gt;&gt;&gt;   loss = rref1.to_here() + rref2.to_here()\n&gt;&gt;&gt;\n&gt;&gt;&gt;   # Backward pass.\n&gt;&gt;&gt;   dist_autograd.backward(context_id, [loss.sum()])\n&gt;&gt;&gt;\n&gt;&gt;&gt;   # Optimizer.\n&gt;&gt;&gt;   dist_optim = DistributedOptimizer(\n&gt;&gt;&gt;      optim.SGD,\n&gt;&gt;&gt;      [rref1, rref2],\n&gt;&gt;&gt;      lr=0.05,\n&gt;&gt;&gt;   )\n&gt;&gt;&gt;   dist_optim.step(context_id)\n</pre> </dd> </dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.optim.DistributedOptimizer.step\">\n<code>step(context_id)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/optim/optimizer.html#DistributedOptimizer.step\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Performs a single optimization step.</p> <p>This will call <a class=\"reference internal\" href=\"generated/torch.optim.optimizer.step#torch.optim.Optimizer.step\" title=\"torch.optim.Optimizer.step\"><code>torch.optim.Optimizer.step()</code></a> on each worker containing parameters to be optimized, and will block until all workers return. The provided <code>context_id</code> will be used to retrieve the corresponding <a class=\"reference internal\" href=\"rpc#torch.distributed.autograd.context\" title=\"torch.distributed.autograd.context\"><code>context</code></a> that contains the gradients that should be applied to the parameters.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>context_id</strong>  the autograd context id for which we should run the optimizer step.</p> </dd> </dl> </dd>\n</dl> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.optim.PostLocalSGDOptimizer\">\n<code>class torch.distributed.optim.PostLocalSGDOptimizer(optim, averager)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/optim/post_localSGD_optimizer.html#PostLocalSGDOptimizer\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Wraps an arbitrary <a class=\"reference internal\" href=\"optim#torch.optim.Optimizer\" title=\"torch.optim.Optimizer\"><code>torch.optim.Optimizer</code></a> and runs <a class=\"reference external\" href=\"https://arxiv.org/abs/1808.07217\">post-local SGD</a>, This optimizer runs local optimizer at every step. After the warm-up stage, it averages parameters periodically afer the local optimizer is applied.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>optim</strong> (<a class=\"reference internal\" href=\"optim#torch.optim.Optimizer\" title=\"torch.optim.optimizer.Optimizer\">Optimizer</a>)  The local optimizer.</li> <li>\n<strong>averager</strong> (<em>ModelAverager</em>)  A model averager instance to run post-localSGD algorithm.</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; import torch\n&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; import torch.distributed.algorithms.model_averaging.averagers as averagers\n&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt; from torch.distributed.optim import PostLocalSGDOptimizer\n&gt;&gt;&gt; from torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook import (\n&gt;&gt;&gt;   PostLocalSGDState,\n&gt;&gt;&gt;   post_localSGD_hook,\n&gt;&gt;&gt; )\n&gt;&gt;&gt;\n&gt;&gt;&gt; model = nn.parallel.DistributedDataParallel(\n&gt;&gt;&gt;    module, device_ids=[rank], output_device=rank\n&gt;&gt;&gt; )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Register a post-localSGD communication hook.\n&gt;&gt;&gt; state = PostLocalSGDState(process_group=None, subgroup=None, start_localSGD_iter=100)\n&gt;&gt;&gt; model.register_comm_hook(state, post_localSGD_hook)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create a post-localSGD optimizer that wraps a local optimizer.\n&gt;&gt;&gt; # Note that ``warmup_steps`` used in ``PostLocalSGDOptimizer`` must be the same as\n&gt;&gt;&gt; # ``start_localSGD_iter`` used in ``PostLocalSGDState``.\n&gt;&gt;&gt; local_optim = torch.optim.SGD(params=model.parameters(), lr=0.01)\n&gt;&gt;&gt; opt = PostLocalSGDOptimizer(\n&gt;&gt;&gt;     optim=local_optim,\n&gt;&gt;&gt;     averager=averagers.PeriodicModelAverager(period=4, warmup_steps=100)\n&gt;&gt;&gt; )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # In the first 100 steps, DDP runs global gradient averaging at every step.\n&gt;&gt;&gt; # After 100 steps, DDP runs gradient averaging within each subgroup (intra-node by default),\n&gt;&gt;&gt; # and post-localSGD optimizer runs global model averaging every 4 steps after applying the local optimizer.\n&gt;&gt;&gt; for step in range(0, 200):\n&gt;&gt;&gt;    opt.zero_grad()\n&gt;&gt;&gt;    loss = loss_fn(output, labels)\n&gt;&gt;&gt;    loss.backward()\n&gt;&gt;&gt;    opt.step()\n</pre> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.optim.PostLocalSGDOptimizer.load_state_dict\">\n<code>load_state_dict(state_dict)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/optim/post_localSGD_optimizer.html#PostLocalSGDOptimizer.load_state_dict\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>This is the same as <a class=\"reference internal\" href=\"optim#torch.optim.Optimizer\" title=\"torch.optim.Optimizer\"><code>torch.optim.Optimizer</code></a> <a class=\"reference internal\" href=\"#torch.distributed.optim.PostLocalSGDOptimizer.load_state_dict\" title=\"torch.distributed.optim.PostLocalSGDOptimizer.load_state_dict\"><code>load_state_dict()</code></a>, but also restores model averagers step value to the one saved in the provided <code>state_dict</code>.</p> <p>If there is no <code>\"step\"</code> entry in <code>state_dict</code>, it will raise a warning and initialize the model averagers step to 0.</p> </dd>\n</dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.optim.PostLocalSGDOptimizer.state_dict\">\n<code>state_dict()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/optim/post_localSGD_optimizer.html#PostLocalSGDOptimizer.state_dict\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>This is the same as <a class=\"reference internal\" href=\"optim#torch.optim.Optimizer\" title=\"torch.optim.Optimizer\"><code>torch.optim.Optimizer</code></a> <a class=\"reference internal\" href=\"#torch.distributed.optim.PostLocalSGDOptimizer.state_dict\" title=\"torch.distributed.optim.PostLocalSGDOptimizer.state_dict\"><code>state_dict()</code></a>, but adds an extra entry to record model averagers step to the checkpoint to ensure reload does not cause unnecessary warm up again.</p> </dd>\n</dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.optim.PostLocalSGDOptimizer.step\">\n<code>step()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/optim/post_localSGD_optimizer.html#PostLocalSGDOptimizer.step\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Performs a single optimization step (parameter update).</p> </dd>\n</dl> </dd>\n</dl> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.optim.ZeroRedundancyOptimizer\">\n<code>class torch.distributed.optim.ZeroRedundancyOptimizer(params, optimizer_class, process_group=None, parameters_as_bucket_view=False, overlap_with_ddp=False, **defaults)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/optim/zero_redundancy_optimizer.html#ZeroRedundancyOptimizer\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>This class wraps an arbitrary <a class=\"reference internal\" href=\"optim#torch.optim.Optimizer\" title=\"torch.optim.Optimizer\"><code>optim.Optimizer</code></a> and shards its states across ranks in the group as described by <a class=\"reference external\" href=\"https://arxiv.org/abs/1910.02054\">ZeRO</a>. The local optimizer instance in each rank is only responsible for updating approximately <code>1 / world_size</code> parameters and hence only needs to keep <code>1 / world_size</code> optimizer states. After parameters are updated locally, each rank will broadcast its parameters to all other peers to keep all model replicas in the same state. <code>ZeroRedundancyOptimizer</code> can be used in conjunction with <a class=\"reference internal\" href=\"generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel\" title=\"torch.nn.parallel.DistributedDataParallel\"><code>torch.nn.parallel.DistributedDataParallel</code></a> to reduce per-rank peak memory consumption.</p> <p><code>ZeroRedundancyOptimizer</code> uses a sorted-greedy algorithm to pack a number of parameters at each rank. Each parameter belongs to a single rank and is not divided among ranks. The partition is arbitrary and might not match the the parameter registration or usage order.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>params</strong> (<code>Iterable</code>)  an <code>Iterable</code> of <a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a> s or <a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#dict\" title=\"(in Python v3.12)\"><code>dict</code></a> s giving all parameters, which will be sharded across ranks.</p> </dd> <dt class=\"field-even\">Keyword Arguments</dt> <dd class=\"field-even\">\n<ul class=\"simple\"> <li>\n<strong>optimizer_class</strong> (<code>torch.nn.Optimizer</code>)  the class of the local optimizer.</li> <li>\n<strong>process_group</strong> (<code>ProcessGroup</code>, optional)  <code>torch.distributed</code> <code>ProcessGroup</code> (default: <code>dist.group.WORLD</code> initialized by <a class=\"reference internal\" href=\"distributed#torch.distributed.init_process_group\" title=\"torch.distributed.init_process_group\"><code>torch.distributed.init_process_group()</code></a>).</li> <li>\n<strong>parameters_as_bucket_view</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  if <code>True</code>, parameters are packed into buckets to speed up communication, and <code>param.data</code> fields point to bucket views at different offsets; if <code>False</code>, each individual parameter is communicated separately, and each <code>params.data</code> stays intact (default: <code>False</code>).</li> <li>\n<strong>overlap_with_ddp</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\">bool</a><em>, </em><em>optional</em>)  if <code>True</code>, <a class=\"reference internal\" href=\"#torch.distributed.optim.ZeroRedundancyOptimizer.step\" title=\"torch.distributed.optim.ZeroRedundancyOptimizer.step\"><code>step()</code></a> is overlapped with <code>DistributedDataParallel</code> s gradient synchronization; this requires (1) either a functional optimizer for the <code>optimizer_class</code> argument or one with a functional equivalent and (2) registering a DDP communication hook constructed from one of the functions in <code>ddp_zero_hook.py</code>; parameters are packed into buckets matching those in <code>DistributedDataParallel</code>, meaning that the <code>parameters_as_bucket_view</code> argument is ignored. If <code>False</code>, <a class=\"reference internal\" href=\"#torch.distributed.optim.ZeroRedundancyOptimizer.step\" title=\"torch.distributed.optim.ZeroRedundancyOptimizer.step\"><code>step()</code></a> runs disjointly after the backward pass (per normal). (default: <code>False</code>)</li> <li>\n<strong>**defaults</strong>  any trailing arguments, which are forwarded to the local optimizer.</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt; from torch.distributed.optim import ZeroRedundancyOptimizer\n&gt;&gt;&gt; from torch.nn.parallel import DistributedDataParallel as DDP\n&gt;&gt;&gt; model = nn.Sequential(*[nn.Linear(2000, 2000).to(rank) for _ in range(20)])\n&gt;&gt;&gt; ddp = DDP(model, device_ids=[rank])\n&gt;&gt;&gt; opt = ZeroRedundancyOptimizer(\n&gt;&gt;&gt;     ddp.parameters(),\n&gt;&gt;&gt;     optimizer_class=torch.optim.Adam,\n&gt;&gt;&gt;     lr=0.01\n&gt;&gt;&gt; )\n&gt;&gt;&gt; ddp(inputs).sum().backward()\n&gt;&gt;&gt; opt.step()\n</pre> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Currently, <code>ZeroRedundancyOptimizer</code> requires that all of the passed-in parameters are the same dense type.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>If you pass <code>overlap_with_ddp=True</code>, be wary of the following: Given the way that overlapping <code>DistributedDataParallel</code> with <a class=\"reference internal\" href=\"#torch.distributed.optim.ZeroRedundancyOptimizer\" title=\"torch.distributed.optim.ZeroRedundancyOptimizer\"><code>ZeroRedundancyOptimizer</code></a> is currently implemented, the first two or three training iterations do not perform parameter updates in the optimizer step, depending on if <code>static_graph=False</code> or <code>static_graph=True</code>, respectively. This is because it needs information about the gradient bucketing strategy used by <code>DistributedDataParallel</code>, which is not finalized until the second forward pass if <code>static_graph=False</code> or until the third forward pass if <code>static_graph=True</code>. To adjust for this, one option is to prepend dummy inputs.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>ZeroRedundancyOptimizer is experimental and subject to change.</p> </div> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.optim.ZeroRedundancyOptimizer.add_param_group\">\n<code>add_param_group(param_group)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/optim/zero_redundancy_optimizer.html#ZeroRedundancyOptimizer.add_param_group\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Add a parameter group to the <code>Optimizer</code> s <code>param_groups</code>.</p> <p>This can be useful when fine tuning a pre-trained network, as frozen layers can be made trainable and added to the <code>Optimizer</code> as training progresses.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>param_group</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#dict\" title=\"(in Python v3.12)\">dict</a>)  specifies the parameters to be optimized and group-specific optimization options.</p> </dd> </dl> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This method handles updating the shards on all partitions but needs to be called on all ranks. Calling this on a subset of the ranks will cause the training to hang because communication primitives are called depending on the managed parameters and expect all the ranks to participate on the same set of parameters.</p> </div> </dd>\n</dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.optim.ZeroRedundancyOptimizer.consolidate_state_dict\">\n<code>consolidate_state_dict(to=0)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/optim/zero_redundancy_optimizer.html#ZeroRedundancyOptimizer.consolidate_state_dict\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Consolidate a list of <code>state_dict</code> s (one per rank) on the target rank.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>to</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.12)\">int</a>)  the rank that receives the optimizer states (default: 0).</p> </dd> <dt class=\"field-even\">Raises</dt> <dd class=\"field-even\">\n<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#RuntimeError\" title=\"(in Python v3.12)\"><strong>RuntimeError</strong></a>  if <code>overlap_with_ddp=True</code> and this method is called before this <a class=\"reference internal\" href=\"#torch.distributed.optim.ZeroRedundancyOptimizer\" title=\"torch.distributed.optim.ZeroRedundancyOptimizer\"><code>ZeroRedundancyOptimizer</code></a> instance has been fully initialized, which happens once <code>DistributedDataParallel</code> gradient buckets have been rebuilt.</p> </dd> </dl> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This needs to be called on all ranks.</p> </div> </dd>\n</dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.optim.ZeroRedundancyOptimizer.join_hook\">\n<code>join_hook(**kwargs)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/optim/zero_redundancy_optimizer.html#ZeroRedundancyOptimizer.join_hook\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the ZeRO join hook, which enables training on uneven inputs by shadowing the collective communications in the optimizer step.</p> <p>Gradients must be properly set before this hook is called.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>kwargs</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#dict\" title=\"(in Python v3.12)\">dict</a>)  a <a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#dict\" title=\"(in Python v3.12)\"><code>dict</code></a> containing any keyword arguments to modify the behavior of the join hook at run time; all <code>Joinable</code> instances sharing the same join context manager are forwarded the same value for <code>kwargs</code>.</p> </dd> </dl> <p>This hook does not support any keyword arguments; i.e. <code>kwargs</code> is unused.</p> </dd>\n</dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.optim.ZeroRedundancyOptimizer.load_state_dict\">\n<code>load_state_dict(state_dict)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/optim/zero_redundancy_optimizer.html#ZeroRedundancyOptimizer.load_state_dict\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Load the state pertaining to the given rank from the input <code>state_dict</code>, updating the local optimizer as needed.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>state_dict</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#dict\" title=\"(in Python v3.12)\">dict</a>)  optimizer state; should be an object returned from a call to <a class=\"reference internal\" href=\"#torch.distributed.optim.ZeroRedundancyOptimizer.state_dict\" title=\"torch.distributed.optim.ZeroRedundancyOptimizer.state_dict\"><code>state_dict()</code></a>.</p> </dd> <dt class=\"field-even\">Raises</dt> <dd class=\"field-even\">\n<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#RuntimeError\" title=\"(in Python v3.12)\"><strong>RuntimeError</strong></a>  if <code>overlap_with_ddp=True</code> and this method is called before this <a class=\"reference internal\" href=\"#torch.distributed.optim.ZeroRedundancyOptimizer\" title=\"torch.distributed.optim.ZeroRedundancyOptimizer\"><code>ZeroRedundancyOptimizer</code></a> instance has been fully initialized, which happens once <code>DistributedDataParallel</code> gradient buckets have been rebuilt.</p> </dd> </dl> </dd>\n</dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.optim.ZeroRedundancyOptimizer.state_dict\">\n<code>state_dict()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/optim/zero_redundancy_optimizer.html#ZeroRedundancyOptimizer.state_dict\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the last global optimizer state known to this rank.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Raises</dt> <dd class=\"field-odd\">\n<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#RuntimeError\" title=\"(in Python v3.12)\"><strong>RuntimeError</strong></a>  if <code>overlap_with_ddp=True</code> and this method is called before this <a class=\"reference internal\" href=\"#torch.distributed.optim.ZeroRedundancyOptimizer\" title=\"torch.distributed.optim.ZeroRedundancyOptimizer\"><code>ZeroRedundancyOptimizer</code></a> instance has been fully initialized, which happens once <code>DistributedDataParallel</code> gradient buckets have been rebuilt; or if this method is called without a preceding call to <a class=\"reference internal\" href=\"#torch.distributed.optim.ZeroRedundancyOptimizer.consolidate_state_dict\" title=\"torch.distributed.optim.ZeroRedundancyOptimizer.consolidate_state_dict\"><code>consolidate_state_dict()</code></a>.</p> </dd> <dt class=\"field-even\">Return type</dt> <dd class=\"field-even\">\n<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Dict\" title=\"(in Python v3.12)\">Dict</a>[<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\">str</a>, <a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Any\" title=\"(in Python v3.12)\">Any</a>]</p> </dd> </dl> </dd>\n</dl> <dl class=\"py method\"> <dt class=\"sig sig-object py\" id=\"torch.distributed.optim.ZeroRedundancyOptimizer.step\">\n<code>step(closure=None, **kwargs)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/2.1/_modules/torch/distributed/optim/zero_redundancy_optimizer.html#ZeroRedundancyOptimizer.step\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Performs a single optimizer step and syncs parameters across all ranks.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>closure</strong> (<em>Callable</em>)  a closure that re-evaluates the model and returns the loss; optional for most optimizers.</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Optional loss depending on the underlying local optimizer.</p> </dd> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Optional\" title=\"(in Python v3.12)\">Optional</a>[<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.12)\">float</a>]</p> </dd> </dl> </dd>\n</dl> </dd>\n</dl><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href=\"https://github.com/pytorch/pytorch/blob/main/LICENSE\">LICENSE</a> file.<br>\n    <a href=\"https://pytorch.org/docs/2.1/distributed.optim.html\" class=\"_attribution-link\">https://pytorch.org/docs/2.1/distributed.optim.html</a>\n  </p>\n</div>\n","tensors":"<h1 id=\"tensor-doc\">torch.Tensor</h1> <p id=\"torch-tensor\">A <a class=\"reference internal\" href=\"#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a> is a multi-dimensional matrix containing elements of a single data type.</p>  <h2 id=\"data-types\">Data types</h2> <p>Torch defines 10 tensor types with CPU and GPU variants which are as follows:</p> <table class=\"docutils colwidths-auto align-default\"> <thead> <tr>\n<th class=\"head\"><p>Data type</p></th> <th class=\"head\"><p>dtype</p></th> <th class=\"head\"><p>CPU tensor</p></th> <th class=\"head\"><p>GPU tensor</p></th> </tr> </thead>  <tr>\n<td><p>32-bit floating point</p></td> <td><p><code>torch.float32</code> or <code>torch.float</code></p></td> <td><p><code>torch.FloatTensor</code></p></td> <td><p><code>torch.cuda.FloatTensor</code></p></td> </tr> <tr>\n<td><p>64-bit floating point</p></td> <td><p><code>torch.float64</code> or <code>torch.double</code></p></td> <td><p><code>torch.DoubleTensor</code></p></td> <td><p><code>torch.cuda.DoubleTensor</code></p></td> </tr> <tr>\n<td><p>16-bit floating point <a class=\"footnote-reference brackets\" href=\"#id4\" id=\"id1\">1</a></p></td> <td><p><code>torch.float16</code> or <code>torch.half</code></p></td> <td><p><code>torch.HalfTensor</code></p></td> <td><p><code>torch.cuda.HalfTensor</code></p></td> </tr> <tr>\n<td><p>16-bit floating point <a class=\"footnote-reference brackets\" href=\"#id5\" id=\"id2\">2</a></p></td> <td><p><code>torch.bfloat16</code></p></td> <td><p><code>torch.BFloat16Tensor</code></p></td> <td><p><code>torch.cuda.BFloat16Tensor</code></p></td> </tr> <tr>\n<td><p>32-bit complex</p></td> <td><p><code>torch.complex32</code> or <code>torch.chalf</code></p></td> <td></td> <td></td> </tr> <tr>\n<td><p>64-bit complex</p></td> <td><p><code>torch.complex64</code> or <code>torch.cfloat</code></p></td> <td></td> <td></td> </tr> <tr>\n<td><p>128-bit complex</p></td> <td><p><code>torch.complex128</code> or <code>torch.cdouble</code></p></td> <td></td> <td></td> </tr> <tr>\n<td><p>8-bit integer (unsigned)</p></td> <td><p><code>torch.uint8</code></p></td> <td><p><code>torch.ByteTensor</code></p></td> <td><p><code>torch.cuda.ByteTensor</code></p></td> </tr> <tr>\n<td><p>8-bit integer (signed)</p></td> <td><p><code>torch.int8</code></p></td> <td><p><code>torch.CharTensor</code></p></td> <td><p><code>torch.cuda.CharTensor</code></p></td> </tr> <tr>\n<td><p>16-bit integer (signed)</p></td> <td><p><code>torch.int16</code> or <code>torch.short</code></p></td> <td><p><code>torch.ShortTensor</code></p></td> <td><p><code>torch.cuda.ShortTensor</code></p></td> </tr> <tr>\n<td><p>32-bit integer (signed)</p></td> <td><p><code>torch.int32</code> or <code>torch.int</code></p></td> <td><p><code>torch.IntTensor</code></p></td> <td><p><code>torch.cuda.IntTensor</code></p></td> </tr> <tr>\n<td><p>64-bit integer (signed)</p></td> <td><p><code>torch.int64</code> or <code>torch.long</code></p></td> <td><p><code>torch.LongTensor</code></p></td> <td><p><code>torch.cuda.LongTensor</code></p></td> </tr> <tr>\n<td><p>Boolean</p></td> <td><p><code>torch.bool</code></p></td> <td><p><code>torch.BoolTensor</code></p></td> <td><p><code>torch.cuda.BoolTensor</code></p></td> </tr> <tr>\n<td><p>quantized 8-bit integer (unsigned)</p></td> <td><p><code>torch.quint8</code></p></td> <td><p><code>torch.ByteTensor</code></p></td> <td><p>/</p></td> </tr> <tr>\n<td><p>quantized 8-bit integer (signed)</p></td> <td><p><code>torch.qint8</code></p></td> <td><p><code>torch.CharTensor</code></p></td> <td><p>/</p></td> </tr> <tr>\n<td><p>quantized 32-bit integer (signed)</p></td> <td><p><code>torch.qint32</code></p></td> <td><p><code>torch.IntTensor</code></p></td> <td><p>/</p></td> </tr> <tr>\n<td><p>quantized 4-bit integer (unsigned) <a class=\"footnote-reference brackets\" href=\"#id6\" id=\"id3\">3</a></p></td> <td><p><code>torch.quint4x2</code></p></td> <td><p><code>torch.ByteTensor</code></p></td> <td><p>/</p></td> </tr>  </table> <dl class=\"footnote brackets\"> <dt class=\"label\" id=\"id4\">\n<code>1</code> </dt> <dd>\n<p>Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10 significand bits. Useful when precision is important at the expense of range.</p> </dd> <dt class=\"label\" id=\"id5\">\n<code>2</code> </dt> <dd>\n<p>Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7 significand bits. Useful when range is important, since it has the same number of exponent bits as <code>float32</code></p> </dd> <dt class=\"label\" id=\"id6\">\n<code>3</code> </dt> <dd>\n<p>quantized 4-bit integer is stored as a 8-bit signed integer. Currently its only supported in EmbeddingBag operator.</p> </dd> </dl> <p><a class=\"reference internal\" href=\"#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a> is an alias for the default tensor type (<code>torch.FloatTensor</code>).</p>   <h2 id=\"initializing-and-basic-operations\">Initializing and basic operations</h2> <p>A tensor can be constructed from a Python <a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.12)\"><code>list</code></a> or sequence using the <a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>torch.tensor()</code></a> constructor:</p> <pre data-language=\"python\">&gt;&gt;&gt; torch.tensor([[1., -1.], [1., -1.]])\ntensor([[ 1.0000, -1.0000],\n        [ 1.0000, -1.0000]])\n&gt;&gt;&gt; torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])\n</pre> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>torch.tensor()</code></a> always copies <code>data</code>. If you have a Tensor <code>data</code> and just want to change its <code>requires_grad</code> flag, use <a class=\"reference internal\" href=\"generated/torch.tensor.requires_grad_#torch.Tensor.requires_grad_\" title=\"torch.Tensor.requires_grad_\"><code>requires_grad_()</code></a> or <a class=\"reference internal\" href=\"generated/torch.tensor.detach#torch.Tensor.detach\" title=\"torch.Tensor.detach\"><code>detach()</code></a> to avoid a copy. If you have a numpy array and want to avoid a copy, use <a class=\"reference internal\" href=\"generated/torch.as_tensor#torch.as_tensor\" title=\"torch.as_tensor\"><code>torch.as_tensor()</code></a>.</p> </div> <p>A tensor of specific data type can be constructed by passing a <a class=\"reference internal\" href=\"tensor_attributes#torch.dtype\" title=\"torch.dtype\"><code>torch.dtype</code></a> and/or a <a class=\"reference internal\" href=\"tensor_attributes#torch.device\" title=\"torch.device\"><code>torch.device</code></a> to a constructor or tensor creation op:</p> <pre data-language=\"python\">&gt;&gt;&gt; torch.zeros([2, 4], dtype=torch.int32)\ntensor([[ 0,  0,  0,  0],\n        [ 0,  0,  0,  0]], dtype=torch.int32)\n&gt;&gt;&gt; cuda0 = torch.device('cuda:0')\n&gt;&gt;&gt; torch.ones([2, 4], dtype=torch.float64, device=cuda0)\ntensor([[ 1.0000,  1.0000,  1.0000,  1.0000],\n        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')\n</pre> <p>For more information about building Tensors, see <a class=\"reference internal\" href=\"torch#tensor-creation-ops\"><span class=\"std std-ref\">Creation Ops</span></a></p> <p>The contents of a tensor can be accessed and modified using Pythons indexing and slicing notation:</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; print(x[1][2])\ntensor(6)\n&gt;&gt;&gt; x[0][1] = 8\n&gt;&gt;&gt; print(x)\ntensor([[ 1,  8,  3],\n        [ 4,  5,  6]])\n</pre> <p>Use <a class=\"reference internal\" href=\"generated/torch.tensor.item#torch.Tensor.item\" title=\"torch.Tensor.item\"><code>torch.Tensor.item()</code></a> to get a Python number from a tensor containing a single value:</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.tensor([[1]])\n&gt;&gt;&gt; x\ntensor([[ 1]])\n&gt;&gt;&gt; x.item()\n1\n&gt;&gt;&gt; x = torch.tensor(2.5)\n&gt;&gt;&gt; x\ntensor(2.5000)\n&gt;&gt;&gt; x.item()\n2.5\n</pre> <p>For more information about indexing, see <a class=\"reference internal\" href=\"torch#indexing-slicing-joining\"><span class=\"std std-ref\">Indexing, Slicing, Joining, Mutating Ops</span></a></p> <p>A tensor can be created with <code>requires_grad=True</code> so that <a class=\"reference internal\" href=\"autograd#module-torch.autograd\" title=\"torch.autograd\"><code>torch.autograd</code></a> records operations on them for automatic differentiation.</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n&gt;&gt;&gt; out = x.pow(2).sum()\n&gt;&gt;&gt; out.backward()\n&gt;&gt;&gt; x.grad\ntensor([[ 2.0000, -2.0000],\n        [ 2.0000,  2.0000]])\n</pre> <p>Each tensor has an associated <code>torch.Storage</code>, which holds its data. The tensor class also provides multi-dimensional, <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Stride_of_an_array\">strided</a> view of a storage and defines numeric operations on it.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>For more information on tensor views, see <a class=\"reference internal\" href=\"tensor_view#tensor-view-doc\"><span class=\"std std-ref\">Tensor Views</span></a>.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>For more information on the <a class=\"reference internal\" href=\"tensor_attributes#torch.dtype\" title=\"torch.dtype\"><code>torch.dtype</code></a>, <a class=\"reference internal\" href=\"tensor_attributes#torch.device\" title=\"torch.device\"><code>torch.device</code></a>, and <a class=\"reference internal\" href=\"tensor_attributes#torch.layout\" title=\"torch.layout\"><code>torch.layout</code></a> attributes of a <a class=\"reference internal\" href=\"#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a>, see <a class=\"reference internal\" href=\"tensor_attributes#tensor-attributes-doc\"><span class=\"std std-ref\">Tensor Attributes</span></a>.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Methods which mutate a tensor are marked with an underscore suffix. For example, <code>torch.FloatTensor.abs_()</code> computes the absolute value in-place and returns the modified tensor, while <code>torch.FloatTensor.abs()</code> computes the result in a new tensor.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>To change an existing tensors <a class=\"reference internal\" href=\"tensor_attributes#torch.device\" title=\"torch.device\"><code>torch.device</code></a> and/or <a class=\"reference internal\" href=\"tensor_attributes#torch.dtype\" title=\"torch.dtype\"><code>torch.dtype</code></a>, consider using <a class=\"reference internal\" href=\"generated/torch.tensor.to#torch.Tensor.to\" title=\"torch.Tensor.to\"><code>to()</code></a> method on the tensor.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Current implementation of <a class=\"reference internal\" href=\"#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a> introduces memory overhead, thus it might lead to unexpectedly high memory usage in the applications with many tiny tensors. If this is your case, consider using one large structure.</p> </div>   <h2 id=\"tensor-class-reference\">Tensor class reference</h2> <dl class=\"py class\"> <dt class=\"sig sig-object py\" id=\"torch.Tensor\">\n<code>class torch.Tensor</code> </dt> <dd>\n<p>There are a few main ways to create a tensor, depending on your use case.</p> <ul class=\"simple\"> <li>To create a tensor with pre-existing data, use <a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>torch.tensor()</code></a>.</li> <li>To create a tensor with specific size, use <code>torch.*</code> tensor creation ops (see <a class=\"reference internal\" href=\"torch#tensor-creation-ops\"><span class=\"std std-ref\">Creation Ops</span></a>).</li> <li>To create a tensor with the same size (and similar types) as another tensor, use <code>torch.*_like</code> tensor creation ops (see <a class=\"reference internal\" href=\"torch#tensor-creation-ops\"><span class=\"std std-ref\">Creation Ops</span></a>).</li> <li>To create a tensor with similar type but different size as another tensor, use <code>tensor.new_*</code> creation ops.</li> </ul> </dd>\n</dl> <dl class=\"py attribute\"> <dt class=\"sig sig-object py\" id=\"torch.Tensor.T\">\n<code>Tensor.T</code> </dt> <dd>\n<p>Returns a view of this tensor with its dimensions reversed.</p> <p>If <code>n</code> is the number of dimensions in <code>x</code>, <code>x.T</code> is equivalent to <code>x.permute(n-1, n-2, ..., 0)</code>.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>The use of <a class=\"reference internal\" href=\"#torch.Tensor.T\" title=\"torch.Tensor.T\"><code>Tensor.T()</code></a> on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider <a class=\"reference internal\" href=\"#torch.Tensor.mT\" title=\"torch.Tensor.mT\"><code>mT</code></a> to transpose batches of matrices or <code>x.permute(*torch.arange(x.ndim - 1, -1, -1))</code> to reverse the dimensions of a tensor.</p> </div> </dd>\n</dl> <dl class=\"py attribute\"> <dt class=\"sig sig-object py\" id=\"torch.Tensor.H\">\n<code>Tensor.H</code> </dt> <dd>\n<p>Returns a view of a matrix (2-D tensor) conjugated and transposed.</p> <p><code>x.H</code> is equivalent to <code>x.transpose(0, 1).conj()</code> for complex matrices and <code>x.transpose(0, 1)</code> for real matrices.</p> <div class=\"admonition seealso\"> <p class=\"admonition-title\">See also</p> <p><a class=\"reference internal\" href=\"#torch.Tensor.mH\" title=\"torch.Tensor.mH\"><code>mH</code></a>: An attribute that also works on batches of matrices.</p> </div> </dd>\n</dl> <dl class=\"py attribute\"> <dt class=\"sig sig-object py\" id=\"torch.Tensor.mT\">\n<code>Tensor.mT</code> </dt> <dd>\n<p>Returns a view of this tensor with the last two dimensions transposed.</p> <p><code>x.mT</code> is equivalent to <code>x.transpose(-2, -1)</code>.</p> </dd>\n</dl> <dl class=\"py attribute\"> <dt class=\"sig sig-object py\" id=\"torch.Tensor.mH\">\n<code>Tensor.mH</code> </dt> <dd>\n<p>Accessing this property is equivalent to calling <a class=\"reference internal\" href=\"generated/torch.adjoint#torch.adjoint\" title=\"torch.adjoint\"><code>adjoint()</code></a>.</p> </dd>\n</dl> <table class=\"autosummary longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.new_tensor#torch.Tensor.new_tensor\" title=\"torch.Tensor.new_tensor\"><code>Tensor.new_tensor</code></a></p></td> <td><p>Returns a new Tensor with <code>data</code> as the tensor data.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.new_full#torch.Tensor.new_full\" title=\"torch.Tensor.new_full\"><code>Tensor.new_full</code></a></p></td> <td><p>Returns a Tensor of size <code>size</code> filled with <code>fill_value</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.new_empty#torch.Tensor.new_empty\" title=\"torch.Tensor.new_empty\"><code>Tensor.new_empty</code></a></p></td> <td><p>Returns a Tensor of size <code>size</code> filled with uninitialized data.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.new_ones#torch.Tensor.new_ones\" title=\"torch.Tensor.new_ones\"><code>Tensor.new_ones</code></a></p></td> <td><p>Returns a Tensor of size <code>size</code> filled with <code>1</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.new_zeros#torch.Tensor.new_zeros\" title=\"torch.Tensor.new_zeros\"><code>Tensor.new_zeros</code></a></p></td> <td><p>Returns a Tensor of size <code>size</code> filled with <code>0</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.is_cuda#torch.Tensor.is_cuda\" title=\"torch.Tensor.is_cuda\"><code>Tensor.is_cuda</code></a></p></td> <td><p>Is <code>True</code> if the Tensor is stored on the GPU, <code>False</code> otherwise.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.is_quantized#torch.Tensor.is_quantized\" title=\"torch.Tensor.is_quantized\"><code>Tensor.is_quantized</code></a></p></td> <td><p>Is <code>True</code> if the Tensor is quantized, <code>False</code> otherwise.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.is_meta#torch.Tensor.is_meta\" title=\"torch.Tensor.is_meta\"><code>Tensor.is_meta</code></a></p></td> <td><p>Is <code>True</code> if the Tensor is a meta tensor, <code>False</code> otherwise.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.device#torch.Tensor.device\" title=\"torch.Tensor.device\"><code>Tensor.device</code></a></p></td> <td><p>Is the <a class=\"reference internal\" href=\"tensor_attributes#torch.device\" title=\"torch.device\"><code>torch.device</code></a> where this Tensor is.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.grad#torch.Tensor.grad\" title=\"torch.Tensor.grad\"><code>Tensor.grad</code></a></p></td> <td><p>This attribute is <code>None</code> by default and becomes a Tensor the first time a call to <code>backward()</code> computes gradients for <code>self</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.ndim#torch.Tensor.ndim\" title=\"torch.Tensor.ndim\"><code>Tensor.ndim</code></a></p></td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.tensor.dim#torch.Tensor.dim\" title=\"torch.Tensor.dim\"><code>dim()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.real#torch.Tensor.real\" title=\"torch.Tensor.real\"><code>Tensor.real</code></a></p></td> <td><p>Returns a new tensor containing real values of the <code>self</code> tensor for a complex-valued input tensor.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.imag#torch.Tensor.imag\" title=\"torch.Tensor.imag\"><code>Tensor.imag</code></a></p></td> <td><p>Returns a new tensor containing imaginary values of the <code>self</code> tensor.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.nbytes#torch.Tensor.nbytes\" title=\"torch.Tensor.nbytes\"><code>Tensor.nbytes</code></a></p></td> <td><p>Returns the number of bytes consumed by the \"view\" of elements of the Tensor if the Tensor does not use sparse storage layout.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.itemsize#torch.Tensor.itemsize\" title=\"torch.Tensor.itemsize\"><code>Tensor.itemsize</code></a></p></td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.tensor.element_size#torch.Tensor.element_size\" title=\"torch.Tensor.element_size\"><code>element_size()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.abs#torch.Tensor.abs\" title=\"torch.Tensor.abs\"><code>Tensor.abs</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.abs#torch.abs\" title=\"torch.abs\"><code>torch.abs()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.abs_#torch.Tensor.abs_\" title=\"torch.Tensor.abs_\"><code>Tensor.abs_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.abs#torch.Tensor.abs\" title=\"torch.Tensor.abs\"><code>abs()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.absolute#torch.Tensor.absolute\" title=\"torch.Tensor.absolute\"><code>Tensor.absolute</code></a></p></td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.abs#torch.abs\" title=\"torch.abs\"><code>abs()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.absolute_#torch.Tensor.absolute_\" title=\"torch.Tensor.absolute_\"><code>Tensor.absolute_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.absolute#torch.Tensor.absolute\" title=\"torch.Tensor.absolute\"><code>absolute()</code></a> Alias for <code>abs_()</code></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.acos#torch.Tensor.acos\" title=\"torch.Tensor.acos\"><code>Tensor.acos</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.acos#torch.acos\" title=\"torch.acos\"><code>torch.acos()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.acos_#torch.Tensor.acos_\" title=\"torch.Tensor.acos_\"><code>Tensor.acos_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.acos#torch.Tensor.acos\" title=\"torch.Tensor.acos\"><code>acos()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.arccos#torch.Tensor.arccos\" title=\"torch.Tensor.arccos\"><code>Tensor.arccos</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.arccos#torch.arccos\" title=\"torch.arccos\"><code>torch.arccos()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.arccos_#torch.Tensor.arccos_\" title=\"torch.Tensor.arccos_\"><code>Tensor.arccos_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.arccos#torch.Tensor.arccos\" title=\"torch.Tensor.arccos\"><code>arccos()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.add#torch.Tensor.add\" title=\"torch.Tensor.add\"><code>Tensor.add</code></a></p></td> <td><p>Add a scalar or tensor to <code>self</code> tensor.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.add_#torch.Tensor.add_\" title=\"torch.Tensor.add_\"><code>Tensor.add_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.add#torch.Tensor.add\" title=\"torch.Tensor.add\"><code>add()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.addbmm#torch.Tensor.addbmm\" title=\"torch.Tensor.addbmm\"><code>Tensor.addbmm</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.addbmm#torch.addbmm\" title=\"torch.addbmm\"><code>torch.addbmm()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.addbmm_#torch.Tensor.addbmm_\" title=\"torch.Tensor.addbmm_\"><code>Tensor.addbmm_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.addbmm#torch.Tensor.addbmm\" title=\"torch.Tensor.addbmm\"><code>addbmm()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.addcdiv#torch.Tensor.addcdiv\" title=\"torch.Tensor.addcdiv\"><code>Tensor.addcdiv</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.addcdiv#torch.addcdiv\" title=\"torch.addcdiv\"><code>torch.addcdiv()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.addcdiv_#torch.Tensor.addcdiv_\" title=\"torch.Tensor.addcdiv_\"><code>Tensor.addcdiv_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.addcdiv#torch.Tensor.addcdiv\" title=\"torch.Tensor.addcdiv\"><code>addcdiv()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.addcmul#torch.Tensor.addcmul\" title=\"torch.Tensor.addcmul\"><code>Tensor.addcmul</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.addcmul#torch.addcmul\" title=\"torch.addcmul\"><code>torch.addcmul()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.addcmul_#torch.Tensor.addcmul_\" title=\"torch.Tensor.addcmul_\"><code>Tensor.addcmul_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.addcmul#torch.Tensor.addcmul\" title=\"torch.Tensor.addcmul\"><code>addcmul()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.addmm#torch.Tensor.addmm\" title=\"torch.Tensor.addmm\"><code>Tensor.addmm</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.addmm#torch.addmm\" title=\"torch.addmm\"><code>torch.addmm()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.addmm_#torch.Tensor.addmm_\" title=\"torch.Tensor.addmm_\"><code>Tensor.addmm_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.addmm#torch.Tensor.addmm\" title=\"torch.Tensor.addmm\"><code>addmm()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.sspaddmm#torch.Tensor.sspaddmm\" title=\"torch.Tensor.sspaddmm\"><code>Tensor.sspaddmm</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.sspaddmm#torch.sspaddmm\" title=\"torch.sspaddmm\"><code>torch.sspaddmm()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.addmv#torch.Tensor.addmv\" title=\"torch.Tensor.addmv\"><code>Tensor.addmv</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.addmv#torch.addmv\" title=\"torch.addmv\"><code>torch.addmv()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.addmv_#torch.Tensor.addmv_\" title=\"torch.Tensor.addmv_\"><code>Tensor.addmv_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.addmv#torch.Tensor.addmv\" title=\"torch.Tensor.addmv\"><code>addmv()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.addr#torch.Tensor.addr\" title=\"torch.Tensor.addr\"><code>Tensor.addr</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.addr#torch.addr\" title=\"torch.addr\"><code>torch.addr()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.addr_#torch.Tensor.addr_\" title=\"torch.Tensor.addr_\"><code>Tensor.addr_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.addr#torch.Tensor.addr\" title=\"torch.Tensor.addr\"><code>addr()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.adjoint#torch.Tensor.adjoint\" title=\"torch.Tensor.adjoint\"><code>Tensor.adjoint</code></a></p></td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.adjoint#torch.adjoint\" title=\"torch.adjoint\"><code>adjoint()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.allclose#torch.Tensor.allclose\" title=\"torch.Tensor.allclose\"><code>Tensor.allclose</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.allclose#torch.allclose\" title=\"torch.allclose\"><code>torch.allclose()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.amax#torch.Tensor.amax\" title=\"torch.Tensor.amax\"><code>Tensor.amax</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.amax#torch.amax\" title=\"torch.amax\"><code>torch.amax()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.amin#torch.Tensor.amin\" title=\"torch.Tensor.amin\"><code>Tensor.amin</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.amin#torch.amin\" title=\"torch.amin\"><code>torch.amin()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.aminmax#torch.Tensor.aminmax\" title=\"torch.Tensor.aminmax\"><code>Tensor.aminmax</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.aminmax#torch.aminmax\" title=\"torch.aminmax\"><code>torch.aminmax()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.angle#torch.Tensor.angle\" title=\"torch.Tensor.angle\"><code>Tensor.angle</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.angle#torch.angle\" title=\"torch.angle\"><code>torch.angle()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.apply_#torch.Tensor.apply_\" title=\"torch.Tensor.apply_\"><code>Tensor.apply_</code></a></p></td> <td><p>Applies the function <code>callable</code> to each element in the tensor, replacing each element with the value returned by <code>callable</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.argmax#torch.Tensor.argmax\" title=\"torch.Tensor.argmax\"><code>Tensor.argmax</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.argmax#torch.argmax\" title=\"torch.argmax\"><code>torch.argmax()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.argmin#torch.Tensor.argmin\" title=\"torch.Tensor.argmin\"><code>Tensor.argmin</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.argmin#torch.argmin\" title=\"torch.argmin\"><code>torch.argmin()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.argsort#torch.Tensor.argsort\" title=\"torch.Tensor.argsort\"><code>Tensor.argsort</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.argsort#torch.argsort\" title=\"torch.argsort\"><code>torch.argsort()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.argwhere#torch.Tensor.argwhere\" title=\"torch.Tensor.argwhere\"><code>Tensor.argwhere</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.argwhere#torch.argwhere\" title=\"torch.argwhere\"><code>torch.argwhere()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.asin#torch.Tensor.asin\" title=\"torch.Tensor.asin\"><code>Tensor.asin</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.asin#torch.asin\" title=\"torch.asin\"><code>torch.asin()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.asin_#torch.Tensor.asin_\" title=\"torch.Tensor.asin_\"><code>Tensor.asin_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.asin#torch.Tensor.asin\" title=\"torch.Tensor.asin\"><code>asin()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.arcsin#torch.Tensor.arcsin\" title=\"torch.Tensor.arcsin\"><code>Tensor.arcsin</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.arcsin#torch.arcsin\" title=\"torch.arcsin\"><code>torch.arcsin()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.arcsin_#torch.Tensor.arcsin_\" title=\"torch.Tensor.arcsin_\"><code>Tensor.arcsin_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.arcsin#torch.Tensor.arcsin\" title=\"torch.Tensor.arcsin\"><code>arcsin()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.as_strided#torch.Tensor.as_strided\" title=\"torch.Tensor.as_strided\"><code>Tensor.as_strided</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.as_strided#torch.as_strided\" title=\"torch.as_strided\"><code>torch.as_strided()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.atan#torch.Tensor.atan\" title=\"torch.Tensor.atan\"><code>Tensor.atan</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.atan#torch.atan\" title=\"torch.atan\"><code>torch.atan()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.atan_#torch.Tensor.atan_\" title=\"torch.Tensor.atan_\"><code>Tensor.atan_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.atan#torch.Tensor.atan\" title=\"torch.Tensor.atan\"><code>atan()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.arctan#torch.Tensor.arctan\" title=\"torch.Tensor.arctan\"><code>Tensor.arctan</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.arctan#torch.arctan\" title=\"torch.arctan\"><code>torch.arctan()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.arctan_#torch.Tensor.arctan_\" title=\"torch.Tensor.arctan_\"><code>Tensor.arctan_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.arctan#torch.Tensor.arctan\" title=\"torch.Tensor.arctan\"><code>arctan()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.atan2#torch.Tensor.atan2\" title=\"torch.Tensor.atan2\"><code>Tensor.atan2</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.atan2#torch.atan2\" title=\"torch.atan2\"><code>torch.atan2()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.atan2_#torch.Tensor.atan2_\" title=\"torch.Tensor.atan2_\"><code>Tensor.atan2_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.atan2#torch.Tensor.atan2\" title=\"torch.Tensor.atan2\"><code>atan2()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.arctan2#torch.Tensor.arctan2\" title=\"torch.Tensor.arctan2\"><code>Tensor.arctan2</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.arctan2#torch.arctan2\" title=\"torch.arctan2\"><code>torch.arctan2()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.arctan2_#torch.Tensor.arctan2_\" title=\"torch.Tensor.arctan2_\"><code>Tensor.arctan2_</code></a></p></td> <td><p>atan2_(other) -&gt; Tensor</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.all#torch.Tensor.all\" title=\"torch.Tensor.all\"><code>Tensor.all</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.all#torch.all\" title=\"torch.all\"><code>torch.all()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.any#torch.Tensor.any\" title=\"torch.Tensor.any\"><code>Tensor.any</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.any#torch.any\" title=\"torch.any\"><code>torch.any()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.backward#torch.Tensor.backward\" title=\"torch.Tensor.backward\"><code>Tensor.backward</code></a></p></td> <td><p>Computes the gradient of current tensor wrt graph leaves.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.baddbmm#torch.Tensor.baddbmm\" title=\"torch.Tensor.baddbmm\"><code>Tensor.baddbmm</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.baddbmm#torch.baddbmm\" title=\"torch.baddbmm\"><code>torch.baddbmm()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.baddbmm_#torch.Tensor.baddbmm_\" title=\"torch.Tensor.baddbmm_\"><code>Tensor.baddbmm_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.baddbmm#torch.Tensor.baddbmm\" title=\"torch.Tensor.baddbmm\"><code>baddbmm()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.bernoulli#torch.Tensor.bernoulli\" title=\"torch.Tensor.bernoulli\"><code>Tensor.bernoulli</code></a></p></td> <td><p>Returns a result tensor where each <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext mathvariant=\"monospace\">result[i]</mtext></mrow><annotation encoding=\"application/x-tex\">\\texttt{result[i]}</annotation></semantics></math></span></span></span> is independently sampled from <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>Bernoulli</mtext><mo stretchy=\"false\">(</mo><mtext mathvariant=\"monospace\">self[i]</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{Bernoulli}(\\texttt{self[i]})</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.bernoulli_#torch.Tensor.bernoulli_\" title=\"torch.Tensor.bernoulli_\"><code>Tensor.bernoulli_</code></a></p></td> <td><p>Fills each location of <code>self</code> with an independent sample from <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>Bernoulli</mtext><mo stretchy=\"false\">(</mo><mtext mathvariant=\"monospace\">p</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{Bernoulli}(\\texttt{p})</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.bfloat16#torch.Tensor.bfloat16\" title=\"torch.Tensor.bfloat16\"><code>Tensor.bfloat16</code></a></p></td> <td><p><code>self.bfloat16()</code> is equivalent to <code>self.to(torch.bfloat16)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.bincount#torch.Tensor.bincount\" title=\"torch.Tensor.bincount\"><code>Tensor.bincount</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.bincount#torch.bincount\" title=\"torch.bincount\"><code>torch.bincount()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.bitwise_not#torch.Tensor.bitwise_not\" title=\"torch.Tensor.bitwise_not\"><code>Tensor.bitwise_not</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.bitwise_not#torch.bitwise_not\" title=\"torch.bitwise_not\"><code>torch.bitwise_not()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.bitwise_not_#torch.Tensor.bitwise_not_\" title=\"torch.Tensor.bitwise_not_\"><code>Tensor.bitwise_not_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.bitwise_not#torch.Tensor.bitwise_not\" title=\"torch.Tensor.bitwise_not\"><code>bitwise_not()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.bitwise_and#torch.Tensor.bitwise_and\" title=\"torch.Tensor.bitwise_and\"><code>Tensor.bitwise_and</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.bitwise_and#torch.bitwise_and\" title=\"torch.bitwise_and\"><code>torch.bitwise_and()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.bitwise_and_#torch.Tensor.bitwise_and_\" title=\"torch.Tensor.bitwise_and_\"><code>Tensor.bitwise_and_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.bitwise_and#torch.Tensor.bitwise_and\" title=\"torch.Tensor.bitwise_and\"><code>bitwise_and()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.bitwise_or#torch.Tensor.bitwise_or\" title=\"torch.Tensor.bitwise_or\"><code>Tensor.bitwise_or</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.bitwise_or#torch.bitwise_or\" title=\"torch.bitwise_or\"><code>torch.bitwise_or()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.bitwise_or_#torch.Tensor.bitwise_or_\" title=\"torch.Tensor.bitwise_or_\"><code>Tensor.bitwise_or_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.bitwise_or#torch.Tensor.bitwise_or\" title=\"torch.Tensor.bitwise_or\"><code>bitwise_or()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.bitwise_xor#torch.Tensor.bitwise_xor\" title=\"torch.Tensor.bitwise_xor\"><code>Tensor.bitwise_xor</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.bitwise_xor#torch.bitwise_xor\" title=\"torch.bitwise_xor\"><code>torch.bitwise_xor()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.bitwise_xor_#torch.Tensor.bitwise_xor_\" title=\"torch.Tensor.bitwise_xor_\"><code>Tensor.bitwise_xor_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.bitwise_xor#torch.Tensor.bitwise_xor\" title=\"torch.Tensor.bitwise_xor\"><code>bitwise_xor()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.bitwise_left_shift#torch.Tensor.bitwise_left_shift\" title=\"torch.Tensor.bitwise_left_shift\"><code>Tensor.bitwise_left_shift</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.bitwise_left_shift#torch.bitwise_left_shift\" title=\"torch.bitwise_left_shift\"><code>torch.bitwise_left_shift()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.bitwise_left_shift_#torch.Tensor.bitwise_left_shift_\" title=\"torch.Tensor.bitwise_left_shift_\"><code>Tensor.bitwise_left_shift_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.bitwise_left_shift#torch.Tensor.bitwise_left_shift\" title=\"torch.Tensor.bitwise_left_shift\"><code>bitwise_left_shift()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.bitwise_right_shift#torch.Tensor.bitwise_right_shift\" title=\"torch.Tensor.bitwise_right_shift\"><code>Tensor.bitwise_right_shift</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.bitwise_right_shift#torch.bitwise_right_shift\" title=\"torch.bitwise_right_shift\"><code>torch.bitwise_right_shift()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.bitwise_right_shift_#torch.Tensor.bitwise_right_shift_\" title=\"torch.Tensor.bitwise_right_shift_\"><code>Tensor.bitwise_right_shift_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.bitwise_right_shift#torch.Tensor.bitwise_right_shift\" title=\"torch.Tensor.bitwise_right_shift\"><code>bitwise_right_shift()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.bmm#torch.Tensor.bmm\" title=\"torch.Tensor.bmm\"><code>Tensor.bmm</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.bmm#torch.bmm\" title=\"torch.bmm\"><code>torch.bmm()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.bool#torch.Tensor.bool\" title=\"torch.Tensor.bool\"><code>Tensor.bool</code></a></p></td> <td><p><code>self.bool()</code> is equivalent to <code>self.to(torch.bool)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.byte#torch.Tensor.byte\" title=\"torch.Tensor.byte\"><code>Tensor.byte</code></a></p></td> <td><p><code>self.byte()</code> is equivalent to <code>self.to(torch.uint8)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.broadcast_to#torch.Tensor.broadcast_to\" title=\"torch.Tensor.broadcast_to\"><code>Tensor.broadcast_to</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.broadcast_to#torch.broadcast_to\" title=\"torch.broadcast_to\"><code>torch.broadcast_to()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.cauchy_#torch.Tensor.cauchy_\" title=\"torch.Tensor.cauchy_\"><code>Tensor.cauchy_</code></a></p></td> <td><p>Fills the tensor with numbers drawn from the Cauchy distribution:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.ceil#torch.Tensor.ceil\" title=\"torch.Tensor.ceil\"><code>Tensor.ceil</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.ceil#torch.ceil\" title=\"torch.ceil\"><code>torch.ceil()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.ceil_#torch.Tensor.ceil_\" title=\"torch.Tensor.ceil_\"><code>Tensor.ceil_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.ceil#torch.Tensor.ceil\" title=\"torch.Tensor.ceil\"><code>ceil()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.char#torch.Tensor.char\" title=\"torch.Tensor.char\"><code>Tensor.char</code></a></p></td> <td><p><code>self.char()</code> is equivalent to <code>self.to(torch.int8)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.cholesky#torch.Tensor.cholesky\" title=\"torch.Tensor.cholesky\"><code>Tensor.cholesky</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.cholesky#torch.cholesky\" title=\"torch.cholesky\"><code>torch.cholesky()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.cholesky_inverse#torch.Tensor.cholesky_inverse\" title=\"torch.Tensor.cholesky_inverse\"><code>Tensor.cholesky_inverse</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.cholesky_inverse#torch.cholesky_inverse\" title=\"torch.cholesky_inverse\"><code>torch.cholesky_inverse()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.cholesky_solve#torch.Tensor.cholesky_solve\" title=\"torch.Tensor.cholesky_solve\"><code>Tensor.cholesky_solve</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.cholesky_solve#torch.cholesky_solve\" title=\"torch.cholesky_solve\"><code>torch.cholesky_solve()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.chunk#torch.Tensor.chunk\" title=\"torch.Tensor.chunk\"><code>Tensor.chunk</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.chunk#torch.chunk\" title=\"torch.chunk\"><code>torch.chunk()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.clamp#torch.Tensor.clamp\" title=\"torch.Tensor.clamp\"><code>Tensor.clamp</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.clamp#torch.clamp\" title=\"torch.clamp\"><code>torch.clamp()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.clamp_#torch.Tensor.clamp_\" title=\"torch.Tensor.clamp_\"><code>Tensor.clamp_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.clamp#torch.Tensor.clamp\" title=\"torch.Tensor.clamp\"><code>clamp()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.clip#torch.Tensor.clip\" title=\"torch.Tensor.clip\"><code>Tensor.clip</code></a></p></td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.tensor.clamp#torch.Tensor.clamp\" title=\"torch.Tensor.clamp\"><code>clamp()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.clip_#torch.Tensor.clip_\" title=\"torch.Tensor.clip_\"><code>Tensor.clip_</code></a></p></td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.tensor.clamp_#torch.Tensor.clamp_\" title=\"torch.Tensor.clamp_\"><code>clamp_()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.clone#torch.Tensor.clone\" title=\"torch.Tensor.clone\"><code>Tensor.clone</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.clone#torch.clone\" title=\"torch.clone\"><code>torch.clone()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.contiguous#torch.Tensor.contiguous\" title=\"torch.Tensor.contiguous\"><code>Tensor.contiguous</code></a></p></td> <td><p>Returns a contiguous in memory tensor containing the same data as <code>self</code> tensor.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.copy_#torch.Tensor.copy_\" title=\"torch.Tensor.copy_\"><code>Tensor.copy_</code></a></p></td> <td><p>Copies the elements from <code>src</code> into <code>self</code> tensor and returns <code>self</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.conj#torch.Tensor.conj\" title=\"torch.Tensor.conj\"><code>Tensor.conj</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.conj#torch.conj\" title=\"torch.conj\"><code>torch.conj()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.conj_physical#torch.Tensor.conj_physical\" title=\"torch.Tensor.conj_physical\"><code>Tensor.conj_physical</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.conj_physical#torch.conj_physical\" title=\"torch.conj_physical\"><code>torch.conj_physical()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.conj_physical_#torch.Tensor.conj_physical_\" title=\"torch.Tensor.conj_physical_\"><code>Tensor.conj_physical_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.conj_physical#torch.Tensor.conj_physical\" title=\"torch.Tensor.conj_physical\"><code>conj_physical()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.resolve_conj#torch.Tensor.resolve_conj\" title=\"torch.Tensor.resolve_conj\"><code>Tensor.resolve_conj</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.resolve_conj#torch.resolve_conj\" title=\"torch.resolve_conj\"><code>torch.resolve_conj()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.resolve_neg#torch.Tensor.resolve_neg\" title=\"torch.Tensor.resolve_neg\"><code>Tensor.resolve_neg</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.resolve_neg#torch.resolve_neg\" title=\"torch.resolve_neg\"><code>torch.resolve_neg()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.copysign#torch.Tensor.copysign\" title=\"torch.Tensor.copysign\"><code>Tensor.copysign</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.copysign#torch.copysign\" title=\"torch.copysign\"><code>torch.copysign()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.copysign_#torch.Tensor.copysign_\" title=\"torch.Tensor.copysign_\"><code>Tensor.copysign_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.copysign#torch.Tensor.copysign\" title=\"torch.Tensor.copysign\"><code>copysign()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.cos#torch.Tensor.cos\" title=\"torch.Tensor.cos\"><code>Tensor.cos</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.cos#torch.cos\" title=\"torch.cos\"><code>torch.cos()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.cos_#torch.Tensor.cos_\" title=\"torch.Tensor.cos_\"><code>Tensor.cos_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.cos#torch.Tensor.cos\" title=\"torch.Tensor.cos\"><code>cos()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.cosh#torch.Tensor.cosh\" title=\"torch.Tensor.cosh\"><code>Tensor.cosh</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.cosh#torch.cosh\" title=\"torch.cosh\"><code>torch.cosh()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.cosh_#torch.Tensor.cosh_\" title=\"torch.Tensor.cosh_\"><code>Tensor.cosh_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.cosh#torch.Tensor.cosh\" title=\"torch.Tensor.cosh\"><code>cosh()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.corrcoef#torch.Tensor.corrcoef\" title=\"torch.Tensor.corrcoef\"><code>Tensor.corrcoef</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.corrcoef#torch.corrcoef\" title=\"torch.corrcoef\"><code>torch.corrcoef()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.count_nonzero#torch.Tensor.count_nonzero\" title=\"torch.Tensor.count_nonzero\"><code>Tensor.count_nonzero</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.count_nonzero#torch.count_nonzero\" title=\"torch.count_nonzero\"><code>torch.count_nonzero()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.cov#torch.Tensor.cov\" title=\"torch.Tensor.cov\"><code>Tensor.cov</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.cov#torch.cov\" title=\"torch.cov\"><code>torch.cov()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.acosh#torch.Tensor.acosh\" title=\"torch.Tensor.acosh\"><code>Tensor.acosh</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.acosh#torch.acosh\" title=\"torch.acosh\"><code>torch.acosh()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.acosh_#torch.Tensor.acosh_\" title=\"torch.Tensor.acosh_\"><code>Tensor.acosh_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.acosh#torch.Tensor.acosh\" title=\"torch.Tensor.acosh\"><code>acosh()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.arccosh#torch.Tensor.arccosh\" title=\"torch.Tensor.arccosh\"><code>Tensor.arccosh</code></a></p></td> <td><p>acosh() -&gt; Tensor</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.arccosh_#torch.Tensor.arccosh_\" title=\"torch.Tensor.arccosh_\"><code>Tensor.arccosh_</code></a></p></td> <td><p>acosh_() -&gt; Tensor</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.cpu#torch.Tensor.cpu\" title=\"torch.Tensor.cpu\"><code>Tensor.cpu</code></a></p></td> <td><p>Returns a copy of this object in CPU memory.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.cross#torch.Tensor.cross\" title=\"torch.Tensor.cross\"><code>Tensor.cross</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.cross#torch.cross\" title=\"torch.cross\"><code>torch.cross()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.cuda#torch.Tensor.cuda\" title=\"torch.Tensor.cuda\"><code>Tensor.cuda</code></a></p></td> <td><p>Returns a copy of this object in CUDA memory.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.logcumsumexp#torch.Tensor.logcumsumexp\" title=\"torch.Tensor.logcumsumexp\"><code>Tensor.logcumsumexp</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.logcumsumexp#torch.logcumsumexp\" title=\"torch.logcumsumexp\"><code>torch.logcumsumexp()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.cummax#torch.Tensor.cummax\" title=\"torch.Tensor.cummax\"><code>Tensor.cummax</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.cummax#torch.cummax\" title=\"torch.cummax\"><code>torch.cummax()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.cummin#torch.Tensor.cummin\" title=\"torch.Tensor.cummin\"><code>Tensor.cummin</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.cummin#torch.cummin\" title=\"torch.cummin\"><code>torch.cummin()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.cumprod#torch.Tensor.cumprod\" title=\"torch.Tensor.cumprod\"><code>Tensor.cumprod</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.cumprod#torch.cumprod\" title=\"torch.cumprod\"><code>torch.cumprod()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.cumprod_#torch.Tensor.cumprod_\" title=\"torch.Tensor.cumprod_\"><code>Tensor.cumprod_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.cumprod#torch.Tensor.cumprod\" title=\"torch.Tensor.cumprod\"><code>cumprod()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.cumsum#torch.Tensor.cumsum\" title=\"torch.Tensor.cumsum\"><code>Tensor.cumsum</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.cumsum#torch.cumsum\" title=\"torch.cumsum\"><code>torch.cumsum()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.cumsum_#torch.Tensor.cumsum_\" title=\"torch.Tensor.cumsum_\"><code>Tensor.cumsum_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.cumsum#torch.Tensor.cumsum\" title=\"torch.Tensor.cumsum\"><code>cumsum()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.chalf#torch.Tensor.chalf\" title=\"torch.Tensor.chalf\"><code>Tensor.chalf</code></a></p></td> <td><p><code>self.chalf()</code> is equivalent to <code>self.to(torch.complex32)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.cfloat#torch.Tensor.cfloat\" title=\"torch.Tensor.cfloat\"><code>Tensor.cfloat</code></a></p></td> <td><p><code>self.cfloat()</code> is equivalent to <code>self.to(torch.complex64)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.cdouble#torch.Tensor.cdouble\" title=\"torch.Tensor.cdouble\"><code>Tensor.cdouble</code></a></p></td> <td><p><code>self.cdouble()</code> is equivalent to <code>self.to(torch.complex128)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.data_ptr#torch.Tensor.data_ptr\" title=\"torch.Tensor.data_ptr\"><code>Tensor.data_ptr</code></a></p></td> <td><p>Returns the address of the first element of <code>self</code> tensor.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.deg2rad#torch.Tensor.deg2rad\" title=\"torch.Tensor.deg2rad\"><code>Tensor.deg2rad</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.deg2rad#torch.deg2rad\" title=\"torch.deg2rad\"><code>torch.deg2rad()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.dequantize#torch.Tensor.dequantize\" title=\"torch.Tensor.dequantize\"><code>Tensor.dequantize</code></a></p></td> <td><p>Given a quantized Tensor, dequantize it and return the dequantized float Tensor.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.det#torch.Tensor.det\" title=\"torch.Tensor.det\"><code>Tensor.det</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.det#torch.det\" title=\"torch.det\"><code>torch.det()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.dense_dim#torch.Tensor.dense_dim\" title=\"torch.Tensor.dense_dim\"><code>Tensor.dense_dim</code></a></p></td> <td><p>Return the number of dense dimensions in a <a class=\"reference internal\" href=\"sparse#sparse-docs\"><span class=\"std std-ref\">sparse tensor</span></a> <code>self</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.detach#torch.Tensor.detach\" title=\"torch.Tensor.detach\"><code>Tensor.detach</code></a></p></td> <td><p>Returns a new Tensor, detached from the current graph.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.detach_#torch.Tensor.detach_\" title=\"torch.Tensor.detach_\"><code>Tensor.detach_</code></a></p></td> <td><p>Detaches the Tensor from the graph that created it, making it a leaf.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.diag#torch.Tensor.diag\" title=\"torch.Tensor.diag\"><code>Tensor.diag</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.diag#torch.diag\" title=\"torch.diag\"><code>torch.diag()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.diag_embed#torch.Tensor.diag_embed\" title=\"torch.Tensor.diag_embed\"><code>Tensor.diag_embed</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.diag_embed#torch.diag_embed\" title=\"torch.diag_embed\"><code>torch.diag_embed()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.diagflat#torch.Tensor.diagflat\" title=\"torch.Tensor.diagflat\"><code>Tensor.diagflat</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.diagflat#torch.diagflat\" title=\"torch.diagflat\"><code>torch.diagflat()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.diagonal#torch.Tensor.diagonal\" title=\"torch.Tensor.diagonal\"><code>Tensor.diagonal</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.diagonal#torch.diagonal\" title=\"torch.diagonal\"><code>torch.diagonal()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.diagonal_scatter#torch.Tensor.diagonal_scatter\" title=\"torch.Tensor.diagonal_scatter\"><code>Tensor.diagonal_scatter</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.diagonal_scatter#torch.diagonal_scatter\" title=\"torch.diagonal_scatter\"><code>torch.diagonal_scatter()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.fill_diagonal_#torch.Tensor.fill_diagonal_\" title=\"torch.Tensor.fill_diagonal_\"><code>Tensor.fill_diagonal_</code></a></p></td> <td><p>Fill the main diagonal of a tensor that has at least 2-dimensions.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.fmax#torch.Tensor.fmax\" title=\"torch.Tensor.fmax\"><code>Tensor.fmax</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.fmax#torch.fmax\" title=\"torch.fmax\"><code>torch.fmax()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.fmin#torch.Tensor.fmin\" title=\"torch.Tensor.fmin\"><code>Tensor.fmin</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.fmin#torch.fmin\" title=\"torch.fmin\"><code>torch.fmin()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.diff#torch.Tensor.diff\" title=\"torch.Tensor.diff\"><code>Tensor.diff</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.diff#torch.diff\" title=\"torch.diff\"><code>torch.diff()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.digamma#torch.Tensor.digamma\" title=\"torch.Tensor.digamma\"><code>Tensor.digamma</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.digamma#torch.digamma\" title=\"torch.digamma\"><code>torch.digamma()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.digamma_#torch.Tensor.digamma_\" title=\"torch.Tensor.digamma_\"><code>Tensor.digamma_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.digamma#torch.Tensor.digamma\" title=\"torch.Tensor.digamma\"><code>digamma()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.dim#torch.Tensor.dim\" title=\"torch.Tensor.dim\"><code>Tensor.dim</code></a></p></td> <td><p>Returns the number of dimensions of <code>self</code> tensor.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.dim_order#torch.Tensor.dim_order\" title=\"torch.Tensor.dim_order\"><code>Tensor.dim_order</code></a></p></td> <td><p>Returns a tuple of int describing the dim order or physical layout of <code>self</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.dist#torch.Tensor.dist\" title=\"torch.Tensor.dist\"><code>Tensor.dist</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.dist#torch.dist\" title=\"torch.dist\"><code>torch.dist()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.div#torch.Tensor.div\" title=\"torch.Tensor.div\"><code>Tensor.div</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.div#torch.div\" title=\"torch.div\"><code>torch.div()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.div_#torch.Tensor.div_\" title=\"torch.Tensor.div_\"><code>Tensor.div_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.div#torch.Tensor.div\" title=\"torch.Tensor.div\"><code>div()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.divide#torch.Tensor.divide\" title=\"torch.Tensor.divide\"><code>Tensor.divide</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.divide#torch.divide\" title=\"torch.divide\"><code>torch.divide()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.divide_#torch.Tensor.divide_\" title=\"torch.Tensor.divide_\"><code>Tensor.divide_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.divide#torch.Tensor.divide\" title=\"torch.Tensor.divide\"><code>divide()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.dot#torch.Tensor.dot\" title=\"torch.Tensor.dot\"><code>Tensor.dot</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.dot#torch.dot\" title=\"torch.dot\"><code>torch.dot()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.double#torch.Tensor.double\" title=\"torch.Tensor.double\"><code>Tensor.double</code></a></p></td> <td><p><code>self.double()</code> is equivalent to <code>self.to(torch.float64)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.dsplit#torch.Tensor.dsplit\" title=\"torch.Tensor.dsplit\"><code>Tensor.dsplit</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.dsplit#torch.dsplit\" title=\"torch.dsplit\"><code>torch.dsplit()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.element_size#torch.Tensor.element_size\" title=\"torch.Tensor.element_size\"><code>Tensor.element_size</code></a></p></td> <td><p>Returns the size in bytes of an individual element.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.eq#torch.Tensor.eq\" title=\"torch.Tensor.eq\"><code>Tensor.eq</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.eq#torch.eq\" title=\"torch.eq\"><code>torch.eq()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.eq_#torch.Tensor.eq_\" title=\"torch.Tensor.eq_\"><code>Tensor.eq_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.eq#torch.Tensor.eq\" title=\"torch.Tensor.eq\"><code>eq()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.equal#torch.Tensor.equal\" title=\"torch.Tensor.equal\"><code>Tensor.equal</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.equal#torch.equal\" title=\"torch.equal\"><code>torch.equal()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.erf#torch.Tensor.erf\" title=\"torch.Tensor.erf\"><code>Tensor.erf</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.erf#torch.erf\" title=\"torch.erf\"><code>torch.erf()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.erf_#torch.Tensor.erf_\" title=\"torch.Tensor.erf_\"><code>Tensor.erf_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.erf#torch.Tensor.erf\" title=\"torch.Tensor.erf\"><code>erf()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.erfc#torch.Tensor.erfc\" title=\"torch.Tensor.erfc\"><code>Tensor.erfc</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.erfc#torch.erfc\" title=\"torch.erfc\"><code>torch.erfc()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.erfc_#torch.Tensor.erfc_\" title=\"torch.Tensor.erfc_\"><code>Tensor.erfc_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.erfc#torch.Tensor.erfc\" title=\"torch.Tensor.erfc\"><code>erfc()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.erfinv#torch.Tensor.erfinv\" title=\"torch.Tensor.erfinv\"><code>Tensor.erfinv</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.erfinv#torch.erfinv\" title=\"torch.erfinv\"><code>torch.erfinv()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.erfinv_#torch.Tensor.erfinv_\" title=\"torch.Tensor.erfinv_\"><code>Tensor.erfinv_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.erfinv#torch.Tensor.erfinv\" title=\"torch.Tensor.erfinv\"><code>erfinv()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.exp#torch.Tensor.exp\" title=\"torch.Tensor.exp\"><code>Tensor.exp</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.exp#torch.exp\" title=\"torch.exp\"><code>torch.exp()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.exp_#torch.Tensor.exp_\" title=\"torch.Tensor.exp_\"><code>Tensor.exp_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.exp#torch.Tensor.exp\" title=\"torch.Tensor.exp\"><code>exp()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.expm1#torch.Tensor.expm1\" title=\"torch.Tensor.expm1\"><code>Tensor.expm1</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.expm1#torch.expm1\" title=\"torch.expm1\"><code>torch.expm1()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.expm1_#torch.Tensor.expm1_\" title=\"torch.Tensor.expm1_\"><code>Tensor.expm1_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.expm1#torch.Tensor.expm1\" title=\"torch.Tensor.expm1\"><code>expm1()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.expand#torch.Tensor.expand\" title=\"torch.Tensor.expand\"><code>Tensor.expand</code></a></p></td> <td><p>Returns a new view of the <code>self</code> tensor with singleton dimensions expanded to a larger size.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.expand_as#torch.Tensor.expand_as\" title=\"torch.Tensor.expand_as\"><code>Tensor.expand_as</code></a></p></td> <td><p>Expand this tensor to the same size as <code>other</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.exponential_#torch.Tensor.exponential_\" title=\"torch.Tensor.exponential_\"><code>Tensor.exponential_</code></a></p></td> <td><p>Fills <code>self</code> tensor with elements drawn from the exponential distribution:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.fix#torch.Tensor.fix\" title=\"torch.Tensor.fix\"><code>Tensor.fix</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.fix#torch.fix\" title=\"torch.fix\"><code>torch.fix()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.fix_#torch.Tensor.fix_\" title=\"torch.Tensor.fix_\"><code>Tensor.fix_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.fix#torch.Tensor.fix\" title=\"torch.Tensor.fix\"><code>fix()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.fill_#torch.Tensor.fill_\" title=\"torch.Tensor.fill_\"><code>Tensor.fill_</code></a></p></td> <td><p>Fills <code>self</code> tensor with the specified value.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.flatten#torch.Tensor.flatten\" title=\"torch.Tensor.flatten\"><code>Tensor.flatten</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.flatten#torch.flatten\" title=\"torch.flatten\"><code>torch.flatten()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.flip#torch.Tensor.flip\" title=\"torch.Tensor.flip\"><code>Tensor.flip</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.flip#torch.flip\" title=\"torch.flip\"><code>torch.flip()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.fliplr#torch.Tensor.fliplr\" title=\"torch.Tensor.fliplr\"><code>Tensor.fliplr</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.fliplr#torch.fliplr\" title=\"torch.fliplr\"><code>torch.fliplr()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.flipud#torch.Tensor.flipud\" title=\"torch.Tensor.flipud\"><code>Tensor.flipud</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.flipud#torch.flipud\" title=\"torch.flipud\"><code>torch.flipud()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.float#torch.Tensor.float\" title=\"torch.Tensor.float\"><code>Tensor.float</code></a></p></td> <td><p><code>self.float()</code> is equivalent to <code>self.to(torch.float32)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.float_power#torch.Tensor.float_power\" title=\"torch.Tensor.float_power\"><code>Tensor.float_power</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.float_power#torch.float_power\" title=\"torch.float_power\"><code>torch.float_power()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.float_power_#torch.Tensor.float_power_\" title=\"torch.Tensor.float_power_\"><code>Tensor.float_power_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.float_power#torch.Tensor.float_power\" title=\"torch.Tensor.float_power\"><code>float_power()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.floor#torch.Tensor.floor\" title=\"torch.Tensor.floor\"><code>Tensor.floor</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.floor#torch.floor\" title=\"torch.floor\"><code>torch.floor()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.floor_#torch.Tensor.floor_\" title=\"torch.Tensor.floor_\"><code>Tensor.floor_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.floor#torch.Tensor.floor\" title=\"torch.Tensor.floor\"><code>floor()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.floor_divide#torch.Tensor.floor_divide\" title=\"torch.Tensor.floor_divide\"><code>Tensor.floor_divide</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.floor_divide#torch.floor_divide\" title=\"torch.floor_divide\"><code>torch.floor_divide()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.floor_divide_#torch.Tensor.floor_divide_\" title=\"torch.Tensor.floor_divide_\"><code>Tensor.floor_divide_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.floor_divide#torch.Tensor.floor_divide\" title=\"torch.Tensor.floor_divide\"><code>floor_divide()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.fmod#torch.Tensor.fmod\" title=\"torch.Tensor.fmod\"><code>Tensor.fmod</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.fmod#torch.fmod\" title=\"torch.fmod\"><code>torch.fmod()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.fmod_#torch.Tensor.fmod_\" title=\"torch.Tensor.fmod_\"><code>Tensor.fmod_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.fmod#torch.Tensor.fmod\" title=\"torch.Tensor.fmod\"><code>fmod()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.frac#torch.Tensor.frac\" title=\"torch.Tensor.frac\"><code>Tensor.frac</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.frac#torch.frac\" title=\"torch.frac\"><code>torch.frac()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.frac_#torch.Tensor.frac_\" title=\"torch.Tensor.frac_\"><code>Tensor.frac_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.frac#torch.Tensor.frac\" title=\"torch.Tensor.frac\"><code>frac()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.frexp#torch.Tensor.frexp\" title=\"torch.Tensor.frexp\"><code>Tensor.frexp</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.frexp#torch.frexp\" title=\"torch.frexp\"><code>torch.frexp()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.gather#torch.Tensor.gather\" title=\"torch.Tensor.gather\"><code>Tensor.gather</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.gather#torch.gather\" title=\"torch.gather\"><code>torch.gather()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.gcd#torch.Tensor.gcd\" title=\"torch.Tensor.gcd\"><code>Tensor.gcd</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.gcd#torch.gcd\" title=\"torch.gcd\"><code>torch.gcd()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.gcd_#torch.Tensor.gcd_\" title=\"torch.Tensor.gcd_\"><code>Tensor.gcd_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.gcd#torch.Tensor.gcd\" title=\"torch.Tensor.gcd\"><code>gcd()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.ge#torch.Tensor.ge\" title=\"torch.Tensor.ge\"><code>Tensor.ge</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.ge#torch.ge\" title=\"torch.ge\"><code>torch.ge()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.ge_#torch.Tensor.ge_\" title=\"torch.Tensor.ge_\"><code>Tensor.ge_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.ge#torch.Tensor.ge\" title=\"torch.Tensor.ge\"><code>ge()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.greater_equal#torch.Tensor.greater_equal\" title=\"torch.Tensor.greater_equal\"><code>Tensor.greater_equal</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.greater_equal#torch.greater_equal\" title=\"torch.greater_equal\"><code>torch.greater_equal()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.greater_equal_#torch.Tensor.greater_equal_\" title=\"torch.Tensor.greater_equal_\"><code>Tensor.greater_equal_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.greater_equal#torch.Tensor.greater_equal\" title=\"torch.Tensor.greater_equal\"><code>greater_equal()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.geometric_#torch.Tensor.geometric_\" title=\"torch.Tensor.geometric_\"><code>Tensor.geometric_</code></a></p></td> <td><p>Fills <code>self</code> tensor with elements drawn from the geometric distribution:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.geqrf#torch.Tensor.geqrf\" title=\"torch.Tensor.geqrf\"><code>Tensor.geqrf</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.geqrf#torch.geqrf\" title=\"torch.geqrf\"><code>torch.geqrf()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.ger#torch.Tensor.ger\" title=\"torch.Tensor.ger\"><code>Tensor.ger</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.ger#torch.ger\" title=\"torch.ger\"><code>torch.ger()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.get_device#torch.Tensor.get_device\" title=\"torch.Tensor.get_device\"><code>Tensor.get_device</code></a></p></td> <td><p>For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.gt#torch.Tensor.gt\" title=\"torch.Tensor.gt\"><code>Tensor.gt</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.gt#torch.gt\" title=\"torch.gt\"><code>torch.gt()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.gt_#torch.Tensor.gt_\" title=\"torch.Tensor.gt_\"><code>Tensor.gt_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.gt#torch.Tensor.gt\" title=\"torch.Tensor.gt\"><code>gt()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.greater#torch.Tensor.greater\" title=\"torch.Tensor.greater\"><code>Tensor.greater</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.greater#torch.greater\" title=\"torch.greater\"><code>torch.greater()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.greater_#torch.Tensor.greater_\" title=\"torch.Tensor.greater_\"><code>Tensor.greater_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.greater#torch.Tensor.greater\" title=\"torch.Tensor.greater\"><code>greater()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.half#torch.Tensor.half\" title=\"torch.Tensor.half\"><code>Tensor.half</code></a></p></td> <td><p><code>self.half()</code> is equivalent to <code>self.to(torch.float16)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.hardshrink#torch.Tensor.hardshrink\" title=\"torch.Tensor.hardshrink\"><code>Tensor.hardshrink</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.nn.functional.hardshrink#torch.nn.functional.hardshrink\" title=\"torch.nn.functional.hardshrink\"><code>torch.nn.functional.hardshrink()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.heaviside#torch.Tensor.heaviside\" title=\"torch.Tensor.heaviside\"><code>Tensor.heaviside</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.heaviside#torch.heaviside\" title=\"torch.heaviside\"><code>torch.heaviside()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.histc#torch.Tensor.histc\" title=\"torch.Tensor.histc\"><code>Tensor.histc</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.histc#torch.histc\" title=\"torch.histc\"><code>torch.histc()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.histogram#torch.Tensor.histogram\" title=\"torch.Tensor.histogram\"><code>Tensor.histogram</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.histogram#torch.histogram\" title=\"torch.histogram\"><code>torch.histogram()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.hsplit#torch.Tensor.hsplit\" title=\"torch.Tensor.hsplit\"><code>Tensor.hsplit</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.hsplit#torch.hsplit\" title=\"torch.hsplit\"><code>torch.hsplit()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.hypot#torch.Tensor.hypot\" title=\"torch.Tensor.hypot\"><code>Tensor.hypot</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.hypot#torch.hypot\" title=\"torch.hypot\"><code>torch.hypot()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.hypot_#torch.Tensor.hypot_\" title=\"torch.Tensor.hypot_\"><code>Tensor.hypot_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.hypot#torch.Tensor.hypot\" title=\"torch.Tensor.hypot\"><code>hypot()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.i0#torch.Tensor.i0\" title=\"torch.Tensor.i0\"><code>Tensor.i0</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.i0#torch.i0\" title=\"torch.i0\"><code>torch.i0()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.i0_#torch.Tensor.i0_\" title=\"torch.Tensor.i0_\"><code>Tensor.i0_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.i0#torch.Tensor.i0\" title=\"torch.Tensor.i0\"><code>i0()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.igamma#torch.Tensor.igamma\" title=\"torch.Tensor.igamma\"><code>Tensor.igamma</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.igamma#torch.igamma\" title=\"torch.igamma\"><code>torch.igamma()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.igamma_#torch.Tensor.igamma_\" title=\"torch.Tensor.igamma_\"><code>Tensor.igamma_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.igamma#torch.Tensor.igamma\" title=\"torch.Tensor.igamma\"><code>igamma()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.igammac#torch.Tensor.igammac\" title=\"torch.Tensor.igammac\"><code>Tensor.igammac</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.igammac#torch.igammac\" title=\"torch.igammac\"><code>torch.igammac()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.igammac_#torch.Tensor.igammac_\" title=\"torch.Tensor.igammac_\"><code>Tensor.igammac_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.igammac#torch.Tensor.igammac\" title=\"torch.Tensor.igammac\"><code>igammac()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.index_add_#torch.Tensor.index_add_\" title=\"torch.Tensor.index_add_\"><code>Tensor.index_add_</code></a></p></td> <td><p>Accumulate the elements of <code>alpha</code> times <code>source</code> into the <code>self</code> tensor by adding to the indices in the order given in <code>index</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.index_add#torch.Tensor.index_add\" title=\"torch.Tensor.index_add\"><code>Tensor.index_add</code></a></p></td> <td><p>Out-of-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.index_add_#torch.Tensor.index_add_\" title=\"torch.Tensor.index_add_\"><code>torch.Tensor.index_add_()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.index_copy_#torch.Tensor.index_copy_\" title=\"torch.Tensor.index_copy_\"><code>Tensor.index_copy_</code></a></p></td> <td><p>Copies the elements of <a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>tensor</code></a> into the <code>self</code> tensor by selecting the indices in the order given in <code>index</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.index_copy#torch.Tensor.index_copy\" title=\"torch.Tensor.index_copy\"><code>Tensor.index_copy</code></a></p></td> <td><p>Out-of-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.index_copy_#torch.Tensor.index_copy_\" title=\"torch.Tensor.index_copy_\"><code>torch.Tensor.index_copy_()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.index_fill_#torch.Tensor.index_fill_\" title=\"torch.Tensor.index_fill_\"><code>Tensor.index_fill_</code></a></p></td> <td><p>Fills the elements of the <code>self</code> tensor with value <code>value</code> by selecting the indices in the order given in <code>index</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.index_fill#torch.Tensor.index_fill\" title=\"torch.Tensor.index_fill\"><code>Tensor.index_fill</code></a></p></td> <td><p>Out-of-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.index_fill_#torch.Tensor.index_fill_\" title=\"torch.Tensor.index_fill_\"><code>torch.Tensor.index_fill_()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.index_put_#torch.Tensor.index_put_\" title=\"torch.Tensor.index_put_\"><code>Tensor.index_put_</code></a></p></td> <td><p>Puts values from the tensor <code>values</code> into the tensor <code>self</code> using the indices specified in <code>indices</code> (which is a tuple of Tensors).</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.index_put#torch.Tensor.index_put\" title=\"torch.Tensor.index_put\"><code>Tensor.index_put</code></a></p></td> <td><p>Out-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.index_put_#torch.Tensor.index_put_\" title=\"torch.Tensor.index_put_\"><code>index_put_()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.index_reduce_#torch.Tensor.index_reduce_\" title=\"torch.Tensor.index_reduce_\"><code>Tensor.index_reduce_</code></a></p></td> <td><p>Accumulate the elements of <code>source</code> into the <code>self</code> tensor by accumulating to the indices in the order given in <code>index</code> using the reduction given by the <code>reduce</code> argument.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.index_reduce#torch.Tensor.index_reduce\" title=\"torch.Tensor.index_reduce\"><code>Tensor.index_reduce</code></a></p></td> <td></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.index_select#torch.Tensor.index_select\" title=\"torch.Tensor.index_select\"><code>Tensor.index_select</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.index_select#torch.index_select\" title=\"torch.index_select\"><code>torch.index_select()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.indices#torch.Tensor.indices\" title=\"torch.Tensor.indices\"><code>Tensor.indices</code></a></p></td> <td><p>Return the indices tensor of a <a class=\"reference internal\" href=\"sparse#sparse-coo-docs\"><span class=\"std std-ref\">sparse COO tensor</span></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.inner#torch.Tensor.inner\" title=\"torch.Tensor.inner\"><code>Tensor.inner</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.inner#torch.inner\" title=\"torch.inner\"><code>torch.inner()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.int#torch.Tensor.int\" title=\"torch.Tensor.int\"><code>Tensor.int</code></a></p></td> <td><p><code>self.int()</code> is equivalent to <code>self.to(torch.int32)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.int_repr#torch.Tensor.int_repr\" title=\"torch.Tensor.int_repr\"><code>Tensor.int_repr</code></a></p></td> <td><p>Given a quantized Tensor, <code>self.int_repr()</code> returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.inverse#torch.Tensor.inverse\" title=\"torch.Tensor.inverse\"><code>Tensor.inverse</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.inverse#torch.inverse\" title=\"torch.inverse\"><code>torch.inverse()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.isclose#torch.Tensor.isclose\" title=\"torch.Tensor.isclose\"><code>Tensor.isclose</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.isclose#torch.isclose\" title=\"torch.isclose\"><code>torch.isclose()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.isfinite#torch.Tensor.isfinite\" title=\"torch.Tensor.isfinite\"><code>Tensor.isfinite</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.isfinite#torch.isfinite\" title=\"torch.isfinite\"><code>torch.isfinite()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.isinf#torch.Tensor.isinf\" title=\"torch.Tensor.isinf\"><code>Tensor.isinf</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.isinf#torch.isinf\" title=\"torch.isinf\"><code>torch.isinf()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.isposinf#torch.Tensor.isposinf\" title=\"torch.Tensor.isposinf\"><code>Tensor.isposinf</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.isposinf#torch.isposinf\" title=\"torch.isposinf\"><code>torch.isposinf()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.isneginf#torch.Tensor.isneginf\" title=\"torch.Tensor.isneginf\"><code>Tensor.isneginf</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.isneginf#torch.isneginf\" title=\"torch.isneginf\"><code>torch.isneginf()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.isnan#torch.Tensor.isnan\" title=\"torch.Tensor.isnan\"><code>Tensor.isnan</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.isnan#torch.isnan\" title=\"torch.isnan\"><code>torch.isnan()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.is_contiguous#torch.Tensor.is_contiguous\" title=\"torch.Tensor.is_contiguous\"><code>Tensor.is_contiguous</code></a></p></td> <td><p>Returns True if <code>self</code> tensor is contiguous in memory in the order specified by memory format.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.is_complex#torch.Tensor.is_complex\" title=\"torch.Tensor.is_complex\"><code>Tensor.is_complex</code></a></p></td> <td><p>Returns True if the data type of <code>self</code> is a complex data type.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.is_conj#torch.Tensor.is_conj\" title=\"torch.Tensor.is_conj\"><code>Tensor.is_conj</code></a></p></td> <td><p>Returns True if the conjugate bit of <code>self</code> is set to true.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.is_floating_point#torch.Tensor.is_floating_point\" title=\"torch.Tensor.is_floating_point\"><code>Tensor.is_floating_point</code></a></p></td> <td><p>Returns True if the data type of <code>self</code> is a floating point data type.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.is_inference#torch.Tensor.is_inference\" title=\"torch.Tensor.is_inference\"><code>Tensor.is_inference</code></a></p></td> <td><p>See <code>torch.is_inference()</code></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.is_leaf#torch.Tensor.is_leaf\" title=\"torch.Tensor.is_leaf\"><code>Tensor.is_leaf</code></a></p></td> <td><p>All Tensors that have <code>requires_grad</code> which is <code>False</code> will be leaf Tensors by convention.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.is_pinned#torch.Tensor.is_pinned\" title=\"torch.Tensor.is_pinned\"><code>Tensor.is_pinned</code></a></p></td> <td><p>Returns true if this tensor resides in pinned memory.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.is_set_to#torch.Tensor.is_set_to\" title=\"torch.Tensor.is_set_to\"><code>Tensor.is_set_to</code></a></p></td> <td><p>Returns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride).</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.is_shared#torch.Tensor.is_shared\" title=\"torch.Tensor.is_shared\"><code>Tensor.is_shared</code></a></p></td> <td><p>Checks if tensor is in shared memory.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.is_signed#torch.Tensor.is_signed\" title=\"torch.Tensor.is_signed\"><code>Tensor.is_signed</code></a></p></td> <td><p>Returns True if the data type of <code>self</code> is a signed data type.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.is_sparse#torch.Tensor.is_sparse\" title=\"torch.Tensor.is_sparse\"><code>Tensor.is_sparse</code></a></p></td> <td><p>Is <code>True</code> if the Tensor uses sparse COO storage layout, <code>False</code> otherwise.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.istft#torch.Tensor.istft\" title=\"torch.Tensor.istft\"><code>Tensor.istft</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.istft#torch.istft\" title=\"torch.istft\"><code>torch.istft()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.isreal#torch.Tensor.isreal\" title=\"torch.Tensor.isreal\"><code>Tensor.isreal</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.isreal#torch.isreal\" title=\"torch.isreal\"><code>torch.isreal()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.item#torch.Tensor.item\" title=\"torch.Tensor.item\"><code>Tensor.item</code></a></p></td> <td><p>Returns the value of this tensor as a standard Python number.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.kthvalue#torch.Tensor.kthvalue\" title=\"torch.Tensor.kthvalue\"><code>Tensor.kthvalue</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.kthvalue#torch.kthvalue\" title=\"torch.kthvalue\"><code>torch.kthvalue()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.lcm#torch.Tensor.lcm\" title=\"torch.Tensor.lcm\"><code>Tensor.lcm</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.lcm#torch.lcm\" title=\"torch.lcm\"><code>torch.lcm()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.lcm_#torch.Tensor.lcm_\" title=\"torch.Tensor.lcm_\"><code>Tensor.lcm_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.lcm#torch.Tensor.lcm\" title=\"torch.Tensor.lcm\"><code>lcm()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.ldexp#torch.Tensor.ldexp\" title=\"torch.Tensor.ldexp\"><code>Tensor.ldexp</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.ldexp#torch.ldexp\" title=\"torch.ldexp\"><code>torch.ldexp()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.ldexp_#torch.Tensor.ldexp_\" title=\"torch.Tensor.ldexp_\"><code>Tensor.ldexp_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.ldexp#torch.Tensor.ldexp\" title=\"torch.Tensor.ldexp\"><code>ldexp()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.le#torch.Tensor.le\" title=\"torch.Tensor.le\"><code>Tensor.le</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.le#torch.le\" title=\"torch.le\"><code>torch.le()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.le_#torch.Tensor.le_\" title=\"torch.Tensor.le_\"><code>Tensor.le_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.le#torch.Tensor.le\" title=\"torch.Tensor.le\"><code>le()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.less_equal#torch.Tensor.less_equal\" title=\"torch.Tensor.less_equal\"><code>Tensor.less_equal</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.less_equal#torch.less_equal\" title=\"torch.less_equal\"><code>torch.less_equal()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.less_equal_#torch.Tensor.less_equal_\" title=\"torch.Tensor.less_equal_\"><code>Tensor.less_equal_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.less_equal#torch.Tensor.less_equal\" title=\"torch.Tensor.less_equal\"><code>less_equal()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.lerp#torch.Tensor.lerp\" title=\"torch.Tensor.lerp\"><code>Tensor.lerp</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.lerp#torch.lerp\" title=\"torch.lerp\"><code>torch.lerp()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.lerp_#torch.Tensor.lerp_\" title=\"torch.Tensor.lerp_\"><code>Tensor.lerp_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.lerp#torch.Tensor.lerp\" title=\"torch.Tensor.lerp\"><code>lerp()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.lgamma#torch.Tensor.lgamma\" title=\"torch.Tensor.lgamma\"><code>Tensor.lgamma</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.lgamma#torch.lgamma\" title=\"torch.lgamma\"><code>torch.lgamma()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.lgamma_#torch.Tensor.lgamma_\" title=\"torch.Tensor.lgamma_\"><code>Tensor.lgamma_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.lgamma#torch.Tensor.lgamma\" title=\"torch.Tensor.lgamma\"><code>lgamma()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.log#torch.Tensor.log\" title=\"torch.Tensor.log\"><code>Tensor.log</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.log#torch.log\" title=\"torch.log\"><code>torch.log()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.log_#torch.Tensor.log_\" title=\"torch.Tensor.log_\"><code>Tensor.log_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.log#torch.Tensor.log\" title=\"torch.Tensor.log\"><code>log()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.logdet#torch.Tensor.logdet\" title=\"torch.Tensor.logdet\"><code>Tensor.logdet</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.logdet#torch.logdet\" title=\"torch.logdet\"><code>torch.logdet()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.log10#torch.Tensor.log10\" title=\"torch.Tensor.log10\"><code>Tensor.log10</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.log10#torch.log10\" title=\"torch.log10\"><code>torch.log10()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.log10_#torch.Tensor.log10_\" title=\"torch.Tensor.log10_\"><code>Tensor.log10_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.log10#torch.Tensor.log10\" title=\"torch.Tensor.log10\"><code>log10()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.log1p#torch.Tensor.log1p\" title=\"torch.Tensor.log1p\"><code>Tensor.log1p</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.log1p#torch.log1p\" title=\"torch.log1p\"><code>torch.log1p()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.log1p_#torch.Tensor.log1p_\" title=\"torch.Tensor.log1p_\"><code>Tensor.log1p_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.log1p#torch.Tensor.log1p\" title=\"torch.Tensor.log1p\"><code>log1p()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.log2#torch.Tensor.log2\" title=\"torch.Tensor.log2\"><code>Tensor.log2</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.log2#torch.log2\" title=\"torch.log2\"><code>torch.log2()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.log2_#torch.Tensor.log2_\" title=\"torch.Tensor.log2_\"><code>Tensor.log2_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.log2#torch.Tensor.log2\" title=\"torch.Tensor.log2\"><code>log2()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.log_normal_#torch.Tensor.log_normal_\" title=\"torch.Tensor.log_normal_\"><code>Tensor.log_normal_</code></a></p></td> <td><p>Fills <code>self</code> tensor with numbers samples from the log-normal distribution parameterized by the given mean <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span></span></span> and standard deviation <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.logaddexp#torch.Tensor.logaddexp\" title=\"torch.Tensor.logaddexp\"><code>Tensor.logaddexp</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.logaddexp#torch.logaddexp\" title=\"torch.logaddexp\"><code>torch.logaddexp()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.logaddexp2#torch.Tensor.logaddexp2\" title=\"torch.Tensor.logaddexp2\"><code>Tensor.logaddexp2</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.logaddexp2#torch.logaddexp2\" title=\"torch.logaddexp2\"><code>torch.logaddexp2()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.logsumexp#torch.Tensor.logsumexp\" title=\"torch.Tensor.logsumexp\"><code>Tensor.logsumexp</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.logsumexp#torch.logsumexp\" title=\"torch.logsumexp\"><code>torch.logsumexp()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.logical_and#torch.Tensor.logical_and\" title=\"torch.Tensor.logical_and\"><code>Tensor.logical_and</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.logical_and#torch.logical_and\" title=\"torch.logical_and\"><code>torch.logical_and()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.logical_and_#torch.Tensor.logical_and_\" title=\"torch.Tensor.logical_and_\"><code>Tensor.logical_and_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.logical_and#torch.Tensor.logical_and\" title=\"torch.Tensor.logical_and\"><code>logical_and()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.logical_not#torch.Tensor.logical_not\" title=\"torch.Tensor.logical_not\"><code>Tensor.logical_not</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.logical_not#torch.logical_not\" title=\"torch.logical_not\"><code>torch.logical_not()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.logical_not_#torch.Tensor.logical_not_\" title=\"torch.Tensor.logical_not_\"><code>Tensor.logical_not_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.logical_not#torch.Tensor.logical_not\" title=\"torch.Tensor.logical_not\"><code>logical_not()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.logical_or#torch.Tensor.logical_or\" title=\"torch.Tensor.logical_or\"><code>Tensor.logical_or</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.logical_or#torch.logical_or\" title=\"torch.logical_or\"><code>torch.logical_or()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.logical_or_#torch.Tensor.logical_or_\" title=\"torch.Tensor.logical_or_\"><code>Tensor.logical_or_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.logical_or#torch.Tensor.logical_or\" title=\"torch.Tensor.logical_or\"><code>logical_or()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.logical_xor#torch.Tensor.logical_xor\" title=\"torch.Tensor.logical_xor\"><code>Tensor.logical_xor</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.logical_xor#torch.logical_xor\" title=\"torch.logical_xor\"><code>torch.logical_xor()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.logical_xor_#torch.Tensor.logical_xor_\" title=\"torch.Tensor.logical_xor_\"><code>Tensor.logical_xor_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.logical_xor#torch.Tensor.logical_xor\" title=\"torch.Tensor.logical_xor\"><code>logical_xor()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.logit#torch.Tensor.logit\" title=\"torch.Tensor.logit\"><code>Tensor.logit</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.logit#torch.logit\" title=\"torch.logit\"><code>torch.logit()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.logit_#torch.Tensor.logit_\" title=\"torch.Tensor.logit_\"><code>Tensor.logit_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.logit#torch.Tensor.logit\" title=\"torch.Tensor.logit\"><code>logit()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.long#torch.Tensor.long\" title=\"torch.Tensor.long\"><code>Tensor.long</code></a></p></td> <td><p><code>self.long()</code> is equivalent to <code>self.to(torch.int64)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.lt#torch.Tensor.lt\" title=\"torch.Tensor.lt\"><code>Tensor.lt</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.lt#torch.lt\" title=\"torch.lt\"><code>torch.lt()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.lt_#torch.Tensor.lt_\" title=\"torch.Tensor.lt_\"><code>Tensor.lt_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.lt#torch.Tensor.lt\" title=\"torch.Tensor.lt\"><code>lt()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.less#torch.Tensor.less\" title=\"torch.Tensor.less\"><code>Tensor.less</code></a></p></td> <td><p>lt(other) -&gt; Tensor</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.less_#torch.Tensor.less_\" title=\"torch.Tensor.less_\"><code>Tensor.less_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.less#torch.Tensor.less\" title=\"torch.Tensor.less\"><code>less()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.lu#torch.Tensor.lu\" title=\"torch.Tensor.lu\"><code>Tensor.lu</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.lu#torch.lu\" title=\"torch.lu\"><code>torch.lu()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.lu_solve#torch.Tensor.lu_solve\" title=\"torch.Tensor.lu_solve\"><code>Tensor.lu_solve</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.lu_solve#torch.lu_solve\" title=\"torch.lu_solve\"><code>torch.lu_solve()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.as_subclass#torch.Tensor.as_subclass\" title=\"torch.Tensor.as_subclass\"><code>Tensor.as_subclass</code></a></p></td> <td><p>Makes a <code>cls</code> instance with the same data pointer as <code>self</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.map_#torch.Tensor.map_\" title=\"torch.Tensor.map_\"><code>Tensor.map_</code></a></p></td> <td><p>Applies <code>callable</code> for each element in <code>self</code> tensor and the given <a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>tensor</code></a> and stores the results in <code>self</code> tensor.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.masked_scatter_#torch.Tensor.masked_scatter_\" title=\"torch.Tensor.masked_scatter_\"><code>Tensor.masked_scatter_</code></a></p></td> <td><p>Copies elements from <code>source</code> into <code>self</code> tensor at positions where the <code>mask</code> is True.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.masked_scatter#torch.Tensor.masked_scatter\" title=\"torch.Tensor.masked_scatter\"><code>Tensor.masked_scatter</code></a></p></td> <td><p>Out-of-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.masked_scatter_#torch.Tensor.masked_scatter_\" title=\"torch.Tensor.masked_scatter_\"><code>torch.Tensor.masked_scatter_()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.masked_fill_#torch.Tensor.masked_fill_\" title=\"torch.Tensor.masked_fill_\"><code>Tensor.masked_fill_</code></a></p></td> <td><p>Fills elements of <code>self</code> tensor with <code>value</code> where <code>mask</code> is True.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.masked_fill#torch.Tensor.masked_fill\" title=\"torch.Tensor.masked_fill\"><code>Tensor.masked_fill</code></a></p></td> <td><p>Out-of-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.masked_fill_#torch.Tensor.masked_fill_\" title=\"torch.Tensor.masked_fill_\"><code>torch.Tensor.masked_fill_()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.masked_select#torch.Tensor.masked_select\" title=\"torch.Tensor.masked_select\"><code>Tensor.masked_select</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.masked_select#torch.masked_select\" title=\"torch.masked_select\"><code>torch.masked_select()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.matmul#torch.Tensor.matmul\" title=\"torch.Tensor.matmul\"><code>Tensor.matmul</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.matmul#torch.matmul\" title=\"torch.matmul\"><code>torch.matmul()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.matrix_power#torch.Tensor.matrix_power\" title=\"torch.Tensor.matrix_power\"><code>Tensor.matrix_power</code></a></p></td> <td>\n\n<div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p><a class=\"reference internal\" href=\"generated/torch.tensor.matrix_power#torch.Tensor.matrix_power\" title=\"torch.Tensor.matrix_power\"><code>matrix_power()</code></a> is deprecated, use <a class=\"reference internal\" href=\"generated/torch.linalg.matrix_power#torch.linalg.matrix_power\" title=\"torch.linalg.matrix_power\"><code>torch.linalg.matrix_power()</code></a> instead.</p> </div> </td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.matrix_exp#torch.Tensor.matrix_exp\" title=\"torch.Tensor.matrix_exp\"><code>Tensor.matrix_exp</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.matrix_exp#torch.matrix_exp\" title=\"torch.matrix_exp\"><code>torch.matrix_exp()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.max#torch.Tensor.max\" title=\"torch.Tensor.max\"><code>Tensor.max</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.max#torch.max\" title=\"torch.max\"><code>torch.max()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.maximum#torch.Tensor.maximum\" title=\"torch.Tensor.maximum\"><code>Tensor.maximum</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.maximum#torch.maximum\" title=\"torch.maximum\"><code>torch.maximum()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.mean#torch.Tensor.mean\" title=\"torch.Tensor.mean\"><code>Tensor.mean</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.mean#torch.mean\" title=\"torch.mean\"><code>torch.mean()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.nanmean#torch.Tensor.nanmean\" title=\"torch.Tensor.nanmean\"><code>Tensor.nanmean</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.nanmean#torch.nanmean\" title=\"torch.nanmean\"><code>torch.nanmean()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.median#torch.Tensor.median\" title=\"torch.Tensor.median\"><code>Tensor.median</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.median#torch.median\" title=\"torch.median\"><code>torch.median()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.nanmedian#torch.Tensor.nanmedian\" title=\"torch.Tensor.nanmedian\"><code>Tensor.nanmedian</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.nanmedian#torch.nanmedian\" title=\"torch.nanmedian\"><code>torch.nanmedian()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.min#torch.Tensor.min\" title=\"torch.Tensor.min\"><code>Tensor.min</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.min#torch.min\" title=\"torch.min\"><code>torch.min()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.minimum#torch.Tensor.minimum\" title=\"torch.Tensor.minimum\"><code>Tensor.minimum</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.minimum#torch.minimum\" title=\"torch.minimum\"><code>torch.minimum()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.mm#torch.Tensor.mm\" title=\"torch.Tensor.mm\"><code>Tensor.mm</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.mm#torch.mm\" title=\"torch.mm\"><code>torch.mm()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.smm#torch.Tensor.smm\" title=\"torch.Tensor.smm\"><code>Tensor.smm</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.smm#torch.smm\" title=\"torch.smm\"><code>torch.smm()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.mode#torch.Tensor.mode\" title=\"torch.Tensor.mode\"><code>Tensor.mode</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.mode#torch.mode\" title=\"torch.mode\"><code>torch.mode()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.movedim#torch.Tensor.movedim\" title=\"torch.Tensor.movedim\"><code>Tensor.movedim</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.movedim#torch.movedim\" title=\"torch.movedim\"><code>torch.movedim()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.moveaxis#torch.Tensor.moveaxis\" title=\"torch.Tensor.moveaxis\"><code>Tensor.moveaxis</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.moveaxis#torch.moveaxis\" title=\"torch.moveaxis\"><code>torch.moveaxis()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.msort#torch.Tensor.msort\" title=\"torch.Tensor.msort\"><code>Tensor.msort</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.msort#torch.msort\" title=\"torch.msort\"><code>torch.msort()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.mul#torch.Tensor.mul\" title=\"torch.Tensor.mul\"><code>Tensor.mul</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.mul#torch.mul\" title=\"torch.mul\"><code>torch.mul()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.mul_#torch.Tensor.mul_\" title=\"torch.Tensor.mul_\"><code>Tensor.mul_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.mul#torch.Tensor.mul\" title=\"torch.Tensor.mul\"><code>mul()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.multiply#torch.Tensor.multiply\" title=\"torch.Tensor.multiply\"><code>Tensor.multiply</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.multiply#torch.multiply\" title=\"torch.multiply\"><code>torch.multiply()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.multiply_#torch.Tensor.multiply_\" title=\"torch.Tensor.multiply_\"><code>Tensor.multiply_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.multiply#torch.Tensor.multiply\" title=\"torch.Tensor.multiply\"><code>multiply()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.multinomial#torch.Tensor.multinomial\" title=\"torch.Tensor.multinomial\"><code>Tensor.multinomial</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.multinomial#torch.multinomial\" title=\"torch.multinomial\"><code>torch.multinomial()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.mv#torch.Tensor.mv\" title=\"torch.Tensor.mv\"><code>Tensor.mv</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.mv#torch.mv\" title=\"torch.mv\"><code>torch.mv()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.mvlgamma#torch.Tensor.mvlgamma\" title=\"torch.Tensor.mvlgamma\"><code>Tensor.mvlgamma</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.mvlgamma#torch.mvlgamma\" title=\"torch.mvlgamma\"><code>torch.mvlgamma()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.mvlgamma_#torch.Tensor.mvlgamma_\" title=\"torch.Tensor.mvlgamma_\"><code>Tensor.mvlgamma_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.mvlgamma#torch.Tensor.mvlgamma\" title=\"torch.Tensor.mvlgamma\"><code>mvlgamma()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.nansum#torch.Tensor.nansum\" title=\"torch.Tensor.nansum\"><code>Tensor.nansum</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.nansum#torch.nansum\" title=\"torch.nansum\"><code>torch.nansum()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.narrow#torch.Tensor.narrow\" title=\"torch.Tensor.narrow\"><code>Tensor.narrow</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.narrow#torch.narrow\" title=\"torch.narrow\"><code>torch.narrow()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.narrow_copy#torch.Tensor.narrow_copy\" title=\"torch.Tensor.narrow_copy\"><code>Tensor.narrow_copy</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.narrow_copy#torch.narrow_copy\" title=\"torch.narrow_copy\"><code>torch.narrow_copy()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.ndimension#torch.Tensor.ndimension\" title=\"torch.Tensor.ndimension\"><code>Tensor.ndimension</code></a></p></td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.tensor.dim#torch.Tensor.dim\" title=\"torch.Tensor.dim\"><code>dim()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.nan_to_num#torch.Tensor.nan_to_num\" title=\"torch.Tensor.nan_to_num\"><code>Tensor.nan_to_num</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.nan_to_num#torch.nan_to_num\" title=\"torch.nan_to_num\"><code>torch.nan_to_num()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.nan_to_num_#torch.Tensor.nan_to_num_\" title=\"torch.Tensor.nan_to_num_\"><code>Tensor.nan_to_num_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.nan_to_num#torch.Tensor.nan_to_num\" title=\"torch.Tensor.nan_to_num\"><code>nan_to_num()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.ne#torch.Tensor.ne\" title=\"torch.Tensor.ne\"><code>Tensor.ne</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.ne#torch.ne\" title=\"torch.ne\"><code>torch.ne()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.ne_#torch.Tensor.ne_\" title=\"torch.Tensor.ne_\"><code>Tensor.ne_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.ne#torch.Tensor.ne\" title=\"torch.Tensor.ne\"><code>ne()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.not_equal#torch.Tensor.not_equal\" title=\"torch.Tensor.not_equal\"><code>Tensor.not_equal</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.not_equal#torch.not_equal\" title=\"torch.not_equal\"><code>torch.not_equal()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.not_equal_#torch.Tensor.not_equal_\" title=\"torch.Tensor.not_equal_\"><code>Tensor.not_equal_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.not_equal#torch.Tensor.not_equal\" title=\"torch.Tensor.not_equal\"><code>not_equal()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.neg#torch.Tensor.neg\" title=\"torch.Tensor.neg\"><code>Tensor.neg</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.neg#torch.neg\" title=\"torch.neg\"><code>torch.neg()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.neg_#torch.Tensor.neg_\" title=\"torch.Tensor.neg_\"><code>Tensor.neg_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.neg#torch.Tensor.neg\" title=\"torch.Tensor.neg\"><code>neg()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.negative#torch.Tensor.negative\" title=\"torch.Tensor.negative\"><code>Tensor.negative</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.negative#torch.negative\" title=\"torch.negative\"><code>torch.negative()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.negative_#torch.Tensor.negative_\" title=\"torch.Tensor.negative_\"><code>Tensor.negative_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.negative#torch.Tensor.negative\" title=\"torch.Tensor.negative\"><code>negative()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.nelement#torch.Tensor.nelement\" title=\"torch.Tensor.nelement\"><code>Tensor.nelement</code></a></p></td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.tensor.numel#torch.Tensor.numel\" title=\"torch.Tensor.numel\"><code>numel()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.nextafter#torch.Tensor.nextafter\" title=\"torch.Tensor.nextafter\"><code>Tensor.nextafter</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.nextafter#torch.nextafter\" title=\"torch.nextafter\"><code>torch.nextafter()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.nextafter_#torch.Tensor.nextafter_\" title=\"torch.Tensor.nextafter_\"><code>Tensor.nextafter_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.nextafter#torch.Tensor.nextafter\" title=\"torch.Tensor.nextafter\"><code>nextafter()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.nonzero#torch.Tensor.nonzero\" title=\"torch.Tensor.nonzero\"><code>Tensor.nonzero</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.nonzero#torch.nonzero\" title=\"torch.nonzero\"><code>torch.nonzero()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.norm#torch.Tensor.norm\" title=\"torch.Tensor.norm\"><code>Tensor.norm</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.norm#torch.norm\" title=\"torch.norm\"><code>torch.norm()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.normal_#torch.Tensor.normal_\" title=\"torch.Tensor.normal_\"><code>Tensor.normal_</code></a></p></td> <td><p>Fills <code>self</code> tensor with elements samples from the normal distribution parameterized by <a class=\"reference internal\" href=\"generated/torch.mean#torch.mean\" title=\"torch.mean\"><code>mean</code></a> and <a class=\"reference internal\" href=\"generated/torch.std#torch.std\" title=\"torch.std\"><code>std</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.numel#torch.Tensor.numel\" title=\"torch.Tensor.numel\"><code>Tensor.numel</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.numel#torch.numel\" title=\"torch.numel\"><code>torch.numel()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.numpy#torch.Tensor.numpy\" title=\"torch.Tensor.numpy\"><code>Tensor.numpy</code></a></p></td> <td><p>Returns the tensor as a NumPy <code>ndarray</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.orgqr#torch.Tensor.orgqr\" title=\"torch.Tensor.orgqr\"><code>Tensor.orgqr</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.orgqr#torch.orgqr\" title=\"torch.orgqr\"><code>torch.orgqr()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.ormqr#torch.Tensor.ormqr\" title=\"torch.Tensor.ormqr\"><code>Tensor.ormqr</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.ormqr#torch.ormqr\" title=\"torch.ormqr\"><code>torch.ormqr()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.outer#torch.Tensor.outer\" title=\"torch.Tensor.outer\"><code>Tensor.outer</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.outer#torch.outer\" title=\"torch.outer\"><code>torch.outer()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.permute#torch.Tensor.permute\" title=\"torch.Tensor.permute\"><code>Tensor.permute</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.permute#torch.permute\" title=\"torch.permute\"><code>torch.permute()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.pin_memory#torch.Tensor.pin_memory\" title=\"torch.Tensor.pin_memory\"><code>Tensor.pin_memory</code></a></p></td> <td><p>Copies the tensor to pinned memory, if it's not already pinned.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.pinverse#torch.Tensor.pinverse\" title=\"torch.Tensor.pinverse\"><code>Tensor.pinverse</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.pinverse#torch.pinverse\" title=\"torch.pinverse\"><code>torch.pinverse()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.polygamma#torch.Tensor.polygamma\" title=\"torch.Tensor.polygamma\"><code>Tensor.polygamma</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.polygamma#torch.polygamma\" title=\"torch.polygamma\"><code>torch.polygamma()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.polygamma_#torch.Tensor.polygamma_\" title=\"torch.Tensor.polygamma_\"><code>Tensor.polygamma_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.polygamma#torch.Tensor.polygamma\" title=\"torch.Tensor.polygamma\"><code>polygamma()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.positive#torch.Tensor.positive\" title=\"torch.Tensor.positive\"><code>Tensor.positive</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.positive#torch.positive\" title=\"torch.positive\"><code>torch.positive()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.pow#torch.Tensor.pow\" title=\"torch.Tensor.pow\"><code>Tensor.pow</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.pow#torch.pow\" title=\"torch.pow\"><code>torch.pow()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.pow_#torch.Tensor.pow_\" title=\"torch.Tensor.pow_\"><code>Tensor.pow_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.pow#torch.Tensor.pow\" title=\"torch.Tensor.pow\"><code>pow()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.prod#torch.Tensor.prod\" title=\"torch.Tensor.prod\"><code>Tensor.prod</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.prod#torch.prod\" title=\"torch.prod\"><code>torch.prod()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.put_#torch.Tensor.put_\" title=\"torch.Tensor.put_\"><code>Tensor.put_</code></a></p></td> <td><p>Copies the elements from <code>source</code> into the positions specified by <code>index</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.qr#torch.Tensor.qr\" title=\"torch.Tensor.qr\"><code>Tensor.qr</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.qr#torch.qr\" title=\"torch.qr\"><code>torch.qr()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.qscheme#torch.Tensor.qscheme\" title=\"torch.Tensor.qscheme\"><code>Tensor.qscheme</code></a></p></td> <td><p>Returns the quantization scheme of a given QTensor.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.quantile#torch.Tensor.quantile\" title=\"torch.Tensor.quantile\"><code>Tensor.quantile</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.quantile#torch.quantile\" title=\"torch.quantile\"><code>torch.quantile()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.nanquantile#torch.Tensor.nanquantile\" title=\"torch.Tensor.nanquantile\"><code>Tensor.nanquantile</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.nanquantile#torch.nanquantile\" title=\"torch.nanquantile\"><code>torch.nanquantile()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.q_scale#torch.Tensor.q_scale\" title=\"torch.Tensor.q_scale\"><code>Tensor.q_scale</code></a></p></td> <td><p>Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.q_zero_point#torch.Tensor.q_zero_point\" title=\"torch.Tensor.q_zero_point\"><code>Tensor.q_zero_point</code></a></p></td> <td><p>Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.q_per_channel_scales#torch.Tensor.q_per_channel_scales\" title=\"torch.Tensor.q_per_channel_scales\"><code>Tensor.q_per_channel_scales</code></a></p></td> <td><p>Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.q_per_channel_zero_points#torch.Tensor.q_per_channel_zero_points\" title=\"torch.Tensor.q_per_channel_zero_points\"><code>Tensor.q_per_channel_zero_points</code></a></p></td> <td><p>Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.q_per_channel_axis#torch.Tensor.q_per_channel_axis\" title=\"torch.Tensor.q_per_channel_axis\"><code>Tensor.q_per_channel_axis</code></a></p></td> <td><p>Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.rad2deg#torch.Tensor.rad2deg\" title=\"torch.Tensor.rad2deg\"><code>Tensor.rad2deg</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.rad2deg#torch.rad2deg\" title=\"torch.rad2deg\"><code>torch.rad2deg()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.random_#torch.Tensor.random_\" title=\"torch.Tensor.random_\"><code>Tensor.random_</code></a></p></td> <td><p>Fills <code>self</code> tensor with numbers sampled from the discrete uniform distribution over <code>[from, to - 1]</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.ravel#torch.Tensor.ravel\" title=\"torch.Tensor.ravel\"><code>Tensor.ravel</code></a></p></td> <td><p>see <a class=\"reference internal\" href=\"generated/torch.ravel#torch.ravel\" title=\"torch.ravel\"><code>torch.ravel()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.reciprocal#torch.Tensor.reciprocal\" title=\"torch.Tensor.reciprocal\"><code>Tensor.reciprocal</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.reciprocal#torch.reciprocal\" title=\"torch.reciprocal\"><code>torch.reciprocal()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.reciprocal_#torch.Tensor.reciprocal_\" title=\"torch.Tensor.reciprocal_\"><code>Tensor.reciprocal_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.reciprocal#torch.Tensor.reciprocal\" title=\"torch.Tensor.reciprocal\"><code>reciprocal()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.record_stream#torch.Tensor.record_stream\" title=\"torch.Tensor.record_stream\"><code>Tensor.record_stream</code></a></p></td> <td><p>Ensures that the tensor memory is not reused for another tensor until all current work queued on <code>stream</code> are complete.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.register_hook#torch.Tensor.register_hook\" title=\"torch.Tensor.register_hook\"><code>Tensor.register_hook</code></a></p></td> <td><p>Registers a backward hook.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.register_post_accumulate_grad_hook#torch.Tensor.register_post_accumulate_grad_hook\" title=\"torch.Tensor.register_post_accumulate_grad_hook\"><code>Tensor.register_post_accumulate_grad_hook</code></a></p></td> <td><p>Registers a backward hook that runs after grad accumulation.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.remainder#torch.Tensor.remainder\" title=\"torch.Tensor.remainder\"><code>Tensor.remainder</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.remainder#torch.remainder\" title=\"torch.remainder\"><code>torch.remainder()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.remainder_#torch.Tensor.remainder_\" title=\"torch.Tensor.remainder_\"><code>Tensor.remainder_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.remainder#torch.Tensor.remainder\" title=\"torch.Tensor.remainder\"><code>remainder()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.renorm#torch.Tensor.renorm\" title=\"torch.Tensor.renorm\"><code>Tensor.renorm</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.renorm#torch.renorm\" title=\"torch.renorm\"><code>torch.renorm()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.renorm_#torch.Tensor.renorm_\" title=\"torch.Tensor.renorm_\"><code>Tensor.renorm_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.renorm#torch.Tensor.renorm\" title=\"torch.Tensor.renorm\"><code>renorm()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.repeat#torch.Tensor.repeat\" title=\"torch.Tensor.repeat\"><code>Tensor.repeat</code></a></p></td> <td><p>Repeats this tensor along the specified dimensions.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.repeat_interleave#torch.Tensor.repeat_interleave\" title=\"torch.Tensor.repeat_interleave\"><code>Tensor.repeat_interleave</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.repeat_interleave#torch.repeat_interleave\" title=\"torch.repeat_interleave\"><code>torch.repeat_interleave()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.requires_grad#torch.Tensor.requires_grad\" title=\"torch.Tensor.requires_grad\"><code>Tensor.requires_grad</code></a></p></td> <td><p>Is <code>True</code> if gradients need to be computed for this Tensor, <code>False</code> otherwise.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.requires_grad_#torch.Tensor.requires_grad_\" title=\"torch.Tensor.requires_grad_\"><code>Tensor.requires_grad_</code></a></p></td> <td><p>Change if autograd should record operations on this tensor: sets this tensor's <code>requires_grad</code> attribute in-place.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.reshape#torch.Tensor.reshape\" title=\"torch.Tensor.reshape\"><code>Tensor.reshape</code></a></p></td> <td><p>Returns a tensor with the same data and number of elements as <code>self</code> but with the specified shape.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.reshape_as#torch.Tensor.reshape_as\" title=\"torch.Tensor.reshape_as\"><code>Tensor.reshape_as</code></a></p></td> <td><p>Returns this tensor as the same shape as <code>other</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.resize_#torch.Tensor.resize_\" title=\"torch.Tensor.resize_\"><code>Tensor.resize_</code></a></p></td> <td><p>Resizes <code>self</code> tensor to the specified size.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.resize_as_#torch.Tensor.resize_as_\" title=\"torch.Tensor.resize_as_\"><code>Tensor.resize_as_</code></a></p></td> <td><p>Resizes the <code>self</code> tensor to be the same size as the specified <a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>tensor</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.retain_grad#torch.Tensor.retain_grad\" title=\"torch.Tensor.retain_grad\"><code>Tensor.retain_grad</code></a></p></td> <td><p>Enables this Tensor to have their <code>grad</code> populated during <code>backward()</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.retains_grad#torch.Tensor.retains_grad\" title=\"torch.Tensor.retains_grad\"><code>Tensor.retains_grad</code></a></p></td> <td><p>Is <code>True</code> if this Tensor is non-leaf and its <code>grad</code> is enabled to be populated during <code>backward()</code>, <code>False</code> otherwise.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.roll#torch.Tensor.roll\" title=\"torch.Tensor.roll\"><code>Tensor.roll</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.roll#torch.roll\" title=\"torch.roll\"><code>torch.roll()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.rot90#torch.Tensor.rot90\" title=\"torch.Tensor.rot90\"><code>Tensor.rot90</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.rot90#torch.rot90\" title=\"torch.rot90\"><code>torch.rot90()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.round#torch.Tensor.round\" title=\"torch.Tensor.round\"><code>Tensor.round</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.round#torch.round\" title=\"torch.round\"><code>torch.round()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.round_#torch.Tensor.round_\" title=\"torch.Tensor.round_\"><code>Tensor.round_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.round#torch.Tensor.round\" title=\"torch.Tensor.round\"><code>round()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.rsqrt#torch.Tensor.rsqrt\" title=\"torch.Tensor.rsqrt\"><code>Tensor.rsqrt</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.rsqrt#torch.rsqrt\" title=\"torch.rsqrt\"><code>torch.rsqrt()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.rsqrt_#torch.Tensor.rsqrt_\" title=\"torch.Tensor.rsqrt_\"><code>Tensor.rsqrt_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.rsqrt#torch.Tensor.rsqrt\" title=\"torch.Tensor.rsqrt\"><code>rsqrt()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.scatter#torch.Tensor.scatter\" title=\"torch.Tensor.scatter\"><code>Tensor.scatter</code></a></p></td> <td><p>Out-of-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.scatter_#torch.Tensor.scatter_\" title=\"torch.Tensor.scatter_\"><code>torch.Tensor.scatter_()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.scatter_#torch.Tensor.scatter_\" title=\"torch.Tensor.scatter_\"><code>Tensor.scatter_</code></a></p></td> <td><p>Writes all values from the tensor <code>src</code> into <code>self</code> at the indices specified in the <code>index</code> tensor.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.scatter_add_#torch.Tensor.scatter_add_\" title=\"torch.Tensor.scatter_add_\"><code>Tensor.scatter_add_</code></a></p></td> <td><p>Adds all values from the tensor <code>src</code> into <code>self</code> at the indices specified in the <code>index</code> tensor in a similar fashion as <a class=\"reference internal\" href=\"generated/torch.tensor.scatter_#torch.Tensor.scatter_\" title=\"torch.Tensor.scatter_\"><code>scatter_()</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.scatter_add#torch.Tensor.scatter_add\" title=\"torch.Tensor.scatter_add\"><code>Tensor.scatter_add</code></a></p></td> <td><p>Out-of-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.scatter_add_#torch.Tensor.scatter_add_\" title=\"torch.Tensor.scatter_add_\"><code>torch.Tensor.scatter_add_()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.scatter_reduce_#torch.Tensor.scatter_reduce_\" title=\"torch.Tensor.scatter_reduce_\"><code>Tensor.scatter_reduce_</code></a></p></td> <td><p>Reduces all values from the <code>src</code> tensor to the indices specified in the <code>index</code> tensor in the <code>self</code> tensor using the applied reduction defined via the <code>reduce</code> argument (<code>\"sum\"</code>, <code>\"prod\"</code>, <code>\"mean\"</code>, <code>\"amax\"</code>, <code>\"amin\"</code>).</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.scatter_reduce#torch.Tensor.scatter_reduce\" title=\"torch.Tensor.scatter_reduce\"><code>Tensor.scatter_reduce</code></a></p></td> <td><p>Out-of-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.scatter_reduce_#torch.Tensor.scatter_reduce_\" title=\"torch.Tensor.scatter_reduce_\"><code>torch.Tensor.scatter_reduce_()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.select#torch.Tensor.select\" title=\"torch.Tensor.select\"><code>Tensor.select</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.select#torch.select\" title=\"torch.select\"><code>torch.select()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.select_scatter#torch.Tensor.select_scatter\" title=\"torch.Tensor.select_scatter\"><code>Tensor.select_scatter</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.select_scatter#torch.select_scatter\" title=\"torch.select_scatter\"><code>torch.select_scatter()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.set_#torch.Tensor.set_\" title=\"torch.Tensor.set_\"><code>Tensor.set_</code></a></p></td> <td><p>Sets the underlying storage, size, and strides.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.share_memory_#torch.Tensor.share_memory_\" title=\"torch.Tensor.share_memory_\"><code>Tensor.share_memory_</code></a></p></td> <td><p>Moves the underlying storage to shared memory.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.short#torch.Tensor.short\" title=\"torch.Tensor.short\"><code>Tensor.short</code></a></p></td> <td><p><code>self.short()</code> is equivalent to <code>self.to(torch.int16)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.sigmoid#torch.Tensor.sigmoid\" title=\"torch.Tensor.sigmoid\"><code>Tensor.sigmoid</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.sigmoid#torch.sigmoid\" title=\"torch.sigmoid\"><code>torch.sigmoid()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.sigmoid_#torch.Tensor.sigmoid_\" title=\"torch.Tensor.sigmoid_\"><code>Tensor.sigmoid_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.sigmoid#torch.Tensor.sigmoid\" title=\"torch.Tensor.sigmoid\"><code>sigmoid()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.sign#torch.Tensor.sign\" title=\"torch.Tensor.sign\"><code>Tensor.sign</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.sign#torch.sign\" title=\"torch.sign\"><code>torch.sign()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.sign_#torch.Tensor.sign_\" title=\"torch.Tensor.sign_\"><code>Tensor.sign_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.sign#torch.Tensor.sign\" title=\"torch.Tensor.sign\"><code>sign()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.signbit#torch.Tensor.signbit\" title=\"torch.Tensor.signbit\"><code>Tensor.signbit</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.signbit#torch.signbit\" title=\"torch.signbit\"><code>torch.signbit()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.sgn#torch.Tensor.sgn\" title=\"torch.Tensor.sgn\"><code>Tensor.sgn</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.sgn#torch.sgn\" title=\"torch.sgn\"><code>torch.sgn()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.sgn_#torch.Tensor.sgn_\" title=\"torch.Tensor.sgn_\"><code>Tensor.sgn_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.sgn#torch.Tensor.sgn\" title=\"torch.Tensor.sgn\"><code>sgn()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.sin#torch.Tensor.sin\" title=\"torch.Tensor.sin\"><code>Tensor.sin</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.sin#torch.sin\" title=\"torch.sin\"><code>torch.sin()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.sin_#torch.Tensor.sin_\" title=\"torch.Tensor.sin_\"><code>Tensor.sin_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.sin#torch.Tensor.sin\" title=\"torch.Tensor.sin\"><code>sin()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.sinc#torch.Tensor.sinc\" title=\"torch.Tensor.sinc\"><code>Tensor.sinc</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.sinc#torch.sinc\" title=\"torch.sinc\"><code>torch.sinc()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.sinc_#torch.Tensor.sinc_\" title=\"torch.Tensor.sinc_\"><code>Tensor.sinc_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.sinc#torch.Tensor.sinc\" title=\"torch.Tensor.sinc\"><code>sinc()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.sinh#torch.Tensor.sinh\" title=\"torch.Tensor.sinh\"><code>Tensor.sinh</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.sinh#torch.sinh\" title=\"torch.sinh\"><code>torch.sinh()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.sinh_#torch.Tensor.sinh_\" title=\"torch.Tensor.sinh_\"><code>Tensor.sinh_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.sinh#torch.Tensor.sinh\" title=\"torch.Tensor.sinh\"><code>sinh()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.asinh#torch.Tensor.asinh\" title=\"torch.Tensor.asinh\"><code>Tensor.asinh</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.asinh#torch.asinh\" title=\"torch.asinh\"><code>torch.asinh()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.asinh_#torch.Tensor.asinh_\" title=\"torch.Tensor.asinh_\"><code>Tensor.asinh_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.asinh#torch.Tensor.asinh\" title=\"torch.Tensor.asinh\"><code>asinh()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.arcsinh#torch.Tensor.arcsinh\" title=\"torch.Tensor.arcsinh\"><code>Tensor.arcsinh</code></a></p></td> <td><p>See <a class=\"reference internal\" href=\"generated/torch.arcsinh#torch.arcsinh\" title=\"torch.arcsinh\"><code>torch.arcsinh()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.arcsinh_#torch.Tensor.arcsinh_\" title=\"torch.Tensor.arcsinh_\"><code>Tensor.arcsinh_</code></a></p></td> <td><p>In-place version of <a class=\"reference internal\" href=\"generated/torch.tensor.arcsinh#torch.Tensor.arcsinh\" title=\"torch.Tensor.arcsinh\"><code>arcsinh()</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.tensor.shape#torch.Tensor.shape\" t