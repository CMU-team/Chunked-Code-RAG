[{"name": "()", "path": "glossary", "type": "Glossary", "text": "Glossary  (n,)\n\nA parenthesized number followed by a comma denotes a tuple with one element. The trailing comma distinguishes a one-element tuple from a parenthesized n.  -1\n\n \nIn a dimension entry, instructs NumPy to choose the length that will keep the total number of array elements the same. >>> np.arange(12).reshape(4, -1).shape\n(4, 3)\n  \nIn an index, any negative value denotes indexing from the right.   \u2026\n\nAn Ellipsis.  \nWhen indexing an array, shorthand that the missing axes, if they exist, are full slices. >>> a = np.arange(24).reshape(2,3,4)\n >>> a[...].shape\n(2, 3, 4)\n >>> a[...,0].shape\n(2, 3)\n >>> a[0,...].shape\n(3, 4)\n >>> a[0,...,0].shape\n(3,)\n It can be used at most once; a[...,0,...] raises an IndexError.  \nIn printouts, NumPy substitutes ... for the middle elements of large arrays. To see the entire array, use numpy.printoptions\n   :\n\nThe Python slice operator. In ndarrays, slicing can be applied to every axis: >>> a = np.arange(24).reshape(2,3,4)\n>>> a\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n\n>>> a[1:,-2:,:-1]\narray([[[16, 17, 18],\n        [20, 21, 22]]])\n Trailing slices can be omitted: >>> a[1] == a[1,:,:]\narray([[ True,  True,  True,  True],\n       [ True,  True,  True,  True],\n       [ True,  True,  True,  True]])\n In contrast to Python, where slicing creates a copy, in NumPy slicing creates a view. For details, see Combining advanced and basic indexing.  <\n\nIn a dtype declaration, indicates that the data is little-endian (the bracket is big on the right). >>> dt = np.dtype('<f')  # little-endian single-precision float\n  >\n\nIn a dtype declaration, indicates that the data is big-endian (the bracket is big on the left). >>> dt = np.dtype('>H')  # big-endian unsigned short\n  advanced indexing\n\nRather than using a scalar or slice as an index, an axis can be indexed with an array, providing fine-grained selection. This is known as advanced indexing or \u201cfancy indexing\u201d.  along an axis\n\nAn operation along axis n of array a behaves as if its argument were an array of slices of a where each slice has a successive index of axis n. For example, if a is a 3 x N array, an operation along axis 0 behaves as if its argument were an array containing slices of each row: >>> np.array((a[0,:], a[1,:], a[2,:])) \n To make it concrete, we can pick the operation to be the array-reversal function numpy.flip, which accepts an axis argument. We construct a 3 x 4 array a: >>> a = np.arange(12).reshape(3,4)\n>>> a\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\n Reversing along axis 0 (the row axis) yields >>> np.flip(a,axis=0)\narray([[ 8,  9, 10, 11],\n       [ 4,  5,  6,  7],\n       [ 0,  1,  2,  3]])\n Recalling the definition of along an axis, flip along axis 0 is treating its argument as if it were >>> np.array((a[0,:], a[1,:], a[2,:]))\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\n and the result of np.flip(a,axis=0) is to reverse the slices: >>> np.array((a[2,:],a[1,:],a[0,:]))\narray([[ 8,  9, 10, 11],\n       [ 4,  5,  6,  7],\n       [ 0,  1,  2,  3]])\n  array\n\nUsed synonymously in the NumPy docs with ndarray.  array_like\n\nAny scalar or sequence that can be interpreted as an ndarray. In addition to ndarrays and scalars this category includes lists (possibly nested and with different element types) and tuples. Any argument accepted by numpy.array is array_like. >>> a = np.array([[1, 2.0], [0, 0], (1+1j, 3.)])\n\n>>> a\narray([[1.+0.j, 2.+0.j],\n       [0.+0.j, 0.+0.j],\n       [1.+1.j, 3.+0.j]])\n  array scalar\n\nAn array scalar is an instance of the types/classes float32, float64, etc.. For uniformity in handling operands, NumPy treats a scalar as an array of zero dimension. In contrast, a 0-dimensional array is an ndarray instance containing precisely one value.  axis\n\nAnother term for an array dimension. Axes are numbered left to right; axis 0 is the first element in the shape tuple. In a two-dimensional vector, the elements of axis 0 are rows and the elements of axis 1 are columns. In higher dimensions, the picture changes. NumPy prints higher-dimensional vectors as replications of row-by-column building blocks, as in this three-dimensional vector: >>> a = np.arange(12).reshape(2,2,3)\n>>> a\narray([[[ 0,  1,  2],\n        [ 3,  4,  5]],\n       [[ 6,  7,  8],\n        [ 9, 10, 11]]])\n a is depicted as a two-element array whose elements are 2x3 vectors. From this point of view, rows and columns are the final two axes, respectively, in any shape. This rule helps you anticipate how a vector will be printed, and conversely how to find the index of any of the printed elements. For instance, in the example, the last two values of 8\u2019s index must be 0 and 2. Since 8 appears in the second of the two 2x3\u2019s, the first index must be 1: >>> a[1,0,2]\n8\n A convenient way to count dimensions in a printed vector is to count [ symbols after the open-parenthesis. This is useful in distinguishing, say, a (1,2,3) shape from a (2,3) shape: >>> a = np.arange(6).reshape(2,3)\n>>> a.ndim\n2\n>>> a\narray([[0, 1, 2],\n       [3, 4, 5]])\n >>> a = np.arange(6).reshape(1,2,3)\n>>> a.ndim\n3\n>>> a\narray([[[0, 1, 2],\n        [3, 4, 5]]])\n  .base\n\nIf an array does not own its memory, then its base attribute returns the object whose memory the array is referencing. That object may be referencing the memory from still another object, so the owning object may be a.base.base.base.... Some writers erroneously claim that testing base determines if arrays are views. For the correct way, see numpy.shares_memory.  big-endian\n\nSee Endianness.  BLAS\n\nBasic Linear Algebra Subprograms  broadcast\n\nbroadcasting is NumPy\u2019s ability to process ndarrays of different sizes as if all were the same size. It permits an elegant do-what-I-mean behavior where, for instance, adding a scalar to a vector adds the scalar value to every element. >>> a = np.arange(3)\n>>> a\narray([0, 1, 2])\n >>> a + [3, 3, 3]\narray([3, 4, 5])\n >>> a + 3\narray([3, 4, 5])\n Ordinarly, vector operands must all be the same size, because NumPy works element by element \u2013 for instance, c = a * b is  c[0,0,0] = a[0,0,0] * b[0,0,0]\n c[0,0,1] = a[0,0,1] * b[0,0,1]\n...\n But in certain useful cases, NumPy can duplicate data along \u201cmissing\u201d axes or \u201ctoo-short\u201d dimensions so shapes will match. The duplication costs no memory or time. For details, see Broadcasting.  C order\n\nSame as row-major.  column-major\n\nSee Row- and column-major order.  contiguous\n\n An array is contiguous if\n\n it occupies an unbroken block of memory, and array elements with higher indexes occupy higher addresses (that is, no stride is negative).     copy\n\nSee view.  dimension\n\nSee axis.  dtype\n\nThe datatype describing the (identically typed) elements in an ndarray. It can be changed to reinterpret the array contents. For details, see Data type objects (dtype).  fancy indexing\n\nAnother term for advanced indexing.  field\n\nIn a structured data type, each subtype is called a field. The field has a name (a string), a type (any valid dtype), and an optional title. See Data type objects (dtype).  Fortran order\n\nSame as column-major.  flattened\n\nSee ravel.  homogeneous\n\nAll elements of a homogeneous array have the same type. ndarrays, in contrast to Python lists, are homogeneous. The type can be complicated, as in a structured array, but all elements have that type. NumPy object arrays, which contain references to Python objects, fill the role of heterogeneous arrays.  itemsize\n\nThe size of the dtype element in bytes.  little-endian\n\nSee Endianness.  mask\n\nA boolean array used to select only certain elements for an operation: >>> x = np.arange(5)\n>>> x\narray([0, 1, 2, 3, 4])\n >>> mask = (x > 2)\n>>> mask\narray([False, False, False, True,  True])\n >>> x[mask] = -1\n>>> x\narray([ 0,  1,  2,  -1, -1])\n  masked array\n\nBad or missing data can be cleanly ignored by putting it in a masked array, which has an internal boolean array indicating invalid entries. Operations with masked arrays ignore these entries. >>> a = np.ma.masked_array([np.nan, 2, np.nan], [True, False, True])\n>>> a\nmasked_array(data=[--, 2.0, --],\n             mask=[ True, False,  True],\n       fill_value=1e+20)\n\n>>> a + [1, 2, 3]\nmasked_array(data=[--, 4.0, --],\n             mask=[ True, False,  True],\n       fill_value=1e+20)\n For details, see Masked arrays.  matrix\n\nNumPy\u2019s two-dimensional matrix class should no longer be used; use regular ndarrays.  ndarray\n\nNumPy\u2019s basic structure.  object array\n\nAn array whose dtype is object; that is, it contains references to Python objects. Indexing the array dereferences the Python objects, so unlike other ndarrays, an object array has the ability to hold heterogeneous objects.  ravel\n\nnumpy.ravel  and numpy.flatten  both flatten an ndarray. ravel will return a view if possible; flatten always returns a copy. Flattening collapses a multimdimensional array to a single dimension; details of how this is done (for instance, whether a[n+1] should be the next row or next column) are parameters.  record array\n\nA structured array with allowing access in an attribute style (a.field) in addition to a['field']. For details, see numpy.recarray.  row-major\n\nSee Row- and column-major order. NumPy creates arrays in row-major order by default.  scalar\n\nIn NumPy, usually a synonym for array scalar.  shape\n\nA tuple showing the length of each dimension of an ndarray. The length of the tuple itself is the number of dimensions (numpy.ndim). The product of the tuple elements is the number of elements in the array. For details, see numpy.ndarray.shape.  stride\n\nPhysical memory is one-dimensional; strides provide a mechanism to map a given index to an address in memory. For an N-dimensional array, its strides attribute is an N-element tuple; advancing from index i to index i+1 on axis n means adding a.strides[n] bytes to the address. Strides are computed automatically from an array\u2019s dtype and shape, but can be directly specified using as_strided. For details, see numpy.ndarray.strides. To see how striding underlies the power of NumPy views, see The NumPy array: a structure for efficient numerical computation.   structured array\n\nArray whose dtype is a structured data type.  structured data type\n\nUsers can create arbitrarily complex dtypes that can include other arrays and dtypes. These composite dtypes are called structured data types.  subarray\n\nAn array nested in a structured data type, as b is here: >>> dt = np.dtype([('a', np.int32), ('b', np.float32, (3,))])\n>>> np.zeros(3, dtype=dt)\narray([(0, [0., 0., 0.]), (0, [0., 0., 0.]), (0, [0., 0., 0.])],\n      dtype=[('a', '<i4'), ('b', '<f4', (3,))])\n  subarray data type\n\nAn element of a structured datatype that behaves like an ndarray.  title\n\nAn alias for a field name in a structured datatype.  type\n\nIn NumPy, usually a synonym for dtype. For the more general Python meaning, see here.  ufunc\n\nNumPy\u2019s fast element-by-element computation (vectorization) gives a choice which function gets applied. The general term for the function is ufunc, short for universal function. NumPy routines have built-in ufuncs, but users can also write their own.  vectorization\n\nNumPy hands off array processing to C, where looping and computation are much faster than in Python. To exploit this, programmers using NumPy eliminate Python loops in favor of array-to-array operations. vectorization can refer both to the C offloading and to structuring NumPy code to leverage it.  view\n\nWithout touching underlying data, NumPy can make one array appear to change its datatype and shape. An array created this way is a view, and NumPy often exploits the performance gain of using a view versus making a new array. A potential drawback is that writing to a view can alter the original as well. If this is a problem, NumPy instead needs to create a physically distinct array \u2013 a copy. Some NumPy routines always return views, some always return copies, some may return one or the other, and for some the choice can be specified. Responsibility for managing views and copies falls to the programmer. numpy.shares_memory will check whether b is a view of a, but an exact answer isn\u2019t always feasible, as the documentation page explains. >>> x = np.arange(5)\n>>> x\narray([0, 1, 2, 3, 4])\n >>> y = x[::2]\n>>> y\narray([0, 2, 4])\n >>> x[0] = 3 # changing x changes y as well, since y is a view on x\n>>> y\narray([3, 2, 4])\n  \n"}, {"name": "--overwrite-signature", "path": "f2py/usage", "type": "Using F2PY", "text": "Using F2PY F2PY can be used either as a command line tool f2py or as a Python module numpy.f2py. While we try to provide the command line tool as part of the numpy setup, some platforms like Windows make it difficult to reliably put the executables on the PATH. We will refer to f2py in this document but you may have to run it as a module: python -m numpy.f2py\n If you run f2py with no arguments, and the line numpy Version at the end matches the NumPy version printed from python -m numpy.f2py, then you can use the shorter version. If not, or if you cannot run f2py, you should replace all calls to f2py here with the longer version.  Command f2py\n When used as a command line tool, f2py has three major modes, distinguished by the usage of -c and -h switches:  Signature file generation  \nTo scan Fortran sources and generate a signature file, use f2py -h <filename.pyf> <options> <fortran files>   \\\n  [[ only: <fortran functions>  : ]                \\\n   [ skip: <fortran functions>  : ]]...            \\\n  [<fortran files> ...]\n  Note A Fortran source file can contain many routines, and it is often not necessary to allow all routines be usable from Python. In such cases, either specify which routines should be wrapped (in the only: .. : part) or which routines F2PY should ignored (in the skip: .. : part).  If <filename.pyf> is specified as stdout then signatures are written to standard output instead of a file. Among other options (see below), the following can be used in this mode:  --overwrite-signature\n\nOverwrites an existing signature file.       Extension module construction  \nTo construct an extension module, use f2py -m <modulename> <options> <fortran files>   \\\n  [[ only: <fortran functions>  : ]              \\\n   [ skip: <fortran functions>  : ]]...          \\\n  [<fortran files> ...]\n The constructed extension module is saved as <modulename>module.c to the current directory. Here <fortran files> may also contain signature files. Among other options (see below), the following options can be used in this mode:  --debug-capi\n\nAdds debugging hooks to the extension module. When using this extension module, various diagnostic information about the wrapper is written to the standard output, for example, the values of variables, the steps taken, etc.  -include'<includefile>'\n\nAdd a CPP #include statement to the extension module source. <includefile> should be given in one of the following forms \"filename.ext\"\n<filename.ext>\n The include statement is inserted just before the wrapper functions. This feature enables using arbitrary C functions (defined in <includefile>) in F2PY generated wrappers.  Note This option is deprecated. Use usercode statement to specify C code snippets directly in signature files.   --[no-]wrap-functions\n\nCreate Fortran subroutine wrappers to Fortran functions. --wrap-functions is default because it ensures maximum portability and compiler independence.  --include-paths <path1>:<path2>:..\n\nSearch include files from given directories.  --help-link [<list of resources names>]\n\nList system resources found by numpy_distutils/system_info.py. For example, try f2py --help-link lapack_opt.       Building a module  \nTo build an extension module, use f2py -c <options> <fortran files>       \\\n  [[ only: <fortran functions>  : ]     \\\n   [ skip: <fortran functions>  : ]]... \\\n  [ <fortran/c source files> ] [ <.o, .a, .so files> ]\n If <fortran files> contains a signature file, then the source for an extension module is constructed, all Fortran and C sources are compiled, and finally all object and library files are linked to the extension module <modulename>.so which is saved into the current directory. If <fortran files> does not contain a signature file, then an extension module is constructed by scanning all Fortran source codes for routine signatures, before proceeding to build the extension module. Among other options (see below) and options described for previous modes, the following options can be used in this mode:  --help-fcompiler\n\nList the available Fortran compilers.  \n--help-compiler [depreciated]\n\n\nList the available Fortran compilers.  --fcompiler=<Vendor>\n\nSpecify a Fortran compiler type by vendor.  --f77exec=<path>\n\nSpecify the path to a F77 compiler  \n--fcompiler-exec=<path> [depreciated]\n\n\nSpecify the path to a F77 compiler  --f90exec=<path>\n\nSpecify the path to a F90 compiler  \n--f90compiler-exec=<path> [depreciated]\n\n\nSpecify the path to a F90 compiler  --f77flags=<string>\n\nSpecify F77 compiler flags  --f90flags=<string>\n\nSpecify F90 compiler flags  --opt=<string>\n\nSpecify optimization flags  --arch=<string>\n\nSpecify architecture specific optimization flags  --noopt\n\nCompile without optimization flags  --noarch\n\nCompile without arch-dependent optimization flags  --debug\n\nCompile with debugging information  -l<libname>\n\nUse the library <libname> when linking.  -D<macro>[=<defn=1>]\n\nDefine macro <macro> as <defn>.  -U<macro>\n\nDefine macro <macro>  -I<dir>\n\nAppend directory <dir> to the list of directories searched for include files.  -L<dir>\n\nAdd directory <dir> to the list of directories to be searched for -l.  link-<resource>\n\nLink the extension module with <resource> as defined by numpy_distutils/system_info.py. E.g. to link with optimized LAPACK libraries (vecLib on MacOSX, ATLAS elsewhere), use --link-lapack_opt. See also --help-link switch.    Note The f2py -c option must be applied either to an existing .pyf file (plus the source/object/library files) or one must specify the -m <modulename> option (plus the sources/object/library files). Use one of the following options:  f2py -c -m fib1 fib1.f\n or f2py -m fib1 fib1.f -h fib1.pyf\nf2py -c fib1.pyf fib1.f\n For more information, see the Building C and C++ Extensions Python documentation for details. When building an extension module, a combination of the following macros may be required for non-gcc Fortran compilers: -DPREPEND_FORTRAN\n-DNO_APPEND_FORTRAN\n-DUPPERCASE_FORTRAN\n To test the performance of F2PY generated interfaces, use -DF2PY_REPORT_ATEXIT. Then a report of various timings is printed out at the exit of Python. This feature may not work on all platforms, currently only Linux platform is supported. To see whether F2PY generated interface performs copies of array arguments, use -DF2PY_REPORT_ON_ARRAY_COPY=<int>. When the size of an array argument is larger than <int>, a message about the coping is sent to stderr.     Other options  -m <modulename>\n\nName of an extension module. Default is untitled.  Warning Don\u2019t use this option if a signature file (*.pyf) is used.   --[no-]lower\n\nDo [not] lower the cases in <fortran files>. By default, --lower is assumed with -h switch, and --no-lower without the -h switch.  --build-dir <dirname>\n\nAll F2PY generated files are created in <dirname>. Default is tempfile.mkdtemp().  --quiet\n\nRun quietly.  --verbose\n\nRun with extra verbosity.  -v\n\nPrint the F2PY version and exit.   Execute f2py without any options to get an up-to-date list of available options.    Python module numpy.f2py\n  Warning The current Python interface to the f2py module is not mature and may change in the future.  Fortran to Python Interface Generator.   numpy.f2py.compile(source, modulename='untitled', extra_args='', verbose=True, source_fn=None, extension='.f', full_output=False)[source]\n \nBuild extension module from a Fortran 77 source string with f2py.  Parameters \n \nsourcestr or bytes\n\n\nFortran source of module / subroutine to compile  Changed in version 1.16.0: Accept str as well as bytes   \nmodulenamestr, optional\n\n\nThe name of the compiled python module  \nextra_argsstr or list, optional\n\n\nAdditional parameters passed to f2py  Changed in version 1.16.0: A list of args may also be provided.   \nverbosebool, optional\n\n\nPrint f2py output to screen  \nsource_fnstr, optional\n\n\nName of the file where the fortran source is written. The default is to use a temporary file with the extension provided by the extension parameter  \nextension{\u2018.f\u2019, \u2018.f90\u2019}, optional\n\n\nFilename extension if source_fn is not provided. The extension tells which fortran standard is used. The default is f, which implies F77 standard.  New in version 1.11.0.   \nfull_outputbool, optional\n\n\nIf True, return a subprocess.CompletedProcess containing the stdout and stderr of the compile process, instead of just the status code.  New in version 1.20.0.     Returns \n \nresultint or subprocess.CompletedProcess\n\n\n0 on success, or a subprocess.CompletedProcess if full_output=True     Examples >>> import numpy.f2py\n>>> fsource = '''\n...       subroutine foo\n...       print*, \"Hello world!\"\n...       end \n... '''\n>>> numpy.f2py.compile(fsource, modulename='hello', verbose=0)\n0\n>>> import hello\n>>> hello.foo()\n Hello world!\n \n   numpy.f2py.get_include()[source]\n \nReturn the directory that contains the fortranobject.c and .h files.  Note This function is not needed when building an extension with numpy.distutils directly from .f and/or .pyf files in one go.  Python extension modules built with f2py-generated code need to use fortranobject.c as a source file, and include the fortranobject.h header. This function can be used to obtain the directory containing both of these files.  Returns \n \ninclude_pathstr\n\n\nAbsolute path to the directory containing fortranobject.c and fortranobject.h.      See also  numpy.get_include\n\nfunction that returns the numpy include directory    Notes  New in version 1.22.0.  Unless the build system you are using has specific support for f2py, building a Python extension using a .pyf signature file is a two-step process. For a module mymod:  Step 1: run python -m numpy.f2py mymod.pyf --quiet. This generates _mymodmodule.c and (if needed) _fblas-f2pywrappers.f files next to mymod.pyf. \nStep 2: build your Python extension module. This requires the following source files:  _mymodmodule.c \n_mymod-f2pywrappers.f (if it was generated in step 1) fortranobject.c    \n   numpy.f2py.run_main(comline_list)[source]\n \nEquivalent to running: f2py <args>\n where <args>=string.join(<list>,' '), but in Python. Unless -h is used, this function returns a dictionary containing information on generated modules and their dependencies on source files. For example, the command f2py -m scalar scalar.f can be executed from Python as follows You cannot build extension modules with this function, that is, using -c is not allowed. Use compile command instead Examples >>> import numpy.f2py\n>>> r = numpy.f2py.run_main(['-m','scalar','doc/source/f2py/scalar.f'])\nReading fortran codes...\n        Reading file 'doc/source/f2py/scalar.f' (format:fix,strict)\nPost-processing...\n        Block: scalar\n                        Block: FOO\nBuilding modules...\n        Building module \"scalar\"...\n        Wrote C/API module \"scalar\" to file \"./scalarmodule.c\"\n>>> print(r)\n{'scalar': {'h': ['/home/users/pearu/src_cvs/f2py/src/fortranobject.h'],\n\t 'csrc': ['./scalarmodule.c', \n                  '/home/users/pearu/src_cvs/f2py/src/fortranobject.c']}}\n \n \n"}, {"name": "1", "path": "reference/arrays.scalars", "type": "Scalars", "text": "Scalars Python defines only one type of a particular data class (there is only one integer type, one floating-point type, etc.). This can be convenient in applications that don\u2019t need to be concerned with all the ways data can be represented in a computer. For scientific computing, however, more control is often needed. In NumPy, there are 24 new fundamental Python types to describe different types of scalars. These type descriptors are mostly based on the types available in the C language that CPython is written in, with several additional types compatible with Python\u2019s types. Array scalars have the same attributes and methods as ndarrays. 1 This allows one to treat items of an array partly on the same footing as arrays, smoothing out rough edges that result when mixing scalar and array operations. Array scalars live in a hierarchy (see the Figure below) of data types. They can be detected using the hierarchy: For example, isinstance(val, np.generic) will return True if val is an array scalar object. Alternatively, what kind of array scalar is present can be determined using other members of the data type hierarchy. Thus, for example isinstance(val, np.complexfloating) will return True if val is a complex valued type, while isinstance(val, np.flexible) will return true if val is one of the flexible itemsize array types (str_, bytes_, void).    Figure: Hierarchy of type objects representing the array data types. Not shown are the two integer types intp and uintp which just point to the integer type that holds a pointer for the platform. All the number types can be obtained using bit-width names as well.    1 \nHowever, array scalars are immutable, so none of the array scalar attributes are settable.    Built-in scalar types The built-in scalar types are shown below. The C-like names are associated with character codes, which are shown in their descriptions. Use of the character codes, however, is discouraged. Some of the scalar types are essentially equivalent to fundamental Python types and therefore inherit from them as well as from the generic array scalar type:   \nArray scalar type Related Python type Inherits?   \nint_ int Python 2 only  \nfloat_ float yes  \ncomplex_ complex yes  \nbytes_ bytes yes  \nstr_ str yes  \nbool_ bool no  \ndatetime64 datetime.datetime no  \ntimedelta64 datetime.timedelta no   The bool_ data type is very similar to the Python bool but does not inherit from it because Python\u2019s bool does not allow itself to be inherited from, and on the C-level the size of the actual bool data is not the same as a Python Boolean scalar.  Warning The int_ type does not inherit from the int built-in under Python 3, because type int is no longer a fixed-width integer type.   Tip The default data type in NumPy is float_.    class numpy.generic[source]\n \nBase class for numpy scalar types. Class from which most (all?) numpy scalar types are derived. For consistency, exposes the same API as ndarray, despite many consequent attributes being either \u201cget-only,\u201d or completely irrelevant. This is the class from which it is strongly suggested users should derive custom scalar types. \n   class numpy.number[source]\n \nAbstract base class of all numeric scalar types. \n  Integer types   class numpy.integer[source]\n \nAbstract base class of all integer scalar types. \n  Note The numpy integer types mirror the behavior of C integers, and can therefore be subject to Overflow Errors.   Signed integer types   class numpy.signedinteger[source]\n \nAbstract base class of all signed integer scalar types. \n   class numpy.byte[source]\n \nSigned integer type, compatible with C char.  Character code \n'b'  Alias on this platform (Linux x86_64) \nnumpy.int8: 8-bit signed integer (-128 to 127).   \n   class numpy.short[source]\n \nSigned integer type, compatible with C short.  Character code \n'h'  Alias on this platform (Linux x86_64) \nnumpy.int16: 16-bit signed integer (-32_768 to 32_767).   \n   class numpy.intc[source]\n \nSigned integer type, compatible with C int.  Character code \n'i'  Alias on this platform (Linux x86_64) \nnumpy.int32: 32-bit signed integer (-2_147_483_648 to 2_147_483_647).   \n   class numpy.int_[source]\n \nSigned integer type, compatible with Python int and C long.  Character code \n'l'  Alias on this platform (Linux x86_64) \nnumpy.int64: 64-bit signed integer (-9_223_372_036_854_775_808 to 9_223_372_036_854_775_807).  Alias on this platform (Linux x86_64) \nnumpy.intp: Signed integer large enough to fit pointer, compatible with C intptr_t.   \n   class numpy.longlong[source]\n \nSigned integer type, compatible with C long long.  Character code \n'q'   \n   Unsigned integer types   class numpy.unsignedinteger[source]\n \nAbstract base class of all unsigned integer scalar types. \n   class numpy.ubyte[source]\n \nUnsigned integer type, compatible with C unsigned char.  Character code \n'B'  Alias on this platform (Linux x86_64) \nnumpy.uint8: 8-bit unsigned integer (0 to 255).   \n   class numpy.ushort[source]\n \nUnsigned integer type, compatible with C unsigned short.  Character code \n'H'  Alias on this platform (Linux x86_64) \nnumpy.uint16: 16-bit unsigned integer (0 to 65_535).   \n   class numpy.uintc[source]\n \nUnsigned integer type, compatible with C unsigned int.  Character code \n'I'  Alias on this platform (Linux x86_64) \nnumpy.uint32: 32-bit unsigned integer (0 to 4_294_967_295).   \n   class numpy.uint[source]\n \nUnsigned integer type, compatible with C unsigned long.  Character code \n'L'  Alias on this platform (Linux x86_64) \nnumpy.uint64: 64-bit unsigned integer (0 to 18_446_744_073_709_551_615).  Alias on this platform (Linux x86_64) \nnumpy.uintp: Unsigned integer large enough to fit pointer, compatible with C uintptr_t.   \n   class numpy.ulonglong[source]\n \nSigned integer type, compatible with C unsigned long long.  Character code \n'Q'   \n    Inexact types   class numpy.inexact[source]\n \nAbstract base class of all numeric scalar types with a (potentially) inexact representation of the values in its range, such as floating-point numbers. \n  Note Inexact scalars are printed using the fewest decimal digits needed to distinguish their value from other values of the same datatype, by judicious rounding. See the unique parameter of format_float_positional and format_float_scientific. This means that variables with equal binary values but whose datatypes are of different precisions may display differently: >>> f16 = np.float16(\"0.1\")\n>>> f32 = np.float32(f16)\n>>> f64 = np.float64(f32)\n>>> f16 == f32 == f64\nTrue\n>>> f16, f32, f64\n(0.1, 0.099975586, 0.0999755859375)\n Note that none of these floats hold the exact value \\(\\frac{1}{10}\\); f16 prints as 0.1 because it is as close to that value as possible, whereas the other types do not as they have more precision and therefore have closer values. Conversely, floating-point scalars of different precisions which approximate the same decimal value may compare unequal despite printing identically: >>> f16 = np.float16(\"0.1\")\n>>> f32 = np.float32(\"0.1\")\n>>> f64 = np.float64(\"0.1\")\n>>> f16 == f32 == f64\nFalse\n>>> f16, f32, f64\n(0.1, 0.1, 0.1)\n   Floating-point types   class numpy.floating[source]\n \nAbstract base class of all floating-point scalar types. \n   class numpy.half[source]\n \nHalf-precision floating-point number type.  Character code \n'e'  Alias on this platform (Linux x86_64) \nnumpy.float16: 16-bit-precision floating-point number type: sign bit, 5 bits exponent, 10 bits mantissa.   \n   class numpy.single[source]\n \nSingle-precision floating-point number type, compatible with C float.  Character code \n'f'  Alias on this platform (Linux x86_64) \nnumpy.float32: 32-bit-precision floating-point number type: sign bit, 8 bits exponent, 23 bits mantissa.   \n   class numpy.double(x=0, /)[source]\n \nDouble-precision floating-point number type, compatible with Python float and C double.  Character code \n'd'  Alias \nnumpy.float_  Alias on this platform (Linux x86_64) \nnumpy.float64: 64-bit precision floating-point number type: sign bit, 11 bits exponent, 52 bits mantissa.   \n   class numpy.longdouble[source]\n \nExtended-precision floating-point number type, compatible with C long double but not necessarily with IEEE 754 quadruple-precision.  Character code \n'g'  Alias \nnumpy.longfloat  Alias on this platform (Linux x86_64) \nnumpy.float128: 128-bit extended-precision floating-point number type.   \n   Complex floating-point types   class numpy.complexfloating[source]\n \nAbstract base class of all complex number scalar types that are made up of floating-point numbers. \n   class numpy.csingle[source]\n \nComplex number type composed of two single-precision floating-point numbers.  Character code \n'F'  Alias \nnumpy.singlecomplex  Alias on this platform (Linux x86_64) \nnumpy.complex64: Complex number type composed of 2 32-bit-precision floating-point numbers.   \n   class numpy.cdouble(real=0, imag=0)[source]\n \nComplex number type composed of two double-precision floating-point numbers, compatible with Python complex.  Character code \n'D'  Alias \nnumpy.cfloat  Alias \nnumpy.complex_  Alias on this platform (Linux x86_64) \nnumpy.complex128: Complex number type composed of 2 64-bit-precision floating-point numbers.   \n   class numpy.clongdouble[source]\n \nComplex number type composed of two extended-precision floating-point numbers.  Character code \n'G'  Alias \nnumpy.clongfloat  Alias \nnumpy.longcomplex  Alias on this platform (Linux x86_64) \nnumpy.complex256: Complex number type composed of 2 128-bit extended-precision floating-point numbers.   \n    Other types   class numpy.bool_[source]\n \nBoolean type (True or False), stored as a byte.  Warning The bool_ type is not a subclass of the int_ type (the bool_ is not even a number type). This is different than Python\u2019s default implementation of bool as a sub-class of int.   Character code \n'?'  Alias \nnumpy.bool8   \n   class numpy.datetime64[source]\n \nIf created from a 64-bit integer, it represents an offset from 1970-01-01T00:00:00. If created from string, the string can be in ISO 8601 date or datetime format. >>> np.datetime64(10, 'Y')\nnumpy.datetime64('1980')\n>>> np.datetime64('1980', 'Y')\nnumpy.datetime64('1980')\n>>> np.datetime64(10, 'D')\nnumpy.datetime64('1970-01-11')\n See Datetimes and Timedeltas for more information.  Character code \n'M'   \n   class numpy.timedelta64[source]\n \nA timedelta stored as a 64-bit integer. See Datetimes and Timedeltas for more information.  Character code \n'm'   \n   class numpy.object_[source]\n \nAny Python object.  Character code \n'O'   \n  Note The data actually stored in object arrays (i.e., arrays having dtype object_) are references to Python objects, not the objects themselves. Hence, object arrays behave more like usual Python lists, in the sense that their contents need not be of the same Python type. The object type is also special because an array containing object_ items does not return an object_ object on item access, but instead returns the actual object that the array item refers to.  The following data types are flexible: they have no predefined size and the data they describe can be of different length in different arrays. (In the character codes # is an integer denoting how many elements the data type consists of.)   class numpy.flexible[source]\n \nAbstract base class of all scalar types without predefined length. The actual size of these types depends on the specific np.dtype instantiation. \n   class numpy.bytes_[source]\n \nA byte string. When used in arrays, this type strips trailing null bytes.  Character code \n'S'  Alias \nnumpy.string_   \n   class numpy.str_[source]\n \nA unicode string. When used in arrays, this type strips trailing null codepoints. Unlike the builtin str, this supports the Buffer Protocol, exposing its contents as UCS4: >>> m = memoryview(np.str_(\"abc\"))\n>>> m.format\n'3w'\n>>> m.tobytes()\nb'a\\x00\\x00\\x00b\\x00\\x00\\x00c\\x00\\x00\\x00'\n  Character code \n'U'  Alias \nnumpy.unicode_   \n   class numpy.void[source]\n \nEither an opaque sequence of bytes, or a structure. >>> np.void(b'abcd')\nvoid(b'\\x61\\x62\\x63\\x64')\n Structured void scalars can only be constructed via extraction from Structured arrays: >>> arr = np.array((1, 2), dtype=[('x', np.int8), ('y', np.int8)])\n>>> arr[()]\n(1, 2)  # looks like a tuple, but is `np.void`\n  Character code \n'V'   \n  Warning See Note on string types. Numeric Compatibility: If you used old typecode characters in your Numeric code (which was never recommended), you will need to change some of them to the new characters. In particular, the needed changes are c -> S1, b -> B, 1 -> b, s -> h, w ->\nH, and u -> I. These changes make the type character convention more consistent with other Python modules such as the struct module.    Sized aliases Along with their (mostly) C-derived names, the integer, float, and complex data-types are also available using a bit-width convention so that an array of the right size can always be ensured. Two aliases (numpy.intp and numpy.uintp) pointing to the integer type that is sufficiently large to hold a C pointer are also provided.   numpy.bool8[source]\n \nalias of numpy.bool_ \n   numpy.int8[source]\n  numpy.int16\n  numpy.int32\n  numpy.int64\n \nAliases for the signed integer types (one of numpy.byte, numpy.short, numpy.intc, numpy.int_ and numpy.longlong) with the specified number of bits. Compatible with the C99 int8_t, int16_t, int32_t, and int64_t, respectively. \n   numpy.uint8[source]\n  numpy.uint16\n  numpy.uint32\n  numpy.uint64\n \nAlias for the unsigned integer types (one of numpy.ubyte, numpy.ushort, numpy.uintc, numpy.uint and numpy.ulonglong) with the specified number of bits. Compatible with the C99 uint8_t, uint16_t, uint32_t, and uint64_t, respectively. \n   numpy.intp[source]\n \nAlias for the signed integer type (one of numpy.byte, numpy.short, numpy.intc, numpy.int_ and np.longlong) that is the same size as a pointer. Compatible with the C intptr_t.  Character code \n'p'   \n   numpy.uintp[source]\n \nAlias for the unsigned integer type (one of numpy.ubyte, numpy.ushort, numpy.uintc, numpy.uint and np.ulonglong) that is the same size as a pointer. Compatible with the C uintptr_t.  Character code \n'P'   \n   numpy.float16[source]\n \nalias of numpy.half \n   numpy.float32[source]\n \nalias of numpy.single \n   numpy.float64[source]\n \nalias of numpy.double \n   numpy.float96\n  numpy.float128[source]\n \nAlias for numpy.longdouble, named after its size in bits. The existence of these aliases depends on the platform. \n   numpy.complex64[source]\n \nalias of numpy.csingle \n   numpy.complex128[source]\n \nalias of numpy.cdouble \n   numpy.complex192\n  numpy.complex256[source]\n \nAlias for numpy.clongdouble, named after its size in bits. The existence of these aliases depends on the platform. \n   Other aliases The first two of these are conveniences which resemble the names of the builtin types, in the same style as bool_, int_, str_, bytes_, and object_:   numpy.float_[source]\n \nalias of numpy.double \n   numpy.complex_[source]\n \nalias of numpy.cdouble \n Some more use alternate naming conventions for extended-precision floats and complex numbers:   numpy.longfloat[source]\n \nalias of numpy.longdouble \n   numpy.singlecomplex[source]\n \nalias of numpy.csingle \n   numpy.cfloat[source]\n \nalias of numpy.cdouble \n   numpy.longcomplex[source]\n \nalias of numpy.clongdouble \n   numpy.clongfloat[source]\n \nalias of numpy.clongdouble \n The following aliases originate from Python 2, and it is recommended that they not be used in new code.   numpy.string_[source]\n \nalias of numpy.bytes_ \n   numpy.unicode_[source]\n \nalias of numpy.str_ \n    Attributes The array scalar objects have an array priority of NPY_SCALAR_PRIORITY (-1,000,000.0). They also do not (yet) have a ctypes attribute. Otherwise, they share the same attributes as arrays:  \ngeneric.flags The integer value of flags.  \ngeneric.shape Tuple of array dimensions.  \ngeneric.strides Tuple of bytes steps in each dimension.  \ngeneric.ndim The number of array dimensions.  \ngeneric.data Pointer to start of data.  \ngeneric.size The number of elements in the gentype.  \ngeneric.itemsize The length of one element in bytes.  \ngeneric.base Scalar attribute identical to the corresponding array attribute.  \ngeneric.dtype Get array data-descriptor.  \ngeneric.real The real part of the scalar.  \ngeneric.imag The imaginary part of the scalar.  \ngeneric.flat A 1-D view of the scalar.  \ngeneric.T Scalar attribute identical to the corresponding array attribute.  \ngeneric.__array_interface__ Array protocol: Python side  \ngeneric.__array_struct__ Array protocol: struct  \ngeneric.__array_priority__ Array priority.  \ngeneric.__array_wrap__ sc.__array_wrap__(obj) return scalar from array     Indexing  See also Indexing routines, Data type objects (dtype)  Array scalars can be indexed like 0-dimensional arrays: if x is an array scalar,  \nx[()] returns a copy of array scalar \nx[...] returns a 0-dimensional ndarray\n \nx['field-name'] returns the array scalar in the field field-name. (x can have fields, for example, when it corresponds to a structured data type.)    Methods Array scalars have exactly the same methods as arrays. The default behavior of these methods is to internally convert the scalar to an equivalent 0-dimensional array and to call the corresponding array method. In addition, math operations on array scalars are defined so that the same hardware flags are set and used to interpret the results as for ufunc, so that the error state used for ufuncs also carries over to the math on array scalars. The exceptions to the above rules are given below:  \ngeneric.__array__ sc.__array__(dtype) return 0-dim array from scalar with specified dtype  \ngeneric.__array_wrap__ sc.__array_wrap__(obj) return scalar from array  \ngeneric.squeeze Scalar method identical to the corresponding array attribute.  \ngeneric.byteswap Scalar method identical to the corresponding array attribute.  \ngeneric.__reduce__ Helper for pickle.  \ngeneric.__setstate__   \ngeneric.setflags Scalar method identical to the corresponding array attribute.   Utility method for typing:  \nnumber.__class_getitem__(item, /) Return a parametrized wrapper around the number type.     Defining new types There are two ways to effectively define a new array scalar type (apart from composing structured types dtypes from the built-in scalar types): One way is to simply subclass the ndarray and overwrite the methods of interest. This will work to a degree, but internally certain behaviors are fixed by the data type of the array. To fully customize the data type of an array you need to define a new data-type, and register it with NumPy. Such new types can only be defined in C, using the NumPy C-API. \n"}, {"name": "1", "path": "reference/random/parallel", "type": "Parallel Applications", "text": "Parallel Random Number Generation There are three strategies implemented that can be used to produce repeatable pseudo-random numbers across multiple processes (local or distributed).  SeedSequence spawning SeedSequence implements an algorithm to process a user-provided seed, typically as an integer of some size, and to convert it into an initial state for a BitGenerator. It uses hashing techniques to ensure that low-quality seeds are turned into high quality initial states (at least, with very high probability). For example, MT19937 has a state consisting of 624 uint32 integers. A naive way to take a 32-bit integer seed would be to just set the last element of the state to the 32-bit seed and leave the rest 0s. This is a valid state for MT19937, but not a good one. The Mersenne Twister algorithm suffers if there are too many 0s. Similarly, two adjacent 32-bit integer seeds (i.e. 12345 and 12346) would produce very similar streams. SeedSequence avoids these problems by using successions of integer hashes with good avalanche properties to ensure that flipping any bit in the input input has about a 50% chance of flipping any bit in the output. Two input seeds that are very close to each other will produce initial states that are very far from each other (with very high probability). It is also constructed in such a way that you can provide arbitrary-sized integers or lists of integers. SeedSequence will take all of the bits that you provide and mix them together to produce however many bits the consuming BitGenerator needs to initialize itself. These properties together mean that we can safely mix together the usual user-provided seed with simple incrementing counters to get BitGenerator states that are (to very high probability) independent of each other. We can wrap this together into an API that is easy to use and difficult to misuse. from numpy.random import SeedSequence, default_rng\n\nss = SeedSequence(12345)\n\n# Spawn off 10 child SeedSequences to pass to child processes.\nchild_seeds = ss.spawn(10)\nstreams = [default_rng(s) for s in child_seeds]\n Child SeedSequence objects can also spawn to make grandchildren, and so on. Each SeedSequence has its position in the tree of spawned SeedSequence objects mixed in with the user-provided seed to generate independent (with very high probability) streams. grandchildren = child_seeds[0].spawn(4)\ngrand_streams = [default_rng(s) for s in grandchildren]\n This feature lets you make local decisions about when and how to split up streams without coordination between processes. You do not have to preallocate space to avoid overlapping or request streams from a common global service. This general \u201ctree-hashing\u201d scheme is not unique to numpy but not yet widespread. Python has increasingly-flexible mechanisms for parallelization available, and this scheme fits in very well with that kind of use. Using this scheme, an upper bound on the probability of a collision can be estimated if one knows the number of streams that you derive. SeedSequence hashes its inputs, both the seed and the spawn-tree-path, down to a 128-bit pool by default. The probability that there is a collision in that pool, pessimistically-estimated (1), will be about \\(n^2*2^{-128}\\) where n is the number of streams spawned. If a program uses an aggressive million streams, about \\(2^{20}\\), then the probability that at least one pair of them are identical is about \\(2^{-88}\\), which is in solidly-ignorable territory (2).  1 \nThe algorithm is carefully designed to eliminate a number of possible ways to collide. For example, if one only does one level of spawning, it is guaranteed that all states will be unique. But it\u2019s easier to estimate the naive upper bound on a napkin and take comfort knowing that the probability is actually lower.  2 \nIn this calculation, we can mostly ignore the amount of numbers drawn from each stream. See Upgrading PCG64 with PCG64DXSM for the technical details about PCG64. The other PRNGs we provide have some extra protection built in that avoids overlaps if the SeedSequence pools differ in the slightest bit. PCG64DXSM has \\(2^{127}\\) separate cycles determined by the seed in addition to the position in the \\(2^{128}\\) long period for each cycle, so one has to both get on or near the same cycle and seed a nearby position in the cycle. Philox has completely independent cycles determined by the seed. SFC64 incorporates a 64-bit counter so every unique seed is at least \\(2^{64}\\) iterations away from any other seed. And finally, MT19937 has just an unimaginably huge period. Getting a collision internal to SeedSequence is the way a failure would be observed.     Independent Streams Philox is a counter-based RNG based which generates values by encrypting an incrementing counter using weak cryptographic primitives. The seed determines the key that is used for the encryption. Unique keys create unique, independent streams. Philox lets you bypass the seeding algorithm to directly set the 128-bit key. Similar, but different, keys will still create independent streams. import secrets\nfrom numpy.random import Philox\n\n# 128-bit number as a seed\nroot_seed = secrets.getrandbits(128)\nstreams = [Philox(key=root_seed + stream_id) for stream_id in range(10)]\n This scheme does require that you avoid reusing stream IDs. This may require coordination between the parallel processes.   Jumping the BitGenerator state jumped advances the state of the BitGenerator as-if a large number of random numbers have been drawn, and returns a new instance with this state. The specific number of draws varies by BitGenerator, and ranges from \\(2^{64}\\) to \\(2^{128}\\). Additionally, the as-if draws also depend on the size of the default random number produced by the specific BitGenerator. The BitGenerators that support jumped, along with the period of the BitGenerator, the size of the jump and the bits in the default unsigned random are listed below.   \nBitGenerator Period Jump Size Bits per Draw   \nMT19937 \\(2^{19937}-1\\) \\(2^{128}\\) 32  \nPCG64 \\(2^{128}\\) \\(~2^{127}\\) (3) 64  \nPCG64DXSM \\(2^{128}\\) \\(~2^{127}\\) (3) 64  \nPhilox \\(2^{256}\\) \\(2^{128}\\) 64    \n3(1,2)\n \nThe jump size is \\((\\phi-1)*2^{128}\\) where \\(\\phi\\) is the golden ratio. As the jumps wrap around the period, the actual distances between neighboring streams will slowly grow smaller than the jump size, but using the golden ratio this way is a classic method of constructing a low-discrepancy sequence that spreads out the states around the period optimally. You will not be able to jump enough to make those distances small enough to overlap in your lifetime.   jumped can be used to produce long blocks which should be long enough to not overlap. import secrets\nfrom numpy.random import PCG64\n\nseed = secrets.getrandbits(128)\nblocked_rng = []\nrng = PCG64(seed)\nfor i in range(10):\n    blocked_rng.append(rng.jumped(i))\n When using jumped, one does have to take care not to jump to a stream that was already used. In the above example, one could not later use blocked_rng[0].jumped() as it would overlap with blocked_rng[1]. Like with the independent streams, if the main process here wants to split off 10 more streams by jumping, then it needs to start with range(10, 20), otherwise it would recreate the same streams. On the other hand, if you carefully construct the streams, then you are guaranteed to have streams that do not overlap. \n"}, {"name": "1", "path": "user/basics.broadcasting", "type": "User Guide", "text": "Broadcasting  See also numpy.broadcast  The term broadcasting describes how NumPy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is \u201cbroadcast\u201d across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations so that looping occurs in C instead of Python. It does this without making needless copies of data and usually leads to efficient algorithm implementations. There are, however, cases where broadcasting is a bad idea because it leads to inefficient use of memory that slows computation. NumPy operations are usually done on pairs of arrays on an element-by-element basis. In the simplest case, the two arrays must have exactly the same shape, as in the following example: >>> a = np.array([1.0, 2.0, 3.0])\n>>> b = np.array([2.0, 2.0, 2.0])\n>>> a * b\narray([ 2.,  4.,  6.])\n NumPy\u2019s broadcasting rule relaxes this constraint when the arrays\u2019 shapes meet certain constraints. The simplest broadcasting example occurs when an array and a scalar value are combined in an operation: >>> a = np.array([1.0, 2.0, 3.0])\n>>> b = 2.0\n>>> a * b\narray([ 2.,  4.,  6.])\n The result is equivalent to the previous example where b was an array. We can think of the scalar b being stretched during the arithmetic operation into an array with the same shape as a. The new elements in b, as shown in Figure 1, are simply copies of the original scalar. The stretching analogy is only conceptual. NumPy is smart enough to use the original scalar value without actually making copies so that broadcasting operations are as memory and computationally efficient as possible.    Figure 1  In the simplest example of broadcasting, the scalar b is stretched to become an array of same shape as a so the shapes are compatible for element-by-element multiplication.    The code in the second example is more efficient than that in the first because broadcasting moves less memory around during the multiplication (b is a scalar rather than an array).  General Broadcasting Rules When operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing (i.e. rightmost) dimensions and works its way left. Two dimensions are compatible when  they are equal, or one of them is 1  If these conditions are not met, a ValueError: operands could not be broadcast together exception is thrown, indicating that the arrays have incompatible shapes. The size of the resulting array is the size that is not 1 along each axis of the inputs. Arrays do not need to have the same number of dimensions. For example, if you have a 256x256x3 array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible: Image  (3d array): 256 x 256 x 3\nScale  (1d array):             3\nResult (3d array): 256 x 256 x 3\n When either of the dimensions compared is one, the other is used. In other words, dimensions with size 1 are stretched or \u201ccopied\u201d to match the other. In the following example, both the A and B arrays have axes with length one that are expanded to a larger size during the broadcast operation: A      (4d array):  8 x 1 x 6 x 1\nB      (3d array):      7 x 1 x 5\nResult (4d array):  8 x 7 x 6 x 5\n   Broadcastable arrays A set of arrays is called \u201cbroadcastable\u201d to the same shape if the above rules produce a valid result. For example, if a.shape is (5,1), b.shape is (1,6), c.shape is (6,) and d.shape is () so that d is a scalar, then a, b, c, and d are all broadcastable to dimension (5,6); and  \na acts like a (5,6) array where a[:,0] is broadcast to the other columns, \nb acts like a (5,6) array where b[0,:] is broadcast to the other rows, \nc acts like a (1,6) array and therefore like a (5,6) array where c[:] is broadcast to every row, and finally, \nd acts like a (5,6) array where the single value is repeated.  Here are some more examples: A      (2d array):  5 x 4\nB      (1d array):      1\nResult (2d array):  5 x 4\n\nA      (2d array):  5 x 4\nB      (1d array):      4\nResult (2d array):  5 x 4\n\nA      (3d array):  15 x 3 x 5\nB      (3d array):  15 x 1 x 5\nResult (3d array):  15 x 3 x 5\n\nA      (3d array):  15 x 3 x 5\nB      (2d array):       3 x 5\nResult (3d array):  15 x 3 x 5\n\nA      (3d array):  15 x 3 x 5\nB      (2d array):       3 x 1\nResult (3d array):  15 x 3 x 5\n Here are examples of shapes that do not broadcast: A      (1d array):  3\nB      (1d array):  4 # trailing dimensions do not match\n\nA      (2d array):      2 x 1\nB      (3d array):  8 x 4 x 3 # second from last dimensions mismatched\n An example of broadcasting when a 1-d array is added to a 2-d array: >>> a = array([[ 0.0,  0.0,  0.0],\n...            [10.0, 10.0, 10.0],\n...            [20.0, 20.0, 20.0],\n...            [30.0, 30.0, 30.0]])\n>>> b = array([1.0, 2.0, 3.0])\n>>> a + b\narray([[  1.,   2.,   3.],\n        [ 11.,  12.,  13.],\n        [ 21.,  22.,  23.],\n        [ 31.,  32.,  33.]])\n>>> b = array([1.0, 2.0, 3.0, 4.0])\n>>> a + b\nTraceback (most recent call last):\nValueError: operands could not be broadcast together with shapes (4,3) (4,)\n As shown in Figure 2, b is added to each row of a. In Figure 3, an exception is raised because of the incompatible shapes.    Figure 2  A one dimensional array added to a two dimensional array results in broadcasting if number of 1-d array elements matches the number of 2-d array columns.       Figure 3  When the trailing dimensions of the arrays are unequal, broadcasting fails because it is impossible to align the values in the rows of the 1st array with the elements of the 2nd arrays for element-by-element addition.    Broadcasting provides a convenient way of taking the outer product (or any other outer operation) of two arrays. The following example shows an outer addition operation of two 1-d arrays: >>> a = np.array([0.0, 10.0, 20.0, 30.0])\n>>> b = np.array([1.0, 2.0, 3.0])\n>>> a[:, np.newaxis] + b\narray([[  1.,   2.,   3.],\n       [ 11.,  12.,  13.],\n       [ 21.,  22.,  23.],\n       [ 31.,  32.,  33.]])\n    Figure 4  In some cases, broadcasting stretches both arrays to form an output array larger than either of the initial arrays.    Here the newaxis index operator inserts a new axis into a, making it a two-dimensional 4x1 array. Combining the 4x1 array with b, which has shape (3,), yields a 4x3 array.   A Practical Example: Vector Quantization Broadcasting comes up quite often in real world problems. A typical example occurs in the vector quantization (VQ) algorithm used in information theory, classification, and other related areas. The basic operation in VQ finds the closest point in a set of points, called codes in VQ jargon, to a given point, called the observation. In the very simple, two-dimensional case shown below, the values in observation describe the weight and height of an athlete to be classified. The codes represent different classes of athletes. 1 Finding the closest point requires calculating the distance between observation and each of the codes. The shortest distance provides the best match. In this example, codes[0] is the closest class indicating that the athlete is likely a basketball player. >>> from numpy import array, argmin, sqrt, sum\n>>> observation = array([111.0, 188.0])\n>>> codes = array([[102.0, 203.0],\n...                [132.0, 193.0],\n...                [45.0, 155.0],\n...                [57.0, 173.0]])\n>>> diff = codes - observation    # the broadcast happens here\n>>> dist = sqrt(sum(diff**2,axis=-1))\n>>> argmin(dist)\n0\n In this example, the observation array is stretched to match the shape of the codes array: Observation      (1d array):      2\nCodes            (2d array):  4 x 2\nDiff             (2d array):  4 x 2\n    Figure 5  The basic operation of vector quantization calculates the distance between an object to be classified, the dark square, and multiple known codes, the gray circles. In this simple case, the codes represent individual classes. More complex cases use multiple codes per class.    Typically, a large number of observations, perhaps read from a database, are compared to a set of codes. Consider this scenario: Observation      (2d array):      10 x 3\nCodes            (2d array):       5 x 3\nDiff             (3d array):  5 x 10 x 3\n The three-dimensional array, diff, is a consequence of broadcasting, not a necessity for the calculation. Large data sets will generate a large intermediate array that is computationally inefficient. Instead, if each observation is calculated individually using a Python loop around the code in the two-dimensional example above, a much smaller array is used. Broadcasting is a powerful tool for writing short and usually intuitive code that does its computations very efficiently in C. However, there are cases when broadcasting uses unnecessarily large amounts of memory for a particular algorithm. In these cases, it is better to write the algorithm\u2019s outer loop in Python. This may also produce more readable code, as algorithms that use broadcasting tend to become more difficult to interpret as the number of dimensions in the broadcast increases. Footnotes  1 \nIn this example, weight has more impact on the distance calculation than height because of the larger values. In practice, it is important to normalize the height and weight, often by their standard deviation across the data set, so that both have equal influence on the distance calculation.   \n"}, {"name": "1", "path": "reference/routines.polynomials", "type": "Polynomials", "text": "Polynomials Polynomials in NumPy can be created, manipulated, and even fitted using the convenience classes of the numpy.polynomial package, introduced in NumPy 1.4. Prior to NumPy 1.4, numpy.poly1d was the class of choice and it is still available in order to maintain backward compatibility. However, the newer polynomial package is more complete and its convenience classes provide a more consistent, better-behaved interface for working with polynomial expressions. Therefore numpy.polynomial is recommended for new coding.  Note Terminology The term polynomial module refers to the old API defined in numpy.lib.polynomial, which includes the numpy.poly1d class and the polynomial functions prefixed with poly accessible from the numpy namespace (e.g. numpy.polyadd, numpy.polyval, numpy.polyfit, etc.). The term polynomial package refers to the new API defined in numpy.polynomial, which includes the convenience classes for the different kinds of polynomials (numpy.polynomial.Polynomial, numpy.polynomial.Chebyshev, etc.).   Transitioning from numpy.poly1d to numpy.polynomial As noted above, the poly1d class and associated functions defined in numpy.lib.polynomial, such as numpy.polyfit and numpy.poly, are considered legacy and should not be used in new code. Since NumPy version 1.4, the numpy.polynomial package is preferred for working with polynomials.  Quick Reference The following table highlights some of the main differences between the legacy polynomial module and the polynomial package for common tasks. The Polynomial class is imported for brevity: from numpy.polynomial import Polynomial\n  \nHow to\u2026 Legacy (numpy.poly1d) numpy.polynomial  \nCreate a polynomial object from coefficients 1 p = np.poly1d([1, 2, 3]) p = Polynomial([3, 2, 1])  \nCreate a polynomial object from roots r = np.poly([-1, 1]) p = np.poly1d(r) p = Polynomial.fromroots([-1, 1])  \nFit a polynomial of degree deg to data np.polyfit(x, y, deg) Polynomial.fit(x, y, deg)    1 \nNote the reversed ordering of the coefficients     Transition Guide There are significant differences between numpy.lib.polynomial and numpy.polynomial. The most significant difference is the ordering of the coefficients for the polynomial expressions. The various routines in numpy.polynomial all deal with series whose coefficients go from degree zero upward, which is the reverse order of the poly1d convention. The easy way to remember this is that indices correspond to degree, i.e., coef[i] is the coefficient of the term of degree i. Though the difference in convention may be confusing, it is straightforward to convert from the legacy polynomial API to the new. For example, the following demonstrates how you would convert a numpy.poly1d instance representing the expression \\(x^{2} + 2x + 3\\) to a Polynomial instance representing the same expression: >>> p1d = np.poly1d([1, 2, 3])\n>>> p = np.polynomial.Polynomial(p1d.coef[::-1])\n In addition to the coef attribute, polynomials from the polynomial package also have domain and window attributes. These attributes are most relevant when fitting polynomials to data, though it should be noted that polynomials with different domain and window attributes are not considered equal, and can\u2019t be mixed in arithmetic: >>> p1 = np.polynomial.Polynomial([1, 2, 3])\n>>> p1\nPolynomial([1., 2., 3.], domain=[-1,  1], window=[-1,  1])\n>>> p2 = np.polynomial.Polynomial([1, 2, 3], domain=[-2, 2])\n>>> p1 == p2\nFalse\n>>> p1 + p2\nTraceback (most recent call last):\n    ...\nTypeError: Domains differ\n See the documentation for the convenience classes for further details on the domain and window attributes. Another major difference between the legacy polynomial module and the polynomial package is polynomial fitting. In the old module, fitting was done via the polyfit function. In the polynomial package, the fit class method is preferred. For example, consider a simple linear fit to the following data: In [1]: rng = np.random.default_rng()\n\nIn [2]: x = np.arange(10)\n\nIn [3]: y = np.arange(10) + rng.standard_normal(10)\n With the legacy polynomial module, a linear fit (i.e. polynomial of degree 1) could be applied to these data with polyfit: In [4]: np.polyfit(x, y, deg=1)\nOut[4]: array([0.89865114, 0.25736425])\n With the new polynomial API, the fit class method is preferred: In [5]: p_fitted = np.polynomial.Polynomial.fit(x, y, deg=1)\n\nIn [6]: p_fitted\nOut[6]: Polynomial([4.30129436, 4.04393012], domain=[0., 9.], window=[-1.,  1.])\n Note that the coefficients are given in the scaled domain defined by the linear mapping between the window and domain. convert can be used to get the coefficients in the unscaled data domain. In [7]: p_fitted.convert()\nOut[7]: Polynomial([0.25736425, 0.89865114], domain=[-1.,  1.], window=[-1.,  1.])\n    Documentation for the polynomial Package In addition to standard power series polynomials, the polynomial package provides several additional kinds of polynomials including Chebyshev, Hermite (two subtypes), Laguerre, and Legendre polynomials. Each of these has an associated convenience class available from the numpy.polynomial namespace that provides a consistent interface for working with polynomials regardless of their type.  Using the Convenience Classes  Documentation pertaining to specific functions defined for each kind of polynomial individually can be found in the corresponding module documentation:  Power Series (numpy.polynomial.polynomial) Chebyshev Series (numpy.polynomial.chebyshev) Hermite Series, \u201cPhysicists\u201d (numpy.polynomial.hermite) HermiteE Series, \u201cProbabilists\u201d (numpy.polynomial.hermite_e) Laguerre Series (numpy.polynomial.laguerre) Legendre Series (numpy.polynomial.legendre) Polyutils    Documentation for Legacy Polynomials  \nPoly1d Basics Fitting Calculus Arithmetic Warnings    \n"}, {"name": "1", "path": "reference/routines.polynomials.chebyshev", "type": "Chebyshev Series ( \n        \n         numpy.polynomial.chebyshev\n        \n        )", "text": "Chebyshev Series (numpy.polynomial.chebyshev) This module provides a number of objects (mostly functions) useful for dealing with Chebyshev series, including a Chebyshev class that encapsulates the usual arithmetic operations. (General information on how this module represents and works with such polynomials is in the docstring for its \u201cparent\u201d sub-package, numpy.polynomial).  Classes  \nChebyshev(coef[, domain, window]) A Chebyshev series class.     Constants  \nchebdomain An array object represents a multidimensional, homogeneous array of fixed-size items.  \nchebzero An array object represents a multidimensional, homogeneous array of fixed-size items.  \nchebone An array object represents a multidimensional, homogeneous array of fixed-size items.  \nchebx An array object represents a multidimensional, homogeneous array of fixed-size items.     Arithmetic  \nchebadd(c1, c2) Add one Chebyshev series to another.  \nchebsub(c1, c2) Subtract one Chebyshev series from another.  \nchebmulx(c) Multiply a Chebyshev series by x.  \nchebmul(c1, c2) Multiply one Chebyshev series by another.  \nchebdiv(c1, c2) Divide one Chebyshev series by another.  \nchebpow(c, pow[, maxpower]) Raise a Chebyshev series to a power.  \nchebval(x, c[, tensor]) Evaluate a Chebyshev series at points x.  \nchebval2d(x, y, c) Evaluate a 2-D Chebyshev series at points (x, y).  \nchebval3d(x, y, z, c) Evaluate a 3-D Chebyshev series at points (x, y, z).  \nchebgrid2d(x, y, c) Evaluate a 2-D Chebyshev series on the Cartesian product of x and y.  \nchebgrid3d(x, y, z, c) Evaluate a 3-D Chebyshev series on the Cartesian product of x, y, and z.     Calculus  \nchebder(c[, m, scl, axis]) Differentiate a Chebyshev series.  \nchebint(c[, m, k, lbnd, scl, axis]) Integrate a Chebyshev series.     Misc Functions  \nchebfromroots(roots) Generate a Chebyshev series with given roots.  \nchebroots(c) Compute the roots of a Chebyshev series.  \nchebvander(x, deg) Pseudo-Vandermonde matrix of given degree.  \nchebvander2d(x, y, deg) Pseudo-Vandermonde matrix of given degrees.  \nchebvander3d(x, y, z, deg) Pseudo-Vandermonde matrix of given degrees.  \nchebgauss(deg) Gauss-Chebyshev quadrature.  \nchebweight(x) The weight function of the Chebyshev polynomials.  \nchebcompanion(c) Return the scaled companion matrix of c.  \nchebfit(x, y, deg[, rcond, full, w]) Least squares fit of Chebyshev series to data.  \nchebpts1(npts) Chebyshev points of the first kind.  \nchebpts2(npts) Chebyshev points of the second kind.  \nchebtrim(c[, tol]) Remove \"small\" \"trailing\" coefficients from a polynomial.  \nchebline(off, scl) Chebyshev series whose graph is a straight line.  \ncheb2poly(c) Convert a Chebyshev series to a polynomial.  \npoly2cheb(pol) Convert a polynomial to a Chebyshev series.  \nchebinterpolate(func, deg[, args]) Interpolate a function at the Chebyshev points of the first kind.     See also numpy.polynomial   Notes The implementations of multiplication, division, integration, and differentiation use the algebraic identities [1]:  \\[\\begin{split}T_n(x) = \\frac{z^n + z^{-n}}{2} \\\\ z\\frac{dx}{dz} = \\frac{z - z^{-1}}{2}.\\end{split}\\] where  \\[x = \\frac{z + z^{-1}}{2}.\\] These identities allow a Chebyshev series to be expressed as a finite, symmetric Laurent series. In this module, this sort of Laurent series is referred to as a \u201cz-series.\u201d   References  1 \nA. T. Benjamin, et al., \u201cCombinatorial Trigonometry with Chebyshev Polynomials,\u201d Journal of Statistical Planning and Inference 14, 2008 (https://web.archive.org/web/20080221202153/https://www.math.hmc.edu/~benjamin/papers/CombTrig.pdf, pg. 4)   \n"}, {"name": "add_data_dir()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.add_data_dir", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": "  add_data_dir(data_path)[source]\n \nRecursively add files under data_path to data_files list. Recursively add files under data_path to the list of data_files to be installed (and distributed). The data_path can be either a relative path-name, or an absolute path-name, or a 2-tuple where the first argument shows where in the install directory the data directory should be installed to.  Parameters \n \ndata_pathseq or str\n\n\nArgument can be either  2-sequence (<datadir suffix>, <path to data directory>) path to data directory where python datadir suffix defaults to package dir.      Notes Rules for installation paths: foo/bar -> (foo/bar, foo/bar) -> parent/foo/bar\n(gun, foo/bar) -> parent/gun\nfoo/* -> (foo/a, foo/a), (foo/b, foo/b) -> parent/foo/a, parent/foo/b\n(gun, foo/*) -> (gun, foo/a), (gun, foo/b) -> gun\n(gun/*, foo/*) -> parent/gun/a, parent/gun/b\n/foo/bar -> (bar, /foo/bar) -> parent/bar\n(gun, /foo/bar) -> parent/gun\n(fun/*/gun/*, sun/foo/bar) -> parent/fun/foo/gun/bar\n Examples For example suppose the source directory contains fun/foo.dat and fun/bar/car.dat: >>> self.add_data_dir('fun')                       \n>>> self.add_data_dir(('sun', 'fun'))              \n>>> self.add_data_dir(('gun', '/full/path/to/fun'))\n Will install data-files to the locations: <package install directory>/\n  fun/\n    foo.dat\n    bar/\n      car.dat\n  sun/\n    foo.dat\n    bar/\n      car.dat\n  gun/\n    foo.dat\n    car.dat\n \n"}, {"name": "add_data_files()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.add_data_files", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": "  add_data_files(*files)[source]\n \nAdd data files to configuration data_files.  Parameters \n \nfilessequence\n\n\nArgument(s) can be either  2-sequence (<datadir prefix>,<path to data file(s)>) paths to data files where python datadir prefix defaults to package dir.      Notes The form of each element of the files sequence is very flexible allowing many combinations of where to get the files from the package and where they should ultimately be installed on the system. The most basic usage is for an element of the files argument sequence to be a simple filename. This will cause that file from the local path to be installed to the installation path of the self.name package (package path). The file argument can also be a relative path in which case the entire relative path will be installed into the package directory. Finally, the file can be an absolute path name in which case the file will be found at the absolute path name but installed to the package path. This basic behavior can be augmented by passing a 2-tuple in as the file argument. The first element of the tuple should specify the relative path (under the package install directory) where the remaining sequence of files should be installed to (it has nothing to do with the file-names in the source distribution). The second element of the tuple is the sequence of files that should be installed. The files in this sequence can be filenames, relative paths, or absolute paths. For absolute paths the file will be installed in the top-level package installation directory (regardless of the first argument). Filenames and relative path names will be installed in the package install directory under the path name given as the first element of the tuple. Rules for installation paths:  file.txt -> (., file.txt)-> parent/file.txt foo/file.txt -> (foo, foo/file.txt) -> parent/foo/file.txt /foo/bar/file.txt -> (., /foo/bar/file.txt) -> parent/file.txt \n*.txt -> parent/a.txt, parent/b.txt foo/*.txt`` -> parent/foo/a.txt, parent/foo/b.txt \n*/*.txt -> (*, */*.txt) -> parent/c/a.txt, parent/d/b.txt (sun, file.txt) -> parent/sun/file.txt (sun, bar/file.txt) -> parent/sun/file.txt (sun, /foo/bar/file.txt) -> parent/sun/file.txt (sun, *.txt) -> parent/sun/a.txt, parent/sun/b.txt (sun, bar/*.txt) -> parent/sun/a.txt, parent/sun/b.txt (sun/*, */*.txt) -> parent/sun/c/a.txt, parent/d/b.txt  An additional feature is that the path to a data-file can actually be a function that takes no arguments and returns the actual path(s) to the data-files. This is useful when the data files are generated while building the package. Examples Add files to the list of data_files to be included with the package. >>> self.add_data_files('foo.dat',\n...     ('fun', ['gun.dat', 'nun/pun.dat', '/tmp/sun.dat']),\n...     'bar/cat.dat',\n...     '/full/path/to/can.dat')                   \n will install these data files to: <package install directory>/\n foo.dat\n fun/\n   gun.dat\n   nun/\n     pun.dat\n sun.dat\n bar/\n   car.dat\n can.dat\n where <package install directory> is the package (or sub-package) directory such as \u2018/usr/lib/python2.4/site-packages/mypackage\u2019 (\u2018C: Python2.4 Lib site-packages mypackage\u2019) or \u2018/usr/lib/python2.4/site- packages/mypackage/mysubpackage\u2019 (\u2018C: Python2.4 Lib site-packages mypackage mysubpackage\u2019). \n"}, {"name": "add_extension()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.add_extension", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": "  add_extension(name, sources, **kw)[source]\n \nAdd extension to configuration. Create and add an Extension instance to the ext_modules list. This method also takes the following optional keyword arguments that are passed on to the Extension constructor.  Parameters \n \nnamestr\n\n\nname of the extension  \nsourcesseq\n\n\nlist of the sources. The list of sources may contain functions (called source generators) which must take an extension instance and a build directory as inputs and return a source file or list of source files or None. If None is returned then no sources are generated. If the Extension instance has no sources after processing all source generators, then no extension module is built.  include_dirs :\ndefine_macros :\nundef_macros :\nlibrary_dirs :\nlibraries :\nruntime_library_dirs :\nextra_objects :\nextra_compile_args :\nextra_link_args :\nextra_f77_compile_args :\nextra_f90_compile_args :\nexport_symbols :\nswig_opts :\ndepends :\n\nThe depends list contains paths to files or directories that the sources of the extension module depend on. If any path in the depends list is newer than the extension module, then the module will be rebuilt.  language :\nf2py_options :\nmodule_dirs :\n\nextra_infodict or list\n\n\ndict or list of dict of keywords to be appended to keywords.     Notes The self.paths(\u2026) method is applied to all lists that may contain paths. \n"}, {"name": "add_headers()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.add_headers", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": "  add_headers(*files)[source]\n \nAdd installable headers to configuration. Add the given sequence of files to the beginning of the headers list. By default, headers will be installed under <python- include>/<self.name.replace(\u2018.\u2019,\u2019/\u2019)>/ directory. If an item of files is a tuple, then its first argument specifies the actual installation location relative to the <python-include> path.  Parameters \n \nfilesstr or seq\n\n\nArgument(s) can be either:  2-sequence (<includedir suffix>,<path to header file(s)>) path(s) to header file(s) where python includedir suffix will default to package name.      \n"}, {"name": "add_include_dirs()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.add_include_dirs", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": "  add_include_dirs(*paths)[source]\n \nAdd paths to configuration include directories. Add the given sequence of paths to the beginning of the include_dirs list. This list will be visible to all extension modules of the current package. \n"}, {"name": "add_installed_library()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.add_installed_library", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": "  add_installed_library(name, sources, install_dir, build_info=None)[source]\n \nSimilar to add_library, but the specified library is installed. Most C libraries used with distutils are only used to build python extensions, but libraries built through this method will be installed so that they can be reused by third-party packages.  Parameters \n \nnamestr\n\n\nName of the installed library.  \nsourcessequence\n\n\nList of the library\u2019s source files. See add_library for details.  \ninstall_dirstr\n\n\nPath to install the library, relative to the current sub-package.  \nbuild_infodict, optional\n\n\nThe following keys are allowed:  depends macros include_dirs extra_compiler_args extra_f77_compile_args extra_f90_compile_args f2py_options language     Returns \n None\n    See also  \nadd_library, add_npy_pkg_config, get_info\n\n  Notes The best way to encode the options required to link against the specified C libraries is to use a \u201clibname.ini\u201d file, and use get_info to retrieve the required options (see add_npy_pkg_config for more information). \n"}, {"name": "add_library()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.add_library", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": "  add_library(name, sources, **build_info)[source]\n \nAdd library to configuration.  Parameters \n \nnamestr\n\n\nName of the extension.  \nsourcessequence\n\n\nList of the sources. The list of sources may contain functions (called source generators) which must take an extension instance and a build directory as inputs and return a source file or list of source files or None. If None is returned then no sources are generated. If the Extension instance has no sources after processing all source generators, then no extension module is built.  \nbuild_infodict, optional\n\n\nThe following keys are allowed:  depends macros include_dirs extra_compiler_args extra_f77_compile_args extra_f90_compile_args f2py_options language      \n"}, {"name": "add_npy_pkg_config()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.add_npy_pkg_config", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": "  add_npy_pkg_config(template, install_dir, subst_dict=None)[source]\n \nGenerate and install a npy-pkg config file from a template. The config file generated from template is installed in the given install directory, using subst_dict for variable substitution.  Parameters \n \ntemplatestr\n\n\nThe path of the template, relatively to the current package path.  \ninstall_dirstr\n\n\nWhere to install the npy-pkg config file, relatively to the current package path.  \nsubst_dictdict, optional\n\n\nIf given, any string of the form @key@ will be replaced by subst_dict[key] in the template file when installed. The install prefix is always available through the variable @prefix@, since the install prefix is not easy to get reliably from setup.py.      See also  \nadd_installed_library, get_info\n\n  Notes This works for both standard installs and in-place builds, i.e. the @prefix@ refer to the source directory for in-place builds. Examples config.add_npy_pkg_config('foo.ini.in', 'lib', {'foo': bar})\n Assuming the foo.ini.in file has the following content: [meta]\nName=@foo@\nVersion=1.0\nDescription=dummy description\n\n[default]\nCflags=-I@prefix@/include\nLibs=\n The generated file will have the following content: [meta]\nName=bar\nVersion=1.0\nDescription=dummy description\n\n[default]\nCflags=-Iprefix_dir/include\nLibs=\n and will be installed as foo.ini in the \u2018lib\u2019 subpath. When cross-compiling with numpy distutils, it might be necessary to use modified npy-pkg-config files. Using the default/generated files will link with the host libraries (i.e. libnpymath.a). For cross-compilation you of-course need to link with target libraries, while using the host Python installation. You can copy out the numpy/core/lib/npy-pkg-config directory, add a pkgdir value to the .ini files and set NPY_PKG_CONFIG_PATH environment variable to point to the directory with the modified npy-pkg-config files. Example npymath.ini modified for cross-compilation: [meta]\nName=npymath\nDescription=Portable, core math library implementing C99 standard\nVersion=0.1\n\n[variables]\npkgname=numpy.core\npkgdir=/build/arm-linux-gnueabi/sysroot/usr/lib/python3.7/site-packages/numpy/core\nprefix=${pkgdir}\nlibdir=${prefix}/lib\nincludedir=${prefix}/include\n\n[default]\nLibs=-L${libdir} -lnpymath\nCflags=-I${includedir}\nRequires=mlib\n\n[msvc]\nLibs=/LIBPATH:${libdir} npymath.lib\nCflags=/INCLUDE:${includedir}\nRequires=mlib\n \n"}, {"name": "add_scripts()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.add_scripts", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": "  add_scripts(*files)[source]\n \nAdd scripts to configuration. Add the sequence of files to the beginning of the scripts list. Scripts will be installed under the <prefix>/bin/ directory. \n"}, {"name": "add_subpackage()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.add_subpackage", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": "  add_subpackage(subpackage_name, subpackage_path=None, standalone=False)[source]\n \nAdd a sub-package to the current Configuration instance. This is useful in a setup.py script for adding sub-packages to a package.  Parameters \n \nsubpackage_namestr\n\n\nname of the subpackage  \nsubpackage_pathstr\n\n\nif given, the subpackage path such as the subpackage is in subpackage_path / subpackage_name. If None,the subpackage is assumed to be located in the local path / subpackage_name.  \nstandalonebool\n\n   \n"}, {"name": "Additional Git Resources", "path": "dev/gitwash/git_resources", "type": "Development", "text": "Additional Git Resources  Tutorials and summaries  \ngithub help has an excellent series of how-to guides. \nlearn.github has an excellent series of tutorials The pro git book is a good in-depth book on git. A git cheat sheet is a page giving summaries of common commands. The git user manual\n The git tutorial\n The git community book\n \ngit ready - a nice series of tutorials \ngit casts - video snippets giving git how-tos. \ngit magic - extended introduction with intermediate detail The git parable is an easy read explaining the concepts behind git. Our own git foundation expands on the git parable. Fernando Perez\u2019 git page - Fernando\u2019s git page - many links and tips A good but technical page on git concepts\n \ngit svn crash course: git for those of us used to subversion\n    Advanced git workflow There are many ways of working with git; here are some posts on the rules of thumb that other projects have come up with:  Linus Torvalds on git management\n Linus Torvalds on linux git workflow . Summary; use the git tools to make the history of your edits as clean as possible; merge from upstream edits as little as possible in branches where you are doing active development.    Manual pages online You can get these on your own machine with (e.g) git help push or (same thing) git push --help, but, for convenience, here are the online manual pages for some common commands:  git add git branch git checkout git clone git commit git config git diff git log git pull git push git remote git status  \n"}, {"name": "Advanced debugging tools", "path": "dev/development_advanced_debugging", "type": "Development", "text": "Advanced debugging tools If you reached here, you want to dive into, or use, more advanced tooling. This is usually not necessary for first time contributors and most day-to-day development. These are used more rarely, for example close to a new NumPy release, or when a large or particular complex change was made. Since not all of these tools are used on a regular bases and only available on some systems, please expect differences, issues, or quirks; we will be happy to help if you get stuck and appreciate any improvements or suggestions to these workflows.  Finding C errors with additional tooling Most development will not require more than a typical debugging toolchain as shown in Debugging. But for example memory leaks can be particularly subtle or difficult to narrow down. We do not expect any of these tools to be run by most contributors. However, you can ensure that we can track down such issues more easily easier:  Tests should cover all code paths, including error paths. Try to write short and simple tests. If you have a very complicated test consider creating an additional simpler test as well. This can be helpful, because often it is only easy to find which test triggers an issue and not which line of the test. Never use np.empty if data is read/used. valgrind will notice this and report an error. When you do not care about values, you can generate random values instead.  This will help us catch any oversights before your change is released and means you do not have to worry about making reference counting errors, which can be intimidating.  Python debug build for finding memory leaks Debug builds of Python are easily available for example on debian systems, and can be used on all platforms. Running a test or terminal is usually as easy as: python3.8d runtests.py\n# or\npython3.8d runtests.py --ipython\n and were already mentioned in Debugging. A Python debug build will help:  Find bugs which may otherwise cause random behaviour. One example is when an object is still used after it has been deleted. \nPython debug builds allows to check correct reference counting. This works using the additional commands: sys.gettotalrefcount()\nsys.getallocatedblocks()\n    Use together with pytest\n Running the test suite only with a debug python build will not find many errors on its own. An additional advantage of a debug build of Python is that it allows detecting memory leaks. A tool to make this easier is pytest-leaks, which can be installed using pip. Unfortunately, pytest itself may leak memory, but good results can usually (currently) be achieved by removing: @pytest.fixture(autouse=True)\ndef add_np(doctest_namespace):\n    doctest_namespace['np'] = numpy\n\n@pytest.fixture(autouse=True)\ndef env_setup(monkeypatch):\n    monkeypatch.setenv('PYTHONHASHSEED', '0')\n from numpy/conftest.py (This may change with new pytest-leaks versions or pytest updates). This allows to run the test suite, or part of it, conveniently: python3.8d runtests.py -t numpy/core/tests/test_multiarray.py -- -R2:3 -s\n where -R2:3 is the pytest-leaks command (see its documentation), the -s causes output to print and may be necessary (in some versions captured output was detected as a leak). Note that some tests are known (or even designed) to leak references, we try to mark them, but expect some false positives.    valgrind Valgrind is a powerful tool to find certain memory access problems and should be run on complicated C code. Basic use of valgrind usually requires no more than: PYTHONMALLOC=malloc python runtests.py\n where PYTHONMALLOC=malloc is necessary to avoid false positives from python itself. Depending on the system and valgrind version, you may see more false positives. valgrind supports \u201csuppressions\u201d to ignore some of these, and Python does have a suppression file (and even a compile time option) which may help if you find it necessary. Valgrind helps:  Find use of uninitialized variables/memory. Detect memory access violations (reading or writing outside of allocated memory). \nFind many memory leaks. Note that for most leaks the python debug build approach (and pytest-leaks) is much more sensitive. The reason is that valgrind can only detect if memory is definitely lost. If: dtype = np.dtype(np.int64)\narr.astype(dtype=dtype)\n Has incorrect reference counting for dtype, this is a bug, but valgrind cannot see it because np.dtype(np.int64) always returns the same object. However, not all dtypes are singletons, so this might leak memory for different input. In rare cases NumPy uses malloc and not the Python memory allocators which are invisible to the Python debug build. malloc should normally be avoided, but there are some exceptions (e.g. the PyArray_Dims structure is public API and cannot use the Python allocators.)   Even though using valgrind for memory leak detection is slow and less sensitive it can be a convenient: you can run most programs with valgrind without modification. Things to be aware of:  Valgrind does not support the numpy longdouble, this means that tests will fail or be flagged errors that are completely fine. Expect some errors before and after running your NumPy code. Caches can mean that errors (specifically memory leaks) may not be detected or are only detect at a later, unrelated time.  A big advantage of valgrind is that it has no requirements aside from valgrind itself (although you probably want to use debug builds for better tracebacks).  Use together with pytest\n You can run the test suite with valgrind which may be sufficient when you are only interested in a few tests: PYTHOMMALLOC=malloc valgrind python runtests.py \\\n -t numpy/core/tests/test_multiarray.py -- --continue-on-collection-errors\n Note the --continue-on-collection-errors, which is currently necessary due to missing longdouble support causing failures (this will usually not be necessary if you do not run the full test suite). If you wish to detect memory leaks you will also require --show-leak-kinds=definite and possibly more valgrind options. Just as for pytest-leaks certain tests are known to leak cause errors in valgrind and may or may not be marked as such. We have developed pytest-valgrind which:  Reports errors for each test individually Narrows down memory leaks to individual tests (by default valgrind only checks for memory leaks after a program stops, which is very cumbersome).  Please refer to its README for more information (it includes an example command for NumPy).   \n"}, {"name": "Advanced F2PY use cases", "path": "f2py/advanced", "type": "Advanced F2PY use cases", "text": "Advanced F2PY use cases  Adding user-defined functions to F2PY generated modules User-defined Python C/API functions can be defined inside signature files using usercode and pymethoddef statements (they must be used inside the python module block). For example, the following signature file spam.pyf !    -*- f90 -*-\npython module spam\n    usercode '''\n  static char doc_spam_system[] = \"Execute a shell command.\";\n  static PyObject *spam_system(PyObject *self, PyObject *args)\n  {\n    char *command;\n    int sts;\n\n    if (!PyArg_ParseTuple(args, \"s\", &command))\n        return NULL;\n    sts = system(command);\n    return Py_BuildValue(\"i\", sts);\n  }\n    '''\n    pymethoddef '''\n    {\"system\",  spam_system, METH_VARARGS, doc_spam_system},\n    '''\nend python module spam\n wraps the C library function system(): f2py -c spam.pyf\n In Python this can then be used as: >>> import spam\n>>> status = spam.system('whoami')\npearu\n>>> status = spam.system('blah')\nsh: line 1: blah: command not found\n   Adding user-defined variables The following example illustrates how to add user-defined variables to a F2PY generated extension module by modifying the dictionary of a F2PY generated module. Consider the following signature file (compiled with f2py -c var.pyf): !    -*- f90 -*-\npython module var\n  usercode '''\n    int BAR = 5;\n  '''\n  interface\n    usercode '''\n      PyDict_SetItemString(d,\"BAR\",PyInt_FromLong(BAR));\n    '''\n  end interface\nend python module\n Notice that the second usercode statement must be defined inside an interface block and the module dictionary is available through the variable d (see varmodule.c generated by f2py var.pyf for additional details). Usage in Python: >>> import var\n>>> var.BAR\n5\n   Dealing with KIND specifiers Currently, F2PY can handle only <type spec>(kind=<kindselector>) declarations where <kindselector> is a numeric integer (e.g. 1, 2, 4,\u2026), but not a function call KIND(..) or any other expression. F2PY needs to know what would be the corresponding C type and a general solution for that would be too complicated to implement. However, F2PY provides a hook to overcome this difficulty, namely, users can define their own <Fortran type> to <C type> maps. For example, if Fortran 90 code contains: REAL(kind=KIND(0.0D0)) ...\n then create a mapping file containing a Python dictionary: {'real': {'KIND(0.0D0)': 'double'}}\n for instance. Use the --f2cmap command-line option to pass the file name to F2PY. By default, F2PY assumes file name is .f2py_f2cmap in the current working directory. More generally, the f2cmap file must contain a dictionary with items: <Fortran typespec> : {<selector_expr>:<C type>}\n that defines mapping between Fortran type: <Fortran typespec>([kind=]<selector_expr>)\n and the corresponding <C type>. The <C type> can be one of the following: char\nsigned_char\nshort\nint\nlong_long\nfloat\ndouble\nlong_double\ncomplex_float\ncomplex_double\ncomplex_long_double\nstring\n For more information, see the F2Py source code numpy/f2py/capi_maps.py. \n"}, {"name": "Array creation", "path": "user/basics.creation", "type": "User Guide", "text": "Array creation  See also Array creation routines   Introduction There are 6 general mechanisms for creating arrays:  Conversion from other Python structures (i.e. lists and tuples) Intrinsic NumPy array creation functions (e.g. arange, ones, zeros, etc.) Replicating, joining, or mutating existing arrays Reading arrays from disk, either from standard or custom formats Creating arrays from raw bytes through the use of strings or buffers Use of special library functions (e.g., random)  You can use these methods to create ndarrays or Structured arrays. This document will cover general methods for ndarray creation.   1) Converting Python sequences to NumPy Arrays NumPy arrays can be defined using Python sequences such as lists and tuples. Lists and tuples are defined using [...] and (...), respectively. Lists and tuples can define ndarray creation:  a list of numbers will create a 1D array, a list of lists will create a 2D array, further nested lists will create higher-dimensional arrays. In general, any array object is called an ndarray in NumPy.  >>> a1D = np.array([1, 2, 3, 4])\n>>> a2D = np.array([[1, 2], [3, 4]])\n>>> a3D = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n When you use numpy.array to define a new array, you should consider the dtype of the elements in the array, which can be specified explicitly. This feature gives you more control over the underlying data structures and how the elements are handled in C/C++ functions. If you are not careful with dtype assignments, you can get unwanted overflow, as such >>> a = np.array([127, 128, 129], dtype=np.int8)\n>>> a\narray([ 127, -128, -127], dtype=int8)\n An 8-bit signed integer represents integers from -128 to 127. Assigning the int8 array to integers outside of this range results in overflow. This feature can often be misunderstood. If you perform calculations with mismatching dtypes, you can get unwanted results, for example: >>> a = np.array([2, 3, 4], dtype=np.uint32)\n>>> b = np.array([5, 6, 7], dtype=np.uint32)\n>>> c_unsigned32 = a - b\n>>> print('unsigned c:', c_unsigned32, c_unsigned32.dtype)\nunsigned c: [4294967293 4294967293 4294967293] uint32\n>>> c_signed32 = a - b.astype(np.int32)\n>>> print('signed c:', c_signed32, c_signed32.dtype)\nsigned c: [-3 -3 -3] int64\n Notice when you perform operations with two arrays of the same dtype: uint32, the resulting array is the same type. When you perform operations with different dtype, NumPy will assign a new type that satisfies all of the array elements involved in the computation, here uint32 and int32 can both be represented in as int64. The default NumPy behavior is to create arrays in either 64-bit signed integers or double precision floating point numbers, int64 and float, respectively. If you expect your arrays to be a certain type, then you need to specify the dtype while you create the array.   2) Intrinsic NumPy array creation functions NumPy has over 40 built-in functions for creating arrays as laid out in the Array creation routines. These functions can be split into roughly three categories, based on the dimension of the array they create:  1D arrays 2D arrays ndarrays   1 - 1D array creation functions The 1D array creation functions e.g. numpy.linspace and numpy.arange generally need at least two inputs, start and stop. numpy.arange creates arrays with regularly incrementing values. Check the documentation for complete information and examples. A few examples are shown: >>> np.arange(10)\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n>>> np.arange(2, 10, dtype=float)\narray([ 2., 3., 4., 5., 6., 7., 8., 9.])\n>>> np.arange(2, 3, 0.1)\narray([ 2. , 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9])\n Note: best practice for numpy.arange is to use integer start, end, and step values. There are some subtleties regarding dtype. In the second example, the dtype is defined. In the third example, the array is dtype=float to accommodate the step size of 0.1. Due to roundoff error, the stop value is sometimes included. numpy.linspace will create arrays with a specified number of elements, and spaced equally between the specified beginning and end values. For example: >>> np.linspace(1., 4., 6)\narray([ 1. ,  1.6,  2.2,  2.8,  3.4,  4. ])\n The advantage of this creation function is that you guarantee the number of elements and the starting and end point. The previous arange(start, stop, step) will not include the value stop.   2 - 2D array creation functions The 2D array creation functions e.g. numpy.eye, numpy.diag, and numpy.vander define properties of special matrices represented as 2D arrays. np.eye(n, m) defines a 2D identity matrix. The elements where i=j (row index and column index are equal) are 1 and the rest are 0, as such: >>> np.eye(3)\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n>>> np.eye(3, 5)\narray([[1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0.]])\n numpy.diag can define either a square 2D array with given values along the diagonal or if given a 2D array returns a 1D array that is only the diagonal elements. The two array creation functions can be helpful while doing linear algebra, as such: >>> np.diag([1, 2, 3])\narray([[1, 0, 0],\n       [0, 2, 0],\n       [0, 0, 3]])\n>>> np.diag([1, 2, 3], 1)\narray([[0, 1, 0, 0],\n       [0, 0, 2, 0],\n       [0, 0, 0, 3],\n       [0, 0, 0, 0]])\n>>> a = np.array([[1, 2], [3, 4]])\n>>> np.diag(a)\narray([1, 4])\n vander(x, n) defines a Vandermonde matrix as a 2D NumPy array. Each column of the Vandermonde matrix is a decreasing power of the input 1D array or list or tuple, x where the highest polynomial order is n-1. This array creation routine is helpful in generating linear least squares models, as such: >>> np.vander(np.linspace(0, 2, 5), 2)\narray([[0. , 1. ],\n      [0.5, 1. ],\n      [1. , 1. ],\n      [1.5, 1. ],\n      [2. , 1. ]])\n>>> np.vander([1, 2, 3, 4], 2)\narray([[1, 1],\n       [2, 1],\n       [3, 1],\n       [4, 1]])\n>>> np.vander((1, 2, 3, 4), 4)\narray([[ 1,  1,  1,  1],\n       [ 8,  4,  2,  1],\n       [27,  9,  3,  1],\n       [64, 16,  4,  1]])\n   3 - general ndarray creation functions The ndarray creation functions e.g. numpy.ones, numpy.zeros, and random define arrays based upon the desired shape. The ndarray creation functions can create arrays with any dimension by specifying how many dimensions and length along that dimension in a tuple or list. numpy.zeros will create an array filled with 0 values with the specified shape. The default dtype is float64: >>> np.zeros((2, 3))\narray([[0., 0., 0.],\n       [0., 0., 0.]])\n>>> np.zeros((2, 3, 2))\narray([[[0., 0.],\n        [0., 0.],\n        [0., 0.]],\n\n       [[0., 0.],\n        [0., 0.],\n        [0., 0.]]])\n numpy.ones will create an array filled with 1 values. It is identical to zeros in all other respects as such: >>> np.ones((2, 3))\narray([[ 1., 1., 1.],\n       [ 1., 1., 1.]])\n>>> np.ones((2, 3, 2))\narray([[[1., 1.],\n        [1., 1.],\n        [1., 1.]],\n\n       [[1., 1.],\n        [1., 1.],\n        [1., 1.]]])\n The random method of the result of default_rng will create an array filled with random values between 0 and 1. It is included with the numpy.random library. Below, two arrays are created with shapes (2,3) and (2,3,2), respectively. The seed is set to 42 so you can reproduce these pseudorandom numbers: >>> from numpy.random import default_rng\n>>> default_rng(42).random((2,3))\narray([[0.77395605, 0.43887844, 0.85859792],\n       [0.69736803, 0.09417735, 0.97562235]])\n>>> default_rng(42).random((2,3,2))\narray([[[0.77395605, 0.43887844],\n        [0.85859792, 0.69736803],\n        [0.09417735, 0.97562235]],\n       [[0.7611397 , 0.78606431],\n        [0.12811363, 0.45038594],\n        [0.37079802, 0.92676499]]])\n numpy.indices will create a set of arrays (stacked as a one-higher dimensioned array), one per dimension with each representing variation in that dimension: >>> np.indices((3,3))\narray([[[0, 0, 0],\n        [1, 1, 1],\n        [2, 2, 2]],\n       [[0, 1, 2],\n        [0, 1, 2],\n        [0, 1, 2]]])\n This is particularly useful for evaluating functions of multiple dimensions on a regular grid.    3) Replicating, joining, or mutating existing arrays Once you have created arrays, you can replicate, join, or mutate those existing arrays to create new arrays. When you assign an array or its elements to a new variable, you have to explicitly numpy.copy the array, otherwise the variable is a view into the original array. Consider the following example: >>> a = np.array([1, 2, 3, 4, 5, 6])\n>>> b = a[:2]\n>>> b += 1\n>>> print('a =', a, '; b =', b)\na = [2 3 3 4 5 6] ; b = [2 3]\n In this example, you did not create a new array. You created a variable, b that viewed the first 2 elements of a. When you added 1 to b you would get the same result by adding 1 to a[:2]. If you want to create a new array, use the numpy.copy array creation routine as such: >>> a = np.array([1, 2, 3, 4])\n>>> b = a[:2].copy()\n>>> b += 1\n>>> print('a = ', a, 'b = ', b)\na =  [1 2 3 4] b =  [2 3]\n For more information and examples look at Copies and Views. There are a number of routines to join existing arrays e.g. numpy.vstack, numpy.hstack, and numpy.block. Here is an example of joining four 2-by-2 arrays into a 4-by-4 array using block: >>> A = np.ones((2, 2))\n>>> B = np.eye(2, 2)\n>>> C = np.zeros((2, 2))\n>>> D = np.diag((-3, -4))\n>>> np.block([[A, B], [C, D]])\narray([[ 1.,  1.,  1.,  0. ],\n       [ 1.,  1.,  0.,  1. ],\n       [ 0.,  0., -3.,  0. ],\n       [ 0.,  0.,  0., -4. ]])\n Other routines use similar syntax to join ndarrays. Check the routine\u2019s documentation for further examples and syntax.   4) Reading arrays from disk, either from standard or custom formats This is the most common case of large array creation. The details depend greatly on the format of data on disk. This section gives general pointers on how to handle various formats. For more detailed examples of IO look at How to Read and Write files.  Standard Binary Formats Various fields have standard formats for array data. The following lists the ones with known Python libraries to read them and return NumPy arrays (there may be others for which it is possible to read and convert to NumPy arrays so check the last section as well) HDF5: h5py\nFITS: Astropy\n Examples of formats that cannot be read directly but for which it is not hard to convert are those formats supported by libraries like PIL (able to read and write many image formats such as jpg, png, etc).   Common ASCII Formats Delimited files such as comma separated value (csv) and tab separated value (tsv) files are used for programs like Excel and LabView. Python functions can read and parse these files line-by-line. NumPy has two standard routines for importing a file with delimited data numpy.loadtxt and numpy.genfromtxt. These functions have more involved use cases in Reading and writing files. A simple example given a simple.csv: $ cat simple.csv\nx, y\n0, 0\n1, 1\n2, 4\n3, 9\n Importing simple.csv is accomplished using loadtxt: >>> np.loadtxt('simple.csv', delimiter = ',', skiprows = 1) \narray([[0., 0.],\n       [1., 1.],\n       [2., 4.],\n       [3., 9.]])\n More generic ASCII files can be read using scipy.io and Pandas.    5) Creating arrays from raw bytes through the use of strings or buffers There are a variety of approaches one can use. If the file has a relatively simple format then one can write a simple I/O library and use the NumPy fromfile() function and .tofile() method to read and write NumPy arrays directly (mind your byteorder though!) If a good C or C++ library exists that read the data, one can wrap that library with a variety of techniques though that certainly is much more work and requires significantly more advanced knowledge to interface with C or C++.   6) Use of special library functions (e.g., SciPy, Pandas, and OpenCV) NumPy is the fundamental library for array containers in the Python Scientific Computing stack. Many Python libraries, including SciPy, Pandas, and OpenCV, use NumPy ndarrays as the common format for data exchange, These libraries can create, operate on, and work with NumPy arrays. \n"}, {"name": "Array creation routines", "path": "reference/routines.array-creation", "type": "Array creation routines", "text": "Array creation routines  See also Array creation   From shape or value  \nempty(shape[, dtype, order, like]) Return a new array of given shape and type, without initializing entries.  \nempty_like(prototype[, dtype, order, subok, ...]) Return a new array with the same shape and type as a given array.  \neye(N[, M, k, dtype, order, like]) Return a 2-D array with ones on the diagonal and zeros elsewhere.  \nidentity(n[, dtype, like]) Return the identity array.  \nones(shape[, dtype, order, like]) Return a new array of given shape and type, filled with ones.  \nones_like(a[, dtype, order, subok, shape]) Return an array of ones with the same shape and type as a given array.  \nzeros(shape[, dtype, order, like]) Return a new array of given shape and type, filled with zeros.  \nzeros_like(a[, dtype, order, subok, shape]) Return an array of zeros with the same shape and type as a given array.  \nfull(shape, fill_value[, dtype, order, like]) Return a new array of given shape and type, filled with fill_value.  \nfull_like(a, fill_value[, dtype, order, ...]) Return a full array with the same shape and type as a given array.     From existing data  \narray(object[, dtype, copy, order, subok, ...]) Create an array.  \nasarray(a[, dtype, order, like]) Convert the input to an array.  \nasanyarray(a[, dtype, order, like]) Convert the input to an ndarray, but pass ndarray subclasses through.  \nascontiguousarray(a[, dtype, like]) Return a contiguous array (ndim >= 1) in memory (C order).  \nasmatrix(data[, dtype]) Interpret the input as a matrix.  \ncopy(a[, order, subok]) Return an array copy of the given object.  \nfrombuffer(buffer[, dtype, count, offset, like]) Interpret a buffer as a 1-dimensional array.  \nfromfile(file[, dtype, count, sep, offset, like]) Construct an array from data in a text or binary file.  \nfromfunction(function, shape, *[, dtype, like]) Construct an array by executing a function over each coordinate.  \nfromiter(iter, dtype[, count, like]) Create a new 1-dimensional array from an iterable object.  \nfromstring(string[, dtype, count, like]) A new 1-D array initialized from text data in a string.  \nloadtxt(fname[, dtype, comments, delimiter, ...]) Load data from a text file.     Creating record arrays (numpy.rec)  Note numpy.rec is the preferred alias for numpy.core.records.   \ncore.records.array(obj[, dtype, shape, ...]) Construct a record array from a wide-variety of objects.  \ncore.records.fromarrays(arrayList[, dtype, ...]) Create a record array from a (flat) list of arrays  \ncore.records.fromrecords(recList[, dtype, ...]) Create a recarray from a list of records in text form.  \ncore.records.fromstring(datastring[, dtype, ...]) Create a record array from binary data  \ncore.records.fromfile(fd[, dtype, shape, ...]) Create an array from binary file data     Creating character arrays (numpy.char)  Note numpy.char is the preferred alias for numpy.core.defchararray.   \ncore.defchararray.array(obj[, itemsize, ...]) Create a chararray.  \ncore.defchararray.asarray(obj[, itemsize, ...]) Convert the input to a chararray, copying the data only if necessary.     Numerical ranges  \narange([start,] stop[, step,][, dtype, like]) Return evenly spaced values within a given interval.  \nlinspace(start, stop[, num, endpoint, ...]) Return evenly spaced numbers over a specified interval.  \nlogspace(start, stop[, num, endpoint, base, ...]) Return numbers spaced evenly on a log scale.  \ngeomspace(start, stop[, num, endpoint, ...]) Return numbers spaced evenly on a log scale (a geometric progression).  \nmeshgrid(*xi[, copy, sparse, indexing]) Return coordinate matrices from coordinate vectors.  \nmgrid nd_grid instance which returns a dense multi-dimensional \"meshgrid\".  \nogrid nd_grid instance which returns an open multi-dimensional \"meshgrid\".     Building matrices  \ndiag(v[, k]) Extract a diagonal or construct a diagonal array.  \ndiagflat(v[, k]) Create a two-dimensional array with the flattened input as a diagonal.  \ntri(N[, M, k, dtype, like]) An array with ones at and below the given diagonal and zeros elsewhere.  \ntril(m[, k]) Lower triangle of an array.  \ntriu(m[, k]) Upper triangle of an array.  \nvander(x[, N, increasing]) Generate a Vandermonde matrix.     The Matrix class  \nmat(data[, dtype]) Interpret the input as a matrix.  \nbmat(obj[, ldict, gdict]) Build a matrix object from a string, nested sequence, or array.   \n"}, {"name": "Array manipulation routines", "path": "reference/routines.array-manipulation", "type": "Array manipulation routines", "text": "Array manipulation routines  Basic operations  \ncopyto(dst, src[, casting, where]) Copies values from one array to another, broadcasting as necessary.  \nshape(a) Return the shape of an array.     Changing array shape  \nreshape(a, newshape[, order]) Gives a new shape to an array without changing its data.  \nravel(a[, order]) Return a contiguous flattened array.  \nndarray.flat A 1-D iterator over the array.  \nndarray.flatten([order]) Return a copy of the array collapsed into one dimension.     Transpose-like operations  \nmoveaxis(a, source, destination) Move axes of an array to new positions.  \nrollaxis(a, axis[, start]) Roll the specified axis backwards, until it lies in a given position.  \nswapaxes(a, axis1, axis2) Interchange two axes of an array.  \nndarray.T The transposed array.  \ntranspose(a[, axes]) Reverse or permute the axes of an array; returns the modified array.     Changing number of dimensions  \natleast_1d(*arys) Convert inputs to arrays with at least one dimension.  \natleast_2d(*arys) View inputs as arrays with at least two dimensions.  \natleast_3d(*arys) View inputs as arrays with at least three dimensions.  \nbroadcast Produce an object that mimics broadcasting.  \nbroadcast_to(array, shape[, subok]) Broadcast an array to a new shape.  \nbroadcast_arrays(*args[, subok]) Broadcast any number of arrays against each other.  \nexpand_dims(a, axis) Expand the shape of an array.  \nsqueeze(a[, axis]) Remove axes of length one from a.     Changing kind of array  \nasarray(a[, dtype, order, like]) Convert the input to an array.  \nasanyarray(a[, dtype, order, like]) Convert the input to an ndarray, but pass ndarray subclasses through.  \nasmatrix(data[, dtype]) Interpret the input as a matrix.  \nasfarray(a[, dtype]) Return an array converted to a float type.  \nasfortranarray(a[, dtype, like]) Return an array (ndim >= 1) laid out in Fortran order in memory.  \nascontiguousarray(a[, dtype, like]) Return a contiguous array (ndim >= 1) in memory (C order).  \nasarray_chkfinite(a[, dtype, order]) Convert the input to an array, checking for NaNs or Infs.  \nasscalar(a) Convert an array of size 1 to its scalar equivalent.  \nrequire(a[, dtype, requirements, like]) Return an ndarray of the provided type that satisfies requirements.     Joining arrays  \nconcatenate([axis, out, dtype, casting]) Join a sequence of arrays along an existing axis.  \nstack(arrays[, axis, out]) Join a sequence of arrays along a new axis.  \nblock(arrays) Assemble an nd-array from nested lists of blocks.  \nvstack(tup) Stack arrays in sequence vertically (row wise).  \nhstack(tup) Stack arrays in sequence horizontally (column wise).  \ndstack(tup) Stack arrays in sequence depth wise (along third axis).  \ncolumn_stack(tup) Stack 1-D arrays as columns into a 2-D array.  \nrow_stack(tup) Stack arrays in sequence vertically (row wise).     Splitting arrays  \nsplit(ary, indices_or_sections[, axis]) Split an array into multiple sub-arrays as views into ary.  \narray_split(ary, indices_or_sections[, axis]) Split an array into multiple sub-arrays.  \ndsplit(ary, indices_or_sections) Split array into multiple sub-arrays along the 3rd axis (depth).  \nhsplit(ary, indices_or_sections) Split an array into multiple sub-arrays horizontally (column-wise).  \nvsplit(ary, indices_or_sections) Split an array into multiple sub-arrays vertically (row-wise).     Tiling arrays  \ntile(A, reps) Construct an array by repeating A the number of times given by reps.  \nrepeat(a, repeats[, axis]) Repeat elements of an array.     Adding and removing elements  \ndelete(arr, obj[, axis]) Return a new array with sub-arrays along an axis deleted.  \ninsert(arr, obj, values[, axis]) Insert values along the given axis before the given indices.  \nappend(arr, values[, axis]) Append values to the end of an array.  \nresize(a, new_shape) Return a new array with the specified shape.  \ntrim_zeros(filt[, trim]) Trim the leading and/or trailing zeros from a 1-D array or sequence.  \nunique(ar[, return_index, return_inverse, ...]) Find the unique elements of an array.     Rearranging elements  \nflip(m[, axis]) Reverse the order of elements in an array along the given axis.  \nfliplr(m) Reverse the order of elements along axis 1 (left/right).  \nflipud(m) Reverse the order of elements along axis 0 (up/down).  \nreshape(a, newshape[, order]) Gives a new shape to an array without changing its data.  \nroll(a, shift[, axis]) Roll array elements along a given axis.  \nrot90(m[, k, axes]) Rotate an array by 90 degrees in the plane specified by axes.   \n"}, {"name": "Array objects", "path": "reference/arrays", "type": "Array objects", "text": "Array objects NumPy provides an N-dimensional array type, the ndarray, which describes a collection of \u201citems\u201d of the same type. The items can be indexed using for example N integers. All ndarrays are homogeneous: every item takes up the same size block of memory, and all blocks are interpreted in exactly the same way. How each item in the array is to be interpreted is specified by a separate data-type object, one of which is associated with every array. In addition to basic types (integers, floats, etc.), the data type objects can also represent data structures. An item extracted from an array, e.g., by indexing, is represented by a Python object whose type is one of the array scalar types built in NumPy. The array scalars allow easy manipulation of also more complicated arrangements of data.    Figure Conceptual diagram showing the relationship between the three fundamental objects used to describe the data in an array: 1) the ndarray itself, 2) the data-type object that describes the layout of a single fixed-size element of the array, 3) the array-scalar Python object that is returned when a single element of the array is accessed.    \nThe N-dimensional array (ndarray) Constructing arrays Indexing arrays Internal memory layout of an ndarray Array attributes Array methods Arithmetic, matrix multiplication, and comparison operations Special methods   \nScalars Built-in scalar types Attributes Indexing Methods Defining new types   \nData type objects (dtype) Specifying and constructing data types dtype   \nIndexing routines Generating index arrays Indexing-like operations Inserting data into arrays Iterating over arrays   \nIterating Over Arrays Single Array Iteration Broadcasting Array Iteration Putting the Inner Loop in Cython   \nStandard array subclasses Special attributes and methods Matrix objects Memory-mapped file arrays Character arrays (numpy.char) Record arrays (numpy.rec) Masked arrays (numpy.ma) Standard container class Array Iterators   \nMasked arrays The numpy.ma module Using numpy.ma Examples Constants of the numpy.ma module The MaskedArray class MaskedArray methods Masked array operations   \nThe Array Interface Python side C-struct access Type description examples Differences with Array interface (Version 2)   \nDatetimes and Timedeltas Basic Datetimes Datetime and Timedelta Arithmetic Datetime Units Business Day Functionality   \n"}, {"name": "Binary operations", "path": "reference/routines.bitwise", "type": "Binary operations", "text": "Binary operations  Elementwise bit operations  \nbitwise_and(x1, x2, /[, out, where, ...]) Compute the bit-wise AND of two arrays element-wise.  \nbitwise_or(x1, x2, /[, out, where, casting, ...]) Compute the bit-wise OR of two arrays element-wise.  \nbitwise_xor(x1, x2, /[, out, where, ...]) Compute the bit-wise XOR of two arrays element-wise.  \ninvert(x, /[, out, where, casting, order, ...]) Compute bit-wise inversion, or bit-wise NOT, element-wise.  \nleft_shift(x1, x2, /[, out, where, casting, ...]) Shift the bits of an integer to the left.  \nright_shift(x1, x2, /[, out, where, ...]) Shift the bits of an integer to the right.     Bit packing  \npackbits(a, /[, axis, bitorder]) Packs the elements of a binary-valued array into bits in a uint8 array.  \nunpackbits(a, /[, axis, count, bitorder]) Unpacks elements of a uint8 array into a binary-valued output array.     Output formatting  \nbinary_repr(num[, width]) Return the binary representation of the input number as a string.   \n"}, {"name": "Bit Generators", "path": "reference/random/bit_generators/index", "type": "Bit Generators", "text": "Bit Generators The random values produced by Generator originate in a BitGenerator. The BitGenerators do not directly provide random numbers and only contains methods used for seeding, getting or setting the state, jumping or advancing the state, and for accessing low-level wrappers for consumption by code that can efficiently access the functions provided, e.g., numba.  Supported BitGenerators The included BitGenerators are:  PCG-64 - The default. A fast generator that can be advanced by an arbitrary amount. See the documentation for advance. PCG-64 has a period of \\(2^{128}\\). See the PCG author\u2019s page for more details about this class of PRNG. PCG-64 DXSM - An upgraded version of PCG-64 with better statistical properties in parallel contexts. See Upgrading PCG64 with PCG64DXSM for more information on these improvements. MT19937 - The standard Python BitGenerator. Adds a MT19937.jumped function that returns a new generator with state as-if \\(2^{128}\\) draws have been made. Philox - A counter-based generator capable of being advanced an arbitrary number of steps or generating independent streams. See the Random123 page for more details about this class of bit generators. SFC64 - A fast generator based on random invertible mappings. Usually the fastest generator of the four. See the SFC author\u2019s page for (a little) more detail.   \nBitGenerator([seed]) Base Class for generic BitGenerators, which provide a stream of random bits based on different algorithms.    MT19937 PCG64 PCG64DXSM Philox SFC64  \n"}, {"name": "broadcast.index", "path": "reference/generated/numpy.broadcast.index", "type": "Standard array subclasses", "text": "numpy.broadcast.index attribute   broadcast.index\n \ncurrent index in broadcasted result Examples >>> x = np.array([[1], [2], [3]])\n>>> y = np.array([4, 5, 6])\n>>> b = np.broadcast(x, y)\n>>> b.index\n0\n>>> next(b), next(b), next(b)\n((1, 4), (1, 5), (1, 6))\n>>> b.index\n3\n \n\n"}, {"name": "broadcast.iters", "path": "reference/generated/numpy.broadcast.iters", "type": "Standard array subclasses", "text": "numpy.broadcast.iters attribute   broadcast.iters\n \ntuple of iterators along self\u2019s \u201ccomponents.\u201d Returns a tuple of numpy.flatiter objects, one for each \u201ccomponent\u201d of self.  See also  numpy.flatiter\n  Examples >>> x = np.array([1, 2, 3])\n>>> y = np.array([[4], [5], [6]])\n>>> b = np.broadcast(x, y)\n>>> row, col = b.iters\n>>> next(row), next(col)\n(1, 4)\n \n\n"}, {"name": "broadcast.nd", "path": "reference/generated/numpy.broadcast.nd", "type": "Standard array subclasses", "text": "numpy.broadcast.nd attribute   broadcast.nd\n \nNumber of dimensions of broadcasted result. For code intended for NumPy 1.12.0 and later the more consistent ndim is preferred. Examples >>> x = np.array([1, 2, 3])\n>>> y = np.array([[4], [5], [6]])\n>>> b = np.broadcast(x, y)\n>>> b.nd\n2\n \n\n"}, {"name": "broadcast.ndim", "path": "reference/generated/numpy.broadcast.ndim", "type": "Standard array subclasses", "text": "numpy.broadcast.ndim attribute   broadcast.ndim\n \nNumber of dimensions of broadcasted result. Alias for nd.  New in version 1.12.0.  Examples >>> x = np.array([1, 2, 3])\n>>> y = np.array([[4], [5], [6]])\n>>> b = np.broadcast(x, y)\n>>> b.ndim\n2\n \n\n"}, {"name": "broadcast.numiter", "path": "reference/generated/numpy.broadcast.numiter", "type": "Standard array subclasses", "text": "numpy.broadcast.numiter attribute   broadcast.numiter\n \nNumber of iterators possessed by the broadcasted result. Examples >>> x = np.array([1, 2, 3])\n>>> y = np.array([[4], [5], [6]])\n>>> b = np.broadcast(x, y)\n>>> b.numiter\n2\n \n\n"}, {"name": "broadcast.reset()", "path": "reference/generated/numpy.broadcast.reset", "type": "numpy.broadcast.reset", "text": "numpy.broadcast.reset method   broadcast.reset()\n \nReset the broadcasted result\u2019s iterator(s).  Parameters \n None\n  Returns \n None\n   Examples >>> x = np.array([1, 2, 3])\n>>> y = np.array([[4], [5], [6]])\n>>> b = np.broadcast(x, y)\n>>> b.index\n0\n>>> next(b), next(b), next(b)\n((1, 4), (2, 4), (3, 4))\n>>> b.index\n3\n>>> b.reset()\n>>> b.index\n0\n \n\n"}, {"name": "broadcast.size", "path": "reference/generated/numpy.broadcast.size", "type": "Standard array subclasses", "text": "numpy.broadcast.size attribute   broadcast.size\n \nTotal size of broadcasted result. Examples >>> x = np.array([1, 2, 3])\n>>> y = np.array([[4], [5], [6]])\n>>> b = np.broadcast(x, y)\n>>> b.size\n9\n \n\n"}, {"name": "build_src", "path": "f2py/buildtools/distutils", "type": "Using via \n        \n         numpy.distutils", "text": "Using via numpy.distutils numpy.distutils is part of NumPy, and extends the standard Python distutils module to deal with Fortran sources and F2PY signature files, e.g. compile Fortran sources, call F2PY to construct extension modules, etc.  Example Consider the following setup_file.py for the fib and scalar examples from Three ways to wrap - getting started section: from numpy.distutils.core import Extension\n\next1 = Extension(name = 'scalar',\n                 sources = ['scalar.f'])\next2 = Extension(name = 'fib2',\n                 sources = ['fib2.pyf', 'fib1.f'])\n\nif __name__ == \"__main__\":\n    from numpy.distutils.core import setup\n    setup(name = 'f2py_example',\n          description       = \"F2PY Users Guide examples\",\n          author            = \"Pearu Peterson\",\n          author_email      = \"pearu@cens.ioc.ee\",\n          ext_modules = [ext1, ext2]\n          )\n# End of setup_example.py\n Running python setup_example.py build\n will build two extension modules scalar and fib2 to the build directory.   Extensions to distutils\n numpy.distutils extends distutils with the following features:  \nExtension class argument sources may contain Fortran source files. In addition, the list sources may contain at most one F2PY signature file, and in this case, the name of an Extension module must match with the <modulename> used in signature file. It is assumed that an F2PY signature file contains exactly one python\nmodule block. If sources do not contain a signature file, then F2PY is used to scan Fortran source files to construct wrappers to the Fortran codes. Additional options to the F2PY executable can be given using the Extension class argument f2py_options.  \nThe following new distutils commands are defined:  build_src\n\nto construct Fortran wrapper extension modules, among many other things.  config_fc\n\nto change Fortran compiler options.   Additionally, the build_ext and build_clib commands are also enhanced to support Fortran sources. Run python <setup.py file> config_fc build_src build_ext --help\n to see available options for these commands.  \nWhen building Python packages containing Fortran sources, one can choose different Fortran compilers by using the build_ext command option --fcompiler=<Vendor>. Here <Vendor> can be one of the following names (on linux systems): absoft compaq fujitsu g95 gnu gnu95 intel intele intelem lahey nag nagfor nv pathf95 pg vast\n See numpy_distutils/fcompiler.py for an up-to-date list of supported compilers for different platforms, or run python -m numpy.f2py -c --help-fcompiler\n   \n"}, {"name": "Building from source", "path": "user/building", "type": "User Guide", "text": "Building from source There are two options for building NumPy- building with Gitpod or locally from source. Your choice depends on your operating system and familiarity with the command line.  Gitpod Gitpod is an open-source platform that automatically creates the correct development environment right in your browser, reducing the need to install local development environments and deal with incompatible dependencies. If you are a Windows user, unfamiliar with using the command line or building NumPy for the first time, it is often faster to build with Gitpod. Here are the in-depth instructions for building NumPy with building NumPy with Gitpod.   Building locally Building locally on your machine gives you more granular control. If you are a MacOS or Linux user familiar with using the command line, you can continue with building NumPy locally by following the instructions below.   Prerequisites Building NumPy requires the following software installed:  \nPython 3.6.x or newer Please note that the Python development headers also need to be installed, e.g., on Debian/Ubuntu one needs to install both python3 and python3-dev. On Windows and macOS this is normally not an issue.  \nCompilers Much of NumPy is written in C. You will need a C compiler that complies with the C99 standard. While a FORTRAN 77 compiler is not necessary for building NumPy, it is needed to run the numpy.f2py tests. These tests are skipped if the compiler is not auto-detected. Note that NumPy is developed mainly using GNU compilers and tested on MSVC and Clang compilers. Compilers from other vendors such as Intel, Absoft, Sun, NAG, Compaq, Vast, Portland, Lahey, HP, IBM are only supported in the form of community feedback, and may not work out of the box. GCC 4.x (and later) compilers are recommended. On ARM64 (aarch64) GCC 8.x (and later) are recommended.  \nLinear Algebra libraries NumPy does not require any external linear algebra libraries to be installed. However, if these are available, NumPy\u2019s setup script can detect them and use them for building. A number of different LAPACK library setups can be used, including optimized LAPACK libraries such as OpenBLAS or MKL. The choice and location of these libraries as well as include paths and other such build options can be specified in a site.cfg file located in the NumPy root repository or a .numpy-site.cfg file in your home directory. See the site.cfg.example example file included in the NumPy repository or sdist for documentation, and below for specifying search priority from environmental variables.  \nCython For building NumPy, you\u2019ll need a recent version of Cython.     Basic Installation To install NumPy, run: pip install .\n To perform an in-place build that can be run from the source folder run: python setup.py build_ext --inplace\n Note: for build instructions to do development work on NumPy itself, see Setting up and using your development environment.   Testing Make sure to test your builds. To ensure everything stays in shape, see if all tests pass: $ python runtests.py -v -m full\n For detailed info on testing, see Testing builds.  Parallel builds It\u2019s possible to do a parallel build with: python setup.py build -j 4 install --prefix $HOME/.local\n This will compile numpy on 4 CPUs and install it into the specified prefix. to perform a parallel in-place build, run: python setup.py build_ext --inplace -j 4\n The number of build jobs can also be specified via the environment variable NPY_NUM_BUILD_JOBS.   Choosing the fortran compiler Compilers are auto-detected; building with a particular compiler can be done with --fcompiler. E.g. to select gfortran: python setup.py build --fcompiler=gnu95\n For more information see: python setup.py build --help-fcompiler\n   How to check the ABI of BLAS/LAPACK libraries One relatively simple and reliable way to check for the compiler used to build a library is to use ldd on the library. If libg2c.so is a dependency, this means that g77 has been used (note: g77 is no longer supported for building NumPy). If libgfortran.so is a dependency, gfortran has been used. If both are dependencies, this means both have been used, which is almost always a very bad idea.    Accelerated BLAS/LAPACK libraries NumPy searches for optimized linear algebra libraries such as BLAS and LAPACK. There are specific orders for searching these libraries, as described below and in the site.cfg.example file.  BLAS Note that both BLAS and CBLAS interfaces are needed for a properly optimized build of NumPy. The default order for the libraries are:  MKL BLIS OpenBLAS ATLAS BLAS (NetLIB)  The detection of BLAS libraries may be bypassed by defining the environment variable NPY_BLAS_LIBS , which should contain the exact linker flags you want to use (interface is assumed to be Fortran 77). Also define NPY_CBLAS_LIBS (even empty if CBLAS is contained in your BLAS library) to trigger use of CBLAS and avoid slow fallback code for matrix calculations. If you wish to build against OpenBLAS but you also have BLIS available one may predefine the order of searching via the environment variable NPY_BLAS_ORDER which is a comma-separated list of the above names which is used to determine what to search for, for instance: NPY_BLAS_ORDER=ATLAS,blis,openblas,MKL python setup.py build\n will prefer to use ATLAS, then BLIS, then OpenBLAS and as a last resort MKL. If neither of these exists the build will fail (names are compared lower case). Alternatively one may use ! or ^ to negate all items: NPY_BLAS_ORDER='^blas,atlas' python setup.py build\n will allow using anything but NetLIB BLAS and ATLAS libraries, the order of the above list is retained. One cannot mix negation and positives, nor have multiple negations, such cases will raise an error.   LAPACK The default order for the libraries are:  MKL OpenBLAS libFLAME ATLAS LAPACK (NetLIB)  The detection of LAPACK libraries may be bypassed by defining the environment variable NPY_LAPACK_LIBS, which should contain the exact linker flags you want to use (language is assumed to be Fortran 77). If you wish to build against OpenBLAS but you also have MKL available one may predefine the order of searching via the environment variable NPY_LAPACK_ORDER which is a comma-separated list of the above names, for instance: NPY_LAPACK_ORDER=ATLAS,openblas,MKL python setup.py build\n will prefer to use ATLAS, then OpenBLAS and as a last resort MKL. If neither of these exists the build will fail (names are compared lower case). Alternatively one may use ! or ^ to negate all items: NPY_LAPACK_ORDER='^lapack' python setup.py build\n will allow using anything but the NetLIB LAPACK library, the order of the above list is retained. One cannot mix negation and positives, nor have multiple negations, such cases will raise an error.  Deprecated since version 1.20: The native libraries on macOS, provided by Accelerate, are not fit for use in NumPy since they have bugs that cause wrong output under easily reproducible conditions. If the vendor fixes those bugs, the library could be reinstated, but until then users compiling for themselves should use another linear algebra library or use the built-in (but slower) default, see the next section.    Disabling ATLAS and other accelerated libraries Usage of ATLAS and other accelerated libraries in NumPy can be disabled via: NPY_BLAS_ORDER= NPY_LAPACK_ORDER= python setup.py build\n or: BLAS=None LAPACK=None ATLAS=None python setup.py build\n   64-bit BLAS and LAPACK You can tell Numpy to use 64-bit BLAS/LAPACK libraries by setting the environment variable: NPY_USE_BLAS_ILP64=1\n when building Numpy. The following 64-bit BLAS/LAPACK libraries are supported:  OpenBLAS ILP64 with 64_ symbol suffix (openblas64_) OpenBLAS ILP64 without symbol suffix (openblas_ilp64)  The order in which they are preferred is determined by NPY_BLAS_ILP64_ORDER and NPY_LAPACK_ILP64_ORDER environment variables. The default value is openblas64_,openblas_ilp64.  Note Using non-symbol-suffixed 64-bit BLAS/LAPACK in a program that also uses 32-bit BLAS/LAPACK can cause crashes under certain conditions (e.g. with embedded Python interpreters on Linux). The 64-bit OpenBLAS with 64_ symbol suffix is obtained by compiling OpenBLAS with settings: make INTERFACE64=1 SYMBOLSUFFIX=64_\n The symbol suffix avoids the symbol name clashes between 32-bit and 64-bit BLAS/LAPACK libraries.     Supplying additional compiler flags Additional compiler flags can be supplied by setting the OPT, FOPT (for Fortran), and CC environment variables. When providing options that should improve the performance of the code ensure that you also set -DNDEBUG so that debugging code is not executed. \n"}, {"name": "Building the NumPy API and reference docs", "path": "dev/howto_build_docs", "type": "Development", "text": "Building the NumPy API and reference docs If you only want to get the documentation, note that pre-built versions can be found at https://numpy.org/doc/ in several different formats.  Development environments Before proceeding further it should be noted that the documentation is built with the make tool, which is not natively available on Windows. MacOS or Linux users can jump to Prerequisites. It is recommended for Windows users to set up their development environment on Gitpod or Windows Subsystem for Linux (WSL). WSL is a good option for a persistent local set-up.  Gitpod Gitpod is an open-source platform that automatically creates the correct development environment right in your browser, reducing the need to install local development environments and deal with incompatible dependencies. If you have good internet connectivity and want a temporary set-up, it is often faster to build with Gitpod. Here are the in-depth instructions for building NumPy with Gitpod.    Prerequisites Building the NumPy documentation and API reference requires the following:  NumPy Since large parts of the main documentation are obtained from NumPy via import numpy and examining the docstrings, you will need to first build and install it so that the correct version is imported. NumPy has to be re-built and re-installed every time you fetch the latest version of the repository, before generating the documentation. This ensures that the NumPy version and the git repository version are in sync. Note that you can e.g. install NumPy to a temporary location and set the PYTHONPATH environment variable appropriately. Alternatively, if using Python virtual environments (via e.g. conda, virtualenv or the venv module), installing NumPy into a new virtual environment is recommended.   Dependencies All of the necessary dependencies for building the NumPy docs except for Doxygen can be installed with: pip install -r doc_requirements.txt\n We currently use Sphinx along with Doxygen for generating the API and reference documentation for NumPy. In addition, building the documentation requires the Sphinx extension plot_directive, which is shipped with Matplotlib. We also use numpydoc to render docstrings in the generated API documentation. SciPy is installed since some parts of the documentation require SciPy functions. For installing Doxygen, please check the official download and installation pages, or if you are using Linux then you can install it through your distribution package manager.  Note Try to install a newer version of Doxygen > 1.8.10 otherwise you may get some warnings during the build.    Submodules If you obtained NumPy via git, also get the git submodules that contain additional parts required for building the documentation: git submodule update --init\n    Instructions Now you are ready to generate the docs, so write: cd doc\nmake html\n If all goes well, this will generate a build/html subdirectory in the /doc directory, containing the built documentation. If you get a message about installed numpy != current repo git version, you must either override the check by setting GITVER or re-install NumPy. If you have built NumPy into a virtual environment and get an error that says numpy not found, cannot build documentation without..., you need to override the makefile PYTHON variable at the command line, so instead of writing make\u00a0 html write: make PYTHON=python html\n To build the PDF documentation, do instead: make latex\nmake -C build/latex all-pdf\n You will need to have LaTeX installed for this, inclusive of support for Greek letters. For example, on Ubuntu xenial texlive-lang-greek and cm-super are needed. Also, latexmk is needed on non-Windows systems. Instead of the above, you can also do: make dist\n which will rebuild NumPy, install it to a temporary location, and build the documentation in all formats. This will most likely again only work on Unix platforms. The documentation for NumPy distributed at https://numpy.org/doc in html and pdf format is also built with make dist. See HOWTO RELEASE for details on how to update https://numpy.org/doc. \n"}, {"name": "busdaycalendar.holidays", "path": "reference/generated/numpy.busdaycalendar.holidays", "type": "Datetime support functions", "text": "numpy.busdaycalendar.holidays attribute   busdaycalendar.holidays\n \nA copy of the holiday array indicating additional invalid days. \n\n"}, {"name": "busdaycalendar.weekmask", "path": "reference/generated/numpy.busdaycalendar.weekmask", "type": "Datetime support functions", "text": "numpy.busdaycalendar.weekmask attribute   busdaycalendar.weekmask\n \nA copy of the seven-element boolean mask indicating valid days. \n\n"}, {"name": "Byte-swapping", "path": "user/basics.byteswapping", "type": "User Guide", "text": "Byte-swapping  Introduction to byte ordering and ndarrays The ndarray is an object that provide a python array interface to data in memory. It often happens that the memory that you want to view with an array is not of the same byte ordering as the computer on which you are running Python. For example, I might be working on a computer with a little-endian CPU - such as an Intel Pentium, but I have loaded some data from a file written by a computer that is big-endian. Let\u2019s say I have loaded 4 bytes from a file written by a Sun (big-endian) computer. I know that these 4 bytes represent two 16-bit integers. On a big-endian machine, a two-byte integer is stored with the Most Significant Byte (MSB) first, and then the Least Significant Byte (LSB). Thus the bytes are, in memory order:  MSB integer 1 LSB integer 1 MSB integer 2 LSB integer 2  Let\u2019s say the two integers were in fact 1 and 770. Because 770 = 256 * 3 + 2, the 4 bytes in memory would contain respectively: 0, 1, 3, 2. The bytes I have loaded from the file would have these contents: >>> big_end_buffer = bytearray([0,1,3,2])\n>>> big_end_buffer\nbytearray(b'\\\\x00\\\\x01\\\\x03\\\\x02')\n We might want to use an ndarray to access these integers. In that case, we can create an array around this memory, and tell numpy that there are two integers, and that they are 16 bit and big-endian: >>> import numpy as np\n>>> big_end_arr = np.ndarray(shape=(2,),dtype='>i2', buffer=big_end_buffer)\n>>> big_end_arr[0]\n1\n>>> big_end_arr[1]\n770\n Note the array dtype above of >i2. The > means \u2018big-endian\u2019 (< is little-endian) and i2 means \u2018signed 2-byte integer\u2019. For example, if our data represented a single unsigned 4-byte little-endian integer, the dtype string would be <u4. In fact, why don\u2019t we try that? >>> little_end_u4 = np.ndarray(shape=(1,),dtype='<u4', buffer=big_end_buffer)\n>>> little_end_u4[0] == 1 * 256**1 + 3 * 256**2 + 2 * 256**3\nTrue\n Returning to our big_end_arr - in this case our underlying data is big-endian (data endianness) and we\u2019ve set the dtype to match (the dtype is also big-endian). However, sometimes you need to flip these around.  Warning Scalars currently do not include byte order information, so extracting a scalar from an array will return an integer in native byte order. Hence: >>> big_end_arr[0].dtype.byteorder == little_end_u4[0].dtype.byteorder\nTrue\n    Changing byte ordering As you can imagine from the introduction, there are two ways you can affect the relationship between the byte ordering of the array and the underlying memory it is looking at:  Change the byte-ordering information in the array dtype so that it interprets the underlying data as being in a different byte order. This is the role of arr.newbyteorder()\n Change the byte-ordering of the underlying data, leaving the dtype interpretation as it was. This is what arr.byteswap() does.  The common situations in which you need to change byte ordering are:  Your data and dtype endianness don\u2019t match, and you want to change the dtype so that it matches the data. Your data and dtype endianness don\u2019t match, and you want to swap the data so that they match the dtype Your data and dtype endianness match, but you want the data swapped and the dtype to reflect this   Data and dtype endianness don\u2019t match, change dtype to match data We make something where they don\u2019t match: >>> wrong_end_dtype_arr = np.ndarray(shape=(2,),dtype='<i2', buffer=big_end_buffer)\n>>> wrong_end_dtype_arr[0]\n256\n The obvious fix for this situation is to change the dtype so it gives the correct endianness: >>> fixed_end_dtype_arr = wrong_end_dtype_arr.newbyteorder()\n>>> fixed_end_dtype_arr[0]\n1\n Note the array has not changed in memory: >>> fixed_end_dtype_arr.tobytes() == big_end_buffer\nTrue\n   Data and type endianness don\u2019t match, change data to match dtype You might want to do this if you need the data in memory to be a certain ordering. For example you might be writing the memory out to a file that needs a certain byte ordering. >>> fixed_end_mem_arr = wrong_end_dtype_arr.byteswap()\n>>> fixed_end_mem_arr[0]\n1\n Now the array has changed in memory: >>> fixed_end_mem_arr.tobytes() == big_end_buffer\nFalse\n   Data and dtype endianness match, swap data and dtype You may have a correctly specified array dtype, but you need the array to have the opposite byte order in memory, and you want the dtype to match so the array values make sense. In this case you just do both of the previous operations: >>> swapped_end_arr = big_end_arr.byteswap().newbyteorder()\n>>> swapped_end_arr[0]\n1\n>>> swapped_end_arr.tobytes() == big_end_buffer\nFalse\n An easier way of casting the data to a specific dtype and byte ordering can be achieved with the ndarray astype method: >>> swapped_end_arr = big_end_arr.astype('<i2')\n>>> swapped_end_arr[0]\n1\n>>> swapped_end_arr.tobytes() == big_end_buffer\nFalse\n  \n"}, {"name": "C API Deprecations", "path": "reference/c-api/deprecations", "type": "C API Deprecations", "text": "C API Deprecations  Background The API exposed by NumPy for third-party extensions has grown over years of releases, and has allowed programmers to directly access NumPy functionality from C. This API can be best described as \u201corganic\u201d. It has emerged from multiple competing desires and from multiple points of view over the years, strongly influenced by the desire to make it easy for users to move to NumPy from Numeric and Numarray. The core API originated with Numeric in 1995 and there are patterns such as the heavy use of macros written to mimic Python\u2019s C-API as well as account for compiler technology of the late 90\u2019s. There is also only a small group of volunteers who have had very little time to spend on improving this API. There is an ongoing effort to improve the API. It is important in this effort to ensure that code that compiles for NumPy 1.X continues to compile for NumPy 1.X. At the same time, certain API\u2019s will be marked as deprecated so that future-looking code can avoid these API\u2019s and follow better practices. Another important role played by deprecation markings in the C API is to move towards hiding internal details of the NumPy implementation. For those needing direct, easy, access to the data of ndarrays, this will not remove this ability. Rather, there are many potential performance optimizations which require changing the implementation details, and NumPy developers have been unable to try them because of the high value of preserving ABI compatibility. By deprecating this direct access, we will in the future be able to improve NumPy\u2019s performance in ways we cannot presently.   Deprecation Mechanism NPY_NO_DEPRECATED_API In C, there is no equivalent to the deprecation warnings that Python supports. One way to do deprecations is to flag them in the documentation and release notes, then remove or change the deprecated features in a future major version (NumPy 2.0 and beyond). Minor versions of NumPy should not have major C-API changes, however, that prevent code that worked on a previous minor release. For example, we will do our best to ensure that code that compiled and worked on NumPy 1.4 should continue to work on NumPy 1.7 (but perhaps with compiler warnings). To use the NPY_NO_DEPRECATED_API mechanism, you need to #define it to the target API version of NumPy before #including any NumPy headers. If you want to confirm that your code is clean against 1.7, use: #define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\n On compilers which support a #warning mechanism, NumPy issues a compiler warning if you do not define the symbol NPY_NO_DEPRECATED_API. This way, the fact that there are deprecations will be flagged for third-party developers who may not have read the release notes closely. \n"}, {"name": "char **NpyIter_GetDataPtrArray()", "path": "reference/c-api/iterator#c.NpyIter_GetDataPtrArray", "type": "Array Iterator API", "text": "  char**NpyIter_GetDataPtrArray(NpyIter*iter)\n \nThis gives back a pointer to the nop data pointers. If NPY_ITER_EXTERNAL_LOOP was not specified, each data pointer points to the current data item of the iterator. If no inner iteration was specified, it points to the first data item of the inner loop. This pointer may be cached before the iteration loop, calling iternext will not change it. This function may be safely called without holding the Python GIL. \n"}, {"name": "char **NpyIter_GetInitialDataPtrArray()", "path": "reference/c-api/iterator#c.NpyIter_GetInitialDataPtrArray", "type": "Array Iterator API", "text": "  char**NpyIter_GetInitialDataPtrArray(NpyIter*iter)\n \nGets the array of data pointers directly into the arrays (never into the buffers), corresponding to iteration index 0. These pointers are different from the pointers accepted by NpyIter_ResetBasePointers, because the direction along some axes may have been reversed. This function may be safely called without holding the Python GIL. \n"}, {"name": "char *core_signature", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.core_signature", "type": "Python Types and C-Structures", "text": "  char*core_signature\n \nCore signature string \n"}, {"name": "char *data", "path": "reference/c-api/types-and-structures#c.NPY_AO.data", "type": "Python Types and C-Structures", "text": "  char*data\n \nAccessible via PyArray_DATA, this data member is a pointer to the first element of the array. This pointer can (and normally should) be recast to the data type of the array. \n"}, {"name": "char *dataptr", "path": "reference/c-api/types-and-structures#c.PyArrayIterObject.dataptr", "type": "Python Types and C-Structures", "text": "  char*dataptr\n \nThis member points to an element in the ndarray indicated by the index. \n"}, {"name": "char *doc", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.doc", "type": "Python Types and C-Structures", "text": "  char*doc\n \nDocumentation for the ufunc. Should not contain the function signature as this is generated dynamically when __doc__ is retrieved. \n"}, {"name": "char *name", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.name", "type": "Python Types and C-Structures", "text": "  char*name\n \nA string name for the ufunc. This is used dynamically to build the __doc__ attribute of ufuncs. \n"}, {"name": "char *PyArray_BYTES()", "path": "reference/c-api/array#c.PyArray_BYTES", "type": "Array API", "text": "  char*PyArray_BYTES(PyArrayObject*arr)\n \nThese two macros are similar and obtain the pointer to the data-buffer for the array. The first macro can (and should be) assigned to a particular pointer where the second is for generic processing. If you have not guaranteed a contiguous and/or aligned array then be sure you understand how to access the data in the array to avoid memory and/or alignment problems. \n"}, {"name": "char *PyArray_One()", "path": "reference/c-api/array#c.PyArray_One", "type": "Array API", "text": "  char*PyArray_One(PyArrayObject*arr)\n \nA pointer to newly created memory of size arr ->itemsize that holds the representation of 1 for that type. The returned pointer, ret, must be freed using PyDataMem_FREE (ret) when it is not needed anymore. \n"}, {"name": "char *PyArray_Zero()", "path": "reference/c-api/array#c.PyArray_Zero", "type": "Array API", "text": "  char*PyArray_Zero(PyArrayObject*arr)\n \nA pointer to newly created memory of size arr ->itemsize that holds the representation of 0 for that type. The returned pointer, ret, must be freed using PyDataMem_FREE (ret) when it is not needed anymore. \n"}, {"name": "char *PyDataMem_RENEW()", "path": "reference/c-api/array#c.PyDataMem_RENEW", "type": "Array API", "text": "  char*PyDataMem_RENEW(void*ptr, size_tnewbytes)\n \nMacros to allocate, free, and reallocate memory. These macros are used internally to create arrays. \n"}, {"name": "char *types", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.types", "type": "Python Types and C-Structures", "text": "  char*types\n \nAn array of \\(nargs \\times ntypes\\) 8-bit type_numbers which contains the type signature for the function for each of the supported (builtin) data types. For each of the ntypes functions, the corresponding set of type numbers in this array shows how the args argument should be interpreted in the 1-d vector loop. These type numbers do not have to be the same type and mixed-type ufuncs are supported. \n"}, {"name": "char byteorder", "path": "reference/c-api/types-and-structures#c.PyArray_Descr.byteorder", "type": "Python Types and C-Structures", "text": "  charbyteorder\n \nA character indicating the byte-order: \u2018>\u2019 (big-endian), \u2018<\u2019 (little- endian), \u2018=\u2019 (native), \u2018|\u2019 (irrelevant, ignore). All builtin data- types have byteorder \u2018=\u2019. \n"}, {"name": "char flags", "path": "reference/c-api/types-and-structures#c.PyArray_Descr.flags", "type": "Python Types and C-Structures", "text": "  charflags\n \nA data-type bit-flag that determines if the data-type exhibits object- array like behavior. Each bit in this member is a flag which are named as: \n"}, {"name": "char kind", "path": "reference/c-api/types-and-structures#c.PyArray_Descr.kind", "type": "Python Types and C-Structures", "text": "  charkind\n \nA character code indicating the kind of array (using the array interface typestring notation). A \u2018b\u2019 represents Boolean, a \u2018i\u2019 represents signed integer, a \u2018u\u2019 represents unsigned integer, \u2018f\u2019 represents floating point, \u2018c\u2019 represents complex floating point, \u2018S\u2019 represents 8-bit zero-terminated bytes, \u2018U\u2019 represents 32-bit/character unicode string, and \u2018V\u2019 represents arbitrary. \n"}, {"name": "char type", "path": "reference/c-api/types-and-structures#c.PyArray_Descr.type", "type": "Python Types and C-Structures", "text": "  chartype\n \nA traditional character code indicating the data type. \n"}, {"name": "char typekind", "path": "reference/c-api/types-and-structures#c.PyArrayInterface.typekind", "type": "Python Types and C-Structures", "text": "  chartypekind\n \nA character indicating what kind of array is present according to the typestring convention with \u2018t\u2019 -> bitfield, \u2018b\u2019 -> Boolean, \u2018i\u2019 -> signed integer, \u2018u\u2019 -> unsigned integer, \u2018f\u2019 -> floating point, \u2018c\u2019 -> complex floating point, \u2018O\u2019 -> object, \u2018S\u2019 -> (byte-)string, \u2018U\u2019 -> unicode, \u2018V\u2019 -> void. \n"}, {"name": "char.add()", "path": "reference/generated/numpy.char.add", "type": "numpy.char.add", "text": "numpy.char.add   char.add(x1, x2)[source]\n \nReturn element-wise string concatenation for two arrays of str or unicode. Arrays x1 and x2 must have the same shape.  Parameters \n \nx1array_like of str or unicode\n\n\nInput array.  \nx2array_like of str or unicode\n\n\nInput array.    Returns \n \naddndarray\n\n\nOutput array of string_ or unicode_, depending on input types of the same shape as x1 and x2.     \n\n"}, {"name": "char.array()", "path": "reference/generated/numpy.char.array", "type": "numpy.char.array", "text": "numpy.char.array   char.array(obj, itemsize=None, copy=True, unicode=None, order=None)[source]\n \nCreate a chararray.  Note This class is provided for numarray backward-compatibility. New code (not concerned with numarray compatibility) should use arrays of type string_ or unicode_ and use the free functions in numpy.char for fast vectorized string operations instead.  Versus a regular NumPy array of type str or unicode, this class adds the following functionality:  values automatically have whitespace removed from the end when indexed comparison operators automatically remove whitespace from the end when comparing values vectorized string operations are provided as methods (e.g. str.endswith) and infix operators (e.g. +, *, %)   Parameters \n \nobjarray of str or unicode-like\n\n\nitemsizeint, optional\n\n\nitemsize is the number of characters per scalar in the resulting array. If itemsize is None, and obj is an object array or a Python list, the itemsize will be automatically determined. If itemsize is provided and obj is of type str or unicode, then the obj string will be chunked into itemsize pieces.  \ncopybool, optional\n\n\nIf true (default), then the object is copied. Otherwise, a copy will only be made if __array__ returns a copy, if obj is a nested sequence, or if a copy is needed to satisfy any of the other requirements (itemsize, unicode, order, etc.).  \nunicodebool, optional\n\n\nWhen true, the resulting chararray can contain Unicode characters, when false only 8-bit characters. If unicode is None and obj is one of the following:  a chararray, an ndarray of type str or unicode\n a Python str or unicode object,  then the unicode setting of the output array will be automatically determined.  \norder{\u2018C\u2019, \u2018F\u2019, \u2018A\u2019}, optional\n\n\nSpecify the order of the array. If order is \u2018C\u2019 (default), then the array will be in C-contiguous order (last-index varies the fastest). If order is \u2018F\u2019, then the returned array will be in Fortran-contiguous order (first-index varies the fastest). If order is \u2018A\u2019, then the returned array may be in any order (either C-, Fortran-contiguous, or even discontiguous).     \n\n"}, {"name": "char.asarray()", "path": "reference/generated/numpy.char.asarray", "type": "numpy.char.asarray", "text": "numpy.char.asarray   char.asarray(obj, itemsize=None, unicode=None, order=None)[source]\n \nConvert the input to a chararray, copying the data only if necessary. Versus a regular NumPy array of type str or unicode, this class adds the following functionality:  values automatically have whitespace removed from the end when indexed comparison operators automatically remove whitespace from the end when comparing values vectorized string operations are provided as methods (e.g. str.endswith) and infix operators (e.g. +, *,``%``)   Parameters \n \nobjarray of str or unicode-like\n\n\nitemsizeint, optional\n\n\nitemsize is the number of characters per scalar in the resulting array. If itemsize is None, and obj is an object array or a Python list, the itemsize will be automatically determined. If itemsize is provided and obj is of type str or unicode, then the obj string will be chunked into itemsize pieces.  \nunicodebool, optional\n\n\nWhen true, the resulting chararray can contain Unicode characters, when false only 8-bit characters. If unicode is None and obj is one of the following:  a chararray, an ndarray of type str or \u2018unicode` a Python str or unicode object,  then the unicode setting of the output array will be automatically determined.  \norder{\u2018C\u2019, \u2018F\u2019}, optional\n\n\nSpecify the order of the array. If order is \u2018C\u2019 (default), then the array will be in C-contiguous order (last-index varies the fastest). If order is \u2018F\u2019, then the returned array will be in Fortran-contiguous order (first-index varies the fastest).     \n\n"}, {"name": "char.capitalize()", "path": "reference/generated/numpy.char.capitalize", "type": "numpy.char.capitalize", "text": "numpy.char.capitalize   char.capitalize(a)[source]\n \nReturn a copy of a with only the first character of each element capitalized. Calls str.capitalize element-wise. For 8-bit strings, this method is locale-dependent.  Parameters \n \naarray_like of str or unicode\n\n\nInput array of strings to capitalize.    Returns \n \noutndarray\n\n\nOutput array of str or unicode, depending on input types      See also  str.capitalize\n  Examples >>> c = np.array(['a1b2','1b2a','b2a1','2a1b'],'S4'); c\narray(['a1b2', '1b2a', 'b2a1', '2a1b'],\n    dtype='|S4')\n>>> np.char.capitalize(c)\narray(['A1b2', '1b2a', 'B2a1', '2a1b'],\n    dtype='|S4')\n \n\n"}, {"name": "char.center()", "path": "reference/generated/numpy.char.center", "type": "numpy.char.center", "text": "numpy.char.center   char.center(a, width, fillchar=' ')[source]\n \nReturn a copy of a with its elements centered in a string of length width. Calls str.center element-wise.  Parameters \n \naarray_like of str or unicode\n\n\nwidthint\n\n\nThe length of the resulting strings  \nfillcharstr or unicode, optional\n\n\nThe padding character to use (default is space).    Returns \n \noutndarray\n\n\nOutput array of str or unicode, depending on input types      See also  str.center\n  \n\n"}, {"name": "char.chararray.argsort()", "path": "reference/generated/numpy.char.chararray.argsort", "type": "numpy.char.chararray.argsort", "text": "numpy.char.chararray.argsort method   char.chararray.argsort(axis=- 1, kind=None, order=None)[source]\n \nReturns the indices that would sort this array. Refer to numpy.argsort for full documentation.  See also  numpy.argsort\n\nequivalent function    \n\n"}, {"name": "char.chararray.astype()", "path": "reference/generated/numpy.char.chararray.astype", "type": "numpy.char.chararray.astype", "text": "numpy.char.chararray.astype method   char.chararray.astype(dtype, order='K', casting='unsafe', subok=True, copy=True)\n \nCopy of the array, cast to a specified type.  Parameters \n \ndtypestr or dtype\n\n\nTypecode or data-type to which the array is cast.  \norder{\u2018C\u2019, \u2018F\u2019, \u2018A\u2019, \u2018K\u2019}, optional\n\n\nControls the memory layout order of the result. \u2018C\u2019 means C order, \u2018F\u2019 means Fortran order, \u2018A\u2019 means \u2018F\u2019 order if all the arrays are Fortran contiguous, \u2018C\u2019 order otherwise, and \u2018K\u2019 means as close to the order the array elements appear in memory as possible. Default is \u2018K\u2019.  \ncasting{\u2018no\u2019, \u2018equiv\u2019, \u2018safe\u2019, \u2018same_kind\u2019, \u2018unsafe\u2019}, optional\n\n\nControls what kind of data casting may occur. Defaults to \u2018unsafe\u2019 for backwards compatibility.  \u2018no\u2019 means the data types should not be cast at all. \u2018equiv\u2019 means only byte-order changes are allowed. \u2018safe\u2019 means only casts which can preserve values are allowed. \u2018same_kind\u2019 means only safe casts or casts within a kind, like float64 to float32, are allowed. \u2018unsafe\u2019 means any data conversions may be done.   \nsubokbool, optional\n\n\nIf True, then sub-classes will be passed-through (default), otherwise the returned array will be forced to be a base-class array.  \ncopybool, optional\n\n\nBy default, astype always returns a newly allocated array. If this is set to false, and the dtype, order, and subok requirements are satisfied, the input array is returned instead of a copy.    Returns \n \narr_tndarray\n\n\nUnless copy is False and the other conditions for returning the input array are satisfied (see description for copy input parameter), arr_t is a new array of the same shape as the input array, with dtype, order given by dtype, order.    Raises \n ComplexWarning\n\nWhen casting from complex to float or int. To avoid this, one should use a.real.astype(t).     Notes  Changed in version 1.17.0: Casting between a simple data type and a structured one is possible only for \u201cunsafe\u201d casting. Casting to multiple fields is allowed, but casting from multiple fields is not.   Changed in version 1.9.0: Casting from numeric to string types in \u2018safe\u2019 casting mode requires that the string dtype length is long enough to store the max integer/float value converted.  Examples >>> x = np.array([1, 2, 2.5])\n>>> x\narray([1. ,  2. ,  2.5])\n >>> x.astype(int)\narray([1, 2, 2])\n \n\n"}, {"name": "char.chararray.base", "path": "reference/generated/numpy.char.chararray.base", "type": "String operations", "text": "numpy.char.chararray.base attribute   char.chararray.base\n \nBase object if memory is from some other object. Examples The base of an array that owns its memory is None: >>> x = np.array([1,2,3,4])\n>>> x.base is None\nTrue\n Slicing creates a view, whose memory is shared with x: >>> y = x[2:]\n>>> y.base is x\nTrue\n \n\n"}, {"name": "char.chararray.copy()", "path": "reference/generated/numpy.char.chararray.copy", "type": "numpy.char.chararray.copy", "text": "numpy.char.chararray.copy method   char.chararray.copy(order='C')\n \nReturn a copy of the array.  Parameters \n \norder{\u2018C\u2019, \u2018F\u2019, \u2018A\u2019, \u2018K\u2019}, optional\n\n\nControls the memory layout of the copy. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. \u2018K\u2019 means match the layout of a as closely as possible. (Note that this function and numpy.copy are very similar but have different default values for their order= arguments, and this function always passes sub-classes through.)      See also  numpy.copy\n\nSimilar function with different default behavior  numpy.copyto\n  Notes This function is the preferred method for creating an array copy. The function numpy.copy is similar, but it defaults to using order \u2018K\u2019, and will not pass sub-classes through by default. Examples >>> x = np.array([[1,2,3],[4,5,6]], order='F')\n >>> y = x.copy()\n >>> x.fill(0)\n >>> x\narray([[0, 0, 0],\n       [0, 0, 0]])\n >>> y\narray([[1, 2, 3],\n       [4, 5, 6]])\n >>> y.flags['C_CONTIGUOUS']\nTrue\n \n\n"}, {"name": "char.chararray.count()", "path": "reference/generated/numpy.char.chararray.count", "type": "numpy.char.chararray.count", "text": "numpy.char.chararray.count method   char.chararray.count(sub, start=0, end=None)[source]\n \nReturns an array with the number of non-overlapping occurrences of substring sub in the range [start, end].  See also  char.count\n  \n\n"}, {"name": "char.chararray.ctypes", "path": "reference/generated/numpy.char.chararray.ctypes", "type": "String operations", "text": "numpy.char.chararray.ctypes attribute   char.chararray.ctypes\n \nAn object to simplify the interaction of the array with the ctypes module. This attribute creates an object that makes it easier to use arrays when calling shared libraries with the ctypes module. The returned object has, among others, data, shape, and strides attributes (see Notes below) which themselves return ctypes objects that can be used as arguments to a shared library.  Parameters \n None\n  Returns \n \ncPython object\n\n\nPossessing attributes data, shape, strides, etc.      See also  numpy.ctypeslib\n  Notes Below are the public attributes of this object which were documented in \u201cGuide to NumPy\u201d (we have omitted undocumented public attributes, as well as documented private attributes):   _ctypes.data\n \nA pointer to the memory area of the array as a Python integer. This memory area may contain data that is not aligned, or not in correct byte-order. The memory area may not even be writeable. The array flags and data-type of this array should be respected when passing this attribute to arbitrary C-code to avoid trouble that can include Python crashing. User Beware! The value of this attribute is exactly the same as self._array_interface_['data'][0]. Note that unlike data_as, a reference will not be kept to the array: code like ctypes.c_void_p((a + b).ctypes.data) will result in a pointer to a deallocated array, and should be spelt (a + b).ctypes.data_as(ctypes.c_void_p) \n   _ctypes.shape\n \n(c_intp*self.ndim): A ctypes array of length self.ndim where the basetype is the C-integer corresponding to dtype('p') on this platform (see c_intp). This base-type could be ctypes.c_int, ctypes.c_long, or ctypes.c_longlong depending on the platform. The ctypes array contains the shape of the underlying array. \n   _ctypes.strides\n \n(c_intp*self.ndim): A ctypes array of length self.ndim where the basetype is the same as for the shape attribute. This ctypes array contains the strides information from the underlying array. This strides information is important for showing how many bytes must be jumped to get to the next element in the array. \n   _ctypes.data_as(obj)[source]\n \nReturn the data pointer cast to a particular c-types object. For example, calling self._as_parameter_ is equivalent to self.data_as(ctypes.c_void_p). Perhaps you want to use the data as a pointer to a ctypes array of floating-point data: self.data_as(ctypes.POINTER(ctypes.c_double)). The returned pointer will keep a reference to the array. \n   _ctypes.shape_as(obj)[source]\n \nReturn the shape tuple as an array of some other c-types type. For example: self.shape_as(ctypes.c_short). \n   _ctypes.strides_as(obj)[source]\n \nReturn the strides tuple as an array of some other c-types type. For example: self.strides_as(ctypes.c_longlong). \n If the ctypes module is not available, then the ctypes attribute of array objects still returns something useful, but ctypes objects are not returned and errors may be raised instead. In particular, the object will still have the as_parameter attribute which will return an integer equal to the data attribute. Examples >>> import ctypes\n>>> x = np.array([[0, 1], [2, 3]], dtype=np.int32)\n>>> x\narray([[0, 1],\n       [2, 3]], dtype=int32)\n>>> x.ctypes.data\n31962608 # may vary\n>>> x.ctypes.data_as(ctypes.POINTER(ctypes.c_uint32))\n<__main__.LP_c_uint object at 0x7ff2fc1fc200> # may vary\n>>> x.ctypes.data_as(ctypes.POINTER(ctypes.c_uint32)).contents\nc_uint(0)\n>>> x.ctypes.data_as(ctypes.POINTER(ctypes.c_uint64)).contents\nc_ulong(4294967296)\n>>> x.ctypes.shape\n<numpy.core._internal.c_long_Array_2 object at 0x7ff2fc1fce60> # may vary\n>>> x.ctypes.strides\n<numpy.core._internal.c_long_Array_2 object at 0x7ff2fc1ff320> # may vary\n \n\n"}, {"name": "char.chararray.data", "path": "reference/generated/numpy.char.chararray.data", "type": "String operations", "text": "numpy.char.chararray.data attribute   char.chararray.data\n \nPython buffer object pointing to the start of the array\u2019s data. \n\n"}, {"name": "char.chararray.decode()", "path": "reference/generated/numpy.char.chararray.decode", "type": "numpy.char.chararray.decode", "text": "numpy.char.chararray.decode method   char.chararray.decode(encoding=None, errors=None)[source]\n \nCalls str.decode element-wise.  See also  char.decode\n  \n\n"}, {"name": "char.chararray.dtype", "path": "reference/generated/numpy.char.chararray.dtype", "type": "String operations", "text": "numpy.char.chararray.dtype attribute   char.chararray.dtype\n \nData-type of the array\u2019s elements.  Parameters \n None\n  Returns \n \ndnumpy dtype object\n\n    See also  numpy.dtype\n  Examples >>> x\narray([[0, 1],\n       [2, 3]])\n>>> x.dtype\ndtype('int32')\n>>> type(x.dtype)\n<type 'numpy.dtype'>\n \n\n"}, {"name": "char.chararray.dump()", "path": "reference/generated/numpy.char.chararray.dump", "type": "numpy.char.chararray.dump", "text": "numpy.char.chararray.dump method   char.chararray.dump(file)\n \nDump a pickle of the array to the specified file. The array can be read back with pickle.load or numpy.load.  Parameters \n \nfilestr or Path\n\n\nA string naming the dump file.  Changed in version 1.17.0: pathlib.Path objects are now accepted.      \n\n"}, {"name": "char.chararray.dumps()", "path": "reference/generated/numpy.char.chararray.dumps", "type": "numpy.char.chararray.dumps", "text": "numpy.char.chararray.dumps method   char.chararray.dumps()\n \nReturns the pickle of the array as a string. pickle.loads will convert the string back to an array.  Parameters \n None\n   \n\n"}, {"name": "char.chararray.encode()", "path": "reference/generated/numpy.char.chararray.encode", "type": "numpy.char.chararray.encode", "text": "numpy.char.chararray.encode method   char.chararray.encode(encoding=None, errors=None)[source]\n \nCalls str.encode element-wise.  See also  char.encode\n  \n\n"}, {"name": "char.chararray.endswith()", "path": "reference/generated/numpy.char.chararray.endswith", "type": "numpy.char.chararray.endswith", "text": "numpy.char.chararray.endswith method   char.chararray.endswith(suffix, start=0, end=None)[source]\n \nReturns a boolean array which is True where the string element in self ends with suffix, otherwise False.  See also  char.endswith\n  \n\n"}, {"name": "char.chararray.expandtabs()", "path": "reference/generated/numpy.char.chararray.expandtabs", "type": "numpy.char.chararray.expandtabs", "text": "numpy.char.chararray.expandtabs method   char.chararray.expandtabs(tabsize=8)[source]\n \nReturn a copy of each string element where all tab characters are replaced by one or more spaces.  See also  char.expandtabs\n  \n\n"}, {"name": "char.chararray.fill()", "path": "reference/generated/numpy.char.chararray.fill", "type": "numpy.char.chararray.fill", "text": "numpy.char.chararray.fill method   char.chararray.fill(value)\n \nFill the array with a scalar value.  Parameters \n \nvaluescalar\n\n\nAll elements of a will be assigned this value.     Examples >>> a = np.array([1, 2])\n>>> a.fill(0)\n>>> a\narray([0, 0])\n>>> a = np.empty(2)\n>>> a.fill(1)\n>>> a\narray([1.,  1.])\n \n\n"}, {"name": "char.chararray.find()", "path": "reference/generated/numpy.char.chararray.find", "type": "numpy.char.chararray.find", "text": "numpy.char.chararray.find method   char.chararray.find(sub, start=0, end=None)[source]\n \nFor each element, return the lowest index in the string where substring sub is found.  See also  char.find\n  \n\n"}, {"name": "char.chararray.flags", "path": "reference/generated/numpy.char.chararray.flags", "type": "String operations", "text": "numpy.char.chararray.flags attribute   char.chararray.flags\n \nInformation about the memory layout of the array. Notes The flags object can be accessed dictionary-like (as in a.flags['WRITEABLE']), or by using lowercased attribute names (as in a.flags.writeable). Short flag names are only supported in dictionary access. Only the WRITEBACKIFCOPY, UPDATEIFCOPY, WRITEABLE, and ALIGNED flags can be changed by the user, via direct assignment to the attribute or dictionary entry, or by calling ndarray.setflags. The array flags cannot be set arbitrarily:  UPDATEIFCOPY can only be set False. WRITEBACKIFCOPY can only be set False. ALIGNED can only be set True if the data is truly aligned. WRITEABLE can only be set True if the array owns its own memory or the ultimate owner of the memory exposes a writeable buffer interface or is a string.  Arrays can be both C-style and Fortran-style contiguous simultaneously. This is clear for 1-dimensional arrays, but can also be true for higher dimensional arrays. Even for contiguous arrays a stride for a given dimension arr.strides[dim] may be arbitrary if arr.shape[dim] == 1 or the array has no elements. It does not generally hold that self.strides[-1] == self.itemsize for C-style contiguous arrays or self.strides[0] == self.itemsize for Fortran-style contiguous arrays is true.  Attributes \n C_CONTIGUOUS (C)\n\nThe data is in a single, C-style contiguous segment.  F_CONTIGUOUS (F)\n\nThe data is in a single, Fortran-style contiguous segment.  OWNDATA (O)\n\nThe array owns the memory it uses or borrows it from another object.  WRITEABLE (W)\n\nThe data area can be written to. Setting this to False locks the data, making it read-only. A view (slice, etc.) inherits WRITEABLE from its base array at creation time, but a view of a writeable array may be subsequently locked while the base array remains writeable. (The opposite is not true, in that a view of a locked array may not be made writeable. However, currently, locking a base object does not lock any views that already reference it, so under that circumstance it is possible to alter the contents of a locked array via a previously created writeable view onto it.) Attempting to change a non-writeable array raises a RuntimeError exception.  ALIGNED (A)\n\nThe data and all elements are aligned appropriately for the hardware.  WRITEBACKIFCOPY (X)\n\nThis array is a copy of some other array. The C-API function PyArray_ResolveWritebackIfCopy must be called before deallocating to the base array will be updated with the contents of this array.  UPDATEIFCOPY (U)\n\n(Deprecated, use WRITEBACKIFCOPY) This array is a copy of some other array. When this array is deallocated, the base array will be updated with the contents of this array.  FNC\n\nF_CONTIGUOUS and not C_CONTIGUOUS.  FORC\n\nF_CONTIGUOUS or C_CONTIGUOUS (one-segment test).  BEHAVED (B)\n\nALIGNED and WRITEABLE.  CARRAY (CA)\n\nBEHAVED and C_CONTIGUOUS.  FARRAY (FA)\n\nBEHAVED and F_CONTIGUOUS and not C_CONTIGUOUS.     \n\n"}, {"name": "char.chararray.flat", "path": "reference/generated/numpy.char.chararray.flat", "type": "String operations", "text": "numpy.char.chararray.flat attribute   char.chararray.flat\n \nA 1-D iterator over the array. This is a numpy.flatiter instance, which acts similarly to, but is not a subclass of, Python\u2019s built-in iterator object.  See also  flatten\n\nReturn a copy of the array collapsed into one dimension.  flatiter\n  Examples >>> x = np.arange(1, 7).reshape(2, 3)\n>>> x\narray([[1, 2, 3],\n       [4, 5, 6]])\n>>> x.flat[3]\n4\n>>> x.T\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n>>> x.T.flat[3]\n5\n>>> type(x.flat)\n<class 'numpy.flatiter'>\n An assignment example: >>> x.flat = 3; x\narray([[3, 3, 3],\n       [3, 3, 3]])\n>>> x.flat[[1,4]] = 1; x\narray([[3, 1, 3],\n       [3, 1, 3]])\n \n\n"}, {"name": "char.chararray.flatten()", "path": "reference/generated/numpy.char.chararray.flatten", "type": "numpy.char.chararray.flatten", "text": "numpy.char.chararray.flatten method   char.chararray.flatten(order='C')\n \nReturn a copy of the array collapsed into one dimension.  Parameters \n \norder{\u2018C\u2019, \u2018F\u2019, \u2018A\u2019, \u2018K\u2019}, optional\n\n\n\u2018C\u2019 means to flatten in row-major (C-style) order. \u2018F\u2019 means to flatten in column-major (Fortran- style) order. \u2018A\u2019 means to flatten in column-major order if a is Fortran contiguous in memory, row-major order otherwise. \u2018K\u2019 means to flatten a in the order the elements occur in memory. The default is \u2018C\u2019.    Returns \n \nyndarray\n\n\nA copy of the input array, flattened to one dimension.      See also  ravel\n\nReturn a flattened array.  flat\n\nA 1-D flat iterator over the array.    Examples >>> a = np.array([[1,2], [3,4]])\n>>> a.flatten()\narray([1, 2, 3, 4])\n>>> a.flatten('F')\narray([1, 3, 2, 4])\n \n\n"}, {"name": "char.chararray.getfield()", "path": "reference/generated/numpy.char.chararray.getfield", "type": "numpy.char.chararray.getfield", "text": "numpy.char.chararray.getfield method   char.chararray.getfield(dtype, offset=0)\n \nReturns a field of the given array as a certain type. A field is a view of the array data with a given data-type. The values in the view are determined by the given type and the offset into the current array in bytes. The offset needs to be such that the view dtype fits in the array dtype; for example an array of dtype complex128 has 16-byte elements. If taking a view with a 32-bit integer (4 bytes), the offset needs to be between 0 and 12 bytes.  Parameters \n \ndtypestr or dtype\n\n\nThe data type of the view. The dtype size of the view can not be larger than that of the array itself.  \noffsetint\n\n\nNumber of bytes to skip before beginning the element view.     Examples >>> x = np.diag([1.+1.j]*2)\n>>> x[1, 1] = 2 + 4.j\n>>> x\narray([[1.+1.j,  0.+0.j],\n       [0.+0.j,  2.+4.j]])\n>>> x.getfield(np.float64)\narray([[1.,  0.],\n       [0.,  2.]])\n By choosing an offset of 8 bytes we can select the complex part of the array for our view: >>> x.getfield(np.float64, offset=8)\narray([[1.,  0.],\n       [0.,  4.]])\n \n\n"}, {"name": "char.chararray.imag", "path": "reference/generated/numpy.char.chararray.imag", "type": "String operations", "text": "numpy.char.chararray.imag attribute   char.chararray.imag\n \nThe imaginary part of the array. Examples >>> x = np.sqrt([1+0j, 0+1j])\n>>> x.imag\narray([ 0.        ,  0.70710678])\n>>> x.imag.dtype\ndtype('float64')\n \n\n"}, {"name": "char.chararray.index()", "path": "reference/generated/numpy.char.chararray.index", "type": "numpy.char.chararray.index", "text": "numpy.char.chararray.index method   char.chararray.index(sub, start=0, end=None)[source]\n \nLike find, but raises ValueError when the substring is not found.  See also  char.index\n  \n\n"}, {"name": "char.chararray.isalnum()", "path": "reference/generated/numpy.char.chararray.isalnum", "type": "numpy.char.chararray.isalnum", "text": "numpy.char.chararray.isalnum method   char.chararray.isalnum()[source]\n \nReturns true for each element if all characters in the string are alphanumeric and there is at least one character, false otherwise.  See also  char.isalnum\n  \n\n"}, {"name": "char.chararray.isalpha()", "path": "reference/generated/numpy.char.chararray.isalpha", "type": "numpy.char.chararray.isalpha", "text": "numpy.char.chararray.isalpha method   char.chararray.isalpha()[source]\n \nReturns true for each element if all characters in the string are alphabetic and there is at least one character, false otherwise.  See also  char.isalpha\n  \n\n"}, {"name": "char.chararray.isdecimal()", "path": "reference/generated/numpy.char.chararray.isdecimal", "type": "numpy.char.chararray.isdecimal", "text": "numpy.char.chararray.isdecimal method   char.chararray.isdecimal()[source]\n \nFor each element in self, return True if there are only decimal characters in the element.  See also  char.isdecimal\n  \n\n"}, {"name": "char.chararray.isdigit()", "path": "reference/generated/numpy.char.chararray.isdigit", "type": "numpy.char.chararray.isdigit", "text": "numpy.char.chararray.isdigit method   char.chararray.isdigit()[source]\n \nReturns true for each element if all characters in the string are digits and there is at least one character, false otherwise.  See also  char.isdigit\n  \n\n"}, {"name": "char.chararray.islower()", "path": "reference/generated/numpy.char.chararray.islower", "type": "numpy.char.chararray.islower", "text": "numpy.char.chararray.islower method   char.chararray.islower()[source]\n \nReturns true for each element if all cased characters in the string are lowercase and there is at least one cased character, false otherwise.  See also  char.islower\n  \n\n"}, {"name": "char.chararray.isnumeric()", "path": "reference/generated/numpy.char.chararray.isnumeric", "type": "numpy.char.chararray.isnumeric", "text": "numpy.char.chararray.isnumeric method   char.chararray.isnumeric()[source]\n \nFor each element in self, return True if there are only numeric characters in the element.  See also  char.isnumeric\n  \n\n"}, {"name": "char.chararray.isspace()", "path": "reference/generated/numpy.char.chararray.isspace", "type": "numpy.char.chararray.isspace", "text": "numpy.char.chararray.isspace method   char.chararray.isspace()[source]\n \nReturns true for each element if there are only whitespace characters in the string and there is at least one character, false otherwise.  See also  char.isspace\n  \n\n"}, {"name": "char.chararray.istitle()", "path": "reference/generated/numpy.char.chararray.istitle", "type": "numpy.char.chararray.istitle", "text": "numpy.char.chararray.istitle method   char.chararray.istitle()[source]\n \nReturns true for each element if the element is a titlecased string and there is at least one character, false otherwise.  See also  char.istitle\n  \n\n"}, {"name": "char.chararray.isupper()", "path": "reference/generated/numpy.char.chararray.isupper", "type": "numpy.char.chararray.isupper", "text": "numpy.char.chararray.isupper method   char.chararray.isupper()[source]\n \nReturns true for each element if all cased characters in the string are uppercase and there is at least one character, false otherwise.  See also  char.isupper\n  \n\n"}, {"name": "char.chararray.item()", "path": "reference/generated/numpy.char.chararray.item", "type": "numpy.char.chararray.item", "text": "numpy.char.chararray.item method   char.chararray.item(*args)\n \nCopy an element of an array to a standard Python scalar and return it.  Parameters \n \n*argsArguments (variable number and type)\n\n\n none: in this case, the method only works for arrays with one element (a.size == 1), which element is copied into a standard Python scalar object and returned. int_type: this argument is interpreted as a flat index into the array, specifying which element to copy and return. tuple of int_types: functions as does a single int_type argument, except that the argument is interpreted as an nd-index into the array.     Returns \n \nzStandard Python scalar object\n\n\nA copy of the specified element of the array as a suitable Python scalar     Notes When the data type of a is longdouble or clongdouble, item() returns a scalar array object because there is no available Python scalar that would not lose information. Void arrays return a buffer object for item(), unless fields are defined, in which case a tuple is returned. item is very similar to a[args], except, instead of an array scalar, a standard Python scalar is returned. This can be useful for speeding up access to elements of the array and doing arithmetic on elements of the array using Python\u2019s optimized math. Examples >>> np.random.seed(123)\n>>> x = np.random.randint(9, size=(3, 3))\n>>> x\narray([[2, 2, 6],\n       [1, 3, 6],\n       [1, 0, 1]])\n>>> x.item(3)\n1\n>>> x.item(7)\n0\n>>> x.item((0, 1))\n2\n>>> x.item((2, 2))\n1\n \n\n"}, {"name": "char.chararray.itemsize", "path": "reference/generated/numpy.char.chararray.itemsize", "type": "String operations", "text": "numpy.char.chararray.itemsize attribute   char.chararray.itemsize\n \nLength of one array element in bytes. Examples >>> x = np.array([1,2,3], dtype=np.float64)\n>>> x.itemsize\n8\n>>> x = np.array([1,2,3], dtype=np.complex128)\n>>> x.itemsize\n16\n \n\n"}, {"name": "char.chararray.join()", "path": "reference/generated/numpy.char.chararray.join", "type": "numpy.char.chararray.join", "text": "numpy.char.chararray.join method   char.chararray.join(seq)[source]\n \nReturn a string which is the concatenation of the strings in the sequence seq.  See also  char.join\n  \n\n"}, {"name": "char.chararray.ljust()", "path": "reference/generated/numpy.char.chararray.ljust", "type": "numpy.char.chararray.ljust", "text": "numpy.char.chararray.ljust method   char.chararray.ljust(width, fillchar=' ')[source]\n \nReturn an array with the elements of self left-justified in a string of length width.  See also  char.ljust\n  \n\n"}, {"name": "char.chararray.lower()", "path": "reference/generated/numpy.char.chararray.lower", "type": "numpy.char.chararray.lower", "text": "numpy.char.chararray.lower method   char.chararray.lower()[source]\n \nReturn an array with the elements of self converted to lowercase.  See also  char.lower\n  \n\n"}, {"name": "char.chararray.lstrip()", "path": "reference/generated/numpy.char.chararray.lstrip", "type": "numpy.char.chararray.lstrip", "text": "numpy.char.chararray.lstrip method   char.chararray.lstrip(chars=None)[source]\n \nFor each element in self, return a copy with the leading characters removed.  See also  char.lstrip\n  \n\n"}, {"name": "char.chararray.nbytes", "path": "reference/generated/numpy.char.chararray.nbytes", "type": "String operations", "text": "numpy.char.chararray.nbytes attribute   char.chararray.nbytes\n \nTotal bytes consumed by the elements of the array. Notes Does not include memory consumed by non-element attributes of the array object. Examples >>> x = np.zeros((3,5,2), dtype=np.complex128)\n>>> x.nbytes\n480\n>>> np.prod(x.shape) * x.itemsize\n480\n \n\n"}, {"name": "char.chararray.ndim", "path": "reference/generated/numpy.char.chararray.ndim", "type": "String operations", "text": "numpy.char.chararray.ndim attribute   char.chararray.ndim\n \nNumber of array dimensions. Examples >>> x = np.array([1, 2, 3])\n>>> x.ndim\n1\n>>> y = np.zeros((2, 3, 4))\n>>> y.ndim\n3\n \n\n"}, {"name": "char.chararray.nonzero()", "path": "reference/generated/numpy.char.chararray.nonzero", "type": "numpy.char.chararray.nonzero", "text": "numpy.char.chararray.nonzero method   char.chararray.nonzero()\n \nReturn the indices of the elements that are non-zero. Refer to numpy.nonzero for full documentation.  See also  numpy.nonzero\n\nequivalent function    \n\n"}, {"name": "char.chararray.put()", "path": "reference/generated/numpy.char.chararray.put", "type": "numpy.char.chararray.put", "text": "numpy.char.chararray.put method   char.chararray.put(indices, values, mode='raise')\n \nSet a.flat[n] = values[n] for all n in indices. Refer to numpy.put for full documentation.  See also  numpy.put\n\nequivalent function    \n\n"}, {"name": "char.chararray.ravel()", "path": "reference/generated/numpy.char.chararray.ravel", "type": "numpy.char.chararray.ravel", "text": "numpy.char.chararray.ravel method   char.chararray.ravel([order])\n \nReturn a flattened array. Refer to numpy.ravel for full documentation.  See also  numpy.ravel\n\nequivalent function  ndarray.flat\n\na flat iterator on the array.    \n\n"}, {"name": "char.chararray.real", "path": "reference/generated/numpy.char.chararray.real", "type": "String operations", "text": "numpy.char.chararray.real attribute   char.chararray.real\n \nThe real part of the array.  See also  numpy.real\n\nequivalent function    Examples >>> x = np.sqrt([1+0j, 0+1j])\n>>> x.real\narray([ 1.        ,  0.70710678])\n>>> x.real.dtype\ndtype('float64')\n \n\n"}, {"name": "char.chararray.repeat()", "path": "reference/generated/numpy.char.chararray.repeat", "type": "numpy.char.chararray.repeat", "text": "numpy.char.chararray.repeat method   char.chararray.repeat(repeats, axis=None)\n \nRepeat elements of an array. Refer to numpy.repeat for full documentation.  See also  numpy.repeat\n\nequivalent function    \n\n"}, {"name": "char.chararray.replace()", "path": "reference/generated/numpy.char.chararray.replace", "type": "numpy.char.chararray.replace", "text": "numpy.char.chararray.replace method   char.chararray.replace(old, new, count=None)[source]\n \nFor each element in self, return a copy of the string with all occurrences of substring old replaced by new.  See also  char.replace\n  \n\n"}, {"name": "char.chararray.reshape()", "path": "reference/generated/numpy.char.chararray.reshape", "type": "numpy.char.chararray.reshape", "text": "numpy.char.chararray.reshape method   char.chararray.reshape(shape, order='C')\n \nReturns an array containing the same data with a new shape. Refer to numpy.reshape for full documentation.  See also  numpy.reshape\n\nequivalent function    Notes Unlike the free function numpy.reshape, this method on ndarray allows the elements of the shape parameter to be passed in as separate arguments. For example, a.reshape(10, 11) is equivalent to a.reshape((10, 11)). \n\n"}, {"name": "char.chararray.resize()", "path": "reference/generated/numpy.char.chararray.resize", "type": "numpy.char.chararray.resize", "text": "numpy.char.chararray.resize method   char.chararray.resize(new_shape, refcheck=True)\n \nChange shape and size of array in-place.  Parameters \n \nnew_shapetuple of ints, or n ints\n\n\nShape of resized array.  \nrefcheckbool, optional\n\n\nIf False, reference count will not be checked. Default is True.    Returns \n None\n  Raises \n ValueError\n\nIf a does not own its own data or references or views to it exist, and the data memory must be changed. PyPy only: will always raise if the data memory must be changed, since there is no reliable way to determine if references or views to it exist.  SystemError\n\nIf the order keyword argument is specified. This behaviour is a bug in NumPy.      See also  resize\n\nReturn a new array with the specified shape.    Notes This reallocates space for the data area if necessary. Only contiguous arrays (data elements consecutive in memory) can be resized. The purpose of the reference count check is to make sure you do not use this array as a buffer for another Python object and then reallocate the memory. However, reference counts can increase in other ways so if you are sure that you have not shared the memory for this array with another Python object, then you may safely set refcheck to False. Examples Shrinking an array: array is flattened (in the order that the data are stored in memory), resized, and reshaped: >>> a = np.array([[0, 1], [2, 3]], order='C')\n>>> a.resize((2, 1))\n>>> a\narray([[0],\n       [1]])\n >>> a = np.array([[0, 1], [2, 3]], order='F')\n>>> a.resize((2, 1))\n>>> a\narray([[0],\n       [2]])\n Enlarging an array: as above, but missing entries are filled with zeros: >>> b = np.array([[0, 1], [2, 3]])\n>>> b.resize(2, 3) # new_shape parameter doesn't have to be a tuple\n>>> b\narray([[0, 1, 2],\n       [3, 0, 0]])\n Referencing an array prevents resizing\u2026 >>> c = a\n>>> a.resize((1, 1))\nTraceback (most recent call last):\n...\nValueError: cannot resize an array that references or is referenced ...\n Unless refcheck is False: >>> a.resize((1, 1), refcheck=False)\n>>> a\narray([[0]])\n>>> c\narray([[0]])\n \n\n"}, {"name": "char.chararray.rfind()", "path": "reference/generated/numpy.char.chararray.rfind", "type": "numpy.char.chararray.rfind", "text": "numpy.char.chararray.rfind method   char.chararray.rfind(sub, start=0, end=None)[source]\n \nFor each element in self, return the highest index in the string where substring sub is found, such that sub is contained within [start, end].  See also  char.rfind\n  \n\n"}, {"name": "char.chararray.rindex()", "path": "reference/generated/numpy.char.chararray.rindex", "type": "numpy.char.chararray.rindex", "text": "numpy.char.chararray.rindex method   char.chararray.rindex(sub, start=0, end=None)[source]\n \nLike rfind, but raises ValueError when the substring sub is not found.  See also  char.rindex\n  \n\n"}, {"name": "char.chararray.rjust()", "path": "reference/generated/numpy.char.chararray.rjust", "type": "numpy.char.chararray.rjust", "text": "numpy.char.chararray.rjust method   char.chararray.rjust(width, fillchar=' ')[source]\n \nReturn an array with the elements of self right-justified in a string of length width.  See also  char.rjust\n  \n\n"}, {"name": "char.chararray.rsplit()", "path": "reference/generated/numpy.char.chararray.rsplit", "type": "numpy.char.chararray.rsplit", "text": "numpy.char.chararray.rsplit method   char.chararray.rsplit(sep=None, maxsplit=None)[source]\n \nFor each element in self, return a list of the words in the string, using sep as the delimiter string.  See also  char.rsplit\n  \n\n"}, {"name": "char.chararray.rstrip()", "path": "reference/generated/numpy.char.chararray.rstrip", "type": "numpy.char.chararray.rstrip", "text": "numpy.char.chararray.rstrip method   char.chararray.rstrip(chars=None)[source]\n \nFor each element in self, return a copy with the trailing characters removed.  See also  char.rstrip\n  \n\n"}, {"name": "char.chararray.searchsorted()", "path": "reference/generated/numpy.char.chararray.searchsorted", "type": "numpy.char.chararray.searchsorted", "text": "numpy.char.chararray.searchsorted method   char.chararray.searchsorted(v, side='left', sorter=None)\n \nFind indices where elements of v should be inserted in a to maintain order. For full documentation, see numpy.searchsorted  See also  numpy.searchsorted\n\nequivalent function    \n\n"}, {"name": "char.chararray.setfield()", "path": "reference/generated/numpy.char.chararray.setfield", "type": "numpy.char.chararray.setfield", "text": "numpy.char.chararray.setfield method   char.chararray.setfield(val, dtype, offset=0)\n \nPut a value into a specified place in a field defined by a data-type. Place val into a\u2019s field defined by dtype and beginning offset bytes into the field.  Parameters \n \nvalobject\n\n\nValue to be placed in field.  \ndtypedtype object\n\n\nData-type of the field in which to place val.  \noffsetint, optional\n\n\nThe number of bytes into the field at which to place val.    Returns \n None\n    See also  getfield\n  Examples >>> x = np.eye(3)\n>>> x.getfield(np.float64)\narray([[1.,  0.,  0.],\n       [0.,  1.,  0.],\n       [0.,  0.,  1.]])\n>>> x.setfield(3, np.int32)\n>>> x.getfield(np.int32)\narray([[3, 3, 3],\n       [3, 3, 3],\n       [3, 3, 3]], dtype=int32)\n>>> x\narray([[1.0e+000, 1.5e-323, 1.5e-323],\n       [1.5e-323, 1.0e+000, 1.5e-323],\n       [1.5e-323, 1.5e-323, 1.0e+000]])\n>>> x.setfield(np.eye(3), np.int32)\n>>> x\narray([[1.,  0.,  0.],\n       [0.,  1.,  0.],\n       [0.,  0.,  1.]])\n \n\n"}, {"name": "char.chararray.setflags()", "path": "reference/generated/numpy.char.chararray.setflags", "type": "numpy.char.chararray.setflags", "text": "numpy.char.chararray.setflags method   char.chararray.setflags(write=None, align=None, uic=None)\n \nSet array flags WRITEABLE, ALIGNED, (WRITEBACKIFCOPY and UPDATEIFCOPY), respectively. These Boolean-valued flags affect how numpy interprets the memory area used by a (see Notes below). The ALIGNED flag can only be set to True if the data is actually aligned according to the type. The WRITEBACKIFCOPY and (deprecated) UPDATEIFCOPY flags can never be set to True. The flag WRITEABLE can only be set to True if the array owns its own memory, or the ultimate owner of the memory exposes a writeable buffer interface, or is a string. (The exception for string is made so that unpickling can be done without copying memory.)  Parameters \n \nwritebool, optional\n\n\nDescribes whether or not a can be written to.  \nalignbool, optional\n\n\nDescribes whether or not a is aligned properly for its type.  \nuicbool, optional\n\n\nDescribes whether or not a is a copy of another \u201cbase\u201d array.     Notes Array flags provide information about how the memory area used for the array is to be interpreted. There are 7 Boolean flags in use, only four of which can be changed by the user: WRITEBACKIFCOPY, UPDATEIFCOPY, WRITEABLE, and ALIGNED. WRITEABLE (W) the data area can be written to; ALIGNED (A) the data and strides are aligned appropriately for the hardware (as determined by the compiler); UPDATEIFCOPY (U) (deprecated), replaced by WRITEBACKIFCOPY; WRITEBACKIFCOPY (X) this array is a copy of some other array (referenced by .base). When the C-API function PyArray_ResolveWritebackIfCopy is called, the base array will be updated with the contents of this array. All flags can be accessed using the single (upper case) letter as well as the full name. Examples >>> y = np.array([[3, 1, 7],\n...               [2, 0, 0],\n...               [8, 5, 9]])\n>>> y\narray([[3, 1, 7],\n       [2, 0, 0],\n       [8, 5, 9]])\n>>> y.flags\n  C_CONTIGUOUS : True\n  F_CONTIGUOUS : False\n  OWNDATA : True\n  WRITEABLE : True\n  ALIGNED : True\n  WRITEBACKIFCOPY : False\n  UPDATEIFCOPY : False\n>>> y.setflags(write=0, align=0)\n>>> y.flags\n  C_CONTIGUOUS : True\n  F_CONTIGUOUS : False\n  OWNDATA : True\n  WRITEABLE : False\n  ALIGNED : False\n  WRITEBACKIFCOPY : False\n  UPDATEIFCOPY : False\n>>> y.setflags(uic=1)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nValueError: cannot set WRITEBACKIFCOPY flag to True\n \n\n"}, {"name": "char.chararray.shape", "path": "reference/generated/numpy.char.chararray.shape", "type": "String operations", "text": "numpy.char.chararray.shape attribute   char.chararray.shape\n \nTuple of array dimensions. The shape property is usually used to get the current shape of an array, but may also be used to reshape the array in-place by assigning a tuple of array dimensions to it. As with numpy.reshape, one of the new shape dimensions can be -1, in which case its value is inferred from the size of the array and the remaining dimensions. Reshaping an array in-place will fail if a copy is required.  See also  numpy.reshape\n\nsimilar function  ndarray.reshape\n\nsimilar method    Examples >>> x = np.array([1, 2, 3, 4])\n>>> x.shape\n(4,)\n>>> y = np.zeros((2, 3, 4))\n>>> y.shape\n(2, 3, 4)\n>>> y.shape = (3, 8)\n>>> y\narray([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n>>> y.shape = (3, 6)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nValueError: total size of new array must be unchanged\n>>> np.zeros((4,2))[::2].shape = (-1,)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: Incompatible shape for in-place modification. Use\n`.reshape()` to make a copy with the desired shape.\n \n\n"}, {"name": "char.chararray.size", "path": "reference/generated/numpy.char.chararray.size", "type": "String operations", "text": "numpy.char.chararray.size attribute   char.chararray.size\n \nNumber of elements in the array. Equal to np.prod(a.shape), i.e., the product of the array\u2019s dimensions. Notes a.size returns a standard arbitrary precision Python integer. This may not be the case with other methods of obtaining the same value (like the suggested np.prod(a.shape), which returns an instance of np.int_), and may be relevant if the value is used further in calculations that may overflow a fixed size integer type. Examples >>> x = np.zeros((3, 5, 2), dtype=np.complex128)\n>>> x.size\n30\n>>> np.prod(x.shape)\n30\n \n\n"}, {"name": "char.chararray.sort()", "path": "reference/generated/numpy.char.chararray.sort", "type": "numpy.char.chararray.sort", "text": "numpy.char.chararray.sort method   char.chararray.sort(axis=- 1, kind=None, order=None)\n \nSort an array in-place. Refer to numpy.sort for full documentation.  Parameters \n \naxisint, optional\n\n\nAxis along which to sort. Default is -1, which means sort along the last axis.  \nkind{\u2018quicksort\u2019, \u2018mergesort\u2019, \u2018heapsort\u2019, \u2018stable\u2019}, optional\n\n\nSorting algorithm. The default is \u2018quicksort\u2019. Note that both \u2018stable\u2019 and \u2018mergesort\u2019 use timsort under the covers and, in general, the actual implementation will vary with datatype. The \u2018mergesort\u2019 option is retained for backwards compatibility.  Changed in version 1.15.0: The \u2018stable\u2019 option was added.   \norderstr or list of str, optional\n\n\nWhen a is an array with fields defined, this argument specifies which fields to compare first, second, etc. A single field can be specified as a string, and not all fields need be specified, but unspecified fields will still be used, in the order in which they come up in the dtype, to break ties.      See also  numpy.sort\n\nReturn a sorted copy of an array.  numpy.argsort\n\nIndirect sort.  numpy.lexsort\n\nIndirect stable sort on multiple keys.  numpy.searchsorted\n\nFind elements in sorted array.  numpy.partition\n\nPartial sort.    Notes See numpy.sort for notes on the different sorting algorithms. Examples >>> a = np.array([[1,4], [3,1]])\n>>> a.sort(axis=1)\n>>> a\narray([[1, 4],\n       [1, 3]])\n>>> a.sort(axis=0)\n>>> a\narray([[1, 3],\n       [1, 4]])\n Use the order keyword to specify a field to use when sorting a structured array: >>> a = np.array([('a', 2), ('c', 1)], dtype=[('x', 'S1'), ('y', int)])\n>>> a.sort(order='y')\n>>> a\narray([(b'c', 1), (b'a', 2)],\n      dtype=[('x', 'S1'), ('y', '<i8')])\n \n\n"}, {"name": "char.chararray.split()", "path": "reference/generated/numpy.char.chararray.split", "type": "numpy.char.chararray.split", "text": "numpy.char.chararray.split method   char.chararray.split(sep=None, maxsplit=None)[source]\n \nFor each element in self, return a list of the words in the string, using sep as the delimiter string.  See also  char.split\n  \n\n"}, {"name": "char.chararray.splitlines()", "path": "reference/generated/numpy.char.chararray.splitlines", "type": "numpy.char.chararray.splitlines", "text": "numpy.char.chararray.splitlines method   char.chararray.splitlines(keepends=None)[source]\n \nFor each element in self, return a list of the lines in the element, breaking at line boundaries.  See also  char.splitlines\n  \n\n"}, {"name": "char.chararray.squeeze()", "path": "reference/generated/numpy.char.chararray.squeeze", "type": "numpy.char.chararray.squeeze", "text": "numpy.char.chararray.squeeze method   char.chararray.squeeze(axis=None)\n \nRemove axes of length one from a. Refer to numpy.squeeze for full documentation.  See also  numpy.squeeze\n\nequivalent function    \n\n"}, {"name": "char.chararray.startswith()", "path": "reference/generated/numpy.char.chararray.startswith", "type": "numpy.char.chararray.startswith", "text": "numpy.char.chararray.startswith method   char.chararray.startswith(prefix, start=0, end=None)[source]\n \nReturns a boolean array which is True where the string element in self starts with prefix, otherwise False.  See also  char.startswith\n  \n\n"}, {"name": "char.chararray.strides", "path": "reference/generated/numpy.char.chararray.strides", "type": "String operations", "text": "numpy.char.chararray.strides attribute   char.chararray.strides\n \nTuple of bytes to step in each dimension when traversing an array. The byte offset of element (i[0], i[1], ..., i[n]) in an array a is: offset = sum(np.array(i) * a.strides)\n A more detailed explanation of strides can be found in the \u201cndarray.rst\u201d file in the NumPy reference guide.  See also  numpy.lib.stride_tricks.as_strided\n  Notes Imagine an array of 32-bit integers (each 4 bytes): x = np.array([[0, 1, 2, 3, 4],\n              [5, 6, 7, 8, 9]], dtype=np.int32)\n This array is stored in memory as 40 bytes, one after the other (known as a contiguous block of memory). The strides of an array tell us how many bytes we have to skip in memory to move to the next position along a certain axis. For example, we have to skip 4 bytes (1 value) to move to the next column, but 20 bytes (5 values) to get to the same position in the next row. As such, the strides for the array x will be (20, 4). Examples >>> y = np.reshape(np.arange(2*3*4), (2,3,4))\n>>> y\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n>>> y.strides\n(48, 16, 4)\n>>> y[1,1,1]\n17\n>>> offset=sum(y.strides * np.array((1,1,1)))\n>>> offset/y.itemsize\n17\n >>> x = np.reshape(np.arange(5*6*7*8), (5,6,7,8)).transpose(2,3,1,0)\n>>> x.strides\n(32, 4, 224, 1344)\n>>> i = np.array([3,5,2,2])\n>>> offset = sum(i * x.strides)\n>>> x[3,5,2,2]\n813\n>>> offset / x.itemsize\n813\n \n\n"}, {"name": "char.chararray.strip()", "path": "reference/generated/numpy.char.chararray.strip", "type": "numpy.char.chararray.strip", "text": "numpy.char.chararray.strip method   char.chararray.strip(chars=None)[source]\n \nFor each element in self, return a copy with the leading and trailing characters removed.  See also  char.strip\n  \n\n"}, {"name": "char.chararray.swapaxes()", "path": "reference/generated/numpy.char.chararray.swapaxes", "type": "numpy.char.chararray.swapaxes", "text": "numpy.char.chararray.swapaxes method   char.chararray.swapaxes(axis1, axis2)\n \nReturn a view of the array with axis1 and axis2 interchanged. Refer to numpy.swapaxes for full documentation.  See also  numpy.swapaxes\n\nequivalent function    \n\n"}, {"name": "char.chararray.swapcase()", "path": "reference/generated/numpy.char.chararray.swapcase", "type": "numpy.char.chararray.swapcase", "text": "numpy.char.chararray.swapcase method   char.chararray.swapcase()[source]\n \nFor each element in self, return a copy of the string with uppercase characters converted to lowercase and vice versa.  See also  char.swapcase\n  \n\n"}, {"name": "char.chararray.T", "path": "reference/generated/numpy.char.chararray.t", "type": "String operations", "text": "numpy.char.chararray.T attribute   char.chararray.T\n \nThe transposed array. Same as self.transpose().  See also  transpose\n  Examples >>> x = np.array([[1.,2.],[3.,4.]])\n>>> x\narray([[ 1.,  2.],\n       [ 3.,  4.]])\n>>> x.T\narray([[ 1.,  3.],\n       [ 2.,  4.]])\n>>> x = np.array([1.,2.,3.,4.])\n>>> x\narray([ 1.,  2.,  3.,  4.])\n>>> x.T\narray([ 1.,  2.,  3.,  4.])\n \n\n"}, {"name": "char.chararray.take()", "path": "reference/generated/numpy.char.chararray.take", "type": "numpy.char.chararray.take", "text": "numpy.char.chararray.take method   char.chararray.take(indices, axis=None, out=None, mode='raise')\n \nReturn an array formed from the elements of a at the given indices. Refer to numpy.take for full documentation.  See also  numpy.take\n\nequivalent function    \n\n"}, {"name": "char.chararray.title()", "path": "reference/generated/numpy.char.chararray.title", "type": "numpy.char.chararray.title", "text": "numpy.char.chararray.title method   char.chararray.title()[source]\n \nFor each element in self, return a titlecased version of the string: words start with uppercase characters, all remaining cased characters are lowercase.  See also  char.title\n  \n\n"}, {"name": "char.chararray.tobytes()", "path": "reference/generated/numpy.char.chararray.tobytes", "type": "String operations", "text": "numpy.char.chararray.tobytes method   char.chararray.tobytes(order='C')\n \nConstruct Python bytes containing the raw data bytes in the array. Constructs Python bytes showing a copy of the raw contents of data memory. The bytes object is produced in C-order by default. This behavior is controlled by the order parameter.  New in version 1.9.0.   Parameters \n \norder{\u2018C\u2019, \u2018F\u2019, \u2018A\u2019}, optional\n\n\nControls the memory layout of the bytes object. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 (short for Any) means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. Default is \u2018C\u2019.    Returns \n \nsbytes\n\n\nPython bytes exhibiting a copy of a\u2019s raw data.     Examples >>> x = np.array([[0, 1], [2, 3]], dtype='<u2')\n>>> x.tobytes()\nb'\\x00\\x00\\x01\\x00\\x02\\x00\\x03\\x00'\n>>> x.tobytes('C') == x.tobytes()\nTrue\n>>> x.tobytes('F')\nb'\\x00\\x00\\x02\\x00\\x01\\x00\\x03\\x00'\n \n\n"}, {"name": "char.chararray.tofile()", "path": "reference/generated/numpy.char.chararray.tofile", "type": "numpy.char.chararray.tofile", "text": "numpy.char.chararray.tofile method   char.chararray.tofile(fid, sep='', format='%s')\n \nWrite array to a file as text or binary (default). Data is always written in \u2018C\u2019 order, independent of the order of a. The data produced by this method can be recovered using the function fromfile().  Parameters \n \nfidfile or str or Path\n\n\nAn open file object, or a string containing a filename.  Changed in version 1.17.0: pathlib.Path objects are now accepted.   \nsepstr\n\n\nSeparator between array items for text output. If \u201c\u201d (empty), a binary file is written, equivalent to file.write(a.tobytes()).  \nformatstr\n\n\nFormat string for text file output. Each entry in the array is formatted to text by first converting it to the closest Python type, and then using \u201cformat\u201d % item.     Notes This is a convenience function for quick storage of array data. Information on endianness and precision is lost, so this method is not a good choice for files intended to archive data or transport data between machines with different endianness. Some of these problems can be overcome by outputting the data as text files, at the expense of speed and file size. When fid is a file object, array contents are directly written to the file, bypassing the file object\u2019s write method. As a result, tofile cannot be used with files objects supporting compression (e.g., GzipFile) or file-like objects that do not support fileno() (e.g., BytesIO). \n\n"}, {"name": "char.chararray.tolist()", "path": "reference/generated/numpy.char.chararray.tolist", "type": "numpy.char.chararray.tolist", "text": "numpy.char.chararray.tolist method   char.chararray.tolist()\n \nReturn the array as an a.ndim-levels deep nested list of Python scalars. Return a copy of the array data as a (nested) Python list. Data items are converted to the nearest compatible builtin Python type, via the item function. If a.ndim is 0, then since the depth of the nested list is 0, it will not be a list at all, but a simple Python scalar.  Parameters \n none\n  Returns \n \nyobject, or list of object, or list of list of object, or \u2026\n\n\nThe possibly nested list of array elements.     Notes The array may be recreated via a = np.array(a.tolist()), although this may sometimes lose precision. Examples For a 1D array, a.tolist() is almost the same as list(a), except that tolist changes numpy scalars to Python scalars: >>> a = np.uint32([1, 2])\n>>> a_list = list(a)\n>>> a_list\n[1, 2]\n>>> type(a_list[0])\n<class 'numpy.uint32'>\n>>> a_tolist = a.tolist()\n>>> a_tolist\n[1, 2]\n>>> type(a_tolist[0])\n<class 'int'>\n Additionally, for a 2D array, tolist applies recursively: >>> a = np.array([[1, 2], [3, 4]])\n>>> list(a)\n[array([1, 2]), array([3, 4])]\n>>> a.tolist()\n[[1, 2], [3, 4]]\n The base case for this recursion is a 0D array: >>> a = np.array(1)\n>>> list(a)\nTraceback (most recent call last):\n  ...\nTypeError: iteration over a 0-d array\n>>> a.tolist()\n1\n \n\n"}, {"name": "char.chararray.tostring()", "path": "reference/generated/numpy.char.chararray.tostring", "type": "numpy.char.chararray.tostring", "text": "numpy.char.chararray.tostring method   char.chararray.tostring(order='C')\n \nA compatibility alias for tobytes, with exactly the same behavior. Despite its name, it returns bytes not strs.  Deprecated since version 1.19.0.  \n\n"}, {"name": "char.chararray.translate()", "path": "reference/generated/numpy.char.chararray.translate", "type": "numpy.char.chararray.translate", "text": "numpy.char.chararray.translate method   char.chararray.translate(table, deletechars=None)[source]\n \nFor each element in self, return a copy of the string where all characters occurring in the optional argument deletechars are removed, and the remaining characters have been mapped through the given translation table.  See also  char.translate\n  \n\n"}, {"name": "char.chararray.transpose()", "path": "reference/generated/numpy.char.chararray.transpose", "type": "numpy.char.chararray.transpose", "text": "numpy.char.chararray.transpose method   char.chararray.transpose(*axes)\n \nReturns a view of the array with axes transposed. For a 1-D array this has no effect, as a transposed vector is simply the same vector. To convert a 1-D array into a 2D column vector, an additional dimension must be added. np.atleast2d(a).T achieves this, as does a[:, np.newaxis]. For a 2-D array, this is a standard matrix transpose. For an n-D array, if axes are given, their order indicates how the axes are permuted (see Examples). If axes are not provided and a.shape = (i[0], i[1], ... i[n-2], i[n-1]), then a.transpose().shape = (i[n-1], i[n-2], ... i[1], i[0]).  Parameters \n \naxesNone, tuple of ints, or n ints\n\n\n None or no argument: reverses the order of the axes. tuple of ints: i in the j-th place in the tuple means a\u2019s i-th axis becomes a.transpose()\u2019s j-th axis. \nn ints: same as an n-tuple of the same ints (this form is intended simply as a \u201cconvenience\u201d alternative to the tuple form)     Returns \n \noutndarray\n\n\nView of a, with axes suitably permuted.      See also  transpose\n\nEquivalent function  ndarray.T\n\nArray property returning the array transposed.  ndarray.reshape\n\nGive a new shape to an array without changing its data.    Examples >>> a = np.array([[1, 2], [3, 4]])\n>>> a\narray([[1, 2],\n       [3, 4]])\n>>> a.transpose()\narray([[1, 3],\n       [2, 4]])\n>>> a.transpose((1, 0))\narray([[1, 3],\n       [2, 4]])\n>>> a.transpose(1, 0)\narray([[1, 3],\n       [2, 4]])\n \n\n"}, {"name": "char.chararray.upper()", "path": "reference/generated/numpy.char.chararray.upper", "type": "numpy.char.chararray.upper", "text": "numpy.char.chararray.upper method   char.chararray.upper()[source]\n \nReturn an array with the elements of self converted to uppercase.  See also  char.upper\n  \n\n"}, {"name": "char.chararray.view()", "path": "reference/generated/numpy.char.chararray.view", "type": "numpy.char.chararray.view", "text": "numpy.char.chararray.view method   char.chararray.view([dtype][, type])\n \nNew view of array with the same data.  Note Passing None for dtype is different from omitting the parameter, since the former invokes dtype(None) which is an alias for dtype('float_').   Parameters \n \ndtypedata-type or ndarray sub-class, optional\n\n\nData-type descriptor of the returned view, e.g., float32 or int16. Omitting it results in the view having the same data-type as a. This argument can also be specified as an ndarray sub-class, which then specifies the type of the returned object (this is equivalent to setting the type parameter).  \ntypePython type, optional\n\n\nType of the returned view, e.g., ndarray or matrix. Again, omission of the parameter results in type preservation.     Notes a.view() is used two different ways: a.view(some_dtype) or a.view(dtype=some_dtype) constructs a view of the array\u2019s memory with a different data-type. This can cause a reinterpretation of the bytes of memory. a.view(ndarray_subclass) or a.view(type=ndarray_subclass) just returns an instance of ndarray_subclass that looks at the same array (same shape, dtype, etc.) This does not cause a reinterpretation of the memory. For a.view(some_dtype), if some_dtype has a different number of bytes per entry than the previous dtype (for example, converting a regular array to a structured array), then the behavior of the view cannot be predicted just from the superficial appearance of a (shown by print(a)). It also depends on exactly how a is stored in memory. Therefore if a is C-ordered versus fortran-ordered, versus defined as a slice or transpose, etc., the view may give different results. Examples >>> x = np.array([(1, 2)], dtype=[('a', np.int8), ('b', np.int8)])\n Viewing array data using a different type and dtype: >>> y = x.view(dtype=np.int16, type=np.matrix)\n>>> y\nmatrix([[513]], dtype=int16)\n>>> print(type(y))\n<class 'numpy.matrix'>\n Creating a view on a structured array so it can be used in calculations >>> x = np.array([(1, 2),(3,4)], dtype=[('a', np.int8), ('b', np.int8)])\n>>> xv = x.view(dtype=np.int8).reshape(-1,2)\n>>> xv\narray([[1, 2],\n       [3, 4]], dtype=int8)\n>>> xv.mean(0)\narray([2.,  3.])\n Making changes to the view changes the underlying array >>> xv[0,1] = 20\n>>> x\narray([(1, 20), (3,  4)], dtype=[('a', 'i1'), ('b', 'i1')])\n Using a view to convert an array to a recarray: >>> z = x.view(np.recarray)\n>>> z.a\narray([1, 3], dtype=int8)\n Views share data: >>> x[0] = (9, 10)\n>>> z[0]\n(9, 10)\n Views that change the dtype size (bytes per entry) should normally be avoided on arrays defined by slices, transposes, fortran-ordering, etc.: >>> x = np.array([[1,2,3],[4,5,6]], dtype=np.int16)\n>>> y = x[:, 0:2]\n>>> y\narray([[1, 2],\n       [4, 5]], dtype=int16)\n>>> y.view(dtype=[('width', np.int16), ('length', np.int16)])\nTraceback (most recent call last):\n    ...\nValueError: To change to a dtype of a different size, the array must be C-contiguous\n>>> z = y.copy()\n>>> z.view(dtype=[('width', np.int16), ('length', np.int16)])\narray([[(1, 2)],\n       [(4, 5)]], dtype=[('width', '<i2'), ('length', '<i2')])\n \n\n"}, {"name": "char.chararray.zfill()", "path": "reference/generated/numpy.char.chararray.zfill", "type": "numpy.char.chararray.zfill", "text": "numpy.char.chararray.zfill method   char.chararray.zfill(width)[source]\n \nReturn the numeric string left-filled with zeros in a string of length width.  See also  char.zfill\n  \n\n"}, {"name": "char.compare_chararrays()", "path": "reference/generated/numpy.char.compare_chararrays", "type": "numpy.char.compare_chararrays", "text": "numpy.char.compare_chararrays   char.compare_chararrays(a1, a2, cmp, rstrip)\n \nPerforms element-wise comparison of two string arrays using the comparison operator specified by cmp_op.  Parameters \n \na1, a2array_like\n\n\nArrays to be compared.  \ncmp{\u201c<\u201d, \u201c<=\u201d, \u201c==\u201d, \u201c>=\u201d, \u201c>\u201d, \u201c!=\u201d}\n\n\nType of comparison.  \nrstripBoolean\n\n\nIf True, the spaces at the end of Strings are removed before the comparison.    Returns \n \noutndarray\n\n\nThe output array of type Boolean with the same shape as a and b.    Raises \n ValueError\n\nIf cmp_op is not valid.  TypeError\n\nIf at least one of a or b is a non-string array     Examples >>> a = np.array([\"a\", \"b\", \"cde\"])\n>>> b = np.array([\"a\", \"a\", \"dec\"])\n>>> np.compare_chararrays(a, b, \">\", True)\narray([False,  True, False])\n \n\n"}, {"name": "char.count()", "path": "reference/generated/numpy.char.count", "type": "numpy.char.count", "text": "numpy.char.count   char.count(a, sub, start=0, end=None)[source]\n \nReturns an array with the number of non-overlapping occurrences of substring sub in the range [start, end]. Calls str.count element-wise.  Parameters \n \naarray_like of str or unicode\n\n\nsubstr or unicode\n\n\nThe substring to search for.  \nstart, endint, optional\n\n\nOptional arguments start and end are interpreted as slice notation to specify the range in which to count.    Returns \n \noutndarray\n\n\nOutput array of ints.      See also  str.count\n  Examples >>> c = np.array(['aAaAaA', '  aA  ', 'abBABba'])\n>>> c\narray(['aAaAaA', '  aA  ', 'abBABba'], dtype='<U7')\n>>> np.char.count(c, 'A')\narray([3, 1, 1])\n>>> np.char.count(c, 'aA')\narray([3, 1, 0])\n>>> np.char.count(c, 'A', start=1, end=4)\narray([2, 1, 1])\n>>> np.char.count(c, 'A', start=1, end=3)\narray([1, 0, 0])\n \n\n"}, {"name": "char.decode()", "path": "reference/generated/numpy.char.decode", "type": "numpy.char.decode", "text": "numpy.char.decode   char.decode(a, encoding=None, errors=None)[source]\n \nCalls str.decode element-wise. The set of available codecs comes from the Python standard library, and may be extended at runtime. For more information, see the codecs module.  Parameters \n \naarray_like of str or unicode\n\n\nencodingstr, optional\n\n\nThe name of an encoding  \nerrorsstr, optional\n\n\nSpecifies how to handle encoding errors    Returns \n \noutndarray\n\n    See also  str.decode\n  Notes The type of the result will depend on the encoding specified. Examples >>> c = np.array(['aAaAaA', '  aA  ', 'abBABba'])\n>>> c\narray(['aAaAaA', '  aA  ', 'abBABba'], dtype='<U7')\n>>> np.char.encode(c, encoding='cp037')\narray(['\\x81\\xc1\\x81\\xc1\\x81\\xc1', '@@\\x81\\xc1@@',\n    '\\x81\\x82\\xc2\\xc1\\xc2\\x82\\x81'],\n    dtype='|S7')\n \n\n"}, {"name": "char.encode()", "path": "reference/generated/numpy.char.encode", "type": "numpy.char.encode", "text": "numpy.char.encode   char.encode(a, encoding=None, errors=None)[source]\n \nCalls str.encode element-wise. The set of available codecs comes from the Python standard library, and may be extended at runtime. For more information, see the codecs module.  Parameters \n \naarray_like of str or unicode\n\n\nencodingstr, optional\n\n\nThe name of an encoding  \nerrorsstr, optional\n\n\nSpecifies how to handle encoding errors    Returns \n \noutndarray\n\n    See also  str.encode\n  Notes The type of the result will depend on the encoding specified. \n\n"}, {"name": "char.endswith()", "path": "reference/generated/numpy.char.endswith", "type": "numpy.char.endswith", "text": "numpy.char.endswith   char.endswith(a, suffix, start=0, end=None)[source]\n \nReturns a boolean array which is True where the string element in a ends with suffix, otherwise False. Calls str.endswith element-wise.  Parameters \n \naarray_like of str or unicode\n\n\nsuffixstr\n\n\nstart, endint, optional\n\n\nWith optional start, test beginning at that position. With optional end, stop comparing at that position.    Returns \n \noutndarray\n\n\nOutputs an array of bools.      See also  str.endswith\n  Examples >>> s = np.array(['foo', 'bar'])\n>>> s[0] = 'foo'\n>>> s[1] = 'bar'\n>>> s\narray(['foo', 'bar'], dtype='<U3')\n>>> np.char.endswith(s, 'ar')\narray([False,  True])\n>>> np.char.endswith(s, 'a', start=1, end=2)\narray([False,  True])\n \n\n"}, {"name": "char.equal()", "path": "reference/generated/numpy.char.equal", "type": "numpy.char.equal", "text": "numpy.char.equal   char.equal(x1, x2)[source]\n \nReturn (x1 == x2) element-wise. Unlike numpy.equal, this comparison is performed by first stripping whitespace characters from the end of the string. This behavior is provided for backward-compatibility with numarray.  Parameters \n \nx1, x2array_like of str or unicode\n\n\nInput arrays of the same shape.    Returns \n \noutndarray\n\n\nOutput array of bools.      See also  \nnot_equal, greater_equal, less_equal, greater, less\n\n  \n\n"}, {"name": "char.expandtabs()", "path": "reference/generated/numpy.char.expandtabs", "type": "numpy.char.expandtabs", "text": "numpy.char.expandtabs   char.expandtabs(a, tabsize=8)[source]\n \nReturn a copy of each string element where all tab characters are replaced by one or more spaces. Calls str.expandtabs element-wise. Return a copy of each string element where all tab characters are replaced by one or more spaces, depending on the current column and the given tabsize. The column number is reset to zero after each newline occurring in the string. This doesn\u2019t understand other non-printing characters or escape sequences.  Parameters \n \naarray_like of str or unicode\n\n\nInput array  \ntabsizeint, optional\n\n\nReplace tabs with tabsize number of spaces. If not given defaults to 8 spaces.    Returns \n \noutndarray\n\n\nOutput array of str or unicode, depending on input type      See also  str.expandtabs\n  \n\n"}, {"name": "char.find()", "path": "reference/generated/numpy.char.find", "type": "numpy.char.find", "text": "numpy.char.find   char.find(a, sub, start=0, end=None)[source]\n \nFor each element, return the lowest index in the string where substring sub is found. Calls str.find element-wise. For each element, return the lowest index in the string where substring sub is found, such that sub is contained in the range [start, end].  Parameters \n \naarray_like of str or unicode\n\n\nsubstr or unicode\n\n\nstart, endint, optional\n\n\nOptional arguments start and end are interpreted as in slice notation.    Returns \n \noutndarray or int\n\n\nOutput array of ints. Returns -1 if sub is not found.      See also  str.find\n  \n\n"}, {"name": "char.greater()", "path": "reference/generated/numpy.char.greater", "type": "numpy.char.greater", "text": "numpy.char.greater   char.greater(x1, x2)[source]\n \nReturn (x1 > x2) element-wise. Unlike numpy.greater, this comparison is performed by first stripping whitespace characters from the end of the string. This behavior is provided for backward-compatibility with numarray.  Parameters \n \nx1, x2array_like of str or unicode\n\n\nInput arrays of the same shape.    Returns \n \noutndarray\n\n\nOutput array of bools.      See also  \nequal, not_equal, greater_equal, less_equal, less\n\n  \n\n"}, {"name": "char.greater_equal()", "path": "reference/generated/numpy.char.greater_equal", "type": "numpy.char.greater_equal", "text": "numpy.char.greater_equal   char.greater_equal(x1, x2)[source]\n \nReturn (x1 >= x2) element-wise. Unlike numpy.greater_equal, this comparison is performed by first stripping whitespace characters from the end of the string. This behavior is provided for backward-compatibility with numarray.  Parameters \n \nx1, x2array_like of str or unicode\n\n\nInput arrays of the same shape.    Returns \n \noutndarray\n\n\nOutput array of bools.      See also  \nequal, not_equal, less_equal, greater, less\n\n  \n\n"}, {"name": "char.index()", "path": "reference/generated/numpy.char.index", "type": "numpy.char.index", "text": "numpy.char.index   char.index(a, sub, start=0, end=None)[source]\n \nLike find, but raises ValueError when the substring is not found. Calls str.index element-wise.  Parameters \n \naarray_like of str or unicode\n\n\nsubstr or unicode\n\n\nstart, endint, optional\n\n  Returns \n \noutndarray\n\n\nOutput array of ints. Returns -1 if sub is not found.      See also  \nfind, str.find\n\n  \n\n"}, {"name": "char.isalnum()", "path": "reference/generated/numpy.char.isalnum", "type": "numpy.char.isalnum", "text": "numpy.char.isalnum   char.isalnum(a)[source]\n \nReturns true for each element if all characters in the string are alphanumeric and there is at least one character, false otherwise. Calls str.isalnum element-wise. For 8-bit strings, this method is locale-dependent.  Parameters \n \naarray_like of str or unicode\n\n  Returns \n \noutndarray\n\n\nOutput array of str or unicode, depending on input type      See also  str.isalnum\n  \n\n"}, {"name": "char.isalpha()", "path": "reference/generated/numpy.char.isalpha", "type": "numpy.char.isalpha", "text": "numpy.char.isalpha   char.isalpha(a)[source]\n \nReturns true for each element if all characters in the string are alphabetic and there is at least one character, false otherwise. Calls str.isalpha element-wise. For 8-bit strings, this method is locale-dependent.  Parameters \n \naarray_like of str or unicode\n\n  Returns \n \noutndarray\n\n\nOutput array of bools      See also  str.isalpha\n  \n\n"}, {"name": "char.isdecimal()", "path": "reference/generated/numpy.char.isdecimal", "type": "numpy.char.isdecimal", "text": "numpy.char.isdecimal   char.isdecimal(a)[source]\n \nFor each element, return True if there are only decimal characters in the element. Calls unicode.isdecimal element-wise. Decimal characters include digit characters, and all characters that can be used to form decimal-radix numbers, e.g. U+0660, ARABIC-INDIC DIGIT ZERO.  Parameters \n \naarray_like, unicode\n\n\nInput array.    Returns \n \noutndarray, bool\n\n\nArray of booleans identical in shape to a.      See also  unicode.isdecimal\n  \n\n"}, {"name": "char.isdigit()", "path": "reference/generated/numpy.char.isdigit", "type": "numpy.char.isdigit", "text": "numpy.char.isdigit   char.isdigit(a)[source]\n \nReturns true for each element if all characters in the string are digits and there is at least one character, false otherwise. Calls str.isdigit element-wise. For 8-bit strings, this method is locale-dependent.  Parameters \n \naarray_like of str or unicode\n\n  Returns \n \noutndarray\n\n\nOutput array of bools      See also  str.isdigit\n  \n\n"}, {"name": "char.islower()", "path": "reference/generated/numpy.char.islower", "type": "numpy.char.islower", "text": "numpy.char.islower   char.islower(a)[source]\n \nReturns true for each element if all cased characters in the string are lowercase and there is at least one cased character, false otherwise. Calls str.islower element-wise. For 8-bit strings, this method is locale-dependent.  Parameters \n \naarray_like of str or unicode\n\n  Returns \n \noutndarray\n\n\nOutput array of bools      See also  str.islower\n  \n\n"}, {"name": "char.isnumeric()", "path": "reference/generated/numpy.char.isnumeric", "type": "numpy.char.isnumeric", "text": "numpy.char.isnumeric   char.isnumeric(a)[source]\n \nFor each element, return True if there are only numeric characters in the element. Calls unicode.isnumeric element-wise. Numeric characters include digit characters, and all characters that have the Unicode numeric value property, e.g. U+2155,\nVULGAR FRACTION ONE FIFTH.  Parameters \n \naarray_like, unicode\n\n\nInput array.    Returns \n \noutndarray, bool\n\n\nArray of booleans of same shape as a.      See also  unicode.isnumeric\n  \n\n"}, {"name": "char.isspace()", "path": "reference/generated/numpy.char.isspace", "type": "numpy.char.isspace", "text": "numpy.char.isspace   char.isspace(a)[source]\n \nReturns true for each element if there are only whitespace characters in the string and there is at least one character, false otherwise. Calls str.isspace element-wise. For 8-bit strings, this method is locale-dependent.  Parameters \n \naarray_like of str or unicode\n\n  Returns \n \noutndarray\n\n\nOutput array of bools      See also  str.isspace\n  \n\n"}, {"name": "char.istitle()", "path": "reference/generated/numpy.char.istitle", "type": "numpy.char.istitle", "text": "numpy.char.istitle   char.istitle(a)[source]\n \nReturns true for each element if the element is a titlecased string and there is at least one character, false otherwise. Call str.istitle element-wise. For 8-bit strings, this method is locale-dependent.  Parameters \n \naarray_like of str or unicode\n\n  Returns \n \noutndarray\n\n\nOutput array of bools      See also  str.istitle\n  \n\n"}, {"name": "char.isupper()", "path": "reference/generated/numpy.char.isupper", "type": "numpy.char.isupper", "text": "numpy.char.isupper   char.isupper(a)[source]\n \nReturns true for each element if all cased characters in the string are uppercase and there is at least one character, false otherwise. Call str.isupper element-wise. For 8-bit strings, this method is locale-dependent.  Parameters \n \naarray_like of str or unicode\n\n  Returns \n \noutndarray\n\n\nOutput array of bools      See also  str.isupper\n  \n\n"}, {"name": "char.join()", "path": "reference/generated/numpy.char.join", "type": "numpy.char.join", "text": "numpy.char.join   char.join(sep, seq)[source]\n \nReturn a string which is the concatenation of the strings in the sequence seq. Calls str.join element-wise.  Parameters \n \nseparray_like of str or unicode\n\n\nseqarray_like of str or unicode\n\n  Returns \n \noutndarray\n\n\nOutput array of str or unicode, depending on input types      See also  str.join\n  \n\n"}, {"name": "char.less()", "path": "reference/generated/numpy.char.less", "type": "numpy.char.less", "text": "numpy.char.less   char.less(x1, x2)[source]\n \nReturn (x1 < x2) element-wise. Unlike numpy.greater, this comparison is performed by first stripping whitespace characters from the end of the string. This behavior is provided for backward-compatibility with numarray.  Parameters \n \nx1, x2array_like of str or unicode\n\n\nInput arrays of the same shape.    Returns \n \noutndarray\n\n\nOutput array of bools.      See also  \nequal, not_equal, greater_equal, less_equal, greater\n\n  \n\n"}, {"name": "char.less_equal()", "path": "reference/generated/numpy.char.less_equal", "type": "numpy.char.less_equal", "text": "numpy.char.less_equal   char.less_equal(x1, x2)[source]\n \nReturn (x1 <= x2) element-wise. Unlike numpy.less_equal, this comparison is performed by first stripping whitespace characters from the end of the string. This behavior is provided for backward-compatibility with numarray.  Parameters \n \nx1, x2array_like of str or unicode\n\n\nInput arrays of the same shape.    Returns \n \noutndarray\n\n\nOutput array of bools.      See also  \nequal, not_equal, greater_equal, greater, less\n\n  \n\n"}, {"name": "char.ljust()", "path": "reference/generated/numpy.char.ljust", "type": "numpy.char.ljust", "text": "numpy.char.ljust   char.ljust(a, width, fillchar=' ')[source]\n \nReturn an array with the elements of a left-justified in a string of length width. Calls str.ljust element-wise.  Parameters \n \naarray_like of str or unicode\n\n\nwidthint\n\n\nThe length of the resulting strings  \nfillcharstr or unicode, optional\n\n\nThe character to use for padding    Returns \n \noutndarray\n\n\nOutput array of str or unicode, depending on input type      See also  str.ljust\n  \n\n"}, {"name": "char.lower()", "path": "reference/generated/numpy.char.lower", "type": "numpy.char.lower", "text": "numpy.char.lower   char.lower(a)[source]\n \nReturn an array with the elements converted to lowercase. Call str.lower element-wise. For 8-bit strings, this method is locale-dependent.  Parameters \n \naarray_like, {str, unicode}\n\n\nInput array.    Returns \n \noutndarray, {str, unicode}\n\n\nOutput array of str or unicode, depending on input type      See also  str.lower\n  Examples >>> c = np.array(['A1B C', '1BCA', 'BCA1']); c\narray(['A1B C', '1BCA', 'BCA1'], dtype='<U5')\n>>> np.char.lower(c)\narray(['a1b c', '1bca', 'bca1'], dtype='<U5')\n \n\n"}, {"name": "char.lstrip()", "path": "reference/generated/numpy.char.lstrip", "type": "numpy.char.lstrip", "text": "numpy.char.lstrip   char.lstrip(a, chars=None)[source]\n \nFor each element in a, return a copy with the leading characters removed. Calls str.lstrip element-wise.  Parameters \n \naarray-like, {str, unicode}\n\n\nInput array.  \nchars{str, unicode}, optional\n\n\nThe chars argument is a string specifying the set of characters to be removed. If omitted or None, the chars argument defaults to removing whitespace. The chars argument is not a prefix; rather, all combinations of its values are stripped.    Returns \n \noutndarray, {str, unicode}\n\n\nOutput array of str or unicode, depending on input type      See also  str.lstrip\n  Examples >>> c = np.array(['aAaAaA', '  aA  ', 'abBABba'])\n>>> c\narray(['aAaAaA', '  aA  ', 'abBABba'], dtype='<U7')\n The \u2018a\u2019 variable is unstripped from c[1] because whitespace leading. >>> np.char.lstrip(c, 'a')\narray(['AaAaA', '  aA  ', 'bBABba'], dtype='<U7')\n >>> np.char.lstrip(c, 'A') # leaves c unchanged\narray(['aAaAaA', '  aA  ', 'abBABba'], dtype='<U7')\n>>> (np.char.lstrip(c, ' ') == np.char.lstrip(c, '')).all()\n... # XXX: is this a regression? This used to return True\n... # np.char.lstrip(c,'') does not modify c at all.\nFalse\n>>> (np.char.lstrip(c, ' ') == np.char.lstrip(c, None)).all()\nTrue\n \n\n"}, {"name": "char.mod()", "path": "reference/generated/numpy.char.mod", "type": "numpy.char.mod", "text": "numpy.char.mod   char.mod(a, values)[source]\n \nReturn (a % i), that is pre-Python 2.6 string formatting (interpolation), element-wise for a pair of array_likes of str or unicode.  Parameters \n \naarray_like of str or unicode\n\n\nvaluesarray_like of values\n\n\nThese values will be element-wise interpolated into the string.    Returns \n \noutndarray\n\n\nOutput array of str or unicode, depending on input types      See also  str.__mod__\n  \n\n"}, {"name": "char.multiply()", "path": "reference/generated/numpy.char.multiply", "type": "numpy.char.multiply", "text": "numpy.char.multiply   char.multiply(a, i)[source]\n \nReturn (a * i), that is string multiple concatenation, element-wise. Values in i of less than 0 are treated as 0 (which yields an empty string).  Parameters \n \naarray_like of str or unicode\n\n\niarray_like of ints\n\n  Returns \n \noutndarray\n\n\nOutput array of str or unicode, depending on input types     \n\n"}, {"name": "char.not_equal()", "path": "reference/generated/numpy.char.not_equal", "type": "numpy.char.not_equal", "text": "numpy.char.not_equal   char.not_equal(x1, x2)[source]\n \nReturn (x1 != x2) element-wise. Unlike numpy.not_equal, this comparison is performed by first stripping whitespace characters from the end of the string. This behavior is provided for backward-compatibility with numarray.  Parameters \n \nx1, x2array_like of str or unicode\n\n\nInput arrays of the same shape.    Returns \n \noutndarray\n\n\nOutput array of bools.      See also  \nequal, greater_equal, less_equal, greater, less\n\n  \n\n"}, {"name": "char.partition()", "path": "reference/generated/numpy.char.partition", "type": "numpy.char.partition", "text": "numpy.char.partition   char.partition(a, sep)[source]\n \nPartition each element in a around sep. Calls str.partition element-wise. For each element in a, split the element as the first occurrence of sep, and return 3 strings containing the part before the separator, the separator itself, and the part after the separator. If the separator is not found, return 3 strings containing the string itself, followed by two empty strings.  Parameters \n \naarray_like, {str, unicode}\n\n\nInput array  \nsep{str, unicode}\n\n\nSeparator to split each string element in a.    Returns \n \noutndarray, {str, unicode}\n\n\nOutput array of str or unicode, depending on input type. The output array will have an extra dimension with 3 elements per input element.      See also  str.partition\n  \n\n"}, {"name": "char.replace()", "path": "reference/generated/numpy.char.replace", "type": "numpy.char.replace", "text": "numpy.char.replace   char.replace(a, old, new, count=None)[source]\n \nFor each element in a, return a copy of the string with all occurrences of substring old replaced by new. Calls str.replace element-wise.  Parameters \n \naarray-like of str or unicode\n\n\nold, newstr or unicode\n\n\ncountint, optional\n\n\nIf the optional argument count is given, only the first count occurrences are replaced.    Returns \n \noutndarray\n\n\nOutput array of str or unicode, depending on input type      See also  str.replace\n  \n\n"}, {"name": "char.rfind()", "path": "reference/generated/numpy.char.rfind", "type": "numpy.char.rfind", "text": "numpy.char.rfind   char.rfind(a, sub, start=0, end=None)[source]\n \nFor each element in a, return the highest index in the string where substring sub is found, such that sub is contained within [start, end]. Calls str.rfind element-wise.  Parameters \n \naarray-like of str or unicode\n\n\nsubstr or unicode\n\n\nstart, endint, optional\n\n\nOptional arguments start and end are interpreted as in slice notation.    Returns \n \noutndarray\n\n\nOutput array of ints. Return -1 on failure.      See also  str.rfind\n  \n\n"}, {"name": "char.rindex()", "path": "reference/generated/numpy.char.rindex", "type": "numpy.char.rindex", "text": "numpy.char.rindex   char.rindex(a, sub, start=0, end=None)[source]\n \nLike rfind, but raises ValueError when the substring sub is not found. Calls str.rindex element-wise.  Parameters \n \naarray-like of str or unicode\n\n\nsubstr or unicode\n\n\nstart, endint, optional\n\n  Returns \n \noutndarray\n\n\nOutput array of ints.      See also  \nrfind, str.rindex\n\n  \n\n"}, {"name": "char.rjust()", "path": "reference/generated/numpy.char.rjust", "type": "numpy.char.rjust", "text": "numpy.char.rjust   char.rjust(a, width, fillchar=' ')[source]\n \nReturn an array with the elements of a right-justified in a string of length width. Calls str.rjust element-wise.  Parameters \n \naarray_like of str or unicode\n\n\nwidthint\n\n\nThe length of the resulting strings  \nfillcharstr or unicode, optional\n\n\nThe character to use for padding    Returns \n \noutndarray\n\n\nOutput array of str or unicode, depending on input type      See also  str.rjust\n  \n\n"}, {"name": "char.rpartition()", "path": "reference/generated/numpy.char.rpartition", "type": "numpy.char.rpartition", "text": "numpy.char.rpartition   char.rpartition(a, sep)[source]\n \nPartition (split) each element around the right-most separator. Calls str.rpartition element-wise. For each element in a, split the element as the last occurrence of sep, and return 3 strings containing the part before the separator, the separator itself, and the part after the separator. If the separator is not found, return 3 strings containing the string itself, followed by two empty strings.  Parameters \n \naarray_like of str or unicode\n\n\nInput array  \nsepstr or unicode\n\n\nRight-most separator to split each element in array.    Returns \n \noutndarray\n\n\nOutput array of string or unicode, depending on input type. The output array will have an extra dimension with 3 elements per input element.      See also  str.rpartition\n  \n\n"}, {"name": "char.rsplit()", "path": "reference/generated/numpy.char.rsplit", "type": "numpy.char.rsplit", "text": "numpy.char.rsplit   char.rsplit(a, sep=None, maxsplit=None)[source]\n \nFor each element in a, return a list of the words in the string, using sep as the delimiter string. Calls str.rsplit element-wise. Except for splitting from the right, rsplit behaves like split.  Parameters \n \naarray_like of str or unicode\n\n\nsepstr or unicode, optional\n\n\nIf sep is not specified or None, any whitespace string is a separator.  \nmaxsplitint, optional\n\n\nIf maxsplit is given, at most maxsplit splits are done, the rightmost ones.    Returns \n \noutndarray\n\n\nArray of list objects      See also  \nstr.rsplit, split\n\n  \n\n"}, {"name": "char.rstrip()", "path": "reference/generated/numpy.char.rstrip", "type": "numpy.char.rstrip", "text": "numpy.char.rstrip   char.rstrip(a, chars=None)[source]\n \nFor each element in a, return a copy with the trailing characters removed. Calls str.rstrip element-wise.  Parameters \n \naarray-like of str or unicode\n\n\ncharsstr or unicode, optional\n\n\nThe chars argument is a string specifying the set of characters to be removed. If omitted or None, the chars argument defaults to removing whitespace. The chars argument is not a suffix; rather, all combinations of its values are stripped.    Returns \n \noutndarray\n\n\nOutput array of str or unicode, depending on input type      See also  str.rstrip\n  Examples >>> c = np.array(['aAaAaA', 'abBABba'], dtype='S7'); c\narray(['aAaAaA', 'abBABba'],\n    dtype='|S7')\n>>> np.char.rstrip(c, b'a')\narray(['aAaAaA', 'abBABb'],\n    dtype='|S7')\n>>> np.char.rstrip(c, b'A')\narray(['aAaAa', 'abBABba'],\n    dtype='|S7')\n \n\n"}, {"name": "char.split()", "path": "reference/generated/numpy.char.split", "type": "numpy.char.split", "text": "numpy.char.split   char.split(a, sep=None, maxsplit=None)[source]\n \nFor each element in a, return a list of the words in the string, using sep as the delimiter string. Calls str.split element-wise.  Parameters \n \naarray_like of str or unicode\n\n\nsepstr or unicode, optional\n\n\nIf sep is not specified or None, any whitespace string is a separator.  \nmaxsplitint, optional\n\n\nIf maxsplit is given, at most maxsplit splits are done.    Returns \n \noutndarray\n\n\nArray of list objects      See also  \nstr.split, rsplit\n\n  \n\n"}, {"name": "char.splitlines()", "path": "reference/generated/numpy.char.splitlines", "type": "numpy.char.splitlines", "text": "numpy.char.splitlines   char.splitlines(a, keepends=None)[source]\n \nFor each element in a, return a list of the lines in the element, breaking at line boundaries. Calls str.splitlines element-wise.  Parameters \n \naarray_like of str or unicode\n\n\nkeependsbool, optional\n\n\nLine breaks are not included in the resulting list unless keepends is given and true.    Returns \n \noutndarray\n\n\nArray of list objects      See also  str.splitlines\n  \n\n"}, {"name": "char.startswith()", "path": "reference/generated/numpy.char.startswith", "type": "numpy.char.startswith", "text": "numpy.char.startswith   char.startswith(a, prefix, start=0, end=None)[source]\n \nReturns a boolean array which is True where the string element in a starts with prefix, otherwise False. Calls str.startswith element-wise.  Parameters \n \naarray_like of str or unicode\n\n\nprefixstr\n\n\nstart, endint, optional\n\n\nWith optional start, test beginning at that position. With optional end, stop comparing at that position.    Returns \n \noutndarray\n\n\nArray of booleans      See also  str.startswith\n  \n\n"}, {"name": "char.str_len()", "path": "reference/generated/numpy.char.str_len", "type": "numpy.char.str_len", "text": "numpy.char.str_len   char.str_len(a)[source]\n \nReturn len(a) element-wise.  Parameters \n \naarray_like of str or unicode\n\n  Returns \n \noutndarray\n\n\nOutput array of integers      See also  builtins.len\n  \n\n"}, {"name": "char.strip()", "path": "reference/generated/numpy.char.strip", "type": "numpy.char.strip", "text": "numpy.char.strip   char.strip(a, chars=None)[source]\n \nFor each element in a, return a copy with the leading and trailing characters removed. Calls str.strip element-wise.  Parameters \n \naarray-like of str or unicode\n\n\ncharsstr or unicode, optional\n\n\nThe chars argument is a string specifying the set of characters to be removed. If omitted or None, the chars argument defaults to removing whitespace. The chars argument is not a prefix or suffix; rather, all combinations of its values are stripped.    Returns \n \noutndarray\n\n\nOutput array of str or unicode, depending on input type      See also  str.strip\n  Examples >>> c = np.array(['aAaAaA', '  aA  ', 'abBABba'])\n>>> c\narray(['aAaAaA', '  aA  ', 'abBABba'], dtype='<U7')\n>>> np.char.strip(c)\narray(['aAaAaA', 'aA', 'abBABba'], dtype='<U7')\n>>> np.char.strip(c, 'a') # 'a' unstripped from c[1] because whitespace leads\narray(['AaAaA', '  aA  ', 'bBABb'], dtype='<U7')\n>>> np.char.strip(c, 'A') # 'A' unstripped from c[1] because (unprinted) ws trails\narray(['aAaAa', '  aA  ', 'abBABba'], dtype='<U7')\n \n\n"}, {"name": "char.swapcase()", "path": "reference/generated/numpy.char.swapcase", "type": "numpy.char.swapcase", "text": "numpy.char.swapcase   char.swapcase(a)[source]\n \nReturn element-wise a copy of the string with uppercase characters converted to lowercase and vice versa. Calls str.swapcase element-wise. For 8-bit strings, this method is locale-dependent.  Parameters \n \naarray_like, {str, unicode}\n\n\nInput array.    Returns \n \noutndarray, {str, unicode}\n\n\nOutput array of str or unicode, depending on input type      See also  str.swapcase\n  Examples >>> c=np.array(['a1B c','1b Ca','b Ca1','cA1b'],'S5'); c\narray(['a1B c', '1b Ca', 'b Ca1', 'cA1b'],\n    dtype='|S5')\n>>> np.char.swapcase(c)\narray(['A1b C', '1B cA', 'B cA1', 'Ca1B'],\n    dtype='|S5')\n \n\n"}, {"name": "char.title()", "path": "reference/generated/numpy.char.title", "type": "numpy.char.title", "text": "numpy.char.title   char.title(a)[source]\n \nReturn element-wise title cased version of string or unicode. Title case words start with uppercase characters, all remaining cased characters are lowercase. Calls str.title element-wise. For 8-bit strings, this method is locale-dependent.  Parameters \n \naarray_like, {str, unicode}\n\n\nInput array.    Returns \n \noutndarray\n\n\nOutput array of str or unicode, depending on input type      See also  str.title\n  Examples >>> c=np.array(['a1b c','1b ca','b ca1','ca1b'],'S5'); c\narray(['a1b c', '1b ca', 'b ca1', 'ca1b'],\n    dtype='|S5')\n>>> np.char.title(c)\narray(['A1B C', '1B Ca', 'B Ca1', 'Ca1B'],\n    dtype='|S5')\n \n\n"}, {"name": "char.translate()", "path": "reference/generated/numpy.char.translate", "type": "numpy.char.translate", "text": "numpy.char.translate   char.translate(a, table, deletechars=None)[source]\n \nFor each element in a, return a copy of the string where all characters occurring in the optional argument deletechars are removed, and the remaining characters have been mapped through the given translation table. Calls str.translate element-wise.  Parameters \n \naarray-like of str or unicode\n\n\ntablestr of length 256\n\n\ndeletecharsstr\n\n  Returns \n \noutndarray\n\n\nOutput array of str or unicode, depending on input type      See also  str.translate\n  \n\n"}, {"name": "char.upper()", "path": "reference/generated/numpy.char.upper", "type": "numpy.char.upper", "text": "numpy.char.upper   char.upper(a)[source]\n \nReturn an array with the elements converted to uppercase. Calls str.upper element-wise. For 8-bit strings, this method is locale-dependent.  Parameters \n \naarray_like, {str, unicode}\n\n\nInput array.    Returns \n \noutndarray, {str, unicode}\n\n\nOutput array of str or unicode, depending on input type      See also  str.upper\n  Examples >>> c = np.array(['a1b c', '1bca', 'bca1']); c\narray(['a1b c', '1bca', 'bca1'], dtype='<U5')\n>>> np.char.upper(c)\narray(['A1B C', '1BCA', 'BCA1'], dtype='<U5')\n \n\n"}, {"name": "char.zfill()", "path": "reference/generated/numpy.char.zfill", "type": "numpy.char.zfill", "text": "numpy.char.zfill   char.zfill(a, width)[source]\n \nReturn the numeric string left-filled with zeros Calls str.zfill element-wise.  Parameters \n \naarray_like, {str, unicode}\n\n\nInput array.  \nwidthint\n\n\nWidth of string to left-fill elements in a.    Returns \n \noutndarray, {str, unicode}\n\n\nOutput array of str or unicode, depending on input type      See also  str.zfill\n  \n\n"}, {"name": "chararray.argsort()", "path": "reference/generated/numpy.chararray.argsort", "type": "numpy.chararray.argsort", "text": "numpy.chararray.argsort method   chararray.argsort(axis=- 1, kind=None, order=None)[source]\n \nReturns the indices that would sort this array. Refer to numpy.argsort for full documentation.  See also  numpy.argsort\n\nequivalent function    \n\n"}, {"name": "chararray.astype()", "path": "reference/generated/numpy.chararray.astype", "type": "numpy.chararray.astype", "text": "numpy.chararray.astype method   chararray.astype(dtype, order='K', casting='unsafe', subok=True, copy=True)\n \nCopy of the array, cast to a specified type.  Parameters \n \ndtypestr or dtype\n\n\nTypecode or data-type to which the array is cast.  \norder{\u2018C\u2019, \u2018F\u2019, \u2018A\u2019, \u2018K\u2019}, optional\n\n\nControls the memory layout order of the result. \u2018C\u2019 means C order, \u2018F\u2019 means Fortran order, \u2018A\u2019 means \u2018F\u2019 order if all the arrays are Fortran contiguous, \u2018C\u2019 order otherwise, and \u2018K\u2019 means as close to the order the array elements appear in memory as possible. Default is \u2018K\u2019.  \ncasting{\u2018no\u2019, \u2018equiv\u2019, \u2018safe\u2019, \u2018same_kind\u2019, \u2018unsafe\u2019}, optional\n\n\nControls what kind of data casting may occur. Defaults to \u2018unsafe\u2019 for backwards compatibility.  \u2018no\u2019 means the data types should not be cast at all. \u2018equiv\u2019 means only byte-order changes are allowed. \u2018safe\u2019 means only casts which can preserve values are allowed. \u2018same_kind\u2019 means only safe casts or casts within a kind, like float64 to float32, are allowed. \u2018unsafe\u2019 means any data conversions may be done.   \nsubokbool, optional\n\n\nIf True, then sub-classes will be passed-through (default), otherwise the returned array will be forced to be a base-class array.  \ncopybool, optional\n\n\nBy default, astype always returns a newly allocated array. If this is set to false, and the dtype, order, and subok requirements are satisfied, the input array is returned instead of a copy.    Returns \n \narr_tndarray\n\n\nUnless copy is False and the other conditions for returning the input array are satisfied (see description for copy input parameter), arr_t is a new array of the same shape as the input array, with dtype, order given by dtype, order.    Raises \n ComplexWarning\n\nWhen casting from complex to float or int. To avoid this, one should use a.real.astype(t).     Notes  Changed in version 1.17.0: Casting between a simple data type and a structured one is possible only for \u201cunsafe\u201d casting. Casting to multiple fields is allowed, but casting from multiple fields is not.   Changed in version 1.9.0: Casting from numeric to string types in \u2018safe\u2019 casting mode requires that the string dtype length is long enough to store the max integer/float value converted.  Examples >>> x = np.array([1, 2, 2.5])\n>>> x\narray([1. ,  2. ,  2.5])\n >>> x.astype(int)\narray([1, 2, 2])\n \n\n"}, {"name": "chararray.base", "path": "reference/generated/numpy.chararray.base", "type": "String operations", "text": "numpy.chararray.base attribute   chararray.base\n \nBase object if memory is from some other object. Examples The base of an array that owns its memory is None: >>> x = np.array([1,2,3,4])\n>>> x.base is None\nTrue\n Slicing creates a view, whose memory is shared with x: >>> y = x[2:]\n>>> y.base is x\nTrue\n \n\n"}, {"name": "chararray.copy()", "path": "reference/generated/numpy.chararray.copy", "type": "numpy.chararray.copy", "text": "numpy.chararray.copy method   chararray.copy(order='C')\n \nReturn a copy of the array.  Parameters \n \norder{\u2018C\u2019, \u2018F\u2019, \u2018A\u2019, \u2018K\u2019}, optional\n\n\nControls the memory layout of the copy. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. \u2018K\u2019 means match the layout of a as closely as possible. (Note that this function and numpy.copy are very similar but have different default values for their order= arguments, and this function always passes sub-classes through.)      See also  numpy.copy\n\nSimilar function with different default behavior  numpy.copyto\n  Notes This function is the preferred method for creating an array copy. The function numpy.copy is similar, but it defaults to using order \u2018K\u2019, and will not pass sub-classes through by default. Examples >>> x = np.array([[1,2,3],[4,5,6]], order='F')\n >>> y = x.copy()\n >>> x.fill(0)\n >>> x\narray([[0, 0, 0],\n       [0, 0, 0]])\n >>> y\narray([[1, 2, 3],\n       [4, 5, 6]])\n >>> y.flags['C_CONTIGUOUS']\nTrue\n \n\n"}, {"name": "chararray.count()", "path": "reference/generated/numpy.chararray.count", "type": "numpy.chararray.count", "text": "numpy.chararray.count method   chararray.count(sub, start=0, end=None)[source]\n \nReturns an array with the number of non-overlapping occurrences of substring sub in the range [start, end].  See also  char.count\n  \n\n"}, {"name": "chararray.ctypes", "path": "reference/generated/numpy.chararray.ctypes", "type": "String operations", "text": "numpy.chararray.ctypes attribute   chararray.ctypes\n \nAn object to simplify the interaction of the array with the ctypes module. This attribute creates an object that makes it easier to use arrays when calling shared libraries with the ctypes module. The returned object has, among others, data, shape, and strides attributes (see Notes below) which themselves return ctypes objects that can be used as arguments to a shared library.  Parameters \n None\n  Returns \n \ncPython object\n\n\nPossessing attributes data, shape, strides, etc.      See also  numpy.ctypeslib\n  Notes Below are the public attributes of this object which were documented in \u201cGuide to NumPy\u201d (we have omitted undocumented public attributes, as well as documented private attributes):   _ctypes.data\n \nA pointer to the memory area of the array as a Python integer. This memory area may contain data that is not aligned, or not in correct byte-order. The memory area may not even be writeable. The array flags and data-type of this array should be respected when passing this attribute to arbitrary C-code to avoid trouble that can include Python crashing. User Beware! The value of this attribute is exactly the same as self._array_interface_['data'][0]. Note that unlike data_as, a reference will not be kept to the array: code like ctypes.c_void_p((a + b).ctypes.data) will result in a pointer to a deallocated array, and should be spelt (a + b).ctypes.data_as(ctypes.c_void_p) \n   _ctypes.shape\n \n(c_intp*self.ndim): A ctypes array of length self.ndim where the basetype is the C-integer corresponding to dtype('p') on this platform (see c_intp). This base-type could be ctypes.c_int, ctypes.c_long, or ctypes.c_longlong depending on the platform. The ctypes array contains the shape of the underlying array. \n   _ctypes.strides\n \n(c_intp*self.ndim): A ctypes array of length self.ndim where the basetype is the same as for the shape attribute. This ctypes array contains the strides information from the underlying array. This strides information is important for showing how many bytes must be jumped to get to the next element in the array. \n   _ctypes.data_as(obj)[source]\n \nReturn the data pointer cast to a particular c-types object. For example, calling self._as_parameter_ is equivalent to self.data_as(ctypes.c_void_p). Perhaps you want to use the data as a pointer to a ctypes array of floating-point data: self.data_as(ctypes.POINTER(ctypes.c_double)). The returned pointer will keep a reference to the array. \n   _ctypes.shape_as(obj)[source]\n \nReturn the shape tuple as an array of some other c-types type. For example: self.shape_as(ctypes.c_short). \n   _ctypes.strides_as(obj)[source]\n \nReturn the strides tuple as an array of some other c-types type. For example: self.strides_as(ctypes.c_longlong). \n If the ctypes module is not available, then the ctypes attribute of array objects still returns something useful, but ctypes objects are not returned and errors may be raised instead. In particular, the object will still have the as_parameter attribute which will return an integer equal to the data attribute. Examples >>> import ctypes\n>>> x = np.array([[0, 1], [2, 3]], dtype=np.int32)\n>>> x\narray([[0, 1],\n       [2, 3]], dtype=int32)\n>>> x.ctypes.data\n31962608 # may vary\n>>> x.ctypes.data_as(ctypes.POINTER(ctypes.c_uint32))\n<__main__.LP_c_uint object at 0x7ff2fc1fc200> # may vary\n>>> x.ctypes.data_as(ctypes.POINTER(ctypes.c_uint32)).contents\nc_uint(0)\n>>> x.ctypes.data_as(ctypes.POINTER(ctypes.c_uint64)).contents\nc_ulong(4294967296)\n>>> x.ctypes.shape\n<numpy.core._internal.c_long_Array_2 object at 0x7ff2fc1fce60> # may vary\n>>> x.ctypes.strides\n<numpy.core._internal.c_long_Array_2 object at 0x7ff2fc1ff320> # may vary\n \n\n"}, {"name": "chararray.data", "path": "reference/generated/numpy.chararray.data", "type": "String operations", "text": "numpy.chararray.data attribute   chararray.data\n \nPython buffer object pointing to the start of the array\u2019s data. \n\n"}, {"name": "chararray.decode()", "path": "reference/generated/numpy.chararray.decode", "type": "numpy.chararray.decode", "text": "numpy.chararray.decode method   chararray.decode(encoding=None, errors=None)[source]\n \nCalls str.decode element-wise.  See also  char.decode\n  \n\n"}, {"name": "chararray.dump()", "path": "reference/generated/numpy.chararray.dump", "type": "numpy.chararray.dump", "text": "numpy.chararray.dump method   chararray.dump(file)\n \nDump a pickle of the array to the specified file. The array can be read back with pickle.load or numpy.load.  Parameters \n \nfilestr or Path\n\n\nA string naming the dump file.  Changed in version 1.17.0: pathlib.Path objects are now accepted.      \n\n"}, {"name": "chararray.dumps()", "path": "reference/generated/numpy.chararray.dumps", "type": "numpy.chararray.dumps", "text": "numpy.chararray.dumps method   chararray.dumps()\n \nReturns the pickle of the array as a string. pickle.loads will convert the string back to an array.  Parameters \n None\n   \n\n"}, {"name": "chararray.encode()", "path": "reference/generated/numpy.chararray.encode", "type": "numpy.chararray.encode", "text": "numpy.chararray.encode method   chararray.encode(encoding=None, errors=None)[source]\n \nCalls str.encode element-wise.  See also  char.encode\n  \n\n"}, {"name": "chararray.endswith()", "path": "reference/generated/numpy.chararray.endswith", "type": "numpy.chararray.endswith", "text": "numpy.chararray.endswith method   chararray.endswith(suffix, start=0, end=None)[source]\n \nReturns a boolean array which is True where the string element in self ends with suffix, otherwise False.  See also  char.endswith\n  \n\n"}, {"name": "chararray.expandtabs()", "path": "reference/generated/numpy.chararray.expandtabs", "type": "numpy.chararray.expandtabs", "text": "numpy.chararray.expandtabs method   chararray.expandtabs(tabsize=8)[source]\n \nReturn a copy of each string element where all tab characters are replaced by one or more spaces.  See also  char.expandtabs\n  \n\n"}, {"name": "chararray.fill()", "path": "reference/generated/numpy.chararray.fill", "type": "numpy.chararray.fill", "text": "numpy.chararray.fill method   chararray.fill(value)\n \nFill the array with a scalar value.  Parameters \n \nvaluescalar\n\n\nAll elements of a will be assigned this value.     Examples >>> a = np.array([1, 2])\n>>> a.fill(0)\n>>> a\narray([0, 0])\n>>> a = np.empty(2)\n>>> a.fill(1)\n>>> a\narray([1.,  1.])\n \n\n"}, {"name": "chararray.find()", "path": "reference/generated/numpy.chararray.find", "type": "numpy.chararray.find", "text": "numpy.chararray.find method   chararray.find(sub, start=0, end=None)[source]\n \nFor each element, return the lowest index in the string where substring sub is found.  See also  char.find\n  \n\n"}, {"name": "chararray.flags", "path": "reference/generated/numpy.chararray.flags", "type": "String operations", "text": "numpy.chararray.flags attribute   chararray.flags\n \nInformation about the memory layout of the array. Notes The flags object can be accessed dictionary-like (as in a.flags['WRITEABLE']), or by using lowercased attribute names (as in a.flags.writeable). Short flag names are only supported in dictionary access. Only the WRITEBACKIFCOPY, UPDATEIFCOPY, WRITEABLE, and ALIGNED flags can be changed by the user, via direct assignment to the attribute or dictionary entry, or by calling ndarray.setflags. The array flags cannot be set arbitrarily:  UPDATEIFCOPY can only be set False. WRITEBACKIFCOPY can only be set False. ALIGNED can only be set True if the data is truly aligned. WRITEABLE can only be set True if the array owns its own memory or the ultimate owner of the memory exposes a writeable buffer interface or is a string.  Arrays can be both C-style and Fortran-style contiguous simultaneously. This is clear for 1-dimensional arrays, but can also be true for higher dimensional arrays. Even for contiguous arrays a stride for a given dimension arr.strides[dim] may be arbitrary if arr.shape[dim] == 1 or the array has no elements. It does not generally hold that self.strides[-1] == self.itemsize for C-style contiguous arrays or self.strides[0] == self.itemsize for Fortran-style contiguous arrays is true.  Attributes \n C_CONTIGUOUS (C)\n\nThe data is in a single, C-style contiguous segment.  F_CONTIGUOUS (F)\n\nThe data is in a single, Fortran-style contiguous segment.  OWNDATA (O)\n\nThe array owns the memory it uses or borrows it from another object.  WRITEABLE (W)\n\nThe data area can be written to. Setting this to False locks the data, making it read-only. A view (slice, etc.) inherits WRITEABLE from its base array at creation time, but a view of a writeable array may be subsequently locked while the base array remains writeable. (The opposite is not true, in that a view of a locked array may not be made writeable. However, currently, locking a base object does not lock any views that already reference it, so under that circumstance it is possible to alter the contents of a locked array via a previously created writeable view onto it.) Attempting to change a non-writeable array raises a RuntimeError exception.  ALIGNED (A)\n\nThe data and all elements are aligned appropriately for the hardware.  WRITEBACKIFCOPY (X)\n\nThis array is a copy of some other array. The C-API function PyArray_ResolveWritebackIfCopy must be called before deallocating to the base array will be updated with the contents of this array.  UPDATEIFCOPY (U)\n\n(Deprecated, use WRITEBACKIFCOPY) This array is a copy of some other array. When this array is deallocated, the base array will be updated with the contents of this array.  FNC\n\nF_CONTIGUOUS and not C_CONTIGUOUS.  FORC\n\nF_CONTIGUOUS or C_CONTIGUOUS (one-segment test).  BEHAVED (B)\n\nALIGNED and WRITEABLE.  CARRAY (CA)\n\nBEHAVED and C_CONTIGUOUS.  FARRAY (FA)\n\nBEHAVED and F_CONTIGUOUS and not C_CONTIGUOUS.     \n\n"}, {"name": "chararray.flat", "path": "reference/generated/numpy.chararray.flat", "type": "String operations", "text": "numpy.chararray.flat attribute   chararray.flat\n \nA 1-D iterator over the array. This is a numpy.flatiter instance, which acts similarly to, but is not a subclass of, Python\u2019s built-in iterator object.  See also  flatten\n\nReturn a copy of the array collapsed into one dimension.  flatiter\n  Examples >>> x = np.arange(1, 7).reshape(2, 3)\n>>> x\narray([[1, 2, 3],\n       [4, 5, 6]])\n>>> x.flat[3]\n4\n>>> x.T\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n>>> x.T.flat[3]\n5\n>>> type(x.flat)\n<class 'numpy.flatiter'>\n An assignment example: >>> x.flat = 3; x\narray([[3, 3, 3],\n       [3, 3, 3]])\n>>> x.flat[[1,4]] = 1; x\narray([[3, 1, 3],\n       [3, 1, 3]])\n \n\n"}, {"name": "chararray.flatten()", "path": "reference/generated/numpy.chararray.flatten", "type": "numpy.chararray.flatten", "text": "numpy.chararray.flatten method   chararray.flatten(order='C')\n \nReturn a copy of the array collapsed into one dimension.  Parameters \n \norder{\u2018C\u2019, \u2018F\u2019, \u2018A\u2019, \u2018K\u2019}, optional\n\n\n\u2018C\u2019 means to flatten in row-major (C-style) order. \u2018F\u2019 means to flatten in column-major (Fortran- style) order. \u2018A\u2019 means to flatten in column-major order if a is Fortran contiguous in memory, row-major order otherwise. \u2018K\u2019 means to flatten a in the order the elements occur in memory. The default is \u2018C\u2019.    Returns \n \nyndarray\n\n\nA copy of the input array, flattened to one dimension.      See also  ravel\n\nReturn a flattened array.  flat\n\nA 1-D flat iterator over the array.    Examples >>> a = np.array([[1,2], [3,4]])\n>>> a.flatten()\narray([1, 2, 3, 4])\n>>> a.flatten('F')\narray([1, 3, 2, 4])\n \n\n"}, {"name": "chararray.getfield()", "path": "reference/generated/numpy.chararray.getfield", "type": "numpy.chararray.getfield", "text": "numpy.chararray.getfield method   chararray.getfield(dtype, offset=0)\n \nReturns a field of the given array as a certain type. A field is a view of the array data with a given data-type. The values in the view are determined by the given type and the offset into the current array in bytes. The offset needs to be such that the view dtype fits in the array dtype; for example an array of dtype complex128 has 16-byte elements. If taking a view with a 32-bit integer (4 bytes), the offset needs to be between 0 and 12 bytes.  Parameters \n \ndtypestr or dtype\n\n\nThe data type of the view. The dtype size of the view can not be larger than that of the array itself.  \noffsetint\n\n\nNumber of bytes to skip before beginning the element view.     Examples >>> x = np.diag([1.+1.j]*2)\n>>> x[1, 1] = 2 + 4.j\n>>> x\narray([[1.+1.j,  0.+0.j],\n       [0.+0.j,  2.+4.j]])\n>>> x.getfield(np.float64)\narray([[1.,  0.],\n       [0.,  2.]])\n By choosing an offset of 8 bytes we can select the complex part of the array for our view: >>> x.getfield(np.float64, offset=8)\narray([[1.,  0.],\n       [0.,  4.]])\n \n\n"}, {"name": "chararray.index()", "path": "reference/generated/numpy.chararray.index", "type": "numpy.chararray.index", "text": "numpy.chararray.index method   chararray.index(sub, start=0, end=None)[source]\n \nLike find, but raises ValueError when the substring is not found.  See also  char.index\n  \n\n"}, {"name": "chararray.isalnum()", "path": "reference/generated/numpy.chararray.isalnum", "type": "numpy.chararray.isalnum", "text": "numpy.chararray.isalnum method   chararray.isalnum()[source]\n \nReturns true for each element if all characters in the string are alphanumeric and there is at least one character, false otherwise.  See also  char.isalnum\n  \n\n"}, {"name": "chararray.isalpha()", "path": "reference/generated/numpy.chararray.isalpha", "type": "numpy.chararray.isalpha", "text": "numpy.chararray.isalpha method   chararray.isalpha()[source]\n \nReturns true for each element if all characters in the string are alphabetic and there is at least one character, false otherwise.  See also  char.isalpha\n  \n\n"}, {"name": "chararray.isdecimal()", "path": "reference/generated/numpy.chararray.isdecimal", "type": "numpy.chararray.isdecimal", "text": "numpy.chararray.isdecimal method   chararray.isdecimal()[source]\n \nFor each element in self, return True if there are only decimal characters in the element.  See also  char.isdecimal\n  \n\n"}, {"name": "chararray.isdigit()", "path": "reference/generated/numpy.chararray.isdigit", "type": "numpy.chararray.isdigit", "text": "numpy.chararray.isdigit method   chararray.isdigit()[source]\n \nReturns true for each element if all characters in the string are digits and there is at least one character, false otherwise.  See also  char.isdigit\n  \n\n"}, {"name": "chararray.islower()", "path": "reference/generated/numpy.chararray.islower", "type": "numpy.chararray.islower", "text": "numpy.chararray.islower method   chararray.islower()[source]\n \nReturns true for each element if all cased characters in the string are lowercase and there is at least one cased character, false otherwise.  See also  char.islower\n  \n\n"}, {"name": "chararray.isnumeric()", "path": "reference/generated/numpy.chararray.isnumeric", "type": "numpy.chararray.isnumeric", "text": "numpy.chararray.isnumeric method   chararray.isnumeric()[source]\n \nFor each element in self, return True if there are only numeric characters in the element.  See also  char.isnumeric\n  \n\n"}, {"name": "chararray.isspace()", "path": "reference/generated/numpy.chararray.isspace", "type": "numpy.chararray.isspace", "text": "numpy.chararray.isspace method   chararray.isspace()[source]\n \nReturns true for each element if there are only whitespace characters in the string and there is at least one character, false otherwise.  See also  char.isspace\n  \n\n"}, {"name": "chararray.istitle()", "path": "reference/generated/numpy.chararray.istitle", "type": "numpy.chararray.istitle", "text": "numpy.chararray.istitle method   chararray.istitle()[source]\n \nReturns true for each element if the element is a titlecased string and there is at least one character, false otherwise.  See also  char.istitle\n  \n\n"}, {"name": "chararray.isupper()", "path": "reference/generated/numpy.chararray.isupper", "type": "numpy.chararray.isupper", "text": "numpy.chararray.isupper method   chararray.isupper()[source]\n \nReturns true for each element if all cased characters in the string are uppercase and there is at least one character, false otherwise.  See also  char.isupper\n  \n\n"}, {"name": "chararray.item()", "path": "reference/generated/numpy.chararray.item", "type": "numpy.chararray.item", "text": "numpy.chararray.item method   chararray.item(*args)\n \nCopy an element of an array to a standard Python scalar and return it.  Parameters \n \n*argsArguments (variable number and type)\n\n\n none: in this case, the method only works for arrays with one element (a.size == 1), which element is copied into a standard Python scalar object and returned. int_type: this argument is interpreted as a flat index into the array, specifying which element to copy and return. tuple of int_types: functions as does a single int_type argument, except that the argument is interpreted as an nd-index into the array.     Returns \n \nzStandard Python scalar object\n\n\nA copy of the specified element of the array as a suitable Python scalar     Notes When the data type of a is longdouble or clongdouble, item() returns a scalar array object because there is no available Python scalar that would not lose information. Void arrays return a buffer object for item(), unless fields are defined, in which case a tuple is returned. item is very similar to a[args], except, instead of an array scalar, a standard Python scalar is returned. This can be useful for speeding up access to elements of the array and doing arithmetic on elements of the array using Python\u2019s optimized math. Examples >>> np.random.seed(123)\n>>> x = np.random.randint(9, size=(3, 3))\n>>> x\narray([[2, 2, 6],\n       [1, 3, 6],\n       [1, 0, 1]])\n>>> x.item(3)\n1\n>>> x.item(7)\n0\n>>> x.item((0, 1))\n2\n>>> x.item((2, 2))\n1\n \n\n"}, {"name": "chararray.itemsize", "path": "reference/generated/numpy.chararray.itemsize", "type": "String operations", "text": "numpy.chararray.itemsize attribute   chararray.itemsize\n \nLength of one array element in bytes. Examples >>> x = np.array([1,2,3], dtype=np.float64)\n>>> x.itemsize\n8\n>>> x = np.array([1,2,3], dtype=np.complex128)\n>>> x.itemsize\n16\n \n\n"}, {"name": "chararray.join()", "path": "reference/generated/numpy.chararray.join", "type": "numpy.chararray.join", "text": "numpy.chararray.join method   chararray.join(seq)[source]\n \nReturn a string which is the concatenation of the strings in the sequence seq.  See also  char.join\n  \n\n"}, {"name": "chararray.ljust()", "path": "reference/generated/numpy.chararray.ljust", "type": "numpy.chararray.ljust", "text": "numpy.chararray.ljust method   chararray.ljust(width, fillchar=' ')[source]\n \nReturn an array with the elements of self left-justified in a string of length width.  See also  char.ljust\n  \n\n"}, {"name": "chararray.lower()", "path": "reference/generated/numpy.chararray.lower", "type": "numpy.chararray.lower", "text": "numpy.chararray.lower method   chararray.lower()[source]\n \nReturn an array with the elements of self converted to lowercase.  See also  char.lower\n  \n\n"}, {"name": "chararray.lstrip()", "path": "reference/generated/numpy.chararray.lstrip", "type": "numpy.chararray.lstrip", "text": "numpy.chararray.lstrip method   chararray.lstrip(chars=None)[source]\n \nFor each element in self, return a copy with the leading characters removed.  See also  char.lstrip\n  \n\n"}, {"name": "chararray.nbytes", "path": "reference/generated/numpy.chararray.nbytes", "type": "String operations", "text": "numpy.chararray.nbytes attribute   chararray.nbytes\n \nTotal bytes consumed by the elements of the array. Notes Does not include memory consumed by non-element attributes of the array object. Examples >>> x = np.zeros((3,5,2), dtype=np.complex128)\n>>> x.nbytes\n480\n>>> np.prod(x.shape) * x.itemsize\n480\n \n\n"}, {"name": "chararray.ndim", "path": "reference/generated/numpy.chararray.ndim", "type": "String operations", "text": "numpy.chararray.ndim attribute   chararray.ndim\n \nNumber of array dimensions. Examples >>> x = np.array([1, 2, 3])\n>>> x.ndim\n1\n>>> y = np.zeros((2, 3, 4))\n>>> y.ndim\n3\n \n\n"}, {"name": "chararray.nonzero()", "path": "reference/generated/numpy.chararray.nonzero", "type": "numpy.chararray.nonzero", "text": "numpy.chararray.nonzero method   chararray.nonzero()\n \nReturn the indices of the elements that are non-zero. Refer to numpy.nonzero for full documentation.  See also  numpy.nonzero\n\nequivalent function    \n\n"}, {"name": "chararray.put()", "path": "reference/generated/numpy.chararray.put", "type": "numpy.chararray.put", "text": "numpy.chararray.put method   chararray.put(indices, values, mode='raise')\n \nSet a.flat[n] = values[n] for all n in indices. Refer to numpy.put for full documentation.  See also  numpy.put\n\nequivalent function    \n\n"}, {"name": "chararray.ravel()", "path": "reference/generated/numpy.chararray.ravel", "type": "numpy.chararray.ravel", "text": "numpy.chararray.ravel method   chararray.ravel([order])\n \nReturn a flattened array. Refer to numpy.ravel for full documentation.  See also  numpy.ravel\n\nequivalent function  ndarray.flat\n\na flat iterator on the array.    \n\n"}, {"name": "chararray.repeat()", "path": "reference/generated/numpy.chararray.repeat", "type": "numpy.chararray.repeat", "text": "numpy.chararray.repeat method   chararray.repeat(repeats, axis=None)\n \nRepeat elements of an array. Refer to numpy.repeat for full documentation.  See also  numpy.repeat\n\nequivalent function    \n\n"}, {"name": "chararray.replace()", "path": "reference/generated/numpy.chararray.replace", "type": "numpy.chararray.replace", "text": "numpy.chararray.replace method   chararray.replace(old, new, count=None)[source]\n \nFor each element in self, return a copy of the string with all occurrences of substring old replaced by new.  See also  char.replace\n  \n\n"}, {"name": "chararray.reshape()", "path": "reference/generated/numpy.chararray.reshape", "type": "numpy.chararray.reshape", "text": "numpy.chararray.reshape method   chararray.reshape(shape, order='C')\n \nReturns an array containing the same data with a new shape. Refer to numpy.reshape for full documentation.  See also  numpy.reshape\n\nequivalent function    Notes Unlike the free function numpy.reshape, this method on ndarray allows the elements of the shape parameter to be passed in as separate arguments. For example, a.reshape(10, 11) is equivalent to a.reshape((10, 11)). \n\n"}, {"name": "chararray.resize()", "path": "reference/generated/numpy.chararray.resize", "type": "numpy.chararray.resize", "text": "numpy.chararray.resize method   chararray.resize(new_shape, refcheck=True)\n \nChange shape and size of array in-place.  Parameters \n \nnew_shapetuple of ints, or n ints\n\n\nShape of resized array.  \nrefcheckbool, optional\n\n\nIf False, reference count will not be checked. Default is True.    Returns \n None\n  Raises \n ValueError\n\nIf a does not own its own data or references or views to it exist, and the data memory must be changed. PyPy only: will always raise if the data memory must be changed, since there is no reliable way to determine if references or views to it exist.  SystemError\n\nIf the order keyword argument is specified. This behaviour is a bug in NumPy.      See also  resize\n\nReturn a new array with the specified shape.    Notes This reallocates space for the data area if necessary. Only contiguous arrays (data elements consecutive in memory) can be resized. The purpose of the reference count check is to make sure you do not use this array as a buffer for another Python object and then reallocate the memory. However, reference counts can increase in other ways so if you are sure that you have not shared the memory for this array with another Python object, then you may safely set refcheck to False. Examples Shrinking an array: array is flattened (in the order that the data are stored in memory), resized, and reshaped: >>> a = np.array([[0, 1], [2, 3]], order='C')\n>>> a.resize((2, 1))\n>>> a\narray([[0],\n       [1]])\n >>> a = np.array([[0, 1], [2, 3]], order='F')\n>>> a.resize((2, 1))\n>>> a\narray([[0],\n       [2]])\n Enlarging an array: as above, but missing entries are filled with zeros: >>> b = np.array([[0, 1], [2, 3]])\n>>> b.resize(2, 3) # new_shape parameter doesn't have to be a tuple\n>>> b\narray([[0, 1, 2],\n       [3, 0, 0]])\n Referencing an array prevents resizing\u2026 >>> c = a\n>>> a.resize((1, 1))\nTraceback (most recent call last):\n...\nValueError: cannot resize an array that references or is referenced ...\n Unless refcheck is False: >>> a.resize((1, 1), refcheck=False)\n>>> a\narray([[0]])\n>>> c\narray([[0]])\n \n\n"}, {"name": "chararray.rfind()", "path": "reference/generated/numpy.chararray.rfind", "type": "numpy.chararray.rfind", "text": "numpy.chararray.rfind method   chararray.rfind(sub, start=0, end=None)[source]\n \nFor each element in self, return the highest index in the string where substring sub is found, such that sub is contained within [start, end].  See also  char.rfind\n  \n\n"}, {"name": "chararray.rindex()", "path": "reference/generated/numpy.chararray.rindex", "type": "numpy.chararray.rindex", "text": "numpy.chararray.rindex method   chararray.rindex(sub, start=0, end=None)[source]\n \nLike rfind, but raises ValueError when the substring sub is not found.  See also  char.rindex\n  \n\n"}, {"name": "chararray.rjust()", "path": "reference/generated/numpy.chararray.rjust", "type": "numpy.chararray.rjust", "text": "numpy.chararray.rjust method   chararray.rjust(width, fillchar=' ')[source]\n \nReturn an array with the elements of self right-justified in a string of length width.  See also  char.rjust\n  \n\n"}, {"name": "chararray.rsplit()", "path": "reference/generated/numpy.chararray.rsplit", "type": "numpy.chararray.rsplit", "text": "numpy.chararray.rsplit method   chararray.rsplit(sep=None, maxsplit=None)[source]\n \nFor each element in self, return a list of the words in the string, using sep as the delimiter string.  See also  char.rsplit\n  \n\n"}, {"name": "chararray.rstrip()", "path": "reference/generated/numpy.chararray.rstrip", "type": "numpy.chararray.rstrip", "text": "numpy.chararray.rstrip method   chararray.rstrip(chars=None)[source]\n \nFor each element in self, return a copy with the trailing characters removed.  See also  char.rstrip\n  \n\n"}, {"name": "chararray.searchsorted()", "path": "reference/generated/numpy.chararray.searchsorted", "type": "numpy.chararray.searchsorted", "text": "numpy.chararray.searchsorted method   chararray.searchsorted(v, side='left', sorter=None)\n \nFind indices where elements of v should be inserted in a to maintain order. For full documentation, see numpy.searchsorted  See also  numpy.searchsorted\n\nequivalent function    \n\n"}, {"name": "chararray.setfield()", "path": "reference/generated/numpy.chararray.setfield", "type": "numpy.chararray.setfield", "text": "numpy.chararray.setfield method   chararray.setfield(val, dtype, offset=0)\n \nPut a value into a specified place in a field defined by a data-type. Place val into a\u2019s field defined by dtype and beginning offset bytes into the field.  Parameters \n \nvalobject\n\n\nValue to be placed in field.  \ndtypedtype object\n\n\nData-type of the field in which to place val.  \noffsetint, optional\n\n\nThe number of bytes into the field at which to place val.    Returns \n None\n    See also  getfield\n  Examples >>> x = np.eye(3)\n>>> x.getfield(np.float64)\narray([[1.,  0.,  0.],\n       [0.,  1.,  0.],\n       [0.,  0.,  1.]])\n>>> x.setfield(3, np.int32)\n>>> x.getfield(np.int32)\narray([[3, 3, 3],\n       [3, 3, 3],\n       [3, 3, 3]], dtype=int32)\n>>> x\narray([[1.0e+000, 1.5e-323, 1.5e-323],\n       [1.5e-323, 1.0e+000, 1.5e-323],\n       [1.5e-323, 1.5e-323, 1.0e+000]])\n>>> x.setfield(np.eye(3), np.int32)\n>>> x\narray([[1.,  0.,  0.],\n       [0.,  1.,  0.],\n       [0.,  0.,  1.]])\n \n\n"}, {"name": "chararray.setflags()", "path": "reference/generated/numpy.chararray.setflags", "type": "numpy.chararray.setflags", "text": "numpy.chararray.setflags method   chararray.setflags(write=None, align=None, uic=None)\n \nSet array flags WRITEABLE, ALIGNED, (WRITEBACKIFCOPY and UPDATEIFCOPY), respectively. These Boolean-valued flags affect how numpy interprets the memory area used by a (see Notes below). The ALIGNED flag can only be set to True if the data is actually aligned according to the type. The WRITEBACKIFCOPY and (deprecated) UPDATEIFCOPY flags can never be set to True. The flag WRITEABLE can only be set to True if the array owns its own memory, or the ultimate owner of the memory exposes a writeable buffer interface, or is a string. (The exception for string is made so that unpickling can be done without copying memory.)  Parameters \n \nwritebool, optional\n\n\nDescribes whether or not a can be written to.  \nalignbool, optional\n\n\nDescribes whether or not a is aligned properly for its type.  \nuicbool, optional\n\n\nDescribes whether or not a is a copy of another \u201cbase\u201d array.     Notes Array flags provide information about how the memory area used for the array is to be interpreted. There are 7 Boolean flags in use, only four of which can be changed by the user: WRITEBACKIFCOPY, UPDATEIFCOPY, WRITEABLE, and ALIGNED. WRITEABLE (W) the data area can be written to; ALIGNED (A) the data and strides are aligned appropriately for the hardware (as determined by the compiler); UPDATEIFCOPY (U) (deprecated), replaced by WRITEBACKIFCOPY; WRITEBACKIFCOPY (X) this array is a copy of some other array (referenced by .base). When the C-API function PyArray_ResolveWritebackIfCopy is called, the base array will be updated with the contents of this array. All flags can be accessed using the single (upper case) letter as well as the full name. Examples >>> y = np.array([[3, 1, 7],\n...               [2, 0, 0],\n...               [8, 5, 9]])\n>>> y\narray([[3, 1, 7],\n       [2, 0, 0],\n       [8, 5, 9]])\n>>> y.flags\n  C_CONTIGUOUS : True\n  F_CONTIGUOUS : False\n  OWNDATA : True\n  WRITEABLE : True\n  ALIGNED : True\n  WRITEBACKIFCOPY : False\n  UPDATEIFCOPY : False\n>>> y.setflags(write=0, align=0)\n>>> y.flags\n  C_CONTIGUOUS : True\n  F_CONTIGUOUS : False\n  OWNDATA : True\n  WRITEABLE : False\n  ALIGNED : False\n  WRITEBACKIFCOPY : False\n  UPDATEIFCOPY : False\n>>> y.setflags(uic=1)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nValueError: cannot set WRITEBACKIFCOPY flag to True\n \n\n"}, {"name": "chararray.size", "path": "reference/generated/numpy.chararray.size", "type": "String operations", "text": "numpy.chararray.size attribute   chararray.size\n \nNumber of elements in the array. Equal to np.prod(a.shape), i.e., the product of the array\u2019s dimensions. Notes a.size returns a standard arbitrary precision Python integer. This may not be the case with other methods of obtaining the same value (like the suggested np.prod(a.shape), which returns an instance of np.int_), and may be relevant if the value is used further in calculations that may overflow a fixed size integer type. Examples >>> x = np.zeros((3, 5, 2), dtype=np.complex128)\n>>> x.size\n30\n>>> np.prod(x.shape)\n30\n \n\n"}, {"name": "chararray.sort()", "path": "reference/generated/numpy.chararray.sort", "type": "numpy.chararray.sort", "text": "numpy.chararray.sort method   chararray.sort(axis=- 1, kind=None, order=None)\n \nSort an array in-place. Refer to numpy.sort for full documentation.  Parameters \n \naxisint, optional\n\n\nAxis along which to sort. Default is -1, which means sort along the last axis.  \nkind{\u2018quicksort\u2019, \u2018mergesort\u2019, \u2018heapsort\u2019, \u2018stable\u2019}, optional\n\n\nSorting algorithm. The default is \u2018quicksort\u2019. Note that both \u2018stable\u2019 and \u2018mergesort\u2019 use timsort under the covers and, in general, the actual implementation will vary with datatype. The \u2018mergesort\u2019 option is retained for backwards compatibility.  Changed in version 1.15.0: The \u2018stable\u2019 option was added.   \norderstr or list of str, optional\n\n\nWhen a is an array with fields defined, this argument specifies which fields to compare first, second, etc. A single field can be specified as a string, and not all fields need be specified, but unspecified fields will still be used, in the order in which they come up in the dtype, to break ties.      See also  numpy.sort\n\nReturn a sorted copy of an array.  numpy.argsort\n\nIndirect sort.  numpy.lexsort\n\nIndirect stable sort on multiple keys.  numpy.searchsorted\n\nFind elements in sorted array.  numpy.partition\n\nPartial sort.    Notes See numpy.sort for notes on the different sorting algorithms. Examples >>> a = np.array([[1,4], [3,1]])\n>>> a.sort(axis=1)\n>>> a\narray([[1, 4],\n       [1, 3]])\n>>> a.sort(axis=0)\n>>> a\narray([[1, 3],\n       [1, 4]])\n Use the order keyword to specify a field to use when sorting a structured array: >>> a = np.array([('a', 2), ('c', 1)], dtype=[('x', 'S1'), ('y', int)])\n>>> a.sort(order='y')\n>>> a\narray([(b'c', 1), (b'a', 2)],\n      dtype=[('x', 'S1'), ('y', '<i8')])\n \n\n"}, {"name": "chararray.split()", "path": "reference/generated/numpy.chararray.split", "type": "numpy.chararray.split", "text": "numpy.chararray.split method   chararray.split(sep=None, maxsplit=None)[source]\n \nFor each element in self, return a list of the words in the string, using sep as the delimiter string.  See also  char.split\n  \n\n"}, {"name": "chararray.splitlines()", "path": "reference/generated/numpy.chararray.splitlines", "type": "numpy.chararray.splitlines", "text": "numpy.chararray.splitlines method   chararray.splitlines(keepends=None)[source]\n \nFor each element in self, return a list of the lines in the element, breaking at line boundaries.  See also  char.splitlines\n  \n\n"}, {"name": "chararray.squeeze()", "path": "reference/generated/numpy.chararray.squeeze", "type": "numpy.chararray.squeeze", "text": "numpy.chararray.squeeze method   chararray.squeeze(axis=None)\n \nRemove axes of length one from a. Refer to numpy.squeeze for full documentation.  See also  numpy.squeeze\n\nequivalent function    \n\n"}, {"name": "chararray.startswith()", "path": "reference/generated/numpy.chararray.startswith", "type": "numpy.chararray.startswith", "text": "numpy.chararray.startswith method   chararray.startswith(prefix, start=0, end=None)[source]\n \nReturns a boolean array which is True where the string element in self starts with prefix, otherwise False.  See also  char.startswith\n  \n\n"}, {"name": "chararray.strides", "path": "reference/generated/numpy.chararray.strides", "type": "String operations", "text": "numpy.chararray.strides attribute   chararray.strides\n \nTuple of bytes to step in each dimension when traversing an array. The byte offset of element (i[0], i[1], ..., i[n]) in an array a is: offset = sum(np.array(i) * a.strides)\n A more detailed explanation of strides can be found in the \u201cndarray.rst\u201d file in the NumPy reference guide.  See also  numpy.lib.stride_tricks.as_strided\n  Notes Imagine an array of 32-bit integers (each 4 bytes): x = np.array([[0, 1, 2, 3, 4],\n              [5, 6, 7, 8, 9]], dtype=np.int32)\n This array is stored in memory as 40 bytes, one after the other (known as a contiguous block of memory). The strides of an array tell us how many bytes we have to skip in memory to move to the next position along a certain axis. For example, we have to skip 4 bytes (1 value) to move to the next column, but 20 bytes (5 values) to get to the same position in the next row. As such, the strides for the array x will be (20, 4). Examples >>> y = np.reshape(np.arange(2*3*4), (2,3,4))\n>>> y\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n>>> y.strides\n(48, 16, 4)\n>>> y[1,1,1]\n17\n>>> offset=sum(y.strides * np.array((1,1,1)))\n>>> offset/y.itemsize\n17\n >>> x = np.reshape(np.arange(5*6*7*8), (5,6,7,8)).transpose(2,3,1,0)\n>>> x.strides\n(32, 4, 224, 1344)\n>>> i = np.array([3,5,2,2])\n>>> offset = sum(i * x.strides)\n>>> x[3,5,2,2]\n813\n>>> offset / x.itemsize\n813\n \n\n"}, {"name": "chararray.strip()", "path": "reference/generated/numpy.chararray.strip", "type": "numpy.chararray.strip", "text": "numpy.chararray.strip method   chararray.strip(chars=None)[source]\n \nFor each element in self, return a copy with the leading and trailing characters removed.  See also  char.strip\n  \n\n"}, {"name": "chararray.swapaxes()", "path": "reference/generated/numpy.chararray.swapaxes", "type": "numpy.chararray.swapaxes", "text": "numpy.chararray.swapaxes method   chararray.swapaxes(axis1, axis2)\n \nReturn a view of the array with axis1 and axis2 interchanged. Refer to numpy.swapaxes for full documentation.  See also  numpy.swapaxes\n\nequivalent function    \n\n"}, {"name": "chararray.swapcase()", "path": "reference/generated/numpy.chararray.swapcase", "type": "numpy.chararray.swapcase", "text": "numpy.chararray.swapcase method   chararray.swapcase()[source]\n \nFor each element in self, return a copy of the string with uppercase characters converted to lowercase and vice versa.  See also  char.swapcase\n  \n\n"}, {"name": "chararray.T", "path": "reference/generated/numpy.chararray.t", "type": "String operations", "text": "numpy.chararray.T attribute   chararray.T\n \nThe transposed array. Same as self.transpose().  See also  transpose\n  Examples >>> x = np.array([[1.,2.],[3.,4.]])\n>>> x\narray([[ 1.,  2.],\n       [ 3.,  4.]])\n>>> x.T\narray([[ 1.,  3.],\n       [ 2.,  4.]])\n>>> x = np.array([1.,2.,3.,4.])\n>>> x\narray([ 1.,  2.,  3.,  4.])\n>>> x.T\narray([ 1.,  2.,  3.,  4.])\n \n\n"}, {"name": "chararray.take()", "path": "reference/generated/numpy.chararray.take", "type": "numpy.chararray.take", "text": "numpy.chararray.take method   chararray.take(indices, axis=None, out=None, mode='raise')\n \nReturn an array formed from the elements of a at the given indices. Refer to numpy.take for full documentation.  See also  numpy.take\n\nequivalent function    \n\n"}, {"name": "chararray.title()", "path": "reference/generated/numpy.chararray.title", "type": "numpy.chararray.title", "text": "numpy.chararray.title method   chararray.title()[source]\n \nFor each element in self, return a titlecased version of the string: words start with uppercase characters, all remaining cased characters are lowercase.  See also  char.title\n  \n\n"}, {"name": "chararray.tobytes()", "path": "reference/generated/numpy.chararray.tobytes", "type": "String operations", "text": "numpy.chararray.tobytes method   chararray.tobytes(order='C')\n \nConstruct Python bytes containing the raw data bytes in the array. Constructs Python bytes showing a copy of the raw contents of data memory. The bytes object is produced in C-order by default. This behavior is controlled by the order parameter.  New in version 1.9.0.   Parameters \n \norder{\u2018C\u2019, \u2018F\u2019, \u2018A\u2019}, optional\n\n\nControls the memory layout of the bytes object. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 (short for Any) means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. Default is \u2018C\u2019.    Returns \n \nsbytes\n\n\nPython bytes exhibiting a copy of a\u2019s raw data.     Examples >>> x = np.array([[0, 1], [2, 3]], dtype='<u2')\n>>> x.tobytes()\nb'\\x00\\x00\\x01\\x00\\x02\\x00\\x03\\x00'\n>>> x.tobytes('C') == x.tobytes()\nTrue\n>>> x.tobytes('F')\nb'\\x00\\x00\\x02\\x00\\x01\\x00\\x03\\x00'\n \n\n"}, {"name": "chararray.tofile()", "path": "reference/generated/numpy.chararray.tofile", "type": "numpy.chararray.tofile", "text": "numpy.chararray.tofile method   chararray.tofile(fid, sep='', format='%s')\n \nWrite array to a file as text or binary (default). Data is always written in \u2018C\u2019 order, independent of the order of a. The data produced by this method can be recovered using the function fromfile().  Parameters \n \nfidfile or str or Path\n\n\nAn open file object, or a string containing a filename.  Changed in version 1.17.0: pathlib.Path objects are now accepted.   \nsepstr\n\n\nSeparator between array items for text output. If \u201c\u201d (empty), a binary file is written, equivalent to file.write(a.tobytes()).  \nformatstr\n\n\nFormat string for text file output. Each entry in the array is formatted to text by first converting it to the closest Python type, and then using \u201cformat\u201d % item.     Notes This is a convenience function for quick storage of array data. Information on endianness and precision is lost, so this method is not a good choice for files intended to archive data or transport data between machines with different endianness. Some of these problems can be overcome by outputting the data as text files, at the expense of speed and file size. When fid is a file object, array contents are directly written to the file, bypassing the file object\u2019s write method. As a result, tofile cannot be used with files objects supporting compression (e.g., GzipFile) or file-like objects that do not support fileno() (e.g., BytesIO). \n\n"}, {"name": "chararray.tolist()", "path": "reference/generated/numpy.chararray.tolist", "type": "numpy.chararray.tolist", "text": "numpy.chararray.tolist method   chararray.tolist()\n \nReturn the array as an a.ndim-levels deep nested list of Python scalars. Return a copy of the array data as a (nested) Python list. Data items are converted to the nearest compatible builtin Python type, via the item function. If a.ndim is 0, then since the depth of the nested list is 0, it will not be a list at all, but a simple Python scalar.  Parameters \n none\n  Returns \n \nyobject, or list of object, or list of list of object, or \u2026\n\n\nThe possibly nested list of array elements.     Notes The array may be recreated via a = np.array(a.tolist()), although this may sometimes lose precision. Examples For a 1D array, a.tolist() is almost the same as list(a), except that tolist changes numpy scalars to Python scalars: >>> a = np.uint32([1, 2])\n>>> a_list = list(a)\n>>> a_list\n[1, 2]\n>>> type(a_list[0])\n<class 'numpy.uint32'>\n>>> a_tolist = a.tolist()\n>>> a_tolist\n[1, 2]\n>>> type(a_tolist[0])\n<class 'int'>\n Additionally, for a 2D array, tolist applies recursively: >>> a = np.array([[1, 2], [3, 4]])\n>>> list(a)\n[array([1, 2]), array([3, 4])]\n>>> a.tolist()\n[[1, 2], [3, 4]]\n The base case for this recursion is a 0D array: >>> a = np.array(1)\n>>> list(a)\nTraceback (most recent call last):\n  ...\nTypeError: iteration over a 0-d array\n>>> a.tolist()\n1\n \n\n"}, {"name": "chararray.tostring()", "path": "reference/generated/numpy.chararray.tostring", "type": "numpy.chararray.tostring", "text": "numpy.chararray.tostring method   chararray.tostring(order='C')\n \nA compatibility alias for tobytes, with exactly the same behavior. Despite its name, it returns bytes not strs.  Deprecated since version 1.19.0.  \n\n"}, {"name": "chararray.translate()", "path": "reference/generated/numpy.chararray.translate", "type": "numpy.chararray.translate", "text": "numpy.chararray.translate method   chararray.translate(table, deletechars=None)[source]\n \nFor each element in self, return a copy of the string where all characters occurring in the optional argument deletechars are removed, and the remaining characters have been mapped through the given translation table.  See also  char.translate\n  \n\n"}, {"name": "chararray.transpose()", "path": "reference/generated/numpy.chararray.transpose", "type": "numpy.chararray.transpose", "text": "numpy.chararray.transpose method   chararray.transpose(*axes)\n \nReturns a view of the array with axes transposed. For a 1-D array this has no effect, as a transposed vector is simply the same vector. To convert a 1-D array into a 2D column vector, an additional dimension must be added. np.atleast2d(a).T achieves this, as does a[:, np.newaxis]. For a 2-D array, this is a standard matrix transpose. For an n-D array, if axes are given, their order indicates how the axes are permuted (see Examples). If axes are not provided and a.shape = (i[0], i[1], ... i[n-2], i[n-1]), then a.transpose().shape = (i[n-1], i[n-2], ... i[1], i[0]).  Parameters \n \naxesNone, tuple of ints, or n ints\n\n\n None or no argument: reverses the order of the axes. tuple of ints: i in the j-th place in the tuple means a\u2019s i-th axis becomes a.transpose()\u2019s j-th axis. \nn ints: same as an n-tuple of the same ints (this form is intended simply as a \u201cconvenience\u201d alternative to the tuple form)     Returns \n \noutndarray\n\n\nView of a, with axes suitably permuted.      See also  transpose\n\nEquivalent function  ndarray.T\n\nArray property returning the array transposed.  ndarray.reshape\n\nGive a new shape to an array without changing its data.    Examples >>> a = np.array([[1, 2], [3, 4]])\n>>> a\narray([[1, 2],\n       [3, 4]])\n>>> a.transpose()\narray([[1, 3],\n       [2, 4]])\n>>> a.transpose((1, 0))\narray([[1, 3],\n       [2, 4]])\n>>> a.transpose(1, 0)\narray([[1, 3],\n       [2, 4]])\n \n\n"}, {"name": "chararray.upper()", "path": "reference/generated/numpy.chararray.upper", "type": "numpy.chararray.upper", "text": "numpy.chararray.upper method   chararray.upper()[source]\n \nReturn an array with the elements of self converted to uppercase.  See also  char.upper\n  \n\n"}, {"name": "chararray.view()", "path": "reference/generated/numpy.chararray.view", "type": "numpy.chararray.view", "text": "numpy.chararray.view method   chararray.view([dtype][, type])\n \nNew view of array with the same data.  Note Passing None for dtype is different from omitting the parameter, since the former invokes dtype(None) which is an alias for dtype('float_').   Parameters \n \ndtypedata-type or ndarray sub-class, optional\n\n\nData-type descriptor of the returned view, e.g., float32 or int16. Omitting it results in the view having the same data-type as a. This argument can also be specified as an ndarray sub-class, which then specifies the type of the returned object (this is equivalent to setting the type parameter).  \ntypePython type, optional\n\n\nType of the returned view, e.g., ndarray or matrix. Again, omission of the parameter results in type preservation.     Notes a.view() is used two different ways: a.view(some_dtype) or a.view(dtype=some_dtype) constructs a view of the array\u2019s memory with a different data-type. This can cause a reinterpretation of the bytes of memory. a.view(ndarray_subclass) or a.view(type=ndarray_subclass) just returns an instance of ndarray_subclass that looks at the same array (same shape, dtype, etc.) This does not cause a reinterpretation of the memory. For a.view(some_dtype), if some_dtype has a different number of bytes per entry than the previous dtype (for example, converting a regular array to a structured array), then the behavior of the view cannot be predicted just from the superficial appearance of a (shown by print(a)). It also depends on exactly how a is stored in memory. Therefore if a is C-ordered versus fortran-ordered, versus defined as a slice or transpose, etc., the view may give different results. Examples >>> x = np.array([(1, 2)], dtype=[('a', np.int8), ('b', np.int8)])\n Viewing array data using a different type and dtype: >>> y = x.view(dtype=np.int16, type=np.matrix)\n>>> y\nmatrix([[513]], dtype=int16)\n>>> print(type(y))\n<class 'numpy.matrix'>\n Creating a view on a structured array so it can be used in calculations >>> x = np.array([(1, 2),(3,4)], dtype=[('a', np.int8), ('b', np.int8)])\n>>> xv = x.view(dtype=np.int8).reshape(-1,2)\n>>> xv\narray([[1, 2],\n       [3, 4]], dtype=int8)\n>>> xv.mean(0)\narray([2.,  3.])\n Making changes to the view changes the underlying array >>> xv[0,1] = 20\n>>> x\narray([(1, 20), (3,  4)], dtype=[('a', 'i1'), ('b', 'i1')])\n Using a view to convert an array to a recarray: >>> z = x.view(np.recarray)\n>>> z.a\narray([1, 3], dtype=int8)\n Views share data: >>> x[0] = (9, 10)\n>>> z[0]\n(9, 10)\n Views that change the dtype size (bytes per entry) should normally be avoided on arrays defined by slices, transposes, fortran-ordering, etc.: >>> x = np.array([[1,2,3],[4,5,6]], dtype=np.int16)\n>>> y = x[:, 0:2]\n>>> y\narray([[1, 2],\n       [4, 5]], dtype=int16)\n>>> y.view(dtype=[('width', np.int16), ('length', np.int16)])\nTraceback (most recent call last):\n    ...\nValueError: To change to a dtype of a different size, the array must be C-contiguous\n>>> z = y.copy()\n>>> z.view(dtype=[('width', np.int16), ('length', np.int16)])\narray([[(1, 2)],\n       [(4, 5)]], dtype=[('width', '<i2'), ('length', '<i2')])\n \n\n"}, {"name": "chararray.zfill()", "path": "reference/generated/numpy.chararray.zfill", "type": "numpy.chararray.zfill", "text": "numpy.chararray.zfill method   chararray.zfill(width)[source]\n \nReturn the numeric string left-filled with zeros in a string of length width.  See also  char.zfill\n  \n\n"}, {"name": "class.__array__()", "path": "reference/arrays.classes#numpy.class.__array__", "type": "Standard array subclasses", "text": "  class.__array__([dtype])\n \nIf a class (ndarray subclass or not) having the __array__ method is used as the output object of an ufunc, results will not be written to the object returned by __array__. This practice will return TypeError. \n"}, {"name": "class.__array_finalize__()", "path": "reference/arrays.classes#numpy.class.__array_finalize__", "type": "Standard array subclasses", "text": "  class.__array_finalize__(obj)\n \nThis method is called whenever the system internally allocates a new array from obj, where obj is a subclass (subtype) of the ndarray. It can be used to change attributes of self after construction (so as to ensure a 2-d matrix for example), or to update meta-information from the \u201cparent.\u201d Subclasses inherit a default implementation of this method that does nothing. \n"}, {"name": "class.__array_function__()", "path": "reference/arrays.classes#numpy.class.__array_function__", "type": "Standard array subclasses", "text": "  class.__array_function__(func, types, args, kwargs)\n \n New in version 1.16.   Note  In NumPy 1.17, the protocol is enabled by default, but can be disabled with NUMPY_EXPERIMENTAL_ARRAY_FUNCTION=0. In NumPy 1.16, you need to set the environment variable NUMPY_EXPERIMENTAL_ARRAY_FUNCTION=1 before importing NumPy to use NumPy function overrides. Eventually, expect to __array_function__ to always be enabled.    \nfunc is an arbitrary callable exposed by NumPy\u2019s public API, which was called in the form func(*args, **kwargs). \ntypes is a collection collections.abc.Collection of unique argument types from the original NumPy function call that implement __array_function__. The tuple args and dict kwargs are directly passed on from the original call.  As a convenience for __array_function__ implementors, types provides all argument types with an '__array_function__' attribute. This allows implementors to quickly identify cases where they should defer to __array_function__ implementations on other arguments. Implementations should not rely on the iteration order of types. Most implementations of __array_function__ will start with two checks:  Is the given function something that we know how to overload? Are all arguments of a type that we know how to handle?  If these conditions hold, __array_function__ should return the result from calling its implementation for func(*args, **kwargs). Otherwise, it should return the sentinel value NotImplemented, indicating that the function is not implemented by these types. There are no general requirements on the return value from __array_function__, although most sensible implementations should probably return array(s) with the same type as one of the function\u2019s arguments. It may also be convenient to define a custom decorators (implements below) for registering __array_function__ implementations. HANDLED_FUNCTIONS = {}\n\nclass MyArray:\n    def __array_function__(self, func, types, args, kwargs):\n        if func not in HANDLED_FUNCTIONS:\n            return NotImplemented\n        # Note: this allows subclasses that don't override\n        # __array_function__ to handle MyArray objects\n        if not all(issubclass(t, MyArray) for t in types):\n            return NotImplemented\n        return HANDLED_FUNCTIONS[func](*args, **kwargs)\n\ndef implements(numpy_function):\n    \"\"\"Register an __array_function__ implementation for MyArray objects.\"\"\"\n    def decorator(func):\n        HANDLED_FUNCTIONS[numpy_function] = func\n        return func\n    return decorator\n\n@implements(np.concatenate)\ndef concatenate(arrays, axis=0, out=None):\n    ...  # implementation of concatenate for MyArray objects\n\n@implements(np.broadcast_to)\ndef broadcast_to(array, shape):\n    ...  # implementation of broadcast_to for MyArray objects\n Note that it is not required for __array_function__ implementations to include all of the corresponding NumPy function\u2019s optional arguments (e.g., broadcast_to above omits the irrelevant subok argument). Optional arguments are only passed in to __array_function__ if they were explicitly used in the NumPy function call. Just like the case for builtin special methods like __add__, properly written __array_function__ methods should always return NotImplemented when an unknown type is encountered. Otherwise, it will be impossible to correctly override NumPy functions from another object if the operation also includes one of your objects. For the most part, the rules for dispatch with __array_function__ match those for __array_ufunc__. In particular:  NumPy will gather implementations of __array_function__ from all specified inputs and call them in order: subclasses before superclasses, and otherwise left to right. Note that in some edge cases involving subclasses, this differs slightly from the current behavior of Python. Implementations of __array_function__ indicate that they can handle the operation by returning any value other than NotImplemented. If all __array_function__ methods return NotImplemented, NumPy will raise TypeError.  If no __array_function__ methods exists, NumPy will default to calling its own implementation, intended for use on NumPy arrays. This case arises, for example, when all array-like arguments are Python numbers or lists. (NumPy arrays do have a __array_function__ method, given below, but it always returns NotImplemented if any argument other than a NumPy array subclass implements __array_function__.) One deviation from the current behavior of __array_ufunc__ is that NumPy will only call __array_function__ on the first argument of each unique type. This matches Python\u2019s rule for calling reflected methods, and this ensures that checking overloads has acceptable performance even when there are a large number of overloaded arguments. \n"}, {"name": "class.__array_prepare__()", "path": "reference/arrays.classes#numpy.class.__array_prepare__", "type": "Standard array subclasses", "text": "  class.__array_prepare__(array, context=None)\n \nAt the beginning of every ufunc, this method is called on the input object with the highest array priority, or the output object if one was specified. The output array is passed in and whatever is returned is passed to the ufunc. Subclasses inherit a default implementation of this method which simply returns the output array unmodified. Subclasses may opt to use this method to transform the output array into an instance of the subclass and update metadata before returning the array to the ufunc for computation.  Note For ufuncs, it is hoped to eventually deprecate this method in favour of __array_ufunc__.  \n"}, {"name": "class.__array_priority__", "path": "reference/arrays.classes#numpy.class.__array_priority__", "type": "Standard array subclasses", "text": "  class.__array_priority__\n \nThe value of this attribute is used to determine what type of object to return in situations where there is more than one possibility for the Python type of the returned object. Subclasses inherit a default value of 0.0 for this attribute.  Note For ufuncs, it is hoped to eventually deprecate this method in favour of __array_ufunc__.  \n"}, {"name": "class.__array_ufunc__()", "path": "reference/arrays.classes", "type": "Standard array subclasses", "text": "Standard array subclasses  Note Subclassing a numpy.ndarray is possible but if your goal is to create an array with modified behavior, as do dask arrays for distributed computation and cupy arrays for GPU-based computation, subclassing is discouraged. Instead, using numpy\u2019s dispatch mechanism is recommended.  The ndarray can be inherited from (in Python or in C) if desired. Therefore, it can form a foundation for many useful classes. Often whether to sub-class the array object or to simply use the core array component as an internal part of a new class is a difficult decision, and can be simply a matter of choice. NumPy has several tools for simplifying how your new object interacts with other array objects, and so the choice may not be significant in the end. One way to simplify the question is by asking yourself if the object you are interested in can be replaced as a single array or does it really require two or more arrays at its core. Note that asarray always returns the base-class ndarray. If you are confident that your use of the array object can handle any subclass of an ndarray, then asanyarray can be used to allow subclasses to propagate more cleanly through your subroutine. In principal a subclass could redefine any aspect of the array and therefore, under strict guidelines, asanyarray would rarely be useful. However, most subclasses of the array object will not redefine certain aspects of the array object such as the buffer interface, or the attributes of the array. One important example, however, of why your subroutine may not be able to handle an arbitrary subclass of an array is that matrices redefine the \u201c*\u201d operator to be matrix-multiplication, rather than element-by-element multiplication.  Special attributes and methods  See also Subclassing ndarray  NumPy provides several hooks that classes can customize:   class.__array_ufunc__(ufunc, method, *inputs, **kwargs)\n \n New in version 1.13.  Any class, ndarray subclass or not, can define this method or set it to None in order to override the behavior of NumPy\u2019s ufuncs. This works quite similarly to Python\u2019s __mul__ and other binary operation routines.  \nufunc is the ufunc object that was called. \nmethod is a string indicating which Ufunc method was called (one of \"__call__\", \"reduce\", \"reduceat\", \"accumulate\", \"outer\", \"inner\"). \ninputs is a tuple of the input arguments to the ufunc. \nkwargs is a dictionary containing the optional input arguments of the ufunc. If given, any out arguments, both positional and keyword, are passed as a tuple in kwargs. See the discussion in Universal functions (ufunc) for details.  The method should return either the result of the operation, or NotImplemented if the operation requested is not implemented. If one of the input or output arguments has a __array_ufunc__ method, it is executed instead of the ufunc. If more than one of the arguments implements __array_ufunc__, they are tried in the order: subclasses before superclasses, inputs before outputs, otherwise left to right. The first routine returning something other than NotImplemented determines the result. If all of the __array_ufunc__ operations return NotImplemented, a TypeError is raised.  Note We intend to re-implement numpy functions as (generalized) Ufunc, in which case it will become possible for them to be overridden by the __array_ufunc__ method. A prime candidate is matmul, which currently is not a Ufunc, but could be relatively easily be rewritten as a (set of) generalized Ufuncs. The same may happen with functions such as median, amin, and argsort.  Like with some other special methods in python, such as __hash__ and __iter__, it is possible to indicate that your class does not support ufuncs by setting __array_ufunc__ = None. Ufuncs always raise TypeError when called on an object that sets __array_ufunc__ = None. The presence of __array_ufunc__ also influences how ndarray handles binary operations like arr + obj and arr\n< obj when arr is an ndarray and obj is an instance of a custom class. There are two possibilities. If obj.__array_ufunc__ is present and not None, then ndarray.__add__ and friends will delegate to the ufunc machinery, meaning that arr + obj becomes np.add(arr, obj), and then add invokes obj.__array_ufunc__. This is useful if you want to define an object that acts like an array. Alternatively, if obj.__array_ufunc__ is set to None, then as a special case, special methods like ndarray.__add__ will notice this and unconditionally raise TypeError. This is useful if you want to create objects that interact with arrays via binary operations, but are not themselves arrays. For example, a units handling system might have an object m representing the \u201cmeters\u201d unit, and want to support the syntax arr * m to represent that the array has units of \u201cmeters\u201d, but not want to otherwise interact with arrays via ufuncs or otherwise. This can be done by setting __array_ufunc__ = None and defining __mul__ and __rmul__ methods. (Note that this means that writing an __array_ufunc__ that always returns NotImplemented is not quite the same as setting __array_ufunc__ = None: in the former case, arr + obj will raise TypeError, while in the latter case it is possible to define a __radd__ method to prevent this.) The above does not hold for in-place operators, for which ndarray never returns NotImplemented. Hence, arr += obj would always lead to a TypeError. This is because for arrays in-place operations cannot generically be replaced by a simple reverse operation. (For instance, by default, arr += obj would be translated to arr =\narr + obj, i.e., arr would be replaced, contrary to what is expected for in-place array operations.)  Note If you define __array_ufunc__:  If you are not a subclass of ndarray, we recommend your class define special methods like __add__ and __lt__ that delegate to ufuncs just like ndarray does. An easy way to do this is to subclass from NDArrayOperatorsMixin. If you subclass ndarray, we recommend that you put all your override logic in __array_ufunc__ and not also override special methods. This ensures the class hierarchy is determined in only one place rather than separately by the ufunc machinery and by the binary operation rules (which gives preference to special methods of subclasses; the alternative way to enforce a one-place only hierarchy, of setting __array_ufunc__ to None, would seem very unexpected and thus confusing, as then the subclass would not work at all with ufuncs). \nndarray defines its own __array_ufunc__, which, evaluates the ufunc if no arguments have overrides, and returns NotImplemented otherwise. This may be useful for subclasses for which __array_ufunc__ converts any instances of its own class to ndarray: it can then pass these on to its superclass using super().__array_ufunc__(*inputs, **kwargs), and finally return the results after possible back-conversion. The advantage of this practice is that it ensures that it is possible to have a hierarchy of subclasses that extend the behaviour. See Subclassing ndarray for details.    Note If a class defines the __array_ufunc__ method, this disables the __array_wrap__, __array_prepare__, __array_priority__ mechanism described below for ufuncs (which may eventually be deprecated).  \n   class.__array_function__(func, types, args, kwargs)\n \n New in version 1.16.   Note  In NumPy 1.17, the protocol is enabled by default, but can be disabled with NUMPY_EXPERIMENTAL_ARRAY_FUNCTION=0. In NumPy 1.16, you need to set the environment variable NUMPY_EXPERIMENTAL_ARRAY_FUNCTION=1 before importing NumPy to use NumPy function overrides. Eventually, expect to __array_function__ to always be enabled.    \nfunc is an arbitrary callable exposed by NumPy\u2019s public API, which was called in the form func(*args, **kwargs). \ntypes is a collection collections.abc.Collection of unique argument types from the original NumPy function call that implement __array_function__. The tuple args and dict kwargs are directly passed on from the original call.  As a convenience for __array_function__ implementors, types provides all argument types with an '__array_function__' attribute. This allows implementors to quickly identify cases where they should defer to __array_function__ implementations on other arguments. Implementations should not rely on the iteration order of types. Most implementations of __array_function__ will start with two checks:  Is the given function something that we know how to overload? Are all arguments of a type that we know how to handle?  If these conditions hold, __array_function__ should return the result from calling its implementation for func(*args, **kwargs). Otherwise, it should return the sentinel value NotImplemented, indicating that the function is not implemented by these types. There are no general requirements on the return value from __array_function__, although most sensible implementations should probably return array(s) with the same type as one of the function\u2019s arguments. It may also be convenient to define a custom decorators (implements below) for registering __array_function__ implementations. HANDLED_FUNCTIONS = {}\n\nclass MyArray:\n    def __array_function__(self, func, types, args, kwargs):\n        if func not in HANDLED_FUNCTIONS:\n            return NotImplemented\n        # Note: this allows subclasses that don't override\n        # __array_function__ to handle MyArray objects\n        if not all(issubclass(t, MyArray) for t in types):\n            return NotImplemented\n        return HANDLED_FUNCTIONS[func](*args, **kwargs)\n\ndef implements(numpy_function):\n    \"\"\"Register an __array_function__ implementation for MyArray objects.\"\"\"\n    def decorator(func):\n        HANDLED_FUNCTIONS[numpy_function] = func\n        return func\n    return decorator\n\n@implements(np.concatenate)\ndef concatenate(arrays, axis=0, out=None):\n    ...  # implementation of concatenate for MyArray objects\n\n@implements(np.broadcast_to)\ndef broadcast_to(array, shape):\n    ...  # implementation of broadcast_to for MyArray objects\n Note that it is not required for __array_function__ implementations to include all of the corresponding NumPy function\u2019s optional arguments (e.g., broadcast_to above omits the irrelevant subok argument). Optional arguments are only passed in to __array_function__ if they were explicitly used in the NumPy function call. Just like the case for builtin special methods like __add__, properly written __array_function__ methods should always return NotImplemented when an unknown type is encountered. Otherwise, it will be impossible to correctly override NumPy functions from another object if the operation also includes one of your objects. For the most part, the rules for dispatch with __array_function__ match those for __array_ufunc__. In particular:  NumPy will gather implementations of __array_function__ from all specified inputs and call them in order: subclasses before superclasses, and otherwise left to right. Note that in some edge cases involving subclasses, this differs slightly from the current behavior of Python. Implementations of __array_function__ indicate that they can handle the operation by returning any value other than NotImplemented. If all __array_function__ methods return NotImplemented, NumPy will raise TypeError.  If no __array_function__ methods exists, NumPy will default to calling its own implementation, intended for use on NumPy arrays. This case arises, for example, when all array-like arguments are Python numbers or lists. (NumPy arrays do have a __array_function__ method, given below, but it always returns NotImplemented if any argument other than a NumPy array subclass implements __array_function__.) One deviation from the current behavior of __array_ufunc__ is that NumPy will only call __array_function__ on the first argument of each unique type. This matches Python\u2019s rule for calling reflected methods, and this ensures that checking overloads has acceptable performance even when there are a large number of overloaded arguments. \n   class.__array_finalize__(obj)\n \nThis method is called whenever the system internally allocates a new array from obj, where obj is a subclass (subtype) of the ndarray. It can be used to change attributes of self after construction (so as to ensure a 2-d matrix for example), or to update meta-information from the \u201cparent.\u201d Subclasses inherit a default implementation of this method that does nothing. \n   class.__array_prepare__(array, context=None)\n \nAt the beginning of every ufunc, this method is called on the input object with the highest array priority, or the output object if one was specified. The output array is passed in and whatever is returned is passed to the ufunc. Subclasses inherit a default implementation of this method which simply returns the output array unmodified. Subclasses may opt to use this method to transform the output array into an instance of the subclass and update metadata before returning the array to the ufunc for computation.  Note For ufuncs, it is hoped to eventually deprecate this method in favour of __array_ufunc__.  \n   class.__array_wrap__(array, context=None)\n \nAt the end of every ufunc, this method is called on the input object with the highest array priority, or the output object if one was specified. The ufunc-computed array is passed in and whatever is returned is passed to the user. Subclasses inherit a default implementation of this method, which transforms the array into a new instance of the object\u2019s class. Subclasses may opt to use this method to transform the output array into an instance of the subclass and update metadata before returning the array to the user.  Note For ufuncs, it is hoped to eventually deprecate this method in favour of __array_ufunc__.  \n   class.__array_priority__\n \nThe value of this attribute is used to determine what type of object to return in situations where there is more than one possibility for the Python type of the returned object. Subclasses inherit a default value of 0.0 for this attribute.  Note For ufuncs, it is hoped to eventually deprecate this method in favour of __array_ufunc__.  \n   class.__array__([dtype])\n \nIf a class (ndarray subclass or not) having the __array__ method is used as the output object of an ufunc, results will not be written to the object returned by __array__. This practice will return TypeError. \n   Matrix objects  Note It is strongly advised not to use the matrix subclass. As described below, it makes writing functions that deal consistently with matrices and regular arrays very difficult. Currently, they are mainly used for interacting with scipy.sparse. We hope to provide an alternative for this use, however, and eventually remove the matrix subclass.  matrix objects inherit from the ndarray and therefore, they have the same attributes and methods of ndarrays. There are six important differences of matrix objects, however, that may lead to unexpected results when you use matrices but expect them to act like arrays:  Matrix objects can be created using a string notation to allow Matlab-style syntax where spaces separate columns and semicolons (\u2018;\u2019) separate rows. Matrix objects are always two-dimensional. This has far-reaching implications, in that m.ravel() is still two-dimensional (with a 1 in the first dimension) and item selection returns two-dimensional objects so that sequence behavior is fundamentally different than arrays. Matrix objects over-ride multiplication to be matrix-multiplication. Make sure you understand this for functions that you may want to receive matrices. Especially in light of the fact that asanyarray(m) returns a matrix when m is a matrix.\n Matrix objects over-ride power to be matrix raised to a power. The same warning about using power inside a function that uses asanyarray(\u2026) to get an array object holds for this fact. The default __array_priority__ of matrix objects is 10.0, and therefore mixed operations with ndarrays always produce matrices. \nMatrices have special attributes which make calculations easier. These are  \nmatrix.T Returns the transpose of the matrix.  \nmatrix.H Returns the (complex) conjugate transpose of self.  \nmatrix.I Returns the (multiplicative) inverse of invertible self.  \nmatrix.A Return self as an ndarray object.      Warning Matrix objects over-ride multiplication, \u2018*\u2019, and power, \u2018**\u2019, to be matrix-multiplication and matrix power, respectively. If your subroutine can accept sub-classes and you do not convert to base- class arrays, then you must use the ufuncs multiply and power to be sure that you are performing the correct operation for all inputs.  The matrix class is a Python subclass of the ndarray and can be used as a reference for how to construct your own subclass of the ndarray. Matrices can be created from other matrices, strings, and anything else that can be converted to an ndarray . The name \u201cmat \u201cis an alias for \u201cmatrix \u201cin NumPy.  \nmatrix(data[, dtype, copy]) \n Note It is no longer recommended to use this class, even for linear    \nasmatrix(data[, dtype]) Interpret the input as a matrix.  \nbmat(obj[, ldict, gdict]) Build a matrix object from a string, nested sequence, or array.   Example 1: Matrix creation from a string >>> a = np.mat('1 2 3; 4 5 3')\n>>> print((a*a.T).I)\n    [[ 0.29239766 -0.13450292]\n     [-0.13450292  0.08187135]]\n Example 2: Matrix creation from nested sequence >>> np.mat([[1,5,10],[1.0,3,4j]])\nmatrix([[  1.+0.j,   5.+0.j,  10.+0.j],\n        [  1.+0.j,   3.+0.j,   0.+4.j]])\n Example 3: Matrix creation from an array >>> np.mat(np.random.rand(3,3)).T\nmatrix([[4.17022005e-01, 3.02332573e-01, 1.86260211e-01],\n        [7.20324493e-01, 1.46755891e-01, 3.45560727e-01],\n        [1.14374817e-04, 9.23385948e-02, 3.96767474e-01]])\n   Memory-mapped file arrays Memory-mapped files are useful for reading and/or modifying small segments of a large file with regular layout, without reading the entire file into memory. A simple subclass of the ndarray uses a memory-mapped file for the data buffer of the array. For small files, the over-head of reading the entire file into memory is typically not significant, however for large files using memory mapping can save considerable resources. Memory-mapped-file arrays have one additional method (besides those they inherit from the ndarray): .flush() which must be called manually by the user to ensure that any changes to the array actually get written to disk.  \nmemmap(filename[, dtype, mode, offset, ...]) Create a memory-map to an array stored in a binary file on disk.  \nmemmap.flush() Write any changes in the array to the file on disk.   Example: >>> a = np.memmap('newfile.dat', dtype=float, mode='w+', shape=1000)\n>>> a[10] = 10.0\n>>> a[30] = 30.0\n>>> del a\n>>> b = np.fromfile('newfile.dat', dtype=float)\n>>> print(b[10], b[30])\n10.0 30.0\n>>> a = np.memmap('newfile.dat', dtype=float)\n>>> print(a[10], a[30])\n10.0 30.0\n   Character arrays (numpy.char)  See also Creating character arrays (numpy.char)   Note The chararray class exists for backwards compatibility with Numarray, it is not recommended for new development. Starting from numpy 1.4, if one needs arrays of strings, it is recommended to use arrays of dtype object_, bytes_ or str_, and use the free functions in the numpy.char module for fast vectorized string operations.  These are enhanced arrays of either str_ type or bytes_ type. These arrays inherit from the ndarray, but specially-define the operations +, *, and % on a (broadcasting) element-by-element basis. These operations are not available on the standard ndarray of character type. In addition, the chararray has all of the standard str (and bytes) methods, executing them on an element-by-element basis. Perhaps the easiest way to create a chararray is to use self.view(chararray) where self is an ndarray of str or unicode data-type. However, a chararray can also be created using the numpy.chararray constructor, or via the numpy.char.array function:  \nchararray(shape[, itemsize, unicode, ...]) Provides a convenient view on arrays of string and unicode values.  \ncore.defchararray.array(obj[, itemsize, ...]) Create a chararray.   Another difference with the standard ndarray of str data-type is that the chararray inherits the feature introduced by Numarray that white-space at the end of any element in the array will be ignored on item retrieval and comparison operations.   Record arrays (numpy.rec)  See also Creating record arrays (numpy.rec), Data type routines, Data type objects (dtype).  NumPy provides the recarray class which allows accessing the fields of a structured array as attributes, and a corresponding scalar data type object record.  \nrecarray(shape[, dtype, buf, offset, ...]) Construct an ndarray that allows field access using attributes.  \nrecord A data-type scalar that allows field access as attribute lookup.     Masked arrays (numpy.ma)  See also Masked arrays    Standard container class For backward compatibility and as a standard \u201ccontainer \u201cclass, the UserArray from Numeric has been brought over to NumPy and named numpy.lib.user_array.container The container class is a Python class whose self.array attribute is an ndarray. Multiple inheritance is probably easier with numpy.lib.user_array.container than with the ndarray itself and so it is included by default. It is not documented here beyond mentioning its existence because you are encouraged to use the ndarray class directly if you can.  \nnumpy.lib.user_array.container(data[, ...]) Standard container-class for easy multiple-inheritance.     Array Iterators Iterators are a powerful concept for array processing. Essentially, iterators implement a generalized for-loop. If myiter is an iterator object, then the Python code: for val in myiter:\n    ...\n    some code involving val\n    ...\n calls val = next(myiter) repeatedly until StopIteration is raised by the iterator. There are several ways to iterate over an array that may be useful: default iteration, flat iteration, and \\(N\\)-dimensional enumeration.  Default iteration The default iterator of an ndarray object is the default Python iterator of a sequence type. Thus, when the array object itself is used as an iterator. The default behavior is equivalent to: for i in range(arr.shape[0]):\n    val = arr[i]\n This default iterator selects a sub-array of dimension \\(N-1\\) from the array. This can be a useful construct for defining recursive algorithms. To loop over the entire array requires \\(N\\) for-loops. >>> a = np.arange(24).reshape(3,2,4)+10\n>>> for val in a:\n...     print('item:', val)\nitem: [[10 11 12 13]\n [14 15 16 17]]\nitem: [[18 19 20 21]\n [22 23 24 25]]\nitem: [[26 27 28 29]\n [30 31 32 33]]\n   Flat iteration  \nndarray.flat A 1-D iterator over the array.   As mentioned previously, the flat attribute of ndarray objects returns an iterator that will cycle over the entire array in C-style contiguous order. >>> for i, val in enumerate(a.flat):\n...     if i%5 == 0: print(i, val)\n0 10\n5 15\n10 20\n15 25\n20 30\n Here, I\u2019ve used the built-in enumerate iterator to return the iterator index as well as the value.   N-dimensional enumeration  \nndenumerate(arr) Multidimensional index iterator.   Sometimes it may be useful to get the N-dimensional index while iterating. The ndenumerate iterator can achieve this. >>> for i, val in np.ndenumerate(a):\n...     if sum(i)%5 == 0: print(i, val)\n(0, 0, 0) 10\n(1, 1, 3) 25\n(2, 0, 3) 29\n(2, 1, 2) 32\n   Iterator for broadcasting  \nbroadcast Produce an object that mimics broadcasting.   The general concept of broadcasting is also available from Python using the broadcast iterator. This object takes \\(N\\) objects as inputs and returns an iterator that returns tuples providing each of the input sequence elements in the broadcasted result. >>> for val in np.broadcast([[1,0],[2,3]],[0,1]):\n...     print(val)\n(1, 0)\n(0, 1)\n(2, 0)\n(3, 1)\n  \n"}, {"name": "class.__array_wrap__()", "path": "reference/arrays.classes#numpy.class.__array_wrap__", "type": "Standard array subclasses", "text": "  class.__array_wrap__(array, context=None)\n \nAt the end of every ufunc, this method is called on the input object with the highest array priority, or the output object if one was specified. The ufunc-computed array is passed in and whatever is returned is passed to the user. Subclasses inherit a default implementation of this method, which transforms the array into a new instance of the object\u2019s class. Subclasses may opt to use this method to transform the output array into an instance of the subclass and update metadata before returning the array to the user.  Note For ufuncs, it is hoped to eventually deprecate this method in favour of __array_ufunc__.  \n"}, {"name": "config.add_library()", "path": "reference/distutils_guide", "type": "NumPy Distutils - Users Guide", "text": "NumPy Distutils - Users Guide  SciPy structure Currently SciPy project consists of two packages:  \nNumPy \u2014 it provides packages like:  numpy.distutils - extension to Python distutils numpy.f2py - a tool to bind Fortran/C codes to Python numpy.core - future replacement of Numeric and numarray packages numpy.lib - extra utility functions numpy.testing - numpy-style tools for unit testing etc   SciPy \u2014 a collection of scientific tools for Python.  The aim of this document is to describe how to add new tools to SciPy.   Requirements for SciPy packages SciPy consists of Python packages, called SciPy packages, that are available to Python users via the scipy namespace. Each SciPy package may contain other SciPy packages. And so on. Therefore, the SciPy directory tree is a tree of packages with arbitrary depth and width. Any SciPy package may depend on NumPy packages but the dependence on other SciPy packages should be kept minimal or zero. A SciPy package contains, in addition to its sources, the following files and directories:  \nsetup.py \u2014 building script \n__init__.py \u2014 package initializer \ntests/ \u2014 directory of unittests  Their contents are described below.   The setup.py file In order to add a Python package to SciPy, its build script (setup.py) must meet certain requirements. The most important requirement is that the package define a configuration(parent_package='',top_path=None) function which returns a dictionary suitable for passing to numpy.distutils.core.setup(..). To simplify the construction of this dictionary, numpy.distutils.misc_util provides the Configuration class, described below.  SciPy pure Python package example Below is an example of a minimal setup.py file for a pure SciPy package: #!/usr/bin/env python3\ndef configuration(parent_package='',top_path=None):\n    from numpy.distutils.misc_util import Configuration\n    config = Configuration('mypackage',parent_package,top_path)\n    return config\n\nif __name__ == \"__main__\":\n    from numpy.distutils.core import setup\n    #setup(**configuration(top_path='').todict())\n    setup(configuration=configuration)\n The arguments of the configuration function specify the name of parent SciPy package (parent_package) and the directory location of the main setup.py script (top_path). These arguments, along with the name of the current package, should be passed to the Configuration constructor. The Configuration constructor has a fourth optional argument, package_path, that can be used when package files are located in a different location than the directory of the setup.py file. Remaining Configuration arguments are all keyword arguments that will be used to initialize attributes of Configuration instance. Usually, these keywords are the same as the ones that setup(..) function would expect, for example, packages, ext_modules, data_files, include_dirs, libraries, headers, scripts, package_dir, etc. However, the direct specification of these keywords is not recommended as the content of these keyword arguments will not be processed or checked for the consistency of SciPy building system. Finally, Configuration has .todict() method that returns all the configuration data as a dictionary suitable for passing on to the setup(..) function.   \nConfiguration instance attributes In addition to attributes that can be specified via keyword arguments to Configuration constructor, Configuration instance (let us denote as config) has the following attributes that can be useful in writing setup scripts:  \nconfig.name - full name of the current package. The names of parent packages can be extracted as config.name.split('.'). \nconfig.local_path - path to the location of current setup.py file. \nconfig.top_path - path to the location of main setup.py file.    \nConfiguration instance methods  \nconfig.todict() \u2014 returns configuration dictionary suitable for passing to numpy.distutils.core.setup(..) function. \nconfig.paths(*paths) --- applies ``glob.glob(..) to items of paths if necessary. Fixes paths item that is relative to config.local_path. \nconfig.get_subpackage(subpackage_name,subpackage_path=None) \u2014 returns a list of subpackage configurations. Subpackage is looked in the current directory under the name subpackage_name but the path can be specified also via optional subpackage_path argument. If subpackage_name is specified as None then the subpackage name will be taken the basename of subpackage_path. Any * used for subpackage names are expanded as wildcards. \nconfig.add_subpackage(subpackage_name,subpackage_path=None) \u2014 add SciPy subpackage configuration to the current one. The meaning and usage of arguments is explained above, see config.get_subpackage() method. \nconfig.add_data_files(*files) \u2014 prepend files to data_files list. If files item is a tuple then its first element defines the suffix of where data files are copied relative to package installation directory and the second element specifies the path to data files. By default data files are copied under package installation directory. For example, config.add_data_files('foo.dat',\n                      ('fun',['gun.dat','nun/pun.dat','/tmp/sun.dat']),\n                      'bar/car.dat'.\n                      '/full/path/to/can.dat',\n                      )\n will install data files to the following locations <installation path of config.name package>/\n  foo.dat\n  fun/\n    gun.dat\n    pun.dat\n    sun.dat\n  bar/\n    car.dat\n  can.dat\n Path to data files can be a function taking no arguments and returning path(s) to data files \u2013 this is a useful when data files are generated while building the package. (XXX: explain the step when this function are called exactly)  \nconfig.add_data_dir(data_path) \u2014 add directory data_path recursively to data_files. The whole directory tree starting at data_path will be copied under package installation directory. If data_path is a tuple then its first element defines the suffix of where data files are copied relative to package installation directory and the second element specifies the path to data directory. By default, data directory are copied under package installation directory under the basename of data_path. For example, config.add_data_dir('fun')  # fun/ contains foo.dat bar/car.dat\nconfig.add_data_dir(('sun','fun'))\nconfig.add_data_dir(('gun','/full/path/to/fun'))\n will install data files to the following locations <installation path of config.name package>/\n  fun/\n     foo.dat\n     bar/\n        car.dat\n  sun/\n     foo.dat\n     bar/\n        car.dat\n  gun/\n     foo.dat\n     bar/\n        car.dat\n  \nconfig.add_include_dirs(*paths) \u2014 prepend paths to include_dirs list. This list will be visible to all extension modules of the current package. \nconfig.add_headers(*files) \u2014 prepend files to headers list. By default, headers will be installed under <prefix>/include/pythonX.X/<config.name.replace('.','/')>/ directory. If files item is a tuple then it\u2019s first argument specifies the installation suffix relative to <prefix>/include/pythonX.X/ path. This is a Python distutils method; its use is discouraged for NumPy and SciPy in favour of config.add_data_files(*files). \nconfig.add_scripts(*files) \u2014 prepend files to scripts list. Scripts will be installed under <prefix>/bin/ directory. \nconfig.add_extension(name,sources,**kw) \u2014 create and add an Extension instance to ext_modules list. The first argument name defines the name of the extension module that will be installed under config.name package. The second argument is a list of sources. add_extension method takes also keyword arguments that are passed on to the Extension constructor. The list of allowed keywords is the following: include_dirs, define_macros, undef_macros, library_dirs, libraries, runtime_library_dirs, extra_objects, extra_compile_args, extra_link_args, export_symbols, swig_opts, depends, language, f2py_options, module_dirs, extra_info, extra_f77_compile_args, extra_f90_compile_args. Note that config.paths method is applied to all lists that may contain paths. extra_info is a dictionary or a list of dictionaries that content will be appended to keyword arguments. The list depends contains paths to files or directories that the sources of the extension module depend on. If any path in the depends list is newer than the extension module, then the module will be rebuilt. The list of sources may contain functions (\u2018source generators\u2019) with a pattern def <funcname>(ext, build_dir): return\n<source(s) or None>. If funcname returns None, no sources are generated. And if the Extension instance has no sources after processing all source generators, no extension module will be built. This is the recommended way to conditionally define extension modules. Source generator functions are called by the build_src sub-command of numpy.distutils. For example, here is a typical source generator function: def generate_source(ext,build_dir):\n    import os\n    from distutils.dep_util import newer\n    target = os.path.join(build_dir,'somesource.c')\n    if newer(target,__file__):\n        # create target file\n    return target\n The first argument contains the Extension instance that can be useful to access its attributes like depends, sources, etc. lists and modify them during the building process. The second argument gives a path to a build directory that must be used when creating files to a disk.  \nconfig.add_library(name, sources, **build_info) \u2014 add a library to libraries list. Allowed keywords arguments are depends, macros, include_dirs, extra_compiler_args, f2py_options, extra_f77_compile_args, extra_f90_compile_args. See .add_extension() method for more information on arguments. \nconfig.have_f77c() \u2014 return True if Fortran 77 compiler is available (read: a simple Fortran 77 code compiled successfully). \nconfig.have_f90c() \u2014 return True if Fortran 90 compiler is available (read: a simple Fortran 90 code compiled successfully). \nconfig.get_version() \u2014 return version string of the current package, None if version information could not be detected. This methods scans files __version__.py, <packagename>_version.py, version.py, __svn_version__.py for string variables version, __version__, <packagename>_version. \nconfig.make_svn_version_py() \u2014 appends a data function to data_files list that will generate __svn_version__.py file to the current package directory. The file will be removed from the source directory when Python exits. \nconfig.get_build_temp_dir() \u2014 return a path to a temporary directory. This is the place where one should build temporary files. \nconfig.get_distribution() \u2014 return distutils Distribution instance. \nconfig.get_config_cmd() \u2014 returns numpy.distutils config command instance. \nconfig.get_info(*names) \u2014    Conversion of .src files using Templates NumPy distutils supports automatic conversion of source files named <somefile>.src. This facility can be used to maintain very similar code blocks requiring only simple changes between blocks. During the build phase of setup, if a template file named <somefile>.src is encountered, a new file named <somefile> is constructed from the template and placed in the build directory to be used instead. Two forms of template conversion are supported. The first form occurs for files named <file>.ext.src where ext is a recognized Fortran extension (f, f90, f95, f77, for, ftn, pyf). The second form is used for all other cases.   Fortran files This template converter will replicate all function and subroutine blocks in the file with names that contain \u2018<\u2026>\u2019 according to the rules in \u2018<\u2026>\u2019. The number of comma-separated words in \u2018<\u2026>\u2019 determines the number of times the block is repeated. What these words are indicates what that repeat rule, \u2018<\u2026>\u2019, should be replaced with in each block. All of the repeat rules in a block must contain the same number of comma-separated words indicating the number of times that block should be repeated. If the word in the repeat rule needs a comma, leftarrow, or rightarrow, then prepend it with a backslash \u2018 '. If a word in the repeat rule matches \u2018 \\<index>\u2019 then it will be replaced with the <index>-th word in the same repeat specification. There are two forms for the repeat rule: named and short.  Named repeat rule A named repeat rule is useful when the same set of repeats must be used several times in a block. It is specified using <rule1=item1, item2, item3,\u2026, itemN>, where N is the number of times the block should be repeated. On each repeat of the block, the entire expression, \u2018<\u2026>\u2019 will be replaced first with item1, and then with item2, and so forth until N repeats are accomplished. Once a named repeat specification has been introduced, the same repeat rule may be used in the current block by referring only to the name (i.e. <rule1>).   Short repeat rule A short repeat rule looks like <item1, item2, item3, \u2026, itemN>. The rule specifies that the entire expression, \u2018<\u2026>\u2019 should be replaced first with item1, and then with item2, and so forth until N repeats are accomplished.   Pre-defined names The following predefined named repeat rules are available:  <prefix=s,d,c,z> <_c=s,d,c,z> <_t=real, double precision, complex, double complex> <ftype=real, double precision, complex, double complex> <ctype=float, double, complex_float, complex_double> <ftypereal=float, double precision, \\0, \\1> <ctypereal=float, double, \\0, \\1>     Other files Non-Fortran files use a separate syntax for defining template blocks that should be repeated using a variable expansion similar to the named repeat rules of the Fortran-specific repeats. NumPy Distutils preprocesses C source files (extension: .c.src) written in a custom templating language to generate C code. The @ symbol is used to wrap macro-style variables to empower a string substitution mechanism that might describe (for instance) a set of data types. The template language blocks are delimited by /**begin repeat and /**end repeat**/ lines, which may also be nested using consecutively numbered delimiting lines such as /**begin repeat1 and /**end repeat1**/:  \n/**begin repeat on a line by itself marks the beginning of a segment that should be repeated. Named variable expansions are defined using #name=item1, item2, item3,\n..., itemN# and placed on successive lines. These variables are replaced in each repeat block with corresponding word. All named variables in the same repeat block must define the same number of words. In specifying the repeat rule for a named variable, item*N is short- hand for item, item, ..., item repeated N times. In addition, parenthesis in combination with *N can be used for grouping several items that should be repeated. Thus, #name=(item1, item2)*4# is equivalent to #name=item1, item2, item1, item2, item1, item2, item1,\nitem2#. \n*/ on a line by itself marks the end of the variable expansion naming. The next line is the first line that will be repeated using the named rules. Inside the block to be repeated, the variables that should be expanded are specified as @name@. \n/**end repeat**/ on a line by itself marks the previous line as the last line of the block to be repeated. A loop in the NumPy C source code may have a @TYPE@ variable, targeted for string substitution, which is preprocessed to a number of otherwise identical loops with several strings such as INT, LONG, UINT, ULONG. The @TYPE@ style syntax thus reduces code duplication and maintenance burden by mimicking languages that have generic type support.  The above rules may be clearer in the following template source example:  1 /* TIMEDELTA to non-float types */\n 2\n 3 /**begin repeat\n 4  *\n 5  * #TOTYPE = BYTE, UBYTE, SHORT, USHORT, INT, UINT, LONG, ULONG,\n 6  *           LONGLONG, ULONGLONG, DATETIME,\n 7  *           TIMEDELTA#\n 8  * #totype = npy_byte, npy_ubyte, npy_short, npy_ushort, npy_int, npy_uint,\n 9  *           npy_long, npy_ulong, npy_longlong, npy_ulonglong,\n10  *           npy_datetime, npy_timedelta#\n11  */\n12\n13 /**begin repeat1\n14  *\n15  * #FROMTYPE = TIMEDELTA#\n16  * #fromtype = npy_timedelta#\n17  */\n18 static void\n19 @FROMTYPE@_to_@TOTYPE@(void *input, void *output, npy_intp n,\n20         void *NPY_UNUSED(aip), void *NPY_UNUSED(aop))\n21 {\n22     const @fromtype@ *ip = input;\n23     @totype@ *op = output;\n24\n25     while (n--) {\n26         *op++ = (@totype@)*ip++;\n27     }\n28 }\n29 /**end repeat1**/\n30\n31 /**end repeat**/\n The preprocessing of generically-typed C source files (whether in NumPy proper or in any third party package using NumPy Distutils) is performed by conv_template.py. The type-specific C files generated (extension: .c) by these modules during the build process are ready to be compiled. This form of generic typing is also supported for C header files (preprocessed to produce .h files).   Useful functions in numpy.distutils.misc_util\n  \nget_numpy_include_dirs() \u2014 return a list of NumPy base include directories. NumPy base include directories contain header files such as numpy/arrayobject.h, numpy/funcobject.h etc. For installed NumPy the returned list has length 1 but when building NumPy the list may contain more directories, for example, a path to config.h file that numpy/base/setup.py file generates and is used by numpy header files. \nappend_path(prefix,path) \u2014 smart append path to prefix. \ngpaths(paths, local_path='') \u2014 apply glob to paths and prepend local_path if needed. \nnjoin(*path) \u2014 join pathname components + convert /-separated path to os.sep-separated path and resolve .., . from paths. Ex. njoin('a',['b','./c'],'..','g') -> os.path.join('a','b','g'). \nminrelpath(path) \u2014 resolves dots in path. \nrel_path(path, parent_path) \u2014 return path relative to parent_path. \ndef get_cmd(cmdname,_cache={}) \u2014 returns numpy.distutils command instance. all_strings(lst) has_f_sources(sources) has_cxx_sources(sources) \nfilter_sources(sources) \u2014 return c_sources, cxx_sources,\nf_sources, fmodule_sources\n get_dependencies(sources) is_local_src_dir(directory) get_ext_source_files(ext) get_script_files(scripts) get_lib_source_files(lib) get_data_files(data) \ndot_join(*args) \u2014 join non-zero arguments with a dot. \nget_frame(level=0) \u2014 return frame object from call stack with given level. cyg2win32(path) \nmingw32() \u2014 return True when using mingw32 environment. \nterminal_has_colors(), red_text(s), green_text(s), yellow_text(s), blue_text(s), cyan_text(s)\n \nget_path(mod_name,parent_path=None) \u2014 return path of a module relative to parent_path when given. Handles also __main__ and __builtin__ modules. \nallpath(name) \u2014 replaces / with os.sep in name. \ncxx_ext_match, fortran_ext_match, f90_ext_match, f90_module_name_match\n    \nnumpy.distutils.system_info module  get_info(name,notfound_action=0) combine_paths(*args,**kws) show_all()    \nnumpy.distutils.cpuinfo module  cpuinfo    \nnumpy.distutils.log module  set_verbosity(v)    \nnumpy.distutils.exec_command module  get_pythonexe() find_executable(exe, path=None) exec_command( command, execute_in='', use_shell=None, use_tee=None, **env )     The __init__.py file The header of a typical SciPy __init__.py is: \"\"\"\nPackage docstring, typically with a brief description and function listing.\n\"\"\"\n\n# import functions into module namespace\nfrom .subpackage import *\n...\n\n__all__ = [s for s in dir() if not s.startswith('_')]\n\nfrom numpy.testing import Tester\ntest = Tester().test\nbench = Tester().bench\n   Extra features in NumPy Distutils  Specifying config_fc options for libraries in setup.py script It is possible to specify config_fc options in setup.py scripts. For example, using  config.add_library(\u2018library\u2019,\n\nsources=[\u2026], config_fc={\u2018noopt\u2019:(__file__,1)})   will compile the library sources without optimization flags. It\u2019s recommended to specify only those config_fc options in such a way that are compiler independent.   Getting extra Fortran 77 compiler options from source Some old Fortran codes need special compiler options in order to work correctly. In order to specify compiler options per source file, numpy.distutils Fortran compiler looks for the following pattern: CF77FLAGS(<fcompiler type>) = <fcompiler f77flags>\n in the first 20 lines of the source and use the f77flags for specified type of the fcompiler (the first character C is optional). TODO: This feature can be easily extended for Fortran 90 codes as well. Let us know if you would need such a feature.  \n"}, {"name": "const Tp *data()", "path": "dev/howto-docs#_CPPv4N9DoxyLimbo4dataEv", "type": "Development", "text": "  constTp*data()\n \nReturns the raw data for the limbo.  \n"}, {"name": "Contributing to NumPy", "path": "dev/index", "type": "Development", "text": "Contributing to NumPy Not a coder? Not a problem! NumPy is multi-faceted, and we can use a lot of help. These are all activities we\u2019d like to get help with (they\u2019re all important, so we list them in alphabetical order):  Code maintenance and development Community coordination DevOps Developing educational content & narrative documentation Fundraising Marketing Project management Translating content Website design and development Writing technical documentation  The rest of this document discusses working on the NumPy code base and documentation. We\u2019re in the process of updating our descriptions of other activities and roles. If you are interested in these other activities, please contact us! You can do this via the numpy-discussion mailing list, or on GitHub (open an issue or comment on a relevant issue). These are our preferred communication channels (open source is open by nature!), however if you prefer to discuss in private first, please reach out to our community coordinators at numpy-team@googlegroups.com or numpy-team.slack.com (send an email to numpy-team@googlegroups.com for an invite the first time).  Development process - summary Here\u2019s the short summary, complete TOC links are below:  \nIf you are a first-time contributor:  Go to https://github.com/numpy/numpy and click the \u201cfork\u201d button to create your own copy of the project. \nClone the project to your local computer: git clone https://github.com/your-username/numpy.git\n  \nChange the directory: cd numpy\n  \nAdd the upstream repository: git remote add upstream https://github.com/numpy/numpy.git\n  \nNow, git remote -v will show two remote repositories named:  \nupstream, which refers to the numpy repository \norigin, which refers to your personal fork     \nDevelop your contribution:  \nPull the latest changes from upstream: git checkout main\ngit pull upstream main\n  \nCreate a branch for the feature you want to work on. Since the branch name will appear in the merge message, use a sensible name such as \u2018linspace-speedups\u2019: git checkout -b linspace-speedups\n  Commit locally as you progress (git add and git commit) Use a properly formatted commit message, write tests that fail before your change and pass afterward, run all the tests locally. Be sure to document any changed behavior in docstrings, keeping to the NumPy docstring standard.   \nTo submit your contribution:  \nPush your changes back to your fork on GitHub: git push origin linspace-speedups\n  Enter your GitHub username and password (repeat contributors or advanced users can remove this step by connecting to GitHub with SSH). Go to GitHub. The new branch will show up with a green Pull Request button. Make sure the title and message are clear, concise, and self- explanatory. Then click the button to submit it. If your commit introduces a new feature or changes functionality, post on the mailing list to explain your changes. For bug fixes, documentation updates, etc., this is generally not necessary, though if you do not get any reaction, do feel free to ask for review.   \nReview process:  Reviewers (the other developers and interested community members) will write inline and/or general comments on your Pull Request (PR) to help you improve its implementation, documentation and style. Every single developer working on the project has their code reviewed, and we\u2019ve come to see it as friendly conversation from which we all learn and the overall code quality benefits. Therefore, please don\u2019t let the review discourage you from contributing: its only aim is to improve the quality of project, not to criticize (we are, after all, very grateful for the time you\u2019re donating!). See our Reviewer Guidelines for more information. To update your PR, make your changes on your local repository, commit, run tests, and only if they succeed push to your fork. As soon as those changes are pushed up (to the same branch as before) the PR will update automatically. If you have no idea how to fix the test failures, you may push your changes anyway and ask for help in a PR comment. Various continuous integration (CI) services are triggered after each PR update to build the code, run unit tests, measure code coverage and check coding style of your branch. The CI tests must pass before your PR can be merged. If CI fails, you can find out why by clicking on the \u201cfailed\u201d icon (red cross) and inspecting the build and test log. To avoid overuse and waste of this resource, test your work locally before committing. A PR must be approved by at least one core team member before merging. Approval means the core team member has carefully reviewed the changes, and the PR is ready for merging.   \nDocument changes Beyond changes to a functions docstring and possible description in the general documentation, if your change introduces any user-facing modifications they may need to be mentioned in the release notes. To add your change to the release notes, you need to create a short file with a summary and place it in doc/release/upcoming_changes. The file doc/release/upcoming_changes/README.rst details the format and filename conventions. If your change introduces a deprecation, make sure to discuss this first on GitHub or the mailing list first. If agreement on the deprecation is reached, follow NEP 23 deprecation policy to add the deprecation.  \nCross referencing issues If the PR relates to any issues, you can add the text xref gh-xxxx where xxxx is the number of the issue to github comments. Likewise, if the PR solves an issue, replace the xref with closes, fixes or any of the other flavors github accepts. In the source code, be sure to preface any issue or PR reference with gh-xxxx.   For a more detailed discussion, read on and follow the links at the bottom of this page.  Divergence between upstream/main and your feature branch If GitHub indicates that the branch of your Pull Request can no longer be merged automatically, you have to incorporate changes that have been made since you started into your branch. Our recommended way to do this is to rebase on main.   Guidelines  All code should have tests (see test coverage below for more details). All code should be documented. No changes are ever committed without review and approval by a core team member. Please ask politely on the PR or on the mailing list if you get no response to your pull request within a week.    Stylistic Guidelines  Set up your editor to follow PEP 8 (remove trailing white space, no tabs, etc.). Check code with pyflakes / flake8. Use NumPy data types instead of strings (np.uint8 instead of \"uint8\"). \nUse the following import conventions: import numpy as np\n  For C code, see NEP 45.    Test coverage Pull requests (PRs) that modify code should either have new tests, or modify existing tests to fail before the PR and pass afterwards. You should run the tests before pushing a PR. Running NumPy\u2019s test suite locally requires some additional packages, such as pytest and hypothesis. The additional testing dependencies are listed in test_requirements.txt in the top-level directory, and can conveniently be installed with: pip install -r test_requirements.txt\n Tests for a module should ideally cover all code in that module, i.e., statement coverage should be at 100%. To measure the test coverage, install pytest-cov and then run: $ python runtests.py --coverage\n This will create a report in build/coverage, which can be viewed with: $ firefox build/coverage/index.html\n   Building docs To build docs, run make from the doc directory. make help lists all targets. For example, to build the HTML documentation, you can run: make html\n To get the appropriate dependencies and other requirements, see Building the NumPy API and reference docs.  Fixing Warnings  \u201ccitation not found: R###\u201d There is probably an underscore after a reference in the first line of a docstring (e.g. [1]_). Use this method to find the source file: $ cd doc/build; grep -rin R#### \u201cDuplicate citation R###, other instance in\u2026\u201d\u201d There is probably a [2] without a [1] in one of the docstrings      Development process - details The rest of the story  \nGit Basics Install git Get the local copy of the code Updating the code Setting up git for NumPy development Git configuration Two and three dots in difference specs Additional Git Resources   \nSetting up and using your development environment Recommended development setup Testing builds Building in-place Other build options Using virtual environments Running tests Running Linting Rebuilding & cleaning the workspace Debugging Understanding the code & getting started   \nUsing Gitpod for NumPy development Gitpod Gitpod GitHub integration Forking the NumPy repository Starting Gitpod Quick workspace tour Development workflow with Gitpod Rendering the NumPy documentation FAQ\u2019s and troubleshooting   \nBuilding the NumPy API and reference docs Development environments Prerequisites Instructions   \nDevelopment workflow Basic workflow Additional things you might want to do   \nAdvanced debugging tools Finding C errors with additional tooling   \nReviewer Guidelines Who can be a reviewer? Communication Guidelines Reviewer Checklist Standard replies for reviewing   \nNumPy benchmarks Usage Writing benchmarks   NumPy C style guide \nReleasing a version How to Prepare a Release Step-by-Step Directions   \nNumPy governance NumPy project governance and decision-making   \nHow to contribute to the NumPy documentation Documentation team meetings What\u2019s needed Contributing fixes Contributing new pages Contributing indirectly Documentation style Documentation reading    NumPy-specific workflow is in numpy-development-workflow. \n"}, {"name": "Convenience Classes", "path": "reference/routines.polynomials.package", "type": "Convenience classes", "text": "Convenience Classes The following lists the various constants and methods common to all of the classes representing the various kinds of polynomials. In the following, the term Poly represents any one of the convenience classes (e.g. Polynomial, Chebyshev, Hermite, etc.) while the lowercase p represents an instance of a polynomial class.  Constants  \nPoly.domain \u2013 Default domain \nPoly.window \u2013 Default window \nPoly.basis_name \u2013 String used to represent the basis \nPoly.maxpower \u2013 Maximum value n such that p**n is allowed \nPoly.nickname \u2013 String used in printing    Creation Methods for creating polynomial instances.  \nPoly.basis(degree) \u2013 Basis polynomial of given degree \nPoly.identity() \u2013 p where p(x) = x for all x\n \nPoly.fit(x, y, deg) \u2013 p of degree deg with coefficients determined by the least-squares fit to the data x, y\n \nPoly.fromroots(roots) \u2013 p with specified roots \np.copy() \u2013 Create a copy of p\n    Conversion Methods for converting a polynomial instance of one kind to another.  \np.cast(Poly) \u2013 Convert p to instance of kind Poly\n \np.convert(Poly) \u2013 Convert p to instance of kind Poly or map between domain and window\n    Calculus  \np.deriv() \u2013 Take the derivative of p\n \np.integ() \u2013 Integrate p\n    Validation  \nPoly.has_samecoef(p1, p2) \u2013 Check if coefficients match \nPoly.has_samedomain(p1, p2) \u2013 Check if domains match \nPoly.has_sametype(p1, p2) \u2013 Check if types match \nPoly.has_samewindow(p1, p2) \u2013 Check if windows match    Misc  \np.linspace() \u2013 Return x, p(x) at equally-spaced points in domain\n \np.mapparms() \u2013 Return the parameters for the linear mapping between domain and window. \np.roots() \u2013 Return the roots of p. \np.trim() \u2013 Remove trailing coefficients. \np.cutdeg(degree) \u2013 Truncate p to given degree \np.truncate(size) \u2013 Truncate p to given size  \n"}, {"name": "Copies and views", "path": "user/basics.copies", "type": "User Guide", "text": "Copies and views When operating on NumPy arrays, it is possible to access the internal data buffer directly using a view without copying data around. This ensures good performance but can also cause unwanted problems if the user is not aware of how this works. Hence, it is important to know the difference between these two terms and to know which operations return copies and which return views. The NumPy array is a data structure consisting of two parts: the contiguous data buffer with the actual data elements and the metadata that contains information about the data buffer. The metadata includes data type, strides, and other important information that helps manipulate the ndarray easily. See the Internal organization of NumPy arrays section for a detailed look.  View It is possible to access the array differently by just changing certain metadata like stride and dtype without changing the data buffer. This creates a new way of looking at the data and these new arrays are called views. The data buffer remains the same, so any changes made to a view reflects in the original copy. A view can be forced through the ndarray.view method.   Copy When a new array is created by duplicating the data buffer as well as the metadata, it is called a copy. Changes made to the copy do not reflect on the original array. Making a copy is slower and memory-consuming but sometimes necessary. A copy can be forced by using ndarray.copy.   Indexing operations  See also Indexing on ndarrays  Views are created when elements can be addressed with offsets and strides in the original array. Hence, basic indexing always creates views. For example: >>> x = np.arange(10)\n>>> x\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n>>> y = x[1:3]  # creates a view\n>>> y\narray([1, 2])\n>>> x[1:3] = [10, 11]\n>>> x\narray([ 0, 10, 11,  3,  4,  5,  6,  7,  8,  9])\n>>> y\narray([10, 11])\n Here, y gets changed when x is changed because it is a view. Advanced indexing, on the other hand, always creates copies. For example: >>> x = np.arange(9).reshape(3, 3)\n>>> x\narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n>>> y = x[[1, 2]]\n>>> y\narray([[3, 4, 5],\n       [6, 7, 8]])\n>>> y.base is None\nTrue\n Here, y is a copy, as signified by the base attribute. We can also confirm this by assigning new values to x[[1, 2]] which in turn will not affect y at all: >>> x[[1, 2]] = [[10, 11, 12], [13, 14, 15]]\n>>> x\narray([[ 0,  1,  2],\n       [10, 11, 12],\n       [13, 14, 15]])\n>>> y\narray([[3, 4, 5],\n       [6, 7, 8]])\n It must be noted here that during the assignment of x[[1, 2]] no view or copy is created as the assignment happens in-place.   Other operations The numpy.reshape function creates a view where possible or a copy otherwise. In most cases, the strides can be modified to reshape the array with a view. However, in some cases where the array becomes non-contiguous (perhaps after a ndarray.transpose operation), the reshaping cannot be done by modifying strides and requires a copy. In these cases, we can raise an error by assigning the new shape to the shape attribute of the array. For example: >>> x = np.ones((2, 3))\n>>> y = x.T  # makes the array non-contiguous\n>>> y\narray([[1., 1.],\n       [1., 1.],\n       [1., 1.]])\n>>> z = y.view()\n>>> z.shape = 6\nTraceback (most recent call last):\n   ...\nAttributeError: Incompatible shape for in-place modification. Use\n`.reshape()` to make a copy with the desired shape.\n Taking the example of another operation, ravel returns a contiguous flattened view of the array wherever possible. On the other hand, ndarray.flatten always returns a flattened copy of the array. However, to guarantee a view in most cases, x.reshape(-1) may be preferable.   How to tell if the array is a view or a copy The base attribute of the ndarray makes it easy to tell if an array is a view or a copy. The base attribute of a view returns the original array while it returns None for a copy. >>> x = np.arange(9)\n>>> x\narray([0, 1, 2, 3, 4, 5, 6, 7, 8])\n>>> y = x.reshape(3, 3)\n>>> y\narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n>>> y.base  # .reshape() creates a view\narray([0, 1, 2, 3, 4, 5, 6, 7, 8])\n>>> z = y[[2, 1]]\n>>> z\narray([[6, 7, 8],\n       [3, 4, 5]])\n>>> z.base is None  # advanced indexing creates a copy\nTrue\n Note that the base attribute should not be used to determine if an ndarray object is new; only if it is a view or a copy of another ndarray. \n"}, {"name": "core.defchararray.array()", "path": "reference/generated/numpy.core.defchararray.array", "type": "numpy.core.defchararray.array", "text": "numpy.core.defchararray.array   core.defchararray.array(obj, itemsize=None, copy=True, unicode=None, order=None)[source]\n \nCreate a chararray.  Note This class is provided for numarray backward-compatibility. New code (not concerned with numarray compatibility) should use arrays of type string_ or unicode_ and use the free functions in numpy.char for fast vectorized string operations instead.  Versus a regular NumPy array of type str or unicode, this class adds the following functionality:  values automatically have whitespace removed from the end when indexed comparison operators automatically remove whitespace from the end when comparing values vectorized string operations are provided as methods (e.g. str.endswith) and infix operators (e.g. +, *, %)   Parameters \n \nobjarray of str or unicode-like\n\n\nitemsizeint, optional\n\n\nitemsize is the number of characters per scalar in the resulting array. If itemsize is None, and obj is an object array or a Python list, the itemsize will be automatically determined. If itemsize is provided and obj is of type str or unicode, then the obj string will be chunked into itemsize pieces.  \ncopybool, optional\n\n\nIf true (default), then the object is copied. Otherwise, a copy will only be made if __array__ returns a copy, if obj is a nested sequence, or if a copy is needed to satisfy any of the other requirements (itemsize, unicode, order, etc.).  \nunicodebool, optional\n\n\nWhen true, the resulting chararray can contain Unicode characters, when false only 8-bit characters. If unicode is None and obj is one of the following:  a chararray, an ndarray of type str or unicode\n a Python str or unicode object,  then the unicode setting of the output array will be automatically determined.  \norder{\u2018C\u2019, \u2018F\u2019, \u2018A\u2019}, optional\n\n\nSpecify the order of the array. If order is \u2018C\u2019 (default), then the array will be in C-contiguous order (last-index varies the fastest). If order is \u2018F\u2019, then the returned array will be in Fortran-contiguous order (first-index varies the fastest). If order is \u2018A\u2019, then the returned array may be in any order (either C-, Fortran-contiguous, or even discontiguous).     \n\n"}, {"name": "core.defchararray.asarray()", "path": "reference/generated/numpy.core.defchararray.asarray", "type": "numpy.core.defchararray.asarray", "text": "numpy.core.defchararray.asarray   core.defchararray.asarray(obj, itemsize=None, unicode=None, order=None)[source]\n \nConvert the input to a chararray, copying the data only if necessary. Versus a regular NumPy array of type str or unicode, this class adds the following functionality:  values automatically have whitespace removed from the end when indexed comparison operators automatically remove whitespace from the end when comparing values vectorized string operations are provided as methods (e.g. str.endswith) and infix operators (e.g. +, *,``%``)   Parameters \n \nobjarray of str or unicode-like\n\n\nitemsizeint, optional\n\n\nitemsize is the number of characters per scalar in the resulting array. If itemsize is None, and obj is an object array or a Python list, the itemsize will be automatically determined. If itemsize is provided and obj is of type str or unicode, then the obj string will be chunked into itemsize pieces.  \nunicodebool, optional\n\n\nWhen true, the resulting chararray can contain Unicode characters, when false only 8-bit characters. If unicode is None and obj is one of the following:  a chararray, an ndarray of type str or \u2018unicode` a Python str or unicode object,  then the unicode setting of the output array will be automatically determined.  \norder{\u2018C\u2019, \u2018F\u2019}, optional\n\n\nSpecify the order of the array. If order is \u2018C\u2019 (default), then the array will be in C-contiguous order (last-index varies the fastest). If order is \u2018F\u2019, then the returned array will be in Fortran-contiguous order (first-index varies the fastest).     \n\n"}, {"name": "core.records.array()", "path": "reference/generated/numpy.core.records.array", "type": "numpy.core.records.array", "text": "numpy.core.records.array   core.records.array(obj, dtype=None, shape=None, offset=0, strides=None, formats=None, names=None, titles=None, aligned=False, byteorder=None, copy=True)[source]\n \nConstruct a record array from a wide-variety of objects. A general-purpose record array constructor that dispatches to the appropriate recarray creation function based on the inputs (see Notes).  Parameters \n \nobjany\n\n\nInput object. See Notes for details on how various input types are treated.  \ndtypedata-type, optional\n\n\nValid dtype for array.  \nshapeint or tuple of ints, optional\n\n\nShape of each array.  \noffsetint, optional\n\n\nPosition in the file or buffer to start reading from.  \nstridestuple of ints, optional\n\n\nBuffer (buf) is interpreted according to these strides (strides define how many bytes each array element, row, column, etc. occupy in memory).  formats, names, titles, aligned, byteorder :\n\nIf dtype is None, these arguments are passed to numpy.format_parser to construct a dtype. See that function for detailed documentation.  \ncopybool, optional\n\n\nWhether to copy the input object (True), or to use a reference instead. This option only applies when the input is an ndarray or recarray. Defaults to True.    Returns \n np.recarray\n\nRecord array created from the specified object.     Notes If obj is None, then call the recarray constructor. If obj is a string, then call the fromstring constructor. If obj is a list or a tuple, then if the first object is an ndarray, call fromarrays, otherwise call fromrecords. If obj is a recarray, then make a copy of the data in the recarray (if copy=True) and use the new formats, names, and titles. If obj is a file, then call fromfile. Finally, if obj is an ndarray, then return obj.view(recarray), making a copy of the data if copy=True. Examples >>> a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n >>> np.core.records.array(a)\nrec.array([[1, 2, 3],\n           [4, 5, 6],\n           [7, 8, 9]],\n    dtype=int32)\n >>> b = [(1, 1), (2, 4), (3, 9)]\n>>> c = np.core.records.array(b, formats = ['i2', 'f2'], names = ('x', 'y'))\n>>> c\nrec.array([(1, 1.0), (2, 4.0), (3, 9.0)],\n          dtype=[('x', '<i2'), ('y', '<f2')])\n >>> c.x\nrec.array([1, 2, 3], dtype=int16)\n >>> c.y\nrec.array([ 1.0,  4.0,  9.0], dtype=float16)\n >>> r = np.rec.array(['abc','def'], names=['col1','col2'])\n>>> print(r.col1)\nabc\n >>> r.col1\narray('abc', dtype='<U3')\n >>> r.col2\narray('def', dtype='<U3')\n \n\n"}, {"name": "core.records.fromarrays()", "path": "reference/generated/numpy.core.records.fromarrays", "type": "numpy.core.records.fromarrays", "text": "numpy.core.records.fromarrays   core.records.fromarrays(arrayList, dtype=None, shape=None, formats=None, names=None, titles=None, aligned=False, byteorder=None)[source]\n \nCreate a record array from a (flat) list of arrays  Parameters \n \narrayListlist or tuple\n\n\nList of array-like objects (such as lists, tuples, and ndarrays).  \ndtypedata-type, optional\n\n\nvalid dtype for all arrays  \nshapeint or tuple of ints, optional\n\n\nShape of the resulting array. If not provided, inferred from arrayList[0].  formats, names, titles, aligned, byteorder :\n\nIf dtype is None, these arguments are passed to numpy.format_parser to construct a dtype. See that function for detailed documentation.    Returns \n np.recarray\n\nRecord array consisting of given arrayList columns.     Examples >>> x1=np.array([1,2,3,4])\n>>> x2=np.array(['a','dd','xyz','12'])\n>>> x3=np.array([1.1,2,3,4])\n>>> r = np.core.records.fromarrays([x1,x2,x3],names='a,b,c')\n>>> print(r[1])\n(2, 'dd', 2.0) # may vary\n>>> x1[1]=34\n>>> r.a\narray([1, 2, 3, 4])\n >>> x1 = np.array([1, 2, 3, 4])\n>>> x2 = np.array(['a', 'dd', 'xyz', '12'])\n>>> x3 = np.array([1.1, 2, 3,4])\n>>> r = np.core.records.fromarrays(\n...     [x1, x2, x3],\n...     dtype=np.dtype([('a', np.int32), ('b', 'S3'), ('c', np.float32)]))\n>>> r\nrec.array([(1, b'a', 1.1), (2, b'dd', 2. ), (3, b'xyz', 3. ),\n           (4, b'12', 4. )],\n          dtype=[('a', '<i4'), ('b', 'S3'), ('c', '<f4')])\n \n\n"}, {"name": "core.records.fromfile()", "path": "reference/generated/numpy.core.records.fromfile", "type": "numpy.core.records.fromfile", "text": "numpy.core.records.fromfile   core.records.fromfile(fd, dtype=None, shape=None, offset=0, formats=None, names=None, titles=None, aligned=False, byteorder=None)[source]\n \nCreate an array from binary file data  Parameters \n \nfdstr or file type\n\n\nIf file is a string or a path-like object then that file is opened, else it is assumed to be a file object. The file object must support random access (i.e. it must have tell and seek methods).  \ndtypedata-type, optional\n\n\nvalid dtype for all arrays  \nshapeint or tuple of ints, optional\n\n\nshape of each array.  \noffsetint, optional\n\n\nPosition in the file to start reading from.  formats, names, titles, aligned, byteorder :\n\nIf dtype is None, these arguments are passed to numpy.format_parser to construct a dtype. See that function for detailed documentation    Returns \n np.recarray\n\nrecord array consisting of data enclosed in file.     Examples >>> from tempfile import TemporaryFile\n>>> a = np.empty(10,dtype='f8,i4,a5')\n>>> a[5] = (0.5,10,'abcde')\n>>>\n>>> fd=TemporaryFile()\n>>> a = a.newbyteorder('<')\n>>> a.tofile(fd)\n>>>\n>>> _ = fd.seek(0)\n>>> r=np.core.records.fromfile(fd, formats='f8,i4,a5', shape=10,\n... byteorder='<')\n>>> print(r[5])\n(0.5, 10, 'abcde')\n>>> r.shape\n(10,)\n \n\n"}, {"name": "core.records.fromrecords()", "path": "reference/generated/numpy.core.records.fromrecords", "type": "numpy.core.records.fromrecords", "text": "numpy.core.records.fromrecords   core.records.fromrecords(recList, dtype=None, shape=None, formats=None, names=None, titles=None, aligned=False, byteorder=None)[source]\n \nCreate a recarray from a list of records in text form.  Parameters \n \nrecListsequence\n\n\ndata in the same field may be heterogeneous - they will be promoted to the highest data type.  \ndtypedata-type, optional\n\n\nvalid dtype for all arrays  \nshapeint or tuple of ints, optional\n\n\nshape of each array.  formats, names, titles, aligned, byteorder :\n\nIf dtype is None, these arguments are passed to numpy.format_parser to construct a dtype. See that function for detailed documentation. If both formats and dtype are None, then this will auto-detect formats. Use list of tuples rather than list of lists for faster processing.    Returns \n np.recarray\n\nrecord array consisting of given recList rows.     Examples >>> r=np.core.records.fromrecords([(456,'dbe',1.2),(2,'de',1.3)],\n... names='col1,col2,col3')\n>>> print(r[0])\n(456, 'dbe', 1.2)\n>>> r.col1\narray([456,   2])\n>>> r.col2\narray(['dbe', 'de'], dtype='<U3')\n>>> import pickle\n>>> pickle.loads(pickle.dumps(r))\nrec.array([(456, 'dbe', 1.2), (  2, 'de', 1.3)],\n          dtype=[('col1', '<i8'), ('col2', '<U3'), ('col3', '<f8')])\n \n\n"}, {"name": "core.records.fromstring()", "path": "reference/generated/numpy.core.records.fromstring", "type": "numpy.core.records.fromstring", "text": "numpy.core.records.fromstring   core.records.fromstring(datastring, dtype=None, shape=None, offset=0, formats=None, names=None, titles=None, aligned=False, byteorder=None)[source]\n \nCreate a record array from binary data Note that despite the name of this function it does not accept str instances.  Parameters \n \ndatastringbytes-like\n\n\nBuffer of binary data  \ndtypedata-type, optional\n\n\nValid dtype for all arrays  \nshapeint or tuple of ints, optional\n\n\nShape of each array.  \noffsetint, optional\n\n\nPosition in the buffer to start reading from.  formats, names, titles, aligned, byteorder :\n\nIf dtype is None, these arguments are passed to numpy.format_parser to construct a dtype. See that function for detailed documentation.    Returns \n np.recarray\n\nRecord array view into the data in datastring. This will be readonly if datastring is readonly.      See also  numpy.frombuffer\n  Examples >>> a = b'\\x01\\x02\\x03abc'\n>>> np.core.records.fromstring(a, dtype='u1,u1,u1,S3')\nrec.array([(1, 2, 3, b'abc')],\n        dtype=[('f0', 'u1'), ('f1', 'u1'), ('f2', 'u1'), ('f3', 'S3')])\n >>> grades_dtype = [('Name', (np.str_, 10)), ('Marks', np.float64),\n...                 ('GradeLevel', np.int32)]\n>>> grades_array = np.array([('Sam', 33.3, 3), ('Mike', 44.4, 5),\n...                         ('Aadi', 66.6, 6)], dtype=grades_dtype)\n>>> np.core.records.fromstring(grades_array.tobytes(), dtype=grades_dtype)\nrec.array([('Sam', 33.3, 3), ('Mike', 44.4, 5), ('Aadi', 66.6, 6)],\n        dtype=[('Name', '<U10'), ('Marks', '<f8'), ('GradeLevel', '<i4')])\n >>> s = '\\x01\\x02\\x03abc'\n>>> np.core.records.fromstring(s, dtype='u1,u1,u1,S3')\nTraceback (most recent call last)\n   ...\nTypeError: a bytes-like object is required, not 'str'\n \n\n"}, {"name": "CT", "path": "reference/routines.fft", "type": "Discrete Fourier Transform ( \n      \n       numpy.fft\n      \n      )", "text": "Discrete Fourier Transform (numpy.fft) The SciPy module scipy.fft is a more comprehensive superset of numpy.fft, which includes only a basic set of routines.  Standard FFTs  \nfft(a[, n, axis, norm]) Compute the one-dimensional discrete Fourier Transform.  \nifft(a[, n, axis, norm]) Compute the one-dimensional inverse discrete Fourier Transform.  \nfft2(a[, s, axes, norm]) Compute the 2-dimensional discrete Fourier Transform.  \nifft2(a[, s, axes, norm]) Compute the 2-dimensional inverse discrete Fourier Transform.  \nfftn(a[, s, axes, norm]) Compute the N-dimensional discrete Fourier Transform.  \nifftn(a[, s, axes, norm]) Compute the N-dimensional inverse discrete Fourier Transform.     Real FFTs  \nrfft(a[, n, axis, norm]) Compute the one-dimensional discrete Fourier Transform for real input.  \nirfft(a[, n, axis, norm]) Computes the inverse of rfft.  \nrfft2(a[, s, axes, norm]) Compute the 2-dimensional FFT of a real array.  \nirfft2(a[, s, axes, norm]) Computes the inverse of rfft2.  \nrfftn(a[, s, axes, norm]) Compute the N-dimensional discrete Fourier Transform for real input.  \nirfftn(a[, s, axes, norm]) Computes the inverse of rfftn.     Hermitian FFTs  \nhfft(a[, n, axis, norm]) Compute the FFT of a signal that has Hermitian symmetry, i.e., a real spectrum.  \nihfft(a[, n, axis, norm]) Compute the inverse FFT of a signal that has Hermitian symmetry.     Helper routines  \nfftfreq(n[, d]) Return the Discrete Fourier Transform sample frequencies.  \nrfftfreq(n[, d]) Return the Discrete Fourier Transform sample frequencies (for usage with rfft, irfft).  \nfftshift(x[, axes]) Shift the zero-frequency component to the center of the spectrum.  \nifftshift(x[, axes]) The inverse of fftshift.     Background information Fourier analysis is fundamentally a method for expressing a function as a sum of periodic components, and for recovering the function from those components. When both the function and its Fourier transform are replaced with discretized counterparts, it is called the discrete Fourier transform (DFT). The DFT has become a mainstay of numerical computing in part because of a very fast algorithm for computing it, called the Fast Fourier Transform (FFT), which was known to Gauss (1805) and was brought to light in its current form by Cooley and Tukey [CT]. Press et al. [NR] provide an accessible introduction to Fourier analysis and its applications. Because the discrete Fourier transform separates its input into components that contribute at discrete frequencies, it has a great number of applications in digital signal processing, e.g., for filtering, and in this context the discretized input to the transform is customarily referred to as a signal, which exists in the time domain. The output is called a spectrum or transform and exists in the frequency domain.   Implementation details There are many ways to define the DFT, varying in the sign of the exponent, normalization, etc. In this implementation, the DFT is defined as  \\[A_k = \\sum_{m=0}^{n-1} a_m \\exp\\left\\{-2\\pi i{mk \\over n}\\right\\} \\qquad k = 0,\\ldots,n-1.\\] The DFT is in general defined for complex inputs and outputs, and a single-frequency component at linear frequency \\(f\\) is represented by a complex exponential \\(a_m = \\exp\\{2\\pi i\\,f m\\Delta t\\}\\), where \\(\\Delta t\\) is the sampling interval. The values in the result follow so-called \u201cstandard\u201d order: If A =\nfft(a, n), then A[0] contains the zero-frequency term (the sum of the signal), which is always purely real for real inputs. Then A[1:n/2] contains the positive-frequency terms, and A[n/2+1:] contains the negative-frequency terms, in order of decreasingly negative frequency. For an even number of input points, A[n/2] represents both positive and negative Nyquist frequency, and is also purely real for real input. For an odd number of input points, A[(n-1)/2] contains the largest positive frequency, while A[(n+1)/2] contains the largest negative frequency. The routine np.fft.fftfreq(n) returns an array giving the frequencies of corresponding elements in the output. The routine np.fft.fftshift(A) shifts transforms and their frequencies to put the zero-frequency components in the middle, and np.fft.ifftshift(A) undoes that shift. When the input a is a time-domain signal and A = fft(a), np.abs(A) is its amplitude spectrum and np.abs(A)**2 is its power spectrum. The phase spectrum is obtained by np.angle(A). The inverse DFT is defined as  \\[a_m = \\frac{1}{n}\\sum_{k=0}^{n-1}A_k\\exp\\left\\{2\\pi i{mk\\over n}\\right\\} \\qquad m = 0,\\ldots,n-1.\\] It differs from the forward transform by the sign of the exponential argument and the default normalization by \\(1/n\\).   Type Promotion numpy.fft promotes float32 and complex64 arrays to float64 and complex128 arrays respectively. For an FFT implementation that does not promote input arrays, see scipy.fftpack.   Normalization The argument norm indicates which direction of the pair of direct/inverse transforms is scaled and with what normalization factor. The default normalization (\"backward\") has the direct (forward) transforms unscaled and the inverse (backward) transforms scaled by \\(1/n\\). It is possible to obtain unitary transforms by setting the keyword argument norm to \"ortho\" so that both direct and inverse transforms are scaled by \\(1/\\sqrt{n}\\). Finally, setting the keyword argument norm to \"forward\" has the direct transforms scaled by \\(1/n\\) and the inverse transforms unscaled (i.e. exactly opposite to the default \"backward\"). None is an alias of the default option \"backward\" for backward compatibility.   Real and Hermitian transforms When the input is purely real, its transform is Hermitian, i.e., the component at frequency \\(f_k\\) is the complex conjugate of the component at frequency \\(-f_k\\), which means that for real inputs there is no information in the negative frequency components that is not already available from the positive frequency components. The family of rfft functions is designed to operate on real inputs, and exploits this symmetry by computing only the positive frequency components, up to and including the Nyquist frequency. Thus, n input points produce n/2+1 complex output points. The inverses of this family assumes the same symmetry of its input, and for an output of n points uses n/2+1 input points. Correspondingly, when the spectrum is purely real, the signal is Hermitian. The hfft family of functions exploits this symmetry by using n/2+1 complex points in the input (time) domain for n real points in the frequency domain. In higher dimensions, FFTs are used, e.g., for image analysis and filtering. The computational efficiency of the FFT means that it can also be a faster way to compute large convolutions, using the property that a convolution in the time domain is equivalent to a point-by-point multiplication in the frequency domain.   Higher dimensions In two dimensions, the DFT is defined as  \\[A_{kl} = \\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} a_{mn}\\exp\\left\\{-2\\pi i \\left({mk\\over M}+{nl\\over N}\\right)\\right\\} \\qquad k = 0, \\ldots, M-1;\\quad l = 0, \\ldots, N-1,\\] which extends in the obvious way to higher dimensions, and the inverses in higher dimensions also extend in the same way.   References  CT \nCooley, James W., and John W. Tukey, 1965, \u201cAn algorithm for the machine calculation of complex Fourier series,\u201d Math. Comput. 19: 297-301.  NR \nPress, W., Teukolsky, S., Vetterline, W.T., and Flannery, B.P., 2007, Numerical Recipes: The Art of Scientific Computing, ch. 12-13. Cambridge Univ. Press, Cambridge, UK.     Examples For examples, see the various functions. \n"}, {"name": "Data type routines", "path": "reference/routines.dtype", "type": "Data type routines", "text": "Data type routines  \ncan_cast(from_, to[, casting]) Returns True if cast between data types can occur according to the casting rule.  \npromote_types(type1, type2) Returns the data type with the smallest size and smallest scalar kind to which both type1 and type2 may be safely cast.  \nmin_scalar_type(a, /) For scalar a, returns the data type with the smallest size and smallest scalar kind which can hold its value.  \nresult_type(*arrays_and_dtypes) Returns the type that results from applying the NumPy type promotion rules to the arguments.  \ncommon_type(*arrays) Return a scalar type which is common to the input arrays.  \nobj2sctype(rep[, default]) Return the scalar dtype or NumPy equivalent of Python type of an object.    Creating data types  \ndtype(dtype[, align, copy]) Create a data type object.  \nformat_parser(formats, names, titles[, ...]) Class to convert formats, names, titles description to a dtype.     Data type information  \nfinfo(dtype) Machine limits for floating point types.  \niinfo(type) Machine limits for integer types.  \nMachAr([float_conv, int_conv, ...]) Diagnosing machine parameters.     Data type testing  \nissctype(rep) Determines whether the given object represents a scalar data-type.  \nissubdtype(arg1, arg2) Returns True if first argument is a typecode lower/equal in type hierarchy.  \nissubsctype(arg1, arg2) Determine if the first argument is a subclass of the second argument.  \nissubclass_(arg1, arg2) Determine if a class is a subclass of a second class.  \nfind_common_type(array_types, scalar_types) Determine common type following standard coercion rules.     Miscellaneous  \ntypename(char) Return a description for the given data type code.  \nsctype2char(sctype) Return the string representation of a scalar dtype.  \nmintypecode(typechars[, typeset, default]) Return the character for the minimum-size type to which given types can be safely cast.  \nmaximum_sctype(t) Return the scalar type of highest precision of the same kind as the input.   \n"}, {"name": "Data types", "path": "user/basics.types", "type": "User Guide", "text": "Data types  See also Data type objects   Array types and conversions between types NumPy supports a much greater variety of numerical types than Python does. This section shows which are available, and how to modify an array\u2019s data-type. The primitive types supported are tied closely to those in C:   \nNumpy type C type Description   \nnumpy.bool_ bool Boolean (True or False) stored as a byte  \nnumpy.byte signed char Platform-defined  \nnumpy.ubyte unsigned char Platform-defined  \nnumpy.short short Platform-defined  \nnumpy.ushort unsigned short Platform-defined  \nnumpy.intc int Platform-defined  \nnumpy.uintc unsigned int Platform-defined  \nnumpy.int_ long Platform-defined  \nnumpy.uint unsigned long Platform-defined  \nnumpy.longlong long long Platform-defined  \nnumpy.ulonglong unsigned long long Platform-defined  \nnumpy.half / numpy.float16  Half precision float: sign bit, 5 bits exponent, 10 bits mantissa  \nnumpy.single float Platform-defined single precision float: typically sign bit, 8 bits exponent, 23 bits mantissa  \nnumpy.double double Platform-defined double precision float: typically sign bit, 11 bits exponent, 52 bits mantissa.  \nnumpy.longdouble long double Platform-defined extended-precision float  \nnumpy.csingle float complex Complex number, represented by two single-precision floats (real and imaginary components)  \nnumpy.cdouble double complex Complex number, represented by two double-precision floats (real and imaginary components).  \nnumpy.clongdouble long double complex Complex number, represented by two extended-precision floats (real and imaginary components).   Since many of these have platform-dependent definitions, a set of fixed-size aliases are provided (See Sized aliases). NumPy numerical types are instances of dtype (data-type) objects, each having unique characteristics. Once you have imported NumPy using >>> import numpy as np\n the dtypes are available as np.bool_, np.float32, etc. Advanced types, not listed above, are explored in section Structured arrays. There are 5 basic numerical types representing booleans (bool), integers (int), unsigned integers (uint) floating point (float) and complex. Those with numbers in their name indicate the bitsize of the type (i.e. how many bits are needed to represent a single value in memory). Some types, such as int and intp, have differing bitsizes, dependent on the platforms (e.g. 32-bit vs. 64-bit machines). This should be taken into account when interfacing with low-level code (such as C or Fortran) where the raw memory is addressed. Data-types can be used as functions to convert python numbers to array scalars (see the array scalar section for an explanation), python sequences of numbers to arrays of that type, or as arguments to the dtype keyword that many numpy functions or methods accept. Some examples: >>> import numpy as np\n>>> x = np.float32(1.0)\n>>> x\n1.0\n>>> y = np.int_([1,2,4])\n>>> y\narray([1, 2, 4])\n>>> z = np.arange(3, dtype=np.uint8)\n>>> z\narray([0, 1, 2], dtype=uint8)\n Array types can also be referred to by character codes, mostly to retain backward compatibility with older packages such as Numeric. Some documentation may still refer to these, for example: >>> np.array([1, 2, 3], dtype='f')\narray([ 1.,  2.,  3.], dtype=float32)\n We recommend using dtype objects instead. To convert the type of an array, use the .astype() method (preferred) or the type itself as a function. For example: >>> z.astype(float)                 \narray([  0.,  1.,  2.])\n>>> np.int8(z)\narray([0, 1, 2], dtype=int8)\n Note that, above, we use the Python float object as a dtype. NumPy knows that int refers to np.int_, bool means np.bool_, that float is np.float_ and complex is np.complex_. The other data-types do not have Python equivalents. To determine the type of an array, look at the dtype attribute: >>> z.dtype\ndtype('uint8')\n dtype objects also contain information about the type, such as its bit-width and its byte-order. The data type can also be used indirectly to query properties of the type, such as whether it is an integer: >>> d = np.dtype(int)\n>>> d\ndtype('int32')\n\n>>> np.issubdtype(d, np.integer)\nTrue\n\n>>> np.issubdtype(d, np.floating)\nFalse\n   Array Scalars NumPy generally returns elements of arrays as array scalars (a scalar with an associated dtype). Array scalars differ from Python scalars, but for the most part they can be used interchangeably (the primary exception is for versions of Python older than v2.x, where integer array scalars cannot act as indices for lists and tuples). There are some exceptions, such as when code requires very specific attributes of a scalar or when it checks specifically whether a value is a Python scalar. Generally, problems are easily fixed by explicitly converting array scalars to Python scalars, using the corresponding Python type function (e.g., int, float, complex, str, unicode). The primary advantage of using array scalars is that they preserve the array type (Python may not have a matching scalar type available, e.g. int16). Therefore, the use of array scalars ensures identical behaviour between arrays and scalars, irrespective of whether the value is inside an array or not. NumPy scalars also have many of the same methods arrays do.   Overflow Errors The fixed size of NumPy numeric types may cause overflow errors when a value requires more memory than available in the data type. For example, numpy.power evaluates 100 ** 8 correctly for 64-bit integers, but gives 1874919424 (incorrect) for a 32-bit integer. >>> np.power(100, 8, dtype=np.int64)\n10000000000000000\n>>> np.power(100, 8, dtype=np.int32)\n1874919424\n The behaviour of NumPy and Python integer types differs significantly for integer overflows and may confuse users expecting NumPy integers to behave similar to Python\u2019s int. Unlike NumPy, the size of Python\u2019s int is flexible. This means Python integers may expand to accommodate any integer and will not overflow. NumPy provides numpy.iinfo and numpy.finfo to verify the minimum or maximum values of NumPy integer and floating point values respectively >>> np.iinfo(int) # Bounds of the default integer on this system.\niinfo(min=-9223372036854775808, max=9223372036854775807, dtype=int64)\n>>> np.iinfo(np.int32) # Bounds of a 32-bit integer\niinfo(min=-2147483648, max=2147483647, dtype=int32)\n>>> np.iinfo(np.int64) # Bounds of a 64-bit integer\niinfo(min=-9223372036854775808, max=9223372036854775807, dtype=int64)\n If 64-bit integers are still too small the result may be cast to a floating point number. Floating point numbers offer a larger, but inexact, range of possible values. >>> np.power(100, 100, dtype=np.int64) # Incorrect even with 64-bit int\n0\n>>> np.power(100, 100, dtype=np.float64)\n1e+200\n   Extended Precision Python\u2019s floating-point numbers are usually 64-bit floating-point numbers, nearly equivalent to np.float64. In some unusual situations it may be useful to use floating-point numbers with more precision. Whether this is possible in numpy depends on the hardware and on the development environment: specifically, x86 machines provide hardware floating-point with 80-bit precision, and while most C compilers provide this as their long double type, MSVC (standard for Windows builds) makes long double identical to double (64 bits). NumPy makes the compiler\u2019s long double available as np.longdouble (and np.clongdouble for the complex numbers). You can find out what your numpy provides with np.finfo(np.longdouble). NumPy does not provide a dtype with more precision than C\u2019s long double\\; in particular, the 128-bit IEEE quad precision data type (FORTRAN\u2019s REAL*16\\) is not available. For efficient memory alignment, np.longdouble is usually stored padded with zero bits, either to 96 or 128 bits. Which is more efficient depends on hardware and development environment; typically on 32-bit systems they are padded to 96 bits, while on 64-bit systems they are typically padded to 128 bits. np.longdouble is padded to the system default; np.float96 and np.float128 are provided for users who want specific padding. In spite of the names, np.float96 and np.float128 provide only as much precision as np.longdouble, that is, 80 bits on most x86 machines and 64 bits in standard Windows builds. Be warned that even if np.longdouble offers more precision than python float, it is easy to lose that extra precision, since python often forces values to pass through float. For example, the % formatting operator requires its arguments to be converted to standard python types, and it is therefore impossible to preserve extended precision even if many decimal places are requested. It can be useful to test your code with the value 1 + np.finfo(np.longdouble).eps. \n"}, {"name": "DataSource.abspath()", "path": "reference/generated/numpy.datasource.abspath", "type": "numpy.DataSource.abspath", "text": "numpy.DataSource.abspath method   DataSource.abspath(path)[source]\n \nReturn absolute path of file in the DataSource directory. If path is an URL, then abspath will return either the location the file exists locally or the location it would exist when opened using the open method.  Parameters \n \npathstr\n\n\nCan be a local file or a remote URL.    Returns \n \noutstr\n\n\nComplete path, including the DataSource destination directory.     Notes The functionality is based on os.path.abspath. \n\n"}, {"name": "DataSource.exists()", "path": "reference/generated/numpy.datasource.exists", "type": "numpy.DataSource.exists", "text": "numpy.DataSource.exists method   DataSource.exists(path)[source]\n \nTest if path exists. Test if path exists as (and in this order):  a local file. a remote URL that has been downloaded and stored locally in the DataSource directory. a remote URL that has not been downloaded, but is valid and accessible.   Parameters \n \npathstr\n\n\nCan be a local file or a remote URL.    Returns \n \noutbool\n\n\nTrue if path exists.     Notes When path is an URL, exists will return True if it\u2019s either stored locally in the DataSource directory, or is a valid remote URL. DataSource does not discriminate between the two, the file is accessible if it exists in either location. \n\n"}, {"name": "DataSource.open()", "path": "reference/generated/numpy.datasource.open", "type": "numpy.DataSource.open", "text": "numpy.DataSource.open method   DataSource.open(path, mode='r', encoding=None, newline=None)[source]\n \nOpen and return file-like object. If path is an URL, it will be downloaded, stored in the DataSource directory and opened from there.  Parameters \n \npathstr\n\n\nLocal file path or URL to open.  \nmode{\u2018r\u2019, \u2018w\u2019, \u2018a\u2019}, optional\n\n\nMode to open path. Mode \u2018r\u2019 for reading, \u2018w\u2019 for writing, \u2018a\u2019 to append. Available modes depend on the type of object specified by path. Default is \u2018r\u2019.  \nencoding{None, str}, optional\n\n\nOpen text file with given encoding. The default encoding will be what io.open uses.  \nnewline{None, str}, optional\n\n\nNewline to use when reading text file.    Returns \n \noutfile object\n\n\nFile object.     \n\n"}, {"name": "Datetime Support Functions", "path": "reference/routines.datetime", "type": "Datetime Support Functions", "text": "Datetime Support Functions  \ndatetime_as_string(arr[, unit, timezone, ...]) Convert an array of datetimes into an array of strings.  \ndatetime_data(dtype, /) Get information about the step size of a date or time type.    Business Day Functions  \nbusdaycalendar([weekmask, holidays]) A business day calendar object that efficiently stores information defining valid days for the busday family of functions.  \nis_busday(dates[, weekmask, holidays, ...]) Calculates which of the given dates are valid days, and which are not.  \nbusday_offset(dates, offsets[, roll, ...]) First adjusts the date to fall on a valid day according to the roll rule, then applies offsets to the given dates counted in valid days.  \nbusday_count(begindates, enddates[, ...]) Counts the number of valid days between begindates and enddates, not including the day of enddates.   \n"}, {"name": "Datetimes and Timedeltas", "path": "reference/arrays.datetime", "type": "Datetimes and Timedeltas", "text": "Datetimes and Timedeltas  New in version 1.7.0.  Starting in NumPy 1.7, there are core array data types which natively support datetime functionality. The data type is called \u201cdatetime64\u201d, so named because \u201cdatetime\u201d is already taken by the datetime library included in Python.  Basic Datetimes The most basic way to create datetimes is from strings in ISO 8601 date or datetime format. It is also possible to create datetimes from an integer by offset relative to the Unix epoch (00:00:00 UTC on 1 January 1970). The unit for internal storage is automatically selected from the form of the string, and can be either a date unit or a time unit. The date units are years (\u2018Y\u2019), months (\u2018M\u2019), weeks (\u2018W\u2019), and days (\u2018D\u2019), while the time units are hours (\u2018h\u2019), minutes (\u2018m\u2019), seconds (\u2018s\u2019), milliseconds (\u2018ms\u2019), and some additional SI-prefix seconds-based units. The datetime64 data type also accepts the string \u201cNAT\u201d, in any combination of lowercase/uppercase letters, for a \u201cNot A Time\u201d value. Example A simple ISO date: >>> np.datetime64('2005-02-25')\nnumpy.datetime64('2005-02-25')\n From an integer and a date unit, 1 year since the UNIX epoch: >>> np.datetime64(1, 'Y')\nnumpy.datetime64('1971')\n Using months for the unit: >>> np.datetime64('2005-02')\nnumpy.datetime64('2005-02')\n Specifying just the month, but forcing a \u2018days\u2019 unit: >>> np.datetime64('2005-02', 'D')\nnumpy.datetime64('2005-02-01')\n From a date and time: >>> np.datetime64('2005-02-25T03:30')\nnumpy.datetime64('2005-02-25T03:30')\n NAT (not a time): >>> np.datetime64('nat')\nnumpy.datetime64('NaT')\n When creating an array of datetimes from a string, it is still possible to automatically select the unit from the inputs, by using the datetime type with generic units. Example >>> np.array(['2007-07-13', '2006-01-13', '2010-08-13'], dtype='datetime64')\narray(['2007-07-13', '2006-01-13', '2010-08-13'], dtype='datetime64[D]')\n >>> np.array(['2001-01-01T12:00', '2002-02-03T13:56:03.172'], dtype='datetime64')\narray(['2001-01-01T12:00:00.000', '2002-02-03T13:56:03.172'],\n      dtype='datetime64[ms]')\n An array of datetimes can be constructed from integers representing POSIX timestamps with the given unit. Example >>> np.array([0, 1577836800], dtype='datetime64[s]')\narray(['1970-01-01T00:00:00', '2020-01-01T00:00:00'],\n      dtype='datetime64[s]')\n >>> np.array([0, 1577836800000]).astype('datetime64[ms]')\narray(['1970-01-01T00:00:00.000', '2020-01-01T00:00:00.000'],\n      dtype='datetime64[ms]')\n The datetime type works with many common NumPy functions, for example arange can be used to generate ranges of dates. Example All the dates for one month: >>> np.arange('2005-02', '2005-03', dtype='datetime64[D]')\narray(['2005-02-01', '2005-02-02', '2005-02-03', '2005-02-04',\n       '2005-02-05', '2005-02-06', '2005-02-07', '2005-02-08',\n       '2005-02-09', '2005-02-10', '2005-02-11', '2005-02-12',\n       '2005-02-13', '2005-02-14', '2005-02-15', '2005-02-16',\n       '2005-02-17', '2005-02-18', '2005-02-19', '2005-02-20',\n       '2005-02-21', '2005-02-22', '2005-02-23', '2005-02-24',\n       '2005-02-25', '2005-02-26', '2005-02-27', '2005-02-28'],\n      dtype='datetime64[D]')\n The datetime object represents a single moment in time. If two datetimes have different units, they may still be representing the same moment of time, and converting from a bigger unit like months to a smaller unit like days is considered a \u2018safe\u2019 cast because the moment of time is still being represented exactly. Example >>> np.datetime64('2005') == np.datetime64('2005-01-01')\nTrue\n >>> np.datetime64('2010-03-14T15') == np.datetime64('2010-03-14T15:00:00.00')\nTrue\n  Deprecated since version 1.11.0: NumPy does not store timezone information. For backwards compatibility, datetime64 still parses timezone offsets, which it handles by converting to UTC. This behaviour is deprecated and will raise an error in the future.    Datetime and Timedelta Arithmetic NumPy allows the subtraction of two Datetime values, an operation which produces a number with a time unit. Because NumPy doesn\u2019t have a physical quantities system in its core, the timedelta64 data type was created to complement datetime64. The arguments for timedelta64 are a number, to represent the number of units, and a date/time unit, such as (D)ay, (M)onth, (Y)ear, (h)ours, (m)inutes, or (s)econds. The timedelta64 data type also accepts the string \u201cNAT\u201d in place of the number for a \u201cNot A Time\u201d value. Example >>> np.timedelta64(1, 'D')\nnumpy.timedelta64(1,'D')\n >>> np.timedelta64(4, 'h')\nnumpy.timedelta64(4,'h')\n >>> np.timedelta64('nAt')\nnumpy.timedelta64('NaT')\n Datetimes and Timedeltas work together to provide ways for simple datetime calculations. Example >>> np.datetime64('2009-01-01') - np.datetime64('2008-01-01')\nnumpy.timedelta64(366,'D')\n >>> np.datetime64('2009') + np.timedelta64(20, 'D')\nnumpy.datetime64('2009-01-21')\n >>> np.datetime64('2011-06-15T00:00') + np.timedelta64(12, 'h')\nnumpy.datetime64('2011-06-15T12:00')\n >>> np.timedelta64(1,'W') / np.timedelta64(1,'D')\n7.0\n >>> np.timedelta64(1,'W') % np.timedelta64(10,'D')\nnumpy.timedelta64(7,'D')\n >>> np.datetime64('nat') - np.datetime64('2009-01-01')\nnumpy.timedelta64('NaT','D')\n >>> np.datetime64('2009-01-01') + np.timedelta64('nat')\nnumpy.datetime64('NaT')\n There are two Timedelta units (\u2018Y\u2019, years and \u2018M\u2019, months) which are treated specially, because how much time they represent changes depending on when they are used. While a timedelta day unit is equivalent to 24 hours, there is no way to convert a month unit into days, because different months have different numbers of days. Example >>> a = np.timedelta64(1, 'Y')\n >>> np.timedelta64(a, 'M')\nnumpy.timedelta64(12,'M')\n >>> np.timedelta64(a, 'D')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: Cannot cast NumPy timedelta64 scalar from metadata [Y] to [D] according to the rule 'same_kind'\n   Datetime Units The Datetime and Timedelta data types support a large number of time units, as well as generic units which can be coerced into any of the other units based on input data. Datetimes are always stored based on POSIX time (though having a TAI mode which allows for accounting of leap-seconds is proposed), with an epoch of 1970-01-01T00:00Z. This means the supported dates are always a symmetric interval around the epoch, called \u201ctime span\u201d in the table below. The length of the span is the range of a 64-bit integer times the length of the date or unit. For example, the time span for \u2018W\u2019 (week) is exactly 7 times longer than the time span for \u2018D\u2019 (day), and the time span for \u2018D\u2019 (day) is exactly 24 times longer than the time span for \u2018h\u2019 (hour). Here are the date units:   \nCode Meaning Time span (relative) Time span (absolute)   \nY year +/- 9.2e18 years [9.2e18 BC, 9.2e18 AD]  \nM month +/- 7.6e17 years [7.6e17 BC, 7.6e17 AD]  \nW week +/- 1.7e17 years [1.7e17 BC, 1.7e17 AD]  \nD day +/- 2.5e16 years [2.5e16 BC, 2.5e16 AD]   And here are the time units:   \nCode Meaning Time span (relative) Time span (absolute)   \nh hour +/- 1.0e15 years [1.0e15 BC, 1.0e15 AD]  \nm minute +/- 1.7e13 years [1.7e13 BC, 1.7e13 AD]  \ns second +/- 2.9e11 years [2.9e11 BC, 2.9e11 AD]  \nms millisecond +/- 2.9e8 years [ 2.9e8 BC, 2.9e8 AD]  \nus / \u03bcs microsecond +/- 2.9e5 years [290301 BC, 294241 AD]  \nns nanosecond +/- 292 years [ 1678 AD, 2262 AD]  \nps picosecond +/- 106 days [ 1969 AD, 1970 AD]  \nfs femtosecond +/- 2.6 hours [ 1969 AD, 1970 AD]  \nas attosecond +/- 9.2 seconds [ 1969 AD, 1970 AD]     Business Day Functionality To allow the datetime to be used in contexts where only certain days of the week are valid, NumPy includes a set of \u201cbusday\u201d (business day) functions. The default for busday functions is that the only valid days are Monday through Friday (the usual business days). The implementation is based on a \u201cweekmask\u201d containing 7 Boolean flags to indicate valid days; custom weekmasks are possible that specify other sets of valid days. The \u201cbusday\u201d functions can additionally check a list of \u201choliday\u201d dates, specific dates that are not valid days. The function busday_offset allows you to apply offsets specified in business days to datetimes with a unit of \u2018D\u2019 (day). Example >>> np.busday_offset('2011-06-23', 1)\nnumpy.datetime64('2011-06-24')\n >>> np.busday_offset('2011-06-23', 2)\nnumpy.datetime64('2011-06-27')\n When an input date falls on the weekend or a holiday, busday_offset first applies a rule to roll the date to a valid business day, then applies the offset. The default rule is \u2018raise\u2019, which simply raises an exception. The rules most typically used are \u2018forward\u2019 and \u2018backward\u2019. Example >>> np.busday_offset('2011-06-25', 2)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nValueError: Non-business day date in busday_offset\n >>> np.busday_offset('2011-06-25', 0, roll='forward')\nnumpy.datetime64('2011-06-27')\n >>> np.busday_offset('2011-06-25', 2, roll='forward')\nnumpy.datetime64('2011-06-29')\n >>> np.busday_offset('2011-06-25', 0, roll='backward')\nnumpy.datetime64('2011-06-24')\n >>> np.busday_offset('2011-06-25', 2, roll='backward')\nnumpy.datetime64('2011-06-28')\n In some cases, an appropriate use of the roll and the offset is necessary to get a desired answer. Example The first business day on or after a date: >>> np.busday_offset('2011-03-20', 0, roll='forward')\nnumpy.datetime64('2011-03-21')\n>>> np.busday_offset('2011-03-22', 0, roll='forward')\nnumpy.datetime64('2011-03-22')\n The first business day strictly after a date: >>> np.busday_offset('2011-03-20', 1, roll='backward')\nnumpy.datetime64('2011-03-21')\n>>> np.busday_offset('2011-03-22', 1, roll='backward')\nnumpy.datetime64('2011-03-23')\n The function is also useful for computing some kinds of days like holidays. In Canada and the U.S., Mother\u2019s day is on the second Sunday in May, which can be computed with a custom weekmask. Example >>> np.busday_offset('2012-05', 1, roll='forward', weekmask='Sun')\nnumpy.datetime64('2012-05-13')\n When performance is important for manipulating many business dates with one particular choice of weekmask and holidays, there is an object busdaycalendar which stores the data necessary in an optimized form.  np.is_busday(): To test a datetime64 value to see if it is a valid day, use is_busday. Example >>> np.is_busday(np.datetime64('2011-07-15'))  # a Friday\nTrue\n>>> np.is_busday(np.datetime64('2011-07-16')) # a Saturday\nFalse\n>>> np.is_busday(np.datetime64('2011-07-16'), weekmask=\"Sat Sun\")\nTrue\n>>> a = np.arange(np.datetime64('2011-07-11'), np.datetime64('2011-07-18'))\n>>> np.is_busday(a)\narray([ True,  True,  True,  True,  True, False, False])\n   np.busday_count(): To find how many valid days there are in a specified range of datetime64 dates, use busday_count: Example >>> np.busday_count(np.datetime64('2011-07-11'), np.datetime64('2011-07-18'))\n5\n>>> np.busday_count(np.datetime64('2011-07-18'), np.datetime64('2011-07-11'))\n-5\n If you have an array of datetime64 day values, and you want a count of how many of them are valid dates, you can do this: Example >>> a = np.arange(np.datetime64('2011-07-11'), np.datetime64('2011-07-18'))\n>>> np.count_nonzero(np.is_busday(a))\n5\n  Custom Weekmasks Here are several examples of custom weekmask values. These examples specify the \u201cbusday\u201d default of Monday through Friday being valid days. Some examples: # Positional sequences; positions are Monday through Sunday.\n# Length of the sequence must be exactly 7.\nweekmask = [1, 1, 1, 1, 1, 0, 0]\n# list or other sequence; 0 == invalid day, 1 == valid day\nweekmask = \"1111100\"\n# string '0' == invalid day, '1' == valid day\n\n# string abbreviations from this list: Mon Tue Wed Thu Fri Sat Sun\nweekmask = \"Mon Tue Wed Thu Fri\"\n# any amount of whitespace is allowed; abbreviations are case-sensitive.\nweekmask = \"MonTue Wed  Thu\\tFri\"\n   \n"}, {"name": "deletechars", "path": "user/basics.io.genfromtxt", "type": "User Guide", "text": "Importing data with genfromtxt NumPy provides several functions to create arrays from tabular data. We focus here on the genfromtxt function. In a nutshell, genfromtxt runs two main loops. The first loop converts each line of the file in a sequence of strings. The second loop converts each string to the appropriate data type. This mechanism is slower than a single loop, but gives more flexibility. In particular, genfromtxt is able to take missing data into account, when other faster and simpler functions like loadtxt cannot.  Note When giving examples, we will use the following conventions: >>> import numpy as np\n>>> from io import StringIO\n   Defining the input The only mandatory argument of genfromtxt is the source of the data. It can be a string, a list of strings, a generator or an open file-like object with a read method, for example, a file or io.StringIO object. If a single string is provided, it is assumed to be the name of a local or remote file. If a list of strings or a generator returning strings is provided, each string is treated as one line in a file. When the URL of a remote file is passed, the file is automatically downloaded to the current directory and opened. Recognized file types are text files and archives. Currently, the function recognizes gzip and bz2 (bzip2) archives. The type of the archive is determined from the extension of the file: if the filename ends with '.gz', a gzip archive is expected; if it ends with 'bz2', a bzip2 archive is assumed.   Splitting the lines into columns  The delimiter argument Once the file is defined and open for reading, genfromtxt splits each non-empty line into a sequence of strings. Empty or commented lines are just skipped. The delimiter keyword is used to define how the splitting should take place. Quite often, a single character marks the separation between columns. For example, comma-separated files (CSV) use a comma (,) or a semicolon (;) as delimiter: >>> data = u\"1, 2, 3\\n4, 5, 6\"\n>>> np.genfromtxt(StringIO(data), delimiter=\",\")\narray([[ 1.,  2.,  3.],\n       [ 4.,  5.,  6.]])\n Another common separator is \"\\t\", the tabulation character. However, we are not limited to a single character, any string will do. By default, genfromtxt assumes delimiter=None, meaning that the line is split along white spaces (including tabs) and that consecutive white spaces are considered as a single white space. Alternatively, we may be dealing with a fixed-width file, where columns are defined as a given number of characters. In that case, we need to set delimiter to a single integer (if all the columns have the same size) or to a sequence of integers (if columns can have different sizes): >>> data = u\"  1  2  3\\n  4  5 67\\n890123  4\"\n>>> np.genfromtxt(StringIO(data), delimiter=3)\narray([[   1.,    2.,    3.],\n       [   4.,    5.,   67.],\n       [ 890.,  123.,    4.]])\n>>> data = u\"123456789\\n   4  7 9\\n   4567 9\"\n>>> np.genfromtxt(StringIO(data), delimiter=(4, 3, 2))\narray([[ 1234.,   567.,    89.],\n       [    4.,     7.,     9.],\n       [    4.,   567.,     9.]])\n   The autostrip argument By default, when a line is decomposed into a series of strings, the individual entries are not stripped of leading nor trailing white spaces. This behavior can be overwritten by setting the optional argument autostrip to a value of True: >>> data = u\"1, abc , 2\\n 3, xxx, 4\"\n>>> # Without autostrip\n>>> np.genfromtxt(StringIO(data), delimiter=\",\", dtype=\"|U5\")\narray([['1', ' abc ', ' 2'],\n       ['3', ' xxx', ' 4']], dtype='<U5')\n>>> # With autostrip\n>>> np.genfromtxt(StringIO(data), delimiter=\",\", dtype=\"|U5\", autostrip=True)\narray([['1', 'abc', '2'],\n       ['3', 'xxx', '4']], dtype='<U5')\n   The comments argument The optional argument comments is used to define a character string that marks the beginning of a comment. By default, genfromtxt assumes comments='#'. The comment marker may occur anywhere on the line. Any character present after the comment marker(s) is simply ignored: >>> data = u\"\"\"#\n... # Skip me !\n... # Skip me too !\n... 1, 2\n... 3, 4\n... 5, 6 #This is the third line of the data\n... 7, 8\n... # And here comes the last line\n... 9, 0\n... \"\"\"\n>>> np.genfromtxt(StringIO(data), comments=\"#\", delimiter=\",\")\narray([[1., 2.],\n       [3., 4.],\n       [5., 6.],\n       [7., 8.],\n       [9., 0.]])\n  New in version 1.7.0: When comments is set to None, no lines are treated as comments.   Note There is one notable exception to this behavior: if the optional argument names=True, the first commented line will be examined for names.     Skipping lines and choosing columns  The skip_header and skip_footer arguments The presence of a header in the file can hinder data processing. In that case, we need to use the skip_header optional argument. The values of this argument must be an integer which corresponds to the number of lines to skip at the beginning of the file, before any other action is performed. Similarly, we can skip the last n lines of the file by using the skip_footer attribute and giving it a value of n: >>> data = u\"\\n\".join(str(i) for i in range(10))\n>>> np.genfromtxt(StringIO(data),)\narray([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])\n>>> np.genfromtxt(StringIO(data),\n...               skip_header=3, skip_footer=5)\narray([ 3.,  4.])\n By default, skip_header=0 and skip_footer=0, meaning that no lines are skipped.   The usecols argument In some cases, we are not interested in all the columns of the data but only a few of them. We can select which columns to import with the usecols argument. This argument accepts a single integer or a sequence of integers corresponding to the indices of the columns to import. Remember that by convention, the first column has an index of 0. Negative integers behave the same as regular Python negative indexes. For example, if we want to import only the first and the last columns, we can use usecols=(0, -1): >>> data = u\"1 2 3\\n4 5 6\"\n>>> np.genfromtxt(StringIO(data), usecols=(0, -1))\narray([[ 1.,  3.],\n       [ 4.,  6.]])\n If the columns have names, we can also select which columns to import by giving their name to the usecols argument, either as a sequence of strings or a comma-separated string: >>> data = u\"1 2 3\\n4 5 6\"\n>>> np.genfromtxt(StringIO(data),\n...               names=\"a, b, c\", usecols=(\"a\", \"c\"))\narray([(1.0, 3.0), (4.0, 6.0)],\n      dtype=[('a', '<f8'), ('c', '<f8')])\n>>> np.genfromtxt(StringIO(data),\n...               names=\"a, b, c\", usecols=(\"a, c\"))\n    array([(1.0, 3.0), (4.0, 6.0)],\n          dtype=[('a', '<f8'), ('c', '<f8')])\n    Choosing the data type The main way to control how the sequences of strings we have read from the file are converted to other types is to set the dtype argument. Acceptable values for this argument are:  a single type, such as dtype=float. The output will be 2D with the given dtype, unless a name has been associated with each column with the use of the names argument (see below). Note that dtype=float is the default for genfromtxt. a sequence of types, such as dtype=(int, float, float). a comma-separated string, such as dtype=\"i4,f8,|U3\". a dictionary with two keys 'names' and 'formats'. a sequence of tuples (name, type), such as dtype=[('A', int), ('B', float)]. an existing numpy.dtype object. the special value None. In that case, the type of the columns will be determined from the data itself (see below).  In all the cases but the first one, the output will be a 1D array with a structured dtype. This dtype has as many fields as items in the sequence. The field names are defined with the names keyword. When dtype=None, the type of each column is determined iteratively from its data. We start by checking whether a string can be converted to a boolean (that is, if the string matches true or false in lower cases); then whether it can be converted to an integer, then to a float, then to a complex and eventually to a string. This behavior may be changed by modifying the default mapper of the StringConverter class. The option dtype=None is provided for convenience. However, it is significantly slower than setting the dtype explicitly.   Setting the names  The names argument A natural approach when dealing with tabular data is to allocate a name to each column. A first possibility is to use an explicit structured dtype, as mentioned previously: >>> data = StringIO(\"1 2 3\\n 4 5 6\")\n>>> np.genfromtxt(data, dtype=[(_, int) for _ in \"abc\"])\narray([(1, 2, 3), (4, 5, 6)],\n      dtype=[('a', '<i8'), ('b', '<i8'), ('c', '<i8')])\n Another simpler possibility is to use the names keyword with a sequence of strings or a comma-separated string: >>> data = StringIO(\"1 2 3\\n 4 5 6\")\n>>> np.genfromtxt(data, names=\"A, B, C\")\narray([(1.0, 2.0, 3.0), (4.0, 5.0, 6.0)],\n      dtype=[('A', '<f8'), ('B', '<f8'), ('C', '<f8')])\n In the example above, we used the fact that by default, dtype=float. By giving a sequence of names, we are forcing the output to a structured dtype. We may sometimes need to define the column names from the data itself. In that case, we must use the names keyword with a value of True. The names will then be read from the first line (after the skip_header ones), even if the line is commented out: >>> data = StringIO(\"So it goes\\n#a b c\\n1 2 3\\n 4 5 6\")\n>>> np.genfromtxt(data, skip_header=1, names=True)\narray([(1.0, 2.0, 3.0), (4.0, 5.0, 6.0)],\n      dtype=[('a', '<f8'), ('b', '<f8'), ('c', '<f8')])\n The default value of names is None. If we give any other value to the keyword, the new names will overwrite the field names we may have defined with the dtype: >>> data = StringIO(\"1 2 3\\n 4 5 6\")\n>>> ndtype=[('a',int), ('b', float), ('c', int)]\n>>> names = [\"A\", \"B\", \"C\"]\n>>> np.genfromtxt(data, names=names, dtype=ndtype)\narray([(1, 2.0, 3), (4, 5.0, 6)],\n      dtype=[('A', '<i8'), ('B', '<f8'), ('C', '<i8')])\n   The defaultfmt argument If names=None but a structured dtype is expected, names are defined with the standard NumPy default of \"f%i\", yielding names like f0, f1 and so forth: >>> data = StringIO(\"1 2 3\\n 4 5 6\")\n>>> np.genfromtxt(data, dtype=(int, float, int))\narray([(1, 2.0, 3), (4, 5.0, 6)],\n      dtype=[('f0', '<i8'), ('f1', '<f8'), ('f2', '<i8')])\n In the same way, if we don\u2019t give enough names to match the length of the dtype, the missing names will be defined with this default template: >>> data = StringIO(\"1 2 3\\n 4 5 6\")\n>>> np.genfromtxt(data, dtype=(int, float, int), names=\"a\")\narray([(1, 2.0, 3), (4, 5.0, 6)],\n      dtype=[('a', '<i8'), ('f0', '<f8'), ('f1', '<i8')])\n We can overwrite this default with the defaultfmt argument, that takes any format string: >>> data = StringIO(\"1 2 3\\n 4 5 6\")\n>>> np.genfromtxt(data, dtype=(int, float, int), defaultfmt=\"var_%02i\")\narray([(1, 2.0, 3), (4, 5.0, 6)],\n      dtype=[('var_00', '<i8'), ('var_01', '<f8'), ('var_02', '<i8')])\n  Note We need to keep in mind that defaultfmt is used only if some names are expected but not defined.    Validating names NumPy arrays with a structured dtype can also be viewed as recarray, where a field can be accessed as if it were an attribute. For that reason, we may need to make sure that the field name doesn\u2019t contain any space or invalid character, or that it does not correspond to the name of a standard attribute (like size or shape), which would confuse the interpreter. genfromtxt accepts three optional arguments that provide a finer control on the names:  deletechars\n\nGives a string combining all the characters that must be deleted from the name. By default, invalid characters are ~!@#$%^&*()-=+~\\|]}[{';:\n/?.>,<.  excludelist\n\nGives a list of the names to exclude, such as return, file, print\u2026 If one of the input name is part of this list, an underscore character ('_') will be appended to it.  case_sensitive\n\nWhether the names should be case-sensitive (case_sensitive=True), converted to upper case (case_sensitive=False or case_sensitive='upper') or to lower case (case_sensitive='lower').      Tweaking the conversion  The converters argument Usually, defining a dtype is sufficient to define how the sequence of strings must be converted. However, some additional control may sometimes be required. For example, we may want to make sure that a date in a format YYYY/MM/DD is converted to a datetime object, or that a string like xx% is properly converted to a float between 0 and 1. In such cases, we should define conversion functions with the converters arguments. The value of this argument is typically a dictionary with column indices or column names as keys and a conversion functions as values. These conversion functions can either be actual functions or lambda functions. In any case, they should accept only a string as input and output only a single element of the wanted type. In the following example, the second column is converted from as string representing a percentage to a float between 0 and 1: >>> convertfunc = lambda x: float(x.strip(b\"%\"))/100.\n>>> data = u\"1, 2.3%, 45.\\n6, 78.9%, 0\"\n>>> names = (\"i\", \"p\", \"n\")\n>>> # General case .....\n>>> np.genfromtxt(StringIO(data), delimiter=\",\", names=names)\narray([(1., nan, 45.), (6., nan, 0.)],\n      dtype=[('i', '<f8'), ('p', '<f8'), ('n', '<f8')])\n We need to keep in mind that by default, dtype=float. A float is therefore expected for the second column. However, the strings ' 2.3%' and ' 78.9%' cannot be converted to float and we end up having np.nan instead. Let\u2019s now use a converter: >>> # Converted case ...\n>>> np.genfromtxt(StringIO(data), delimiter=\",\", names=names,\n...               converters={1: convertfunc})\narray([(1.0, 0.023, 45.0), (6.0, 0.78900000000000003, 0.0)],\n      dtype=[('i', '<f8'), ('p', '<f8'), ('n', '<f8')])\n The same results can be obtained by using the name of the second column (\"p\") as key instead of its index (1): >>> # Using a name for the converter ...\n>>> np.genfromtxt(StringIO(data), delimiter=\",\", names=names,\n...               converters={\"p\": convertfunc})\narray([(1.0, 0.023, 45.0), (6.0, 0.78900000000000003, 0.0)],\n      dtype=[('i', '<f8'), ('p', '<f8'), ('n', '<f8')])\n Converters can also be used to provide a default for missing entries. In the following example, the converter convert transforms a stripped string into the corresponding float or into -999 if the string is empty. We need to explicitly strip the string from white spaces as it is not done by default: >>> data = u\"1, , 3\\n 4, 5, 6\"\n>>> convert = lambda x: float(x.strip() or -999)\n>>> np.genfromtxt(StringIO(data), delimiter=\",\",\n...               converters={1: convert})\narray([[   1., -999.,    3.],\n       [   4.,    5.,    6.]])\n   Using missing and filling values Some entries may be missing in the dataset we are trying to import. In a previous example, we used a converter to transform an empty string into a float. However, user-defined converters may rapidly become cumbersome to manage. The genfromtxt function provides two other complementary mechanisms: the missing_values argument is used to recognize missing data and a second argument, filling_values, is used to process these missing data.   missing_values By default, any empty string is marked as missing. We can also consider more complex strings, such as \"N/A\" or \"???\" to represent missing or invalid data. The missing_values argument accepts three kinds of values:  a string or a comma-separated string\n\nThis string will be used as the marker for missing data for all the columns  a sequence of strings\n\nIn that case, each item is associated to a column, in order.  a dictionary\n\nValues of the dictionary are strings or sequence of strings. The corresponding keys can be column indices (integers) or column names (strings). In addition, the special key None can be used to define a default applicable to all columns.     filling_values We know how to recognize missing data, but we still need to provide a value for these missing entries. By default, this value is determined from the expected dtype according to this table:   \nExpected type Default   \nbool False  \nint -1  \nfloat np.nan  \ncomplex np.nan+0j  \nstring '???'   We can get a finer control on the conversion of missing values with the filling_values optional argument. Like missing_values, this argument accepts different kind of values:  a single value\n\nThis will be the default for all columns  a sequence of values\n\nEach entry will be the default for the corresponding column  a dictionary\n\nEach key can be a column index or a column name, and the corresponding value should be a single object. We can use the special key None to define a default for all columns.   In the following example, we suppose that the missing values are flagged with \"N/A\" in the first column and by \"???\" in the third column. We wish to transform these missing values to 0 if they occur in the first and second column, and to -999 if they occur in the last column: >>> data = u\"N/A, 2, 3\\n4, ,???\"\n>>> kwargs = dict(delimiter=\",\",\n...               dtype=int,\n...               names=\"a,b,c\",\n...               missing_values={0:\"N/A\", 'b':\" \", 2:\"???\"},\n...               filling_values={0:0, 'b':0, 2:-999})\n>>> np.genfromtxt(StringIO(data), **kwargs)\narray([(0, 2, 3), (4, 0, -999)],\n      dtype=[('a', '<i8'), ('b', '<i8'), ('c', '<i8')])\n   usemask We may also want to keep track of the occurrence of missing data by constructing a boolean mask, with True entries where data was missing and False otherwise. To do that, we just have to set the optional argument usemask to True (the default is False). The output array will then be a MaskedArray.    Shortcut functions In addition to genfromtxt, the numpy.lib.npyio module provides several convenience functions derived from genfromtxt. These functions work the same way as the original, but they have different default values.  recfromtxt\n\nReturns a standard numpy.recarray (if usemask=False) or a MaskedRecords array (if usemaske=True). The default dtype is dtype=None, meaning that the types of each column will be automatically determined.  recfromcsv\n\nLike recfromtxt, but with a default delimiter=\",\".   \n"}, {"name": "Development workflow", "path": "dev/development_workflow", "type": "Development", "text": "Development workflow You already have your own forked copy of the NumPy repository, by following Create a NumPy fork, Make the local copy, you have configured git by following Git configuration, and have linked the upstream repository as explained in Linking your repository to the upstream repo. What is described below is a recommended workflow with Git.  Basic workflow In short:  Start a new feature branch for each set of edits that you do. See below. Hack away! See below\n \nWhen finished:  \nContributors: push your feature branch to your own Github repo, and create a pull request. \nCore developers: If you want to push changes without further review, see the notes below.    This way of working helps to keep work well organized and the history as clear as possible.  See also There are many online tutorials to help you learn git. For discussions of specific git workflows, see these discussions on linux git workflow, and ipython git workflow.   Making a new feature branch First, fetch new commits from the upstream repository: git fetch upstream\n Then, create a new branch based on the main branch of the upstream repository: git checkout -b my-new-feature upstream/main\n   The editing workflow  Overview # hack hack\ngit status # Optional\ngit diff # Optional\ngit add modified_file\ngit commit\n# push the branch to your own Github repo\ngit push origin my-new-feature\n   In more detail  Make some changes. When you feel that you\u2019ve made a complete, working set of related changes, move on to the next steps. \nOptional: Check which files have changed with git status (see git status). You\u2019ll see a listing like this one: # On branch my-new-feature\n# Changed but not updated:\n#   (use \"git add <file>...\" to update what will be committed)\n#   (use \"git checkout -- <file>...\" to discard changes in working directory)\n#\n#  modified:   README\n#\n# Untracked files:\n#   (use \"git add <file>...\" to include in what will be committed)\n#\n#  INSTALL\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n  Optional: Compare the changes with the previous version using with git\ndiff (git diff). This brings up a simple text browser interface that highlights the difference between your files and the previous version. Add any relevant modified or new files using git add modified_file (see git add). This puts the files into a staging area, which is a queue of files that will be added to your next commit. Only add files that have related, complete changes. Leave files with unfinished changes for later commits. \nTo commit the staged files into the local copy of your repo, do git\ncommit. At this point, a text editor will open up to allow you to write a commit message. Read the commit message section to be sure that you are writing a properly formatted and sufficiently detailed commit message. After saving your message and closing the editor, your commit will be saved. For trivial commits, a short commit message can be passed in through the command line using the -m flag. For example, git commit -am \"ENH: Some message\". In some cases, you will see this form of the commit command: git commit\n-a. The extra -a flag automatically commits all modified files and removes all deleted files. This can save you some typing of numerous git\nadd commands; however, it can add unwanted changes to a commit if you\u2019re not careful. For more information, see why the -a flag? - and the helpful use-case description in the tangled working copy problem.  \nPush the changes to your forked repo on github: git push origin my-new-feature\n For more information, see git push.    Note Assuming you have followed the instructions in these pages, git will create a default link to your github repo called origin. In git >= 1.7 you can ensure that the link to origin is permanently set by using the --set-upstream option: git push --set-upstream origin my-new-feature\n From now on git will know that my-new-feature is related to the my-new-feature branch in your own github repo. Subsequent push calls are then simplified to the following: git push\n You have to use --set-upstream for each new branch that you create.  It may be the case that while you were working on your edits, new commits have been added to upstream that affect your work. In this case, follow the Rebasing on main section of this document to apply those changes to your branch.   Writing the commit message Commit messages should be clear and follow a few basic rules. Example: ENH: add functionality X to numpy.<submodule>.\n\nThe first line of the commit message starts with a capitalized acronym\n(options listed below) indicating what type of commit this is.  Then a blank\nline, then more text if needed.  Lines shouldn't be longer than 72\ncharacters.  If the commit is related to a ticket, indicate that with\n\"See #3456\", \"See ticket 3456\", \"Closes #3456\" or similar.\n Describing the motivation for a change, the nature of a bug for bug fixes or some details on what an enhancement does are also good to include in a commit message. Messages should be understandable without looking at the code changes. A commit message like MAINT: fixed another one is an example of what not to do; the reader has to go look for context elsewhere. Standard acronyms to start the commit message with are: API: an (incompatible) API change\nBENCH: changes to the benchmark suite\nBLD: change related to building numpy\nBUG: bug fix\nDEP: deprecate something, or remove a deprecated object\nDEV: development tool or utility\nDOC: documentation\nENH: enhancement\nMAINT: maintenance commit (refactoring, typos, etc.)\nREV: revert an earlier commit\nSTY: style fix (whitespace, PEP8)\nTST: addition or modification of tests\nREL: related to releasing numpy\n    Get the mailing list\u2019s opinion If you plan a new feature or API change, it\u2019s wisest to first email the NumPy mailing list asking for comment. If you haven\u2019t heard back in a week, it\u2019s OK to ping the list again.   Asking for your changes to be merged with the main repo When you feel your work is finished, you can create a pull request (PR). Github has a nice help page that outlines the process for filing pull requests. If your changes involve modifications to the API or addition/modification of a function, add a release note to the doc/release/upcoming_changes/ directory, following the instructions and format in the doc/release/upcoming_changes/README.rst file.   Getting your PR reviewed We review pull requests as soon as we can, typically within a week. If you get no review comments within two weeks, feel free to ask for feedback by adding a comment on your PR (this will notify maintainers). If your PR is large or complicated, asking for input on the numpy-discussion mailing list may also be useful.   Rebasing on main This updates your feature branch with changes from the upstream NumPy github repo. If you do not absolutely need to do this, try to avoid doing it, except perhaps when you are finished. The first step will be to update the remote repository with new commits from upstream: git fetch upstream\n Next, you need to update the feature branch: # go to the feature branch\ngit checkout my-new-feature\n# make a backup in case you mess up\ngit branch tmp my-new-feature\n# rebase on upstream main branch\ngit rebase upstream/main\n If you have made changes to files that have changed also upstream, this may generate merge conflicts that you need to resolve. See below for help in this case. Finally, remove the backup branch upon a successful rebase: git branch -D tmp\n  Note Rebasing on main is preferred over merging upstream back to your branch. Using git merge and git pull is discouraged when working on feature branches.    Recovering from mess-ups Sometimes, you mess up merges or rebases. Luckily, in Git it is relatively straightforward to recover from such mistakes. If you mess up during a rebase: git rebase --abort\n If you notice you messed up after the rebase: # reset branch back to the saved point\ngit reset --hard tmp\n If you forgot to make a backup branch: # look at the reflog of the branch\ngit reflog show my-feature-branch\n\n8630830 my-feature-branch@{0}: commit: BUG: io: close file handles immediately\n278dd2a my-feature-branch@{1}: rebase finished: refs/heads/my-feature-branch onto 11ee694744f2552d\n26aa21a my-feature-branch@{2}: commit: BUG: lib: make seek_gzip_factory not leak gzip obj\n...\n\n# reset the branch to where it was before the botched rebase\ngit reset --hard my-feature-branch@{2}\n If you didn\u2019t actually mess up but there are merge conflicts, you need to resolve those. This can be one of the trickier things to get right. For a good description of how to do this, see this article on merging conflicts.    Additional things you might want to do  Rewriting commit history  Note Do this only for your own feature branches.  There\u2019s an embarrassing typo in a commit you made? Or perhaps you made several false starts you would like the posterity not to see. This can be done via interactive rebasing. Suppose that the commit history looks like this: git log --oneline\neadc391 Fix some remaining bugs\na815645 Modify it so that it works\n2dec1ac Fix a few bugs + disable\n13d7934 First implementation\n6ad92e5 * masked is now an instance of a new object, MaskedConstant\n29001ed Add pre-nep for a couple of structured_array_extensions.\n...\n and 6ad92e5 is the last commit in the main branch. Suppose we want to make the following changes:  Rewrite the commit message for 13d7934 to something more sensible. Combine the commits 2dec1ac, a815645, eadc391 into a single one.  We do as follows: # make a backup of the current state\ngit branch tmp HEAD\n# interactive rebase\ngit rebase -i 6ad92e5\n This will open an editor with the following text in it: pick 13d7934 First implementation\npick 2dec1ac Fix a few bugs + disable\npick a815645 Modify it so that it works\npick eadc391 Fix some remaining bugs\n\n# Rebase 6ad92e5..eadc391 onto 6ad92e5\n#\n# Commands:\n#  p, pick = use commit\n#  r, reword = use commit, but edit the commit message\n#  e, edit = use commit, but stop for amending\n#  s, squash = use commit, but meld into previous commit\n#  f, fixup = like \"squash\", but discard this commit's log message\n#\n# If you remove a line here THAT COMMIT WILL BE LOST.\n# However, if you remove everything, the rebase will be aborted.\n#\n To achieve what we want, we will make the following changes to it: r 13d7934 First implementation\npick 2dec1ac Fix a few bugs + disable\nf a815645 Modify it so that it works\nf eadc391 Fix some remaining bugs\n This means that (i) we want to edit the commit message for 13d7934, and (ii) collapse the last three commits into one. Now we save and quit the editor. Git will then immediately bring up an editor for editing the commit message. After revising it, we get the output: [detached HEAD 721fc64] FOO: First implementation\n 2 files changed, 199 insertions(+), 66 deletions(-)\n[detached HEAD 0f22701] Fix a few bugs + disable\n 1 files changed, 79 insertions(+), 61 deletions(-)\nSuccessfully rebased and updated refs/heads/my-feature-branch.\n and the history looks now like this: 0f22701 Fix a few bugs + disable\n721fc64 ENH: Sophisticated feature\n6ad92e5 * masked is now an instance of a new object, MaskedConstant\n If it went wrong, recovery is again possible as explained above.   Deleting a branch on github\n git checkout main\n# delete branch locally\ngit branch -D my-unwanted-branch\n# delete branch on github\ngit push origin --delete my-unwanted-branch\n See also: https://stackoverflow.com/questions/2003505/how-do-i-delete-a-git-branch-locally-and-remotely   Several people sharing a single repository If you want to work on some stuff with other people, where you are all committing into the same repository, or even the same branch, then just share it via github. First fork NumPy into your account, as from Create a NumPy fork. Then, go to your forked repository github page, say https://github.com/your-user-name/numpy Click on the \u2018Admin\u2019 button, and add anyone else to the repo as a collaborator:  Now all those people can do: git clone git@github.com:your-user-name/numpy.git\n Remember that links starting with git@ use the ssh protocol and are read-write; links starting with git:// are read-only. Your collaborators can then commit directly into that repo with the usual: git commit -am 'ENH - much better code'\ngit push origin my-feature-branch # pushes directly into your repo\n   Exploring your repository To see a graphical representation of the repository branches and commits: gitk --all\n To see a linear list of commits for this branch: git log\n You can also look at the network graph visualizer for your github repo.   Backporting Backporting is the process of copying new feature/fixes committed in numpy/main back to stable release branches. To do this you make a branch off the branch you are backporting to, cherry pick the commits you want from numpy/main, and then submit a pull request for the branch containing the backport.  \nFirst, you need to make the branch you will work on. This needs to be based on the older version of NumPy (not main): # Make a new branch based on numpy/maintenance/1.8.x,\n# backport-3324 is our new name for the branch.\ngit checkout -b backport-3324 upstream/maintenance/1.8.x\n  \nNow you need to apply the changes from main to this branch using git cherry-pick: # Update remote\ngit fetch upstream\n# Check the commit log for commits to cherry pick\ngit log upstream/main\n# This pull request included commits aa7a047 to c098283 (inclusive)\n# so you use the .. syntax (for a range of commits), the ^ makes the\n# range inclusive.\ngit cherry-pick aa7a047^..c098283\n...\n# Fix any conflicts, then if needed:\ngit cherry-pick --continue\n  You might run into some conflicts cherry picking here. These are resolved the same way as merge/rebase conflicts. Except here you can use git blame to see the difference between main and the backported branch to make sure nothing gets screwed up. \nPush the new branch to your Github repository: git push -u origin backport-3324\n  Finally make a pull request using Github. Make sure it is against the maintenance branch and not main, Github will usually suggest you make the pull request against main.    Pushing changes to the main repo Requires commit rights to the main NumPy repo. When you have a set of \u201cready\u201d changes in a feature branch ready for NumPy\u2019s main or maintenance branches, you can push them to upstream as follows:  \nFirst, merge or rebase on the target branch.  \nOnly a few, unrelated commits then prefer rebasing: git fetch upstream\ngit rebase upstream/main\n See Rebasing on main.  \nIf all of the commits are related, create a merge commit: git fetch upstream\ngit merge --no-ff upstream/main\n    \nCheck that what you are going to push looks sensible: git log -p upstream/main..\ngit log --oneline --graph\n  \nPush to upstream: git push upstream my-feature-branch:main\n    Note It\u2019s usually a good idea to use the -n flag to git push to check first that you\u2019re about to push the changes you want to the place you want.   \n"}, {"name": "distutils.ccompiler.CCompiler_compile()", "path": "reference/generated/numpy.distutils.ccompiler.ccompiler_compile", "type": "numpy.distutils.ccompiler.CCompiler_compile", "text": "numpy.distutils.ccompiler.CCompiler_compile   distutils.ccompiler.CCompiler_compile(self, sources, output_dir=None, macros=None, include_dirs=None, debug=0, extra_preargs=None, extra_postargs=None, depends=None)[source]\n \nCompile one or more source files. Please refer to the Python distutils API reference for more details.  Parameters \n \nsourceslist of str\n\n\nA list of filenames  \noutput_dirstr, optional\n\n\nPath to the output directory.  \nmacroslist of tuples\n\n\nA list of macro definitions.  \ninclude_dirslist of str, optional\n\n\nThe directories to add to the default include file search path for this compilation only.  \ndebugbool, optional\n\n\nWhether or not to output debug symbols in or alongside the object file(s).  \nextra_preargs, extra_postargs?\n\n\nExtra pre- and post-arguments.  \ndependslist of str, optional\n\n\nA list of file names that all targets depend on.    Returns \n \nobjectslist of str\n\n\nA list of object file names, one per source file sources.    Raises \n CompileError\n\nIf compilation fails.     \n\n"}, {"name": "distutils.ccompiler.CCompiler_customize()", "path": "reference/generated/numpy.distutils.ccompiler.ccompiler_customize", "type": "numpy.distutils.ccompiler.CCompiler_customize", "text": "numpy.distutils.ccompiler.CCompiler_customize   distutils.ccompiler.CCompiler_customize(self, dist, need_cxx=0)[source]\n \nDo any platform-specific customization of a compiler instance. This method calls distutils.sysconfig.customize_compiler for platform-specific customization, as well as optionally remove a flag to suppress spurious warnings in case C++ code is being compiled.  Parameters \n \ndistobject\n\n\nThis parameter is not used for anything.  \nneed_cxxbool, optional\n\n\nWhether or not C++ has to be compiled. If so (True), the \"-Wstrict-prototypes\" option is removed to prevent spurious warnings. Default is False.    Returns \n None\n   Notes All the default options used by distutils can be extracted with: from distutils import sysconfig\nsysconfig.get_config_vars('CC', 'CXX', 'OPT', 'BASECFLAGS',\n                          'CCSHARED', 'LDSHARED', 'SO')\n \n\n"}, {"name": "distutils.ccompiler.CCompiler_customize_cmd()", "path": "reference/generated/numpy.distutils.ccompiler.ccompiler_customize_cmd", "type": "numpy.distutils.ccompiler.CCompiler_customize_cmd", "text": "numpy.distutils.ccompiler.CCompiler_customize_cmd   distutils.ccompiler.CCompiler_customize_cmd(self, cmd, ignore=())[source]\n \nCustomize compiler using distutils command.  Parameters \n \ncmdclass instance\n\n\nAn instance inheriting from distutils.cmd.Command.  \nignoresequence of str, optional\n\n\nList of CCompiler commands (without 'set_') that should not be altered. Strings that are checked for are: ('include_dirs', 'define', 'undef', 'libraries', 'library_dirs',\n'rpath', 'link_objects').    Returns \n None\n   \n\n"}, {"name": "distutils.ccompiler.CCompiler_cxx_compiler()", "path": "reference/generated/numpy.distutils.ccompiler.ccompiler_cxx_compiler", "type": "numpy.distutils.ccompiler.CCompiler_cxx_compiler", "text": "numpy.distutils.ccompiler.CCompiler_cxx_compiler   distutils.ccompiler.CCompiler_cxx_compiler(self)[source]\n \nReturn the C++ compiler.  Parameters \n None\n  Returns \n \ncxxclass instance\n\n\nThe C++ compiler, as a CCompiler instance.     \n\n"}, {"name": "distutils.ccompiler.CCompiler_find_executables()", "path": "reference/generated/numpy.distutils.ccompiler.ccompiler_find_executables", "type": "numpy.distutils.ccompiler.CCompiler_find_executables", "text": "numpy.distutils.ccompiler.CCompiler_find_executables   distutils.ccompiler.CCompiler_find_executables(self)[source]\n \nDoes nothing here, but is called by the get_version method and can be overridden by subclasses. In particular it is redefined in the FCompiler class where more documentation can be found. \n\n"}, {"name": "distutils.ccompiler.CCompiler_get_version()", "path": "reference/generated/numpy.distutils.ccompiler.ccompiler_get_version", "type": "numpy.distutils.ccompiler.CCompiler_get_version", "text": "numpy.distutils.ccompiler.CCompiler_get_version   distutils.ccompiler.CCompiler_get_version(self, force=False, ok_status=[0])[source]\n \nReturn compiler version, or None if compiler is not available.  Parameters \n \nforcebool, optional\n\n\nIf True, force a new determination of the version, even if the compiler already has a version attribute. Default is False.  \nok_statuslist of int, optional\n\n\nThe list of status values returned by the version look-up process for which a version string is returned. If the status value is not in ok_status, None is returned. Default is [0].    Returns \n \nversionstr or None\n\n\nVersion string, in the format of distutils.version.LooseVersion.     \n\n"}, {"name": "distutils.ccompiler.CCompiler_object_filenames()", "path": "reference/generated/numpy.distutils.ccompiler.ccompiler_object_filenames", "type": "numpy.distutils.ccompiler.CCompiler_object_filenames", "text": "numpy.distutils.ccompiler.CCompiler_object_filenames   distutils.ccompiler.CCompiler_object_filenames(self, source_filenames, strip_dir=0, output_dir='')[source]\n \nReturn the name of the object files for the given source files.  Parameters \n \nsource_filenameslist of str\n\n\nThe list of paths to source files. Paths can be either relative or absolute, this is handled transparently.  \nstrip_dirbool, optional\n\n\nWhether to strip the directory from the returned paths. If True, the file name prepended by output_dir is returned. Default is False.  \noutput_dirstr, optional\n\n\nIf given, this path is prepended to the returned paths to the object files.    Returns \n \nobj_nameslist of str\n\n\nThe list of paths to the object files corresponding to the source files in source_filenames.     \n\n"}, {"name": "distutils.ccompiler.CCompiler_show_customization()", "path": "reference/generated/numpy.distutils.ccompiler.ccompiler_show_customization", "type": "numpy.distutils.ccompiler.CCompiler_show_customization", "text": "numpy.distutils.ccompiler.CCompiler_show_customization   distutils.ccompiler.CCompiler_show_customization(self)[source]\n \nPrint the compiler customizations to stdout.  Parameters \n None\n  Returns \n None\n   Notes Printing is only done if the distutils log threshold is < 2. \n\n"}, {"name": "distutils.ccompiler.CCompiler_spawn()", "path": "reference/generated/numpy.distutils.ccompiler.ccompiler_spawn", "type": "numpy.distutils.ccompiler.CCompiler_spawn", "text": "numpy.distutils.ccompiler.CCompiler_spawn   distutils.ccompiler.CCompiler_spawn(self, cmd, display=None, env=None)[source]\n \nExecute a command in a sub-process.  Parameters \n \ncmdstr\n\n\nThe command to execute.  \ndisplaystr or sequence of str, optional\n\n\nThe text to add to the log file kept by numpy.distutils. If not given, display is equal to cmd.  env: a dictionary for environment variables, optional\n  Returns \n None\n  Raises \n DistutilsExecError\n\nIf the command failed, i.e. the exit status was not 0.     \n\n"}, {"name": "distutils.ccompiler.gen_lib_options()", "path": "reference/generated/numpy.distutils.ccompiler.gen_lib_options", "type": "numpy.distutils.ccompiler.gen_lib_options", "text": "numpy.distutils.ccompiler.gen_lib_options   distutils.ccompiler.gen_lib_options(compiler, library_dirs, runtime_library_dirs, libraries)[source]\n\n\n"}, {"name": "distutils.ccompiler.new_compiler()", "path": "reference/generated/numpy.distutils.ccompiler.new_compiler", "type": "numpy.distutils.ccompiler.new_compiler", "text": "numpy.distutils.ccompiler.new_compiler   distutils.ccompiler.new_compiler(plat=None, compiler=None, verbose=None, dry_run=0, force=0)[source]\n\n\n"}, {"name": "distutils.ccompiler.replace_method()", "path": "reference/generated/numpy.distutils.ccompiler.replace_method", "type": "numpy.distutils.ccompiler.replace_method", "text": "numpy.distutils.ccompiler.replace_method   distutils.ccompiler.replace_method(klass, method_name, func)[source]\n\n\n"}, {"name": "distutils.ccompiler.simple_version_match()", "path": "reference/generated/numpy.distutils.ccompiler.simple_version_match", "type": "numpy.distutils.ccompiler.simple_version_match", "text": "numpy.distutils.ccompiler.simple_version_match   distutils.ccompiler.simple_version_match(pat='[-.\\\\d]+', ignore='', start='')[source]\n \nSimple matching of version numbers, for use in CCompiler and FCompiler.  Parameters \n \npatstr, optional\n\n\nA regular expression matching version numbers. Default is r'[-.\\d]+'.  \nignorestr, optional\n\n\nA regular expression matching patterns to skip. Default is '', in which case nothing is skipped.  \nstartstr, optional\n\n\nA regular expression matching the start of where to start looking for version numbers. Default is '', in which case searching is started at the beginning of the version string given to matcher.    Returns \n \nmatchercallable\n\n\nA function that is appropriate to use as the .version_match attribute of a CCompiler class. matcher takes a single parameter, a version string.     \n\n"}, {"name": "distutils.ccompiler_opt.CCompilerOpt.cache_flush()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.cache_flush", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.cache_flush", "text": "numpy.distutils.ccompiler_opt.CCompilerOpt.cache_flush method   distutils.ccompiler_opt.CCompilerOpt.cache_flush()[source]\n \nForce update the cache. \n\n"}, {"name": "distutils.ccompiler_opt.CCompilerOpt.cc_normalize_flags()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.cc_normalize_flags", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.cc_normalize_flags", "text": "numpy.distutils.ccompiler_opt.CCompilerOpt.cc_normalize_flags method   distutils.ccompiler_opt.CCompilerOpt.cc_normalize_flags(flags)[source]\n \nRemove the conflicts that caused due gathering implied features flags.  Parameters \n \u2018flags\u2019 list, compiler flags\n\nflags should be sorted from the lowest to the highest interest.    Returns \n list, filtered from any conflicts.\n   Examples >>> self.cc_normalize_flags(['-march=armv8.2-a+fp16', '-march=armv8.2-a+dotprod'])\n['armv8.2-a+fp16+dotprod']\n >>> self.cc_normalize_flags(\n    ['-msse', '-msse2', '-msse3', '-mssse3', '-msse4.1', '-msse4.2', '-mavx', '-march=core-avx2']\n)\n['-march=core-avx2']\n \n\n"}, {"name": "distutils.ccompiler_opt.CCompilerOpt.conf_features", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.conf_features", "type": "NumPy.distutils.ccompiler_opt.ccompileropt.conf_features", "text": "numpy.distutils.ccompiler_opt.CCompilerOpt.conf_features attribute   distutils.ccompiler_opt.CCompilerOpt.conf_features = {'ASIMD': {'implies': 'NEON_FP16 NEON_VFPV4', 'implies_detect': False, 'interest': 4}, 'ASIMDDP': {'implies': 'ASIMD', 'interest': 6}, 'ASIMDFHM': {'implies': 'ASIMDHP', 'interest': 7}, 'ASIMDHP': {'implies': 'ASIMD', 'interest': 5}, 'AVX': {'headers': 'immintrin.h', 'implies': 'SSE42', 'implies_detect': False, 'interest': 8}, 'AVX2': {'implies': 'F16C', 'interest': 13}, 'AVX512CD': {'implies': 'AVX512F', 'interest': 21}, 'AVX512F': {'extra_checks': 'AVX512F_REDUCE', 'implies': 'FMA3 AVX2', 'implies_detect': False, 'interest': 20}, 'AVX512_CLX': {'detect': 'AVX512_CLX', 'group': 'AVX512VNNI', 'implies': 'AVX512_SKX', 'interest': 43}, 'AVX512_CNL': {'detect': 'AVX512_CNL', 'group': 'AVX512IFMA AVX512VBMI', 'implies': 'AVX512_SKX', 'implies_detect': False, 'interest': 44}, 'AVX512_ICL': {'detect': 'AVX512_ICL', 'group': 'AVX512VBMI2 AVX512BITALG AVX512VPOPCNTDQ', 'implies': 'AVX512_CLX AVX512_CNL', 'implies_detect': False, 'interest': 45}, 'AVX512_KNL': {'detect': 'AVX512_KNL', 'group': 'AVX512ER AVX512PF', 'implies': 'AVX512CD', 'implies_detect': False, 'interest': 40}, 'AVX512_KNM': {'detect': 'AVX512_KNM', 'group': 'AVX5124FMAPS AVX5124VNNIW AVX512VPOPCNTDQ', 'implies': 'AVX512_KNL', 'implies_detect': False, 'interest': 41}, 'AVX512_SKX': {'detect': 'AVX512_SKX', 'extra_checks': 'AVX512BW_MASK AVX512DQ_MASK', 'group': 'AVX512VL AVX512BW AVX512DQ', 'implies': 'AVX512CD', 'implies_detect': False, 'interest': 42}, 'F16C': {'implies': 'AVX', 'interest': 11}, 'FMA3': {'implies': 'F16C', 'interest': 12}, 'FMA4': {'headers': 'x86intrin.h', 'implies': 'AVX', 'interest': 10}, 'NEON': {'headers': 'arm_neon.h', 'interest': 1}, 'NEON_FP16': {'implies': 'NEON', 'interest': 2}, 'NEON_VFPV4': {'implies': 'NEON_FP16', 'interest': 3}, 'POPCNT': {'headers': 'popcntintrin.h', 'implies': 'SSE41', 'interest': 6}, 'SSE': {'headers': 'xmmintrin.h', 'implies': 'SSE2', 'interest': 1}, 'SSE2': {'headers': 'emmintrin.h', 'implies': 'SSE', 'interest': 2}, 'SSE3': {'headers': 'pmmintrin.h', 'implies': 'SSE2', 'interest': 3}, 'SSE41': {'headers': 'smmintrin.h', 'implies': 'SSSE3', 'interest': 5}, 'SSE42': {'implies': 'POPCNT', 'interest': 7}, 'SSSE3': {'headers': 'tmmintrin.h', 'implies': 'SSE3', 'interest': 4}, 'VSX': {'extra_checks': 'VSX_ASM', 'headers': 'altivec.h', 'interest': 1}, 'VSX2': {'implies': 'VSX', 'implies_detect': False, 'interest': 2}, 'VSX3': {'implies': 'VSX2', 'implies_detect': False, 'interest': 3}, 'XOP': {'headers': 'x86intrin.h', 'implies': 'AVX', 'interest': 9}}\n\n\n"}, {"name": "distutils.ccompiler_opt.CCompilerOpt.conf_features_partial()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.conf_features_partial", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.conf_features_partial", "text": "numpy.distutils.ccompiler_opt.CCompilerOpt.conf_features_partial method   distutils.ccompiler_opt.CCompilerOpt.conf_features_partial()[source]\n \nReturn a dictionary of supported CPU features by the platform, and accumulate the rest of undefined options in conf_features, the returned dict has same rules and notes in class attribute conf_features, also its override any options that been set in \u2018conf_features\u2019. \n\n"}, {"name": "distutils.ccompiler_opt.CCompilerOpt.cpu_baseline_flags()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.cpu_baseline_flags", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.cpu_baseline_flags", "text": "numpy.distutils.ccompiler_opt.CCompilerOpt.cpu_baseline_flags method   distutils.ccompiler_opt.CCompilerOpt.cpu_baseline_flags()[source]\n \nReturns a list of final CPU baseline compiler flags \n\n"}, {"name": "distutils.ccompiler_opt.CCompilerOpt.cpu_baseline_names()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.cpu_baseline_names", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.cpu_baseline_names", "text": "numpy.distutils.ccompiler_opt.CCompilerOpt.cpu_baseline_names method   distutils.ccompiler_opt.CCompilerOpt.cpu_baseline_names()[source]\n \nreturn a list of final CPU baseline feature names \n\n"}, {"name": "distutils.ccompiler_opt.CCompilerOpt.cpu_dispatch_names()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.cpu_dispatch_names", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.cpu_dispatch_names", "text": "numpy.distutils.ccompiler_opt.CCompilerOpt.cpu_dispatch_names method   distutils.ccompiler_opt.CCompilerOpt.cpu_dispatch_names()[source]\n \nreturn a list of final CPU dispatch feature names \n\n"}, {"name": "distutils.ccompiler_opt.CCompilerOpt.dist_compile()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.dist_compile", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.dist_compile", "text": "numpy.distutils.ccompiler_opt.CCompilerOpt.dist_compile method   distutils.ccompiler_opt.CCompilerOpt.dist_compile(sources, flags, ccompiler=None, **kwargs)[source]\n \nWrap CCompiler.compile() \n\n"}, {"name": "distutils.ccompiler_opt.CCompilerOpt.dist_info()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.dist_info", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.dist_info", "text": "numpy.distutils.ccompiler_opt.CCompilerOpt.dist_info method   distutils.ccompiler_opt.CCompilerOpt.dist_info()[source]\n \nReturn a tuple containing info about (platform, compiler, extra_args), required by the abstract class \u2018_CCompiler\u2019 for discovering the platform environment. This is also used as a cache factor in order to detect any changes happening from outside. \n\n"}, {"name": "distutils.ccompiler_opt.CCompilerOpt.dist_test()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.dist_test", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.dist_test", "text": "numpy.distutils.ccompiler_opt.CCompilerOpt.dist_test method   distutils.ccompiler_opt.CCompilerOpt.dist_test(source, flags, macros=[])[source]\n \nReturn True if \u2018CCompiler.compile()\u2019 able to compile a source file with certain flags. \n\n"}, {"name": "distutils.ccompiler_opt.CCompilerOpt.feature_ahead()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.feature_ahead", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_ahead", "text": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_ahead method   distutils.ccompiler_opt.CCompilerOpt.feature_ahead(names)[source]\n \nReturn list of features in \u2018names\u2019 after remove any implied features and keep the origins.  Parameters \n \u2018names\u2019: sequence\n\nsequence of CPU feature names in uppercase.    Returns \n list of CPU features sorted as-is \u2018names\u2019\n   Examples >>> self.feature_ahead([\"SSE2\", \"SSE3\", \"SSE41\"])\n[\"SSE41\"]\n# assume AVX2 and FMA3 implies each other and AVX2\n# is the highest interest\n>>> self.feature_ahead([\"SSE2\", \"SSE3\", \"SSE41\", \"AVX2\", \"FMA3\"])\n[\"AVX2\"]\n# assume AVX2 and FMA3 don't implies each other\n>>> self.feature_ahead([\"SSE2\", \"SSE3\", \"SSE41\", \"AVX2\", \"FMA3\"])\n[\"AVX2\", \"FMA3\"]\n \n\n"}, {"name": "distutils.ccompiler_opt.CCompilerOpt.feature_c_preprocessor()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.feature_c_preprocessor", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_c_preprocessor", "text": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_c_preprocessor method   distutils.ccompiler_opt.CCompilerOpt.feature_c_preprocessor(feature_name, tabs=0)[source]\n \nGenerate C preprocessor definitions and include headers of a CPU feature.  Parameters \n \u2018feature_name\u2019: str\n\nCPU feature name in uppercase.  \u2018tabs\u2019: int\n\nif > 0, align the generated strings to the right depend on number of tabs.    Returns \n str, generated C preprocessor\n   Examples >>> self.feature_c_preprocessor(\"SSE3\")\n/** SSE3 **/\n#define NPY_HAVE_SSE3 1\n#include <pmmintrin.h>\n \n\n"}, {"name": "distutils.ccompiler_opt.CCompilerOpt.feature_detect()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.feature_detect", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_detect", "text": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_detect method   distutils.ccompiler_opt.CCompilerOpt.feature_detect(names)[source]\n \nReturn a list of CPU features that required to be detected sorted from the lowest to highest interest. \n\n"}, {"name": "distutils.ccompiler_opt.CCompilerOpt.feature_get_til()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.feature_get_til", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_get_til", "text": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_get_til method   distutils.ccompiler_opt.CCompilerOpt.feature_get_til(names, keyisfalse)[source]\n \nsame as feature_implies_c() but stop collecting implied features when feature\u2019s option that provided through parameter \u2018keyisfalse\u2019 is False, also sorting the returned features. \n\n"}, {"name": "distutils.ccompiler_opt.CCompilerOpt.feature_implies()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.feature_implies", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_implies", "text": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_implies method   distutils.ccompiler_opt.CCompilerOpt.feature_implies(names, keep_origins=False)[source]\n \nReturn a set of CPU features that implied by \u2018names\u2019  Parameters \n names: str or sequence of str\n\nCPU feature name(s) in uppercase.  keep_origins: bool\n\nif False(default) then the returned set will not contain any features from \u2018names\u2019. This case happens only when two features imply each other.     Examples >>> self.feature_implies(\"SSE3\")\n{'SSE', 'SSE2'}\n>>> self.feature_implies(\"SSE2\")\n{'SSE'}\n>>> self.feature_implies(\"SSE2\", keep_origins=True)\n# 'SSE2' found here since 'SSE' and 'SSE2' imply each other\n{'SSE', 'SSE2'}\n \n\n"}, {"name": "distutils.ccompiler_opt.CCompilerOpt.feature_implies_c()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.feature_implies_c", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_implies_c", "text": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_implies_c method   distutils.ccompiler_opt.CCompilerOpt.feature_implies_c(names)[source]\n \nsame as feature_implies() but combining \u2018names\u2019 \n\n"}, {"name": "distutils.ccompiler_opt.CCompilerOpt.feature_is_exist()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.feature_is_exist", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_is_exist", "text": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_is_exist method   distutils.ccompiler_opt.CCompilerOpt.feature_is_exist(name)[source]\n \nReturns True if a certain feature is exist and covered within _Config.conf_features.  Parameters \n \u2018name\u2019: str\n\nfeature name in uppercase.     \n\n"}, {"name": "distutils.ccompiler_opt.CCompilerOpt.feature_names()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.feature_names", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_names", "text": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_names method   distutils.ccompiler_opt.CCompilerOpt.feature_names(names=None, force_flags=None, macros=[])[source]\n \nReturns a set of CPU feature names that supported by platform and the C compiler.  Parameters \n names: sequence or None, optional\n\nSpecify certain CPU features to test it against the C compiler. if None(default), it will test all current supported features. Note: feature names must be in upper-case.  force_flags: list or None, optional\n\nIf None(default), default compiler flags for every CPU feature will be used during the test.  \nmacroslist of tuples, optional\n\n\nA list of C macro definitions.     \n\n"}, {"name": "distutils.ccompiler_opt.CCompilerOpt.feature_sorted()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.feature_sorted", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_sorted", "text": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_sorted method   distutils.ccompiler_opt.CCompilerOpt.feature_sorted(names, reverse=False)[source]\n \nSort a list of CPU features ordered by the lowest interest.  Parameters \n \u2018names\u2019: sequence\n\nsequence of supported feature names in uppercase.  \u2018reverse\u2019: bool, optional\n\nIf true, the sorted features is reversed. (highest interest)    Returns \n list, sorted CPU features\n   \n\n"}, {"name": "distutils.ccompiler_opt.CCompilerOpt.feature_untied()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.feature_untied", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_untied", "text": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_untied method   distutils.ccompiler_opt.CCompilerOpt.feature_untied(names)[source]\n \nsame as \u2018feature_ahead()\u2019 but if both features implied each other and keep the highest interest.  Parameters \n \u2018names\u2019: sequence\n\nsequence of CPU feature names in uppercase.    Returns \n list of CPU features sorted as-is \u2018names\u2019\n   Examples >>> self.feature_untied([\"SSE2\", \"SSE3\", \"SSE41\"])\n[\"SSE2\", \"SSE3\", \"SSE41\"]\n# assume AVX2 and FMA3 implies each other\n>>> self.feature_untied([\"SSE2\", \"SSE3\", \"SSE41\", \"FMA3\", \"AVX2\"])\n[\"SSE2\", \"SSE3\", \"SSE41\", \"AVX2\"]\n \n\n"}, {"name": "distutils.ccompiler_opt.CCompilerOpt.generate_dispatch_header()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.generate_dispatch_header", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.generate_dispatch_header", "text": "numpy.distutils.ccompiler_opt.CCompilerOpt.generate_dispatch_header method   distutils.ccompiler_opt.CCompilerOpt.generate_dispatch_header(header_path)[source]\n \nGenerate the dispatch header which contains the #definitions and headers for platform-specific instruction-sets for the enabled CPU baseline and dispatch-able features. Its highly recommended to take a look at the generated header also the generated source files via try_dispatch() in order to get the full picture. \n\n"}, {"name": "distutils.ccompiler_opt.CCompilerOpt.is_cached()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.is_cached", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.is_cached", "text": "numpy.distutils.ccompiler_opt.CCompilerOpt.is_cached method   distutils.ccompiler_opt.CCompilerOpt.is_cached()[source]\n \nReturns True if the class loaded from the cache file \n\n"}, {"name": "distutils.ccompiler_opt.CCompilerOpt.parse_targets()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.parse_targets", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.parse_targets", "text": "numpy.distutils.ccompiler_opt.CCompilerOpt.parse_targets method   distutils.ccompiler_opt.CCompilerOpt.parse_targets(source)[source]\n \nFetch and parse configuration statements that required for defining the targeted CPU features, statements should be declared in the top of source in between C comment and start with a special mark @targets. Configuration statements are sort of keywords representing CPU features names, group of statements and policies, combined together to determine the required optimization.  Parameters \n source: str\n\nthe path of C source file.    Returns \n \n bool, True if group has the \u2018baseline\u2019 option\n  \n list, list of CPU features\n  \n list, list of extra compiler flags\n     \n\n"}, {"name": "distutils.ccompiler_opt.CCompilerOpt.try_dispatch()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.try_dispatch", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.try_dispatch", "text": "numpy.distutils.ccompiler_opt.CCompilerOpt.try_dispatch method   distutils.ccompiler_opt.CCompilerOpt.try_dispatch(sources, src_dir=None, ccompiler=None, **kwargs)[source]\n \nCompile one or more dispatch-able sources and generates object files, also generates abstract C config headers and macros that used later for the final runtime dispatching process. The mechanism behind it is to takes each source file that specified in \u2018sources\u2019 and branching it into several files depend on special configuration statements that must be declared in the top of each source which contains targeted CPU features, then it compiles every branched source with the proper compiler flags.  Parameters \n \nsourceslist\n\n\nMust be a list of dispatch-able sources file paths, and configuration statements must be declared inside each file.  \nsrc_dirstr\n\n\nPath of parent directory for the generated headers and wrapped sources. If None(default) the files will generated in-place.  ccompiler: CCompiler\n\nDistutils CCompiler instance to be used for compilation. If None (default), the provided instance during the initialization will be used instead.  \n**kwargsany\n\n\nArguments to pass on to the CCompiler.compile()    Returns \n \nlistgenerated object files\n\n  Raises \n CompileError\n\nRaises by CCompiler.compile() on compiling failure.  DistutilsError\n\nSome errors during checking the sanity of configuration statements.      See also  parse_targets\n\nParsing the configuration statements of dispatch-able sources.    \n\n"}, {"name": "distutils.ccompiler_opt.new_ccompiler_opt()", "path": "reference/generated/numpy.distutils.ccompiler_opt.new_ccompiler_opt", "type": "numpy.distutils.ccompiler_opt.new_ccompiler_opt", "text": "numpy.distutils.ccompiler_opt.new_ccompiler_opt   distutils.ccompiler_opt.new_ccompiler_opt(compiler, dispatch_hpath, **kwargs)[source]\n \nCreate a new instance of \u2018CCompilerOpt\u2019 and generate the dispatch header which contains the #definitions and headers of platform-specific instruction-sets for the enabled CPU baseline and dispatch-able features.  Parameters \n \ncompilerCCompiler instance\n\n\ndispatch_hpathstr\n\n\npath of the dispatch header  **kwargs: passed as-is to `CCompilerOpt(\u2026)`\nReturns\n\u2014\u2014-\nnew instance of CCompilerOpt\n   \n\n"}, {"name": "distutils.cpuinfo.cpu", "path": "reference/generated/numpy.distutils.cpuinfo.cpu", "type": "numpy.distutils.cpuinfo.cpu", "text": "numpy.distutils.cpuinfo.cpu   distutils.cpuinfo.cpu = <numpy.distutils.cpuinfo.LinuxCPUInfo object>\n\n\n"}, {"name": "distutils.exec_command.exec_command()", "path": "reference/generated/numpy.distutils.exec_command.exec_command", "type": "numpy.distutils.exec_command.exec_command", "text": "numpy.distutils.exec_command.exec_command   distutils.exec_command.exec_command(command, execute_in='', use_shell=None, use_tee=None, _with_python=1, **env)[source]\n \nReturn (status,output) of executed command.  Deprecated since version 1.17: Use subprocess.Popen instead   Parameters \n \ncommandstr\n\n\nA concatenated string of executable and arguments.  \nexecute_instr\n\n\nBefore running command cd execute_in and after cd -.  \nuse_shell{bool, None}, optional\n\n\nIf True, execute sh -c command. Default None (True)  \nuse_tee{bool, None}, optional\n\n\nIf True use tee. Default None (True)    Returns \n \nresstr\n\n\nBoth stdout and stderr messages.     Notes On NT, DOS systems the returned status is correct for external commands. Wild cards will not work for non-posix systems or when use_shell=0. \n\n"}, {"name": "distutils.exec_command.filepath_from_subprocess_output()", "path": "reference/generated/numpy.distutils.exec_command.filepath_from_subprocess_output", "type": "numpy.distutils.exec_command.filepath_from_subprocess_output", "text": "numpy.distutils.exec_command.filepath_from_subprocess_output   distutils.exec_command.filepath_from_subprocess_output(output)[source]\n \nConvert bytes in the encoding used by a subprocess into a filesystem-appropriate str. Inherited from exec_command, and possibly incorrect. \n\n"}, {"name": "distutils.exec_command.find_executable()", "path": "reference/generated/numpy.distutils.exec_command.find_executable", "type": "numpy.distutils.exec_command.find_executable", "text": "numpy.distutils.exec_command.find_executable   distutils.exec_command.find_executable(exe, path=None, _cache={})[source]\n \nReturn full path of a executable or None. Symbolic links are not followed. \n\n"}, {"name": "distutils.exec_command.forward_bytes_to_stdout()", "path": "reference/generated/numpy.distutils.exec_command.forward_bytes_to_stdout", "type": "numpy.distutils.exec_command.forward_bytes_to_stdout", "text": "numpy.distutils.exec_command.forward_bytes_to_stdout   distutils.exec_command.forward_bytes_to_stdout(val)[source]\n \nForward bytes from a subprocess call to the console, without attempting to decode them. The assumption is that the subprocess call already returned bytes in a suitable encoding. \n\n"}, {"name": "distutils.exec_command.get_pythonexe()", "path": "reference/generated/numpy.distutils.exec_command.get_pythonexe", "type": "numpy.distutils.exec_command.get_pythonexe", "text": "numpy.distutils.exec_command.get_pythonexe   distutils.exec_command.get_pythonexe()[source]\n\n\n"}, {"name": "distutils.exec_command.temp_file_name()", "path": "reference/generated/numpy.distutils.exec_command.temp_file_name", "type": "numpy.distutils.exec_command.temp_file_name", "text": "numpy.distutils.exec_command.temp_file_name   distutils.exec_command.temp_file_name()[source]\n\n\n"}, {"name": "distutils.log.set_verbosity()", "path": "reference/generated/numpy.distutils.log.set_verbosity", "type": "numpy.distutils.log.set_verbosity", "text": "numpy.distutils.log.set_verbosity   distutils.log.set_verbosity(v, force=False)[source]\n\n\n"}, {"name": "distutils.system_info.get_info()", "path": "reference/generated/numpy.distutils.system_info.get_info", "type": "numpy.distutils.system_info.get_info", "text": "numpy.distutils.system_info.get_info   distutils.system_info.get_info(name, notfound_action=0)[source]\n \n notfound_action:\n\n0 - do nothing 1 - display warning message 2 - raise error   \n\n"}, {"name": "distutils.system_info.get_standard_file()", "path": "reference/generated/numpy.distutils.system_info.get_standard_file", "type": "numpy.distutils.system_info.get_standard_file", "text": "numpy.distutils.system_info.get_standard_file   distutils.system_info.get_standard_file(fname)[source]\n \nReturns a list of files named \u2018fname\u2019 from 1) System-wide directory (directory-location of this module) 2) Users HOME directory (os.environ[\u2018HOME\u2019]) 3) Local directory \n\n"}, {"name": "double npy_half_to_double()", "path": "reference/c-api/coremath#c.npy_half_to_double", "type": "NumPy core libraries", "text": "  doublenpy_half_to_double(npy_halfh)\n \nConverts a half-precision float to a double-precision float. \n"}, {"name": "double npy_spacing()", "path": "reference/c-api/coremath#c.npy_spacing", "type": "NumPy core libraries", "text": "  doublenpy_spacing(doublex)\n \nThis is a function equivalent to Fortran intrinsic. Return distance between x and next representable floating point value from x, e.g. spacing(1) == eps. spacing of nan and +/- inf return nan. Single and extended precisions are available with suffix f and l.  New in version 1.4.0.  \n"}, {"name": "double PyArray_GetPriority()", "path": "reference/c-api/array#c.PyArray_GetPriority", "type": "Array API", "text": "  doublePyArray_GetPriority(PyObject*obj, doubledef)\n \nReturn the __array_priority__ attribute (converted to a double) of obj or def if no attribute of that name exists. Fast returns that avoid the attribute lookup are provided for objects of type PyArray_Type. \n"}, {"name": "double random_beta()", "path": "reference/random/c-api#c.random_beta", "type": "C API for random", "text": "  doublerandom_beta(bitgen_t*bitgen_state, doublea, doubleb)\n\n"}, {"name": "double random_chisquare()", "path": "reference/random/c-api#c.random_chisquare", "type": "C API for random", "text": "  doublerandom_chisquare(bitgen_t*bitgen_state, doubledf)\n\n"}, {"name": "double random_exponential()", "path": "reference/random/c-api#c.random_exponential", "type": "C API for random", "text": "  doublerandom_exponential(bitgen_t*bitgen_state, doublescale)\n\n"}, {"name": "double random_f()", "path": "reference/random/c-api#c.random_f", "type": "C API for random", "text": "  doublerandom_f(bitgen_t*bitgen_state, doubledfnum, doubledfden)\n\n"}, {"name": "double random_gamma()", "path": "reference/random/c-api#c.random_gamma", "type": "C API for random", "text": "  doublerandom_gamma(bitgen_t*bitgen_state, doubleshape, doublescale)\n\n"}, {"name": "double random_gumbel()", "path": "reference/random/c-api#c.random_gumbel", "type": "C API for random", "text": "  doublerandom_gumbel(bitgen_t*bitgen_state, doubleloc, doublescale)\n\n"}, {"name": "double random_laplace()", "path": "reference/random/c-api#c.random_laplace", "type": "C API for random", "text": "  doublerandom_laplace(bitgen_t*bitgen_state, doubleloc, doublescale)\n\n"}, {"name": "double random_logistic()", "path": "reference/random/c-api#c.random_logistic", "type": "C API for random", "text": "  doublerandom_logistic(bitgen_t*bitgen_state, doubleloc, doublescale)\n\n"}, {"name": "double random_lognormal()", "path": "reference/random/c-api#c.random_lognormal", "type": "C API for random", "text": "  doublerandom_lognormal(bitgen_t*bitgen_state, doublemean, doublesigma)\n\n"}, {"name": "double random_noncentral_chisquare()", "path": "reference/random/c-api#c.random_noncentral_chisquare", "type": "C API for random", "text": "  doublerandom_noncentral_chisquare(bitgen_t*bitgen_state, doubledf, doublenonc)\n\n"}, {"name": "double random_noncentral_f()", "path": "reference/random/c-api#c.random_noncentral_f", "type": "C API for random", "text": "  doublerandom_noncentral_f(bitgen_t*bitgen_state, doubledfnum, doubledfden, doublenonc)\n\n"}, {"name": "double random_normal()", "path": "reference/random/c-api#c.random_normal", "type": "C API for random", "text": "  doublerandom_normal(bitgen_t*bitgen_state, doubleloc, doublescale)\n\n"}, {"name": "double random_pareto()", "path": "reference/random/c-api#c.random_pareto", "type": "C API for random", "text": "  doublerandom_pareto(bitgen_t*bitgen_state, doublea)\n\n"}, {"name": "double random_power()", "path": "reference/random/c-api#c.random_power", "type": "C API for random", "text": "  doublerandom_power(bitgen_t*bitgen_state, doublea)\n\n"}, {"name": "double random_rayleigh()", "path": "reference/random/c-api#c.random_rayleigh", "type": "C API for random", "text": "  doublerandom_rayleigh(bitgen_t*bitgen_state, doublemode)\n\n"}, {"name": "double random_standard_cauchy()", "path": "reference/random/c-api#c.random_standard_cauchy", "type": "C API for random", "text": "  doublerandom_standard_cauchy(bitgen_t*bitgen_state)\n\n"}, {"name": "double random_standard_exponential()", "path": "reference/random/c-api#c.random_standard_exponential", "type": "C API for random", "text": "  doublerandom_standard_exponential(bitgen_t*bitgen_state)\n\n"}, {"name": "double random_standard_gamma()", "path": "reference/random/c-api#c.random_standard_gamma", "type": "C API for random", "text": "  doublerandom_standard_gamma(bitgen_t*bitgen_state, doubleshape)\n\n"}, {"name": "double random_standard_normal()", "path": "reference/random/c-api#c.random_standard_normal", "type": "C API for random", "text": "  doublerandom_standard_normal(bitgen_t*bitgen_state)\n\n"}, {"name": "double random_standard_t()", "path": "reference/random/c-api#c.random_standard_t", "type": "C API for random", "text": "  doublerandom_standard_t(bitgen_t*bitgen_state, doubledf)\n\n"}, {"name": "double random_standard_uniform()", "path": "reference/random/c-api#c.random_standard_uniform", "type": "C API for random", "text": "  doublerandom_standard_uniform(bitgen_t*bitgen_state)\n\n"}, {"name": "double random_triangular()", "path": "reference/random/c-api#c.random_triangular", "type": "C API for random", "text": "  doublerandom_triangular(bitgen_t*bitgen_state, doubleleft, doublemode, doubleright)\n\n"}, {"name": "double random_uniform()", "path": "reference/random/c-api#c.random_uniform", "type": "C API for random", "text": "  doublerandom_uniform(bitgen_t*bitgen_state, doublelower, doublerange)\n\n"}, {"name": "double random_vonmises()", "path": "reference/random/c-api#c.random_vonmises", "type": "C API for random", "text": "  doublerandom_vonmises(bitgen_t*bitgen_state, doublemu, doublekappa)\n\n"}, {"name": "double random_wald()", "path": "reference/random/c-api#c.random_wald", "type": "C API for random", "text": "  doublerandom_wald(bitgen_t*bitgen_state, doublemean, doublescale)\n\n"}, {"name": "double random_weibull()", "path": "reference/random/c-api#c.random_weibull", "type": "C API for random", "text": "  doublerandom_weibull(bitgen_t*bitgen_state, doublea)\n\n"}, {"name": "DoxyLimbo()", "path": "dev/howto-docs#_CPPv4N9DoxyLimbo9DoxyLimboERK9DoxyLimboI2Tp1NE", "type": "Development", "text": "  DoxyLimbo(constDoxyLimbo<Tp,N>&l)\n \nSet Default behavior for copy the limbo.  \n"}, {"name": "dtype object", "path": "reference/arrays.dtypes", "type": "Data type objects ( \n      \n       dtype\n      \n      )", "text": "Data type objects (dtype) A data type object (an instance of numpy.dtype class) describes how the bytes in the fixed-size block of memory corresponding to an array item should be interpreted. It describes the following aspects of the data:  Type of the data (integer, float, Python object, etc.) Size of the data (how many bytes is in e.g. the integer) Byte order of the data (little-endian or big-endian) \nIf the data type is structured data type, an aggregate of other data types, (e.g., describing an array item consisting of an integer and a float),  what are the names of the \u201cfields\u201d of the structure, by which they can be accessed, what is the data-type of each field, and which part of the memory block each field takes.   If the data type is a sub-array, what is its shape and data type.  To describe the type of scalar data, there are several built-in scalar types in NumPy for various precision of integers, floating-point numbers, etc. An item extracted from an array, e.g., by indexing, will be a Python object whose type is the scalar type associated with the data type of the array. Note that the scalar types are not dtype objects, even though they can be used in place of one whenever a data type specification is needed in NumPy. Structured data types are formed by creating a data type whose field contain other data types. Each field has a name by which it can be accessed. The parent data type should be of sufficient size to contain all its fields; the parent is nearly always based on the void type which allows an arbitrary item size. Structured data types may also contain nested structured sub-array data types in their fields. Finally, a data type can describe items that are themselves arrays of items of another data type. These sub-arrays must, however, be of a fixed size. If an array is created using a data-type describing a sub-array, the dimensions of the sub-array are appended to the shape of the array when the array is created. Sub-arrays in a field of a structured type behave differently, see Field access. Sub-arrays always have a C-contiguous memory layout. Example A simple data type containing a 32-bit big-endian integer: (see Specifying and constructing data types for details on construction) >>> dt = np.dtype('>i4')\n>>> dt.byteorder\n'>'\n>>> dt.itemsize\n4\n>>> dt.name\n'int32'\n>>> dt.type is np.int32\nTrue\n The corresponding array scalar type is int32. Example A structured data type containing a 16-character string (in field \u2018name\u2019) and a sub-array of two 64-bit floating-point number (in field \u2018grades\u2019): >>> dt = np.dtype([('name', np.unicode_, 16), ('grades', np.float64, (2,))])\n>>> dt['name']\ndtype('<U16')\n>>> dt['grades']\ndtype(('<f8', (2,)))\n Items of an array of this data type are wrapped in an array scalar type that also has two fields: >>> x = np.array([('Sarah', (8.0, 7.0)), ('John', (6.0, 7.0))], dtype=dt)\n>>> x[1]\n('John', [6., 7.])\n>>> x[1]['grades']\narray([6.,  7.])\n>>> type(x[1])\n<class 'numpy.void'>\n>>> type(x[1]['grades'])\n<class 'numpy.ndarray'>\n  Specifying and constructing data types Whenever a data-type is required in a NumPy function or method, either a dtype object or something that can be converted to one can be supplied. Such conversions are done by the dtype constructor:  \ndtype(dtype[, align, copy]) Create a data type object.   What can be converted to a data-type object is described below:  \ndtype object\n\nUsed as-is.  None\n\nThe default data type: float_.    Array-scalar types\n\nThe 24 built-in array scalar type objects all convert to an associated data-type object. This is true for their sub-classes as well. Note that not all data-type information can be supplied with a type-object: for example, flexible data-types have a default itemsize of 0, and require an explicitly given size to be useful. Example >>> dt = np.dtype(np.int32)      # 32-bit integer\n>>> dt = np.dtype(np.complex128) # 128-bit complex floating-point number\n  Generic types\n\nThe generic hierarchical type objects convert to corresponding type objects according to the associations:  \nnumber, inexact, floating float  \ncomplexfloating cfloat  \ninteger, signedinteger int_  \nunsignedinteger uint  \ncharacter string  \ngeneric, flexible void    Deprecated since version 1.19: This conversion of generic scalar types is deprecated. This is because it can be unexpected in a context such as arr.astype(dtype=np.floating), which casts an array of float32 to an array of float64, even though float32 is a subdtype of np.floating.   Built-in Python types\n\nSeveral python types are equivalent to a corresponding array scalar when used to generate a dtype object:  \nint int_  \nbool bool_  \nfloat float_  \ncomplex cfloat  \nbytes bytes_  \nstr str_  \nbuffer void  \n(all others) object_   Note that str refers to either null terminated bytes or unicode strings depending on the Python version. In code targeting both Python 2 and 3 np.unicode_ should be used as a dtype for strings. See Note on string types. Example >>> dt = np.dtype(float)   # Python-compatible floating-point number\n>>> dt = np.dtype(int)     # Python-compatible integer\n>>> dt = np.dtype(object)  # Python object\n  Note All other types map to object_ for convenience. Code should expect that such types may map to a specific (new) dtype in the future.   Types with .dtype\n\n\nAny type object with a dtype attribute: The attribute will be accessed and used directly. The attribute must return something that is convertible into a dtype object.   Several kinds of strings can be converted. Recognized strings can be prepended with '>' (big-endian), '<' (little-endian), or '=' (hardware-native, the default), to specify the byte order.  One-character strings\n\nEach built-in data-type has a character code (the updated Numeric typecodes), that uniquely identifies it. Example >>> dt = np.dtype('b')  # byte, native byte order\n>>> dt = np.dtype('>H') # big-endian unsigned short\n>>> dt = np.dtype('<f') # little-endian single-precision float\n>>> dt = np.dtype('d')  # double-precision floating-point number\n  Array-protocol type strings (see The Array Interface)\n\nThe first character specifies the kind of data and the remaining characters specify the number of bytes per item, except for Unicode, where it is interpreted as the number of characters. The item size must correspond to an existing type, or an error will be raised. The supported kinds are  \n'?' boolean  \n'b' (signed) byte  \n'B' unsigned byte  \n'i' (signed) integer  \n'u' unsigned integer  \n'f' floating-point  \n'c' complex-floating point  \n'm' timedelta  \n'M' datetime  \n'O' (Python) objects  \n'S', 'a' zero-terminated bytes (not recommended)  \n'U' Unicode string  \n'V' raw data (void)   Example >>> dt = np.dtype('i4')   # 32-bit signed integer\n>>> dt = np.dtype('f8')   # 64-bit floating-point number\n>>> dt = np.dtype('c16')  # 128-bit complex floating-point number\n>>> dt = np.dtype('a25')  # 25-length zero-terminated bytes\n>>> dt = np.dtype('U25')  # 25-character string\n  Note on string types For backward compatibility with Python 2 the S and a typestrings remain zero-terminated bytes and numpy.string_ continues to alias numpy.bytes_. To use actual strings in Python 3 use U or numpy.str_. For signed bytes that do not need zero-termination b or i1 can be used.   String with comma-separated fields\n\nA short-hand notation for specifying the format of a structured data type is a comma-separated string of basic formats. A basic format in this context is an optional shape specifier followed by an array-protocol type string. Parenthesis are required on the shape if it has more than one dimension. NumPy allows a modification on the format in that any string that can uniquely identify the type can be used to specify the data-type in a field. The generated data-type fields are named 'f0', 'f1', \u2026, 'f<N-1>' where N (>1) is the number of comma-separated basic formats in the string. If the optional shape specifier is provided, then the data-type for the corresponding field describes a sub-array. Example  field named f0 containing a 32-bit integer field named f1 containing a 2 x 3 sub-array of 64-bit floating-point numbers field named f2 containing a 32-bit floating-point number  >>> dt = np.dtype(\"i4, (2,3)f8, f4\")\n  field named f0 containing a 3-character string field named f1 containing a sub-array of shape (3,) containing 64-bit unsigned integers field named f2 containing a 3 x 4 sub-array containing 10-character strings  >>> dt = np.dtype(\"a3, 3u8, (3,4)a10\")\n  Type strings\n\nAny string in numpy.sctypeDict.keys(): Example >>> dt = np.dtype('uint32')   # 32-bit unsigned integer\n>>> dt = np.dtype('float64')  # 64-bit floating-point number\n    (flexible_dtype, itemsize)\n\nThe first argument must be an object that is converted to a zero-sized flexible data-type object, the second argument is an integer providing the desired itemsize. Example >>> dt = np.dtype((np.void, 10))  # 10-byte wide data block\n>>> dt = np.dtype(('U', 10))   # 10-character unicode string\n  (fixed_dtype, shape)\n\nThe first argument is any object that can be converted into a fixed-size data-type object. The second argument is the desired shape of this type. If the shape parameter is 1, then the data-type object used to be equivalent to fixed dtype. This behaviour is deprecated since NumPy 1.17 and will raise an error in the future. If shape is a tuple, then the new dtype defines a sub-array of the given shape. Example >>> dt = np.dtype((np.int32, (2,2)))          # 2 x 2 integer sub-array\n>>> dt = np.dtype(('i4, (2,3)f8, f4', (2,3))) # 2 x 3 structured sub-array\n    [(field_name, field_dtype, field_shape), ...]\n\nobj should be a list of fields where each field is described by a tuple of length 2 or 3. (Equivalent to the descr item in the __array_interface__ attribute.) The first element, field_name, is the field name (if this is '' then a standard field name, 'f#', is assigned). The field name may also be a 2-tuple of strings where the first string is either a \u201ctitle\u201d (which may be any string or unicode string) or meta-data for the field which can be any object, and the second string is the \u201cname\u201d which must be a valid Python identifier. The second element, field_dtype, can be anything that can be interpreted as a data-type. The optional third element field_shape contains the shape if this field represents an array of the data-type in the second element. Note that a 3-tuple with a third argument equal to 1 is equivalent to a 2-tuple. This style does not accept align in the dtype constructor as it is assumed that all of the memory is accounted for by the array interface description. Example Data-type with fields big (big-endian 32-bit integer) and little (little-endian 32-bit integer): >>> dt = np.dtype([('big', '>i4'), ('little', '<i4')])\n Data-type with fields R, G, B, A, each being an unsigned 8-bit integer: >>> dt = np.dtype([('R','u1'), ('G','u1'), ('B','u1'), ('A','u1')])\n    {'names': ..., 'formats': ..., 'offsets': ..., 'titles': ..., 'itemsize': ...}\n\nThis style has two required and three optional keys. The names and formats keys are required. Their respective values are equal-length lists with the field names and the field formats. The field names must be strings and the field formats can be any object accepted by dtype constructor. When the optional keys offsets and titles are provided, their values must each be lists of the same length as the names and formats lists. The offsets value is a list of byte offsets (limited to ctypes.c_int) for each field, while the titles value is a list of titles for each field (None can be used if no title is desired for that field). The titles can be any object, but when a str object will add another entry to the fields dictionary keyed by the title and referencing the same field tuple which will contain the title as an additional tuple member. The itemsize key allows the total size of the dtype to be set, and must be an integer large enough so all the fields are within the dtype. If the dtype being constructed is aligned, the itemsize must also be divisible by the struct alignment. Total dtype itemsize is limited to ctypes.c_int. Example Data type with fields r, g, b, a, each being an 8-bit unsigned integer: >>> dt = np.dtype({'names': ['r','g','b','a'],\n...                'formats': [np.uint8, np.uint8, np.uint8, np.uint8]})\n Data type with fields r and b (with the given titles), both being 8-bit unsigned integers, the first at byte position 0 from the start of the field and the second at position 2: >>> dt = np.dtype({'names': ['r','b'], 'formats': ['u1', 'u1'],\n...                'offsets': [0, 2],\n...                'titles': ['Red pixel', 'Blue pixel']})\n  {'field1': ..., 'field2': ..., ...}\n\nThis usage is discouraged, because it is ambiguous with the other dict-based construction method. If you have a field called \u2018names\u2019 and a field called \u2018formats\u2019 there will be a conflict. This style allows passing in the fields attribute of a data-type object. obj should contain string or unicode keys that refer to (data-type, offset) or (data-type, offset, title) tuples. Example Data type containing field col1 (10-character string at byte position 0), col2 (32-bit float at byte position 10), and col3 (integers at byte position 14): >>> dt = np.dtype({'col1': ('U10', 0), 'col2': (np.float32, 10),\n...                'col3': (int, 14)})\n  (base_dtype, new_dtype)\n\nIn NumPy 1.7 and later, this form allows base_dtype to be interpreted as a structured dtype. Arrays created with this dtype will have underlying dtype base_dtype but will have fields and flags taken from new_dtype. This is useful for creating custom structured dtypes, as done in record arrays. This form also makes it possible to specify struct dtypes with overlapping fields, functioning like the \u2018union\u2019 type in C. This usage is discouraged, however, and the union mechanism is preferred. Both arguments must be convertible to data-type objects with the same total size. Example 32-bit integer, whose first two bytes are interpreted as an integer via field real, and the following two bytes via field imag. >>> dt = np.dtype((np.int32,{'real':(np.int16, 0),'imag':(np.int16, 2)}))\n 32-bit integer, which is interpreted as consisting of a sub-array of shape (4,) containing 8-bit integers: >>> dt = np.dtype((np.int32, (np.int8, 4)))\n 32-bit integer, containing fields r, g, b, a that interpret the 4 bytes in the integer as four unsigned integers: >>> dt = np.dtype(('i4', [('r','u1'),('g','u1'),('b','u1'),('a','u1')]))\n     dtype NumPy data type descriptions are instances of the dtype class.  Attributes The type of the data is described by the following dtype attributes:  \ndtype.type   \ndtype.kind A character code (one of 'biufcmMOSUV') identifying the general kind of data.  \ndtype.char A unique character code for each of the 21 different built-in types.  \ndtype.num A unique number for each of the 21 different built-in types.  \ndtype.str The array-protocol typestring of this data-type object.   Size of the data is in turn described by:  \ndtype.name A bit-width name for this data-type.  \ndtype.itemsize The element size of this data-type object.   Endianness of this data:  \ndtype.byteorder A character indicating the byte-order of this data-type object.   Information about sub-data-types in a structured data type:  \ndtype.fields Dictionary of named fields defined for this data type, or None.  \ndtype.names Ordered list of field names, or None if there are no fields.   For data types that describe sub-arrays:  \ndtype.subdtype Tuple (item_dtype, shape) if this dtype describes a sub-array, and None otherwise.  \ndtype.shape Shape tuple of the sub-array if this data type describes a sub-array, and () otherwise.   Attributes providing additional information:  \ndtype.hasobject Boolean indicating whether this dtype contains any reference-counted objects in any fields or sub-dtypes.  \ndtype.flags Bit-flags describing how this data type is to be interpreted.  \ndtype.isbuiltin Integer indicating how this dtype relates to the built-in dtypes.  \ndtype.isnative Boolean indicating whether the byte order of this dtype is native to the platform.  \ndtype.descr __array_interface__ description of the data-type.  \ndtype.alignment The required alignment (bytes) of this data-type according to the compiler.  \ndtype.base Returns dtype for the base element of the subarrays, regardless of their dimension or shape.   Metadata attached by the user:  \ndtype.metadata Either None or a readonly dictionary of metadata (mappingproxy).     Methods Data types have the following method for changing the byte order:  \ndtype.newbyteorder([new_order]) Return a new dtype with a different byte order.   The following methods implement the pickle protocol:  \ndtype.__reduce__ Helper for pickle.  \ndtype.__setstate__    Utility method for typing:  \ndtype.__class_getitem__(item, /) Return a parametrized wrapper around the dtype type.   Comparison operations:  \ndtype.__ge__(value, /) Return self>=value.  \ndtype.__gt__(value, /) Return self>value.  \ndtype.__le__(value, /) Return self<=value.  \ndtype.__lt__(value, /) Return self<value.    \n"}, {"name": "dtype.__class_getitem__()", "path": "reference/generated/numpy.dtype.__class_getitem__", "type": "numpy.dtype.__class_getitem__", "text": "numpy.dtype.__class_getitem__ method   dtype.__class_getitem__(item, /)\n \nReturn a parametrized wrapper around the dtype type.  New in version 1.22.   Returns \n \naliastypes.GenericAlias\n\n\nA parametrized dtype type.      See also  PEP 585\n\nType hinting generics in standard collections.    Notes This method is only available for python 3.9 and later. Examples >>> import numpy as np\n >>> np.dtype[np.int64]\nnumpy.dtype[numpy.int64]\n \n\n"}, {"name": "dtype.__ge__()", "path": "reference/generated/numpy.dtype.__ge__", "type": "numpy.dtype.__ge__", "text": "numpy.dtype.__ge__ method   dtype.__ge__(value, /)\n \nReturn self>=value. \n\n"}, {"name": "dtype.__gt__()", "path": "reference/generated/numpy.dtype.__gt__", "type": "numpy.dtype.__gt__", "text": "numpy.dtype.__gt__ method   dtype.__gt__(value, /)\n \nReturn self>value. \n\n"}, {"name": "dtype.__le__()", "path": "reference/generated/numpy.dtype.__le__", "type": "numpy.dtype.__le__", "text": "numpy.dtype.__le__ method   dtype.__le__(value, /)\n \nReturn self<=value. \n\n"}, {"name": "dtype.__lt__()", "path": "reference/generated/numpy.dtype.__lt__", "type": "numpy.dtype.__lt__", "text": "numpy.dtype.__lt__ method   dtype.__lt__(value, /)\n \nReturn self<value. \n\n"}, {"name": "dtype.__reduce__()", "path": "reference/generated/numpy.dtype.__reduce__", "type": "numpy.dtype.__reduce__", "text": "numpy.dtype.__reduce__ method   dtype.__reduce__()\n \nHelper for pickle. \n\n"}, {"name": "dtype.__setstate__()", "path": "reference/generated/numpy.dtype.__setstate__", "type": "numpy.dtype.__setstate__", "text": "numpy.dtype.__setstate__ method   dtype.__setstate__()\n\n\n"}, {"name": "dtype.alignment", "path": "reference/generated/numpy.dtype.alignment", "type": "numpy.dtype.alignment", "text": "numpy.dtype.alignment attribute   dtype.alignment\n \nThe required alignment (bytes) of this data-type according to the compiler. More information is available in the C-API section of the manual. Examples >>> x = np.dtype('i4')\n>>> x.alignment\n4\n >>> x = np.dtype(float)\n>>> x.alignment\n8\n \n\n"}, {"name": "dtype.base", "path": "reference/generated/numpy.dtype.base", "type": "numpy.dtype.base", "text": "numpy.dtype.base attribute   dtype.base\n \nReturns dtype for the base element of the subarrays, regardless of their dimension or shape.  See also  dtype.subdtype\n  Examples >>> x = numpy.dtype('8f')\n>>> x.base\ndtype('float32')\n >>> x =  numpy.dtype('i2')\n>>> x.base\ndtype('int16')\n \n\n"}, {"name": "dtype.byteorder", "path": "reference/generated/numpy.dtype.byteorder", "type": "numpy.dtype.byteorder", "text": "numpy.dtype.byteorder attribute   dtype.byteorder\n \nA character indicating the byte-order of this data-type object. One of:  \n\u2018=\u2019 native  \n\u2018<\u2019 little-endian  \n\u2018>\u2019 big-endian  \n\u2018|\u2019 not applicable   All built-in data-type objects have byteorder either \u2018=\u2019 or \u2018|\u2019. Examples >>> dt = np.dtype('i2')\n>>> dt.byteorder\n'='\n>>> # endian is not relevant for 8 bit numbers\n>>> np.dtype('i1').byteorder\n'|'\n>>> # or ASCII strings\n>>> np.dtype('S2').byteorder\n'|'\n>>> # Even if specific code is given, and it is native\n>>> # '=' is the byteorder\n>>> import sys\n>>> sys_is_le = sys.byteorder == 'little'\n>>> native_code = sys_is_le and '<' or '>'\n>>> swapped_code = sys_is_le and '>' or '<'\n>>> dt = np.dtype(native_code + 'i2')\n>>> dt.byteorder\n'='\n>>> # Swapped code shows up as itself\n>>> dt = np.dtype(swapped_code + 'i2')\n>>> dt.byteorder == swapped_code\nTrue\n \n\n"}, {"name": "dtype.char", "path": "reference/generated/numpy.dtype.char", "type": "numpy.dtype.char", "text": "numpy.dtype.char attribute   dtype.char\n \nA unique character code for each of the 21 different built-in types. Examples >>> x = np.dtype(float)\n>>> x.char\n'd'\n \n\n"}, {"name": "dtype.descr", "path": "reference/generated/numpy.dtype.descr", "type": "numpy.dtype.descr", "text": "numpy.dtype.descr attribute   dtype.descr\n \n__array_interface__ description of the data-type. The format is that required by the \u2018descr\u2019 key in the __array_interface__ attribute. Warning: This attribute exists specifically for __array_interface__, and passing it directly to np.dtype will not accurately reconstruct some dtypes (e.g., scalar and subarray dtypes). Examples >>> x = np.dtype(float)\n>>> x.descr\n[('', '<f8')]\n >>> dt = np.dtype([('name', np.str_, 16), ('grades', np.float64, (2,))])\n>>> dt.descr\n[('name', '<U16'), ('grades', '<f8', (2,))]\n \n\n"}, {"name": "dtype.fields", "path": "reference/generated/numpy.dtype.fields", "type": "numpy.dtype.fields", "text": "numpy.dtype.fields attribute   dtype.fields\n \nDictionary of named fields defined for this data type, or None. The dictionary is indexed by keys that are the names of the fields. Each entry in the dictionary is a tuple fully describing the field: (dtype, offset[, title])\n Offset is limited to C int, which is signed and usually 32 bits. If present, the optional title can be any object (if it is a string or unicode then it will also be a key in the fields dictionary, otherwise it\u2019s meta-data). Notice also that the first two elements of the tuple can be passed directly as arguments to the ndarray.getfield and ndarray.setfield methods.  See also  \nndarray.getfield, ndarray.setfield\n\n  Examples >>> dt = np.dtype([('name', np.str_, 16), ('grades', np.float64, (2,))])\n>>> print(dt.fields)\n{'grades': (dtype(('float64',(2,))), 16), 'name': (dtype('|S16'), 0)}\n \n\n"}, {"name": "dtype.flags", "path": "reference/generated/numpy.dtype.flags", "type": "numpy.dtype.flags", "text": "numpy.dtype.flags attribute   dtype.flags\n \nBit-flags describing how this data type is to be interpreted. Bit-masks are in numpy.core.multiarray as the constants ITEM_HASOBJECT, LIST_PICKLE, ITEM_IS_POINTER, NEEDS_INIT, NEEDS_PYAPI, USE_GETITEM, USE_SETITEM. A full explanation of these flags is in C-API documentation; they are largely useful for user-defined data-types. The following example demonstrates that operations on this particular dtype requires Python C-API. Examples >>> x = np.dtype([('a', np.int32, 8), ('b', np.float64, 6)])\n>>> x.flags\n16\n>>> np.core.multiarray.NEEDS_PYAPI\n16\n \n\n"}, {"name": "dtype.hasobject", "path": "reference/generated/numpy.dtype.hasobject", "type": "numpy.dtype.hasobject", "text": "numpy.dtype.hasobject attribute   dtype.hasobject\n \nBoolean indicating whether this dtype contains any reference-counted objects in any fields or sub-dtypes. Recall that what is actually in the ndarray memory representing the Python object is the memory address of that object (a pointer). Special handling may be required, and this attribute is useful for distinguishing data types that may contain arbitrary Python objects and data-types that won\u2019t. \n\n"}, {"name": "dtype.isalignedstruct", "path": "reference/generated/numpy.dtype.isalignedstruct", "type": "Data type objects", "text": "numpy.dtype.isalignedstruct attribute   dtype.isalignedstruct\n \nBoolean indicating whether the dtype is a struct which maintains field alignment. This flag is sticky, so when combining multiple structs together, it is preserved and produces new dtypes which are also aligned. \n\n"}, {"name": "dtype.isbuiltin", "path": "reference/generated/numpy.dtype.isbuiltin", "type": "numpy.dtype.isbuiltin", "text": "numpy.dtype.isbuiltin attribute   dtype.isbuiltin\n \nInteger indicating how this dtype relates to the built-in dtypes. Read-only.  \n0 if this is a structured array type, with fields  \n1 if this is a dtype compiled into numpy (such as ints, floats etc)  \n2 if the dtype is for a user-defined numpy type A user-defined type uses the numpy C-API machinery to extend numpy to handle a new array type. See User-defined data-types in the NumPy manual.   Examples >>> dt = np.dtype('i2')\n>>> dt.isbuiltin\n1\n>>> dt = np.dtype('f8')\n>>> dt.isbuiltin\n1\n>>> dt = np.dtype([('field1', 'f8')])\n>>> dt.isbuiltin\n0\n \n\n"}, {"name": "dtype.isnative", "path": "reference/generated/numpy.dtype.isnative", "type": "numpy.dtype.isnative", "text": "numpy.dtype.isnative attribute   dtype.isnative\n \nBoolean indicating whether the byte order of this dtype is native to the platform. \n\n"}, {"name": "dtype.itemsize", "path": "reference/generated/numpy.dtype.itemsize", "type": "numpy.dtype.itemsize", "text": "numpy.dtype.itemsize attribute   dtype.itemsize\n \nThe element size of this data-type object. For 18 of the 21 types this number is fixed by the data-type. For the flexible data-types, this number can be anything. Examples >>> arr = np.array([[1, 2], [3, 4]])\n>>> arr.dtype\ndtype('int64')\n>>> arr.itemsize\n8\n >>> dt = np.dtype([('name', np.str_, 16), ('grades', np.float64, (2,))])\n>>> dt.itemsize\n80\n \n\n"}, {"name": "dtype.kind", "path": "reference/generated/numpy.dtype.kind", "type": "numpy.dtype.kind", "text": "numpy.dtype.kind attribute   dtype.kind\n \nA character code (one of \u2018biufcmMOSUV\u2019) identifying the general kind of data.  \nb boolean  \ni signed integer  \nu unsigned integer  \nf floating-point  \nc complex floating-point  \nm timedelta  \nM datetime  \nO object  \nS (byte-)string  \nU Unicode  \nV void   Examples >>> dt = np.dtype('i4')\n>>> dt.kind\n'i'\n>>> dt = np.dtype('f8')\n>>> dt.kind\n'f'\n>>> dt = np.dtype([('field1', 'f8')])\n>>> dt.kind\n'V'\n \n\n"}, {"name": "dtype.metadata", "path": "reference/generated/numpy.dtype.metadata", "type": "numpy.dtype.metadata", "text": "numpy.dtype.metadata attribute   dtype.metadata\n \nEither None or a readonly dictionary of metadata (mappingproxy). The metadata field can be set using any dictionary at data-type creation. NumPy currently has no uniform approach to propagating metadata; although some array operations preserve it, there is no guarantee that others will.  Warning Although used in certain projects, this feature was long undocumented and is not well supported. Some aspects of metadata propagation are expected to change in the future.  Examples >>> dt = np.dtype(float, metadata={\"key\": \"value\"})\n>>> dt.metadata[\"key\"]\n'value'\n>>> arr = np.array([1, 2, 3], dtype=dt)\n>>> arr.dtype.metadata\nmappingproxy({'key': 'value'})\n Adding arrays with identical datatypes currently preserves the metadata: >>> (arr + arr).dtype.metadata\nmappingproxy({'key': 'value'})\n But if the arrays have different dtype metadata, the metadata may be dropped: >>> dt2 = np.dtype(float, metadata={\"key2\": \"value2\"})\n>>> arr2 = np.array([3, 2, 1], dtype=dt2)\n>>> (arr + arr2).dtype.metadata is None\nTrue  # The metadata field is cleared so None is returned\n \n\n"}, {"name": "dtype.name", "path": "reference/generated/numpy.dtype.name", "type": "numpy.dtype.name", "text": "numpy.dtype.name attribute   dtype.name\n \nA bit-width name for this data-type. Un-sized flexible data-type objects do not have this attribute. Examples >>> x = np.dtype(float)\n>>> x.name\n'float64'\n>>> x = np.dtype([('a', np.int32, 8), ('b', np.float64, 6)])\n>>> x.name\n'void640'\n \n\n"}, {"name": "dtype.names", "path": "reference/generated/numpy.dtype.names", "type": "numpy.dtype.names", "text": "numpy.dtype.names attribute   dtype.names\n \nOrdered list of field names, or None if there are no fields. The names are ordered according to increasing byte offset. This can be used, for example, to walk through all of the named fields in offset order. Examples >>> dt = np.dtype([('name', np.str_, 16), ('grades', np.float64, (2,))])\n>>> dt.names\n('name', 'grades')\n \n\n"}, {"name": "dtype.ndim", "path": "reference/generated/numpy.dtype.ndim", "type": "Data type objects", "text": "numpy.dtype.ndim attribute   dtype.ndim\n \nNumber of dimensions of the sub-array if this data type describes a sub-array, and 0 otherwise.  New in version 1.13.0.  Examples >>> x = np.dtype(float)\n>>> x.ndim\n0\n >>> x = np.dtype((float, 8))\n>>> x.ndim\n1\n >>> x = np.dtype(('i4', (3, 4)))\n>>> x.ndim\n2\n \n\n"}, {"name": "dtype.newbyteorder()", "path": "reference/generated/numpy.dtype.newbyteorder", "type": "numpy.dtype.newbyteorder", "text": "numpy.dtype.newbyteorder method   dtype.newbyteorder(new_order='S', /)\n \nReturn a new dtype with a different byte order. Changes are also made in all fields and sub-arrays of the data type.  Parameters \n \nnew_orderstring, optional\n\n\nByte order to force; a value from the byte order specifications below. The default value (\u2018S\u2019) results in swapping the current byte order. new_order codes can be any of:  \u2018S\u2019 - swap dtype from current to opposite endian {\u2018<\u2019, \u2018little\u2019} - little endian {\u2018>\u2019, \u2018big\u2019} - big endian {\u2018=\u2019, \u2018native\u2019} - native order {\u2018|\u2019, \u2018I\u2019} - ignore (no change to byte order)     Returns \n \nnew_dtypedtype\n\n\nNew dtype object with the given change to the byte order.     Notes Changes are also made in all fields and sub-arrays of the data type. Examples >>> import sys\n>>> sys_is_le = sys.byteorder == 'little'\n>>> native_code = sys_is_le and '<' or '>'\n>>> swapped_code = sys_is_le and '>' or '<'\n>>> native_dt = np.dtype(native_code+'i2')\n>>> swapped_dt = np.dtype(swapped_code+'i2')\n>>> native_dt.newbyteorder('S') == swapped_dt\nTrue\n>>> native_dt.newbyteorder() == swapped_dt\nTrue\n>>> native_dt == swapped_dt.newbyteorder('S')\nTrue\n>>> native_dt == swapped_dt.newbyteorder('=')\nTrue\n>>> native_dt == swapped_dt.newbyteorder('N')\nTrue\n>>> native_dt == native_dt.newbyteorder('|')\nTrue\n>>> np.dtype('<i2') == native_dt.newbyteorder('<')\nTrue\n>>> np.dtype('<i2') == native_dt.newbyteorder('L')\nTrue\n>>> np.dtype('>i2') == native_dt.newbyteorder('>')\nTrue\n>>> np.dtype('>i2') == native_dt.newbyteorder('B')\nTrue\n \n\n"}, {"name": "dtype.num", "path": "reference/generated/numpy.dtype.num", "type": "numpy.dtype.num", "text": "numpy.dtype.num attribute   dtype.num\n \nA unique number for each of the 21 different built-in types. These are roughly ordered from least-to-most precision. Examples >>> dt = np.dtype(str)\n>>> dt.num\n19\n >>> dt = np.dtype(float)\n>>> dt.num\n12\n \n\n"}, {"name": "dtype.shape", "path": "reference/generated/numpy.dtype.shape", "type": "numpy.dtype.shape", "text": "numpy.dtype.shape attribute   dtype.shape\n \nShape tuple of the sub-array if this data type describes a sub-array, and () otherwise. Examples >>> dt = np.dtype(('i4', 4))\n>>> dt.shape\n(4,)\n >>> dt = np.dtype(('i4', (2, 3)))\n>>> dt.shape\n(2, 3)\n \n\n"}, {"name": "dtype.str", "path": "reference/generated/numpy.dtype.str", "type": "numpy.dtype.str", "text": "numpy.dtype.str attribute   dtype.str\n \nThe array-protocol typestring of this data-type object. \n\n"}, {"name": "dtype.subdtype", "path": "reference/generated/numpy.dtype.subdtype", "type": "numpy.dtype.subdtype", "text": "numpy.dtype.subdtype attribute   dtype.subdtype\n \nTuple (item_dtype, shape) if this dtype describes a sub-array, and None otherwise. The shape is the fixed shape of the sub-array described by this data type, and item_dtype the data type of the array. If a field whose dtype object has this attribute is retrieved, then the extra dimensions implied by shape are tacked on to the end of the retrieved array.  See also  dtype.base\n  Examples >>> x = numpy.dtype('8f')\n>>> x.subdtype\n(dtype('float32'), (8,))\n >>> x =  numpy.dtype('i2')\n>>> x.subdtype\n>>>\n \n\n"}, {"name": "dtype.type", "path": "reference/generated/numpy.dtype.type", "type": "numpy.dtype.type", "text": "numpy.dtype.type attribute   dtype.type = None\n\n\n"}, {"name": "Elementary Function", "path": "reference/c-api/generalized-ufuncs", "type": "Generalized Universal Function API", "text": "Generalized Universal Function API There is a general need for looping over not only functions on scalars but also over functions on vectors (or arrays). This concept is realized in NumPy by generalizing the universal functions (ufuncs). In regular ufuncs, the elementary function is limited to element-by-element operations, whereas the generalized version (gufuncs) supports \u201csub-array\u201d by \u201csub-array\u201d operations. The Perl vector library PDL provides a similar functionality and its terms are re-used in the following. Each generalized ufunc has information associated with it that states what the \u201ccore\u201d dimensionality of the inputs is, as well as the corresponding dimensionality of the outputs (the element-wise ufuncs have zero core dimensions). The list of the core dimensions for all arguments is called the \u201csignature\u201d of a ufunc. For example, the ufunc numpy.add has signature (),()->() defining two scalar inputs and one scalar output. Another example is the function inner1d(a, b) with a signature of (i),(i)->(). This applies the inner product along the last axis of each input, but keeps the remaining indices intact. For example, where a is of shape (3, 5, N) and b is of shape (5, N), this will return an output of shape (3,5). The underlying elementary function is called 3 * 5 times. In the signature, we specify one core dimension (i) for each input and zero core dimensions () for the output, since it takes two 1-d arrays and returns a scalar. By using the same name i, we specify that the two corresponding dimensions should be of the same size. The dimensions beyond the core dimensions are called \u201cloop\u201d dimensions. In the above example, this corresponds to (3, 5). The signature determines how the dimensions of each input/output array are split into core and loop dimensions:  Each dimension in the signature is matched to a dimension of the corresponding passed-in array, starting from the end of the shape tuple. These are the core dimensions, and they must be present in the arrays, or an error will be raised. Core dimensions assigned to the same label in the signature (e.g. the i in inner1d\u2019s (i),(i)->()) must have exactly matching sizes, no broadcasting is performed. The core dimensions are removed from all inputs and the remaining dimensions are broadcast together, defining the loop dimensions. The shape of each output is determined from the loop dimensions plus the output\u2019s core dimensions  Typically, the size of all core dimensions in an output will be determined by the size of a core dimension with the same label in an input array. This is not a requirement, and it is possible to define a signature where a label comes up for the first time in an output, although some precautions must be taken when calling such a function. An example would be the function euclidean_pdist(a), with signature (n,d)->(p), that given an array of n d-dimensional vectors, computes all unique pairwise Euclidean distances among them. The output dimension p must therefore be equal to n * (n - 1) / 2, but it is the caller\u2019s responsibility to pass in an output array of the right size. If the size of a core dimension of an output cannot be determined from a passed in input or output array, an error will be raised. Note: Prior to NumPy 1.10.0, less strict checks were in place: missing core dimensions were created by prepending 1\u2019s to the shape as necessary, core dimensions with the same label were broadcast together, and undetermined dimensions were created with size 1.  Definitions  Elementary Function\n\nEach ufunc consists of an elementary function that performs the most basic operation on the smallest portion of array arguments (e.g. adding two numbers is the most basic operation in adding two arrays). The ufunc applies the elementary function multiple times on different parts of the arrays. The input/output of elementary functions can be vectors; e.g., the elementary function of inner1d takes two vectors as input.  Signature\n\nA signature is a string describing the input/output dimensions of the elementary function of a ufunc. See section below for more details.  Core Dimension\n\nThe dimensionality of each input/output of an elementary function is defined by its core dimensions (zero core dimensions correspond to a scalar input/output). The core dimensions are mapped to the last dimensions of the input/output arrays.  Dimension Name\n\nA dimension name represents a core dimension in the signature. Different dimensions may share a name, indicating that they are of the same size.  Dimension Index\n\nA dimension index is an integer representing a dimension name. It enumerates the dimension names according to the order of the first occurrence of each name in the signature.     Details of Signature The signature defines \u201ccore\u201d dimensionality of input and output variables, and thereby also defines the contraction of the dimensions. The signature is represented by a string of the following format:  Core dimensions of each input or output array are represented by a list of dimension names in parentheses, (i_1,...,i_N); a scalar input/output is denoted by (). Instead of i_1, i_2, etc, one can use any valid Python variable name. Dimension lists for different arguments are separated by \",\". Input/output arguments are separated by \"->\". If one uses the same dimension name in multiple locations, this enforces the same size of the corresponding dimensions.  The formal syntax of signatures is as follows: <Signature>            ::= <Input arguments> \"->\" <Output arguments>\n<Input arguments>      ::= <Argument list>\n<Output arguments>     ::= <Argument list>\n<Argument list>        ::= nil | <Argument> | <Argument> \",\" <Argument list>\n<Argument>             ::= \"(\" <Core dimension list> \")\"\n<Core dimension list>  ::= nil | <Core dimension> |\n                           <Core dimension> \",\" <Core dimension list>\n<Core dimension>       ::= <Dimension name> <Dimension modifier>\n<Dimension name>       ::= valid Python variable name | valid integer\n<Dimension modifier>   ::= nil | \"?\"\n Notes:  All quotes are for clarity. Unmodified core dimensions that share the same name must have the same size. Each dimension name typically corresponds to one level of looping in the elementary function\u2019s implementation. White spaces are ignored. An integer as a dimension name freezes that dimension to the value. If the name is suffixed with the \u201c?\u201d modifier, the dimension is a core dimension only if it exists on all inputs and outputs that share it; otherwise it is ignored (and replaced by a dimension of size 1 for the elementary function).  Here are some examples of signatures:   \nname signature common usage   \nadd (),()->() binary ufunc  \nsum1d (i)->() reduction  \ninner1d (i),(i)->() vector-vector multiplication  \nmatmat (m,n),(n,p)->(m,p) matrix multiplication  \nvecmat (n),(n,p)->(p) vector-matrix multiplication  \nmatvec (m,n),(n)->(m) matrix-vector multiplication  \nmatmul (m?,n),(n,p?)->(m?,p?) combination of the four above  \nouter_inner (i,t),(j,t)->(i,j) inner over the last dimension, outer over the second to last, and loop/broadcast over the rest.  \ncross1d (3),(3)->(3) cross product where the last dimension is frozen and must be 3   The last is an instance of freezing a core dimension and can be used to improve ufunc performance   C-API for implementing Elementary Functions The current interface remains unchanged, and PyUFunc_FromFuncAndData can still be used to implement (specialized) ufuncs, consisting of scalar elementary functions. One can use PyUFunc_FromFuncAndDataAndSignature to declare a more general ufunc. The argument list is the same as PyUFunc_FromFuncAndData, with an additional argument specifying the signature as C string. Furthermore, the callback function is of the same type as before, void (*foo)(char **args, intp *dimensions, intp *steps, void *func). When invoked, args is a list of length nargs containing the data of all input/output arguments. For a scalar elementary function, steps is also of length nargs, denoting the strides used for the arguments. dimensions is a pointer to a single integer defining the size of the axis to be looped over. For a non-trivial signature, dimensions will also contain the sizes of the core dimensions as well, starting at the second entry. Only one size is provided for each unique dimension name and the sizes are given according to the first occurrence of a dimension name in the signature. The first nargs elements of steps remain the same as for scalar ufuncs. The following elements contain the strides of all core dimensions for all arguments in order. For example, consider a ufunc with signature (i,j),(i)->(). In this case, args will contain three pointers to the data of the input/output arrays a, b, c. Furthermore, dimensions will be [N, I, J] to define the size of N of the loop and the sizes I and J for the core dimensions i and j. Finally, steps will be [a_N, b_N, c_N, a_i, a_j, b_i], containing all necessary strides. \n"}, {"name": "enum NPY_CASTING", "path": "reference/c-api/array#c.NPY_CASTING", "type": "Array API", "text": "  enumNPY_CASTING\n \n New in version 1.6.  An enumeration type indicating how permissive data conversions should be. This is used by the iterator added in NumPy 1.6, and is intended to be used more broadly in a future version.   enumeratorNPY_NO_CASTING\n \nOnly allow identical types. \n   enumeratorNPY_EQUIV_CASTING\n \nAllow identical and casts involving byte swapping. \n   enumeratorNPY_SAFE_CASTING\n \nOnly allow casts which will not cause values to be rounded, truncated, or otherwise changed. \n   enumeratorNPY_SAME_KIND_CASTING\n \nAllow any safe casts, and casts between types of the same kind. For example, float64 -> float32 is permitted with this rule. \n   enumeratorNPY_UNSAFE_CASTING\n \nAllow any cast, no matter what kind of data loss may occur. \n \n"}, {"name": "enum NPY_CLIPMODE", "path": "reference/c-api/array#c.NPY_CLIPMODE", "type": "Array API", "text": "  enumNPY_CLIPMODE\n \nA variable type indicating the kind of clipping that should be applied in certain functions.   enumeratorNPY_RAISE\n \nThe default for most operations, raises an exception if an index is out of bounds. \n   enumeratorNPY_CLIP\n \nClips an index to the valid range if it is out of bounds. \n   enumeratorNPY_WRAP\n \nWraps an index to the valid range if it is out of bounds. \n \n"}, {"name": "enum NPY_ORDER", "path": "reference/c-api/array#c.NPY_ORDER", "type": "Array API", "text": "  enumNPY_ORDER\n \nAn enumeration type indicating the element order that an array should be interpreted in. When a brand new array is created, generally only NPY_CORDER and NPY_FORTRANORDER are used, whereas when one or more inputs are provided, the order can be based on them.   enumeratorNPY_ANYORDER\n \nFortran order if all the inputs are Fortran, C otherwise. \n   enumeratorNPY_CORDER\n \nC order. \n   enumeratorNPY_FORTRANORDER\n \nFortran order. \n   enumeratorNPY_KEEPORDER\n \nAn order as close to the order of the inputs as possible, even if the input is in neither C nor Fortran order. \n \n"}, {"name": "enum NPY_SCALARKIND", "path": "reference/c-api/array#c.NPY_SCALARKIND", "type": "Array API", "text": "  enumNPY_SCALARKIND\n \nA special variable type indicating the number of \u201ckinds\u201d of scalars distinguished in determining scalar-coercion rules. This variable can take on the values:   enumeratorNPY_NOSCALAR\n\n   enumeratorNPY_BOOL_SCALAR\n\n   enumeratorNPY_INTPOS_SCALAR\n\n   enumeratorNPY_INTNEG_SCALAR\n\n   enumeratorNPY_FLOAT_SCALAR\n\n   enumeratorNPY_COMPLEX_SCALAR\n\n   enumeratorNPY_OBJECT_SCALAR\n\n   enumeratorNPY_NSCALARKINDS\n \nDefined to be the number of scalar kinds (not including NPY_NOSCALAR). \n \n"}, {"name": "enum NPY_SEARCHSIDE", "path": "reference/c-api/array#c.NPY_SEARCHSIDE", "type": "Array API", "text": "  enumNPY_SEARCHSIDE\n \nA variable type indicating whether the index returned should be that of the first suitable location (if NPY_SEARCHLEFT) or of the last (if NPY_SEARCHRIGHT).   enumeratorNPY_SEARCHLEFT\n\n   enumeratorNPY_SEARCHRIGHT\n\n \n"}, {"name": "enum NPY_SELECTKIND", "path": "reference/c-api/array#c.NPY_SELECTKIND", "type": "Array API", "text": "  enumNPY_SELECTKIND\n \nA variable type indicating the selection algorithm being used.   enumeratorNPY_INTROSELECT\n\n \n"}, {"name": "enumerator NPY_BOOL", "path": "reference/c-api/dtype#c.NPY_BOOL", "type": "Data Type API", "text": "  enumeratorNPY_BOOL\n \nThe enumeration value for the boolean type, stored as one byte. It may only be set to the values 0 and 1. \n"}, {"name": "enumerator NPY_BOOL_SCALAR", "path": "reference/c-api/array#c.NPY_SCALARKIND.NPY_BOOL_SCALAR", "type": "Array API", "text": "  enumeratorNPY_BOOL_SCALAR\n\n"}, {"name": "enumerator NPY_BYTE", "path": "reference/c-api/dtype#c.NPY_BYTE", "type": "Data Type API", "text": "  enumeratorNPY_BYTE\n\n"}, {"name": "enumerator NPY_CDOUBLE", "path": "reference/c-api/dtype#c.NPY_CDOUBLE", "type": "Data Type API", "text": "  enumeratorNPY_CDOUBLE\n\n"}, {"name": "enumerator NPY_CFLOAT", "path": "reference/c-api/dtype#c.NPY_CFLOAT", "type": "Data Type API", "text": "  enumeratorNPY_CFLOAT\n\n"}, {"name": "enumerator NPY_CLIP", "path": "reference/c-api/array#c.NPY_CLIPMODE.NPY_CLIP", "type": "Array API", "text": "  enumeratorNPY_CLIP\n \nClips an index to the valid range if it is out of bounds. \n"}, {"name": "enumerator NPY_CLONGDOUBLE", "path": "reference/c-api/dtype#c.NPY_CLONGDOUBLE", "type": "Data Type API", "text": "  enumeratorNPY_CLONGDOUBLE\n \nThe enumeration value for a platform-specific complex floating point type which is made up of two NPY_LONGDOUBLE values. \n"}, {"name": "enumerator NPY_COMPLEX128", "path": "reference/c-api/dtype#c.NPY_COMPLEX128", "type": "Data Type API", "text": "  enumeratorNPY_COMPLEX128\n \nThe enumeration value for a 128-bit/16-byte complex type made up of two NPY_DOUBLE values. \n"}, {"name": "enumerator NPY_COMPLEX64", "path": "reference/c-api/dtype#c.NPY_COMPLEX64", "type": "Data Type API", "text": "  enumeratorNPY_COMPLEX64\n \nThe enumeration value for a 64-bit/8-byte complex type made up of two NPY_FLOAT values. \n"}, {"name": "enumerator NPY_COMPLEX_SCALAR", "path": "reference/c-api/array#c.NPY_SCALARKIND.NPY_COMPLEX_SCALAR", "type": "Array API", "text": "  enumeratorNPY_COMPLEX_SCALAR\n\n"}, {"name": "enumerator NPY_CORDER", "path": "reference/c-api/array#c.NPY_ORDER.NPY_CORDER", "type": "Array API", "text": "  enumeratorNPY_CORDER\n \nC order. \n"}, {"name": "enumerator NPY_DATETIME", "path": "reference/c-api/dtype#c.NPY_DATETIME", "type": "Data Type API", "text": "  enumeratorNPY_DATETIME\n \nThe enumeration value for a data type which holds dates or datetimes with a precision based on selectable date or time units. \n"}, {"name": "enumerator NPY_DEFAULT_TYPE", "path": "reference/c-api/dtype#c.NPY_DEFAULT_TYPE", "type": "Data Type API", "text": "  enumeratorNPY_DEFAULT_TYPE\n \nThe default type to use when no dtype is explicitly specified, for example when calling np.zero(shape). This is equivalent to NPY_DOUBLE. \n"}, {"name": "enumerator NPY_DOUBLE", "path": "reference/c-api/dtype#c.NPY_DOUBLE", "type": "Data Type API", "text": "  enumeratorNPY_DOUBLE\n\n"}, {"name": "enumerator NPY_EQUIV_CASTING", "path": "reference/c-api/array#c.NPY_CASTING.NPY_EQUIV_CASTING", "type": "Array API", "text": "  enumeratorNPY_EQUIV_CASTING\n \nAllow identical and casts involving byte swapping. \n"}, {"name": "enumerator NPY_FLOAT", "path": "reference/c-api/dtype#c.NPY_FLOAT", "type": "Data Type API", "text": "  enumeratorNPY_FLOAT\n\n"}, {"name": "enumerator NPY_FLOAT16", "path": "reference/c-api/dtype#c.NPY_FLOAT16", "type": "Data Type API", "text": "  enumeratorNPY_FLOAT16\n \nThe enumeration value for a 16-bit/2-byte IEEE 754-2008 compatible floating point type. \n"}, {"name": "enumerator NPY_FLOAT32", "path": "reference/c-api/dtype#c.NPY_FLOAT32", "type": "Data Type API", "text": "  enumeratorNPY_FLOAT32\n \nThe enumeration value for a 32-bit/4-byte IEEE 754 compatible floating point type. \n"}, {"name": "enumerator NPY_FLOAT64", "path": "reference/c-api/dtype#c.NPY_FLOAT64", "type": "Data Type API", "text": "  enumeratorNPY_FLOAT64\n \nThe enumeration value for a 64-bit/8-byte IEEE 754 compatible floating point type. \n"}, {"name": "enumerator NPY_FLOAT_SCALAR", "path": "reference/c-api/array#c.NPY_SCALARKIND.NPY_FLOAT_SCALAR", "type": "Array API", "text": "  enumeratorNPY_FLOAT_SCALAR\n\n"}, {"name": "enumerator NPY_FORTRANORDER", "path": "reference/c-api/array#c.NPY_ORDER.NPY_FORTRANORDER", "type": "Array API", "text": "  enumeratorNPY_FORTRANORDER\n \nFortran order. \n"}, {"name": "enumerator NPY_HALF", "path": "reference/c-api/dtype#c.NPY_HALF", "type": "Data Type API", "text": "  enumeratorNPY_HALF\n\n"}, {"name": "enumerator NPY_HEAPSORT", "path": "reference/c-api/array#c.NPY_SORTKIND.NPY_HEAPSORT", "type": "Array API", "text": "  enumeratorNPY_HEAPSORT\n\n"}, {"name": "enumerator NPY_INT", "path": "reference/c-api/dtype#c.NPY_INT", "type": "Data Type API", "text": "  enumeratorNPY_INT\n\n"}, {"name": "enumerator NPY_INT16", "path": "reference/c-api/dtype#c.NPY_INT16", "type": "Data Type API", "text": "  enumeratorNPY_INT16\n \nThe enumeration value for a 16-bit/2-byte signed integer. \n"}, {"name": "enumerator NPY_INT32", "path": "reference/c-api/dtype#c.NPY_INT32", "type": "Data Type API", "text": "  enumeratorNPY_INT32\n \nThe enumeration value for a 32-bit/4-byte signed integer. \n"}, {"name": "enumerator NPY_INT64", "path": "reference/c-api/dtype#c.NPY_INT64", "type": "Data Type API", "text": "  enumeratorNPY_INT64\n \nThe enumeration value for a 64-bit/8-byte signed integer. \n"}, {"name": "enumerator NPY_INT8", "path": "reference/c-api/dtype#c.NPY_INT8", "type": "Data Type API", "text": "  enumeratorNPY_INT8\n \nThe enumeration value for an 8-bit/1-byte signed integer. \n"}, {"name": "enumerator NPY_INTNEG_SCALAR", "path": "reference/c-api/array#c.NPY_SCALARKIND.NPY_INTNEG_SCALAR", "type": "Array API", "text": "  enumeratorNPY_INTNEG_SCALAR\n\n"}, {"name": "enumerator NPY_INTP", "path": "reference/c-api/dtype#c.NPY_INTP", "type": "Data Type API", "text": "  enumeratorNPY_INTP\n \nThe enumeration value for a signed integer type which is the same size as a (void *) pointer. This is the type used by all arrays of indices. \n"}, {"name": "enumerator NPY_INTPOS_SCALAR", "path": "reference/c-api/array#c.NPY_SCALARKIND.NPY_INTPOS_SCALAR", "type": "Array API", "text": "  enumeratorNPY_INTPOS_SCALAR\n\n"}, {"name": "enumerator NPY_KEEPORDER", "path": "reference/c-api/array#c.NPY_ORDER.NPY_KEEPORDER", "type": "Array API", "text": "  enumeratorNPY_KEEPORDER\n \nAn order as close to the order of the inputs as possible, even if the input is in neither C nor Fortran order. \n"}, {"name": "enumerator NPY_LONG", "path": "reference/c-api/dtype#c.NPY_LONG", "type": "Data Type API", "text": "  enumeratorNPY_LONG\n \nEquivalent to either NPY_INT or NPY_LONGLONG, depending on the platform. \n"}, {"name": "enumerator NPY_LONGDOUBLE", "path": "reference/c-api/dtype#c.NPY_LONGDOUBLE", "type": "Data Type API", "text": "  enumeratorNPY_LONGDOUBLE\n \nThe enumeration value for a platform-specific floating point type which is at least as large as NPY_DOUBLE, but larger on many platforms. \n"}, {"name": "enumerator NPY_LONGLONG", "path": "reference/c-api/dtype#c.NPY_LONGLONG", "type": "Data Type API", "text": "  enumeratorNPY_LONGLONG\n\n"}, {"name": "enumerator NPY_MASK", "path": "reference/c-api/dtype#c.NPY_MASK", "type": "Data Type API", "text": "  enumeratorNPY_MASK\n \nThe enumeration value of the type used for masks, such as with the NPY_ITER_ARRAYMASK iterator flag. This is equivalent to NPY_UINT8. \n"}, {"name": "enumerator NPY_MERGESORT", "path": "reference/c-api/array#c.NPY_SORTKIND.NPY_MERGESORT", "type": "Array API", "text": "  enumeratorNPY_MERGESORT\n\n"}, {"name": "enumerator NPY_NSCALARKINDS", "path": "reference/c-api/array#c.NPY_SCALARKIND.NPY_NSCALARKINDS", "type": "Array API", "text": "  enumeratorNPY_NSCALARKINDS\n \nDefined to be the number of scalar kinds (not including NPY_NOSCALAR). \n"}, {"name": "enumerator NPY_NSORTS", "path": "reference/c-api/array#c.NPY_SORTKIND.NPY_NSORTS", "type": "Array API", "text": "  enumeratorNPY_NSORTS\n \nDefined to be the number of sorts. It is fixed at three by the need for backwards compatibility, and consequently NPY_MERGESORT and NPY_STABLESORT are aliased to each other and may refer to one of several stable sorting algorithms depending on the data type. \n"}, {"name": "enumerator NPY_OBJECT", "path": "reference/c-api/dtype#c.NPY_OBJECT", "type": "Data Type API", "text": "  enumeratorNPY_OBJECT\n \nThe enumeration value for references to arbitrary Python objects. \n"}, {"name": "enumerator NPY_OBJECT_SCALAR", "path": "reference/c-api/array#c.NPY_SCALARKIND.NPY_OBJECT_SCALAR", "type": "Array API", "text": "  enumeratorNPY_OBJECT_SCALAR\n\n"}, {"name": "enumerator NPY_SAFE_CASTING", "path": "reference/c-api/array#c.NPY_CASTING.NPY_SAFE_CASTING", "type": "Array API", "text": "  enumeratorNPY_SAFE_CASTING\n \nOnly allow casts which will not cause values to be rounded, truncated, or otherwise changed. \n"}, {"name": "enumerator NPY_SAME_KIND_CASTING", "path": "reference/c-api/array#c.NPY_CASTING.NPY_SAME_KIND_CASTING", "type": "Array API", "text": "  enumeratorNPY_SAME_KIND_CASTING\n \nAllow any safe casts, and casts between types of the same kind. For example, float64 -> float32 is permitted with this rule. \n"}, {"name": "enumerator NPY_SEARCHRIGHT", "path": "reference/c-api/array#c.NPY_SEARCHSIDE.NPY_SEARCHRIGHT", "type": "Array API", "text": "  enumeratorNPY_SEARCHRIGHT\n\n"}, {"name": "enumerator NPY_SHORT", "path": "reference/c-api/dtype#c.NPY_SHORT", "type": "Data Type API", "text": "  enumeratorNPY_SHORT\n\n"}, {"name": "enumerator NPY_STABLESORT", "path": "reference/c-api/array#c.NPY_SORTKIND.NPY_STABLESORT", "type": "Array API", "text": "  enumeratorNPY_STABLESORT\n \nUsed as an alias of NPY_MERGESORT and vica versa. \n"}, {"name": "enumerator NPY_STRING", "path": "reference/c-api/dtype#c.NPY_STRING", "type": "Data Type API", "text": "  enumeratorNPY_STRING\n \nThe enumeration value for ASCII strings of a selectable size. The strings have a fixed maximum size within a given array. \n"}, {"name": "enumerator NPY_TIMEDELTA", "path": "reference/c-api/dtype#c.NPY_TIMEDELTA", "type": "Data Type API", "text": "  enumeratorNPY_TIMEDELTA\n \nThe enumeration value for a data type which holds lengths of times in integers of selectable date or time units. \n"}, {"name": "enumerator NPY_TYPES", "path": "reference/c-api/dtype", "type": "Data Type API", "text": "Data Type API The standard array can have 24 different data types (and has some support for adding your own types). These data types all have an enumerated type, an enumerated type-character, and a corresponding array scalar Python type object (placed in a hierarchy). There are also standard C typedefs to make it easier to manipulate elements of the given data type. For the numeric types, there are also bit-width equivalent C typedefs and named typenumbers that make it easier to select the precision desired.  Warning The names for the types in c code follows c naming conventions more closely. The Python names for these types follow Python conventions. Thus, NPY_FLOAT picks up a 32-bit float in C, but numpy.float_ in Python corresponds to a 64-bit double. The bit-width names can be used in both Python and C for clarity.   Enumerated Types   enumeratorNPY_TYPES\n\n There is a list of enumerated types defined providing the basic 24 data types plus some useful generic names. Whenever the code requires a type number, one of these enumerated types is requested. The types are all called NPY_{NAME}:   enumeratorNPY_BOOL\n \nThe enumeration value for the boolean type, stored as one byte. It may only be set to the values 0 and 1. \n   enumeratorNPY_BYTE\n\n   enumeratorNPY_INT8\n \nThe enumeration value for an 8-bit/1-byte signed integer. \n   enumeratorNPY_SHORT\n\n   enumeratorNPY_INT16\n \nThe enumeration value for a 16-bit/2-byte signed integer. \n   enumeratorNPY_INT\n\n   enumeratorNPY_INT32\n \nThe enumeration value for a 32-bit/4-byte signed integer. \n   enumeratorNPY_LONG\n \nEquivalent to either NPY_INT or NPY_LONGLONG, depending on the platform. \n   enumeratorNPY_LONGLONG\n\n   enumeratorNPY_INT64\n \nThe enumeration value for a 64-bit/8-byte signed integer. \n   enumeratorNPY_UBYTE\n\n   enumeratorNPY_UINT8\n \nThe enumeration value for an 8-bit/1-byte unsigned integer. \n   enumeratorNPY_USHORT\n\n   enumeratorNPY_UINT16\n \nThe enumeration value for a 16-bit/2-byte unsigned integer. \n   enumeratorNPY_UINT\n\n   enumeratorNPY_UINT32\n \nThe enumeration value for a 32-bit/4-byte unsigned integer. \n   enumeratorNPY_ULONG\n \nEquivalent to either NPY_UINT or NPY_ULONGLONG, depending on the platform. \n   enumeratorNPY_ULONGLONG\n\n   enumeratorNPY_UINT64\n \nThe enumeration value for a 64-bit/8-byte unsigned integer. \n   enumeratorNPY_HALF\n\n   enumeratorNPY_FLOAT16\n \nThe enumeration value for a 16-bit/2-byte IEEE 754-2008 compatible floating point type. \n   enumeratorNPY_FLOAT\n\n   enumeratorNPY_FLOAT32\n \nThe enumeration value for a 32-bit/4-byte IEEE 754 compatible floating point type. \n   enumeratorNPY_DOUBLE\n\n   enumeratorNPY_FLOAT64\n \nThe enumeration value for a 64-bit/8-byte IEEE 754 compatible floating point type. \n   enumeratorNPY_LONGDOUBLE\n \nThe enumeration value for a platform-specific floating point type which is at least as large as NPY_DOUBLE, but larger on many platforms. \n   enumeratorNPY_CFLOAT\n\n   enumeratorNPY_COMPLEX64\n \nThe enumeration value for a 64-bit/8-byte complex type made up of two NPY_FLOAT values. \n   enumeratorNPY_CDOUBLE\n\n   enumeratorNPY_COMPLEX128\n \nThe enumeration value for a 128-bit/16-byte complex type made up of two NPY_DOUBLE values. \n   enumeratorNPY_CLONGDOUBLE\n \nThe enumeration value for a platform-specific complex floating point type which is made up of two NPY_LONGDOUBLE values. \n   enumeratorNPY_DATETIME\n \nThe enumeration value for a data type which holds dates or datetimes with a precision based on selectable date or time units. \n   enumeratorNPY_TIMEDELTA\n \nThe enumeration value for a data type which holds lengths of times in integers of selectable date or time units. \n   enumeratorNPY_STRING\n \nThe enumeration value for ASCII strings of a selectable size. The strings have a fixed maximum size within a given array. \n   enumeratorNPY_UNICODE\n \nThe enumeration value for UCS4 strings of a selectable size. The strings have a fixed maximum size within a given array. \n   enumeratorNPY_OBJECT\n \nThe enumeration value for references to arbitrary Python objects. \n   enumeratorNPY_VOID\n \nPrimarily used to hold struct dtypes, but can contain arbitrary binary data. \n Some useful aliases of the above types are   enumeratorNPY_INTP\n \nThe enumeration value for a signed integer type which is the same size as a (void *) pointer. This is the type used by all arrays of indices. \n   enumeratorNPY_UINTP\n \nThe enumeration value for an unsigned integer type which is the same size as a (void *) pointer. \n   enumeratorNPY_MASK\n \nThe enumeration value of the type used for masks, such as with the NPY_ITER_ARRAYMASK iterator flag. This is equivalent to NPY_UINT8. \n   enumeratorNPY_DEFAULT_TYPE\n \nThe default type to use when no dtype is explicitly specified, for example when calling np.zero(shape). This is equivalent to NPY_DOUBLE. \n Other useful related constants are   NPY_NTYPES\n \nThe total number of built-in NumPy types. The enumeration covers the range from 0 to NPY_NTYPES-1. \n   NPY_NOTYPE\n \nA signal value guaranteed not to be a valid type enumeration number. \n   NPY_USERDEF\n \nThe start of type numbers used for Custom Data types. \n The various character codes indicating certain types are also part of an enumerated list. References to type characters (should they be needed at all) should always use these enumerations. The form of them is NPY_{NAME}LTR where {NAME} can be BOOL, BYTE, UBYTE, SHORT, USHORT, INT, UINT, LONG, ULONG, LONGLONG, ULONGLONG, HALF, FLOAT, DOUBLE, LONGDOUBLE, CFLOAT, CDOUBLE, CLONGDOUBLE, DATETIME, TIMEDELTA, OBJECT, STRING, VOID INTP, UINTP GENBOOL, SIGNED, UNSIGNED, FLOATING, COMPLEX The latter group of {NAME}s corresponds to letters used in the array interface typestring specification.   Defines  Max and min values for integers  \nNPY_MAX_INT{bits}, NPY_MAX_UINT{bits}, NPY_MIN_INT{bits}\n\n\nThese are defined for {bits} = 8, 16, 32, 64, 128, and 256 and provide the maximum (minimum) value of the corresponding (unsigned) integer type. Note: the actual integer type may not be available on all platforms (i.e. 128-bit and 256-bit integers are rare).  NPY_MIN_{type}\n\nThis is defined for {type} = BYTE, SHORT, INT, LONG, LONGLONG, INTP  NPY_MAX_{type}\n\nThis is defined for all defined for {type} = BYTE, UBYTE, SHORT, USHORT, INT, UINT, LONG, ULONG, LONGLONG, ULONGLONG, INTP, UINTP     Number of bits in data types All NPY_SIZEOF_{CTYPE} constants have corresponding NPY_BITSOF_{CTYPE} constants defined. The NPY_BITSOF_{CTYPE} constants provide the number of bits in the data type. Specifically, the available {CTYPE}s are BOOL, CHAR, SHORT, INT, LONG, LONGLONG, FLOAT, DOUBLE, LONGDOUBLE   Bit-width references to enumerated typenums All of the numeric data types (integer, floating point, and complex) have constants that are defined to be a specific enumerated type number. Exactly which enumerated type a bit-width type refers to is platform dependent. In particular, the constants available are PyArray_{NAME}{BITS} where {NAME} is INT, UINT, FLOAT, COMPLEX and {BITS} can be 8, 16, 32, 64, 80, 96, 128, 160, 192, 256, and 512. Obviously not all bit-widths are available on all platforms for all the kinds of numeric types. Commonly 8-, 16-, 32-, 64-bit integers; 32-, 64-bit floats; and 64-, 128-bit complex types are available.   Integer that can hold a pointer The constants NPY_INTP and NPY_UINTP refer to an enumerated integer type that is large enough to hold a pointer on the platform. Index arrays should always be converted to NPY_INTP , because the dimension of the array is of type npy_intp.    C-type names There are standard variable types for each of the numeric data types and the bool data type. Some of these are already available in the C-specification. You can create variables in extension code with these types.  Boolean   typenpy_bool\n \nunsigned char; The constants NPY_FALSE and NPY_TRUE are also defined. \n   (Un)Signed Integer Unsigned versions of the integers can be defined by pre-pending a \u2018u\u2019 to the front of the integer name.   typenpy_byte\n \nchar \n   typenpy_ubyte\n \nunsigned char \n   typenpy_short\n \nshort \n   typenpy_ushort\n \nunsigned short \n   typenpy_int\n \nint \n   typenpy_uint\n \nunsigned int \n   typenpy_int16\n \n16-bit integer \n   typenpy_uint16\n \n16-bit unsigned integer \n   typenpy_int32\n \n32-bit integer \n   typenpy_uint32\n \n32-bit unsigned integer \n   typenpy_int64\n \n64-bit integer \n   typenpy_uint64\n \n64-bit unsigned integer \n   typenpy_long\n \nlong int \n   typenpy_ulong\n \nunsigned long int \n   typenpy_longlong\n \nlong long int \n   typenpy_ulonglong\n \nunsigned long long int \n   typenpy_intp\n \nPy_intptr_t (an integer that is the size of a pointer on the platform). \n   typenpy_uintp\n \nunsigned Py_intptr_t (an integer that is the size of a pointer on the platform). \n   (Complex) Floating point   typenpy_half\n \n16-bit float \n   typenpy_float\n \n32-bit float \n   typenpy_cfloat\n \n32-bit complex float \n   typenpy_double\n \n64-bit double \n   typenpy_cdouble\n \n64-bit complex double \n   typenpy_longdouble\n \nlong double \n   typenpy_clongdouble\n \nlong complex double \n complex types are structures with .real and .imag members (in that order).   Bit-width names There are also typedefs for signed integers, unsigned integers, floating point, and complex floating point types of specific bit- widths. The available type names are npy_int{bits}, npy_uint{bits}, npy_float{bits}, and npy_complex{bits} where {bits} is the number of bits in the type and can be 8, 16, 32, 64, 128, and 256 for integer types; 16, 32 , 64, 80, 96, 128, and 256 for floating-point types; and 32, 64, 128, 160, 192, and 512 for complex-valued types. Which bit-widths are available is platform dependent. The bolded bit-widths are usually available on all platforms.    Printf Formatting For help in printing, the following strings are defined as the correct format specifier in printf and related commands.   NPY_LONGLONG_FMT\n\n   NPY_ULONGLONG_FMT\n\n   NPY_INTP_FMT\n\n   NPY_UINTP_FMT\n\n   NPY_LONGDOUBLE_FMT\n\n \n"}, {"name": "enumerator NPY_UBYTE", "path": "reference/c-api/dtype#c.NPY_UBYTE", "type": "Data Type API", "text": "  enumeratorNPY_UBYTE\n\n"}, {"name": "enumerator NPY_UINT", "path": "reference/c-api/dtype#c.NPY_UINT", "type": "Data Type API", "text": "  enumeratorNPY_UINT\n\n"}, {"name": "enumerator NPY_UINT16", "path": "reference/c-api/dtype#c.NPY_UINT16", "type": "Data Type API", "text": "  enumeratorNPY_UINT16\n \nThe enumeration value for a 16-bit/2-byte unsigned integer. \n"}, {"name": "enumerator NPY_UINT32", "path": "reference/c-api/dtype#c.NPY_UINT32", "type": "Data Type API", "text": "  enumeratorNPY_UINT32\n \nThe enumeration value for a 32-bit/4-byte unsigned integer. \n"}, {"name": "enumerator NPY_UINT64", "path": "reference/c-api/dtype#c.NPY_UINT64", "type": "Data Type API", "text": "  enumeratorNPY_UINT64\n \nThe enumeration value for a 64-bit/8-byte unsigned integer. \n"}, {"name": "enumerator NPY_UINT8", "path": "reference/c-api/dtype#c.NPY_UINT8", "type": "Data Type API", "text": "  enumeratorNPY_UINT8\n \nThe enumeration value for an 8-bit/1-byte unsigned integer. \n"}, {"name": "enumerator NPY_UINTP", "path": "reference/c-api/dtype#c.NPY_UINTP", "type": "Data Type API", "text": "  enumeratorNPY_UINTP\n \nThe enumeration value for an unsigned integer type which is the same size as a (void *) pointer. \n"}, {"name": "enumerator NPY_ULONG", "path": "reference/c-api/dtype#c.NPY_ULONG", "type": "Data Type API", "text": "  enumeratorNPY_ULONG\n \nEquivalent to either NPY_UINT or NPY_ULONGLONG, depending on the platform. \n"}, {"name": "enumerator NPY_ULONGLONG", "path": "reference/c-api/dtype#c.NPY_ULONGLONG", "type": "Data Type API", "text": "  enumeratorNPY_ULONGLONG\n\n"}, {"name": "enumerator NPY_UNICODE", "path": "reference/c-api/dtype#c.NPY_UNICODE", "type": "Data Type API", "text": "  enumeratorNPY_UNICODE\n \nThe enumeration value for UCS4 strings of a selectable size. The strings have a fixed maximum size within a given array. \n"}, {"name": "enumerator NPY_UNSAFE_CASTING", "path": "reference/c-api/array#c.NPY_CASTING.NPY_UNSAFE_CASTING", "type": "Array API", "text": "  enumeratorNPY_UNSAFE_CASTING\n \nAllow any cast, no matter what kind of data loss may occur. \n"}, {"name": "enumerator NPY_USHORT", "path": "reference/c-api/dtype#c.NPY_USHORT", "type": "Data Type API", "text": "  enumeratorNPY_USHORT\n\n"}, {"name": "enumerator NPY_VOID", "path": "reference/c-api/dtype#c.NPY_VOID", "type": "Data Type API", "text": "  enumeratorNPY_VOID\n \nPrimarily used to hold struct dtypes, but can contain arbitrary binary data. \n"}, {"name": "enumerator NPY_WRAP", "path": "reference/c-api/array#c.NPY_CLIPMODE.NPY_WRAP", "type": "Array API", "text": "  enumeratorNPY_WRAP\n \nWraps an index to the valid range if it is out of bounds. \n"}, {"name": "errstate.__call__()", "path": "reference/generated/numpy.errstate.__call__", "type": "numpy.errstate.__call__", "text": "numpy.errstate.__call__ method   errstate.__call__(func)\n \nCall self as a function. \n\n"}, {"name": "exec_command", "path": "reference/generated/numpy.distutils.exec_command", "type": "numpy.distutils.exec_command", "text": "numpy.distutils.exec_command exec_command Implements exec_command function that is (almost) equivalent to commands.getstatusoutput function but on NT, DOS systems the returned status is actually correct (though, the returned status values may be different by a factor). In addition, exec_command takes keyword arguments for (re-)defining environment variables. Provides functions:  exec_command \u2014 execute command in a specified directory and\n\nin the modified environment.  find_executable \u2014 locate a command using info from environment\n\nvariable PATH. Equivalent to posix which command.   Author: Pearu Peterson <pearu@cens.ioc.ee> Created: 11 January 2003 Requires: Python 2.x Successfully tested on:   \nos.name sys.platform comments   \nposix linux2 Debian (sid) Linux, Python 2.1.3+, 2.2.3+, 2.3.3 PyCrust 0.9.3, Idle 1.0.2  \nposix linux2 Red Hat 9 Linux, Python 2.1.3, 2.2.2, 2.3.2  \nposix sunos5 SunOS 5.9, Python 2.2, 2.3.2  \nposix darwin Darwin 7.2.0, Python 2.3  \nnt win32 Windows Me Python 2.3(EE), Idle 1.0, PyCrust 0.7.2 Python 2.1.1 Idle 0.8  \nnt win32 Windows 98, Python 2.1.1. Idle 0.8  \nnt win32 Cygwin 98-4.10, Python 2.1.1(MSC) - echo tests fail i.e. redefining environment variables may not work. FIXED: don\u2019t use cygwin echo! Comment: also cmd /c echo will not work but redefining environment variables do work.  \nposix cygwin Cygwin 98-4.10, Python 2.3.3(cygming special)  \nnt win32 Windows XP, Python 2.3.3   Known bugs:  Tests, that send messages to stderr, fail when executed from MSYS prompt because the messages are lost at some point.  Functions  \nexec_command(command[, execute_in, ...]) Return (status,output) of executed command.  \nfilepath_from_subprocess_output(output) Convert bytes in the encoding used by a subprocess into a filesystem-appropriate str.  \nfind_executable(exe[, path, _cache]) Return full path of a executable or None.  \nforward_bytes_to_stdout(val) Forward bytes from a subprocess call to the console, without attempting to decode them.  \nget_pythonexe()   \ntemp_file_name()   \n"}, {"name": "Extending", "path": "reference/random/extending", "type": "Examples of using Numba, Cython, CFFI", "text": "Extending The BitGenerators have been designed to be extendable using standard tools for high-performance Python \u2013 numba and Cython. The Generator object can also be used with user-provided BitGenerators as long as these export a small set of required functions.  Numba Numba can be used with either CTypes or CFFI. The current iteration of the BitGenerators all export a small set of functions through both interfaces. This example shows how numba can be used to produce gaussian samples using a pure Python implementation which is then compiled. The random numbers are provided by ctypes.next_double. import numpy as np\nimport numba as nb\n\nfrom numpy.random import PCG64\nfrom timeit import timeit\n\nbit_gen = PCG64()\nnext_d = bit_gen.cffi.next_double\nstate_addr = bit_gen.cffi.state_address\n\ndef normals(n, state):\n    out = np.empty(n)\n    for i in range((n + 1) // 2):\n        x1 = 2.0 * next_d(state) - 1.0\n        x2 = 2.0 * next_d(state) - 1.0\n        r2 = x1 * x1 + x2 * x2\n        while r2 >= 1.0 or r2 == 0.0:\n            x1 = 2.0 * next_d(state) - 1.0\n            x2 = 2.0 * next_d(state) - 1.0\n            r2 = x1 * x1 + x2 * x2\n        f = np.sqrt(-2.0 * np.log(r2) / r2)\n        out[2 * i] = f * x1\n        if 2 * i + 1 < n:\n            out[2 * i + 1] = f * x2\n    return out\n\n# Compile using Numba\nnormalsj = nb.jit(normals, nopython=True)\n# Must use state address not state with numba\nn = 10000\n\ndef numbacall():\n    return normalsj(n, state_addr)\n\nrg = np.random.Generator(PCG64())\n\ndef numpycall():\n    return rg.normal(size=n)\n\n# Check that the functions work\nr1 = numbacall()\nr2 = numpycall()\nassert r1.shape == (n,)\nassert r1.shape == r2.shape\n\nt1 = timeit(numbacall, number=1000)\nprint(f'{t1:.2f} secs for {n} PCG64 (Numba/PCG64) gaussian randoms')\nt2 = timeit(numpycall, number=1000)\nprint(f'{t2:.2f} secs for {n} PCG64 (NumPy/PCG64) gaussian randoms')\n\n Both CTypes and CFFI allow the more complicated distributions to be used directly in Numba after compiling the file distributions.c into a DLL or so. An example showing the use of a more complicated distribution is in the examples section below.   Cython Cython can be used to unpack the PyCapsule provided by a BitGenerator. This example uses PCG64 and the example from above. The usual caveats for writing high-performance code using Cython \u2013 removing bounds checks and wrap around, providing array alignment information \u2013 still apply. #!/usr/bin/env python3\n#cython: language_level=3\n\"\"\"\nThis file shows how the to use a BitGenerator to create a distribution.\n\"\"\"\nimport numpy as np\ncimport numpy as np\ncimport cython\nfrom cpython.pycapsule cimport PyCapsule_IsValid, PyCapsule_GetPointer\nfrom libc.stdint cimport uint16_t, uint64_t\nfrom numpy.random cimport bitgen_t\nfrom numpy.random import PCG64\nfrom numpy.random.c_distributions cimport (\n      random_standard_uniform_fill, random_standard_uniform_fill_f)\n\n\n@cython.boundscheck(False)\n@cython.wraparound(False)\ndef uniforms(Py_ssize_t n):\n    \"\"\"\n    Create an array of `n` uniformly distributed doubles.\n    A 'real' distribution would want to process the values into\n    some non-uniform distribution\n    \"\"\"\n    cdef Py_ssize_t i\n    cdef bitgen_t *rng\n    cdef const char *capsule_name = \"BitGenerator\"\n    cdef double[::1] random_values\n\n    x = PCG64()\n    capsule = x.capsule\n    # Optional check that the capsule if from a BitGenerator\n    if not PyCapsule_IsValid(capsule, capsule_name):\n        raise ValueError(\"Invalid pointer to anon_func_state\")\n    # Cast the pointer\n    rng = <bitgen_t *> PyCapsule_GetPointer(capsule, capsule_name)\n    random_values = np.empty(n, dtype='float64')\n    with x.lock, nogil:\n        for i in range(n):\n            # Call the function\n            random_values[i] = rng.next_double(rng.state)\n    randoms = np.asarray(random_values)\n\n    return randoms\n The BitGenerator can also be directly accessed using the members of the bitgen_t struct. @cython.boundscheck(False)\n@cython.wraparound(False)\ndef uint10_uniforms(Py_ssize_t n):\n    \"\"\"Uniform 10 bit integers stored as 16-bit unsigned integers\"\"\"\n    cdef Py_ssize_t i\n    cdef bitgen_t *rng\n    cdef const char *capsule_name = \"BitGenerator\"\n    cdef uint16_t[::1] random_values\n    cdef int bits_remaining\n    cdef int width = 10\n    cdef uint64_t buff, mask = 0x3FF\n\n    x = PCG64()\n    capsule = x.capsule\n    if not PyCapsule_IsValid(capsule, capsule_name):\n        raise ValueError(\"Invalid pointer to anon_func_state\")\n    rng = <bitgen_t *> PyCapsule_GetPointer(capsule, capsule_name)\n    random_values = np.empty(n, dtype='uint16')\n    # Best practice is to release GIL and acquire the lock\n    bits_remaining = 0\n    with x.lock, nogil:\n        for i in range(n):\n            if bits_remaining < width:\n                buff = rng.next_uint64(rng.state)\n            random_values[i] = buff & mask\n            buff >>= width\n\n    randoms = np.asarray(random_values)\n    return randoms\n Cython can be used to directly access the functions in numpy/random/c_distributions.pxd. This requires linking with the npyrandom library located in numpy/random/lib. def uniforms_ex(bit_generator, Py_ssize_t n, dtype=np.float64):\n    \"\"\"\n    Create an array of `n` uniformly distributed doubles via a \"fill\" function.\n\n    A 'real' distribution would want to process the values into\n    some non-uniform distribution\n\n    Parameters\n    ----------\n    bit_generator: BitGenerator instance\n    n: int\n        Output vector length\n    dtype: {str, dtype}, optional\n        Desired dtype, either 'd' (or 'float64') or 'f' (or 'float32'). The\n        default dtype value is 'd'\n    \"\"\"\n    cdef Py_ssize_t i\n    cdef bitgen_t *rng\n    cdef const char *capsule_name = \"BitGenerator\"\n    cdef np.ndarray randoms\n\n    capsule = bit_generator.capsule\n    # Optional check that the capsule if from a BitGenerator\n    if not PyCapsule_IsValid(capsule, capsule_name):\n        raise ValueError(\"Invalid pointer to anon_func_state\")\n    # Cast the pointer\n    rng = <bitgen_t *> PyCapsule_GetPointer(capsule, capsule_name)\n\n    _dtype = np.dtype(dtype)\n    randoms = np.empty(n, dtype=_dtype)\n    if _dtype == np.float32:\n        with bit_generator.lock:\n            random_standard_uniform_fill_f(rng, n, <float*>np.PyArray_DATA(randoms))\n    elif _dtype == np.float64:\n        with bit_generator.lock:\n            random_standard_uniform_fill(rng, n, <double*>np.PyArray_DATA(randoms))\n    else:\n        raise TypeError('Unsupported dtype %r for random' % _dtype)\n    return randoms\n See Extending numpy.random via Cython for the complete listings of these examples and a minimal setup.py to build the c-extension modules.   CFFI CFFI can be used to directly access the functions in include/numpy/random/distributions.h. Some \u201cmassaging\u201d of the header file is required: \"\"\"\nUse cffi to access any of the underlying C functions from distributions.h\n\"\"\"\nimport os\nimport numpy as np\nimport cffi\nfrom .parse import parse_distributions_h\nffi = cffi.FFI()\n\ninc_dir = os.path.join(np.get_include(), 'numpy')\n\n# Basic numpy types\nffi.cdef('''\n    typedef intptr_t npy_intp;\n    typedef unsigned char npy_bool;\n\n''')\n\nparse_distributions_h(ffi, inc_dir)\n\n Once the header is parsed by ffi.cdef, the functions can be accessed directly from the _generator shared object, using the BitGenerator.cffi interface. \n# Compare the distributions.h random_standard_normal_fill to\n# Generator.standard_random\nbit_gen = np.random.PCG64()\nrng = np.random.Generator(bit_gen)\nstate = bit_gen.state\n\ninterface = rng.bit_generator.cffi\nn = 100\nvals_cffi = ffi.new('double[%d]' % n)\nlib.random_standard_normal_fill(interface.bit_generator, n, vals_cffi)\n\n# reset the state\nbit_gen.state = state\n\nvals = rng.standard_normal(n)\n\nfor i in range(n):\n    assert vals[i] == vals_cffi[i]\n   New Bit Generators Generator can be used with user-provided BitGenerators. The simplest way to write a new BitGenerator is to examine the pyx file of one of the existing BitGenerators. The key structure that must be provided is the capsule which contains a PyCapsule to a struct pointer of type bitgen_t, typedef struct bitgen {\n  void *state;\n  uint64_t (*next_uint64)(void *st);\n  uint32_t (*next_uint32)(void *st);\n  double (*next_double)(void *st);\n  uint64_t (*next_raw)(void *st);\n} bitgen_t;\n which provides 5 pointers. The first is an opaque pointer to the data structure used by the BitGenerators. The next three are function pointers which return the next 64- and 32-bit unsigned integers, the next random double and the next raw value. This final function is used for testing and so can be set to the next 64-bit unsigned integer function if not needed. Functions inside Generator use this structure as in bitgen_state->next_uint64(bitgen_state->state)\n   Examples  Numba CFFI + Numba \nCython setup.py extending.pyx extending_distributions.pyx   CFFI  \n"}, {"name": "Extending numpy.random via Cython", "path": "reference/random/examples/cython/index", "type": "Cython", "text": "Extending numpy.random via Cython  setup.py extending.pyx extending_distributions.pyx \n"}, {"name": "Extending via CFFI", "path": "reference/random/examples/cffi", "type": "CFFI", "text": "Extending via CFFI \"\"\"\nUse cffi to access any of the underlying C functions from distributions.h\n\"\"\"\nimport os\nimport numpy as np\nimport cffi\nfrom .parse import parse_distributions_h\nffi = cffi.FFI()\n\ninc_dir = os.path.join(np.get_include(), 'numpy')\n\n# Basic numpy types\nffi.cdef('''\n    typedef intptr_t npy_intp;\n    typedef unsigned char npy_bool;\n\n''')\n\nparse_distributions_h(ffi, inc_dir)\n\nlib = ffi.dlopen(np.random._generator.__file__)\n\n# Compare the distributions.h random_standard_normal_fill to\n# Generator.standard_random\nbit_gen = np.random.PCG64()\nrng = np.random.Generator(bit_gen)\nstate = bit_gen.state\n\ninterface = rng.bit_generator.cffi\nn = 100\nvals_cffi = ffi.new('double[%d]' % n)\nlib.random_standard_normal_fill(interface.bit_generator, n, vals_cffi)\n\n# reset the state\nbit_gen.state = state\n\nvals = rng.standard_normal(n)\n\nfor i in range(n):\n    assert vals[i] == vals_cffi[i]\n\n"}, {"name": "Extending via Numba", "path": "reference/random/examples/numba", "type": "Numba", "text": "Extending via Numba import numpy as np\nimport numba as nb\n\nfrom numpy.random import PCG64\nfrom timeit import timeit\n\nbit_gen = PCG64()\nnext_d = bit_gen.cffi.next_double\nstate_addr = bit_gen.cffi.state_address\n\ndef normals(n, state):\n    out = np.empty(n)\n    for i in range((n + 1) // 2):\n        x1 = 2.0 * next_d(state) - 1.0\n        x2 = 2.0 * next_d(state) - 1.0\n        r2 = x1 * x1 + x2 * x2\n        while r2 >= 1.0 or r2 == 0.0:\n            x1 = 2.0 * next_d(state) - 1.0\n            x2 = 2.0 * next_d(state) - 1.0\n            r2 = x1 * x1 + x2 * x2\n        f = np.sqrt(-2.0 * np.log(r2) / r2)\n        out[2 * i] = f * x1\n        if 2 * i + 1 < n:\n            out[2 * i + 1] = f * x2\n    return out\n\n# Compile using Numba\nnormalsj = nb.jit(normals, nopython=True)\n# Must use state address not state with numba\nn = 10000\n\ndef numbacall():\n    return normalsj(n, state_addr)\n\nrg = np.random.Generator(PCG64())\n\ndef numpycall():\n    return rg.normal(size=n)\n\n# Check that the functions work\nr1 = numbacall()\nr2 = numpycall()\nassert r1.shape == (n,)\nassert r1.shape == r2.shape\n\nt1 = timeit(numbacall, number=1000)\nprint(f'{t1:.2f} secs for {n} PCG64 (Numba/PCG64) gaussian randoms')\nt2 = timeit(numpycall, number=1000)\nprint(f'{t2:.2f} secs for {n} PCG64 (NumPy/PCG64) gaussian randoms')\n\n# example 2\n\nnext_u32 = bit_gen.ctypes.next_uint32\nctypes_state = bit_gen.ctypes.state\n\n@nb.jit(nopython=True)\ndef bounded_uint(lb, ub, state):\n    mask = delta = ub - lb\n    mask |= mask >> 1\n    mask |= mask >> 2\n    mask |= mask >> 4\n    mask |= mask >> 8\n    mask |= mask >> 16\n\n    val = next_u32(state) & mask\n    while val > delta:\n        val = next_u32(state) & mask\n\n    return lb + val\n\n\nprint(bounded_uint(323, 2394691, ctypes_state.value))\n\n\n@nb.jit(nopython=True)\ndef bounded_uints(lb, ub, n, state):\n    out = np.empty(n, dtype=np.uint32)\n    for i in range(n):\n        out[i] = bounded_uint(lb, ub, state)\n\n\nbounded_uints(323, 2394691, 10000000, ctypes_state.value)\n\n\n\n"}, {"name": "Extending via Numba and CFFI", "path": "reference/random/examples/numba_cffi", "type": "CFFI + Numba", "text": "Extending via Numba and CFFI r\"\"\"\nBuilding the required library in this example requires a source distribution\nof NumPy or clone of the NumPy git repository since distributions.c is not\nincluded in binary distributions.\n\nOn *nix, execute in numpy/random/src/distributions\n\nexport ${PYTHON_VERSION}=3.8 # Python version\nexport PYTHON_INCLUDE=#path to Python's include folder, usually \\\n    ${PYTHON_HOME}/include/python${PYTHON_VERSION}m\nexport NUMPY_INCLUDE=#path to numpy's include folder, usually \\\n    ${PYTHON_HOME}/lib/python${PYTHON_VERSION}/site-packages/numpy/core/include\ngcc -shared -o libdistributions.so -fPIC distributions.c \\\n    -I${NUMPY_INCLUDE} -I${PYTHON_INCLUDE}\nmv libdistributions.so ../../_examples/numba/\n\nOn Windows\n\nrem PYTHON_HOME and PYTHON_VERSION are setup dependent, this is an example\nset PYTHON_HOME=c:\\Anaconda\nset PYTHON_VERSION=38\ncl.exe /LD .\\distributions.c -DDLL_EXPORT \\\n    -I%PYTHON_HOME%\\lib\\site-packages\\numpy\\core\\include \\\n    -I%PYTHON_HOME%\\include %PYTHON_HOME%\\libs\\python%PYTHON_VERSION%.lib\nmove distributions.dll ../../_examples/numba/\n\"\"\"\nimport os\n\nimport numba as nb\nimport numpy as np\nfrom cffi import FFI\n\nfrom numpy.random import PCG64\n\nffi = FFI()\nif os.path.exists('./distributions.dll'):\n    lib = ffi.dlopen('./distributions.dll')\nelif os.path.exists('./libdistributions.so'):\n    lib = ffi.dlopen('./libdistributions.so')\nelse:\n    raise RuntimeError('Required DLL/so file was not found.')\n\nffi.cdef(\"\"\"\ndouble random_standard_normal(void *bitgen_state);\n\"\"\")\nx = PCG64()\nxffi = x.cffi\nbit_generator = xffi.bit_generator\n\nrandom_standard_normal = lib.random_standard_normal\n\n\ndef normals(n, bit_generator):\n    out = np.empty(n)\n    for i in range(n):\n        out[i] = random_standard_normal(bit_generator)\n    return out\n\n\nnormalsj = nb.jit(normals, nopython=True)\n\n# Numba requires a memory address for void *\n# Can also get address from x.ctypes.bit_generator.value\nbit_generator_address = int(ffi.cast('uintptr_t', bit_generator))\n\nnorm = normalsj(1000, bit_generator_address)\nprint(norm[:12])\n\n"}, {"name": "extending.pyx", "path": "reference/random/examples/cython/extending.pyx", "type": "Cython", "text": "extending.pyx #!/usr/bin/env python3\n#cython: language_level=3\n\nfrom libc.stdint cimport uint32_t\nfrom cpython.pycapsule cimport PyCapsule_IsValid, PyCapsule_GetPointer\n\nimport numpy as np\ncimport numpy as np\ncimport cython\n\nfrom numpy.random cimport bitgen_t\nfrom numpy.random import PCG64\n\nnp.import_array()\n\n\n@cython.boundscheck(False)\n@cython.wraparound(False)\ndef uniform_mean(Py_ssize_t n):\n    cdef Py_ssize_t i\n    cdef bitgen_t *rng\n    cdef const char *capsule_name = \"BitGenerator\"\n    cdef double[::1] random_values\n    cdef np.ndarray randoms\n\n    x = PCG64()\n    capsule = x.capsule\n    if not PyCapsule_IsValid(capsule, capsule_name):\n        raise ValueError(\"Invalid pointer to anon_func_state\")\n    rng = <bitgen_t *> PyCapsule_GetPointer(capsule, capsule_name)\n    random_values = np.empty(n)\n    # Best practice is to acquire the lock whenever generating random values.\n    # This prevents other threads from modifying the state. Acquiring the lock\n    # is only necessary if if the GIL is also released, as in this example.\n    with x.lock, nogil:\n        for i in range(n):\n            random_values[i] = rng.next_double(rng.state)\n    randoms = np.asarray(random_values)\n    return randoms.mean()\n\n\n# This function is declared nogil so it can be used without the GIL below\ncdef uint32_t bounded_uint(uint32_t lb, uint32_t ub, bitgen_t *rng) nogil:\n    cdef uint32_t mask, delta, val\n    mask = delta = ub - lb\n    mask |= mask >> 1\n    mask |= mask >> 2\n    mask |= mask >> 4\n    mask |= mask >> 8\n    mask |= mask >> 16\n\n    val = rng.next_uint32(rng.state) & mask\n    while val > delta:\n        val = rng.next_uint32(rng.state) & mask\n\n    return lb + val\n\n\n@cython.boundscheck(False)\n@cython.wraparound(False)\ndef bounded_uints(uint32_t lb, uint32_t ub, Py_ssize_t n):\n    cdef Py_ssize_t i\n    cdef bitgen_t *rng\n    cdef uint32_t[::1] out\n    cdef const char *capsule_name = \"BitGenerator\"\n\n    x = PCG64()\n    out = np.empty(n, dtype=np.uint32)\n    capsule = x.capsule\n\n    if not PyCapsule_IsValid(capsule, capsule_name):\n        raise ValueError(\"Invalid pointer to anon_func_state\")\n    rng = <bitgen_t *>PyCapsule_GetPointer(capsule, capsule_name)\n\n    with x.lock, nogil:\n        for i in range(n):\n            out[i] = bounded_uint(lb, ub, rng)\n    return np.asarray(out)\n\n"}, {"name": "extending_distributions.pyx", "path": "reference/random/examples/cython/extending_distributions.pyx", "type": "Cython", "text": "extending_distributions.pyx #!/usr/bin/env python3\n#cython: language_level=3\n\"\"\"\nThis file shows how the to use a BitGenerator to create a distribution.\n\"\"\"\nimport numpy as np\ncimport numpy as np\ncimport cython\nfrom cpython.pycapsule cimport PyCapsule_IsValid, PyCapsule_GetPointer\nfrom libc.stdint cimport uint16_t, uint64_t\nfrom numpy.random cimport bitgen_t\nfrom numpy.random import PCG64\nfrom numpy.random.c_distributions cimport (\n      random_standard_uniform_fill, random_standard_uniform_fill_f)\n\n\n@cython.boundscheck(False)\n@cython.wraparound(False)\ndef uniforms(Py_ssize_t n):\n    \"\"\"\n    Create an array of `n` uniformly distributed doubles.\n    A 'real' distribution would want to process the values into\n    some non-uniform distribution\n    \"\"\"\n    cdef Py_ssize_t i\n    cdef bitgen_t *rng\n    cdef const char *capsule_name = \"BitGenerator\"\n    cdef double[::1] random_values\n\n    x = PCG64()\n    capsule = x.capsule\n    # Optional check that the capsule if from a BitGenerator\n    if not PyCapsule_IsValid(capsule, capsule_name):\n        raise ValueError(\"Invalid pointer to anon_func_state\")\n    # Cast the pointer\n    rng = <bitgen_t *> PyCapsule_GetPointer(capsule, capsule_name)\n    random_values = np.empty(n, dtype='float64')\n    with x.lock, nogil:\n        for i in range(n):\n            # Call the function\n            random_values[i] = rng.next_double(rng.state)\n    randoms = np.asarray(random_values)\n\n    return randoms\n\n# cython example 2\n@cython.boundscheck(False)\n@cython.wraparound(False)\ndef uint10_uniforms(Py_ssize_t n):\n    \"\"\"Uniform 10 bit integers stored as 16-bit unsigned integers\"\"\"\n    cdef Py_ssize_t i\n    cdef bitgen_t *rng\n    cdef const char *capsule_name = \"BitGenerator\"\n    cdef uint16_t[::1] random_values\n    cdef int bits_remaining\n    cdef int width = 10\n    cdef uint64_t buff, mask = 0x3FF\n\n    x = PCG64()\n    capsule = x.capsule\n    if not PyCapsule_IsValid(capsule, capsule_name):\n        raise ValueError(\"Invalid pointer to anon_func_state\")\n    rng = <bitgen_t *> PyCapsule_GetPointer(capsule, capsule_name)\n    random_values = np.empty(n, dtype='uint16')\n    # Best practice is to release GIL and acquire the lock\n    bits_remaining = 0\n    with x.lock, nogil:\n        for i in range(n):\n            if bits_remaining < width:\n                buff = rng.next_uint64(rng.state)\n            random_values[i] = buff & mask\n            buff >>= width\n\n    randoms = np.asarray(random_values)\n    return randoms\n\n# cython example 3\ndef uniforms_ex(bit_generator, Py_ssize_t n, dtype=np.float64):\n    \"\"\"\n    Create an array of `n` uniformly distributed doubles via a \"fill\" function.\n\n    A 'real' distribution would want to process the values into\n    some non-uniform distribution\n\n    Parameters\n    ----------\n    bit_generator: BitGenerator instance\n    n: int\n        Output vector length\n    dtype: {str, dtype}, optional\n        Desired dtype, either 'd' (or 'float64') or 'f' (or 'float32'). The\n        default dtype value is 'd'\n    \"\"\"\n    cdef Py_ssize_t i\n    cdef bitgen_t *rng\n    cdef const char *capsule_name = \"BitGenerator\"\n    cdef np.ndarray randoms\n\n    capsule = bit_generator.capsule\n    # Optional check that the capsule if from a BitGenerator\n    if not PyCapsule_IsValid(capsule, capsule_name):\n        raise ValueError(\"Invalid pointer to anon_func_state\")\n    # Cast the pointer\n    rng = <bitgen_t *> PyCapsule_GetPointer(capsule, capsule_name)\n\n    _dtype = np.dtype(dtype)\n    randoms = np.empty(n, dtype=_dtype)\n    if _dtype == np.float32:\n        with bit_generator.lock:\n            random_standard_uniform_fill_f(rng, n, <float*>np.PyArray_DATA(randoms))\n    elif _dtype == np.float64:\n        with bit_generator.lock:\n            random_standard_uniform_fill(rng, n, <double*>np.PyArray_DATA(randoms))\n    else:\n        raise TypeError('Unsupported dtype %r for random' % _dtype)\n    return randoms\n\n"}, {"name": "F2PY user guide and reference manual", "path": "f2py/index", "type": "F2PY user guide and reference manual", "text": "F2PY user guide and reference manual The purpose of the F2PY \u2013Fortran to Python interface generator\u2013 utility is to provide a connection between Python and Fortran languages. F2PY is a part of NumPy (numpy.f2py) and also available as a standalone command line tool f2py when numpy is installed that facilitates creating/building Python C/API extension modules that make it possible  to call Fortran 77/90/95 external subroutines and Fortran 90/95 module subroutines as well as C functions; to access Fortran 77 COMMON blocks and Fortran 90/95 module data, including allocatable arrays  from Python.  \nUsing F2PY Command f2py Python module numpy.f2py   \nThree ways to wrap - getting started The quick way The smart way The quick and smart way   \nUsing F2PY bindings in Python Scalar arguments String arguments Array arguments Call-back arguments Resolving arguments to call-back functions Common blocks Fortran 90 module data Allocatable arrays   \nSignature file Python module block Fortran/C routine signatures Type declarations Statements Attributes Extensions   \nF2PY and Build Systems Basic Concepts Build Systems   \nAdvanced F2PY use cases Adding user-defined functions to F2PY generated modules Adding user-defined variables Dealing with KIND specifiers   \n"}, {"name": "fft.fft()", "path": "reference/generated/numpy.fft.fft", "type": "numpy.fft.fft", "text": "numpy.fft.fft   fft.fft(a, n=None, axis=- 1, norm=None)[source]\n \nCompute the one-dimensional discrete Fourier Transform. This function computes the one-dimensional n-point discrete Fourier Transform (DFT) with the efficient Fast Fourier Transform (FFT) algorithm [CT].  Parameters \n \naarray_like\n\n\nInput array, can be complex.  \nnint, optional\n\n\nLength of the transformed axis of the output. If n is smaller than the length of the input, the input is cropped. If it is larger, the input is padded with zeros. If n is not given, the length of the input along the axis specified by axis is used.  \naxisint, optional\n\n\nAxis over which to compute the FFT. If not given, the last axis is used.  \nnorm{\u201cbackward\u201d, \u201cortho\u201d, \u201cforward\u201d}, optional\n\n\n New in version 1.10.0.  Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.  New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.     Returns \n \noutcomplex ndarray\n\n\nThe truncated or zero-padded input, transformed along the axis indicated by axis, or the last one if axis is not specified.    Raises \n IndexError\n\nIf axis is not a valid axis of a.      See also  numpy.fft\n\nfor definition of the DFT and conventions used.  ifft\n\nThe inverse of fft.  fft2\n\nThe two-dimensional FFT.  fftn\n\nThe n-dimensional FFT.  rfftn\n\nThe n-dimensional FFT of real input.  fftfreq\n\nFrequency bins for given FFT parameters.    Notes FFT (Fast Fourier Transform) refers to a way the discrete Fourier Transform (DFT) can be calculated efficiently, by using symmetries in the calculated terms. The symmetry is highest when n is a power of 2, and the transform is therefore most efficient for these sizes. The DFT is defined, with the conventions used in this implementation, in the documentation for the numpy.fft module. References  CT \nCooley, James W., and John W. Tukey, 1965, \u201cAn algorithm for the machine calculation of complex Fourier series,\u201d Math. Comput. 19: 297-301.   Examples >>> np.fft.fft(np.exp(2j * np.pi * np.arange(8) / 8))\narray([-2.33486982e-16+1.14423775e-17j,  8.00000000e+00-1.25557246e-15j,\n        2.33486982e-16+2.33486982e-16j,  0.00000000e+00+1.22464680e-16j,\n       -1.14423775e-17+2.33486982e-16j,  0.00000000e+00+5.20784380e-16j,\n        1.14423775e-17+1.14423775e-17j,  0.00000000e+00+1.22464680e-16j])\n In this example, real input has an FFT which is Hermitian, i.e., symmetric in the real part and anti-symmetric in the imaginary part, as described in the numpy.fft documentation: >>> import matplotlib.pyplot as plt\n>>> t = np.arange(256)\n>>> sp = np.fft.fft(np.sin(t))\n>>> freq = np.fft.fftfreq(t.shape[-1])\n>>> plt.plot(freq, sp.real, freq, sp.imag)\n[<matplotlib.lines.Line2D object at 0x...>, <matplotlib.lines.Line2D object at 0x...>]\n>>> plt.show()\n    \n\n"}, {"name": "fft.fft2()", "path": "reference/generated/numpy.fft.fft2", "type": "numpy.fft.fft2", "text": "numpy.fft.fft2   fft.fft2(a, s=None, axes=(- 2, - 1), norm=None)[source]\n \nCompute the 2-dimensional discrete Fourier Transform. This function computes the n-dimensional discrete Fourier Transform over any axes in an M-dimensional array by means of the Fast Fourier Transform (FFT). By default, the transform is computed over the last two axes of the input array, i.e., a 2-dimensional FFT.  Parameters \n \naarray_like\n\n\nInput array, can be complex  \nssequence of ints, optional\n\n\nShape (length of each transformed axis) of the output (s[0] refers to axis 0, s[1] to axis 1, etc.). This corresponds to n for fft(x, n). Along each axis, if the given shape is smaller than that of the input, the input is cropped. If it is larger, the input is padded with zeros. if s is not given, the shape of the input along the axes specified by axes is used.  \naxessequence of ints, optional\n\n\nAxes over which to compute the FFT. If not given, the last two axes are used. A repeated index in axes means the transform over that axis is performed multiple times. A one-element sequence means that a one-dimensional FFT is performed.  \nnorm{\u201cbackward\u201d, \u201cortho\u201d, \u201cforward\u201d}, optional\n\n\n New in version 1.10.0.  Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.  New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.     Returns \n \noutcomplex ndarray\n\n\nThe truncated or zero-padded input, transformed along the axes indicated by axes, or the last two axes if axes is not given.    Raises \n ValueError\n\nIf s and axes have different length, or axes not given and len(s) != 2.  IndexError\n\nIf an element of axes is larger than than the number of axes of a.      See also  numpy.fft\n\nOverall view of discrete Fourier transforms, with definitions and conventions used.  ifft2\n\nThe inverse two-dimensional FFT.  fft\n\nThe one-dimensional FFT.  fftn\n\nThe n-dimensional FFT.  fftshift\n\nShifts zero-frequency terms to the center of the array. For two-dimensional input, swaps first and third quadrants, and second and fourth quadrants.    Notes fft2 is just fftn with a different default for axes. The output, analogously to fft, contains the term for zero frequency in the low-order corner of the transformed axes, the positive frequency terms in the first half of these axes, the term for the Nyquist frequency in the middle of the axes and the negative frequency terms in the second half of the axes, in order of decreasingly negative frequency. See fftn for details and a plotting example, and numpy.fft for definitions and conventions used. Examples >>> a = np.mgrid[:5, :5][0]\n>>> np.fft.fft2(a)\narray([[ 50.  +0.j        ,   0.  +0.j        ,   0.  +0.j        , # may vary\n          0.  +0.j        ,   0.  +0.j        ],\n       [-12.5+17.20477401j,   0.  +0.j        ,   0.  +0.j        ,\n          0.  +0.j        ,   0.  +0.j        ],\n       [-12.5 +4.0614962j ,   0.  +0.j        ,   0.  +0.j        ,\n          0.  +0.j        ,   0.  +0.j        ],\n       [-12.5 -4.0614962j ,   0.  +0.j        ,   0.  +0.j        ,\n          0.  +0.j        ,   0.  +0.j        ],\n       [-12.5-17.20477401j,   0.  +0.j        ,   0.  +0.j        ,\n          0.  +0.j        ,   0.  +0.j        ]])\n \n\n"}, {"name": "fft.fftfreq()", "path": "reference/generated/numpy.fft.fftfreq", "type": "numpy.fft.fftfreq", "text": "numpy.fft.fftfreq   fft.fftfreq(n, d=1.0)[source]\n \nReturn the Discrete Fourier Transform sample frequencies. The returned float array f contains the frequency bin centers in cycles per unit of the sample spacing (with zero at the start). For instance, if the sample spacing is in seconds, then the frequency unit is cycles/second. Given a window length n and a sample spacing d: f = [0, 1, ...,   n/2-1,     -n/2, ..., -1] / (d*n)   if n is even\nf = [0, 1, ..., (n-1)/2, -(n-1)/2, ..., -1] / (d*n)   if n is odd\n  Parameters \n \nnint\n\n\nWindow length.  \ndscalar, optional\n\n\nSample spacing (inverse of the sampling rate). Defaults to 1.    Returns \n \nfndarray\n\n\nArray of length n containing the sample frequencies.     Examples >>> signal = np.array([-2, 8, 6, 4, 1, 0, 3, 5], dtype=float)\n>>> fourier = np.fft.fft(signal)\n>>> n = signal.size\n>>> timestep = 0.1\n>>> freq = np.fft.fftfreq(n, d=timestep)\n>>> freq\narray([ 0.  ,  1.25,  2.5 , ..., -3.75, -2.5 , -1.25])\n \n\n"}, {"name": "fft.fftn()", "path": "reference/generated/numpy.fft.fftn", "type": "numpy.fft.fftn", "text": "numpy.fft.fftn   fft.fftn(a, s=None, axes=None, norm=None)[source]\n \nCompute the N-dimensional discrete Fourier Transform. This function computes the N-dimensional discrete Fourier Transform over any number of axes in an M-dimensional array by means of the Fast Fourier Transform (FFT).  Parameters \n \naarray_like\n\n\nInput array, can be complex.  \nssequence of ints, optional\n\n\nShape (length of each transformed axis) of the output (s[0] refers to axis 0, s[1] to axis 1, etc.). This corresponds to n for fft(x, n). Along any axis, if the given shape is smaller than that of the input, the input is cropped. If it is larger, the input is padded with zeros. if s is not given, the shape of the input along the axes specified by axes is used.  \naxessequence of ints, optional\n\n\nAxes over which to compute the FFT. If not given, the last len(s) axes are used, or all axes if s is also not specified. Repeated indices in axes means that the transform over that axis is performed multiple times.  \nnorm{\u201cbackward\u201d, \u201cortho\u201d, \u201cforward\u201d}, optional\n\n\n New in version 1.10.0.  Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.  New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.     Returns \n \noutcomplex ndarray\n\n\nThe truncated or zero-padded input, transformed along the axes indicated by axes, or by a combination of s and a, as explained in the parameters section above.    Raises \n ValueError\n\nIf s and axes have different length.  IndexError\n\nIf an element of axes is larger than than the number of axes of a.      See also  numpy.fft\n\nOverall view of discrete Fourier transforms, with definitions and conventions used.  ifftn\n\nThe inverse of fftn, the inverse n-dimensional FFT.  fft\n\nThe one-dimensional FFT, with definitions and conventions used.  rfftn\n\nThe n-dimensional FFT of real input.  fft2\n\nThe two-dimensional FFT.  fftshift\n\nShifts zero-frequency terms to centre of array    Notes The output, analogously to fft, contains the term for zero frequency in the low-order corner of all axes, the positive frequency terms in the first half of all axes, the term for the Nyquist frequency in the middle of all axes and the negative frequency terms in the second half of all axes, in order of decreasingly negative frequency. See numpy.fft for details, definitions and conventions used. Examples >>> a = np.mgrid[:3, :3, :3][0]\n>>> np.fft.fftn(a, axes=(1, 2))\narray([[[ 0.+0.j,   0.+0.j,   0.+0.j], # may vary\n        [ 0.+0.j,   0.+0.j,   0.+0.j],\n        [ 0.+0.j,   0.+0.j,   0.+0.j]],\n       [[ 9.+0.j,   0.+0.j,   0.+0.j],\n        [ 0.+0.j,   0.+0.j,   0.+0.j],\n        [ 0.+0.j,   0.+0.j,   0.+0.j]],\n       [[18.+0.j,   0.+0.j,   0.+0.j],\n        [ 0.+0.j,   0.+0.j,   0.+0.j],\n        [ 0.+0.j,   0.+0.j,   0.+0.j]]])\n>>> np.fft.fftn(a, (2, 2), axes=(0, 1))\narray([[[ 2.+0.j,  2.+0.j,  2.+0.j], # may vary\n        [ 0.+0.j,  0.+0.j,  0.+0.j]],\n       [[-2.+0.j, -2.+0.j, -2.+0.j],\n        [ 0.+0.j,  0.+0.j,  0.+0.j]]])\n >>> import matplotlib.pyplot as plt\n>>> [X, Y] = np.meshgrid(2 * np.pi * np.arange(200) / 12,\n...                      2 * np.pi * np.arange(200) / 34)\n>>> S = np.sin(X) + np.cos(Y) + np.random.uniform(0, 1, X.shape)\n>>> FS = np.fft.fftn(S)\n>>> plt.imshow(np.log(np.abs(np.fft.fftshift(FS))**2))\n<matplotlib.image.AxesImage object at 0x...>\n>>> plt.show()\n    \n\n"}, {"name": "fft.fftshift()", "path": "reference/generated/numpy.fft.fftshift", "type": "numpy.fft.fftshift", "text": "numpy.fft.fftshift   fft.fftshift(x, axes=None)[source]\n \nShift the zero-frequency component to the center of the spectrum. This function swaps half-spaces for all axes listed (defaults to all). Note that y[0] is the Nyquist component only if len(x) is even.  Parameters \n \nxarray_like\n\n\nInput array.  \naxesint or shape tuple, optional\n\n\nAxes over which to shift. Default is None, which shifts all axes.    Returns \n \nyndarray\n\n\nThe shifted array.      See also  ifftshift\n\nThe inverse of fftshift.    Examples >>> freqs = np.fft.fftfreq(10, 0.1)\n>>> freqs\narray([ 0.,  1.,  2., ..., -3., -2., -1.])\n>>> np.fft.fftshift(freqs)\narray([-5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.])\n Shift the zero-frequency component only along the second axis: >>> freqs = np.fft.fftfreq(9, d=1./9).reshape(3, 3)\n>>> freqs\narray([[ 0.,  1.,  2.],\n       [ 3.,  4., -4.],\n       [-3., -2., -1.]])\n>>> np.fft.fftshift(freqs, axes=(1,))\narray([[ 2.,  0.,  1.],\n       [-4.,  3.,  4.],\n       [-1., -3., -2.]])\n \n\n"}, {"name": "fft.hfft()", "path": "reference/generated/numpy.fft.hfft", "type": "numpy.fft.hfft", "text": "numpy.fft.hfft   fft.hfft(a, n=None, axis=- 1, norm=None)[source]\n \nCompute the FFT of a signal that has Hermitian symmetry, i.e., a real spectrum.  Parameters \n \naarray_like\n\n\nThe input array.  \nnint, optional\n\n\nLength of the transformed axis of the output. For n output points, n//2 + 1 input points are necessary. If the input is longer than this, it is cropped. If it is shorter than this, it is padded with zeros. If n is not given, it is taken to be 2*(m-1) where m is the length of the input along the axis specified by axis.  \naxisint, optional\n\n\nAxis over which to compute the FFT. If not given, the last axis is used.  \nnorm{\u201cbackward\u201d, \u201cortho\u201d, \u201cforward\u201d}, optional\n\n\n New in version 1.10.0.  Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.  New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.     Returns \n \noutndarray\n\n\nThe truncated or zero-padded input, transformed along the axis indicated by axis, or the last one if axis is not specified. The length of the transformed axis is n, or, if n is not given, 2*m - 2 where m is the length of the transformed axis of the input. To get an odd number of output points, n must be specified, for instance as 2*m - 1 in the typical case,    Raises \n IndexError\n\nIf axis is not a valid axis of a.      See also  rfft\n\nCompute the one-dimensional FFT for real input.  ihfft\n\nThe inverse of hfft.    Notes hfft/ihfft are a pair analogous to rfft/irfft, but for the opposite case: here the signal has Hermitian symmetry in the time domain and is real in the frequency domain. So here it\u2019s hfft for which you must supply the length of the result if it is to be odd.  even: ihfft(hfft(a, 2*len(a) - 2)) == a, within roundoff error, odd: ihfft(hfft(a, 2*len(a) - 1)) == a, within roundoff error.  The correct interpretation of the hermitian input depends on the length of the original data, as given by n. This is because each input shape could correspond to either an odd or even length signal. By default, hfft assumes an even output length which puts the last entry at the Nyquist frequency; aliasing with its symmetric counterpart. By Hermitian symmetry, the value is thus treated as purely real. To avoid losing information, the shape of the full signal must be given. Examples >>> signal = np.array([1, 2, 3, 4, 3, 2])\n>>> np.fft.fft(signal)\narray([15.+0.j,  -4.+0.j,   0.+0.j,  -1.-0.j,   0.+0.j,  -4.+0.j]) # may vary\n>>> np.fft.hfft(signal[:4]) # Input first half of signal\narray([15.,  -4.,   0.,  -1.,   0.,  -4.])\n>>> np.fft.hfft(signal, 6)  # Input entire signal and truncate\narray([15.,  -4.,   0.,  -1.,   0.,  -4.])\n >>> signal = np.array([[1, 1.j], [-1.j, 2]])\n>>> np.conj(signal.T) - signal   # check Hermitian symmetry\narray([[ 0.-0.j,  -0.+0.j], # may vary\n       [ 0.+0.j,  0.-0.j]])\n>>> freq_spectrum = np.fft.hfft(signal)\n>>> freq_spectrum\narray([[ 1.,  1.],\n       [ 2., -2.]])\n \n\n"}, {"name": "fft.ifft()", "path": "reference/generated/numpy.fft.ifft", "type": "numpy.fft.ifft", "text": "numpy.fft.ifft   fft.ifft(a, n=None, axis=- 1, norm=None)[source]\n \nCompute the one-dimensional inverse discrete Fourier Transform. This function computes the inverse of the one-dimensional n-point discrete Fourier transform computed by fft. In other words, ifft(fft(a)) == a to within numerical accuracy. For a general description of the algorithm and definitions, see numpy.fft. The input should be ordered in the same way as is returned by fft, i.e.,  \na[0] should contain the zero frequency term, \na[1:n//2] should contain the positive-frequency terms, \na[n//2 + 1:] should contain the negative-frequency terms, in increasing order starting from the most negative frequency.  For an even number of input points, A[n//2] represents the sum of the values at the positive and negative Nyquist frequencies, as the two are aliased together. See numpy.fft for details.  Parameters \n \naarray_like\n\n\nInput array, can be complex.  \nnint, optional\n\n\nLength of the transformed axis of the output. If n is smaller than the length of the input, the input is cropped. If it is larger, the input is padded with zeros. If n is not given, the length of the input along the axis specified by axis is used. See notes about padding issues.  \naxisint, optional\n\n\nAxis over which to compute the inverse DFT. If not given, the last axis is used.  \nnorm{\u201cbackward\u201d, \u201cortho\u201d, \u201cforward\u201d}, optional\n\n\n New in version 1.10.0.  Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.  New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.     Returns \n \noutcomplex ndarray\n\n\nThe truncated or zero-padded input, transformed along the axis indicated by axis, or the last one if axis is not specified.    Raises \n IndexError\n\nIf axis is not a valid axis of a.      See also  numpy.fft\n\nAn introduction, with definitions and general explanations.  fft\n\nThe one-dimensional (forward) FFT, of which ifft is the inverse  ifft2\n\nThe two-dimensional inverse FFT.  ifftn\n\nThe n-dimensional inverse FFT.    Notes If the input parameter n is larger than the size of the input, the input is padded by appending zeros at the end. Even though this is the common approach, it might lead to surprising results. If a different padding is desired, it must be performed before calling ifft. Examples >>> np.fft.ifft([0, 4, 0, 0])\narray([ 1.+0.j,  0.+1.j, -1.+0.j,  0.-1.j]) # may vary\n Create and plot a band-limited signal with random phases: >>> import matplotlib.pyplot as plt\n>>> t = np.arange(400)\n>>> n = np.zeros((400,), dtype=complex)\n>>> n[40:60] = np.exp(1j*np.random.uniform(0, 2*np.pi, (20,)))\n>>> s = np.fft.ifft(n)\n>>> plt.plot(t, s.real, label='real')\n[<matplotlib.lines.Line2D object at ...>]\n>>> plt.plot(t, s.imag, '--', label='imaginary')\n[<matplotlib.lines.Line2D object at ...>]\n>>> plt.legend()\n<matplotlib.legend.Legend object at ...>\n>>> plt.show()\n    \n\n"}, {"name": "fft.ifft2()", "path": "reference/generated/numpy.fft.ifft2", "type": "numpy.fft.ifft2", "text": "numpy.fft.ifft2   fft.ifft2(a, s=None, axes=(- 2, - 1), norm=None)[source]\n \nCompute the 2-dimensional inverse discrete Fourier Transform. This function computes the inverse of the 2-dimensional discrete Fourier Transform over any number of axes in an M-dimensional array by means of the Fast Fourier Transform (FFT). In other words, ifft2(fft2(a)) == a to within numerical accuracy. By default, the inverse transform is computed over the last two axes of the input array. The input, analogously to ifft, should be ordered in the same way as is returned by fft2, i.e. it should have the term for zero frequency in the low-order corner of the two axes, the positive frequency terms in the first half of these axes, the term for the Nyquist frequency in the middle of the axes and the negative frequency terms in the second half of both axes, in order of decreasingly negative frequency.  Parameters \n \naarray_like\n\n\nInput array, can be complex.  \nssequence of ints, optional\n\n\nShape (length of each axis) of the output (s[0] refers to axis 0, s[1] to axis 1, etc.). This corresponds to n for ifft(x, n). Along each axis, if the given shape is smaller than that of the input, the input is cropped. If it is larger, the input is padded with zeros. if s is not given, the shape of the input along the axes specified by axes is used. See notes for issue on ifft zero padding.  \naxessequence of ints, optional\n\n\nAxes over which to compute the FFT. If not given, the last two axes are used. A repeated index in axes means the transform over that axis is performed multiple times. A one-element sequence means that a one-dimensional FFT is performed.  \nnorm{\u201cbackward\u201d, \u201cortho\u201d, \u201cforward\u201d}, optional\n\n\n New in version 1.10.0.  Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.  New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.     Returns \n \noutcomplex ndarray\n\n\nThe truncated or zero-padded input, transformed along the axes indicated by axes, or the last two axes if axes is not given.    Raises \n ValueError\n\nIf s and axes have different length, or axes not given and len(s) != 2.  IndexError\n\nIf an element of axes is larger than than the number of axes of a.      See also  numpy.fft\n\nOverall view of discrete Fourier transforms, with definitions and conventions used.  fft2\n\nThe forward 2-dimensional FFT, of which ifft2 is the inverse.  ifftn\n\nThe inverse of the n-dimensional FFT.  fft\n\nThe one-dimensional FFT.  ifft\n\nThe one-dimensional inverse FFT.    Notes ifft2 is just ifftn with a different default for axes. See ifftn for details and a plotting example, and numpy.fft for definition and conventions used. Zero-padding, analogously with ifft, is performed by appending zeros to the input along the specified dimension. Although this is the common approach, it might lead to surprising results. If another form of zero padding is desired, it must be performed before ifft2 is called. Examples >>> a = 4 * np.eye(4)\n>>> np.fft.ifft2(a)\narray([[1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j], # may vary\n       [0.+0.j,  0.+0.j,  0.+0.j,  1.+0.j],\n       [0.+0.j,  0.+0.j,  1.+0.j,  0.+0.j],\n       [0.+0.j,  1.+0.j,  0.+0.j,  0.+0.j]])\n \n\n"}, {"name": "fft.ifftn()", "path": "reference/generated/numpy.fft.ifftn", "type": "numpy.fft.ifftn", "text": "numpy.fft.ifftn   fft.ifftn(a, s=None, axes=None, norm=None)[source]\n \nCompute the N-dimensional inverse discrete Fourier Transform. This function computes the inverse of the N-dimensional discrete Fourier Transform over any number of axes in an M-dimensional array by means of the Fast Fourier Transform (FFT). In other words, ifftn(fftn(a)) == a to within numerical accuracy. For a description of the definitions and conventions used, see numpy.fft. The input, analogously to ifft, should be ordered in the same way as is returned by fftn, i.e. it should have the term for zero frequency in all axes in the low-order corner, the positive frequency terms in the first half of all axes, the term for the Nyquist frequency in the middle of all axes and the negative frequency terms in the second half of all axes, in order of decreasingly negative frequency.  Parameters \n \naarray_like\n\n\nInput array, can be complex.  \nssequence of ints, optional\n\n\nShape (length of each transformed axis) of the output (s[0] refers to axis 0, s[1] to axis 1, etc.). This corresponds to n for ifft(x, n). Along any axis, if the given shape is smaller than that of the input, the input is cropped. If it is larger, the input is padded with zeros. if s is not given, the shape of the input along the axes specified by axes is used. See notes for issue on ifft zero padding.  \naxessequence of ints, optional\n\n\nAxes over which to compute the IFFT. If not given, the last len(s) axes are used, or all axes if s is also not specified. Repeated indices in axes means that the inverse transform over that axis is performed multiple times.  \nnorm{\u201cbackward\u201d, \u201cortho\u201d, \u201cforward\u201d}, optional\n\n\n New in version 1.10.0.  Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.  New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.     Returns \n \noutcomplex ndarray\n\n\nThe truncated or zero-padded input, transformed along the axes indicated by axes, or by a combination of s or a, as explained in the parameters section above.    Raises \n ValueError\n\nIf s and axes have different length.  IndexError\n\nIf an element of axes is larger than than the number of axes of a.      See also  numpy.fft\n\nOverall view of discrete Fourier transforms, with definitions and conventions used.  fftn\n\nThe forward n-dimensional FFT, of which ifftn is the inverse.  ifft\n\nThe one-dimensional inverse FFT.  ifft2\n\nThe two-dimensional inverse FFT.  ifftshift\n\nUndoes fftshift, shifts zero-frequency terms to beginning of array.    Notes See numpy.fft for definitions and conventions used. Zero-padding, analogously with ifft, is performed by appending zeros to the input along the specified dimension. Although this is the common approach, it might lead to surprising results. If another form of zero padding is desired, it must be performed before ifftn is called. Examples >>> a = np.eye(4)\n>>> np.fft.ifftn(np.fft.fftn(a, axes=(0,)), axes=(1,))\narray([[1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j], # may vary\n       [0.+0.j,  1.+0.j,  0.+0.j,  0.+0.j],\n       [0.+0.j,  0.+0.j,  1.+0.j,  0.+0.j],\n       [0.+0.j,  0.+0.j,  0.+0.j,  1.+0.j]])\n Create and plot an image with band-limited frequency content: >>> import matplotlib.pyplot as plt\n>>> n = np.zeros((200,200), dtype=complex)\n>>> n[60:80, 20:40] = np.exp(1j*np.random.uniform(0, 2*np.pi, (20, 20)))\n>>> im = np.fft.ifftn(n).real\n>>> plt.imshow(im)\n<matplotlib.image.AxesImage object at 0x...>\n>>> plt.show()\n    \n\n"}, {"name": "fft.ifftshift()", "path": "reference/generated/numpy.fft.ifftshift", "type": "numpy.fft.ifftshift", "text": "numpy.fft.ifftshift   fft.ifftshift(x, axes=None)[source]\n \nThe inverse of fftshift. Although identical for even-length x, the functions differ by one sample for odd-length x.  Parameters \n \nxarray_like\n\n\nInput array.  \naxesint or shape tuple, optional\n\n\nAxes over which to calculate. Defaults to None, which shifts all axes.    Returns \n \nyndarray\n\n\nThe shifted array.      See also  fftshift\n\nShift zero-frequency component to the center of the spectrum.    Examples >>> freqs = np.fft.fftfreq(9, d=1./9).reshape(3, 3)\n>>> freqs\narray([[ 0.,  1.,  2.],\n       [ 3.,  4., -4.],\n       [-3., -2., -1.]])\n>>> np.fft.ifftshift(np.fft.fftshift(freqs))\narray([[ 0.,  1.,  2.],\n       [ 3.,  4., -4.],\n       [-3., -2., -1.]])\n \n\n"}, {"name": "fft.ihfft()", "path": "reference/generated/numpy.fft.ihfft", "type": "numpy.fft.ihfft", "text": "numpy.fft.ihfft   fft.ihfft(a, n=None, axis=- 1, norm=None)[source]\n \nCompute the inverse FFT of a signal that has Hermitian symmetry.  Parameters \n \naarray_like\n\n\nInput array.  \nnint, optional\n\n\nLength of the inverse FFT, the number of points along transformation axis in the input to use. If n is smaller than the length of the input, the input is cropped. If it is larger, the input is padded with zeros. If n is not given, the length of the input along the axis specified by axis is used.  \naxisint, optional\n\n\nAxis over which to compute the inverse FFT. If not given, the last axis is used.  \nnorm{\u201cbackward\u201d, \u201cortho\u201d, \u201cforward\u201d}, optional\n\n\n New in version 1.10.0.  Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.  New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.     Returns \n \noutcomplex ndarray\n\n\nThe truncated or zero-padded input, transformed along the axis indicated by axis, or the last one if axis is not specified. The length of the transformed axis is n//2 + 1.      See also  \nhfft, irfft\n\n  Notes hfft/ihfft are a pair analogous to rfft/irfft, but for the opposite case: here the signal has Hermitian symmetry in the time domain and is real in the frequency domain. So here it\u2019s hfft for which you must supply the length of the result if it is to be odd:  even: ihfft(hfft(a, 2*len(a) - 2)) == a, within roundoff error, odd: ihfft(hfft(a, 2*len(a) - 1)) == a, within roundoff error.  Examples >>> spectrum = np.array([ 15, -4, 0, -1, 0, -4])\n>>> np.fft.ifft(spectrum)\narray([1.+0.j,  2.+0.j,  3.+0.j,  4.+0.j,  3.+0.j,  2.+0.j]) # may vary\n>>> np.fft.ihfft(spectrum)\narray([ 1.-0.j,  2.-0.j,  3.-0.j,  4.-0.j]) # may vary\n \n\n"}, {"name": "fft.irfft()", "path": "reference/generated/numpy.fft.irfft", "type": "numpy.fft.irfft", "text": "numpy.fft.irfft   fft.irfft(a, n=None, axis=- 1, norm=None)[source]\n \nComputes the inverse of rfft. This function computes the inverse of the one-dimensional n-point discrete Fourier Transform of real input computed by rfft. In other words, irfft(rfft(a), len(a)) == a to within numerical accuracy. (See Notes below for why len(a) is necessary here.) The input is expected to be in the form returned by rfft, i.e. the real zero-frequency term followed by the complex positive frequency terms in order of increasing frequency. Since the discrete Fourier Transform of real input is Hermitian-symmetric, the negative frequency terms are taken to be the complex conjugates of the corresponding positive frequency terms.  Parameters \n \naarray_like\n\n\nThe input array.  \nnint, optional\n\n\nLength of the transformed axis of the output. For n output points, n//2+1 input points are necessary. If the input is longer than this, it is cropped. If it is shorter than this, it is padded with zeros. If n is not given, it is taken to be 2*(m-1) where m is the length of the input along the axis specified by axis.  \naxisint, optional\n\n\nAxis over which to compute the inverse FFT. If not given, the last axis is used.  \nnorm{\u201cbackward\u201d, \u201cortho\u201d, \u201cforward\u201d}, optional\n\n\n New in version 1.10.0.  Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.  New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.     Returns \n \noutndarray\n\n\nThe truncated or zero-padded input, transformed along the axis indicated by axis, or the last one if axis is not specified. The length of the transformed axis is n, or, if n is not given, 2*(m-1) where m is the length of the transformed axis of the input. To get an odd number of output points, n must be specified.    Raises \n IndexError\n\nIf axis is not a valid axis of a.      See also  numpy.fft\n\nFor definition of the DFT and conventions used.  rfft\n\nThe one-dimensional FFT of real input, of which irfft is inverse.  fft\n\nThe one-dimensional FFT.  irfft2\n\nThe inverse of the two-dimensional FFT of real input.  irfftn\n\nThe inverse of the n-dimensional FFT of real input.    Notes Returns the real valued n-point inverse discrete Fourier transform of a, where a contains the non-negative frequency terms of a Hermitian-symmetric sequence. n is the length of the result, not the input. If you specify an n such that a must be zero-padded or truncated, the extra/removed values will be added/removed at high frequencies. One can thus resample a series to m points via Fourier interpolation by: a_resamp = irfft(rfft(a), m). The correct interpretation of the hermitian input depends on the length of the original data, as given by n. This is because each input shape could correspond to either an odd or even length signal. By default, irfft assumes an even output length which puts the last entry at the Nyquist frequency; aliasing with its symmetric counterpart. By Hermitian symmetry, the value is thus treated as purely real. To avoid losing information, the correct length of the real input must be given. Examples >>> np.fft.ifft([1, -1j, -1, 1j])\narray([0.+0.j,  1.+0.j,  0.+0.j,  0.+0.j]) # may vary\n>>> np.fft.irfft([1, -1j, -1])\narray([0.,  1.,  0.,  0.])\n Notice how the last term in the input to the ordinary ifft is the complex conjugate of the second term, and the output has zero imaginary part everywhere. When calling irfft, the negative frequencies are not specified, and the output array is purely real. \n\n"}, {"name": "fft.irfft2()", "path": "reference/generated/numpy.fft.irfft2", "type": "numpy.fft.irfft2", "text": "numpy.fft.irfft2   fft.irfft2(a, s=None, axes=(- 2, - 1), norm=None)[source]\n \nComputes the inverse of rfft2.  Parameters \n \naarray_like\n\n\nThe input array  \nssequence of ints, optional\n\n\nShape of the real output to the inverse FFT.  \naxessequence of ints, optional\n\n\nThe axes over which to compute the inverse fft. Default is the last two axes.  \nnorm{\u201cbackward\u201d, \u201cortho\u201d, \u201cforward\u201d}, optional\n\n\n New in version 1.10.0.  Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.  New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.     Returns \n \noutndarray\n\n\nThe result of the inverse real 2-D FFT.      See also  rfft2\n\nThe forward two-dimensional FFT of real input, of which irfft2 is the inverse.  rfft\n\nThe one-dimensional FFT for real input.  irfft\n\nThe inverse of the one-dimensional FFT of real input.  irfftn\n\nCompute the inverse of the N-dimensional FFT of real input.    Notes This is really irfftn with different defaults. For more details see irfftn. Examples >>> a = np.mgrid[:5, :5][0]\n>>> A = np.fft.rfft2(a)\n>>> np.fft.irfft2(A, s=a.shape)\narray([[0., 0., 0., 0., 0.],\n       [1., 1., 1., 1., 1.],\n       [2., 2., 2., 2., 2.],\n       [3., 3., 3., 3., 3.],\n       [4., 4., 4., 4., 4.]])\n \n\n"}, {"name": "fft.irfftn()", "path": "reference/generated/numpy.fft.irfftn", "type": "numpy.fft.irfftn", "text": "numpy.fft.irfftn   fft.irfftn(a, s=None, axes=None, norm=None)[source]\n \nComputes the inverse of rfftn. This function computes the inverse of the N-dimensional discrete Fourier Transform for real input over any number of axes in an M-dimensional array by means of the Fast Fourier Transform (FFT). In other words, irfftn(rfftn(a), a.shape) == a to within numerical accuracy. (The a.shape is necessary like len(a) is for irfft, and for the same reason.) The input should be ordered in the same way as is returned by rfftn, i.e. as for irfft for the final transformation axis, and as for ifftn along all the other axes.  Parameters \n \naarray_like\n\n\nInput array.  \nssequence of ints, optional\n\n\nShape (length of each transformed axis) of the output (s[0] refers to axis 0, s[1] to axis 1, etc.). s is also the number of input points used along this axis, except for the last axis, where s[-1]//2+1 points of the input are used. Along any axis, if the shape indicated by s is smaller than that of the input, the input is cropped. If it is larger, the input is padded with zeros. If s is not given, the shape of the input along the axes specified by axes is used. Except for the last axis which is taken to be 2*(m-1) where m is the length of the input along that axis.  \naxessequence of ints, optional\n\n\nAxes over which to compute the inverse FFT. If not given, the last len(s) axes are used, or all axes if s is also not specified. Repeated indices in axes means that the inverse transform over that axis is performed multiple times.  \nnorm{\u201cbackward\u201d, \u201cortho\u201d, \u201cforward\u201d}, optional\n\n\n New in version 1.10.0.  Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.  New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.     Returns \n \noutndarray\n\n\nThe truncated or zero-padded input, transformed along the axes indicated by axes, or by a combination of s or a, as explained in the parameters section above. The length of each transformed axis is as given by the corresponding element of s, or the length of the input in every axis except for the last one if s is not given. In the final transformed axis the length of the output when s is not given is 2*(m-1) where m is the length of the final transformed axis of the input. To get an odd number of output points in the final axis, s must be specified.    Raises \n ValueError\n\nIf s and axes have different length.  IndexError\n\nIf an element of axes is larger than than the number of axes of a.      See also  rfftn\n\nThe forward n-dimensional FFT of real input, of which ifftn is the inverse.  fft\n\nThe one-dimensional FFT, with definitions and conventions used.  irfft\n\nThe inverse of the one-dimensional FFT of real input.  irfft2\n\nThe inverse of the two-dimensional FFT of real input.    Notes See fft for definitions and conventions used. See rfft for definitions and conventions used for real input. The correct interpretation of the hermitian input depends on the shape of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, irfftn assumes an even output length which puts the last entry at the Nyquist frequency; aliasing with its symmetric counterpart. When performing the final complex to real transform, the last value is thus treated as purely real. To avoid losing information, the correct shape of the real input must be given. Examples >>> a = np.zeros((3, 2, 2))\n>>> a[0, 0, 0] = 3 * 2 * 2\n>>> np.fft.irfftn(a)\narray([[[1.,  1.],\n        [1.,  1.]],\n       [[1.,  1.],\n        [1.,  1.]],\n       [[1.,  1.],\n        [1.,  1.]]])\n \n\n"}, {"name": "fft.rfft()", "path": "reference/generated/numpy.fft.rfft", "type": "numpy.fft.rfft", "text": "numpy.fft.rfft   fft.rfft(a, n=None, axis=- 1, norm=None)[source]\n \nCompute the one-dimensional discrete Fourier Transform for real input. This function computes the one-dimensional n-point discrete Fourier Transform (DFT) of a real-valued array by means of an efficient algorithm called the Fast Fourier Transform (FFT).  Parameters \n \naarray_like\n\n\nInput array  \nnint, optional\n\n\nNumber of points along transformation axis in the input to use. If n is smaller than the length of the input, the input is cropped. If it is larger, the input is padded with zeros. If n is not given, the length of the input along the axis specified by axis is used.  \naxisint, optional\n\n\nAxis over which to compute the FFT. If not given, the last axis is used.  \nnorm{\u201cbackward\u201d, \u201cortho\u201d, \u201cforward\u201d}, optional\n\n\n New in version 1.10.0.  Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.  New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.     Returns \n \noutcomplex ndarray\n\n\nThe truncated or zero-padded input, transformed along the axis indicated by axis, or the last one if axis is not specified. If n is even, the length of the transformed axis is (n/2)+1. If n is odd, the length is (n+1)/2.    Raises \n IndexError\n\nIf axis is not a valid axis of a.      See also  numpy.fft\n\nFor definition of the DFT and conventions used.  irfft\n\nThe inverse of rfft.  fft\n\nThe one-dimensional FFT of general (complex) input.  fftn\n\nThe n-dimensional FFT.  rfftn\n\nThe n-dimensional FFT of real input.    Notes When the DFT is computed for purely real input, the output is Hermitian-symmetric, i.e. the negative frequency terms are just the complex conjugates of the corresponding positive-frequency terms, and the negative-frequency terms are therefore redundant. This function does not compute the negative frequency terms, and the length of the transformed axis of the output is therefore n//2 + 1. When A = rfft(a) and fs is the sampling frequency, A[0] contains the zero-frequency term 0*fs, which is real due to Hermitian symmetry. If n is even, A[-1] contains the term representing both positive and negative Nyquist frequency (+fs/2 and -fs/2), and must also be purely real. If n is odd, there is no term at fs/2; A[-1] contains the largest positive frequency (fs/2*(n-1)/n), and is complex in the general case. If the input a contains an imaginary part, it is silently discarded. Examples >>> np.fft.fft([0, 1, 0, 0])\narray([ 1.+0.j,  0.-1.j, -1.+0.j,  0.+1.j]) # may vary\n>>> np.fft.rfft([0, 1, 0, 0])\narray([ 1.+0.j,  0.-1.j, -1.+0.j]) # may vary\n Notice how the final element of the fft output is the complex conjugate of the second element, for real input. For rfft, this symmetry is exploited to compute only the non-negative frequency terms. \n\n"}, {"name": "fft.rfft2()", "path": "reference/generated/numpy.fft.rfft2", "type": "numpy.fft.rfft2", "text": "numpy.fft.rfft2   fft.rfft2(a, s=None, axes=(- 2, - 1), norm=None)[source]\n \nCompute the 2-dimensional FFT of a real array.  Parameters \n \naarray\n\n\nInput array, taken to be real.  \nssequence of ints, optional\n\n\nShape of the FFT.  \naxessequence of ints, optional\n\n\nAxes over which to compute the FFT.  \nnorm{\u201cbackward\u201d, \u201cortho\u201d, \u201cforward\u201d}, optional\n\n\n New in version 1.10.0.  Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.  New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.     Returns \n \noutndarray\n\n\nThe result of the real 2-D FFT.      See also  rfftn\n\nCompute the N-dimensional discrete Fourier Transform for real input.    Notes This is really just rfftn with different default behavior. For more details see rfftn. Examples >>> a = np.mgrid[:5, :5][0]\n>>> np.fft.rfft2(a)\narray([[ 50.  +0.j        ,   0.  +0.j        ,   0.  +0.j        ],\n       [-12.5+17.20477401j,   0.  +0.j        ,   0.  +0.j        ],\n       [-12.5 +4.0614962j ,   0.  +0.j        ,   0.  +0.j        ],\n       [-12.5 -4.0614962j ,   0.  +0.j        ,   0.  +0.j        ],\n       [-12.5-17.20477401j,   0.  +0.j        ,   0.  +0.j        ]])\n \n\n"}, {"name": "fft.rfftfreq()", "path": "reference/generated/numpy.fft.rfftfreq", "type": "numpy.fft.rfftfreq", "text": "numpy.fft.rfftfreq   fft.rfftfreq(n, d=1.0)[source]\n \nReturn the Discrete Fourier Transform sample frequencies (for usage with rfft, irfft). The returned float array f contains the frequency bin centers in cycles per unit of the sample spacing (with zero at the start). For instance, if the sample spacing is in seconds, then the frequency unit is cycles/second. Given a window length n and a sample spacing d: f = [0, 1, ...,     n/2-1,     n/2] / (d*n)   if n is even\nf = [0, 1, ..., (n-1)/2-1, (n-1)/2] / (d*n)   if n is odd\n Unlike fftfreq (but like scipy.fftpack.rfftfreq) the Nyquist frequency component is considered to be positive.  Parameters \n \nnint\n\n\nWindow length.  \ndscalar, optional\n\n\nSample spacing (inverse of the sampling rate). Defaults to 1.    Returns \n \nfndarray\n\n\nArray of length n//2 + 1 containing the sample frequencies.     Examples >>> signal = np.array([-2, 8, 6, 4, 1, 0, 3, 5, -3, 4], dtype=float)\n>>> fourier = np.fft.rfft(signal)\n>>> n = signal.size\n>>> sample_rate = 100\n>>> freq = np.fft.fftfreq(n, d=1./sample_rate)\n>>> freq\narray([  0.,  10.,  20., ..., -30., -20., -10.])\n>>> freq = np.fft.rfftfreq(n, d=1./sample_rate)\n>>> freq\narray([  0.,  10.,  20.,  30.,  40.,  50.])\n \n\n"}, {"name": "fft.rfftn()", "path": "reference/generated/numpy.fft.rfftn", "type": "numpy.fft.rfftn", "text": "numpy.fft.rfftn   fft.rfftn(a, s=None, axes=None, norm=None)[source]\n \nCompute the N-dimensional discrete Fourier Transform for real input. This function computes the N-dimensional discrete Fourier Transform over any number of axes in an M-dimensional real array by means of the Fast Fourier Transform (FFT). By default, all axes are transformed, with the real transform performed over the last axis, while the remaining transforms are complex.  Parameters \n \naarray_like\n\n\nInput array, taken to be real.  \nssequence of ints, optional\n\n\nShape (length along each transformed axis) to use from the input. (s[0] refers to axis 0, s[1] to axis 1, etc.). The final element of s corresponds to n for rfft(x, n), while for the remaining axes, it corresponds to n for fft(x, n). Along any axis, if the given shape is smaller than that of the input, the input is cropped. If it is larger, the input is padded with zeros. if s is not given, the shape of the input along the axes specified by axes is used.  \naxessequence of ints, optional\n\n\nAxes over which to compute the FFT. If not given, the last len(s) axes are used, or all axes if s is also not specified.  \nnorm{\u201cbackward\u201d, \u201cortho\u201d, \u201cforward\u201d}, optional\n\n\n New in version 1.10.0.  Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.  New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.     Returns \n \noutcomplex ndarray\n\n\nThe truncated or zero-padded input, transformed along the axes indicated by axes, or by a combination of s and a, as explained in the parameters section above. The length of the last axis transformed will be s[-1]//2+1, while the remaining transformed axes will have lengths according to s, or unchanged from the input.    Raises \n ValueError\n\nIf s and axes have different length.  IndexError\n\nIf an element of axes is larger than than the number of axes of a.      See also  irfftn\n\nThe inverse of rfftn, i.e. the inverse of the n-dimensional FFT of real input.  fft\n\nThe one-dimensional FFT, with definitions and conventions used.  rfft\n\nThe one-dimensional FFT of real input.  fftn\n\nThe n-dimensional FFT.  rfft2\n\nThe two-dimensional FFT of real input.    Notes The transform for real input is performed over the last transformation axis, as by rfft, then the transform over the remaining axes is performed as by fftn. The order of the output is as for rfft for the final transformation axis, and as for fftn for the remaining transformation axes. See fft for details, definitions and conventions used. Examples >>> a = np.ones((2, 2, 2))\n>>> np.fft.rfftn(a)\narray([[[8.+0.j,  0.+0.j], # may vary\n        [0.+0.j,  0.+0.j]],\n       [[0.+0.j,  0.+0.j],\n        [0.+0.j,  0.+0.j]]])\n >>> np.fft.rfftn(a, axes=(2, 0))\narray([[[4.+0.j,  0.+0.j], # may vary\n        [4.+0.j,  0.+0.j]],\n       [[0.+0.j,  0.+0.j],\n        [0.+0.j,  0.+0.j]]])\n \n\n"}, {"name": "final class numpy.typing.NBitBase", "path": "reference/typing#numpy.typing.NBitBase", "type": "Typing ( \n    \n     numpy.typing\n    \n    )", "text": "  final class numpy.typing.NBitBase[source]\n \nA type representing numpy.number precision during static type checking. Used exclusively for the purpose static type checking, NBitBase represents the base of a hierarchical set of subclasses. Each subsequent subclass is herein used for representing a lower level of precision, e.g. 64Bit > 32Bit > 16Bit.  New in version 1.20.  Examples Below is a typical usage example: NBitBase is herein used for annotating a function that takes a float and integer of arbitrary precision as arguments and returns a new float of whichever precision is largest (e.g. np.float16 + np.int64 -> np.float64). >>> from __future__ import annotations\n>>> from typing import TypeVar, TYPE_CHECKING\n>>> import numpy as np\n>>> import numpy.typing as npt\n\n>>> T1 = TypeVar(\"T1\", bound=npt.NBitBase)\n>>> T2 = TypeVar(\"T2\", bound=npt.NBitBase)\n\n>>> def add(a: np.floating[T1], b: np.integer[T2]) -> np.floating[T1 | T2]:\n...     return a + b\n\n>>> a = np.float16()\n>>> b = np.int64()\n>>> out = add(a, b)\n\n>>> if TYPE_CHECKING:\n...     reveal_locals()\n...     # note: Revealed local types are:\n...     # note:     a: numpy.floating[numpy.typing._16Bit*]\n...     # note:     b: numpy.signedinteger[numpy.typing._64Bit*]\n...     # note:     out: numpy.floating[numpy.typing._64Bit*]\n \n"}, {"name": "flatiter.base", "path": "reference/generated/numpy.flatiter.base", "type": "Indexing routines", "text": "numpy.flatiter.base attribute   flatiter.base\n \nA reference to the array that is iterated over. Examples >>> x = np.arange(5)\n>>> fl = x.flat\n>>> fl.base is x\nTrue\n \n\n"}, {"name": "flatiter.coords", "path": "reference/generated/numpy.flatiter.coords", "type": "Indexing routines", "text": "numpy.flatiter.coords attribute   flatiter.coords\n \nAn N-dimensional tuple of current coordinates. Examples >>> x = np.arange(6).reshape(2, 3)\n>>> fl = x.flat\n>>> fl.coords\n(0, 0)\n>>> next(fl)\n0\n>>> fl.coords\n(0, 1)\n \n\n"}, {"name": "flatiter.copy()", "path": "reference/generated/numpy.flatiter.copy", "type": "numpy.flatiter.copy", "text": "numpy.flatiter.copy method   flatiter.copy()\n \nGet a copy of the iterator as a 1-D array. Examples >>> x = np.arange(6).reshape(2, 3)\n>>> x\narray([[0, 1, 2],\n       [3, 4, 5]])\n>>> fl = x.flat\n>>> fl.copy()\narray([0, 1, 2, 3, 4, 5])\n \n\n"}, {"name": "flatiter.index", "path": "reference/generated/numpy.flatiter.index", "type": "Indexing routines", "text": "numpy.flatiter.index attribute   flatiter.index\n \nCurrent flat index into the array. Examples >>> x = np.arange(6).reshape(2, 3)\n>>> fl = x.flat\n>>> fl.index\n0\n>>> next(fl)\n0\n>>> fl.index\n1\n \n\n"}, {"name": "float npy_half_to_float()", "path": "reference/c-api/coremath#c.npy_half_to_float", "type": "NumPy core libraries", "text": "  floatnpy_half_to_float(npy_halfh)\n \nConverts a half-precision float to a single-precision float. \n"}, {"name": "float random_gamma_f()", "path": "reference/random/c-api#c.random_gamma_f", "type": "C API for random", "text": "  floatrandom_gamma_f(bitgen_t*bitgen_state, floatshape, floatscale)\n\n"}, {"name": "float random_standard_exponential_f()", "path": "reference/random/c-api#c.random_standard_exponential_f", "type": "C API for random", "text": "  floatrandom_standard_exponential_f(bitgen_t*bitgen_state)\n\n"}, {"name": "float random_standard_gamma_f()", "path": "reference/random/c-api#c.random_standard_gamma_f", "type": "C API for random", "text": "  floatrandom_standard_gamma_f(bitgen_t*bitgen_state, floatshape)\n\n"}, {"name": "float random_standard_normal_f()", "path": "reference/random/c-api#c.random_standard_normal_f", "type": "C API for random", "text": "  floatrandom_standard_normal_f(bitgen_t*bitgen_state)\n\n"}, {"name": "float random_standard_uniform_f()", "path": "reference/random/c-api#c.random_standard_uniform_f", "type": "C API for random", "text": "  floatrandom_standard_uniform_f(bitgen_t*bitgen_state)\n\n"}, {"name": "Floating point error handling", "path": "reference/routines.err", "type": "Floating point error handling", "text": "Floating point error handling  Setting and getting error handling  \nseterr([all, divide, over, under, invalid]) Set how floating-point errors are handled.  \ngeterr() Get the current way of handling floating-point errors.  \nseterrcall(func) Set the floating-point error callback function or log object.  \ngeterrcall() Return the current callback function used on floating-point errors.  \nerrstate(**kwargs) Context manager for floating-point error handling.     Internal functions  \nseterrobj(errobj, /) Set the object that defines floating-point error handling.  \ngeterrobj() Return the current object that defines floating-point error handling.   \n"}, {"name": "For downstream package authors", "path": "user/depending_on_numpy", "type": "User Guide", "text": "For downstream package authors This document aims to explain some best practices for authoring a package that depends on NumPy.  Understanding NumPy\u2019s versioning and API/ABI stability NumPy uses a standard, PEP 440 compliant, versioning scheme: major.minor.bugfix. A major release is highly unusual (NumPy is still at version 1.xx) and if it happens it will likely indicate an ABI break. Minor versions are released regularly, typically every 6 months. Minor versions contain new features, deprecations, and removals of previously deprecated code. Bugfix releases are made even more frequently; they do not contain any new features or deprecations. It is important to know that NumPy, like Python itself and most other well known scientific Python projects, does not use semantic versioning. Instead, backwards incompatible API changes require deprecation warnings for at least two releases. For more details, see NEP 23 \u2014 Backwards compatibility and deprecation policy. NumPy has both a Python API and a C API. The C API can be used directly or via Cython, f2py, or other such tools. If your package uses the C API, then ABI (application binary interface) stability of NumPy is important. NumPy\u2019s ABI is forward but not backward compatible. This means: binaries compiled against a given version of NumPy will still run correctly with newer NumPy versions, but not with older versions.   Testing against the NumPy main branch or pre-releases For large, actively maintained packages that depend on NumPy, we recommend testing against the development version of NumPy in CI. To make this easy, nightly builds are provided as wheels at https://anaconda.org/scipy-wheels-nightly/. This helps detect regressions in NumPy that need fixing before the next NumPy release. Furthermore, we recommend to raise errors on warnings in CI for this job, either all warnings or otherwise at least DeprecationWarning and FutureWarning. This gives you an early warning about changes in NumPy to adapt your code.   Adding a dependency on NumPy  Build-time dependency If a package either uses the NumPy C API directly or it uses some other tool that depends on it like Cython or Pythran, NumPy is a build-time dependency of the package. Because the NumPy ABI is only forward compatible, you must build your own binaries (wheels or other package formats) against the lowest NumPy version that you support (or an even older version). Picking the correct NumPy version to build against for each Python version and platform can get complicated. There are a couple of ways to do this. Build-time dependencies are specified in pyproject.toml (see PEP 517), which is the file used to build wheels by PEP 517 compliant tools (e.g., when using pip wheel). You can specify everything manually in pyproject.toml, or you can instead rely on the oldest-supported-numpy metapackage. oldest-supported-numpy will specify the correct NumPy version at build time for wheels, taking into account Python version, Python implementation (CPython or PyPy), operating system and hardware platform. It will specify the oldest NumPy version that supports that combination of characteristics. Note: for platforms for which NumPy provides wheels on PyPI, it will be the first version with wheels (even if some older NumPy version happens to build). For conda-forge it\u2019s a little less complicated: there\u2019s dedicated handling for NumPy in build-time and runtime dependencies, so typically this is enough (see here for docs): host:\n  - numpy\nrun:\n  - {{ pin_compatible('numpy') }}\n  Note pip has --no-use-pep517 and --no-build-isolation flags that may ignore pyproject.toml or treat it differently - if users use those flags, they are responsible for installing the correct build dependencies themselves. conda will always use -no-build-isolation; dependencies for conda builds are given in the conda recipe (meta.yaml), the ones in pyproject.toml have no effect. Please do not use setup_requires (it is deprecated and may invoke easy_install).  Because for NumPy you have to care about ABI compatibility, you specify the version with == to the lowest supported version. For your other build dependencies you can probably be looser, however it\u2019s still important to set lower and upper bounds for each dependency. It\u2019s fine to specify either a range or a specific version for a dependency like wheel or setuptools. It\u2019s recommended to set the upper bound of the range to the latest already released version of wheel and setuptools - this prevents future releases from breaking your packages on PyPI.   Runtime dependency & version ranges NumPy itself and many core scientific Python packages have agreed on a schedule for dropping support for old Python and NumPy versions: NEP 29 \u2014 Recommend Python and NumPy version support as a community policy standard. We recommend all packages depending on NumPy to follow the recommendations in NEP 29. For run-time dependencies, you specify the range of versions in install_requires in setup.py (assuming you use numpy.distutils or setuptools to build). Getting the upper bound right for NumPy is slightly tricky. If we don\u2019t set any bound, a too-new version will be pulled in a few years down the line, and NumPy may have deprecated and removed some API that your package depended on by then. On the other hand if you set the upper bound to the newest already-released version, then as soon as a new NumPy version is released there will be no matching version of your package that works with it. What to do here depends on your release frequency. Given that NumPy releases come in a 6-monthly cadence and that features that get deprecated in NumPy should stay around for another two releases, a good upper bound is <1.(xx+3).0 - where xx is the minor version of the latest already-released NumPy. This is safe to do if you release at least once a year. If your own releases are much less frequent, you may set the upper bound a little further into the future - this is a trade-off between a future NumPy version _maybe_ removing something you rely on, and the upper bound being exceeded which _may_ lead to your package being hard to install in combination with other packages relying on the latest NumPy.  Note SciPy has more documentation on how it builds wheels and deals with its build-time and runtime dependencies here. NumPy and SciPy wheel build CI may also be useful as a reference, it can be found here for NumPy and here for SciPy.   \n"}, {"name": "Fortran 77 programs", "path": "f2py/buildtools/index", "type": "F2PY and Build Systems", "text": "F2PY and Build Systems In this section we will cover the various popular build systems and their usage with f2py.  Note As of November 2021 The default build system for F2PY has traditionally been the through the enhanced numpy.distutils module. This module is based on distutils which will be removed in Python 3.12.0 in October 2023; setuptools does not have support for Fortran or F2PY and it is unclear if it will be supported in the future. Alternative methods are thus increasingly more important.   Basic Concepts Building an extension module which includes Python and Fortran consists of:  Fortran source(s) \nOne or more generated files from f2py  A C wrapper file is always created Code with modules require an additional .f90 wrapper   \nfortranobject.{c,h}  Distributed with numpy\n Can be queried via python -c \"import numpy.f2py; print(numpy.f2py.get_include())\"\n   \nNumPy headers  Can be queried via python -c \"import numpy; print(numpy.get_include())\"\n   Python libraries and development headers  Broadly speaking there are three cases which arise when considering the outputs of f2py:  Fortran 77 programs\n\n Input file blah.f\n \nGenerates  blahmodule.c f2pywrappers.f    When no COMMON blocks are present only a C wrapper file is generated. Wrappers are also generated to rewrite assumed shape arrays as automatic arrays.  Fortran 90 programs\n\n Input file blah.f90\n \nGenerates:  blahmodule.c blah-f2pywrappers2.f90    The secondary wrapper is used to handle code which is subdivided into modules. It rewrites assumed shape arrays as automatic arrays.  Signature files\n\n Input file blah.pyf\n \nGenerates:  blahmodule.c \nblah-f2pywrappers2.f90 (occasionally) \nf2pywrappers.f (occasionally)    Signature files .pyf do not signal their language standard via the file extension, they may generate the F90 and F77 specific wrappers depending on their contents; which shifts the burden of checking for generated files onto the build system.    Note The signature file output situation is being reconsidered in issue 20385 .  In theory keeping the above requirements in hand, any build system can be adapted to generate f2py extension modules. Here we will cover a subset of the more popular systems.  Note make has no place in a modern multi-language setup, and so is not discussed further.    Build Systems  \nUsing via numpy.distutils Extensions to distutils   \nUsing via meson Fibonacci Walkthrough (F77) Salient points   \nUsing via cmake Fibonacci Walkthrough (F77)   \nUsing via scikit-build Fibonacci Walkthrough (F77)    \n"}, {"name": "Functional programming", "path": "reference/routines.functional", "type": "Functional programming", "text": "Functional programming  \napply_along_axis(func1d, axis, arr, *args, ...) Apply a function to 1-D slices along the given axis.  \napply_over_axes(func, a, axes) Apply a function repeatedly over multiple axes.  \nvectorize(pyfunc[, otypes, doc, excluded, ...]) Generalized function class.  \nfrompyfunc(func, /, nin, nout, *[, identity]) Takes an arbitrary Python function and returns a NumPy ufunc.  \npiecewise(x, condlist, funclist, *args, **kw) Evaluate a piecewise-defined function.  \n"}, {"name": "generic.__array__()", "path": "reference/generated/numpy.generic.__array__", "type": "numpy.generic.__array__", "text": "numpy.generic.__array__ method   generic.__array__()\n \nsc.__array__(dtype) return 0-dim array from scalar with specified dtype \n\n"}, {"name": "generic.__array_interface__", "path": "reference/generated/numpy.generic.__array_interface__", "type": "numpy.generic.__array_interface__", "text": "numpy.generic.__array_interface__ attribute   generic.__array_interface__\n \nArray protocol: Python side \n\n"}, {"name": "generic.__array_priority__", "path": "reference/generated/numpy.generic.__array_priority__", "type": "numpy.generic.__array_priority__", "text": "numpy.generic.__array_priority__ attribute   generic.__array_priority__\n \nArray priority. \n\n"}, {"name": "generic.__array_struct__", "path": "reference/generated/numpy.generic.__array_struct__", "type": "numpy.generic.__array_struct__", "text": "numpy.generic.__array_struct__ attribute   generic.__array_struct__\n \nArray protocol: struct \n\n"}, {"name": "generic.__array_wrap__()", "path": "reference/generated/numpy.generic.__array_wrap__", "type": "numpy.generic.__array_wrap__", "text": "numpy.generic.__array_wrap__ method   generic.__array_wrap__()\n \nsc.__array_wrap__(obj) return scalar from array \n\n"}, {"name": "generic.__reduce__()", "path": "reference/generated/numpy.generic.__reduce__", "type": "numpy.generic.__reduce__", "text": "numpy.generic.__reduce__ method   generic.__reduce__()\n \nHelper for pickle. \n\n"}, {"name": "generic.__setstate__()", "path": "reference/generated/numpy.generic.__setstate__", "type": "numpy.generic.__setstate__", "text": "numpy.generic.__setstate__ method   generic.__setstate__()\n\n\n"}, {"name": "generic.base", "path": "reference/generated/numpy.generic.base", "type": "numpy.generic.base", "text": "numpy.generic.base attribute   generic.base\n \nScalar attribute identical to the corresponding array attribute. Please see ndarray.base. \n\n"}, {"name": "generic.byteswap()", "path": "reference/generated/numpy.generic.byteswap", "type": "numpy.generic.byteswap", "text": "numpy.generic.byteswap method   generic.byteswap()\n \nScalar method identical to the corresponding array attribute. Please see ndarray.byteswap. \n\n"}, {"name": "generic.data", "path": "reference/generated/numpy.generic.data", "type": "numpy.generic.data", "text": "numpy.generic.data attribute   generic.data\n \nPointer to start of data. \n\n"}, {"name": "generic.dtype", "path": "reference/generated/numpy.generic.dtype", "type": "numpy.generic.dtype", "text": "numpy.generic.dtype attribute   generic.dtype\n \nGet array data-descriptor. \n\n"}, {"name": "generic.flags", "path": "reference/generated/numpy.generic.flags", "type": "numpy.generic.flags", "text": "numpy.generic.flags attribute   generic.flags\n \nThe integer value of flags. \n\n"}, {"name": "generic.flat", "path": "reference/generated/numpy.generic.flat", "type": "numpy.generic.flat", "text": "numpy.generic.flat attribute   generic.flat\n \nA 1-D view of the scalar. \n\n"}, {"name": "generic.imag", "path": "reference/generated/numpy.generic.imag", "type": "numpy.generic.imag", "text": "numpy.generic.imag attribute   generic.imag\n \nThe imaginary part of the scalar. \n\n"}, {"name": "generic.itemsize", "path": "reference/generated/numpy.generic.itemsize", "type": "numpy.generic.itemsize", "text": "numpy.generic.itemsize attribute   generic.itemsize\n \nThe length of one element in bytes. \n\n"}, {"name": "generic.ndim", "path": "reference/generated/numpy.generic.ndim", "type": "numpy.generic.ndim", "text": "numpy.generic.ndim attribute   generic.ndim\n \nThe number of array dimensions. \n\n"}, {"name": "generic.real", "path": "reference/generated/numpy.generic.real", "type": "numpy.generic.real", "text": "numpy.generic.real attribute   generic.real\n \nThe real part of the scalar. \n\n"}, {"name": "generic.setflags()", "path": "reference/generated/numpy.generic.setflags", "type": "numpy.generic.setflags", "text": "numpy.generic.setflags method   generic.setflags()\n \nScalar method identical to the corresponding array attribute. Please see ndarray.setflags. \n\n"}, {"name": "generic.shape", "path": "reference/generated/numpy.generic.shape", "type": "numpy.generic.shape", "text": "numpy.generic.shape attribute   generic.shape\n \nTuple of array dimensions. \n\n"}, {"name": "generic.size", "path": "reference/generated/numpy.generic.size", "type": "numpy.generic.size", "text": "numpy.generic.size attribute   generic.size\n \nThe number of elements in the gentype. \n\n"}, {"name": "generic.squeeze()", "path": "reference/generated/numpy.generic.squeeze", "type": "numpy.generic.squeeze", "text": "numpy.generic.squeeze method   generic.squeeze()\n \nScalar method identical to the corresponding array attribute. Please see ndarray.squeeze. \n\n"}, {"name": "generic.strides", "path": "reference/generated/numpy.generic.strides", "type": "numpy.generic.strides", "text": "numpy.generic.strides attribute   generic.strides\n \nTuple of bytes steps in each dimension. \n\n"}, {"name": "generic.T", "path": "reference/generated/numpy.generic.t", "type": "numpy.generic.T", "text": "numpy.generic.T attribute   generic.T\n \nScalar attribute identical to the corresponding array attribute. Please see ndarray.T. \n\n"}, {"name": "Get the local copy of the code", "path": "dev/gitwash/following_latest", "type": "Development", "text": "Get the local copy of the code From the command line: git clone git://github.com/numpy/numpy.git\n You now have a copy of the code tree in the new numpy directory. If this doesn\u2019t work you can try the alternative read-only url: git clone https://github.com/numpy/numpy.git\n\n"}, {"name": "get_build_temp_dir()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.get_build_temp_dir", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": "  get_build_temp_dir()[source]\n \nReturn a path to a temporary directory where temporary files should be placed. \n"}, {"name": "get_config_cmd()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.get_config_cmd", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": "  get_config_cmd()[source]\n \nReturns the numpy.distutils config command instance. \n"}, {"name": "get_distribution()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.get_distribution", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": "  get_distribution()[source]\n \nReturn the distutils distribution object for self. \n"}, {"name": "get_info()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.get_info", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": "  get_info(*names)[source]\n \nGet resources information. Return information (from system_info.get_info) for all of the names in the argument list in a single dictionary. \n"}, {"name": "get_subpackage()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.get_subpackage", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": "  get_subpackage(subpackage_name, subpackage_path=None, parent_name=None, caller_level=1)[source]\n \nReturn list of subpackage configurations.  Parameters \n \nsubpackage_namestr or None\n\n\nName of the subpackage to get the configuration. \u2018*\u2019 in subpackage_name is handled as a wildcard.  \nsubpackage_pathstr\n\n\nIf None, then the path is assumed to be the local path plus the subpackage_name. If a setup.py file is not found in the subpackage_path, then a default configuration is used.  \nparent_namestr\n\n\nParent name.     \n"}, {"name": "get_version()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.get_version", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": "  get_version(version_file=None, version_variable=None)[source]\n \nTry to get version string of a package. Return a version string of the current package or None if the version information could not be detected. Notes This method scans files named __version__.py, <packagename>_version.py, version.py, and __svn_version__.py for string variables version, __version__, and <packagename>_version, until a version number is found. \n"}, {"name": "Git configuration", "path": "dev/gitwash/configure_git", "type": "Development", "text": "Git configuration  Overview Your personal git configurations are saved in the .gitconfig file in your home directory. Here is an example .gitconfig file: [user]\n        name = Your Name\n        email = you@yourdomain.example.com\n\n[alias]\n        ci = commit -a\n        co = checkout\n        st = status -a\n        stat = status -a\n        br = branch\n        wdiff = diff --color-words\n\n[core]\n        editor = vim\n\n[merge]\n        summary = true\n You can edit this file directly or you can use the git config --global command: git config --global user.name \"Your Name\"\ngit config --global user.email you@yourdomain.example.com\ngit config --global alias.ci \"commit -a\"\ngit config --global alias.co checkout\ngit config --global alias.st \"status -a\"\ngit config --global alias.stat \"status -a\"\ngit config --global alias.br branch\ngit config --global alias.wdiff \"diff --color-words\"\ngit config --global core.editor vim\ngit config --global merge.summary true\n To set up on another computer, you can copy your ~/.gitconfig file, or run the commands above.   In detail  user.name and user.email It is good practice to tell git who you are, for labeling any changes you make to the code. The simplest way to do this is from the command line: git config --global user.name \"Your Name\"\ngit config --global user.email you@yourdomain.example.com\n This will write the settings into your git configuration file, which should now contain a user section with your name and email: [user]\n      name = Your Name\n      email = you@yourdomain.example.com\n Of course you\u2019ll need to replace Your Name and you@yourdomain.example.com with your actual name and email address.   Aliases You might well benefit from some aliases to common commands. For example, you might well want to be able to shorten git checkout to git co. Or you may want to alias git diff --color-words (which gives a nicely formatted output of the diff) to git wdiff The following git config --global commands: git config --global alias.ci \"commit -a\"\ngit config --global alias.co checkout\ngit config --global alias.st \"status -a\"\ngit config --global alias.stat \"status -a\"\ngit config --global alias.br branch\ngit config --global alias.wdiff \"diff --color-words\"\n will create an alias section in your .gitconfig file with contents like this: [alias]\n        ci = commit -a\n        co = checkout\n        st = status -a\n        stat = status -a\n        br = branch\n        wdiff = diff --color-words\n   Editor You may also want to make sure that your editor of choice is used git config --global core.editor vim\n   Merging To enforce summaries when doing merges (~/.gitconfig file again): [merge]\n   log = true\n Or from the command line: git config --global merge.log true\n  \n"}, {"name": "Git for development", "path": "dev/gitwash/index", "type": "Development", "text": "Git for development These pages describe a general git and github workflow. This is not a comprehensive git reference. It\u2019s tailored to the github hosting service. You may well find better or quicker ways of getting stuff done with git, but these should get you started. For general resources for learning git see Additional Git Resources. Have a look at the github install help pages available from github help Contents:  Install git Get the local copy of the code Updating the code \nSetting up git for NumPy development Install git Create a GitHub account Create a NumPy fork Look it over Optional: set up SSH keys to avoid passwords   \nGit configuration Overview In detail   Two and three dots in difference specs \nAdditional Git Resources Tutorials and summaries Advanced git workflow Manual pages online   \n"}, {"name": "Global State", "path": "reference/global_state", "type": "Global State", "text": "Global State NumPy has a few import-time, compile-time, or runtime options which change the global behaviour. Most of these are related to performance or for debugging purposes and will not be interesting to the vast majority of users.  Performance-Related Options  Number of Threads used for Linear Algebra NumPy itself is normally intentionally limited to a single thread during function calls, however it does support multiple Python threads running at the same time. Note that for performant linear algebra NumPy uses a BLAS backend such as OpenBLAS or MKL, which may use multiple threads that may be controlled by environment variables such as OMP_NUM_THREADS depending on what is used. One way to control the number of threads is the package threadpoolctl   Madvise Hugepage on Linux When working with very large arrays on modern Linux kernels, you can experience a significant speedup when transparent hugepage is used. The current system policy for transparent hugepages can be seen by: cat /sys/kernel/mm/transparent_hugepage/enabled\n When set to madvise NumPy will typically use hugepages for a performance boost. This behaviour can be modified by setting the environment variable: NUMPY_MADVISE_HUGEPAGE=0\n or setting it to 1 to always enable it. When not set, the default is to use madvise on Kernels 4.6 and newer. These kernels presumably experience a large speedup with hugepage support. This flag is checked at import time.    Interoperability-Related Options The array function protocol which allows array-like objects to hook into the NumPy API is currently enabled by default. This option exists since NumPy 1.16 and is enabled by default since NumPy 1.17. It can be disabled using: NUMPY_EXPERIMENTAL_ARRAY_FUNCTION=0\n See also numpy.class.__array_function__ for more information. This flag is checked at import time.   Debugging-Related Options  Relaxed Strides Checking The compile-time environment variables: NPY_RELAXED_STRIDES_DEBUG=0\nNPY_RELAXED_STRIDES_CHECKING=1\n control how NumPy reports contiguity for arrays. The default that it is enabled and the debug mode is disabled. This setting should always be enabled. Setting the debug option can be interesting for testing code written in C which iterates through arrays that may or may not be contiguous in memory. Most users will have no reason to change these; for details see the memory layout documentation.   Warn if no memory allocation policy when deallocating data Some users might pass ownership of the data pointer to the ndarray by setting the OWNDATA flag. If they do this without setting (manually) a memory allocation policy, the default will be to call free. If NUMPY_WARN_IF_NO_MEM_POLICY is set to \"1\", a RuntimeWarning will be emitted. A better alternative is to use a PyCapsule with a deallocator and set the ndarray.base.  \n"}, {"name": "have_f77c()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.have_f77c", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": "  have_f77c()[source]\n \nCheck for availability of Fortran 77 compiler. Use it inside source generating function to ensure that setup distribution instance has been initialized. Notes True if a Fortran 77 compiler is available (because a simple Fortran 77 code was able to be compiled successfully). \n"}, {"name": "have_f90c()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.have_f90c", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": "  have_f90c()[source]\n \nCheck for availability of Fortran 90 compiler. Use it inside source generating function to ensure that setup distribution instance has been initialized. Notes True if a Fortran 90 compiler is available (because a simple Fortran 90 code was able to be compiled successfully) \n"}, {"name": "Hermite Series, \u201cPhysicists\u201d (numpy.polynomial.hermite)", "path": "reference/routines.polynomials.hermite", "type": "Hermite Series, \u201cPhysicists\u201d ( \n        \n         numpy.polynomial.hermite\n        \n        )", "text": "Hermite Series, \u201cPhysicists\u201d (numpy.polynomial.hermite) This module provides a number of objects (mostly functions) useful for dealing with Hermite series, including a Hermite class that encapsulates the usual arithmetic operations. (General information on how this module represents and works with such polynomials is in the docstring for its \u201cparent\u201d sub-package, numpy.polynomial).  Classes  \nHermite(coef[, domain, window]) An Hermite series class.     Constants  \nhermdomain An array object represents a multidimensional, homogeneous array of fixed-size items.  \nhermzero An array object represents a multidimensional, homogeneous array of fixed-size items.  \nhermone An array object represents a multidimensional, homogeneous array of fixed-size items.  \nhermx An array object represents a multidimensional, homogeneous array of fixed-size items.     Arithmetic  \nhermadd(c1, c2) Add one Hermite series to another.  \nhermsub(c1, c2) Subtract one Hermite series from another.  \nhermmulx(c) Multiply a Hermite series by x.  \nhermmul(c1, c2) Multiply one Hermite series by another.  \nhermdiv(c1, c2) Divide one Hermite series by another.  \nhermpow(c, pow[, maxpower]) Raise a Hermite series to a power.  \nhermval(x, c[, tensor]) Evaluate an Hermite series at points x.  \nhermval2d(x, y, c) Evaluate a 2-D Hermite series at points (x, y).  \nhermval3d(x, y, z, c) Evaluate a 3-D Hermite series at points (x, y, z).  \nhermgrid2d(x, y, c) Evaluate a 2-D Hermite series on the Cartesian product of x and y.  \nhermgrid3d(x, y, z, c) Evaluate a 3-D Hermite series on the Cartesian product of x, y, and z.     Calculus  \nhermder(c[, m, scl, axis]) Differentiate a Hermite series.  \nhermint(c[, m, k, lbnd, scl, axis]) Integrate a Hermite series.     Misc Functions  \nhermfromroots(roots) Generate a Hermite series with given roots.  \nhermroots(c) Compute the roots of a Hermite series.  \nhermvander(x, deg) Pseudo-Vandermonde matrix of given degree.  \nhermvander2d(x, y, deg) Pseudo-Vandermonde matrix of given degrees.  \nhermvander3d(x, y, z, deg) Pseudo-Vandermonde matrix of given degrees.  \nhermgauss(deg) Gauss-Hermite quadrature.  \nhermweight(x) Weight function of the Hermite polynomials.  \nhermcompanion(c) Return the scaled companion matrix of c.  \nhermfit(x, y, deg[, rcond, full, w]) Least squares fit of Hermite series to data.  \nhermtrim(c[, tol]) Remove \"small\" \"trailing\" coefficients from a polynomial.  \nhermline(off, scl) Hermite series whose graph is a straight line.  \nherm2poly(c) Convert a Hermite series to a polynomial.  \npoly2herm(pol) Convert a polynomial to a Hermite series.     See also numpy.polynomial \n"}, {"name": "HermiteE Series, \u201cProbabilists\u201d (numpy.polynomial.hermite_e)", "path": "reference/routines.polynomials.hermite_e", "type": "HermiteE Series, \u201cProbabilists\u201d ( \n        \n         numpy.polynomial.hermite_e\n        \n        )", "text": "HermiteE Series, \u201cProbabilists\u201d (numpy.polynomial.hermite_e) This module provides a number of objects (mostly functions) useful for dealing with Hermite_e series, including a HermiteE class that encapsulates the usual arithmetic operations. (General information on how this module represents and works with such polynomials is in the docstring for its \u201cparent\u201d sub-package, numpy.polynomial).  Classes  \nHermiteE(coef[, domain, window]) An HermiteE series class.     Constants  \nhermedomain An array object represents a multidimensional, homogeneous array of fixed-size items.  \nhermezero An array object represents a multidimensional, homogeneous array of fixed-size items.  \nhermeone An array object represents a multidimensional, homogeneous array of fixed-size items.  \nhermex An array object represents a multidimensional, homogeneous array of fixed-size items.     Arithmetic  \nhermeadd(c1, c2) Add one Hermite series to another.  \nhermesub(c1, c2) Subtract one Hermite series from another.  \nhermemulx(c) Multiply a Hermite series by x.  \nhermemul(c1, c2) Multiply one Hermite series by another.  \nhermediv(c1, c2) Divide one Hermite series by another.  \nhermepow(c, pow[, maxpower]) Raise a Hermite series to a power.  \nhermeval(x, c[, tensor]) Evaluate an HermiteE series at points x.  \nhermeval2d(x, y, c) Evaluate a 2-D HermiteE series at points (x, y).  \nhermeval3d(x, y, z, c) Evaluate a 3-D Hermite_e series at points (x, y, z).  \nhermegrid2d(x, y, c) Evaluate a 2-D HermiteE series on the Cartesian product of x and y.  \nhermegrid3d(x, y, z, c) Evaluate a 3-D HermiteE series on the Cartesian product of x, y, and z.     Calculus  \nhermeder(c[, m, scl, axis]) Differentiate a Hermite_e series.  \nhermeint(c[, m, k, lbnd, scl, axis]) Integrate a Hermite_e series.     Misc Functions  \nhermefromroots(roots) Generate a HermiteE series with given roots.  \nhermeroots(c) Compute the roots of a HermiteE series.  \nhermevander(x, deg) Pseudo-Vandermonde matrix of given degree.  \nhermevander2d(x, y, deg) Pseudo-Vandermonde matrix of given degrees.  \nhermevander3d(x, y, z, deg) Pseudo-Vandermonde matrix of given degrees.  \nhermegauss(deg) Gauss-HermiteE quadrature.  \nhermeweight(x) Weight function of the Hermite_e polynomials.  \nhermecompanion(c) Return the scaled companion matrix of c.  \nhermefit(x, y, deg[, rcond, full, w]) Least squares fit of Hermite series to data.  \nhermetrim(c[, tol]) Remove \"small\" \"trailing\" coefficients from a polynomial.  \nhermeline(off, scl) Hermite series whose graph is a straight line.  \nherme2poly(c) Convert a Hermite series to a polynomial.  \npoly2herme(pol) Convert a polynomial to a Hermite series.     See also numpy.polynomial \n"}, {"name": "How to write a NumPy how-to", "path": "user/how-to-how-to", "type": "User Guide", "text": "How to write a NumPy how-to How-tos get straight to the point \u2013 they  answer a focused question, or narrow a broad question into focused questions that the user can choose among.   A stranger has asked for directions\u2026 \u201cI need to refuel my car.\u201d   Give a brief but explicit answer  \u201cThree kilometers/miles, take a right at Hayseed Road, it\u2019s on your left.\u201d  Add helpful details for newcomers (\u201cHayseed Road\u201d, even though it\u2019s the only turnoff at three km/mi). But not irrelevant ones:  Don\u2019t also give directions from Route 7. Don\u2019t explain why the town has only one filling station.  If there\u2019s related background (tutorial, explanation, reference, alternative approach), bring it to the user\u2019s attention with a link (\u201cDirections from Route 7,\u201d \u201cWhy so few filling stations?\u201d).   Delegate  \u201cThree km/mi, take a right at Hayseed Road, follow the signs.\u201d  If the information is already documented and succinct enough for a how-to, just link to it, possibly after an introduction (\u201cThree km/mi, take a right\u201d).   If the question is broad, narrow and redirect it \u201cI want to see the sights.\u201d The See the sights how-to should link to a set of narrower how-tos:  Find historic buildings Find scenic lookouts Find the town center  and these might in turn link to still narrower how-tos \u2013 so the town center page might link to  Find the court house Find city hall  By organizing how-tos this way, you not only display the options for people who need to narrow their question, you also have provided answers for users who start with narrower questions (\u201cI want to see historic buildings,\u201d \u201cWhich way to city hall?\u201d).   If there are many steps, break them up If a how-to has many steps:  Consider breaking a step out into an individual how-to and linking to it. Include subheadings. They help readers grasp what\u2019s coming and return where they left off.    Why write how-tos when there\u2019s Stack Overflow, Reddit, Gitter\u2026?  We have authoritative answers. How-tos make the site less forbidding to non-experts. How-tos bring people into the site and help them discover other information that\u2019s here . Creating how-tos helps us see NumPy usability through new eyes.    Aren\u2019t how-tos and tutorials the same thing? People use the terms \u201chow-to\u201d and \u201ctutorial\u201d interchangeably, but we draw a distinction, following Daniele Procida\u2019s taxonomy of documentation. Documentation needs to meet users where they are. How-tos offer get-it-done information; the user wants steps to copy and doesn\u2019t necessarily want to understand NumPy. Tutorials are warm-fuzzy information; the user wants a feel for some aspect of NumPy (and again, may or may not care about deeper knowledge). We distinguish both tutorials and how-tos from Explanations, which are deep dives intended to give understanding rather than immediate assistance, and References, which give complete, authoritative data on some concrete part of NumPy (like its API) but aren\u2019t obligated to paint a broader picture. For more on tutorials, see Learn to write a NumPy tutorial   Is this page an example of a how-to? Yes \u2013 until the sections with question-mark headings; they explain rather than giving directions. In a how-to, those would be links. \n"}, {"name": "I/O with NumPy", "path": "user/basics.io", "type": "User Guide", "text": "I/O with NumPy  \nImporting data with genfromtxt Defining the input Splitting the lines into columns Skipping lines and choosing columns Choosing the data type Setting the names Tweaking the conversion Shortcut functions   \n"}, {"name": "include statements", "path": "f2py/signature-file", "type": "Signature file", "text": "Signature file The syntax specification for signature files (.pyf files) is modeled on the Fortran 90/95 language specification. Almost all Fortran 90/95 standard constructs are understood, both in free and fixed format (recall that Fortran 77 is a subset of Fortran 90/95). F2PY introduces some extensions to the Fortran 90/95 language specification that help in the design of the Fortran to Python interface, making it more \u201cPythonic\u201d. Signature files may contain arbitrary Fortran code so that any Fortran 90/95 codes can be treated as signature files. F2PY silently ignores Fortran constructs that are irrelevant for creating the interface. However, this also means that syntax errors are not caught by F2PY and will only be caught when the library is built. In general, the contents of the signature files are case-sensitive. When scanning Fortran codes to generate a signature file, F2PY lowers all cases automatically except in multi-line blocks or when the --no-lower option is used. The syntax of signature files is presented below.  Python module block A signature file may contain one (recommended) or more python\nmodule blocks. The python module block describes the contents of a Python/C extension module <modulename>module.c that F2PY generates.  Warning Exception: if <modulename> contains a substring __user__, then the corresponding python module block describes the signatures of call-back functions (see Call-back arguments).  A python module block has the following structure: python module <modulename>\n  [<usercode statement>]...\n  [\n  interface\n    <usercode statement>\n    <Fortran block data signatures>\n    <Fortran/C routine signatures>\n  end [interface]\n  ]...\n  [\n  interface\n    module <F90 modulename>\n      [<F90 module data type declarations>]\n      [<F90 module routine signatures>]\n    end [module [<F90 modulename>]]\n  end [interface]\n  ]...\nend [python module [<modulename>]]\n Here brackets [] indicate an optional section, dots ... indicate one or more of a previous section. So, []... is to be read as zero or more of a previous section.   Fortran/C routine signatures The signature of a Fortran routine has the following structure: [<typespec>] function | subroutine <routine name> \\\n              [ ( [<arguments>] ) ] [ result ( <entityname> ) ]\n  [<argument/variable type declarations>]\n  [<argument/variable attribute statements>]\n  [<use statements>]\n  [<common block statements>]\n  [<other statements>]\nend [ function | subroutine [<routine name>] ]\n From a Fortran routine signature F2PY generates a Python/C extension function that has the following signature: def <routine name>(<required arguments>[,<optional arguments>]):\n     ...\n     return <return variables>\n The signature of a Fortran block data has the following structure: block data [ <block data name> ]\n  [<variable type declarations>]\n  [<variable attribute statements>]\n  [<use statements>]\n  [<common block statements>]\n  [<include statements>]\nend [ block data [<block data name>] ]\n   Type declarations The definition of the <argument/variable type declaration> part is <typespec> [ [<attrspec>] :: ] <entitydecl>\n where <typespec> := byte | character [<charselector>]\n           | complex [<kindselector>] | real [<kindselector>]\n           | double complex | double precision\n           | integer [<kindselector>] | logical [<kindselector>]\n\n<charselector> := * <charlen>\n               | ( [len=] <len> [ , [kind=] <kind>] )\n               | ( kind= <kind> [ , len= <len> ] )\n<kindselector> := * <intlen> | ( [kind=] <kind> )\n\n<entitydecl> := <name> [ [ * <charlen> ] [ ( <arrayspec> ) ]\n                      | [ ( <arrayspec> ) ] * <charlen> ]\n                     | [ / <init_expr> / | = <init_expr> ] \\\n                       [ , <entitydecl> ]\n and  \n<attrspec> is a comma separated list of attributes; \n<arrayspec> is a comma separated list of dimension bounds; \n<init_expr> is a C expression; \n<intlen> may be negative integer for integer type specifications. In such cases integer*<negintlen> represents unsigned C integers;  If an argument has no <argument type declaration>, its type is determined by applying implicit rules to its name.   Statements  Attribute statements  The <argument/variable attribute statement> is <argument/variable type declaration> without <typespec>. In addition, in an attribute statement one cannot use other attributes, also <entitydecl> can be only a list of names.    Use statements  \nThe definition of the <use statement> part is use <modulename> [ , <rename_list> | , ONLY : <only_list> ]\n where <rename_list> := <local_name> => <use_name> [ , <rename_list> ]\n  Currently F2PY uses use statement only for linking call-back modules and external arguments (call-back functions), see Call-back arguments.    Common block statements  \nThe definition of the <common block statement> part is common / <common name> / <shortentitydecl>\n where <shortentitydecl> := <name> [ ( <arrayspec> ) ] [ , <shortentitydecl> ]\n  If a python module block contains two or more common blocks with the same name, the variables from the additional declarations are appended. The types of variables in <shortentitydecl> are defined using <argument type declarations>. Note that the corresponding <argument type declarations> may contain array specifications; then these need not be specified in <shortentitydecl>.    Other statements  \nThe <other statement> part refers to any other Fortran language constructs that are not described above. F2PY ignores most of them except the following:  \ncall statements and function calls of external arguments (more details?);  \n \ninclude statements\n\ninclude '<filename>'\ninclude \"<filename>\"\n If a file <filename> does not exist, the include statement is ignored. Otherwise, the file <filename> is included to a signature file. include statements can be used in any part of a signature file, also outside the Fortran/C routine signature blocks.    \n \nimplicit statements\n\nimplicit none\nimplicit <list of implicit maps>\n where <implicit map> := <typespec> ( <list of letters or range of letters> )\n Implicit rules are used to determine the type specification of a variable (from the first-letter of its name) if the variable is not defined using <variable type declaration>. Default implicit rules are given by: implicit real (a-h,o-z,$_), integer (i-m)\n    \n \nentry statements\n\nentry <entry name> [([<arguments>])]\n F2PY generates wrappers for all entry names using the signature of the routine block.  Note The entry statement can be used to describe the signature of an arbitrary subroutine or function allowing F2PY to generate a number of wrappers from only one routine block signature. There are few restrictions while doing this: fortranname cannot be used, callstatement and callprotoargument can be used only if they are valid for all entry routines, etc.          F2PY statements In addition, F2PY introduces the following statements:  threadsafe\n\nUses a Py_BEGIN_ALLOW_THREADS .. Py_END_ALLOW_THREADS block around the call to Fortran/C function.  callstatement <C-expr|multi-line block>\n\nReplaces the F2PY generated call statement to Fortran/C function with <C-expr|multi-line block>. The wrapped Fortran/C function is available as (*f2py_func). To raise an exception, set f2py_success = 0 in <C-expr|multi-line\nblock>.  callprotoargument <C-typespecs>\n\nWhen the callstatement statement is used then F2PY may not generate proper prototypes for Fortran/C functions (because <C-expr> may contain any function calls and F2PY has no way to determine what should be the proper prototype). With this statement you can explicitly specify the arguments of the corresponding prototype: extern <return type> FUNC_F(<routine name>,<ROUTINE NAME>)(<callprotoargument>);\n  fortranname [<actual Fortran/C routine name>]\n\nF2PY allows for the use of an arbitrary <routine name> for a given Fortran/C function. Then this statement is used for the <actual\nFortran/C routine name>. If fortranname statement is used without <actual Fortran/C routine name> then a dummy wrapper is generated.  usercode <multi-line block>\n\nWhen this is used inside a python module block, the given C code will be inserted to generated C/API source just before wrapper function definitions. Here you can define arbitrary C functions to be used for the initialization of optional arguments. For example, if usercode is used twice inside python module block then the second multi-line block is inserted after the definition of the external routines. When used inside <routine signature>, then the given C code will be inserted into the corresponding wrapper function just after the declaration of variables but before any C statements. So, the usercode follow-up can contain both declarations and C statements. When used inside the first interface block, then the given C code will be inserted at the end of the initialization function of the extension module. This is how the extension modules dictionary can be modified and has many use-cases; for example, to define additional variables.  pymethoddef <multiline block>\n\nThis is a multi-line block which will be inserted into the definition of a module methods PyMethodDef-array. It must be a comma-separated list of C arrays (see Extending and Embedding Python documentation for details). pymethoddef statement can be used only inside python\nmodule block.      Attributes The following attributes are used by F2PY:  optional\n\nThe corresponding argument is moved to the end of <optional\narguments> list. A default value for an optional argument can be specified via <init_expr>, see the entitydecl definition.  Note  The default value must be given as a valid C expression. Whenever <init_expr> is used, optional attribute is set automatically by F2PY. For an optional array argument, all its dimensions must be bounded.    required\n\nThe corresponding argument with this attribute considered mandatory. This is the default. required should only be specified if there is a need to disable the automatic optional setting when <init_expr> is used. If a Python None object is used as a required argument, the argument is treated as optional. That is, in the case of array argument, the memory is allocated. If <init_expr> is given, then the corresponding initialization is carried out.  dimension(<arrayspec>)\n\nThe corresponding variable is considered as an array with dimensions given in <arrayspec>.  intent(<intentspec>)\n\nThis specifies the \u201cintention\u201d of the corresponding argument. <intentspec> is a comma separated list of the following keys:  \n in\n\nThe corresponding argument is considered to be input-only. This means that the value of the argument is passed to a Fortran/C function and that the function is expected to not change the value of this argument.    \n inout\n The corresponding argument is marked for input/output or as an in situ output argument. intent(inout) arguments can be only \u201ccontiguous\u201d NumPy arrays with proper type and size. Here \u201ccontiguous\u201d can be either in the Fortran or C sense. The latter coincides with the default contiguous concept used in NumPy and is effective only if intent(c) is used. F2PY assumes Fortran contiguous arguments by default.  Note Using intent(inout) is generally not recommended, use intent(in,out) instead.  See also the intent(inplace) attribute.    \n inplace\n\nThe corresponding argument is considered to be an input/output or in situ output argument. intent(inplace) arguments must be NumPy arrays of a proper size. If the type of an array is not \u201cproper\u201d or the array is non-contiguous then the array will be modified in-place to fix the type and make it contiguous.  Note Using intent(inplace) is generally not recommended either. For example, when slices have been taken from an intent(inplace) argument then after in-place changes, the data pointers for the slices may point to an unallocated memory area.     \n out\n\nThe corresponding argument is considered to be a return variable. It is appended to the <returned variables> list. Using intent(out) sets intent(hide) automatically, unless intent(in) or intent(inout) are specified as well. By default, returned multidimensional arrays are Fortran-contiguous. If intent(c) attribute is used, then the returned multidimensional arrays are C-contiguous.    \n hide\n\nThe corresponding argument is removed from the list of required or optional arguments. Typically intent(hide) is used with intent(out) or when <init_expr> completely determines the value of the argument like in the following example: integer intent(hide),depend(a) :: n = len(a)\nreal intent(in),dimension(n) :: a\n    \n c\n\nThe corresponding argument is treated as a C scalar or C array argument. For the case of a scalar argument, its value is passed to a C function as a C scalar argument (recall that Fortran scalar arguments are actually C pointer arguments). For array arguments, the wrapper function is assumed to treat multidimensional arrays as C-contiguous arrays. There is no need to use intent(c) for one-dimensional arrays, irrespective of whether the wrapped function is in Fortran or C. This is because the concepts of Fortran- and C contiguity overlap in one-dimensional cases. If intent(c) is used as a statement but without an entity declaration list, then F2PY adds the intent(c) attribute to all arguments. Also, when wrapping C functions, one must use intent(c) attribute for <routine name> in order to disable Fortran specific F_FUNC(..,..) macros.    \n cache\n\nThe corresponding argument is treated as junk memory. No Fortran nor C contiguity checks are carried out. Using intent(cache) makes sense only for array arguments, also in conjunction with intent(hide) or optional attributes.    \n copy\n\nEnsures that the original contents of intent(in) argument is preserved. Typically used with the intent(in,out) attribute. F2PY creates an optional argument overwrite_<argument name> with the default value 0.    \n overwrite\n\nThis indicates that the original contents of the intent(in) argument may be altered by the Fortran/C function. F2PY creates an optional argument overwrite_<argument name> with the default value 1.    \n out=<new name>\n\nReplaces the returned name with <new name> in the __doc__ string of the wrapper function.    \n callback\n\nConstructs an external function suitable for calling Python functions from Fortran. intent(callback) must be specified before the corresponding external statement. If the \u2018argument\u2019 is not in the argument list then it will be added to Python wrapper but only by initializing an external function.  Note Use intent(callback) in situations where the Fortran/C code assumes that the user implemented a function with a given prototype and linked it to an executable. Don\u2019t use intent(callback) if the function appears in the argument list of a Fortran routine.  With intent(hide) or optional attributes specified and using a wrapper function without specifying the callback argument in the argument list; then the call-back function is assumed to be found in the namespace of the F2PY generated extension module where it can be set as a module attribute by a user.    \n aux\n\nDefines an auxiliary C variable in the F2PY generated wrapper function. Useful to save parameter values so that they can be accessed in initialization expressions for other variables.  Note intent(aux) silently implies intent(c).      The following rules apply:  \nIf none of intent(in | inout | out | hide) are specified, intent(in) is assumed.  \nintent(in,inout) is intent(in); \nintent(in,hide) or intent(inout,hide) is intent(hide); \nintent(out) is intent(out,hide) unless intent(in) or intent(inout) is specified.   \nIf intent(copy) or intent(overwrite) is used, then an additional optional argument is introduced with a name overwrite_<argument name> and a default value 0 or 1, respectively.  \nintent(inout,inplace) is intent(inplace); \nintent(in,inplace) is intent(inplace); \nintent(hide) disables optional and required.     check([<C-booleanexpr>])\n\nPerforms a consistency check on the arguments by evaluating <C-booleanexpr>; if <C-booleanexpr> returns 0, an exception is raised.  Note If check(..) is not used then F2PY automatically generates a few standard checks (e.g. in a case of an array argument, it checks for the proper shape and size). Use check() to disable checks generated by F2PY.   depend([<names>])\n\nThis declares that the corresponding argument depends on the values of variables in the <names> list. For example, <init_expr> may use the values of other arguments. Using information given by depend(..) attributes, F2PY ensures that arguments are initialized in a proper order. If the depend(..) attribute is not used then F2PY determines dependence relations automatically. Use depend() to disable the dependence relations generated by F2PY. When you edit dependence relations that were initially generated by F2PY, be careful not to break the dependence relations of other relevant variables. Another thing to watch out for is cyclic dependencies. F2PY is able to detect cyclic dependencies when constructing wrappers and it complains if any are found.  allocatable\n\nThe corresponding variable is a Fortran 90 allocatable array defined as Fortran 90 module data.    external\n\nThe corresponding argument is a function provided by user. The signature of this call-back function can be defined  in __user__ module block, or by demonstrative (or real, if the signature file is a real Fortran code) call in the <other statements> block.  For example, F2PY generates from: external cb_sub, cb_fun\ninteger n\nreal a(n),r\ncall cb_sub(a,n)\nr = cb_fun(4)\n the following call-back signatures: subroutine cb_sub(a,n)\n    real dimension(n) :: a\n    integer optional,check(len(a)>=n),depend(a) :: n=len(a)\nend subroutine cb_sub\nfunction cb_fun(e_4_e) result (r)\n    integer :: e_4_e\n    real :: r\nend function cb_fun\n The corresponding user-provided Python function are then: def cb_sub(a,[n]):\n    ...\n    return\ndef cb_fun(e_4_e):\n    ...\n    return r\n See also the intent(callback) attribute.  parameter\n\nThis indicates that the corresponding variable is a parameter and it must have a fixed value. F2PY replaces all parameter occurrences by their corresponding values.     Extensions  F2PY directives The F2PY directives allow using F2PY signature file constructs in Fortran 77/90 source codes. With this feature one can (almost) completely skip the intermediate signature file generation and apply F2PY directly to Fortran source codes. F2PY directives have the following form: <comment char>f2py ...\n where allowed comment characters for fixed and free format Fortran codes are cC*!# and !, respectively. Everything that follows <comment char>f2py is ignored by a compiler but read by F2PY as a normal non-comment Fortran line:  Note When F2PY finds a line with F2PY directive, the directive is first replaced by 5 spaces and then the line is reread.  For fixed format Fortran codes, <comment char> must be at the first column of a file, of course. For free format Fortran codes, the F2PY directives can appear anywhere in a file.   C expressions C expressions are used in the following parts of signature files:  \n<init_expr> for variable initialization; \n<C-booleanexpr> of the check attribute; \n<arrayspec> of the dimension attribute; \ncallstatement statement, here also a C multi-line block can be used.  A C expression may contain:  standard C constructs; functions from math.h and Python.h; variables from the argument list, presumably initialized before according to given dependence relations; \nthe following CPP macros:  \nrank(<name>) Returns the rank of an array <name>. \nshape(<name>,<n>) Returns the <n>-th dimension of an array <name>. \nlen(<name>) Returns the length of an array <name>. \nsize(<name>) Returns the size of an array <name>. \nslen(<name>) Returns the length of a string <name>.    For initializing an array <array name>, F2PY generates a loop over all indices and dimensions that executes the following pseudo-statement: <array name>(_i[0],_i[1],...) = <init_expr>;\n where _i[<i>] refers to the <i>-th index value and that runs from 0 to shape(<array name>,<i>)-1. For example, a function myrange(n) generated from the following signature subroutine myrange(a,n)\n  fortranname        ! myrange is a dummy wrapper\n  integer intent(in) :: n\n  real*8 intent(c,out),dimension(n),depend(n) :: a = _i[0]\nend subroutine myrange\n is equivalent to numpy.arange(n,dtype=float).  Warning F2PY may lower cases also in C expressions when scanning Fortran codes (see --[no]-lower option).    Multi-line blocks A multi-line block starts with ''' (triple single-quotes) and ends with ''' in some strictly subsequent line. Multi-line blocks can be used only within .pyf files. The contents of a multi-line block can be arbitrary (except that it cannot contain ''') and no transformations (e.g. lowering cases) are applied to it. Currently, multi-line blocks can be used in the following constructs:  as a C expression of the callstatement statement; as a C type specification of the callprotoargument statement; as a C code block of the usercode statement; as a list of C arrays of the pymethoddef statement; as a documentation string.   \n"}, {"name": "Indexing on ndarrays", "path": "user/basics.indexing", "type": "User Guide", "text": "Indexing on ndarrays  See also Indexing routines  ndarrays can be indexed using the standard Python x[obj] syntax, where x is the array and obj the selection. There are different kinds of indexing available depending on obj: basic indexing, advanced indexing and field access. Most of the following examples show the use of indexing when referencing data in an array. The examples work just as well when assigning to an array. See Assigning values to indexed arrays for specific examples and explanations on how assignments work. Note that in Python, x[(exp1, exp2, ..., expN)] is equivalent to x[exp1, exp2, ..., expN]; the latter is just syntactic sugar for the former.  Basic indexing  Single element indexing Single element indexing works exactly like that for other standard Python sequences. It is 0-based, and accepts negative indices for indexing from the end of the array. >>> x = np.arange(10)\n>>> x[2]\n2\n>>> x[-2]\n8\n It is not necessary to separate each dimension\u2019s index into its own set of square brackets. >>> x.shape = (2, 5)  # now x is 2-dimensional\n>>> x[1, 3]\n8\n>>> x[1, -1]\n9\n Note that if one indexes a multidimensional array with fewer indices than dimensions, one gets a subdimensional array. For example: >>> x[0]\narray([0, 1, 2, 3, 4])\n That is, each index specified selects the array corresponding to the rest of the dimensions selected. In the above example, choosing 0 means that the remaining dimension of length 5 is being left unspecified, and that what is returned is an array of that dimensionality and size. It must be noted that the returned array is a view, i.e., it is not a copy of the original, but points to the same values in memory as does the original array. In this case, the 1-D array at the first position (0) is returned. So using a single index on the returned array, results in a single element being returned. That is: >>> x[0][2]\n2\n So note that x[0, 2] == x[0][2] though the second case is more inefficient as a new temporary array is created after the first index that is subsequently indexed by 2.  Note NumPy uses C-order indexing. That means that the last index usually represents the most rapidly changing memory location, unlike Fortran or IDL, where the first index represents the most rapidly changing location in memory. This difference represents a great potential for confusion.    Slicing and striding Basic slicing extends Python\u2019s basic concept of slicing to N dimensions. Basic slicing occurs when obj is a slice object (constructed by start:stop:step notation inside of brackets), an integer, or a tuple of slice objects and integers. Ellipsis and newaxis objects can be interspersed with these as well.  Deprecated since version 1.15.0: In order to remain backward compatible with a common usage in Numeric, basic slicing is also initiated if the selection object is any non-ndarray and non-tuple sequence (such as a list) containing slice objects, the Ellipsis object, or the newaxis object, but not for integer arrays or other embedded sequences.  The simplest case of indexing with N integers returns an array scalar representing the corresponding item. As in Python, all indices are zero-based: for the i-th index \\(n_i\\), the valid range is \\(0 \\le n_i < d_i\\) where \\(d_i\\) is the i-th element of the shape of the array. Negative indices are interpreted as counting from the end of the array (i.e., if \\(n_i < 0\\), it means \\(n_i + d_i\\)). All arrays generated by basic slicing are always views of the original array.  Note NumPy slicing creates a view instead of a copy as in the case of built-in Python sequences such as string, tuple and list. Care must be taken when extracting a small portion from a large array which becomes useless after the extraction, because the small portion extracted contains a reference to the large original array whose memory will not be released until all arrays derived from it are garbage-collected. In such cases an explicit copy() is recommended.  The standard rules of sequence slicing apply to basic slicing on a per-dimension basis (including using a step index). Some useful concepts to remember include:  \nThe basic slice syntax is i:j:k where i is the starting index, j is the stopping index, and k is the step (\\(k\\neq0\\)). This selects the m elements (in the corresponding dimension) with index values i, i + k, \u2026, i + (m - 1) k where \\(m = q + (r\\neq0)\\) and q and r are the quotient and remainder obtained by dividing j - i by k: j - i = q k + r, so that i + (m - 1) k < j. For example: >>> x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n>>> x[1:7:2]\narray([1, 3, 5])\n  \nNegative i and j are interpreted as n + i and n + j where n is the number of elements in the corresponding dimension. Negative k makes stepping go towards smaller indices. From the above example: >>> x[-2:10]\narray([8, 9])\n>>> x[-3:3:-1]\narray([7, 6, 5, 4])\n  \nAssume n is the number of elements in the dimension being sliced. Then, if i is not given it defaults to 0 for k > 0 and n - 1 for k < 0 . If j is not given it defaults to n for k > 0 and -n-1 for k < 0 . If k is not given it defaults to 1. Note that :: is the same as : and means select all indices along this axis. From the above example: >>> x[5:]\narray([5, 6, 7, 8, 9])\n  \nIf the number of objects in the selection tuple is less than N, then : is assumed for any subsequent dimensions. For example: >>> x = np.array([[[1],[2],[3]], [[4],[5],[6]]])\n>>> x.shape\n(2, 3, 1)\n>>> x[1:2]\narray([[[4],\n        [5],\n        [6]]])\n  An integer, i, returns the same values as i:i+1 except the dimensionality of the returned object is reduced by 1. In particular, a selection tuple with the p-th element an integer (and all other entries :) returns the corresponding sub-array with dimension N - 1. If N = 1 then the returned object is an array scalar. These objects are explained in Scalars. If the selection tuple has all entries : except the p-th entry which is a slice object i:j:k, then the returned array has dimension N formed by concatenating the sub-arrays returned by integer indexing of elements i, i+k, \u2026, i + (m - 1) k < j, \nBasic slicing with more than one non-: entry in the slicing tuple, acts like repeated application of slicing using a single non-: entry, where the non-: entries are successively taken (with all other non-: entries replaced by :). Thus, x[ind1, ..., ind2,:] acts like x[ind1][..., ind2, :] under basic slicing.  Warning The above is not true for advanced indexing.   You may use slicing to set values in the array, but (unlike lists) you can never grow the array. The size of the value to be set in x[obj] = value must be (broadcastable) to the same shape as x[obj]. A slicing tuple can always be constructed as obj and used in the x[obj] notation. Slice objects can be used in the construction in place of the [start:stop:step] notation. For example, x[1:10:5, ::-1] can also be implemented as obj = (slice(1, 10, 5), slice(None, None, -1)); x[obj] . This can be useful for constructing generic code that works on arrays of arbitrary dimensions. See Dealing with variable numbers of indices within programs for more information.    Dimensional indexing tools There are some tools to facilitate the easy matching of array shapes with expressions and in assignments. Ellipsis expands to the number of : objects needed for the selection tuple to index all dimensions. In most cases, this means that the length of the expanded selection tuple is x.ndim. There may only be a single ellipsis present. From the above example: >>> x[..., 0]\narray([[1, 2, 3],\n      [4, 5, 6]])\n This is equivalent to: >>> x[:, :, 0]\narray([[1, 2, 3],\n      [4, 5, 6]])\n Each newaxis object in the selection tuple serves to expand the dimensions of the resulting selection by one unit-length dimension. The added dimension is the position of the newaxis object in the selection tuple. newaxis is an alias for None, and None can be used in place of this with the same result. From the above example: >>> x[:, np.newaxis, :, :].shape\n(2, 1, 3, 1)\n>>> x[:, None, :, :].shape\n(2, 1, 3, 1)\n This can be handy to combine two arrays in a way that otherwise would require explicit reshaping operations. For example: >>> x = np.arange(5)\n>>> x[:, np.newaxis] + x[np.newaxis, :]\narray([[0, 1, 2, 3, 4],\n      [1, 2, 3, 4, 5],\n      [2, 3, 4, 5, 6],\n      [3, 4, 5, 6, 7],\n      [4, 5, 6, 7, 8]])\n    Advanced indexing Advanced indexing is triggered when the selection object, obj, is a non-tuple sequence object, an ndarray (of data type integer or bool), or a tuple with at least one sequence object or ndarray (of data type integer or bool). There are two types of advanced indexing: integer and Boolean. Advanced indexing always returns a copy of the data (contrast with basic slicing that returns a view).  Warning The definition of advanced indexing means that x[(1, 2, 3),] is fundamentally different than x[(1, 2, 3)]. The latter is equivalent to x[1, 2, 3] which will trigger basic selection while the former will trigger advanced indexing. Be sure to understand why this occurs. Also recognize that x[[1, 2, 3]] will trigger advanced indexing, whereas due to the deprecated Numeric compatibility mentioned above, x[[1, 2, slice(None)]] will trigger basic slicing.   Integer array indexing Integer array indexing allows selection of arbitrary items in the array based on their N-dimensional index. Each integer array represents a number of indices into that dimension. Negative values are permitted in the index arrays and work as they do with single indices or slices: >>> x = np.arange(10, 1, -1)\n>>> x\narray([10,  9,  8,  7,  6,  5,  4,  3,  2])\n>>> x[np.array([3, 3, 1, 8])]\narray([7, 7, 9, 2])\n>>> x[np.array([3, 3, -3, 8])]\narray([7, 7, 4, 2])\n If the index values are out of bounds then an IndexError is thrown: >>> x = np.array([[1, 2], [3, 4], [5, 6]])\n>>> x[np.array([1, -1])]\narray([[3, 4],\n      [5, 6]])\n>>> x[np.array([3, 4])]\nIndexError: index 3 is out of bounds for axis 0 with size 3\n When the index consists of as many integer arrays as dimensions of the array being indexed, the indexing is straightforward, but different from slicing. Advanced indices always are broadcast and iterated as one: result[i_1, ..., i_M] == x[ind_1[i_1, ..., i_M], ind_2[i_1, ..., i_M],\n                           ..., ind_N[i_1, ..., i_M]]\n Note that the resulting shape is identical to the (broadcast) indexing array shapes ind_1, ..., ind_N. If the indices cannot be broadcast to the same shape, an exception IndexError: shape mismatch: indexing arrays could\nnot be broadcast together with shapes... is raised. Indexing with multidimensional index arrays tend to be more unusual uses, but they are permitted, and they are useful for some problems. We\u2019ll start with the simplest multidimensional case: >>> y = np.arange(35).reshape(5, 7)\n>>> y\narray([[ 0,  1,  2,  3,  4,  5,  6],\n       [ 7,  8,  9, 10, 11, 12, 13],\n       [14, 15, 16, 17, 18, 19, 20],\n       [21, 22, 23, 24, 25, 26, 27],\n       [28, 29, 30, 31, 32, 33, 34]])\n>>> y[np.array([0, 2, 4]), np.array([0, 1, 2])]\narray([ 0, 15, 30])\n In this case, if the index arrays have a matching shape, and there is an index array for each dimension of the array being indexed, the resultant array has the same shape as the index arrays, and the values correspond to the index set for each position in the index arrays. In this example, the first index value is 0 for both index arrays, and thus the first value of the resultant array is y[0, 0]. The next value is y[2, 1], and the last is y[4, 2]. If the index arrays do not have the same shape, there is an attempt to broadcast them to the same shape. If they cannot be broadcast to the same shape, an exception is raised: >>> y[np.array([0, 2, 4]), np.array([0, 1])]\nIndexError: shape mismatch: indexing arrays could not be broadcast\ntogether with shapes (3,) (2,)\n The broadcasting mechanism permits index arrays to be combined with scalars for other indices. The effect is that the scalar value is used for all the corresponding values of the index arrays: >>> y[np.array([0, 2, 4]), 1]\narray([ 1, 15, 29])\n Jumping to the next level of complexity, it is possible to only partially index an array with index arrays. It takes a bit of thought to understand what happens in such cases. For example if we just use one index array with y: >>> y[np.array([0, 2, 4])]\narray([[ 0,  1,  2,  3,  4,  5,  6],\n      [14, 15, 16, 17, 18, 19, 20],\n      [28, 29, 30, 31, 32, 33, 34]])\n It results in the construction of a new array where each value of the index array selects one row from the array being indexed and the resultant array has the resulting shape (number of index elements, size of row). In general, the shape of the resultant array will be the concatenation of the shape of the index array (or the shape that all the index arrays were broadcast to) with the shape of any unused dimensions (those not indexed) in the array being indexed. Example From each row, a specific element should be selected. The row index is just [0, 1, 2] and the column index specifies the element to choose for the corresponding row, here [0, 1, 0]. Using both together the task can be solved using advanced indexing: >>> x = np.array([[1, 2], [3, 4], [5, 6]])\n>>> x[[0, 1, 2], [0, 1, 0]]\narray([1, 4, 5])\n To achieve a behaviour similar to the basic slicing above, broadcasting can be used. The function ix_ can help with this broadcasting. This is best understood with an example. Example From a 4x3 array the corner elements should be selected using advanced indexing. Thus all elements for which the column is one of [0, 2] and the row is one of [0, 3] need to be selected. To use advanced indexing one needs to select all elements explicitly. Using the method explained previously one could write: >>> x = np.array([[ 0,  1,  2],\n...               [ 3,  4,  5],\n...               [ 6,  7,  8],\n...               [ 9, 10, 11]])\n>>> rows = np.array([[0, 0],\n...                  [3, 3]], dtype=np.intp)\n>>> columns = np.array([[0, 2],\n...                     [0, 2]], dtype=np.intp)\n>>> x[rows, columns]\narray([[ 0,  2],\n       [ 9, 11]])\n However, since the indexing arrays above just repeat themselves, broadcasting can be used (compare operations such as rows[:, np.newaxis] + columns) to simplify this: >>> rows = np.array([0, 3], dtype=np.intp)\n>>> columns = np.array([0, 2], dtype=np.intp)\n>>> rows[:, np.newaxis]\narray([[0],\n       [3]])\n>>> x[rows[:, np.newaxis], columns]\narray([[ 0,  2],\n       [ 9, 11]])\n This broadcasting can also be achieved using the function ix_: >>> x[np.ix_(rows, columns)]\narray([[ 0,  2],\n       [ 9, 11]])\n Note that without the np.ix_ call, only the diagonal elements would be selected: >>> x[rows, columns]\narray([ 0, 11])\n This difference is the most important thing to remember about indexing with multiple advanced indices. Example A real-life example of where advanced indexing may be useful is for a color lookup table where we want to map the values of an image into RGB triples for display. The lookup table could have a shape (nlookup, 3). Indexing such an array with an image with shape (ny, nx) with dtype=np.uint8 (or any integer type so long as values are with the bounds of the lookup table) will result in an array of shape (ny, nx, 3) where a triple of RGB values is associated with each pixel location.   Boolean array indexing This advanced indexing occurs when obj is an array object of Boolean type, such as may be returned from comparison operators. A single boolean index array is practically identical to x[obj.nonzero()] where, as described above, obj.nonzero() returns a tuple (of length obj.ndim) of integer index arrays showing the True elements of obj. However, it is faster when obj.shape == x.shape. If obj.ndim == x.ndim, x[obj] returns a 1-dimensional array filled with the elements of x corresponding to the True values of obj. The search order will be row-major, C-style. If obj has True values at entries that are outside of the bounds of x, then an index error will be raised. If obj is smaller than x it is identical to filling it with False. A common use case for this is filtering for desired element values. For example, one may wish to select all entries from an array which are not NaN: >>> x = np.array([[1., 2.], [np.nan, 3.], [np.nan, np.nan]])\n>>> x[~np.isnan(x)]\narray([1., 2., 3.])\n Or wish to add a constant to all negative elements: >>> x = np.array([1., -1., -2., 3])\n>>> x[x < 0] += 20\n>>> x\narray([1., 19., 18., 3.])\n In general if an index includes a Boolean array, the result will be identical to inserting obj.nonzero() into the same position and using the integer array indexing mechanism described above. x[ind_1, boolean_array, ind_2] is equivalent to x[(ind_1,) + boolean_array.nonzero() + (ind_2,)]. If there is only one Boolean array and no integer indexing array present, this is straightforward. Care must only be taken to make sure that the boolean index has exactly as many dimensions as it is supposed to work with. In general, when the boolean array has fewer dimensions than the array being indexed, this is equivalent to x[b, ...], which means x is indexed by b followed by as many : as are needed to fill out the rank of x. Thus the shape of the result is one dimension containing the number of True elements of the boolean array, followed by the remaining dimensions of the array being indexed: >>> x = np.arange(35).reshape(5, 7)\n>>> b = x > 20\n>>> b[:, 5]\narray([False, False, False,  True,  True])\n>>> x[b[:, 5]]\narray([[21, 22, 23, 24, 25, 26, 27],\n      [28, 29, 30, 31, 32, 33, 34]])\n Here the 4th and 5th rows are selected from the indexed array and combined to make a 2-D array. Example From an array, select all rows which sum up to less or equal two: >>> x = np.array([[0, 1], [1, 1], [2, 2]])\n>>> rowsum = x.sum(-1)\n>>> x[rowsum <= 2, :]\narray([[0, 1],\n       [1, 1]])\n Combining multiple Boolean indexing arrays or a Boolean with an integer indexing array can best be understood with the obj.nonzero() analogy. The function ix_ also supports boolean arrays and will work without any surprises. Example Use boolean indexing to select all rows adding up to an even number. At the same time columns 0 and 2 should be selected with an advanced integer index. Using the ix_ function this can be done with: >>> x = np.array([[ 0,  1,  2],\n...               [ 3,  4,  5],\n...               [ 6,  7,  8],\n...               [ 9, 10, 11]])\n>>> rows = (x.sum(-1) % 2) == 0\n>>> rows\narray([False,  True, False,  True])\n>>> columns = [0, 2]\n>>> x[np.ix_(rows, columns)]\narray([[ 3,  5],\n       [ 9, 11]])\n Without the np.ix_ call, only the diagonal elements would be selected. Or without np.ix_ (compare the integer array examples): >>> rows = rows.nonzero()[0]\n>>> x[rows[:, np.newaxis], columns]\narray([[ 3,  5],\n       [ 9, 11]])\n Example Use a 2-D boolean array of shape (2, 3) with four True elements to select rows from a 3-D array of shape (2, 3, 5) results in a 2-D result of shape (4, 5): >>> x = np.arange(30).reshape(2, 3, 5)\n>>> x\narray([[[ 0,  1,  2,  3,  4],\n        [ 5,  6,  7,  8,  9],\n        [10, 11, 12, 13, 14]],\n      [[15, 16, 17, 18, 19],\n        [20, 21, 22, 23, 24],\n        [25, 26, 27, 28, 29]]])\n>>> b = np.array([[True, True, False], [False, True, True]])\n>>> x[b]\narray([[ 0,  1,  2,  3,  4],\n      [ 5,  6,  7,  8,  9],\n      [20, 21, 22, 23, 24],\n      [25, 26, 27, 28, 29]])\n   Combining advanced and basic indexing When there is at least one slice (:), ellipsis (...) or newaxis in the index (or the array has more dimensions than there are advanced indices), then the behaviour can be more complicated. It is like concatenating the indexing result for each advanced index element. In the simplest case, there is only a single advanced index combined with a slice. For example: >>> y = np.arange(35).reshape(5,7)\n>>> y[np.array([0, 2, 4]), 1:3]\narray([[ 1,  2],\n       [15, 16],\n       [29, 30]])\n In effect, the slice and index array operation are independent. The slice operation extracts columns with index 1 and 2, (i.e. the 2nd and 3rd columns), followed by the index array operation which extracts rows with index 0, 2 and 4 (i.e the first, third and fifth rows). This is equivalent to: >>> y[:, 1:3][np.array([0, 2, 4]), :]\narray([[ 1,  2],\n       [15, 16],\n       [29, 30]])\n A single advanced index can, for example, replace a slice and the result array will be the same. However, it is a copy and may have a different memory layout. A slice is preferable when it is possible. For example: >>> x = np.array([[ 0,  1,  2],\n...               [ 3,  4,  5],\n...               [ 6,  7,  8],\n...               [ 9, 10, 11]])\n>>> x[1:2, 1:3]\narray([[4, 5]])\n>>> x[1:2, [1, 2]]\narray([[4, 5]])\n The easiest way to understand a combination of multiple advanced indices may be to think in terms of the resulting shape. There are two parts to the indexing operation, the subspace defined by the basic indexing (excluding integers) and the subspace from the advanced indexing part. Two cases of index combination need to be distinguished:  The advanced indices are separated by a slice, Ellipsis or newaxis. For example x[arr1, :, arr2]. The advanced indices are all next to each other. For example x[..., arr1, arr2, :] but not x[arr1, :, 1] since 1 is an advanced index in this regard.  In the first case, the dimensions resulting from the advanced indexing operation come first in the result array, and the subspace dimensions after that. In the second case, the dimensions from the advanced indexing operations are inserted into the result array at the same spot as they were in the initial array (the latter logic is what makes simple advanced indexing behave just like slicing). Example Suppose x.shape is (10, 20, 30) and ind is a (2, 3, 4)-shaped indexing intp array, then result = x[..., ind, :] has shape (10, 2, 3, 4, 30) because the (20,)-shaped subspace has been replaced with a (2, 3, 4)-shaped broadcasted indexing subspace. If we let i, j, k loop over the (2, 3, 4)-shaped subspace then result[..., i, j, k, :] = x[..., ind[i, j, k], :]. This example produces the same result as x.take(ind, axis=-2). Example Let x.shape be (10, 20, 30, 40, 50) and suppose ind_1 and ind_2 can be broadcast to the shape (2, 3, 4). Then x[:, ind_1, ind_2] has shape (10, 2, 3, 4, 40, 50) because the (20, 30)-shaped subspace from X has been replaced with the (2, 3, 4) subspace from the indices. However, x[:, ind_1, :, ind_2] has shape (2, 3, 4, 10, 30, 50) because there is no unambiguous place to drop in the indexing subspace, thus it is tacked-on to the beginning. It is always possible to use .transpose() to move the subspace anywhere desired. Note that this example cannot be replicated using take. Example Slicing can be combined with broadcasted boolean indices: >>> x = np.arange(35).reshape(5, 7)\n>>> b = x > 20\n>>> b\narray([[False, False, False, False, False, False, False],\n      [False, False, False, False, False, False, False],\n      [False, False, False, False, False, False, False],\n      [ True,  True,  True,  True,  True,  True,  True],\n      [ True,  True,  True,  True,  True,  True,  True]])\n>>> x[b[:, 5], 1:3]\narray([[22, 23],\n      [29, 30]])\n    Field access  See also Structured arrays  If the ndarray object is a structured array the fields of the array can be accessed by indexing the array with strings, dictionary-like. Indexing x['field-name'] returns a new view to the array, which is of the same shape as x (except when the field is a sub-array) but of data type x.dtype['field-name'] and contains only the part of the data in the specified field. Also, record array scalars can be \u201cindexed\u201d this way. Indexing into a structured array can also be done with a list of field names, e.g. x[['field-name1', 'field-name2']]. As of NumPy 1.16, this returns a view containing only those fields. In older versions of NumPy, it returned a copy. See the user guide section on Structured arrays for more information on multifield indexing. If the accessed field is a sub-array, the dimensions of the sub-array are appended to the shape of the result. For example: >>> x = np.zeros((2, 2), dtype=[('a', np.int32), ('b', np.float64, (3, 3))])\n>>> x['a'].shape\n(2, 2)\n>>> x['a'].dtype\ndtype('int32')\n>>> x['b'].shape\n(2, 2, 3, 3)\n>>> x['b'].dtype\ndtype('float64')\n   Flat Iterator indexing x.flat returns an iterator that will iterate over the entire array (in C-contiguous style with the last index varying the fastest). This iterator object can also be indexed using basic slicing or advanced indexing as long as the selection object is not a tuple. This should be clear from the fact that x.flat is a 1-dimensional view. It can be used for integer indexing with 1-dimensional C-style-flat indices. The shape of any returned array is therefore the shape of the integer indexing object.   Assigning values to indexed arrays As mentioned, one can select a subset of an array to assign to using a single index, slices, and index and mask arrays. The value being assigned to the indexed array must be shape consistent (the same shape or broadcastable to the shape the index produces). For example, it is permitted to assign a constant to a slice: >>> x = np.arange(10)\n>>> x[2:7] = 1\n or an array of the right size: >>> x[2:7] = np.arange(5)\n Note that assignments may result in changes if assigning higher types to lower types (like floats to ints) or even exceptions (assigning complex to floats or ints): >>> x[1] = 1.2\n>>> x[1]\n1\n>>> x[1] = 1.2j\nTypeError: can't convert complex to int\n Unlike some of the references (such as array and mask indices) assignments are always made to the original data in the array (indeed, nothing else would make sense!). Note though, that some actions may not work as one may naively expect. This particular example is often surprising to people: >>> x = np.arange(0, 50, 10)\n>>> x\narray([ 0, 10, 20, 30, 40])\n>>> x[np.array([1, 1, 3, 1])] += 1\n>>> x\narray([ 0, 11, 20, 31, 40])\n Where people expect that the 1st location will be incremented by 3. In fact, it will only be incremented by 1. The reason is that a new array is extracted from the original (as a temporary) containing the values at 1, 1, 3, 1, then the value 1 is added to the temporary, and then the temporary is assigned back to the original array. Thus the value of the array at x[1] + 1 is assigned to x[1] three times, rather than being incremented 3 times.   Dealing with variable numbers of indices within programs The indexing syntax is very powerful but limiting when dealing with a variable number of indices. For example, if you want to write a function that can handle arguments with various numbers of dimensions without having to write special case code for each number of possible dimensions, how can that be done? If one supplies to the index a tuple, the tuple will be interpreted as a list of indices. For example: >>> z = np.arange(81).reshape(3, 3, 3, 3)\n>>> indices = (1, 1, 1, 1)\n>>> z[indices]\n40\n So one can use code to construct tuples of any number of indices and then use these within an index. Slices can be specified within programs by using the slice() function in Python. For example: >>> indices = (1, 1, 1, slice(0, 2))  # same as [1, 1, 1, 0:2]\n>>> z[indices]\narray([39, 40])\n Likewise, ellipsis can be specified by code by using the Ellipsis object: >>> indices = (1, Ellipsis, 1)  # same as [1, ..., 1]\n>>> z[indices]\narray([[28, 31, 34],\n       [37, 40, 43],\n       [46, 49, 52]])\n For this reason, it is possible to use the output from the np.nonzero() function directly as an index since it always returns a tuple of index arrays. Because the special treatment of tuples, they are not automatically converted to an array as a list would be. As an example: >>> z[[1, 1, 1, 1]]  # produces a large array\narray([[[[27, 28, 29],\n         [30, 31, 32], ...\n>>> z[(1, 1, 1, 1)]  # returns a single value\n40\n   Detailed notes These are some detailed notes, which are not of importance for day to day indexing (in no particular order):  The native NumPy indexing type is intp and may differ from the default integer array type. intp is the smallest data type sufficient to safely index any array; for advanced indexing it may be faster than other types. For advanced assignments, there is in general no guarantee for the iteration order. This means that if an element is set more than once, it is not possible to predict the final result. An empty (tuple) index is a full scalar index into a zero-dimensional array. x[()] returns a scalar if x is zero-dimensional and a view otherwise. On the other hand, x[...] always returns a view. If a zero-dimensional array is present in the index and it is a full integer index the result will be a scalar and not a zero-dimensional array. (Advanced indexing is not triggered.) When an ellipsis (...) is present but has no size (i.e. replaces zero :) the result will still always be an array. A view if no advanced index is present, otherwise a copy. The nonzero equivalence for Boolean arrays does not hold for zero dimensional boolean arrays. When the result of an advanced indexing operation has no elements but an individual index is out of bounds, whether or not an IndexError is raised is undefined (e.g. x[[], [123]] with 123 being out of bounds). When a casting error occurs during assignment (for example updating a numerical array using a sequence of strings), the array being assigned to may end up in an unpredictable partially updated state. However, if any other error (such as an out of bounds index) occurs, the array will remain unchanged. The memory layout of an advanced indexing result is optimized for each indexing operation and no particular memory order can be assumed. When using a subclass (especially one which manipulates its shape), the default ndarray.__setitem__ behaviour will call __getitem__ for basic indexing but not for advanced indexing. For such a subclass it may be preferable to call ndarray.__setitem__ with a base class ndarray view on the data. This must be done if the subclasses __getitem__ does not return views.  \n"}, {"name": "Indexing routines", "path": "reference/arrays.indexing", "type": "Indexing routines", "text": "Indexing routines  See also Indexing on ndarrays   Generating index arrays  \nc_ Translates slice objects to concatenation along the second axis.  \nr_ Translates slice objects to concatenation along the first axis.  \ns_ A nicer way to build up index tuples for arrays.  \nnonzero(a) Return the indices of the elements that are non-zero.  \nwhere(condition, [x, y], /) Return elements chosen from x or y depending on condition.  \nindices(dimensions[, dtype, sparse]) Return an array representing the indices of a grid.  \nix_(*args) Construct an open mesh from multiple sequences.  \nogrid nd_grid instance which returns an open multi-dimensional \"meshgrid\".  \nravel_multi_index(multi_index, dims[, mode, ...]) Converts a tuple of index arrays into an array of flat indices, applying boundary modes to the multi-index.  \nunravel_index(indices, shape[, order]) Converts a flat index or array of flat indices into a tuple of coordinate arrays.  \ndiag_indices(n[, ndim]) Return the indices to access the main diagonal of an array.  \ndiag_indices_from(arr) Return the indices to access the main diagonal of an n-dimensional array.  \nmask_indices(n, mask_func[, k]) Return the indices to access (n, n) arrays, given a masking function.  \ntril_indices(n[, k, m]) Return the indices for the lower-triangle of an (n, m) array.  \ntril_indices_from(arr[, k]) Return the indices for the lower-triangle of arr.  \ntriu_indices(n[, k, m]) Return the indices for the upper-triangle of an (n, m) array.  \ntriu_indices_from(arr[, k]) Return the indices for the upper-triangle of arr.     Indexing-like operations  \ntake(a, indices[, axis, out, mode]) Take elements from an array along an axis.  \ntake_along_axis(arr, indices, axis) Take values from the input array by matching 1d index and data slices.  \nchoose(a, choices[, out, mode]) Construct an array from an index array and a list of arrays to choose from.  \ncompress(condition, a[, axis, out]) Return selected slices of an array along given axis.  \ndiag(v[, k]) Extract a diagonal or construct a diagonal array.  \ndiagonal(a[, offset, axis1, axis2]) Return specified diagonals.  \nselect(condlist, choicelist[, default]) Return an array drawn from elements in choicelist, depending on conditions.  \nlib.stride_tricks.sliding_window_view(x, ...) Create a sliding window view into the array with the given window shape.  \nlib.stride_tricks.as_strided(x[, shape, ...]) Create a view into the array with the given shape and strides.     Inserting data into arrays  \nplace(arr, mask, vals) Change elements of an array based on conditional and input values.  \nput(a, ind, v[, mode]) Replaces specified elements of an array with given values.  \nput_along_axis(arr, indices, values, axis) Put values into the destination array by matching 1d index and data slices.  \nputmask(a, mask, values) Changes elements of an array based on conditional and input values.  \nfill_diagonal(a, val[, wrap]) Fill the main diagonal of the given array of any dimensionality.     Iterating over arrays  \nnditer(op[, flags, op_flags, op_dtypes, ...]) Efficient multi-dimensional iterator object to iterate over arrays.  \nndenumerate(arr) Multidimensional index iterator.  \nndindex(*shape) An N-dimensional iterator object to index arrays.  \nnested_iters(op, axes[, flags, op_flags, ...]) Create nditers for use in nested loops  \nflatiter() Flat iterator object to iterate over arrays.  \nlib.Arrayterator(var[, buf_size]) Buffered iterator for big arrays.   \n"}, {"name": "Input and output", "path": "reference/routines.io", "type": "Input and output", "text": "Input and output  NumPy binary files (NPY, NPZ)  \nload(file[, mmap_mode, allow_pickle, ...]) Load arrays or pickled objects from .npy, .npz or pickled files.  \nsave(file, arr[, allow_pickle, fix_imports]) Save an array to a binary file in NumPy .npy format.  \nsavez(file, *args, **kwds) Save several arrays into a single file in uncompressed .npz format.  \nsavez_compressed(file, *args, **kwds) Save several arrays into a single file in compressed .npz format.   The format of these binary file types is documented in numpy.lib.format   Text files  \nloadtxt(fname[, dtype, comments, delimiter, ...]) Load data from a text file.  \nsavetxt(fname, X[, fmt, delimiter, newline, ...]) Save an array to a text file.  \ngenfromtxt(fname[, dtype, comments, ...]) Load data from a text file, with missing values handled as specified.  \nfromregex(file, regexp, dtype[, encoding]) Construct an array from a text file, using regular expression parsing.  \nfromstring(string[, dtype, count, like]) A new 1-D array initialized from text data in a string.  \nndarray.tofile(fid[, sep, format]) Write array to a file as text or binary (default).  \nndarray.tolist() Return the array as an a.ndim-levels deep nested list of Python scalars.     Raw binary files  \nfromfile(file[, dtype, count, sep, offset, like]) Construct an array from data in a text or binary file.  \nndarray.tofile(fid[, sep, format]) Write array to a file as text or binary (default).     String formatting  \narray2string(a[, max_line_width, precision, ...]) Return a string representation of an array.  \narray_repr(arr[, max_line_width, precision, ...]) Return the string representation of an array.  \narray_str(a[, max_line_width, precision, ...]) Return a string representation of the data in an array.  \nformat_float_positional(x[, precision, ...]) Format a floating-point scalar as a decimal string in positional notation.  \nformat_float_scientific(x[, precision, ...]) Format a floating-point scalar as a decimal string in scientific notation.     Memory mapping files  \nmemmap(filename[, dtype, mode, offset, ...]) Create a memory-map to an array stored in a binary file on disk.  \nlib.format.open_memmap(filename[, mode, ...]) Open a .npy file as a memory-mapped array.     Text formatting options  \nset_printoptions([precision, threshold, ...]) Set printing options.  \nget_printoptions() Return the current print options.  \nset_string_function(f[, repr]) Set a Python function to be used when pretty printing arrays.  \nprintoptions(*args, **kwargs) Context manager for setting print options.     Base-n representations  \nbinary_repr(num[, width]) Return the binary representation of the input number as a string.  \nbase_repr(number[, base, padding]) Return a string representation of a number in the given base system.     Data sources  \nDataSource([destpath]) A generic data source file (file, http, ftp, ...).     Binary Format Description  \nlib.format Binary serialization   \n"}, {"name": "Install git", "path": "dev/gitwash/git_intro", "type": "Development", "text": "Install git Developing with git can be done entirely without github. Git is a distributed version control system. In order to use git on your machine you must install it.\n"}, {"name": "int **cancastscalarkindto", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.cancastscalarkindto", "type": "Python Types and C-Structures", "text": "  int**cancastscalarkindto\n \nEither NULL or an array of NPY_NSCALARKINDS pointers. These pointers should each be either NULL or a pointer to an array of integers (terminated by NPY_NOTYPE) indicating data-types that a scalar of this data-type of the specified kind can be cast to safely (this usually means without losing precision). \n"}, {"name": "int *cancastto", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.cancastto", "type": "Python Types and C-Structures", "text": "  int*cancastto\n \nEither NULL or an array of integers (terminated by NPY_NOTYPE ) indicated data-types that this data-type can be cast to safely (this usually means without losing precision). \n"}, {"name": "int *core_dim_ixs", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.core_dim_ixs", "type": "Python Types and C-Structures", "text": "  int*core_dim_ixs\n \nDimension indices in a flattened form; indices of argument k are stored in core_dim_ixs[core_offsets[k] : core_offsets[k] +\ncore_numdims[k]] \n"}, {"name": "int *core_num_dims", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.core_num_dims", "type": "Python Types and C-Structures", "text": "  int*core_num_dims\n \nNumber of core dimensions of each argument \n"}, {"name": "int *core_offsets", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.core_offsets", "type": "Python Types and C-Structures", "text": "  int*core_offsets\n \nPosition of 1st core dimension of each argument in core_dim_ixs, equivalent to cumsum(core_num_dims) \n"}, {"name": "int alignment", "path": "reference/c-api/types-and-structures#c.NPY_USE_SETITEM.alignment", "type": "Python Types and C-Structures", "text": "  intalignment\n \nA number providing alignment information for this data type. Specifically, it shows how far from the start of a 2-element structure (whose first element is a char ), the compiler places an item of this type: offsetof(struct {char c; type v;},\nv) \n"}, {"name": "int argmax()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.argmax", "type": "Python Types and C-Structures", "text": "  intargmax(void*data, npy_intpn, npy_intp*max_ind, void*arr)\n \nA pointer to a function that retrieves the index of the largest of n elements in arr beginning at the element pointed to by data. This function requires that the memory segment be contiguous and behaved. The return value is always 0. The index of the largest element is returned in max_ind. \n"}, {"name": "int argmin()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.argmin", "type": "Python Types and C-Structures", "text": "  intargmin(void*data, npy_intpn, npy_intp*min_ind, void*arr)\n \nA pointer to a function that retrieves the index of the smallest of n elements in arr beginning at the element pointed to by data. This function requires that the memory segment be contiguous and behaved. The return value is always 0. The index of the smallest element is returned in min_ind. \n"}, {"name": "int argsort()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.argsort", "type": "Python Types and C-Structures", "text": "  intargsort(void*start, npy_intp*result, npy_intplength, void*arr)\n \nAn array of function pointers to sorting algorithms for this data type. The same sorting algorithms as for sort are available. The indices producing the sort are returned in result (which must be initialized with indices 0 to length-1 inclusive). \n"}, {"name": "int compare()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.compare", "type": "Python Types and C-Structures", "text": "  intcompare(constvoid*d1, constvoid*d2, void*arr)\n \nA pointer to a function that compares two elements of the array, arr, pointed to by d1 and d2. This function requires behaved (aligned and not swapped) arrays. The return value is 1 if * d1 > * d2, 0 if * d1 == * d2, and -1 if * d1 < * d2. The array object arr is used to retrieve itemsize and field information for flexible arrays. \n"}, {"name": "int core_enabled", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.core_enabled", "type": "Python Types and C-Structures", "text": "  intcore_enabled\n \n0 for scalar ufuncs; 1 for generalized ufuncs \n"}, {"name": "int core_num_dim_ix", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.core_num_dim_ix", "type": "Python Types and C-Structures", "text": "  intcore_num_dim_ix\n \nNumber of distinct core dimension names in the signature \n"}, {"name": "int doxy_javadoc_example()", "path": "dev/howto-docs", "type": "Development", "text": "How to contribute to the NumPy documentation This guide will help you decide what to contribute and how to submit it to the official NumPy documentation.  Documentation team meetings The NumPy community has set a firm goal of improving its documentation. We hold regular documentation meetings on Zoom (dates are announced on the numpy-discussion mailing list), and everyone is welcome. Reach out if you have questions or need someone to guide you through your first steps \u2013 we\u2019re happy to help. Minutes are taken on hackmd.io and stored in the NumPy Archive repository.   What\u2019s needed The NumPy Documentation has the details covered. API reference documentation is generated directly from docstrings in the code when the documentation is built. Although we have mostly complete reference documentation for each function and class exposed to users, there is a lack of usage examples for some of them. What we lack are docs with broader scope \u2013 tutorials, how-tos, and explanations. Reporting defects is another way to contribute. We discuss both.   Contributing fixes We\u2019re eager to hear about and fix doc defects. But to attack the biggest problems we end up having to defer or overlook some bug reports. Here are the best defects to go after. Top priority goes to technical inaccuracies \u2013 a docstring missing a parameter, a faulty description of a function/parameter/method, and so on. Other \u201cstructural\u201d defects like broken links also get priority. All these fixes are easy to confirm and put in place. You can submit a pull request (PR) with the fix, if you know how to do that; otherwise please open an issue. Typos and misspellings fall on a lower rung; we welcome hearing about them but may not be able to fix them promptly. These too can be handled as pull requests or issues. Obvious wording mistakes (like leaving out a \u201cnot\u201d) fall into the typo category, but other rewordings \u2013 even for grammar \u2013 require a judgment call, which raises the bar. Test the waters by first presenting the fix as an issue. Some functions/objects like numpy.ndarray.transpose, numpy.array etc. defined in C-extension modules have their docstrings defined separately in _add_newdocs.py   Contributing new pages Your frustrations using our documents are our best guide to what needs fixing. If you write a missing doc you join the front line of open source, but it\u2019s a meaningful contribution just to let us know what\u2019s missing. If you want to compose a doc, run your thoughts by the mailing list for further ideas and feedback. If you want to alert us to a gap, open an issue. See this issue for an example. If you\u2019re looking for subjects, our formal roadmap for documentation is a NumPy Enhancement Proposal (NEP), NEP 44 - Restructuring the NumPy Documentation. It identifies areas where our docs need help and lists several additions we\u2019d like to see, including Jupyter notebooks.  Documentation framework There are formulas for writing useful documents, and four formulas cover nearly everything. There are four formulas because there are four categories of document \u2013 tutorial, how-to guide, explanation, and reference. The insight that docs divide up this way belongs to Daniele Procida and his Di\u00e1taxis Framework. When you begin a document or propose one, have in mind which of these types it will be.   NumPy tutorials In addition to the documentation that is part of the NumPy source tree, you can submit content in Jupyter Notebook format to the NumPy Tutorials page. This set of tutorials and educational materials is meant to provide high-quality resources by the NumPy project, both for self-learning and for teaching classes with. These resources are developed in a separate GitHub repository, numpy-tutorials, where you can check out existing notebooks, open issues to suggest new topics or submit your own tutorials as pull requests.   More on contributing Don\u2019t worry if English is not your first language, or if you can only come up with a rough draft. Open source is a community effort. Do your best \u2013 we\u2019ll help fix issues. Images and real-life data make text more engaging and powerful, but be sure what you use is appropriately licensed and available. Here again, even a rough idea for artwork can be polished by others. For now, the only data formats accepted by NumPy are those also used by other Python scientific libraries like pandas, SciPy, or Matplotlib. We\u2019re developing a package to accept more formats; contact us for details. NumPy documentation is kept in the source code tree. To get your document into the docbase you must download the tree, build it, and submit a pull request. If GitHub and pull requests are new to you, check our Contributor Guide. Our markup language is reStructuredText (rST), which is more elaborate than Markdown. Sphinx, the tool many Python projects use to build and link project documentation, converts the rST into HTML and other formats. For more on rST, see the Quick reStructuredText Guide or the reStructuredText Primer    Contributing indirectly If you run across outside material that would be a useful addition to the NumPy docs, let us know by opening an issue. You don\u2019t have to contribute here to contribute to NumPy. You\u2019ve contributed if you write a tutorial on your blog, create a YouTube video, or answer questions on Stack Overflow and other sites.   Documentation style  User documentation  In general, we follow the Google developer documentation style guide for the User Guide. \nNumPy style governs cases where:  Google has no guidance, or We prefer not to use the Google style  Our current rules:  We pluralize index as indices rather than indexes, following the precedent of numpy.indices. For consistency we also pluralize matrix as matrices.   Grammatical issues inadequately addressed by the NumPy or Google rules are decided by the section on \u201cGrammar and Usage\u201d in the most recent edition of the Chicago Manual of Style. We welcome being alerted to cases we should add to the NumPy style rules.    Docstrings When using Sphinx in combination with the NumPy conventions, you should use the numpydoc extension so that your docstrings will be handled correctly. For example, Sphinx will extract the Parameters section from your docstring and convert it into a field list. Using numpydoc will also avoid the reStructuredText errors produced by plain Sphinx when it encounters NumPy docstring conventions like section headers (e.g. -------------) that sphinx does not expect to find in docstrings. It is available from:  numpydoc on PyPI numpydoc on GitHub  Note that for documentation within NumPy, it is not necessary to do import numpy as np at the beginning of an example. Please use the numpydoc formatting standard as shown in their example.   Documenting C/C++ Code NumPy uses Doxygen to parse specially-formatted C/C++ comment blocks. This generates XML files, which are converted by Breathe into RST, which is used by Sphinx. It takes three steps to complete the documentation process:  1. Writing the comment blocks Although there is still no commenting style set to follow, the Javadoc is more preferable than the others due to the similarities with the current existing non-indexed comment blocks.  Note Please see \u201cDocumenting the code\u201d.  This is what Javadoc style looks like: /**\n * This a simple brief.\n *\n * And the details goes here.\n * Multi lines are welcome.\n *\n * @param  num  leave a comment for parameter num.\n * @param  str  leave a comment for the second parameter.\n * @return      leave a comment for the returned value.\n */\nint doxy_javadoc_example(int num, const char *str);\n And here is how it is rendered:   intdoxy_javadoc_example(intnum, constchar*str)\n \nThis a simple brief.  And the details goes here. Multi lines are welcome.  Parameters \n \nnum \u2013 leave a comment for parameter num.  \nstr \u2013 leave a comment for the second parameter.    Returns \nleave a comment for the returned value.    \n For line comment, you can use a triple forward slash. For example: /**\n *  Template to represent limbo numbers.\n *\n *  Specializations for integer types that are part of nowhere.\n *  It doesn't support with any real types.\n *\n *  @param Tp Type of the integer. Required to be an integer type.\n *  @param N  Number of elements.\n*/\ntemplate<typename Tp, std::size_t N>\nclass DoxyLimbo {\n public:\n    /// Default constructor. Initialize nothing.\n    DoxyLimbo();\n    /// Set Default behavior for copy the limbo.\n    DoxyLimbo(const DoxyLimbo<Tp, N> &l);\n    /// Returns the raw data for the limbo.\n    const Tp *data();\n protected:\n    Tp p_data[N]; ///< Example for inline comment.\n};\n And here is how it is rendered:   template<typenameTp,std::size_tN>classDoxyLimbo\n \nTemplate to represent limbo numbers.  Specializations for integer types that are part of nowhere. It doesn\u2019t support with any real types.\n param Tp \nType of the integer. Required to be an integer type.   param N \nNumber of elements.     Public Functions   DoxyLimbo()\n \nDefault constructor. Initialize nothing.  \n   DoxyLimbo(constDoxyLimbo<Tp,N>&l)\n \nSet Default behavior for copy the limbo.  \n   constTp*data()\n \nReturns the raw data for the limbo.  \n   Protected Attributes   Tpp_data[N]\n \nExample for inline comment.  \n  \n  Common Doxygen Tags:  Note For more tags/commands, please take a look at https://www.doxygen.nl/manual/commands.html  @brief Starts a paragraph that serves as a brief description. By default the first sentence of the documentation block is automatically treated as a brief description, since option JAVADOC_AUTOBRIEF is enabled within doxygen configurations. @details Just like @brief starts a brief description, @details starts the detailed description. You can also start a new paragraph (blank line) then the @details command is not needed. @param Starts a parameter description for a function parameter with name <parameter-name>, followed by a description of the parameter. The existence of the parameter is checked and a warning is given if the documentation of this (or any other) parameter is missing or not present in the function declaration or definition. @return Starts a return value description for a function. Multiple adjacent @return commands will be joined into a single paragraph. The @return description ends when a blank line or some other sectioning command is encountered. @code/@endcode Starts/Ends a block of code. A code block is treated differently from ordinary text. It is interpreted as source code. @rst/@endrst Starts/Ends a block of reST markup.  Example Take a look at the following example: /**\n * A comment block contains reST markup.\n * @rst\n * .. note::\n *\n *   Thanks to Breathe_, we were able to bring it to Doxygen_\n *\n * Some code example::\n *\n *   int example(int x) {\n *       return x * 2;\n *   }\n * @endrst\n */\nvoid doxy_reST_example(void);\n And here is how it is rendered:   voiddoxy_reST_example(void)\n \nA comment block contains reST markup. \nSome code example: int example(int x) {\n    return x * 2;\n}\n  Note Thanks to Breathe, we were able to bring it to Doxygen  \n     2. Feeding Doxygen Not all headers files are collected automatically. You have to add the desired C/C++ header paths within the sub-config files of Doxygen. Sub-config files have the unique name .doxyfile, which you can usually find near directories that contain documented headers. You need to create a new config file if there\u2019s not one located in a path close(2-depth) to the headers you want to add. Sub-config files can accept any of Doxygen configuration options, but do not override or re-initialize any configuration option, rather only use the concatenation operator \u201c+=\u201d. For example: # to specfiy certain headers\nINPUT += @CUR_DIR/header1.h \\\n         @CUR_DIR/header2.h\n# to add all headers in certain path\nINPUT += @CUR_DIR/to/headers\n# to define certain macros\nPREDEFINED += C_MACRO(X)=X\n# to enable certain branches\nPREDEFINED += NPY_HAVE_FEATURE \\\n              NPY_HAVE_FEATURE2\n  Note @CUR_DIR is a template constant returns the current dir path of the sub-config file.    3. Inclusion directives Breathe provides a wide range of custom directives to allow converting the documents generated by Doxygen into reST files.  Note For more information, please check out \u201cDirectives & Config Variables\u201d   Common directives: doxygenfunction This directive generates the appropriate output for a single function. The function name is required to be unique in the project. .. doxygenfunction:: <function name>\n    :outline:\n    :no-link:\n Checkout the example to see it in action. doxygenclass This directive generates the appropriate output for a single class. It takes the standard project, path, outline and no-link options and additionally the members, protected-members, private-members, undoc-members, membergroups and members-only options: .. doxygenclass:: <class name>\n   :members: [...]\n   :protected-members:\n   :private-members:\n   :undoc-members:\n   :membergroups: ...\n   :members-only:\n   :outline:\n   :no-link:\n Checkout the doxygenclass documentation <https://breathe.readthedocs.io/en/latest/class.html#class-example>_ for more details and to see it in action. doxygennamespace This directive generates the appropriate output for the contents of a namespace. It takes the standard project, path, outline and no-link options and additionally the content-only, members, protected-members, private-members and undoc-members options. To reference a nested namespace, the full namespaced path must be provided, e.g. foo::bar for the bar namespace inside the foo namespace. .. doxygennamespace:: <namespace>\n   :content-only:\n   :outline:\n   :members:\n   :protected-members:\n   :private-members:\n   :undoc-members:\n   :no-link:\n Checkout the doxygennamespace documentation for more details and to see it in action. doxygengroup This directive generates the appropriate output for the contents of a doxygen group. A doxygen group can be declared with specific doxygen markup in the source comments as covered in the doxygen grouping documentation. It takes the standard project, path, outline and no-link options and additionally the content-only, members, protected-members, private-members and undoc-members options. .. doxygengroup:: <group name>\n   :content-only:\n   :outline:\n   :members:\n   :protected-members:\n   :private-members:\n   :undoc-members:\n   :no-link:\n   :inner:\n Checkout the doxygengroup documentation for more details and to see it in action.      Documentation reading  The leading organization of technical writers, Write the Docs, holds conferences, hosts learning resources, and runs a Slack channel. \u201cEvery engineer is also a writer,\u201d says Google\u2019s collection of technical writing resources, which includes free online courses for developers in planning and writing documents. \nSoftware Carpentry\u2019s mission is teaching software to researchers. In addition to hosting the curriculum, the website explains how to present ideas effectively.  \n"}, {"name": "int elsize", "path": "reference/c-api/types-and-structures#c.NPY_USE_SETITEM.elsize", "type": "Python Types and C-Structures", "text": "  intelsize\n \nFor data types that are always the same size (such as long), this holds the size of the data type. For flexible data types where different arrays can have a different elementsize, this should be 0. \n"}, {"name": "int flags", "path": "reference/c-api/types-and-structures#c.PyArray_Chunk.flags", "type": "Python Types and C-Structures", "text": "  intflags\n \nAny data flags (e.g. NPY_ARRAY_WRITEABLE ) that should be used to interpret the memory. \n"}, {"name": "int flags", "path": "reference/c-api/types-and-structures#c.NPY_AO.flags", "type": "Python Types and C-Structures", "text": "  intflags\n \nPointed to by the macro PyArray_FLAGS, this data member represents the flags indicating how the memory pointed to by data is to be interpreted. Possible flags are NPY_ARRAY_C_CONTIGUOUS, NPY_ARRAY_F_CONTIGUOUS, NPY_ARRAY_OWNDATA, NPY_ARRAY_ALIGNED, NPY_ARRAY_WRITEABLE, NPY_ARRAY_WRITEBACKIFCOPY, and NPY_ARRAY_UPDATEIFCOPY. \n"}, {"name": "int flags", "path": "reference/c-api/types-and-structures#c.PyArrayInterface.flags", "type": "Python Types and C-Structures", "text": "  intflags\n \nAny of the bits NPY_ARRAY_C_CONTIGUOUS (1), NPY_ARRAY_F_CONTIGUOUS (2), NPY_ARRAY_ALIGNED (0x100), NPY_ARRAY_NOTSWAPPED (0x200), or NPY_ARRAY_WRITEABLE (0x400) to indicate something about the data. The NPY_ARRAY_ALIGNED, NPY_ARRAY_C_CONTIGUOUS, and NPY_ARRAY_F_CONTIGUOUS flags can actually be determined from the other parameters. The flag NPY_ARR_HAS_DESCR (0x800) can also be set to indicate to objects consuming the version 3 array interface that the descr member of the structure is present (it will be ignored by objects consuming version 2 of the array interface). \n"}, {"name": "int fromstr()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.fromstr", "type": "Python Types and C-Structures", "text": "  intfromstr(char*str, void*ip, char**endptr, void*arr)\n \nA pointer to a function that converts the string pointed to by str to one element of the corresponding type and places it in the memory location pointed to by ip. After the conversion is completed, *endptr points to the rest of the string. The last argument arr is the array into which ip points (needed for variable-size data- types). Returns 0 on success or -1 on failure. Requires a behaved array. This function should be called without holding the Python GIL, and has to grab it for error reporting. \n"}, {"name": "int identity", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.identity", "type": "Python Types and C-Structures", "text": "  intidentity\n \nEither PyUFunc_One, PyUFunc_Zero, PyUFunc_MinusOne, PyUFunc_None, PyUFunc_ReorderableNone, or PyUFunc_IdentityValue to indicate the identity for this operation. It is only used for a reduce-like call on an empty array. \n"}, {"name": "int itemsize", "path": "reference/c-api/types-and-structures#c.PyArrayInterface.itemsize", "type": "Python Types and C-Structures", "text": "  intitemsize\n \nThe number of bytes each item in the array requires. \n"}, {"name": "int len", "path": "reference/c-api/types-and-structures#c.PyArray_Dims.len", "type": "Python Types and C-Structures", "text": "  intlen\n \nThe length of the list of integers. It is assumed safe to access ptr [0] to ptr [len-1]. \n"}, {"name": "int nargs", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.nargs", "type": "Python Types and C-Structures", "text": "  intnargs\n \nThe total number of arguments (nin + nout). This must be less than NPY_MAXARGS. \n"}, {"name": "int nd", "path": "reference/c-api/types-and-structures#c.NPY_AO.nd", "type": "Python Types and C-Structures", "text": "  intnd\n \nAn integer providing the number of dimensions for this array. When nd is 0, the array is sometimes called a rank-0 array. Such arrays have undefined dimensions and strides and cannot be accessed. Macro PyArray_NDIM defined in ndarraytypes.h points to this data member. NPY_MAXDIMS is the largest number of dimensions for any array. \n"}, {"name": "int nd", "path": "reference/c-api/types-and-structures#c.PyArrayMultiIterObject.nd", "type": "Python Types and C-Structures", "text": "  intnd\n \nThe number of dimensions in the broadcasted result. \n"}, {"name": "int nd", "path": "reference/c-api/types-and-structures#c.PyArrayInterface.nd", "type": "Python Types and C-Structures", "text": "  intnd\n \nthe number of dimensions in the array. \n"}, {"name": "int nout", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.nout", "type": "Python Types and C-Structures", "text": "  intnout\n \nThe number of output arguments. \n"}, {"name": "int npy_clear_floatstatus()", "path": "reference/c-api/coremath#c.npy_clear_floatstatus", "type": "NumPy core libraries", "text": "  intnpy_clear_floatstatus()\n \nClears the floating point status. Returns the previous status mask. Note that npy_clear_floatstatus_barrier is preferable as it prevents aggressive compiler optimizations reordering the call relative to the code setting the status, which could lead to incorrect results.  New in version 1.9.0.  \n"}, {"name": "int npy_clear_floatstatus_barrier()", "path": "reference/c-api/coremath#c.npy_clear_floatstatus_barrier", "type": "NumPy core libraries", "text": "  intnpy_clear_floatstatus_barrier(char*)\n \nClears the floating point status. A pointer to a local variable is passed in to prevent aggressive compiler optimizations from reordering this function call. Returns the previous status mask.  New in version 1.15.0.  \n"}, {"name": "int npy_get_floatstatus()", "path": "reference/c-api/coremath#c.npy_get_floatstatus", "type": "NumPy core libraries", "text": "  intnpy_get_floatstatus()\n \nGet floating point status. Returns a bitmask with following possible flags:  NPY_FPE_DIVIDEBYZERO NPY_FPE_OVERFLOW NPY_FPE_UNDERFLOW NPY_FPE_INVALID  Note that npy_get_floatstatus_barrier is preferable as it prevents aggressive compiler optimizations reordering the call relative to the code setting the status, which could lead to incorrect results.  New in version 1.9.0.  \n"}, {"name": "int npy_get_floatstatus_barrier()", "path": "reference/c-api/coremath#c.npy_get_floatstatus_barrier", "type": "NumPy core libraries", "text": "  intnpy_get_floatstatus_barrier(char*)\n \nGet floating point status. A pointer to a local variable is passed in to prevent aggressive compiler optimizations from reordering this function call relative to the code setting the status, which could lead to incorrect results. Returns a bitmask with following possible flags:  NPY_FPE_DIVIDEBYZERO NPY_FPE_OVERFLOW NPY_FPE_UNDERFLOW NPY_FPE_INVALID   New in version 1.15.0.  \n"}, {"name": "int npy_half_eq()", "path": "reference/c-api/coremath#c.npy_half_eq", "type": "NumPy core libraries", "text": "  intnpy_half_eq(npy_halfh1, npy_halfh2)\n \nCompares two half-precision floats (h1 == h2). \n"}, {"name": "int npy_half_eq_nonan()", "path": "reference/c-api/coremath#c.npy_half_eq_nonan", "type": "NumPy core libraries", "text": "  intnpy_half_eq_nonan(npy_halfh1, npy_halfh2)\n \nCompares two half-precision floats that are known to not be NaN (h1 == h2). If a value is NaN, the result is undefined. \n"}, {"name": "int npy_half_ge()", "path": "reference/c-api/coremath#c.npy_half_ge", "type": "NumPy core libraries", "text": "  intnpy_half_ge(npy_halfh1, npy_halfh2)\n \nCompares two half-precision floats (h1 >= h2). \n"}, {"name": "int npy_half_gt()", "path": "reference/c-api/coremath#c.npy_half_gt", "type": "NumPy core libraries", "text": "  intnpy_half_gt(npy_halfh1, npy_halfh2)\n \nCompares two half-precision floats (h1 > h2). \n"}, {"name": "int npy_half_isfinite()", "path": "reference/c-api/coremath#c.npy_half_isfinite", "type": "NumPy core libraries", "text": "  intnpy_half_isfinite(npy_halfh)\n \nTests whether the half-precision float is finite (not NaN or Inf). \n"}, {"name": "int npy_half_isinf()", "path": "reference/c-api/coremath#c.npy_half_isinf", "type": "NumPy core libraries", "text": "  intnpy_half_isinf(npy_halfh)\n \nTests whether the half-precision float is plus or minus Inf. \n"}, {"name": "int npy_half_isnan()", "path": "reference/c-api/coremath#c.npy_half_isnan", "type": "NumPy core libraries", "text": "  intnpy_half_isnan(npy_halfh)\n \nTests whether the half-precision float is a NaN. \n"}, {"name": "int npy_half_iszero()", "path": "reference/c-api/coremath#c.npy_half_iszero", "type": "NumPy core libraries", "text": "  intnpy_half_iszero(npy_halfh)\n \nTests whether the half-precision float has a value equal to zero. This may be slightly faster than calling npy_half_eq(h, NPY_ZERO). \n"}, {"name": "int npy_half_le()", "path": "reference/c-api/coremath#c.npy_half_le", "type": "NumPy core libraries", "text": "  intnpy_half_le(npy_halfh1, npy_halfh2)\n \nCompares two half-precision floats (h1 <= h2). \n"}, {"name": "int npy_half_le_nonan()", "path": "reference/c-api/coremath#c.npy_half_le_nonan", "type": "NumPy core libraries", "text": "  intnpy_half_le_nonan(npy_halfh1, npy_halfh2)\n \nCompares two half-precision floats that are known to not be NaN (h1 <= h2). If a value is NaN, the result is undefined. \n"}, {"name": "int npy_half_lt()", "path": "reference/c-api/coremath#c.npy_half_lt", "type": "NumPy core libraries", "text": "  intnpy_half_lt(npy_halfh1, npy_halfh2)\n \nCompares two half-precision floats (h1 < h2). \n"}, {"name": "int npy_half_lt_nonan()", "path": "reference/c-api/coremath#c.npy_half_lt_nonan", "type": "NumPy core libraries", "text": "  intnpy_half_lt_nonan(npy_halfh1, npy_halfh2)\n \nCompares two half-precision floats that are known to not be NaN (h1 < h2). If a value is NaN, the result is undefined. \n"}, {"name": "int npy_half_ne()", "path": "reference/c-api/coremath#c.npy_half_ne", "type": "NumPy core libraries", "text": "  intnpy_half_ne(npy_halfh1, npy_halfh2)\n \nCompares two half-precision floats (h1 != h2). \n"}, {"name": "int npy_half_signbit()", "path": "reference/c-api/coremath#c.npy_half_signbit", "type": "NumPy core libraries", "text": "  intnpy_half_signbit(npy_halfh)\n \nReturns 1 is h is negative, 0 otherwise. \n"}, {"name": "int NpyIter_CreateCompatibleStrides()", "path": "reference/c-api/iterator#c.NpyIter_CreateCompatibleStrides", "type": "Array Iterator API", "text": "  intNpyIter_CreateCompatibleStrides(NpyIter*iter, npy_intpitemsize, npy_intp*outstrides)\n \nBuilds a set of strides which are the same as the strides of an output array created using the NPY_ITER_ALLOCATE flag, where NULL was passed for op_axes. This is for data packed contiguously, but not necessarily in C or Fortran order. This should be used together with NpyIter_GetShape and NpyIter_GetNDim with the flag NPY_ITER_MULTI_INDEX passed into the constructor. A use case for this function is to match the shape and layout of the iterator and tack on one or more dimensions. For example, in order to generate a vector per input value for a numerical gradient, you pass in ndim*itemsize for itemsize, then add another dimension to the end with size ndim and stride itemsize. To do the Hessian matrix, you do the same thing but add two dimensions, or take advantage of the symmetry and pack it into 1 dimension with a particular encoding. This function may only be called if the iterator is tracking a multi-index and if NPY_ITER_DONT_NEGATE_STRIDES was used to prevent an axis from being iterated in reverse order. If an array is created with this method, simply adding \u2018itemsize\u2019 for each iteration will traverse the new array matching the iterator. Returns NPY_SUCCEED or NPY_FAIL. \n"}, {"name": "int NpyIter_Deallocate()", "path": "reference/c-api/iterator#c.NpyIter_Deallocate", "type": "Array Iterator API", "text": "  intNpyIter_Deallocate(NpyIter*iter)\n \nDeallocates the iterator object and resolves any needed writebacks. Returns NPY_SUCCEED or NPY_FAIL. \n"}, {"name": "int NpyIter_EnableExternalLoop()", "path": "reference/c-api/iterator#c.NpyIter_EnableExternalLoop", "type": "Array Iterator API", "text": "  intNpyIter_EnableExternalLoop(NpyIter*iter)\n \nIf NpyIter_RemoveMultiIndex was called, you may want to enable the flag NPY_ITER_EXTERNAL_LOOP. This flag is not permitted together with NPY_ITER_MULTI_INDEX, so this function is provided to enable the feature after NpyIter_RemoveMultiIndex is called. This function also resets the iterator to its initial state. WARNING: This function changes the internal logic of the iterator. Any cached functions or pointers from the iterator must be retrieved again! Returns NPY_SUCCEED or NPY_FAIL. \n"}, {"name": "int NpyIter_GetNDim()", "path": "reference/c-api/iterator#c.NpyIter_GetNDim", "type": "Array Iterator API", "text": "  intNpyIter_GetNDim(NpyIter*iter)\n \nReturns the number of dimensions being iterated. If a multi-index was not requested in the iterator constructor, this value may be smaller than the number of dimensions in the original objects. \n"}, {"name": "int NpyIter_GetNOp()", "path": "reference/c-api/iterator#c.NpyIter_GetNOp", "type": "Array Iterator API", "text": "  intNpyIter_GetNOp(NpyIter*iter)\n \nReturns the number of operands in the iterator. \n"}, {"name": "int NpyIter_GetShape()", "path": "reference/c-api/iterator#c.NpyIter_GetShape", "type": "Array Iterator API", "text": "  intNpyIter_GetShape(NpyIter*iter, npy_intp*outshape)\n \nReturns the broadcast shape of the iterator in outshape. This can only be called on an iterator which is tracking a multi-index. Returns NPY_SUCCEED or NPY_FAIL. \n"}, {"name": "int NpyIter_GotoIndex()", "path": "reference/c-api/iterator#c.NpyIter_GotoIndex", "type": "Array Iterator API", "text": "  intNpyIter_GotoIndex(NpyIter*iter, npy_intpindex)\n \nAdjusts the iterator to point to the index specified. If the iterator was constructed with the flag NPY_ITER_C_INDEX, index is the C-order index, and if the iterator was constructed with the flag NPY_ITER_F_INDEX, index is the Fortran-order index. Returns an error if there is no index being tracked, the index is out of bounds, or inner loop iteration is disabled. Returns NPY_SUCCEED or NPY_FAIL. \n"}, {"name": "int NpyIter_GotoIterIndex()", "path": "reference/c-api/iterator#c.NpyIter_GotoIterIndex", "type": "Array Iterator API", "text": "  intNpyIter_GotoIterIndex(NpyIter*iter, npy_intpiterindex)\n \nAdjusts the iterator to point to the iterindex specified. The IterIndex is an index matching the iteration order of the iterator. Returns an error if the iterindex is out of bounds, buffering is enabled, or inner loop iteration is disabled. Returns NPY_SUCCEED or NPY_FAIL. \n"}, {"name": "int NpyIter_GotoMultiIndex()", "path": "reference/c-api/iterator#c.NpyIter_GotoMultiIndex", "type": "Array Iterator API", "text": "  intNpyIter_GotoMultiIndex(NpyIter*iter, npy_intpconst*multi_index)\n \nAdjusts the iterator to point to the ndim indices pointed to by multi_index. Returns an error if a multi-index is not being tracked, the indices are out of bounds, or inner loop iteration is disabled. Returns NPY_SUCCEED or NPY_FAIL. \n"}, {"name": "int NpyIter_RemoveAxis()", "path": "reference/c-api/iterator#c.NpyIter_RemoveAxis", "type": "Array Iterator API", "text": "  intNpyIter_RemoveAxis(NpyIter*iter, intaxis)\n \nRemoves an axis from iteration. This requires that NPY_ITER_MULTI_INDEX was set for iterator creation, and does not work if buffering is enabled or an index is being tracked. This function also resets the iterator to its initial state. This is useful for setting up an accumulation loop, for example. The iterator can first be created with all the dimensions, including the accumulation axis, so that the output gets created correctly. Then, the accumulation axis can be removed, and the calculation done in a nested fashion. WARNING: This function may change the internal memory layout of the iterator. Any cached functions or pointers from the iterator must be retrieved again! The iterator range will be reset as well. Returns NPY_SUCCEED or NPY_FAIL. \n"}, {"name": "int NpyIter_RemoveMultiIndex()", "path": "reference/c-api/iterator#c.NpyIter_RemoveMultiIndex", "type": "Array Iterator API", "text": "  intNpyIter_RemoveMultiIndex(NpyIter*iter)\n \nIf the iterator is tracking a multi-index, this strips support for them, and does further iterator optimizations that are possible if multi-indices are not needed. This function also resets the iterator to its initial state. WARNING: This function may change the internal memory layout of the iterator. Any cached functions or pointers from the iterator must be retrieved again! After calling this function, NpyIter_HasMultiIndex(iter) will return false. Returns NPY_SUCCEED or NPY_FAIL. \n"}, {"name": "int NpyIter_Reset()", "path": "reference/c-api/iterator#c.NpyIter_Reset", "type": "Array Iterator API", "text": "  intNpyIter_Reset(NpyIter*iter, char**errmsg)\n \nResets the iterator back to its initial state, at the beginning of the iteration range. Returns NPY_SUCCEED or NPY_FAIL. If errmsg is non-NULL, no Python exception is set when NPY_FAIL is returned. Instead, *errmsg is set to an error message. When errmsg is non-NULL, the function may be safely called without holding the Python GIL. \n"}, {"name": "int NpyIter_ResetBasePointers()", "path": "reference/c-api/iterator#c.NpyIter_ResetBasePointers", "type": "Array Iterator API", "text": "  intNpyIter_ResetBasePointers(NpyIter*iter, char**baseptrs, char**errmsg)\n \nResets the iterator back to its initial state, but using the values in baseptrs for the data instead of the pointers from the arrays being iterated. This functions is intended to be used, together with the op_axes parameter, by nested iteration code with two or more iterators. Returns NPY_SUCCEED or NPY_FAIL. If errmsg is non-NULL, no Python exception is set when NPY_FAIL is returned. Instead, *errmsg is set to an error message. When errmsg is non-NULL, the function may be safely called without holding the Python GIL. TODO: Move the following into a special section on nested iterators. Creating iterators for nested iteration requires some care. All the iterator operands must match exactly, or the calls to NpyIter_ResetBasePointers will be invalid. This means that automatic copies and output allocation should not be used haphazardly. It is possible to still use the automatic data conversion and casting features of the iterator by creating one of the iterators with all the conversion parameters enabled, then grabbing the allocated operands with the NpyIter_GetOperandArray function and passing them into the constructors for the rest of the iterators. WARNING: When creating iterators for nested iteration, the code must not use a dimension more than once in the different iterators. If this is done, nested iteration will produce out-of-bounds pointers during iteration. WARNING: When creating iterators for nested iteration, buffering can only be applied to the innermost iterator. If a buffered iterator is used as the source for baseptrs, it will point into a small buffer instead of the array and the inner iteration will be invalid. The pattern for using nested iterators is as follows. NpyIter *iter1, *iter1;\nNpyIter_IterNextFunc *iternext1, *iternext2;\nchar **dataptrs1;\n\n/*\n * With the exact same operands, no copies allowed, and\n * no axis in op_axes used both in iter1 and iter2.\n * Buffering may be enabled for iter2, but not for iter1.\n */\niter1 = ...; iter2 = ...;\n\niternext1 = NpyIter_GetIterNext(iter1);\niternext2 = NpyIter_GetIterNext(iter2);\ndataptrs1 = NpyIter_GetDataPtrArray(iter1);\n\ndo {\n    NpyIter_ResetBasePointers(iter2, dataptrs1);\n    do {\n        /* Use the iter2 values */\n    } while (iternext2(iter2));\n} while (iternext1(iter1));\n \n"}, {"name": "int NpyIter_ResetToIterIndexRange()", "path": "reference/c-api/iterator#c.NpyIter_ResetToIterIndexRange", "type": "Array Iterator API", "text": "  intNpyIter_ResetToIterIndexRange(NpyIter*iter, npy_intpistart, npy_intpiend, char**errmsg)\n \nResets the iterator and restricts it to the iterindex range [istart, iend). See NpyIter_Copy for an explanation of how to use this for multi-threaded iteration. This requires that the flag NPY_ITER_RANGED was passed to the iterator constructor. If you want to reset both the iterindex range and the base pointers at the same time, you can do the following to avoid extra buffer copying (be sure to add the return code error checks when you copy this code). /* Set to a trivial empty range */\nNpyIter_ResetToIterIndexRange(iter, 0, 0);\n/* Set the base pointers */\nNpyIter_ResetBasePointers(iter, baseptrs);\n/* Set to the desired range */\nNpyIter_ResetToIterIndexRange(iter, istart, iend);\n Returns NPY_SUCCEED or NPY_FAIL. If errmsg is non-NULL, no Python exception is set when NPY_FAIL is returned. Instead, *errmsg is set to an error message. When errmsg is non-NULL, the function may be safely called without holding the Python GIL. \n"}, {"name": "int ntypes", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.ntypes", "type": "Python Types and C-Structures", "text": "  intntypes\n \nThe number of supported data types for the ufunc. This number specifies how many different 1-d loops (of the builtin data types) are available. \n"}, {"name": "int PyArray_AxisConverter()", "path": "reference/c-api/array#c.PyArray_AxisConverter", "type": "Array API", "text": "  intPyArray_AxisConverter(PyObject*obj, int*axis)\n \nConvert a Python object, obj, representing an axis argument to the proper value for passing to the functions that take an integer axis. Specifically, if obj is None, axis is set to NPY_MAXDIMS which is interpreted correctly by the C-API functions that take axis arguments. \n"}, {"name": "int PyArray_BoolConverter()", "path": "reference/c-api/array#c.PyArray_BoolConverter", "type": "Array API", "text": "  intPyArray_BoolConverter(PyObject*obj, npy_bool*value)\n \nConvert any Python object, obj, to NPY_TRUE or NPY_FALSE, and place the result in value. \n"}, {"name": "int PyArray_Broadcast()", "path": "reference/c-api/array#c.PyArray_Broadcast", "type": "Array API", "text": "  intPyArray_Broadcast(PyArrayMultiIterObject*mit)\n \nThis function encapsulates the broadcasting rules. The mit container should already contain iterators for all the arrays that need to be broadcast. On return, these iterators will be adjusted so that iteration over each simultaneously will accomplish the broadcasting. A negative number is returned if an error occurs. \n"}, {"name": "int PyArray_BufferConverter()", "path": "reference/c-api/array#c.PyArray_BufferConverter", "type": "Array API", "text": "  intPyArray_BufferConverter(PyObject*obj, PyArray_Chunk*buf)\n \nConvert any Python object, obj, with a (single-segment) buffer interface to a variable with members that detail the object\u2019s use of its chunk of memory. The buf variable is a pointer to a structure with base, ptr, len, and flags members. The PyArray_Chunk structure is binary compatible with the Python\u2019s buffer object (through its len member on 32-bit platforms and its ptr member on 64-bit platforms or in Python 2.5). On return, the base member is set to obj (or its base if obj is already a buffer object pointing to another object). If you need to hold on to the memory be sure to INCREF the base member. The chunk of memory is pointed to by buf ->ptr member and has length buf ->len. The flags member of buf is NPY_ARRAY_ALIGNED with the NPY_ARRAY_WRITEABLE flag set if obj has a writeable buffer interface. \n"}, {"name": "int PyArray_ByteorderConverter()", "path": "reference/c-api/array#c.PyArray_ByteorderConverter", "type": "Array API", "text": "  intPyArray_ByteorderConverter(PyObject*obj, char*endian)\n \nConvert Python strings into the corresponding byte-order character: \u2018>\u2019, \u2018<\u2019, \u2018s\u2019, \u2018=\u2019, or \u2018|\u2019. \n"}, {"name": "int PyArray_CanCastArrayTo()", "path": "reference/c-api/array#c.PyArray_CanCastArrayTo", "type": "Array API", "text": "  intPyArray_CanCastArrayTo(PyArrayObject*arr, PyArray_Descr*totype, NPY_CASTINGcasting)\n \n New in version 1.6.  Returns non-zero if arr can be cast to totype according to the casting rule given in casting. If arr is an array scalar, its value is taken into account, and non-zero is also returned when the value will not overflow or be truncated to an integer when converting to a smaller type. This is almost the same as the result of PyArray_CanCastTypeTo(PyArray_MinScalarType(arr), totype, casting), but it also handles a special case arising because the set of uint values is not a subset of the int values for types with the same number of bits. \n"}, {"name": "int PyArray_CanCastSafely()", "path": "reference/c-api/array#c.PyArray_CanCastSafely", "type": "Array API", "text": "  intPyArray_CanCastSafely(intfromtype, inttotype)\n \nReturns non-zero if an array of data type fromtype can be cast to an array of data type totype without losing information. An exception is that 64-bit integers are allowed to be cast to 64-bit floating point values even though this can lose precision on large integers so as not to proliferate the use of long doubles without explicit requests. Flexible array types are not checked according to their lengths with this function. \n"}, {"name": "int PyArray_CanCastTo()", "path": "reference/c-api/array#c.PyArray_CanCastTo", "type": "Array API", "text": "  intPyArray_CanCastTo(PyArray_Descr*fromtype, PyArray_Descr*totype)\n \nPyArray_CanCastTypeTo supersedes this function in NumPy 1.6 and later. Equivalent to PyArray_CanCastTypeTo(fromtype, totype, NPY_SAFE_CASTING). \n"}, {"name": "int PyArray_CanCastTypeTo()", "path": "reference/c-api/array#c.PyArray_CanCastTypeTo", "type": "Array API", "text": "  intPyArray_CanCastTypeTo(PyArray_Descr*fromtype, PyArray_Descr*totype, NPY_CASTINGcasting)\n \n New in version 1.6.  Returns non-zero if an array of data type fromtype (which can include flexible types) can be cast safely to an array of data type totype (which can include flexible types) according to the casting rule casting. For simple types with NPY_SAFE_CASTING, this is basically a wrapper around PyArray_CanCastSafely, but for flexible types such as strings or unicode, it produces results taking into account their sizes. Integer and float types can only be cast to a string or unicode type using NPY_SAFE_CASTING if the string or unicode type is big enough to hold the max value of the integer/float type being cast from. \n"}, {"name": "int PyArray_CanCoerceScalar()", "path": "reference/c-api/array#c.PyArray_CanCoerceScalar", "type": "Array API", "text": "  intPyArray_CanCoerceScalar(charthistype, charneededtype, NPY_SCALARKINDscalar)\n \nSee the function PyArray_ResultType for details of NumPy type promotion, updated in NumPy 1.6.0. Implements the rules for scalar coercion. Scalars are only silently coerced from thistype to neededtype if this function returns nonzero. If scalar is NPY_NOSCALAR, then this function is equivalent to PyArray_CanCastSafely. The rule is that scalars of the same KIND can be coerced into arrays of the same KIND. This rule means that high-precision scalars will never cause low-precision arrays of the same KIND to be upcast. \n"}, {"name": "int PyArray_CastingConverter()", "path": "reference/c-api/array#c.PyArray_CastingConverter", "type": "Array API", "text": "  intPyArray_CastingConverter(PyObject*obj, NPY_CASTING*casting)\n \nConvert the Python strings \u2018no\u2019, \u2018equiv\u2019, \u2018safe\u2019, \u2018same_kind\u2019, and \u2018unsafe\u2019 into the NPY_CASTING enumeration NPY_NO_CASTING, NPY_EQUIV_CASTING, NPY_SAFE_CASTING, NPY_SAME_KIND_CASTING, and NPY_UNSAFE_CASTING. \n"}, {"name": "int PyArray_CastTo()", "path": "reference/c-api/array#c.PyArray_CastTo", "type": "Array API", "text": "  intPyArray_CastTo(PyArrayObject*out, PyArrayObject*in)\n \nAs of 1.6, this function simply calls PyArray_CopyInto, which handles the casting. Cast the elements of the array in into the array out. The output array should be writeable, have an integer-multiple of the number of elements in the input array (more than one copy can be placed in out), and have a data type that is one of the builtin types. Returns 0 on success and -1 if an error occurs. \n"}, {"name": "int PyArray_CheckAnyScalar()", "path": "reference/c-api/array#c.PyArray_CheckAnyScalar", "type": "Array API", "text": "  intPyArray_CheckAnyScalar(PyObject*op)\n \nEvaluates true if op is a Python scalar object (see PyArray_IsPythonScalar), an array scalar (an instance of a sub-type of PyGenericArr_Type) or an instance of a sub-type of PyArray_Type whose dimensionality is 0. \n"}, {"name": "int PyArray_CheckExact()", "path": "reference/c-api/array#c.PyArray_CheckExact", "type": "Array API", "text": "  intPyArray_CheckExact(PyObject*op)\n \nEvaluates true if op is a Python object with type PyArray_Type. \n"}, {"name": "int PyArray_CheckScalar()", "path": "reference/c-api/array#c.PyArray_CheckScalar", "type": "Array API", "text": "  intPyArray_CheckScalar(PyObject*op)\n \nEvaluates true if op is either an array scalar (an instance of a sub-type of PyGenericArr_Type ), or an instance of (a sub-class of) PyArray_Type whose dimensionality is 0. \n"}, {"name": "int PyArray_ClipmodeConverter()", "path": "reference/c-api/array#c.PyArray_ClipmodeConverter", "type": "Array API", "text": "  intPyArray_ClipmodeConverter(PyObject*object, NPY_CLIPMODE*val)\n \nConvert the Python strings \u2018clip\u2019, \u2018wrap\u2019, and \u2018raise\u2019 into the NPY_CLIPMODE enumeration NPY_CLIP, NPY_WRAP, and NPY_RAISE. \n"}, {"name": "int PyArray_CompareLists()", "path": "reference/c-api/array#c.PyArray_CompareLists", "type": "Array API", "text": "  intPyArray_CompareLists(npy_intpconst*l1, npy_intpconst*l2, intn)\n \nGiven two n -length arrays of integers, l1, and l2, return 1 if the lists are identical; otherwise, return 0. \n"}, {"name": "int PyArray_ConvertClipmodeSequence()", "path": "reference/c-api/array#c.PyArray_ConvertClipmodeSequence", "type": "Array API", "text": "  intPyArray_ConvertClipmodeSequence(PyObject*object, NPY_CLIPMODE*modes, intn)\n \nConverts either a sequence of clipmodes or a single clipmode into a C array of NPY_CLIPMODE values. The number of clipmodes n must be known before calling this function. This function is provided to help functions allow a different clipmode for each dimension. \n"}, {"name": "int PyArray_CopyInto()", "path": "reference/c-api/array#c.PyArray_CopyInto", "type": "Array API", "text": "  intPyArray_CopyInto(PyArrayObject*dest, PyArrayObject*src)\n \nCopy from the source array, src, into the destination array, dest, performing a data-type conversion if necessary. If an error occurs return -1 (otherwise 0). The shape of src must be broadcastable to the shape of dest. The data areas of dest and src must not overlap. \n"}, {"name": "int PyArray_CopyObject()", "path": "reference/c-api/array#c.PyArray_CopyObject", "type": "Array API", "text": "  intPyArray_CopyObject(PyArrayObject*dest, PyObject*src)\n \nAssign an object src to a NumPy array dest according to array-coercion rules. This is basically identical to PyArray_FromAny, but assigns directly to the output array. Returns 0 on success and -1 on failures. \n"}, {"name": "int Pyarray_DescrAlignConverter()", "path": "reference/c-api/array#c.Pyarray_DescrAlignConverter", "type": "Array API", "text": "  intPyarray_DescrAlignConverter(PyObject*obj, PyArray_Descr**dtype)\n \nLike PyArray_DescrConverter except it aligns C-struct-like objects on word-boundaries as the compiler would. \n"}, {"name": "int Pyarray_DescrAlignConverter2()", "path": "reference/c-api/array#c.Pyarray_DescrAlignConverter2", "type": "Array API", "text": "  intPyarray_DescrAlignConverter2(PyObject*obj, PyArray_Descr**dtype)\n \nLike PyArray_DescrConverter2 except it aligns C-struct-like objects on word-boundaries as the compiler would. \n"}, {"name": "int PyArray_DescrConverter()", "path": "reference/c-api/array#c.PyArray_DescrConverter", "type": "Array API", "text": "  intPyArray_DescrConverter(PyObject*obj, PyArray_Descr**dtype)\n \nConvert any compatible Python object, obj, to a data-type object in dtype. A large number of Python objects can be converted to data-type objects. See Data type objects (dtype) for a complete description. This version of the converter converts None objects to a NPY_DEFAULT_TYPE data-type object. This function can be used with the \u201cO&\u201d character code in PyArg_ParseTuple processing. \n"}, {"name": "int PyArray_DescrConverter2()", "path": "reference/c-api/array#c.PyArray_DescrConverter2", "type": "Array API", "text": "  intPyArray_DescrConverter2(PyObject*obj, PyArray_Descr**dtype)\n \nConvert any compatible Python object, obj, to a data-type object in dtype. This version of the converter converts None objects so that the returned data-type is NULL. This function can also be used with the \u201cO&\u201d character in PyArg_ParseTuple processing. \n"}, {"name": "int PyArray_Dump()", "path": "reference/c-api/array#c.PyArray_Dump", "type": "Array API", "text": "  intPyArray_Dump(PyObject*self, PyObject*file, intprotocol)\n \nPickle the object in self to the given file (either a string or a Python file object). If file is a Python string it is considered to be the name of a file which is then opened in binary mode. The given protocol is used (if protocol is negative, or the highest available is used). This is a simple wrapper around cPickle.dump(self, file, protocol). \n"}, {"name": "int PyArray_EquivByteorders()", "path": "reference/c-api/array#c.PyArray_EquivByteorders", "type": "Array API", "text": "  intPyArray_EquivByteorders(intb1, intb2)\n \nTrue if byteorder characters b1 and b2 ( NPY_LITTLE, NPY_BIG, NPY_NATIVE, NPY_IGNORE ) are either equal or equivalent as to their specification of a native byte order. Thus, on a little-endian machine NPY_LITTLE and NPY_NATIVE are equivalent where they are not equivalent on a big-endian machine. \n"}, {"name": "int PyArray_FillWithScalar()", "path": "reference/c-api/array#c.PyArray_FillWithScalar", "type": "Array API", "text": "  intPyArray_FillWithScalar(PyArrayObject*arr, PyObject*obj)\n \nFill the array, arr, with the given scalar object, obj. The object is first converted to the data type of arr, and then copied into every location. A -1 is returned if an error occurs, otherwise 0 is returned. \n"}, {"name": "int PyArray_FinalizeFunc()", "path": "reference/c-api/array#c.PyArray_FinalizeFunc", "type": "Array API", "text": "  intPyArray_FinalizeFunc(PyArrayObject*arr, PyObject*obj)\n \nThe function pointed to by the CObject __array_finalize__. The first argument is the newly created sub-type. The second argument (if not NULL) is the \u201cparent\u201d array (if the array was created using slicing or some other operation where a clearly-distinguishable parent is present). This routine can do anything it wants to. It should return a -1 on error and 0 otherwise. \n"}, {"name": "int PyArray_FLAGS()", "path": "reference/c-api/array#c.PyArray_FLAGS", "type": "Array API", "text": "  intPyArray_FLAGS(PyArrayObject*arr)\n \nReturns an integer representing the array-flags. \n"}, {"name": "int PyArray_Free()", "path": "reference/c-api/array#c.PyArray_Free", "type": "Array API", "text": "  intPyArray_Free(PyObject*op, void*ptr)\n \nMust be called with the same objects and memory locations returned from PyArray_AsCArray (\u2026). This function cleans up memory that otherwise would get leaked. \n"}, {"name": "int PyArray_GetArrayParamsFromObject()", "path": "reference/c-api/array#c.PyArray_GetArrayParamsFromObject", "type": "Array API", "text": "  intPyArray_GetArrayParamsFromObject(PyObject*op, PyArray_Descr*requested_dtype, npy_boolwriteable, PyArray_Descr**out_dtype, int*out_ndim, npy_intp*out_dims, PyArrayObject**out_arr, PyObject*context)\n \n Deprecated since version NumPy: 1.19 Unless NumPy is made aware of an issue with this, this function is scheduled for rapid removal without replacement.   Changed in version NumPy: 1.19 context is never used. Its use results in an error.   New in version 1.6.  \n"}, {"name": "int PyArray_GetEndianness()", "path": "reference/c-api/config#c.PyArray_GetEndianness", "type": "System configuration", "text": "  intPyArray_GetEndianness()\n \n New in version 1.3.0.  Returns the endianness of the current platform. One of NPY_CPU_BIG, NPY_CPU_LITTLE, or NPY_CPU_UNKNOWN_ENDIAN.   NPY_CPU_BIG\n\n   NPY_CPU_LITTLE\n\n   NPY_CPU_UNKNOWN_ENDIAN\n\n \n"}, {"name": "int PyArray_HasArrayInterface()", "path": "reference/c-api/array#c.PyArray_HasArrayInterface", "type": "Array API", "text": "  intPyArray_HasArrayInterface(PyObject*op, PyObject*out)\n \nIf op implements any part of the array interface, then out will contain a new reference to the newly created ndarray using the interface or out will contain NULL if an error during conversion occurs. Otherwise, out will contain a borrowed reference to Py_NotImplemented and no error condition is set. \n"}, {"name": "int PyArray_HasArrayInterfaceType()", "path": "reference/c-api/array#c.PyArray_HasArrayInterfaceType", "type": "Array API", "text": "  intPyArray_HasArrayInterfaceType(PyObject*op, PyArray_Descr*dtype, PyObject*context, PyObject*out)\n \nIf op implements any part of the array interface, then out will contain a new reference to the newly created ndarray using the interface or out will contain NULL if an error during conversion occurs. Otherwise, out will contain a borrowed reference to Py_NotImplemented and no error condition is set. This version allows setting of the dtype in the part of the array interface that looks for the __array__ attribute. context is unused. \n"}, {"name": "int PyArray_HASFIELDS()", "path": "reference/c-api/array#c.PyArray_HASFIELDS", "type": "Array API", "text": "  intPyArray_HASFIELDS(PyArrayObject*obj)\n \nType has fields associated with it. \n"}, {"name": "int PyArray_IntpConverter()", "path": "reference/c-api/array#c.PyArray_IntpConverter", "type": "Array API", "text": "  intPyArray_IntpConverter(PyObject*obj, PyArray_Dims*seq)\n \nConvert any Python sequence, obj, smaller than NPY_MAXDIMS to a C-array of npy_intp. The Python object could also be a single number. The seq variable is a pointer to a structure with members ptr and len. On successful return, seq ->ptr contains a pointer to memory that must be freed, by calling PyDimMem_FREE, to avoid a memory leak. The restriction on memory size allows this converter to be conveniently used for sequences intended to be interpreted as array shapes. \n"}, {"name": "int PyArray_IntpFromSequence()", "path": "reference/c-api/array#c.PyArray_IntpFromSequence", "type": "Array API", "text": "  intPyArray_IntpFromSequence(PyObject*seq, npy_intp*vals, intmaxvals)\n \nConvert any Python sequence (or single Python number) passed in as seq to (up to) maxvals pointer-sized integers and place them in the vals array. The sequence can be smaller then maxvals as the number of converted objects is returned. \n"}, {"name": "int PyArray_IS_C_CONTIGUOUS()", "path": "reference/c-api/array#c.PyArray_IS_C_CONTIGUOUS", "type": "Array API", "text": "  intPyArray_IS_C_CONTIGUOUS(PyObject*arr)\n \nEvaluates true if arr is C-style contiguous. \n"}, {"name": "int PyArray_IS_F_CONTIGUOUS()", "path": "reference/c-api/array#c.PyArray_IS_F_CONTIGUOUS", "type": "Array API", "text": "  intPyArray_IS_F_CONTIGUOUS(PyObject*arr)\n \nEvaluates true if arr is Fortran-style contiguous. \n"}, {"name": "int PyArray_ISALIGNED()", "path": "reference/c-api/array#c.PyArray_ISALIGNED", "type": "Array API", "text": "  intPyArray_ISALIGNED(PyObject*arr)\n \nEvaluates true if the data area of arr is properly aligned on the machine. \n"}, {"name": "int PyArray_IsAnyScalar()", "path": "reference/c-api/array#c.PyArray_IsAnyScalar", "type": "Array API", "text": "  intPyArray_IsAnyScalar(PyObject*op)\n \nEvaluates true if op is either a Python scalar object (see PyArray_IsPythonScalar) or an array scalar (an instance of a sub- type of PyGenericArr_Type ). \n"}, {"name": "int PyArray_ISBEHAVED()", "path": "reference/c-api/array#c.PyArray_ISBEHAVED", "type": "Array API", "text": "  intPyArray_ISBEHAVED(PyObject*arr)\n \nEvaluates true if the data area of arr is aligned and writeable and in machine byte-order according to its descriptor. \n"}, {"name": "int PyArray_ISBEHAVED_RO()", "path": "reference/c-api/array#c.PyArray_ISBEHAVED_RO", "type": "Array API", "text": "  intPyArray_ISBEHAVED_RO(PyObject*arr)\n \nEvaluates true if the data area of arr is aligned and in machine byte-order. \n"}, {"name": "int PyArray_ISBOOL()", "path": "reference/c-api/array#c.PyArray_ISBOOL", "type": "Array API", "text": "  intPyArray_ISBOOL(PyArrayObject*obj)\n \nType represents Boolean data type. \n"}, {"name": "int PyArray_ISBYTESWAPPED()", "path": "reference/c-api/array#c.PyArray_ISBYTESWAPPED", "type": "Array API", "text": "  intPyArray_ISBYTESWAPPED(PyArrayObject*m)\n \nEvaluates true if the data area of the ndarray m is not in machine byte-order according to the array\u2019s data-type descriptor. \n"}, {"name": "int PyArray_ISCARRAY()", "path": "reference/c-api/array#c.PyArray_ISCARRAY", "type": "Array API", "text": "  intPyArray_ISCARRAY(PyObject*arr)\n \nEvaluates true if the data area of arr is C-style contiguous, and PyArray_ISBEHAVED (arr) is true. \n"}, {"name": "int PyArray_ISCARRAY_RO()", "path": "reference/c-api/array#c.PyArray_ISCARRAY_RO", "type": "Array API", "text": "  intPyArray_ISCARRAY_RO(PyObject*arr)\n \nEvaluates true if the data area of arr is C-style contiguous, aligned, and in machine byte-order. \n"}, {"name": "int PyArray_ISCOMPLEX()", "path": "reference/c-api/array#c.PyArray_ISCOMPLEX", "type": "Array API", "text": "  intPyArray_ISCOMPLEX(PyArrayObject*obj)\n \nType represents any complex floating point number. \n"}, {"name": "int PyArray_ISEXTENDED()", "path": "reference/c-api/array#c.PyArray_ISEXTENDED", "type": "Array API", "text": "  intPyArray_ISEXTENDED(PyArrayObject*obj)\n \nType is either flexible or user-defined. \n"}, {"name": "int PyArray_ISFARRAY()", "path": "reference/c-api/array#c.PyArray_ISFARRAY", "type": "Array API", "text": "  intPyArray_ISFARRAY(PyObject*arr)\n \nEvaluates true if the data area of arr is Fortran-style contiguous and PyArray_ISBEHAVED (arr) is true. \n"}, {"name": "int PyArray_ISFARRAY_RO()", "path": "reference/c-api/array#c.PyArray_ISFARRAY_RO", "type": "Array API", "text": "  intPyArray_ISFARRAY_RO(PyObject*arr)\n \nEvaluates true if the data area of arr is Fortran-style contiguous, aligned, and in machine byte-order . \n"}, {"name": "int PyArray_ISFLEXIBLE()", "path": "reference/c-api/array#c.PyArray_ISFLEXIBLE", "type": "Array API", "text": "  intPyArray_ISFLEXIBLE(PyArrayObject*obj)\n \nType represents one of the flexible array types ( NPY_STRING, NPY_UNICODE, or NPY_VOID ). \n"}, {"name": "int PyArray_ISFLOAT()", "path": "reference/c-api/array#c.PyArray_ISFLOAT", "type": "Array API", "text": "  intPyArray_ISFLOAT(PyArrayObject*obj)\n \nType represents any floating point number. \n"}, {"name": "int PyArray_ISFORTRAN()", "path": "reference/c-api/array#c.PyArray_ISFORTRAN", "type": "Array API", "text": "  intPyArray_ISFORTRAN(PyObject*arr)\n \nEvaluates true if arr is Fortran-style contiguous and not C-style contiguous. PyArray_IS_F_CONTIGUOUS is the correct way to test for Fortran-style contiguity. \n"}, {"name": "int PyArray_ISINTEGER()", "path": "reference/c-api/array#c.PyArray_ISINTEGER", "type": "Array API", "text": "  intPyArray_ISINTEGER(PyArrayObject*obj)\n \nType represents any integer. \n"}, {"name": "int PyArray_ISNOTSWAPPED()", "path": "reference/c-api/array#c.PyArray_ISNOTSWAPPED", "type": "Array API", "text": "  intPyArray_ISNOTSWAPPED(PyArrayObject*m)\n \nEvaluates true if the data area of the ndarray m is in machine byte-order according to the array\u2019s data-type descriptor. \n"}, {"name": "int PyArray_ISNUMBER()", "path": "reference/c-api/array#c.PyArray_ISNUMBER", "type": "Array API", "text": "  intPyArray_ISNUMBER(PyArrayObject*obj)\n \nType represents any integer, floating point, or complex floating point number. \n"}, {"name": "int PyArray_ISOBJECT()", "path": "reference/c-api/array#c.PyArray_ISOBJECT", "type": "Array API", "text": "  intPyArray_ISOBJECT(PyArrayObject*obj)\n \nType represents object data type. \n"}, {"name": "int PyArray_ISONESEGMENT()", "path": "reference/c-api/array#c.PyArray_ISONESEGMENT", "type": "Array API", "text": "  intPyArray_ISONESEGMENT(PyObject*arr)\n \nEvaluates true if the data area of arr consists of a single (C-style or Fortran-style) contiguous segment. \n"}, {"name": "int PyArray_ISPYTHON()", "path": "reference/c-api/array#c.PyArray_ISPYTHON", "type": "Array API", "text": "  intPyArray_ISPYTHON(PyArrayObject*obj)\n \nType represents an enumerated type corresponding to one of the standard Python scalar (bool, int, float, or complex). \n"}, {"name": "int PyArray_IsPythonNumber()", "path": "reference/c-api/array#c.PyArray_IsPythonNumber", "type": "Array API", "text": "  intPyArray_IsPythonNumber(PyObject*op)\n \nEvaluates true if op is an instance of a builtin numeric type (int, float, complex, long, bool) \n"}, {"name": "int PyArray_IsPythonScalar()", "path": "reference/c-api/array#c.PyArray_IsPythonScalar", "type": "Array API", "text": "  intPyArray_IsPythonScalar(PyObject*op)\n \nEvaluates true if op is a builtin Python scalar object (int, float, complex, bytes, str, long, bool). \n"}, {"name": "int PyArray_ISSIGNED()", "path": "reference/c-api/array#c.PyArray_ISSIGNED", "type": "Array API", "text": "  intPyArray_ISSIGNED(PyArrayObject*obj)\n \nType represents a signed integer. \n"}, {"name": "int PyArray_ISSTRING()", "path": "reference/c-api/array#c.PyArray_ISSTRING", "type": "Array API", "text": "  intPyArray_ISSTRING(PyArrayObject*obj)\n \nType represents a string data type. \n"}, {"name": "int PyArray_ISUNSIGNED()", "path": "reference/c-api/array#c.PyArray_ISUNSIGNED", "type": "Array API", "text": "  intPyArray_ISUNSIGNED(PyArrayObject*obj)\n \nType represents an unsigned integer. \n"}, {"name": "int PyArray_ISUSERDEF()", "path": "reference/c-api/array#c.PyArray_ISUSERDEF", "type": "Array API", "text": "  intPyArray_ISUSERDEF(PyArrayObject*obj)\n \nType represents a user-defined type. \n"}, {"name": "int PyArray_ISWRITEABLE()", "path": "reference/c-api/array#c.PyArray_ISWRITEABLE", "type": "Array API", "text": "  intPyArray_ISWRITEABLE(PyObject*arr)\n \nEvaluates true if the data area of arr can be written to \n"}, {"name": "int PyArray_IsZeroDim()", "path": "reference/c-api/array#c.PyArray_IsZeroDim", "type": "Array API", "text": "  intPyArray_IsZeroDim(PyObject*op)\n \nEvaluates true if op is an instance of (a subclass of) PyArray_Type and has 0 dimensions. \n"}, {"name": "int PyArray_ITER_NOTDONE()", "path": "reference/c-api/array#c.PyArray_ITER_NOTDONE", "type": "Array API", "text": "  intPyArray_ITER_NOTDONE(PyObject*iterator)\n \nEvaluates TRUE as long as the iterator has not looped through all of the elements, otherwise it evaluates FALSE. \n"}, {"name": "int PyArray_MoveInto()", "path": "reference/c-api/array#c.PyArray_MoveInto", "type": "Array API", "text": "  intPyArray_MoveInto(PyArrayObject*dest, PyArrayObject*src)\n \nMove data from the source array, src, into the destination array, dest, performing a data-type conversion if necessary. If an error occurs return -1 (otherwise 0). The shape of src must be broadcastable to the shape of dest. The data areas of dest and src may overlap. \n"}, {"name": "int PyArray_MultiIter_NOTDONE()", "path": "reference/c-api/array#c.PyArray_MultiIter_NOTDONE", "type": "Array API", "text": "  intPyArray_MultiIter_NOTDONE(PyObject*multi)\n \nEvaluates TRUE as long as the multi-iterator has not looped through all of the elements (of the broadcasted result), otherwise it evaluates FALSE. \n"}, {"name": "int PyArray_MultiplyIntList()", "path": "reference/c-api/array#c.PyArray_MultiplyIntList", "type": "Array API", "text": "  intPyArray_MultiplyIntList(intconst*seq, intn)\n \nBoth of these routines multiply an n -length array, seq, of integers and return the result. No overflow checking is performed. \n"}, {"name": "int PyArray_NDIM()", "path": "reference/c-api/array", "type": "Array API", "text": "Array API  Array structure and data access These macros access the PyArrayObject structure members and are defined in ndarraytypes.h. The input argument, arr, can be any PyObject* that is directly interpretable as a PyArrayObject* (any instance of the PyArray_Type and its sub-types).   intPyArray_NDIM(PyArrayObject*arr)\n \nThe number of dimensions in the array. \n   intPyArray_FLAGS(PyArrayObject*arr)\n \nReturns an integer representing the array-flags. \n   intPyArray_TYPE(PyArrayObject*arr)\n \nReturn the (builtin) typenumber for the elements of this array. \n   intPyArray_SETITEM(PyArrayObject*arr, void*itemptr, PyObject*obj)\n \nConvert obj and place it in the ndarray, arr, at the place pointed to by itemptr. Return -1 if an error occurs or 0 on success. \n   voidPyArray_ENABLEFLAGS(PyArrayObject*arr, intflags)\n \n New in version 1.7.  Enables the specified array flags. This function does no validation, and assumes that you know what you\u2019re doing. \n   voidPyArray_CLEARFLAGS(PyArrayObject*arr, intflags)\n \n New in version 1.7.  Clears the specified array flags. This function does no validation, and assumes that you know what you\u2019re doing. \n   void*PyArray_DATA(PyArrayObject*arr)\n\n   char*PyArray_BYTES(PyArrayObject*arr)\n \nThese two macros are similar and obtain the pointer to the data-buffer for the array. The first macro can (and should be) assigned to a particular pointer where the second is for generic processing. If you have not guaranteed a contiguous and/or aligned array then be sure you understand how to access the data in the array to avoid memory and/or alignment problems. \n   npy_intp*PyArray_DIMS(PyArrayObject*arr)\n \nReturns a pointer to the dimensions/shape of the array. The number of elements matches the number of dimensions of the array. Can return NULL for 0-dimensional arrays. \n   npy_intp*PyArray_SHAPE(PyArrayObject*arr)\n \n New in version 1.7.  A synonym for PyArray_DIMS, named to be consistent with the shape usage within Python. \n   npy_intp*PyArray_STRIDES(PyArrayObject*arr)\n \nReturns a pointer to the strides of the array. The number of elements matches the number of dimensions of the array. \n   npy_intpPyArray_DIM(PyArrayObject*arr, intn)\n \nReturn the shape in the n \\(^{\\textrm{th}}\\) dimension. \n   npy_intpPyArray_STRIDE(PyArrayObject*arr, intn)\n \nReturn the stride in the n \\(^{\\textrm{th}}\\) dimension. \n   npy_intpPyArray_ITEMSIZE(PyArrayObject*arr)\n \nReturn the itemsize for the elements of this array. Note that, in the old API that was deprecated in version 1.7, this function had the return type int. \n   npy_intpPyArray_SIZE(PyArrayObject*arr)\n \nReturns the total size (in number of elements) of the array. \n   npy_intpPyArray_Size(PyArrayObject*obj)\n \nReturns 0 if obj is not a sub-class of ndarray. Otherwise, returns the total number of elements in the array. Safer version of PyArray_SIZE (obj). \n   npy_intpPyArray_NBYTES(PyArrayObject*arr)\n \nReturns the total number of bytes consumed by the array. \n   PyObject*PyArray_BASE(PyArrayObject*arr)\n \nThis returns the base object of the array. In most cases, this means the object which owns the memory the array is pointing at. If you are constructing an array using the C API, and specifying your own memory, you should use the function PyArray_SetBaseObject to set the base to an object which owns the memory. If the (deprecated) NPY_ARRAY_UPDATEIFCOPY or the NPY_ARRAY_WRITEBACKIFCOPY flags are set, it has a different meaning, namely base is the array into which the current array will be copied upon copy resolution. This overloading of the base property for two functions is likely to change in a future version of NumPy. \n   PyArray_Descr*PyArray_DESCR(PyArrayObject*arr)\n \nReturns a borrowed reference to the dtype property of the array. \n   PyArray_Descr*PyArray_DTYPE(PyArrayObject*arr)\n \n New in version 1.7.  A synonym for PyArray_DESCR, named to be consistent with the \u2018dtype\u2019 usage within Python. \n   PyObject*PyArray_GETITEM(PyArrayObject*arr, void*itemptr)\n \nGet a Python object of a builtin type from the ndarray, arr, at the location pointed to by itemptr. Return NULL on failure. numpy.ndarray.item is identical to PyArray_GETITEM. \n   intPyArray_FinalizeFunc(PyArrayObject*arr, PyObject*obj)\n \nThe function pointed to by the CObject __array_finalize__. The first argument is the newly created sub-type. The second argument (if not NULL) is the \u201cparent\u201d array (if the array was created using slicing or some other operation where a clearly-distinguishable parent is present). This routine can do anything it wants to. It should return a -1 on error and 0 otherwise. \n  Data access These functions and macros provide easy access to elements of the ndarray from C. These work for all arrays. You may need to take care when accessing the data in the array, however, if it is not in machine byte-order, misaligned, or not writeable. In other words, be sure to respect the state of the flags unless you know what you are doing, or have previously guaranteed an array that is writeable, aligned, and in machine byte-order using PyArray_FromAny. If you wish to handle all types of arrays, the copyswap function for each type is useful for handling misbehaved arrays. Some platforms (e.g. Solaris) do not like misaligned data and will crash if you de-reference a misaligned pointer. Other platforms (e.g. x86 Linux) will just work more slowly with misaligned data.   void*PyArray_GetPtr(PyArrayObject*aobj, npy_intp*ind)\n \nReturn a pointer to the data of the ndarray, aobj, at the N-dimensional index given by the c-array, ind, (which must be at least aobj ->nd in size). You may want to typecast the returned pointer to the data type of the ndarray. \n   void*PyArray_GETPTR1(PyArrayObject*obj, npy_intpi)\n\n   void*PyArray_GETPTR2(PyArrayObject*obj, npy_intpi, npy_intpj)\n\n   void*PyArray_GETPTR3(PyArrayObject*obj, npy_intpi, npy_intpj, npy_intpk)\n\n   void*PyArray_GETPTR4(PyArrayObject*obj, npy_intpi, npy_intpj, npy_intpk, npy_intpl)\n \nQuick, inline access to the element at the given coordinates in the ndarray, obj, which must have respectively 1, 2, 3, or 4 dimensions (this is not checked). The corresponding i, j, k, and l coordinates can be any integer but will be interpreted as npy_intp. You may want to typecast the returned pointer to the data type of the ndarray. \n    Creating arrays  From scratch   PyObject*PyArray_NewFromDescr(PyTypeObject*subtype, PyArray_Descr*descr, intnd, npy_intpconst*dims, npy_intpconst*strides, void*data, intflags, PyObject*obj)\n \nThis function steals a reference to descr. The easiest way to get one is using PyArray_DescrFromType. This is the main array creation function. Most new arrays are created with this flexible function. The returned object is an object of Python-type subtype, which must be a subtype of PyArray_Type. The array has nd dimensions, described by dims. The data-type descriptor of the new array is descr. If subtype is of an array subclass instead of the base &PyArray_Type, then obj is the object to pass to the __array_finalize__ method of the subclass. If data is NULL, then new unitinialized memory will be allocated and flags can be non-zero to indicate a Fortran-style contiguous array. Use PyArray_FILLWBYTE to initialize the memory. If data is not NULL, then it is assumed to point to the memory to be used for the array and the flags argument is used as the new flags for the array (except the state of NPY_ARRAY_OWNDATA, NPY_ARRAY_WRITEBACKIFCOPY and NPY_ARRAY_UPDATEIFCOPY flags of the new array will be reset). In addition, if data is non-NULL, then strides can also be provided. If strides is NULL, then the array strides are computed as C-style contiguous (default) or Fortran-style contiguous (flags is nonzero for data = NULL or flags & NPY_ARRAY_F_CONTIGUOUS is nonzero non-NULL data). Any provided dims and strides are copied into newly allocated dimension and strides arrays for the new array object. PyArray_CheckStrides can help verify non- NULL stride information. If data is provided, it must stay alive for the life of the array. One way to manage this is through PyArray_SetBaseObject \n   PyObject*PyArray_NewLikeArray(PyArrayObject*prototype, NPY_ORDERorder, PyArray_Descr*descr, intsubok)\n \n New in version 1.6.  This function steals a reference to descr if it is not NULL. This array creation routine allows for the convenient creation of a new array matching an existing array\u2019s shapes and memory layout, possibly changing the layout and/or data type. When order is NPY_ANYORDER, the result order is NPY_FORTRANORDER if prototype is a fortran array, NPY_CORDER otherwise. When order is NPY_KEEPORDER, the result order matches that of prototype, even when the axes of prototype aren\u2019t in C or Fortran order. If descr is NULL, the data type of prototype is used. If subok is 1, the newly created array will use the sub-type of prototype to create the new array, otherwise it will create a base-class array. \n   PyObject*PyArray_New(PyTypeObject*subtype, intnd, npy_intpconst*dims, inttype_num, npy_intpconst*strides, void*data, intitemsize, intflags, PyObject*obj)\n \nThis is similar to PyArray_NewFromDescr (\u2026) except you specify the data-type descriptor with type_num and itemsize, where type_num corresponds to a builtin (or user-defined) type. If the type always has the same number of bytes, then itemsize is ignored. Otherwise, itemsize specifies the particular size of this array. \n  Warning If data is passed to PyArray_NewFromDescr or PyArray_New, this memory must not be deallocated until the new array is deleted. If this data came from another Python object, this can be accomplished using Py_INCREF on that object and setting the base member of the new array to point to that object. If strides are passed in they must be consistent with the dimensions, the itemsize, and the data of the array.    PyObject*PyArray_SimpleNew(intnd, npy_intpconst*dims, inttypenum)\n \nCreate a new uninitialized array of type, typenum, whose size in each of nd dimensions is given by the integer array, dims.The memory for the array is uninitialized (unless typenum is NPY_OBJECT in which case each element in the array is set to NULL). The typenum argument allows specification of any of the builtin data-types such as NPY_FLOAT or NPY_LONG. The memory for the array can be set to zero if desired using PyArray_FILLWBYTE (return_object, 0).This function cannot be used to create a flexible-type array (no itemsize given). \n   PyObject*PyArray_SimpleNewFromData(intnd, npy_intpconst*dims, inttypenum, void*data)\n \nCreate an array wrapper around data pointed to by the given pointer. The array flags will have a default that the data area is well-behaved and C-style contiguous. The shape of the array is given by the dims c-array of length nd. The data-type of the array is indicated by typenum. If data comes from another reference-counted Python object, the reference count on this object should be increased after the pointer is passed in, and the base member of the returned ndarray should point to the Python object that owns the data. This will ensure that the provided memory is not freed while the returned array is in existence. \n   PyObject*PyArray_SimpleNewFromDescr(intnd, npy_intconst*dims, PyArray_Descr*descr)\n \nThis function steals a reference to descr. Create a new array with the provided data-type descriptor, descr, of the shape determined by nd and dims. \n   voidPyArray_FILLWBYTE(PyObject*obj, intval)\n \nFill the array pointed to by obj \u2014which must be a (subclass of) ndarray\u2014with the contents of val (evaluated as a byte). This macro calls memset, so obj must be contiguous. \n   PyObject*PyArray_Zeros(intnd, npy_intpconst*dims, PyArray_Descr*dtype, intfortran)\n \nConstruct a new nd -dimensional array with shape given by dims and data type given by dtype. If fortran is non-zero, then a Fortran-order array is created, otherwise a C-order array is created. Fill the memory with zeros (or the 0 object if dtype corresponds to NPY_OBJECT ). \n   PyObject*PyArray_ZEROS(intnd, npy_intpconst*dims, inttype_num, intfortran)\n \nMacro form of PyArray_Zeros which takes a type-number instead of a data-type object. \n   PyObject*PyArray_Empty(intnd, npy_intpconst*dims, PyArray_Descr*dtype, intfortran)\n \nConstruct a new nd -dimensional array with shape given by dims and data type given by dtype. If fortran is non-zero, then a Fortran-order array is created, otherwise a C-order array is created. The array is uninitialized unless the data type corresponds to NPY_OBJECT in which case the array is filled with Py_None. \n   PyObject*PyArray_EMPTY(intnd, npy_intpconst*dims, inttypenum, intfortran)\n \nMacro form of PyArray_Empty which takes a type-number, typenum, instead of a data-type object. \n   PyObject*PyArray_Arange(doublestart, doublestop, doublestep, inttypenum)\n \nConstruct a new 1-dimensional array of data-type, typenum, that ranges from start to stop (exclusive) in increments of step . Equivalent to arange (start, stop, step, dtype). \n   PyObject*PyArray_ArangeObj(PyObject*start, PyObject*stop, PyObject*step, PyArray_Descr*descr)\n \nConstruct a new 1-dimensional array of data-type determined by descr, that ranges from start to stop (exclusive) in increments of step. Equivalent to arange( start, stop, step, typenum ). \n   intPyArray_SetBaseObject(PyArrayObject*arr, PyObject*obj)\n \n New in version 1.7.  This function steals a reference to obj and sets it as the base property of arr. If you construct an array by passing in your own memory buffer as a parameter, you need to set the array\u2019s base property to ensure the lifetime of the memory buffer is appropriate. The return value is 0 on success, -1 on failure. If the object provided is an array, this function traverses the chain of base pointers so that each array points to the owner of the memory directly. Once the base is set, it may not be changed to another value. \n   From other objects   PyObject*PyArray_FromAny(PyObject*op, PyArray_Descr*dtype, intmin_depth, intmax_depth, intrequirements, PyObject*context)\n \nThis is the main function used to obtain an array from any nested sequence, or object that exposes the array interface, op. The parameters allow specification of the required dtype, the minimum (min_depth) and maximum (max_depth) number of dimensions acceptable, and other requirements for the array. This function steals a reference to the dtype argument, which needs to be a PyArray_Descr structure indicating the desired data-type (including required byteorder). The dtype argument may be NULL, indicating that any data-type (and byteorder) is acceptable. Unless NPY_ARRAY_FORCECAST is present in flags, this call will generate an error if the data type cannot be safely obtained from the object. If you want to use NULL for the dtype and ensure the array is notswapped then use PyArray_CheckFromAny. A value of 0 for either of the depth parameters causes the parameter to be ignored. Any of the following array flags can be added (e.g. using |) to get the requirements argument. If your code can handle general (e.g. strided, byte-swapped, or unaligned arrays) then requirements may be 0. Also, if op is not already an array (or does not expose the array interface), then a new array will be created (and filled from op using the sequence protocol). The new array will have NPY_ARRAY_DEFAULT as its flags member. The context argument is unused.   NPY_ARRAY_C_CONTIGUOUS\n \nMake sure the returned array is C-style contiguous \n   NPY_ARRAY_F_CONTIGUOUS\n \nMake sure the returned array is Fortran-style contiguous. \n   NPY_ARRAY_ALIGNED\n \nMake sure the returned array is aligned on proper boundaries for its data type. An aligned array has the data pointer and every strides factor as a multiple of the alignment factor for the data-type- descriptor. \n   NPY_ARRAY_WRITEABLE\n \nMake sure the returned array can be written to. \n   NPY_ARRAY_ENSURECOPY\n \nMake sure a copy is made of op. If this flag is not present, data is not copied if it can be avoided. \n   NPY_ARRAY_ENSUREARRAY\n \nMake sure the result is a base-class ndarray. By default, if op is an instance of a subclass of ndarray, an instance of that same subclass is returned. If this flag is set, an ndarray object will be returned instead. \n   NPY_ARRAY_FORCECAST\n \nForce a cast to the output type even if it cannot be done safely. Without this flag, a data cast will occur only if it can be done safely, otherwise an error is raised. \n   NPY_ARRAY_WRITEBACKIFCOPY\n \nIf op is already an array, but does not satisfy the requirements, then a copy is made (which will satisfy the requirements). If this flag is present and a copy (of an object that is already an array) must be made, then the corresponding NPY_ARRAY_WRITEBACKIFCOPY flag is set in the returned copy and op is made to be read-only. You must be sure to call PyArray_ResolveWritebackIfCopy to copy the contents back into op and the op array will be made writeable again. If op is not writeable to begin with, or if it is not already an array, then an error is raised. \n   NPY_ARRAY_UPDATEIFCOPY\n \nDeprecated. Use NPY_ARRAY_WRITEBACKIFCOPY, which is similar. This flag \u201cautomatically\u201d copies the data back when the returned array is deallocated, which is not supported in all python implementations. \n   NPY_ARRAY_BEHAVED\n \nNPY_ARRAY_ALIGNED | NPY_ARRAY_WRITEABLE \n   NPY_ARRAY_CARRAY\n \nNPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_BEHAVED \n   NPY_ARRAY_CARRAY_RO\n \nNPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_ALIGNED \n   NPY_ARRAY_FARRAY\n \nNPY_ARRAY_F_CONTIGUOUS | NPY_ARRAY_BEHAVED \n   NPY_ARRAY_FARRAY_RO\n \nNPY_ARRAY_F_CONTIGUOUS | NPY_ARRAY_ALIGNED \n   NPY_ARRAY_DEFAULT\n \nNPY_ARRAY_CARRAY \n \n   NPY_ARRAY_IN_ARRAY\n \nNPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_ALIGNED   NPY_ARRAY_IN_FARRAY\n \nNPY_ARRAY_F_CONTIGUOUS | NPY_ARRAY_ALIGNED \n \n   NPY_OUT_ARRAY\n \nNPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_WRITEABLE | NPY_ARRAY_ALIGNED \n   NPY_ARRAY_OUT_ARRAY\n \nNPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_ALIGNED | NPY_ARRAY_WRITEABLE   NPY_ARRAY_OUT_FARRAY\n \nNPY_ARRAY_F_CONTIGUOUS | NPY_ARRAY_WRITEABLE | NPY_ARRAY_ALIGNED \n \n   NPY_ARRAY_INOUT_ARRAY\n \nNPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_WRITEABLE | NPY_ARRAY_ALIGNED | NPY_ARRAY_WRITEBACKIFCOPY | NPY_ARRAY_UPDATEIFCOPY   NPY_ARRAY_INOUT_FARRAY\n \nNPY_ARRAY_F_CONTIGUOUS | NPY_ARRAY_WRITEABLE | NPY_ARRAY_ALIGNED | NPY_ARRAY_WRITEBACKIFCOPY | NPY_ARRAY_UPDATEIFCOPY \n \n   intPyArray_GetArrayParamsFromObject(PyObject*op, PyArray_Descr*requested_dtype, npy_boolwriteable, PyArray_Descr**out_dtype, int*out_ndim, npy_intp*out_dims, PyArrayObject**out_arr, PyObject*context)\n \n Deprecated since version NumPy: 1.19 Unless NumPy is made aware of an issue with this, this function is scheduled for rapid removal without replacement.   Changed in version NumPy: 1.19 context is never used. Its use results in an error.   New in version 1.6.  \n   PyObject*PyArray_CheckFromAny(PyObject*op, PyArray_Descr*dtype, intmin_depth, intmax_depth, intrequirements, PyObject*context)\n \nNearly identical to PyArray_FromAny (\u2026) except requirements can contain NPY_ARRAY_NOTSWAPPED (over-riding the specification in dtype) and NPY_ARRAY_ELEMENTSTRIDES which indicates that the array should be aligned in the sense that the strides are multiples of the element size. In versions 1.6 and earlier of NumPy, the following flags did not have the _ARRAY_ macro namespace in them. That form of the constant names is deprecated in 1.7. \n   NPY_ARRAY_NOTSWAPPED\n \nMake sure the returned array has a data-type descriptor that is in machine byte-order, over-riding any specification in the dtype argument. Normally, the byte-order requirement is determined by the dtype argument. If this flag is set and the dtype argument does not indicate a machine byte-order descriptor (or is NULL and the object is already an array with a data-type descriptor that is not in machine byte- order), then a new data-type descriptor is created and used with its byte-order field set to native.   NPY_ARRAY_BEHAVED_NS\n \nNPY_ARRAY_ALIGNED | NPY_ARRAY_WRITEABLE | NPY_ARRAY_NOTSWAPPED \n \n   NPY_ARRAY_ELEMENTSTRIDES\n \nMake sure the returned array has strides that are multiples of the element size. \n   PyObject*PyArray_FromArray(PyArrayObject*op, PyArray_Descr*newtype, intrequirements)\n \nSpecial case of PyArray_FromAny for when op is already an array but it needs to be of a specific newtype (including byte-order) or has certain requirements. \n   PyObject*PyArray_FromStructInterface(PyObject*op)\n \nReturns an ndarray object from a Python object that exposes the __array_struct__ attribute and follows the array interface protocol. If the object does not contain this attribute then a borrowed reference to Py_NotImplemented is returned. \n   PyObject*PyArray_FromInterface(PyObject*op)\n \nReturns an ndarray object from a Python object that exposes the __array_interface__ attribute following the array interface protocol. If the object does not contain this attribute then a borrowed reference to Py_NotImplemented is returned. \n   PyObject*PyArray_FromArrayAttr(PyObject*op, PyArray_Descr*dtype, PyObject*context)\n \nReturn an ndarray object from a Python object that exposes the __array__ method. The __array__ method can take 0, or 1 argument ([dtype]). context is unused. \n   PyObject*PyArray_ContiguousFromAny(PyObject*op, inttypenum, intmin_depth, intmax_depth)\n \nThis function returns a (C-style) contiguous and behaved function array from any nested sequence or array interface exporting object, op, of (non-flexible) type given by the enumerated typenum, of minimum depth min_depth, and of maximum depth max_depth. Equivalent to a call to PyArray_FromAny with requirements set to NPY_ARRAY_DEFAULT and the type_num member of the type argument set to typenum. \n   PyObject*PyArray_ContiguousFromObject(PyObject*op, inttypenum, intmin_depth, intmax_depth)\n \nThis function returns a well-behaved C-style contiguous array from any nested sequence or array-interface exporting object. The minimum number of dimensions the array can have is given by min_depth while the maximum is max_depth. This is equivalent to call PyArray_FromAny with requirements NPY_ARRAY_DEFAULT and NPY_ARRAY_ENSUREARRAY. \n   PyObject*PyArray_FromObject(PyObject*op, inttypenum, intmin_depth, intmax_depth)\n \nReturn an aligned and in native-byteorder array from any nested sequence or array-interface exporting object, op, of a type given by the enumerated typenum. The minimum number of dimensions the array can have is given by min_depth while the maximum is max_depth. This is equivalent to a call to PyArray_FromAny with requirements set to BEHAVED. \n   PyObject*PyArray_EnsureArray(PyObject*op)\n \nThis function steals a reference to op and makes sure that op is a base-class ndarray. It special cases array scalars, but otherwise calls PyArray_FromAny ( op, NULL, 0, 0, NPY_ARRAY_ENSUREARRAY, NULL). \n   PyObject*PyArray_FromString(char*string, npy_intpslen, PyArray_Descr*dtype, npy_intpnum, char*sep)\n \nConstruct a one-dimensional ndarray of a single type from a binary or (ASCII) text string of length slen. The data-type of the array to-be-created is given by dtype. If num is -1, then copy the entire string and return an appropriately sized array, otherwise, num is the number of items to copy from the string. If sep is NULL (or \u201c\u201d), then interpret the string as bytes of binary data, otherwise convert the sub-strings separated by sep to items of data-type dtype. Some data-types may not be readable in text mode and an error will be raised if that occurs. All errors return NULL. \n   PyObject*PyArray_FromFile(FILE*fp, PyArray_Descr*dtype, npy_intpnum, char*sep)\n \nConstruct a one-dimensional ndarray of a single type from a binary or text file. The open file pointer is fp, the data-type of the array to be created is given by dtype. This must match the data in the file. If num is -1, then read until the end of the file and return an appropriately sized array, otherwise, num is the number of items to read. If sep is NULL (or \u201c\u201d), then read from the file in binary mode, otherwise read from the file in text mode with sep providing the item separator. Some array types cannot be read in text mode in which case an error is raised. \n   PyObject*PyArray_FromBuffer(PyObject*buf, PyArray_Descr*dtype, npy_intpcount, npy_intpoffset)\n \nConstruct a one-dimensional ndarray of a single type from an object, buf, that exports the (single-segment) buffer protocol (or has an attribute __buffer__ that returns an object that exports the buffer protocol). A writeable buffer will be tried first followed by a read- only buffer. The NPY_ARRAY_WRITEABLE flag of the returned array will reflect which one was successful. The data is assumed to start at offset bytes from the start of the memory location for the object. The type of the data in the buffer will be interpreted depending on the data- type descriptor, dtype. If count is negative then it will be determined from the size of the buffer and the requested itemsize, otherwise, count represents how many elements should be converted from the buffer. \n   intPyArray_CopyInto(PyArrayObject*dest, PyArrayObject*src)\n \nCopy from the source array, src, into the destination array, dest, performing a data-type conversion if necessary. If an error occurs return -1 (otherwise 0). The shape of src must be broadcastable to the shape of dest. The data areas of dest and src must not overlap. \n   intPyArray_CopyObject(PyArrayObject*dest, PyObject*src)\n \nAssign an object src to a NumPy array dest according to array-coercion rules. This is basically identical to PyArray_FromAny, but assigns directly to the output array. Returns 0 on success and -1 on failures. \n   intPyArray_MoveInto(PyArrayObject*dest, PyArrayObject*src)\n \nMove data from the source array, src, into the destination array, dest, performing a data-type conversion if necessary. If an error occurs return -1 (otherwise 0). The shape of src must be broadcastable to the shape of dest. The data areas of dest and src may overlap. \n   PyArrayObject*PyArray_GETCONTIGUOUS(PyObject*op)\n \nIf op is already (C-style) contiguous and well-behaved then just return a reference, otherwise return a (contiguous and well-behaved) copy of the array. The parameter op must be a (sub-class of an) ndarray and no checking for that is done. \n   PyObject*PyArray_FROM_O(PyObject*obj)\n \nConvert obj to an ndarray. The argument can be any nested sequence or object that exports the array interface. This is a macro form of PyArray_FromAny using NULL, 0, 0, 0 for the other arguments. Your code must be able to handle any data-type descriptor and any combination of data-flags to use this macro. \n   PyObject*PyArray_FROM_OF(PyObject*obj, intrequirements)\n \nSimilar to PyArray_FROM_O except it can take an argument of requirements indicating properties the resulting array must have. Available requirements that can be enforced are NPY_ARRAY_C_CONTIGUOUS, NPY_ARRAY_F_CONTIGUOUS, NPY_ARRAY_ALIGNED, NPY_ARRAY_WRITEABLE, NPY_ARRAY_NOTSWAPPED, NPY_ARRAY_ENSURECOPY, NPY_ARRAY_WRITEBACKIFCOPY, NPY_ARRAY_UPDATEIFCOPY, NPY_ARRAY_FORCECAST, and NPY_ARRAY_ENSUREARRAY. Standard combinations of flags can also be used: \n   PyObject*PyArray_FROM_OT(PyObject*obj, inttypenum)\n \nSimilar to PyArray_FROM_O except it can take an argument of typenum specifying the type-number the returned array. \n   PyObject*PyArray_FROM_OTF(PyObject*obj, inttypenum, intrequirements)\n \nCombination of PyArray_FROM_OF and PyArray_FROM_OT allowing both a typenum and a flags argument to be provided. \n   PyObject*PyArray_FROMANY(PyObject*obj, inttypenum, intmin, intmax, intrequirements)\n \nSimilar to PyArray_FromAny except the data-type is specified using a typenumber. PyArray_DescrFromType (typenum) is passed directly to PyArray_FromAny. This macro also adds NPY_ARRAY_DEFAULT to requirements if NPY_ARRAY_ENSURECOPY is passed in as requirements. \n   PyObject*PyArray_CheckAxis(PyObject*obj, int*axis, intrequirements)\n \nEncapsulate the functionality of functions and methods that take the axis= keyword and work properly with None as the axis argument. The input array is obj, while *axis is a converted integer (so that >=MAXDIMS is the None value), and requirements gives the needed properties of obj. The output is a converted version of the input so that requirements are met and if needed a flattening has occurred. On output negative values of *axis are converted and the new value is checked to ensure consistency with the shape of obj. \n    Dealing with types  General check of Python Type   intPyArray_Check(PyObject*op)\n \nEvaluates true if op is a Python object whose type is a sub-type of PyArray_Type. \n   intPyArray_CheckExact(PyObject*op)\n \nEvaluates true if op is a Python object with type PyArray_Type. \n   intPyArray_HasArrayInterface(PyObject*op, PyObject*out)\n \nIf op implements any part of the array interface, then out will contain a new reference to the newly created ndarray using the interface or out will contain NULL if an error during conversion occurs. Otherwise, out will contain a borrowed reference to Py_NotImplemented and no error condition is set. \n   intPyArray_HasArrayInterfaceType(PyObject*op, PyArray_Descr*dtype, PyObject*context, PyObject*out)\n \nIf op implements any part of the array interface, then out will contain a new reference to the newly created ndarray using the interface or out will contain NULL if an error during conversion occurs. Otherwise, out will contain a borrowed reference to Py_NotImplemented and no error condition is set. This version allows setting of the dtype in the part of the array interface that looks for the __array__ attribute. context is unused. \n   intPyArray_IsZeroDim(PyObject*op)\n \nEvaluates true if op is an instance of (a subclass of) PyArray_Type and has 0 dimensions. \n   PyArray_IsScalar(op, cls)\n \nEvaluates true if op is an instance of Py{cls}ArrType_Type. \n   intPyArray_CheckScalar(PyObject*op)\n \nEvaluates true if op is either an array scalar (an instance of a sub-type of PyGenericArr_Type ), or an instance of (a sub-class of) PyArray_Type whose dimensionality is 0. \n   intPyArray_IsPythonNumber(PyObject*op)\n \nEvaluates true if op is an instance of a builtin numeric type (int, float, complex, long, bool) \n   intPyArray_IsPythonScalar(PyObject*op)\n \nEvaluates true if op is a builtin Python scalar object (int, float, complex, bytes, str, long, bool). \n   intPyArray_IsAnyScalar(PyObject*op)\n \nEvaluates true if op is either a Python scalar object (see PyArray_IsPythonScalar) or an array scalar (an instance of a sub- type of PyGenericArr_Type ). \n   intPyArray_CheckAnyScalar(PyObject*op)\n \nEvaluates true if op is a Python scalar object (see PyArray_IsPythonScalar), an array scalar (an instance of a sub-type of PyGenericArr_Type) or an instance of a sub-type of PyArray_Type whose dimensionality is 0. \n   Data-type checking For the typenum macros, the argument is an integer representing an enumerated array data type. For the array type checking macros the argument must be a PyObject* that can be directly interpreted as a PyArrayObject*.   intPyTypeNum_ISUNSIGNED(intnum)\n\n   intPyDataType_ISUNSIGNED(PyArray_Descr*descr)\n\n   intPyArray_ISUNSIGNED(PyArrayObject*obj)\n \nType represents an unsigned integer. \n   intPyTypeNum_ISSIGNED(intnum)\n\n   intPyDataType_ISSIGNED(PyArray_Descr*descr)\n\n   intPyArray_ISSIGNED(PyArrayObject*obj)\n \nType represents a signed integer. \n   intPyTypeNum_ISINTEGER(intnum)\n\n   intPyDataType_ISINTEGER(PyArray_Descr*descr)\n\n   intPyArray_ISINTEGER(PyArrayObject*obj)\n \nType represents any integer. \n   intPyTypeNum_ISFLOAT(intnum)\n\n   intPyDataType_ISFLOAT(PyArray_Descr*descr)\n\n   intPyArray_ISFLOAT(PyArrayObject*obj)\n \nType represents any floating point number. \n   intPyTypeNum_ISCOMPLEX(intnum)\n\n   intPyDataType_ISCOMPLEX(PyArray_Descr*descr)\n\n   intPyArray_ISCOMPLEX(PyArrayObject*obj)\n \nType represents any complex floating point number. \n   intPyTypeNum_ISNUMBER(intnum)\n\n   intPyDataType_ISNUMBER(PyArray_Descr*descr)\n\n   intPyArray_ISNUMBER(PyArrayObject*obj)\n \nType represents any integer, floating point, or complex floating point number. \n   intPyTypeNum_ISSTRING(intnum)\n\n   intPyDataType_ISSTRING(PyArray_Descr*descr)\n\n   intPyArray_ISSTRING(PyArrayObject*obj)\n \nType represents a string data type. \n   intPyTypeNum_ISPYTHON(intnum)\n\n   intPyDataType_ISPYTHON(PyArray_Descr*descr)\n\n   intPyArray_ISPYTHON(PyArrayObject*obj)\n \nType represents an enumerated type corresponding to one of the standard Python scalar (bool, int, float, or complex). \n   intPyTypeNum_ISFLEXIBLE(intnum)\n\n   intPyDataType_ISFLEXIBLE(PyArray_Descr*descr)\n\n   intPyArray_ISFLEXIBLE(PyArrayObject*obj)\n \nType represents one of the flexible array types ( NPY_STRING, NPY_UNICODE, or NPY_VOID ). \n   intPyDataType_ISUNSIZED(PyArray_Descr*descr)\n \nType has no size information attached, and can be resized. Should only be called on flexible dtypes. Types that are attached to an array will always be sized, hence the array form of this macro not existing.  Changed in version 1.18.  For structured datatypes with no fields this function now returns False. \n   intPyTypeNum_ISUSERDEF(intnum)\n\n   intPyDataType_ISUSERDEF(PyArray_Descr*descr)\n\n   intPyArray_ISUSERDEF(PyArrayObject*obj)\n \nType represents a user-defined type. \n   intPyTypeNum_ISEXTENDED(intnum)\n\n   intPyDataType_ISEXTENDED(PyArray_Descr*descr)\n\n   intPyArray_ISEXTENDED(PyArrayObject*obj)\n \nType is either flexible or user-defined. \n   intPyTypeNum_ISOBJECT(intnum)\n\n   intPyDataType_ISOBJECT(PyArray_Descr*descr)\n\n   intPyArray_ISOBJECT(PyArrayObject*obj)\n \nType represents object data type. \n   intPyTypeNum_ISBOOL(intnum)\n\n   intPyDataType_ISBOOL(PyArray_Descr*descr)\n\n   intPyArray_ISBOOL(PyArrayObject*obj)\n \nType represents Boolean data type. \n   intPyDataType_HASFIELDS(PyArray_Descr*descr)\n\n   intPyArray_HASFIELDS(PyArrayObject*obj)\n \nType has fields associated with it. \n   intPyArray_ISNOTSWAPPED(PyArrayObject*m)\n \nEvaluates true if the data area of the ndarray m is in machine byte-order according to the array\u2019s data-type descriptor. \n   intPyArray_ISBYTESWAPPED(PyArrayObject*m)\n \nEvaluates true if the data area of the ndarray m is not in machine byte-order according to the array\u2019s data-type descriptor. \n   npy_boolPyArray_EquivTypes(PyArray_Descr*type1, PyArray_Descr*type2)\n \nReturn NPY_TRUE if type1 and type2 actually represent equivalent types for this platform (the fortran member of each type is ignored). For example, on 32-bit platforms, NPY_LONG and NPY_INT are equivalent. Otherwise return NPY_FALSE. \n   npy_boolPyArray_EquivArrTypes(PyArrayObject*a1, PyArrayObject*a2)\n \nReturn NPY_TRUE if a1 and a2 are arrays with equivalent types for this platform. \n   npy_boolPyArray_EquivTypenums(inttypenum1, inttypenum2)\n \nSpecial case of PyArray_EquivTypes (\u2026) that does not accept flexible data types but may be easier to call. \n   intPyArray_EquivByteorders(intb1, intb2)\n \nTrue if byteorder characters b1 and b2 ( NPY_LITTLE, NPY_BIG, NPY_NATIVE, NPY_IGNORE ) are either equal or equivalent as to their specification of a native byte order. Thus, on a little-endian machine NPY_LITTLE and NPY_NATIVE are equivalent where they are not equivalent on a big-endian machine. \n   Converting data types   PyObject*PyArray_Cast(PyArrayObject*arr, inttypenum)\n \nMainly for backwards compatibility to the Numeric C-API and for simple casts to non-flexible types. Return a new array object with the elements of arr cast to the data-type typenum which must be one of the enumerated types and not a flexible type. \n   PyObject*PyArray_CastToType(PyArrayObject*arr, PyArray_Descr*type, intfortran)\n \nReturn a new array of the type specified, casting the elements of arr as appropriate. The fortran argument specifies the ordering of the output array. \n   intPyArray_CastTo(PyArrayObject*out, PyArrayObject*in)\n \nAs of 1.6, this function simply calls PyArray_CopyInto, which handles the casting. Cast the elements of the array in into the array out. The output array should be writeable, have an integer-multiple of the number of elements in the input array (more than one copy can be placed in out), and have a data type that is one of the builtin types. Returns 0 on success and -1 if an error occurs. \n   PyArray_VectorUnaryFunc*PyArray_GetCastFunc(PyArray_Descr*from, inttotype)\n \nReturn the low-level casting function to cast from the given descriptor to the builtin type number. If no casting function exists return NULL and set an error. Using this function instead of direct access to from ->f->cast will allow support of any user-defined casting functions added to a descriptors casting dictionary. \n   intPyArray_CanCastSafely(intfromtype, inttotype)\n \nReturns non-zero if an array of data type fromtype can be cast to an array of data type totype without losing information. An exception is that 64-bit integers are allowed to be cast to 64-bit floating point values even though this can lose precision on large integers so as not to proliferate the use of long doubles without explicit requests. Flexible array types are not checked according to their lengths with this function. \n   intPyArray_CanCastTo(PyArray_Descr*fromtype, PyArray_Descr*totype)\n \nPyArray_CanCastTypeTo supersedes this function in NumPy 1.6 and later. Equivalent to PyArray_CanCastTypeTo(fromtype, totype, NPY_SAFE_CASTING). \n   intPyArray_CanCastTypeTo(PyArray_Descr*fromtype, PyArray_Descr*totype, NPY_CASTINGcasting)\n \n New in version 1.6.  Returns non-zero if an array of data type fromtype (which can include flexible types) can be cast safely to an array of data type totype (which can include flexible types) according to the casting rule casting. For simple types with NPY_SAFE_CASTING, this is basically a wrapper around PyArray_CanCastSafely, but for flexible types such as strings or unicode, it produces results taking into account their sizes. Integer and float types can only be cast to a string or unicode type using NPY_SAFE_CASTING if the string or unicode type is big enough to hold the max value of the integer/float type being cast from. \n   intPyArray_CanCastArrayTo(PyArrayObject*arr, PyArray_Descr*totype, NPY_CASTINGcasting)\n \n New in version 1.6.  Returns non-zero if arr can be cast to totype according to the casting rule given in casting. If arr is an array scalar, its value is taken into account, and non-zero is also returned when the value will not overflow or be truncated to an integer when converting to a smaller type. This is almost the same as the result of PyArray_CanCastTypeTo(PyArray_MinScalarType(arr), totype, casting), but it also handles a special case arising because the set of uint values is not a subset of the int values for types with the same number of bits. \n   PyArray_Descr*PyArray_MinScalarType(PyArrayObject*arr)\n \n New in version 1.6.  If arr is an array, returns its data type descriptor, but if arr is an array scalar (has 0 dimensions), it finds the data type of smallest size to which the value may be converted without overflow or truncation to an integer. This function will not demote complex to float or anything to boolean, but will demote a signed integer to an unsigned integer when the scalar value is positive. \n   PyArray_Descr*PyArray_PromoteTypes(PyArray_Descr*type1, PyArray_Descr*type2)\n \n New in version 1.6.  Finds the data type of smallest size and kind to which type1 and type2 may be safely converted. This function is symmetric and associative. A string or unicode result will be the proper size for storing the max value of the input types converted to a string or unicode. \n   PyArray_Descr*PyArray_ResultType(npy_intpnarrs, PyArrayObject**arrs, npy_intpndtypes, PyArray_Descr**dtypes)\n \n New in version 1.6.  This applies type promotion to all the inputs, using the NumPy rules for combining scalars and arrays, to determine the output type of a set of operands. This is the same result type that ufuncs produce. The specific algorithm used is as follows. Categories are determined by first checking which of boolean, integer (int/uint), or floating point (float/complex) the maximum kind of all the arrays and the scalars are. If there are only scalars or the maximum category of the scalars is higher than the maximum category of the arrays, the data types are combined with PyArray_PromoteTypes to produce the return value. Otherwise, PyArray_MinScalarType is called on each array, and the resulting data types are all combined with PyArray_PromoteTypes to produce the return value. The set of int values is not a subset of the uint values for types with the same number of bits, something not reflected in PyArray_MinScalarType, but handled as a special case in PyArray_ResultType. \n   intPyArray_ObjectType(PyObject*op, intmintype)\n \nThis function is superseded by PyArray_MinScalarType and/or PyArray_ResultType. This function is useful for determining a common type that two or more arrays can be converted to. It only works for non-flexible array types as no itemsize information is passed. The mintype argument represents the minimum type acceptable, and op represents the object that will be converted to an array. The return value is the enumerated typenumber that represents the data-type that op should have. \n   voidPyArray_ArrayType(PyObject*op, PyArray_Descr*mintype, PyArray_Descr*outtype)\n \nThis function is superseded by PyArray_ResultType. This function works similarly to PyArray_ObjectType (\u2026) except it handles flexible arrays. The mintype argument can have an itemsize member and the outtype argument will have an itemsize member at least as big but perhaps bigger depending on the object op. \n   PyArrayObject**PyArray_ConvertToCommonType(PyObject*op, int*n)\n \nThe functionality this provides is largely superseded by iterator NpyIter introduced in 1.6, with flag NPY_ITER_COMMON_DTYPE or with the same dtype parameter for all operands. Convert a sequence of Python objects contained in op to an array of ndarrays each having the same data type. The type is selected in the same way as PyArray_ResultType. The length of the sequence is returned in n, and an n -length array of PyArrayObject pointers is the return value (or NULL if an error occurs). The returned array must be freed by the caller of this routine (using PyDataMem_FREE ) and all the array objects in it DECREF \u2018d or a memory-leak will occur. The example template-code below shows a typically usage:  Changed in version 1.18.0: A mix of scalars and zero-dimensional arrays now produces a type capable of holding the scalar value. Previously priority was given to the dtype of the arrays.  mps = PyArray_ConvertToCommonType(obj, &n);\nif (mps==NULL) return NULL;\n{code}\n<before return>\nfor (i=0; i<n; i++) Py_DECREF(mps[i]);\nPyDataMem_FREE(mps);\n{return}\n \n   char*PyArray_Zero(PyArrayObject*arr)\n \nA pointer to newly created memory of size arr ->itemsize that holds the representation of 0 for that type. The returned pointer, ret, must be freed using PyDataMem_FREE (ret) when it is not needed anymore. \n   char*PyArray_One(PyArrayObject*arr)\n \nA pointer to newly created memory of size arr ->itemsize that holds the representation of 1 for that type. The returned pointer, ret, must be freed using PyDataMem_FREE (ret) when it is not needed anymore. \n   intPyArray_ValidType(inttypenum)\n \nReturns NPY_TRUE if typenum represents a valid type-number (builtin or user-defined or character code). Otherwise, this function returns NPY_FALSE. \n   User-defined data types   voidPyArray_InitArrFuncs(PyArray_ArrFuncs*f)\n \nInitialize all function pointers and members to NULL. \n   intPyArray_RegisterDataType(PyArray_Descr*dtype)\n \nRegister a data-type as a new user-defined data type for arrays. The type must have most of its entries filled in. This is not always checked and errors can produce segfaults. In particular, the typeobj member of the dtype structure must be filled with a Python type that has a fixed-size element-size that corresponds to the elsize member of dtype. Also the f member must have the required functions: nonzero, copyswap, copyswapn, getitem, setitem, and cast (some of the cast functions may be NULL if no support is desired). To avoid confusion, you should choose a unique character typecode but this is not enforced and not relied on internally. A user-defined type number is returned that uniquely identifies the type. A pointer to the new structure can then be obtained from PyArray_DescrFromType using the returned type number. A -1 is returned if an error occurs. If this dtype has already been registered (checked only by the address of the pointer), then return the previously-assigned type-number. \n   intPyArray_RegisterCastFunc(PyArray_Descr*descr, inttotype, PyArray_VectorUnaryFunc*castfunc)\n \nRegister a low-level casting function, castfunc, to convert from the data-type, descr, to the given data-type number, totype. Any old casting function is over-written. A 0 is returned on success or a -1 on failure. \n   intPyArray_RegisterCanCast(PyArray_Descr*descr, inttotype, NPY_SCALARKINDscalar)\n \nRegister the data-type number, totype, as castable from data-type object, descr, of the given scalar kind. Use scalar = NPY_NOSCALAR to register that an array of data-type descr can be cast safely to a data-type whose type_number is totype. The return value is 0 on success or -1 on failure. \n   intPyArray_TypeNumFromName(charconst*str)\n \nGiven a string return the type-number for the data-type with that string as the type-object name. Returns NPY_NOTYPE without setting an error if no type can be found. Only works for user-defined data-types. \n   Special functions for NPY_OBJECT   intPyArray_INCREF(PyArrayObject*op)\n \nUsed for an array, op, that contains any Python objects. It increments the reference count of every object in the array according to the data-type of op. A -1 is returned if an error occurs, otherwise 0 is returned. \n   voidPyArray_Item_INCREF(char*ptr, PyArray_Descr*dtype)\n \nA function to INCREF all the objects at the location ptr according to the data-type dtype. If ptr is the start of a structured type with an object at any offset, then this will (recursively) increment the reference count of all object-like items in the structured type. \n   intPyArray_XDECREF(PyArrayObject*op)\n \nUsed for an array, op, that contains any Python objects. It decrements the reference count of every object in the array according to the data-type of op. Normal return value is 0. A -1 is returned if an error occurs. \n   voidPyArray_Item_XDECREF(char*ptr, PyArray_Descr*dtype)\n \nA function to XDECREF all the object-like items at the location ptr as recorded in the data-type, dtype. This works recursively so that if dtype itself has fields with data-types that contain object-like items, all the object-like fields will be XDECREF 'd. \n   voidPyArray_FillObjectArray(PyArrayObject*arr, PyObject*obj)\n \nFill a newly created array with a single value obj at all locations in the structure with object data-types. No checking is performed but arr must be of data-type NPY_OBJECT and be single-segment and uninitialized (no previous objects in position). Use PyArray_XDECREF (arr) if you need to decrement all the items in the object array prior to calling this function. \n   intPyArray_SetUpdateIfCopyBase(PyArrayObject*arr, PyArrayObject*base)\n \nPrecondition: arr is a copy of base (though possibly with different strides, ordering, etc.) Set the UPDATEIFCOPY flag and arr->base so that when arr is destructed, it will copy any changes back to base. DEPRECATED, use PyArray_SetWritebackIfCopyBase. Returns 0 for success, -1 for failure. \n   intPyArray_SetWritebackIfCopyBase(PyArrayObject*arr, PyArrayObject*base)\n \nPrecondition: arr is a copy of base (though possibly with different strides, ordering, etc.) Sets the NPY_ARRAY_WRITEBACKIFCOPY flag and arr->base, and set base to READONLY. Call PyArray_ResolveWritebackIfCopy before calling Py_DECREF in order copy any changes back to base and reset the READONLY flag. Returns 0 for success, -1 for failure. \n    Array flags The flags attribute of the PyArrayObject structure contains important information about the memory used by the array (pointed to by the data member) This flag information must be kept accurate or strange results and even segfaults may result. There are 6 (binary) flags that describe the memory area used by the data buffer. These constants are defined in arrayobject.h and determine the bit-position of the flag. Python exposes a nice attribute- based interface as well as a dictionary-like interface for getting (and, if appropriate, setting) these flags. Memory areas of all kinds can be pointed to by an ndarray, necessitating these flags. If you get an arbitrary PyArrayObject in C-code, you need to be aware of the flags that are set. If you need to guarantee a certain kind of array (like NPY_ARRAY_C_CONTIGUOUS and NPY_ARRAY_BEHAVED), then pass these requirements into the PyArray_FromAny function.  Basic Array Flags An ndarray can have a data segment that is not a simple contiguous chunk of well-behaved memory you can manipulate. It may not be aligned with word boundaries (very important on some platforms). It might have its data in a different byte-order than the machine recognizes. It might not be writeable. It might be in Fortran-contiguous order. The array flags are used to indicate what can be said about data associated with an array. In versions 1.6 and earlier of NumPy, the following flags did not have the _ARRAY_ macro namespace in them. That form of the constant names is deprecated in 1.7.   NPY_ARRAY_C_CONTIGUOUS\n \nThe data area is in C-style contiguous order (last index varies the fastest). \n   NPY_ARRAY_F_CONTIGUOUS\n \nThe data area is in Fortran-style contiguous order (first index varies the fastest). \n  Note Arrays can be both C-style and Fortran-style contiguous simultaneously. This is clear for 1-dimensional arrays, but can also be true for higher dimensional arrays. Even for contiguous arrays a stride for a given dimension arr.strides[dim] may be arbitrary if arr.shape[dim] == 1 or the array has no elements. It does not generally hold that self.strides[-1] == self.itemsize for C-style contiguous arrays or self.strides[0] == self.itemsize for Fortran-style contiguous arrays is true. The correct way to access the itemsize of an array from the C API is PyArray_ITEMSIZE(arr).  See also Internal memory layout of an ndarray     NPY_ARRAY_OWNDATA\n \nThe data area is owned by this array. Should never be set manually, instead create a PyObject wrapping the data and set the array\u2019s base to that object. For an example, see the test in test_mem_policy. \n   NPY_ARRAY_ALIGNED\n \nThe data area and all array elements are aligned appropriately. \n   NPY_ARRAY_WRITEABLE\n \nThe data area can be written to. Notice that the above 3 flags are defined so that a new, well- behaved array has these flags defined as true. \n   NPY_ARRAY_WRITEBACKIFCOPY\n \nThe data area represents a (well-behaved) copy whose information should be transferred back to the original when PyArray_ResolveWritebackIfCopy is called. This is a special flag that is set if this array represents a copy made because a user required certain flags in PyArray_FromAny and a copy had to be made of some other array (and the user asked for this flag to be set in such a situation). The base attribute then points to the \u201cmisbehaved\u201d array (which is set read_only). :c:func`PyArray_ResolveWritebackIfCopy` will copy its contents back to the \u201cmisbehaved\u201d array (casting if necessary) and will reset the \u201cmisbehaved\u201d array to NPY_ARRAY_WRITEABLE. If the \u201cmisbehaved\u201d array was not NPY_ARRAY_WRITEABLE to begin with then PyArray_FromAny would have returned an error because NPY_ARRAY_WRITEBACKIFCOPY would not have been possible. \n   NPY_ARRAY_UPDATEIFCOPY\n \nA deprecated version of NPY_ARRAY_WRITEBACKIFCOPY which depends upon dealloc to trigger the writeback. For backwards compatibility, PyArray_ResolveWritebackIfCopy is called at dealloc but relying on that behavior is deprecated and not supported in PyPy. \n PyArray_UpdateFlags (obj, flags) will update the obj->flags for flags which can be any of NPY_ARRAY_C_CONTIGUOUS, NPY_ARRAY_F_CONTIGUOUS, NPY_ARRAY_ALIGNED, or NPY_ARRAY_WRITEABLE.   Combinations of array flags   NPY_ARRAY_BEHAVED\n \nNPY_ARRAY_ALIGNED | NPY_ARRAY_WRITEABLE \n   NPY_ARRAY_CARRAY\n \nNPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_BEHAVED \n   NPY_ARRAY_CARRAY_RO\n \nNPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_ALIGNED \n   NPY_ARRAY_FARRAY\n \nNPY_ARRAY_F_CONTIGUOUS | NPY_ARRAY_BEHAVED \n   NPY_ARRAY_FARRAY_RO\n \nNPY_ARRAY_F_CONTIGUOUS | NPY_ARRAY_ALIGNED \n   NPY_ARRAY_DEFAULT\n \nNPY_ARRAY_CARRAY \n   NPY_ARRAY_UPDATE_ALL\n \nNPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_F_CONTIGUOUS | NPY_ARRAY_ALIGNED \n   Flag-like constants These constants are used in PyArray_FromAny (and its macro forms) to specify desired properties of the new array.   NPY_ARRAY_FORCECAST\n \nCast to the desired type, even if it can\u2019t be done without losing information. \n   NPY_ARRAY_ENSURECOPY\n \nMake sure the resulting array is a copy of the original. \n   NPY_ARRAY_ENSUREARRAY\n \nMake sure the resulting object is an actual ndarray, and not a sub-class. \n   Flag checking For all of these macros arr must be an instance of a (subclass of) PyArray_Type.   intPyArray_CHKFLAGS(PyObject*arr, intflags)\n \nThe first parameter, arr, must be an ndarray or subclass. The parameter, flags, should be an integer consisting of bitwise combinations of the possible flags an array can have: NPY_ARRAY_C_CONTIGUOUS, NPY_ARRAY_F_CONTIGUOUS, NPY_ARRAY_OWNDATA, NPY_ARRAY_ALIGNED, NPY_ARRAY_WRITEABLE, NPY_ARRAY_WRITEBACKIFCOPY, NPY_ARRAY_UPDATEIFCOPY. \n   intPyArray_IS_C_CONTIGUOUS(PyObject*arr)\n \nEvaluates true if arr is C-style contiguous. \n   intPyArray_IS_F_CONTIGUOUS(PyObject*arr)\n \nEvaluates true if arr is Fortran-style contiguous. \n   intPyArray_ISFORTRAN(PyObject*arr)\n \nEvaluates true if arr is Fortran-style contiguous and not C-style contiguous. PyArray_IS_F_CONTIGUOUS is the correct way to test for Fortran-style contiguity. \n   intPyArray_ISWRITEABLE(PyObject*arr)\n \nEvaluates true if the data area of arr can be written to \n   intPyArray_ISALIGNED(PyObject*arr)\n \nEvaluates true if the data area of arr is properly aligned on the machine. \n   intPyArray_ISBEHAVED(PyObject*arr)\n \nEvaluates true if the data area of arr is aligned and writeable and in machine byte-order according to its descriptor. \n   intPyArray_ISBEHAVED_RO(PyObject*arr)\n \nEvaluates true if the data area of arr is aligned and in machine byte-order. \n   intPyArray_ISCARRAY(PyObject*arr)\n \nEvaluates true if the data area of arr is C-style contiguous, and PyArray_ISBEHAVED (arr) is true. \n   intPyArray_ISFARRAY(PyObject*arr)\n \nEvaluates true if the data area of arr is Fortran-style contiguous and PyArray_ISBEHAVED (arr) is true. \n   intPyArray_ISCARRAY_RO(PyObject*arr)\n \nEvaluates true if the data area of arr is C-style contiguous, aligned, and in machine byte-order. \n   intPyArray_ISFARRAY_RO(PyObject*arr)\n \nEvaluates true if the data area of arr is Fortran-style contiguous, aligned, and in machine byte-order . \n   intPyArray_ISONESEGMENT(PyObject*arr)\n \nEvaluates true if the data area of arr consists of a single (C-style or Fortran-style) contiguous segment. \n   voidPyArray_UpdateFlags(PyArrayObject*arr, intflagmask)\n \nThe NPY_ARRAY_C_CONTIGUOUS, NPY_ARRAY_ALIGNED, and NPY_ARRAY_F_CONTIGUOUS array flags can be \u201ccalculated\u201d from the array object itself. This routine updates one or more of these flags of arr as specified in flagmask by performing the required calculation. \n  Warning It is important to keep the flags updated (using PyArray_UpdateFlags can help) whenever a manipulation with an array is performed that might cause them to change. Later calculations in NumPy that rely on the state of these flags do not repeat the calculation to update them.     Array method alternative API  Conversion   PyObject*PyArray_GetField(PyArrayObject*self, PyArray_Descr*dtype, intoffset)\n \nEquivalent to ndarray.getfield (self, dtype, offset). This function steals a reference to PyArray_Descr and returns a new array of the given dtype using the data in the current array at a specified offset in bytes. The offset plus the itemsize of the new array type must be less than self\n->descr->elsize or an error is raised. The same shape and strides as the original array are used. Therefore, this function has the effect of returning a field from a structured array. But, it can also be used to select specific bytes or groups of bytes from any array type. \n   intPyArray_SetField(PyArrayObject*self, PyArray_Descr*dtype, intoffset, PyObject*val)\n \nEquivalent to ndarray.setfield (self, val, dtype, offset ). Set the field starting at offset in bytes and of the given dtype to val. The offset plus dtype ->elsize must be less than self ->descr->elsize or an error is raised. Otherwise, the val argument is converted to an array and copied into the field pointed to. If necessary, the elements of val are repeated to fill the destination array, But, the number of elements in the destination must be an integer multiple of the number of elements in val. \n   PyObject*PyArray_Byteswap(PyArrayObject*self, npy_boolinplace)\n \nEquivalent to ndarray.byteswap (self, inplace). Return an array whose data area is byteswapped. If inplace is non-zero, then do the byteswap inplace and return a reference to self. Otherwise, create a byteswapped copy and leave self unchanged. \n   PyObject*PyArray_NewCopy(PyArrayObject*old, NPY_ORDERorder)\n \nEquivalent to ndarray.copy (self, fortran). Make a copy of the old array. The returned array is always aligned and writeable with data interpreted the same as the old array. If order is NPY_CORDER, then a C-style contiguous array is returned. If order is NPY_FORTRANORDER, then a Fortran-style contiguous array is returned. If order is NPY_ANYORDER, then the array returned is Fortran-style contiguous only if the old one is; otherwise, it is C-style contiguous. \n   PyObject*PyArray_ToList(PyArrayObject*self)\n \nEquivalent to ndarray.tolist (self). Return a nested Python list from self. \n   PyObject*PyArray_ToString(PyArrayObject*self, NPY_ORDERorder)\n \nEquivalent to ndarray.tobytes (self, order). Return the bytes of this array in a Python string. \n   PyObject*PyArray_ToFile(PyArrayObject*self, FILE*fp, char*sep, char*format)\n \nWrite the contents of self to the file pointer fp in C-style contiguous fashion. Write the data as binary bytes if sep is the string \u201c\u201dor NULL. Otherwise, write the contents of self as text using the sep string as the item separator. Each item will be printed to the file. If the format string is not NULL or \u201c\u201d, then it is a Python print statement format string showing how the items are to be written. \n   intPyArray_Dump(PyObject*self, PyObject*file, intprotocol)\n \nPickle the object in self to the given file (either a string or a Python file object). If file is a Python string it is considered to be the name of a file which is then opened in binary mode. The given protocol is used (if protocol is negative, or the highest available is used). This is a simple wrapper around cPickle.dump(self, file, protocol). \n   PyObject*PyArray_Dumps(PyObject*self, intprotocol)\n \nPickle the object in self to a Python string and return it. Use the Pickle protocol provided (or the highest available if protocol is negative). \n   intPyArray_FillWithScalar(PyArrayObject*arr, PyObject*obj)\n \nFill the array, arr, with the given scalar object, obj. The object is first converted to the data type of arr, and then copied into every location. A -1 is returned if an error occurs, otherwise 0 is returned. \n   PyObject*PyArray_View(PyArrayObject*self, PyArray_Descr*dtype, PyTypeObject*ptype)\n \nEquivalent to ndarray.view (self, dtype). Return a new view of the array self as possibly a different data-type, dtype, and different array subclass ptype. If dtype is NULL, then the returned array will have the same data type as self. The new data-type must be consistent with the size of self. Either the itemsizes must be identical, or self must be single-segment and the total number of bytes must be the same. In the latter case the dimensions of the returned array will be altered in the last (or first for Fortran-style contiguous arrays) dimension. The data area of the returned array and self is exactly the same. \n   Shape Manipulation   PyObject*PyArray_Newshape(PyArrayObject*self, PyArray_Dims*newshape, NPY_ORDERorder)\n \nResult will be a new array (pointing to the same memory location as self if possible), but having a shape given by newshape. If the new shape is not compatible with the strides of self, then a copy of the array with the new specified shape will be returned. \n   PyObject*PyArray_Reshape(PyArrayObject*self, PyObject*shape)\n \nEquivalent to ndarray.reshape (self, shape) where shape is a sequence. Converts shape to a PyArray_Dims structure and calls PyArray_Newshape internally. For back-ward compatibility \u2013 Not recommended \n   PyObject*PyArray_Squeeze(PyArrayObject*self)\n \nEquivalent to ndarray.squeeze (self). Return a new view of self with all of the dimensions of length 1 removed from the shape. \n  Warning matrix objects are always 2-dimensional. Therefore, PyArray_Squeeze has no effect on arrays of matrix sub-class.    PyObject*PyArray_SwapAxes(PyArrayObject*self, inta1, inta2)\n \nEquivalent to ndarray.swapaxes (self, a1, a2). The returned array is a new view of the data in self with the given axes, a1 and a2, swapped. \n   PyObject*PyArray_Resize(PyArrayObject*self, PyArray_Dims*newshape, intrefcheck, NPY_ORDERfortran)\n \nEquivalent to ndarray.resize (self, newshape, refcheck = refcheck, order= fortran ). This function only works on single-segment arrays. It changes the shape of self inplace and will reallocate the memory for self if newshape has a different total number of elements then the old shape. If reallocation is necessary, then self must own its data, have self - >base==NULL, have self - >weakrefs==NULL, and (unless refcheck is 0) not be referenced by any other array. The fortran argument can be NPY_ANYORDER, NPY_CORDER, or NPY_FORTRANORDER. It currently has no effect. Eventually it could be used to determine how the resize operation should view the data when constructing a differently-dimensioned array. Returns None on success and NULL on error. \n   PyObject*PyArray_Transpose(PyArrayObject*self, PyArray_Dims*permute)\n \nEquivalent to ndarray.transpose (self, permute). Permute the axes of the ndarray object self according to the data structure permute and return the result. If permute is NULL, then the resulting array has its axes reversed. For example if self has shape \\(10\\times20\\times30\\), and permute .ptr is (0,2,1) the shape of the result is \\(10\\times30\\times20.\\) If permute is NULL, the shape of the result is \\(30\\times20\\times10.\\) \n   PyObject*PyArray_Flatten(PyArrayObject*self, NPY_ORDERorder)\n \nEquivalent to ndarray.flatten (self, order). Return a 1-d copy of the array. If order is NPY_FORTRANORDER the elements are scanned out in Fortran order (first-dimension varies the fastest). If order is NPY_CORDER, the elements of self are scanned in C-order (last dimension varies the fastest). If order NPY_ANYORDER, then the result of PyArray_ISFORTRAN (self) is used to determine which order to flatten. \n   PyObject*PyArray_Ravel(PyArrayObject*self, NPY_ORDERorder)\n \nEquivalent to self.ravel(order). Same basic functionality as PyArray_Flatten (self, order) except if order is 0 and self is C-style contiguous, the shape is altered but no copy is performed. \n   Item selection and manipulation   PyObject*PyArray_TakeFrom(PyArrayObject*self, PyObject*indices, intaxis, PyArrayObject*ret, NPY_CLIPMODEclipmode)\n \nEquivalent to ndarray.take (self, indices, axis, ret, clipmode) except axis =None in Python is obtained by setting axis = NPY_MAXDIMS in C. Extract the items from self indicated by the integer-valued indices along the given axis. The clipmode argument can be NPY_RAISE, NPY_WRAP, or NPY_CLIP to indicate what to do with out-of-bound indices. The ret argument can specify an output array rather than having one created internally. \n   PyObject*PyArray_PutTo(PyArrayObject*self, PyObject*values, PyObject*indices, NPY_CLIPMODEclipmode)\n \nEquivalent to self.put(values, indices, clipmode ). Put values into self at the corresponding (flattened) indices. If values is too small it will be repeated as necessary. \n   PyObject*PyArray_PutMask(PyArrayObject*self, PyObject*values, PyObject*mask)\n \nPlace the values in self wherever corresponding positions (using a flattened context) in mask are true. The mask and self arrays must have the same total number of elements. If values is too small, it will be repeated as necessary. \n   PyObject*PyArray_Repeat(PyArrayObject*self, PyObject*op, intaxis)\n \nEquivalent to ndarray.repeat (self, op, axis). Copy the elements of self, op times along the given axis. Either op is a scalar integer or a sequence of length self ->dimensions[ axis ] indicating how many times to repeat each item along the axis. \n   PyObject*PyArray_Choose(PyArrayObject*self, PyObject*op, PyArrayObject*ret, NPY_CLIPMODEclipmode)\n \nEquivalent to ndarray.choose (self, op, ret, clipmode). Create a new array by selecting elements from the sequence of arrays in op based on the integer values in self. The arrays must all be broadcastable to the same shape and the entries in self should be between 0 and len(op). The output is placed in ret unless it is NULL in which case a new output is created. The clipmode argument determines behavior for when entries in self are not between 0 and len(op).   NPY_RAISE\n \nraise a ValueError; \n   NPY_WRAP\n \nwrap values < 0 by adding len(op) and values >=len(op) by subtracting len(op) until they are in range; \n   NPY_CLIP\n \nall values are clipped to the region [0, len(op) ). \n \n   PyObject*PyArray_Sort(PyArrayObject*self, intaxis, NPY_SORTKINDkind)\n \nEquivalent to ndarray.sort (self, axis, kind). Return an array with the items of self sorted along axis. The array is sorted using the algorithm denoted by kind, which is an integer/enum pointing to the type of sorting algorithms used. \n   PyObject*PyArray_ArgSort(PyArrayObject*self, intaxis)\n \nEquivalent to ndarray.argsort (self, axis). Return an array of indices such that selection of these indices along the given axis would return a sorted version of self. If self ->descr is a data-type with fields defined, then self->descr->names is used to determine the sort order. A comparison where the first field is equal will use the second field and so on. To alter the sort order of a structured array, create a new data-type with a different order of names and construct a view of the array with that new data-type. \n   PyObject*PyArray_LexSort(PyObject*sort_keys, intaxis)\n \nGiven a sequence of arrays (sort_keys) of the same shape, return an array of indices (similar to PyArray_ArgSort (\u2026)) that would sort the arrays lexicographically. A lexicographic sort specifies that when two keys are found to be equal, the order is based on comparison of subsequent keys. A merge sort (which leaves equal entries unmoved) is required to be defined for the types. The sort is accomplished by sorting the indices first using the first sort_key and then using the second sort_key and so forth. This is equivalent to the lexsort(sort_keys, axis) Python command. Because of the way the merge-sort works, be sure to understand the order the sort_keys must be in (reversed from the order you would use when comparing two elements). If these arrays are all collected in a structured array, then PyArray_Sort (\u2026) can also be used to sort the array directly. \n   PyObject*PyArray_SearchSorted(PyArrayObject*self, PyObject*values, NPY_SEARCHSIDEside, PyObject*perm)\n \nEquivalent to ndarray.searchsorted (self, values, side, perm). Assuming self is a 1-d array in ascending order, then the output is an array of indices the same shape as values such that, if the elements in values were inserted before the indices, the order of self would be preserved. No checking is done on whether or not self is in ascending order. The side argument indicates whether the index returned should be that of the first suitable location (if NPY_SEARCHLEFT) or of the last (if NPY_SEARCHRIGHT). The sorter argument, if not NULL, must be a 1D array of integer indices the same length as self, that sorts it into ascending order. This is typically the result of a call to PyArray_ArgSort (\u2026) Binary search is used to find the required insertion points. \n   intPyArray_Partition(PyArrayObject*self, PyArrayObject*ktharray, intaxis, NPY_SELECTKINDwhich)\n \nEquivalent to ndarray.partition (self, ktharray, axis, kind). Partitions the array so that the values of the element indexed by ktharray are in the positions they would be if the array is fully sorted and places all elements smaller than the kth before and all elements equal or greater after the kth element. The ordering of all elements within the partitions is undefined. If self->descr is a data-type with fields defined, then self->descr->names is used to determine the sort order. A comparison where the first field is equal will use the second field and so on. To alter the sort order of a structured array, create a new data-type with a different order of names and construct a view of the array with that new data-type. Returns zero on success and -1 on failure. \n   PyObject*PyArray_ArgPartition(PyArrayObject*op, PyArrayObject*ktharray, intaxis, NPY_SELECTKINDwhich)\n \nEquivalent to ndarray.argpartition (self, ktharray, axis, kind). Return an array of indices such that selection of these indices along the given axis would return a partitioned version of self. \n   PyObject*PyArray_Diagonal(PyArrayObject*self, intoffset, intaxis1, intaxis2)\n \nEquivalent to ndarray.diagonal (self, offset, axis1, axis2 ). Return the offset diagonals of the 2-d arrays defined by axis1 and axis2. \n   npy_intpPyArray_CountNonzero(PyArrayObject*self)\n \n New in version 1.6.  Counts the number of non-zero elements in the array object self. \n   PyObject*PyArray_Nonzero(PyArrayObject*self)\n \nEquivalent to ndarray.nonzero (self). Returns a tuple of index arrays that select elements of self that are nonzero. If (nd= PyArray_NDIM ( self ))==1, then a single index array is returned. The index arrays have data type NPY_INTP. If a tuple is returned (nd \\(\\neq\\) 1), then its length is nd. \n   PyObject*PyArray_Compress(PyArrayObject*self, PyObject*condition, intaxis, PyArrayObject*out)\n \nEquivalent to ndarray.compress (self, condition, axis ). Return the elements along axis corresponding to elements of condition that are true. \n   Calculation  Tip Pass in NPY_MAXDIMS for axis in order to achieve the same effect that is obtained by passing in axis=None in Python (treating the array as a 1-d array).   Note The out argument specifies where to place the result. If out is NULL, then the output array is created, otherwise the output is placed in out which must be the correct size and type. A new reference to the output array is always returned even when out is not NULL. The caller of the routine has the responsibility to Py_DECREF out if not NULL or a memory-leak will occur.    PyObject*PyArray_ArgMax(PyArrayObject*self, intaxis, PyArrayObject*out)\n \nEquivalent to ndarray.argmax (self, axis). Return the index of the largest element of self along axis. \n   PyObject*PyArray_ArgMin(PyArrayObject*self, intaxis, PyArrayObject*out)\n \nEquivalent to ndarray.argmin (self, axis). Return the index of the smallest element of self along axis. \n   PyObject*PyArray_Max(PyArrayObject*self, intaxis, PyArrayObject*out)\n \nEquivalent to ndarray.max (self, axis). Returns the largest element of self along the given axis. When the result is a single element, returns a numpy scalar instead of an ndarray. \n   PyObject*PyArray_Min(PyArrayObject*self, intaxis, PyArrayObject*out)\n \nEquivalent to ndarray.min (self, axis). Return the smallest element of self along the given axis. When the result is a single element, returns a numpy scalar instead of an ndarray. \n   PyObject*PyArray_Ptp(PyArrayObject*self, intaxis, PyArrayObject*out)\n \nEquivalent to ndarray.ptp (self, axis). Return the difference between the largest element of self along axis and the smallest element of self along axis. When the result is a single element, returns a numpy scalar instead of an ndarray. \n  Note The rtype argument specifies the data-type the reduction should take place over. This is important if the data-type of the array is not \u201clarge\u201d enough to handle the output. By default, all integer data-types are made at least as large as NPY_LONG for the \u201cadd\u201d and \u201cmultiply\u201d ufuncs (which form the basis for mean, sum, cumsum, prod, and cumprod functions).    PyObject*PyArray_Mean(PyArrayObject*self, intaxis, intrtype, PyArrayObject*out)\n \nEquivalent to ndarray.mean (self, axis, rtype). Returns the mean of the elements along the given axis, using the enumerated type rtype as the data type to sum in. Default sum behavior is obtained using NPY_NOTYPE for rtype. \n   PyObject*PyArray_Trace(PyArrayObject*self, intoffset, intaxis1, intaxis2, intrtype, PyArrayObject*out)\n \nEquivalent to ndarray.trace (self, offset, axis1, axis2, rtype). Return the sum (using rtype as the data type of summation) over the offset diagonal elements of the 2-d arrays defined by axis1 and axis2 variables. A positive offset chooses diagonals above the main diagonal. A negative offset selects diagonals below the main diagonal. \n   PyObject*PyArray_Clip(PyArrayObject*self, PyObject*min, PyObject*max)\n \nEquivalent to ndarray.clip (self, min, max). Clip an array, self, so that values larger than max are fixed to max and values less than min are fixed to min. \n   PyObject*PyArray_Conjugate(PyArrayObject*self)\n \nEquivalent to ndarray.conjugate (self). Return the complex conjugate of self. If self is not of complex data type, then return self with a reference. \n   PyObject*PyArray_Round(PyArrayObject*self, intdecimals, PyArrayObject*out)\n \nEquivalent to ndarray.round (self, decimals, out). Returns the array with elements rounded to the nearest decimal place. The decimal place is defined as the \\(10^{-\\textrm{decimals}}\\) digit so that negative decimals cause rounding to the nearest 10\u2019s, 100\u2019s, etc. If out is NULL, then the output array is created, otherwise the output is placed in out which must be the correct size and type. \n   PyObject*PyArray_Std(PyArrayObject*self, intaxis, intrtype, PyArrayObject*out)\n \nEquivalent to ndarray.std (self, axis, rtype). Return the standard deviation using data along axis converted to data type rtype. \n   PyObject*PyArray_Sum(PyArrayObject*self, intaxis, intrtype, PyArrayObject*out)\n \nEquivalent to ndarray.sum (self, axis, rtype). Return 1-d vector sums of elements in self along axis. Perform the sum after converting data to data type rtype. \n   PyObject*PyArray_CumSum(PyArrayObject*self, intaxis, intrtype, PyArrayObject*out)\n \nEquivalent to ndarray.cumsum (self, axis, rtype). Return cumulative 1-d sums of elements in self along axis. Perform the sum after converting data to data type rtype. \n   PyObject*PyArray_Prod(PyArrayObject*self, intaxis, intrtype, PyArrayObject*out)\n \nEquivalent to ndarray.prod (self, axis, rtype). Return 1-d products of elements in self along axis. Perform the product after converting data to data type rtype. \n   PyObject*PyArray_CumProd(PyArrayObject*self, intaxis, intrtype, PyArrayObject*out)\n \nEquivalent to ndarray.cumprod (self, axis, rtype). Return 1-d cumulative products of elements in self along axis. Perform the product after converting data to data type rtype. \n   PyObject*PyArray_All(PyArrayObject*self, intaxis, PyArrayObject*out)\n \nEquivalent to ndarray.all (self, axis). Return an array with True elements for every 1-d sub-array of self defined by axis in which all the elements are True. \n   PyObject*PyArray_Any(PyArrayObject*self, intaxis, PyArrayObject*out)\n \nEquivalent to ndarray.any (self, axis). Return an array with True elements for every 1-d sub-array of self defined by axis in which any of the elements are True. \n    Functions  Array Functions   intPyArray_AsCArray(PyObject**op, void*ptr, npy_intp*dims, intnd, inttypenum, intitemsize)\n \nSometimes it is useful to access a multidimensional array as a C-style multi-dimensional array so that algorithms can be implemented using C\u2019s a[i][j][k] syntax. This routine returns a pointer, ptr, that simulates this kind of C-style array, for 1-, 2-, and 3-d ndarrays.  Parameters \n \nop \u2013 The address to any Python object. This Python object will be replaced with an equivalent well-behaved, C-style contiguous, ndarray of the given data type specified by the last two arguments. Be sure that stealing a reference in this way to the input object is justified. \nptr \u2013 The address to a (ctype* for 1-d, ctype** for 2-d or ctype*** for 3-d) variable where ctype is the equivalent C-type for the data type. On return, ptr will be addressable as a 1-d, 2-d, or 3-d array. \ndims \u2013 An output array that contains the shape of the array object. This array gives boundaries on any looping that will take place. \nnd \u2013 The dimensionality of the array (1, 2, or 3). \ntypenum \u2013 The expected data type of the array. \nitemsize \u2013 This argument is only needed when typenum represents a flexible array. Otherwise it should be 0.    \n  Note The simulation of a C-style array is not complete for 2-d and 3-d arrays. For example, the simulated arrays of pointers cannot be passed to subroutines expecting specific, statically-defined 2-d and 3-d arrays. To pass to functions requiring those kind of inputs, you must statically define the required array and copy data.    intPyArray_Free(PyObject*op, void*ptr)\n \nMust be called with the same objects and memory locations returned from PyArray_AsCArray (\u2026). This function cleans up memory that otherwise would get leaked. \n   PyObject*PyArray_Concatenate(PyObject*obj, intaxis)\n \nJoin the sequence of objects in obj together along axis into a single array. If the dimensions or types are not compatible an error is raised. \n   PyObject*PyArray_InnerProduct(PyObject*obj1, PyObject*obj2)\n \nCompute a product-sum over the last dimensions of obj1 and obj2. Neither array is conjugated. \n   PyObject*PyArray_MatrixProduct(PyObject*obj1, PyObject*obj)\n \nCompute a product-sum over the last dimension of obj1 and the second-to-last dimension of obj2. For 2-d arrays this is a matrix-product. Neither array is conjugated. \n   PyObject*PyArray_MatrixProduct2(PyObject*obj1, PyObject*obj, PyArrayObject*out)\n \n New in version 1.6.  Same as PyArray_MatrixProduct, but store the result in out. The output array must have the correct shape, type, and be C-contiguous, or an exception is raised. \n   PyObject*PyArray_EinsteinSum(char*subscripts, npy_intpnop, PyArrayObject**op_in, PyArray_Descr*dtype, NPY_ORDERorder, NPY_CASTINGcasting, PyArrayObject*out)\n \n New in version 1.6.  Applies the Einstein summation convention to the array operands provided, returning a new array or placing the result in out. The string in subscripts is a comma separated list of index letters. The number of operands is in nop, and op_in is an array containing those operands. The data type of the output can be forced with dtype, the output order can be forced with order (NPY_KEEPORDER is recommended), and when dtype is specified, casting indicates how permissive the data conversion should be. See the einsum function for more details. \n   PyObject*PyArray_CopyAndTranspose(PyObject*op)\n \nA specialized copy and transpose function that works only for 2-d arrays. The returned array is a transposed copy of op. \n   PyObject*PyArray_Correlate(PyObject*op1, PyObject*op2, intmode)\n \nCompute the 1-d correlation of the 1-d arrays op1 and op2 . The correlation is computed at each output point by multiplying op1 by a shifted version of op2 and summing the result. As a result of the shift, needed values outside of the defined range of op1 and op2 are interpreted as zero. The mode determines how many shifts to return: 0 - return only shifts that did not need to assume zero- values; 1 - return an object that is the same size as op1, 2 - return all possible shifts (any overlap at all is accepted). Notes This does not compute the usual correlation: if op2 is larger than op1, the arguments are swapped, and the conjugate is never taken for complex arrays. See PyArray_Correlate2 for the usual signal processing correlation. \n   PyObject*PyArray_Correlate2(PyObject*op1, PyObject*op2, intmode)\n \nUpdated version of PyArray_Correlate, which uses the usual definition of correlation for 1d arrays. The correlation is computed at each output point by multiplying op1 by a shifted version of op2 and summing the result. As a result of the shift, needed values outside of the defined range of op1 and op2 are interpreted as zero. The mode determines how many shifts to return: 0 - return only shifts that did not need to assume zero- values; 1 - return an object that is the same size as op1, 2 - return all possible shifts (any overlap at all is accepted). Notes Compute z as follows: z[k] = sum_n op1[n] * conj(op2[n+k])\n \n   PyObject*PyArray_Where(PyObject*condition, PyObject*x, PyObject*y)\n \nIf both x and y are NULL, then return PyArray_Nonzero (condition). Otherwise, both x and y must be given and the object returned is shaped like condition and has elements of x and y where condition is respectively True or False. \n   Other functions   npy_boolPyArray_CheckStrides(intelsize, intnd, npy_intpnumbytes, npy_intpconst*dims, npy_intpconst*newstrides)\n \nDetermine if newstrides is a strides array consistent with the memory of an nd -dimensional array with shape dims and element-size, elsize. The newstrides array is checked to see if jumping by the provided number of bytes in each direction will ever mean jumping more than numbytes which is the assumed size of the available memory segment. If numbytes is 0, then an equivalent numbytes is computed assuming nd, dims, and elsize refer to a single-segment array. Return NPY_TRUE if newstrides is acceptable, otherwise return NPY_FALSE. \n   npy_intpPyArray_MultiplyList(npy_intpconst*seq, intn)\n\n   intPyArray_MultiplyIntList(intconst*seq, intn)\n \nBoth of these routines multiply an n -length array, seq, of integers and return the result. No overflow checking is performed. \n   intPyArray_CompareLists(npy_intpconst*l1, npy_intpconst*l2, intn)\n \nGiven two n -length arrays of integers, l1, and l2, return 1 if the lists are identical; otherwise, return 0. \n    Auxiliary Data With Object Semantics  New in version 1.7.0.    typeNpyAuxData\n\n When working with more complex dtypes which are composed of other dtypes, such as the struct dtype, creating inner loops that manipulate the dtypes requires carrying along additional data. NumPy supports this idea through a struct NpyAuxData, mandating a few conventions so that it is possible to do this. Defining an NpyAuxData is similar to defining a class in C++, but the object semantics have to be tracked manually since the API is in C. Here\u2019s an example for a function which doubles up an element using an element copier function as a primitive. typedef struct {\n    NpyAuxData base;\n    ElementCopier_Func *func;\n    NpyAuxData *funcdata;\n} eldoubler_aux_data;\n\nvoid free_element_doubler_aux_data(NpyAuxData *data)\n{\n    eldoubler_aux_data *d = (eldoubler_aux_data *)data;\n    /* Free the memory owned by this auxdata */\n    NPY_AUXDATA_FREE(d->funcdata);\n    PyArray_free(d);\n}\n\nNpyAuxData *clone_element_doubler_aux_data(NpyAuxData *data)\n{\n    eldoubler_aux_data *ret = PyArray_malloc(sizeof(eldoubler_aux_data));\n    if (ret == NULL) {\n        return NULL;\n    }\n\n    /* Raw copy of all data */\n    memcpy(ret, data, sizeof(eldoubler_aux_data));\n\n    /* Fix up the owned auxdata so we have our own copy */\n    ret->funcdata = NPY_AUXDATA_CLONE(ret->funcdata);\n    if (ret->funcdata == NULL) {\n        PyArray_free(ret);\n        return NULL;\n    }\n\n    return (NpyAuxData *)ret;\n}\n\nNpyAuxData *create_element_doubler_aux_data(\n                            ElementCopier_Func *func,\n                            NpyAuxData *funcdata)\n{\n    eldoubler_aux_data *ret = PyArray_malloc(sizeof(eldoubler_aux_data));\n    if (ret == NULL) {\n        PyErr_NoMemory();\n        return NULL;\n    }\n    memset(&ret, 0, sizeof(eldoubler_aux_data));\n    ret->base->free = &free_element_doubler_aux_data;\n    ret->base->clone = &clone_element_doubler_aux_data;\n    ret->func = func;\n    ret->funcdata = funcdata;\n\n    return (NpyAuxData *)ret;\n}\n   typeNpyAuxData_FreeFunc\n \nThe function pointer type for NpyAuxData free functions. \n   typeNpyAuxData_CloneFunc\n \nThe function pointer type for NpyAuxData clone functions. These functions should never set the Python exception on error, because they may be called from a multi-threaded context. \n   voidNPY_AUXDATA_FREE(NpyAuxData*auxdata)\n \nA macro which calls the auxdata\u2019s free function appropriately, does nothing if auxdata is NULL. \n   NpyAuxData*NPY_AUXDATA_CLONE(NpyAuxData*auxdata)\n \nA macro which calls the auxdata\u2019s clone function appropriately, returning a deep copy of the auxiliary data. \n   Array Iterators As of NumPy 1.6.0, these array iterators are superseded by the new array iterator, NpyIter. An array iterator is a simple way to access the elements of an N-dimensional array quickly and efficiently. Section 2 provides more description and examples of this useful approach to looping over an array.   PyObject*PyArray_IterNew(PyObject*arr)\n \nReturn an array iterator object from the array, arr. This is equivalent to arr. flat. The array iterator object makes it easy to loop over an N-dimensional non-contiguous array in C-style contiguous fashion. \n   PyObject*PyArray_IterAllButAxis(PyObject*arr, int*axis)\n \nReturn an array iterator that will iterate over all axes but the one provided in *axis. The returned iterator cannot be used with PyArray_ITER_GOTO1D. This iterator could be used to write something similar to what ufuncs do wherein the loop over the largest axis is done by a separate sub-routine. If *axis is negative then *axis will be set to the axis having the smallest stride and that axis will be used. \n   PyObject*PyArray_BroadcastToShape(PyObject*arr, npy_intpconst*dimensions, intnd)\n \nReturn an array iterator that is broadcast to iterate as an array of the shape provided by dimensions and nd. \n   intPyArrayIter_Check(PyObject*op)\n \nEvaluates true if op is an array iterator (or instance of a subclass of the array iterator type). \n   voidPyArray_ITER_RESET(PyObject*iterator)\n \nReset an iterator to the beginning of the array. \n   voidPyArray_ITER_NEXT(PyObject*iterator)\n \nIncremement the index and the dataptr members of the iterator to point to the next element of the array. If the array is not (C-style) contiguous, also increment the N-dimensional coordinates array. \n   void*PyArray_ITER_DATA(PyObject*iterator)\n \nA pointer to the current element of the array. \n   voidPyArray_ITER_GOTO(PyObject*iterator, npy_intp*destination)\n \nSet the iterator index, dataptr, and coordinates members to the location in the array indicated by the N-dimensional c-array, destination, which must have size at least iterator ->nd_m1+1. \n   voidPyArray_ITER_GOTO1D(PyObject*iterator, npy_intpindex)\n \nSet the iterator index and dataptr to the location in the array indicated by the integer index which points to an element in the C-styled flattened array. \n   intPyArray_ITER_NOTDONE(PyObject*iterator)\n \nEvaluates TRUE as long as the iterator has not looped through all of the elements, otherwise it evaluates FALSE. \n   Broadcasting (multi-iterators)   PyObject*PyArray_MultiIterNew(intnum, ...)\n \nA simplified interface to broadcasting. This function takes the number of arrays to broadcast and then num extra ( PyObject * ) arguments. These arguments are converted to arrays and iterators are created. PyArray_Broadcast is then called on the resulting multi-iterator object. The resulting, broadcasted mult-iterator object is then returned. A broadcasted operation can then be performed using a single loop and using PyArray_MultiIter_NEXT (..) \n   voidPyArray_MultiIter_RESET(PyObject*multi)\n \nReset all the iterators to the beginning in a multi-iterator object, multi. \n   voidPyArray_MultiIter_NEXT(PyObject*multi)\n \nAdvance each iterator in a multi-iterator object, multi, to its next (broadcasted) element. \n   void*PyArray_MultiIter_DATA(PyObject*multi, inti)\n \nReturn the data-pointer of the i \\(^{\\textrm{th}}\\) iterator in a multi-iterator object. \n   voidPyArray_MultiIter_NEXTi(PyObject*multi, inti)\n \nAdvance the pointer of only the i \\(^{\\textrm{th}}\\) iterator. \n   voidPyArray_MultiIter_GOTO(PyObject*multi, npy_intp*destination)\n \nAdvance each iterator in a multi-iterator object, multi, to the given \\(N\\) -dimensional destination where \\(N\\) is the number of dimensions in the broadcasted array. \n   voidPyArray_MultiIter_GOTO1D(PyObject*multi, npy_intpindex)\n \nAdvance each iterator in a multi-iterator object, multi, to the corresponding location of the index into the flattened broadcasted array. \n   intPyArray_MultiIter_NOTDONE(PyObject*multi)\n \nEvaluates TRUE as long as the multi-iterator has not looped through all of the elements (of the broadcasted result), otherwise it evaluates FALSE. \n   intPyArray_Broadcast(PyArrayMultiIterObject*mit)\n \nThis function encapsulates the broadcasting rules. The mit container should already contain iterators for all the arrays that need to be broadcast. On return, these iterators will be adjusted so that iteration over each simultaneously will accomplish the broadcasting. A negative number is returned if an error occurs. \n   intPyArray_RemoveSmallest(PyArrayMultiIterObject*mit)\n \nThis function takes a multi-iterator object that has been previously \u201cbroadcasted,\u201d finds the dimension with the smallest \u201csum of strides\u201d in the broadcasted result and adapts all the iterators so as not to iterate over that dimension (by effectively making them of length-1 in that dimension). The corresponding dimension is returned unless mit ->nd is 0, then -1 is returned. This function is useful for constructing ufunc-like routines that broadcast their inputs correctly and then call a strided 1-d version of the routine as the inner-loop. This 1-d version is usually optimized for speed and for this reason the loop should be performed over the axis that won\u2019t require large stride jumps. \n   Neighborhood iterator  New in version 1.4.0.  Neighborhood iterators are subclasses of the iterator object, and can be used to iter over a neighborhood of a point. For example, you may want to iterate over every voxel of a 3d image, and for every such voxel, iterate over an hypercube. Neighborhood iterator automatically handle boundaries, thus making this kind of code much easier to write than manual boundaries handling, at the cost of a slight overhead.   PyObject*PyArray_NeighborhoodIterNew(PyArrayIterObject*iter, npy_intpbounds, intmode, PyArrayObject*fill_value)\n \nThis function creates a new neighborhood iterator from an existing iterator. The neighborhood will be computed relatively to the position currently pointed by iter, the bounds define the shape of the neighborhood iterator, and the mode argument the boundaries handling mode. The bounds argument is expected to be a (2 * iter->ao->nd) arrays, such as the range bound[2*i]->bounds[2*i+1] defines the range where to walk for dimension i (both bounds are included in the walked coordinates). The bounds should be ordered for each dimension (bounds[2*i] <= bounds[2*i+1]). The mode should be one of:   NPY_NEIGHBORHOOD_ITER_ZERO_PADDING\n \nZero padding. Outside bounds values will be 0. \n   NPY_NEIGHBORHOOD_ITER_ONE_PADDING\n \nOne padding, Outside bounds values will be 1. \n   NPY_NEIGHBORHOOD_ITER_CONSTANT_PADDING\n \nConstant padding. Outside bounds values will be the same as the first item in fill_value. \n   NPY_NEIGHBORHOOD_ITER_MIRROR_PADDING\n \nMirror padding. Outside bounds values will be as if the array items were mirrored. For example, for the array [1, 2, 3, 4], x[-2] will be 2, x[-2] will be 1, x[4] will be 4, x[5] will be 1, etc\u2026 \n   NPY_NEIGHBORHOOD_ITER_CIRCULAR_PADDING\n \nCircular padding. Outside bounds values will be as if the array was repeated. For example, for the array [1, 2, 3, 4], x[-2] will be 3, x[-2] will be 4, x[4] will be 1, x[5] will be 2, etc\u2026 \n If the mode is constant filling (NPY_NEIGHBORHOOD_ITER_CONSTANT_PADDING), fill_value should point to an array object which holds the filling value (the first item will be the filling value if the array contains more than one item). For other cases, fill_value may be NULL.  The iterator holds a reference to iter Return NULL on failure (in which case the reference count of iter is not changed) iter itself can be a Neighborhood iterator: this can be useful for .e.g automatic boundaries handling the object returned by this function should be safe to use as a normal iterator If the position of iter is changed, any subsequent call to PyArrayNeighborhoodIter_Next is undefined behavior, and PyArrayNeighborhoodIter_Reset must be called. If the position of iter is not the beginning of the data and the underlying data for iter is contiguous, the iterator will point to the start of the data instead of position pointed by iter. To avoid this situation, iter should be moved to the required position only after the creation of iterator, and PyArrayNeighborhoodIter_Reset must be called.  PyArrayIterObject *iter;\nPyArrayNeighborhoodIterObject *neigh_iter;\niter = PyArray_IterNew(x);\n\n/*For a 3x3 kernel */\nbounds = {-1, 1, -1, 1};\nneigh_iter = (PyArrayNeighborhoodIterObject*)PyArray_NeighborhoodIterNew(\n     iter, bounds, NPY_NEIGHBORHOOD_ITER_ZERO_PADDING, NULL);\n\nfor(i = 0; i < iter->size; ++i) {\n     for (j = 0; j < neigh_iter->size; ++j) {\n             /* Walk around the item currently pointed by iter->dataptr */\n             PyArrayNeighborhoodIter_Next(neigh_iter);\n     }\n\n     /* Move to the next point of iter */\n     PyArrayIter_Next(iter);\n     PyArrayNeighborhoodIter_Reset(neigh_iter);\n}\n \n   intPyArrayNeighborhoodIter_Reset(PyArrayNeighborhoodIterObject*iter)\n \nReset the iterator position to the first point of the neighborhood. This should be called whenever the iter argument given at PyArray_NeighborhoodIterObject is changed (see example) \n   intPyArrayNeighborhoodIter_Next(PyArrayNeighborhoodIterObject*iter)\n \nAfter this call, iter->dataptr points to the next point of the neighborhood. Calling this function after every point of the neighborhood has been visited is undefined. \n   Array mapping Array mapping is the machinery behind advanced indexing.   PyObject*PyArray_MapIterArray(PyArrayObject*a, PyObject*index)\n \nUse advanced indexing to iterate an array. \n   voidPyArray_MapIterSwapAxes(PyArrayMapIterObject*mit, PyArrayObject**ret, intgetmap)\n \nSwap the axes to or from their inserted form. MapIter always puts the advanced (array) indices first in the iteration. But if they are consecutive, it will insert/transpose them back before returning. This is stored as mit->consec != 0 (the place where they are inserted). For assignments, the opposite happens: the values to be assigned are transposed (getmap=1 instead of getmap=0). getmap=0 and getmap=1 undo the other operation. \n   voidPyArray_MapIterNext(PyArrayMapIterObject*mit)\n \nThis function needs to update the state of the map iterator and point mit->dataptr to the memory-location of the next object. Note that this function never handles an extra operand but provides compatibility for an old (exposed) API. \n   PyObject*PyArray_MapIterArrayCopyIfOverlap(PyArrayObject*a, PyObject*index, intcopy_if_overlap, PyArrayObject*extra_op)\n \nSimilar to PyArray_MapIterArray but with an additional copy_if_overlap argument. If copy_if_overlap != 0, checks if a has memory overlap with any of the arrays in index and with extra_op, and make copies as appropriate to avoid problems if the input is modified during the iteration. iter->array may contain a copied array (UPDATEIFCOPY/WRITEBACKIFCOPY set). \n   Array Scalars   PyObject*PyArray_Return(PyArrayObject*arr)\n \nThis function steals a reference to arr. This function checks to see if arr is a 0-dimensional array and, if so, returns the appropriate array scalar. It should be used whenever 0-dimensional arrays could be returned to Python. \n   PyObject*PyArray_Scalar(void*data, PyArray_Descr*dtype, PyObject*base)\n \nReturn an array scalar object of the given dtype by copying from memory pointed to by data. base is expected to be the array object that is the owner of the data. base is required if dtype is a void scalar, or if the NPY_USE_GETITEM flag is set and it is known that the getitem method uses the arr argument without checking if it is NULL. Otherwise base may be NULL. If the data is not in native byte order (as indicated by dtype->byteorder) then this function will byteswap the data, because array scalars are always in correct machine-byte order. \n   PyObject*PyArray_ToScalar(void*data, PyArrayObject*arr)\n \nReturn an array scalar object of the type and itemsize indicated by the array object arr copied from the memory pointed to by data and swapping if the data in arr is not in machine byte-order. \n   PyObject*PyArray_FromScalar(PyObject*scalar, PyArray_Descr*outcode)\n \nReturn a 0-dimensional array of type determined by outcode from scalar which should be an array-scalar object. If outcode is NULL, then the type is determined from scalar. \n   voidPyArray_ScalarAsCtype(PyObject*scalar, void*ctypeptr)\n \nReturn in ctypeptr a pointer to the actual value in an array scalar. There is no error checking so scalar must be an array-scalar object, and ctypeptr must have enough space to hold the correct type. For flexible-sized types, a pointer to the data is copied into the memory of ctypeptr, for all other types, the actual data is copied into the address pointed to by ctypeptr. \n   voidPyArray_CastScalarToCtype(PyObject*scalar, void*ctypeptr, PyArray_Descr*outcode)\n \nReturn the data (cast to the data type indicated by outcode) from the array-scalar, scalar, into the memory pointed to by ctypeptr (which must be large enough to handle the incoming memory). \n   PyObject*PyArray_TypeObjectFromType(inttype)\n \nReturns a scalar type-object from a type-number, type . Equivalent to PyArray_DescrFromType (type)->typeobj except for reference counting and error-checking. Returns a new reference to the typeobject on success or NULL on failure. \n   NPY_SCALARKINDPyArray_ScalarKind(inttypenum, PyArrayObject**arr)\n \nSee the function PyArray_MinScalarType for an alternative mechanism introduced in NumPy 1.6.0. Return the kind of scalar represented by typenum and the array in *arr (if arr is not NULL ). The array is assumed to be rank-0 and only used if typenum represents a signed integer. If arr is not NULL and the first element is negative then NPY_INTNEG_SCALAR is returned, otherwise NPY_INTPOS_SCALAR is returned. The possible return values are the enumerated values in NPY_SCALARKIND. \n   intPyArray_CanCoerceScalar(charthistype, charneededtype, NPY_SCALARKINDscalar)\n \nSee the function PyArray_ResultType for details of NumPy type promotion, updated in NumPy 1.6.0. Implements the rules for scalar coercion. Scalars are only silently coerced from thistype to neededtype if this function returns nonzero. If scalar is NPY_NOSCALAR, then this function is equivalent to PyArray_CanCastSafely. The rule is that scalars of the same KIND can be coerced into arrays of the same KIND. This rule means that high-precision scalars will never cause low-precision arrays of the same KIND to be upcast. \n   Data-type descriptors  Warning Data-type objects must be reference counted so be aware of the action on the data-type reference of different C-API calls. The standard rule is that when a data-type object is returned it is a new reference. Functions that take PyArray_Descr* objects and return arrays steal references to the data-type their inputs unless otherwise noted. Therefore, you must own a reference to any data-type object used as input to such a function.    intPyArray_DescrCheck(PyObject*obj)\n \nEvaluates as true if obj is a data-type object ( PyArray_Descr* ). \n   PyArray_Descr*PyArray_DescrNew(PyArray_Descr*obj)\n \nReturn a new data-type object copied from obj (the fields reference is just updated so that the new object points to the same fields dictionary if any). \n   PyArray_Descr*PyArray_DescrNewFromType(inttypenum)\n \nCreate a new data-type object from the built-in (or user-registered) data-type indicated by typenum. All builtin types should not have any of their fields changed. This creates a new copy of the PyArray_Descr structure so that you can fill it in as appropriate. This function is especially needed for flexible data-types which need to have a new elsize member in order to be meaningful in array construction. \n   PyArray_Descr*PyArray_DescrNewByteorder(PyArray_Descr*obj, charnewendian)\n \nCreate a new data-type object with the byteorder set according to newendian. All referenced data-type objects (in subdescr and fields members of the data-type object) are also changed (recursively). The value of newendian is one of these macros: \n   NPY_IGNORE\n  NPY_SWAP\n  NPY_NATIVE\n  NPY_LITTLE\n  NPY_BIG\n \nIf a byteorder of NPY_IGNORE is encountered it is left alone. If newendian is NPY_SWAP, then all byte-orders are swapped. Other valid newendian values are NPY_NATIVE, NPY_LITTLE, and NPY_BIG which all cause the returned data-typed descriptor (and all it\u2019s referenced data-type descriptors) to have the corresponding byte- order. \n   PyArray_Descr*PyArray_DescrFromObject(PyObject*op, PyArray_Descr*mintype)\n \nDetermine an appropriate data-type object from the object op (which should be a \u201cnested\u201d sequence object) and the minimum data-type descriptor mintype (which can be NULL ). Similar in behavior to array(op).dtype. Don\u2019t confuse this function with PyArray_DescrConverter. This function essentially looks at all the objects in the (nested) sequence and determines the data-type from the elements it finds. \n   PyArray_Descr*PyArray_DescrFromScalar(PyObject*scalar)\n \nReturn a data-type object from an array-scalar object. No checking is done to be sure that scalar is an array scalar. If no suitable data-type can be determined, then a data-type of NPY_OBJECT is returned by default. \n   PyArray_Descr*PyArray_DescrFromType(inttypenum)\n \nReturns a data-type object corresponding to typenum. The typenum can be one of the enumerated types, a character code for one of the enumerated types, or a user-defined type. If you want to use a flexible size array, then you need to flexible typenum and set the results elsize parameter to the desired size. The typenum is one of the NPY_TYPES. \n   intPyArray_DescrConverter(PyObject*obj, PyArray_Descr**dtype)\n \nConvert any compatible Python object, obj, to a data-type object in dtype. A large number of Python objects can be converted to data-type objects. See Data type objects (dtype) for a complete description. This version of the converter converts None objects to a NPY_DEFAULT_TYPE data-type object. This function can be used with the \u201cO&\u201d character code in PyArg_ParseTuple processing. \n   intPyArray_DescrConverter2(PyObject*obj, PyArray_Descr**dtype)\n \nConvert any compatible Python object, obj, to a data-type object in dtype. This version of the converter converts None objects so that the returned data-type is NULL. This function can also be used with the \u201cO&\u201d character in PyArg_ParseTuple processing. \n   intPyarray_DescrAlignConverter(PyObject*obj, PyArray_Descr**dtype)\n \nLike PyArray_DescrConverter except it aligns C-struct-like objects on word-boundaries as the compiler would. \n   intPyarray_DescrAlignConverter2(PyObject*obj, PyArray_Descr**dtype)\n \nLike PyArray_DescrConverter2 except it aligns C-struct-like objects on word-boundaries as the compiler would. \n   PyObject*PyArray_FieldNames(PyObject*dict)\n \nTake the fields dictionary, dict, such as the one attached to a data-type object and construct an ordered-list of field names such as is stored in the names field of the PyArray_Descr object. \n   Conversion Utilities  For use with PyArg_ParseTuple\n All of these functions can be used in PyArg_ParseTuple (\u2026) with the \u201cO&\u201d format specifier to automatically convert any Python object to the required C-object. All of these functions return NPY_SUCCEED if successful and NPY_FAIL if not. The first argument to all of these function is a Python object. The second argument is the address of the C-type to convert the Python object to.  Warning Be sure to understand what steps you should take to manage the memory when using these conversion functions. These functions can require freeing memory, and/or altering the reference counts of specific objects based on your use.    intPyArray_Converter(PyObject*obj, PyObject**address)\n \nConvert any Python object to a PyArrayObject. If PyArray_Check (obj) is TRUE then its reference count is incremented and a reference placed in address. If obj is not an array, then convert it to an array using PyArray_FromAny . No matter what is returned, you must DECREF the object returned by this routine in address when you are done with it. \n   intPyArray_OutputConverter(PyObject*obj, PyArrayObject**address)\n \nThis is a default converter for output arrays given to functions. If obj is Py_None or NULL, then *address will be NULL but the call will succeed. If PyArray_Check ( obj) is TRUE then it is returned in *address without incrementing its reference count. \n   intPyArray_IntpConverter(PyObject*obj, PyArray_Dims*seq)\n \nConvert any Python sequence, obj, smaller than NPY_MAXDIMS to a C-array of npy_intp. The Python object could also be a single number. The seq variable is a pointer to a structure with members ptr and len. On successful return, seq ->ptr contains a pointer to memory that must be freed, by calling PyDimMem_FREE, to avoid a memory leak. The restriction on memory size allows this converter to be conveniently used for sequences intended to be interpreted as array shapes. \n   intPyArray_BufferConverter(PyObject*obj, PyArray_Chunk*buf)\n \nConvert any Python object, obj, with a (single-segment) buffer interface to a variable with members that detail the object\u2019s use of its chunk of memory. The buf variable is a pointer to a structure with base, ptr, len, and flags members. The PyArray_Chunk structure is binary compatible with the Python\u2019s buffer object (through its len member on 32-bit platforms and its ptr member on 64-bit platforms or in Python 2.5). On return, the base member is set to obj (or its base if obj is already a buffer object pointing to another object). If you need to hold on to the memory be sure to INCREF the base member. The chunk of memory is pointed to by buf ->ptr member and has length buf ->len. The flags member of buf is NPY_ARRAY_ALIGNED with the NPY_ARRAY_WRITEABLE flag set if obj has a writeable buffer interface. \n   intPyArray_AxisConverter(PyObject*obj, int*axis)\n \nConvert a Python object, obj, representing an axis argument to the proper value for passing to the functions that take an integer axis. Specifically, if obj is None, axis is set to NPY_MAXDIMS which is interpreted correctly by the C-API functions that take axis arguments. \n   intPyArray_BoolConverter(PyObject*obj, npy_bool*value)\n \nConvert any Python object, obj, to NPY_TRUE or NPY_FALSE, and place the result in value. \n   intPyArray_ByteorderConverter(PyObject*obj, char*endian)\n \nConvert Python strings into the corresponding byte-order character: \u2018>\u2019, \u2018<\u2019, \u2018s\u2019, \u2018=\u2019, or \u2018|\u2019. \n   intPyArray_SortkindConverter(PyObject*obj, NPY_SORTKIND*sort)\n \nConvert Python strings into one of NPY_QUICKSORT (starts with \u2018q\u2019 or \u2018Q\u2019), NPY_HEAPSORT (starts with \u2018h\u2019 or \u2018H\u2019), NPY_MERGESORT (starts with \u2018m\u2019 or \u2018M\u2019) or NPY_STABLESORT (starts with \u2018t\u2019 or \u2018T\u2019). NPY_MERGESORT and NPY_STABLESORT are aliased to each other for backwards compatibility and may refer to one of several stable sorting algorithms depending on the data type. \n   intPyArray_SearchsideConverter(PyObject*obj, NPY_SEARCHSIDE*side)\n \nConvert Python strings into one of NPY_SEARCHLEFT (starts with \u2018l\u2019 or \u2018L\u2019), or NPY_SEARCHRIGHT (starts with \u2018r\u2019 or \u2018R\u2019). \n   intPyArray_OrderConverter(PyObject*obj, NPY_ORDER*order)\n \nConvert the Python strings \u2018C\u2019, \u2018F\u2019, \u2018A\u2019, and \u2018K\u2019 into the NPY_ORDER enumeration NPY_CORDER, NPY_FORTRANORDER, NPY_ANYORDER, and NPY_KEEPORDER. \n   intPyArray_CastingConverter(PyObject*obj, NPY_CASTING*casting)\n \nConvert the Python strings \u2018no\u2019, \u2018equiv\u2019, \u2018safe\u2019, \u2018same_kind\u2019, and \u2018unsafe\u2019 into the NPY_CASTING enumeration NPY_NO_CASTING, NPY_EQUIV_CASTING, NPY_SAFE_CASTING, NPY_SAME_KIND_CASTING, and NPY_UNSAFE_CASTING. \n   intPyArray_ClipmodeConverter(PyObject*object, NPY_CLIPMODE*val)\n \nConvert the Python strings \u2018clip\u2019, \u2018wrap\u2019, and \u2018raise\u2019 into the NPY_CLIPMODE enumeration NPY_CLIP, NPY_WRAP, and NPY_RAISE. \n   intPyArray_ConvertClipmodeSequence(PyObject*object, NPY_CLIPMODE*modes, intn)\n \nConverts either a sequence of clipmodes or a single clipmode into a C array of NPY_CLIPMODE values. The number of clipmodes n must be known before calling this function. This function is provided to help functions allow a different clipmode for each dimension. \n   Other conversions   intPyArray_PyIntAsInt(PyObject*op)\n \nConvert all kinds of Python objects (including arrays and array scalars) to a standard integer. On error, -1 is returned and an exception set. You may find useful the macro: #define error_converting(x) (((x) == -1) && PyErr_Occurred())\n \n   npy_intpPyArray_PyIntAsIntp(PyObject*op)\n \nConvert all kinds of Python objects (including arrays and array scalars) to a (platform-pointer-sized) integer. On error, -1 is returned and an exception set. \n   intPyArray_IntpFromSequence(PyObject*seq, npy_intp*vals, intmaxvals)\n \nConvert any Python sequence (or single Python number) passed in as seq to (up to) maxvals pointer-sized integers and place them in the vals array. The sequence can be smaller then maxvals as the number of converted objects is returned. \n   intPyArray_TypestrConvert(intitemsize, intgentype)\n \nConvert typestring characters (with itemsize) to basic enumerated data types. The typestring character corresponding to signed and unsigned integers, floating point numbers, and complex-floating point numbers are recognized and converted. Other values of gentype are returned. This function can be used to convert, for example, the string \u2018f4\u2019 to NPY_FLOAT32. \n    Miscellaneous  Importing the API In order to make use of the C-API from another extension module, the import_array function must be called. If the extension module is self-contained in a single .c file, then that is all that needs to be done. If, however, the extension module involves multiple files where the C-API is needed then some additional steps must be taken.   voidimport_array(void)\n \nThis function must be called in the initialization section of a module that will make use of the C-API. It imports the module where the function-pointer table is stored and points the correct variable to it. \n   PY_ARRAY_UNIQUE_SYMBOL\n\n   NO_IMPORT_ARRAY\n \nUsing these #defines you can use the C-API in multiple files for a single extension module. In each file you must define PY_ARRAY_UNIQUE_SYMBOL to some name that will hold the C-API (e.g. myextension_ARRAY_API). This must be done before including the numpy/arrayobject.h file. In the module initialization routine you call import_array. In addition, in the files that do not have the module initialization sub_routine define NO_IMPORT_ARRAY prior to including numpy/arrayobject.h. Suppose I have two files coolmodule.c and coolhelper.c which need to be compiled and linked into a single extension module. Suppose coolmodule.c contains the required initcool module initialization function (with the import_array() function called). Then, coolmodule.c would have at the top: #define PY_ARRAY_UNIQUE_SYMBOL cool_ARRAY_API\n#include numpy/arrayobject.h\n On the other hand, coolhelper.c would contain at the top: #define NO_IMPORT_ARRAY\n#define PY_ARRAY_UNIQUE_SYMBOL cool_ARRAY_API\n#include numpy/arrayobject.h\n You can also put the common two last lines into an extension-local header file as long as you make sure that NO_IMPORT_ARRAY is #defined before #including that file. Internally, these #defines work as follows:  If neither is defined, the C-API is declared to be static void**, so it is only visible within the compilation unit that #includes numpy/arrayobject.h. If PY_ARRAY_UNIQUE_SYMBOL is #defined, but NO_IMPORT_ARRAY is not, the C-API is declared to be void**, so that it will also be visible to other compilation units. If NO_IMPORT_ARRAY is #defined, regardless of whether PY_ARRAY_UNIQUE_SYMBOL is, the C-API is declared to be extern void**, so it is expected to be defined in another compilation unit. Whenever PY_ARRAY_UNIQUE_SYMBOL is #defined, it also changes the name of the variable holding the C-API, which defaults to PyArray_API, to whatever the macro is #defined to.  \n   Checking the API Version Because python extensions are not used in the same way as usual libraries on most platforms, some errors cannot be automatically detected at build time or even runtime. For example, if you build an extension using a function available only for numpy >= 1.3.0, and you import the extension later with numpy 1.2, you will not get an import error (but almost certainly a segmentation fault when calling the function). That\u2019s why several functions are provided to check for numpy versions. The macros NPY_VERSION and NPY_FEATURE_VERSION corresponds to the numpy version used to build the extension, whereas the versions returned by the functions PyArray_GetNDArrayCVersion and PyArray_GetNDArrayCFeatureVersion corresponds to the runtime numpy\u2019s version. The rules for ABI and API compatibilities can be summarized as follows:  Whenever NPY_VERSION != PyArray_GetNDArrayCVersion(), the extension has to be recompiled (ABI incompatibility). \nNPY_VERSION == PyArray_GetNDArrayCVersion() and NPY_FEATURE_VERSION <= PyArray_GetNDArrayCFeatureVersion() means backward compatible changes.  ABI incompatibility is automatically detected in every numpy\u2019s version. API incompatibility detection was added in numpy 1.4.0. If you want to supported many different numpy versions with one extension binary, you have to build your extension with the lowest NPY_FEATURE_VERSION as possible.   NPY_VERSION\n \nThe current version of the ndarray object (check to see if this variable is defined to guarantee the numpy/arrayobject.h header is being used). \n   NPY_FEATURE_VERSION\n \nThe current version of the C-API. \n   unsignedintPyArray_GetNDArrayCVersion(void)\n \nThis just returns the value NPY_VERSION. NPY_VERSION changes whenever a backward incompatible change at the ABI level. Because it is in the C-API, however, comparing the output of this function from the value defined in the current header gives a way to test if the C-API has changed thus requiring a re-compilation of extension modules that use the C-API. This is automatically checked in the function import_array. \n   unsignedintPyArray_GetNDArrayCFeatureVersion(void)\n \n New in version 1.4.0.  This just returns the value NPY_FEATURE_VERSION. NPY_FEATURE_VERSION changes whenever the API changes (e.g. a function is added). A changed value does not always require a recompile. \n   Internal Flexibility   intPyArray_SetNumericOps(PyObject*dict)\n \nNumPy stores an internal table of Python callable objects that are used to implement arithmetic operations for arrays as well as certain array calculation methods. This function allows the user to replace any or all of these Python objects with their own versions. The keys of the dictionary, dict, are the named functions to replace and the paired value is the Python callable object to use. Care should be taken that the function used to replace an internal array operation does not itself call back to that internal array operation (unless you have designed the function to handle that), or an unchecked infinite recursion can result (possibly causing program crash). The key names that represent operations that can be replaced are: add, subtract, multiply, divide, remainder, power, square, reciprocal, ones_like, sqrt, negative, positive, absolute, invert, left_shift, right_shift, bitwise_and, bitwise_xor, bitwise_or, less, less_equal, equal, not_equal, greater, greater_equal, floor_divide, true_divide, logical_or, logical_and, floor, ceil, maximum, minimum, rint. These functions are included here because they are used at least once in the array object\u2019s methods. The function returns -1 (without setting a Python Error) if one of the objects being assigned is not callable.  Deprecated since version 1.16.  \n   PyObject*PyArray_GetNumericOps(void)\n \nReturn a Python dictionary containing the callable Python objects stored in the internal arithmetic operation table. The keys of this dictionary are given in the explanation for PyArray_SetNumericOps.  Deprecated since version 1.16.  \n   voidPyArray_SetStringFunction(PyObject*op, intrepr)\n \nThis function allows you to alter the tp_str and tp_repr methods of the array object to any Python function. Thus you can alter what happens for all arrays when str(arr) or repr(arr) is called from Python. The function to be called is passed in as op. If repr is non-zero, then this function will be called in response to repr(arr), otherwise the function will be called in response to str(arr). No check on whether or not op is callable is performed. The callable passed in to op should expect an array argument and should return a string to be printed. \n   Memory management   char*PyDataMem_NEW(size_tnbytes)\n\n   voidPyDataMem_FREE(char*ptr)\n\n   char*PyDataMem_RENEW(void*ptr, size_tnewbytes)\n \nMacros to allocate, free, and reallocate memory. These macros are used internally to create arrays. \n   npy_intp*PyDimMem_NEW(intnd)\n\n   voidPyDimMem_FREE(char*ptr)\n\n   npy_intp*PyDimMem_RENEW(void*ptr, size_tnewnd)\n \nMacros to allocate, free, and reallocate dimension and strides memory. \n   void*PyArray_malloc(size_tnbytes)\n\n   voidPyArray_free(void*ptr)\n\n   void*PyArray_realloc(npy_intp*ptr, size_tnbytes)\n \nThese macros use different memory allocators, depending on the constant NPY_USE_PYMEM. The system malloc is used when NPY_USE_PYMEM is 0, if NPY_USE_PYMEM is 1, then the Python memory allocator is used.   NPY_USE_PYMEM\n\n \n   intPyArray_ResolveWritebackIfCopy(PyArrayObject*obj)\n \nIf obj.flags has NPY_ARRAY_WRITEBACKIFCOPY or (deprecated) NPY_ARRAY_UPDATEIFCOPY, this function clears the flags, DECREF s obj->base and makes it writeable, and sets obj->base to NULL. It then copies obj->data to obj->base->data, and returns the error state of the copy operation. This is the opposite of PyArray_SetWritebackIfCopyBase. Usually this is called once you are finished with obj, just before Py_DECREF(obj). It may be called multiple times, or with NULL input. See also PyArray_DiscardWritebackIfCopy. Returns 0 if nothing was done, -1 on error, and 1 if action was taken. \n   Threading support These macros are only meaningful if NPY_ALLOW_THREADS evaluates True during compilation of the extension module. Otherwise, these macros are equivalent to whitespace. Python uses a single Global Interpreter Lock (GIL) for each Python process so that only a single thread may execute at a time (even on multi-cpu machines). When calling out to a compiled function that may take time to compute (and does not have side-effects for other threads like updated global variables), the GIL should be released so that other Python threads can run while the time-consuming calculations are performed. This can be accomplished using two groups of macros. Typically, if one macro in a group is used in a code block, all of them must be used in the same code block. Currently, NPY_ALLOW_THREADS is defined to the python-defined WITH_THREADS constant unless the environment variable NPY_NOSMP is set in which case NPY_ALLOW_THREADS is defined to be 0.   NPY_ALLOW_THREADS\n\n   WITH_THREADS\n\n  Group 1 This group is used to call code that may take some time but does not use any Python C-API calls. Thus, the GIL should be released during its calculation.   NPY_BEGIN_ALLOW_THREADS\n \nEquivalent to Py_BEGIN_ALLOW_THREADS except it uses NPY_ALLOW_THREADS to determine if the macro if replaced with white-space or not. \n   NPY_END_ALLOW_THREADS\n \nEquivalent to Py_END_ALLOW_THREADS except it uses NPY_ALLOW_THREADS to determine if the macro if replaced with white-space or not. \n   NPY_BEGIN_THREADS_DEF\n \nPlace in the variable declaration area. This macro sets up the variable needed for storing the Python state. \n   NPY_BEGIN_THREADS\n \nPlace right before code that does not need the Python interpreter (no Python C-API calls). This macro saves the Python state and releases the GIL. \n   NPY_END_THREADS\n \nPlace right after code that does not need the Python interpreter. This macro acquires the GIL and restores the Python state from the saved variable. \n   voidNPY_BEGIN_THREADS_DESCR(PyArray_Descr*dtype)\n \nUseful to release the GIL only if dtype does not contain arbitrary Python objects which may need the Python interpreter during execution of the loop. \n   voidNPY_END_THREADS_DESCR(PyArray_Descr*dtype)\n \nUseful to regain the GIL in situations where it was released using the BEGIN form of this macro. \n   voidNPY_BEGIN_THREADS_THRESHOLDED(intloop_size)\n \nUseful to release the GIL only if loop_size exceeds a minimum threshold, currently set to 500. Should be matched with a NPY_END_THREADS to regain the GIL. \n   Group 2 This group is used to re-acquire the Python GIL after it has been released. For example, suppose the GIL has been released (using the previous calls), and then some path in the code (perhaps in a different subroutine) requires use of the Python C-API, then these macros are useful to acquire the GIL. These macros accomplish essentially a reverse of the previous three (acquire the LOCK saving what state it had) and then re-release it with the saved state.   NPY_ALLOW_C_API_DEF\n \nPlace in the variable declaration area to set up the necessary variable. \n   NPY_ALLOW_C_API\n \nPlace before code that needs to call the Python C-API (when it is known that the GIL has already been released). \n   NPY_DISABLE_C_API\n \nPlace after code that needs to call the Python C-API (to re-release the GIL). \n  Tip Never use semicolons after the threading support macros.     Priority   NPY_PRIORITY\n \nDefault priority for arrays. \n   NPY_SUBTYPE_PRIORITY\n \nDefault subtype priority. \n   NPY_SCALAR_PRIORITY\n \nDefault scalar priority (very small) \n   doublePyArray_GetPriority(PyObject*obj, doubledef)\n \nReturn the __array_priority__ attribute (converted to a double) of obj or def if no attribute of that name exists. Fast returns that avoid the attribute lookup are provided for objects of type PyArray_Type. \n   Default buffers   NPY_BUFSIZE\n \nDefault size of the user-settable internal buffers. \n   NPY_MIN_BUFSIZE\n \nSmallest size of user-settable internal buffers. \n   NPY_MAX_BUFSIZE\n \nLargest size allowed for the user-settable buffers. \n   Other constants   NPY_NUM_FLOATTYPE\n \nThe number of floating-point types \n   NPY_MAXDIMS\n \nThe maximum number of dimensions allowed in arrays. \n   NPY_MAXARGS\n \nThe maximum number of array arguments that can be used in functions. \n   NPY_FALSE\n \nDefined as 0 for use with Bool. \n   NPY_TRUE\n \nDefined as 1 for use with Bool. \n   NPY_FAIL\n \nThe return value of failed converter functions which are called using the \u201cO&\u201d syntax in PyArg_ParseTuple-like functions. \n   NPY_SUCCEED\n \nThe return value of successful converter functions which are called using the \u201cO&\u201d syntax in PyArg_ParseTuple-like functions. \n   Miscellaneous Macros   intPyArray_SAMESHAPE(PyArrayObject*a1, PyArrayObject*a2)\n \nEvaluates as True if arrays a1 and a2 have the same shape. \n   PyArray_MAX(a, b)\n \nReturns the maximum of a and b. If (a) or (b) are expressions they are evaluated twice. \n   PyArray_MIN(a, b)\n \nReturns the minimum of a and b. If (a) or (b) are expressions they are evaluated twice. \n   PyArray_CLT(a, b)\n\n   PyArray_CGT(a, b)\n\n   PyArray_CLE(a, b)\n\n   PyArray_CGE(a, b)\n\n   PyArray_CEQ(a, b)\n\n   PyArray_CNE(a, b)\n \nImplements the complex comparisons between two complex numbers (structures with a real and imag member) using NumPy\u2019s definition of the ordering which is lexicographic: comparing the real parts first and then the complex parts if the real parts are equal. \n   npy_intpPyArray_REFCOUNT(PyObject*op)\n \nReturns the reference count of any Python object. \n   voidPyArray_DiscardWritebackIfCopy(PyObject*obj)\n \nIf obj.flags has NPY_ARRAY_WRITEBACKIFCOPY or (deprecated) NPY_ARRAY_UPDATEIFCOPY, this function clears the flags, DECREF s obj->base and makes it writeable, and sets obj->base to NULL. In contrast to PyArray_DiscardWritebackIfCopy it makes no attempt to copy the data from obj->base This undoes PyArray_SetWritebackIfCopyBase. Usually this is called after an error when you are finished with obj, just before Py_DECREF(obj). It may be called multiple times, or with NULL input. \n   voidPyArray_XDECREF_ERR(PyObject*obj)\n \nDeprecated in 1.14, use PyArray_DiscardWritebackIfCopy followed by Py_XDECREF DECREF\u2019s an array object which may have the (deprecated) NPY_ARRAY_UPDATEIFCOPY or NPY_ARRAY_WRITEBACKIFCOPY flag set without causing the contents to be copied back into the original array. Resets the NPY_ARRAY_WRITEABLE flag on the base object. This is useful for recovering from an error condition when writeback semantics are used, but will lead to wrong results. \n   Enumerated Types   enumNPY_SORTKIND\n \nA special variable-type which can take on different values to indicate the sorting algorithm being used.   enumeratorNPY_QUICKSORT\n\n   enumeratorNPY_HEAPSORT\n\n   enumeratorNPY_MERGESORT\n\n   enumeratorNPY_STABLESORT\n \nUsed as an alias of NPY_MERGESORT and vica versa. \n   enumeratorNPY_NSORTS\n \nDefined to be the number of sorts. It is fixed at three by the need for backwards compatibility, and consequently NPY_MERGESORT and NPY_STABLESORT are aliased to each other and may refer to one of several stable sorting algorithms depending on the data type. \n \n   enumNPY_SCALARKIND\n \nA special variable type indicating the number of \u201ckinds\u201d of scalars distinguished in determining scalar-coercion rules. This variable can take on the values:   enumeratorNPY_NOSCALAR\n\n   enumeratorNPY_BOOL_SCALAR\n\n   enumeratorNPY_INTPOS_SCALAR\n\n   enumeratorNPY_INTNEG_SCALAR\n\n   enumeratorNPY_FLOAT_SCALAR\n\n   enumeratorNPY_COMPLEX_SCALAR\n\n   enumeratorNPY_OBJECT_SCALAR\n\n   enumeratorNPY_NSCALARKINDS\n \nDefined to be the number of scalar kinds (not including NPY_NOSCALAR). \n \n   enumNPY_ORDER\n \nAn enumeration type indicating the element order that an array should be interpreted in. When a brand new array is created, generally only NPY_CORDER and NPY_FORTRANORDER are used, whereas when one or more inputs are provided, the order can be based on them.   enumeratorNPY_ANYORDER\n \nFortran order if all the inputs are Fortran, C otherwise. \n   enumeratorNPY_CORDER\n \nC order. \n   enumeratorNPY_FORTRANORDER\n \nFortran order. \n   enumeratorNPY_KEEPORDER\n \nAn order as close to the order of the inputs as possible, even if the input is in neither C nor Fortran order. \n \n   enumNPY_CLIPMODE\n \nA variable type indicating the kind of clipping that should be applied in certain functions.   enumeratorNPY_RAISE\n \nThe default for most operations, raises an exception if an index is out of bounds. \n   enumeratorNPY_CLIP\n \nClips an index to the valid range if it is out of bounds. \n   enumeratorNPY_WRAP\n \nWraps an index to the valid range if it is out of bounds. \n \n   enumNPY_SEARCHSIDE\n \nA variable type indicating whether the index returned should be that of the first suitable location (if NPY_SEARCHLEFT) or of the last (if NPY_SEARCHRIGHT).   enumeratorNPY_SEARCHLEFT\n\n   enumeratorNPY_SEARCHRIGHT\n\n \n   enumNPY_SELECTKIND\n \nA variable type indicating the selection algorithm being used.   enumeratorNPY_INTROSELECT\n\n \n   enumNPY_CASTING\n \n New in version 1.6.  An enumeration type indicating how permissive data conversions should be. This is used by the iterator added in NumPy 1.6, and is intended to be used more broadly in a future version.   enumeratorNPY_NO_CASTING\n \nOnly allow identical types. \n   enumeratorNPY_EQUIV_CASTING\n \nAllow identical and casts involving byte swapping. \n   enumeratorNPY_SAFE_CASTING\n \nOnly allow casts which will not cause values to be rounded, truncated, or otherwise changed. \n   enumeratorNPY_SAME_KIND_CASTING\n \nAllow any safe casts, and casts between types of the same kind. For example, float64 -> float32 is permitted with this rule. \n   enumeratorNPY_UNSAFE_CASTING\n \nAllow any cast, no matter what kind of data loss may occur. \n \n  \n"}, {"name": "int PyArray_ObjectType()", "path": "reference/c-api/array#c.PyArray_ObjectType", "type": "Array API", "text": "  intPyArray_ObjectType(PyObject*op, intmintype)\n \nThis function is superseded by PyArray_MinScalarType and/or PyArray_ResultType. This function is useful for determining a common type that two or more arrays can be converted to. It only works for non-flexible array types as no itemsize information is passed. The mintype argument represents the minimum type acceptable, and op represents the object that will be converted to an array. The return value is the enumerated typenumber that represents the data-type that op should have. \n"}, {"name": "int PyArray_OrderConverter()", "path": "reference/c-api/array#c.PyArray_OrderConverter", "type": "Array API", "text": "  intPyArray_OrderConverter(PyObject*obj, NPY_ORDER*order)\n \nConvert the Python strings \u2018C\u2019, \u2018F\u2019, \u2018A\u2019, and \u2018K\u2019 into the NPY_ORDER enumeration NPY_CORDER, NPY_FORTRANORDER, NPY_ANYORDER, and NPY_KEEPORDER. \n"}, {"name": "int PyArray_OutputConverter()", "path": "reference/c-api/array#c.PyArray_OutputConverter", "type": "Array API", "text": "  intPyArray_OutputConverter(PyObject*obj, PyArrayObject**address)\n \nThis is a default converter for output arrays given to functions. If obj is Py_None or NULL, then *address will be NULL but the call will succeed. If PyArray_Check ( obj) is TRUE then it is returned in *address without incrementing its reference count. \n"}, {"name": "int PyArray_Partition()", "path": "reference/c-api/array#c.PyArray_Partition", "type": "Array API", "text": "  intPyArray_Partition(PyArrayObject*self, PyArrayObject*ktharray, intaxis, NPY_SELECTKINDwhich)\n \nEquivalent to ndarray.partition (self, ktharray, axis, kind). Partitions the array so that the values of the element indexed by ktharray are in the positions they would be if the array is fully sorted and places all elements smaller than the kth before and all elements equal or greater after the kth element. The ordering of all elements within the partitions is undefined. If self->descr is a data-type with fields defined, then self->descr->names is used to determine the sort order. A comparison where the first field is equal will use the second field and so on. To alter the sort order of a structured array, create a new data-type with a different order of names and construct a view of the array with that new data-type. Returns zero on success and -1 on failure. \n"}, {"name": "int PyArray_RegisterCanCast()", "path": "reference/c-api/array#c.PyArray_RegisterCanCast", "type": "Array API", "text": "  intPyArray_RegisterCanCast(PyArray_Descr*descr, inttotype, NPY_SCALARKINDscalar)\n \nRegister the data-type number, totype, as castable from data-type object, descr, of the given scalar kind. Use scalar = NPY_NOSCALAR to register that an array of data-type descr can be cast safely to a data-type whose type_number is totype. The return value is 0 on success or -1 on failure. \n"}, {"name": "int PyArray_RegisterCastFunc()", "path": "reference/c-api/array#c.PyArray_RegisterCastFunc", "type": "Array API", "text": "  intPyArray_RegisterCastFunc(PyArray_Descr*descr, inttotype, PyArray_VectorUnaryFunc*castfunc)\n \nRegister a low-level casting function, castfunc, to convert from the data-type, descr, to the given data-type number, totype. Any old casting function is over-written. A 0 is returned on success or a -1 on failure. \n"}, {"name": "int PyArray_RegisterDataType()", "path": "reference/c-api/array#c.PyArray_RegisterDataType", "type": "Array API", "text": "  intPyArray_RegisterDataType(PyArray_Descr*dtype)\n \nRegister a data-type as a new user-defined data type for arrays. The type must have most of its entries filled in. This is not always checked and errors can produce segfaults. In particular, the typeobj member of the dtype structure must be filled with a Python type that has a fixed-size element-size that corresponds to the elsize member of dtype. Also the f member must have the required functions: nonzero, copyswap, copyswapn, getitem, setitem, and cast (some of the cast functions may be NULL if no support is desired). To avoid confusion, you should choose a unique character typecode but this is not enforced and not relied on internally. A user-defined type number is returned that uniquely identifies the type. A pointer to the new structure can then be obtained from PyArray_DescrFromType using the returned type number. A -1 is returned if an error occurs. If this dtype has already been registered (checked only by the address of the pointer), then return the previously-assigned type-number. \n"}, {"name": "int PyArray_RemoveSmallest()", "path": "reference/c-api/array#c.PyArray_RemoveSmallest", "type": "Array API", "text": "  intPyArray_RemoveSmallest(PyArrayMultiIterObject*mit)\n \nThis function takes a multi-iterator object that has been previously \u201cbroadcasted,\u201d finds the dimension with the smallest \u201csum of strides\u201d in the broadcasted result and adapts all the iterators so as not to iterate over that dimension (by effectively making them of length-1 in that dimension). The corresponding dimension is returned unless mit ->nd is 0, then -1 is returned. This function is useful for constructing ufunc-like routines that broadcast their inputs correctly and then call a strided 1-d version of the routine as the inner-loop. This 1-d version is usually optimized for speed and for this reason the loop should be performed over the axis that won\u2019t require large stride jumps. \n"}, {"name": "int PyArray_ResolveWritebackIfCopy()", "path": "reference/c-api/array#c.PyArray_ResolveWritebackIfCopy", "type": "Array API", "text": "  intPyArray_ResolveWritebackIfCopy(PyArrayObject*obj)\n \nIf obj.flags has NPY_ARRAY_WRITEBACKIFCOPY or (deprecated) NPY_ARRAY_UPDATEIFCOPY, this function clears the flags, DECREF s obj->base and makes it writeable, and sets obj->base to NULL. It then copies obj->data to obj->base->data, and returns the error state of the copy operation. This is the opposite of PyArray_SetWritebackIfCopyBase. Usually this is called once you are finished with obj, just before Py_DECREF(obj). It may be called multiple times, or with NULL input. See also PyArray_DiscardWritebackIfCopy. Returns 0 if nothing was done, -1 on error, and 1 if action was taken. \n"}, {"name": "int PyArray_SearchsideConverter()", "path": "reference/c-api/array#c.PyArray_SearchsideConverter", "type": "Array API", "text": "  intPyArray_SearchsideConverter(PyObject*obj, NPY_SEARCHSIDE*side)\n \nConvert Python strings into one of NPY_SEARCHLEFT (starts with \u2018l\u2019 or \u2018L\u2019), or NPY_SEARCHRIGHT (starts with \u2018r\u2019 or \u2018R\u2019). \n"}, {"name": "int PyArray_SetBaseObject()", "path": "reference/c-api/array#c.PyArray_SetBaseObject", "type": "Array API", "text": "  intPyArray_SetBaseObject(PyArrayObject*arr, PyObject*obj)\n \n New in version 1.7.  This function steals a reference to obj and sets it as the base property of arr. If you construct an array by passing in your own memory buffer as a parameter, you need to set the array\u2019s base property to ensure the lifetime of the memory buffer is appropriate. The return value is 0 on success, -1 on failure. If the object provided is an array, this function traverses the chain of base pointers so that each array points to the owner of the memory directly. Once the base is set, it may not be changed to another value. \n"}, {"name": "int PyArray_SetField()", "path": "reference/c-api/array#c.PyArray_SetField", "type": "Array API", "text": "  intPyArray_SetField(PyArrayObject*self, PyArray_Descr*dtype, intoffset, PyObject*val)\n \nEquivalent to ndarray.setfield (self, val, dtype, offset ). Set the field starting at offset in bytes and of the given dtype to val. The offset plus dtype ->elsize must be less than self ->descr->elsize or an error is raised. Otherwise, the val argument is converted to an array and copied into the field pointed to. If necessary, the elements of val are repeated to fill the destination array, But, the number of elements in the destination must be an integer multiple of the number of elements in val. \n"}, {"name": "int PyArray_SETITEM()", "path": "reference/c-api/array#c.PyArray_SETITEM", "type": "Array API", "text": "  intPyArray_SETITEM(PyArrayObject*arr, void*itemptr, PyObject*obj)\n \nConvert obj and place it in the ndarray, arr, at the place pointed to by itemptr. Return -1 if an error occurs or 0 on success. \n"}, {"name": "int PyArray_SetUpdateIfCopyBase()", "path": "reference/c-api/array#c.PyArray_SetUpdateIfCopyBase", "type": "Array API", "text": "  intPyArray_SetUpdateIfCopyBase(PyArrayObject*arr, PyArrayObject*base)\n \nPrecondition: arr is a copy of base (though possibly with different strides, ordering, etc.) Set the UPDATEIFCOPY flag and arr->base so that when arr is destructed, it will copy any changes back to base. DEPRECATED, use PyArray_SetWritebackIfCopyBase. Returns 0 for success, -1 for failure. \n"}, {"name": "int PyArray_SetWritebackIfCopyBase()", "path": "reference/c-api/array#c.PyArray_SetWritebackIfCopyBase", "type": "Array API", "text": "  intPyArray_SetWritebackIfCopyBase(PyArrayObject*arr, PyArrayObject*base)\n \nPrecondition: arr is a copy of base (though possibly with different strides, ordering, etc.) Sets the NPY_ARRAY_WRITEBACKIFCOPY flag and arr->base, and set base to READONLY. Call PyArray_ResolveWritebackIfCopy before calling Py_DECREF in order copy any changes back to base and reset the READONLY flag. Returns 0 for success, -1 for failure. \n"}, {"name": "int PyArray_SortkindConverter()", "path": "reference/c-api/array#c.PyArray_SortkindConverter", "type": "Array API", "text": "  intPyArray_SortkindConverter(PyObject*obj, NPY_SORTKIND*sort)\n \nConvert Python strings into one of NPY_QUICKSORT (starts with \u2018q\u2019 or \u2018Q\u2019), NPY_HEAPSORT (starts with \u2018h\u2019 or \u2018H\u2019), NPY_MERGESORT (starts with \u2018m\u2019 or \u2018M\u2019) or NPY_STABLESORT (starts with \u2018t\u2019 or \u2018T\u2019). NPY_MERGESORT and NPY_STABLESORT are aliased to each other for backwards compatibility and may refer to one of several stable sorting algorithms depending on the data type. \n"}, {"name": "int PyArray_TYPE()", "path": "reference/c-api/array#c.PyArray_TYPE", "type": "Array API", "text": "  intPyArray_TYPE(PyArrayObject*arr)\n \nReturn the (builtin) typenumber for the elements of this array. \n"}, {"name": "int PyArray_TypeNumFromName()", "path": "reference/c-api/array#c.PyArray_TypeNumFromName", "type": "Array API", "text": "  intPyArray_TypeNumFromName(charconst*str)\n \nGiven a string return the type-number for the data-type with that string as the type-object name. Returns NPY_NOTYPE without setting an error if no type can be found. Only works for user-defined data-types. \n"}, {"name": "int PyArray_TypestrConvert()", "path": "reference/c-api/array#c.PyArray_TypestrConvert", "type": "Array API", "text": "  intPyArray_TypestrConvert(intitemsize, intgentype)\n \nConvert typestring characters (with itemsize) to basic enumerated data types. The typestring character corresponding to signed and unsigned integers, floating point numbers, and complex-floating point numbers are recognized and converted. Other values of gentype are returned. This function can be used to convert, for example, the string \u2018f4\u2019 to NPY_FLOAT32. \n"}, {"name": "int PyArray_ValidType()", "path": "reference/c-api/array#c.PyArray_ValidType", "type": "Array API", "text": "  intPyArray_ValidType(inttypenum)\n \nReturns NPY_TRUE if typenum represents a valid type-number (builtin or user-defined or character code). Otherwise, this function returns NPY_FALSE. \n"}, {"name": "int PyArray_XDECREF()", "path": "reference/c-api/array#c.PyArray_XDECREF", "type": "Array API", "text": "  intPyArray_XDECREF(PyArrayObject*op)\n \nUsed for an array, op, that contains any Python objects. It decrements the reference count of every object in the array according to the data-type of op. Normal return value is 0. A -1 is returned if an error occurs. \n"}, {"name": "int PyArrayIter_Check()", "path": "reference/c-api/array#c.PyArrayIter_Check", "type": "Array API", "text": "  intPyArrayIter_Check(PyObject*op)\n \nEvaluates true if op is an array iterator (or instance of a subclass of the array iterator type). \n"}, {"name": "int PyArrayNeighborhoodIter_Next()", "path": "reference/c-api/array#c.PyArrayNeighborhoodIter_Next", "type": "Array API", "text": "  intPyArrayNeighborhoodIter_Next(PyArrayNeighborhoodIterObject*iter)\n \nAfter this call, iter->dataptr points to the next point of the neighborhood. Calling this function after every point of the neighborhood has been visited is undefined. \n"}, {"name": "int PyArrayNeighborhoodIter_Reset()", "path": "reference/c-api/array#c.PyArrayNeighborhoodIter_Reset", "type": "Array API", "text": "  intPyArrayNeighborhoodIter_Reset(PyArrayNeighborhoodIterObject*iter)\n \nReset the iterator position to the first point of the neighborhood. This should be called whenever the iter argument given at PyArray_NeighborhoodIterObject is changed (see example) \n"}, {"name": "int PyDataType_FLAGCHK()", "path": "reference/c-api/types-and-structures#c.NPY_USE_SETITEM.PyDataType_FLAGCHK", "type": "Python Types and C-Structures", "text": "  intPyDataType_FLAGCHK(PyArray_Descr*dtype, intflags)\n \nReturn true if all the given flags are set for the data-type object. \n"}, {"name": "int PyDataType_HASFIELDS()", "path": "reference/c-api/array#c.PyDataType_HASFIELDS", "type": "Array API", "text": "  intPyDataType_HASFIELDS(PyArray_Descr*descr)\n\n"}, {"name": "int PyDataType_ISBOOL()", "path": "reference/c-api/array#c.PyDataType_ISBOOL", "type": "Array API", "text": "  intPyDataType_ISBOOL(PyArray_Descr*descr)\n\n"}, {"name": "int PyDataType_ISCOMPLEX()", "path": "reference/c-api/array#c.PyDataType_ISCOMPLEX", "type": "Array API", "text": "  intPyDataType_ISCOMPLEX(PyArray_Descr*descr)\n\n"}, {"name": "int PyDataType_ISEXTENDED()", "path": "reference/c-api/array#c.PyDataType_ISEXTENDED", "type": "Array API", "text": "  intPyDataType_ISEXTENDED(PyArray_Descr*descr)\n\n"}, {"name": "int PyDataType_ISFLEXIBLE()", "path": "reference/c-api/array#c.PyDataType_ISFLEXIBLE", "type": "Array API", "text": "  intPyDataType_ISFLEXIBLE(PyArray_Descr*descr)\n\n"}, {"name": "int PyDataType_ISFLOAT()", "path": "reference/c-api/array#c.PyDataType_ISFLOAT", "type": "Array API", "text": "  intPyDataType_ISFLOAT(PyArray_Descr*descr)\n\n"}, {"name": "int PyDataType_ISINTEGER()", "path": "reference/c-api/array#c.PyDataType_ISINTEGER", "type": "Array API", "text": "  intPyDataType_ISINTEGER(PyArray_Descr*descr)\n\n"}, {"name": "int PyDataType_ISNUMBER()", "path": "reference/c-api/array#c.PyDataType_ISNUMBER", "type": "Array API", "text": "  intPyDataType_ISNUMBER(PyArray_Descr*descr)\n\n"}, {"name": "int PyDataType_ISOBJECT()", "path": "reference/c-api/array#c.PyDataType_ISOBJECT", "type": "Array API", "text": "  intPyDataType_ISOBJECT(PyArray_Descr*descr)\n\n"}, {"name": "int PyDataType_ISPYTHON()", "path": "reference/c-api/array#c.PyDataType_ISPYTHON", "type": "Array API", "text": "  intPyDataType_ISPYTHON(PyArray_Descr*descr)\n\n"}, {"name": "int PyDataType_ISSIGNED()", "path": "reference/c-api/array#c.PyDataType_ISSIGNED", "type": "Array API", "text": "  intPyDataType_ISSIGNED(PyArray_Descr*descr)\n\n"}, {"name": "int PyDataType_ISSTRING()", "path": "reference/c-api/array#c.PyDataType_ISSTRING", "type": "Array API", "text": "  intPyDataType_ISSTRING(PyArray_Descr*descr)\n\n"}, {"name": "int PyDataType_ISUNSIGNED()", "path": "reference/c-api/array#c.PyDataType_ISUNSIGNED", "type": "Array API", "text": "  intPyDataType_ISUNSIGNED(PyArray_Descr*descr)\n\n"}, {"name": "int PyDataType_ISUNSIZED()", "path": "reference/c-api/array#c.PyDataType_ISUNSIZED", "type": "Array API", "text": "  intPyDataType_ISUNSIZED(PyArray_Descr*descr)\n \nType has no size information attached, and can be resized. Should only be called on flexible dtypes. Types that are attached to an array will always be sized, hence the array form of this macro not existing.  Changed in version 1.18.  For structured datatypes with no fields this function now returns False. \n"}, {"name": "int PyDataType_ISUSERDEF()", "path": "reference/c-api/array#c.PyDataType_ISUSERDEF", "type": "Array API", "text": "  intPyDataType_ISUSERDEF(PyArray_Descr*descr)\n\n"}, {"name": "int PyDataType_REFCHK()", "path": "reference/c-api/types-and-structures#c.NPY_USE_SETITEM.PyDataType_REFCHK", "type": "Python Types and C-Structures", "text": "  intPyDataType_REFCHK(PyArray_Descr*dtype)\n \nEquivalent to PyDataType_FLAGCHK (dtype, NPY_ITEM_REFCOUNT). \n"}, {"name": "int PyModule_AddIntConstant()", "path": "user/c-info.how-to-extend#c.PyModule_AddIntConstant", "type": "User Guide", "text": "  intPyModule_AddIntConstant(PyObject*module, char*name, longvalue)\n\n"}, {"name": "int PyModule_AddObject()", "path": "user/c-info.how-to-extend", "type": "User Guide", "text": "How to extend NumPy  Writing an extension module While the ndarray object is designed to allow rapid computation in Python, it is also designed to be general-purpose and satisfy a wide- variety of computational needs. As a result, if absolute speed is essential, there is no replacement for a well-crafted, compiled loop specific to your application and hardware. This is one of the reasons that numpy includes f2py so that an easy-to-use mechanisms for linking (simple) C/C++ and (arbitrary) Fortran code directly into Python are available. You are encouraged to use and improve this mechanism. The purpose of this section is not to document this tool but to document the more basic steps to writing an extension module that this tool depends on. When an extension module is written, compiled, and installed to somewhere in the Python path (sys.path), the code can then be imported into Python as if it were a standard python file. It will contain objects and methods that have been defined and compiled in C code. The basic steps for doing this in Python are well-documented and you can find more information in the documentation for Python itself available online at www.python.org . In addition to the Python C-API, there is a full and rich C-API for NumPy allowing sophisticated manipulations on a C-level. However, for most applications, only a few API calls will typically be used. For example, if you need to just extract a pointer to memory along with some shape information to pass to another calculation routine, then you will use very different calls than if you are trying to create a new array-like type or add a new data type for ndarrays. This chapter documents the API calls and macros that are most commonly used.   Required subroutine There is exactly one function that must be defined in your C-code in order for Python to use it as an extension module. The function must be called init{name} where {name} is the name of the module from Python. This function must be declared so that it is visible to code outside of the routine. Besides adding the methods and constants you desire, this subroutine must also contain calls like import_array() and/or import_ufunc() depending on which C-API is needed. Forgetting to place these commands will show itself as an ugly segmentation fault (crash) as soon as any C-API subroutine is actually called. It is actually possible to have multiple init{name} functions in a single file in which case multiple modules will be defined by that file. However, there are some tricks to get that to work correctly and it is not covered here. A minimal init{name} method looks like: PyMODINIT_FUNC\ninit{name}(void)\n{\n   (void)Py_InitModule({name}, mymethods);\n   import_array();\n}\n The mymethods must be an array (usually statically declared) of PyMethodDef structures which contain method names, actual C-functions, a variable indicating whether the method uses keyword arguments or not, and docstrings. These are explained in the next section. If you want to add constants to the module, then you store the returned value from Py_InitModule which is a module object. The most general way to add items to the module is to get the module dictionary using PyModule_GetDict(module). With the module dictionary, you can add whatever you like to the module manually. An easier way to add objects to the module is to use one of three additional Python C-API calls that do not require a separate extraction of the module dictionary. These are documented in the Python documentation, but repeated here for convenience:   intPyModule_AddObject(PyObject*module, char*name, PyObject*value)\n\n   intPyModule_AddIntConstant(PyObject*module, char*name, longvalue)\n\n   intPyModule_AddStringConstant(PyObject*module, char*name, char*value)\n \nAll three of these functions require the module object (the return value of Py_InitModule). The name is a string that labels the value in the module. Depending on which function is called, the value argument is either a general object (PyModule_AddObject steals a reference to it), an integer constant, or a string constant. \n   Defining functions The second argument passed in to the Py_InitModule function is a structure that makes it easy to to define functions in the module. In the example given above, the mymethods structure would have been defined earlier in the file (usually right before the init{name} subroutine) to: static PyMethodDef mymethods[] = {\n    { nokeywordfunc,nokeyword_cfunc,\n      METH_VARARGS,\n      Doc string},\n    { keywordfunc, keyword_cfunc,\n      METH_VARARGS|METH_KEYWORDS,\n      Doc string},\n    {NULL, NULL, 0, NULL} /* Sentinel */\n}\n Each entry in the mymethods array is a PyMethodDef structure containing 1) the Python name, 2) the C-function that implements the function, 3) flags indicating whether or not keywords are accepted for this function, and 4) The docstring for the function. Any number of functions may be defined for a single module by adding more entries to this table. The last entry must be all NULL as shown to act as a sentinel. Python looks for this entry to know that all of the functions for the module have been defined. The last thing that must be done to finish the extension module is to actually write the code that performs the desired functions. There are two kinds of functions: those that don\u2019t accept keyword arguments, and those that do.  Functions without keyword arguments Functions that don\u2019t accept keyword arguments should be written as: static PyObject*\nnokeyword_cfunc (PyObject *dummy, PyObject *args)\n{\n    /* convert Python arguments */\n    /* do function */\n    /* return something */\n}\n The dummy argument is not used in this context and can be safely ignored. The args argument contains all of the arguments passed in to the function as a tuple. You can do anything you want at this point, but usually the easiest way to manage the input arguments is to call PyArg_ParseTuple (args, format_string, addresses_to_C_variables\u2026) or PyArg_UnpackTuple (tuple, \u201cname\u201d, min, max, \u2026). A good description of how to use the first function is contained in the Python C-API reference manual under section 5.5 (Parsing arguments and building values). You should pay particular attention to the \u201cO&\u201d format which uses converter functions to go between the Python object and the C object. All of the other format functions can be (mostly) thought of as special cases of this general rule. There are several converter functions defined in the NumPy C-API that may be of use. In particular, the PyArray_DescrConverter function is very useful to support arbitrary data-type specification. This function transforms any valid data-type Python object into a PyArray_Descr* object. Remember to pass in the address of the C-variables that should be filled in. There are lots of examples of how to use PyArg_ParseTuple throughout the NumPy source code. The standard usage is like this: PyObject *input;\nPyArray_Descr *dtype;\nif (!PyArg_ParseTuple(args, \"OO&\", &input,\n                      PyArray_DescrConverter,\n                      &dtype)) return NULL;\n It is important to keep in mind that you get a borrowed reference to the object when using the \u201cO\u201d format string. However, the converter functions usually require some form of memory handling. In this example, if the conversion is successful, dtype will hold a new reference to a PyArray_Descr* object, while input will hold a borrowed reference. Therefore, if this conversion were mixed with another conversion (say to an integer) and the data-type conversion was successful but the integer conversion failed, then you would need to release the reference count to the data-type object before returning. A typical way to do this is to set dtype to NULL before calling PyArg_ParseTuple and then use Py_XDECREF on dtype before returning. After the input arguments are processed, the code that actually does the work is written (likely calling other functions as needed). The final step of the C-function is to return something. If an error is encountered then NULL should be returned (making sure an error has actually been set). If nothing should be returned then increment Py_None and return it. If a single object should be returned then it is returned (ensuring that you own a reference to it first). If multiple objects should be returned then you need to return a tuple. The Py_BuildValue (format_string, c_variables\u2026) function makes it easy to build tuples of Python objects from C variables. Pay special attention to the difference between \u2018N\u2019 and \u2018O\u2019 in the format string or you can easily create memory leaks. The \u2018O\u2019 format string increments the reference count of the PyObject* C-variable it corresponds to, while the \u2018N\u2019 format string steals a reference to the corresponding PyObject* C-variable. You should use \u2018N\u2019 if you have already created a reference for the object and just want to give that reference to the tuple. You should use \u2018O\u2019 if you only have a borrowed reference to an object and need to create one to provide for the tuple.   Functions with keyword arguments These functions are very similar to functions without keyword arguments. The only difference is that the function signature is: static PyObject*\nkeyword_cfunc (PyObject *dummy, PyObject *args, PyObject *kwds)\n{\n...\n}\n The kwds argument holds a Python dictionary whose keys are the names of the keyword arguments and whose values are the corresponding keyword-argument values. This dictionary can be processed however you see fit. The easiest way to handle it, however, is to replace the PyArg_ParseTuple (args, format_string, addresses\u2026) function with a call to PyArg_ParseTupleAndKeywords (args, kwds, format_string, char *kwlist[], addresses\u2026). The kwlist parameter to this function is a NULL -terminated array of strings providing the expected keyword arguments. There should be one string for each entry in the format_string. Using this function will raise a TypeError if invalid keyword arguments are passed in. For more help on this function please see section 1.8 (Keyword Parameters for Extension Functions) of the Extending and Embedding tutorial in the Python documentation.   Reference counting The biggest difficulty when writing extension modules is reference counting. It is an important reason for the popularity of f2py, weave, Cython, ctypes, etc\u2026. If you mis-handle reference counts you can get problems from memory-leaks to segmentation faults. The only strategy I know of to handle reference counts correctly is blood, sweat, and tears. First, you force it into your head that every Python variable has a reference count. Then, you understand exactly what each function does to the reference count of your objects, so that you can properly use DECREF and INCREF when you need them. Reference counting can really test the amount of patience and diligence you have towards your programming craft. Despite the grim depiction, most cases of reference counting are quite straightforward with the most common difficulty being not using DECREF on objects before exiting early from a routine due to some error. In second place, is the common error of not owning the reference on an object that is passed to a function or macro that is going to steal the reference ( e.g. PyTuple_SET_ITEM, and most functions that take PyArray_Descr objects). Typically you get a new reference to a variable when it is created or is the return value of some function (there are some prominent exceptions, however \u2014 such as getting an item out of a tuple or a dictionary). When you own the reference, you are responsible to make sure that Py_DECREF (var) is called when the variable is no longer necessary (and no other function has \u201cstolen\u201d its reference). Also, if you are passing a Python object to a function that will \u201csteal\u201d the reference, then you need to make sure you own it (or use Py_INCREF to get your own reference). You will also encounter the notion of borrowing a reference. A function that borrows a reference does not alter the reference count of the object and does not expect to \u201chold on \u201cto the reference. It\u2019s just going to use the object temporarily. When you use PyArg_ParseTuple or PyArg_UnpackTuple you receive a borrowed reference to the objects in the tuple and should not alter their reference count inside your function. With practice, you can learn to get reference counting right, but it can be frustrating at first. One common source of reference-count errors is the Py_BuildValue function. Pay careful attention to the difference between the \u2018N\u2019 format character and the \u2018O\u2019 format character. If you create a new object in your subroutine (such as an output array), and you are passing it back in a tuple of return values, then you should most- likely use the \u2018N\u2019 format character in Py_BuildValue. The \u2018O\u2019 character will increase the reference count by one. This will leave the caller with two reference counts for a brand-new array. When the variable is deleted and the reference count decremented by one, there will still be that extra reference count, and the array will never be deallocated. You will have a reference-counting induced memory leak. Using the \u2018N\u2019 character will avoid this situation as it will return to the caller an object (inside the tuple) with a single reference count.    Dealing with array objects Most extension modules for NumPy will need to access the memory for an ndarray object (or one of it\u2019s sub-classes). The easiest way to do this doesn\u2019t require you to know much about the internals of NumPy. The method is to  \nEnsure you are dealing with a well-behaved array (aligned, in machine byte-order and single-segment) of the correct type and number of dimensions.  By converting it from some Python object using PyArray_FromAny or a macro built on it. By constructing a new ndarray of your desired shape and type using PyArray_NewFromDescr or a simpler macro or function based on it.   Get the shape of the array and a pointer to its actual data. Pass the data and shape information on to a subroutine or other section of code that actually performs the computation. If you are writing the algorithm, then I recommend that you use the stride information contained in the array to access the elements of the array (the PyArray_GetPtr macros make this painless). Then, you can relax your requirements so as not to force a single-segment array and the data-copying that might result.  Each of these sub-topics is covered in the following sub-sections.  Converting an arbitrary sequence object The main routine for obtaining an array from any Python object that can be converted to an array is PyArray_FromAny. This function is very flexible with many input arguments. Several macros make it easier to use the basic function. PyArray_FROM_OTF is arguably the most useful of these macros for the most common uses. It allows you to convert an arbitrary Python object to an array of a specific builtin data-type ( e.g. float), while specifying a particular set of requirements ( e.g. contiguous, aligned, and writeable). The syntax is  PyArray_FROM_OTF\n\nReturn an ndarray from any Python object, obj, that can be converted to an array. The number of dimensions in the returned array is determined by the object. The desired data-type of the returned array is provided in typenum which should be one of the enumerated types. The requirements for the returned array can be any combination of standard array flags. Each of these arguments is explained in more detail below. You receive a new reference to the array on success. On failure, NULL is returned and an exception is set.  obj\n\nThe object can be any Python object convertible to an ndarray. If the object is already (a subclass of) the ndarray that satisfies the requirements then a new reference is returned. Otherwise, a new array is constructed. The contents of obj are copied to the new array unless the array interface is used so that data does not have to be copied. Objects that can be converted to an array include: 1) any nested sequence object, 2) any object exposing the array interface, 3) any object with an __array__ method (which should return an ndarray), and 4) any scalar object (becomes a zero-dimensional array). Sub-classes of the ndarray that otherwise fit the requirements will be passed through. If you want to ensure a base-class ndarray, then use NPY_ARRAY_ENSUREARRAY in the requirements flag. A copy is made only if necessary. If you want to guarantee a copy, then pass in NPY_ARRAY_ENSURECOPY to the requirements flag.  typenum\n\nOne of the enumerated types or NPY_NOTYPE if the data-type should be determined from the object itself. The C-based names can be used: NPY_BOOL, NPY_BYTE, NPY_UBYTE, NPY_SHORT, NPY_USHORT, NPY_INT, NPY_UINT, NPY_LONG, NPY_ULONG, NPY_LONGLONG, NPY_ULONGLONG, NPY_DOUBLE, NPY_LONGDOUBLE, NPY_CFLOAT, NPY_CDOUBLE, NPY_CLONGDOUBLE, NPY_OBJECT. Alternatively, the bit-width names can be used as supported on the platform. For example: NPY_INT8, NPY_INT16, NPY_INT32, NPY_INT64, NPY_UINT8, NPY_UINT16, NPY_UINT32, NPY_UINT64, NPY_FLOAT32, NPY_FLOAT64, NPY_COMPLEX64, NPY_COMPLEX128. The object will be converted to the desired type only if it can be done without losing precision. Otherwise NULL will be returned and an error raised. Use NPY_ARRAY_FORCECAST in the requirements flag to override this behavior.  requirements\n\nThe memory model for an ndarray admits arbitrary strides in each dimension to advance to the next element of the array. Often, however, you need to interface with code that expects a C-contiguous or a Fortran-contiguous memory layout. In addition, an ndarray can be misaligned (the address of an element is not at an integral multiple of the size of the element) which can cause your program to crash (or at least work more slowly) if you try and dereference a pointer into the array data. Both of these problems can be solved by converting the Python object into an array that is more \u201cwell-behaved\u201d for your specific usage. The requirements flag allows specification of what kind of array is acceptable. If the object passed in does not satisfy this requirements then a copy is made so that the returned object will satisfy the requirements. these ndarray can use a very generic pointer to memory. This flag allows specification of the desired properties of the returned array object. All of the flags are explained in the detailed API chapter. The flags most commonly needed are NPY_ARRAY_IN_ARRAY, NPY_OUT_ARRAY, and NPY_ARRAY_INOUT_ARRAY:  NPY_ARRAY_IN_ARRAY\n\nThis flag is useful for arrays that must be in C-contiguous order and aligned. These kinds of arrays are usually input arrays for some algorithm.  NPY_ARRAY_OUT_ARRAY\n\nThis flag is useful to specify an array that is in C-contiguous order, is aligned, and can be written to as well. Such an array is usually returned as output (although normally such output arrays are created from scratch).  NPY_ARRAY_INOUT_ARRAY\n\nThis flag is useful to specify an array that will be used for both input and output. PyArray_ResolveWritebackIfCopy must be called before Py_DECREF at the end of the interface routine to write back the temporary data into the original array passed in. Use of the NPY_ARRAY_WRITEBACKIFCOPY or NPY_ARRAY_UPDATEIFCOPY flags requires that the input object is already an array (because other objects cannot be automatically updated in this fashion). If an error occurs use PyArray_DiscardWritebackIfCopy (obj) on an array with these flags set. This will set the underlying base array writable without causing the contents to be copied back into the original array.   Other useful flags that can be OR\u2019d as additional requirements are:  NPY_ARRAY_FORCECAST\n\nCast to the desired type, even if it can\u2019t be done without losing information.  NPY_ARRAY_ENSURECOPY\n\nMake sure the resulting array is a copy of the original.  NPY_ARRAY_ENSUREARRAY\n\nMake sure the resulting object is an actual ndarray and not a sub- class.        Note Whether or not an array is byte-swapped is determined by the data-type of the array. Native byte-order arrays are always requested by PyArray_FROM_OTF and so there is no need for a NPY_ARRAY_NOTSWAPPED flag in the requirements argument. There is also no way to get a byte-swapped array from this routine.    Creating a brand-new ndarray Quite often, new arrays must be created from within extension-module code. Perhaps an output array is needed and you don\u2019t want the caller to have to supply it. Perhaps only a temporary array is needed to hold an intermediate calculation. Whatever the need there are simple ways to get an ndarray object of whatever data-type is needed. The most general function for doing this is PyArray_NewFromDescr. All array creation functions go through this heavily re-used code. Because of its flexibility, it can be somewhat confusing to use. As a result, simpler forms exist that are easier to use. These forms are part of the PyArray_SimpleNew family of functions, which simplify the interface by providing default values for common use cases.   Getting at ndarray memory and accessing elements of the ndarray If obj is an ndarray (PyArrayObject*), then the data-area of the ndarray is pointed to by the void* pointer PyArray_DATA (obj) or the char* pointer PyArray_BYTES (obj). Remember that (in general) this data-area may not be aligned according to the data-type, it may represent byte-swapped data, and/or it may not be writeable. If the data area is aligned and in native byte-order, then how to get at a specific element of the array is determined only by the array of npy_intp variables, PyArray_STRIDES (obj). In particular, this c-array of integers shows how many bytes must be added to the current element pointer to get to the next element in each dimension. For arrays less than 4-dimensions there are PyArray_GETPTR{k} (obj, \u2026) macros where {k} is the integer 1, 2, 3, or 4 that make using the array strides easier. The arguments \u2026. represent {k} non- negative integer indices into the array. For example, suppose E is a 3-dimensional ndarray. A (void*) pointer to the element E[i,j,k] is obtained as PyArray_GETPTR3 (E, i, j, k). As explained previously, C-style contiguous arrays and Fortran-style contiguous arrays have particular striding patterns. Two array flags (NPY_ARRAY_C_CONTIGUOUS and NPY_ARRAY_F_CONTIGUOUS) indicate whether or not the striding pattern of a particular array matches the C-style contiguous or Fortran-style contiguous or neither. Whether or not the striding pattern matches a standard C or Fortran one can be tested Using PyArray_IS_C_CONTIGUOUS (obj) and PyArray_ISFORTRAN (obj) respectively. Most third-party libraries expect contiguous arrays. But, often it is not difficult to support general-purpose striding. I encourage you to use the striding information in your own code whenever possible, and reserve single-segment requirements for wrapping third-party code. Using the striding information provided with the ndarray rather than requiring a contiguous striding reduces copying that otherwise must be made.    Example The following example shows how you might write a wrapper that accepts two input arguments (that will be converted to an array) and an output argument (that must be an array). The function returns None and updates the output array. Note the updated use of WRITEBACKIFCOPY semantics for NumPy v1.14 and above static PyObject *\nexample_wrapper(PyObject *dummy, PyObject *args)\n{\n    PyObject *arg1=NULL, *arg2=NULL, *out=NULL;\n    PyObject *arr1=NULL, *arr2=NULL, *oarr=NULL;\n\n    if (!PyArg_ParseTuple(args, \"OOO!\", &arg1, &arg2,\n        &PyArray_Type, &out)) return NULL;\n\n    arr1 = PyArray_FROM_OTF(arg1, NPY_DOUBLE, NPY_ARRAY_IN_ARRAY);\n    if (arr1 == NULL) return NULL;\n    arr2 = PyArray_FROM_OTF(arg2, NPY_DOUBLE, NPY_ARRAY_IN_ARRAY);\n    if (arr2 == NULL) goto fail;\n#if NPY_API_VERSION >= 0x0000000c\n    oarr = PyArray_FROM_OTF(out, NPY_DOUBLE, NPY_ARRAY_INOUT_ARRAY2);\n#else\n    oarr = PyArray_FROM_OTF(out, NPY_DOUBLE, NPY_ARRAY_INOUT_ARRAY);\n#endif\n    if (oarr == NULL) goto fail;\n\n    /* code that makes use of arguments */\n    /* You will probably need at least\n       nd = PyArray_NDIM(<..>)    -- number of dimensions\n       dims = PyArray_DIMS(<..>)  -- npy_intp array of length nd\n                                     showing length in each dim.\n       dptr = (double *)PyArray_DATA(<..>) -- pointer to data.\n\n       If an error occurs goto fail.\n     */\n\n    Py_DECREF(arr1);\n    Py_DECREF(arr2);\n#if NPY_API_VERSION >= 0x0000000c\n    PyArray_ResolveWritebackIfCopy(oarr);\n#endif\n    Py_DECREF(oarr);\n    Py_INCREF(Py_None);\n    return Py_None;\n\n fail:\n    Py_XDECREF(arr1);\n    Py_XDECREF(arr2);\n#if NPY_API_VERSION >= 0x0000000c\n    PyArray_DiscardWritebackIfCopy(oarr);\n#endif\n    Py_XDECREF(oarr);\n    return NULL;\n}\n \n"}, {"name": "int PyModule_AddStringConstant()", "path": "user/c-info.how-to-extend#c.PyModule_AddStringConstant", "type": "User Guide", "text": "  intPyModule_AddStringConstant(PyObject*module, char*name, char*value)\n \nAll three of these functions require the module object (the return value of Py_InitModule). The name is a string that labels the value in the module. Depending on which function is called, the value argument is either a general object (PyModule_AddObject steals a reference to it), an integer constant, or a string constant. \n"}, {"name": "int PyTypeNum_ISBOOL()", "path": "reference/c-api/array#c.PyTypeNum_ISBOOL", "type": "Array API", "text": "  intPyTypeNum_ISBOOL(intnum)\n\n"}, {"name": "int PyTypeNum_ISCOMPLEX()", "path": "reference/c-api/array#c.PyTypeNum_ISCOMPLEX", "type": "Array API", "text": "  intPyTypeNum_ISCOMPLEX(intnum)\n\n"}, {"name": "int PyTypeNum_ISEXTENDED()", "path": "reference/c-api/array#c.PyTypeNum_ISEXTENDED", "type": "Array API", "text": "  intPyTypeNum_ISEXTENDED(intnum)\n\n"}, {"name": "int PyTypeNum_ISFLEXIBLE()", "path": "reference/c-api/array#c.PyTypeNum_ISFLEXIBLE", "type": "Array API", "text": "  intPyTypeNum_ISFLEXIBLE(intnum)\n\n"}, {"name": "int PyTypeNum_ISFLOAT()", "path": "reference/c-api/array#c.PyTypeNum_ISFLOAT", "type": "Array API", "text": "  intPyTypeNum_ISFLOAT(intnum)\n\n"}, {"name": "int PyTypeNum_ISINTEGER()", "path": "reference/c-api/array#c.PyTypeNum_ISINTEGER", "type": "Array API", "text": "  intPyTypeNum_ISINTEGER(intnum)\n\n"}, {"name": "int PyTypeNum_ISNUMBER()", "path": "reference/c-api/array#c.PyTypeNum_ISNUMBER", "type": "Array API", "text": "  intPyTypeNum_ISNUMBER(intnum)\n\n"}, {"name": "int PyTypeNum_ISOBJECT()", "path": "reference/c-api/array#c.PyTypeNum_ISOBJECT", "type": "Array API", "text": "  intPyTypeNum_ISOBJECT(intnum)\n\n"}, {"name": "int PyTypeNum_ISPYTHON()", "path": "reference/c-api/array#c.PyTypeNum_ISPYTHON", "type": "Array API", "text": "  intPyTypeNum_ISPYTHON(intnum)\n\n"}, {"name": "int PyTypeNum_ISSIGNED()", "path": "reference/c-api/array#c.PyTypeNum_ISSIGNED", "type": "Array API", "text": "  intPyTypeNum_ISSIGNED(intnum)\n\n"}, {"name": "int PyTypeNum_ISSTRING()", "path": "reference/c-api/array#c.PyTypeNum_ISSTRING", "type": "Array API", "text": "  intPyTypeNum_ISSTRING(intnum)\n\n"}, {"name": "int PyTypeNum_ISUSERDEF()", "path": "reference/c-api/array#c.PyTypeNum_ISUSERDEF", "type": "Array API", "text": "  intPyTypeNum_ISUSERDEF(intnum)\n\n"}, {"name": "int PyUFunc_checkfperr()", "path": "reference/c-api/ufunc#c.PyUFunc_checkfperr", "type": "UFunc API", "text": "  intPyUFunc_checkfperr(interrmask, PyObject*errobj)\n \nA simple interface to the IEEE error-flag checking support. The errmask argument is a mask of UFUNC_MASK_{ERR} bitmasks indicating which errors to check for (and how to check for them). The errobj must be a Python tuple with two elements: a string containing the name which will be used in any communication of error and either a callable Python object (call-back function) or Py_None. The callable object will only be used if UFUNC_ERR_CALL is set as the desired error checking method. This routine manages the GIL and is safe to call even after releasing the GIL. If an error in the IEEE-compatible hardware is determined a -1 is returned, otherwise a 0 is returned. \n"}, {"name": "int PyUFunc_RegisterLoopForDescr()", "path": "reference/c-api/ufunc#c.PyUFunc_RegisterLoopForDescr", "type": "UFunc API", "text": "  intPyUFunc_RegisterLoopForDescr(PyUFuncObject*ufunc, PyArray_Descr*userdtype, PyUFuncGenericFunctionfunction, PyArray_Descr**arg_dtypes, void*data)\n \nThis function behaves like PyUFunc_RegisterLoopForType above, except that it allows the user to register a 1-d loop using PyArray_Descr objects instead of dtype type num values. This allows a 1-d loop to be registered for structured array data-dtypes and custom data-types instead of scalar data-types. \n"}, {"name": "int PyUFunc_RegisterLoopForType()", "path": "reference/c-api/ufunc#c.PyUFunc_RegisterLoopForType", "type": "UFunc API", "text": "  intPyUFunc_RegisterLoopForType(PyUFuncObject*ufunc, intusertype, PyUFuncGenericFunctionfunction, int*arg_types, void*data)\n \nThis function allows the user to register a 1-d loop with an already- created ufunc to be used whenever the ufunc is called with any of its input arguments as the user-defined data-type. This is needed in order to make ufuncs work with built-in data-types. The data-type must have been previously registered with the numpy system. The loop is passed in as function. This loop can take arbitrary data which should be passed in as data. The data-types the loop requires are passed in as arg_types which must be a pointer to memory at least as large as ufunc->nargs. \n"}, {"name": "int PyUFunc_ReplaceLoopBySignature()", "path": "reference/c-api/ufunc#c.PyUFunc_ReplaceLoopBySignature", "type": "UFunc API", "text": "  intPyUFunc_ReplaceLoopBySignature(PyUFuncObject*ufunc, PyUFuncGenericFunctionnewfunc, int*signature, PyUFuncGenericFunction*oldfunc)\n \nReplace a 1-d loop matching the given signature in the already-created ufunc with the new 1-d loop newfunc. Return the old 1-d loop function in oldfunc. Return 0 on success and -1 on failure. This function works only with built-in types (use PyUFunc_RegisterLoopForType for user-defined types). A signature is an array of data-type numbers indicating the inputs followed by the outputs assumed by the 1-d loop. \n"}, {"name": "int random_multivariate_hypergeometric_count()", "path": "reference/random/c-api#c.random_multivariate_hypergeometric_count", "type": "C API for random", "text": "  intrandom_multivariate_hypergeometric_count(bitgen_t*bitgen_state, npy_int64total, size_tnum_colors, npy_int64*colors, npy_int64nsample, size_tnum_variates, npy_int64*variates)\n\n"}, {"name": "int reserved1", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.reserved1", "type": "Python Types and C-Structures", "text": "  intreserved1\n \nUnused. \n"}, {"name": "int scanfunc()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.scanfunc", "type": "Python Types and C-Structures", "text": "  intscanfunc(FILE*fd, void*ip, void*arr)\n \nA pointer to a function that scans (scanf style) one element of the corresponding type from the file descriptor fd into the array memory pointed to by ip. The array is assumed to be behaved. The last argument arr is the array to be scanned into. Returns number of receiving arguments successfully assigned (which may be zero in case a matching failure occurred before the first receiving argument was assigned), or EOF if input failure occurs before the first receiving argument was assigned. This function should be called without holding the Python GIL, and has to grab it for error reporting. \n"}, {"name": "int setitem()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.setitem", "type": "Python Types and C-Structures", "text": "  intsetitem(PyObject*item, void*data, void*arr)\n \nA pointer to a function that sets the Python object item into the array, arr, at the position pointed to by data . This function deals with \u201cmisbehaved\u201d arrays. If successful, a zero is returned, otherwise, a negative one is returned (and a Python error set). \n"}, {"name": "int sort()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.sort", "type": "Python Types and C-Structures", "text": "  intsort(void*start, npy_intplength, void*arr)\n \nAn array of function pointers to a particular sorting algorithms. A particular sorting algorithm is obtained using a key (so far NPY_QUICKSORT, NPY_HEAPSORT, and NPY_MERGESORT are defined). These sorts are done in-place assuming contiguous and aligned data. \n"}, {"name": "Internal organization of NumPy arrays", "path": "dev/internals", "type": "Development", "text": "Internal organization of NumPy arrays It helps to understand a bit about how NumPy arrays are handled under the covers to help understand NumPy better. This section will not go into great detail. Those wishing to understand the full details are requested to refer to Travis Oliphant\u2019s book Guide to NumPy. NumPy arrays consist of two major components: the raw array data (from now on, referred to as the data buffer), and the information about the raw array data. The data buffer is typically what people think of as arrays in C or Fortran, a contiguous (and fixed) block of memory containing fixed-sized data items. NumPy also contains a significant set of data that describes how to interpret the data in the data buffer. This extra information contains (among other things):  The basic data element\u2019s size in bytes. The start of the data within the data buffer (an offset relative to the beginning of the data buffer). The number of dimensions and the size of each dimension. The separation between elements for each dimension (the stride). This does not have to be a multiple of the element size. The byte order of the data (which may not be the native byte order). Whether the buffer is read-only. Information (via the dtype object) about the interpretation of the basic data element. The basic data element may be as simple as an int or a float, or it may be a compound object (e.g., struct-like), a fixed character field, or Python object pointers. Whether the array is to be interpreted as C-order or Fortran-order.  This arrangement allows for the very flexible use of arrays. One thing that it allows is simple changes to the metadata to change the interpretation of the array buffer. Changing the byteorder of the array is a simple change involving no rearrangement of the data. The shape of the array can be changed very easily without changing anything in the data buffer or any data copying at all. Among other things that are made possible is one can create a new array metadata object that uses the same data buffer to create a new view of that data buffer that has a different interpretation of the buffer (e.g., different shape, offset, byte order, strides, etc) but shares the same data bytes. Many operations in NumPy do just this such as slicing. Other operations, such as transpose, don\u2019t move data elements around in the array, but rather change the information about the shape and strides so that the indexing of the array changes, but the data in the doesn\u2019t move. Typically these new versions of the array metadata but the same data buffer are new views into the data buffer. There is a different ndarray object, but it uses the same data buffer. This is why it is necessary to force copies through the use of the copy method if one really wants to make a new and independent copy of the data buffer. New views into arrays mean the object reference counts for the data buffer increase. Simply doing away with the original array object will not remove the data buffer if other views of it still exist.  Multidimensional array indexing order issues  See also Indexing on ndarrays  What is the right way to index multi-dimensional arrays? Before you jump to conclusions about the one and true way to index multi-dimensional arrays, it pays to understand why this is a confusing issue. This section will try to explain in detail how NumPy indexing works and why we adopt the convention we do for images, and when it may be appropriate to adopt other conventions. The first thing to understand is that there are two conflicting conventions for indexing 2-dimensional arrays. Matrix notation uses the first index to indicate which row is being selected and the second index to indicate which column is selected. This is opposite the geometrically oriented-convention for images where people generally think the first index represents x position (i.e., column) and the second represents y position (i.e., row). This alone is the source of much confusion; matrix-oriented users and image-oriented users expect two different things with regard to indexing. The second issue to understand is how indices correspond to the order in which the array is stored in memory. In Fortran, the first index is the most rapidly varying index when moving through the elements of a two-dimensional array as it is stored in memory. If you adopt the matrix convention for indexing, then this means the matrix is stored one column at a time (since the first index moves to the next row as it changes). Thus Fortran is considered a Column-major language. C has just the opposite convention. In C, the last index changes most rapidly as one moves through the array as stored in memory. Thus C is a Row-major language. The matrix is stored by rows. Note that in both cases it presumes that the matrix convention for indexing is being used, i.e., for both Fortran and C, the first index is the row. Note this convention implies that the indexing convention is invariant and that the data order changes to keep that so. But that\u2019s not the only way to look at it. Suppose one has large two-dimensional arrays (images or matrices) stored in data files. Suppose the data are stored by rows rather than by columns. If we are to preserve our index convention (whether matrix or image) that means that depending on the language we use, we may be forced to reorder the data if it is read into memory to preserve our indexing convention. For example, if we read row-ordered data into memory without reordering, it will match the matrix indexing convention for C, but not for Fortran. Conversely, it will match the image indexing convention for Fortran, but not for C. For C, if one is using data stored in row order, and one wants to preserve the image index convention, the data must be reordered when reading into memory. In the end, what you do for Fortran or C depends on which is more important, not reordering data or preserving the indexing convention. For large images, reordering data is potentially expensive, and often the indexing convention is inverted to avoid that. The situation with NumPy makes this issue yet more complicated. The internal machinery of NumPy arrays is flexible enough to accept any ordering of indices. One can simply reorder indices by manipulating the internal stride information for arrays without reordering the data at all. NumPy will know how to map the new index order to the data without moving the data. So if this is true, why not choose the index order that matches what you most expect? In particular, why not define row-ordered images to use the image convention? (This is sometimes referred to as the Fortran convention vs the C convention, thus the \u2018C\u2019 and \u2018FORTRAN\u2019 order options for array ordering in NumPy.) The drawback of doing this is potential performance penalties. It\u2019s common to access the data sequentially, either implicitly in array operations or explicitly by looping over rows of an image. When that is done, then the data will be accessed in non-optimal order. As the first index is incremented, what is actually happening is that elements spaced far apart in memory are being sequentially accessed, with usually poor memory access speeds. For example, for a two-dimensional image im defined so that im[0, 10] represents the value at x = 0, y = 10. To be consistent with usual Python behavior then im[0] would represent a column at x = 0. Yet that data would be spread over the whole array since the data are stored in row order. Despite the flexibility of NumPy\u2019s indexing, it can\u2019t really paper over the fact basic operations are rendered inefficient because of data order or that getting contiguous subarrays is still awkward (e.g., im[:, 0] for the first row, vs im[0]). Thus one can\u2019t use an idiom such as for row in im; for col in im does work, but doesn\u2019t yield contiguous column data. As it turns out, NumPy is smart enough when dealing with ufuncs to determine which index is the most rapidly varying one in memory and uses that for the innermost loop. Thus for ufuncs, there is no large intrinsic advantage to either approach in most cases. On the other hand, use of ndarray.flat with a FORTRAN ordered array will lead to non-optimal memory access as adjacent elements in the flattened array (iterator, actually) are not contiguous in memory. Indeed, the fact is that Python indexing on lists and other sequences naturally leads to an outside-to-inside ordering (the first index gets the largest grouping, the next largest, and the last gets the smallest element). Since image data are normally stored in rows, this corresponds to the position within rows being the last item indexed. If you do want to use Fortran ordering realize that there are two approaches to consider: 1) accept that the first index is just not the most rapidly changing in memory and have all your I/O routines reorder your data when going from memory to disk or visa versa, or use NumPy\u2019s mechanism for mapping the first index to the most rapidly varying data. We recommend the former if possible. The disadvantage of the latter is that many of NumPy\u2019s functions will yield arrays without Fortran ordering unless you are careful to use the order keyword. Doing this would be highly inconvenient. Otherwise, we recommend simply learning to reverse the usual order of indices when accessing elements of an array. Granted, it goes against the grain, but it is more in line with Python semantics and the natural order of the data. \n"}, {"name": "Is the intended behavior clear under all conditions? Some things to watch:", "path": "dev/reviewer_guidelines", "type": "Development", "text": "Reviewer Guidelines Reviewing open pull requests (PRs) helps move the project forward. We encourage people outside the project to get involved as well; it\u2019s a great way to get familiar with the codebase.  Who can be a reviewer? Reviews can come from outside the NumPy team \u2013 we welcome contributions from domain experts (for instance, linalg or fft) or maintainers of other projects. You do not need to be a NumPy maintainer (a NumPy team member with permission to merge a PR) to review. If we do not know you yet, consider introducing yourself in the mailing list or Slack before you start reviewing pull requests.   Communication Guidelines  Every PR, good or bad, is an act of generosity. Opening with a positive comment will help the author feel rewarded, and your subsequent remarks may be heard more clearly. You may feel good also. Begin if possible with the large issues, so the author knows they\u2019ve been understood. Resist the temptation to immediately go line by line, or to open with small pervasive issues. You are the face of the project, and NumPy some time ago decided the kind of project it will be: open, empathetic, welcoming, friendly and patient. Be kind to contributors. Do not let perfect be the enemy of the good, particularly for documentation. If you find yourself making many small suggestions, or being too nitpicky on style or grammar, consider merging the current PR when all important concerns are addressed. Then, either push a commit directly (if you are a maintainer) or open a follow-up PR yourself. If you need help writing replies in reviews, check out some standard replies for reviewing.    Reviewer Checklist  \n Is the intended behavior clear under all conditions? Some things to watch:\n\n What happens with unexpected inputs like empty arrays or nan/inf values? Are axis or shape arguments tested to be int or tuples? Are unusual dtypes tested if a function supports those?     Should variable names be improved for clarity or consistency? Should comments be added, or rather removed as unhelpful or extraneous? Does the documentation follow the NumPy guidelines? Are the docstrings properly formatted? Does the code follow NumPy\u2019s Stylistic Guidelines? If you are a maintainer, and it is not obvious from the PR description, add a short explanation of what a branch did to the merge message and, if closing an issue, also add \u201cCloses gh-123\u201d where 123 is the issue number. For code changes, at least one maintainer (i.e. someone with commit rights) should review and approve a pull request. If you are the first to review a PR and approve of the changes use the GitHub approve review tool to mark it as such. If a PR is straightforward, for example it\u2019s a clearly correct bug fix, it can be merged straight away. If it\u2019s more complex or changes public API, please leave it open for at least a couple of days so other maintainers get a chance to review. If you are a subsequent reviewer on an already approved PR, please use the same review method as for a new PR (focus on the larger issues, resist the temptation to add only a few nitpicks). If you have commit rights and think no more review is needed, merge the PR.   For maintainers  Make sure all automated CI tests pass before merging a PR, and that the documentation builds without any errors. In case of merge conflicts, ask the PR submitter to rebase on main. For PRs that add new features or are in some way complex, wait at least a day or two before merging it. That way, others get a chance to comment before the code goes in. Consider adding it to the release notes. When merging contributions, a committer is responsible for ensuring that those meet the requirements outlined in the Development process guidelines for NumPy. Also, check that new features and backwards compatibility breaks were discussed on the numpy-discussion mailing list. Squashing commits or cleaning up commit messages of a PR that you consider too messy is OK. Remember to retain the original author\u2019s name when doing this. Make sure commit messages follow the rules for NumPy. When you want to reject a PR: if it\u2019s very obvious, you can just close it and explain why. If it\u2019s not, then it\u2019s a good idea to first explain why you think the PR is not suitable for inclusion in NumPy and then let a second committer comment or close.    GitHub Workflow When reviewing pull requests, please use workflow tracking features on GitHub as appropriate:  After you have finished reviewing, if you want to ask for the submitter to make changes, change your review status to \u201cChanges requested.\u201d This can be done on GitHub, PR page, Files changed tab, Review changes (button on the top right). If you\u2019re happy about the current status, mark the pull request as Approved (same way as Changes requested). Alternatively (for maintainers): merge the pull request, if you think it is ready to be merged.  It may be helpful to have a copy of the pull request code checked out on your own machine so that you can play with it locally. You can use the GitHub CLI to do this by clicking the Open with button in the upper right-hand corner of the PR page. Assuming you have your development environment set up, you can now build the code and test it.    Standard replies for reviewing It may be helpful to store some of these in GitHub\u2019s saved replies for reviewing:  Usage question\n\nYou are asking a usage question. The issue tracker is for bugs and new features.\nI'm going to close this issue, feel free to ask for help via our [help channels](https://numpy.org/gethelp/).\n  You\u2019re welcome to update the docs\n\nPlease feel free to offer a pull request updating the documentation if you feel it could be improved.\n  Self-contained example for bug\n\nPlease provide a [self-contained example code](https://stackoverflow.com/help/mcve), including imports and data (if possible), so that other contributors can just run it and reproduce your issue.\nIdeally your example code should be minimal.\n  Software versions\n\nTo help diagnose your issue, please paste the output of:\n```\npython -c 'import numpy; print(numpy.version.version)'\n```\nThanks.\n  Code blocks\n\nReadability can be greatly improved if you [format](https://help.github.com/articles/creating-and-highlighting-code-blocks/) your code snippets and complete error messages appropriately.\nYou can edit your issue descriptions and comments at any time to improve readability.\nThis helps maintainers a lot. Thanks!\n  Linking to code\n\nFor clarity's sake, you can link to code like [this](https://help.github.com/articles/creating-a-permanent-link-to-a-code-snippet/).\n  Better description and title\n\nPlease make the title of the PR more descriptive.\nThe title will become the commit message when this is merged.\nYou should state what issue (or PR) it fixes/resolves in the description using the syntax described [here](https://docs.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).\n  Regression test needed\n\nPlease add a [non-regression test](https://en.wikipedia.org/wiki/Non-regression_testing) that would fail at main but pass in this PR.\n  Don\u2019t change unrelated\n\nPlease do not change unrelated lines. It makes your contribution harder to review and may introduce merge conflicts to other pull requests.\n   \n"}, {"name": "is_array()", "path": "reference/swig.interface-file", "type": "numpy.i: a SWIG Interface File for NumPy", "text": "numpy.i: a SWIG Interface File for NumPy  Introduction The Simple Wrapper and Interface Generator (or SWIG) is a powerful tool for generating wrapper code for interfacing to a wide variety of scripting languages. SWIG can parse header files, and using only the code prototypes, create an interface to the target language. But SWIG is not omnipotent. For example, it cannot know from the prototype: double rms(double* seq, int n);\n what exactly seq is. Is it a single value to be altered in-place? Is it an array, and if so what is its length? Is it input-only? Output-only? Input-output? SWIG cannot determine these details, and does not attempt to do so. If we designed rms, we probably made it a routine that takes an input-only array of length n of double values called seq and returns the root mean square. The default behavior of SWIG, however, will be to create a wrapper function that compiles, but is nearly impossible to use from the scripting language in the way the C routine was intended. For Python, the preferred way of handling contiguous (or technically, strided) blocks of homogeneous data is with NumPy, which provides full object-oriented access to multidimensial arrays of data. Therefore, the most logical Python interface for the rms function would be (including doc string): def rms(seq):\n    \"\"\"\n    rms: return the root mean square of a sequence\n    rms(numpy.ndarray) -> double\n    rms(list) -> double\n    rms(tuple) -> double\n    \"\"\"\n where seq would be a NumPy array of double values, and its length n would be extracted from seq internally before being passed to the C routine. Even better, since NumPy supports construction of arrays from arbitrary Python sequences, seq itself could be a nearly arbitrary sequence (so long as each element can be converted to a double) and the wrapper code would internally convert it to a NumPy array before extracting its data and length. SWIG allows these types of conversions to be defined via a mechanism called typemaps. This document provides information on how to use numpy.i, a SWIG interface file that defines a series of typemaps intended to make the type of array-related conversions described above relatively simple to implement. For example, suppose that the rms function prototype defined above was in a header file named rms.h. To obtain the Python interface discussed above, your SWIG interface file would need the following: %{\n#define SWIG_FILE_WITH_INIT\n#include \"rms.h\"\n%}\n\n%include \"numpy.i\"\n\n%init %{\nimport_array();\n%}\n\n%apply (double* IN_ARRAY1, int DIM1) {(double* seq, int n)};\n%include \"rms.h\"\n Typemaps are keyed off a list of one or more function arguments, either by type or by type and name. We will refer to such lists as signatures. One of the many typemaps defined by numpy.i is used above and has the signature (double* IN_ARRAY1, int DIM1). The argument names are intended to suggest that the double* argument is an input array of one dimension and that the int represents the size of that dimension. This is precisely the pattern in the rms prototype. Most likely, no actual prototypes to be wrapped will have the argument names IN_ARRAY1 and DIM1. We use the SWIG %apply directive to apply the typemap for one-dimensional input arrays of type double to the actual prototype used by rms. Using numpy.i effectively, therefore, requires knowing what typemaps are available and what they do. A SWIG interface file that includes the SWIG directives given above will produce wrapper code that looks something like:  1 PyObject *_wrap_rms(PyObject *args) {\n 2   PyObject *resultobj = 0;\n 3   double *arg1 = (double *) 0 ;\n 4   int arg2 ;\n 5   double result;\n 6   PyArrayObject *array1 = NULL ;\n 7   int is_new_object1 = 0 ;\n 8   PyObject * obj0 = 0 ;\n 9\n10   if (!PyArg_ParseTuple(args,(char *)\"O:rms\",&obj0)) SWIG_fail;\n11   {\n12     array1 = obj_to_array_contiguous_allow_conversion(\n13                  obj0, NPY_DOUBLE, &is_new_object1);\n14     npy_intp size[1] = {\n15       -1\n16     };\n17     if (!array1 || !require_dimensions(array1, 1) ||\n18         !require_size(array1, size, 1)) SWIG_fail;\n19     arg1 = (double*) array1->data;\n20     arg2 = (int) array1->dimensions[0];\n21   }\n22   result = (double)rms(arg1,arg2);\n23   resultobj = SWIG_From_double((double)(result));\n24   {\n25     if (is_new_object1 && array1) Py_DECREF(array1);\n26   }\n27   return resultobj;\n28 fail:\n29   {\n30     if (is_new_object1 && array1) Py_DECREF(array1);\n31   }\n32   return NULL;\n33 }\n The typemaps from numpy.i are responsible for the following lines of code: 12\u201320, 25 and 30. Line 10 parses the input to the rms function. From the format string \"O:rms\", we can see that the argument list is expected to be a single Python object (specified by the O before the colon) and whose pointer is stored in obj0. A number of functions, supplied by numpy.i, are called to make and check the (possible) conversion from a generic Python object to a NumPy array. These functions are explained in the section Helper Functions, but hopefully their names are self-explanatory. At line 12 we use obj0 to construct a NumPy array. At line 17, we check the validity of the result: that it is non-null and that it has a single dimension of arbitrary length. Once these states are verified, we extract the data buffer and length in lines 19 and 20 so that we can call the underlying C function at line 22. Line 25 performs memory management for the case where we have created a new array that is no longer needed. This code has a significant amount of error handling. Note the SWIG_fail is a macro for goto fail, referring to the label at line 28. If the user provides the wrong number of arguments, this will be caught at line 10. If construction of the NumPy array fails or produces an array with the wrong number of dimensions, these errors are caught at line 17. And finally, if an error is detected, memory is still managed correctly at line 30. Note that if the C function signature was in a different order: double rms(int n, double* seq);\n that SWIG would not match the typemap signature given above with the argument list for rms. Fortunately, numpy.i has a set of typemaps with the data pointer given last: %apply (int DIM1, double* IN_ARRAY1) {(int n, double* seq)};\n This simply has the effect of switching the definitions of arg1 and arg2 in lines 3 and 4 of the generated code above, and their assignments in lines 19 and 20.   Using numpy.i The numpy.i file is currently located in the tools/swig sub-directory under the numpy installation directory. Typically, you will want to copy it to the directory where you are developing your wrappers. A simple module that only uses a single SWIG interface file should include the following: %{\n#define SWIG_FILE_WITH_INIT\n%}\n%include \"numpy.i\"\n%init %{\nimport_array();\n%}\n Within a compiled Python module, import_array() should only get called once. This could be in a C/C++ file that you have written and is linked to the module. If this is the case, then none of your interface files should #define SWIG_FILE_WITH_INIT or call import_array(). Or, this initialization call could be in a wrapper file generated by SWIG from an interface file that has the %init block as above. If this is the case, and you have more than one SWIG interface file, then only one interface file should #define SWIG_FILE_WITH_INIT and call import_array().   Available Typemaps The typemap directives provided by numpy.i for arrays of different data types, say double and int, and dimensions of different types, say int or long, are identical to one another except for the C and NumPy type specifications. The typemaps are therefore implemented (typically behind the scenes) via a macro: %numpy_typemaps(DATA_TYPE, DATA_TYPECODE, DIM_TYPE)\n that can be invoked for appropriate (DATA_TYPE, DATA_TYPECODE,\nDIM_TYPE) triplets. For example: %numpy_typemaps(double, NPY_DOUBLE, int)\n%numpy_typemaps(int,    NPY_INT   , int)\n The numpy.i interface file uses the %numpy_typemaps macro to implement typemaps for the following C data types and int dimension types:  signed char unsigned char short unsigned short int unsigned int long unsigned long long long unsigned long long float double  In the following descriptions, we reference a generic DATA_TYPE, which could be any of the C data types listed above, and DIM_TYPE which should be one of the many types of integers. The typemap signatures are largely differentiated on the name given to the buffer pointer. Names with FARRAY are for Fortran-ordered arrays, and names with ARRAY are for C-ordered (or 1D arrays).  Input Arrays Input arrays are defined as arrays of data that are passed into a routine but are not altered in-place or returned to the user. The Python input array is therefore allowed to be almost any Python sequence (such as a list) that can be converted to the requested type of array. The input array signatures are 1D:  ( DATA_TYPE IN_ARRAY1[ANY] ) ( DATA_TYPE* IN_ARRAY1, int DIM1 ) ( int DIM1, DATA_TYPE* IN_ARRAY1 )  2D:  ( DATA_TYPE IN_ARRAY2[ANY][ANY] ) ( DATA_TYPE* IN_ARRAY2, int DIM1, int DIM2 ) ( int DIM1, int DIM2, DATA_TYPE* IN_ARRAY2 ) ( DATA_TYPE* IN_FARRAY2, int DIM1, int DIM2 ) ( int DIM1, int DIM2, DATA_TYPE* IN_FARRAY2 )  3D:  ( DATA_TYPE IN_ARRAY3[ANY][ANY][ANY] ) ( DATA_TYPE* IN_ARRAY3, int DIM1, int DIM2, int DIM3 ) ( int DIM1, int DIM2, int DIM3, DATA_TYPE* IN_ARRAY3 ) ( DATA_TYPE* IN_FARRAY3, int DIM1, int DIM2, int DIM3 ) ( int DIM1, int DIM2, int DIM3, DATA_TYPE* IN_FARRAY3 )  4D:  (DATA_TYPE IN_ARRAY4[ANY][ANY][ANY][ANY]) (DATA_TYPE* IN_ARRAY4, DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3, DIM_TYPE DIM4) (DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3, , DIM_TYPE DIM4, DATA_TYPE* IN_ARRAY4) (DATA_TYPE* IN_FARRAY4, DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3, DIM_TYPE DIM4) (DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3, DIM_TYPE DIM4, DATA_TYPE* IN_FARRAY4)  The first signature listed, ( DATA_TYPE IN_ARRAY[ANY] ) is for one-dimensional arrays with hard-coded dimensions. Likewise, ( DATA_TYPE IN_ARRAY2[ANY][ANY] ) is for two-dimensional arrays with hard-coded dimensions, and similarly for three-dimensional.   In-Place Arrays In-place arrays are defined as arrays that are modified in-place. The input values may or may not be used, but the values at the time the function returns are significant. The provided Python argument must therefore be a NumPy array of the required type. The in-place signatures are 1D:  ( DATA_TYPE INPLACE_ARRAY1[ANY] ) ( DATA_TYPE* INPLACE_ARRAY1, int DIM1 ) ( int DIM1, DATA_TYPE* INPLACE_ARRAY1 )  2D:  ( DATA_TYPE INPLACE_ARRAY2[ANY][ANY] ) ( DATA_TYPE* INPLACE_ARRAY2, int DIM1, int DIM2 ) ( int DIM1, int DIM2, DATA_TYPE* INPLACE_ARRAY2 ) ( DATA_TYPE* INPLACE_FARRAY2, int DIM1, int DIM2 ) ( int DIM1, int DIM2, DATA_TYPE* INPLACE_FARRAY2 )  3D:  ( DATA_TYPE INPLACE_ARRAY3[ANY][ANY][ANY] ) ( DATA_TYPE* INPLACE_ARRAY3, int DIM1, int DIM2, int DIM3 ) ( int DIM1, int DIM2, int DIM3, DATA_TYPE* INPLACE_ARRAY3 ) ( DATA_TYPE* INPLACE_FARRAY3, int DIM1, int DIM2, int DIM3 ) ( int DIM1, int DIM2, int DIM3, DATA_TYPE* INPLACE_FARRAY3 )  4D:  (DATA_TYPE INPLACE_ARRAY4[ANY][ANY][ANY][ANY]) (DATA_TYPE* INPLACE_ARRAY4, DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3, DIM_TYPE DIM4) (DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3, , DIM_TYPE DIM4, DATA_TYPE* INPLACE_ARRAY4) (DATA_TYPE* INPLACE_FARRAY4, DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3, DIM_TYPE DIM4) (DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3, DIM_TYPE DIM4, DATA_TYPE* INPLACE_FARRAY4)  These typemaps now check to make sure that the INPLACE_ARRAY arguments use native byte ordering. If not, an exception is raised. There is also a \u201cflat\u201d in-place array for situations in which you would like to modify or process each element, regardless of the number of dimensions. One example is a \u201cquantization\u201d function that quantizes each element of an array in-place, be it 1D, 2D or whatever. This form checks for continuity but allows either C or Fortran ordering. ND:  (DATA_TYPE* INPLACE_ARRAY_FLAT, DIM_TYPE DIM_FLAT)    Argout Arrays Argout arrays are arrays that appear in the input arguments in C, but are in fact output arrays. This pattern occurs often when there is more than one output variable and the single return argument is therefore not sufficient. In Python, the conventional way to return multiple arguments is to pack them into a sequence (tuple, list, etc.) and return the sequence. This is what the argout typemaps do. If a wrapped function that uses these argout typemaps has more than one return argument, they are packed into a tuple or list, depending on the version of Python. The Python user does not pass these arrays in, they simply get returned. For the case where a dimension is specified, the python user must provide that dimension as an argument. The argout signatures are 1D:  ( DATA_TYPE ARGOUT_ARRAY1[ANY] ) ( DATA_TYPE* ARGOUT_ARRAY1, int DIM1 ) ( int DIM1, DATA_TYPE* ARGOUT_ARRAY1 )  2D:  ( DATA_TYPE ARGOUT_ARRAY2[ANY][ANY] )  3D:  ( DATA_TYPE ARGOUT_ARRAY3[ANY][ANY][ANY] )  4D:  ( DATA_TYPE ARGOUT_ARRAY4[ANY][ANY][ANY][ANY] )  These are typically used in situations where in C/C++, you would allocate a(n) array(s) on the heap, and call the function to fill the array(s) values. In Python, the arrays are allocated for you and returned as new array objects. Note that we support DATA_TYPE* argout typemaps in 1D, but not 2D or 3D. This is because of a quirk with the SWIG typemap syntax and cannot be avoided. Note that for these types of 1D typemaps, the Python function will take a single argument representing DIM1.   Argout View Arrays Argoutview arrays are for when your C code provides you with a view of its internal data and does not require any memory to be allocated by the user. This can be dangerous. There is almost no way to guarantee that the internal data from the C code will remain in existence for the entire lifetime of the NumPy array that encapsulates it. If the user destroys the object that provides the view of the data before destroying the NumPy array, then using that array may result in bad memory references or segmentation faults. Nevertheless, there are situations, working with large data sets, where you simply have no other choice. The C code to be wrapped for argoutview arrays are characterized by pointers: pointers to the dimensions and double pointers to the data, so that these values can be passed back to the user. The argoutview typemap signatures are therefore 1D:  ( DATA_TYPE** ARGOUTVIEW_ARRAY1, DIM_TYPE* DIM1 ) ( DIM_TYPE* DIM1, DATA_TYPE** ARGOUTVIEW_ARRAY1 )  2D:  ( DATA_TYPE** ARGOUTVIEW_ARRAY2, DIM_TYPE* DIM1, DIM_TYPE* DIM2 ) ( DIM_TYPE* DIM1, DIM_TYPE* DIM2, DATA_TYPE** ARGOUTVIEW_ARRAY2 ) ( DATA_TYPE** ARGOUTVIEW_FARRAY2, DIM_TYPE* DIM1, DIM_TYPE* DIM2 ) ( DIM_TYPE* DIM1, DIM_TYPE* DIM2, DATA_TYPE** ARGOUTVIEW_FARRAY2 )  3D:  ( DATA_TYPE** ARGOUTVIEW_ARRAY3, DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3) ( DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DATA_TYPE** ARGOUTVIEW_ARRAY3) ( DATA_TYPE** ARGOUTVIEW_FARRAY3, DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3) ( DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DATA_TYPE** ARGOUTVIEW_FARRAY3)  4D:  (DATA_TYPE** ARGOUTVIEW_ARRAY4, DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DIM_TYPE* DIM4) (DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DIM_TYPE* DIM4, DATA_TYPE** ARGOUTVIEW_ARRAY4) (DATA_TYPE** ARGOUTVIEW_FARRAY4, DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DIM_TYPE* DIM4) (DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DIM_TYPE* DIM4, DATA_TYPE** ARGOUTVIEW_FARRAY4)  Note that arrays with hard-coded dimensions are not supported. These cannot follow the double pointer signatures of these typemaps.   Memory Managed Argout View Arrays A recent addition to numpy.i are typemaps that permit argout arrays with views into memory that is managed. See the discussion here. 1D:  (DATA_TYPE** ARGOUTVIEWM_ARRAY1, DIM_TYPE* DIM1) (DIM_TYPE* DIM1, DATA_TYPE** ARGOUTVIEWM_ARRAY1)  2D:  (DATA_TYPE** ARGOUTVIEWM_ARRAY2, DIM_TYPE* DIM1, DIM_TYPE* DIM2) (DIM_TYPE* DIM1, DIM_TYPE* DIM2, DATA_TYPE** ARGOUTVIEWM_ARRAY2) (DATA_TYPE** ARGOUTVIEWM_FARRAY2, DIM_TYPE* DIM1, DIM_TYPE* DIM2) (DIM_TYPE* DIM1, DIM_TYPE* DIM2, DATA_TYPE** ARGOUTVIEWM_FARRAY2)  3D:  (DATA_TYPE** ARGOUTVIEWM_ARRAY3, DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3) (DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DATA_TYPE** ARGOUTVIEWM_ARRAY3) (DATA_TYPE** ARGOUTVIEWM_FARRAY3, DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3) (DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DATA_TYPE** ARGOUTVIEWM_FARRAY3)  4D:  (DATA_TYPE** ARGOUTVIEWM_ARRAY4, DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DIM_TYPE* DIM4) (DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DIM_TYPE* DIM4, DATA_TYPE** ARGOUTVIEWM_ARRAY4) (DATA_TYPE** ARGOUTVIEWM_FARRAY4, DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DIM_TYPE* DIM4) (DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DIM_TYPE* DIM4, DATA_TYPE** ARGOUTVIEWM_FARRAY4)    Output Arrays The numpy.i interface file does not support typemaps for output arrays, for several reasons. First, C/C++ return arguments are limited to a single value. This prevents obtaining dimension information in a general way. Second, arrays with hard-coded lengths are not permitted as return arguments. In other words: double[3] newVector(double x, double y, double z);\n is not legal C/C++ syntax. Therefore, we cannot provide typemaps of the form: %typemap(out) (TYPE[ANY]);\n If you run into a situation where a function or method is returning a pointer to an array, your best bet is to write your own version of the function to be wrapped, either with %extend for the case of class methods or %ignore and %rename for the case of functions.   Other Common Types: bool Note that C++ type bool is not supported in the list in the Available Typemaps section. NumPy bools are a single byte, while the C++ bool is four bytes (at least on my system). Therefore: %numpy_typemaps(bool, NPY_BOOL, int)\n will result in typemaps that will produce code that reference improper data lengths. You can implement the following macro expansion: %numpy_typemaps(bool, NPY_UINT, int)\n to fix the data length problem, and Input Arrays will work fine, but In-Place Arrays might fail type-checking.   Other Common Types: complex Typemap conversions for complex floating-point types is also not supported automatically. This is because Python and NumPy are written in C, which does not have native complex types. Both Python and NumPy implement their own (essentially equivalent) struct definitions for complex variables: /* Python */\ntypedef struct {double real; double imag;} Py_complex;\n\n/* NumPy */\ntypedef struct {float  real, imag;} npy_cfloat;\ntypedef struct {double real, imag;} npy_cdouble;\n We could have implemented: %numpy_typemaps(Py_complex , NPY_CDOUBLE, int)\n%numpy_typemaps(npy_cfloat , NPY_CFLOAT , int)\n%numpy_typemaps(npy_cdouble, NPY_CDOUBLE, int)\n which would have provided automatic type conversions for arrays of type Py_complex, npy_cfloat and npy_cdouble. However, it seemed unlikely that there would be any independent (non-Python, non-NumPy) application code that people would be using SWIG to generate a Python interface to, that also used these definitions for complex types. More likely, these application codes will define their own complex types, or in the case of C++, use std::complex. Assuming these data structures are compatible with Python and NumPy complex types, %numpy_typemap expansions as above (with the user\u2019s complex type substituted for the first argument) should work.    NumPy Array Scalars and SWIG SWIG has sophisticated type checking for numerical types. For example, if your C/C++ routine expects an integer as input, the code generated by SWIG will check for both Python integers and Python long integers, and raise an overflow error if the provided Python integer is too big to cast down to a C integer. With the introduction of NumPy scalar arrays into your Python code, you might conceivably extract an integer from a NumPy array and attempt to pass this to a SWIG-wrapped C/C++ function that expects an int, but the SWIG type checking will not recognize the NumPy array scalar as an integer. (Often, this does in fact work \u2013 it depends on whether NumPy recognizes the integer type you are using as inheriting from the Python integer type on the platform you are using. Sometimes, this means that code that works on a 32-bit machine will fail on a 64-bit machine.) If you get a Python error that looks like the following: TypeError: in method 'MyClass_MyMethod', argument 2 of type 'int'\n and the argument you are passing is an integer extracted from a NumPy array, then you have stumbled upon this problem. The solution is to modify the SWIG type conversion system to accept NumPy array scalars in addition to the standard integer types. Fortunately, this capability has been provided for you. Simply copy the file: pyfragments.swg\n to the working build directory for you project, and this problem will be fixed. It is suggested that you do this anyway, as it only increases the capabilities of your Python interface.  Why is There a Second File? The SWIG type checking and conversion system is a complicated combination of C macros, SWIG macros, SWIG typemaps and SWIG fragments. Fragments are a way to conditionally insert code into your wrapper file if it is needed, and not insert it if not needed. If multiple typemaps require the same fragment, the fragment only gets inserted into your wrapper code once. There is a fragment for converting a Python integer to a C long. There is a different fragment that converts a Python integer to a C int, that calls the routine defined in the long fragment. We can make the changes we want here by changing the definition for the long fragment. SWIG determines the active definition for a fragment using a \u201cfirst come, first served\u201d system. That is, we need to define the fragment for long conversions prior to SWIG doing it internally. SWIG allows us to do this by putting our fragment definitions in the file pyfragments.swg. If we were to put the new fragment definitions in numpy.i, they would be ignored.    Helper Functions The numpy.i file contains several macros and routines that it uses internally to build its typemaps. However, these functions may be useful elsewhere in your interface file. These macros and routines are implemented as fragments, which are described briefly in the previous section. If you try to use one or more of the following macros or functions, but your compiler complains that it does not recognize the symbol, then you need to force these fragments to appear in your code using: %fragment(\"NumPy_Fragments\");\n in your SWIG interface file.  Macros  is_array(a)\n\nEvaluates as true if a is non-NULL and can be cast to a PyArrayObject*.  array_type(a)\n\nEvaluates to the integer data type code of a, assuming a can be cast to a PyArrayObject*.  array_numdims(a)\n\nEvaluates to the integer number of dimensions of a, assuming a can be cast to a PyArrayObject*.  array_dimensions(a)\n\nEvaluates to an array of type npy_intp and length array_numdims(a), giving the lengths of all of the dimensions of a, assuming a can be cast to a PyArrayObject*.  array_size(a,i)\n\nEvaluates to the i-th dimension size of a, assuming a can be cast to a PyArrayObject*.  array_strides(a)\n\nEvaluates to an array of type npy_intp and length array_numdims(a), giving the stridess of all of the dimensions of a, assuming a can be cast to a PyArrayObject*. A stride is the distance in bytes between an element and its immediate neighbor along the same axis.  array_stride(a,i)\n\nEvaluates to the i-th stride of a, assuming a can be cast to a PyArrayObject*.  array_data(a)\n\nEvaluates to a pointer of type void* that points to the data buffer of a, assuming a can be cast to a PyArrayObject*.  array_descr(a)\n\nReturns a borrowed reference to the dtype property (PyArray_Descr*) of a, assuming a can be cast to a PyArrayObject*.  array_flags(a)\n\nReturns an integer representing the flags of a, assuming a can be cast to a PyArrayObject*.  array_enableflags(a,f)\n\nSets the flag represented by f of a, assuming a can be cast to a PyArrayObject*.  array_is_contiguous(a)\n\nEvaluates as true if a is a contiguous array. Equivalent to (PyArray_ISCONTIGUOUS(a)).  array_is_native(a)\n\nEvaluates as true if the data buffer of a uses native byte order. Equivalent to (PyArray_ISNOTSWAPPED(a)).  array_is_fortran(a)\n\nEvaluates as true if a is FORTRAN ordered.     Routines pytype_string() Return type: const char* Arguments:  \nPyObject* py_obj, a general Python object.  Return a string describing the type of py_obj. typecode_string() Return type: const char* Arguments:  \nint typecode, a NumPy integer typecode.  Return a string describing the type corresponding to the NumPy typecode. type_match() Return type: int Arguments:  \nint actual_type, the NumPy typecode of a NumPy array. \nint desired_type, the desired NumPy typecode.  Make sure that actual_type is compatible with desired_type. For example, this allows character and byte types, or int and long types, to match. This is now equivalent to PyArray_EquivTypenums(). obj_to_array_no_conversion() Return type: PyArrayObject* Arguments:  \nPyObject* input, a general Python object. \nint typecode, the desired NumPy typecode.  Cast input to a PyArrayObject* if legal, and ensure that it is of type typecode. If input cannot be cast, or the typecode is wrong, set a Python error and return NULL. obj_to_array_allow_conversion() Return type: PyArrayObject* Arguments:  \nPyObject* input, a general Python object. \nint typecode, the desired NumPy typecode of the resulting array. \nint* is_new_object, returns a value of 0 if no conversion performed, else 1.  Convert input to a NumPy array with the given typecode. On success, return a valid PyArrayObject* with the correct type. On failure, the Python error string will be set and the routine returns NULL. make_contiguous() Return type: PyArrayObject* Arguments:  \nPyArrayObject* ary, a NumPy array. \nint* is_new_object, returns a value of 0 if no conversion performed, else 1. \nint min_dims, minimum allowable dimensions. \nint max_dims, maximum allowable dimensions.  Check to see if ary is contiguous. If so, return the input pointer and flag it as not a new object. If it is not contiguous, create a new PyArrayObject* using the original data, flag it as a new object and return the pointer. make_fortran() Return type: PyArrayObject* Arguments  \nPyArrayObject* ary, a NumPy array. \nint* is_new_object, returns a value of 0 if no conversion performed, else 1.  Check to see if ary is Fortran contiguous. If so, return the input pointer and flag it as not a new object. If it is not Fortran contiguous, create a new PyArrayObject* using the original data, flag it as a new object and return the pointer. obj_to_array_contiguous_allow_conversion() Return type: PyArrayObject* Arguments:  \nPyObject* input, a general Python object. \nint typecode, the desired NumPy typecode of the resulting array. \nint* is_new_object, returns a value of 0 if no conversion performed, else 1.  Convert input to a contiguous PyArrayObject* of the specified type. If the input object is not a contiguous PyArrayObject*, a new one will be created and the new object flag will be set. obj_to_array_fortran_allow_conversion() Return type: PyArrayObject* Arguments:  \nPyObject* input, a general Python object. \nint typecode, the desired NumPy typecode of the resulting array. \nint* is_new_object, returns a value of 0 if no conversion performed, else 1.  Convert input to a Fortran contiguous PyArrayObject* of the specified type. If the input object is not a Fortran contiguous PyArrayObject*, a new one will be created and the new object flag will be set. require_contiguous() Return type: int Arguments:  \nPyArrayObject* ary, a NumPy array.  Test whether ary is contiguous. If so, return 1. Otherwise, set a Python error and return 0. require_native() Return type: int Arguments:  \nPyArray_Object* ary, a NumPy array.  Require that ary is not byte-swapped. If the array is not byte-swapped, return 1. Otherwise, set a Python error and return 0. require_dimensions() Return type: int Arguments:  \nPyArrayObject* ary, a NumPy array. \nint exact_dimensions, the desired number of dimensions.  Require ary to have a specified number of dimensions. If the array has the specified number of dimensions, return 1. Otherwise, set a Python error and return 0. require_dimensions_n() Return type: int Arguments:  \nPyArrayObject* ary, a NumPy array. \nint* exact_dimensions, an array of integers representing acceptable numbers of dimensions. \nint n, the length of exact_dimensions.  Require ary to have one of a list of specified number of dimensions. If the array has one of the specified number of dimensions, return 1. Otherwise, set the Python error string and return 0. require_size() Return type: int Arguments:  \nPyArrayObject* ary, a NumPy array. \nnpy_int* size, an array representing the desired lengths of each dimension. \nint n, the length of size.  Require ary to have a specified shape. If the array has the specified shape, return 1. Otherwise, set the Python error string and return 0. require_fortran() Return type: int Arguments:  \nPyArrayObject* ary, a NumPy array.  Require the given PyArrayObject to to be Fortran ordered. If the PyArrayObject is already Fortran ordered, do nothing. Else, set the Fortran ordering flag and recompute the strides.    Beyond the Provided Typemaps There are many C or C++ array/NumPy array situations not covered by a simple %include \"numpy.i\" and subsequent %apply directives.  A Common Example Consider a reasonable prototype for a dot product function: double dot(int len, double* vec1, double* vec2);\n The Python interface that we want is: def dot(vec1, vec2):\n    \"\"\"\n    dot(PyObject,PyObject) -> double\n    \"\"\"\n The problem here is that there is one dimension argument and two array arguments, and our typemaps are set up for dimensions that apply to a single array (in fact, SWIG does not provide a mechanism for associating len with vec2 that takes two Python input arguments). The recommended solution is the following: %apply (int DIM1, double* IN_ARRAY1) {(int len1, double* vec1),\n                                      (int len2, double* vec2)}\n%rename (dot) my_dot;\n%exception my_dot {\n    $action\n    if (PyErr_Occurred()) SWIG_fail;\n}\n%inline %{\ndouble my_dot(int len1, double* vec1, int len2, double* vec2) {\n    if (len1 != len2) {\n        PyErr_Format(PyExc_ValueError,\n                     \"Arrays of lengths (%d,%d) given\",\n                     len1, len2);\n        return 0.0;\n    }\n    return dot(len1, vec1, vec2);\n}\n%}\n If the header file that contains the prototype for double dot() also contains other prototypes that you want to wrap, so that you need to %include this header file, then you will also need a %ignore\ndot; directive, placed after the %rename and before the %include directives. Or, if the function in question is a class method, you will want to use %extend rather than %inline in addition to %ignore. A note on error handling: Note that my_dot returns a double but that it can also raise a Python error. The resulting wrapper function will return a Python float representation of 0.0 when the vector lengths do not match. Since this is not NULL, the Python interpreter will not know to check for an error. For this reason, we add the %exception directive above for my_dot to get the behavior we want (note that $action is a macro that gets expanded to a valid call to my_dot). In general, you will probably want to write a SWIG macro to perform this task.   Other Situations There are other wrapping situations in which numpy.i may be helpful when you encounter them.  \nIn some situations, it is possible that you could use the %numpy_typemaps macro to implement typemaps for your own types. See the Other Common Types: bool or Other Common Types: complex sections for examples. Another situation is if your dimensions are of a type other than int (say long for example): %numpy_typemaps(double, NPY_DOUBLE, long)\n  You can use the code in numpy.i to write your own typemaps. For example, if you had a five-dimensional array as a function argument, you could cut-and-paste the appropriate four-dimensional typemaps into your interface file. The modifications for the fourth dimension would be trivial. Sometimes, the best approach is to use the %extend directive to define new methods for your classes (or overload existing ones) that take a PyObject* (that either is or can be converted to a PyArrayObject*) instead of a pointer to a buffer. In this case, the helper routines in numpy.i can be very useful. Writing typemaps can be a bit nonintuitive. If you have specific questions about writing SWIG typemaps for NumPy, the developers of numpy.i do monitor the Numpy-discussion and Swig-user mail lists.    A Final Note When you use the %apply directive, as is usually necessary to use numpy.i, it will remain in effect until you tell SWIG that it shouldn\u2019t be. If the arguments to the functions or methods that you are wrapping have common names, such as length or vector, these typemaps may get applied in situations you do not expect or want. Therefore, it is always a good idea to add a %clear directive after you are done with a specific typemap: %apply (double* IN_ARRAY1, int DIM1) {(double* vector, int length)}\n%include \"my_header.h\"\n%clear (double* vector, int length);\n In general, you should target these typemap signatures specifically where you want them, and then clear them after you are done.    Summary Out of the box, numpy.i provides typemaps that support conversion between NumPy arrays and C arrays:  That can be one of 12 different scalar types: signed char, unsigned char, short, unsigned short, int, unsigned int, long, unsigned long, long long, unsigned long long, float and double. \nThat support 74 different argument signatures for each data type, including:  One-dimensional, two-dimensional, three-dimensional and four-dimensional arrays. Input-only, in-place, argout, argoutview, and memory managed argoutview behavior. Hard-coded dimensions, data-buffer-then-dimensions specification, and dimensions-then-data-buffer specification. Both C-ordering (\u201clast dimension fastest\u201d) or Fortran-ordering (\u201cfirst dimension fastest\u201d) support for 2D, 3D and 4D arrays.    The numpy.i interface file also provides additional tools for wrapper developers, including:  A SWIG macro (%numpy_typemaps) with three arguments for implementing the 74 argument signatures for the user\u2019s choice of (1) C data type, (2) NumPy data type (assuming they match), and (3) dimension type. Fourteen C macros and fifteen C functions that can be used to write specialized typemaps, extensions, or inlined functions that handle cases not covered by the provided typemaps. Note that the macros and functions are coded specifically to work with the NumPy C/API regardless of NumPy version number, both before and after the deprecation of some aspects of the API after version 1.6.  \n"}, {"name": "Iterating Over Arrays", "path": "reference/arrays.nditer", "type": "Iterating Over Arrays", "text": "Iterating Over Arrays  Note Arrays support the iterator protocol and can be iterated over like Python lists. See the Indexing, Slicing and Iterating section in the Quickstart guide for basic usage and examples. The remainder of this document presents the nditer object and covers more advanced usage.  The iterator object nditer, introduced in NumPy 1.6, provides many flexible ways to visit all the elements of one or more arrays in a systematic fashion. This page introduces some basic ways to use the object for computations on arrays in Python, then concludes with how one can accelerate the inner loop in Cython. Since the Python exposure of nditer is a relatively straightforward mapping of the C array iterator API, these ideas will also provide help working with array iteration from C or C++.  Single Array Iteration The most basic task that can be done with the nditer is to visit every element of an array. Each element is provided one by one using the standard Python iterator interface. Example >>> a = np.arange(6).reshape(2,3)\n>>> for x in np.nditer(a):\n...     print(x, end=' ')\n...\n0 1 2 3 4 5\n An important thing to be aware of for this iteration is that the order is chosen to match the memory layout of the array instead of using a standard C or Fortran ordering. This is done for access efficiency, reflecting the idea that by default one simply wants to visit each element without concern for a particular ordering. We can see this by iterating over the transpose of our previous array, compared to taking a copy of that transpose in C order. Example >>> a = np.arange(6).reshape(2,3)\n>>> for x in np.nditer(a.T):\n...     print(x, end=' ')\n...\n0 1 2 3 4 5\n >>> for x in np.nditer(a.T.copy(order='C')):\n...     print(x, end=' ')\n...\n0 3 1 4 2 5\n The elements of both a and a.T get traversed in the same order, namely the order they are stored in memory, whereas the elements of a.T.copy(order=\u2019C\u2019) get visited in a different order because they have been put into a different memory layout.  Controlling Iteration Order There are times when it is important to visit the elements of an array in a specific order, irrespective of the layout of the elements in memory. The nditer object provides an order parameter to control this aspect of iteration. The default, having the behavior described above, is order=\u2019K\u2019 to keep the existing order. This can be overridden with order=\u2019C\u2019 for C order and order=\u2019F\u2019 for Fortran order. Example >>> a = np.arange(6).reshape(2,3)\n>>> for x in np.nditer(a, order='F'):\n...     print(x, end=' ')\n...\n0 3 1 4 2 5\n>>> for x in np.nditer(a.T, order='C'):\n...     print(x, end=' ')\n...\n0 3 1 4 2 5\n   Modifying Array Values By default, the nditer treats the input operand as a read-only object. To be able to modify the array elements, you must specify either read-write or write-only mode using the \u2018readwrite\u2019 or \u2018writeonly\u2019 per-operand flags. The nditer will then yield writeable buffer arrays which you may modify. However, because the nditer must copy this buffer data back to the original array once iteration is finished, you must signal when the iteration is ended, by one of two methods. You may either:  used the nditer as a context manager using the with statement, and the temporary data will be written back when the context is exited. call the iterator\u2019s close method once finished iterating, which will trigger the write-back.  The nditer can no longer be iterated once either close is called or its context is exited. Example >>> a = np.arange(6).reshape(2,3)\n>>> a\narray([[0, 1, 2],\n       [3, 4, 5]])\n>>> with np.nditer(a, op_flags=['readwrite']) as it:\n...    for x in it:\n...        x[...] = 2 * x\n...\n>>> a\narray([[ 0,  2,  4],\n       [ 6,  8, 10]])\n If you are writing code that needs to support older versions of numpy, note that prior to 1.15, nditer was not a context manager and did not have a close method. Instead it relied on the destructor to initiate the w