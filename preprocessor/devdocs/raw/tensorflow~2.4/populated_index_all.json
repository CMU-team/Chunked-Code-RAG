[{"name": "tf.AggregationMethod", "path": "aggregationmethod", "type": "tf", "text": "tf.AggregationMethod     View source on GitHub    A class listing aggregation methods used to combine gradients.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.AggregationMethod  Computing partial derivatives can require aggregating gradient contributions. This class lists the various methods that can be used to combine gradients in the graph. The following aggregation methods are part of the stable API for aggregating gradients:  \nADD_N: All of the gradient terms are summed as part of one operation using the \"AddN\" op (see tf.add_n). This method has the property that all gradients must be ready and buffered separately in memory before any aggregation is performed. \nDEFAULT: The system-chosen default aggregation method.  The following aggregation methods are experimental and may not be supported in future releases:  \nEXPERIMENTAL_TREE: Gradient terms are summed in pairs using using the \"AddN\" op. This method of summing gradients may reduce performance, but it can improve memory utilization because the gradients can be released earlier. \n \n\n\n Class Variables\n  ADD_N   0  \n  DEFAULT   0  \n  EXPERIMENTAL_ACCUMULATE_N   2  \n  EXPERIMENTAL_TREE   1     \n"}, {"name": "tf.argsort", "path": "argsort", "type": "tf", "text": "tf.argsort     View source on GitHub    Returns the indices of a tensor that give its sorted order along an axis.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.argsort  \ntf.argsort(\n    values, axis=-1, direction='ASCENDING', stable=False, name=None\n)\n For a 1D tensor, tf.gather(values, tf.argsort(values)) is equivalent to tf.sort(values). For higher dimensions, the output has the same shape as values, but along the given axis, values represent the index of the sorted element in that slice of the tensor at the given position. Usage: import tensorflow as tf\na = [1, 10, 26.9, 2.8, 166.32, 62.3]\nb = tf.argsort(a,axis=-1,direction='ASCENDING',stable=False,name=None)\nc = tf.keras.backend.eval(b)\n# Here, c = [0 3 1 2 5 4]\n\n \n\n\n Args\n  values   1-D or higher numeric Tensor.  \n  axis   The axis along which to sort. The default is -1, which sorts the last axis.  \n  direction   The direction in which to sort the values ('ASCENDING' or 'DESCENDING').  \n  stable   If True, equal elements in the original tensor will not be re-ordered in the returned order. Unstable sort is not yet implemented, but will eventually be the default for performance reasons. If you require a stable order, pass stable=True for forwards compatibility.  \n  name   Optional name for the operation.   \n \n\n\n Returns   An int32 Tensor with the same shape as values. The indices that would sort each slice of the given values along the given axis.  \n\n \n\n\n Raises\n  ValueError   If axis is not a constant scalar, or the direction is invalid.     \n"}, {"name": "tf.audio", "path": "audio", "type": "tf.audio", "text": "Module: tf.audio Public API for tf.audio namespace. Functions decode_wav(...): Decode a 16-bit PCM WAV file to a float tensor. encode_wav(...): Encode audio data using the WAV file format.  \n"}, {"name": "tf.audio.decode_wav", "path": "audio/decode_wav", "type": "tf.audio", "text": "tf.audio.decode_wav Decode a 16-bit PCM WAV file to a float tensor.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.audio.decode_wav  \ntf.audio.decode_wav(\n    contents, desired_channels=-1, desired_samples=-1, name=None\n)\n The -32768 to 32767 signed 16-bit values will be scaled to -1.0 to 1.0 in float. When desired_channels is set, if the input contains fewer channels than this then the last channel will be duplicated to give the requested number, else if the input has more channels than requested then the additional channels will be ignored. If desired_samples is set, then the audio will be cropped or padded with zeroes to the requested length. The first output contains a Tensor with the content of the audio samples. The lowest dimension will be the number of channels, and the second will be the number of samples. For example, a ten-sample-long stereo WAV file should give an output shape of [10, 2].\n \n\n\n Args\n  contents   A Tensor of type string. The WAV-encoded audio, usually from a file.  \n  desired_channels   An optional int. Defaults to -1. Number of sample channels wanted.  \n  desired_samples   An optional int. Defaults to -1. Length of audio requested.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A tuple of Tensor objects (audio, sample_rate).     audio   A Tensor of type float32.  \n  sample_rate   A Tensor of type int32.     \n"}, {"name": "tf.audio.encode_wav", "path": "audio/encode_wav", "type": "tf.audio", "text": "tf.audio.encode_wav Encode audio data using the WAV file format.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.audio.encode_wav  \ntf.audio.encode_wav(\n    audio, sample_rate, name=None\n)\n This operation will generate a string suitable to be saved out to create a .wav audio file. It will be encoded in the 16-bit PCM format. It takes in float values in the range -1.0f to 1.0f, and any outside that value will be clamped to that range. audio is a 2-D float Tensor of shape [length, channels]. sample_rate is a scalar Tensor holding the rate to use (e.g. 44100).\n \n\n\n Args\n  audio   A Tensor of type float32. 2-D with shape [length, channels].  \n  sample_rate   A Tensor of type int32. Scalar containing the sample frequency.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type string.  \n  \n"}, {"name": "tf.autodiff", "path": "autodiff", "type": "tf.autodiff", "text": "Module: tf.autodiff Public API for tf.autodiff namespace. Classes class ForwardAccumulator: Computes Jacobian-vector products (\"JVP\"s) using forward-mode autodiff. class GradientTape: Record operations for automatic differentiation.  \n"}, {"name": "tf.autodiff.ForwardAccumulator", "path": "autodiff/forwardaccumulator", "type": "tf.autodiff", "text": "tf.autodiff.ForwardAccumulator Computes Jacobian-vector products (\"JVP\"s) using forward-mode autodiff. \ntf.autodiff.ForwardAccumulator(\n    primals, tangents\n)\n Compare to tf.GradientTape which computes vector-Jacobian products (\"VJP\"s) using reverse-mode autodiff (backprop). Reverse mode is more attractive when computing gradients of a scalar-valued function with respect to many inputs (e.g. a neural network with many parameters and a scalar loss). Forward mode works best on functions with many outputs and few inputs. Since it does not hold on to intermediate activations, it is much more memory efficient than backprop where it is applicable. Consider a simple linear regression: \nx = tf.constant([[2.0, 3.0], [1.0, 4.0]])\ndense = tf.keras.layers.Dense(1)\ndense.build([None, 2])\nwith tf.autodiff.ForwardAccumulator(\n   primals=dense.kernel,\n   tangents=tf.constant([[1.], [0.]])) as acc:\n  loss = tf.reduce_sum((dense(x) - tf.constant([1., -1.])) ** 2.)\nacc.jvp(loss)\n<tf.Tensor: shape=(), dtype=float32, numpy=...>\n The example has two variables containing parameters, dense.kernel (2 parameters) and dense.bias (1 parameter). Considering the training data x as a constant, this means the Jacobian matrix for the function mapping from parameters to loss has one row and three columns. With forwardprop, we specify a length-three vector in advance which multiplies the Jacobian. The primals constructor argument is the parameter (a tf.Tensor or tf.Variable) we're specifying a vector for, and the tangents argument is the \"vector\" in Jacobian-vector product. If our goal is to compute the entire Jacobian matrix, forwardprop computes one column at a time while backprop computes one row at a time. Since the Jacobian in the linear regression example has only one row, backprop requires fewer invocations: \nx = tf.constant([[2.0, 3.0], [1.0, 4.0]])\ndense = tf.keras.layers.Dense(1)\ndense.build([None, 2])\nloss_fn = lambda: tf.reduce_sum((dense(x) - tf.constant([1., -1.])) ** 2.)\nkernel_fprop = []\nwith tf.autodiff.ForwardAccumulator(\n    dense.kernel, tf.constant([[1.], [0.]])) as acc:\n  kernel_fprop.append(acc.jvp(loss_fn()))\nwith tf.autodiff.ForwardAccumulator(\n    dense.kernel, tf.constant([[0.], [1.]])) as acc:\n  kernel_fprop.append(acc.jvp(loss_fn()))\nwith tf.autodiff.ForwardAccumulator(dense.bias, tf.constant([1.])) as acc:\n  bias_fprop = acc.jvp(loss_fn())\nwith tf.GradientTape() as tape:\n  loss = loss_fn()\nkernel_grad, bias_grad = tape.gradient(loss, (dense.kernel, dense.bias))\nnp.testing.assert_allclose(\n    kernel_grad, tf.stack(kernel_fprop)[:, tf.newaxis])\nnp.testing.assert_allclose(bias_grad, bias_fprop[tf.newaxis])\n Implicit in the tape.gradient call is a length-one vector which left-multiplies the Jacobian, a vector-Jacobian product. ForwardAccumulator maintains JVPs corresponding primal tensors it is watching, derived from the original primals specified in the constructor. As soon as a primal tensor is deleted, ForwardAccumulator deletes the corresponding JVP. acc.jvp(x) retrieves acc's JVP corresponding to the primal tensor x. It does not perform any computation. acc.jvp calls can be repeated as long as acc is accessible, whether the context manager is active or not. New JVPs are only computed while the context manager is active. Note that ForwardAccumulators are always applied in the order their context managers were entered, so inner accumulators will not see JVP computation from outer accumulators. Take higher-order JVPs from outer accumulators: \nprimal = tf.constant(1.1)\nwith tf.autodiff.ForwardAccumulator(primal, tf.constant(1.)) as outer:\n  with tf.autodiff.ForwardAccumulator(primal, tf.constant(1.)) as inner:\n    primal_out = primal ** tf.constant(3.5)\ninner_jvp = inner.jvp(primal_out)\ninner_jvp  # 3.5 * 1.1 ** 2.5\n<tf.Tensor: shape=(), dtype=float32, numpy=4.4417057>\nouter.jvp(inner_jvp)  # 3.5 * 2.5 * 1.1 ** 1.5\n<tf.Tensor: shape=(), dtype=float32, numpy=10.094786>\n Reversing the collection in the last line to instead retrieve inner.jvp(outer.jvp(primal_out)) will not work. Strict nesting also applies to combinations of ForwardAccumulator and tf.GradientTape. More deeply nested GradientTape objects will ignore the products of outer ForwardAccumulator objects. This allows (for example) memory-efficient forward-over-backward computation of Hessian-vector products, where the inner GradientTape would otherwise hold on to all intermediate JVPs: \nv = tf.Variable([1., 2.])\nwith tf.autodiff.ForwardAccumulator(\n    v,\n    # The \"vector\" in Hessian-vector product.\n    tf.constant([1., 0.])) as acc:\n  with tf.GradientTape() as tape:\n    y = tf.reduce_sum(v ** 3.)\n  backward = tape.gradient(y, v)\nbackward  # gradient from backprop\n<tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 3., 12.], dtype=float32)>\nacc.jvp(backward)  # forward-over-backward Hessian-vector product\n<tf.Tensor: shape=(2,), dtype=float32, numpy=array([6., 0.], dtype=float32)>\n\n \n\n\n Args\n  primals   A tensor or nested structure of tensors to watch.  \n  tangents   A tensor or nested structure of tensors, with the same nesting structure as primals, with each element being a vector with the same size as the corresponding primal element.   \n \n\n\n Raises\n  ValueError   If the same tensor or variable is specified multiple times in primals.    Methods jvp View source \njvp(\n    primals, unconnected_gradients=tf.UnconnectedGradients.NONE\n)\n Fetches the Jacobian-vector product computed for primals. Note that this method performs no computation, and simply looks up a JVP that was already computed (unlike backprop using a tf.GradientTape, where the computation happens on the call to tape.gradient).\n \n\n\n Args\n  primals   A watched Tensor or structure of Tensors to fetch the JVPs for.  \n  unconnected_gradients   A value which can either hold 'none' or 'zero' and alters the value which will be returned if no JVP was computed for primals. The possible values and effects are detailed in 'tf.UnconnectedGradients' and it defaults to 'none'.   \n \n\n\n Returns   Tensors with the same shapes and dtypes as primals, or None if no JVP is available.  \n __enter__ View source \n__enter__()\n __exit__ View source \n__exit__(\n    typ, value, traceback\n)\n  \n"}, {"name": "tf.autograph", "path": "autograph", "type": "tf.autograph", "text": "Module: tf.autograph Conversion of plain Python into TensorFlow graph code. \nNote: In TensorFlow 2.0, AutoGraph is automatically applied when using tf.function. This module contains lower-level APIs for advanced use.\n For more information, see the AutoGraph guide. By equivalent graph code we mean code that generates a TensorFlow graph when run. The generated graph has the same effects as the original code when executed (for example with tf.function or tf.compat.v1.Session.run). In other words, using AutoGraph can be thought of as running Python in TensorFlow. Modules experimental module: Public API for tf.autograph.experimental namespace. Functions set_verbosity(...): Sets the AutoGraph verbosity level. to_code(...): Returns the source code generated by AutoGraph, as a string. to_graph(...): Converts a Python entity into a TensorFlow graph. trace(...): Traces argument information at compilation time.  \n"}, {"name": "tf.autograph.experimental", "path": "autograph/experimental", "type": "tf.autograph", "text": "Module: tf.autograph.experimental Public API for tf.autograph.experimental namespace. Classes class Feature: This enumeration represents optional conversion options. Functions do_not_convert(...): Decorator that suppresses the conversion of a function. set_loop_options(...): Specifies additional arguments to be passed to the enclosing while_loop.  \n"}, {"name": "tf.autograph.experimental.do_not_convert", "path": "autograph/experimental/do_not_convert", "type": "tf.autograph", "text": "tf.autograph.experimental.do_not_convert     View source on GitHub    Decorator that suppresses the conversion of a function.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.autograph.experimental.do_not_convert  \ntf.autograph.experimental.do_not_convert(\n    func=None\n)\n\n \n\n\n Args\n  func   function to decorate.   \n \n\n\n Returns   If func is not None, returns a Callable which is equivalent to func, but is not converted by AutoGraph. If func is None, returns a decorator that, when invoked with a single func argument, returns a Callable equivalent to the above case.  \n  \n"}, {"name": "tf.autograph.experimental.Feature", "path": "autograph/experimental/feature", "type": "tf.autograph", "text": "tf.autograph.experimental.Feature     View source on GitHub    This enumeration represents optional conversion options.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.autograph.experimental.Feature  These conversion options are experimental. They are subject to change without notice and offer no guarantees. Example Usage optionals= tf.autograph.experimental.Feature.EQUALITY_OPERATORS\n@tf.function(experimental_autograph_options=optionals)\ndef f(i):\n  if i == 0:  # EQUALITY_OPERATORS allows the use of == here.\n    tf.print('i is zero')\n\n \n\n\n Attributes\n  ALL   Enable all features.  \n  AUTO_CONTROL_DEPS   Insert of control dependencies in the generated code.  \n  ASSERT_STATEMENTS   Convert Tensor-dependent assert statements to tf.Assert.  \n  BUILTIN_FUNCTIONS   Convert builtin functions applied to Tensors to their TF counterparts.  \n  EQUALITY_OPERATORS   Whether to convert the comparison operators, like equality. This is soon to be deprecated as support is being added to the Tensor class.  \n  LISTS   Convert list idioms, like initializers, slices, append, etc.  \n  NAME_SCOPES   Insert name scopes that name ops according to context, like the function they were defined in.   \n \n\n\n Class Variables\n  ALL   tf.autograph.experimental.Feature  \n  ASSERT_STATEMENTS   tf.autograph.experimental.Feature  \n  AUTO_CONTROL_DEPS   tf.autograph.experimental.Feature  \n  BUILTIN_FUNCTIONS   tf.autograph.experimental.Feature  \n  EQUALITY_OPERATORS   tf.autograph.experimental.Feature  \n  LISTS   tf.autograph.experimental.Feature  \n  NAME_SCOPES   tf.autograph.experimental.Feature     \n"}, {"name": "tf.autograph.experimental.set_loop_options", "path": "autograph/experimental/set_loop_options", "type": "tf.autograph", "text": "tf.autograph.experimental.set_loop_options Specifies additional arguments to be passed to the enclosing while_loop.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.autograph.experimental.set_loop_options  \ntf.autograph.experimental.set_loop_options(\n    parallel_iterations=UNSPECIFIED, swap_memory=UNSPECIFIED,\n    maximum_iterations=UNSPECIFIED, shape_invariants=UNSPECIFIED\n)\n The parameters apply to and only to the immediately enclosing loop. It only has effect if the loop is staged as a TF while_loop; otherwise the parameters have no effect. Usage: \n@tf.function(autograph=True)\ndef f():\n  n = 0\n  for i in tf.range(10):\n    tf.autograph.experimental.set_loop_options(maximum_iterations=3)\n    n += 1\n  return n\n \n@tf.function(autograph=True)\ndef f():\n  v = tf.constant((0,))\n  for i in tf.range(3):\n    tf.autograph.experimental.set_loop_options(\n        shape_invariants=[(v, tf.TensorShape([None]))]\n    )\n    v = tf.concat((v, [i]), 0)\n  return v\n Also see tf.while_loop.\n \n\n\n Args\n  parallel_iterations   The maximum number of iterations allowed to run in parallel at any given time. Note that this does not guarantee parallel execution.  \n  swap_memory   Whether to store intermediate values needed for gradients on the CPU instead of GPU.  \n  maximum_iterations   Allows limiting the total number of iterations executed by the loop.  \n  shape_invariants   Allows controlling the argument with the same name passed to tf.while_loop. Unlike tf.while_loop, this is a list of (tensor, shape) pairs.     \n"}, {"name": "tf.autograph.set_verbosity", "path": "autograph/set_verbosity", "type": "tf.autograph", "text": "tf.autograph.set_verbosity     View source on GitHub    Sets the AutoGraph verbosity level.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.autograph.set_verbosity  \ntf.autograph.set_verbosity(\n    level, alsologtostdout=False\n)\n Debug logging in AutoGraph More verbose logging is useful to enable when filing bug reports or doing more in-depth debugging. There are two means to control the logging verbosity:  The set_verbosity function The AUTOGRAPH_VERBOSITY environment variable  set_verbosity takes precedence over the environment variable. For example: import os\nimport tensorflow as tf\n\nos.environ['AUTOGRAPH_VERBOSITY'] = 5\n# Verbosity is now 5\n\ntf.autograph.set_verbosity(0)\n# Verbosity is now 0\n\nos.environ['AUTOGRAPH_VERBOSITY'] = 1\n# No effect, because set_verbosity was already called.\n Logs entries are output to absl's default output, with INFO level. Logs can be mirrored to stdout by using the alsologtostdout argument. Mirroring is enabled by default when Python runs in interactive mode.\n \n\n\n Args\n  level   int, the verbosity level; larger values specify increased verbosity; 0 means no logging. When reporting bugs, it is recommended to set this value to a larger number, like 10.  \n  alsologtostdout   bool, whether to also output log messages to sys.stdout.     \n"}, {"name": "tf.autograph.to_code", "path": "autograph/to_code", "type": "tf.autograph", "text": "tf.autograph.to_code     View source on GitHub    Returns the source code generated by AutoGraph, as a string. \ntf.autograph.to_code(\n    entity, recursive=True, experimental_optional_features=None\n)\n Example usage: \ndef f(x):\n  if x < 0:\n    x = -x\n  return x\ntf.autograph.to_code(f)\n\"...def tf__f(x):...\"\n Also see: tf.autograph.to_graph. \nNote: If a function has been decorated with tf.function, pass its underlying Python function, rather than the callable that `tf.function creates:\n \n@tf.function\ndef f(x):\n  if x < 0:\n    x = -x\n  return x\ntf.autograph.to_code(f.python_function)\n\"...def tf__f(x):...\"\n\n \n\n\n Args\n  entity   Python callable or class to convert.  \n  recursive   Whether to recursively convert any functions that the converted function may call.  \n  experimental_optional_features   None, a tuple of, or a single tf.autograph.experimental.Feature value.   \n \n\n\n Returns   The converted code as string.  \n  \n"}, {"name": "tf.autograph.to_graph", "path": "autograph/to_graph", "type": "tf.autograph", "text": "tf.autograph.to_graph     View source on GitHub    Converts a Python entity into a TensorFlow graph. \ntf.autograph.to_graph(\n    entity, recursive=True, experimental_optional_features=None\n)\n Also see: tf.autograph.to_code, tf.function. Unlike tf.function, to_graph is a low-level transpiler that converts Python code to TensorFlow graph code. It does not implement any caching, variable management or create any actual ops, and is best used where greater control over the generated TensorFlow graph is desired. Another difference from tf.function is that to_graph will not wrap the graph into a TensorFlow function or a Python callable. Internally, tf.function uses to_graph. Example usage: \ndef f(x):\n  if x > 0:\n    y = x * x\n  else:\n    y = -x\n  return y\n\nconverted_f = to_graph(f)\nx = tf.constant(2)\nconverted_f(x)  # converted_foo is like a TensorFlow Op.\n<tf.Tensor: shape=(), dtype=int32, numpy=4>\n Supported Python entities include:  functions classes object methods  Functions are converted into new functions with converted code. Classes are converted by generating a new class whose methods use converted code. Methods are converted into unbound function that have an additional first argument called self. For a tutorial, see the tf.function and AutoGraph guide. For more detailed information, see the AutoGraph reference documentation.\n \n\n\n Args\n  entity   Python callable or class to convert.  \n  recursive   Whether to recursively convert any functions that the converted function may call.  \n  experimental_optional_features   None, a tuple of, or a single tf.autograph.experimental.Feature value.   \n \n\n\n Returns   Same as entity, the converted Python function or class.  \n\n \n\n\n Raises\n  ValueError   If the entity could not be converted.     \n"}, {"name": "tf.autograph.trace", "path": "autograph/trace", "type": "tf.autograph", "text": "tf.autograph.trace     View source on GitHub    Traces argument information at compilation time.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.autograph.trace  \ntf.autograph.trace(\n    *args\n)\n trace is useful when debugging, and it always executes during the tracing phase, that is, when the TF graph is constructed. Example usage import tensorflow as tf\n\nfor i in tf.range(10):\n  tf.autograph.trace(i)\n# Output: <Tensor ...>\n\n \n\n\n Args\n  *args   Arguments to print to sys.stdout.     \n"}, {"name": "tf.batch_to_space", "path": "batch_to_space", "type": "tf", "text": "tf.batch_to_space     View source on GitHub    BatchToSpace for N-D tensors of type T. \ntf.batch_to_space(\n    input, block_shape, crops, name=None\n)\n This operation reshapes the \"batch\" dimension 0 into M + 1 dimensions of shape block_shape + [batch], interleaves these blocks back into the grid defined by the spatial dimensions [1, ..., M], to obtain a result with the same rank as the input. The spatial dimensions of this intermediate result are then optionally cropped according to crops to produce the output. This is the reverse of SpaceToBatch (see tf.space_to_batch).\n \n\n\n Args\n  input   A N-D Tensor with shape input_shape = [batch] + spatial_shape + remaining_shape, where spatial_shape has M dimensions.  \n  block_shape   A 1-D Tensor with shape [M]. Must be one of the following types: int32, int64. All values must be >= 1. For backwards compatibility with TF 1.0, this parameter may be an int, in which case it is converted to numpy.array([block_shape, block_shape], dtype=numpy.int64).  \n  crops   A 2-D Tensor with shape [M, 2]. Must be one of the following types: int32, int64. All values must be >= 0. crops[i] = [crop_start, crop_end] specifies the amount to crop from input dimension i + 1, which corresponds to spatial dimension i. It is required that crop_start[i] + crop_end[i] <= block_shape[i] * input_shape[i + 1]. This operation is equivalent to the following steps:  Reshape input to reshaped of shape: [block_shape[0], ..., block_shape[M-1], batch / prod(block_shape), input_shape[1], ..., input_shape[N-1]] Permute dimensions of reshaped to produce permuted of shape [batch / prod(block_shape), input_shape[1], block_shape[0], ..., input_shape[M], block_shape[M-1], input_shape[M+1], ..., input_shape[N-1]] Reshape permuted to produce reshaped_permuted of shape [batch / prod(block_shape), input_shape[1] * block_shape[0], ..., input_shape[M] * block_shape[M-1], input_shape[M+1], ..., input_shape[N-1]] Crop the start and end of dimensions [1, ..., M] of reshaped_permuted according to crops to produce the output of shape: [batch / prod(block_shape), input_shape[1] * block_shape[0] - crops[0,0] - crops[0,1], ..., input_shape[M] * block_shape[M-1] - crops[M-1,0] - crops[M-1,1], input_shape[M+1], ..., input_shape[N-1]] \n\n \n  name   A name for the operation (optional).    Examples: (1) For the following input of shape [4, 1, 1, 1], block_shape = [2, 2], and crops = [[0, 0], [0, 0]]: [[[[1]]],\n [[[2]]],\n [[[3]]],\n [[[4]]]]\n The output tensor has shape [1, 2, 2, 1] and value: x = [[[[1], [2]],\n    [[3], [4]]]]\n (2) For the following input of shape [4, 1, 1, 3], block_shape = [2, 2], and crops = [[0, 0], [0, 0]]: [[[1,  2,   3]],\n [[4,  5,   6]],\n [[7,  8,   9]],\n [[10, 11, 12]]]\n The output tensor has shape [1, 2, 2, 3] and value:    x = [[[[1, 2, 3], [4,  5,  6 ]],\n         [[7, 8, 9], [10, 11, 12]]]]\n   ```\n\n(3) For the following\n   input of shape `[4, 2, 2, 1]`,\n   `block_shape = [2, 2]`, and `crops = [[0, 0], [0, 0]]`:\n\n   ```python\n   x = [[[[1], [3]], [[ 9], [11]]],\n        [[[2], [4]], [[10], [12]]],\n        [[[5], [7]], [[13], [15]]],\n        [[[6], [8]], [[14], [16]]]]\n   ```\n\n  The output tensor has shape `[1, 4, 4, 1]` and value:\n\n  ```python\n   x = [[[1],  [2],  [ 3], [ 4]],\n        [[5],  [6],  [ 7], [ 8]],\n        [[9],  [10], [11], [12]],\n        [[13], [14], [15], [16]]]\n   ```\n\n (4) For the following input of shape\n    `[8, 1, 3, 1]`,\n    `block_shape = [2, 2]`, and `crops = [[0, 0], [2, 0]]`:\n\n    ```python\n    x = [[[[0], [ 1], [ 3]]],\n         [[[0], [ 9], [11]]],\n         [[[0], [ 2], [ 4]]],\n         [[[0], [10], [12]]],\n         [[[0], [ 5], [ 7]]],\n         [[[0], [13], [15]]],\n         [[[0], [ 6], [ 8]]],\n         [[[0], [14], [16]]]]\n    ```\n\n    The output tensor has shape `[2, 2, 4, 1]` and value:\n\n    ```python\n    x = [[[[ 1], [ 2], [ 3], [ 4]],\n          [[ 5], [ 6], [ 7], [ 8]]],\n         [[[ 9], [10], [11], [12]],\n          [[13], [14], [15], [16]]]]\n    ```\n\n<!-- Tabular view -->\n <table class=\"responsive fixed orange\">\n<colgroup><col width=\"214px\"><col></colgroup>\n<tr><th colspan=\"2\"><h2 class=\"add-link\">Returns</h2></th></tr>\n<tr class=\"alt\">\n<td colspan=\"2\">\nA `Tensor`. Has the same type as `input`.\n</td>\n</tr>\n\n</table>\n  \n"}, {"name": "tf.bitcast", "path": "bitcast", "type": "tf", "text": "tf.bitcast Bitcasts a tensor from one type to another without copying data.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.bitcast  \ntf.bitcast(\n    input, type, name=None\n)\n Given a tensor input, this operation returns a tensor that has the same buffer data as input with datatype type. If the input datatype T is larger than the output datatype type then the shape changes from [...] to [..., sizeof(T)/sizeof(type)]. If T is smaller than type, the operator requires that the rightmost dimension be equal to sizeof(type)/sizeof(T). The shape then goes from [..., sizeof(type)/sizeof(T)] to [...]. tf.bitcast() and tf.cast() work differently when real dtype is casted as a complex dtype (e.g. tf.complex64 or tf.complex128) as tf.cast() make imaginary part 0 while tf.bitcast() gives module error. For example, Example 1: \na = [1., 2., 3.]\nequality_bitcast = tf.bitcast(a, tf.complex128)\nTraceback (most recent call last):\n\nInvalidArgumentError: Cannot bitcast from 1 to 18 [Op:Bitcast]\nequality_cast = tf.cast(a, tf.complex128)\nprint(equality_cast)\ntf.Tensor([1.+0.j 2.+0.j 3.+0.j], shape=(3,), dtype=complex128)\n Example 2: \ntf.bitcast(tf.constant(0xffffffff, dtype=tf.uint32), tf.uint8)\n<tf.Tensor: shape=(4,), dtype=uint8, numpy=array([255, 255, 255, 255], dtype=uint8)>\n Example 3: \nx = [1., 2., 3.]\ny = [0., 2., 3.]\nequality= tf.equal(x,y)\nequality_cast = tf.cast(equality,tf.float32)\nequality_bitcast = tf.bitcast(equality_cast,tf.uint8)\nprint(equality)\ntf.Tensor([False True True], shape=(3,), dtype=bool)\nprint(equality_cast)\ntf.Tensor([0. 1. 1.], shape=(3,), dtype=float32)\nprint(equality_bitcast)\ntf.Tensor(\n    [[  0   0   0   0]\n     [  0   0 128  63]\n     [  0   0 128  63]], shape=(3, 4), dtype=uint8)\n \nNote: Bitcast is implemented as a low-level cast, so machines with different endian orderings will give different results.\n\n \n\n\n Args\n  input   A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int64, int32, uint8, uint16, uint32, uint64, int8, int16, complex64, complex128, qint8, quint8, qint16, quint16, qint32.  \n  type   A tf.DType from: tf.bfloat16, tf.half, tf.float32, tf.float64, tf.int64, tf.int32, tf.uint8, tf.uint16, tf.uint32, tf.uint64, tf.int8, tf.int16, tf.complex64, tf.complex128, tf.qint8, tf.quint8, tf.qint16, tf.quint16, tf.qint32.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type type.  \n  \n"}, {"name": "tf.bitwise", "path": "bitwise", "type": "tf.bitwise", "text": "Module: tf.bitwise Operations for manipulating the binary representations of integers. Functions bitwise_and(...): Elementwise computes the bitwise AND of x and y. bitwise_or(...): Elementwise computes the bitwise OR of x and y. bitwise_xor(...): Elementwise computes the bitwise XOR of x and y. invert(...): Invert (flip) each bit of supported types; for example, type uint8 value 01010101 becomes 10101010. left_shift(...): Elementwise computes the bitwise left-shift of x and y. right_shift(...): Elementwise computes the bitwise right-shift of x and y.  \n"}, {"name": "tf.bitwise.bitwise_and", "path": "bitwise/bitwise_and", "type": "tf.bitwise", "text": "tf.bitwise.bitwise_and Elementwise computes the bitwise AND of x and y.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.bitwise.bitwise_and  \ntf.bitwise.bitwise_and(\n    x, y, name=None\n)\n The result will have those bits set, that are set in both x and y. The computation is performed on the underlying representations of x and y. For example: import tensorflow as tf\nfrom tensorflow.python.ops import bitwise_ops\ndtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,\n              tf.uint8, tf.uint16, tf.uint32, tf.uint64]\n\nfor dtype in dtype_list:\n  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)\n  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)\n  exp = tf.constant([0, 0, 3, 10], dtype=tf.float32)\n\n  res = bitwise_ops.bitwise_and(lhs, rhs)\n  tf.assert_equal(tf.cast(res, tf.float32), exp) # TRUE\n\n \n\n\n Args\n  x   A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64.  \n  y   A Tensor. Must have the same type as x.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as x.  \n  \n"}, {"name": "tf.bitwise.bitwise_or", "path": "bitwise/bitwise_or", "type": "tf.bitwise", "text": "tf.bitwise.bitwise_or Elementwise computes the bitwise OR of x and y.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.bitwise.bitwise_or  \ntf.bitwise.bitwise_or(\n    x, y, name=None\n)\n The result will have those bits set, that are set in x, y or both. The computation is performed on the underlying representations of x and y. For example: import tensorflow as tf\nfrom tensorflow.python.ops import bitwise_ops\ndtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,\n              tf.uint8, tf.uint16, tf.uint32, tf.uint64]\n\nfor dtype in dtype_list:\n  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)\n  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)\n  exp = tf.constant([5, 5, 7, 15], dtype=tf.float32)\n\n  res = bitwise_ops.bitwise_or(lhs, rhs)\n  tf.assert_equal(tf.cast(res,  tf.float32), exp)  # TRUE\n\n \n\n\n Args\n  x   A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64.  \n  y   A Tensor. Must have the same type as x.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as x.  \n  \n"}, {"name": "tf.bitwise.bitwise_xor", "path": "bitwise/bitwise_xor", "type": "tf.bitwise", "text": "tf.bitwise.bitwise_xor Elementwise computes the bitwise XOR of x and y.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.bitwise.bitwise_xor  \ntf.bitwise.bitwise_xor(\n    x, y, name=None\n)\n The result will have those bits set, that are different in x and y. The computation is performed on the underlying representations of x and y. For example: import tensorflow as tf\nfrom tensorflow.python.ops import bitwise_ops\ndtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,\n              tf.uint8, tf.uint16, tf.uint32, tf.uint64]\n\nfor dtype in dtype_list:\n  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)\n  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)\n  exp = tf.constant([5, 5, 4, 5],  dtype=tf.float32)\n\n  res = bitwise_ops.bitwise_xor(lhs, rhs)\n  tf.assert_equal(tf.cast(res, tf.float32), exp) # TRUE\n\n \n\n\n Args\n  x   A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64.  \n  y   A Tensor. Must have the same type as x.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as x.  \n  \n"}, {"name": "tf.bitwise.invert", "path": "bitwise/invert", "type": "tf.bitwise", "text": "tf.bitwise.invert Invert (flip) each bit of supported types; for example, type uint8 value 01010101 becomes 10101010.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.bitwise.invert  \ntf.bitwise.invert(\n    x, name=None\n)\n Flip each bit of supported types. For example, type int8 (decimal 2) binary 00000010 becomes (decimal -3) binary 11111101. This operation is performed on each element of the tensor argument x. Example: import tensorflow as tf\nfrom tensorflow.python.ops import bitwise_ops\n\n# flip 2 (00000010) to -3 (11111101)\ntf.assert_equal(-3, bitwise_ops.invert(2))\n\ndtype_list = [dtypes.int8, dtypes.int16, dtypes.int32, dtypes.int64,\n              dtypes.uint8, dtypes.uint16, dtypes.uint32, dtypes.uint64]\n\ninputs = [0, 5, 3, 14]\nfor dtype in dtype_list:\n  # Because of issues with negative numbers, let's test this indirectly.\n  # 1. invert(a) and a = 0\n  # 2. invert(a) or a = invert(0)\n  input_tensor = tf.constant([0, 5, 3, 14], dtype=dtype)\n  not_a_and_a, not_a_or_a, not_0 = [bitwise_ops.bitwise_and(\n                                      input_tensor, bitwise_ops.invert(input_tensor)),\n                                    bitwise_ops.bitwise_or(\n                                      input_tensor, bitwise_ops.invert(input_tensor)),\n                                    bitwise_ops.invert(\n                                      tf.constant(0, dtype=dtype))]\n\n  expected = tf.constant([0, 0, 0, 0], dtype=tf.float32)\n  tf.assert_equal(tf.cast(not_a_and_a, tf.float32), expected)\n\n  expected = tf.cast([not_0] * 4, tf.float32)\n  tf.assert_equal(tf.cast(not_a_or_a, tf.float32), expected)\n\n  # For unsigned dtypes let's also check the result directly.\n  if dtype.is_unsigned:\n    inverted = bitwise_ops.invert(input_tensor)\n    expected = tf.constant([dtype.max - x for x in inputs], dtype=tf.float32)\n    tf.assert_equal(tf.cast(inverted, tf.float32), tf.cast(expected, tf.float32))\n\n \n\n\n Args\n  x   A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as x.  \n  \n"}, {"name": "tf.bitwise.left_shift", "path": "bitwise/left_shift", "type": "tf.bitwise", "text": "tf.bitwise.left_shift Elementwise computes the bitwise left-shift of x and y.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.bitwise.left_shift  \ntf.bitwise.left_shift(\n    x, y, name=None\n)\n If y is negative, or greater than or equal to the width of x in bits the result is implementation defined. Example: import tensorflow as tf\nfrom tensorflow.python.ops import bitwise_ops\nimport numpy as np\ndtype_list = [tf.int8, tf.int16, tf.int32, tf.int64]\n\nfor dtype in dtype_list:\n  lhs = tf.constant([-1, -5, -3, -14], dtype=dtype)\n  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)\n\n  left_shift_result = bitwise_ops.left_shift(lhs, rhs)\n\n  print(left_shift_result)\n\n# This will print:\n# tf.Tensor([ -32   -5 -128    0], shape=(4,), dtype=int8)\n# tf.Tensor([   -32     -5   -384 -28672], shape=(4,), dtype=int16)\n# tf.Tensor([   -32     -5   -384 -28672], shape=(4,), dtype=int32)\n# tf.Tensor([   -32     -5   -384 -28672], shape=(4,), dtype=int64)\n\nlhs = np.array([-2, 64, 101, 32], dtype=np.int8)\nrhs = np.array([-1, -5, -3, -14], dtype=np.int8)\nbitwise_ops.left_shift(lhs, rhs)\n# <tf.Tensor: shape=(4,), dtype=int8, numpy=array([ -2,  64, 101,  32], dtype=int8)>\n\n \n\n\n Args\n  x   A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64.  \n  y   A Tensor. Must have the same type as x.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as x.  \n  \n"}, {"name": "tf.bitwise.right_shift", "path": "bitwise/right_shift", "type": "tf.bitwise", "text": "tf.bitwise.right_shift Elementwise computes the bitwise right-shift of x and y.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.bitwise.right_shift  \ntf.bitwise.right_shift(\n    x, y, name=None\n)\n Performs a logical shift for unsigned integer types, and an arithmetic shift for signed integer types. If y is negative, or greater than or equal to than the width of x in bits the result is implementation defined. Example: import tensorflow as tf\nfrom tensorflow.python.ops import bitwise_ops\nimport numpy as np\ndtype_list = [tf.int8, tf.int16, tf.int32, tf.int64]\n\nfor dtype in dtype_list:\n  lhs = tf.constant([-1, -5, -3, -14], dtype=dtype)\n  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)\n\n  right_shift_result = bitwise_ops.right_shift(lhs, rhs)\n\n  print(right_shift_result)\n\n# This will print:\n# tf.Tensor([-1 -5 -1 -1], shape=(4,), dtype=int8)\n# tf.Tensor([-1 -5 -1 -1], shape=(4,), dtype=int16)\n# tf.Tensor([-1 -5 -1 -1], shape=(4,), dtype=int32)\n# tf.Tensor([-1 -5 -1 -1], shape=(4,), dtype=int64)\n\nlhs = np.array([-2, 64, 101, 32], dtype=np.int8)\nrhs = np.array([-1, -5, -3, -14], dtype=np.int8)\nbitwise_ops.right_shift(lhs, rhs)\n# <tf.Tensor: shape=(4,), dtype=int8, numpy=array([ -2,  64, 101,  32], dtype=int8)>\n\n \n\n\n Args\n  x   A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64.  \n  y   A Tensor. Must have the same type as x.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as x.  \n  \n"}, {"name": "tf.boolean_mask", "path": "boolean_mask", "type": "tf", "text": "tf.boolean_mask     View source on GitHub    Apply boolean mask to tensor. \ntf.boolean_mask(\n    tensor, mask, axis=None, name='boolean_mask'\n)\n Numpy equivalent is tensor[mask]. In general, 0 < dim(mask) = K <= dim(tensor), and mask's shape must match the first K dimensions of tensor's shape. We then have: boolean_mask(tensor, mask)[i, j1,...,jd] = tensor[i1,...,iK,j1,...,jd] where (i1,...,iK) is the ith True entry of mask (row-major order). The axis could be used with mask to indicate the axis to mask from. In that case, axis + dim(mask) <= dim(tensor) and mask's shape must match the first axis + dim(mask) dimensions of tensor's shape. See also: tf.ragged.boolean_mask, which can be applied to both dense and ragged tensors, and can be used if you need to preserve the masked dimensions of tensor (rather than flattening them, as tf.boolean_mask does). Examples: \ntensor = [0, 1, 2, 3]  # 1-D example\nmask = np.array([True, False, True, False])\ntf.boolean_mask(tensor, mask)\n<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 2], dtype=int32)>\n \ntensor = [[1, 2], [3, 4], [5, 6]] # 2-D example\nmask = np.array([True, False, True])\ntf.boolean_mask(tensor, mask)\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 2],\n       [5, 6]], dtype=int32)>\n\n \n\n\n Args\n  tensor   N-D Tensor.  \n  mask   K-D boolean Tensor, K <= N and K must be known statically.  \n  axis   A 0-D int Tensor representing the axis in tensor to mask from. By default, axis is 0 which will mask from the first dimension. Otherwise K + axis <= N.  \n  name   A name for this operation (optional).   \n \n\n\n Returns   (N-K+1)-dimensional tensor populated by entries in tensor corresponding to True values in mask.  \n\n \n\n\n Raises\n  ValueError   If shapes do not conform.    Examples: # 2-D example\ntensor = [[1, 2], [3, 4], [5, 6]]\nmask = np.array([True, False, True])\nboolean_mask(tensor, mask)  # [[1, 2], [5, 6]]\n  \n"}, {"name": "tf.broadcast_dynamic_shape", "path": "broadcast_dynamic_shape", "type": "tf", "text": "tf.broadcast_dynamic_shape     View source on GitHub    Computes the shape of a broadcast given symbolic shapes.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.broadcast_dynamic_shape  \ntf.broadcast_dynamic_shape(\n    shape_x, shape_y\n)\n When shape_x and shape_y are Tensors representing shapes (i.e. the result of calling tf.shape on another Tensor) this computes a Tensor which is the shape of the result of a broadcasting op applied in tensors of shapes shape_x and shape_y. This is useful when validating the result of a broadcasting operation when the tensors do not have statically known shapes. Example: \nshape_x = (1, 2, 3)\nshape_y = (5, 1, 3)\ntf.broadcast_dynamic_shape(shape_x, shape_y)\n<tf.Tensor: shape=(3,), dtype=int32, numpy=array([5, 2, 3], ...>\n\n \n\n\n Args\n  shape_x   A rank 1 integer Tensor, representing the shape of x.  \n  shape_y   A rank 1 integer Tensor, representing the shape of y.   \n \n\n\n Returns   A rank 1 integer Tensor representing the broadcasted shape.  \n\n \n\n\n Raises\n  InvalidArgumentError   If the two shapes are incompatible for broadcasting.     \n"}, {"name": "tf.broadcast_static_shape", "path": "broadcast_static_shape", "type": "tf", "text": "tf.broadcast_static_shape     View source on GitHub    Computes the shape of a broadcast given known shapes.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.broadcast_static_shape  \ntf.broadcast_static_shape(\n    shape_x, shape_y\n)\n When shape_x and shape_y are fully known TensorShapes this computes a TensorShape which is the shape of the result of a broadcasting op applied in tensors of shapes shape_x and shape_y. For example, if shape_x is TensorShape([1, 2, 3]) and shape_y is TensorShape([5, 1, 3]), the result is a TensorShape whose value is TensorShape([5, 2, 3]). This is useful when validating the result of a broadcasting operation when the tensors have statically known shapes. Example: \nshape_x = tf.TensorShape([1, 2, 3])\nshape_y = tf.TensorShape([5, 1 ,3])\ntf.broadcast_static_shape(shape_x, shape_y)\nTensorShape([5, 2, 3])\n\n \n\n\n Args\n  shape_x   A TensorShape  \n  shape_y   A TensorShape   \n \n\n\n Returns   A TensorShape representing the broadcasted shape.  \n\n \n\n\n Raises\n  ValueError   If the two shapes can not be broadcasted.     \n"}, {"name": "tf.broadcast_to", "path": "broadcast_to", "type": "tf", "text": "tf.broadcast_to Broadcast an array for a compatible shape.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.broadcast_to  \ntf.broadcast_to(\n    input, shape, name=None\n)\n Broadcasting is the process of making arrays to have compatible shapes for arithmetic operations. Two shapes are compatible if for each dimension pair they are either equal or one of them is one. When trying to broadcast a Tensor to a shape, it starts with the trailing dimensions, and works its way forward. For example, \nx = tf.constant([1, 2, 3])\ny = tf.broadcast_to(x, [3, 3])\nprint(y)\ntf.Tensor(\n    [[1 2 3]\n     [1 2 3]\n     [1 2 3]], shape=(3, 3), dtype=int32)\n In the above example, the input Tensor with the shape of [1, 3] is broadcasted to output Tensor with shape of [3, 3]. When doing broadcasted operations such as multiplying a tensor by a scalar, broadcasting (usually) confers some time or space benefit, as the broadcasted tensor is never materialized. However, broadcast_to does not carry with it any such benefits. The newly-created tensor takes the full memory of the broadcasted shape. (In a graph context, broadcast_to might be fused to subsequent operation and then be optimized away, however.)\n \n\n\n Args\n  input   A Tensor. A Tensor to broadcast.  \n  shape   A Tensor. Must be one of the following types: int32, int64. An 1-D int Tensor. The shape of the desired output.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as input.  \n  \n"}, {"name": "tf.case", "path": "case", "type": "tf", "text": "tf.case     View source on GitHub    Create a case operation. \ntf.case(\n    pred_fn_pairs, default=None, exclusive=False, strict=False,\n    name='case'\n)\n See also tf.switch_case. The pred_fn_pairs parameter is a list of pairs of size N. Each pair contains a boolean scalar tensor and a python callable that creates the tensors to be returned if the boolean evaluates to True. default is a callable generating a list of tensors. All the callables in pred_fn_pairs as well as default (if provided) should return the same number and types of tensors. If exclusive==True, all predicates are evaluated, and an exception is thrown if more than one of the predicates evaluates to True. If exclusive==False, execution stops at the first predicate which evaluates to True, and the tensors generated by the corresponding function are returned immediately. If none of the predicates evaluate to True, this operation returns the tensors generated by default. tf.case supports nested structures as implemented in tf.contrib.framework.nest. All of the callables must return the same (possibly nested) value structure of lists, tuples, and/or named tuples. Singleton lists and tuples form the only exceptions to this: when returned by a callable, they are implicitly unpacked to single values. This behavior is disabled by passing strict=True. Example 1: Pseudocode: if (x < y) return 17;\nelse return 23;\n Expressions: f1 = lambda: tf.constant(17)\nf2 = lambda: tf.constant(23)\nr = tf.case([(tf.less(x, y), f1)], default=f2)\n Example 2: Pseudocode: if (x < y && x > z) raise OpError(\"Only one predicate may evaluate to True\");\nif (x < y) return 17;\nelse if (x > z) return 23;\nelse return -1;\n Expressions: def f1(): return tf.constant(17)\ndef f2(): return tf.constant(23)\ndef f3(): return tf.constant(-1)\nr = tf.case([(tf.less(x, y), f1), (tf.greater(x, z), f2)],\n         default=f3, exclusive=True)\n\n \n\n\n Args\n  pred_fn_pairs   List of pairs of a boolean scalar tensor and a callable which returns a list of tensors.  \n  default   Optional callable that returns a list of tensors.  \n  exclusive   True iff at most one predicate is allowed to evaluate to True.  \n  strict   A boolean that enables/disables 'strict' mode; see above.  \n  name   A name for this operation (optional).   \n \n\n\n Returns   The tensors returned by the first pair whose predicate evaluated to True, or those returned by default if none does.  \n\n \n\n\n Raises\n  TypeError   If pred_fn_pairs is not a list/tuple.  \n  TypeError   If pred_fn_pairs is a list but does not contain 2-tuples.  \n  TypeError   If fns[i] is not callable for any i, or default is not callable.    V2 Compatibility pred_fn_pairs could be a dictionary in v1. However, tf.Tensor and tf.Variable are no longer hashable in v2, so cannot be used as a key for a dictionary. Please use a list or a tuple instead.  \n"}, {"name": "tf.cast", "path": "cast", "type": "tf", "text": "tf.cast     View source on GitHub    Casts a tensor to a new type.  View aliases  Main aliases \ntf.dtypes.cast Compat aliases for migration See Migration guide for more details. tf.compat.v1.cast, tf.compat.v1.dtypes.cast  \ntf.cast(\n    x, dtype, name=None\n)\n The operation casts x (in case of Tensor) or x.values (in case of SparseTensor or IndexedSlices) to dtype. For example: \nx = tf.constant([1.8, 2.2], dtype=tf.float32)\ntf.dtypes.cast(x, tf.int32)\n<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>\n The operation supports data types (for x and dtype) of uint8, uint16, uint32, uint64, int8, int16, int32, int64, float16, float32, float64, complex64, complex128, bfloat16. In case of casting from complex types (complex64, complex128) to real types, only the real part of x is returned. In case of casting from real types to complex types (complex64, complex128), the imaginary part of the returned value is set to 0. The handling of complex types here matches the behavior of numpy. Note casting nan and inf values to integral types has undefined behavior.\n \n\n\n Args\n  x   A Tensor or SparseTensor or IndexedSlices of numeric type. It could be uint8, uint16, uint32, uint64, int8, int16, int32, int64, float16, float32, float64, complex64, complex128, bfloat16.  \n  dtype   The destination type. The list of supported dtypes is the same as x.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor or SparseTensor or IndexedSlices with same shape as x and same type as dtype.  \n\n \n\n\n Raises\n  TypeError   If x cannot be cast to the dtype.     \n"}, {"name": "tf.clip_by_global_norm", "path": "clip_by_global_norm", "type": "tf", "text": "tf.clip_by_global_norm     View source on GitHub    Clips values of multiple tensors by the ratio of the sum of their norms.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.clip_by_global_norm  \ntf.clip_by_global_norm(\n    t_list, clip_norm, use_norm=None, name=None\n)\n Given a tuple or list of tensors t_list, and a clipping ratio clip_norm, this operation returns a list of clipped tensors list_clipped and the global norm (global_norm) of all tensors in t_list. Optionally, if you've already computed the global norm for t_list, you can specify the global norm with use_norm. To perform the clipping, the values t_list[i] are set to: t_list[i] * clip_norm / max(global_norm, clip_norm)\n where: global_norm = sqrt(sum([l2norm(t)**2 for t in t_list]))\n If clip_norm > global_norm then the entries in t_list remain as they are, otherwise they're all shrunk by the global ratio. If global_norm == infinity then the entries in t_list are all set to NaN to signal that an error occurred. Any of the entries of t_list that are of type None are ignored. This is the correct way to perform gradient clipping (Pascanu et al., 2012). However, it is slower than clip_by_norm() because all the parameters must be ready before the clipping operation can be performed.\n \n\n\n Args\n  t_list   A tuple or list of mixed Tensors, IndexedSlices, or None.  \n  clip_norm   A 0-D (scalar) Tensor > 0. The clipping ratio.  \n  use_norm   A 0-D (scalar) Tensor of type float (optional). The global norm to use. If not provided, global_norm() is used to compute the norm.  \n  name   A name for the operation (optional).   \n \n\n\n Returns\n  list_clipped   A list of Tensors of the same type as list_t.  \n  global_norm   A 0-D (scalar) Tensor representing the global norm.   \n \n\n\n Raises\n  TypeError   If t_list is not a sequence.    References: On the difficulty of training Recurrent Neural Networks: Pascanu et al., 2012 (pdf)  \n"}, {"name": "tf.clip_by_norm", "path": "clip_by_norm", "type": "tf", "text": "tf.clip_by_norm     View source on GitHub    Clips tensor values to a maximum L2-norm.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.clip_by_norm  \ntf.clip_by_norm(\n    t, clip_norm, axes=None, name=None\n)\n Given a tensor t, and a maximum clip value clip_norm, this operation normalizes t so that its L2-norm is less than or equal to clip_norm, along the dimensions given in axes. Specifically, in the default case where all dimensions are used for calculation, if the L2-norm of t is already less than or equal to clip_norm, then t is not modified. If the L2-norm is greater than clip_norm, then this operation returns a tensor of the same type and shape as t with its values set to: t * clip_norm / l2norm(t) In this case, the L2-norm of the output tensor is clip_norm. As another example, if t is a matrix and axes == [1], then each row of the output will have L2-norm less than or equal to clip_norm. If axes == [0] instead, each column of the output will be clipped. Code example: \nsome_nums = tf.constant([[1, 2, 3, 4, 5]], dtype=tf.float32)\ntf.clip_by_norm(some_nums, 2.0).numpy()\narray([[0.26967996, 0.5393599 , 0.80903983, 1.0787199 , 1.3483998 ]],\n      dtype=float32)\n This operation is typically used to clip gradients before applying them with an optimizer. Most gradient data is a collection of different shaped tensors for different parts of the model. Thus, this is a common usage: # Get your gradients after training\nloss_value, grads = grad(model, features, labels)\n\n# Apply some clipping\ngrads = [tf.clip_by_norm(g, norm)\n             for g in grads]\n\n# Continue on with training\noptimizer.apply_gradients(grads)\n\n \n\n\n Args\n  t   A Tensor or IndexedSlices. This must be a floating point type.  \n  clip_norm   A 0-D (scalar) Tensor > 0. A maximum clipping value, also floating point  \n  axes   A 1-D (vector) Tensor of type int32 containing the dimensions to use for computing the L2-norm. If None (the default), uses all dimensions.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A clipped Tensor or IndexedSlices.  \n\n \n\n\n Raises\n  ValueError   If the clip_norm tensor is not a 0-D scalar tensor.  \n  TypeError   If dtype of the input is not a floating point or complex type.     \n"}, {"name": "tf.clip_by_value", "path": "clip_by_value", "type": "tf", "text": "tf.clip_by_value     View source on GitHub    Clips tensor values to a specified min and max.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.clip_by_value  \ntf.clip_by_value(\n    t, clip_value_min, clip_value_max, name=None\n)\n Given a tensor t, this operation returns a tensor of the same type and shape as t with its values clipped to clip_value_min and clip_value_max. Any values less than clip_value_min are set to clip_value_min. Any values greater than clip_value_max are set to clip_value_max. \nNote: clip_value_min needs to be smaller or equal to clip_value_max for correct results.\n For example: Basic usage passes a scalar as the min and max value. \nt = tf.constant([[-10., -1., 0.], [0., 2., 10.]])\nt2 = tf.clip_by_value(t, clip_value_min=-1, clip_value_max=1)\nt2.numpy()\narray([[-1., -1.,  0.],\n       [ 0.,  1.,  1.]], dtype=float32)\n The min and max can be the same size as t, or broadcastable to that size. \nt = tf.constant([[-1, 0., 10.], [-1, 0, 10]])\nclip_min = [[2],[1]]\nt3 = tf.clip_by_value(t, clip_value_min=clip_min, clip_value_max=100)\nt3.numpy()\narray([[ 2.,  2., 10.],\n       [ 1.,  1., 10.]], dtype=float32)\n Broadcasting fails, intentionally, if you would expand the dimensions of t \nt = tf.constant([[-1, 0., 10.], [-1, 0, 10]])\nclip_min = [[[2, 1]]] # Has a third axis\nt4 = tf.clip_by_value(t, clip_value_min=clip_min, clip_value_max=100)\nTraceback (most recent call last):\n\nInvalidArgumentError: Incompatible shapes: [2,3] vs. [1,1,2]\n It throws a TypeError if you try to clip an int to a float value (tf.cast the input to float first). \nt = tf.constant([[1, 2], [3, 4]], dtype=tf.int32)\nt5 = tf.clip_by_value(t, clip_value_min=-3.1, clip_value_max=3.1)\nTraceback (most recent call last):\n\nTypeError: Cannot convert ...\n\n \n\n\n Args\n  t   A Tensor or IndexedSlices.  \n  clip_value_min   The minimum value to clip to. A scalar Tensor or one that is broadcastable to the shape of t.  \n  clip_value_max   The maximum value to clip to. A scalar Tensor or one that is broadcastable to the shape of t.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A clipped Tensor or IndexedSlices.  \n\n \n\n\n Raises   tf.errors.InvalidArgumentError: If the clip tensors would trigger array broadcasting that would make the returned tensor larger than the input.     TypeError   If dtype of the input is int32 and dtype of the clip_value_min or clip_value_max is float32     \n"}, {"name": "tf.compat", "path": "compat", "type": "tf.compat", "text": "Module: tf.compat Compatibility functions. The tf.compat module contains two sets of compatibility functions. Tensorflow 1.x and 2.x APIs The compat.v1 and compat.v2 submodules provide a complete copy of both the v1 and v2 APIs for backwards and forwards compatibility across TensorFlow versions 1.x and 2.x. See the migration guide for details. Utilities for writing compatible code Aside from the compat.v1 and compat.v2 submodules, tf.compat also contains a set of helper functions for writing code that works in both:  TensorFlow 1.x and 2.x Python 2 and 3  Type collections The compatibility module also provides the following aliases for common sets of python types:  bytes_or_text_types complex_types integral_types real_types  Modules v1 module: Bring in all of the public TensorFlow interface into this module. Functions as_bytes(...): Converts bytearray, bytes, or unicode python input types to bytes. as_str(...) as_str_any(...): Converts input to str type. as_text(...): Converts any string-like python input types to unicode. dimension_at_index(...): Compatibility utility required to allow for both V1 and V2 behavior in TF. dimension_value(...): Compatibility utility required to allow for both V1 and V2 behavior in TF. forward_compatibility_horizon(...): Context manager for testing forward compatibility of generated graphs. forward_compatible(...): Return true if the forward compatibility window has expired. path_to_str(...): Converts input which is a PathLike object to str type.\n \n\n\n Other Members\n  bytes_or_text_types  \n \n  complex_types  \n \n  integral_types  \n \n  real_types  \n    \n"}, {"name": "tf.compat.as_bytes", "path": "compat/as_bytes", "type": "tf.compat", "text": "tf.compat.as_bytes     View source on GitHub    Converts bytearray, bytes, or unicode python input types to bytes.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.compat.as_bytes  \ntf.compat.as_bytes(\n    bytes_or_text, encoding='utf-8'\n)\n Uses utf-8 encoding for text by default.\n \n\n\n Args\n  bytes_or_text   A bytearray, bytes, str, or unicode object.  \n  encoding   A string indicating the charset for encoding unicode.   \n \n\n\n Returns   A bytes object.  \n\n \n\n\n Raises\n  TypeError   If bytes_or_text is not a binary or unicode string.     \n"}, {"name": "tf.compat.as_str", "path": "compat/as_str", "type": "tf.compat", "text": "tf.compat.as_str  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.compat.as_str  \ntf.compat.as_str(\n    bytes_or_text, encoding='utf-8'\n)\n  \n"}, {"name": "tf.compat.as_str_any", "path": "compat/as_str_any", "type": "tf.compat", "text": "tf.compat.as_str_any     View source on GitHub    Converts input to str type.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.compat.as_str_any  \ntf.compat.as_str_any(\n    value\n)\n Uses str(value), except for bytes typed inputs, which are converted using as_str.\n \n\n\n Args\n  value   A object that can be converted to str.   \n \n\n\n Returns   A str object.  \n  \n"}, {"name": "tf.compat.as_text", "path": "compat/as_text", "type": "tf.compat", "text": "tf.compat.as_text     View source on GitHub    Converts any string-like python input types to unicode.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.compat.as_text  \ntf.compat.as_text(\n    bytes_or_text, encoding='utf-8'\n)\n Returns the input as a unicode string. Uses utf-8 encoding for text by default.\n \n\n\n Args\n  bytes_or_text   A bytes, str, or unicode object.  \n  encoding   A string indicating the charset for decoding unicode.   \n \n\n\n Returns   A unicode (Python 2) or str (Python 3) object.  \n\n \n\n\n Raises\n  TypeError   If bytes_or_text is not a binary or unicode string.     \n"}, {"name": "tf.compat.dimension_at_index", "path": "compat/dimension_at_index", "type": "tf.compat", "text": "tf.compat.dimension_at_index     View source on GitHub    Compatibility utility required to allow for both V1 and V2 behavior in TF.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.compat.dimension_at_index, tf.compat.v1.dimension_at_index  \ntf.compat.dimension_at_index(\n    shape, index\n)\n Until the release of TF 2.0, we need the legacy behavior of TensorShape to coexist with the new behavior. This utility is a bridge between the two. If you want to retrieve the Dimension instance corresponding to a certain index in a TensorShape instance, use this utility, like this: # If you had this in your V1 code:\ndim = tensor_shape[i]\n\n# Use `dimension_at_index` as direct replacement compatible with both V1 & V2:\ndim = dimension_at_index(tensor_shape, i)\n\n# Another possibility would be this, but WARNING: it only works if the\n# tensor_shape instance has a defined rank.\ndim = tensor_shape.dims[i]  # `dims` may be None if the rank is undefined!\n\n# In native V2 code, we recommend instead being more explicit:\nif tensor_shape.rank is None:\n  dim = Dimension(None)\nelse:\n  dim = tensor_shape.dims[i]\n\n# Being more explicit will save you from the following trap (present in V1):\n# you might do in-place modifications to `dim` and expect them to be reflected\n# in `tensor_shape[i]`, but they would not be (as the Dimension object was\n# instantiated on the fly.\n\n \n\n\n Arguments\n  shape   A TensorShape instance.  \n  index   An integer index.   \n \n\n\n Returns   A dimension object.  \n  \n"}, {"name": "tf.compat.dimension_value", "path": "compat/dimension_value", "type": "tf.compat", "text": "tf.compat.dimension_value     View source on GitHub    Compatibility utility required to allow for both V1 and V2 behavior in TF.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.compat.dimension_value, tf.compat.v1.dimension_value  \ntf.compat.dimension_value(\n    dimension\n)\n Until the release of TF 2.0, we need the legacy behavior of TensorShape to coexist with the new behavior. This utility is a bridge between the two. When accessing the value of a TensorShape dimension, use this utility, like this: # If you had this in your V1 code:\nvalue = tensor_shape[i].value\n\n# Use `dimension_value` as direct replacement compatible with both V1 & V2:\nvalue = dimension_value(tensor_shape[i])\n\n# This would be the V2 equivalent:\nvalue = tensor_shape[i]  # Warning: this will return the dim value in V2!\n\n \n\n\n Arguments\n  dimension   Either a Dimension instance, an integer, or None.   \n \n\n\n Returns   A plain value, i.e. an integer or None.  \n  \n"}, {"name": "tf.compat.forward_compatibility_horizon", "path": "compat/forward_compatibility_horizon", "type": "tf.compat", "text": "tf.compat.forward_compatibility_horizon     View source on GitHub    Context manager for testing forward compatibility of generated graphs.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.compat.forward_compatibility_horizon  \n@tf_contextlib.contextmanager\ntf.compat.forward_compatibility_horizon(\n    year, month, day\n)\n See Version compatibility. To ensure forward compatibility of generated graphs (see forward_compatible) with older binaries, new features can be gated with: if compat.forward_compatible(year=2018, month=08, date=01):\n  generate_graph_with_new_features()\nelse:\n  generate_graph_so_older_binaries_can_consume_it()\n However, when adding new features, one may want to unittest it before the forward compatibility window expires. This context manager enables such tests. For example: from tensorflow.python.compat import compat\n\ndef testMyNewFeature(self):\n  with compat.forward_compatibility_horizon(2018, 08, 02):\n     # Test that generate_graph_with_new_features() has an effect\n\n \n\n\n Args\n  year   A year (e.g., 2018). Must be an int.  \n  month   A month (1 <= month <= 12) in year. Must be an int.  \n  day   A day (1 <= day <= 31, or 30, or 29, or 28) in month. Must be an int.   \n \n\n\n Yields   Nothing.  \n  \n"}, {"name": "tf.compat.forward_compatible", "path": "compat/forward_compatible", "type": "tf.compat", "text": "tf.compat.forward_compatible     View source on GitHub    Return true if the forward compatibility window has expired.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.compat.forward_compatible  \ntf.compat.forward_compatible(\n    year, month, day\n)\n See Version compatibility. Forward-compatibility refers to scenarios where the producer of a TensorFlow model (a GraphDef or SavedModel) is compiled against a version of the TensorFlow library newer than what the consumer was compiled against. The \"producer\" is typically a Python program that constructs and trains a model while the \"consumer\" is typically another program that loads and serves the model. TensorFlow has been supporting a 3 week forward-compatibility window for programs compiled from source at HEAD. For example, consider the case where a new operation MyNewAwesomeAdd is created with the intent of replacing the implementation of an existing Python wrapper - tf.add. The Python wrapper implementation should change from something like: def add(inputs, name=None):\n  return gen_math_ops.add(inputs, name)\n to: from tensorflow.python.compat import compat\n\ndef add(inputs, name=None):\n  if compat.forward_compatible(year, month, day):\n    # Can use the awesome new implementation.\n    return gen_math_ops.my_new_awesome_add(inputs, name)\n  # To maintain forward compatibility, use the old implementation.\n  return gen_math_ops.add(inputs, name)\n Where year, month, and day specify the date beyond which binaries that consume a model are expected to have been updated to include the new operations. This date is typically at least 3 weeks beyond the date the code that adds the new operation is committed.\n \n\n\n Args\n  year   A year (e.g., 2018). Must be an int.  \n  month   A month (1 <= month <= 12) in year. Must be an int.  \n  day   A day (1 <= day <= 31, or 30, or 29, or 28) in month. Must be an int.   \n \n\n\n Returns   True if the caller can expect that serialized TensorFlow graphs produced can be consumed by programs that are compiled with the TensorFlow library source code after (year, month, day).  \n  \n"}, {"name": "tf.compat.path_to_str", "path": "compat/path_to_str", "type": "tf.compat", "text": "tf.compat.path_to_str       View source on GitHub    Converts input which is a PathLike object to str type.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.compat.path_to_str  \ntf.compat.path_to_str(\n    path\n)\n Converts from any python constant representation of a PathLike object to a string. If the input is not a PathLike object, simply returns the input.\n \n\n\n Args\n  path   An object that can be converted to path representation.   \n \n\n\n Returns   A str object.  \n Usage: In case a simplified str version of the path is needed from an os.PathLike object Examples: $ tf.compat.path_to_str('C:\\XYZ\\tensorflow\\./.././tensorflow')\n'C:\\XYZ\\tensorflow\\./.././tensorflow' # Windows OS\n$ tf.compat.path_to_str(Path('C:\\XYZ\\tensorflow\\./.././tensorflow'))\n'C:\\XYZ\\tensorflow\\..\\tensorflow' # Windows OS\n$ tf.compat.path_to_str(Path('./corpus'))\n'corpus' # Linux OS\n$ tf.compat.path_to_str('./.././Corpus')\n'./.././Corpus' # Linux OS\n$ tf.compat.path_to_str(Path('./.././Corpus'))\n'../Corpus' # Linux OS\n$ tf.compat.path_to_str(Path('./..////../'))\n'../..' # Linux OS\n\n  \n"}, {"name": "tf.compat.v1", "path": "compat/v1", "type": "tf.compat", "text": "Module: tf.compat.v1       View source on GitHub    Bring in all of the public TensorFlow interface into this module. Modules app module: Generic entry point script. audio module: Public API for tf.audio namespace. autograph module: Conversion of plain Python into TensorFlow graph code. bitwise module: Operations for manipulating the binary representations of integers. compat module: Compatibility functions. config module: Public API for tf.config namespace. data module: tf.data.Dataset API for input pipelines. debugging module: Public API for tf.debugging namespace. distribute module: Library for running a computation across multiple devices. distributions module: Core module for TensorFlow distribution objects and helpers. dtypes module: Public API for tf.dtypes namespace. errors module: Exception types for TensorFlow errors. estimator module: Estimator: High level tools for working with models. experimental module: Public API for tf.experimental namespace. feature_column module: Public API for tf.feature_column namespace. flags module: Import router for absl.flags. See https://github.com/abseil/abseil-py gfile module: Import router for file_io. graph_util module: Helpers to manipulate a tensor graph in python. image module: Image ops. initializers module: Public API for tf.initializers namespace. io module: Public API for tf.io namespace. keras module: Implementation of the Keras API meant to be a high-level API for TensorFlow. layers module: Public API for tf.layers namespace. linalg module: Operations for linear algebra. lite module: Public API for tf.lite namespace. logging module: Logging and Summary Operations. lookup module: Public API for tf.lookup namespace. losses module: Loss operations for use in neural networks. manip module: Operators for manipulating tensors. math module: Math Operations. metrics module: Evaluation-related metrics. mixed_precision module: Public API for tf.mixed_precision namespace. mlir module: Public API for tf.mlir namespace. nest module: Public API for tf.nest namespace. nn module: Wrappers for primitive Neural Net (NN) Operations. profiler module: Public API for tf.profiler namespace. python_io module: Python functions for directly manipulating TFRecord-formatted files. quantization module: Public API for tf.quantization namespace. queue module: Public API for tf.queue namespace. ragged module: Ragged Tensors. random module: Public API for tf.random namespace. raw_ops module: Public API for tf.raw_ops namespace. resource_loader module: Resource management library. saved_model module: Public API for tf.saved_model namespace. sets module: Tensorflow set operations. signal module: Signal processing operations. sparse module: Sparse Tensor Representation. spectral module: Public API for tf.spectral namespace. strings module: Operations for working with string Tensors. summary module: Operations for writing summary data, for use in analysis and visualization. sysconfig module: System configuration library. test module: Testing. tpu module: Ops related to Tensor Processing Units. train module: Support for training models. types module: Public TensorFlow type definitions. user_ops module: Public API for tf.user_ops namespace. version module: Public API for tf.version namespace. xla module: Public API for tf.xla namespace. Classes class AggregationMethod: A class listing aggregation methods used to combine gradients. class AttrValue: A ProtocolMessage class ConditionalAccumulator: A conditional accumulator for aggregating gradients. class ConditionalAccumulatorBase: A conditional accumulator for aggregating gradients. class ConfigProto: A ProtocolMessage class CriticalSection: Critical section. class DType: Represents the type of the elements in a Tensor. class DeviceSpec: Represents a (possibly partial) specification for a TensorFlow device. class Dimension: Represents the value of one dimension in a TensorShape. class Event: A ProtocolMessage class FIFOQueue: A queue implementation that dequeues elements in first-in first-out order. class FixedLenFeature: Configuration for parsing a fixed-length input feature. class FixedLenSequenceFeature: Configuration for parsing a variable-length input feature into a Tensor. class FixedLengthRecordReader: A Reader that outputs fixed-length records from a file. class GPUOptions: A ProtocolMessage class GradientTape: Record operations for automatic differentiation. class Graph: A TensorFlow computation, represented as a dataflow graph. class GraphDef: A ProtocolMessage class GraphKeys: Standard names to use for graph collections. class GraphOptions: A ProtocolMessage class HistogramProto: A ProtocolMessage class IdentityReader: A Reader that outputs the queued work as both the key and value. class IndexedSlices: A sparse representation of a set of tensor slices at given indices. class IndexedSlicesSpec: Type specification for a tf.IndexedSlices. class InteractiveSession: A TensorFlow Session for use in interactive contexts, such as a shell. class LMDBReader: A Reader that outputs the records from a LMDB file. class LogMessage: A ProtocolMessage class MetaGraphDef: A ProtocolMessage class Module: Base neural network module class. class NameAttrList: A ProtocolMessage class NodeDef: A ProtocolMessage class OpError: A generic error that is raised when TensorFlow execution fails. class Operation: Represents a graph node that performs computation on tensors. class OptimizerOptions: A ProtocolMessage class OptionalSpec: Type specification for tf.experimental.Optional. class PaddingFIFOQueue: A FIFOQueue that supports batching variable-sized tensors by padding. class PriorityQueue: A queue implementation that dequeues elements in prioritized order. class QueueBase: Base class for queue implementations. class RaggedTensor: Represents a ragged tensor. class RaggedTensorSpec: Type specification for a tf.RaggedTensor. class RandomShuffleQueue: A queue implementation that dequeues elements in a random order. class ReaderBase: Base class for different Reader types, that produce a record every step. class RegisterGradient: A decorator for registering the gradient function for an op type. class RunMetadata: A ProtocolMessage class RunOptions: A ProtocolMessage class Session: A class for running TensorFlow operations. class SessionLog: A ProtocolMessage class SparseConditionalAccumulator: A conditional accumulator for aggregating sparse gradients. class SparseFeature: Configuration for parsing a sparse input feature from an Example. class SparseTensor: Represents a sparse tensor. class SparseTensorSpec: Type specification for a tf.sparse.SparseTensor. class SparseTensorValue: SparseTensorValue(indices, values, dense_shape) class Summary: A ProtocolMessage class SummaryMetadata: A ProtocolMessage class TFRecordReader: A Reader that outputs the records from a TFRecords file. class Tensor: A tensor is a multidimensional array of elements represented by a class TensorArray: Class wrapping dynamic-sized, per-time-step, write-once Tensor arrays. class TensorArraySpec: Type specification for a tf.TensorArray. class TensorInfo: A ProtocolMessage class TensorShape: Represents the shape of a Tensor. class TensorSpec: Describes a tf.Tensor. class TextLineReader: A Reader that outputs the lines of a file delimited by newlines. class TypeSpec: Specifies a TensorFlow value type. class UnconnectedGradients: Controls how gradient computation behaves when y does not depend on x. class VarLenFeature: Configuration for parsing a variable-length input feature. class Variable: See the Variables Guide. class VariableAggregation: Indicates how a distributed variable will be aggregated. class VariableScope: Variable scope object to carry defaults to provide to get_variable. class VariableSynchronization: Indicates when a distributed variable will be synced. class WholeFileReader: A Reader that outputs the entire contents of a file as a value. class constant_initializer: Initializer that generates tensors with constant values. class glorot_normal_initializer: The Glorot normal initializer, also called Xavier normal initializer. class glorot_uniform_initializer: The Glorot uniform initializer, also called Xavier uniform initializer. class name_scope: A context manager for use when defining a Python op. class ones_initializer: Initializer that generates tensors initialized to 1. class orthogonal_initializer: Initializer that generates an orthogonal matrix. class random_normal_initializer: Initializer that generates tensors with a normal distribution. class random_uniform_initializer: Initializer that generates tensors with a uniform distribution. class truncated_normal_initializer: Initializer that generates a truncated normal distribution. class uniform_unit_scaling_initializer: Initializer that generates tensors without scaling variance. class variable_scope: A context manager for defining ops that creates variables (layers). class variance_scaling_initializer: Initializer capable of adapting its scale to the shape of weights tensors. class zeros_initializer: Initializer that generates tensors initialized to 0. Functions Assert(...): Asserts that the given condition is true. NoGradient(...): Specifies that ops of type op_type is not differentiable. NotDifferentiable(...): Specifies that ops of type op_type is not differentiable. Print(...): Prints a list of tensors. (deprecated) abs(...): Computes the absolute value of a tensor. accumulate_n(...): Returns the element-wise sum of a list of tensors. acos(...): Computes acos of x element-wise. acosh(...): Computes inverse hyperbolic cosine of x element-wise. add(...): Returns x + y element-wise. add_check_numerics_ops(...): Connect a tf.debugging.check_numerics to every floating point tensor. add_n(...): Adds all input tensors element-wise. add_to_collection(...): Wrapper for Graph.add_to_collection() using the default graph. add_to_collections(...): Wrapper for Graph.add_to_collections() using the default graph. all_variables(...): Use tf.compat.v1.global_variables instead. (deprecated) angle(...): Returns the element-wise argument of a complex (or real) tensor. arg_max(...): Returns the index with the largest value across dimensions of a tensor. arg_min(...): Returns the index with the smallest value across dimensions of a tensor. argmax(...): Returns the index with the largest value across axes of a tensor. (deprecated arguments) argmin(...): Returns the index with the smallest value across axes of a tensor. (deprecated arguments) argsort(...): Returns the indices of a tensor that give its sorted order along an axis. as_dtype(...): Converts the given type_value to a DType. as_string(...): Converts each entry in the given tensor to strings. asin(...): Computes the trignometric inverse sine of x element-wise. asinh(...): Computes inverse hyperbolic sine of x element-wise. assert_equal(...): Assert the condition x == y holds element-wise. assert_greater(...): Assert the condition x > y holds element-wise. assert_greater_equal(...): Assert the condition x >= y holds element-wise. assert_integer(...): Assert that x is of integer dtype. assert_less(...): Assert the condition x < y holds element-wise. assert_less_equal(...): Assert the condition x <= y holds element-wise. assert_near(...): Assert the condition x and y are close element-wise. assert_negative(...): Assert the condition x < 0 holds element-wise. assert_non_negative(...): Assert the condition x >= 0 holds element-wise. assert_non_positive(...): Assert the condition x <= 0 holds element-wise. assert_none_equal(...): Assert the condition x != y holds element-wise. assert_positive(...): Assert the condition x > 0 holds element-wise. assert_proper_iterable(...): Static assert that values is a \"proper\" iterable. assert_rank(...): Assert x has rank equal to rank. assert_rank_at_least(...): Assert x has rank equal to rank or higher. assert_rank_in(...): Assert x has rank in ranks. assert_same_float_dtype(...): Validate and return float type based on tensors and dtype. assert_scalar(...): Asserts that the given tensor is a scalar (i.e. zero-dimensional). assert_type(...): Statically asserts that the given Tensor is of the specified type. assert_variables_initialized(...): Returns an Op to check if variables are initialized. assign(...): Update ref by assigning value to it. assign_add(...): Update ref by adding value to it. assign_sub(...): Update ref by subtracting value from it. atan(...): Computes the trignometric inverse tangent of x element-wise. atan2(...): Computes arctangent of y/x element-wise, respecting signs of the arguments. atanh(...): Computes inverse hyperbolic tangent of x element-wise. batch_gather(...): Gather slices from params according to indices with leading batch dims. (deprecated) batch_scatter_update(...): Generalization of tf.compat.v1.scatter_update to axis different than 0. (deprecated) batch_to_space(...): BatchToSpace for 4-D tensors of type T. batch_to_space_nd(...): BatchToSpace for N-D tensors of type T. betainc(...): Compute the regularized incomplete beta integral \\(I_x(a, b)\\). bincount(...): Counts the number of occurrences of each value in an integer array. bitcast(...): Bitcasts a tensor from one type to another without copying data. boolean_mask(...): Apply boolean mask to tensor. broadcast_dynamic_shape(...): Computes the shape of a broadcast given symbolic shapes. broadcast_static_shape(...): Computes the shape of a broadcast given known shapes. broadcast_to(...): Broadcast an array for a compatible shape. case(...): Create a case operation. cast(...): Casts a tensor to a new type. ceil(...): Return the ceiling of the input, element-wise. check_numerics(...): Checks a tensor for NaN and Inf values. cholesky(...): Computes the Cholesky decomposition of one or more square matrices. cholesky_solve(...): Solves systems of linear eqns A X = RHS, given Cholesky factorizations. clip_by_average_norm(...): Clips tensor values to a maximum average L2-norm. (deprecated) clip_by_global_norm(...): Clips values of multiple tensors by the ratio of the sum of their norms. clip_by_norm(...): Clips tensor values to a maximum L2-norm. clip_by_value(...): Clips tensor values to a specified min and max. colocate_with(...): DEPRECATED FUNCTION complex(...): Converts two real numbers to a complex number. concat(...): Concatenates tensors along one dimension. cond(...): Return true_fn() if the predicate pred is true else false_fn(). (deprecated arguments) confusion_matrix(...): Computes the confusion matrix from predictions and labels. conj(...): Returns the complex conjugate of a complex number. constant(...): Creates a constant tensor. container(...): Wrapper for Graph.container() using the default graph. control_dependencies(...): Wrapper for Graph.control_dependencies() using the default graph. control_flow_v2_enabled(...): Returns True if v2 control flow is enabled. convert_to_tensor(...): Converts the given value to a Tensor. convert_to_tensor_or_indexed_slices(...): Converts the given object to a Tensor or an IndexedSlices. convert_to_tensor_or_sparse_tensor(...): Converts value to a SparseTensor or Tensor. cos(...): Computes cos of x element-wise. cosh(...): Computes hyperbolic cosine of x element-wise. count_nonzero(...): Computes number of nonzero elements across dimensions of a tensor. (deprecated arguments) (deprecated arguments) count_up_to(...): Increments 'ref' until it reaches 'limit'. (deprecated) create_partitioned_variables(...): Create a list of partitioned variables according to the given slicing. (deprecated) cross(...): Compute the pairwise cross product. cumprod(...): Compute the cumulative product of the tensor x along axis. cumsum(...): Compute the cumulative sum of the tensor x along axis. custom_gradient(...): Decorator to define a function with a custom gradient. decode_base64(...): Decode web-safe base64-encoded strings. decode_compressed(...): Decompress strings. decode_csv(...): Convert CSV records to tensors. Each column maps to one tensor. decode_json_example(...): Convert JSON-encoded Example records to binary protocol buffer strings. decode_raw(...): Convert raw byte strings into tensors. (deprecated arguments) delete_session_tensor(...): Delete the tensor for the given tensor handle. depth_to_space(...): DepthToSpace for tensors of type T. dequantize(...): Dequantize the 'input' tensor into a float or bfloat16 Tensor. deserialize_many_sparse(...): Deserialize and concatenate SparseTensors from a serialized minibatch. device(...): Wrapper for Graph.device() using the default graph. diag(...): Returns a diagonal tensor with a given diagonal values. diag_part(...): Returns the diagonal part of the tensor. digamma(...): Computes Psi, the derivative of Lgamma (the log of the absolute value of dimension_at_index(...): Compatibility utility required to allow for both V1 and V2 behavior in TF. dimension_value(...): Compatibility utility required to allow for both V1 and V2 behavior in TF. disable_control_flow_v2(...): Opts out of control flow v2. disable_eager_execution(...): Disables eager execution. disable_resource_variables(...): Opts out of resource variables. (deprecated) disable_tensor_equality(...): Compare Tensors by their id and be hashable. disable_v2_behavior(...): Disables TensorFlow 2.x behaviors. disable_v2_tensorshape(...): Disables the V2 TensorShape behavior and reverts to V1 behavior. div(...): Divides x / y elementwise (using Python 2 division operator semantics). (deprecated) div_no_nan(...): Computes a safe divide which returns 0 if the y is zero. divide(...): Computes Python style division of x by y. dynamic_partition(...): Partitions data into num_partitions tensors using indices from partitions. dynamic_stitch(...): Interleave the values from the data tensors into a single tensor. edit_distance(...): Computes the Levenshtein distance between sequences. einsum(...): Tensor contraction over specified indices and outer product. enable_control_flow_v2(...): Use control flow v2. enable_eager_execution(...): Enables eager execution for the lifetime of this program. enable_resource_variables(...): Creates resource variables by default. enable_tensor_equality(...): Compare Tensors with element-wise comparison and thus be unhashable. enable_v2_behavior(...): Enables TensorFlow 2.x behaviors. enable_v2_tensorshape(...): In TensorFlow 2.0, iterating over a TensorShape instance returns values. encode_base64(...): Encode strings into web-safe base64 format. ensure_shape(...): Updates the shape of a tensor and checks at runtime that the shape holds. equal(...): Returns the truth value of (x == y) element-wise. erf(...): Computes the Gauss error function of x element-wise. erfc(...): Computes the complementary error function of x element-wise. executing_eagerly(...): Checks whether the current thread has eager execution enabled. executing_eagerly_outside_functions(...): Returns True if executing eagerly, even if inside a graph function. exp(...): Computes exponential of x element-wise. \\(y = e^x\\). expand_dims(...): Returns a tensor with a length 1 axis inserted at index axis. (deprecated arguments) expm1(...): Computes exp(x) - 1 element-wise. extract_image_patches(...): Extract patches from images and put them in the \"depth\" output dimension. extract_volume_patches(...): Extract patches from input and put them in the \"depth\" output dimension. 3D extension of extract_image_patches. eye(...): Construct an identity matrix, or a batch of matrices. fake_quant_with_min_max_args(...): Fake-quantize the 'inputs' tensor, type float to 'outputs' tensor of same type. fake_quant_with_min_max_args_gradient(...): Compute gradients for a FakeQuantWithMinMaxArgs operation. fake_quant_with_min_max_vars(...): Fake-quantize the 'inputs' tensor of type float via global float scalars fake_quant_with_min_max_vars_gradient(...): Compute gradients for a FakeQuantWithMinMaxVars operation. fake_quant_with_min_max_vars_per_channel(...): Fake-quantize the 'inputs' tensor of type float via per-channel floats fake_quant_with_min_max_vars_per_channel_gradient(...): Compute gradients for a FakeQuantWithMinMaxVarsPerChannel operation. fft(...): Fast Fourier transform. fft2d(...): 2D fast Fourier transform. fft3d(...): 3D fast Fourier transform. fill(...): Creates a tensor filled with a scalar value. fingerprint(...): Generates fingerprint values. fixed_size_partitioner(...): Partitioner to specify a fixed number of shards along given axis. floor(...): Returns element-wise largest integer not greater than x. floor_div(...): Returns x // y element-wise. floordiv(...): Divides x / y elementwise, rounding toward the most negative integer. floormod(...): Returns element-wise remainder of division. When x < 0 xor y < 0 is foldl(...): foldl on the list of tensors unpacked from elems on dimension 0. foldr(...): foldr on the list of tensors unpacked from elems on dimension 0. function(...): Compiles a function into a callable TensorFlow graph. gather(...): Gather slices from params axis axis according to indices. gather_nd(...): Gather slices from params into a Tensor with shape specified by indices. get_collection(...): Wrapper for Graph.get_collection() using the default graph. get_collection_ref(...): Wrapper for Graph.get_collection_ref() using the default graph. get_default_graph(...): Returns the default graph for the current thread. get_default_session(...): Returns the default session for the current thread. get_local_variable(...): Gets an existing local variable or creates a new one. get_logger(...): Return TF logger instance. get_seed(...): Returns the local seeds an operation should use given an op-specific seed. get_session_handle(...): Return the handle of data. get_session_tensor(...): Get the tensor of type dtype by feeding a tensor handle. get_static_value(...): Returns the constant value of the given tensor, if efficiently calculable. get_variable(...): Gets an existing variable with these parameters or create a new one. get_variable_scope(...): Returns the current variable scope. global_norm(...): Computes the global norm of multiple tensors. global_variables(...): Returns global variables. global_variables_initializer(...): Returns an Op that initializes global variables. grad_pass_through(...): Creates a grad-pass-through op with the forward behavior provided in f. gradients(...): Constructs symbolic derivatives of sum of ys w.r.t. x in xs. greater(...): Returns the truth value of (x > y) element-wise. greater_equal(...): Returns the truth value of (x >= y) element-wise. group(...): Create an op that groups multiple operations. guarantee_const(...): Gives a guarantee to the TF runtime that the input tensor is a constant. hessians(...): Constructs the Hessian of sum of ys with respect to x in xs. histogram_fixed_width(...): Return histogram of values. histogram_fixed_width_bins(...): Bins the given values for use in a histogram. identity(...): Return a Tensor with the same shape and contents as input. identity_n(...): Returns a list of tensors with the same shapes and contents as the input ifft(...): Inverse fast Fourier transform. ifft2d(...): Inverse 2D fast Fourier transform. ifft3d(...): Inverse 3D fast Fourier transform. igamma(...): Compute the lower regularized incomplete Gamma function P(a, x). igammac(...): Compute the upper regularized incomplete Gamma function Q(a, x). imag(...): Returns the imaginary part of a complex (or real) tensor. import_graph_def(...): Imports the graph from graph_def into the current default Graph. (deprecated arguments) init_scope(...): A context manager that lifts ops out of control-flow scopes and function-building graphs. initialize_all_tables(...): Returns an Op that initializes all tables of the default graph. (deprecated) initialize_all_variables(...): See tf.compat.v1.global_variables_initializer. (deprecated) initialize_local_variables(...): See tf.compat.v1.local_variables_initializer. (deprecated) initialize_variables(...): See tf.compat.v1.variables_initializer. (deprecated) invert_permutation(...): Computes the inverse permutation of a tensor. is_finite(...): Returns which elements of x are finite. is_inf(...): Returns which elements of x are Inf. is_nan(...): Returns which elements of x are NaN. is_non_decreasing(...): Returns True if x is non-decreasing. is_numeric_tensor(...): Returns True if the elements of tensor are numbers. is_strictly_increasing(...): Returns True if x is strictly increasing. is_tensor(...): Checks whether x is a TF-native type that can be passed to many TF ops. is_variable_initialized(...): Tests if a variable has been initialized. lbeta(...): Computes \\(ln(|Beta(x)|)\\), reducing along the last dimension. less(...): Returns the truth value of (x < y) element-wise. less_equal(...): Returns the truth value of (x <= y) element-wise. lgamma(...): Computes the log of the absolute value of Gamma(x) element-wise. lin_space(...): Generates evenly-spaced values in an interval along a given axis. linspace(...): Generates evenly-spaced values in an interval along a given axis. load_file_system_library(...): Loads a TensorFlow plugin, containing file system implementation. (deprecated) load_library(...): Loads a TensorFlow plugin. load_op_library(...): Loads a TensorFlow plugin, containing custom ops and kernels. local_variables(...): Returns local variables. local_variables_initializer(...): Returns an Op that initializes all local variables. log(...): Computes natural logarithm of x element-wise. log1p(...): Computes natural logarithm of (1 + x) element-wise. log_sigmoid(...): Computes log sigmoid of x element-wise. logical_and(...): Logical AND function. logical_not(...): Returns the truth value of NOT x element-wise. logical_or(...): Returns the truth value of x OR y element-wise. logical_xor(...): Logical XOR function. make_ndarray(...): Create a numpy ndarray from a tensor. make_template(...): Given an arbitrary function, wrap it so that it does variable sharing. make_tensor_proto(...): Create a TensorProto. map_fn(...): Transforms elems by applying fn to each element unstacked on axis 0. (deprecated arguments) matching_files(...): Returns the set of files matching one or more glob patterns. matmul(...): Multiplies matrix a by matrix b, producing a * b. matrix_band_part(...): Copy a tensor setting everything outside a central band in each innermost matrix to zero. matrix_determinant(...): Computes the determinant of one or more square matrices. matrix_diag(...): Returns a batched diagonal tensor with given batched diagonal values. matrix_diag_part(...): Returns the batched diagonal part of a batched tensor. matrix_inverse(...): Computes the inverse of one or more square invertible matrices or their adjoints (conjugate transposes). matrix_set_diag(...): Returns a batched matrix tensor with new batched diagonal values. matrix_solve(...): Solves systems of linear equations. matrix_solve_ls(...): Solves one or more linear least-squares problems. matrix_square_root(...): Computes the matrix square root of one or more square matrices: matrix_transpose(...): Transposes last two dimensions of tensor a. matrix_triangular_solve(...): Solve systems of linear equations with upper or lower triangular matrices. maximum(...): Returns the max of x and y (i.e. x > y ? x : y) element-wise. meshgrid(...): Broadcasts parameters for evaluation on an N-D grid. min_max_variable_partitioner(...): Partitioner to allocate minimum size per slice. minimum(...): Returns the min of x and y (i.e. x < y ? x : y) element-wise. mod(...): Returns element-wise remainder of division. When x < 0 xor y < 0 is model_variables(...): Returns all variables in the MODEL_VARIABLES collection. moving_average_variables(...): Returns all variables that maintain their moving averages. multinomial(...): Draws samples from a multinomial distribution. (deprecated) multiply(...): Returns an element-wise x * y. negative(...): Computes numerical negative value element-wise. no_gradient(...): Specifies that ops of type op_type is not differentiable. no_op(...): Does nothing. Only useful as a placeholder for control edges. no_regularizer(...): Use this function to prevent regularization of variables. nondifferentiable_batch_function(...): Batches the computation done by the decorated function. norm(...): Computes the norm of vectors, matrices, and tensors. (deprecated arguments) not_equal(...): Returns the truth value of (x != y) element-wise. numpy_function(...): Wraps a python function and uses it as a TensorFlow op. one_hot(...): Returns a one-hot tensor. ones(...): Creates a tensor with all elements set to one (1). ones_like(...): Creates a tensor with all elements set to 1. op_scope(...): DEPRECATED. Same as name_scope above, just different argument order. pad(...): Pads a tensor. parallel_stack(...): Stacks a list of rank-R tensors into one rank-(R+1) tensor in parallel. parse_example(...): Parses Example protos into a dict of tensors. parse_single_example(...): Parses a single Example proto. parse_single_sequence_example(...): Parses a single SequenceExample proto. parse_tensor(...): Transforms a serialized tensorflow.TensorProto proto into a Tensor. placeholder(...): Inserts a placeholder for a tensor that will be always fed. placeholder_with_default(...): A placeholder op that passes through input when its output is not fed. polygamma(...): Compute the polygamma function \\(\\psi^{(n)}(x)\\). pow(...): Computes the power of one value to another. print(...): Print the specified inputs. py_func(...): Wraps a python function and uses it as a TensorFlow op. py_function(...): Wraps a python function into a TensorFlow op that executes it eagerly. qr(...): Computes the QR decompositions of one or more matrices. quantize(...): Quantize the 'input' tensor of type float to 'output' tensor of type 'T'. quantize_and_dequantize_v4(...): Returns the gradient of QuantizeAndDequantizeV4. quantize_v2(...): Please use tf.quantization.quantize instead. quantized_concat(...): Concatenates quantized tensors along one dimension. random_crop(...): Randomly crops a tensor to a given size. random_gamma(...): Draws shape samples from each of the given Gamma distribution(s). random_normal(...): Outputs random values from a normal distribution. random_poisson(...): Draws shape samples from each of the given Poisson distribution(s). random_shuffle(...): Randomly shuffles a tensor along its first dimension. random_uniform(...): Outputs random values from a uniform distribution. range(...): Creates a sequence of numbers. rank(...): Returns the rank of a tensor. read_file(...): Reads and outputs the entire contents of the input filename. real(...): Returns the real part of a complex (or real) tensor. realdiv(...): Returns x / y element-wise for real types. reciprocal(...): Computes the reciprocal of x element-wise. recompute_grad(...): An eager-compatible version of recompute_grad. reduce_all(...): Computes the \"logical and\" of elements across dimensions of a tensor. (deprecated arguments) reduce_any(...): Computes the \"logical or\" of elements across dimensions of a tensor. (deprecated arguments) reduce_join(...): Joins all strings into a single string, or joins along an axis. reduce_logsumexp(...): Computes log(sum(exp(elements across dimensions of a tensor))). (deprecated arguments) reduce_max(...): Computes the maximum of elements across dimensions of a tensor. (deprecated arguments) reduce_mean(...): Computes the mean of elements across dimensions of a tensor. reduce_min(...): Computes the minimum of elements across dimensions of a tensor. (deprecated arguments) reduce_prod(...): Computes the product of elements across dimensions of a tensor. (deprecated arguments) reduce_sum(...): Computes the sum of elements across dimensions of a tensor. (deprecated arguments) regex_replace(...): Replace elements of input matching regex pattern with rewrite. register_tensor_conversion_function(...): Registers a function for converting objects of base_type to Tensor. repeat(...): Repeat elements of input. report_uninitialized_variables(...): Adds ops to list the names of uninitialized variables. required_space_to_batch_paddings(...): Calculate padding required to make block_shape divide input_shape. reset_default_graph(...): Clears the default graph stack and resets the global default graph. reshape(...): Reshapes a tensor. resource_variables_enabled(...): Returns True if resource variables are enabled. reverse(...): Reverses specific dimensions of a tensor. reverse_sequence(...): Reverses variable length slices. (deprecated arguments) (deprecated arguments) reverse_v2(...): Reverses specific dimensions of a tensor. rint(...): Returns element-wise integer closest to x. roll(...): Rolls the elements of a tensor along an axis. round(...): Rounds the values of a tensor to the nearest integer, element-wise. rsqrt(...): Computes reciprocal of square root of x element-wise. saturate_cast(...): Performs a safe saturating cast of value to dtype. scalar_mul(...): Multiplies a scalar times a Tensor or IndexedSlices object. scan(...): scan on the list of tensors unpacked from elems on dimension 0. scatter_add(...): Adds sparse updates to the variable referenced by resource. scatter_div(...): Divides a variable reference by sparse updates. scatter_max(...): Reduces sparse updates into a variable reference using the max operation. scatter_min(...): Reduces sparse updates into a variable reference using the min operation. scatter_mul(...): Multiplies sparse updates into a variable reference. scatter_nd(...): Scatter updates into a new tensor according to indices. scatter_nd_add(...): Applies sparse addition to individual values or slices in a Variable. scatter_nd_sub(...): Applies sparse subtraction to individual values or slices in a Variable. scatter_nd_update(...): Applies sparse updates to individual values or slices in a Variable. scatter_sub(...): Subtracts sparse updates to a variable reference. scatter_update(...): Applies sparse updates to a variable reference. searchsorted(...): Searches input tensor for values on the innermost dimension. segment_max(...): Computes the maximum along segments of a tensor. segment_mean(...): Computes the mean along segments of a tensor. segment_min(...): Computes the minimum along segments of a tensor. segment_prod(...): Computes the product along segments of a tensor. segment_sum(...): Computes the sum along segments of a tensor. self_adjoint_eig(...): Computes the eigen decomposition of a batch of self-adjoint matrices. self_adjoint_eigvals(...): Computes the eigenvalues of one or more self-adjoint matrices. sequence_mask(...): Returns a mask tensor representing the first N positions of each cell. serialize_many_sparse(...): Serialize N-minibatch SparseTensor into an [N, 3] Tensor. serialize_sparse(...): Serialize a SparseTensor into a 3-vector (1-D Tensor) object. serialize_tensor(...): Transforms a Tensor into a serialized TensorProto proto. set_random_seed(...): Sets the graph-level random seed for the default graph. setdiff1d(...): Computes the difference between two lists of numbers or strings. shape(...): Returns the shape of a tensor. shape_n(...): Returns shape of tensors. sigmoid(...): Computes sigmoid of x element-wise. sign(...): Returns an element-wise indication of the sign of a number. sin(...): Computes sine of x element-wise. sinh(...): Computes hyperbolic sine of x element-wise. size(...): Returns the size of a tensor. slice(...): Extracts a slice from a tensor. sort(...): Sorts a tensor. space_to_batch(...): SpaceToBatch for 4-D tensors of type T. space_to_batch_nd(...): SpaceToBatch for N-D tensors of type T. space_to_depth(...): SpaceToDepth for tensors of type T. sparse_add(...): Adds two tensors, at least one of each is a SparseTensor. (deprecated arguments) sparse_concat(...): Concatenates a list of SparseTensor along the specified dimension. (deprecated arguments) sparse_fill_empty_rows(...): Fills empty rows in the input 2-D SparseTensor with a default value. sparse_mask(...): Masks elements of IndexedSlices. sparse_matmul(...): Multiply matrix \"a\" by matrix \"b\". sparse_maximum(...): Returns the element-wise max of two SparseTensors. sparse_merge(...): Combines a batch of feature ids and values into a single SparseTensor. (deprecated) sparse_minimum(...): Returns the element-wise min of two SparseTensors. sparse_placeholder(...): Inserts a placeholder for a sparse tensor that will be always fed. sparse_reduce_max(...): Computes the max of elements across dimensions of a SparseTensor. (deprecated arguments) (deprecated arguments) sparse_reduce_max_sparse(...): Computes the max of elements across dimensions of a SparseTensor. (deprecated arguments) sparse_reduce_sum(...): Computes the sum of elements across dimensions of a SparseTensor. (deprecated arguments) (deprecated arguments) sparse_reduce_sum_sparse(...): Computes the sum of elements across dimensions of a SparseTensor. (deprecated arguments) sparse_reorder(...): Reorders a SparseTensor into the canonical, row-major ordering. sparse_reset_shape(...): Resets the shape of a SparseTensor with indices and values unchanged. sparse_reshape(...): Reshapes a SparseTensor to represent values in a new dense shape. sparse_retain(...): Retains specified non-empty values within a SparseTensor. sparse_segment_mean(...): Computes the mean along sparse segments of a tensor. sparse_segment_sqrt_n(...): Computes the sum along sparse segments of a tensor divided by the sqrt(N). sparse_segment_sum(...): Computes the sum along sparse segments of a tensor. sparse_slice(...): Slice a SparseTensor based on the start and `size. sparse_softmax(...): Applies softmax to a batched N-D SparseTensor. sparse_split(...): Split a SparseTensor into num_split tensors along axis. (deprecated arguments) sparse_tensor_dense_matmul(...): Multiply SparseTensor (or dense Matrix) (of rank 2) \"A\" by dense matrix sparse_tensor_to_dense(...): Converts a SparseTensor into a dense tensor. sparse_to_dense(...): Converts a sparse representation into a dense tensor. (deprecated) sparse_to_indicator(...): Converts a SparseTensor of ids into a dense bool indicator tensor. sparse_transpose(...): Transposes a SparseTensor split(...): Splits a tensor value into a list of sub tensors. sqrt(...): Computes element-wise square root of the input tensor. square(...): Computes square of x element-wise. squared_difference(...): Returns conj(x - y)(x - y) element-wise. squeeze(...): Removes dimensions of size 1 from the shape of a tensor. (deprecated arguments) stack(...): Stacks a list of rank-R tensors into one rank-(R+1) tensor. stop_gradient(...): Stops gradient computation. strided_slice(...): Extracts a strided slice of a tensor (generalized Python array indexing). string_join(...): Perform element-wise concatenation of a list of string tensors. string_split(...): Split elements of source based on delimiter. (deprecated arguments) string_strip(...): Strip leading and trailing whitespaces from the Tensor. string_to_hash_bucket(...): Converts each string in the input Tensor to its hash mod by a number of buckets. string_to_hash_bucket_fast(...): Converts each string in the input Tensor to its hash mod by a number of buckets. string_to_hash_bucket_strong(...): Converts each string in the input Tensor to its hash mod by a number of buckets. string_to_number(...): Converts each string in the input Tensor to the specified numeric type. substr(...): Return substrings from Tensor of strings. subtract(...): Returns x - y element-wise. svd(...): Computes the singular value decompositions of one or more matrices. switch_case(...): Create a switch/case operation, i.e. an integer-indexed conditional. tables_initializer(...): Returns an Op that initializes all tables of the default graph. tan(...): Computes tan of x element-wise. tanh(...): Computes hyperbolic tangent of x element-wise. tensor_scatter_add(...): Adds sparse updates to an existing tensor according to indices. tensor_scatter_nd_add(...): Adds sparse updates to an existing tensor according to indices. tensor_scatter_nd_max(...) tensor_scatter_nd_min(...) tensor_scatter_nd_sub(...): Subtracts sparse updates from an existing tensor according to indices. tensor_scatter_nd_update(...): \"Scatter updates into an existing tensor according to indices. tensor_scatter_sub(...): Subtracts sparse updates from an existing tensor according to indices. tensor_scatter_update(...): \"Scatter updates into an existing tensor according to indices. tensordot(...): Tensor contraction of a and b along specified axes and outer product. tile(...): Constructs a tensor by tiling a given tensor. timestamp(...): Provides the time since epoch in seconds. to_bfloat16(...): Casts a tensor to type bfloat16. (deprecated) to_complex128(...): Casts a tensor to type complex128. (deprecated) to_complex64(...): Casts a tensor to type complex64. (deprecated) to_double(...): Casts a tensor to type float64. (deprecated) to_float(...): Casts a tensor to type float32. (deprecated) to_int32(...): Casts a tensor to type int32. (deprecated) to_int64(...): Casts a tensor to type int64. (deprecated) trace(...): Compute the trace of a tensor x. trainable_variables(...): Returns all variables created with trainable=True. transpose(...): Transposes a. truediv(...): Divides x / y elementwise (using Python 3 division operator semantics). truncated_normal(...): Outputs random values from a truncated normal distribution. truncatediv(...): Returns x / y element-wise for integer types. truncatemod(...): Returns element-wise remainder of division. This emulates C semantics in that tuple(...): Group tensors together. type_spec_from_value(...): Returns a tf.TypeSpec that represents the given value. unique(...): Finds unique elements in a 1-D tensor. unique_with_counts(...): Finds unique elements in a 1-D tensor. unravel_index(...): Converts an array of flat indices into a tuple of coordinate arrays. unsorted_segment_max(...): Computes the maximum along segments of a tensor. unsorted_segment_mean(...): Computes the mean along segments of a tensor. unsorted_segment_min(...): Computes the minimum along segments of a tensor. unsorted_segment_prod(...): Computes the product along segments of a tensor. unsorted_segment_sqrt_n(...): Computes the sum along segments of a tensor divided by the sqrt(N). unsorted_segment_sum(...): Computes the sum along segments of a tensor. unstack(...): Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors. variable_axis_size_partitioner(...): Get a partitioner for VariableScope to keep shards below max_shard_bytes. variable_creator_scope(...): Scope which defines a variable creation function to be used by variable(). variable_op_scope(...): Deprecated: context manager for defining an op that creates variables. variables_initializer(...): Returns an Op that initializes a list of variables. vectorized_map(...): Parallel map on the list of tensors unpacked from elems on dimension 0. verify_tensor_all_finite(...): Assert that the tensor does not contain any NaN's or Inf's. where(...): Return the elements, either from x or y, depending on the condition. where_v2(...): Return the elements where condition is True (multiplexing x and y). while_loop(...): Repeat body while the condition cond is true. wrap_function(...): Wraps the TF 1.x function fn into a graph function. write_file(...): Writes contents to the file at input filename. Creates file and recursively zeros(...): Creates a tensor with all elements set to zero. zeros_like(...): Creates a tensor with all elements set to zero. zeta(...): Compute the Hurwitz zeta function \\(\\zeta(x, q)\\).\n \n\n\n Other Members\n  AUTO_REUSE  \n \n  COMPILER_VERSION   '7.3.1 20180303'  \n  CXX11_ABI_FLAG   0  \n  GIT_VERSION   'v2.4.0-rc4-71-g582c8d236cb'  \n  GRAPH_DEF_VERSION   561  \n  GRAPH_DEF_VERSION_MIN_CONSUMER   0  \n  GRAPH_DEF_VERSION_MIN_PRODUCER   0  \n  MONOLITHIC_BUILD   0  \n  QUANTIZED_DTYPES  \n \n  VERSION   '2.4.0'  \n  version   '2.4.0'  \n  bfloat16   tf.dtypes.DType  \n  bool   tf.dtypes.DType  \n  complex128   tf.dtypes.DType  \n  complex64   tf.dtypes.DType  \n  double   tf.dtypes.DType  \n  float16   tf.dtypes.DType  \n  float32   tf.dtypes.DType  \n  float64   tf.dtypes.DType  \n  half   tf.dtypes.DType  \n  int16   tf.dtypes.DType  \n  int32   tf.dtypes.DType  \n  int64   tf.dtypes.DType  \n  int8   tf.dtypes.DType  \n  newaxis   None  \n  qint16   tf.dtypes.DType  \n  qint32   tf.dtypes.DType  \n  qint8   tf.dtypes.DType  \n  quint16   tf.dtypes.DType  \n  quint8   tf.dtypes.DType  \n  resource   tf.dtypes.DType  \n  string   tf.dtypes.DType  \n  uint16   tf.dtypes.DType  \n  uint32   tf.dtypes.DType  \n  uint64   tf.dtypes.DType  \n  uint8   tf.dtypes.DType  \n  variant   tf.dtypes.DType     \n"}, {"name": "tf.compat.v1.add_check_numerics_ops", "path": "compat/v1/add_check_numerics_ops", "type": "tf.compat", "text": "tf.compat.v1.add_check_numerics_ops Connect a tf.debugging.check_numerics to every floating point tensor. \ntf.compat.v1.add_check_numerics_ops()\n check_numerics operations themselves are added for each half, float, or double tensor in the current default graph. For all ops in the graph, the check_numerics op for all of its (half, float, or double) inputs is guaranteed to run before the check_numerics op on any of its outputs. \nNote: This API is not compatible with the use of tf.cond or tf.while_loop, and will raise a ValueError if you attempt to call it in such a graph.\n\n \n\n\n Returns   A group op depending on all check_numerics ops added.  \n\n \n\n\n Raises\n  ValueError   If the graph contains any numeric operations in a control flow structure.  \n  RuntimeError   If called with eager execution enabled.    Eager Compatibility Not compatible with eager execution. To check for Infs and NaNs under eager execution, call tf.debugging.enable_check_numerics() once before executing the checked operations.  \n"}, {"name": "tf.compat.v1.add_to_collection", "path": "compat/v1/add_to_collection", "type": "tf.compat", "text": "tf.compat.v1.add_to_collection Wrapper for Graph.add_to_collection() using the default graph. \ntf.compat.v1.add_to_collection(\n    name, value\n)\n See tf.Graph.add_to_collection for more details.\n \n\n\n Args\n  name   The key for the collection. For example, the GraphKeys class contains many standard names for collections.  \n  value   The value to add to the collection.    Eager Compatibility Collections are only supported in eager when variables are created inside an EagerVariableStore (e.g. as part of a layer or template).  \n"}, {"name": "tf.compat.v1.add_to_collections", "path": "compat/v1/add_to_collections", "type": "tf.compat", "text": "tf.compat.v1.add_to_collections Wrapper for Graph.add_to_collections() using the default graph. \ntf.compat.v1.add_to_collections(\n    names, value\n)\n See tf.Graph.add_to_collections for more details.\n \n\n\n Args\n  names   The key for the collections. The GraphKeys class contains many standard names for collections.  \n  value   The value to add to the collections.    Eager Compatibility Collections are only supported in eager when variables are created inside an EagerVariableStore (e.g. as part of a layer or template).  \n"}, {"name": "tf.compat.v1.all_variables", "path": "compat/v1/all_variables", "type": "tf.compat", "text": "tf.compat.v1.all_variables Use tf.compat.v1.global_variables instead. (deprecated) \ntf.compat.v1.all_variables()\n Warning: THIS FUNCTION IS DEPRECATED. It will be removed after 2017-03-02. Instructions for updating: Please use tf.global_variables instead.  \n"}, {"name": "tf.compat.v1.app", "path": "compat/v1/app", "type": "tf.compat", "text": "Module: tf.compat.v1.app Generic entry point script. Modules flags module: Import router for absl.flags. See https://github.com/abseil/abseil-py Functions run(...): Runs the program with an optional 'main' function and 'argv' list.  \n"}, {"name": "tf.compat.v1.app.run", "path": "compat/v1/app/run", "type": "tf.compat", "text": "tf.compat.v1.app.run Runs the program with an optional 'main' function and 'argv' list. \ntf.compat.v1.app.run(\n    main=None, argv=None\n)\n  \n"}, {"name": "tf.compat.v1.argmax", "path": "compat/v1/argmax", "type": "tf.compat", "text": "tf.compat.v1.argmax Returns the index with the largest value across axes of a tensor. (deprecated arguments)  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.math.argmax  \ntf.compat.v1.argmax(\n    input, axis=None, name=None, dimension=None, output_type=tf.dtypes.int64\n)\n Warning: SOME ARGUMENTS ARE DEPRECATED: (dimension). They will be removed in a future version. Instructions for updating: Use the axis argument instead Note that in case of ties the identity of the return value is not guaranteed. Usage: import tensorflow as tf\na = [1, 10, 26.9, 2.8, 166.32, 62.3]\nb = tf.math.argmax(input = a)\nc = tf.keras.backend.eval(b)\n# c = 4\n# here a[4] = 166.32 which is the largest element of a across axis 0\n\n \n\n\n Args\n  input   A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64, bool.  \n  axis   A Tensor. Must be one of the following types: int32, int64. int32 or int64, must be in the range [-rank(input), rank(input)). Describes which axis of the input Tensor to reduce across. For vectors, use axis = 0.  \n  output_type   An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int64.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type output_type.  \n  \n"}, {"name": "tf.compat.v1.argmin", "path": "compat/v1/argmin", "type": "tf.compat", "text": "tf.compat.v1.argmin Returns the index with the smallest value across axes of a tensor. (deprecated arguments)  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.math.argmin  \ntf.compat.v1.argmin(\n    input, axis=None, name=None, dimension=None, output_type=tf.dtypes.int64\n)\n Warning: SOME ARGUMENTS ARE DEPRECATED: (dimension). They will be removed in a future version. Instructions for updating: Use the axis argument instead Note that in case of ties the identity of the return value is not guaranteed. Usage: import tensorflow as tf\na = [1, 10, 26.9, 2.8, 166.32, 62.3]\nb = tf.math.argmin(input = a)\nc = tf.keras.backend.eval(b)\n# c = 0\n# here a[0] = 1 which is the smallest element of a across axis 0\n\n \n\n\n Args\n  input   A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64, bool.  \n  axis   A Tensor. Must be one of the following types: int32, int64. int32 or int64, must be in the range [-rank(input), rank(input)). Describes which axis of the input Tensor to reduce across. For vectors, use axis = 0.  \n  output_type   An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int64.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type output_type.  \n  \n"}, {"name": "tf.compat.v1.arg_max", "path": "compat/v1/arg_max", "type": "tf.compat", "text": "tf.compat.v1.arg_max Returns the index with the largest value across dimensions of a tensor. \ntf.compat.v1.arg_max(\n    input, dimension, output_type=tf.dtypes.int64, name=None\n)\n Note that in case of ties the identity of the return value is not guaranteed. Usage: import tensorflow as tf\na = [1, 10, 26.9, 2.8, 166.32, 62.3]\nb = tf.math.argmax(input = a)\nc = tf.keras.backend.eval(b)\n# c = 4\n# here a[4] = 166.32 which is the largest element of a across axis 0\n\n \n\n\n Args\n  input   A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64, bool.  \n  dimension   A Tensor. Must be one of the following types: int32, int64. int32 or int64, must be in the range [-rank(input), rank(input)). Describes which dimension of the input Tensor to reduce across. For vectors, use dimension = 0.  \n  output_type   An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int64.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type output_type.  \n  \n"}, {"name": "tf.compat.v1.arg_min", "path": "compat/v1/arg_min", "type": "tf.compat", "text": "tf.compat.v1.arg_min Returns the index with the smallest value across dimensions of a tensor. \ntf.compat.v1.arg_min(\n    input, dimension, output_type=tf.dtypes.int64, name=None\n)\n Note that in case of ties the identity of the return value is not guaranteed. Usage: import tensorflow as tf\na = [1, 10, 26.9, 2.8, 166.32, 62.3]\nb = tf.math.argmin(input = a)\nc = tf.keras.backend.eval(b)\n# c = 0\n# here a[0] = 1 which is the smallest element of a across axis 0\n\n \n\n\n Args\n  input   A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64, bool.  \n  dimension   A Tensor. Must be one of the following types: int32, int64. int32 or int64, must be in the range [-rank(input), rank(input)). Describes which dimension of the input Tensor to reduce across. For vectors, use dimension = 0.  \n  output_type   An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int64.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type output_type.  \n  \n"}, {"name": "tf.compat.v1.assert_equal", "path": "compat/v1/assert_equal", "type": "tf.compat", "text": "tf.compat.v1.assert_equal Assert the condition x == y holds element-wise.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.debugging.assert_equal  \ntf.compat.v1.assert_equal(\n    x, y, data=None, summarize=None, message=None, name=None\n)\n This condition holds if for every pair of (possibly broadcast) elements x[i], y[i], we have x[i] == y[i]. If both x and y are empty, this is trivially satisfied. When running in graph mode, you should add a dependency on this operation to ensure that it runs. Example of adding a dependency to an operation: with tf.control_dependencies([tf.compat.v1.assert_equal(x, y)]):\n  output = tf.reduce_sum(x)\n\n \n\n\n Args\n  x   Numeric Tensor.  \n  y   Numeric Tensor, same dtype as and broadcastable to x.  \n  data   The tensors to print out if the condition is False. Defaults to error message and first few entries of x, y.  \n  summarize   Print this many entries of each tensor.  \n  message   A string to prefix to the default message.  \n  name   A name for this operation (optional). Defaults to \"assert_equal\".   \n \n\n\n Returns   Op that raises InvalidArgumentError if x == y is False.  \n\n \n\n\n Raises\n  InvalidArgumentError   if the check can be performed immediately and x == y is False. The check can be performed immediately during eager execution or if x and y are statically known.    Eager Compatibility returns None  \n"}, {"name": "tf.compat.v1.assert_greater", "path": "compat/v1/assert_greater", "type": "tf.compat", "text": "tf.compat.v1.assert_greater Assert the condition x > y holds element-wise.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.debugging.assert_greater  \ntf.compat.v1.assert_greater(\n    x, y, data=None, summarize=None, message=None, name=None\n)\n This condition holds if for every pair of (possibly broadcast) elements x[i], y[i], we have x[i] > y[i]. If both x and y are empty, this is trivially satisfied. When running in graph mode, you should add a dependency on this operation to ensure that it runs. Example of adding a dependency to an operation: with tf.control_dependencies([tf.compat.v1.assert_greater(x, y)]):\n  output = tf.reduce_sum(x)\n\n \n\n\n Args\n  x   Numeric Tensor.  \n  y   Numeric Tensor, same dtype as and broadcastable to x.  \n  data   The tensors to print out if the condition is False. Defaults to error message and first few entries of x, y.  \n  summarize   Print this many entries of each tensor.  \n  message   A string to prefix to the default message.  \n  name   A name for this operation (optional). Defaults to \"assert_greater\".   \n \n\n\n Returns   Op that raises InvalidArgumentError if x > y is False.  \n\n \n\n\n Raises\n  InvalidArgumentError   if the check can be performed immediately and x > y is False. The check can be performed immediately during eager execution or if x and y are statically known.    Eager Compatibility returns None  \n"}, {"name": "tf.compat.v1.assert_greater_equal", "path": "compat/v1/assert_greater_equal", "type": "tf.compat", "text": "tf.compat.v1.assert_greater_equal Assert the condition x >= y holds element-wise.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.debugging.assert_greater_equal  \ntf.compat.v1.assert_greater_equal(\n    x, y, data=None, summarize=None, message=None, name=None\n)\n This condition holds if for every pair of (possibly broadcast) elements x[i], y[i], we have x[i] >= y[i]. If both x and y are empty, this is trivially satisfied. When running in graph mode, you should add a dependency on this operation to ensure that it runs. Example of adding a dependency to an operation: with tf.control_dependencies([tf.compat.v1.assert_greater_equal(x, y)]):\n  output = tf.reduce_sum(x)\n\n \n\n\n Args\n  x   Numeric Tensor.  \n  y   Numeric Tensor, same dtype as and broadcastable to x.  \n  data   The tensors to print out if the condition is False. Defaults to error message and first few entries of x, y.  \n  summarize   Print this many entries of each tensor.  \n  message   A string to prefix to the default message.  \n  name   A name for this operation (optional). Defaults to \"assert_greater_equal\".   \n \n\n\n Returns   Op that raises InvalidArgumentError if x >= y is False.  \n\n \n\n\n Raises\n  InvalidArgumentError   if the check can be performed immediately and x >= y is False. The check can be performed immediately during eager execution or if x and y are statically known.    Eager Compatibility returns None  \n"}, {"name": "tf.compat.v1.assert_integer", "path": "compat/v1/assert_integer", "type": "tf.compat", "text": "tf.compat.v1.assert_integer Assert that x is of integer dtype.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.debugging.assert_integer  \ntf.compat.v1.assert_integer(\n    x, message=None, name=None\n)\n Example of adding a dependency to an operation: with tf.control_dependencies([tf.compat.v1.assert_integer(x)]):\n  output = tf.reduce_sum(x)\n\n \n\n\n Args\n  x   Tensor whose basetype is integer and is not quantized.  \n  message   A string to prefix to the default message.  \n  name   A name for this operation (optional). Defaults to \"assert_integer\".   \n \n\n\n Raises\n  TypeError   If x.dtype is anything other than non-quantized integer.   \n \n\n\n Returns   A no_op that does nothing. Type can be determined statically.  \n  \n"}, {"name": "tf.compat.v1.assert_less", "path": "compat/v1/assert_less", "type": "tf.compat", "text": "tf.compat.v1.assert_less Assert the condition x < y holds element-wise.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.debugging.assert_less  \ntf.compat.v1.assert_less(\n    x, y, data=None, summarize=None, message=None, name=None\n)\n This condition holds if for every pair of (possibly broadcast) elements x[i], y[i], we have x[i] < y[i]. If both x and y are empty, this is trivially satisfied. When running in graph mode, you should add a dependency on this operation to ensure that it runs. Example of adding a dependency to an operation: with tf.control_dependencies([tf.compat.v1.assert_less(x, y)]):\n  output = tf.reduce_sum(x)\n\n \n\n\n Args\n  x   Numeric Tensor.  \n  y   Numeric Tensor, same dtype as and broadcastable to x.  \n  data   The tensors to print out if the condition is False. Defaults to error message and first few entries of x, y.  \n  summarize   Print this many entries of each tensor.  \n  message   A string to prefix to the default message.  \n  name   A name for this operation (optional). Defaults to \"assert_less\".   \n \n\n\n Returns   Op that raises InvalidArgumentError if x < y is False.  \n\n \n\n\n Raises\n  InvalidArgumentError   if the check can be performed immediately and x < y is False. The check can be performed immediately during eager execution or if x and y are statically known.    Eager Compatibility returns None  \n"}, {"name": "tf.compat.v1.assert_less_equal", "path": "compat/v1/assert_less_equal", "type": "tf.compat", "text": "tf.compat.v1.assert_less_equal Assert the condition x <= y holds element-wise.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.debugging.assert_less_equal  \ntf.compat.v1.assert_less_equal(\n    x, y, data=None, summarize=None, message=None, name=None\n)\n This condition holds if for every pair of (possibly broadcast) elements x[i], y[i], we have x[i] <= y[i]. If both x and y are empty, this is trivially satisfied. When running in graph mode, you should add a dependency on this operation to ensure that it runs. Example of adding a dependency to an operation: with tf.control_dependencies([tf.compat.v1.assert_less_equal(x, y)]):\n  output = tf.reduce_sum(x)\n\n \n\n\n Args\n  x   Numeric Tensor.  \n  y   Numeric Tensor, same dtype as and broadcastable to x.  \n  data   The tensors to print out if the condition is False. Defaults to error message and first few entries of x, y.  \n  summarize   Print this many entries of each tensor.  \n  message   A string to prefix to the default message.  \n  name   A name for this operation (optional). Defaults to \"assert_less_equal\".   \n \n\n\n Returns   Op that raises InvalidArgumentError if x <= y is False.  \n\n \n\n\n Raises\n  InvalidArgumentError   if the check can be performed immediately and x <= y is False. The check can be performed immediately during eager execution or if x and y are statically known.    Eager Compatibility returns None  \n"}, {"name": "tf.compat.v1.assert_near", "path": "compat/v1/assert_near", "type": "tf.compat", "text": "tf.compat.v1.assert_near Assert the condition x and y are close element-wise.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.debugging.assert_near  \ntf.compat.v1.assert_near(\n    x, y, rtol=None, atol=None, data=None, summarize=None, message=None, name=None\n)\n Example of adding a dependency to an operation: with tf.control_dependencies([tf.compat.v1.assert_near(x, y)]):\n  output = tf.reduce_sum(x)\n This condition holds if for every pair of (possibly broadcast) elements x[i], y[i], we have tf.abs(x[i] - y[i]) <= atol + rtol * tf.abs(y[i]). If both x and y are empty, this is trivially satisfied. The default atol and rtol is 10 * eps, where eps is the smallest representable positive number such that 1 + eps != 1. This is about 1.2e-6 in 32bit, 2.22e-15 in 64bit, and 0.00977 in 16bit. See numpy.finfo.\n \n\n\n Args\n  x   Float or complex Tensor.  \n  y   Float or complex Tensor, same dtype as, and broadcastable to, x.  \n  rtol   Tensor. Same dtype as, and broadcastable to, x. The relative tolerance. Default is 10 * eps.  \n  atol   Tensor. Same dtype as, and broadcastable to, x. The absolute tolerance. Default is 10 * eps.  \n  data   The tensors to print out if the condition is False. Defaults to error message and first few entries of x, y.  \n  summarize   Print this many entries of each tensor.  \n  message   A string to prefix to the default message.  \n  name   A name for this operation (optional). Defaults to \"assert_near\".   \n \n\n\n Returns   Op that raises InvalidArgumentError if x and y are not close enough.  \n Numpy Compatibility Similar to numpy.testing.assert_allclose, except tolerance depends on data type. This is due to the fact that TensorFlow is often used with 32bit, 64bit, and even 16bit data.  \n"}, {"name": "tf.compat.v1.assert_negative", "path": "compat/v1/assert_negative", "type": "tf.compat", "text": "tf.compat.v1.assert_negative Assert the condition x < 0 holds element-wise.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.debugging.assert_negative  \ntf.compat.v1.assert_negative(\n    x, data=None, summarize=None, message=None, name=None\n)\n When running in graph mode, you should add a dependency on this operation to ensure that it runs. Example of adding a dependency to an operation: with tf.control_dependencies([tf.debugging.assert_negative(x, y)]):\n  output = tf.reduce_sum(x)\n Negative means, for every element x[i] of x, we have x[i] < 0. If x is empty this is trivially satisfied.\n \n\n\n Args\n  x   Numeric Tensor.  \n  data   The tensors to print out if the condition is False. Defaults to error message and first few entries of x.  \n  summarize   Print this many entries of each tensor.  \n  message   A string to prefix to the default message.  \n  name   A name for this operation (optional). Defaults to \"assert_negative\".   \n \n\n\n Returns   Op that raises InvalidArgumentError if x < 0 is False.  \n\n \n\n\n Raises\n  InvalidArgumentError   if the check can be performed immediately and x < 0 is False. The check can be performed immediately during eager execution or if x is statically known.    Eager Compatibility returns None  \n"}, {"name": "tf.compat.v1.assert_none_equal", "path": "compat/v1/assert_none_equal", "type": "tf.compat", "text": "tf.compat.v1.assert_none_equal Assert the condition x != y holds element-wise.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.debugging.assert_none_equal  \ntf.compat.v1.assert_none_equal(\n    x, y, data=None, summarize=None, message=None, name=None\n)\n This condition holds if for every pair of (possibly broadcast) elements x[i], y[i], we have x[i] != y[i]. If both x and y are empty, this is trivially satisfied. When running in graph mode, you should add a dependency on this operation to ensure that it runs. Example of adding a dependency to an operation: with tf.control_dependencies([tf.compat.v1.assert_none_equal(x, y)]):\n  output = tf.reduce_sum(x)\n\n \n\n\n Args\n  x   Numeric Tensor.  \n  y   Numeric Tensor, same dtype as and broadcastable to x.  \n  data   The tensors to print out if the condition is False. Defaults to error message and first few entries of x, y.  \n  summarize   Print this many entries of each tensor.  \n  message   A string to prefix to the default message.  \n  name   A name for this operation (optional). Defaults to \"assert_none_equal\".   \n \n\n\n Returns   Op that raises InvalidArgumentError if x != y is False.  \n\n \n\n\n Raises\n  InvalidArgumentError   if the check can be performed immediately and x != y is False. The check can be performed immediately during eager execution or if x and y are statically known.    Eager Compatibility returns None  \n"}, {"name": "tf.compat.v1.assert_non_negative", "path": "compat/v1/assert_non_negative", "type": "tf.compat", "text": "tf.compat.v1.assert_non_negative Assert the condition x >= 0 holds element-wise.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.debugging.assert_non_negative  \ntf.compat.v1.assert_non_negative(\n    x, data=None, summarize=None, message=None, name=None\n)\n When running in graph mode, you should add a dependency on this operation to ensure that it runs. Example of adding a dependency to an operation: with tf.control_dependencies([tf.debugging.assert_non_negative(x, y)]):\n  output = tf.reduce_sum(x)\n Non-negative means, for every element x[i] of x, we have x[i] >= 0. If x is empty this is trivially satisfied.\n \n\n\n Args\n  x   Numeric Tensor.  \n  data   The tensors to print out if the condition is False. Defaults to error message and first few entries of x.  \n  summarize   Print this many entries of each tensor.  \n  message   A string to prefix to the default message.  \n  name   A name for this operation (optional). Defaults to \"assert_non_negative\".   \n \n\n\n Returns   Op that raises InvalidArgumentError if x >= 0 is False.  \n\n \n\n\n Raises\n  InvalidArgumentError   if the check can be performed immediately and x >= 0 is False. The check can be performed immediately during eager execution or if x is statically known.    Eager Compatibility returns None  \n"}, {"name": "tf.compat.v1.assert_non_positive", "path": "compat/v1/assert_non_positive", "type": "tf.compat", "text": "tf.compat.v1.assert_non_positive Assert the condition x <= 0 holds element-wise.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.debugging.assert_non_positive  \ntf.compat.v1.assert_non_positive(\n    x, data=None, summarize=None, message=None, name=None\n)\n When running in graph mode, you should add a dependency on this operation to ensure that it runs. Example of adding a dependency to an operation: with tf.control_dependencies([tf.debugging.assert_non_positive(x, y)]):\n  output = tf.reduce_sum(x)\n Non-positive means, for every element x[i] of x, we have x[i] <= 0. If x is empty this is trivially satisfied.\n \n\n\n Args\n  x   Numeric Tensor.  \n  data   The tensors to print out if the condition is False. Defaults to error message and first few entries of x.  \n  summarize   Print this many entries of each tensor.  \n  message   A string to prefix to the default message.  \n  name   A name for this operation (optional). Defaults to \"assert_non_positive\".   \n \n\n\n Returns   Op that raises InvalidArgumentError if x <= 0 is False.  \n\n \n\n\n Raises\n  InvalidArgumentError   if the check can be performed immediately and x <= 0 is False. The check can be performed immediately during eager execution or if x is statically known.    Eager Compatibility returns None  \n"}, {"name": "tf.compat.v1.assert_positive", "path": "compat/v1/assert_positive", "type": "tf.compat", "text": "tf.compat.v1.assert_positive Assert the condition x > 0 holds element-wise.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.debugging.assert_positive  \ntf.compat.v1.assert_positive(\n    x, data=None, summarize=None, message=None, name=None\n)\n When running in graph mode, you should add a dependency on this operation to ensure that it runs. Example of adding a dependency to an operation: with tf.control_dependencies([tf.debugging.assert_positive(x, y)]):\n  output = tf.reduce_sum(x)\n Positive means, for every element x[i] of x, we have x[i] > 0. If x is empty this is trivially satisfied.\n \n\n\n Args\n  x   Numeric Tensor.  \n  data   The tensors to print out if the condition is False. Defaults to error message and first few entries of x.  \n  summarize   Print this many entries of each tensor.  \n  message   A string to prefix to the default message.  \n  name   A name for this operation (optional). Defaults to \"assert_positive\".   \n \n\n\n Returns   Op that raises InvalidArgumentError if x > 0 is False.  \n\n \n\n\n Raises\n  InvalidArgumentError   if the check can be performed immediately and x > 0 is False. The check can be performed immediately during eager execution or if x is statically known.    Eager Compatibility returns None  \n"}, {"name": "tf.compat.v1.assert_rank", "path": "compat/v1/assert_rank", "type": "tf.compat", "text": "tf.compat.v1.assert_rank Assert x has rank equal to rank.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.debugging.assert_rank  \ntf.compat.v1.assert_rank(\n    x, rank, data=None, summarize=None, message=None, name=None\n)\n Example of adding a dependency to an operation: with tf.control_dependencies([tf.compat.v1.assert_rank(x, 2)]):\n  output = tf.reduce_sum(x)\n\n \n\n\n Args\n  x   Numeric Tensor.  \n  rank   Scalar integer Tensor.  \n  data   The tensors to print out if the condition is False. Defaults to error message and the shape of x.  \n  summarize   Print this many entries of each tensor.  \n  message   A string to prefix to the default message.  \n  name   A name for this operation (optional). Defaults to \"assert_rank\".   \n \n\n\n Returns   Op raising InvalidArgumentError unless x has specified rank. If static checks determine x has correct rank, a no_op is returned.  \n\n \n\n\n Raises\n  ValueError   If static checks determine x has wrong rank.     \n"}, {"name": "tf.compat.v1.assert_rank_at_least", "path": "compat/v1/assert_rank_at_least", "type": "tf.compat", "text": "tf.compat.v1.assert_rank_at_least Assert x has rank equal to rank or higher.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.debugging.assert_rank_at_least  \ntf.compat.v1.assert_rank_at_least(\n    x, rank, data=None, summarize=None, message=None, name=None\n)\n Example of adding a dependency to an operation: with tf.control_dependencies([tf.compat.v1.assert_rank_at_least(x, 2)]):\n  output = tf.reduce_sum(x)\n\n \n\n\n Args\n  x   Numeric Tensor.  \n  rank   Scalar Tensor.  \n  data   The tensors to print out if the condition is False. Defaults to error message and first few entries of x.  \n  summarize   Print this many entries of each tensor.  \n  message   A string to prefix to the default message.  \n  name   A name for this operation (optional). Defaults to \"assert_rank_at_least\".   \n \n\n\n Returns   Op raising InvalidArgumentError unless x has specified rank or higher. If static checks determine x has correct rank, a no_op is returned.  \n\n \n\n\n Raises\n  ValueError   If static checks determine x has wrong rank.     \n"}, {"name": "tf.compat.v1.assert_rank_in", "path": "compat/v1/assert_rank_in", "type": "tf.compat", "text": "tf.compat.v1.assert_rank_in Assert x has rank in ranks.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.debugging.assert_rank_in  \ntf.compat.v1.assert_rank_in(\n    x, ranks, data=None, summarize=None, message=None, name=None\n)\n Example of adding a dependency to an operation: with tf.control_dependencies([tf.compat.v1.assert_rank_in(x, (2, 4))]):\n  output = tf.reduce_sum(x)\n\n \n\n\n Args\n  x   Numeric Tensor.  \n  ranks   Iterable of scalar Tensor objects.  \n  data   The tensors to print out if the condition is False. Defaults to error message and first few entries of x.  \n  summarize   Print this many entries of each tensor.  \n  message   A string to prefix to the default message.  \n  name   A name for this operation (optional). Defaults to \"assert_rank_in\".   \n \n\n\n Returns   Op raising InvalidArgumentError unless rank of x is in ranks. If static checks determine x has matching rank, a no_op is returned.  \n\n \n\n\n Raises\n  ValueError   If static checks determine x has mismatched rank.     \n"}, {"name": "tf.compat.v1.assert_scalar", "path": "compat/v1/assert_scalar", "type": "tf.compat", "text": "tf.compat.v1.assert_scalar Asserts that the given tensor is a scalar (i.e. zero-dimensional).  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.debugging.assert_scalar  \ntf.compat.v1.assert_scalar(\n    tensor, name=None, message=None\n)\n This function raises ValueError unless it can be certain that the given tensor is a scalar. ValueError is also raised if the shape of tensor is unknown.\n \n\n\n Args\n  tensor   A Tensor.  \n  name   A name for this operation. Defaults to \"assert_scalar\"  \n  message   A string to prefix to the default message.   \n \n\n\n Returns   The input tensor (potentially converted to a Tensor).  \n\n \n\n\n Raises\n  ValueError   If the tensor is not scalar (rank 0), or if its shape is unknown.     \n"}, {"name": "tf.compat.v1.assert_type", "path": "compat/v1/assert_type", "type": "tf.compat", "text": "tf.compat.v1.assert_type Statically asserts that the given Tensor is of the specified type.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.debugging.assert_type  \ntf.compat.v1.assert_type(\n    tensor, tf_type, message=None, name=None\n)\n\n \n\n\n Args\n  tensor   A Tensor or SparseTensor.  \n  tf_type   A tensorflow type (dtypes.float32, tf.int64, dtypes.bool, etc).  \n  message   A string to prefix to the default message.  \n  name   A name to give this Op. Defaults to \"assert_type\"   \n \n\n\n Raises\n  TypeError   If the tensors data type doesn't match tf_type.   \n \n\n\n Returns   A no_op that does nothing. Type can be determined statically.  \n  \n"}, {"name": "tf.compat.v1.assert_variables_initialized", "path": "compat/v1/assert_variables_initialized", "type": "tf.compat", "text": "tf.compat.v1.assert_variables_initialized Returns an Op to check if variables are initialized. \ntf.compat.v1.assert_variables_initialized(\n    var_list=None\n)\n \nNote: This function is obsolete and will be removed in 6 months. Please change your implementation to use report_uninitialized_variables().\n When run, the returned Op will raise the exception FailedPreconditionError if any of the variables has not yet been initialized. \nNote: This function is implemented by trying to fetch the values of the variables. If one of the variables is not initialized a message may be logged by the C++ runtime. This is expected.\n\n \n\n\n Args\n  var_list   List of Variable objects to check. Defaults to the value of global_variables().   \n \n\n\n Returns   An Op, or None if there are no variables.  \n \nNote: The output of this function should be used. If it is not, a warning will be logged or an error may be raised. To mark the output as used, call its .mark_used() method.\n  \n"}, {"name": "tf.compat.v1.assign", "path": "compat/v1/assign", "type": "tf.compat", "text": "tf.compat.v1.assign Update ref by assigning value to it. \ntf.compat.v1.assign(\n    ref, value, validate_shape=None, use_locking=None, name=None\n)\n This operation outputs a Tensor that holds the new value of ref after the value has been assigned. This makes it easier to chain operations that need to use the reset value.\n \n\n\n Args\n  ref   A mutable Tensor. Should be from a Variable node. May be uninitialized.  \n  value   A Tensor. Must have the same shape and dtype as ref. The value to be assigned to the variable.  \n  validate_shape   An optional bool. Defaults to True. If true, the operation will validate that the shape of 'value' matches the shape of the Tensor being assigned to. If false, 'ref' will take on the shape of 'value'.  \n  use_locking   An optional bool. Defaults to True. If True, the assignment will be protected by a lock; otherwise the behavior is undefined, but may exhibit less contention.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor that will hold the new value of ref after the assignment has completed.  \n  \n"}, {"name": "tf.compat.v1.assign_add", "path": "compat/v1/assign_add", "type": "tf.compat", "text": "tf.compat.v1.assign_add Update ref by adding value to it. \ntf.compat.v1.assign_add(\n    ref, value, use_locking=None, name=None\n)\n This operation outputs \"ref\" after the update is done. This makes it easier to chain operations that need to use the reset value. Unlike tf.math.add, this op does not broadcast. ref and value must have the same shape.\n \n\n\n Args\n  ref   A mutable Tensor. Must be one of the following types: float32, float64, int64, int32, uint8, uint16, int16, int8, complex64, complex128, qint8, quint8, qint32, half. Should be from a Variable node.  \n  value   A Tensor. Must have the same shape and dtype as ref. The value to be added to the variable.  \n  use_locking   An optional bool. Defaults to False. If True, the addition will be protected by a lock; otherwise the behavior is undefined, but may exhibit less contention.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   Same as \"ref\". Returned as a convenience for operations that want to use the new value after the variable has been updated.  \n  \n"}, {"name": "tf.compat.v1.assign_sub", "path": "compat/v1/assign_sub", "type": "tf.compat", "text": "tf.compat.v1.assign_sub Update ref by subtracting value from it. \ntf.compat.v1.assign_sub(\n    ref, value, use_locking=None, name=None\n)\n This operation outputs ref after the update is done. This makes it easier to chain operations that need to use the reset value. Unlike tf.math.subtract, this op does not broadcast. ref and value must have the same shape.\n \n\n\n Args\n  ref   A mutable Tensor. Must be one of the following types: float32, float64, int64, int32, uint8, uint16, int16, int8, complex64, complex128, qint8, quint8, qint32, half. Should be from a Variable node.  \n  value   A Tensor. Must have the same shape and dtype as ref. The value to be subtracted to the variable.  \n  use_locking   An optional bool. Defaults to False. If True, the subtraction will be protected by a lock; otherwise the behavior is undefined, but may exhibit less contention.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   Same as \"ref\". Returned as a convenience for operations that want to use the new value after the variable has been updated.  \n  \n"}, {"name": "tf.compat.v1.AttrValue", "path": "compat/v1/attrvalue", "type": "tf.compat", "text": "tf.compat.v1.AttrValue A ProtocolMessage\n \n\n\n Attributes\n  b   bool b  \n  f   float f  \n  func   NameAttrList func  \n  i   int64 i  \n  list   ListValue list  \n  placeholder   string placeholder  \n  s   bytes s  \n  shape   TensorShapeProto shape  \n  tensor   TensorProto tensor  \n  type   DataType type    Child Classes class ListValue  \n"}, {"name": "tf.compat.v1.AttrValue.ListValue", "path": "compat/v1/attrvalue/listvalue", "type": "tf.compat", "text": "tf.compat.v1.AttrValue.ListValue A ProtocolMessage\n \n\n\n Attributes\n  b   repeated bool b  \n  f   repeated float f  \n  func   repeated NameAttrList func  \n  i   repeated int64 i  \n  s   repeated bytes s  \n  shape   repeated TensorShapeProto shape  \n  tensor   repeated TensorProto tensor  \n  type   repeated DataType type     \n"}, {"name": "tf.compat.v1.audio", "path": "compat/v1/audio", "type": "tf.compat", "text": "Module: tf.compat.v1.audio Public API for tf.audio namespace. Functions decode_wav(...): Decode a 16-bit PCM WAV file to a float tensor. encode_wav(...): Encode audio data using the WAV file format.  \n"}, {"name": "tf.compat.v1.autograph", "path": "compat/v1/autograph", "type": "tf.compat", "text": "Module: tf.compat.v1.autograph Conversion of plain Python into TensorFlow graph code. \nNote: In TensorFlow 2.0, AutoGraph is automatically applied when using tf.function. This module contains lower-level APIs for advanced use.\n For more information, see the AutoGraph guide. By equivalent graph code we mean code that generates a TensorFlow graph when run. The generated graph has the same effects as the original code when executed (for example with tf.function or tf.compat.v1.Session.run). In other words, using AutoGraph can be thought of as running Python in TensorFlow. Modules experimental module: Public API for tf.autograph.experimental namespace. Functions set_verbosity(...): Sets the AutoGraph verbosity level. to_code(...): Returns the source code generated by AutoGraph, as a string. to_graph(...): Converts a Python entity into a TensorFlow graph. trace(...): Traces argument information at compilation time.  \n"}, {"name": "tf.compat.v1.autograph.experimental", "path": "compat/v1/autograph/experimental", "type": "tf.compat", "text": "Module: tf.compat.v1.autograph.experimental Public API for tf.autograph.experimental namespace. Classes class Feature: This enumeration represents optional conversion options. Functions do_not_convert(...): Decorator that suppresses the conversion of a function. set_loop_options(...): Specifies additional arguments to be passed to the enclosing while_loop.  \n"}, {"name": "tf.compat.v1.autograph.to_code", "path": "compat/v1/autograph/to_code", "type": "tf.compat", "text": "tf.compat.v1.autograph.to_code Returns the source code generated by AutoGraph, as a string. \ntf.compat.v1.autograph.to_code(\n    entity, recursive=True, arg_values=None, arg_types=None, indentation='\n    ', experimental_optional_features=None\n)\n Example usage: \ndef f(x):\n  if x < 0:\n    x = -x\n  return x\ntf.autograph.to_code(f)\n\"...def tf__f(x):...\"\n Also see: tf.autograph.to_graph. \nNote: If a function has been decorated with tf.function, pass its underlying Python function, rather than the callable that `tf.function creates:\n \n@tf.function\ndef f(x):\n  if x < 0:\n    x = -x\n  return x\ntf.autograph.to_code(f.python_function)\n\"...def tf__f(x):...\"\n\n \n\n\n Args\n  entity   Python callable or class.  \n  recursive   Whether to recursively convert any functions that the converted function may call.  \n  arg_values   Deprecated.  \n  arg_types   Deprecated.  \n  indentation   Deprecated.  \n  experimental_optional_features   None, a tuple of, or a single tf.autograph.experimental.Feature value.   \n \n\n\n Returns   The converted code as string.  \n  \n"}, {"name": "tf.compat.v1.autograph.to_graph", "path": "compat/v1/autograph/to_graph", "type": "tf.compat", "text": "tf.compat.v1.autograph.to_graph Converts a Python entity into a TensorFlow graph. \ntf.compat.v1.autograph.to_graph(\n    entity, recursive=True, arg_values=None, arg_types=None,\n    experimental_optional_features=None\n)\n Also see: tf.autograph.to_code, tf.function. Unlike tf.function, to_graph is a low-level transpiler that converts Python code to TensorFlow graph code. It does not implement any caching, variable management or create any actual ops, and is best used where greater control over the generated TensorFlow graph is desired. Another difference from tf.function is that to_graph will not wrap the graph into a TensorFlow function or a Python callable. Internally, tf.function uses to_graph. Example Usage def foo(x):\n  if x > 0:\n    y = x * x\n  else:\n    y = -x\n  return y\n\nconverted_foo = to_graph(foo)\n\nx = tf.constant(1)\ny = converted_foo(x)  # converted_foo is a TensorFlow Op-like.\nassert is_tensor(y)\n Supported Python entities include:  functions classes object methods  Functions are converted into new functions with converted code. Classes are converted by generating a new class whose methods use converted code. Methods are converted into unbound function that have an additional first argument called self.\n \n\n\n Args\n  entity   Python callable or class to convert.  \n  recursive   Whether to recursively convert any functions that the converted function may call.  \n  arg_values   Deprecated.  \n  arg_types   Deprecated.  \n  experimental_optional_features   None, a tuple of, or a single tf.autograph.experimental.Feature value.   \n \n\n\n Returns   Same as entity, the converted Python function or class.  \n\n \n\n\n Raises\n  ValueError   If the entity could not be converted.     \n"}, {"name": "tf.compat.v1.batch_gather", "path": "compat/v1/batch_gather", "type": "tf.compat", "text": "tf.compat.v1.batch_gather Gather slices from params according to indices with leading batch dims. (deprecated) \ntf.compat.v1.batch_gather(\n    params, indices, name=None\n)\n Warning: THIS FUNCTION IS DEPRECATED. It will be removed after 2017-10-25. Instructions for updating: tf.batch_gather is deprecated, please use tf.gather with batch_dims=-1 instead.  \n"}, {"name": "tf.compat.v1.batch_scatter_update", "path": "compat/v1/batch_scatter_update", "type": "tf.compat", "text": "tf.compat.v1.batch_scatter_update Generalization of tf.compat.v1.scatter_update to axis different than 0. (deprecated) \ntf.compat.v1.batch_scatter_update(\n    ref, indices, updates, use_locking=True, name=None\n)\n Warning: THIS FUNCTION IS DEPRECATED. It will be removed after 2018-11-29. Instructions for updating: Use the batch_scatter_update method of Variable instead. Analogous to batch_gather. This assumes that ref, indices and updates have a series of leading dimensions that are the same for all of them, and the updates are performed on the last dimension of indices. In other words, the dimensions should be the following: num_prefix_dims = indices.ndims - 1 batch_dim = num_prefix_dims + 1 updates.shape = indices.shape + var.shape[batch_dim:] where updates.shape[:num_prefix_dims] == indices.shape[:num_prefix_dims] == var.shape[:num_prefix_dims] And the operation performed can be expressed as: var[i_1, ..., i_n, indices[i_1, ..., i_n, j]] = updates[i_1, ..., i_n, j] When indices is a 1D tensor, this operation is equivalent to tf.compat.v1.scatter_update. To avoid this operation there would be 2 alternatives: 1) Reshaping the variable by merging the first ndims dimensions. However, this is not possible because tf.reshape returns a Tensor, which we cannot use tf.compat.v1.scatter_update on. 2) Looping over the first ndims of the variable and using tf.compat.v1.scatter_update on the subtensors that result of slicing the first dimension. This is a valid option for ndims = 1, but less efficient than this implementation. See also tf.compat.v1.scatter_update and tf.compat.v1.scatter_nd_update.\n \n\n\n Args\n  ref   Variable to scatter onto.  \n  indices   Tensor containing indices as described above.  \n  updates   Tensor of updates to apply to ref.  \n  use_locking   Boolean indicating whether to lock the writing operation.  \n  name   Optional scope name string.   \n \n\n\n Returns   Ref to variable after it has been modified.  \n\n \n\n\n Raises\n  ValueError   If the initial ndims of ref, indices, and updates are not the same.     \n"}, {"name": "tf.compat.v1.batch_to_space", "path": "compat/v1/batch_to_space", "type": "tf.compat", "text": "tf.compat.v1.batch_to_space BatchToSpace for 4-D tensors of type T. \ntf.compat.v1.batch_to_space(\n    input, crops, block_size, name=None, block_shape=None\n)\n This is a legacy version of the more general BatchToSpaceND. Rearranges (permutes) data from batch into blocks of spatial data, followed by cropping. This is the reverse transformation of SpaceToBatch. More specifically, this op outputs a copy of the input tensor where values from the batch dimension are moved in spatial blocks to the height and width dimensions, followed by cropping along the height and width dimensions.\n \n\n\n Args\n  input   A Tensor. 4-D tensor with shape [batch*block_size*block_size, height_pad/block_size, width_pad/block_size, depth]. Note that the batch size of the input tensor must be divisible by block_size * block_size.  \n  crops   A Tensor. Must be one of the following types: int32, int64. 2-D tensor of non-negative integers with shape [2, 2]. It specifies how many elements to crop from the intermediate result across the spatial dimensions as follows: crops = [[crop_top, crop_bottom], [crop_left, crop_right]] \n \n  block_size   An int that is >= 2.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as input.  \n  \n"}, {"name": "tf.compat.v1.batch_to_space_nd", "path": "compat/v1/batch_to_space_nd", "type": "tf.compat", "text": "tf.compat.v1.batch_to_space_nd BatchToSpace for N-D tensors of type T.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.manip.batch_to_space_nd  \ntf.compat.v1.batch_to_space_nd(\n    input, block_shape, crops, name=None\n)\n This operation reshapes the \"batch\" dimension 0 into M + 1 dimensions of shape block_shape + [batch], interleaves these blocks back into the grid defined by the spatial dimensions [1, ..., M], to obtain a result with the same rank as the input. The spatial dimensions of this intermediate result are then optionally cropped according to crops to produce the output. This is the reverse of SpaceToBatch. See below for a precise description.\n \n\n\n Args\n  input   A Tensor. N-D with shape input_shape = [batch] + spatial_shape + remaining_shape, where spatial_shape has M dimensions.  \n  block_shape   A Tensor. Must be one of the following types: int32, int64. 1-D with shape [M], all values must be >= 1.  \n  crops   A Tensor. Must be one of the following types: int32, int64. 2-D with shape [M, 2], all values must be >= 0. crops[i] = [crop_start, crop_end] specifies the amount to crop from input dimension i + 1, which corresponds to spatial dimension i. It is required that crop_start[i] + crop_end[i] <= block_shape[i] * input_shape[i + 1]. This operation is equivalent to the following steps:  Reshape input to reshaped of shape: [block_shape[0], ..., block_shape[M-1], batch / prod(block_shape), input_shape[1], ..., input_shape[N-1]] Permute dimensions of reshaped to produce permuted of shape [batch / prod(block_shape),  input_shape[1], block_shape[0], ..., input_shape[M], block_shape[M-1], input_shape[M+1], ..., input_shape[N-1]]  Reshape permuted to produce reshaped_permuted of shape [batch / prod(block_shape),  input_shape[1] * block_shape[0], ..., input_shape[M] * block_shape[M-1], input_shape[M+1], ..., input_shape[N-1]]  Crop the start and end of dimensions [1, ..., M] of reshaped_permuted according to crops to produce the output of shape: [batch / prod(block_shape),  input_shape[1] * block_shape[0] - crops[0,0] - crops[0,1], ..., input_shape[M] * block_shape[M-1] - crops[M-1,0] - crops[M-1,1], input_shape[M+1], ..., input_shape[N-1]] Some examples: (1) For the following input of shape [4, 1, 1, 1], block_shape = [2, 2], and crops = [[0, 0], [0, 0]]: [[[[1]]], [[[2]]], [[[3]]], [[[4]]]]\n The output tensor has shape [1, 2, 2, 1] and value: x = [[[[1], [2]], [[3], [4]]]]\n (2) For the following input of shape [4, 1, 1, 3], block_shape = [2, 2], and crops = [[0, 0], [0, 0]]: [[[[1, 2, 3]]], [[[4, 5, 6]]], [[[7, 8, 9]]], [[[10, 11, 12]]]]\n The output tensor has shape [1, 2, 2, 3] and value: x = [[[[1, 2, 3], [4, 5, 6]],\n[[7, 8, 9], [10, 11, 12]]]]\n (3) For the following input of shape [4, 2, 2, 1], block_shape = [2, 2], and crops = [[0, 0], [0, 0]]: x = [[[[1], [3]], [[9], [11]]],\n[[[2], [4]], [[10], [12]]],\n[[[5], [7]], [[13], [15]]],\n[[[6], [8]], [[14], [16]]]]\n The output tensor has shape [1, 4, 4, 1] and value: x = [[[[1],   [2],  [3],  [4]],\n[[5],   [6],  [7],  [8]],\n[[9],  [10], [11],  [12]],\n[[13], [14], [15],  [16]]]]\n (4) For the following input of shape [8, 1, 3, 1], block_shape = [2, 2], and crops = [[0, 0], [2, 0]]: x = [[[[0], [1], [3]]], [[[0], [9], [11]]],\n[[[0], [2], [4]]], [[[0], [10], [12]]],\n[[[0], [5], [7]]], [[[0], [13], [15]]],\n[[[0], [6], [8]]], [[[0], [14], [16]]]]\n The output tensor has shape [2, 2, 4, 1] and value: x = [[[[1],   [2],  [3],  [4]],\n[[5],   [6],  [7],  [8]]],\n[[[9],  [10], [11],  [12]],\n[[13], [14], [15],  [16]]]]\n\n \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as input.  \n  \n"}, {"name": "tf.compat.v1.bincount", "path": "compat/v1/bincount", "type": "tf.compat", "text": "tf.compat.v1.bincount Counts the number of occurrences of each value in an integer array.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.math.bincount  \ntf.compat.v1.bincount(\n    arr, weights=None, minlength=None, maxlength=None, dtype=tf.dtypes.int32\n)\n If minlength and maxlength are not given, returns a vector with length tf.reduce_max(arr) + 1 if arr is non-empty, and length 0 otherwise. If weights are non-None, then index i of the output stores the sum of the value in weights at each index where the corresponding value in arr is i.\n \n\n\n Args\n  arr   An int32 tensor of non-negative values.  \n  weights   If non-None, must be the same shape as arr. For each value in arr, the bin will be incremented by the corresponding weight instead of 1.  \n  minlength   If given, ensures the output has length at least minlength, padding with zeros at the end if necessary.  \n  maxlength   If given, skips values in arr that are equal or greater than maxlength, ensuring that the output has length at most maxlength.  \n  dtype   If weights is None, determines the type of the output bins.   \n \n\n\n Returns   A vector with the same dtype as weights or the given dtype. The bin values.  \n  \n"}, {"name": "tf.compat.v1.bitwise", "path": "compat/v1/bitwise", "type": "tf.compat", "text": "Module: tf.compat.v1.bitwise Operations for manipulating the binary representations of integers. Functions bitwise_and(...): Elementwise computes the bitwise AND of x and y. bitwise_or(...): Elementwise computes the bitwise OR of x and y. bitwise_xor(...): Elementwise computes the bitwise XOR of x and y. invert(...): Invert (flip) each bit of supported types; for example, type uint8 value 01010101 becomes 10101010. left_shift(...): Elementwise computes the bitwise left-shift of x and y. right_shift(...): Elementwise computes the bitwise right-shift of x and y.  \n"}, {"name": "tf.compat.v1.boolean_mask", "path": "compat/v1/boolean_mask", "type": "tf.compat", "text": "tf.compat.v1.boolean_mask Apply boolean mask to tensor. \ntf.compat.v1.boolean_mask(\n    tensor, mask, name='boolean_mask', axis=None\n)\n Numpy equivalent is tensor[mask]. In general, 0 < dim(mask) = K <= dim(tensor), and mask's shape must match the first K dimensions of tensor's shape. We then have: boolean_mask(tensor, mask)[i, j1,...,jd] = tensor[i1,...,iK,j1,...,jd] where (i1,...,iK) is the ith True entry of mask (row-major order). The axis could be used with mask to indicate the axis to mask from. In that case, axis + dim(mask) <= dim(tensor) and mask's shape must match the first axis + dim(mask) dimensions of tensor's shape. See also: tf.ragged.boolean_mask, which can be applied to both dense and ragged tensors, and can be used if you need to preserve the masked dimensions of tensor (rather than flattening them, as tf.boolean_mask does). Examples: # 1-D example\ntensor = [0, 1, 2, 3]\nmask = np.array([True, False, True, False])\ntf.boolean_mask(tensor, mask)  # [0, 2]\n\n# 2-D example\ntensor = [[1, 2], [3, 4], [5, 6]]\nmask = np.array([True, False, True])\ntf.boolean_mask(tensor, mask)  # [[1, 2], [5, 6]]\n\n \n\n\n Args\n  tensor   N-D Tensor.  \n  mask   K-D boolean Tensor, K <= N and K must be known statically.  \n  name   A name for this operation (optional).  \n  axis   A 0-D int Tensor representing the axis in tensor to mask from. By default, axis is 0 which will mask from the first dimension. Otherwise K + axis <= N.   \n \n\n\n Returns   (N-K+1)-dimensional tensor populated by entries in tensor corresponding to True values in mask.  \n\n \n\n\n Raises\n  ValueError   If shapes do not conform.     \n"}, {"name": "tf.compat.v1.case", "path": "compat/v1/case", "type": "tf.compat", "text": "tf.compat.v1.case Create a case operation. \ntf.compat.v1.case(\n    pred_fn_pairs, default=None, exclusive=False, strict=False,\n    name='case'\n)\n See also tf.switch_case. The pred_fn_pairs parameter is a dict or list of pairs of size N. Each pair contains a boolean scalar tensor and a python callable that creates the tensors to be returned if the boolean evaluates to True. default is a callable generating a list of tensors. All the callables in pred_fn_pairs as well as default (if provided) should return the same number and types of tensors. If exclusive==True, all predicates are evaluated, and an exception is thrown if more than one of the predicates evaluates to True. If exclusive==False, execution stops at the first predicate which evaluates to True, and the tensors generated by the corresponding function are returned immediately. If none of the predicates evaluate to True, this operation returns the tensors generated by default. tf.case supports nested structures as implemented in tf.contrib.framework.nest. All of the callables must return the same (possibly nested) value structure of lists, tuples, and/or named tuples. Singleton lists and tuples form the only exceptions to this: when returned by a callable, they are implicitly unpacked to single values. This behavior is disabled by passing strict=True. If an unordered dictionary is used for pred_fn_pairs, the order of the conditional tests is not guaranteed. However, the order is guaranteed to be deterministic, so that variables created in conditional branches are created in fixed order across runs. Example 1: Pseudocode: if (x < y) return 17;\nelse return 23;\n Expressions: f1 = lambda: tf.constant(17)\nf2 = lambda: tf.constant(23)\nr = tf.case([(tf.less(x, y), f1)], default=f2)\n Example 2: Pseudocode: if (x < y && x > z) raise OpError(\"Only one predicate may evaluate to True\");\nif (x < y) return 17;\nelse if (x > z) return 23;\nelse return -1;\n Expressions: def f1(): return tf.constant(17)\ndef f2(): return tf.constant(23)\ndef f3(): return tf.constant(-1)\nr = tf.case({tf.less(x, y): f1, tf.greater(x, z): f2},\n         default=f3, exclusive=True)\n\n \n\n\n Args\n  pred_fn_pairs   Dict or list of pairs of a boolean scalar tensor and a callable which returns a list of tensors.  \n  default   Optional callable that returns a list of tensors.  \n  exclusive   True iff at most one predicate is allowed to evaluate to True.  \n  strict   A boolean that enables/disables 'strict' mode; see above.  \n  name   A name for this operation (optional).   \n \n\n\n Returns   The tensors returned by the first pair whose predicate evaluated to True, or those returned by default if none does.  \n\n \n\n\n Raises\n  TypeError   If pred_fn_pairs is not a list/dictionary.  \n  TypeError   If pred_fn_pairs is a list but does not contain 2-tuples.  \n  TypeError   If fns[i] is not callable for any i, or default is not callable.    Eager Compatibility Unordered dictionaries are not supported in eager mode when exclusive=False. Use a list of tuples instead.  \n"}, {"name": "tf.compat.v1.clip_by_average_norm", "path": "compat/v1/clip_by_average_norm", "type": "tf.compat", "text": "tf.compat.v1.clip_by_average_norm Clips tensor values to a maximum average L2-norm. (deprecated) \ntf.compat.v1.clip_by_average_norm(\n    t, clip_norm, name=None\n)\n Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: clip_by_average_norm is deprecated in TensorFlow 2.0. Please use clip_by_norm(t, clip_norm * tf.cast(tf.size(t), tf.float32), name) instead. Given a tensor t, and a maximum clip value clip_norm, this operation normalizes t so that its average L2-norm is less than or equal to clip_norm. Specifically, if the average L2-norm is already less than or equal to clip_norm, then t is not modified. If the average L2-norm is greater than clip_norm, then this operation returns a tensor of the same type and shape as t with its values set to: t * clip_norm / l2norm_avg(t) In this case, the average L2-norm of the output tensor is clip_norm. This operation is typically used to clip gradients before applying them with an optimizer.\n \n\n\n Args\n  t   A Tensor.  \n  clip_norm   A 0-D (scalar) Tensor > 0. A maximum clipping value.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A clipped Tensor.  \n  \n"}, {"name": "tf.compat.v1.colocate_with", "path": "compat/v1/colocate_with", "type": "tf.compat", "text": "tf.compat.v1.colocate_with DEPRECATED FUNCTION \ntf.compat.v1.colocate_with(\n    op, ignore_existing=False\n)\n Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Colocations handled automatically by placer.  \n"}, {"name": "tf.compat.v1.compat", "path": "compat/v1/compat", "type": "tf.compat", "text": "Module: tf.compat.v1.compat Compatibility functions. The tf.compat module contains two sets of compatibility functions. Tensorflow 1.x and 2.x APIs The compat.v1 and compat.v2 submodules provide a complete copy of both the v1 and v2 APIs for backwards and forwards compatibility across TensorFlow versions 1.x and 2.x. See the migration guide for details. Utilities for writing compatible code Aside from the compat.v1 and compat.v2 submodules, tf.compat also contains a set of helper functions for writing code that works in both:  TensorFlow 1.x and 2.x Python 2 and 3  Type collections The compatibility module also provides the following aliases for common sets of python types:  bytes_or_text_types complex_types integral_types real_types  Functions as_bytes(...): Converts bytearray, bytes, or unicode python input types to bytes. as_str(...) as_str_any(...): Converts input to str type. as_text(...): Converts any string-like python input types to unicode. dimension_at_index(...): Compatibility utility required to allow for both V1 and V2 behavior in TF. dimension_value(...): Compatibility utility required to allow for both V1 and V2 behavior in TF. forward_compatibility_horizon(...): Context manager for testing forward compatibility of generated graphs. forward_compatible(...): Return true if the forward compatibility window has expired. path_to_str(...): Converts input which is a PathLike object to str type.\n \n\n\n Other Members\n  bytes_or_text_types  \n \n  complex_types  \n \n  integral_types  \n \n  real_types  \n    \n"}, {"name": "tf.compat.v1.cond", "path": "compat/v1/cond", "type": "tf.compat", "text": "tf.compat.v1.cond Return true_fn() if the predicate pred is true else false_fn(). (deprecated arguments) \ntf.compat.v1.cond(\n    pred, true_fn=None, false_fn=None, strict=False, name=None, fn1=None, fn2=None\n)\n Warning: SOME ARGUMENTS ARE DEPRECATED: (fn1, fn2). They will be removed in a future version. Instructions for updating: fn1/fn2 are deprecated in favor of the true_fn/false_fn arguments. true_fn and false_fn both return lists of output tensors. true_fn and false_fn must have the same non-zero number and type of outputs. Warning: Any Tensors or Operations created outside of true_fn and false_fn will be executed regardless of which branch is selected at runtime. Although this behavior is consistent with the dataflow model of TensorFlow, it has frequently surprised users who expected a lazier semantics. Consider the following simple program: z = tf.multiply(a, b)\nresult = tf.cond(x < y, lambda: tf.add(x, z), lambda: tf.square(y))\n If x < y, the tf.add operation will be executed and tf.square operation will not be executed. Since z is needed for at least one branch of the cond, the tf.multiply operation is always executed, unconditionally. Note that cond calls true_fn and false_fn exactly once (inside the call to cond, and not at all during Session.run()). cond stitches together the graph fragments created during the true_fn and false_fn calls with some additional graph nodes to ensure that the right branch gets executed depending on the value of pred. tf.cond supports nested structures as implemented in tensorflow.python.util.nest. Both true_fn and false_fn must return the same (possibly nested) value structure of lists, tuples, and/or named tuples. Singleton lists and tuples form the only exceptions to this: when returned by true_fn and/or false_fn, they are implicitly unpacked to single values. This behavior is disabled by passing strict=True.\n \n\n\n Args\n  pred   A scalar determining whether to return the result of true_fn or false_fn.  \n  true_fn   The callable to be performed if pred is true.  \n  false_fn   The callable to be performed if pred is false.  \n  strict   A boolean that enables/disables 'strict' mode; see above.  \n  name   Optional name prefix for the returned tensors.   \n \n\n\n Returns   Tensors returned by the call to either true_fn or false_fn. If the callables return a singleton list, the element is extracted from the list.  \n\n \n\n\n Raises\n  TypeError   if true_fn or false_fn is not callable.  \n  ValueError   if true_fn and false_fn do not return the same number of tensors, or return tensors of different types.    Example: x = tf.constant(2)\ny = tf.constant(5)\ndef f1(): return tf.multiply(x, 17)\ndef f2(): return tf.add(y, 23)\nr = tf.cond(tf.less(x, y), f1, f2)\n# r is set to f1().\n# Operations in f2 (e.g., tf.add) are not executed.\n  \n"}, {"name": "tf.compat.v1.ConditionalAccumulator", "path": "compat/v1/conditionalaccumulator", "type": "tf.compat", "text": "tf.compat.v1.ConditionalAccumulator A conditional accumulator for aggregating gradients. Inherits From: ConditionalAccumulatorBase \ntf.compat.v1.ConditionalAccumulator(\n    dtype, shape=None, shared_name=None, name='conditional_accumulator',\n    reduction_type='MEAN'\n)\n Up-to-date gradients (i.e., time step at which gradient was computed is equal to the accumulator's time step) are added to the accumulator. Extraction of the average gradient is blocked until the required number of gradients has been accumulated.\n \n\n\n Args\n  dtype   Datatype of the accumulated gradients.  \n  shape   Shape of the accumulated gradients.  \n  shared_name   Optional. If non-empty, this accumulator will be shared under the given name across multiple sessions.  \n  name   Optional name for the accumulator.  \n  reduction_type   Reduction type to use when taking the gradient.   \n \n\n\n Attributes\n  accumulator_ref   The underlying accumulator reference.  \n  dtype   The datatype of the gradients accumulated by this accumulator.  \n  name   The name of the underlying accumulator.    Methods apply_grad View source \napply_grad(\n    grad, local_step=0, name=None\n)\n Attempts to apply a gradient to the accumulator. The attempt is silently dropped if the gradient is stale, i.e., local_step is less than the accumulator's global time step.\n \n\n\n Args\n  grad   The gradient tensor to be applied.  \n  local_step   Time step at which the gradient was computed.  \n  name   Optional name for the operation.   \n \n\n\n Returns   The operation that (conditionally) applies a gradient to the accumulator.  \n\n \n\n\n Raises\n  ValueError   If grad is of the wrong shape    num_accumulated View source \nnum_accumulated(\n    name=None\n)\n Number of gradients that have currently been aggregated in accumulator.\n \n\n\n Args\n  name   Optional name for the operation.   \n \n\n\n Returns   Number of accumulated gradients currently in accumulator.  \n set_global_step View source \nset_global_step(\n    new_global_step, name=None\n)\n Sets the global time step of the accumulator. The operation logs a warning if we attempt to set to a time step that is lower than the accumulator's own time step.\n \n\n\n Args\n  new_global_step   Value of new time step. Can be a variable or a constant  \n  name   Optional name for the operation.   \n \n\n\n Returns   Operation that sets the accumulator's time step.  \n take_grad View source \ntake_grad(\n    num_required, name=None\n)\n Attempts to extract the average gradient from the accumulator. The operation blocks until sufficient number of gradients have been successfully applied to the accumulator. Once successful, the following actions are also triggered:  Counter of accumulated gradients is reset to 0. Aggregated gradient is reset to 0 tensor. Accumulator's internal time step is incremented by 1. \n \n\n\n Args\n  num_required   Number of gradients that needs to have been aggregated  \n  name   Optional name for the operation   \n \n\n\n Returns   A tensor holding the value of the average gradient.  \n\n \n\n\n Raises\n  InvalidArgumentError   If num_required < 1     \n"}, {"name": "tf.compat.v1.ConditionalAccumulatorBase", "path": "compat/v1/conditionalaccumulatorbase", "type": "tf.compat", "text": "tf.compat.v1.ConditionalAccumulatorBase A conditional accumulator for aggregating gradients. \ntf.compat.v1.ConditionalAccumulatorBase(\n    dtype, shape, accumulator_ref\n)\n Up-to-date gradients (i.e., time step at which gradient was computed is equal to the accumulator's time step) are added to the accumulator. Extraction of the average gradient is blocked until the required number of gradients has been accumulated.\n \n\n\n Args\n  dtype   Datatype of the accumulated gradients.  \n  shape   Shape of the accumulated gradients.  \n  accumulator_ref   A handle to the conditional accumulator, created by sub- classes   \n \n\n\n Attributes\n  accumulator_ref   The underlying accumulator reference.  \n  dtype   The datatype of the gradients accumulated by this accumulator.  \n  name   The name of the underlying accumulator.    Methods num_accumulated View source \nnum_accumulated(\n    name=None\n)\n Number of gradients that have currently been aggregated in accumulator.\n \n\n\n Args\n  name   Optional name for the operation.   \n \n\n\n Returns   Number of accumulated gradients currently in accumulator.  \n set_global_step View source \nset_global_step(\n    new_global_step, name=None\n)\n Sets the global time step of the accumulator. The operation logs a warning if we attempt to set to a time step that is lower than the accumulator's own time step.\n \n\n\n Args\n  new_global_step   Value of new time step. Can be a variable or a constant  \n  name   Optional name for the operation.   \n \n\n\n Returns   Operation that sets the accumulator's time step.  \n  \n"}, {"name": "tf.compat.v1.config", "path": "compat/v1/config", "type": "tf.compat", "text": "Module: tf.compat.v1.config Public API for tf.config namespace. Modules experimental module: Public API for tf.config.experimental namespace. optimizer module: Public API for tf.config.optimizer namespace. threading module: Public API for tf.config.threading namespace. Classes class LogicalDevice: Abstraction for a logical device initialized by the runtime. class LogicalDeviceConfiguration: Configuration class for a logical devices. class PhysicalDevice: Abstraction for a locally visible physical device. Functions experimental_connect_to_cluster(...): Connects to the given cluster. experimental_connect_to_host(...): Connects to a single machine to enable remote execution on it. experimental_functions_run_eagerly(...): Returns the value of the experimental_run_functions_eagerly setting. (deprecated) experimental_run_functions_eagerly(...): Enables / disables eager execution of tf.functions. (deprecated) functions_run_eagerly(...): Returns the value of the run_functions_eagerly setting. get_logical_device_configuration(...): Get the virtual device configuration for a tf.config.PhysicalDevice. get_soft_device_placement(...): Get if soft device placement is enabled. get_visible_devices(...): Get the list of visible physical devices. list_logical_devices(...): Return a list of logical devices created by runtime. list_physical_devices(...): Return a list of physical devices visible to the host runtime. run_functions_eagerly(...): Enables / disables eager execution of tf.functions. set_logical_device_configuration(...): Set the logical device configuration for a tf.config.PhysicalDevice. set_soft_device_placement(...): Set if soft device placement is enabled. set_visible_devices(...): Set the list of visible devices.  \n"}, {"name": "tf.compat.v1.config.experimental", "path": "compat/v1/config/experimental", "type": "tf.compat", "text": "Module: tf.compat.v1.config.experimental Public API for tf.config.experimental namespace. Classes class ClusterDeviceFilters: Represent a collection of device filters for the remote workers in cluster. class VirtualDeviceConfiguration: Configuration class for a logical devices. Functions disable_mlir_bridge(...): Disables experimental MLIR-Based TensorFlow Compiler Bridge. disable_mlir_graph_optimization(...): Disables experimental MLIR-Based TensorFlow Compiler Optimizations. enable_mlir_bridge(...): Enables experimental MLIR-Based TensorFlow Compiler Bridge. enable_mlir_graph_optimization(...): Enables experimental MLIR-Based TensorFlow Compiler Optimizations. enable_tensor_float_32_execution(...): Enable or disable the use of TensorFloat-32 on supported hardware. get_device_details(...): Returns details about a physical devices. get_device_policy(...): Gets the current device policy. get_memory_growth(...): Get if memory growth is enabled for a PhysicalDevice. get_memory_usage(...): Get the memory usage, in bytes, for the chosen device. get_synchronous_execution(...): Gets whether operations are executed synchronously or asynchronously. get_virtual_device_configuration(...): Get the virtual device configuration for a tf.config.PhysicalDevice. get_visible_devices(...): Get the list of visible physical devices. list_logical_devices(...): Return a list of logical devices created by runtime. list_physical_devices(...): Return a list of physical devices visible to the host runtime. set_device_policy(...): Sets the current thread device policy. set_memory_growth(...): Set if memory growth should be enabled for a PhysicalDevice. set_synchronous_execution(...): Specifies whether operations are executed synchronously or asynchronously. set_virtual_device_configuration(...): Set the logical device configuration for a tf.config.PhysicalDevice. set_visible_devices(...): Set the list of visible devices. tensor_float_32_execution_enabled(...): Returns whether TensorFloat-32 is enabled.  \n"}, {"name": "tf.compat.v1.config.optimizer", "path": "compat/v1/config/optimizer", "type": "tf.compat", "text": "Module: tf.compat.v1.config.optimizer Public API for tf.config.optimizer namespace. Functions get_experimental_options(...): Get experimental optimizer options. get_jit(...): Get if JIT compilation is enabled. set_experimental_options(...): Set experimental optimizer options. set_jit(...): Set if JIT compilation is enabled.  \n"}, {"name": "tf.compat.v1.config.threading", "path": "compat/v1/config/threading", "type": "tf.compat", "text": "Module: tf.compat.v1.config.threading Public API for tf.config.threading namespace. Functions get_inter_op_parallelism_threads(...): Get number of threads used for parallelism between independent operations. get_intra_op_parallelism_threads(...): Get number of threads used within an individual op for parallelism. set_inter_op_parallelism_threads(...): Set number of threads used for parallelism between independent operations. set_intra_op_parallelism_threads(...): Set number of threads used within an individual op for parallelism.  \n"}, {"name": "tf.compat.v1.ConfigProto", "path": "compat/v1/configproto", "type": "tf.compat", "text": "tf.compat.v1.ConfigProto A ProtocolMessage\n \n\n\n Attributes\n  allow_soft_placement   bool allow_soft_placement  \n  cluster_def   ClusterDef cluster_def  \n  device_count   repeated DeviceCountEntry device_count  \n  device_filters   repeated string device_filters  \n  experimental   Experimental experimental  \n  gpu_options   GPUOptions gpu_options  \n  graph_options   GraphOptions graph_options  \n  inter_op_parallelism_threads   int32 inter_op_parallelism_threads  \n  intra_op_parallelism_threads   int32 intra_op_parallelism_threads  \n  isolate_session_state   bool isolate_session_state  \n  log_device_placement   bool log_device_placement  \n  operation_timeout_in_ms   int64 operation_timeout_in_ms  \n  placement_period   int32 placement_period  \n  rpc_options   RPCOptions rpc_options  \n  session_inter_op_thread_pool   repeated ThreadPoolOptionProto session_inter_op_thread_pool  \n  share_cluster_devices_in_session   bool share_cluster_devices_in_session  \n  use_per_session_threads   bool use_per_session_threads    Child Classes class DeviceCountEntry class Experimental  \n"}, {"name": "tf.compat.v1.ConfigProto.DeviceCountEntry", "path": "compat/v1/configproto/devicecountentry", "type": "tf.compat", "text": "tf.compat.v1.ConfigProto.DeviceCountEntry A ProtocolMessage\n \n\n\n Attributes\n  key   string key  \n  value   int32 value     \n"}, {"name": "tf.compat.v1.ConfigProto.Experimental", "path": "compat/v1/configproto/experimental", "type": "tf.compat", "text": "tf.compat.v1.ConfigProto.Experimental A ProtocolMessage\n \n\n\n Attributes\n  collective_deterministic_sequential_execution   bool collective_deterministic_sequential_execution  \n  collective_group_leader   string collective_group_leader  \n  collective_nccl   bool collective_nccl  \n  disable_output_partition_graphs   bool disable_output_partition_graphs  \n  disable_thread_spinning   bool disable_thread_spinning  \n  enable_mlir_bridge   bool enable_mlir_bridge  \n  enable_mlir_graph_optimization   bool enable_mlir_graph_optimization  \n  executor_type   string executor_type  \n  mlir_bridge_rollout   MlirBridgeRollout mlir_bridge_rollout  \n  optimize_for_static_graph   bool optimize_for_static_graph  \n  recv_buf_max_chunk   int32 recv_buf_max_chunk  \n  session_metadata   SessionMetadata session_metadata  \n  share_cluster_devices_in_session   bool share_cluster_devices_in_session  \n  share_session_state_in_clusterspec_propagation   bool share_session_state_in_clusterspec_propagation  \n  use_numa_affinity   bool use_numa_affinity  \n  xla_fusion_autotuner_thresh   int64 xla_fusion_autotuner_thresh   \n \n\n\n Class Variables\n  MLIR_BRIDGE_ROLLOUT_DISABLED   2  \n  MLIR_BRIDGE_ROLLOUT_ENABLED   1  \n  MLIR_BRIDGE_ROLLOUT_UNSPECIFIED   0  \n  MlirBridgeRollout  \n    \n"}, {"name": "tf.compat.v1.confusion_matrix", "path": "compat/v1/confusion_matrix", "type": "tf.compat", "text": "tf.compat.v1.confusion_matrix Computes the confusion matrix from predictions and labels.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.math.confusion_matrix  \ntf.compat.v1.confusion_matrix(\n    labels, predictions, num_classes=None, dtype=tf.dtypes.int32, name=None,\n    weights=None\n)\n The matrix columns represent the prediction labels and the rows represent the real labels. The confusion matrix is always a 2-D array of shape [n, n], where n is the number of valid labels for a given classification task. Both prediction and labels must be 1-D arrays of the same shape in order for this function to work. If num_classes is None, then num_classes will be set to one plus the maximum value in either predictions or labels. Class labels are expected to start at 0. For example, if num_classes is 3, then the possible labels would be [0, 1, 2]. If weights is not None, then each prediction contributes its corresponding weight to the total value of the confusion matrix cell. For example: tf.math.confusion_matrix([1, 2, 4], [2, 2, 4]) ==>\n    [[0 0 0 0 0]\n     [0 0 1 0 0]\n     [0 0 1 0 0]\n     [0 0 0 0 0]\n     [0 0 0 0 1]]\n Note that the possible labels are assumed to be [0, 1, 2, 3, 4], resulting in a 5x5 confusion matrix.\n \n\n\n Args\n  labels   1-D Tensor of real labels for the classification task.  \n  predictions   1-D Tensor of predictions for a given classification.  \n  num_classes   The possible number of labels the classification task can have. If this value is not provided, it will be calculated using both predictions and labels array.  \n  dtype   Data type of the confusion matrix.  \n  name   Scope name.  \n  weights   An optional Tensor whose shape matches predictions.   \n \n\n\n Returns   A Tensor of type dtype with shape [n, n] representing the confusion matrix, where n is the number of possible labels in the classification task.  \n\n \n\n\n Raises\n  ValueError   If both predictions and labels are not 1-D vectors and have mismatched shapes, or if weights is not None and its shape doesn't match predictions.     \n"}, {"name": "tf.compat.v1.constant", "path": "compat/v1/constant", "type": "tf.compat", "text": "tf.compat.v1.constant Creates a constant tensor. \ntf.compat.v1.constant(\n    value, dtype=None, shape=None, name='Const', verify_shape=False\n)\n The resulting tensor is populated with values of type dtype, as specified by arguments value and (optionally) shape (see examples below). The argument value can be a constant value, or a list of values of type dtype. If value is a list, then the length of the list must be less than or equal to the number of elements implied by the shape argument (if specified). In the case where the list length is less than the number of elements specified by shape, the last element in the list will be used to fill the remaining entries. The argument shape is optional. If present, it specifies the dimensions of the resulting tensor. If not present, the shape of value is used. If the argument dtype is not specified, then the type is inferred from the type of value. For example: # Constant 1-D Tensor populated with value list.\ntensor = tf.constant([1, 2, 3, 4, 5, 6, 7]) => [1 2 3 4 5 6 7]\n\n# Constant 2-D tensor populated with scalar value -1.\ntensor = tf.constant(-1.0, shape=[2, 3]) => [[-1. -1. -1.]\n                                             [-1. -1. -1.]]\n tf.constant differs from tf.fill in a few ways:  \ntf.constant supports arbitrary constants, not just uniform scalar Tensors like tf.fill. \ntf.constant creates a Const node in the computation graph with the exact value at graph construction time. On the other hand, tf.fill creates an Op in the graph that is expanded at runtime. Because tf.constant only embeds constant values in the graph, it does not support dynamic shapes based on other runtime Tensors, whereas tf.fill does. \n \n\n\n Args\n  value   A constant value (or list) of output type dtype.  \n  dtype   The type of the elements of the resulting tensor.  \n  shape   Optional dimensions of resulting tensor.  \n  name   Optional name for the tensor.  \n  verify_shape   Boolean that enables verification of a shape of values.   \n \n\n\n Returns   A Constant Tensor.  \n\n \n\n\n Raises\n  TypeError   if shape is incorrectly specified or unsupported.     \n"}, {"name": "tf.compat.v1.container", "path": "compat/v1/container", "type": "tf.compat", "text": "tf.compat.v1.container Wrapper for Graph.container() using the default graph. \ntf.compat.v1.container(\n    container_name\n)\n\n \n\n\n Args\n  container_name   The container string to use in the context.   \n \n\n\n Returns   A context manager that specifies the default container to use for newly created stateful ops.  \n  \n"}, {"name": "tf.compat.v1.control_flow_v2_enabled", "path": "compat/v1/control_flow_v2_enabled", "type": "tf.compat", "text": "tf.compat.v1.control_flow_v2_enabled Returns True if v2 control flow is enabled. \ntf.compat.v1.control_flow_v2_enabled()\n \nNote: v2 control flow is always enabled inside of tf.function.\n  \n"}, {"name": "tf.compat.v1.convert_to_tensor", "path": "compat/v1/convert_to_tensor", "type": "tf.compat", "text": "tf.compat.v1.convert_to_tensor Converts the given value to a Tensor. \ntf.compat.v1.convert_to_tensor(\n    value, dtype=None, name=None, preferred_dtype=None, dtype_hint=None\n)\n This function converts Python objects of various types to Tensor objects. It accepts Tensor objects, numpy arrays, Python lists, and Python scalars. For example: import numpy as np\n\ndef my_func(arg):\n  arg = tf.convert_to_tensor(arg, dtype=tf.float32)\n  return tf.matmul(arg, arg) + arg\n\n# The following calls are equivalent.\nvalue_1 = my_func(tf.constant([[1.0, 2.0], [3.0, 4.0]]))\nvalue_2 = my_func([[1.0, 2.0], [3.0, 4.0]])\nvalue_3 = my_func(np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32))\n This function can be useful when composing a new operation in Python (such as my_func in the example above). All standard Python op constructors apply this function to each of their Tensor-valued inputs, which allows those ops to accept numpy arrays, Python lists, and scalars in addition to Tensor objects. \nNote: This function diverges from default Numpy behavior for float and string types when None is present in a Python list or scalar. Rather than silently converting None values, an error will be thrown.\n\n \n\n\n Args\n  value   An object whose type has a registered Tensor conversion function.  \n  dtype   Optional element type for the returned tensor. If missing, the type is inferred from the type of value.  \n  name   Optional name to use if a new Tensor is created.  \n  preferred_dtype   Optional element type for the returned tensor, used when dtype is None. In some cases, a caller may not have a dtype in mind when converting to a tensor, so preferred_dtype can be used as a soft preference. If the conversion to preferred_dtype is not possible, this argument has no effect.  \n  dtype_hint   same meaning as preferred_dtype, and overrides it.   \n \n\n\n Returns   A Tensor based on value.  \n\n \n\n\n Raises\n  TypeError   If no conversion function is registered for value to dtype.  \n  RuntimeError   If a registered conversion function returns an invalid value.  \n  ValueError   If the value is a tensor not of given dtype in graph mode.     \n"}, {"name": "tf.compat.v1.convert_to_tensor_or_indexed_slices", "path": "compat/v1/convert_to_tensor_or_indexed_slices", "type": "tf.compat", "text": "tf.compat.v1.convert_to_tensor_or_indexed_slices Converts the given object to a Tensor or an IndexedSlices. \ntf.compat.v1.convert_to_tensor_or_indexed_slices(\n    value, dtype=None, name=None\n)\n If value is an IndexedSlices or SparseTensor it is returned unmodified. Otherwise, it is converted to a Tensor using convert_to_tensor().\n \n\n\n Args\n  value   An IndexedSlices, SparseTensor, or an object that can be consumed by convert_to_tensor().  \n  dtype   (Optional.) The required DType of the returned Tensor or IndexedSlices.  \n  name   (Optional.) A name to use if a new Tensor is created.   \n \n\n\n Returns   A Tensor, IndexedSlices, or SparseTensor based on value.  \n\n \n\n\n Raises\n  ValueError   If dtype does not match the element type of value.     \n"}, {"name": "tf.compat.v1.convert_to_tensor_or_sparse_tensor", "path": "compat/v1/convert_to_tensor_or_sparse_tensor", "type": "tf.compat", "text": "tf.compat.v1.convert_to_tensor_or_sparse_tensor Converts value to a SparseTensor or Tensor. \ntf.compat.v1.convert_to_tensor_or_sparse_tensor(\n    value, dtype=None, name=None\n)\n\n \n\n\n Args\n  value   A SparseTensor, SparseTensorValue, or an object whose type has a registered Tensor conversion function.  \n  dtype   Optional element type for the returned tensor. If missing, the type is inferred from the type of value.  \n  name   Optional name to use if a new Tensor is created.   \n \n\n\n Returns   A SparseTensor or Tensor based on value.  \n\n \n\n\n Raises\n  RuntimeError   If result type is incompatible with dtype.     \n"}, {"name": "tf.compat.v1.count_nonzero", "path": "compat/v1/count_nonzero", "type": "tf.compat", "text": "tf.compat.v1.count_nonzero Computes number of nonzero elements across dimensions of a tensor. (deprecated arguments) (deprecated arguments)  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.math.count_nonzero  \ntf.compat.v1.count_nonzero(\n    input_tensor=None, axis=None, keepdims=None, dtype=tf.dtypes.int64, name=None,\n    reduction_indices=None, keep_dims=None, input=None\n)\n Warning: SOME ARGUMENTS ARE DEPRECATED: (keep_dims). They will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims insteadWarning: SOME ARGUMENTS ARE DEPRECATED: (reduction_indices). They will be removed in a future version. Instructions for updating: reduction_indices is deprecated, use axis instead Reduces input_tensor along the dimensions given in axis. Unless keepdims is true, the rank of the tensor is reduced by 1 for each entry in axis. If keepdims is true, the reduced dimensions are retained with length 1. If axis has no entries, all dimensions are reduced, and a tensor with a single element is returned. \nNote: Floating point comparison to zero is done by exact floating point equality check. Small values are not rounded to zero for purposes of the nonzero check.\n For example: x = tf.constant([[0, 1, 0], [1, 1, 0]])\ntf.math.count_nonzero(x)  # 3\ntf.math.count_nonzero(x, 0)  # [1, 2, 0]\ntf.math.count_nonzero(x, 1)  # [1, 2]\ntf.math.count_nonzero(x, 1, keepdims=True)  # [[1], [2]]\ntf.math.count_nonzero(x, [0, 1])  # 3\n\n\nNote: Strings are compared against zero-length empty string \"\". Any string with a size greater than zero is already considered as nonzero.\n For example: x = tf.constant([\"\", \"a\", \"  \", \"b\", \"\"])\ntf.math.count_nonzero(x) # 3, with \"a\", \"  \", and \"b\" as nonzero strings.\n\n \n\n\n Args\n  input_tensor   The tensor to reduce. Should be of numeric type, bool, or string.  \n  axis   The dimensions to reduce. If None (the default), reduces all dimensions. Must be in the range [-rank(input_tensor), rank(input_tensor)).  \n  keepdims   If true, retains reduced dimensions with length 1.  \n  dtype   The output dtype; defaults to tf.int64.  \n  name   A name for the operation (optional).  \n  reduction_indices   The old (deprecated) name for axis.  \n  keep_dims   Deprecated alias for keepdims.  \n  input   Overrides input_tensor. For compatibility.   \n \n\n\n Returns   The reduced tensor (number of nonzero values).  \n  \n"}, {"name": "tf.compat.v1.count_up_to", "path": "compat/v1/count_up_to", "type": "tf.compat", "text": "tf.compat.v1.count_up_to Increments 'ref' until it reaches 'limit'. (deprecated) \ntf.compat.v1.count_up_to(\n    ref, limit, name=None\n)\n Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Prefer Dataset.range instead.\n \n\n\n Args\n  ref   A Variable. Must be one of the following types: int32, int64. Should be from a scalar Variable node.  \n  limit   An int. If incrementing ref would bring it above limit, instead generates an 'OutOfRange' error.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as ref. A copy of the input before increment. If nothing else modifies the input, the values produced will all be distinct.  \n  \n"}, {"name": "tf.compat.v1.create_partitioned_variables", "path": "compat/v1/create_partitioned_variables", "type": "tf.compat", "text": "tf.compat.v1.create_partitioned_variables Create a list of partitioned variables according to the given slicing. (deprecated) \ntf.compat.v1.create_partitioned_variables(\n    shape, slicing, initializer, dtype=tf.dtypes.float32, trainable=True,\n    collections=None, name=None, reuse=None\n)\n Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.get_variable with a partitioner set. Currently only one dimension of the full variable can be sliced, and the full variable can be reconstructed by the concatenation of the returned list along that dimension.\n \n\n\n Args\n  shape   List of integers. The shape of the full variable.  \n  slicing   List of integers. How to partition the variable. Must be of the same length as shape. Each value indicate how many slices to create in the corresponding dimension. Presently only one of the values can be more than 1; that is, the variable can only be sliced along one dimension. For convenience, The requested number of partitions does not have to divide the corresponding dimension evenly. If it does not, the shapes of the partitions are incremented by 1 starting from partition 0 until all slack is absorbed. The adjustment rules may change in the future, but as you can save/restore these variables with different slicing specifications this should not be a problem. \n \n  initializer   A Tensor of shape shape or a variable initializer function. If a function, it will be called once for each slice, passing the shape and data type of the slice as parameters. The function must return a tensor with the same shape as the slice.  \n  dtype   Type of the variables. Ignored if initializer is a Tensor.  \n  trainable   If True also add all the variables to the graph collection GraphKeys.TRAINABLE_VARIABLES.  \n  collections   List of graph collections keys to add the variables to. Defaults to [GraphKeys.GLOBAL_VARIABLES].  \n  name   Optional name for the full variable. Defaults to \"PartitionedVariable\" and gets uniquified automatically.  \n  reuse   Boolean or None; if True and name is set, it would reuse previously created variables. if False it will create new variables. if None, it would inherit the parent scope reuse.   \n \n\n\n Returns   A list of Variables corresponding to the slicing.  \n\n \n\n\n Raises\n  ValueError   If any of the arguments is malformed.     \n"}, {"name": "tf.compat.v1.data", "path": "compat/v1/data", "type": "tf.compat", "text": "Module: tf.compat.v1.data tf.data.Dataset API for input pipelines. See Importing Data for an overview. Modules experimental module: Experimental API for building input pipelines. Classes class Dataset: Represents a potentially large set of elements. class DatasetSpec: Type specification for tf.data.Dataset. class FixedLengthRecordDataset: A Dataset of fixed-length records from one or more binary files. class Iterator: Represents the state of iterating through a Dataset. class Options: Represents options for tf.data.Dataset. class TFRecordDataset: A Dataset comprising records from one or more TFRecord files. class TextLineDataset: A Dataset comprising lines from one or more text files. Functions get_output_classes(...): Returns the output classes for elements of the input dataset / iterator. get_output_shapes(...): Returns the output shapes for elements of the input dataset / iterator. get_output_types(...): Returns the output shapes for elements of the input dataset / iterator. make_initializable_iterator(...): Creates an iterator for elements of dataset. make_one_shot_iterator(...): Creates an iterator for elements of dataset.\n \n\n\n Other Members\n  AUTOTUNE   -1  \n  INFINITE_CARDINALITY   -1  \n  UNKNOWN_CARDINALITY   -2     \n"}, {"name": "tf.compat.v1.data.Dataset", "path": "compat/v1/data/dataset", "type": "tf.compat", "text": "tf.compat.v1.data.Dataset Represents a potentially large set of elements. Inherits From: Dataset \ntf.compat.v1.data.Dataset()\n A Dataset can be used to represent an input pipeline as a collection of elements and a \"logical plan\" of transformations that act on those elements.\n \n\n\n Args\n  variant_tensor   A DT_VARIANT tensor that represents the dataset.   \n \n\n\n Attributes\n  element_spec   The type specification of an element of this dataset. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset.element_spec\nTensorSpec(shape=(), dtype=tf.int32, name=None)\n\n \n  output_classes   Returns the class of each component of an element of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.compat.v1.data.get_output_classes(dataset). \n \n  output_shapes   Returns the shape of each component of an element of this dataset. (deprecated)Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.compat.v1.data.get_output_shapes(dataset). \n \n  output_types   Returns the type of each component of an element of this dataset. (deprecated)Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.compat.v1.data.get_output_types(dataset). \n   Methods apply View source \napply(\n    transformation_func\n)\n Applies a transformation function to this dataset. apply enables chaining of custom Dataset transformations, which are represented as functions that take one Dataset argument and return a transformed Dataset. \ndataset = tf.data.Dataset.range(100)\ndef dataset_fn(ds):\n  return ds.filter(lambda x: x < 5)\ndataset = dataset.apply(dataset_fn)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2, 3, 4]\n\n \n\n\n Args\n  transformation_func   A function that takes one Dataset argument and returns a Dataset.   \n \n\n\n Returns\n  Dataset   The Dataset returned by applying transformation_func to this dataset.    as_numpy_iterator View source \nas_numpy_iterator()\n Returns an iterator which converts all elements of the dataset to numpy. Use as_numpy_iterator to inspect the content of your dataset. To see element shapes and types, print dataset elements directly instead of using as_numpy_iterator. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nfor element in dataset:\n  print(element)\ntf.Tensor(1, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(3, shape=(), dtype=int32)\n This method requires that you are running in eager mode and the dataset's element_spec contains only TensorSpec components. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nfor element in dataset.as_numpy_iterator():\n  print(element)\n1\n2\n3\n \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nprint(list(dataset.as_numpy_iterator()))\n[1, 2, 3]\n as_numpy_iterator() will preserve the nested structure of dataset elements. \ndataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),\n                                              'b': [5, 6]})\nlist(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5},\n                                      {'a': (2, 4), 'b': 6}]\nTrue\n\n \n\n\n Returns   An iterable over the elements of the dataset, with their tensors converted to numpy arrays.  \n\n \n\n\n Raises\n  TypeError   if an element contains a non-Tensor value.  \n  RuntimeError   if eager execution is not enabled.    batch View source \nbatch(\n    batch_size, drop_remainder=False\n)\n Combines consecutive elements of this dataset into batches. \ndataset = tf.data.Dataset.range(8)\ndataset = dataset.batch(3)\nlist(dataset.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5]), array([6, 7])]\n \ndataset = tf.data.Dataset.range(8)\ndataset = dataset.batch(3, drop_remainder=True)\nlist(dataset.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5])]\n The components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced.\n \n\n\n Args\n  batch_size   A tf.int64 scalar tf.Tensor, representing the number of consecutive elements of this dataset to combine in a single batch.  \n  drop_remainder   (Optional.) A tf.bool scalar tf.Tensor, representing whether the last batch should be dropped in the case it has fewer than batch_size elements; the default behavior is not to drop the smaller batch.   \n \n\n\n Returns\n  Dataset   A Dataset.    cache View source \ncache(\n    filename=''\n)\n Caches the elements in this dataset. The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data. \nNote: For the cache to be finalized, the input dataset must be iterated through in its entirety. Otherwise, subsequent iterations will not use cached data.\n \ndataset = tf.data.Dataset.range(5)\ndataset = dataset.map(lambda x: x**2)\ndataset = dataset.cache()\n# The first time reading through the data will generate the data using\n# `range` and `map`.\nlist(dataset.as_numpy_iterator())\n[0, 1, 4, 9, 16]\n# Subsequent iterations read from the cache.\nlist(dataset.as_numpy_iterator())\n[0, 1, 4, 9, 16]\n When caching to a file, the cached data will persist across runs. Even the first iteration through the data will read from the cache file. Changing the input pipeline before the call to .cache() will have no effect until the cache file is removed or the filename is changed. \ndataset = tf.data.Dataset.range(5)\ndataset = dataset.cache(\"/path/to/file\")  # doctest: +SKIP\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[0, 1, 2, 3, 4]\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.cache(\"/path/to/file\")  # Same file! # doctest: +SKIP\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[0, 1, 2, 3, 4]\n \nNote: cache will produce exactly the same elements during each iteration through the dataset. If you wish to randomize the iteration order, make sure to call shuffle after calling cache.\n\n \n\n\n Args\n  filename   A tf.string scalar tf.Tensor, representing the name of a directory on the filesystem to use for caching elements in this Dataset. If a filename is not provided, the dataset will be cached in memory.   \n \n\n\n Returns\n  Dataset   A Dataset.    cardinality View source \ncardinality()\n Returns the cardinality of the dataset, if known. cardinality may return tf.data.INFINITE_CARDINALITY if the dataset contains an infinite number of elements or tf.data.UNKNOWN_CARDINALITY if the analysis fails to determine the number of elements in the dataset (e.g. when the dataset source is a file). \ndataset = tf.data.Dataset.range(42)\nprint(dataset.cardinality().numpy())\n42\ndataset = dataset.repeat()\ncardinality = dataset.cardinality()\nprint((cardinality == tf.data.INFINITE_CARDINALITY).numpy())\nTrue\ndataset = dataset.filter(lambda x: True)\ncardinality = dataset.cardinality()\nprint((cardinality == tf.data.UNKNOWN_CARDINALITY).numpy())\nTrue\n\n \n\n\n Returns   A scalar tf.int64 Tensor representing the cardinality of the dataset. If the cardinality is infinite or unknown, cardinality returns the named constants tf.data.INFINITE_CARDINALITY and tf.data.UNKNOWN_CARDINALITY respectively.  \n concatenate View source \nconcatenate(\n    dataset\n)\n Creates a Dataset by concatenating the given dataset with this dataset. \na = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\nb = tf.data.Dataset.range(4, 8)  # ==> [ 4, 5, 6, 7 ]\nds = a.concatenate(b)\nlist(ds.as_numpy_iterator())\n[1, 2, 3, 4, 5, 6, 7]\n# The input dataset and dataset to be concatenated should have the same\n# nested structures and output types.\nc = tf.data.Dataset.zip((a, b))\na.concatenate(c)\nTraceback (most recent call last):\nTypeError: Two datasets to concatenate have different types\n<dtype: 'int64'> and (tf.int64, tf.int64)\nd = tf.data.Dataset.from_tensor_slices([\"a\", \"b\", \"c\"])\na.concatenate(d)\nTraceback (most recent call last):\nTypeError: Two datasets to concatenate have different types\n<dtype: 'int64'> and <dtype: 'string'>\n\n \n\n\n Args\n  dataset   Dataset to be concatenated.   \n \n\n\n Returns\n  Dataset   A Dataset.    enumerate View source \nenumerate(\n    start=0\n)\n Enumerates the elements of this dataset. It is similar to python's enumerate. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.enumerate(start=5)\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(5, 1)\n(6, 2)\n(7, 3)\n \n# The nested structure of the input dataset determines the structure of\n# elements in the resulting dataset.\ndataset = tf.data.Dataset.from_tensor_slices([(7, 8), (9, 10)])\ndataset = dataset.enumerate()\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(0, array([7, 8], dtype=int32))\n(1, array([ 9, 10], dtype=int32))\n\n \n\n\n Args\n  start   A tf.int64 scalar tf.Tensor, representing the start value for enumeration.   \n \n\n\n Returns\n  Dataset   A Dataset.    filter View source \nfilter(\n    predicate\n)\n Filters this dataset according to predicate. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.filter(lambda x: x < 3)\nlist(dataset.as_numpy_iterator())\n[1, 2]\n# `tf.math.equal(x, y)` is required for equality comparison\ndef filter_fn(x):\n  return tf.math.equal(x, 1)\ndataset = dataset.filter(filter_fn)\nlist(dataset.as_numpy_iterator())\n[1]\n\n \n\n\n Args\n  predicate   A function mapping a dataset element to a boolean.   \n \n\n\n Returns\n  Dataset   The Dataset containing the elements of this dataset for which predicate is True.    filter_with_legacy_function View source \nfilter_with_legacy_function(\n    predicate\n)\n Filters this dataset according to predicate. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use `tf.data.Dataset.filter()\nNote: This is an escape hatch for existing uses of filter that do not work with V2 functions. New uses are strongly discouraged and existing uses should migrate to filter as this method will be removed in V2.\n\n \n\n\n Args\n  predicate   A function mapping a nested structure of tensors (having shapes and types defined by self.output_shapes and self.output_types) to a scalar tf.bool tensor.   \n \n\n\n Returns\n  Dataset   The Dataset containing the elements of this dataset for which predicate is True.    flat_map View source \nflat_map(\n    map_func\n)\n Maps map_func across this dataset and flattens the result. Use flat_map if you want to make sure that the order of your dataset stays the same. For example, to flatten a dataset of batches into a dataset of their elements: \ndataset = tf.data.Dataset.from_tensor_slices(\n               [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ndataset = dataset.flat_map(lambda x: Dataset.from_tensor_slices(x))\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n tf.data.Dataset.interleave() is a generalization of flat_map, since flat_map produces the same output as tf.data.Dataset.interleave(cycle_length=1)\n \n\n\n Args\n  map_func   A function mapping a dataset element to a dataset.   \n \n\n\n Returns\n  Dataset   A Dataset.    from_generator View source \n@staticmethod\nfrom_generator(\n    generator, output_types=None, output_shapes=None, args=None,\n    output_signature=None\n)\n Creates a Dataset whose elements are generated by generator. (deprecated arguments) Warning: SOME ARGUMENTS ARE DEPRECATED: (output_shapes, output_types). They will be removed in a future version. Instructions for updating: Use output_signature instead The generator argument must be a callable object that returns an object that supports the iter() protocol (e.g. a generator function). The elements generated by generator must be compatible with either the given output_signature argument or with the given output_types and (optionally) output_shapes arguments, whichiver was specified. The recommended way to call from_generator is to use the output_signature argument. In this case the output will be assumed to consist of objects with the classes, shapes and types defined by tf.TypeSpec objects from output_signature argument: \ndef gen():\n  ragged_tensor = tf.ragged.constant([[1, 2], [3]])\n  yield 42, ragged_tensor\n\ndataset = tf.data.Dataset.from_generator(\n     gen,\n     output_signature=(\n         tf.TensorSpec(shape=(), dtype=tf.int32),\n         tf.RaggedTensorSpec(shape=(2, None), dtype=tf.int32)))\n\nlist(dataset.take(1))\n[(<tf.Tensor: shape=(), dtype=int32, numpy=42>,\n<tf.RaggedTensor [[1, 2], [3]]>)]\n There is also a deprecated way to call from_generator by either with output_types argument alone or together with output_shapes argument. In this case the output of the function will be assumed to consist of tf.Tensor objects with with the types defined by output_types and with the shapes which are either unknown or defined by output_shapes. \nNote: The current implementation of Dataset.from_generator() uses tf.numpy_function and inherits the same constraints. In particular, it requires the dataset and iterator related operations to be placed on a device in the same process as the Python program that called Dataset.from_generator(). The body of generator will not be serialized in a GraphDef, and you should not use this method if you need to serialize your model and restore it in a different environment.\n\n\nNote: If generator depends on mutable global variables or other external state, be aware that the runtime may invoke generator multiple times (in order to support repeating the Dataset) and at any time between the call to Dataset.from_generator() and the production of the first element from the generator. Mutating global variables or external state can cause undefined behavior, and we recommend that you explicitly cache any external state in generator before calling Dataset.from_generator().\n\n \n\n\n Args\n  generator   A callable object that returns an object that supports the iter() protocol. If args is not specified, generator must take no arguments; otherwise it must take as many arguments as there are values in args.  \n  output_types   (Optional.) A nested structure of tf.DType objects corresponding to each component of an element yielded by generator.  \n  output_shapes   (Optional.) A nested structure of tf.TensorShape objects corresponding to each component of an element yielded by generator.  \n  args   (Optional.) A tuple of tf.Tensor objects that will be evaluated and passed to generator as NumPy-array arguments.  \n  output_signature   (Optional.) A nested structure of tf.TypeSpec objects corresponding to each component of an element yielded by generator.   \n \n\n\n Returns\n  Dataset   A Dataset.    from_sparse_tensor_slices View source \n@staticmethod\nfrom_sparse_tensor_slices(\n    sparse_tensor\n)\n Splits each rank-N tf.sparse.SparseTensor in this dataset row-wise. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.data.Dataset.from_tensor_slices().\n \n\n\n Args\n  sparse_tensor   A tf.sparse.SparseTensor.   \n \n\n\n Returns\n  Dataset   A Dataset of rank-(N-1) sparse tensors.    from_tensor_slices View source \n@staticmethod\nfrom_tensor_slices(\n    tensors\n)\n Creates a Dataset whose elements are slices of the given tensors. The given tensors are sliced along their first dimension. This operation preserves the structure of the input tensors, removing the first dimension of each tensor and using it as the dataset dimension. All input tensors must have the same size in their first dimensions. \n# Slicing a 1D tensor produces scalar tensor elements.\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nlist(dataset.as_numpy_iterator())\n[1, 2, 3]\n \n# Slicing a 2D tensor produces 1D tensor elements.\ndataset = tf.data.Dataset.from_tensor_slices([[1, 2], [3, 4]])\nlist(dataset.as_numpy_iterator())\n[array([1, 2], dtype=int32), array([3, 4], dtype=int32)]\n \n# Slicing a tuple of 1D tensors produces tuple elements containing\n# scalar tensors.\ndataset = tf.data.Dataset.from_tensor_slices(([1, 2], [3, 4], [5, 6]))\nlist(dataset.as_numpy_iterator())\n[(1, 3, 5), (2, 4, 6)]\n \n# Dictionary structure is also preserved.\ndataset = tf.data.Dataset.from_tensor_slices({\"a\": [1, 2], \"b\": [3, 4]})\nlist(dataset.as_numpy_iterator()) == [{'a': 1, 'b': 3},\n                                      {'a': 2, 'b': 4}]\nTrue\n \n# Two tensors can be combined into one Dataset object.\nfeatures = tf.constant([[1, 3], [2, 1], [3, 3]]) # ==> 3x2 tensor\nlabels = tf.constant(['A', 'B', 'A']) # ==> 3x1 tensor\ndataset = Dataset.from_tensor_slices((features, labels))\n# Both the features and the labels tensors can be converted\n# to a Dataset object separately and combined after.\nfeatures_dataset = Dataset.from_tensor_slices(features)\nlabels_dataset = Dataset.from_tensor_slices(labels)\ndataset = Dataset.zip((features_dataset, labels_dataset))\n# A batched feature and label set can be converted to a Dataset\n# in similar fashion.\nbatched_features = tf.constant([[[1, 3], [2, 3]],\n                                [[2, 1], [1, 2]],\n                                [[3, 3], [3, 2]]], shape=(3, 2, 2))\nbatched_labels = tf.constant([['A', 'A'],\n                              ['B', 'B'],\n                              ['A', 'B']], shape=(3, 2, 1))\ndataset = Dataset.from_tensor_slices((batched_features, batched_labels))\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(array([[1, 3],\n       [2, 3]], dtype=int32), array([[b'A'],\n       [b'A']], dtype=object))\n(array([[2, 1],\n       [1, 2]], dtype=int32), array([[b'B'],\n       [b'B']], dtype=object))\n(array([[3, 3],\n       [3, 2]], dtype=int32), array([[b'A'],\n       [b'B']], dtype=object))\n Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.\n \n\n\n Args\n  tensors   A dataset element, with each component having the same size in the first dimension.   \n \n\n\n Returns\n  Dataset   A Dataset.    from_tensors View source \n@staticmethod\nfrom_tensors(\n    tensors\n)\n Creates a Dataset with a single element, comprising the given tensors. from_tensors produces a dataset containing only a single element. To slice the input tensor into multiple elements, use from_tensor_slices instead. \ndataset = tf.data.Dataset.from_tensors([1, 2, 3])\nlist(dataset.as_numpy_iterator())\n[array([1, 2, 3], dtype=int32)]\ndataset = tf.data.Dataset.from_tensors(([1, 2, 3], 'A'))\nlist(dataset.as_numpy_iterator())\n[(array([1, 2, 3], dtype=int32), b'A')]\n \n# You can use `from_tensors` to produce a dataset which repeats\n# the same example many times.\nexample = tf.constant([1,2,3])\ndataset = tf.data.Dataset.from_tensors(example).repeat(2)\nlist(dataset.as_numpy_iterator())\n[array([1, 2, 3], dtype=int32), array([1, 2, 3], dtype=int32)]\n Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.\n \n\n\n Args\n  tensors   A dataset element.   \n \n\n\n Returns\n  Dataset   A Dataset.    interleave View source \ninterleave(\n    map_func, cycle_length=None, block_length=None, num_parallel_calls=None,\n    deterministic=None\n)\n Maps map_func across this dataset, and interleaves the results. For example, you can use Dataset.interleave() to process many input files concurrently: \n# Preprocess 4 files concurrently, and interleave blocks of 16 records\n# from each file.\nfilenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n             \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\ndataset = tf.data.Dataset.from_tensor_slices(filenames)\ndef parse_fn(filename):\n  return tf.data.Dataset.range(10)\ndataset = dataset.interleave(lambda x:\n    tf.data.TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n    cycle_length=4, block_length=16)\n The cycle_length and block_length arguments control the order in which elements are produced. cycle_length controls the number of input elements that are processed concurrently. If you set cycle_length to 1, this transformation will handle one input element at a time, and will produce identical results to tf.data.Dataset.flat_map. In general, this transformation will apply map_func to cycle_length input elements, open iterators on the returned Dataset objects, and cycle through them producing block_length consecutive elements from each iterator, and consuming the next input element each time it reaches the end of an iterator. For example: \ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n# NOTE: New lines indicate \"block\" boundaries.\ndataset = dataset.interleave(\n    lambda x: Dataset.from_tensors(x).repeat(6),\n    cycle_length=2, block_length=4)\nlist(dataset.as_numpy_iterator())\n[1, 1, 1, 1,\n 2, 2, 2, 2,\n 1, 1,\n 2, 2,\n 3, 3, 3, 3,\n 4, 4, 4, 4,\n 3, 3,\n 4, 4,\n 5, 5, 5, 5,\n 5, 5]\n \nNote: The order of elements yielded by this transformation is deterministic, as long as map_func is a pure function and deterministic=True. If map_func contains any stateful operations, the order in which that state is accessed is undefined.\n Performance can often be improved by setting num_parallel_calls so that interleave will use multiple threads to fetch elements. If determinism isn't required, it can also improve performance to set deterministic=False. \nfilenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n             \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\ndataset = tf.data.Dataset.from_tensor_slices(filenames)\ndataset = dataset.interleave(lambda x: tf.data.TFRecordDataset(x),\n    cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE,\n    deterministic=False)\n\n \n\n\n Args\n  map_func   A function mapping a dataset element to a dataset.  \n  cycle_length   (Optional.) The number of input elements that will be processed concurrently. If not set, the tf.data runtime decides what it should be based on available CPU. If num_parallel_calls is set to tf.data.AUTOTUNE, the cycle_length argument identifies the maximum degree of parallelism.  \n  block_length   (Optional.) The number of consecutive elements to produce from each input element before cycling to another input element. If not set, defaults to 1.  \n  num_parallel_calls   (Optional.) If specified, the implementation creates a threadpool, which is used to fetch inputs from cycle elements asynchronously and in parallel. The default behavior is to fetch inputs from cycle elements synchronously with no parallelism. If the value tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available CPU.  \n  deterministic   (Optional.) A boolean controlling whether determinism should be traded for performance by allowing elements to be produced out of order. If deterministic is None, the tf.data.Options.experimental_deterministic dataset option (True by default) is used to decide whether to produce elements deterministically.   \n \n\n\n Returns\n  Dataset   A Dataset.    list_files View source \n@staticmethod\nlist_files(\n    file_pattern, shuffle=None, seed=None\n)\n A dataset of all files matching one or more glob patterns. The file_pattern argument should be a small number of glob patterns. If your filenames have already been globbed, use Dataset.from_tensor_slices(filenames) instead, as re-globbing every filename with list_files may result in poor performance with remote storage systems. \nNote: The default behavior of this method is to return filenames in a non-deterministic random shuffled order. Pass a seed or shuffle=False to get results in a deterministic order.\n Example: If we had the following files on our filesystem:  /path/to/dir/a.txt /path/to/dir/b.py /path/to/dir/c.py  If we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:  /path/to/dir/b.py /path/to/dir/c.py \n \n\n\n Args\n  file_pattern   A string, a list of strings, or a tf.Tensor of string type (scalar or vector), representing the filename glob (i.e. shell wildcard) pattern(s) that will be matched.  \n  shuffle   (Optional.) If True, the file names will be shuffled randomly. Defaults to True.  \n  seed   (Optional.) A tf.int64 scalar tf.Tensor, representing the random seed that will be used to create the distribution. See tf.random.set_seed for behavior.   \n \n\n\n Returns\n  Dataset   A Dataset of strings corresponding to file names.    make_initializable_iterator View source \nmake_initializable_iterator(\n    shared_name=None\n)\n Creates an iterator for elements of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through tf.compat.v1. In all other situations -- namely, eager mode and inside tf.function -- you can consume dataset elements using for elem in dataset: ... or by explicitly creating iterator via iterator = iter(dataset) and fetching its elements via values = next(iterator). Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use tf.compat.v1.data.make_initializable_iterator(dataset) to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\nNote: The returned iterator will be in an uninitialized state, and you must run the iterator.initializer operation before using it:\n\n# Building graph ...\ndataset = ...\niterator = dataset.make_initializable_iterator()\nnext_value = iterator.get_next()  # This is a Tensor.\n\n# ... from within a session ...\nsess.run(iterator.initializer)\ntry:\n  while True:\n    value = sess.run(next_value)\n    ...\nexcept tf.errors.OutOfRangeError:\n    pass\n\n \n\n\n Args\n  shared_name   (Optional.) If non-empty, the returned iterator will be shared under the given name across multiple sessions that share the same devices (e.g. when using a remote server).   \n \n\n\n Returns   A tf.data.Iterator for elements of this dataset.  \n\n \n\n\n Raises\n  RuntimeError   If eager execution is enabled.    make_one_shot_iterator View source \nmake_one_shot_iterator()\n Creates an iterator for elements of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through tf.compat.v1. In all other situations -- namely, eager mode and inside tf.function -- you can consume dataset elements using for elem in dataset: ... or by explicitly creating iterator via iterator = iter(dataset) and fetching its elements via values = next(iterator). Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use tf.compat.v1.data.make_one_shot_iterator(dataset) to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\nNote: The returned iterator will be initialized automatically. A \"one-shot\" iterator does not currently support re-initialization. For that see make_initializable_iterator.\n Example: # Building graph ...\ndataset = ...\nnext_value = dataset.make_one_shot_iterator().get_next()\n\n# ... from within a session ...\ntry:\n  while True:\n    value = sess.run(next_value)\n    ...\nexcept tf.errors.OutOfRangeError:\n    pass\n\n \n\n\n Returns   An tf.data.Iterator for elements of this dataset.  \n map View source \nmap(\n    map_func, num_parallel_calls=None, deterministic=None\n)\n Maps map_func across the elements of this dataset. This transformation applies map_func to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. map_func can be used to change both the values and the structure of a dataset's elements. For example, adding 1 to each element, or projecting a subset of element components. \ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\ndataset = dataset.map(lambda x: x + 1)\nlist(dataset.as_numpy_iterator())\n[2, 3, 4, 5, 6]\n The input signature of map_func is determined by the structure of each element in this dataset. \ndataset = Dataset.range(5)\n# `map_func` takes a single argument of type `tf.Tensor` with the same\n# shape and dtype.\nresult = dataset.map(lambda x: x + 1)\n \n# Each element is a tuple containing two `tf.Tensor` objects.\nelements = [(1, \"foo\"), (2, \"bar\"), (3, \"baz\")]\ndataset = tf.data.Dataset.from_generator(\n    lambda: elements, (tf.int32, tf.string))\n# `map_func` takes two arguments of type `tf.Tensor`. This function\n# projects out just the first component.\nresult = dataset.map(lambda x_int, y_str: x_int)\nlist(result.as_numpy_iterator())\n[1, 2, 3]\n \n# Each element is a dictionary mapping strings to `tf.Tensor` objects.\nelements =  ([{\"a\": 1, \"b\": \"foo\"},\n              {\"a\": 2, \"b\": \"bar\"},\n              {\"a\": 3, \"b\": \"baz\"}])\ndataset = tf.data.Dataset.from_generator(\n    lambda: elements, {\"a\": tf.int32, \"b\": tf.string})\n# `map_func` takes a single argument of type `dict` with the same keys\n# as the elements.\nresult = dataset.map(lambda d: str(d[\"a\"]) + d[\"b\"])\n The value or values returned by map_func determine the structure of each element in the returned dataset. \ndataset = tf.data.Dataset.range(3)\n# `map_func` returns two `tf.Tensor` objects.\ndef g(x):\n  return tf.constant(37.0), tf.constant([\"Foo\", \"Bar\", \"Baz\"])\nresult = dataset.map(g)\nresult.element_spec\n(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(3,), dtype=tf.string, name=None))\n# Python primitives, lists, and NumPy arrays are implicitly converted to\n# `tf.Tensor`.\ndef h(x):\n  return 37.0, [\"Foo\", \"Bar\"], np.array([1.0, 2.0], dtype=np.float64)\nresult = dataset.map(h)\nresult.element_spec\n(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(2,), dtype=tf.string, name=None), TensorSpec(shape=(2,), dtype=tf.float64, name=None))\n# `map_func` can return nested structures.\ndef i(x):\n  return (37.0, [42, 16]), \"foo\"\nresult = dataset.map(i)\nresult.element_spec\n((TensorSpec(shape=(), dtype=tf.float32, name=None),\n  TensorSpec(shape=(2,), dtype=tf.int32, name=None)),\n TensorSpec(shape=(), dtype=tf.string, name=None))\n map_func can accept as arguments and return any type of dataset element. Note that irrespective of the context in which map_func is defined (eager vs. graph), tf.data traces the function and executes it as a graph. To use Python code inside of the function you have a few options: 1) Rely on AutoGraph to convert Python code into an equivalent graph computation. The downside of this approach is that AutoGraph can convert some but not all Python code. 2) Use tf.py_function, which allows you to write arbitrary Python code but will generally result in worse performance than 1). For example: \nd = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n# transform a string tensor to upper case string using a Python function\ndef upper_case_fn(t: tf.Tensor):\n  return t.numpy().decode('utf-8').upper()\nd = d.map(lambda x: tf.py_function(func=upper_case_fn,\n          inp=[x], Tout=tf.string))\nlist(d.as_numpy_iterator())\n[b'HELLO', b'WORLD']\n 3) Use tf.numpy_function, which also allows you to write arbitrary Python code. Note that tf.py_function accepts tf.Tensor whereas tf.numpy_function accepts numpy arrays and returns only numpy arrays. For example: \nd = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\ndef upper_case_fn(t: np.ndarray):\n  return t.decode('utf-8').upper()\nd = d.map(lambda x: tf.numpy_function(func=upper_case_fn,\n          inp=[x], Tout=tf.string))\nlist(d.as_numpy_iterator())\n[b'HELLO', b'WORLD']\n Note that the use of tf.numpy_function and tf.py_function in general precludes the possibility of executing user-defined transformations in parallel (because of Python GIL). Performance can often be improved by setting num_parallel_calls so that map will use multiple threads to process elements. If deterministic order isn't required, it can also improve performance to set deterministic=False. \ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\ndataset = dataset.map(lambda x: x + 1,\n    num_parallel_calls=tf.data.AUTOTUNE,\n    deterministic=False)\n\n \n\n\n Args\n  map_func   A function mapping a dataset element to another dataset element.  \n  num_parallel_calls   (Optional.) A tf.int32 scalar tf.Tensor, representing the number elements to process asynchronously in parallel. If not specified, elements will be processed sequentially. If the value tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available CPU.  \n  deterministic   (Optional.) A boolean controlling whether determinism should be traded for performance by allowing elements to be produced out of order. If deterministic is None, the tf.data.Options.experimental_deterministic dataset option (True by default) is used to decide whether to produce elements deterministically.   \n \n\n\n Returns\n  Dataset   A Dataset.    map_with_legacy_function View source \nmap_with_legacy_function(\n    map_func, num_parallel_calls=None, deterministic=None\n)\n Maps map_func across the elements of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use `tf.data.Dataset.map()\nNote: This is an escape hatch for existing uses of map that do not work with V2 functions. New uses are strongly discouraged and existing uses should migrate to map as this method will be removed in V2.\n\n \n\n\n Args\n  map_func   A function mapping a nested structure of tensors (having shapes and types defined by self.output_shapes and self.output_types) to another nested structure of tensors.  \n  num_parallel_calls   (Optional.) A tf.int32 scalar tf.Tensor, representing the number elements to process asynchronously in parallel. If not specified, elements will be processed sequentially. If the value tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available CPU.  \n  deterministic   (Optional.) A boolean controlling whether determinism should be traded for performance by allowing elements to be produced out of order. If deterministic is None, the tf.data.Options.experimental_deterministic dataset option (True by default) is used to decide whether to produce elements deterministically.   \n \n\n\n Returns\n  Dataset   A Dataset.    options View source \noptions()\n Returns the options for this dataset and its inputs.\n \n\n\n Returns   A tf.data.Options object representing the dataset options.  \n padded_batch View source \npadded_batch(\n    batch_size, padded_shapes=None, padding_values=None, drop_remainder=False\n)\n Combines consecutive elements of this dataset into padded batches. This transformation combines multiple consecutive elements of the input dataset into a single element. Like tf.data.Dataset.batch, the components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced. Unlike tf.data.Dataset.batch, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in padded_shapes. The padded_shapes argument determines the resulting shape for each dimension of each component in an output element:  If the dimension is a constant, the component will be padded out to that length in that dimension. If the dimension is unknown, the component will be padded out to the maximum length of all elements in that dimension.  \nA = (tf.data.Dataset\n     .range(1, 5, output_type=tf.int32)\n     .map(lambda x: tf.fill([x], x)))\n# Pad to the smallest per-batch size that fits all elements.\nB = A.padded_batch(2)\nfor element in B.as_numpy_iterator():\n  print(element)\n[[1 0]\n [2 2]]\n[[3 3 3 0]\n [4 4 4 4]]\n# Pad to a fixed size.\nC = A.padded_batch(2, padded_shapes=5)\nfor element in C.as_numpy_iterator():\n  print(element)\n[[1 0 0 0 0]\n [2 2 0 0 0]]\n[[3 3 3 0 0]\n [4 4 4 4 0]]\n# Pad with a custom value.\nD = A.padded_batch(2, padded_shapes=5, padding_values=-1)\nfor element in D.as_numpy_iterator():\n  print(element)\n[[ 1 -1 -1 -1 -1]\n [ 2  2 -1 -1 -1]]\n[[ 3  3  3 -1 -1]\n [ 4  4  4  4 -1]]\n# Components of nested elements can be padded independently.\nelements = [([1, 2, 3], [10]),\n            ([4, 5], [11, 12])]\ndataset = tf.data.Dataset.from_generator(\n    lambda: iter(elements), (tf.int32, tf.int32))\n# Pad the first component of the tuple to length 4, and the second\n# component to the smallest size that fits.\ndataset = dataset.padded_batch(2,\n    padded_shapes=([4], [None]),\n    padding_values=(-1, 100))\nlist(dataset.as_numpy_iterator())\n[(array([[ 1,  2,  3, -1], [ 4,  5, -1, -1]], dtype=int32),\n  array([[ 10, 100], [ 11,  12]], dtype=int32))]\n# Pad with a single value and multiple components.\nE = tf.data.Dataset.zip((A, A)).padded_batch(2, padding_values=-1)\nfor element in E.as_numpy_iterator():\n  print(element)\n(array([[ 1, -1],\n       [ 2,  2]], dtype=int32), array([[ 1, -1],\n       [ 2,  2]], dtype=int32))\n(array([[ 3,  3,  3, -1],\n       [ 4,  4,  4,  4]], dtype=int32), array([[ 3,  3,  3, -1],\n       [ 4,  4,  4,  4]], dtype=int32))\n See also tf.data.experimental.dense_to_sparse_batch, which combines elements that may have different shapes into a tf.sparse.SparseTensor.\n \n\n\n Args\n  batch_size   A tf.int64 scalar tf.Tensor, representing the number of consecutive elements of this dataset to combine in a single batch.  \n  padded_shapes   (Optional.) A nested structure of tf.TensorShape or tf.int64 vector tensor-like objects representing the shape to which the respective component of each input element should be padded prior to batching. Any unknown dimensions will be padded to the maximum size of that dimension in each batch. If unset, all dimensions of all components are padded to the maximum size in the batch. padded_shapes must be set if any component has an unknown rank.  \n  padding_values   (Optional.) A nested structure of scalar-shaped tf.Tensor, representing the padding values to use for the respective components. None represents that the nested structure should be padded with default values. Defaults are 0 for numeric types and the empty string for string types. The padding_values should have the same structure as the input dataset. If padding_values is a single element and the input dataset has multiple components, then the same padding_values will be used to pad every component of the dataset. If padding_values is a scalar, then its value will be broadcasted to match the shape of each component.  \n  drop_remainder   (Optional.) A tf.bool scalar tf.Tensor, representing whether the last batch should be dropped in the case it has fewer than batch_size elements; the default behavior is not to drop the smaller batch.   \n \n\n\n Returns\n  Dataset   A Dataset.   \n \n\n\n Raises\n  ValueError   If a component has an unknown rank, and the padded_shapes argument is not set.    prefetch View source \nprefetch(\n    buffer_size\n)\n Creates a Dataset that prefetches elements from this dataset. Most dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements. \nNote: Like other Dataset methods, prefetch operates on the elements of the input dataset. It has no concept of examples vs. batches. examples.prefetch(2) will prefetch two elements (2 examples), while examples.batch(20).prefetch(2) will prefetch 2 elements (2 batches, of 20 examples each).\n \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.prefetch(2)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2]\n\n \n\n\n Args\n  buffer_size   A tf.int64 scalar tf.Tensor, representing the maximum number of elements that will be buffered when prefetching.   \n \n\n\n Returns\n  Dataset   A Dataset.    range View source \n@staticmethod\nrange(\n    *args, **kwargs\n)\n Creates a Dataset of a step-separated range of values. \nlist(Dataset.range(5).as_numpy_iterator())\n[0, 1, 2, 3, 4]\nlist(Dataset.range(2, 5).as_numpy_iterator())\n[2, 3, 4]\nlist(Dataset.range(1, 5, 2).as_numpy_iterator())\n[1, 3]\nlist(Dataset.range(1, 5, -2).as_numpy_iterator())\n[]\nlist(Dataset.range(5, 1).as_numpy_iterator())\n[]\nlist(Dataset.range(5, 1, -2).as_numpy_iterator())\n[5, 3]\nlist(Dataset.range(2, 5, output_type=tf.int32).as_numpy_iterator())\n[2, 3, 4]\nlist(Dataset.range(1, 5, 2, output_type=tf.float32).as_numpy_iterator())\n[1.0, 3.0]\n\n \n\n\n Args\n  *args   follows the same semantics as python's xrange. len(args) == 1 -> start = 0, stop = args[0], step = 1. len(args) == 2 -> start = args[0], stop = args[1], step = 1. len(args) == 3 -> start = args[0], stop = args[1], step = args[2].  \n  **kwargs    output_type: Its expected dtype. (Optional, default: tf.int64). \n\n  \n \n\n\n Returns\n  Dataset   A RangeDataset.   \n \n\n\n Raises\n  ValueError   if len(args) == 0.    reduce View source \nreduce(\n    initial_state, reduce_func\n)\n Reduces the input dataset to a single element. The transformation calls reduce_func successively on every element of the input dataset until the dataset is exhausted, aggregating information in its internal state. The initial_state argument is used for the initial state and the final state is returned as the result. \ntf.data.Dataset.range(5).reduce(np.int64(0), lambda x, _: x + 1).numpy()\n5\ntf.data.Dataset.range(5).reduce(np.int64(0), lambda x, y: x + y).numpy()\n10\n\n \n\n\n Args\n  initial_state   An element representing the initial state of the transformation.  \n  reduce_func   A function that maps (old_state, input_element) to new_state. It must take two arguments and return a new element The structure of new_state must match the structure of initial_state.   \n \n\n\n Returns   A dataset element corresponding to the final state of the transformation.  \n repeat View source \nrepeat(\n    count=None\n)\n Repeats this dataset so each original value is seen count times. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.repeat(3)\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 1, 2, 3, 1, 2, 3]\n \nNote: If this dataset is a function of global state (e.g. a random number generator), then different repetitions may produce different elements.\n\n \n\n\n Args\n  count   (Optional.) A tf.int64 scalar tf.Tensor, representing the number of times the dataset should be repeated. The default behavior (if count is None or -1) is for the dataset be repeated indefinitely.   \n \n\n\n Returns\n  Dataset   A Dataset.    shard View source \nshard(\n    num_shards, index\n)\n Creates a Dataset that includes only 1/num_shards of this dataset. shard is deterministic. The Dataset produced by A.shard(n, i) will contain all elements of A whose index mod n = i. \nA = tf.data.Dataset.range(10)\nB = A.shard(num_shards=3, index=0)\nlist(B.as_numpy_iterator())\n[0, 3, 6, 9]\nC = A.shard(num_shards=3, index=1)\nlist(C.as_numpy_iterator())\n[1, 4, 7]\nD = A.shard(num_shards=3, index=2)\nlist(D.as_numpy_iterator())\n[2, 5, 8]\n This dataset operator is very useful when running distributed training, as it allows each worker to read a unique subset. When reading a single input file, you can shard elements as follows: d = tf.data.TFRecordDataset(input_file)\nd = d.shard(num_workers, worker_index)\nd = d.repeat(num_epochs)\nd = d.shuffle(shuffle_buffer_size)\nd = d.map(parser_fn, num_parallel_calls=num_map_threads)\n Important caveats:  Be sure to shard before you use any randomizing operator (such as shuffle). Generally it is best if the shard operator is used early in the dataset pipeline. For example, when reading from a set of TFRecord files, shard before converting the dataset to input samples. This avoids reading every file on every worker. The following is an example of an efficient sharding strategy within a complete pipeline:  d = Dataset.list_files(pattern)\nd = d.shard(num_workers, worker_index)\nd = d.repeat(num_epochs)\nd = d.shuffle(shuffle_buffer_size)\nd = d.interleave(tf.data.TFRecordDataset,\n                 cycle_length=num_readers, block_length=1)\nd = d.map(parser_fn, num_parallel_calls=num_map_threads)\n\n \n\n\n Args\n  num_shards   A tf.int64 scalar tf.Tensor, representing the number of shards operating in parallel.  \n  index   A tf.int64 scalar tf.Tensor, representing the worker index.   \n \n\n\n Returns\n  Dataset   A Dataset.   \n \n\n\n Raises\n  InvalidArgumentError   if num_shards or index are illegal values. \nNote: error checking is done on a best-effort basis, and errors aren't guaranteed to be caught upon dataset creation. (e.g. providing in a placeholder tensor bypasses the early checking, and will instead result in an error during a session.run call.) \n\n   shuffle View source \nshuffle(\n    buffer_size, seed=None, reshuffle_each_iteration=None\n)\n Randomly shuffles the elements of this dataset. This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required. For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then shuffle will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer. reshuffle_each_iteration controls whether the shuffle order should be different for each epoch. In TF 1.X, the idiomatic way to create epochs was through the repeat transformation: \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=True)\ndataset = dataset.repeat(2)  # doctest: +SKIP\n[1, 0, 2, 1, 2, 0]\n \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=False)\ndataset = dataset.repeat(2)  # doctest: +SKIP\n[1, 0, 2, 1, 0, 2]\n In TF 2.0, tf.data.Dataset objects are Python iterables which makes it possible to also create epochs through Python iteration: \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=True)\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 0, 2]\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 2, 0]\n \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=False)\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 0, 2]\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 0, 2]\n\n \n\n\n Args\n  buffer_size   A tf.int64 scalar tf.Tensor, representing the number of elements from this dataset from which the new dataset will sample.  \n  seed   (Optional.) A tf.int64 scalar tf.Tensor, representing the random seed that will be used to create the distribution. See tf.random.set_seed for behavior.  \n  reshuffle_each_iteration   (Optional.) A boolean, which if true indicates that the dataset should be pseudorandomly reshuffled each time it is iterated over. (Defaults to True.)   \n \n\n\n Returns\n  Dataset   A Dataset.    skip View source \nskip(\n    count\n)\n Creates a Dataset that skips count elements from this dataset. \ndataset = tf.data.Dataset.range(10)\ndataset = dataset.skip(7)\nlist(dataset.as_numpy_iterator())\n[7, 8, 9]\n\n \n\n\n Args\n  count   A tf.int64 scalar tf.Tensor, representing the number of elements of this dataset that should be skipped to form the new dataset. If count is greater than the size of this dataset, the new dataset will contain no elements. If count is -1, skips the entire dataset.   \n \n\n\n Returns\n  Dataset   A Dataset.    take View source \ntake(\n    count\n)\n Creates a Dataset with at most count elements from this dataset. \ndataset = tf.data.Dataset.range(10)\ndataset = dataset.take(3)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2]\n\n \n\n\n Args\n  count   A tf.int64 scalar tf.Tensor, representing the number of elements of this dataset that should be taken to form the new dataset. If count is -1, or if count is greater than the size of this dataset, the new dataset will contain all elements of this dataset.   \n \n\n\n Returns\n  Dataset   A Dataset.    unbatch View source \nunbatch()\n Splits elements of a dataset into multiple elements. For example, if elements of the dataset are shaped [B, a0, a1, ...], where B may vary for each input element, then for each element in the dataset, the unbatched dataset will contain B consecutive elements of shape [a0, a1, ...]. \nelements = [ [1, 2, 3], [1, 2], [1, 2, 3, 4] ]\ndataset = tf.data.Dataset.from_generator(lambda: elements, tf.int64)\ndataset = dataset.unbatch()\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 1, 2, 1, 2, 3, 4]\n \nNote: unbatch requires a data copy to slice up the batched tensor into smaller, unbatched tensors. When optimizing performance, try to avoid unnecessary usage of unbatch.\n\n \n\n\n Returns   A Dataset.  \n window View source \nwindow(\n    size, shift=None, stride=1, drop_remainder=False\n)\n Combines (nests of) input elements into a dataset of (nests of) windows. A \"window\" is a finite dataset of flat elements of size size (or possibly fewer if there are not enough input elements to fill the window and drop_remainder evaluates to False). The shift argument determines the number of input elements by which the window moves on each iteration. If windows and elements are both numbered starting at 0, the first element in window k will be element k * shift of the input dataset. In particular, the first element of the first window will always be the first element of the input dataset. The stride argument determines the stride of the input elements, and the shift argument determines the shift of the window. For example: \ndataset = tf.data.Dataset.range(7).window(2)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 1]\n[2, 3]\n[4, 5]\n[6]\ndataset = tf.data.Dataset.range(7).window(3, 2, 1, True)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 1, 2]\n[2, 3, 4]\n[4, 5, 6]\ndataset = tf.data.Dataset.range(7).window(3, 1, 2, True)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 2, 4]\n[1, 3, 5]\n[2, 4, 6]\n Note that when the window transformation is applied to a dataset of nested elements, it produces a dataset of nested windows. \nnested = ([1, 2, 3, 4], [5, 6, 7, 8])\ndataset = tf.data.Dataset.from_tensor_slices(nested).window(2)\nfor window in dataset:\n  def to_numpy(ds):\n    return list(ds.as_numpy_iterator())\n  print(tuple(to_numpy(component) for component in window))\n([1, 2], [5, 6])\n([3, 4], [7, 8])\n \ndataset = tf.data.Dataset.from_tensor_slices({'a': [1, 2, 3, 4]})\ndataset = dataset.window(2)\nfor window in dataset:\n  def to_numpy(ds):\n    return list(ds.as_numpy_iterator())\n  print({'a': to_numpy(window['a'])})\n{'a': [1, 2]}\n{'a': [3, 4]}\n\n \n\n\n Args\n  size   A tf.int64 scalar tf.Tensor, representing the number of elements of the input dataset to combine into a window. Must be positive.  \n  shift   (Optional.) A tf.int64 scalar tf.Tensor, representing the number of input elements by which the window moves in each iteration. Defaults to size. Must be positive.  \n  stride   (Optional.) A tf.int64 scalar tf.Tensor, representing the stride of the input elements in the sliding window. Must be positive. The default value of 1 means \"retain every input element\".  \n  drop_remainder   (Optional.) A tf.bool scalar tf.Tensor, representing whether the last windows should be dropped if their size is smaller than size.   \n \n\n\n Returns\n  Dataset   A Dataset of (nests of) windows -- a finite datasets of flat elements created from the (nests of) input elements.    with_options View source \nwith_options(\n    options\n)\n Returns a new tf.data.Dataset with the given options set. The options are \"global\" in the sense they apply to the entire dataset. If options are set multiple times, they are merged as long as different options do not use different non-default values. \nds = tf.data.Dataset.range(5)\nds = ds.interleave(lambda x: tf.data.Dataset.range(5),\n                   cycle_length=3,\n                   num_parallel_calls=3)\noptions = tf.data.Options()\n# This will make the interleave order non-deterministic.\noptions.experimental_deterministic = False\nds = ds.with_options(options)\n\n \n\n\n Args\n  options   A tf.data.Options that identifies the options the use.   \n \n\n\n Returns\n  Dataset   A Dataset with the given options.   \n \n\n\n Raises\n  ValueError   when an option is set more than once to a non-default value    zip View source \n@staticmethod\nzip(\n    datasets\n)\n Creates a Dataset by zipping together the given datasets. This method has similar semantics to the built-in zip() function in Python, with the main difference being that the datasets argument can be an arbitrary nested structure of Dataset objects. \n# The nested structure of the `datasets` argument determines the\n# structure of elements in the resulting dataset.\na = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\nb = tf.data.Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]\nds = tf.data.Dataset.zip((a, b))\nlist(ds.as_numpy_iterator())\n[(1, 4), (2, 5), (3, 6)]\nds = tf.data.Dataset.zip((b, a))\nlist(ds.as_numpy_iterator())\n[(4, 1), (5, 2), (6, 3)]\n\n# The `datasets` argument may contain an arbitrary number of datasets.\nc = tf.data.Dataset.range(7, 13).batch(2)  # ==> [ [7, 8],\n                                           #       [9, 10],\n                                           #       [11, 12] ]\nds = tf.data.Dataset.zip((a, b, c))\nfor element in ds.as_numpy_iterator():\n  print(element)\n(1, 4, array([7, 8]))\n(2, 5, array([ 9, 10]))\n(3, 6, array([11, 12]))\n\n# The number of elements in the resulting dataset is the same as\n# the size of the smallest dataset in `datasets`.\nd = tf.data.Dataset.range(13, 15)  # ==> [ 13, 14 ]\nds = tf.data.Dataset.zip((a, d))\nlist(ds.as_numpy_iterator())\n[(1, 13), (2, 14)]\n\n \n\n\n Args\n  datasets   A nested structure of datasets.   \n \n\n\n Returns\n  Dataset   A Dataset.    __bool__ View source \n__bool__()\n __iter__ View source \n__iter__()\n Creates an iterator for elements of this dataset. The returned iterator implements the Python Iterator protocol.\n \n\n\n Returns   An tf.data.Iterator for the elements of this dataset.  \n\n \n\n\n Raises\n  RuntimeError   If not inside of tf.function and not executing eagerly.    __len__ View source \n__len__()\n Returns the length of the dataset if it is known and finite. This method requires that you are running in eager mode, and that the length of the dataset is known and non-infinite. When the length may be unknown or infinite, or if you are running in graph mode, use tf.data.Dataset.cardinality instead.\n \n\n\n Returns   An integer representing the length of the dataset.  \n\n \n\n\n Raises\n  RuntimeError   If the dataset length is unknown or infinite, or if eager execution is not enabled.    __nonzero__ View source \n__nonzero__()\n  \n"}, {"name": "tf.compat.v1.data.experimental", "path": "compat/v1/data/experimental", "type": "tf.compat", "text": "Module: tf.compat.v1.data.experimental Experimental API for building input pipelines. This module contains experimental Dataset sources and transformations that can be used in conjunction with the tf.data.Dataset API. Note that the tf.data.experimental API is not subject to the same backwards compatibility guarantees as tf.data, but we will provide deprecation advice in advance of removing existing functionality. See Importing Data for an overview. Modules service module: API for using the tf.data service. Classes class AutoShardPolicy: Represents the type of auto-sharding we enable. class CheckpointInputPipelineHook: Checkpoints input pipeline state every N steps or seconds. class CsvDataset: A Dataset comprising lines from one or more CSV files. class DatasetStructure: Type specification for tf.data.Dataset. class DistributeOptions: Represents options for distributed data processing. class MapVectorizationOptions: Represents options for the MapVectorization optimization. class OptimizationOptions: Represents options for dataset optimizations. class Optional: Represents a value that may or may not be present. class OptionalStructure: Type specification for tf.experimental.Optional. class RandomDataset: A Dataset of pseudorandom values. class Reducer: A reducer is used for reducing a set of elements. class SqlDataset: A Dataset consisting of the results from a SQL query. class StatsAggregator: A stateful resource that aggregates statistics from one or more iterators. class StatsOptions: Represents options for collecting dataset stats using StatsAggregator. class Structure: Specifies a TensorFlow value type. class TFRecordWriter: Writes a dataset to a TFRecord file. class ThreadingOptions: Represents options for dataset threading. Functions Counter(...): Creates a Dataset that counts from start in steps of size step. RaggedTensorStructure(...): DEPRECATED FUNCTION SparseTensorStructure(...): DEPRECATED FUNCTION TensorArrayStructure(...): DEPRECATED FUNCTION TensorStructure(...): DEPRECATED FUNCTION assert_cardinality(...): Asserts the cardinality of the input dataset. bucket_by_sequence_length(...): A transformation that buckets elements in a Dataset by length. bytes_produced_stats(...): Records the number of bytes produced by each element of the input dataset. cardinality(...): Returns the cardinality of dataset, if known. choose_from_datasets(...): Creates a dataset that deterministically chooses elements from datasets. copy_to_device(...): A transformation that copies dataset elements to the given target_device. dense_to_ragged_batch(...): A transformation that batches ragged elements into tf.RaggedTensors. dense_to_sparse_batch(...): A transformation that batches ragged elements into tf.sparse.SparseTensors. enumerate_dataset(...): A transformation that enumerates the elements of a dataset. (deprecated) from_variant(...): Constructs a dataset from the given variant and structure. get_next_as_optional(...): Returns a tf.experimental.Optional with the next element of the iterator. (deprecated) get_single_element(...): Returns the single element in dataset as a nested structure of tensors. get_structure(...): Returns the type signature for elements of the input dataset / iterator. group_by_reducer(...): A transformation that groups elements and performs a reduction. group_by_window(...): A transformation that groups windows of elements by key and reduces them. ignore_errors(...): Creates a Dataset from another Dataset and silently ignores any errors. latency_stats(...): Records the latency of producing each element of the input dataset. make_batched_features_dataset(...): Returns a Dataset of feature dictionaries from Example protos. make_csv_dataset(...): Reads CSV files into a dataset. make_saveable_from_iterator(...): Returns a SaveableObject for saving/restoring iterator state using Saver. (deprecated) map_and_batch(...): Fused implementation of map and batch. (deprecated) map_and_batch_with_legacy_function(...): Fused implementation of map and batch. (deprecated) parallel_interleave(...): A parallel version of the Dataset.interleave() transformation. (deprecated) parse_example_dataset(...): A transformation that parses Example protos into a dict of tensors. prefetch_to_device(...): A transformation that prefetches dataset values to the given device. rejection_resample(...): A transformation that resamples a dataset to achieve a target distribution. sample_from_datasets(...): Samples elements at random from the datasets in datasets. scan(...): A transformation that scans a function across an input dataset. shuffle_and_repeat(...): Shuffles and repeats a Dataset, reshuffling with each repetition. (deprecated) snapshot(...): API to persist the output of the input dataset. take_while(...): A transformation that stops dataset iteration based on a predicate. to_variant(...): Returns a variant representing the given dataset. unbatch(...): Splits elements of a dataset into multiple elements on the batch dimension. (deprecated) unique(...): Creates a Dataset from another Dataset, discarding duplicates.\n \n\n\n Other Members\n  AUTOTUNE   -1  \n  INFINITE_CARDINALITY   -1  \n  UNKNOWN_CARDINALITY   -2     \n"}, {"name": "tf.compat.v1.data.experimental.choose_from_datasets", "path": "compat/v1/data/experimental/choose_from_datasets", "type": "tf.compat", "text": "tf.compat.v1.data.experimental.choose_from_datasets Creates a dataset that deterministically chooses elements from datasets. \ntf.compat.v1.data.experimental.choose_from_datasets(\n    datasets, choice_dataset\n)\n For example, given the following datasets: datasets = [tf.data.Dataset.from_tensors(\"foo\").repeat(),\n            tf.data.Dataset.from_tensors(\"bar\").repeat(),\n            tf.data.Dataset.from_tensors(\"baz\").repeat()]\n\n# Define a dataset containing `[0, 1, 2, 0, 1, 2, 0, 1, 2]`.\nchoice_dataset = tf.data.Dataset.range(3).repeat(3)\n\nresult = tf.data.experimental.choose_from_datasets(datasets, choice_dataset)\n The elements of result will be: \"foo\", \"bar\", \"baz\", \"foo\", \"bar\", \"baz\", \"foo\", \"bar\", \"baz\"\n\n \n\n\n Args\n  datasets   A list of tf.data.Dataset objects with compatible structure.  \n  choice_dataset   A tf.data.Dataset of scalar tf.int64 tensors between 0 and len(datasets) - 1.   \n \n\n\n Returns   A dataset that interleaves elements from datasets according to the values of choice_dataset.  \n\n \n\n\n Raises\n  TypeError   If the datasets or choice_dataset arguments have the wrong type.     \n"}, {"name": "tf.compat.v1.data.experimental.Counter", "path": "compat/v1/data/experimental/counter", "type": "tf.compat", "text": "tf.compat.v1.data.experimental.Counter Creates a Dataset that counts from start in steps of size step. \ntf.compat.v1.data.experimental.Counter(\n    start=0, step=1, dtype=tf.dtypes.int64\n)\n For example: Dataset.count() == [0, 1, 2, ...)\nDataset.count(2) == [2, 3, ...)\nDataset.count(2, 5) == [2, 7, 12, ...)\nDataset.count(0, -1) == [0, -1, -2, ...)\nDataset.count(10, -1) == [10, 9, ...)\n\n \n\n\n Args\n  start   (Optional.) The starting value for the counter. Defaults to 0.  \n  step   (Optional.) The step size for the counter. Defaults to 1.  \n  dtype   (Optional.) The data type for counter elements. Defaults to tf.int64.   \n \n\n\n Returns   A Dataset of scalar dtype elements.  \n  \n"}, {"name": "tf.compat.v1.data.experimental.CsvDataset", "path": "compat/v1/data/experimental/csvdataset", "type": "tf.compat", "text": "tf.compat.v1.data.experimental.CsvDataset A Dataset comprising lines from one or more CSV files. Inherits From: Dataset, Dataset \ntf.compat.v1.data.experimental.CsvDataset(\n    filenames, record_defaults, compression_type=None, buffer_size=None,\n    header=False, field_delim=',', use_quote_delim=True,\n    na_value='', select_cols=None, exclude_cols=None\n)\n\n \n\n\n Args\n  filenames   A tf.string tensor containing one or more filenames.  \n  record_defaults   A list of default values for the CSV fields. Each item in the list is either a valid CSV DType (float32, float64, int32, int64, string), or a Tensor object with one of the above types. One per column of CSV data, with either a scalar Tensor default value for the column if it is optional, or DType or empty Tensor if required. If both this and select_columns are specified, these must have the same lengths, and column_defaults is assumed to be sorted in order of increasing column index. If both this and 'exclude_cols' are specified, the sum of lengths of record_defaults and exclude_cols should equal the total number of columns in the CSV file.  \n  compression_type   (Optional.) A tf.string scalar evaluating to one of \"\" (no compression), \"ZLIB\", or \"GZIP\". Defaults to no compression.  \n  buffer_size   (Optional.) A tf.int64 scalar denoting the number of bytes to buffer while reading files. Defaults to 4MB.  \n  header   (Optional.) A tf.bool scalar indicating whether the CSV file(s) have header line(s) that should be skipped when parsing. Defaults to False.  \n  field_delim   (Optional.) A tf.string scalar containing the delimiter character that separates fields in a record. Defaults to \",\".  \n  use_quote_delim   (Optional.) A tf.bool scalar. If False, treats double quotation marks as regular characters inside of string fields (ignoring RFC 4180, Section 2, Bullet 5). Defaults to True.  \n  na_value   (Optional.) A tf.string scalar indicating a value that will be treated as NA/NaN.  \n  select_cols   (Optional.) A sorted list of column indices to select from the input data. If specified, only this subset of columns will be parsed. Defaults to parsing all columns. At most one of select_cols and exclude_cols can be specified.   \n \n\n\n Attributes\n  element_spec   The type specification of an element of this dataset. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset.element_spec\nTensorSpec(shape=(), dtype=tf.int32, name=None)\n\n \n  output_classes   Returns the class of each component of an element of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.compat.v1.data.get_output_classes(dataset). \n \n  output_shapes   Returns the shape of each component of an element of this dataset. (deprecated)Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.compat.v1.data.get_output_shapes(dataset). \n \n  output_types   Returns the type of each component of an element of this dataset. (deprecated)Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.compat.v1.data.get_output_types(dataset). \n   Methods apply View source \napply(\n    transformation_func\n)\n Applies a transformation function to this dataset. apply enables chaining of custom Dataset transformations, which are represented as functions that take one Dataset argument and return a transformed Dataset. \ndataset = tf.data.Dataset.range(100)\ndef dataset_fn(ds):\n  return ds.filter(lambda x: x < 5)\ndataset = dataset.apply(dataset_fn)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2, 3, 4]\n\n \n\n\n Args\n  transformation_func   A function that takes one Dataset argument and returns a Dataset.   \n \n\n\n Returns\n  Dataset   The Dataset returned by applying transformation_func to this dataset.    as_numpy_iterator View source \nas_numpy_iterator()\n Returns an iterator which converts all elements of the dataset to numpy. Use as_numpy_iterator to inspect the content of your dataset. To see element shapes and types, print dataset elements directly instead of using as_numpy_iterator. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nfor element in dataset:\n  print(element)\ntf.Tensor(1, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(3, shape=(), dtype=int32)\n This method requires that you are running in eager mode and the dataset's element_spec contains only TensorSpec components. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nfor element in dataset.as_numpy_iterator():\n  print(element)\n1\n2\n3\n \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nprint(list(dataset.as_numpy_iterator()))\n[1, 2, 3]\n as_numpy_iterator() will preserve the nested structure of dataset elements. \ndataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),\n                                              'b': [5, 6]})\nlist(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5},\n                                      {'a': (2, 4), 'b': 6}]\nTrue\n\n \n\n\n Returns   An iterable over the elements of the dataset, with their tensors converted to numpy arrays.  \n\n \n\n\n Raises\n  TypeError   if an element contains a non-Tensor value.  \n  RuntimeError   if eager execution is not enabled.    batch View source \nbatch(\n    batch_size, drop_remainder=False\n)\n Combines consecutive elements of this dataset into batches. \ndataset = tf.data.Dataset.range(8)\ndataset = dataset.batch(3)\nlist(dataset.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5]), array([6, 7])]\n \ndataset = tf.data.Dataset.range(8)\ndataset = dataset.batch(3, drop_remainder=True)\nlist(dataset.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5])]\n The components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced.\n \n\n\n Args\n  batch_size   A tf.int64 scalar tf.Tensor, representing the number of consecutive elements of this dataset to combine in a single batch.  \n  drop_remainder   (Optional.) A tf.bool scalar tf.Tensor, representing whether the last batch should be dropped in the case it has fewer than batch_size elements; the default behavior is not to drop the smaller batch.   \n \n\n\n Returns\n  Dataset   A Dataset.    cache View source \ncache(\n    filename=''\n)\n Caches the elements in this dataset. The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data. \nNote: For the cache to be finalized, the input dataset must be iterated through in its entirety. Otherwise, subsequent iterations will not use cached data.\n \ndataset = tf.data.Dataset.range(5)\ndataset = dataset.map(lambda x: x**2)\ndataset = dataset.cache()\n# The first time reading through the data will generate the data using\n# `range` and `map`.\nlist(dataset.as_numpy_iterator())\n[0, 1, 4, 9, 16]\n# Subsequent iterations read from the cache.\nlist(dataset.as_numpy_iterator())\n[0, 1, 4, 9, 16]\n When caching to a file, the cached data will persist across runs. Even the first iteration through the data will read from the cache file. Changing the input pipeline before the call to .cache() will have no effect until the cache file is removed or the filename is changed. \ndataset = tf.data.Dataset.range(5)\ndataset = dataset.cache(\"/path/to/file\")  # doctest: +SKIP\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[0, 1, 2, 3, 4]\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.cache(\"/path/to/file\")  # Same file! # doctest: +SKIP\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[0, 1, 2, 3, 4]\n \nNote: cache will produce exactly the same elements during each iteration through the dataset. If you wish to randomize the iteration order, make sure to call shuffle after calling cache.\n\n \n\n\n Args\n  filename   A tf.string scalar tf.Tensor, representing the name of a directory on the filesystem to use for caching elements in this Dataset. If a filename is not provided, the dataset will be cached in memory.   \n \n\n\n Returns\n  Dataset   A Dataset.    cardinality View source \ncardinality()\n Returns the cardinality of the dataset, if known. cardinality may return tf.data.INFINITE_CARDINALITY if the dataset contains an infinite number of elements or tf.data.UNKNOWN_CARDINALITY if the analysis fails to determine the number of elements in the dataset (e.g. when the dataset source is a file). \ndataset = tf.data.Dataset.range(42)\nprint(dataset.cardinality().numpy())\n42\ndataset = dataset.repeat()\ncardinality = dataset.cardinality()\nprint((cardinality == tf.data.INFINITE_CARDINALITY).numpy())\nTrue\ndataset = dataset.filter(lambda x: True)\ncardinality = dataset.cardinality()\nprint((cardinality == tf.data.UNKNOWN_CARDINALITY).numpy())\nTrue\n\n \n\n\n Returns   A scalar tf.int64 Tensor representing the cardinality of the dataset. If the cardinality is infinite or unknown, cardinality returns the named constants tf.data.INFINITE_CARDINALITY and tf.data.UNKNOWN_CARDINALITY respectively.  \n concatenate View source \nconcatenate(\n    dataset\n)\n Creates a Dataset by concatenating the given dataset with this dataset. \na = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\nb = tf.data.Dataset.range(4, 8)  # ==> [ 4, 5, 6, 7 ]\nds = a.concatenate(b)\nlist(ds.as_numpy_iterator())\n[1, 2, 3, 4, 5, 6, 7]\n# The input dataset and dataset to be concatenated should have the same\n# nested structures and output types.\nc = tf.data.Dataset.zip((a, b))\na.concatenate(c)\nTraceback (most recent call last):\nTypeError: Two datasets to concatenate have different types\n<dtype: 'int64'> and (tf.int64, tf.int64)\nd = tf.data.Dataset.from_tensor_slices([\"a\", \"b\", \"c\"])\na.concatenate(d)\nTraceback (most recent call last):\nTypeError: Two datasets to concatenate have different types\n<dtype: 'int64'> and <dtype: 'string'>\n\n \n\n\n Args\n  dataset   Dataset to be concatenated.   \n \n\n\n Returns\n  Dataset   A Dataset.    enumerate View source \nenumerate(\n    start=0\n)\n Enumerates the elements of this dataset. It is similar to python's enumerate. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.enumerate(start=5)\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(5, 1)\n(6, 2)\n(7, 3)\n \n# The nested structure of the input dataset determines the structure of\n# elements in the resulting dataset.\ndataset = tf.data.Dataset.from_tensor_slices([(7, 8), (9, 10)])\ndataset = dataset.enumerate()\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(0, array([7, 8], dtype=int32))\n(1, array([ 9, 10], dtype=int32))\n\n \n\n\n Args\n  start   A tf.int64 scalar tf.Tensor, representing the start value for enumeration.   \n \n\n\n Returns\n  Dataset   A Dataset.    filter View source \nfilter(\n    predicate\n)\n Filters this dataset according to predicate. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.filter(lambda x: x < 3)\nlist(dataset.as_numpy_iterator())\n[1, 2]\n# `tf.math.equal(x, y)` is required for equality comparison\ndef filter_fn(x):\n  return tf.math.equal(x, 1)\ndataset = dataset.filter(filter_fn)\nlist(dataset.as_numpy_iterator())\n[1]\n\n \n\n\n Args\n  predicate   A function mapping a dataset element to a boolean.   \n \n\n\n Returns\n  Dataset   The Dataset containing the elements of this dataset for which predicate is True.    filter_with_legacy_function View source \nfilter_with_legacy_function(\n    predicate\n)\n Filters this dataset according to predicate. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use `tf.data.Dataset.filter()\nNote: This is an escape hatch for existing uses of filter that do not work with V2 functions. New uses are strongly discouraged and existing uses should migrate to filter as this method will be removed in V2.\n\n \n\n\n Args\n  predicate   A function mapping a nested structure of tensors (having shapes and types defined by self.output_shapes and self.output_types) to a scalar tf.bool tensor.   \n \n\n\n Returns\n  Dataset   The Dataset containing the elements of this dataset for which predicate is True.    flat_map View source \nflat_map(\n    map_func\n)\n Maps map_func across this dataset and flattens the result. Use flat_map if you want to make sure that the order of your dataset stays the same. For example, to flatten a dataset of batches into a dataset of their elements: \ndataset = tf.data.Dataset.from_tensor_slices(\n               [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ndataset = dataset.flat_map(lambda x: Dataset.from_tensor_slices(x))\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n tf.data.Dataset.interleave() is a generalization of flat_map, since flat_map produces the same output as tf.data.Dataset.interleave(cycle_length=1)\n \n\n\n Args\n  map_func   A function mapping a dataset element to a dataset.   \n \n\n\n Returns\n  Dataset   A Dataset.    from_generator View source \n@staticmethod\nfrom_generator(\n    generator, output_types=None, output_shapes=None, args=None,\n    output_signature=None\n)\n Creates a Dataset whose elements are generated by generator. (deprecated arguments) Warning: SOME ARGUMENTS ARE DEPRECATED: (output_shapes, output_types). They will be removed in a future version. Instructions for updating: Use output_signature instead The generator argument must be a callable object that returns an object that supports the iter() protocol (e.g. a generator function). The elements generated by generator must be compatible with either the given output_signature argument or with the given output_types and (optionally) output_shapes arguments, whichiver was specified. The recommended way to call from_generator is to use the output_signature argument. In this case the output will be assumed to consist of objects with the classes, shapes and types defined by tf.TypeSpec objects from output_signature argument: \ndef gen():\n  ragged_tensor = tf.ragged.constant([[1, 2], [3]])\n  yield 42, ragged_tensor\n\ndataset = tf.data.Dataset.from_generator(\n     gen,\n     output_signature=(\n         tf.TensorSpec(shape=(), dtype=tf.int32),\n         tf.RaggedTensorSpec(shape=(2, None), dtype=tf.int32)))\n\nlist(dataset.take(1))\n[(<tf.Tensor: shape=(), dtype=int32, numpy=42>,\n<tf.RaggedTensor [[1, 2], [3]]>)]\n There is also a deprecated way to call from_generator by either with output_types argument alone or together with output_shapes argument. In this case the output of the function will be assumed to consist of tf.Tensor objects with with the types defined by output_types and with the shapes which are either unknown or defined by output_shapes. \nNote: The current implementation of Dataset.from_generator() uses tf.numpy_function and inherits the same constraints. In particular, it requires the dataset and iterator related operations to be placed on a device in the same process as the Python program that called Dataset.from_generator(). The body of generator will not be serialized in a GraphDef, and you should not use this method if you need to serialize your model and restore it in a different environment.\n\n\nNote: If generator depends on mutable global variables or other external state, be aware that the runtime may invoke generator multiple times (in order to support repeating the Dataset) and at any time between the call to Dataset.from_generator() and the production of the first element from the generator. Mutating global variables or external state can cause undefined behavior, and we recommend that you explicitly cache any external state in generator before calling Dataset.from_generator().\n\n \n\n\n Args\n  generator   A callable object that returns an object that supports the iter() protocol. If args is not specified, generator must take no arguments; otherwise it must take as many arguments as there are values in args.  \n  output_types   (Optional.) A nested structure of tf.DType objects corresponding to each component of an element yielded by generator.  \n  output_shapes   (Optional.) A nested structure of tf.TensorShape objects corresponding to each component of an element yielded by generator.  \n  args   (Optional.) A tuple of tf.Tensor objects that will be evaluated and passed to generator as NumPy-array arguments.  \n  output_signature   (Optional.) A nested structure of tf.TypeSpec objects corresponding to each component of an element yielded by generator.   \n \n\n\n Returns\n  Dataset   A Dataset.    from_sparse_tensor_slices View source \n@staticmethod\nfrom_sparse_tensor_slices(\n    sparse_tensor\n)\n Splits each rank-N tf.sparse.SparseTensor in this dataset row-wise. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.data.Dataset.from_tensor_slices().\n \n\n\n Args\n  sparse_tensor   A tf.sparse.SparseTensor.   \n \n\n\n Returns\n  Dataset   A Dataset of rank-(N-1) sparse tensors.    from_tensor_slices View source \n@staticmethod\nfrom_tensor_slices(\n    tensors\n)\n Creates a Dataset whose elements are slices of the given tensors. The given tensors are sliced along their first dimension. This operation preserves the structure of the input tensors, removing the first dimension of each tensor and using it as the dataset dimension. All input tensors must have the same size in their first dimensions. \n# Slicing a 1D tensor produces scalar tensor elements.\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nlist(dataset.as_numpy_iterator())\n[1, 2, 3]\n \n# Slicing a 2D tensor produces 1D tensor elements.\ndataset = tf.data.Dataset.from_tensor_slices([[1, 2], [3, 4]])\nlist(dataset.as_numpy_iterator())\n[array([1, 2], dtype=int32), array([3, 4], dtype=int32)]\n \n# Slicing a tuple of 1D tensors produces tuple elements containing\n# scalar tensors.\ndataset = tf.data.Dataset.from_tensor_slices(([1, 2], [3, 4], [5, 6]))\nlist(dataset.as_numpy_iterator())\n[(1, 3, 5), (2, 4, 6)]\n \n# Dictionary structure is also preserved.\ndataset = tf.data.Dataset.from_tensor_slices({\"a\": [1, 2], \"b\": [3, 4]})\nlist(dataset.as_numpy_iterator()) == [{'a': 1, 'b': 3},\n                                      {'a': 2, 'b': 4}]\nTrue\n \n# Two tensors can be combined into one Dataset object.\nfeatures = tf.constant([[1, 3], [2, 1], [3, 3]]) # ==> 3x2 tensor\nlabels = tf.constant(['A', 'B', 'A']) # ==> 3x1 tensor\ndataset = Dataset.from_tensor_slices((features, labels))\n# Both the features and the labels tensors can be converted\n# to a Dataset object separately and combined after.\nfeatures_dataset = Dataset.from_tensor_slices(features)\nlabels_dataset = Dataset.from_tensor_slices(labels)\ndataset = Dataset.zip((features_dataset, labels_dataset))\n# A batched feature and label set can be converted to a Dataset\n# in similar fashion.\nbatched_features = tf.constant([[[1, 3], [2, 3]],\n                                [[2, 1], [1, 2]],\n                                [[3, 3], [3, 2]]], shape=(3, 2, 2))\nbatched_labels = tf.constant([['A', 'A'],\n                              ['B', 'B'],\n                              ['A', 'B']], shape=(3, 2, 1))\ndataset = Dataset.from_tensor_slices((batched_features, batched_labels))\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(array([[1, 3],\n       [2, 3]], dtype=int32), array([[b'A'],\n       [b'A']], dtype=object))\n(array([[2, 1],\n       [1, 2]], dtype=int32), array([[b'B'],\n       [b'B']], dtype=object))\n(array([[3, 3],\n       [3, 2]], dtype=int32), array([[b'A'],\n       [b'B']], dtype=object))\n Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.\n \n\n\n Args\n  tensors   A dataset element, with each component having the same size in the first dimension.   \n \n\n\n Returns\n  Dataset   A Dataset.    from_tensors View source \n@staticmethod\nfrom_tensors(\n    tensors\n)\n Creates a Dataset with a single element, comprising the given tensors. from_tensors produces a dataset containing only a single element. To slice the input tensor into multiple elements, use from_tensor_slices instead. \ndataset = tf.data.Dataset.from_tensors([1, 2, 3])\nlist(dataset.as_numpy_iterator())\n[array([1, 2, 3], dtype=int32)]\ndataset = tf.data.Dataset.from_tensors(([1, 2, 3], 'A'))\nlist(dataset.as_numpy_iterator())\n[(array([1, 2, 3], dtype=int32), b'A')]\n \n# You can use `from_tensors` to produce a dataset which repeats\n# the same example many times.\nexample = tf.constant([1,2,3])\ndataset = tf.data.Dataset.from_tensors(example).repeat(2)\nlist(dataset.as_numpy_iterator())\n[array([1, 2, 3], dtype=int32), array([1, 2, 3], dtype=int32)]\n Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.\n \n\n\n Args\n  tensors   A dataset element.   \n \n\n\n Returns\n  Dataset   A Dataset.    interleave View source \ninterleave(\n    map_func, cycle_length=None, block_length=None, num_parallel_calls=None,\n    deterministic=None\n)\n Maps map_func across this dataset, and interleaves the results. For example, you can use Dataset.interleave() to process many input files concurrently: \n# Preprocess 4 files concurrently, and interleave blocks of 16 records\n# from each file.\nfilenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n             \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\ndataset = tf.data.Dataset.from_tensor_slices(filenames)\ndef parse_fn(filename):\n  return tf.data.Dataset.range(10)\ndataset = dataset.interleave(lambda x:\n    tf.data.TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n    cycle_length=4, block_length=16)\n The cycle_length and block_length arguments control the order in which elements are produced. cycle_length controls the number of input elements that are processed concurrently. If you set cycle_length to 1, this transformation will handle one input element at a time, and will produce identical results to tf.data.Dataset.flat_map. In general, this transformation will apply map_func to cycle_length input elements, open iterators on the returned Dataset objects, and cycle through them producing block_length consecutive elements from each iterator, and consuming the next input element each time it reaches the end of an iterator. For example: \ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n# NOTE: New lines indicate \"block\" boundaries.\ndataset = dataset.interleave(\n    lambda x: Dataset.from_tensors(x).repeat(6),\n    cycle_length=2, block_length=4)\nlist(dataset.as_numpy_iterator())\n[1, 1, 1, 1,\n 2, 2, 2, 2,\n 1, 1,\n 2, 2,\n 3, 3, 3, 3,\n 4, 4, 4, 4,\n 3, 3,\n 4, 4,\n 5, 5, 5, 5,\n 5, 5]\n \nNote: The order of elements yielded by this transformation is deterministic, as long as map_func is a pure function and deterministic=True. If map_func contains any stateful operations, the order in which that state is accessed is undefined.\n Performance can often be improved by setting num_parallel_calls so that interleave will use multiple threads to fetch elements. If determinism isn't required, it can also improve performance to set deterministic=False. \nfilenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n             \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\ndataset = tf.data.Dataset.from_tensor_slices(filenames)\ndataset = dataset.interleave(lambda x: tf.data.TFRecordDataset(x),\n    cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE,\n    deterministic=False)\n\n \n\n\n Args\n  map_func   A function mapping a dataset element to a dataset.  \n  cycle_length   (Optional.) The number of input elements that will be processed concurrently. If not set, the tf.data runtime decides what it should be based on available CPU. If num_parallel_calls is set to tf.data.AUTOTUNE, the cycle_length argument identifies the maximum degree of parallelism.  \n  block_length   (Optional.) The number of consecutive elements to produce from each input element before cycling to another input element. If not set, defaults to 1.  \n  num_parallel_calls   (Optional.) If specified, the implementation creates a threadpool, which is used to fetch inputs from cycle elements asynchronously and in parallel. The default behavior is to fetch inputs from cycle elements synchronously with no parallelism. If the value tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available CPU.  \n  deterministic   (Optional.) A boolean controlling whether determinism should be traded for performance by allowing elements to be produced out of order. If deterministic is None, the tf.data.Options.experimental_deterministic dataset option (True by default) is used to decide whether to produce elements deterministically.   \n \n\n\n Returns\n  Dataset   A Dataset.    list_files View source \n@staticmethod\nlist_files(\n    file_pattern, shuffle=None, seed=None\n)\n A dataset of all files matching one or more glob patterns. The file_pattern argument should be a small number of glob patterns. If your filenames have already been globbed, use Dataset.from_tensor_slices(filenames) instead, as re-globbing every filename with list_files may result in poor performance with remote storage systems. \nNote: The default behavior of this method is to return filenames in a non-deterministic random shuffled order. Pass a seed or shuffle=False to get results in a deterministic order.\n Example: If we had the following files on our filesystem:  /path/to/dir/a.txt /path/to/dir/b.py /path/to/dir/c.py  If we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:  /path/to/dir/b.py /path/to/dir/c.py \n \n\n\n Args\n  file_pattern   A string, a list of strings, or a tf.Tensor of string type (scalar or vector), representing the filename glob (i.e. shell wildcard) pattern(s) that will be matched.  \n  shuffle   (Optional.) If True, the file names will be shuffled randomly. Defaults to True.  \n  seed   (Optional.) A tf.int64 scalar tf.Tensor, representing the random seed that will be used to create the distribution. See tf.random.set_seed for behavior.   \n \n\n\n Returns\n  Dataset   A Dataset of strings corresponding to file names.    make_initializable_iterator View source \nmake_initializable_iterator(\n    shared_name=None\n)\n Creates an iterator for elements of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through tf.compat.v1. In all other situations -- namely, eager mode and inside tf.function -- you can consume dataset elements using for elem in dataset: ... or by explicitly creating iterator via iterator = iter(dataset) and fetching its elements via values = next(iterator). Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use tf.compat.v1.data.make_initializable_iterator(dataset) to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\nNote: The returned iterator will be in an uninitialized state, and you must run the iterator.initializer operation before using it:\n\n# Building graph ...\ndataset = ...\niterator = dataset.make_initializable_iterator()\nnext_value = iterator.get_next()  # This is a Tensor.\n\n# ... from within a session ...\nsess.run(iterator.initializer)\ntry:\n  while True:\n    value = sess.run(next_value)\n    ...\nexcept tf.errors.OutOfRangeError:\n    pass\n\n \n\n\n Args\n  shared_name   (Optional.) If non-empty, the returned iterator will be shared under the given name across multiple sessions that share the same devices (e.g. when using a remote server).   \n \n\n\n Returns   A tf.data.Iterator for elements of this dataset.  \n\n \n\n\n Raises\n  RuntimeError   If eager execution is enabled.    make_one_shot_iterator View source \nmake_one_shot_iterator()\n Creates an iterator for elements of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through tf.compat.v1. In all other situations -- namely, eager mode and inside tf.function -- you can consume dataset elements using for elem in dataset: ... or by explicitly creating iterator via iterator = iter(dataset) and fetching its elements via values = next(iterator). Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use tf.compat.v1.data.make_one_shot_iterator(dataset) to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\nNote: The returned iterator will be initialized automatically. A \"one-shot\" iterator does not currently support re-initialization. For that see make_initializable_iterator.\n Example: # Building graph ...\ndataset = ...\nnext_value = dataset.make_one_shot_iterator().get_next()\n\n# ... from within a session ...\ntry:\n  while True:\n    value = sess.run(next_value)\n    ...\nexcept tf.errors.OutOfRangeError:\n    pass\n\n \n\n\n Returns   An tf.data.Iterator for elements of this dataset.  \n map View source \nmap(\n    map_func, num_parallel_calls=None, deterministic=None\n)\n Maps map_func across the elements of this dataset. This transformation applies map_func to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. map_func can be used to change both the values and the structure of a dataset's elements. For example, adding 1 to each element, or projecting a subset of element components. \ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\ndataset = dataset.map(lambda x: x + 1)\nlist(dataset.as_numpy_iterator())\n[2, 3, 4, 5, 6]\n The input signature of map_func is determined by the structure of each element in this dataset. \ndataset = Dataset.range(5)\n# `map_func` takes a single argument of type `tf.Tensor` with the same\n# shape and dtype.\nresult = dataset.map(lambda x: x + 1)\n \n# Each element is a tuple containing two `tf.Tensor` objects.\nelements = [(1, \"foo\"), (2, \"bar\"), (3, \"baz\")]\ndataset = tf.data.Dataset.from_generator(\n    lambda: elements, (tf.int32, tf.string))\n# `map_func` takes two arguments of type `tf.Tensor`. This function\n# projects out just the first component.\nresult = dataset.map(lambda x_int, y_str: x_int)\nlist(result.as_numpy_iterator())\n[1, 2, 3]\n \n# Each element is a dictionary mapping strings to `tf.Tensor` objects.\nelements =  ([{\"a\": 1, \"b\": \"foo\"},\n              {\"a\": 2, \"b\": \"bar\"},\n              {\"a\": 3, \"b\": \"baz\"}])\ndataset = tf.data.Dataset.from_generator(\n    lambda: elements, {\"a\": tf.int32, \"b\": tf.string})\n# `map_func` takes a single argument of type `dict` with the same keys\n# as the elements.\nresult = dataset.map(lambda d: str(d[\"a\"]) + d[\"b\"])\n The value or values returned by map_func determine the structure of each element in the returned dataset. \ndataset = tf.data.Dataset.range(3)\n# `map_func` returns two `tf.Tensor` objects.\ndef g(x):\n  return tf.constant(37.0), tf.constant([\"Foo\", \"Bar\", \"Baz\"])\nresult = dataset.map(g)\nresult.element_spec\n(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(3,), dtype=tf.string, name=None))\n# Python primitives, lists, and NumPy arrays are implicitly converted to\n# `tf.Tensor`.\ndef h(x):\n  return 37.0, [\"Foo\", \"Bar\"], np.array([1.0, 2.0], dtype=np.float64)\nresult = dataset.map(h)\nresult.element_spec\n(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(2,), dtype=tf.string, name=None), TensorSpec(shape=(2,), dtype=tf.float64, name=None))\n# `map_func` can return nested structures.\ndef i(x):\n  return (37.0, [42, 16]), \"foo\"\nresult = dataset.map(i)\nresult.element_spec\n((TensorSpec(shape=(), dtype=tf.float32, name=None),\n  TensorSpec(shape=(2,), dtype=tf.int32, name=None)),\n TensorSpec(shape=(), dtype=tf.string, name=None))\n map_func can accept as arguments and return any type of dataset element. Note that irrespective of the context in which map_func is defined (eager vs. graph), tf.data traces the function and executes it as a graph. To use Python code inside of the function you have a few options: 1) Rely on AutoGraph to convert Python code into an equivalent graph computation. The downside of this approach is that AutoGraph can convert some but not all Python code. 2) Use tf.py_function, which allows you to write arbitrary Python code but will generally result in worse performance than 1). For example: \nd = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n# transform a string tensor to upper case string using a Python function\ndef upper_case_fn(t: tf.Tensor):\n  return t.numpy().decode('utf-8').upper()\nd = d.map(lambda x: tf.py_function(func=upper_case_fn,\n          inp=[x], Tout=tf.string))\nlist(d.as_numpy_iterator())\n[b'HELLO', b'WORLD']\n 3) Use tf.numpy_function, which also allows you to write arbitrary Python code. Note that tf.py_function accepts tf.Tensor whereas tf.numpy_function accepts numpy arrays and returns only numpy arrays. For example: \nd = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\ndef upper_case_fn(t: np.ndarray):\n  return t.decode('utf-8').upper()\nd = d.map(lambda x: tf.numpy_function(func=upper_case_fn,\n          inp=[x], Tout=tf.string))\nlist(d.as_numpy_iterator())\n[b'HELLO', b'WORLD']\n Note that the use of tf.numpy_function and tf.py_function in general precludes the possibility of executing user-defined transformations in parallel (because of Python GIL). Performance can often be improved by setting num_parallel_calls so that map will use multiple threads to process elements. If deterministic order isn't required, it can also improve performance to set deterministic=False. \ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\ndataset = dataset.map(lambda x: x + 1,\n    num_parallel_calls=tf.data.AUTOTUNE,\n    deterministic=False)\n\n \n\n\n Args\n  map_func   A function mapping a dataset element to another dataset element.  \n  num_parallel_calls   (Optional.) A tf.int32 scalar tf.Tensor, representing the number elements to process asynchronously in parallel. If not specified, elements will be processed sequentially. If the value tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available CPU.  \n  deterministic   (Optional.) A boolean controlling whether determinism should be traded for performance by allowing elements to be produced out of order. If deterministic is None, the tf.data.Options.experimental_deterministic dataset option (True by default) is used to decide whether to produce elements deterministically.   \n \n\n\n Returns\n  Dataset   A Dataset.    map_with_legacy_function View source \nmap_with_legacy_function(\n    map_func, num_parallel_calls=None, deterministic=None\n)\n Maps map_func across the elements of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use `tf.data.Dataset.map()\nNote: This is an escape hatch for existing uses of map that do not work with V2 functions. New uses are strongly discouraged and existing uses should migrate to map as this method will be removed in V2.\n\n \n\n\n Args\n  map_func   A function mapping a nested structure of tensors (having shapes and types defined by self.output_shapes and self.output_types) to another nested structure of tensors.  \n  num_parallel_calls   (Optional.) A tf.int32 scalar tf.Tensor, representing the number elements to process asynchronously in parallel. If not specified, elements will be processed sequentially. If the value tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available CPU.  \n  deterministic   (Optional.) A boolean controlling whether determinism should be traded for performance by allowing elements to be produced out of order. If deterministic is None, the tf.data.Options.experimental_deterministic dataset option (True by default) is used to decide whether to produce elements deterministically.   \n \n\n\n Returns\n  Dataset   A Dataset.    options View source \noptions()\n Returns the options for this dataset and its inputs.\n \n\n\n Returns   A tf.data.Options object representing the dataset options.  \n padded_batch View source \npadded_batch(\n    batch_size, padded_shapes=None, padding_values=None, drop_remainder=False\n)\n Combines consecutive elements of this dataset into padded batches. This transformation combines multiple consecutive elements of the input dataset into a single element. Like tf.data.Dataset.batch, the components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced. Unlike tf.data.Dataset.batch, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in padded_shapes. The padded_shapes argument determines the resulting shape for each dimension of each component in an output element:  If the dimension is a constant, the component will be padded out to that length in that dimension. If the dimension is unknown, the component will be padded out to the maximum length of all elements in that dimension.  \nA = (tf.data.Dataset\n     .range(1, 5, output_type=tf.int32)\n     .map(lambda x: tf.fill([x], x)))\n# Pad to the smallest per-batch size that fits all elements.\nB = A.padded_batch(2)\nfor element in B.as_numpy_iterator():\n  print(element)\n[[1 0]\n [2 2]]\n[[3 3 3 0]\n [4 4 4 4]]\n# Pad to a fixed size.\nC = A.padded_batch(2, padded_shapes=5)\nfor element in C.as_numpy_iterator():\n  print(element)\n[[1 0 0 0 0]\n [2 2 0 0 0]]\n[[3 3 3 0 0]\n [4 4 4 4 0]]\n# Pad with a custom value.\nD = A.padded_batch(2, padded_shapes=5, padding_values=-1)\nfor element in D.as_numpy_iterator():\n  print(element)\n[[ 1 -1 -1 -1 -1]\n [ 2  2 -1 -1 -1]]\n[[ 3  3  3 -1 -1]\n [ 4  4  4  4 -1]]\n# Components of nested elements can be padded independently.\nelements = [([1, 2, 3], [10]),\n            ([4, 5], [11, 12])]\ndataset = tf.data.Dataset.from_generator(\n    lambda: iter(elements), (tf.int32, tf.int32))\n# Pad the first component of the tuple to length 4, and the second\n# component to the smallest size that fits.\ndataset = dataset.padded_batch(2,\n    padded_shapes=([4], [None]),\n    padding_values=(-1, 100))\nlist(dataset.as_numpy_iterator())\n[(array([[ 1,  2,  3, -1], [ 4,  5, -1, -1]], dtype=int32),\n  array([[ 10, 100], [ 11,  12]], dtype=int32))]\n# Pad with a single value and multiple components.\nE = tf.data.Dataset.zip((A, A)).padded_batch(2, padding_values=-1)\nfor element in E.as_numpy_iterator():\n  print(element)\n(array([[ 1, -1],\n       [ 2,  2]], dtype=int32), array([[ 1, -1],\n       [ 2,  2]], dtype=int32))\n(array([[ 3,  3,  3, -1],\n       [ 4,  4,  4,  4]], dtype=int32), array([[ 3,  3,  3, -1],\n       [ 4,  4,  4,  4]], dtype=int32))\n See also tf.data.experimental.dense_to_sparse_batch, which combines elements that may have different shapes into a tf.sparse.SparseTensor.\n \n\n\n Args\n  batch_size   A tf.int64 scalar tf.Tensor, representing the number of consecutive elements of this dataset to combine in a single batch.  \n  padded_shapes   (Optional.) A nested structure of tf.TensorShape or tf.int64 vector tensor-like objects representing the shape to which the respective component of each input element should be padded prior to batching. Any unknown dimensions will be padded to the maximum size of that dimension in each batch. If unset, all dimensions of all components are padded to the maximum size in the batch. padded_shapes must be set if any component has an unknown rank.  \n  padding_values   (Optional.) A nested structure of scalar-shaped tf.Tensor, representing the padding values to use for the respective components. None represents that the nested structure should be padded with default values. Defaults are 0 for numeric types and the empty string for string types. The padding_values should have the same structure as the input dataset. If padding_values is a single element and the input dataset has multiple components, then the same padding_values will be used to pad every component of the dataset. If padding_values is a scalar, then its value will be broadcasted to match the shape of each component.  \n  drop_remainder   (Optional.) A tf.bool scalar tf.Tensor, representing whether the last batch should be dropped in the case it has fewer than batch_size elements; the default behavior is not to drop the smaller batch.   \n \n\n\n Returns\n  Dataset   A Dataset.   \n \n\n\n Raises\n  ValueError   If a component has an unknown rank, and the padded_shapes argument is not set.    prefetch View source \nprefetch(\n    buffer_size\n)\n Creates a Dataset that prefetches elements from this dataset. Most dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements. \nNote: Like other Dataset methods, prefetch operates on the elements of the input dataset. It has no concept of examples vs. batches. examples.prefetch(2) will prefetch two elements (2 examples), while examples.batch(20).prefetch(2) will prefetch 2 elements (2 batches, of 20 examples each).\n \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.prefetch(2)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2]\n\n \n\n\n Args\n  buffer_size   A tf.int64 scalar tf.Tensor, representing the maximum number of elements that will be buffered when prefetching.   \n \n\n\n Returns\n  Dataset   A Dataset.    range View source \n@staticmethod\nrange(\n    *args, **kwargs\n)\n Creates a Dataset of a step-separated range of values. \nlist(Dataset.range(5).as_numpy_iterator())\n[0, 1, 2, 3, 4]\nlist(Dataset.range(2, 5).as_numpy_iterator())\n[2, 3, 4]\nlist(Dataset.range(1, 5, 2).as_numpy_iterator())\n[1, 3]\nlist(Dataset.range(1, 5, -2).as_numpy_iterator())\n[]\nlist(Dataset.range(5, 1).as_numpy_iterator())\n[]\nlist(Dataset.range(5, 1, -2).as_numpy_iterator())\n[5, 3]\nlist(Dataset.range(2, 5, output_type=tf.int32).as_numpy_iterator())\n[2, 3, 4]\nlist(Dataset.range(1, 5, 2, output_type=tf.float32).as_numpy_iterator())\n[1.0, 3.0]\n\n \n\n\n Args\n  *args   follows the same semantics as python's xrange. len(args) == 1 -> start = 0, stop = args[0], step = 1. len(args) == 2 -> start = args[0], stop = args[1], step = 1. len(args) == 3 -> start = args[0], stop = args[1], step = args[2].  \n  **kwargs    output_type: Its expected dtype. (Optional, default: tf.int64). \n\n  \n \n\n\n Returns\n  Dataset   A RangeDataset.   \n \n\n\n Raises\n  ValueError   if len(args) == 0.    reduce View source \nreduce(\n    initial_state, reduce_func\n)\n Reduces the input dataset to a single element. The transformation calls reduce_func successively on every element of the input dataset until the dataset is exhausted, aggregating information in its internal state. The initial_state argument is used for the initial state and the final state is returned as the result. \ntf.data.Dataset.range(5).reduce(np.int64(0), lambda x, _: x + 1).numpy()\n5\ntf.data.Dataset.range(5).reduce(np.int64(0), lambda x, y: x + y).numpy()\n10\n\n \n\n\n Args\n  initial_state   An element representing the initial state of the transformation.  \n  reduce_func   A function that maps (old_state, input_element) to new_state. It must take two arguments and return a new element The structure of new_state must match the structure of initial_state.   \n \n\n\n Returns   A dataset element corresponding to the final state of the transformation.  \n repeat View source \nrepeat(\n    count=None\n)\n Repeats this dataset so each original value is seen count times. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.repeat(3)\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 1, 2, 3, 1, 2, 3]\n \nNote: If this dataset is a function of global state (e.g. a random number generator), then different repetitions may produce different elements.\n\n \n\n\n Args\n  count   (Optional.) A tf.int64 scalar tf.Tensor, representing the number of times the dataset should be repeated. The default behavior (if count is None or -1) is for the dataset be repeated indefinitely.   \n \n\n\n Returns\n  Dataset   A Dataset.    shard View source \nshard(\n    num_shards, index\n)\n Creates a Dataset that includes only 1/num_shards of this dataset. shard is deterministic. The Dataset produced by A.shard(n, i) will contain all elements of A whose index mod n = i. \nA = tf.data.Dataset.range(10)\nB = A.shard(num_shards=3, index=0)\nlist(B.as_numpy_iterator())\n[0, 3, 6, 9]\nC = A.shard(num_shards=3, index=1)\nlist(C.as_numpy_iterator())\n[1, 4, 7]\nD = A.shard(num_shards=3, index=2)\nlist(D.as_numpy_iterator())\n[2, 5, 8]\n This dataset operator is very useful when running distributed training, as it allows each worker to read a unique subset. When reading a single input file, you can shard elements as follows: d = tf.data.TFRecordDataset(input_file)\nd = d.shard(num_workers, worker_index)\nd = d.repeat(num_epochs)\nd = d.shuffle(shuffle_buffer_size)\nd = d.map(parser_fn, num_parallel_calls=num_map_threads)\n Important caveats:  Be sure to shard before you use any randomizing operator (such as shuffle). Generally it is best if the shard operator is used early in the dataset pipeline. For example, when reading from a set of TFRecord files, shard before converting the dataset to input samples. This avoids reading every file on every worker. The following is an example of an efficient sharding strategy within a complete pipeline:  d = Dataset.list_files(pattern)\nd = d.shard(num_workers, worker_index)\nd = d.repeat(num_epochs)\nd = d.shuffle(shuffle_buffer_size)\nd = d.interleave(tf.data.TFRecordDataset,\n                 cycle_length=num_readers, block_length=1)\nd = d.map(parser_fn, num_parallel_calls=num_map_threads)\n\n \n\n\n Args\n  num_shards   A tf.int64 scalar tf.Tensor, representing the number of shards operating in parallel.  \n  index   A tf.int64 scalar tf.Tensor, representing the worker index.   \n \n\n\n Returns\n  Dataset   A Dataset.   \n \n\n\n Raises\n  InvalidArgumentError   if num_shards or index are illegal values. \nNote: error checking is done on a best-effort basis, and errors aren't guaranteed to be caught upon dataset creation. (e.g. providing in a placeholder tensor bypasses the early checking, and will instead result in an error during a session.run call.) \n\n   shuffle View source \nshuffle(\n    buffer_size, seed=None, reshuffle_each_iteration=None\n)\n Randomly shuffles the elements of this dataset. This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required. For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then shuffle will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer. reshuffle_each_iteration controls whether the shuffle order should be different for each epoch. In TF 1.X, the idiomatic way to create epochs was through the repeat transformation: \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=True)\ndataset = dataset.repeat(2)  # doctest: +SKIP\n[1, 0, 2, 1, 2, 0]\n \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=False)\ndataset = dataset.repeat(2)  # doctest: +SKIP\n[1, 0, 2, 1, 0, 2]\n In TF 2.0, tf.data.Dataset objects are Python iterables which makes it possible to also create epochs through Python iteration: \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=True)\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 0, 2]\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 2, 0]\n \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=False)\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 0, 2]\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 0, 2]\n\n \n\n\n Args\n  buffer_size   A tf.int64 scalar tf.Tensor, representing the number of elements from this dataset from which the new dataset will sample.  \n  seed   (Optional.) A tf.int64 scalar tf.Tensor, representing the random seed that will be used to create the distribution. See tf.random.set_seed for behavior.  \n  reshuffle_each_iteration   (Optional.) A boolean, which if true indicates that the dataset should be pseudorandomly reshuffled each time it is iterated over. (Defaults to True.)   \n \n\n\n Returns\n  Dataset   A Dataset.    skip View source \nskip(\n    count\n)\n Creates a Dataset that skips count elements from this dataset. \ndataset = tf.data.Dataset.range(10)\ndataset = dataset.skip(7)\nlist(dataset.as_numpy_iterator())\n[7, 8, 9]\n\n \n\n\n Args\n  count   A tf.int64 scalar tf.Tensor, representing the number of elements of this dataset that should be skipped to form the new dataset. If count is greater than the size of this dataset, the new dataset will contain no elements. If count is -1, skips the entire dataset.   \n \n\n\n Returns\n  Dataset   A Dataset.    take View source \ntake(\n    count\n)\n Creates a Dataset with at most count elements from this dataset. \ndataset = tf.data.Dataset.range(10)\ndataset = dataset.take(3)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2]\n\n \n\n\n Args\n  count   A tf.int64 scalar tf.Tensor, representing the number of elements of this dataset that should be taken to form the new dataset. If count is -1, or if count is greater than the size of this dataset, the new dataset will contain all elements of this dataset.   \n \n\n\n Returns\n  Dataset   A Dataset.    unbatch View source \nunbatch()\n Splits elements of a dataset into multiple elements. For example, if elements of the dataset are shaped [B, a0, a1, ...], where B may vary for each input element, then for each element in the dataset, the unbatched dataset will contain B consecutive elements of shape [a0, a1, ...]. \nelements = [ [1, 2, 3], [1, 2], [1, 2, 3, 4] ]\ndataset = tf.data.Dataset.from_generator(lambda: elements, tf.int64)\ndataset = dataset.unbatch()\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 1, 2, 1, 2, 3, 4]\n \nNote: unbatch requires a data copy to slice up the batched tensor into smaller, unbatched tensors. When optimizing performance, try to avoid unnecessary usage of unbatch.\n\n \n\n\n Returns   A Dataset.  \n window View source \nwindow(\n    size, shift=None, stride=1, drop_remainder=False\n)\n Combines (nests of) input elements into a dataset of (nests of) windows. A \"window\" is a finite dataset of flat elements of size size (or possibly fewer if there are not enough input elements to fill the window and drop_remainder evaluates to False). The shift argument determines the number of input elements by which the window moves on each iteration. If windows and elements are both numbered starting at 0, the first element in window k will be element k * shift of the input dataset. In particular, the first element of the first window will always be the first element of the input dataset. The stride argument determines the stride of the input elements, and the shift argument determines the shift of the window. For example: \ndataset = tf.data.Dataset.range(7).window(2)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 1]\n[2, 3]\n[4, 5]\n[6]\ndataset = tf.data.Dataset.range(7).window(3, 2, 1, True)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 1, 2]\n[2, 3, 4]\n[4, 5, 6]\ndataset = tf.data.Dataset.range(7).window(3, 1, 2, True)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 2, 4]\n[1, 3, 5]\n[2, 4, 6]\n Note that when the window transformation is applied to a dataset of nested elements, it produces a dataset of nested windows. \nnested = ([1, 2, 3, 4], [5, 6, 7, 8])\ndataset = tf.data.Dataset.from_tensor_slices(nested).window(2)\nfor window in dataset:\n  def to_numpy(ds):\n    return list(ds.as_numpy_iterator())\n  print(tuple(to_numpy(component) for component in window))\n([1, 2], [5, 6])\n([3, 4], [7, 8])\n \ndataset = tf.data.Dataset.from_tensor_slices({'a': [1, 2, 3, 4]})\ndataset = dataset.window(2)\nfor window in dataset:\n  def to_numpy(ds):\n    return list(ds.as_numpy_iterator())\n  print({'a': to_numpy(window['a'])})\n{'a': [1, 2]}\n{'a': [3, 4]}\n\n \n\n\n Args\n  size   A tf.int64 scalar tf.Tensor, representing the number of elements of the input dataset to combine into a window. Must be positive.  \n  shift   (Optional.) A tf.int64 scalar tf.Tensor, representing the number of input elements by which the window moves in each iteration. Defaults to size. Must be positive.  \n  stride   (Optional.) A tf.int64 scalar tf.Tensor, representing the stride of the input elements in the sliding window. Must be positive. The default value of 1 means \"retain every input element\".  \n  drop_remainder   (Optional.) A tf.bool scalar tf.Tensor, representing whether the last windows should be dropped if their size is smaller than size.   \n \n\n\n Returns\n  Dataset   A Dataset of (nests of) windows -- a finite datasets of flat elements created from the (nests of) input elements.    with_options View source \nwith_options(\n    options\n)\n Returns a new tf.data.Dataset with the given options set. The options are \"global\" in the sense they apply to the entire dataset. If options are set multiple times, they are merged as long as different options do not use different non-default values. \nds = tf.data.Dataset.range(5)\nds = ds.interleave(lambda x: tf.data.Dataset.range(5),\n                   cycle_length=3,\n                   num_parallel_calls=3)\noptions = tf.data.Options()\n# This will make the interleave order non-deterministic.\noptions.experimental_deterministic = False\nds = ds.with_options(options)\n\n \n\n\n Args\n  options   A tf.data.Options that identifies the options the use.   \n \n\n\n Returns\n  Dataset   A Dataset with the given options.   \n \n\n\n Raises\n  ValueError   when an option is set more than once to a non-default value    zip View source \n@staticmethod\nzip(\n    datasets\n)\n Creates a Dataset by zipping together the given datasets. This method has similar semantics to the built-in zip() function in Python, with the main difference being that the datasets argument can be an arbitrary nested structure of Dataset objects. \n# The nested structure of the `datasets` argument determines the\n# structure of elements in the resulting dataset.\na = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\nb = tf.data.Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]\nds = tf.data.Dataset.zip((a, b))\nlist(ds.as_numpy_iterator())\n[(1, 4), (2, 5), (3, 6)]\nds = tf.data.Dataset.zip((b, a))\nlist(ds.as_numpy_iterator())\n[(4, 1), (5, 2), (6, 3)]\n\n# The `datasets` argument may contain an arbitrary number of datasets.\nc = tf.data.Dataset.range(7, 13).batch(2)  # ==> [ [7, 8],\n                                           #       [9, 10],\n                                           #       [11, 12] ]\nds = tf.data.Dataset.zip((a, b, c))\nfor element in ds.as_numpy_iterator():\n  print(element)\n(1, 4, array([7, 8]))\n(2, 5, array([ 9, 10]))\n(3, 6, array([11, 12]))\n\n# The number of elements in the resulting dataset is the same as\n# the size of the smallest dataset in `datasets`.\nd = tf.data.Dataset.range(13, 15)  # ==> [ 13, 14 ]\nds = tf.data.Dataset.zip((a, d))\nlist(ds.as_numpy_iterator())\n[(1, 13), (2, 14)]\n\n \n\n\n Args\n  datasets   A nested structure of datasets.   \n \n\n\n Returns\n  Dataset   A Dataset.    __bool__ View source \n__bool__()\n __iter__ View source \n__iter__()\n Creates an iterator for elements of this dataset. The returned iterator implements the Python Iterator protocol.\n \n\n\n Returns   An tf.data.Iterator for the elements of this dataset.  \n\n \n\n\n Raises\n  RuntimeError   If not inside of tf.function and not executing eagerly.    __len__ View source \n__len__()\n Returns the length of the dataset if it is known and finite. This method requires that you are running in eager mode, and that the length of the dataset is known and non-infinite. When the length may be unknown or infinite, or if you are running in graph mode, use tf.data.Dataset.cardinality instead.\n \n\n\n Returns   An integer representing the length of the dataset.  \n\n \n\n\n Raises\n  RuntimeError   If the dataset length is unknown or infinite, or if eager execution is not enabled.    __nonzero__ View source \n__nonzero__()\n  \n"}, {"name": "tf.compat.v1.data.experimental.make_batched_features_dataset", "path": "compat/v1/data/experimental/make_batched_features_dataset", "type": "tf.compat", "text": "tf.compat.v1.data.experimental.make_batched_features_dataset Returns a Dataset of feature dictionaries from Example protos. \ntf.compat.v1.data.experimental.make_batched_features_dataset(\n    file_pattern, batch_size, features, reader=None, label_key=None,\n    reader_args=None, num_epochs=None, shuffle=True, shuffle_buffer_size=10000,\n    shuffle_seed=None, prefetch_buffer_size=None, reader_num_threads=None,\n    parser_num_threads=None, sloppy_ordering=False, drop_final_batch=False\n)\n If label_key argument is provided, returns a Dataset of tuple comprising of feature dictionaries and label. Example: serialized_examples = [\n  features {\n    feature { key: \"age\" value { int64_list { value: [ 0 ] } } }\n    feature { key: \"gender\" value { bytes_list { value: [ \"f\" ] } } }\n    feature { key: \"kws\" value { bytes_list { value: [ \"code\", \"art\" ] } } }\n  },\n  features {\n    feature { key: \"age\" value { int64_list { value: [] } } }\n    feature { key: \"gender\" value { bytes_list { value: [ \"f\" ] } } }\n    feature { key: \"kws\" value { bytes_list { value: [ \"sports\" ] } } }\n  }\n]\n We can use arguments: features: {\n  \"age\": FixedLenFeature([], dtype=tf.int64, default_value=-1),\n  \"gender\": FixedLenFeature([], dtype=tf.string),\n  \"kws\": VarLenFeature(dtype=tf.string),\n}\n And the expected output is: {\n  \"age\": [[0], [-1]],\n  \"gender\": [[\"f\"], [\"f\"]],\n  \"kws\": SparseTensor(\n    indices=[[0, 0], [0, 1], [1, 0]],\n    values=[\"code\", \"art\", \"sports\"]\n    dense_shape=[2, 2]),\n}\n\n \n\n\n Args\n  file_pattern   List of files or patterns of file paths containing Example records. See tf.io.gfile.glob for pattern rules.  \n  batch_size   An int representing the number of records to combine in a single batch.  \n  features   A dict mapping feature keys to FixedLenFeature or VarLenFeature values. See tf.io.parse_example.  \n  reader   A function or class that can be called with a filenames tensor and (optional) reader_args and returns a Dataset of Example tensors. Defaults to tf.data.TFRecordDataset.  \n  label_key   (Optional) A string corresponding to the key labels are stored in tf.Examples. If provided, it must be one of the features key, otherwise results in ValueError.  \n  reader_args   Additional arguments to pass to the reader class.  \n  num_epochs   Integer specifying the number of times to read through the dataset. If None, cycles through the dataset forever. Defaults to None.  \n  shuffle   A boolean, indicates whether the input should be shuffled. Defaults to True.  \n  shuffle_buffer_size   Buffer size of the ShuffleDataset. A large capacity ensures better shuffling but would increase memory usage and startup time.  \n  shuffle_seed   Randomization seed to use for shuffling.  \n  prefetch_buffer_size   Number of feature batches to prefetch in order to improve performance. Recommended value is the number of batches consumed per training step. Defaults to auto-tune.  \n  reader_num_threads   Number of threads used to read Example records. If >1, the results will be interleaved. Defaults to 1.  \n  parser_num_threads   Number of threads to use for parsing Example tensors into a dictionary of Feature tensors. Defaults to 2.  \n  sloppy_ordering   If True, reading performance will be improved at the cost of non-deterministic ordering. If False, the order of elements produced is deterministic prior to shuffling (elements are still randomized if shuffle=True. Note that if the seed is set, then order of elements after shuffling is deterministic). Defaults to False.  \n  drop_final_batch   If True, and the batch size does not evenly divide the input dataset size, the final smaller batch will be dropped. Defaults to False.   \n \n\n\n Returns   A dataset of dict elements, (or a tuple of dict elements and label). Each dict maps feature keys to Tensor or SparseTensor objects.  \n\n \n\n\n Raises\n  TypeError   If reader is of the wrong type.  \n  ValueError   If label_key is not one of the features keys.     \n"}, {"name": "tf.compat.v1.data.experimental.make_csv_dataset", "path": "compat/v1/data/experimental/make_csv_dataset", "type": "tf.compat", "text": "tf.compat.v1.data.experimental.make_csv_dataset Reads CSV files into a dataset. \ntf.compat.v1.data.experimental.make_csv_dataset(\n    file_pattern, batch_size, column_names=None, column_defaults=None,\n    label_name=None, select_columns=None, field_delim=',',\n    use_quote_delim=True, na_value='', header=True, num_epochs=None,\n    shuffle=True, shuffle_buffer_size=10000, shuffle_seed=None,\n    prefetch_buffer_size=None, num_parallel_reads=None, sloppy=False,\n    num_rows_for_inference=100, compression_type=None, ignore_errors=False\n)\n Reads CSV files into a dataset, where each element is a (features, labels) tuple that corresponds to a batch of CSV rows. The features dictionary maps feature column names to Tensors containing the corresponding feature data, and labels is a Tensor containing the batch's label data.\n \n\n\n Args\n  file_pattern   List of files or patterns of file paths containing CSV records. See tf.io.gfile.glob for pattern rules.  \n  batch_size   An int representing the number of records to combine in a single batch.  \n  column_names   An optional list of strings that corresponds to the CSV columns, in order. One per column of the input record. If this is not provided, infers the column names from the first row of the records. These names will be the keys of the features dict of each dataset element.  \n  column_defaults   A optional list of default values for the CSV fields. One item per selected column of the input record. Each item in the list is either a valid CSV dtype (float32, float64, int32, int64, or string), or a Tensor with one of the aforementioned types. The tensor can either be a scalar default value (if the column is optional), or an empty tensor (if the column is required). If a dtype is provided instead of a tensor, the column is also treated as required. If this list is not provided, tries to infer types based on reading the first num_rows_for_inference rows of files specified, and assumes all columns are optional, defaulting to 0 for numeric values and \"\" for string values. If both this and select_columns are specified, these must have the same lengths, and column_defaults is assumed to be sorted in order of increasing column index.  \n  label_name   A optional string corresponding to the label column. If provided, the data for this column is returned as a separate Tensor from the features dictionary, so that the dataset complies with the format expected by a tf.Estimator.train or tf.Estimator.evaluate input function.  \n  select_columns   An optional list of integer indices or string column names, that specifies a subset of columns of CSV data to select. If column names are provided, these must correspond to names provided in column_names or inferred from the file header lines. When this argument is specified, only a subset of CSV columns will be parsed and returned, corresponding to the columns specified. Using this results in faster parsing and lower memory usage. If both this and column_defaults are specified, these must have the same lengths, and column_defaults is assumed to be sorted in order of increasing column index.  \n  field_delim   An optional string. Defaults to \",\". Char delimiter to separate fields in a record.  \n  use_quote_delim   An optional bool. Defaults to True. If false, treats double quotation marks as regular characters inside of the string fields.  \n  na_value   Additional string to recognize as NA/NaN.  \n  header   A bool that indicates whether the first rows of provided CSV files correspond to header lines with column names, and should not be included in the data.  \n  num_epochs   An int specifying the number of times this dataset is repeated. If None, cycles through the dataset forever.  \n  shuffle   A bool that indicates whether the input should be shuffled.  \n  shuffle_buffer_size   Buffer size to use for shuffling. A large buffer size ensures better shuffling, but increases memory usage and startup time.  \n  shuffle_seed   Randomization seed to use for shuffling.  \n  prefetch_buffer_size   An int specifying the number of feature batches to prefetch for performance improvement. Recommended value is the number of batches consumed per training step. Defaults to auto-tune.  \n  num_parallel_reads   Number of threads used to read CSV records from files. If >1, the results will be interleaved. Defaults to 1.  \n  sloppy   If True, reading performance will be improved at the cost of non-deterministic ordering. If False, the order of elements produced is deterministic prior to shuffling (elements are still randomized if shuffle=True. Note that if the seed is set, then order of elements after shuffling is deterministic). Defaults to False.  \n  num_rows_for_inference   Number of rows of a file to use for type inference if record_defaults is not provided. If None, reads all the rows of all the files. Defaults to 100.  \n  compression_type   (Optional.) A tf.string scalar evaluating to one of \"\" (no compression), \"ZLIB\", or \"GZIP\". Defaults to no compression.  \n  ignore_errors   (Optional.) If True, ignores errors with CSV file parsing, such as malformed data or empty lines, and moves on to the next valid CSV record. Otherwise, the dataset raises an error and stops processing when encountering any invalid records. Defaults to False.   \n \n\n\n Returns   A dataset, where each element is a (features, labels) tuple that corresponds to a batch of batch_size CSV rows. The features dictionary maps feature column names to Tensors containing the corresponding column data, and labels is a Tensor containing the column data for the label column specified by label_name.  \n\n \n\n\n Raises\n  ValueError   If any of the arguments is malformed.     \n"}, {"name": "tf.compat.v1.data.experimental.map_and_batch_with_legacy_function", "path": "compat/v1/data/experimental/map_and_batch_with_legacy_function", "type": "tf.compat", "text": "tf.compat.v1.data.experimental.map_and_batch_with_legacy_function Fused implementation of map and batch. (deprecated) \ntf.compat.v1.data.experimental.map_and_batch_with_legacy_function(\n    map_func, batch_size, num_parallel_batches=None, drop_remainder=False,\n    num_parallel_calls=None\n)\n Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use `tf.data.experimental.map_and_batch()\nNote: This is an escape hatch for existing uses of map_and_batch that do not work with V2 functions. New uses are strongly discouraged and existing uses should migrate to map_and_batch as this method will not be removed in V2.\n\n \n\n\n Args\n  map_func   A function mapping a nested structure of tensors to another nested structure of tensors.  \n  batch_size   A tf.int64 scalar tf.Tensor, representing the number of consecutive elements of this dataset to combine in a single batch.  \n  num_parallel_batches   (Optional.) A tf.int64 scalar tf.Tensor, representing the number of batches to create in parallel. On one hand, higher values can help mitigate the effect of stragglers. On the other hand, higher values can increase contention if CPU is scarce.  \n  drop_remainder   (Optional.) A tf.bool scalar tf.Tensor, representing whether the last batch should be dropped in case its size is smaller than desired; the default behavior is not to drop the smaller batch.  \n  num_parallel_calls   (Optional.) A tf.int32 scalar tf.Tensor, representing the number of elements to process in parallel. If not specified, batch_size * num_parallel_batches elements will be processed in parallel. If the value tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available CPU.   \n \n\n\n Returns   A Dataset transformation function, which can be passed to tf.data.Dataset.apply.  \n\n \n\n\n Raises\n  ValueError   If both num_parallel_batches and num_parallel_calls are specified.     \n"}, {"name": "tf.compat.v1.data.experimental.RaggedTensorStructure", "path": "compat/v1/data/experimental/raggedtensorstructure", "type": "tf.compat", "text": "tf.compat.v1.data.experimental.RaggedTensorStructure DEPRECATED FUNCTION \ntf.compat.v1.data.experimental.RaggedTensorStructure(\n    dtype, shape, ragged_rank\n)\n Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.RaggedTensorSpec instead.  \n"}, {"name": "tf.compat.v1.data.experimental.RandomDataset", "path": "compat/v1/data/experimental/randomdataset", "type": "tf.compat", "text": "tf.compat.v1.data.experimental.RandomDataset A Dataset of pseudorandom values. Inherits From: Dataset, Dataset \ntf.compat.v1.data.experimental.RandomDataset(\n    seed=None\n)\n\n \n\n\n Attributes\n  element_spec   The type specification of an element of this dataset. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset.element_spec\nTensorSpec(shape=(), dtype=tf.int32, name=None)\n\n \n  output_classes   Returns the class of each component of an element of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.compat.v1.data.get_output_classes(dataset). \n \n  output_shapes   Returns the shape of each component of an element of this dataset. (deprecated)Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.compat.v1.data.get_output_shapes(dataset). \n \n  output_types   Returns the type of each component of an element of this dataset. (deprecated)Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.compat.v1.data.get_output_types(dataset). \n   Methods apply View source \napply(\n    transformation_func\n)\n Applies a transformation function to this dataset. apply enables chaining of custom Dataset transformations, which are represented as functions that take one Dataset argument and return a transformed Dataset. \ndataset = tf.data.Dataset.range(100)\ndef dataset_fn(ds):\n  return ds.filter(lambda x: x < 5)\ndataset = dataset.apply(dataset_fn)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2, 3, 4]\n\n \n\n\n Args\n  transformation_func   A function that takes one Dataset argument and returns a Dataset.   \n \n\n\n Returns\n  Dataset   The Dataset returned by applying transformation_func to this dataset.    as_numpy_iterator View source \nas_numpy_iterator()\n Returns an iterator which converts all elements of the dataset to numpy. Use as_numpy_iterator to inspect the content of your dataset. To see element shapes and types, print dataset elements directly instead of using as_numpy_iterator. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nfor element in dataset:\n  print(element)\ntf.Tensor(1, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(3, shape=(), dtype=int32)\n This method requires that you are running in eager mode and the dataset's element_spec contains only TensorSpec components. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nfor element in dataset.as_numpy_iterator():\n  print(element)\n1\n2\n3\n \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nprint(list(dataset.as_numpy_iterator()))\n[1, 2, 3]\n as_numpy_iterator() will preserve the nested structure of dataset elements. \ndataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),\n                                              'b': [5, 6]})\nlist(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5},\n                                      {'a': (2, 4), 'b': 6}]\nTrue\n\n \n\n\n Returns   An iterable over the elements of the dataset, with their tensors converted to numpy arrays.  \n\n \n\n\n Raises\n  TypeError   if an element contains a non-Tensor value.  \n  RuntimeError   if eager execution is not enabled.    batch View source \nbatch(\n    batch_size, drop_remainder=False\n)\n Combines consecutive elements of this dataset into batches. \ndataset = tf.data.Dataset.range(8)\ndataset = dataset.batch(3)\nlist(dataset.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5]), array([6, 7])]\n \ndataset = tf.data.Dataset.range(8)\ndataset = dataset.batch(3, drop_remainder=True)\nlist(dataset.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5])]\n The components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced.\n \n\n\n Args\n  batch_size   A tf.int64 scalar tf.Tensor, representing the number of consecutive elements of this dataset to combine in a single batch.  \n  drop_remainder   (Optional.) A tf.bool scalar tf.Tensor, representing whether the last batch should be dropped in the case it has fewer than batch_size elements; the default behavior is not to drop the smaller batch.   \n \n\n\n Returns\n  Dataset   A Dataset.    cache View source \ncache(\n    filename=''\n)\n Caches the elements in this dataset. The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data. \nNote: For the cache to be finalized, the input dataset must be iterated through in its entirety. Otherwise, subsequent iterations will not use cached data.\n \ndataset = tf.data.Dataset.range(5)\ndataset = dataset.map(lambda x: x**2)\ndataset = dataset.cache()\n# The first time reading through the data will generate the data using\n# `range` and `map`.\nlist(dataset.as_numpy_iterator())\n[0, 1, 4, 9, 16]\n# Subsequent iterations read from the cache.\nlist(dataset.as_numpy_iterator())\n[0, 1, 4, 9, 16]\n When caching to a file, the cached data will persist across runs. Even the first iteration through the data will read from the cache file. Changing the input pipeline before the call to .cache() will have no effect until the cache file is removed or the filename is changed. \ndataset = tf.data.Dataset.range(5)\ndataset = dataset.cache(\"/path/to/file\")  # doctest: +SKIP\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[0, 1, 2, 3, 4]\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.cache(\"/path/to/file\")  # Same file! # doctest: +SKIP\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[0, 1, 2, 3, 4]\n \nNote: cache will produce exactly the same elements during each iteration through the dataset. If you wish to randomize the iteration order, make sure to call shuffle after calling cache.\n\n \n\n\n Args\n  filename   A tf.string scalar tf.Tensor, representing the name of a directory on the filesystem to use for caching elements in this Dataset. If a filename is not provided, the dataset will be cached in memory.   \n \n\n\n Returns\n  Dataset   A Dataset.    cardinality View source \ncardinality()\n Returns the cardinality of the dataset, if known. cardinality may return tf.data.INFINITE_CARDINALITY if the dataset contains an infinite number of elements or tf.data.UNKNOWN_CARDINALITY if the analysis fails to determine the number of elements in the dataset (e.g. when the dataset source is a file). \ndataset = tf.data.Dataset.range(42)\nprint(dataset.cardinality().numpy())\n42\ndataset = dataset.repeat()\ncardinality = dataset.cardinality()\nprint((cardinality == tf.data.INFINITE_CARDINALITY).numpy())\nTrue\ndataset = dataset.filter(lambda x: True)\ncardinality = dataset.cardinality()\nprint((cardinality == tf.data.UNKNOWN_CARDINALITY).numpy())\nTrue\n\n \n\n\n Returns   A scalar tf.int64 Tensor representing the cardinality of the dataset. If the cardinality is infinite or unknown, cardinality returns the named constants tf.data.INFINITE_CARDINALITY and tf.data.UNKNOWN_CARDINALITY respectively.  \n concatenate View source \nconcatenate(\n    dataset\n)\n Creates a Dataset by concatenating the given dataset with this dataset. \na = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\nb = tf.data.Dataset.range(4, 8)  # ==> [ 4, 5, 6, 7 ]\nds = a.concatenate(b)\nlist(ds.as_numpy_iterator())\n[1, 2, 3, 4, 5, 6, 7]\n# The input dataset and dataset to be concatenated should have the same\n# nested structures and output types.\nc = tf.data.Dataset.zip((a, b))\na.concatenate(c)\nTraceback (most recent call last):\nTypeError: Two datasets to concatenate have different types\n<dtype: 'int64'> and (tf.int64, tf.int64)\nd = tf.data.Dataset.from_tensor_slices([\"a\", \"b\", \"c\"])\na.concatenate(d)\nTraceback (most recent call last):\nTypeError: Two datasets to concatenate have different types\n<dtype: 'int64'> and <dtype: 'string'>\n\n \n\n\n Args\n  dataset   Dataset to be concatenated.   \n \n\n\n Returns\n  Dataset   A Dataset.    enumerate View source \nenumerate(\n    start=0\n)\n Enumerates the elements of this dataset. It is similar to python's enumerate. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.enumerate(start=5)\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(5, 1)\n(6, 2)\n(7, 3)\n \n# The nested structure of the input dataset determines the structure of\n# elements in the resulting dataset.\ndataset = tf.data.Dataset.from_tensor_slices([(7, 8), (9, 10)])\ndataset = dataset.enumerate()\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(0, array([7, 8], dtype=int32))\n(1, array([ 9, 10], dtype=int32))\n\n \n\n\n Args\n  start   A tf.int64 scalar tf.Tensor, representing the start value for enumeration.   \n \n\n\n Returns\n  Dataset   A Dataset.    filter View source \nfilter(\n    predicate\n)\n Filters this dataset according to predicate. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.filter(lambda x: x < 3)\nlist(dataset.as_numpy_iterator())\n[1, 2]\n# `tf.math.equal(x, y)` is required for equality comparison\ndef filter_fn(x):\n  return tf.math.equal(x, 1)\ndataset = dataset.filter(filter_fn)\nlist(dataset.as_numpy_iterator())\n[1]\n\n \n\n\n Args\n  predicate   A function mapping a dataset element to a boolean.   \n \n\n\n Returns\n  Dataset   The Dataset containing the elements of this dataset for which predicate is True.    filter_with_legacy_function View source \nfilter_with_legacy_function(\n    predicate\n)\n Filters this dataset according to predicate. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use `tf.data.Dataset.filter()\nNote: This is an escape hatch for existing uses of filter that do not work with V2 functions. New uses are strongly discouraged and existing uses should migrate to filter as this method will be removed in V2.\n\n \n\n\n Args\n  predicate   A function mapping a nested structure of tensors (having shapes and types defined by self.output_shapes and self.output_types) to a scalar tf.bool tensor.   \n \n\n\n Returns\n  Dataset   The Dataset containing the elements of this dataset for which predicate is True.    flat_map View source \nflat_map(\n    map_func\n)\n Maps map_func across this dataset and flattens the result. Use flat_map if you want to make sure that the order of your dataset stays the same. For example, to flatten a dataset of batches into a dataset of their elements: \ndataset = tf.data.Dataset.from_tensor_slices(\n               [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ndataset = dataset.flat_map(lambda x: Dataset.from_tensor_slices(x))\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n tf.data.Dataset.interleave() is a generalization of flat_map, since flat_map produces the same output as tf.data.Dataset.interleave(cycle_length=1)\n \n\n\n Args\n  map_func   A function mapping a dataset element to a dataset.   \n \n\n\n Returns\n  Dataset   A Dataset.    from_generator View source \n@staticmethod\nfrom_generator(\n    generator, output_types=None, output_shapes=None, args=None,\n    output_signature=None\n)\n Creates a Dataset whose elements are generated by generator. (deprecated arguments) Warning: SOME ARGUMENTS ARE DEPRECATED: (output_shapes, output_types). They will be removed in a future version. Instructions for updating: Use output_signature instead The generator argument must be a callable object that returns an object that supports the iter() protocol (e.g. a generator function). The elements generated by generator must be compatible with either the given output_signature argument or with the given output_types and (optionally) output_shapes arguments, whichiver was specified. The recommended way to call from_generator is to use the output_signature argument. In this case the output will be assumed to consist of objects with the classes, shapes and types defined by tf.TypeSpec objects from output_signature argument: \ndef gen():\n  ragged_tensor = tf.ragged.constant([[1, 2], [3]])\n  yield 42, ragged_tensor\n\ndataset = tf.data.Dataset.from_generator(\n     gen,\n     output_signature=(\n         tf.TensorSpec(shape=(), dtype=tf.int32),\n         tf.RaggedTensorSpec(shape=(2, None), dtype=tf.int32)))\n\nlist(dataset.take(1))\n[(<tf.Tensor: shape=(), dtype=int32, numpy=42>,\n<tf.RaggedTensor [[1, 2], [3]]>)]\n There is also a deprecated way to call from_generator by either with output_types argument alone or together with output_shapes argument. In this case the output of the function will be assumed to consist of tf.Tensor objects with with the types defined by output_types and with the shapes which are either unknown or defined by output_shapes. \nNote: The current implementation of Dataset.from_generator() uses tf.numpy_function and inherits the same constraints. In particular, it requires the dataset and iterator related operations to be placed on a device in the same process as the Python program that called Dataset.from_generator(). The body of generator will not be serialized in a GraphDef, and you should not use this method if you need to serialize your model and restore it in a different environment.\n\n\nNote: If generator depends on mutable global variables or other external state, be aware that the runtime may invoke generator multiple times (in order to support repeating the Dataset) and at any time between the call to Dataset.from_generator() and the production of the first element from the generator. Mutating global variables or external state can cause undefined behavior, and we recommend that you explicitly cache any external state in generator before calling Dataset.from_generator().\n\n \n\n\n Args\n  generator   A callable object that returns an object that supports the iter() protocol. If args is not specified, generator must take no arguments; otherwise it must take as many arguments as there are values in args.  \n  output_types   (Optional.) A nested structure of tf.DType objects corresponding to each component of an element yielded by generator.  \n  output_shapes   (Optional.) A nested structure of tf.TensorShape objects corresponding to each component of an element yielded by generator.  \n  args   (Optional.) A tuple of tf.Tensor objects that will be evaluated and passed to generator as NumPy-array arguments.  \n  output_signature   (Optional.) A nested structure of tf.TypeSpec objects corresponding to each component of an element yielded by generator.   \n \n\n\n Returns\n  Dataset   A Dataset.    from_sparse_tensor_slices View source \n@staticmethod\nfrom_sparse_tensor_slices(\n    sparse_tensor\n)\n Splits each rank-N tf.sparse.SparseTensor in this dataset row-wise. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.data.Dataset.from_tensor_slices().\n \n\n\n Args\n  sparse_tensor   A tf.sparse.SparseTensor.   \n \n\n\n Returns\n  Dataset   A Dataset of rank-(N-1) sparse tensors.    from_tensor_slices View source \n@staticmethod\nfrom_tensor_slices(\n    tensors\n)\n Creates a Dataset whose elements are slices of the given tensors. The given tensors are sliced along their first dimension. This operation preserves the structure of the input tensors, removing the first dimension of each tensor and using it as the dataset dimension. All input tensors must have the same size in their first dimensions. \n# Slicing a 1D tensor produces scalar tensor elements.\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nlist(dataset.as_numpy_iterator())\n[1, 2, 3]\n \n# Slicing a 2D tensor produces 1D tensor elements.\ndataset = tf.data.Dataset.from_tensor_slices([[1, 2], [3, 4]])\nlist(dataset.as_numpy_iterator())\n[array([1, 2], dtype=int32), array([3, 4], dtype=int32)]\n \n# Slicing a tuple of 1D tensors produces tuple elements containing\n# scalar tensors.\ndataset = tf.data.Dataset.from_tensor_slices(([1, 2], [3, 4], [5, 6]))\nlist(dataset.as_numpy_iterator())\n[(1, 3, 5), (2, 4, 6)]\n \n# Dictionary structure is also preserved.\ndataset = tf.data.Dataset.from_tensor_slices({\"a\": [1, 2], \"b\": [3, 4]})\nlist(dataset.as_numpy_iterator()) == [{'a': 1, 'b': 3},\n                                      {'a': 2, 'b': 4}]\nTrue\n \n# Two tensors can be combined into one Dataset object.\nfeatures = tf.constant([[1, 3], [2, 1], [3, 3]]) # ==> 3x2 tensor\nlabels = tf.constant(['A', 'B', 'A']) # ==> 3x1 tensor\ndataset = Dataset.from_tensor_slices((features, labels))\n# Both the features and the labels tensors can be converted\n# to a Dataset object separately and combined after.\nfeatures_dataset = Dataset.from_tensor_slices(features)\nlabels_dataset = Dataset.from_tensor_slices(labels)\ndataset = Dataset.zip((features_dataset, labels_dataset))\n# A batched feature and label set can be converted to a Dataset\n# in similar fashion.\nbatched_features = tf.constant([[[1, 3], [2, 3]],\n                                [[2, 1], [1, 2]],\n                                [[3, 3], [3, 2]]], shape=(3, 2, 2))\nbatched_labels = tf.constant([['A', 'A'],\n                              ['B', 'B'],\n                              ['A', 'B']], shape=(3, 2, 1))\ndataset = Dataset.from_tensor_slices((batched_features, batched_labels))\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(array([[1, 3],\n       [2, 3]], dtype=int32), array([[b'A'],\n       [b'A']], dtype=object))\n(array([[2, 1],\n       [1, 2]], dtype=int32), array([[b'B'],\n       [b'B']], dtype=object))\n(array([[3, 3],\n       [3, 2]], dtype=int32), array([[b'A'],\n       [b'B']], dtype=object))\n Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.\n \n\n\n Args\n  tensors   A dataset element, with each component having the same size in the first dimension.   \n \n\n\n Returns\n  Dataset   A Dataset.    from_tensors View source \n@staticmethod\nfrom_tensors(\n    tensors\n)\n Creates a Dataset with a single element, comprising the given tensors. from_tensors produces a dataset containing only a single element. To slice the input tensor into multiple elements, use from_tensor_slices instead. \ndataset = tf.data.Dataset.from_tensors([1, 2, 3])\nlist(dataset.as_numpy_iterator())\n[array([1, 2, 3], dtype=int32)]\ndataset = tf.data.Dataset.from_tensors(([1, 2, 3], 'A'))\nlist(dataset.as_numpy_iterator())\n[(array([1, 2, 3], dtype=int32), b'A')]\n \n# You can use `from_tensors` to produce a dataset which repeats\n# the same example many times.\nexample = tf.constant([1,2,3])\ndataset = tf.data.Dataset.from_tensors(example).repeat(2)\nlist(dataset.as_numpy_iterator())\n[array([1, 2, 3], dtype=int32), array([1, 2, 3], dtype=int32)]\n Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.\n \n\n\n Args\n  tensors   A dataset element.   \n \n\n\n Returns\n  Dataset   A Dataset.    interleave View source \ninterleave(\n    map_func, cycle_length=None, block_length=None, num_parallel_calls=None,\n    deterministic=None\n)\n Maps map_func across this dataset, and interleaves the results. For example, you can use Dataset.interleave() to process many input files concurrently: \n# Preprocess 4 files concurrently, and interleave blocks of 16 records\n# from each file.\nfilenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n             \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\ndataset = tf.data.Dataset.from_tensor_slices(filenames)\ndef parse_fn(filename):\n  return tf.data.Dataset.range(10)\ndataset = dataset.interleave(lambda x:\n    tf.data.TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n    cycle_length=4, block_length=16)\n The cycle_length and block_length arguments control the order in which elements are produced. cycle_length controls the number of input elements that are processed concurrently. If you set cycle_length to 1, this transformation will handle one input element at a time, and will produce identical results to tf.data.Dataset.flat_map. In general, this transformation will apply map_func to cycle_length input elements, open iterators on the returned Dataset objects, and cycle through them producing block_length consecutive elements from each iterator, and consuming the next input element each time it reaches the end of an iterator. For example: \ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n# NOTE: New lines indicate \"block\" boundaries.\ndataset = dataset.interleave(\n    lambda x: Dataset.from_tensors(x).repeat(6),\n    cycle_length=2, block_length=4)\nlist(dataset.as_numpy_iterator())\n[1, 1, 1, 1,\n 2, 2, 2, 2,\n 1, 1,\n 2, 2,\n 3, 3, 3, 3,\n 4, 4, 4, 4,\n 3, 3,\n 4, 4,\n 5, 5, 5, 5,\n 5, 5]\n \nNote: The order of elements yielded by this transformation is deterministic, as long as map_func is a pure function and deterministic=True. If map_func contains any stateful operations, the order in which that state is accessed is undefined.\n Performance can often be improved by setting num_parallel_calls so that interleave will use multiple threads to fetch elements. If determinism isn't required, it can also improve performance to set deterministic=False. \nfilenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n             \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\ndataset = tf.data.Dataset.from_tensor_slices(filenames)\ndataset = dataset.interleave(lambda x: tf.data.TFRecordDataset(x),\n    cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE,\n    deterministic=False)\n\n \n\n\n Args\n  map_func   A function mapping a dataset element to a dataset.  \n  cycle_length   (Optional.) The number of input elements that will be processed concurrently. If not set, the tf.data runtime decides what it should be based on available CPU. If num_parallel_calls is set to tf.data.AUTOTUNE, the cycle_length argument identifies the maximum degree of parallelism.  \n  block_length   (Optional.) The number of consecutive elements to produce from each input element before cycling to another input element. If not set, defaults to 1.  \n  num_parallel_calls   (Optional.) If specified, the implementation creates a threadpool, which is used to fetch inputs from cycle elements asynchronously and in parallel. The default behavior is to fetch inputs from cycle elements synchronously with no parallelism. If the value tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available CPU.  \n  deterministic   (Optional.) A boolean controlling whether determinism should be traded for performance by allowing elements to be produced out of order. If deterministic is None, the tf.data.Options.experimental_deterministic dataset option (True by default) is used to decide whether to produce elements deterministically.   \n \n\n\n Returns\n  Dataset   A Dataset.    list_files View source \n@staticmethod\nlist_files(\n    file_pattern, shuffle=None, seed=None\n)\n A dataset of all files matching one or more glob patterns. The file_pattern argument should be a small number of glob patterns. If your filenames have already been globbed, use Dataset.from_tensor_slices(filenames) instead, as re-globbing every filename with list_files may result in poor performance with remote storage systems. \nNote: The default behavior of this method is to return filenames in a non-deterministic random shuffled order. Pass a seed or shuffle=False to get results in a deterministic order.\n Example: If we had the following files on our filesystem:  /path/to/dir/a.txt /path/to/dir/b.py /path/to/dir/c.py  If we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:  /path/to/dir/b.py /path/to/dir/c.py \n \n\n\n Args\n  file_pattern   A string, a list of strings, or a tf.Tensor of string type (scalar or vector), representing the filename glob (i.e. shell wildcard) pattern(s) that will be matched.  \n  shuffle   (Optional.) If True, the file names will be shuffled randomly. Defaults to True.  \n  seed   (Optional.) A tf.int64 scalar tf.Tensor, representing the random seed that will be used to create the distribution. See tf.random.set_seed for behavior.   \n \n\n\n Returns\n  Dataset   A Dataset of strings corresponding to file names.    make_initializable_iterator View source \nmake_initializable_iterator(\n    shared_name=None\n)\n Creates an iterator for elements of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through tf.compat.v1. In all other situations -- namely, eager mode and inside tf.function -- you can consume dataset elements using for elem in dataset: ... or by explicitly creating iterator via iterator = iter(dataset) and fetching its elements via values = next(iterator). Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use tf.compat.v1.data.make_initializable_iterator(dataset) to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\nNote: The returned iterator will be in an uninitialized state, and you must run the iterator.initializer operation before using it:\n\n# Building graph ...\ndataset = ...\niterator = dataset.make_initializable_iterator()\nnext_value = iterator.get_next()  # This is a Tensor.\n\n# ... from within a session ...\nsess.run(iterator.initializer)\ntry:\n  while True:\n    value = sess.run(next_value)\n    ...\nexcept tf.errors.OutOfRangeError:\n    pass\n\n \n\n\n Args\n  shared_name   (Optional.) If non-empty, the returned iterator will be shared under the given name across multiple sessions that share the same devices (e.g. when using a remote server).   \n \n\n\n Returns   A tf.data.Iterator for elements of this dataset.  \n\n \n\n\n Raises\n  RuntimeError   If eager execution is enabled.    make_one_shot_iterator View source \nmake_one_shot_iterator()\n Creates an iterator for elements of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through tf.compat.v1. In all other situations -- namely, eager mode and inside tf.function -- you can consume dataset elements using for elem in dataset: ... or by explicitly creating iterator via iterator = iter(dataset) and fetching its elements via values = next(iterator). Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use tf.compat.v1.data.make_one_shot_iterator(dataset) to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\nNote: The returned iterator will be initialized automatically. A \"one-shot\" iterator does not currently support re-initialization. For that see make_initializable_iterator.\n Example: # Building graph ...\ndataset = ...\nnext_value = dataset.make_one_shot_iterator().get_next()\n\n# ... from within a session ...\ntry:\n  while True:\n    value = sess.run(next_value)\n    ...\nexcept tf.errors.OutOfRangeError:\n    pass\n\n \n\n\n Returns   An tf.data.Iterator for elements of this dataset.  \n map View source \nmap(\n    map_func, num_parallel_calls=None, deterministic=None\n)\n Maps map_func across the elements of this dataset. This transformation applies map_func to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. map_func can be used to change both the values and the structure of a dataset's elements. For example, adding 1 to each element, or projecting a subset of element components. \ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\ndataset = dataset.map(lambda x: x + 1)\nlist(dataset.as_numpy_iterator())\n[2, 3, 4, 5, 6]\n The input signature of map_func is determined by the structure of each element in this dataset. \ndataset = Dataset.range(5)\n# `map_func` takes a single argument of type `tf.Tensor` with the same\n# shape and dtype.\nresult = dataset.map(lambda x: x + 1)\n \n# Each element is a tuple containing two `tf.Tensor` objects.\nelements = [(1, \"foo\"), (2, \"bar\"), (3, \"baz\")]\ndataset = tf.data.Dataset.from_generator(\n    lambda: elements, (tf.int32, tf.string))\n# `map_func` takes two arguments of type `tf.Tensor`. This function\n# projects out just the first component.\nresult = dataset.map(lambda x_int, y_str: x_int)\nlist(result.as_numpy_iterator())\n[1, 2, 3]\n \n# Each element is a dictionary mapping strings to `tf.Tensor` objects.\nelements =  ([{\"a\": 1, \"b\": \"foo\"},\n              {\"a\": 2, \"b\": \"bar\"},\n              {\"a\": 3, \"b\": \"baz\"}])\ndataset = tf.data.Dataset.from_generator(\n    lambda: elements, {\"a\": tf.int32, \"b\": tf.string})\n# `map_func` takes a single argument of type `dict` with the same keys\n# as the elements.\nresult = dataset.map(lambda d: str(d[\"a\"]) + d[\"b\"])\n The value or values returned by map_func determine the structure of each element in the returned dataset. \ndataset = tf.data.Dataset.range(3)\n# `map_func` returns two `tf.Tensor` objects.\ndef g(x):\n  return tf.constant(37.0), tf.constant([\"Foo\", \"Bar\", \"Baz\"])\nresult = dataset.map(g)\nresult.element_spec\n(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(3,), dtype=tf.string, name=None))\n# Python primitives, lists, and NumPy arrays are implicitly converted to\n# `tf.Tensor`.\ndef h(x):\n  return 37.0, [\"Foo\", \"Bar\"], np.array([1.0, 2.0], dtype=np.float64)\nresult = dataset.map(h)\nresult.element_spec\n(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(2,), dtype=tf.string, name=None), TensorSpec(shape=(2,), dtype=tf.float64, name=None))\n# `map_func` can return nested structures.\ndef i(x):\n  return (37.0, [42, 16]), \"foo\"\nresult = dataset.map(i)\nresult.element_spec\n((TensorSpec(shape=(), dtype=tf.float32, name=None),\n  TensorSpec(shape=(2,), dtype=tf.int32, name=None)),\n TensorSpec(shape=(), dtype=tf.string, name=None))\n map_func can accept as arguments and return any type of dataset element. Note that irrespective of the context in which map_func is defined (eager vs. graph), tf.data traces the function and executes it as a graph. To use Python code inside of the function you have a few options: 1) Rely on AutoGraph to convert Python code into an equivalent graph computation. The downside of this approach is that AutoGraph can convert some but not all Python code. 2) Use tf.py_function, which allows you to write arbitrary Python code but will generally result in worse performance than 1). For example: \nd = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n# transform a string tensor to upper case string using a Python function\ndef upper_case_fn(t: tf.Tensor):\n  return t.numpy().decode('utf-8').upper()\nd = d.map(lambda x: tf.py_function(func=upper_case_fn,\n          inp=[x], Tout=tf.string))\nlist(d.as_numpy_iterator())\n[b'HELLO', b'WORLD']\n 3) Use tf.numpy_function, which also allows you to write arbitrary Python code. Note that tf.py_function accepts tf.Tensor whereas tf.numpy_function accepts numpy arrays and returns only numpy arrays. For example: \nd = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\ndef upper_case_fn(t: np.ndarray):\n  return t.decode('utf-8').upper()\nd = d.map(lambda x: tf.numpy_function(func=upper_case_fn,\n          inp=[x], Tout=tf.string))\nlist(d.as_numpy_iterator())\n[b'HELLO', b'WORLD']\n Note that the use of tf.numpy_function and tf.py_function in general precludes the possibility of executing user-defined transformations in parallel (because of Python GIL). Performance can often be improved by setting num_parallel_calls so that map will use multiple threads to process elements. If deterministic order isn't required, it can also improve performance to set deterministic=False. \ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\ndataset = dataset.map(lambda x: x + 1,\n    num_parallel_calls=tf.data.AUTOTUNE,\n    deterministic=False)\n\n \n\n\n Args\n  map_func   A function mapping a dataset element to another dataset element.  \n  num_parallel_calls   (Optional.) A tf.int32 scalar tf.Tensor, representing the number elements to process asynchronously in parallel. If not specified, elements will be processed sequentially. If the value tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available CPU.  \n  deterministic   (Optional.) A boolean controlling whether determinism should be traded for performance by allowing elements to be produced out of order. If deterministic is None, the tf.data.Options.experimental_deterministic dataset option (True by default) is used to decide whether to produce elements deterministically.   \n \n\n\n Returns\n  Dataset   A Dataset.    map_with_legacy_function View source \nmap_with_legacy_function(\n    map_func, num_parallel_calls=None, deterministic=None\n)\n Maps map_func across the elements of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use `tf.data.Dataset.map()\nNote: This is an escape hatch for existing uses of map that do not work with V2 functions. New uses are strongly discouraged and existing uses should migrate to map as this method will be removed in V2.\n\n \n\n\n Args\n  map_func   A function mapping a nested structure of tensors (having shapes and types defined by self.output_shapes and self.output_types) to another nested structure of tensors.  \n  num_parallel_calls   (Optional.) A tf.int32 scalar tf.Tensor, representing the number elements to process asynchronously in parallel. If not specified, elements will be processed sequentially. If the value tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available CPU.  \n  deterministic   (Optional.) A boolean controlling whether determinism should be traded for performance by allowing elements to be produced out of order. If deterministic is None, the tf.data.Options.experimental_deterministic dataset option (True by default) is used to decide whether to produce elements deterministically.   \n \n\n\n Returns\n  Dataset   A Dataset.    options View source \noptions()\n Returns the options for this dataset and its inputs.\n \n\n\n Returns   A tf.data.Options object representing the dataset options.  \n padded_batch View source \npadded_batch(\n    batch_size, padded_shapes=None, padding_values=None, drop_remainder=False\n)\n Combines consecutive elements of this dataset into padded batches. This transformation combines multiple consecutive elements of the input dataset into a single element. Like tf.data.Dataset.batch, the components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced. Unlike tf.data.Dataset.batch, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in padded_shapes. The padded_shapes argument determines the resulting shape for each dimension of each component in an output element:  If the dimension is a constant, the component will be padded out to that length in that dimension. If the dimension is unknown, the component will be padded out to the maximum length of all elements in that dimension.  \nA = (tf.data.Dataset\n     .range(1, 5, output_type=tf.int32)\n     .map(lambda x: tf.fill([x], x)))\n# Pad to the smallest per-batch size that fits all elements.\nB = A.padded_batch(2)\nfor element in B.as_numpy_iterator():\n  print(element)\n[[1 0]\n [2 2]]\n[[3 3 3 0]\n [4 4 4 4]]\n# Pad to a fixed size.\nC = A.padded_batch(2, padded_shapes=5)\nfor element in C.as_numpy_iterator():\n  print(element)\n[[1 0 0 0 0]\n [2 2 0 0 0]]\n[[3 3 3 0 0]\n [4 4 4 4 0]]\n# Pad with a custom value.\nD = A.padded_batch(2, padded_shapes=5, padding_values=-1)\nfor element in D.as_numpy_iterator():\n  print(element)\n[[ 1 -1 -1 -1 -1]\n [ 2  2 -1 -1 -1]]\n[[ 3  3  3 -1 -1]\n [ 4  4  4  4 -1]]\n# Components of nested elements can be padded independently.\nelements = [([1, 2, 3], [10]),\n            ([4, 5], [11, 12])]\ndataset = tf.data.Dataset.from_generator(\n    lambda: iter(elements), (tf.int32, tf.int32))\n# Pad the first component of the tuple to length 4, and the second\n# component to the smallest size that fits.\ndataset = dataset.padded_batch(2,\n    padded_shapes=([4], [None]),\n    padding_values=(-1, 100))\nlist(dataset.as_numpy_iterator())\n[(array([[ 1,  2,  3, -1], [ 4,  5, -1, -1]], dtype=int32),\n  array([[ 10, 100], [ 11,  12]], dtype=int32))]\n# Pad with a single value and multiple components.\nE = tf.data.Dataset.zip((A, A)).padded_batch(2, padding_values=-1)\nfor element in E.as_numpy_iterator():\n  print(element)\n(array([[ 1, -1],\n       [ 2,  2]], dtype=int32), array([[ 1, -1],\n       [ 2,  2]], dtype=int32))\n(array([[ 3,  3,  3, -1],\n       [ 4,  4,  4,  4]], dtype=int32), array([[ 3,  3,  3, -1],\n       [ 4,  4,  4,  4]], dtype=int32))\n See also tf.data.experimental.dense_to_sparse_batch, which combines elements that may have different shapes into a tf.sparse.SparseTensor.\n \n\n\n Args\n  batch_size   A tf.int64 scalar tf.Tensor, representing the number of consecutive elements of this dataset to combine in a single batch.  \n  padded_shapes   (Optional.) A nested structure of tf.TensorShape or tf.int64 vector tensor-like objects representing the shape to which the respective component of each input element should be padded prior to batching. Any unknown dimensions will be padded to the maximum size of that dimension in each batch. If unset, all dimensions of all components are padded to the maximum size in the batch. padded_shapes must be set if any component has an unknown rank.  \n  padding_values   (Optional.) A nested structure of scalar-shaped tf.Tensor, representing the padding values to use for the respective components. None represents that the nested structure should be padded with default values. Defaults are 0 for numeric types and the empty string for string types. The padding_values should have the same structure as the input dataset. If padding_values is a single element and the input dataset has multiple components, then the same padding_values will be used to pad every component of the dataset. If padding_values is a scalar, then its value will be broadcasted to match the shape of each component.  \n  drop_remainder   (Optional.) A tf.bool scalar tf.Tensor, representing whether the last batch should be dropped in the case it has fewer than batch_size elements; the default behavior is not to drop the smaller batch.   \n \n\n\n Returns\n  Dataset   A Dataset.   \n \n\n\n Raises\n  ValueError   If a component has an unknown rank, and the padded_shapes argument is not set.    prefetch View source \nprefetch(\n    buffer_size\n)\n Creates a Dataset that prefetches elements from this dataset. Most dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements. \nNote: Like other Dataset methods, prefetch operates on the elements of the input dataset. It has no concept of examples vs. batches. examples.prefetch(2) will prefetch two elements (2 examples), while examples.batch(20).prefetch(2) will prefetch 2 elements (2 batches, of 20 examples each).\n \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.prefetch(2)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2]\n\n \n\n\n Args\n  buffer_size   A tf.int64 scalar tf.Tensor, representing the maximum number of elements that will be buffered when prefetching.   \n \n\n\n Returns\n  Dataset   A Dataset.    range View source \n@staticmethod\nrange(\n    *args, **kwargs\n)\n Creates a Dataset of a step-separated range of values. \nlist(Dataset.range(5).as_numpy_iterator())\n[0, 1, 2, 3, 4]\nlist(Dataset.range(2, 5).as_numpy_iterator())\n[2, 3, 4]\nlist(Dataset.range(1, 5, 2).as_numpy_iterator())\n[1, 3]\nlist(Dataset.range(1, 5, -2).as_numpy_iterator())\n[]\nlist(Dataset.range(5, 1).as_numpy_iterator())\n[]\nlist(Dataset.range(5, 1, -2).as_numpy_iterator())\n[5, 3]\nlist(Dataset.range(2, 5, output_type=tf.int32).as_numpy_iterator())\n[2, 3, 4]\nlist(Dataset.range(1, 5, 2, output_type=tf.float32).as_numpy_iterator())\n[1.0, 3.0]\n\n \n\n\n Args\n  *args   follows the same semantics as python's xrange. len(args) == 1 -> start = 0, stop = args[0], step = 1. len(args) == 2 -> start = args[0], stop = args[1], step = 1. len(args) == 3 -> start = args[0], stop = args[1], step = args[2].  \n  **kwargs    output_type: Its expected dtype. (Optional, default: tf.int64). \n\n  \n \n\n\n Returns\n  Dataset   A RangeDataset.   \n \n\n\n Raises\n  ValueError   if len(args) == 0.    reduce View source \nreduce(\n    initial_state, reduce_func\n)\n Reduces the input dataset to a single element. The transformation calls reduce_func successively on every element of the input dataset until the dataset is exhausted, aggregating information in its internal state. The initial_state argument is used for the initial state and the final state is returned as the result. \ntf.data.Dataset.range(5).reduce(np.int64(0), lambda x, _: x + 1).numpy()\n5\ntf.data.Dataset.range(5).reduce(np.int64(0), lambda x, y: x + y).numpy()\n10\n\n \n\n\n Args\n  initial_state   An element representing the initial state of the transformation.  \n  reduce_func   A function that maps (old_state, input_element) to new_state. It must take two arguments and return a new element The structure of new_state must match the structure of initial_state.   \n \n\n\n Returns   A dataset element corresponding to the final state of the transformation.  \n repeat View source \nrepeat(\n    count=None\n)\n Repeats this dataset so each original value is seen count times. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.repeat(3)\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 1, 2, 3, 1, 2, 3]\n \nNote: If this dataset is a function of global state (e.g. a random number generator), then different repetitions may produce different elements.\n\n \n\n\n Args\n  count   (Optional.) A tf.int64 scalar tf.Tensor, representing the number of times the dataset should be repeated. The default behavior (if count is None or -1) is for the dataset be repeated indefinitely.   \n \n\n\n Returns\n  Dataset   A Dataset.    shard View source \nshard(\n    num_shards, index\n)\n Creates a Dataset that includes only 1/num_shards of this dataset. shard is deterministic. The Dataset produced by A.shard(n, i) will contain all elements of A whose index mod n = i. \nA = tf.data.Dataset.range(10)\nB = A.shard(num_shards=3, index=0)\nlist(B.as_numpy_iterator())\n[0, 3, 6, 9]\nC = A.shard(num_shards=3, index=1)\nlist(C.as_numpy_iterator())\n[1, 4, 7]\nD = A.shard(num_shards=3, index=2)\nlist(D.as_numpy_iterator())\n[2, 5, 8]\n This dataset operator is very useful when running distributed training, as it allows each worker to read a unique subset. When reading a single input file, you can shard elements as follows: d = tf.data.TFRecordDataset(input_file)\nd = d.shard(num_workers, worker_index)\nd = d.repeat(num_epochs)\nd = d.shuffle(shuffle_buffer_size)\nd = d.map(parser_fn, num_parallel_calls=num_map_threads)\n Important caveats:  Be sure to shard before you use any randomizing operator (such as shuffle). Generally it is best if the shard operator is used early in the dataset pipeline. For example, when reading from a set of TFRecord files, shard before converting the dataset to input samples. This avoids reading every file on every worker. The following is an example of an efficient sharding strategy within a complete pipeline:  d = Dataset.list_files(pattern)\nd = d.shard(num_workers, worker_index)\nd = d.repeat(num_epochs)\nd = d.shuffle(shuffle_buffer_size)\nd = d.interleave(tf.data.TFRecordDataset,\n                 cycle_length=num_readers, block_length=1)\nd = d.map(parser_fn, num_parallel_calls=num_map_threads)\n\n \n\n\n Args\n  num_shards   A tf.int64 scalar tf.Tensor, representing the number of shards operating in parallel.  \n  index   A tf.int64 scalar tf.Tensor, representing the worker index.   \n \n\n\n Returns\n  Dataset   A Dataset.   \n \n\n\n Raises\n  InvalidArgumentError   if num_shards or index are illegal values. \nNote: error checking is done on a best-effort basis, and errors aren't guaranteed to be caught upon dataset creation. (e.g. providing in a placeholder tensor bypasses the early checking, and will instead result in an error during a session.run call.) \n\n   shuffle View source \nshuffle(\n    buffer_size, seed=None, reshuffle_each_iteration=None\n)\n Randomly shuffles the elements of this dataset. This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required. For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then shuffle will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer. reshuffle_each_iteration controls whether the shuffle order should be different for each epoch. In TF 1.X, the idiomatic way to create epochs was through the repeat transformation: \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=True)\ndataset = dataset.repeat(2)  # doctest: +SKIP\n[1, 0, 2, 1, 2, 0]\n \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=False)\ndataset = dataset.repeat(2)  # doctest: +SKIP\n[1, 0, 2, 1, 0, 2]\n In TF 2.0, tf.data.Dataset objects are Python iterables which makes it possible to also create epochs through Python iteration: \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=True)\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 0, 2]\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 2, 0]\n \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=False)\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 0, 2]\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 0, 2]\n\n \n\n\n Args\n  buffer_size   A tf.int64 scalar tf.Tensor, representing the number of elements from this dataset from which the new dataset will sample.  \n  seed   (Optional.) A tf.int64 scalar tf.Tensor, representing the random seed that will be used to create the distribution. See tf.random.set_seed for behavior.  \n  reshuffle_each_iteration   (Optional.) A boolean, which if true indicates that the dataset should be pseudorandomly reshuffled each time it is iterated over. (Defaults to True.)   \n \n\n\n Returns\n  Dataset   A Dataset.    skip View source \nskip(\n    count\n)\n Creates a Dataset that skips count elements from this dataset. \ndataset = tf.data.Dataset.range(10)\ndataset = dataset.skip(7)\nlist(dataset.as_numpy_iterator())\n[7, 8, 9]\n\n \n\n\n Args\n  count   A tf.int64 scalar tf.Tensor, representing the number of elements of this dataset that should be skipped to form the new dataset. If count is greater than the size of this dataset, the new dataset will contain no elements. If count is -1, skips the entire dataset.   \n \n\n\n Returns\n  Dataset   A Dataset.    take View source \ntake(\n    count\n)\n Creates a Dataset with at most count elements from this dataset. \ndataset = tf.data.Dataset.range(10)\ndataset = dataset.take(3)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2]\n\n \n\n\n Args\n  count   A tf.int64 scalar tf.Tensor, representing the number of elements of this dataset that should be taken to form the new dataset. If count is -1, or if count is greater than the size of this dataset, the new dataset will contain all elements of this dataset.   \n \n\n\n Returns\n  Dataset   A Dataset.    unbatch View source \nunbatch()\n Splits elements of a dataset into multiple elements. For example, if elements of the dataset are shaped [B, a0, a1, ...], where B may vary for each input element, then for each element in the dataset, the unbatched dataset will contain B consecutive elements of shape [a0, a1, ...]. \nelements = [ [1, 2, 3], [1, 2], [1, 2, 3, 4] ]\ndataset = tf.data.Dataset.from_generator(lambda: elements, tf.int64)\ndataset = dataset.unbatch()\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 1, 2, 1, 2, 3, 4]\n \nNote: unbatch requires a data copy to slice up the batched tensor into smaller, unbatched tensors. When optimizing performance, try to avoid unnecessary usage of unbatch.\n\n \n\n\n Returns   A Dataset.  \n window View source \nwindow(\n    size, shift=None, stride=1, drop_remainder=False\n)\n Combines (nests of) input elements into a dataset of (nests of) windows. A \"window\" is a finite dataset of flat elements of size size (or possibly fewer if there are not enough input elements to fill the window and drop_remainder evaluates to False). The shift argument determines the number of input elements by which the window moves on each iteration. If windows and elements are both numbered starting at 0, the first element in window k will be element k * shift of the input dataset. In particular, the first element of the first window will always be the first element of the input dataset. The stride argument determines the stride of the input elements, and the shift argument determines the shift of the window. For example: \ndataset = tf.data.Dataset.range(7).window(2)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 1]\n[2, 3]\n[4, 5]\n[6]\ndataset = tf.data.Dataset.range(7).window(3, 2, 1, True)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 1, 2]\n[2, 3, 4]\n[4, 5, 6]\ndataset = tf.data.Dataset.range(7).window(3, 1, 2, True)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 2, 4]\n[1, 3, 5]\n[2, 4, 6]\n Note that when the window transformation is applied to a dataset of nested elements, it produces a dataset of nested windows. \nnested = ([1, 2, 3, 4], [5, 6, 7, 8])\ndataset = tf.data.Dataset.from_tensor_slices(nested).window(2)\nfor window in dataset:\n  def to_numpy(ds):\n    return list(ds.as_numpy_iterator())\n  print(tuple(to_numpy(component) for component in window))\n([1, 2], [5, 6])\n([3, 4], [7, 8])\n \ndataset = tf.data.Dataset.from_tensor_slices({'a': [1, 2, 3, 4]})\ndataset = dataset.window(2)\nfor window in dataset:\n  def to_numpy(ds):\n    return list(ds.as_numpy_iterator())\n  print({'a': to_numpy(window['a'])})\n{'a': [1, 2]}\n{'a': [3, 4]}\n\n \n\n\n Args\n  size   A tf.int64 scalar tf.Tensor, representing the number of elements of the input dataset to combine into a window. Must be positive.  \n  shift   (Optional.) A tf.int64 scalar tf.Tensor, representing the number of input elements by which the window moves in each iteration. Defaults to size. Must be positive.  \n  stride   (Optional.) A tf.int64 scalar tf.Tensor, representing the stride of the input elements in the sliding window. Must be positive. The default value of 1 means \"retain every input element\".  \n  drop_remainder   (Optional.) A tf.bool scalar tf.Tensor, representing whether the last windows should be dropped if their size is smaller than size.   \n \n\n\n Returns\n  Dataset   A Dataset of (nests of) windows -- a finite datasets of flat elements created from the (nests of) input elements.    with_options View source \nwith_options(\n    options\n)\n Returns a new tf.data.Dataset with the given options set. The options are \"global\" in the sense they apply to the entire dataset. If options are set multiple times, they are merged as long as different options do not use different non-default values. \nds = tf.data.Dataset.range(5)\nds = ds.interleave(lambda x: tf.data.Dataset.range(5),\n                   cycle_length=3,\n                   num_parallel_calls=3)\noptions = tf.data.Options()\n# This will make the interleave order non-deterministic.\noptions.experimental_deterministic = False\nds = ds.with_options(options)\n\n \n\n\n Args\n  options   A tf.data.Options that identifies the options the use.   \n \n\n\n Returns\n  Dataset   A Dataset with the given options.   \n \n\n\n Raises\n  ValueError   when an option is set more than once to a non-default value    zip View source \n@staticmethod\nzip(\n    datasets\n)\n Creates a Dataset by zipping together the given datasets. This method has similar semantics to the built-in zip() function in Python, with the main difference being that the datasets argument can be an arbitrary nested structure of Dataset objects. \n# The nested structure of the `datasets` argument determines the\n# structure of elements in the resulting dataset.\na = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\nb = tf.data.Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]\nds = tf.data.Dataset.zip((a, b))\nlist(ds.as_numpy_iterator())\n[(1, 4), (2, 5), (3, 6)]\nds = tf.data.Dataset.zip((b, a))\nlist(ds.as_numpy_iterator())\n[(4, 1), (5, 2), (6, 3)]\n\n# The `datasets` argument may contain an arbitrary number of datasets.\nc = tf.data.Dataset.range(7, 13).batch(2)  # ==> [ [7, 8],\n                                           #       [9, 10],\n                                           #       [11, 12] ]\nds = tf.data.Dataset.zip((a, b, c))\nfor element in ds.as_numpy_iterator():\n  print(element)\n(1, 4, array([7, 8]))\n(2, 5, array([ 9, 10]))\n(3, 6, array([11, 12]))\n\n# The number of elements in the resulting dataset is the same as\n# the size of the smallest dataset in `datasets`.\nd = tf.data.Dataset.range(13, 15)  # ==> [ 13, 14 ]\nds = tf.data.Dataset.zip((a, d))\nlist(ds.as_numpy_iterator())\n[(1, 13), (2, 14)]\n\n \n\n\n Args\n  datasets   A nested structure of datasets.   \n \n\n\n Returns\n  Dataset   A Dataset.    __bool__ View source \n__bool__()\n __iter__ View source \n__iter__()\n Creates an iterator for elements of this dataset. The returned iterator implements the Python Iterator protocol.\n \n\n\n Returns   An tf.data.Iterator for the elements of this dataset.  \n\n \n\n\n Raises\n  RuntimeError   If not inside of tf.function and not executing eagerly.    __len__ View source \n__len__()\n Returns the length of the dataset if it is known and finite. This method requires that you are running in eager mode, and that the length of the dataset is known and non-infinite. When the length may be unknown or infinite, or if you are running in graph mode, use tf.data.Dataset.cardinality instead.\n \n\n\n Returns   An integer representing the length of the dataset.  \n\n \n\n\n Raises\n  RuntimeError   If the dataset length is unknown or infinite, or if eager execution is not enabled.    __nonzero__ View source \n__nonzero__()\n  \n"}, {"name": "tf.compat.v1.data.experimental.sample_from_datasets", "path": "compat/v1/data/experimental/sample_from_datasets", "type": "tf.compat", "text": "tf.compat.v1.data.experimental.sample_from_datasets Samples elements at random from the datasets in datasets. \ntf.compat.v1.data.experimental.sample_from_datasets(\n    datasets, weights=None, seed=None\n)\n\n \n\n\n Args\n  datasets   A list of tf.data.Dataset objects with compatible structure.  \n  weights   (Optional.) A list of len(datasets) floating-point values where weights[i] represents the probability with which an element should be sampled from datasets[i], or a tf.data.Dataset object where each element is such a list. Defaults to a uniform distribution across datasets.  \n  seed   (Optional.) A tf.int64 scalar tf.Tensor, representing the random seed that will be used to create the distribution. See tf.random.set_seed for behavior.   \n \n\n\n Returns   A dataset that interleaves elements from datasets at random, according to weights if provided, otherwise with uniform probability.  \n\n \n\n\n Raises\n  TypeError   If the datasets or weights arguments have the wrong type.  \n  ValueError   If the weights argument is specified and does not match the length of the datasets element.     \n"}, {"name": "tf.compat.v1.data.experimental.service", "path": "compat/v1/data/experimental/service", "type": "tf.compat", "text": "Module: tf.compat.v1.data.experimental.service API for using the tf.data service. This module contains:  tf.data server implementations for running the tf.data service. A distribute dataset transformation that moves a dataset's preprocessing to happen in the tf.data service.  The tf.data service offers a way to improve training speed when the host attached to a training device can't keep up with the data consumption of the model. For example, suppose a host can generate 100 examples/second, but the model can process 200 examples/second. Training speed could be doubled by using the tf.data service to generate 200 examples/second. Before using the tf.data service There are a few things to do before using the tf.data service to speed up training. Understand processing_mode The tf.data service uses a cluster of workers to prepare data for training your model. The processing_mode argument to tf.data.experimental.service.distribute describes how to leverage multiple workers to process the input dataset. Currently, there are two processing modes to choose from: \"distributed_epoch\" and \"parallel_epochs\". \"distributed_epoch\" means that the dataset will be split across all tf.data service workers. The dispatcher produces \"splits\" for the dataset and sends them to workers for further processing. For example, if a dataset begins with a list of filenames, the dispatcher will iterate through the filenames and send the filenames to tf.data workers, which will perform the rest of the dataset transformations on those files. \"distributed_epoch\" is useful when your model needs to see each element of the dataset exactly once, or if it needs to see the data in a generally-sequential order. \"distributed_epoch\" only works for datasets with splittable sources, such as Dataset.from_tensor_slices, Dataset.list_files, or Dataset.range. \"parallel_epochs\" means that the entire input dataset will be processed independently by each of the tf.data service workers. For this reason, it is important to shuffle data (e.g. filenames) non-deterministically, so that each worker will process the elements of the dataset in a different order. \"parallel_epochs\" can be used to distribute datasets that aren't splittable. Measure potential impact Before using the tf.data service, it is useful to first measure the potential performance improvement. To do this, add dataset = dataset.take(1).cache().repeat()\n at the end of your dataset, and see how it affects your model's step time. take(1).cache().repeat() will cache the first element of your dataset and produce it repeatedly. This should make the dataset very fast, so that the model becomes the bottleneck and you can identify the ideal model speed. With enough workers, the tf.data service should be able to achieve similar speed. Running the tf.data service tf.data servers should be brought up alongside your training jobs, and brought down when the jobs are finished. The tf.data service uses one DispatchServer and any number of WorkerServers. See https://github.com/tensorflow/ecosystem/tree/master/data_service for an example of using Google Kubernetes Engine (GKE) to manage the tf.data service. The server implementation in tf_std_data_server.py is not GKE-specific, and can be used to run the tf.data service in other contexts. Fault tolerance By default, the tf.data dispatch server stores its state in-memory, making it a single point of failure during training. To avoid this, pass fault_tolerant_mode=True when creating your DispatchServer. Dispatcher fault tolerance requires work_dir to be configured and accessible from the dispatcher both before and after restart (e.g. a GCS path). With fault tolerant mode enabled, the dispatcher will journal its state to the work directory so that no state is lost when the dispatcher is restarted. WorkerServers may be freely restarted, added, or removed during training. At startup, workers will register with the dispatcher and begin processing all outstanding jobs from the beginning. Using the tf.data service from your training job Once you have a tf.data service cluster running, take note of the dispatcher IP address and port. To connect to the service, you will use a string in the format \"grpc://:\". # Create the dataset however you were before using the tf.data service.\ndataset = your_dataset_factory()\n\nservice = \"grpc://{}:{}\".format(dispatcher_address, dispatcher_port)\n# This will register the dataset with the tf.data service cluster so that\n# tf.data workers can run the dataset to produce elements. The dataset returned\n# from applying `distribute` will fetch elements produced by tf.data workers.\ndataset = dataset.apply(tf.data.experimental.service.distribute(\n    processing_mode=\"parallel_epochs\", service=service))\n Below is a toy example that you can run yourself. \ndispatcher = tf.data.experimental.service.DispatchServer()\ndispatcher_address = dispatcher.target.split(\"://\")[1]\nworker = tf.data.experimental.service.WorkerServer(\n    tf.data.experimental.service.WorkerConfig(\n        dispatcher_address=dispatcher_address))\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.apply(tf.data.experimental.service.distribute(\n    processing_mode=\"parallel_epochs\", service=dispatcher.target))\nprint(list(dataset.as_numpy_iterator()))\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n See the documentation of tf.data.experimental.service.distribute for more details about using the distribute transformation. Classes class DispatcherConfig: Configuration class for tf.data service dispatchers. class WorkerConfig: Configuration class for tf.data service dispatchers. Functions distribute(...): A transformation that moves dataset processing to the tf.data service. from_dataset_id(...): Creates a dataset which reads data from the tf.data service. register_dataset(...): Registers a dataset with the tf.data service.  \n"}, {"name": "tf.compat.v1.data.experimental.SparseTensorStructure", "path": "compat/v1/data/experimental/sparsetensorstructure", "type": "tf.compat", "text": "tf.compat.v1.data.experimental.SparseTensorStructure DEPRECATED FUNCTION \ntf.compat.v1.data.experimental.SparseTensorStructure(\n    dtype, shape\n)\n Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.SparseTensorSpec instead.  \n"}, {"name": "tf.compat.v1.data.experimental.SqlDataset", "path": "compat/v1/data/experimental/sqldataset", "type": "tf.compat", "text": "tf.compat.v1.data.experimental.SqlDataset A Dataset consisting of the results from a SQL query. Inherits From: Dataset, Dataset \ntf.compat.v1.data.experimental.SqlDataset(\n    driver_name, data_source_name, query, output_types\n)\n\n \n\n\n Args\n  driver_name   A 0-D tf.string tensor containing the database type. Currently, the only supported value is 'sqlite'.  \n  data_source_name   A 0-D tf.string tensor containing a connection string to connect to the database.  \n  query   A 0-D tf.string tensor containing the SQL query to execute.  \n  output_types   A tuple of tf.DType objects representing the types of the columns returned by query.   \n \n\n\n Attributes\n  element_spec   The type specification of an element of this dataset. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset.element_spec\nTensorSpec(shape=(), dtype=tf.int32, name=None)\n\n \n  output_classes   Returns the class of each component of an element of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.compat.v1.data.get_output_classes(dataset). \n \n  output_shapes   Returns the shape of each component of an element of this dataset. (deprecated)Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.compat.v1.data.get_output_shapes(dataset). \n \n  output_types   Returns the type of each component of an element of this dataset. (deprecated)Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.compat.v1.data.get_output_types(dataset). \n   Methods apply View source \napply(\n    transformation_func\n)\n Applies a transformation function to this dataset. apply enables chaining of custom Dataset transformations, which are represented as functions that take one Dataset argument and return a transformed Dataset. \ndataset = tf.data.Dataset.range(100)\ndef dataset_fn(ds):\n  return ds.filter(lambda x: x < 5)\ndataset = dataset.apply(dataset_fn)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2, 3, 4]\n\n \n\n\n Args\n  transformation_func   A function that takes one Dataset argument and returns a Dataset.   \n \n\n\n Returns\n  Dataset   The Dataset returned by applying transformation_func to this dataset.    as_numpy_iterator View source \nas_numpy_iterator()\n Returns an iterator which converts all elements of the dataset to numpy. Use as_numpy_iterator to inspect the content of your dataset. To see element shapes and types, print dataset elements directly instead of using as_numpy_iterator. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nfor element in dataset:\n  print(element)\ntf.Tensor(1, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(3, shape=(), dtype=int32)\n This method requires that you are running in eager mode and the dataset's element_spec contains only TensorSpec components. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nfor element in dataset.as_numpy_iterator():\n  print(element)\n1\n2\n3\n \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nprint(list(dataset.as_numpy_iterator()))\n[1, 2, 3]\n as_numpy_iterator() will preserve the nested structure of dataset elements. \ndataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),\n                                              'b': [5, 6]})\nlist(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5},\n                                      {'a': (2, 4), 'b': 6}]\nTrue\n\n \n\n\n Returns   An iterable over the elements of the dataset, with their tensors converted to numpy arrays.  \n\n \n\n\n Raises\n  TypeError   if an element contains a non-Tensor value.  \n  RuntimeError   if eager execution is not enabled.    batch View source \nbatch(\n    batch_size, drop_remainder=False\n)\n Combines consecutive elements of this dataset into batches. \ndataset = tf.data.Dataset.range(8)\ndataset = dataset.batch(3)\nlist(dataset.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5]), array([6, 7])]\n \ndataset = tf.data.Dataset.range(8)\ndataset = dataset.batch(3, drop_remainder=True)\nlist(dataset.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5])]\n The components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced.\n \n\n\n Args\n  batch_size   A tf.int64 scalar tf.Tensor, representing the number of consecutive elements of this dataset to combine in a single batch.  \n  drop_remainder   (Optional.) A tf.bool scalar tf.Tensor, representing whether the last batch should be dropped in the case it has fewer than batch_size elements; the default behavior is not to drop the smaller batch.   \n \n\n\n Returns\n  Dataset   A Dataset.    cache View source \ncache(\n    filename=''\n)\n Caches the elements in this dataset. The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data. \nNote: For the cache to be finalized, the input dataset must be iterated through in its entirety. Otherwise, subsequent iterations will not use cached data.\n \ndataset = tf.data.Dataset.range(5)\ndataset = dataset.map(lambda x: x**2)\ndataset = dataset.cache()\n# The first time reading through the data will generate the data using\n# `range` and `map`.\nlist(dataset.as_numpy_iterator())\n[0, 1, 4, 9, 16]\n# Subsequent iterations read from the cache.\nlist(dataset.as_numpy_iterator())\n[0, 1, 4, 9, 16]\n When caching to a file, the cached data will persist across runs. Even the first iteration through the data will read from the cache file. Changing the input pipeline before the call to .cache() will have no effect until the cache file is removed or the filename is changed. \ndataset = tf.data.Dataset.range(5)\ndataset = dataset.cache(\"/path/to/file\")  # doctest: +SKIP\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[0, 1, 2, 3, 4]\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.cache(\"/path/to/file\")  # Same file! # doctest: +SKIP\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[0, 1, 2, 3, 4]\n \nNote: cache will produce exactly the same elements during each iteration through the dataset. If you wish to randomize the iteration order, make sure to call shuffle after calling cache.\n\n \n\n\n Args\n  filename   A tf.string scalar tf.Tensor, representing the name of a directory on the filesystem to use for caching elements in this Dataset. If a filename is not provided, the dataset will be cached in memory.   \n \n\n\n Returns\n  Dataset   A Dataset.    cardinality View source \ncardinality()\n Returns the cardinality of the dataset, if known. cardinality may return tf.data.INFINITE_CARDINALITY if the dataset contains an infinite number of elements or tf.data.UNKNOWN_CARDINALITY if the analysis fails to determine the number of elements in the dataset (e.g. when the dataset source is a file). \ndataset = tf.data.Dataset.range(42)\nprint(dataset.cardinality().numpy())\n42\ndataset = dataset.repeat()\ncardinality = dataset.cardinality()\nprint((cardinality == tf.data.INFINITE_CARDINALITY).numpy())\nTrue\ndataset = dataset.filter(lambda x: True)\ncardinality = dataset.cardinality()\nprint((cardinality == tf.data.UNKNOWN_CARDINALITY).numpy())\nTrue\n\n \n\n\n Returns   A scalar tf.int64 Tensor representing the cardinality of the dataset. If the cardinality is infinite or unknown, cardinality returns the named constants tf.data.INFINITE_CARDINALITY and tf.data.UNKNOWN_CARDINALITY respectively.  \n concatenate View source \nconcatenate(\n    dataset\n)\n Creates a Dataset by concatenating the given dataset with this dataset. \na = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\nb = tf.data.Dataset.range(4, 8)  # ==> [ 4, 5, 6, 7 ]\nds = a.concatenate(b)\nlist(ds.as_numpy_iterator())\n[1, 2, 3, 4, 5, 6, 7]\n# The input dataset and dataset to be concatenated should have the same\n# nested structures and output types.\nc = tf.data.Dataset.zip((a, b))\na.concatenate(c)\nTraceback (most recent call last):\nTypeError: Two datasets to concatenate have different types\n<dtype: 'int64'> and (tf.int64, tf.int64)\nd = tf.data.Dataset.from_tensor_slices([\"a\", \"b\", \"c\"])\na.concatenate(d)\nTraceback (most recent call last):\nTypeError: Two datasets to concatenate have different types\n<dtype: 'int64'> and <dtype: 'string'>\n\n \n\n\n Args\n  dataset   Dataset to be concatenated.   \n \n\n\n Returns\n  Dataset   A Dataset.    enumerate View source \nenumerate(\n    start=0\n)\n Enumerates the elements of this dataset. It is similar to python's enumerate. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.enumerate(start=5)\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(5, 1)\n(6, 2)\n(7, 3)\n \n# The nested structure of the input dataset determines the structure of\n# elements in the resulting dataset.\ndataset = tf.data.Dataset.from_tensor_slices([(7, 8), (9, 10)])\ndataset = dataset.enumerate()\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(0, array([7, 8], dtype=int32))\n(1, array([ 9, 10], dtype=int32))\n\n \n\n\n Args\n  start   A tf.int64 scalar tf.Tensor, representing the start value for enumeration.   \n \n\n\n Returns\n  Dataset   A Dataset.    filter View source \nfilter(\n    predicate\n)\n Filters this dataset according to predicate. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.filter(lambda x: x < 3)\nlist(dataset.as_numpy_iterator())\n[1, 2]\n# `tf.math.equal(x, y)` is required for equality comparison\ndef filter_fn(x):\n  return tf.math.equal(x, 1)\ndataset = dataset.filter(filter_fn)\nlist(dataset.as_numpy_iterator())\n[1]\n\n \n\n\n Args\n  predicate   A function mapping a dataset element to a boolean.   \n \n\n\n Returns\n  Dataset   The Dataset containing the elements of this dataset for which predicate is True.    filter_with_legacy_function View source \nfilter_with_legacy_function(\n    predicate\n)\n Filters this dataset according to predicate. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use `tf.data.Dataset.filter()\nNote: This is an escape hatch for existing uses of filter that do not work with V2 functions. New uses are strongly discouraged and existing uses should migrate to filter as this method will be removed in V2.\n\n \n\n\n Args\n  predicate   A function mapping a nested structure of tensors (having shapes and types defined by self.output_shapes and self.output_types) to a scalar tf.bool tensor.   \n \n\n\n Returns\n  Dataset   The Dataset containing the elements of this dataset for which predicate is True.    flat_map View source \nflat_map(\n    map_func\n)\n Maps map_func across this dataset and flattens the result. Use flat_map if you want to make sure that the order of your dataset stays the same. For example, to flatten a dataset of batches into a dataset of their elements: \ndataset = tf.data.Dataset.from_tensor_slices(\n               [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ndataset = dataset.flat_map(lambda x: Dataset.from_tensor_slices(x))\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n tf.data.Dataset.interleave() is a generalization of flat_map, since flat_map produces the same output as tf.data.Dataset.interleave(cycle_length=1)\n \n\n\n Args\n  map_func   A function mapping a dataset element to a dataset.   \n \n\n\n Returns\n  Dataset   A Dataset.    from_generator View source \n@staticmethod\nfrom_generator(\n    generator, output_types=None, output_shapes=None, args=None,\n    output_signature=None\n)\n Creates a Dataset whose elements are generated by generator. (deprecated arguments) Warning: SOME ARGUMENTS ARE DEPRECATED: (output_shapes, output_types). They will be removed in a future version. Instructions for updating: Use output_signature instead The generator argument must be a callable object that returns an object that supports the iter() protocol (e.g. a generator function). The elements generated by generator must be compatible with either the given output_signature argument or with the given output_types and (optionally) output_shapes arguments, whichiver was specified. The recommended way to call from_generator is to use the output_signature argument. In this case the output will be assumed to consist of objects with the classes, shapes and types defined by tf.TypeSpec objects from output_signature argument: \ndef gen():\n  ragged_tensor = tf.ragged.constant([[1, 2], [3]])\n  yield 42, ragged_tensor\n\ndataset = tf.data.Dataset.from_generator(\n     gen,\n     output_signature=(\n         tf.TensorSpec(shape=(), dtype=tf.int32),\n         tf.RaggedTensorSpec(shape=(2, None), dtype=tf.int32)))\n\nlist(dataset.take(1))\n[(<tf.Tensor: shape=(), dtype=int32, numpy=42>,\n<tf.RaggedTensor [[1, 2], [3]]>)]\n There is also a deprecated way to call from_generator by either with output_types argument alone or together with output_shapes argument. In this case the output of the function will be assumed to consist of tf.Tensor objects with with the types defined by output_types and with the shapes which are either unknown or defined by output_shapes. \nNote: The current implementation of Dataset.from_generator() uses tf.numpy_function and inherits the same constraints. In particular, it requires the dataset and iterator related operations to be placed on a device in the same process as the Python program that called Dataset.from_generator(). The body of generator will not be serialized in a GraphDef, and you should not use this method if you need to serialize your model and restore it in a different environment.\n\n\nNote: If generator depends on mutable global variables or other external state, be aware that the runtime may invoke generator multiple times (in order to support repeating the Dataset) and at any time between the call to Dataset.from_generator() and the production of the first element from the generator. Mutating global variables or external state can cause undefined behavior, and we recommend that you explicitly cache any external state in generator before calling Dataset.from_generator().\n\n \n\n\n Args\n  generator   A callable object that returns an object that supports the iter() protocol. If args is not specified, generator must take no arguments; otherwise it must take as many arguments as there are values in args.  \n  output_types   (Optional.) A nested structure of tf.DType objects corresponding to each component of an element yielded by generator.  \n  output_shapes   (Optional.) A nested structure of tf.TensorShape objects corresponding to each component of an element yielded by generator.  \n  args   (Optional.) A tuple of tf.Tensor objects that will be evaluated and passed to generator as NumPy-array arguments.  \n  output_signature   (Optional.) A nested structure of tf.TypeSpec objects corresponding to each component of an element yielded by generator.   \n \n\n\n Returns\n  Dataset   A Dataset.    from_sparse_tensor_slices View source \n@staticmethod\nfrom_sparse_tensor_slices(\n    sparse_tensor\n)\n Splits each rank-N tf.sparse.SparseTensor in this dataset row-wise. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.data.Dataset.from_tensor_slices().\n \n\n\n Args\n  sparse_tensor   A tf.sparse.SparseTensor.   \n \n\n\n Returns\n  Dataset   A Dataset of rank-(N-1) sparse tensors.    from_tensor_slices View source \n@staticmethod\nfrom_tensor_slices(\n    tensors\n)\n Creates a Dataset whose elements are slices of the given tensors. The given tensors are sliced along their first dimension. This operation preserves the structure of the input tensors, removing the first dimension of each tensor and using it as the dataset dimension. All input tensors must have the same size in their first dimensions. \n# Slicing a 1D tensor produces scalar tensor elements.\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nlist(dataset.as_numpy_iterator())\n[1, 2, 3]\n \n# Slicing a 2D tensor produces 1D tensor elements.\ndataset = tf.data.Dataset.from_tensor_slices([[1, 2], [3, 4]])\nlist(dataset.as_numpy_iterator())\n[array([1, 2], dtype=int32), array([3, 4], dtype=int32)]\n \n# Slicing a tuple of 1D tensors produces tuple elements containing\n# scalar tensors.\ndataset = tf.data.Dataset.from_tensor_slices(([1, 2], [3, 4], [5, 6]))\nlist(dataset.as_numpy_iterator())\n[(1, 3, 5), (2, 4, 6)]\n \n# Dictionary structure is also preserved.\ndataset = tf.data.Dataset.from_tensor_slices({\"a\": [1, 2], \"b\": [3, 4]})\nlist(dataset.as_numpy_iterator()) == [{'a': 1, 'b': 3},\n                                      {'a': 2, 'b': 4}]\nTrue\n \n# Two tensors can be combined into one Dataset object.\nfeatures = tf.constant([[1, 3], [2, 1], [3, 3]]) # ==> 3x2 tensor\nlabels = tf.constant(['A', 'B', 'A']) # ==> 3x1 tensor\ndataset = Dataset.from_tensor_slices((features, labels))\n# Both the features and the labels tensors can be converted\n# to a Dataset object separately and combined after.\nfeatures_dataset = Dataset.from_tensor_slices(features)\nlabels_dataset = Dataset.from_tensor_slices(labels)\ndataset = Dataset.zip((features_dataset, labels_dataset))\n# A batched feature and label set can be converted to a Dataset\n# in similar fashion.\nbatched_features = tf.constant([[[1, 3], [2, 3]],\n                                [[2, 1], [1, 2]],\n                                [[3, 3], [3, 2]]], shape=(3, 2, 2))\nbatched_labels = tf.constant([['A', 'A'],\n                              ['B', 'B'],\n                              ['A', 'B']], shape=(3, 2, 1))\ndataset = Dataset.from_tensor_slices((batched_features, batched_labels))\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(array([[1, 3],\n       [2, 3]], dtype=int32), array([[b'A'],\n       [b'A']], dtype=object))\n(array([[2, 1],\n       [1, 2]], dtype=int32), array([[b'B'],\n       [b'B']], dtype=object))\n(array([[3, 3],\n       [3, 2]], dtype=int32), array([[b'A'],\n       [b'B']], dtype=object))\n Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.\n \n\n\n Args\n  tensors   A dataset element, with each component having the same size in the first dimension.   \n \n\n\n Returns\n  Dataset   A Dataset.    from_tensors View source \n@staticmethod\nfrom_tensors(\n    tensors\n)\n Creates a Dataset with a single element, comprising the given tensors. from_tensors produces a dataset containing only a single element. To slice the input tensor into multiple elements, use from_tensor_slices instead. \ndataset = tf.data.Dataset.from_tensors([1, 2, 3])\nlist(dataset.as_numpy_iterator())\n[array([1, 2, 3], dtype=int32)]\ndataset = tf.data.Dataset.from_tensors(([1, 2, 3], 'A'))\nlist(dataset.as_numpy_iterator())\n[(array([1, 2, 3], dtype=int32), b'A')]\n \n# You can use `from_tensors` to produce a dataset which repeats\n# the same example many times.\nexample = tf.constant([1,2,3])\ndataset = tf.data.Dataset.from_tensors(example).repeat(2)\nlist(dataset.as_numpy_iterator())\n[array([1, 2, 3], dtype=int32), array([1, 2, 3], dtype=int32)]\n Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.\n \n\n\n Args\n  tensors   A dataset element.   \n \n\n\n Returns\n  Dataset   A Dataset.    interleave View source \ninterleave(\n    map_func, cycle_length=None, block_length=None, num_parallel_calls=None,\n    deterministic=None\n)\n Maps map_func across this dataset, and interleaves the results. For example, you can use Dataset.interleave() to process many input files concurrently: \n# Preprocess 4 files concurrently, and interleave blocks of 16 records\n# from each file.\nfilenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n             \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\ndataset = tf.data.Dataset.from_tensor_slices(filenames)\ndef parse_fn(filename):\n  return tf.data.Dataset.range(10)\ndataset = dataset.interleave(lambda x:\n    tf.data.TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n    cycle_length=4, block_length=16)\n The cycle_length and block_length arguments control the order in which elements are produced. cycle_length controls the number of input elements that are processed concurrently. If you set cycle_length to 1, this transformation will handle one input element at a time, and will produce identical results to tf.data.Dataset.flat_map. In general, this transformation will apply map_func to cycle_length input elements, open iterators on the returned Dataset objects, and cycle through them producing block_length consecutive elements from each iterator, and consuming the next input element each time it reaches the end of an iterator. For example: \ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n# NOTE: New lines indicate \"block\" boundaries.\ndataset = dataset.interleave(\n    lambda x: Dataset.from_tensors(x).repeat(6),\n    cycle_length=2, block_length=4)\nlist(dataset.as_numpy_iterator())\n[1, 1, 1, 1,\n 2, 2, 2, 2,\n 1, 1,\n 2, 2,\n 3, 3, 3, 3,\n 4, 4, 4, 4,\n 3, 3,\n 4, 4,\n 5, 5, 5, 5,\n 5, 5]\n \nNote: The order of elements yielded by this transformation is deterministic, as long as map_func is a pure function and deterministic=True. If map_func contains any stateful operations, the order in which that state is accessed is undefined.\n Performance can often be improved by setting num_parallel_calls so that interleave will use multiple threads to fetch elements. If determinism isn't required, it can also improve performance to set deterministic=False. \nfilenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n             \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\ndataset = tf.data.Dataset.from_tensor_slices(filenames)\ndataset = dataset.interleave(lambda x: tf.data.TFRecordDataset(x),\n    cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE,\n    deterministic=False)\n\n \n\n\n Args\n  map_func   A function mapping a dataset element to a dataset.  \n  cycle_length   (Optional.) The number of input elements that will be processed concurrently. If not set, the tf.data runtime decides what it should be based on available CPU. If num_parallel_calls is set to tf.data.AUTOTUNE, the cycle_length argument identifies the maximum degree of parallelism.  \n  block_length   (Optional.) The number of consecutive elements to produce from each input element before cycling to another input element. If not set, defaults to 1.  \n  num_parallel_calls   (Optional.) If specified, the implementation creates a threadpool, which is used to fetch inputs from cycle elements asynchronously and in parallel. The default behavior is to fetch inputs from cycle elements synchronously with no parallelism. If the value tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available CPU.  \n  deterministic   (Optional.) A boolean controlling whether determinism should be traded for performance by allowing elements to be produced out of order. If deterministic is None, the tf.data.Options.experimental_deterministic dataset option (True by default) is used to decide whether to produce elements deterministically.   \n \n\n\n Returns\n  Dataset   A Dataset.    list_files View source \n@staticmethod\nlist_files(\n    file_pattern, shuffle=None, seed=None\n)\n A dataset of all files matching one or more glob patterns. The file_pattern argument should be a small number of glob patterns. If your filenames have already been globbed, use Dataset.from_tensor_slices(filenames) instead, as re-globbing every filename with list_files may result in poor performance with remote storage systems. \nNote: The default behavior of this method is to return filenames in a non-deterministic random shuffled order. Pass a seed or shuffle=False to get results in a deterministic order.\n Example: If we had the following files on our filesystem:  /path/to/dir/a.txt /path/to/dir/b.py /path/to/dir/c.py  If we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:  /path/to/dir/b.py /path/to/dir/c.py \n \n\n\n Args\n  file_pattern   A string, a list of strings, or a tf.Tensor of string type (scalar or vector), representing the filename glob (i.e. shell wildcard) pattern(s) that will be matched.  \n  shuffle   (Optional.) If True, the file names will be shuffled randomly. Defaults to True.  \n  seed   (Optional.) A tf.int64 scalar tf.Tensor, representing the random seed that will be used to create the distribution. See tf.random.set_seed for behavior.   \n \n\n\n Returns\n  Dataset   A Dataset of strings corresponding to file names.    make_initializable_iterator View source \nmake_initializable_iterator(\n    shared_name=None\n)\n Creates an iterator for elements of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through tf.compat.v1. In all other situations -- namely, eager mode and inside tf.function -- you can consume dataset elements using for elem in dataset: ... or by explicitly creating iterator via iterator = iter(dataset) and fetching its elements via values = next(iterator). Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use tf.compat.v1.data.make_initializable_iterator(dataset) to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\nNote: The returned iterator will be in an uninitialized state, and you must run the iterator.initializer operation before using it:\n\n# Building graph ...\ndataset = ...\niterator = dataset.make_initializable_iterator()\nnext_value = iterator.get_next()  # This is a Tensor.\n\n# ... from within a session ...\nsess.run(iterator.initializer)\ntry:\n  while True:\n    value = sess.run(next_value)\n    ...\nexcept tf.errors.OutOfRangeError:\n    pass\n\n \n\n\n Args\n  shared_name   (Optional.) If non-empty, the returned iterator will be shared under the given name across multiple sessions that share the same devices (e.g. when using a remote server).   \n \n\n\n Returns   A tf.data.Iterator for elements of this dataset.  \n\n \n\n\n Raises\n  RuntimeError   If eager execution is enabled.    make_one_shot_iterator View source \nmake_one_shot_iterator()\n Creates an iterator for elements of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through tf.compat.v1. In all other situations -- namely, eager mode and inside tf.function -- you can consume dataset elements using for elem in dataset: ... or by explicitly creating iterator via iterator = iter(dataset) and fetching its elements via values = next(iterator). Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use tf.compat.v1.data.make_one_shot_iterator(dataset) to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\nNote: The returned iterator will be initialized automatically. A \"one-shot\" iterator does not currently support re-initialization. For that see make_initializable_iterator.\n Example: # Building graph ...\ndataset = ...\nnext_value = dataset.make_one_shot_iterator().get_next()\n\n# ... from within a session ...\ntry:\n  while True:\n    value = sess.run(next_value)\n    ...\nexcept tf.errors.OutOfRangeError:\n    pass\n\n \n\n\n Returns   An tf.data.Iterator for elements of this dataset.  \n map View source \nmap(\n    map_func, num_parallel_calls=None, deterministic=None\n)\n Maps map_func across the elements of this dataset. This transformation applies map_func to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. map_func can be used to change both the values and the structure of a dataset's elements. For example, adding 1 to each element, or projecting a subset of element components. \ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\ndataset = dataset.map(lambda x: x + 1)\nlist(dataset.as_numpy_iterator())\n[2, 3, 4, 5, 6]\n The input signature of map_func is determined by the structure of each element in this dataset. \ndataset = Dataset.range(5)\n# `map_func` takes a single argument of type `tf.Tensor` with the same\n# shape and dtype.\nresult = dataset.map(lambda x: x + 1)\n \n# Each element is a tuple containing two `tf.Tensor` objects.\nelements = [(1, \"foo\"), (2, \"bar\"), (3, \"baz\")]\ndataset = tf.data.Dataset.from_generator(\n    lambda: elements, (tf.int32, tf.string))\n# `map_func` takes two arguments of type `tf.Tensor`. This function\n# projects out just the first component.\nresult = dataset.map(lambda x_int, y_str: x_int)\nlist(result.as_numpy_iterator())\n[1, 2, 3]\n \n# Each element is a dictionary mapping strings to `tf.Tensor` objects.\nelements =  ([{\"a\": 1, \"b\": \"foo\"},\n              {\"a\": 2, \"b\": \"bar\"},\n              {\"a\": 3, \"b\": \"baz\"}])\ndataset = tf.data.Dataset.from_generator(\n    lambda: elements, {\"a\": tf.int32, \"b\": tf.string})\n# `map_func` takes a single argument of type `dict` with the same keys\n# as the elements.\nresult = dataset.map(lambda d: str(d[\"a\"]) + d[\"b\"])\n The value or values returned by map_func determine the structure of each element in the returned dataset. \ndataset = tf.data.Dataset.range(3)\n# `map_func` returns two `tf.Tensor` objects.\ndef g(x):\n  return tf.constant(37.0), tf.constant([\"Foo\", \"Bar\", \"Baz\"])\nresult = dataset.map(g)\nresult.element_spec\n(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(3,), dtype=tf.string, name=None))\n# Python primitives, lists, and NumPy arrays are implicitly converted to\n# `tf.Tensor`.\ndef h(x):\n  return 37.0, [\"Foo\", \"Bar\"], np.array([1.0, 2.0], dtype=np.float64)\nresult = dataset.map(h)\nresult.element_spec\n(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(2,), dtype=tf.string, name=None), TensorSpec(shape=(2,), dtype=tf.float64, name=None))\n# `map_func` can return nested structures.\ndef i(x):\n  return (37.0, [42, 16]), \"foo\"\nresult = dataset.map(i)\nresult.element_spec\n((TensorSpec(shape=(), dtype=tf.float32, name=None),\n  TensorSpec(shape=(2,), dtype=tf.int32, name=None)),\n TensorSpec(shape=(), dtype=tf.string, name=None))\n map_func can accept as arguments and return any type of dataset element. Note that irrespective of the context in which map_func is defined (eager vs. graph), tf.data traces the function and executes it as a graph. To use Python code inside of the function you have a few options: 1) Rely on AutoGraph to convert Python code into an equivalent graph computation. The downside of this approach is that AutoGraph can convert some but not all Python code. 2) Use tf.py_function, which allows you to write arbitrary Python code but will generally result in worse performance than 1). For example: \nd = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n# transform a string tensor to upper case string using a Python function\ndef upper_case_fn(t: tf.Tensor):\n  return t.numpy().decode('utf-8').upper()\nd = d.map(lambda x: tf.py_function(func=upper_case_fn,\n          inp=[x], Tout=tf.string))\nlist(d.as_numpy_iterator())\n[b'HELLO', b'WORLD']\n 3) Use tf.numpy_function, which also allows you to write arbitrary Python code. Note that tf.py_function accepts tf.Tensor whereas tf.numpy_function accepts numpy arrays and returns only numpy arrays. For example: \nd = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\ndef upper_case_fn(t: np.ndarray):\n  return t.decode('utf-8').upper()\nd = d.map(lambda x: tf.numpy_function(func=upper_case_fn,\n          inp=[x], Tout=tf.string))\nlist(d.as_numpy_iterator())\n[b'HELLO', b'WORLD']\n Note that the use of tf.numpy_function and tf.py_function in general precludes the possibility of executing user-defined transformations in parallel (because of Python GIL). Performance can often be improved by setting num_parallel_calls so that map will use multiple threads to process elements. If deterministic order isn't required, it can also improve performance to set deterministic=False. \ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\ndataset = dataset.map(lambda x: x + 1,\n    num_parallel_calls=tf.data.AUTOTUNE,\n    deterministic=False)\n\n \n\n\n Args\n  map_func   A function mapping a dataset element to another dataset element.  \n  num_parallel_calls   (Optional.) A tf.int32 scalar tf.Tensor, representing the number elements to process asynchronously in parallel. If not specified, elements will be processed sequentially. If the value tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available CPU.  \n  deterministic   (Optional.) A boolean controlling whether determinism should be traded for performance by allowing elements to be produced out of order. If deterministic is None, the tf.data.Options.experimental_deterministic dataset option (True by default) is used to decide whether to produce elements deterministically.   \n \n\n\n Returns\n  Dataset   A Dataset.    map_with_legacy_function View source \nmap_with_legacy_function(\n    map_func, num_parallel_calls=None, deterministic=None\n)\n Maps map_func across the elements of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use `tf.data.Dataset.map()\nNote: This is an escape hatch for existing uses of map that do not work with V2 functions. New uses are strongly discouraged and existing uses should migrate to map as this method will be removed in V2.\n\n \n\n\n Args\n  map_func   A function mapping a nested structure of tensors (having shapes and types defined by self.output_shapes and self.output_types) to another nested structure of tensors.  \n  num_parallel_calls   (Optional.) A tf.int32 scalar tf.Tensor, representing the number elements to process asynchronously in parallel. If not specified, elements will be processed sequentially. If the value tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available CPU.  \n  deterministic   (Optional.) A boolean controlling whether determinism should be traded for performance by allowing elements to be produced out of order. If deterministic is None, the tf.data.Options.experimental_deterministic dataset option (True by default) is used to decide whether to produce elements deterministically.   \n \n\n\n Returns\n  Dataset   A Dataset.    options View source \noptions()\n Returns the options for this dataset and its inputs.\n \n\n\n Returns   A tf.data.Options object representing the dataset options.  \n padded_batch View source \npadded_batch(\n    batch_size, padded_shapes=None, padding_values=None, drop_remainder=False\n)\n Combines consecutive elements of this dataset into padded batches. This transformation combines multiple consecutive elements of the input dataset into a single element. Like tf.data.Dataset.batch, the components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced. Unlike tf.data.Dataset.batch, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in padded_shapes. The padded_shapes argument determines the resulting shape for each dimension of each component in an output element:  If the dimension is a constant, the component will be padded out to that length in that dimension. If the dimension is unknown, the component will be padded out to the maximum length of all elements in that dimension.  \nA = (tf.data.Dataset\n     .range(1, 5, output_type=tf.int32)\n     .map(lambda x: tf.fill([x], x)))\n# Pad to the smallest per-batch size that fits all elements.\nB = A.padded_batch(2)\nfor element in B.as_numpy_iterator():\n  print(element)\n[[1 0]\n [2 2]]\n[[3 3 3 0]\n [4 4 4 4]]\n# Pad to a fixed size.\nC = A.padded_batch(2, padded_shapes=5)\nfor element in C.as_numpy_iterator():\n  print(element)\n[[1 0 0 0 0]\n [2 2 0 0 0]]\n[[3 3 3 0 0]\n [4 4 4 4 0]]\n# Pad with a custom value.\nD = A.padded_batch(2, padded_shapes=5, padding_values=-1)\nfor element in D.as_numpy_iterator():\n  print(element)\n[[ 1 -1 -1 -1 -1]\n [ 2  2 -1 -1 -1]]\n[[ 3  3  3 -1 -1]\n [ 4  4  4  4 -1]]\n# Components of nested elements can be padded independently.\nelements = [([1, 2, 3], [10]),\n            ([4, 5], [11, 12])]\ndataset = tf.data.Dataset.from_generator(\n    lambda: iter(elements), (tf.int32, tf.int32))\n# Pad the first component of the tuple to length 4, and the second\n# component to the smallest size that fits.\ndataset = dataset.padded_batch(2,\n    padded_shapes=([4], [None]),\n    padding_values=(-1, 100))\nlist(dataset.as_numpy_iterator())\n[(array([[ 1,  2,  3, -1], [ 4,  5, -1, -1]], dtype=int32),\n  array([[ 10, 100], [ 11,  12]], dtype=int32))]\n# Pad with a single value and multiple components.\nE = tf.data.Dataset.zip((A, A)).padded_batch(2, padding_values=-1)\nfor element in E.as_numpy_iterator():\n  print(element)\n(array([[ 1, -1],\n       [ 2,  2]], dtype=int32), array([[ 1, -1],\n       [ 2,  2]], dtype=int32))\n(array([[ 3,  3,  3, -1],\n       [ 4,  4,  4,  4]], dtype=int32), array([[ 3,  3,  3, -1],\n       [ 4,  4,  4,  4]], dtype=int32))\n See also tf.data.experimental.dense_to_sparse_batch, which combines elements that may have different shapes into a tf.sparse.SparseTensor.\n \n\n\n Args\n  batch_size   A tf.int64 scalar tf.Tensor, representing the number of consecutive elements of this dataset to combine in a single batch.  \n  padded_shapes   (Optional.) A nested structure of tf.TensorShape or tf.int64 vector tensor-like objects representing the shape to which the respective component of each input element should be padded prior to batching. Any unknown dimensions will be padded to the maximum size of that dimension in each batch. If unset, all dimensions of all components are padded to the maximum size in the batch. padded_shapes must be set if any component has an unknown rank.  \n  padding_values   (Optional.) A nested structure of scalar-shaped tf.Tensor, representing the padding values to use for the respective components. None represents that the nested structure should be padded with default values. Defaults are 0 for numeric types and the empty string for string types. The padding_values should have the same structure as the input dataset. If padding_values is a single element and the input dataset has multiple components, then the same padding_values will be used to pad every component of the dataset. If padding_values is a scalar, then its value will be broadcasted to match the shape of each component.  \n  drop_remainder   (Optional.) A tf.bool scalar tf.Tensor, representing whether the last batch should be dropped in the case it has fewer than batch_size elements; the default behavior is not to drop the smaller batch.   \n \n\n\n Returns\n  Dataset   A Dataset.   \n \n\n\n Raises\n  ValueError   If a component has an unknown rank, and the padded_shapes argument is not set.    prefetch View source \nprefetch(\n    buffer_size\n)\n Creates a Dataset that prefetches elements from this dataset. Most dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements. \nNote: Like other Dataset methods, prefetch operates on the elements of the input dataset. It has no concept of examples vs. batches. examples.prefetch(2) will prefetch two elements (2 examples), while examples.batch(20).prefetch(2) will prefetch 2 elements (2 batches, of 20 examples each).\n \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.prefetch(2)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2]\n\n \n\n\n Args\n  buffer_size   A tf.int64 scalar tf.Tensor, representing the maximum number of elements that will be buffered when prefetching.   \n \n\n\n Returns\n  Dataset   A Dataset.    range View source \n@staticmethod\nrange(\n    *args, **kwargs\n)\n Creates a Dataset of a step-separated range of values. \nlist(Dataset.range(5).as_numpy_iterator())\n[0, 1, 2, 3, 4]\nlist(Dataset.range(2, 5).as_numpy_iterator())\n[2, 3, 4]\nlist(Dataset.range(1, 5, 2).as_numpy_iterator())\n[1, 3]\nlist(Dataset.range(1, 5, -2).as_numpy_iterator())\n[]\nlist(Dataset.range(5, 1).as_numpy_iterator())\n[]\nlist(Dataset.range(5, 1, -2).as_numpy_iterator())\n[5, 3]\nlist(Dataset.range(2, 5, output_type=tf.int32).as_numpy_iterator())\n[2, 3, 4]\nlist(Dataset.range(1, 5, 2, output_type=tf.float32).as_numpy_iterator())\n[1.0, 3.0]\n\n \n\n\n Args\n  *args   follows the same semantics as python's xrange. len(args) == 1 -> start = 0, stop = args[0], step = 1. len(args) == 2 -> start = args[0], stop = args[1], step = 1. len(args) == 3 -> start = args[0], stop = args[1], step = args[2].  \n  **kwargs    output_type: Its expected dtype. (Optional, default: tf.int64). \n\n  \n \n\n\n Returns\n  Dataset   A RangeDataset.   \n \n\n\n Raises\n  ValueError   if len(args) == 0.    reduce View source \nreduce(\n    initial_state, reduce_func\n)\n Reduces the input dataset to a single element. The transformation calls reduce_func successively on every element of the input dataset until the dataset is exhausted, aggregating information in its internal state. The initial_state argument is used for the initial state and the final state is returned as the result. \ntf.data.Dataset.range(5).reduce(np.int64(0), lambda x, _: x + 1).numpy()\n5\ntf.data.Dataset.range(5).reduce(np.int64(0), lambda x, y: x + y).numpy()\n10\n\n \n\n\n Args\n  initial_state   An element representing the initial state of the transformation.  \n  reduce_func   A function that maps (old_state, input_element) to new_state. It must take two arguments and return a new element The structure of new_state must match the structure of initial_state.   \n \n\n\n Returns   A dataset element corresponding to the final state of the transformation.  \n repeat View source \nrepeat(\n    count=None\n)\n Repeats this dataset so each original value is seen count times. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.repeat(3)\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 1, 2, 3, 1, 2, 3]\n \nNote: If this dataset is a function of global state (e.g. a random number generator), then different repetitions may produce different elements.\n\n \n\n\n Args\n  count   (Optional.) A tf.int64 scalar tf.Tensor, representing the number of times the dataset should be repeated. The default behavior (if count is None or -1) is for the dataset be repeated indefinitely.   \n \n\n\n Returns\n  Dataset   A Dataset.    shard View source \nshard(\n    num_shards, index\n)\n Creates a Dataset that includes only 1/num_shards of this dataset. shard is deterministic. The Dataset produced by A.shard(n, i) will contain all elements of A whose index mod n = i. \nA = tf.data.Dataset.range(10)\nB = A.shard(num_shards=3, index=0)\nlist(B.as_numpy_iterator())\n[0, 3, 6, 9]\nC = A.shard(num_shards=3, index=1)\nlist(C.as_numpy_iterator())\n[1, 4, 7]\nD = A.shard(num_shards=3, index=2)\nlist(D.as_numpy_iterator())\n[2, 5, 8]\n This dataset operator is very useful when running distributed training, as it allows each worker to read a unique subset. When reading a single input file, you can shard elements as follows: d = tf.data.TFRecordDataset(input_file)\nd = d.shard(num_workers, worker_index)\nd = d.repeat(num_epochs)\nd = d.shuffle(shuffle_buffer_size)\nd = d.map(parser_fn, num_parallel_calls=num_map_threads)\n Important caveats:  Be sure to shard before you use any randomizing operator (such as shuffle). Generally it is best if the shard operator is used early in the dataset pipeline. For example, when reading from a set of TFRecord files, shard before converting the dataset to input samples. This avoids reading every file on every worker. The following is an example of an efficient sharding strategy within a complete pipeline:  d = Dataset.list_files(pattern)\nd = d.shard(num_workers, worker_index)\nd = d.repeat(num_epochs)\nd = d.shuffle(shuffle_buffer_size)\nd = d.interleave(tf.data.TFRecordDataset,\n                 cycle_length=num_readers, block_length=1)\nd = d.map(parser_fn, num_parallel_calls=num_map_threads)\n\n \n\n\n Args\n  num_shards   A tf.int64 scalar tf.Tensor, representing the number of shards operating in parallel.  \n  index   A tf.int64 scalar tf.Tensor, representing the worker index.   \n \n\n\n Returns\n  Dataset   A Dataset.   \n \n\n\n Raises\n  InvalidArgumentError   if num_shards or index are illegal values. \nNote: error checking is done on a best-effort basis, and errors aren't guaranteed to be caught upon dataset creation. (e.g. providing in a placeholder tensor bypasses the early checking, and will instead result in an error during a session.run call.) \n\n   shuffle View source \nshuffle(\n    buffer_size, seed=None, reshuffle_each_iteration=None\n)\n Randomly shuffles the elements of this dataset. This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required. For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then shuffle will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer. reshuffle_each_iteration controls whether the shuffle order should be different for each epoch. In TF 1.X, the idiomatic way to create epochs was through the repeat transformation: \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=True)\ndataset = dataset.repeat(2)  # doctest: +SKIP\n[1, 0, 2, 1, 2, 0]\n \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=False)\ndataset = dataset.repeat(2)  # doctest: +SKIP\n[1, 0, 2, 1, 0, 2]\n In TF 2.0, tf.data.Dataset objects are Python iterables which makes it possible to also create epochs through Python iteration: \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=True)\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 0, 2]\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 2, 0]\n \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=False)\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 0, 2]\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 0, 2]\n\n \n\n\n Args\n  buffer_size   A tf.int64 scalar tf.Tensor, representing the number of elements from this dataset from which the new dataset will sample.  \n  seed   (Optional.) A tf.int64 scalar tf.Tensor, representing the random seed that will be used to create the distribution. See tf.random.set_seed for behavior.  \n  reshuffle_each_iteration   (Optional.) A boolean, which if true indicates that the dataset should be pseudorandomly reshuffled each time it is iterated over. (Defaults to True.)   \n \n\n\n Returns\n  Dataset   A Dataset.    skip View source \nskip(\n    count\n)\n Creates a Dataset that skips count elements from this dataset. \ndataset = tf.data.Dataset.range(10)\ndataset = dataset.skip(7)\nlist(dataset.as_numpy_iterator())\n[7, 8, 9]\n\n \n\n\n Args\n  count   A tf.int64 scalar tf.Tensor, representing the number of elements of this dataset that should be skipped to form the new dataset. If count is greater than the size of this dataset, the new dataset will contain no elements. If count is -1, skips the entire dataset.   \n \n\n\n Returns\n  Dataset   A Dataset.    take View source \ntake(\n    count\n)\n Creates a Dataset with at most count elements from this dataset. \ndataset = tf.data.Dataset.range(10)\ndataset = dataset.take(3)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2]\n\n \n\n\n Args\n  count   A tf.int64 scalar tf.Tensor, representing the number of elements of this dataset that should be taken to form the new dataset. If count is -1, or if count is greater than the size of this dataset, the new dataset will contain all elements of this dataset.   \n \n\n\n Returns\n  Dataset   A Dataset.    unbatch View source \nunbatch()\n Splits elements of a dataset into multiple elements. For example, if elements of the dataset are shaped [B, a0, a1, ...], where B may vary for each input element, then for each element in the dataset, the unbatched dataset will contain B consecutive elements of shape [a0, a1, ...]. \nelements = [ [1, 2, 3], [1, 2], [1, 2, 3, 4] ]\ndataset = tf.data.Dataset.from_generator(lambda: elements, tf.int64)\ndataset = dataset.unbatch()\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 1, 2, 1, 2, 3, 4]\n \nNote: unbatch requires a data copy to slice up the batched tensor into smaller, unbatched tensors. When optimizing performance, try to avoid unnecessary usage of unbatch.\n\n \n\n\n Returns   A Dataset.  \n window View source \nwindow(\n    size, shift=None, stride=1, drop_remainder=False\n)\n Combines (nests of) input elements into a dataset of (nests of) windows. A \"window\" is a finite dataset of flat elements of size size (or possibly fewer if there are not enough input elements to fill the window and drop_remainder evaluates to False). The shift argument determines the number of input elements by which the window moves on each iteration. If windows and elements are both numbered starting at 0, the first element in window k will be element k * shift of the input dataset. In particular, the first element of the first window will always be the first element of the input dataset. The stride argument determines the stride of the input elements, and the shift argument determines the shift of the window. For example: \ndataset = tf.data.Dataset.range(7).window(2)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 1]\n[2, 3]\n[4, 5]\n[6]\ndataset = tf.data.Dataset.range(7).window(3, 2, 1, True)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 1, 2]\n[2, 3, 4]\n[4, 5, 6]\ndataset = tf.data.Dataset.range(7).window(3, 1, 2, True)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 2, 4]\n[1, 3, 5]\n[2, 4, 6]\n Note that when the window transformation is applied to a dataset of nested elements, it produces a dataset of nested windows. \nnested = ([1, 2, 3, 4], [5, 6, 7, 8])\ndataset = tf.data.Dataset.from_tensor_slices(nested).window(2)\nfor window in dataset:\n  def to_numpy(ds):\n    return list(ds.as_numpy_iterator())\n  print(tuple(to_numpy(component) for component in window))\n([1, 2], [5, 6])\n([3, 4], [7, 8])\n \ndataset = tf.data.Dataset.from_tensor_slices({'a': [1, 2, 3, 4]})\ndataset = dataset.window(2)\nfor window in dataset:\n  def to_numpy(ds):\n    return list(ds.as_numpy_iterator())\n  print({'a': to_numpy(window['a'])})\n{'a': [1, 2]}\n{'a': [3, 4]}\n\n \n\n\n Args\n  size   A tf.int64 scalar tf.Tensor, representing the number of elements of the input dataset to combine into a window. Must be positive.  \n  shift   (Optional.) A tf.int64 scalar tf.Tensor, representing the number of input elements by which the window moves in each iteration. Defaults to size. Must be positive.  \n  stride   (Optional.) A tf.int64 scalar tf.Tensor, representing the stride of the input elements in the sliding window. Must be positive. The default value of 1 means \"retain every input element\".  \n  drop_remainder   (Optional.) A tf.bool scalar tf.Tensor, representing whether the last windows should be dropped if their size is smaller than size.   \n \n\n\n Returns\n  Dataset   A Dataset of (nests of) windows -- a finite datasets of flat elements created from the (nests of) input elements.    with_options View source \nwith_options(\n    options\n)\n Returns a new tf.data.Dataset with the given options set. The options are \"global\" in the sense they apply to the entire dataset. If options are set multiple times, they are merged as long as different options do not use different non-default values. \nds = tf.data.Dataset.range(5)\nds = ds.interleave(lambda x: tf.data.Dataset.range(5),\n                   cycle_length=3,\n                   num_parallel_calls=3)\noptions = tf.data.Options()\n# This will make the interleave order non-deterministic.\noptions.experimental_deterministic = False\nds = ds.with_options(options)\n\n \n\n\n Args\n  options   A tf.data.Options that identifies the options the use.   \n \n\n\n Returns\n  Dataset   A Dataset with the given options.   \n \n\n\n Raises\n  ValueError   when an option is set more than once to a non-default value    zip View source \n@staticmethod\nzip(\n    datasets\n)\n Creates a Dataset by zipping together the given datasets. This method has similar semantics to the built-in zip() function in Python, with the main difference being that the datasets argument can be an arbitrary nested structure of Dataset objects. \n# The nested structure of the `datasets` argument determines the\n# structure of elements in the resulting dataset.\na = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\nb = tf.data.Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]\nds = tf.data.Dataset.zip((a, b))\nlist(ds.as_numpy_iterator())\n[(1, 4), (2, 5), (3, 6)]\nds = tf.data.Dataset.zip((b, a))\nlist(ds.as_numpy_iterator())\n[(4, 1), (5, 2), (6, 3)]\n\n# The `datasets` argument may contain an arbitrary number of datasets.\nc = tf.data.Dataset.range(7, 13).batch(2)  # ==> [ [7, 8],\n                                           #       [9, 10],\n                                           #       [11, 12] ]\nds = tf.data.Dataset.zip((a, b, c))\nfor element in ds.as_numpy_iterator():\n  print(element)\n(1, 4, array([7, 8]))\n(2, 5, array([ 9, 10]))\n(3, 6, array([11, 12]))\n\n# The number of elements in the resulting dataset is the same as\n# the size of the smallest dataset in `datasets`.\nd = tf.data.Dataset.range(13, 15)  # ==> [ 13, 14 ]\nds = tf.data.Dataset.zip((a, d))\nlist(ds.as_numpy_iterator())\n[(1, 13), (2, 14)]\n\n \n\n\n Args\n  datasets   A nested structure of datasets.   \n \n\n\n Returns\n  Dataset   A Dataset.    __bool__ View source \n__bool__()\n __iter__ View source \n__iter__()\n Creates an iterator for elements of this dataset. The returned iterator implements the Python Iterator protocol.\n \n\n\n Returns   An tf.data.Iterator for the elements of this dataset.  \n\n \n\n\n Raises\n  RuntimeError   If not inside of tf.function and not executing eagerly.    __len__ View source \n__len__()\n Returns the length of the dataset if it is known and finite. This method requires that you are running in eager mode, and that the length of the dataset is known and non-infinite. When the length may be unknown or infinite, or if you are running in graph mode, use tf.data.Dataset.cardinality instead.\n \n\n\n Returns   An integer representing the length of the dataset.  \n\n \n\n\n Raises\n  RuntimeError   If the dataset length is unknown or infinite, or if eager execution is not enabled.    __nonzero__ View source \n__nonzero__()\n  \n"}, {"name": "tf.compat.v1.data.experimental.StatsAggregator", "path": "compat/v1/data/experimental/statsaggregator", "type": "tf.compat", "text": "tf.compat.v1.data.experimental.StatsAggregator A stateful resource that aggregates statistics from one or more iterators. \ntf.compat.v1.data.experimental.StatsAggregator()\n To record statistics, use one of the custom transformation functions defined in this module when defining your tf.data.Dataset. All statistics will be aggregated by the StatsAggregator that is associated with a particular iterator (see below). For example, to record the latency of producing each element by iterating over a dataset: dataset = ...\ndataset = dataset.apply(tf.data.experimental.latency_stats(\"total_bytes\"))\n To associate a StatsAggregator with a tf.data.Dataset object, use the following pattern: aggregator = tf.data.experimental.StatsAggregator()\ndataset = ...\n\n# Apply `StatsOptions` to associate `dataset` with `aggregator`.\noptions = tf.data.Options()\noptions.experimental_stats.aggregator = aggregator\ndataset = dataset.with_options(options)\n To get a protocol buffer summary of the currently aggregated statistics, use the StatsAggregator.get_summary() tensor. The easiest way to do this is to add the returned tensor to the tf.GraphKeys.SUMMARIES collection, so that the summaries will be included with any existing summaries. aggregator = tf.data.experimental.StatsAggregator()\n# ...\nstats_summary = aggregator.get_summary()\ntf.compat.v1.add_to_collection(tf.GraphKeys.SUMMARIES, stats_summary)\n\n\nNote: This interface is experimental and expected to change. In particular, we expect to add other implementations of StatsAggregator that provide different ways of exporting statistics, and add more types of statistics.\n Methods get_summary View source \nget_summary()\n Returns a string tf.Tensor that summarizes the aggregated statistics. The returned tensor will contain a serialized tf.compat.v1.summary.Summary protocol buffer, which can be used with the standard TensorBoard logging facilities.\n \n\n\n Returns   A scalar string tf.Tensor that summarizes the aggregated statistics.  \n  \n"}, {"name": "tf.compat.v1.data.experimental.TensorArrayStructure", "path": "compat/v1/data/experimental/tensorarraystructure", "type": "tf.compat", "text": "tf.compat.v1.data.experimental.TensorArrayStructure DEPRECATED FUNCTION \ntf.compat.v1.data.experimental.TensorArrayStructure(\n    dtype, element_shape, dynamic_size, infer_shape\n)\n Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.TensorArraySpec instead.  \n"}, {"name": "tf.compat.v1.data.experimental.TensorStructure", "path": "compat/v1/data/experimental/tensorstructure", "type": "tf.compat", "text": "tf.compat.v1.data.experimental.TensorStructure DEPRECATED FUNCTION \ntf.compat.v1.data.experimental.TensorStructure(\n    dtype, shape\n)\n Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.TensorSpec instead.  \n"}, {"name": "tf.compat.v1.data.FixedLengthRecordDataset", "path": "compat/v1/data/fixedlengthrecorddataset", "type": "tf.compat", "text": "tf.compat.v1.data.FixedLengthRecordDataset A Dataset of fixed-length records from one or more binary files. Inherits From: Dataset, Dataset \ntf.compat.v1.data.FixedLengthRecordDataset(\n    filenames, record_bytes, header_bytes=None, footer_bytes=None, buffer_size=None,\n    compression_type=None, num_parallel_reads=None\n)\n\n \n\n\n Args\n  filenames   A tf.string tensor or tf.data.Dataset containing one or more filenames.  \n  record_bytes   A tf.int64 scalar representing the number of bytes in each record.  \n  header_bytes   (Optional.) A tf.int64 scalar representing the number of bytes to skip at the start of a file.  \n  footer_bytes   (Optional.) A tf.int64 scalar representing the number of bytes to ignore at the end of a file.  \n  buffer_size   (Optional.) A tf.int64 scalar representing the number of bytes to buffer when reading.  \n  compression_type   (Optional.) A tf.string scalar evaluating to one of \"\" (no compression), \"ZLIB\", or \"GZIP\".  \n  num_parallel_reads   (Optional.) A tf.int64 scalar representing the number of files to read in parallel. If greater than one, the records of files read in parallel are outputted in an interleaved order. If your input pipeline is I/O bottlenecked, consider setting this parameter to a value greater than one to parallelize the I/O. If None, files will be read sequentially.   \n \n\n\n Attributes\n  element_spec   The type specification of an element of this dataset. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset.element_spec\nTensorSpec(shape=(), dtype=tf.int32, name=None)\n\n \n  output_classes   Returns the class of each component of an element of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.compat.v1.data.get_output_classes(dataset). \n \n  output_shapes   Returns the shape of each component of an element of this dataset. (deprecated)Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.compat.v1.data.get_output_shapes(dataset). \n \n  output_types   Returns the type of each component of an element of this dataset. (deprecated)Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.compat.v1.data.get_output_types(dataset). \n   Methods apply View source \napply(\n    transformation_func\n)\n Applies a transformation function to this dataset. apply enables chaining of custom Dataset transformations, which are represented as functions that take one Dataset argument and return a transformed Dataset. \ndataset = tf.data.Dataset.range(100)\ndef dataset_fn(ds):\n  return ds.filter(lambda x: x < 5)\ndataset = dataset.apply(dataset_fn)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2, 3, 4]\n\n \n\n\n Args\n  transformation_func   A function that takes one Dataset argument and returns a Dataset.   \n \n\n\n Returns\n  Dataset   The Dataset returned by applying transformation_func to this dataset.    as_numpy_iterator View source \nas_numpy_iterator()\n Returns an iterator which converts all elements of the dataset to numpy. Use as_numpy_iterator to inspect the content of your dataset. To see element shapes and types, print dataset elements directly instead of using as_numpy_iterator. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nfor element in dataset:\n  print(element)\ntf.Tensor(1, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(3, shape=(), dtype=int32)\n This method requires that you are running in eager mode and the dataset's element_spec contains only TensorSpec components. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nfor element in dataset.as_numpy_iterator():\n  print(element)\n1\n2\n3\n \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nprint(list(dataset.as_numpy_iterator()))\n[1, 2, 3]\n as_numpy_iterator() will preserve the nested structure of dataset elements. \ndataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),\n                                              'b': [5, 6]})\nlist(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5},\n                                      {'a': (2, 4), 'b': 6}]\nTrue\n\n \n\n\n Returns   An iterable over the elements of the dataset, with their tensors converted to numpy arrays.  \n\n \n\n\n Raises\n  TypeError   if an element contains a non-Tensor value.  \n  RuntimeError   if eager execution is not enabled.    batch View source \nbatch(\n    batch_size, drop_remainder=False\n)\n Combines consecutive elements of this dataset into batches. \ndataset = tf.data.Dataset.range(8)\ndataset = dataset.batch(3)\nlist(dataset.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5]), array([6, 7])]\n \ndataset = tf.data.Dataset.range(8)\ndataset = dataset.batch(3, drop_remainder=True)\nlist(dataset.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5])]\n The components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced.\n \n\n\n Args\n  batch_size   A tf.int64 scalar tf.Tensor, representing the number of consecutive elements of this dataset to combine in a single batch.  \n  drop_remainder   (Optional.) A tf.bool scalar tf.Tensor, representing whether the last batch should be dropped in the case it has fewer than batch_size elements; the default behavior is not to drop the smaller batch.   \n \n\n\n Returns\n  Dataset   A Dataset.    cache View source \ncache(\n    filename=''\n)\n Caches the elements in this dataset. The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data. \nNote: For the cache to be finalized, the input dataset must be iterated through in its entirety. Otherwise, subsequent iterations will not use cached data.\n \ndataset = tf.data.Dataset.range(5)\ndataset = dataset.map(lambda x: x**2)\ndataset = dataset.cache()\n# The first time reading through the data will generate the data using\n# `range` and `map`.\nlist(dataset.as_numpy_iterator())\n[0, 1, 4, 9, 16]\n# Subsequent iterations read from the cache.\nlist(dataset.as_numpy_iterator())\n[0, 1, 4, 9, 16]\n When caching to a file, the cached data will persist across runs. Even the first iteration through the data will read from the cache file. Changing the input pipeline before the call to .cache() will have no effect until the cache file is removed or the filename is changed. \ndataset = tf.data.Dataset.range(5)\ndataset = dataset.cache(\"/path/to/file\")  # doctest: +SKIP\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[0, 1, 2, 3, 4]\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.cache(\"/path/to/file\")  # Same file! # doctest: +SKIP\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[0, 1, 2, 3, 4]\n \nNote: cache will produce exactly the same elements during each iteration through the dataset. If you wish to randomize the iteration order, make sure to call shuffle after calling cache.\n\n \n\n\n Args\n  filename   A tf.string scalar tf.Tensor, representing the name of a directory on the filesystem to use for caching elements in this Dataset. If a filename is not provided, the dataset will be cached in memory.   \n \n\n\n Returns\n  Dataset   A Dataset.    cardinality View source \ncardinality()\n Returns the cardinality of the dataset, if known. cardinality may return tf.data.INFINITE_CARDINALITY if the dataset contains an infinite number of elements or tf.data.UNKNOWN_CARDINALITY if the analysis fails to determine the number of elements in the dataset (e.g. when the dataset source is a file). \ndataset = tf.data.Dataset.range(42)\nprint(dataset.cardinality().numpy())\n42\ndataset = dataset.repeat()\ncardinality = dataset.cardinality()\nprint((cardinality == tf.data.INFINITE_CARDINALITY).numpy())\nTrue\ndataset = dataset.filter(lambda x: True)\ncardinality = dataset.cardinality()\nprint((cardinality == tf.data.UNKNOWN_CARDINALITY).numpy())\nTrue\n\n \n\n\n Returns   A scalar tf.int64 Tensor representing the cardinality of the dataset. If the cardinality is infinite or unknown, cardinality returns the named constants tf.data.INFINITE_CARDINALITY and tf.data.UNKNOWN_CARDINALITY respectively.  \n concatenate View source \nconcatenate(\n    dataset\n)\n Creates a Dataset by concatenating the given dataset with this dataset. \na = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\nb = tf.data.Dataset.range(4, 8)  # ==> [ 4, 5, 6, 7 ]\nds = a.concatenate(b)\nlist(ds.as_numpy_iterator())\n[1, 2, 3, 4, 5, 6, 7]\n# The input dataset and dataset to be concatenated should have the same\n# nested structures and output types.\nc = tf.data.Dataset.zip((a, b))\na.concatenate(c)\nTraceback (most recent call last):\nTypeError: Two datasets to concatenate have different types\n<dtype: 'int64'> and (tf.int64, tf.int64)\nd = tf.data.Dataset.from_tensor_slices([\"a\", \"b\", \"c\"])\na.concatenate(d)\nTraceback (most recent call last):\nTypeError: Two datasets to concatenate have different types\n<dtype: 'int64'> and <dtype: 'string'>\n\n \n\n\n Args\n  dataset   Dataset to be concatenated.   \n \n\n\n Returns\n  Dataset   A Dataset.    enumerate View source \nenumerate(\n    start=0\n)\n Enumerates the elements of this dataset. It is similar to python's enumerate. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.enumerate(start=5)\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(5, 1)\n(6, 2)\n(7, 3)\n \n# The nested structure of the input dataset determines the structure of\n# elements in the resulting dataset.\ndataset = tf.data.Dataset.from_tensor_slices([(7, 8), (9, 10)])\ndataset = dataset.enumerate()\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(0, array([7, 8], dtype=int32))\n(1, array([ 9, 10], dtype=int32))\n\n \n\n\n Args\n  start   A tf.int64 scalar tf.Tensor, representing the start value for enumeration.   \n \n\n\n Returns\n  Dataset   A Dataset.    filter View source \nfilter(\n    predicate\n)\n Filters this dataset according to predicate. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.filter(lambda x: x < 3)\nlist(dataset.as_numpy_iterator())\n[1, 2]\n# `tf.math.equal(x, y)` is required for equality comparison\ndef filter_fn(x):\n  return tf.math.equal(x, 1)\ndataset = dataset.filter(filter_fn)\nlist(dataset.as_numpy_iterator())\n[1]\n\n \n\n\n Args\n  predicate   A function mapping a dataset element to a boolean.   \n \n\n\n Returns\n  Dataset   The Dataset containing the elements of this dataset for which predicate is True.    filter_with_legacy_function View source \nfilter_with_legacy_function(\n    predicate\n)\n Filters this dataset according to predicate. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use `tf.data.Dataset.filter()\nNote: This is an escape hatch for existing uses of filter that do not work with V2 functions. New uses are strongly discouraged and existing uses should migrate to filter as this method will be removed in V2.\n\n \n\n\n Args\n  predicate   A function mapping a nested structure of tensors (having shapes and types defined by self.output_shapes and self.output_types) to a scalar tf.bool tensor.   \n \n\n\n Returns\n  Dataset   The Dataset containing the elements of this dataset for which predicate is True.    flat_map View source \nflat_map(\n    map_func\n)\n Maps map_func across this dataset and flattens the result. Use flat_map if you want to make sure that the order of your dataset stays the same. For example, to flatten a dataset of batches into a dataset of their elements: \ndataset = tf.data.Dataset.from_tensor_slices(\n               [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ndataset = dataset.flat_map(lambda x: Dataset.from_tensor_slices(x))\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n tf.data.Dataset.interleave() is a generalization of flat_map, since flat_map produces the same output as tf.data.Dataset.interleave(cycle_length=1)\n \n\n\n Args\n  map_func   A function mapping a dataset element to a dataset.   \n \n\n\n Returns\n  Dataset   A Dataset.    from_generator View source \n@staticmethod\nfrom_generator(\n    generator, output_types=None, output_shapes=None, args=None,\n    output_signature=None\n)\n Creates a Dataset whose elements are generated by generator. (deprecated arguments) Warning: SOME ARGUMENTS ARE DEPRECATED: (output_shapes, output_types). They will be removed in a future version. Instructions for updating: Use output_signature instead The generator argument must be a callable object that returns an object that supports the iter() protocol (e.g. a generator function). The elements generated by generator must be compatible with either the given output_signature argument or with the given output_types and (optionally) output_shapes arguments, whichiver was specified. The recommended way to call from_generator is to use the output_signature argument. In this case the output will be assumed to consist of objects with the classes, shapes and types defined by tf.TypeSpec objects from output_signature argument: \ndef gen():\n  ragged_tensor = tf.ragged.constant([[1, 2], [3]])\n  yield 42, ragged_tensor\n\ndataset = tf.data.Dataset.from_generator(\n     gen,\n     output_signature=(\n         tf.TensorSpec(shape=(), dtype=tf.int32),\n         tf.RaggedTensorSpec(shape=(2, None), dtype=tf.int32)))\n\nlist(dataset.take(1))\n[(<tf.Tensor: shape=(), dtype=int32, numpy=42>,\n<tf.RaggedTensor [[1, 2], [3]]>)]\n There is also a deprecated way to call from_generator by either with output_types argument alone or together with output_shapes argument. In this case the output of the function will be assumed to consist of tf.Tensor objects with with the types defined by output_types and with the shapes which are either unknown or defined by output_shapes. \nNote: The current implementation of Dataset.from_generator() uses tf.numpy_function and inherits the same constraints. In particular, it requires the dataset and iterator related operations to be placed on a device in the same process as the Python program that called Dataset.from_generator(). The body of generator will not be serialized in a GraphDef, and you should not use this method if you need to serialize your model and restore it in a different environment.\n\n\nNote: If generator depends on mutable global variables or other external state, be aware that the runtime may invoke generator multiple times (in order to support repeating the Dataset) and at any time between the call to Dataset.from_generator() and the production of the first element from the generator. Mutating global variables or external state can cause undefined behavior, and we recommend that you explicitly cache any external state in generator before calling Dataset.from_generator().\n\n \n\n\n Args\n  generator   A callable object that returns an object that supports the iter() protocol. If args is not specified, generator must take no arguments; otherwise it must take as many arguments as there are values in args.  \n  output_types   (Optional.) A nested structure of tf.DType objects corresponding to each component of an element yielded by generator.  \n  output_shapes   (Optional.) A nested structure of tf.TensorShape objects corresponding to each component of an element yielded by generator.  \n  args   (Optional.) A tuple of tf.Tensor objects that will be evaluated and passed to generator as NumPy-array arguments.  \n  output_signature   (Optional.) A nested structure of tf.TypeSpec objects corresponding to each component of an element yielded by generator.   \n \n\n\n Returns\n  Dataset   A Dataset.    from_sparse_tensor_slices View source \n@staticmethod\nfrom_sparse_tensor_slices(\n    sparse_tensor\n)\n Splits each rank-N tf.sparse.SparseTensor in this dataset row-wise. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.data.Dataset.from_tensor_slices().\n \n\n\n Args\n  sparse_tensor   A tf.sparse.SparseTensor.   \n \n\n\n Returns\n  Dataset   A Dataset of rank-(N-1) sparse tensors.    from_tensor_slices View source \n@staticmethod\nfrom_tensor_slices(\n    tensors\n)\n Creates a Dataset whose elements are slices of the given tensors. The given tensors are sliced along their first dimension. This operation preserves the structure of the input tensors, removing the first dimension of each tensor and using it as the dataset dimension. All input tensors must have the same size in their first dimensions. \n# Slicing a 1D tensor produces scalar tensor elements.\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nlist(dataset.as_numpy_iterator())\n[1, 2, 3]\n \n# Slicing a 2D tensor produces 1D tensor elements.\ndataset = tf.data.Dataset.from_tensor_slices([[1, 2], [3, 4]])\nlist(dataset.as_numpy_iterator())\n[array([1, 2], dtype=int32), array([3, 4], dtype=int32)]\n \n# Slicing a tuple of 1D tensors produces tuple elements containing\n# scalar tensors.\ndataset = tf.data.Dataset.from_tensor_slices(([1, 2], [3, 4], [5, 6]))\nlist(dataset.as_numpy_iterator())\n[(1, 3, 5), (2, 4, 6)]\n \n# Dictionary structure is also preserved.\ndataset = tf.data.Dataset.from_tensor_slices({\"a\": [1, 2], \"b\": [3, 4]})\nlist(dataset.as_numpy_iterator()) == [{'a': 1, 'b': 3},\n                                      {'a': 2, 'b': 4}]\nTrue\n \n# Two tensors can be combined into one Dataset object.\nfeatures = tf.constant([[1, 3], [2, 1], [3, 3]]) # ==> 3x2 tensor\nlabels = tf.constant(['A', 'B', 'A']) # ==> 3x1 tensor\ndataset = Dataset.from_tensor_slices((features, labels))\n# Both the features and the labels tensors can be converted\n# to a Dataset object separately and combined after.\nfeatures_dataset = Dataset.from_tensor_slices(features)\nlabels_dataset = Dataset.from_tensor_slices(labels)\ndataset = Dataset.zip((features_dataset, labels_dataset))\n# A batched feature and label set can be converted to a Dataset\n# in similar fashion.\nbatched_features = tf.constant([[[1, 3], [2, 3]],\n                                [[2, 1], [1, 2]],\n                                [[3, 3], [3, 2]]], shape=(3, 2, 2))\nbatched_labels = tf.constant([['A', 'A'],\n                              ['B', 'B'],\n                              ['A', 'B']], shape=(3, 2, 1))\ndataset = Dataset.from_tensor_slices((batched_features, batched_labels))\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(array([[1, 3],\n       [2, 3]], dtype=int32), array([[b'A'],\n       [b'A']], dtype=object))\n(array([[2, 1],\n       [1, 2]], dtype=int32), array([[b'B'],\n       [b'B']], dtype=object))\n(array([[3, 3],\n       [3, 2]], dtype=int32), array([[b'A'],\n       [b'B']], dtype=object))\n Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.\n \n\n\n Args\n  tensors   A dataset element, with each component having the same size in the first dimension.   \n \n\n\n Returns\n  Dataset   A Dataset.    from_tensors View source \n@staticmethod\nfrom_tensors(\n    tensors\n)\n Creates a Dataset with a single element, comprising the given tensors. from_tensors produces a dataset containing only a single element. To slice the input tensor into multiple elements, use from_tensor_slices instead. \ndataset = tf.data.Dataset.from_tensors([1, 2, 3])\nlist(dataset.as_numpy_iterator())\n[array([1, 2, 3], dtype=int32)]\ndataset = tf.data.Dataset.from_tensors(([1, 2, 3], 'A'))\nlist(dataset.as_numpy_iterator())\n[(array([1, 2, 3], dtype=int32), b'A')]\n \n# You can use `from_tensors` to produce a dataset which repeats\n# the same example many times.\nexample = tf.constant([1,2,3])\ndataset = tf.data.Dataset.from_tensors(example).repeat(2)\nlist(dataset.as_numpy_iterator())\n[array([1, 2, 3], dtype=int32), array([1, 2, 3], dtype=int32)]\n Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.\n \n\n\n Args\n  tensors   A dataset element.   \n \n\n\n Returns\n  Dataset   A Dataset.    interleave View source \ninterleave(\n    map_func, cycle_length=None, block_length=None, num_parallel_calls=None,\n    deterministic=None\n)\n Maps map_func across this dataset, and interleaves the results. For example, you can use Dataset.interleave() to process many input files concurrently: \n# Preprocess 4 files concurrently, and interleave blocks of 16 records\n# from each file.\nfilenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n             \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\ndataset = tf.data.Dataset.from_tensor_slices(filenames)\ndef parse_fn(filename):\n  return tf.data.Dataset.range(10)\ndataset = dataset.interleave(lambda x:\n    tf.data.TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n    cycle_length=4, block_length=16)\n The cycle_length and block_length arguments control the order in which elements are produced. cycle_length controls the number of input elements that are processed concurrently. If you set cycle_length to 1, this transformation will handle one input element at a time, and will produce identical results to tf.data.Dataset.flat_map. In general, this transformation will apply map_func to cycle_length input elements, open iterators on the returned Dataset objects, and cycle through them producing block_length consecutive elements from each iterator, and consuming the next input element each time it reaches the end of an iterator. For example: \ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n# NOTE: New lines indicate \"block\" boundaries.\ndataset = dataset.interleave(\n    lambda x: Dataset.from_tensors(x).repeat(6),\n    cycle_length=2, block_length=4)\nlist(dataset.as_numpy_iterator())\n[1, 1, 1, 1,\n 2, 2, 2, 2,\n 1, 1,\n 2, 2,\n 3, 3, 3, 3,\n 4, 4, 4, 4,\n 3, 3,\n 4, 4,\n 5, 5, 5, 5,\n 5, 5]\n \nNote: The order of elements yielded by this transformation is deterministic, as long as map_func is a pure function and deterministic=True. If map_func contains any stateful operations, the order in which that state is accessed is undefined.\n Performance can often be improved by setting num_parallel_calls so that interleave will use multiple threads to fetch elements. If determinism isn't required, it can also improve performance to set deterministic=False. \nfilenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n             \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\ndataset = tf.data.Dataset.from_tensor_slices(filenames)\ndataset = dataset.interleave(lambda x: tf.data.TFRecordDataset(x),\n    cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE,\n    deterministic=False)\n\n \n\n\n Args\n  map_func   A function mapping a dataset element to a dataset.  \n  cycle_length   (Optional.) The number of input elements that will be processed concurrently. If not set, the tf.data runtime decides what it should be based on available CPU. If num_parallel_calls is set to tf.data.AUTOTUNE, the cycle_length argument identifies the maximum degree of parallelism.  \n  block_length   (Optional.) The number of consecutive elements to produce from each input element before cycling to another input element. If not set, defaults to 1.  \n  num_parallel_calls   (Optional.) If specified, the implementation creates a threadpool, which is used to fetch inputs from cycle elements asynchronously and in parallel. The default behavior is to fetch inputs from cycle elements synchronously with no parallelism. If the value tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available CPU.  \n  deterministic   (Optional.) A boolean controlling whether determinism should be traded for performance by allowing elements to be produced out of order. If deterministic is None, the tf.data.Options.experimental_deterministic dataset option (True by default) is used to decide whether to produce elements deterministically.   \n \n\n\n Returns\n  Dataset   A Dataset.    list_files View source \n@staticmethod\nlist_files(\n    file_pattern, shuffle=None, seed=None\n)\n A dataset of all files matching one or more glob patterns. The file_pattern argument should be a small number of glob patterns. If your filenames have already been globbed, use Dataset.from_tensor_slices(filenames) instead, as re-globbing every filename with list_files may result in poor performance with remote storage systems. \nNote: The default behavior of this method is to return filenames in a non-deterministic random shuffled order. Pass a seed or shuffle=False to get results in a deterministic order.\n Example: If we had the following files on our filesystem:  /path/to/dir/a.txt /path/to/dir/b.py /path/to/dir/c.py  If we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:  /path/to/dir/b.py /path/to/dir/c.py \n \n\n\n Args\n  file_pattern   A string, a list of strings, or a tf.Tensor of string type (scalar or vector), representing the filename glob (i.e. shell wildcard) pattern(s) that will be matched.  \n  shuffle   (Optional.) If True, the file names will be shuffled randomly. Defaults to True.  \n  seed   (Optional.) A tf.int64 scalar tf.Tensor, representing the random seed that will be used to create the distribution. See tf.random.set_seed for behavior.   \n \n\n\n Returns\n  Dataset   A Dataset of strings corresponding to file names.    make_initializable_iterator View source \nmake_initializable_iterator(\n    shared_name=None\n)\n Creates an iterator for elements of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through tf.compat.v1. In all other situations -- namely, eager mode and inside tf.function -- you can consume dataset elements using for elem in dataset: ... or by explicitly creating iterator via iterator = iter(dataset) and fetching its elements via values = next(iterator). Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use tf.compat.v1.data.make_initializable_iterator(dataset) to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\nNote: The returned iterator will be in an uninitialized state, and you must run the iterator.initializer operation before using it:\n\n# Building graph ...\ndataset = ...\niterator = dataset.make_initializable_iterator()\nnext_value = iterator.get_next()  # This is a Tensor.\n\n# ... from within a session ...\nsess.run(iterator.initializer)\ntry:\n  while True:\n    value = sess.run(next_value)\n    ...\nexcept tf.errors.OutOfRangeError:\n    pass\n\n \n\n\n Args\n  shared_name   (Optional.) If non-empty, the returned iterator will be shared under the given name across multiple sessions that share the same devices (e.g. when using a remote server).   \n \n\n\n Returns   A tf.data.Iterator for elements of this dataset.  \n\n \n\n\n Raises\n  RuntimeError   If eager execution is enabled.    make_one_shot_iterator View source \nmake_one_shot_iterator()\n Creates an iterator for elements of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through tf.compat.v1. In all other situations -- namely, eager mode and inside tf.function -- you can consume dataset elements using for elem in dataset: ... or by explicitly creating iterator via iterator = iter(dataset) and fetching its elements via values = next(iterator). Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use tf.compat.v1.data.make_one_shot_iterator(dataset) to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\nNote: The returned iterator will be initialized automatically. A \"one-shot\" iterator does not currently support re-initialization. For that see make_initializable_iterator.\n Example: # Building graph ...\ndataset = ...\nnext_value = dataset.make_one_shot_iterator().get_next()\n\n# ... from within a session ...\ntry:\n  while True:\n    value = sess.run(next_value)\n    ...\nexcept tf.errors.OutOfRangeError:\n    pass\n\n \n\n\n Returns   An tf.data.Iterator for elements of this dataset.  \n map View source \nmap(\n    map_func, num_parallel_calls=None, deterministic=None\n)\n Maps map_func across the elements of this dataset. This transformation applies map_func to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. map_func can be used to change both the values and the structure of a dataset's elements. For example, adding 1 to each element, or projecting a subset of element components. \ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\ndataset = dataset.map(lambda x: x + 1)\nlist(dataset.as_numpy_iterator())\n[2, 3, 4, 5, 6]\n The input signature of map_func is determined by the structure of each element in this dataset. \ndataset = Dataset.range(5)\n# `map_func` takes a single argument of type `tf.Tensor` with the same\n# shape and dtype.\nresult = dataset.map(lambda x: x + 1)\n \n# Each element is a tuple containing two `tf.Tensor` objects.\nelements = [(1, \"foo\"), (2, \"bar\"), (3, \"baz\")]\ndataset = tf.data.Dataset.from_generator(\n    lambda: elements, (tf.int32, tf.string))\n# `map_func` takes two arguments of type `tf.Tensor`. This function\n# projects out just the first component.\nresult = dataset.map(lambda x_int, y_str: x_int)\nlist(result.as_numpy_iterator())\n[1, 2, 3]\n \n# Each element is a dictionary mapping strings to `tf.Tensor` objects.\nelements =  ([{\"a\": 1, \"b\": \"foo\"},\n              {\"a\": 2, \"b\": \"bar\"},\n              {\"a\": 3, \"b\": \"baz\"}])\ndataset = tf.data.Dataset.from_generator(\n    lambda: elements, {\"a\": tf.int32, \"b\": tf.string})\n# `map_func` takes a single argument of type `dict` with the same keys\n# as the elements.\nresult = dataset.map(lambda d: str(d[\"a\"]) + d[\"b\"])\n The value or values returned by map_func determine the structure of each element in the returned dataset. \ndataset = tf.data.Dataset.range(3)\n# `map_func` returns two `tf.Tensor` objects.\ndef g(x):\n  return tf.constant(37.0), tf.constant([\"Foo\", \"Bar\", \"Baz\"])\nresult = dataset.map(g)\nresult.element_spec\n(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(3,), dtype=tf.string, name=None))\n# Python primitives, lists, and NumPy arrays are implicitly converted to\n# `tf.Tensor`.\ndef h(x):\n  return 37.0, [\"Foo\", \"Bar\"], np.array([1.0, 2.0], dtype=np.float64)\nresult = dataset.map(h)\nresult.element_spec\n(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(2,), dtype=tf.string, name=None), TensorSpec(shape=(2,), dtype=tf.float64, name=None))\n# `map_func` can return nested structures.\ndef i(x):\n  return (37.0, [42, 16]), \"foo\"\nresult = dataset.map(i)\nresult.element_spec\n((TensorSpec(shape=(), dtype=tf.float32, name=None),\n  TensorSpec(shape=(2,), dtype=tf.int32, name=None)),\n TensorSpec(shape=(), dtype=tf.string, name=None))\n map_func can accept as arguments and return any type of dataset element. Note that irrespective of the context in which map_func is defined (eager vs. graph), tf.data traces the function and executes it as a graph. To use Python code inside of the function you have a few options: 1) Rely on AutoGraph to convert Python code into an equivalent graph computation. The downside of this approach is that AutoGraph can convert some but not all Python code. 2) Use tf.py_function, which allows you to write arbitrary Python code but will generally result in worse performance than 1). For example: \nd = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n# transform a string tensor to upper case string using a Python function\ndef upper_case_fn(t: tf.Tensor):\n  return t.numpy().decode('utf-8').upper()\nd = d.map(lambda x: tf.py_function(func=upper_case_fn,\n          inp=[x], Tout=tf.string))\nlist(d.as_numpy_iterator())\n[b'HELLO', b'WORLD']\n 3) Use tf.numpy_function, which also allows you to write arbitrary Python code. Note that tf.py_function accepts tf.Tensor whereas tf.numpy_function accepts numpy arrays and returns only numpy arrays. For example: \nd = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\ndef upper_case_fn(t: np.ndarray):\n  return t.decode('utf-8').upper()\nd = d.map(lambda x: tf.numpy_function(func=upper_case_fn,\n          inp=[x], Tout=tf.string))\nlist(d.as_numpy_iterator())\n[b'HELLO', b'WORLD']\n Note that the use of tf.numpy_function and tf.py_function in general precludes the possibility of executing user-defined transformations in parallel (because of Python GIL). Performance can often be improved by setting num_parallel_calls so that map will use multiple threads to process elements. If deterministic order isn't required, it can also improve performance to set deterministic=False. \ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\ndataset = dataset.map(lambda x: x + 1,\n    num_parallel_calls=tf.data.AUTOTUNE,\n    deterministic=False)\n\n \n\n\n Args\n  map_func   A function mapping a dataset element to another dataset element.  \n  num_parallel_calls   (Optional.) A tf.int32 scalar tf.Tensor, representing the number elements to process asynchronously in parallel. If not specified, elements will be processed sequentially. If the value tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available CPU.  \n  deterministic   (Optional.) A boolean controlling whether determinism should be traded for performance by allowing elements to be produced out of order. If deterministic is None, the tf.data.Options.experimental_deterministic dataset option (True by default) is used to decide whether to produce elements deterministically.   \n \n\n\n Returns\n  Dataset   A Dataset.    map_with_legacy_function View source \nmap_with_legacy_function(\n    map_func, num_parallel_calls=None, deterministic=None\n)\n Maps map_func across the elements of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use `tf.data.Dataset.map()\nNote: This is an escape hatch for existing uses of map that do not work with V2 functions. New uses are strongly discouraged and existing uses should migrate to map as this method will be removed in V2.\n\n \n\n\n Args\n  map_func   A function mapping a nested structure of tensors (having shapes and types defined by self.output_shapes and self.output_types) to another nested structure of tensors.  \n  num_parallel_calls   (Optional.) A tf.int32 scalar tf.Tensor, representing the number elements to process asynchronously in parallel. If not specified, elements will be processed sequentially. If the value tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available CPU.  \n  deterministic   (Optional.) A boolean controlling whether determinism should be traded for performance by allowing elements to be produced out of order. If deterministic is None, the tf.data.Options.experimental_deterministic dataset option (True by default) is used to decide whether to produce elements deterministically.   \n \n\n\n Returns\n  Dataset   A Dataset.    options View source \noptions()\n Returns the options for this dataset and its inputs.\n \n\n\n Returns   A tf.data.Options object representing the dataset options.  \n padded_batch View source \npadded_batch(\n    batch_size, padded_shapes=None, padding_values=None, drop_remainder=False\n)\n Combines consecutive elements of this dataset into padded batches. This transformation combines multiple consecutive elements of the input dataset into a single element. Like tf.data.Dataset.batch, the components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced. Unlike tf.data.Dataset.batch, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in padded_shapes. The padded_shapes argument determines the resulting shape for each dimension of each component in an output element:  If the dimension is a constant, the component will be padded out to that length in that dimension. If the dimension is unknown, the component will be padded out to the maximum length of all elements in that dimension.  \nA = (tf.data.Dataset\n     .range(1, 5, output_type=tf.int32)\n     .map(lambda x: tf.fill([x], x)))\n# Pad to the smallest per-batch size that fits all elements.\nB = A.padded_batch(2)\nfor element in B.as_numpy_iterator():\n  print(element)\n[[1 0]\n [2 2]]\n[[3 3 3 0]\n [4 4 4 4]]\n# Pad to a fixed size.\nC = A.padded_batch(2, padded_shapes=5)\nfor element in C.as_numpy_iterator():\n  print(element)\n[[1 0 0 0 0]\n [2 2 0 0 0]]\n[[3 3 3 0 0]\n [4 4 4 4 0]]\n# Pad with a custom value.\nD = A.padded_batch(2, padded_shapes=5, padding_values=-1)\nfor element in D.as_numpy_iterator():\n  print(element)\n[[ 1 -1 -1 -1 -1]\n [ 2  2 -1 -1 -1]]\n[[ 3  3  3 -1 -1]\n [ 4  4  4  4 -1]]\n# Components of nested elements can be padded independently.\nelements = [([1, 2, 3], [10]),\n            ([4, 5], [11, 12])]\ndataset = tf.data.Dataset.from_generator(\n    lambda: iter(elements), (tf.int32, tf.int32))\n# Pad the first component of the tuple to length 4, and the second\n# component to the smallest size that fits.\ndataset = dataset.padded_batch(2,\n    padded_shapes=([4], [None]),\n    padding_values=(-1, 100))\nlist(dataset.as_numpy_iterator())\n[(array([[ 1,  2,  3, -1], [ 4,  5, -1, -1]], dtype=int32),\n  array([[ 10, 100], [ 11,  12]], dtype=int32))]\n# Pad with a single value and multiple components.\nE = tf.data.Dataset.zip((A, A)).padded_batch(2, padding_values=-1)\nfor element in E.as_numpy_iterator():\n  print(element)\n(array([[ 1, -1],\n       [ 2,  2]], dtype=int32), array([[ 1, -1],\n       [ 2,  2]], dtype=int32))\n(array([[ 3,  3,  3, -1],\n       [ 4,  4,  4,  4]], dtype=int32), array([[ 3,  3,  3, -1],\n       [ 4,  4,  4,  4]], dtype=int32))\n See also tf.data.experimental.dense_to_sparse_batch, which combines elements that may have different shapes into a tf.sparse.SparseTensor.\n \n\n\n Args\n  batch_size   A tf.int64 scalar tf.Tensor, representing the number of consecutive elements of this dataset to combine in a single batch.  \n  padded_shapes   (Optional.) A nested structure of tf.TensorShape or tf.int64 vector tensor-like objects representing the shape to which the respective component of each input element should be padded prior to batching. Any unknown dimensions will be padded to the maximum size of that dimension in each batch. If unset, all dimensions of all components are padded to the maximum size in the batch. padded_shapes must be set if any component has an unknown rank.  \n  padding_values   (Optional.) A nested structure of scalar-shaped tf.Tensor, representing the padding values to use for the respective components. None represents that the nested structure should be padded with default values. Defaults are 0 for numeric types and the empty string for string types. The padding_values should have the same structure as the input dataset. If padding_values is a single element and the input dataset has multiple components, then the same padding_values will be used to pad every component of the dataset. If padding_values is a scalar, then its value will be broadcasted to match the shape of each component.  \n  drop_remainder   (Optional.) A tf.bool scalar tf.Tensor, representing whether the last batch should be dropped in the case it has fewer than batch_size elements; the default behavior is not to drop the smaller batch.   \n \n\n\n Returns\n  Dataset   A Dataset.   \n \n\n\n Raises\n  ValueError   If a component has an unknown rank, and the padded_shapes argument is not set.    prefetch View source \nprefetch(\n    buffer_size\n)\n Creates a Dataset that prefetches elements from this dataset. Most dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements. \nNote: Like other Dataset methods, prefetch operates on the elements of the input dataset. It has no concept of examples vs. batches. examples.prefetch(2) will prefetch two elements (2 examples), while examples.batch(20).prefetch(2) will prefetch 2 elements (2 batches, of 20 examples each).\n \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.prefetch(2)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2]\n\n \n\n\n Args\n  buffer_size   A tf.int64 scalar tf.Tensor, representing the maximum number of elements that will be buffered when prefetching.   \n \n\n\n Returns\n  Dataset   A Dataset.    range View source \n@staticmethod\nrange(\n    *args, **kwargs\n)\n Creates a Dataset of a step-separated range of values. \nlist(Dataset.range(5).as_numpy_iterator())\n[0, 1, 2, 3, 4]\nlist(Dataset.range(2, 5).as_numpy_iterator())\n[2, 3, 4]\nlist(Dataset.range(1, 5, 2).as_numpy_iterator())\n[1, 3]\nlist(Dataset.range(1, 5, -2).as_numpy_iterator())\n[]\nlist(Dataset.range(5, 1).as_numpy_iterator())\n[]\nlist(Dataset.range(5, 1, -2).as_numpy_iterator())\n[5, 3]\nlist(Dataset.range(2, 5, output_type=tf.int32).as_numpy_iterator())\n[2, 3, 4]\nlist(Dataset.range(1, 5, 2, output_type=tf.float32).as_numpy_iterator())\n[1.0, 3.0]\n\n \n\n\n Args\n  *args   follows the same semantics as python's xrange. len(args) == 1 -> start = 0, stop = args[0], step = 1. len(args) == 2 -> start = args[0], stop = args[1], step = 1. len(args) == 3 -> start = args[0], stop = args[1], step = args[2].  \n  **kwargs    output_type: Its expected dtype. (Optional, default: tf.int64). \n\n  \n \n\n\n Returns\n  Dataset   A RangeDataset.   \n \n\n\n Raises\n  ValueError   if len(args) == 0.    reduce View source \nreduce(\n    initial_state, reduce_func\n)\n Reduces the input dataset to a single element. The transformation calls reduce_func successively on every element of the input dataset until the dataset is exhausted, aggregating information in its internal state. The initial_state argument is used for the initial state and the final state is returned as the result. \ntf.data.Dataset.range(5).reduce(np.int64(0), lambda x, _: x + 1).numpy()\n5\ntf.data.Dataset.range(5).reduce(np.int64(0), lambda x, y: x + y).numpy()\n10\n\n \n\n\n Args\n  initial_state   An element representing the initial state of the transformation.  \n  reduce_func   A function that maps (old_state, input_element) to new_state. It must take two arguments and return a new element The structure of new_state must match the structure of initial_state.   \n \n\n\n Returns   A dataset element corresponding to the final state of the transformation.  \n repeat View source \nrepeat(\n    count=None\n)\n Repeats this dataset so each original value is seen count times. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.repeat(3)\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 1, 2, 3, 1, 2, 3]\n \nNote: If this dataset is a function of global state (e.g. a random number generator), then different repetitions may produce different elements.\n\n \n\n\n Args\n  count   (Optional.) A tf.int64 scalar tf.Tensor, representing the number of times the dataset should be repeated. The default behavior (if count is None or -1) is for the dataset be repeated indefinitely.   \n \n\n\n Returns\n  Dataset   A Dataset.    shard View source \nshard(\n    num_shards, index\n)\n Creates a Dataset that includes only 1/num_shards of this dataset. shard is deterministic. The Dataset produced by A.shard(n, i) will contain all elements of A whose index mod n = i. \nA = tf.data.Dataset.range(10)\nB = A.shard(num_shards=3, index=0)\nlist(B.as_numpy_iterator())\n[0, 3, 6, 9]\nC = A.shard(num_shards=3, index=1)\nlist(C.as_numpy_iterator())\n[1, 4, 7]\nD = A.shard(num_shards=3, index=2)\nlist(D.as_numpy_iterator())\n[2, 5, 8]\n This dataset operator is very useful when running distributed training, as it allows each worker to read a unique subset. When reading a single input file, you can shard elements as follows: d = tf.data.TFRecordDataset(input_file)\nd = d.shard(num_workers, worker_index)\nd = d.repeat(num_epochs)\nd = d.shuffle(shuffle_buffer_size)\nd = d.map(parser_fn, num_parallel_calls=num_map_threads)\n Important caveats:  Be sure to shard before you use any randomizing operator (such as shuffle). Generally it is best if the shard operator is used early in the dataset pipeline. For example, when reading from a set of TFRecord files, shard before converting the dataset to input samples. This avoids reading every file on every worker. The following is an example of an efficient sharding strategy within a complete pipeline:  d = Dataset.list_files(pattern)\nd = d.shard(num_workers, worker_index)\nd = d.repeat(num_epochs)\nd = d.shuffle(shuffle_buffer_size)\nd = d.interleave(tf.data.TFRecordDataset,\n                 cycle_length=num_readers, block_length=1)\nd = d.map(parser_fn, num_parallel_calls=num_map_threads)\n\n \n\n\n Args\n  num_shards   A tf.int64 scalar tf.Tensor, representing the number of shards operating in parallel.  \n  index   A tf.int64 scalar tf.Tensor, representing the worker index.   \n \n\n\n Returns\n  Dataset   A Dataset.   \n \n\n\n Raises\n  InvalidArgumentError   if num_shards or index are illegal values. \nNote: error checking is done on a best-effort basis, and errors aren't guaranteed to be caught upon dataset creation. (e.g. providing in a placeholder tensor bypasses the early checking, and will instead result in an error during a session.run call.) \n\n   shuffle View source \nshuffle(\n    buffer_size, seed=None, reshuffle_each_iteration=None\n)\n Randomly shuffles the elements of this dataset. This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required. For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then shuffle will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer. reshuffle_each_iteration controls whether the shuffle order should be different for each epoch. In TF 1.X, the idiomatic way to create epochs was through the repeat transformation: \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=True)\ndataset = dataset.repeat(2)  # doctest: +SKIP\n[1, 0, 2, 1, 2, 0]\n \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=False)\ndataset = dataset.repeat(2)  # doctest: +SKIP\n[1, 0, 2, 1, 0, 2]\n In TF 2.0, tf.data.Dataset objects are Python iterables which makes it possible to also create epochs through Python iteration: \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=True)\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 0, 2]\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 2, 0]\n \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=False)\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 0, 2]\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 0, 2]\n\n \n\n\n Args\n  buffer_size   A tf.int64 scalar tf.Tensor, representing the number of elements from this dataset from which the new dataset will sample.  \n  seed   (Optional.) A tf.int64 scalar tf.Tensor, representing the random seed that will be used to create the distribution. See tf.random.set_seed for behavior.  \n  reshuffle_each_iteration   (Optional.) A boolean, which if true indicates that the dataset should be pseudorandomly reshuffled each time it is iterated over. (Defaults to True.)   \n \n\n\n Returns\n  Dataset   A Dataset.    skip View source \nskip(\n    count\n)\n Creates a Dataset that skips count elements from this dataset. \ndataset = tf.data.Dataset.range(10)\ndataset = dataset.skip(7)\nlist(dataset.as_numpy_iterator())\n[7, 8, 9]\n\n \n\n\n Args\n  count   A tf.int64 scalar tf.Tensor, representing the number of elements of this dataset that should be skipped to form the new dataset. If count is greater than the size of this dataset, the new dataset will contain no elements. If count is -1, skips the entire dataset.   \n \n\n\n Returns\n  Dataset   A Dataset.    take View source \ntake(\n    count\n)\n Creates a Dataset with at most count elements from this dataset. \ndataset = tf.data.Dataset.range(10)\ndataset = dataset.take(3)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2]\n\n \n\n\n Args\n  count   A tf.int64 scalar tf.Tensor, representing the number of elements of this dataset that should be taken to form the new dataset. If count is -1, or if count is greater than the size of this dataset, the new dataset will contain all elements of this dataset.   \n \n\n\n Returns\n  Dataset   A Dataset.    unbatch View source \nunbatch()\n Splits elements of a dataset into multiple elements. For example, if elements of the dataset are shaped [B, a0, a1, ...], where B may vary for each input element, then for each element in the dataset, the unbatched dataset will contain B consecutive elements of shape [a0, a1, ...]. \nelements = [ [1, 2, 3], [1, 2], [1, 2, 3, 4] ]\ndataset = tf.data.Dataset.from_generator(lambda: elements, tf.int64)\ndataset = dataset.unbatch()\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 1, 2, 1, 2, 3, 4]\n \nNote: unbatch requires a data copy to slice up the batched tensor into smaller, unbatched tensors. When optimizing performance, try to avoid unnecessary usage of unbatch.\n\n \n\n\n Returns   A Dataset.  \n window View source \nwindow(\n    size, shift=None, stride=1, drop_remainder=False\n)\n Combines (nests of) input elements into a dataset of (nests of) windows. A \"window\" is a finite dataset of flat elements of size size (or possibly fewer if there are not enough input elements to fill the window and drop_remainder evaluates to False). The shift argument determines the number of input elements by which the window moves on each iteration. If windows and elements are both numbered starting at 0, the first element in window k will be element k * shift of the input dataset. In particular, the first element of the first window will always be the first element of the input dataset. The stride argument determines the stride of the input elements, and the shift argument determines the shift of the window. For example: \ndataset = tf.data.Dataset.range(7).window(2)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 1]\n[2, 3]\n[4, 5]\n[6]\ndataset = tf.data.Dataset.range(7).window(3, 2, 1, True)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 1, 2]\n[2, 3, 4]\n[4, 5, 6]\ndataset = tf.data.Dataset.range(7).window(3, 1, 2, True)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 2, 4]\n[1, 3, 5]\n[2, 4, 6]\n Note that when the window transformation is applied to a dataset of nested elements, it produces a dataset of nested windows. \nnested = ([1, 2, 3, 4], [5, 6, 7, 8])\ndataset = tf.data.Dataset.from_tensor_slices(nested).window(2)\nfor window in dataset:\n  def to_numpy(ds):\n    return list(ds.as_numpy_iterator())\n  print(tuple(to_numpy(component) for component in window))\n([1, 2], [5, 6])\n([3, 4], [7, 8])\n \ndataset = tf.data.Dataset.from_tensor_slices({'a': [1, 2, 3, 4]})\ndataset = dataset.window(2)\nfor window in dataset:\n  def to_numpy(ds):\n    return list(ds.as_numpy_iterator())\n  print({'a': to_numpy(window['a'])})\n{'a': [1, 2]}\n{'a': [3, 4]}\n\n \n\n\n Args\n  size   A tf.int64 scalar tf.Tensor, representing the number of elements of the input dataset to combine into a window. Must be positive.  \n  shift   (Optional.) A tf.int64 scalar tf.Tensor, representing the number of input elements by which the window moves in each iteration. Defaults to size. Must be positive.  \n  stride   (Optional.) A tf.int64 scalar tf.Tensor, representing the stride of the input elements in the sliding window. Must be positive. The default value of 1 means \"retain every input element\".  \n  drop_remainder   (Optional.) A tf.bool scalar tf.Tensor, representing whether the last windows should be dropped if their size is smaller than size.   \n \n\n\n Returns\n  Dataset   A Dataset of (nests of) windows -- a finite datasets of flat elements created from the (nests of) input elements.    with_options View source \nwith_options(\n    options\n)\n Returns a new tf.data.Dataset with the given options set. The options are \"global\" in the sense they apply to the entire dataset. If options are set multiple times, they are merged as long as different options do not use different non-default values. \nds = tf.data.Dataset.range(5)\nds = ds.interleave(lambda x: tf.data.Dataset.range(5),\n                   cycle_length=3,\n                   num_parallel_calls=3)\noptions = tf.data.Options()\n# This will make the interleave order non-deterministic.\noptions.experimental_deterministic = False\nds = ds.with_options(options)\n\n \n\n\n Args\n  options   A tf.data.Options that identifies the options the use.   \n \n\n\n Returns\n  Dataset   A Dataset with the given options.   \n \n\n\n Raises\n  ValueError   when an option is set more than once to a non-default value    zip View source \n@staticmethod\nzip(\n    datasets\n)\n Creates a Dataset by zipping together the given datasets. This method has similar semantics to the built-in zip() function in Python, with the main difference being that the datasets argument can be an arbitrary nested structure of Dataset objects. \n# The nested structure of the `datasets` argument determines the\n# structure of elements in the resulting dataset.\na = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\nb = tf.data.Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]\nds = tf.data.Dataset.zip((a, b))\nlist(ds.as_numpy_iterator())\n[(1, 4), (2, 5), (3, 6)]\nds = tf.data.Dataset.zip((b, a))\nlist(ds.as_numpy_iterator())\n[(4, 1), (5, 2), (6, 3)]\n\n# The `datasets` argument may contain an arbitrary number of datasets.\nc = tf.data.Dataset.range(7, 13).batch(2)  # ==> [ [7, 8],\n                                           #       [9, 10],\n                                           #       [11, 12] ]\nds = tf.data.Dataset.zip((a, b, c))\nfor element in ds.as_numpy_iterator():\n  print(element)\n(1, 4, array([7, 8]))\n(2, 5, array([ 9, 10]))\n(3, 6, array([11, 12]))\n\n# The number of elements in the resulting dataset is the same as\n# the size of the smallest dataset in `datasets`.\nd = tf.data.Dataset.range(13, 15)  # ==> [ 13, 14 ]\nds = tf.data.Dataset.zip((a, d))\nlist(ds.as_numpy_iterator())\n[(1, 13), (2, 14)]\n\n \n\n\n Args\n  datasets   A nested structure of datasets.   \n \n\n\n Returns\n  Dataset   A Dataset.    __bool__ View source \n__bool__()\n __iter__ View source \n__iter__()\n Creates an iterator for elements of this dataset. The returned iterator implements the Python Iterator protocol.\n \n\n\n Returns   An tf.data.Iterator for the elements of this dataset.  \n\n \n\n\n Raises\n  RuntimeError   If not inside of tf.function and not executing eagerly.    __len__ View source \n__len__()\n Returns the length of the dataset if it is known and finite. This method requires that you are running in eager mode, and that the length of the dataset is known and non-infinite. When the length may be unknown or infinite, or if you are running in graph mode, use tf.data.Dataset.cardinality instead.\n \n\n\n Returns   An integer representing the length of the dataset.  \n\n \n\n\n Raises\n  RuntimeError   If the dataset length is unknown or infinite, or if eager execution is not enabled.    __nonzero__ View source \n__nonzero__()\n  \n"}, {"name": "tf.compat.v1.data.get_output_classes", "path": "compat/v1/data/get_output_classes", "type": "tf.compat", "text": "tf.compat.v1.data.get_output_classes Returns the output classes for elements of the input dataset / iterator. \ntf.compat.v1.data.get_output_classes(\n    dataset_or_iterator\n)\n\n \n\n\n Args\n  dataset_or_iterator   A tf.data.Dataset or tf.data.Iterator.   \n \n\n\n Returns   A nested structure of Python type objects matching the structure of the dataset / iterator elements and specifying the class of the individual components.  \n  \n"}, {"name": "tf.compat.v1.data.get_output_shapes", "path": "compat/v1/data/get_output_shapes", "type": "tf.compat", "text": "tf.compat.v1.data.get_output_shapes Returns the output shapes for elements of the input dataset / iterator. \ntf.compat.v1.data.get_output_shapes(\n    dataset_or_iterator\n)\n\n \n\n\n Args\n  dataset_or_iterator   A tf.data.Dataset or tf.data.Iterator.   \n \n\n\n Returns   A nested structure of tf.TensorShape objects matching the structure of the dataset / iterator elements and specifying the shape of the individual components.  \n  \n"}, {"name": "tf.compat.v1.data.get_output_types", "path": "compat/v1/data/get_output_types", "type": "tf.compat", "text": "tf.compat.v1.data.get_output_types Returns the output shapes for elements of the input dataset / iterator. \ntf.compat.v1.data.get_output_types(\n    dataset_or_iterator\n)\n\n \n\n\n Args\n  dataset_or_iterator   A tf.data.Dataset or tf.data.Iterator.   \n \n\n\n Returns   A nested structure of tf.DType objects objects matching the structure of dataset / iterator elements and specifying the shape of the individual components.  \n  \n"}, {"name": "tf.compat.v1.data.Iterator", "path": "compat/v1/data/iterator", "type": "tf.compat", "text": "tf.compat.v1.data.Iterator Represents the state of iterating through a Dataset. \ntf.compat.v1.data.Iterator(\n    iterator_resource, initializer, output_types, output_shapes, output_classes\n)\n\n \n\n\n Args\n  iterator_resource   A tf.resource scalar tf.Tensor representing the iterator.  \n  initializer   A tf.Operation that should be run to initialize this iterator.  \n  output_types   A nested structure of tf.DType objects corresponding to each component of an element of this iterator.  \n  output_shapes   A nested structure of tf.TensorShape objects corresponding to each component of an element of this iterator.  \n  output_classes   A nested structure of Python type objects corresponding to each component of an element of this iterator.   \n \n\n\n Attributes\n  element_spec  \n \n  initializer   A tf.Operation that should be run to initialize this iterator.  \n  output_classes   Returns the class of each component of an element of this iterator. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.compat.v1.data.get_output_classes(iterator). The expected values are tf.Tensor and tf.sparse.SparseTensor. \n \n  output_shapes   Returns the shape of each component of an element of this iterator. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.compat.v1.data.get_output_shapes(iterator). \n \n  output_types   Returns the type of each component of an element of this iterator. (deprecated)Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.compat.v1.data.get_output_types(iterator). \n   Methods from_string_handle View source \n@staticmethod\nfrom_string_handle(\n    string_handle, output_types, output_shapes=None, output_classes=None\n)\n Creates a new, uninitialized Iterator based on the given handle. This method allows you to define a \"feedable\" iterator where you can choose between concrete iterators by feeding a value in a tf.Session.run call. In that case, string_handle would be a tf.compat.v1.placeholder, and you would feed it with the value of tf.data.Iterator.string_handle in each step. For example, if you had two iterators that marked the current position in a training dataset and a test dataset, you could choose which to use in each step as follows: train_iterator = tf.data.Dataset(...).make_one_shot_iterator()\ntrain_iterator_handle = sess.run(train_iterator.string_handle())\n\ntest_iterator = tf.data.Dataset(...).make_one_shot_iterator()\ntest_iterator_handle = sess.run(test_iterator.string_handle())\n\nhandle = tf.compat.v1.placeholder(tf.string, shape=[])\niterator = tf.data.Iterator.from_string_handle(\n    handle, train_iterator.output_types)\n\nnext_element = iterator.get_next()\nloss = f(next_element)\n\ntrain_loss = sess.run(loss, feed_dict={handle: train_iterator_handle})\ntest_loss = sess.run(loss, feed_dict={handle: test_iterator_handle})\n\n \n\n\n Args\n  string_handle   A scalar tf.Tensor of type tf.string that evaluates to a handle produced by the Iterator.string_handle() method.  \n  output_types   A nested structure of tf.DType objects corresponding to each component of an element of this dataset.  \n  output_shapes   (Optional.) A nested structure of tf.TensorShape objects corresponding to each component of an element of this dataset. If omitted, each component will have an unconstrainted shape.  \n  output_classes   (Optional.) A nested structure of Python type objects corresponding to each component of an element of this iterator. If omitted, each component is assumed to be of type tf.Tensor.   \n \n\n\n Returns   An Iterator.  \n from_structure View source \n@staticmethod\nfrom_structure(\n    output_types, output_shapes=None, shared_name=None, output_classes=None\n)\n Creates a new, uninitialized Iterator with the given structure. This iterator-constructing method can be used to create an iterator that is reusable with many different datasets. The returned iterator is not bound to a particular dataset, and it has no initializer. To initialize the iterator, run the operation returned by Iterator.make_initializer(dataset). The following is an example iterator = Iterator.from_structure(tf.int64, tf.TensorShape([]))\n\ndataset_range = Dataset.range(10)\nrange_initializer = iterator.make_initializer(dataset_range)\n\ndataset_evens = dataset_range.filter(lambda x: x % 2 == 0)\nevens_initializer = iterator.make_initializer(dataset_evens)\n\n# Define a model based on the iterator; in this example, the model_fn\n# is expected to take scalar tf.int64 Tensors as input (see\n# the definition of 'iterator' above).\nprediction, loss = model_fn(iterator.get_next())\n\n# Train for `num_epochs`, where for each epoch, we first iterate over\n# dataset_range, and then iterate over dataset_evens.\nfor _ in range(num_epochs):\n  # Initialize the iterator to `dataset_range`\n  sess.run(range_initializer)\n  while True:\n    try:\n      pred, loss_val = sess.run([prediction, loss])\n    except tf.errors.OutOfRangeError:\n      break\n\n  # Initialize the iterator to `dataset_evens`\n  sess.run(evens_initializer)\n  while True:\n    try:\n      pred, loss_val = sess.run([prediction, loss])\n    except tf.errors.OutOfRangeError:\n      break\n\n \n\n\n Args\n  output_types   A nested structure of tf.DType objects corresponding to each component of an element of this dataset.  \n  output_shapes   (Optional.) A nested structure of tf.TensorShape objects corresponding to each component of an element of this dataset. If omitted, each component will have an unconstrainted shape.  \n  shared_name   (Optional.) If non-empty, this iterator will be shared under the given name across multiple sessions that share the same devices (e.g. when using a remote server).  \n  output_classes   (Optional.) A nested structure of Python type objects corresponding to each component of an element of this iterator. If omitted, each component is assumed to be of type tf.Tensor.   \n \n\n\n Returns   An Iterator.  \n\n \n\n\n Raises\n  TypeError   If the structures of output_shapes and output_types are not the same.    get_next View source \nget_next(\n    name=None\n)\n Returns a nested structure of tf.Tensors representing the next element. In graph mode, you should typically call this method once and use its result as the input to another computation. A typical loop will then call tf.Session.run on the result of that computation. The loop will terminate when the Iterator.get_next() operation raises tf.errors.OutOfRangeError. The following skeleton shows how to use this method when building a training loop: dataset = ...  # A `tf.data.Dataset` object.\niterator = dataset.make_initializable_iterator()\nnext_element = iterator.get_next()\n\n# Build a TensorFlow graph that does something with each element.\nloss = model_function(next_element)\noptimizer = ...  # A `tf.compat.v1.train.Optimizer` object.\ntrain_op = optimizer.minimize(loss)\n\nwith tf.compat.v1.Session() as sess:\n  try:\n    while True:\n      sess.run(train_op)\n  except tf.errors.OutOfRangeError:\n    pass\n\n\nNote: It is legitimate to call Iterator.get_next() multiple times, e.g. when you are distributing different elements to multiple devices in a single step. However, a common pitfall arises when users call Iterator.get_next() in each iteration of their training loop. Iterator.get_next() adds ops to the graph, and executing each op allocates resources (including threads); as a consequence, invoking it in every iteration of a training loop causes slowdown and eventual resource exhaustion. To guard against this outcome, we log a warning when the number of uses crosses a fixed threshold of suspiciousness.\n\n \n\n\n Args\n  name   (Optional.) A name for the created operation.   \n \n\n\n Returns   A nested structure of tf.Tensor objects.  \n get_next_as_optional View source \nget_next_as_optional()\n make_initializer View source \nmake_initializer(\n    dataset, name=None\n)\n Returns a tf.Operation that initializes this iterator on dataset.\n \n\n\n Args\n  dataset   A Dataset with compatible structure to this iterator.  \n  name   (Optional.) A name for the created operation.   \n \n\n\n Returns   A tf.Operation that can be run to initialize this iterator on the given dataset.  \n\n \n\n\n Raises\n  TypeError   If dataset and this iterator do not have a compatible element structure.    string_handle View source \nstring_handle(\n    name=None\n)\n Returns a string-valued tf.Tensor that represents this iterator.\n \n\n\n Args\n  name   (Optional.) A name for the created operation.   \n \n\n\n Returns   A scalar tf.Tensor of type tf.string.  \n  \n"}, {"name": "tf.compat.v1.data.make_initializable_iterator", "path": "compat/v1/data/make_initializable_iterator", "type": "tf.compat", "text": "tf.compat.v1.data.make_initializable_iterator Creates an iterator for elements of dataset. \ntf.compat.v1.data.make_initializable_iterator(\n    dataset, shared_name=None\n)\n \nNote: The returned iterator will be in an uninitialized state, and you must run the iterator.initializer operation before using it:\n\ndataset = ...\niterator = tf.compat.v1.data.make_initializable_iterator(dataset)\n# ...\nsess.run(iterator.initializer)\n\n \n\n\n Args\n  dataset   A tf.data.Dataset.  \n  shared_name   (Optional.) If non-empty, the returned iterator will be shared under the given name across multiple sessions that share the same devices (e.g. when using a remote server).   \n \n\n\n Returns   A tf.data.Iterator for elements of dataset.  \n\n \n\n\n Raises\n  RuntimeError   If eager execution is enabled.     \n"}, {"name": "tf.compat.v1.data.make_one_shot_iterator", "path": "compat/v1/data/make_one_shot_iterator", "type": "tf.compat", "text": "tf.compat.v1.data.make_one_shot_iterator Creates an iterator for elements of dataset. \ntf.compat.v1.data.make_one_shot_iterator(\n    dataset\n)\n \nNote: The returned iterator will be initialized automatically. A \"one-shot\" iterator does not support re-initialization.\n\n \n\n\n Args\n  dataset   A tf.data.Dataset.   \n \n\n\n Returns   A tf.data.Iterator for elements of dataset.  \n  \n"}, {"name": "tf.compat.v1.data.TextLineDataset", "path": "compat/v1/data/textlinedataset", "type": "tf.compat", "text": "tf.compat.v1.data.TextLineDataset A Dataset comprising lines from one or more text files. Inherits From: Dataset, Dataset \ntf.compat.v1.data.TextLineDataset(\n    filenames, compression_type=None, buffer_size=None, num_parallel_reads=None\n)\n\n \n\n\n Args\n  filenames   A tf.string tensor or tf.data.Dataset containing one or more filenames.  \n  compression_type   (Optional.) A tf.string scalar evaluating to one of \"\" (no compression), \"ZLIB\", or \"GZIP\".  \n  buffer_size   (Optional.) A tf.int64 scalar denoting the number of bytes to buffer. A value of 0 results in the default buffering values chosen based on the compression type.  \n  num_parallel_reads   (Optional.) A tf.int64 scalar representing the number of files to read in parallel. If greater than one, the records of files read in parallel are outputted in an interleaved order. If your input pipeline is I/O bottlenecked, consider setting this parameter to a value greater than one to parallelize the I/O. If None, files will be read sequentially.   \n \n\n\n Attributes\n  element_spec   The type specification of an element of this dataset. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset.element_spec\nTensorSpec(shape=(), dtype=tf.int32, name=None)\n\n \n  output_classes   Returns the class of each component of an element of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.compat.v1.data.get_output_classes(dataset). \n \n  output_shapes   Returns the shape of each component of an element of this dataset. (deprecated)Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.compat.v1.data.get_output_shapes(dataset). \n \n  output_types   Returns the type of each component of an element of this dataset. (deprecated)Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.compat.v1.data.get_output_types(dataset). \n   Methods apply View source \napply(\n    transformation_func\n)\n Applies a transformation function to this dataset. apply enables chaining of custom Dataset transformations, which are represented as functions that take one Dataset argument and return a transformed Dataset. \ndataset = tf.data.Dataset.range(100)\ndef dataset_fn(ds):\n  return ds.filter(lambda x: x < 5)\ndataset = dataset.apply(dataset_fn)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2, 3, 4]\n\n \n\n\n Args\n  transformation_func   A function that takes one Dataset argument and returns a Dataset.   \n \n\n\n Returns\n  Dataset   The Dataset returned by applying transformation_func to this dataset.    as_numpy_iterator View source \nas_numpy_iterator()\n Returns an iterator which converts all elements of the dataset to numpy. Use as_numpy_iterator to inspect the content of your dataset. To see element shapes and types, print dataset elements directly instead of using as_numpy_iterator. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nfor element in dataset:\n  print(element)\ntf.Tensor(1, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(3, shape=(), dtype=int32)\n This method requires that you are running in eager mode and the dataset's element_spec contains only TensorSpec components. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nfor element in dataset.as_numpy_iterator():\n  print(element)\n1\n2\n3\n \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nprint(list(dataset.as_numpy_iterator()))\n[1, 2, 3]\n as_numpy_iterator() will preserve the nested structure of dataset elements. \ndataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),\n                                              'b': [5, 6]})\nlist(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5},\n                                      {'a': (2, 4), 'b': 6}]\nTrue\n\n \n\n\n Returns   An iterable over the elements of the dataset, with their tensors converted to numpy arrays.  \n\n \n\n\n Raises\n  TypeError   if an element contains a non-Tensor value.  \n  RuntimeError   if eager execution is not enabled.    batch View source \nbatch(\n    batch_size, drop_remainder=False\n)\n Combines consecutive elements of this dataset into batches. \ndataset = tf.data.Dataset.range(8)\ndataset = dataset.batch(3)\nlist(dataset.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5]), array([6, 7])]\n \ndataset = tf.data.Dataset.range(8)\ndataset = dataset.batch(3, drop_remainder=True)\nlist(dataset.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5])]\n The components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced.\n \n\n\n Args\n  batch_size   A tf.int64 scalar tf.Tensor, representing the number of consecutive elements of this dataset to combine in a single batch.  \n  drop_remainder   (Optional.) A tf.bool scalar tf.Tensor, representing whether the last batch should be dropped in the case it has fewer than batch_size elements; the default behavior is not to drop the smaller batch.   \n \n\n\n Returns\n  Dataset   A Dataset.    cache View source \ncache(\n    filename=''\n)\n Caches the elements in this dataset. The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data. \nNote: For the cache to be finalized, the input dataset must be iterated through in its entirety. Otherwise, subsequent iterations will not use cached data.\n \ndataset = tf.data.Dataset.range(5)\ndataset = dataset.map(lambda x: x**2)\ndataset = dataset.cache()\n# The first time reading through the data will generate the data using\n# `range` and `map`.\nlist(dataset.as_numpy_iterator())\n[0, 1, 4, 9, 16]\n# Subsequent iterations read from the cache.\nlist(dataset.as_numpy_iterator())\n[0, 1, 4, 9, 16]\n When caching to a file, the cached data will persist across runs. Even the first iteration through the data will read from the cache file. Changing the input pipeline before the call to .cache() will have no effect until the cache file is removed or the filename is changed. \ndataset = tf.data.Dataset.range(5)\ndataset = dataset.cache(\"/path/to/file\")  # doctest: +SKIP\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[0, 1, 2, 3, 4]\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.cache(\"/path/to/file\")  # Same file! # doctest: +SKIP\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[0, 1, 2, 3, 4]\n \nNote: cache will produce exactly the same elements during each iteration through the dataset. If you wish to randomize the iteration order, make sure to call shuffle after calling cache.\n\n \n\n\n Args\n  filename   A tf.string scalar tf.Tensor, representing the name of a directory on the filesystem to use for caching elements in this Dataset. If a filename is not provided, the dataset will be cached in memory.   \n \n\n\n Returns\n  Dataset   A Dataset.    cardinality View source \ncardinality()\n Returns the cardinality of the dataset, if known. cardinality may return tf.data.INFINITE_CARDINALITY if the dataset contains an infinite number of elements or tf.data.UNKNOWN_CARDINALITY if the analysis fails to determine the number of elements in the dataset (e.g. when the dataset source is a file). \ndataset = tf.data.Dataset.range(42)\nprint(dataset.cardinality().numpy())\n42\ndataset = dataset.repeat()\ncardinality = dataset.cardinality()\nprint((cardinality == tf.data.INFINITE_CARDINALITY).numpy())\nTrue\ndataset = dataset.filter(lambda x: True)\ncardinality = dataset.cardinality()\nprint((cardinality == tf.data.UNKNOWN_CARDINALITY).numpy())\nTrue\n\n \n\n\n Returns   A scalar tf.int64 Tensor representing the cardinality of the dataset. If the cardinality is infinite or unknown, cardinality returns the named constants tf.data.INFINITE_CARDINALITY and tf.data.UNKNOWN_CARDINALITY respectively.  \n concatenate View source \nconcatenate(\n    dataset\n)\n Creates a Dataset by concatenating the given dataset with this dataset. \na = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\nb = tf.data.Dataset.range(4, 8)  # ==> [ 4, 5, 6, 7 ]\nds = a.concatenate(b)\nlist(ds.as_numpy_iterator())\n[1, 2, 3, 4, 5, 6, 7]\n# The input dataset and dataset to be concatenated should have the same\n# nested structures and output types.\nc = tf.data.Dataset.zip((a, b))\na.concatenate(c)\nTraceback (most recent call last):\nTypeError: Two datasets to concatenate have different types\n<dtype: 'int64'> and (tf.int64, tf.int64)\nd = tf.data.Dataset.from_tensor_slices([\"a\", \"b\", \"c\"])\na.concatenate(d)\nTraceback (most recent call last):\nTypeError: Two datasets to concatenate have different types\n<dtype: 'int64'> and <dtype: 'string'>\n\n \n\n\n Args\n  dataset   Dataset to be concatenated.   \n \n\n\n Returns\n  Dataset   A Dataset.    enumerate View source \nenumerate(\n    start=0\n)\n Enumerates the elements of this dataset. It is similar to python's enumerate. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.enumerate(start=5)\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(5, 1)\n(6, 2)\n(7, 3)\n \n# The nested structure of the input dataset determines the structure of\n# elements in the resulting dataset.\ndataset = tf.data.Dataset.from_tensor_slices([(7, 8), (9, 10)])\ndataset = dataset.enumerate()\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(0, array([7, 8], dtype=int32))\n(1, array([ 9, 10], dtype=int32))\n\n \n\n\n Args\n  start   A tf.int64 scalar tf.Tensor, representing the start value for enumeration.   \n \n\n\n Returns\n  Dataset   A Dataset.    filter View source \nfilter(\n    predicate\n)\n Filters this dataset according to predicate. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.filter(lambda x: x < 3)\nlist(dataset.as_numpy_iterator())\n[1, 2]\n# `tf.math.equal(x, y)` is required for equality comparison\ndef filter_fn(x):\n  return tf.math.equal(x, 1)\ndataset = dataset.filter(filter_fn)\nlist(dataset.as_numpy_iterator())\n[1]\n\n \n\n\n Args\n  predicate   A function mapping a dataset element to a boolean.   \n \n\n\n Returns\n  Dataset   The Dataset containing the elements of this dataset for which predicate is True.    filter_with_legacy_function View source \nfilter_with_legacy_function(\n    predicate\n)\n Filters this dataset according to predicate. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use `tf.data.Dataset.filter()\nNote: This is an escape hatch for existing uses of filter that do not work with V2 functions. New uses are strongly discouraged and existing uses should migrate to filter as this method will be removed in V2.\n\n \n\n\n Args\n  predicate   A function mapping a nested structure of tensors (having shapes and types defined by self.output_shapes and self.output_types) to a scalar tf.bool tensor.   \n \n\n\n Returns\n  Dataset   The Dataset containing the elements of this dataset for which predicate is True.    flat_map View source \nflat_map(\n    map_func\n)\n Maps map_func across this dataset and flattens the result. Use flat_map if you want to make sure that the order of your dataset stays the same. For example, to flatten a dataset of batches into a dataset of their elements: \ndataset = tf.data.Dataset.from_tensor_slices(\n               [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ndataset = dataset.flat_map(lambda x: Dataset.from_tensor_slices(x))\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n tf.data.Dataset.interleave() is a generalization of flat_map, since flat_map produces the same output as tf.data.Dataset.interleave(cycle_length=1)\n \n\n\n Args\n  map_func   A function mapping a dataset element to a dataset.   \n \n\n\n Returns\n  Dataset   A Dataset.    from_generator View source \n@staticmethod\nfrom_generator(\n    generator, output_types=None, output_shapes=None, args=None,\n    output_signature=None\n)\n Creates a Dataset whose elements are generated by generator. (deprecated arguments) Warning: SOME ARGUMENTS ARE DEPRECATED: (output_shapes, output_types). They will be removed in a future version. Instructions for updating: Use output_signature instead The generator argument must be a callable object that returns an object that supports the iter() protocol (e.g. a generator function). The elements generated by generator must be compatible with either the given output_signature argument or with the given output_types and (optionally) output_shapes arguments, whichiver was specified. The recommended way to call from_generator is to use the output_signature argument. In this case the output will be assumed to consist of objects with the classes, shapes and types defined by tf.TypeSpec objects from output_signature argument: \ndef gen():\n  ragged_tensor = tf.ragged.constant([[1, 2], [3]])\n  yield 42, ragged_tensor\n\ndataset = tf.data.Dataset.from_generator(\n     gen,\n     output_signature=(\n         tf.TensorSpec(shape=(), dtype=tf.int32),\n         tf.RaggedTensorSpec(shape=(2, None), dtype=tf.int32)))\n\nlist(dataset.take(1))\n[(<tf.Tensor: shape=(), dtype=int32, numpy=42>,\n<tf.RaggedTensor [[1, 2], [3]]>)]\n There is also a deprecated way to call from_generator by either with output_types argument alone or together with output_shapes argument. In this case the output of the function will be assumed to consist of tf.Tensor objects with with the types defined by output_types and with the shapes which are either unknown or defined by output_shapes. \nNote: The current implementation of Dataset.from_generator() uses tf.numpy_function and inherits the same constraints. In particular, it requires the dataset and iterator related operations to be placed on a device in the same process as the Python program that called Dataset.from_generator(). The body of generator will not be serialized in a GraphDef, and you should not use this method if you need to serialize your model and restore it in a different environment.\n\n\nNote: If generator depends on mutable global variables or other external state, be aware that the runtime may invoke generator multiple times (in order to support repeating the Dataset) and at any time between the call to Dataset.from_generator() and the production of the first element from the generator. Mutating global variables or external state can cause undefined behavior, and we recommend that you explicitly cache any external state in generator before calling Dataset.from_generator().\n\n \n\n\n Args\n  generator   A callable object that returns an object that supports the iter() protocol. If args is not specified, generator must take no arguments; otherwise it must take as many arguments as there are values in args.  \n  output_types   (Optional.) A nested structure of tf.DType objects corresponding to each component of an element yielded by generator.  \n  output_shapes   (Optional.) A nested structure of tf.TensorShape objects corresponding to each component of an element yielded by generator.  \n  args   (Optional.) A tuple of tf.Tensor objects that will be evaluated and passed to generator as NumPy-array arguments.  \n  output_signature   (Optional.) A nested structure of tf.TypeSpec objects corresponding to each component of an element yielded by generator.   \n \n\n\n Returns\n  Dataset   A Dataset.    from_sparse_tensor_slices View source \n@staticmethod\nfrom_sparse_tensor_slices(\n    sparse_tensor\n)\n Splits each rank-N tf.sparse.SparseTensor in this dataset row-wise. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.data.Dataset.from_tensor_slices().\n \n\n\n Args\n  sparse_tensor   A tf.sparse.SparseTensor.   \n \n\n\n Returns\n  Dataset   A Dataset of rank-(N-1) sparse tensors.    from_tensor_slices View source \n@staticmethod\nfrom_tensor_slices(\n    tensors\n)\n Creates a Dataset whose elements are slices of the given tensors. The given tensors are sliced along their first dimension. This operation preserves the structure of the input tensors, removing the first dimension of each tensor and using it as the dataset dimension. All input tensors must have the same size in their first dimensions. \n# Slicing a 1D tensor produces scalar tensor elements.\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nlist(dataset.as_numpy_iterator())\n[1, 2, 3]\n \n# Slicing a 2D tensor produces 1D tensor elements.\ndataset = tf.data.Dataset.from_tensor_slices([[1, 2], [3, 4]])\nlist(dataset.as_numpy_iterator())\n[array([1, 2], dtype=int32), array([3, 4], dtype=int32)]\n \n# Slicing a tuple of 1D tensors produces tuple elements containing\n# scalar tensors.\ndataset = tf.data.Dataset.from_tensor_slices(([1, 2], [3, 4], [5, 6]))\nlist(dataset.as_numpy_iterator())\n[(1, 3, 5), (2, 4, 6)]\n \n# Dictionary structure is also preserved.\ndataset = tf.data.Dataset.from_tensor_slices({\"a\": [1, 2], \"b\": [3, 4]})\nlist(dataset.as_numpy_iterator()) == [{'a': 1, 'b': 3},\n                                      {'a': 2, 'b': 4}]\nTrue\n \n# Two tensors can be combined into one Dataset object.\nfeatures = tf.constant([[1, 3], [2, 1], [3, 3]]) # ==> 3x2 tensor\nlabels = tf.constant(['A', 'B', 'A']) # ==> 3x1 tensor\ndataset = Dataset.from_tensor_slices((features, labels))\n# Both the features and the labels tensors can be converted\n# to a Dataset object separately and combined after.\nfeatures_dataset = Dataset.from_tensor_slices(features)\nlabels_dataset = Dataset.from_tensor_slices(labels)\ndataset = Dataset.zip((features_dataset, labels_dataset))\n# A batched feature and label set can be converted to a Dataset\n# in similar fashion.\nbatched_features = tf.constant([[[1, 3], [2, 3]],\n                                [[2, 1], [1, 2]],\n                                [[3, 3], [3, 2]]], shape=(3, 2, 2))\nbatched_labels = tf.constant([['A', 'A'],\n                              ['B', 'B'],\n                              ['A', 'B']], shape=(3, 2, 1))\ndataset = Dataset.from_tensor_slices((batched_features, batched_labels))\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(array([[1, 3],\n       [2, 3]], dtype=int32), array([[b'A'],\n       [b'A']], dtype=object))\n(array([[2, 1],\n       [1, 2]], dtype=int32), array([[b'B'],\n       [b'B']], dtype=object))\n(array([[3, 3],\n       [3, 2]], dtype=int32), array([[b'A'],\n       [b'B']], dtype=object))\n Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.\n \n\n\n Args\n  tensors   A dataset element, with each component having the same size in the first dimension.   \n \n\n\n Returns\n  Dataset   A Dataset.    from_tensors View source \n@staticmethod\nfrom_tensors(\n    tensors\n)\n Creates a Dataset with a single element, comprising the given tensors. from_tensors produces a dataset containing only a single element. To slice the input tensor into multiple elements, use from_tensor_slices instead. \ndataset = tf.data.Dataset.from_tensors([1, 2, 3])\nlist(dataset.as_numpy_iterator())\n[array([1, 2, 3], dtype=int32)]\ndataset = tf.data.Dataset.from_tensors(([1, 2, 3], 'A'))\nlist(dataset.as_numpy_iterator())\n[(array([1, 2, 3], dtype=int32), b'A')]\n \n# You can use `from_tensors` to produce a dataset which repeats\n# the same example many times.\nexample = tf.constant([1,2,3])\ndataset = tf.data.Dataset.from_tensors(example).repeat(2)\nlist(dataset.as_numpy_iterator())\n[array([1, 2, 3], dtype=int32), array([1, 2, 3], dtype=int32)]\n Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.\n \n\n\n Args\n  tensors   A dataset element.   \n \n\n\n Returns\n  Dataset   A Dataset.    interleave View source \ninterleave(\n    map_func, cycle_length=None, block_length=None, num_parallel_calls=None,\n    deterministic=None\n)\n Maps map_func across this dataset, and interleaves the results. For example, you can use Dataset.interleave() to process many input files concurrently: \n# Preprocess 4 files concurrently, and interleave blocks of 16 records\n# from each file.\nfilenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n             \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\ndataset = tf.data.Dataset.from_tensor_slices(filenames)\ndef parse_fn(filename):\n  return tf.data.Dataset.range(10)\ndataset = dataset.interleave(lambda x:\n    tf.data.TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n    cycle_length=4, block_length=16)\n The cycle_length and block_length arguments control the order in which elements are produced. cycle_length controls the number of input elements that are processed concurrently. If you set cycle_length to 1, this transformation will handle one input element at a time, and will produce identical results to tf.data.Dataset.flat_map. In general, this transformation will apply map_func to cycle_length input elements, open iterators on the returned Dataset objects, and cycle through them producing block_length consecutive elements from each iterator, and consuming the next input element each time it reaches the end of an iterator. For example: \ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n# NOTE: New lines indicate \"block\" boundaries.\ndataset = dataset.interleave(\n    lambda x: Dataset.from_tensors(x).repeat(6),\n    cycle_length=2, block_length=4)\nlist(dataset.as_numpy_iterator())\n[1, 1, 1, 1,\n 2, 2, 2, 2,\n 1, 1,\n 2, 2,\n 3, 3, 3, 3,\n 4, 4, 4, 4,\n 3, 3,\n 4, 4,\n 5, 5, 5, 5,\n 5, 5]\n \nNote: The order of elements yielded by this transformation is deterministic, as long as map_func is a pure function and deterministic=True. If map_func contains any stateful operations, the order in which that state is accessed is undefined.\n Performance can often be improved by setting num_parallel_calls so that interleave will use multiple threads to fetch elements. If determinism isn't required, it can also improve performance to set deterministic=False. \nfilenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n             \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\ndataset = tf.data.Dataset.from_tensor_slices(filenames)\ndataset = dataset.interleave(lambda x: tf.data.TFRecordDataset(x),\n    cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE,\n    deterministic=False)\n\n \n\n\n Args\n  map_func   A function mapping a dataset element to a dataset.  \n  cycle_length   (Optional.) The number of input elements that will be processed concurrently. If not set, the tf.data runtime decides what it should be based on available CPU. If num_parallel_calls is set to tf.data.AUTOTUNE, the cycle_length argument identifies the maximum degree of parallelism.  \n  block_length   (Optional.) The number of consecutive elements to produce from each input element before cycling to another input element. If not set, defaults to 1.  \n  num_parallel_calls   (Optional.) If specified, the implementation creates a threadpool, which is used to fetch inputs from cycle elements asynchronously and in parallel. The default behavior is to fetch inputs from cycle elements synchronously with no parallelism. If the value tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available CPU.  \n  deterministic   (Optional.) A boolean controlling whether determinism should be traded for performance by allowing elements to be produced out of order. If deterministic is None, the tf.data.Options.experimental_deterministic dataset option (True by default) is used to decide whether to produce elements deterministically.   \n \n\n\n Returns\n  Dataset   A Dataset.    list_files View source \n@staticmethod\nlist_files(\n    file_pattern, shuffle=None, seed=None\n)\n A dataset of all files matching one or more glob patterns. The file_pattern argument should be a small number of glob patterns. If your filenames have already been globbed, use Dataset.from_tensor_slices(filenames) instead, as re-globbing every filename with list_files may result in poor performance with remote storage systems. \nNote: The default behavior of this method is to return filenames in a non-deterministic random shuffled order. Pass a seed or shuffle=False to get results in a deterministic order.\n Example: If we had the following files on our filesystem:  /path/to/dir/a.txt /path/to/dir/b.py /path/to/dir/c.py  If we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:  /path/to/dir/b.py /path/to/dir/c.py \n \n\n\n Args\n  file_pattern   A string, a list of strings, or a tf.Tensor of string type (scalar or vector), representing the filename glob (i.e. shell wildcard) pattern(s) that will be matched.  \n  shuffle   (Optional.) If True, the file names will be shuffled randomly. Defaults to True.  \n  seed   (Optional.) A tf.int64 scalar tf.Tensor, representing the random seed that will be used to create the distribution. See tf.random.set_seed for behavior.   \n \n\n\n Returns\n  Dataset   A Dataset of strings corresponding to file names.    make_initializable_iterator View source \nmake_initializable_iterator(\n    shared_name=None\n)\n Creates an iterator for elements of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through tf.compat.v1. In all other situations -- namely, eager mode and inside tf.function -- you can consume dataset elements using for elem in dataset: ... or by explicitly creating iterator via iterator = iter(dataset) and fetching its elements via values = next(iterator). Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use tf.compat.v1.data.make_initializable_iterator(dataset) to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\nNote: The returned iterator will be in an uninitialized state, and you must run the iterator.initializer operation before using it:\n\n# Building graph ...\ndataset = ...\niterator = dataset.make_initializable_iterator()\nnext_value = iterator.get_next()  # This is a Tensor.\n\n# ... from within a session ...\nsess.run(iterator.initializer)\ntry:\n  while True:\n    value = sess.run(next_value)\n    ...\nexcept tf.errors.OutOfRangeError:\n    pass\n\n \n\n\n Args\n  shared_name   (Optional.) If non-empty, the returned iterator will be shared under the given name across multiple sessions that share the same devices (e.g. when using a remote server).   \n \n\n\n Returns   A tf.data.Iterator for elements of this dataset.  \n\n \n\n\n Raises\n  RuntimeError   If eager execution is enabled.    make_one_shot_iterator View source \nmake_one_shot_iterator()\n Creates an iterator for elements of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through tf.compat.v1. In all other situations -- namely, eager mode and inside tf.function -- you can consume dataset elements using for elem in dataset: ... or by explicitly creating iterator via iterator = iter(dataset) and fetching its elements via values = next(iterator). Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use tf.compat.v1.data.make_one_shot_iterator(dataset) to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\nNote: The returned iterator will be initialized automatically. A \"one-shot\" iterator does not currently support re-initialization. For that see make_initializable_iterator.\n Example: # Building graph ...\ndataset = ...\nnext_value = dataset.make_one_shot_iterator().get_next()\n\n# ... from within a session ...\ntry:\n  while True:\n    value = sess.run(next_value)\n    ...\nexcept tf.errors.OutOfRangeError:\n    pass\n\n \n\n\n Returns   An tf.data.Iterator for elements of this dataset.  \n map View source \nmap(\n    map_func, num_parallel_calls=None, deterministic=None\n)\n Maps map_func across the elements of this dataset. This transformation applies map_func to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. map_func can be used to change both the values and the structure of a dataset's elements. For example, adding 1 to each element, or projecting a subset of element components. \ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\ndataset = dataset.map(lambda x: x + 1)\nlist(dataset.as_numpy_iterator())\n[2, 3, 4, 5, 6]\n The input signature of map_func is determined by the structure of each element in this dataset. \ndataset = Dataset.range(5)\n# `map_func` takes a single argument of type `tf.Tensor` with the same\n# shape and dtype.\nresult = dataset.map(lambda x: x + 1)\n \n# Each element is a tuple containing two `tf.Tensor` objects.\nelements = [(1, \"foo\"), (2, \"bar\"), (3, \"baz\")]\ndataset = tf.data.Dataset.from_generator(\n    lambda: elements, (tf.int32, tf.string))\n# `map_func` takes two arguments of type `tf.Tensor`. This function\n# projects out just the first component.\nresult = dataset.map(lambda x_int, y_str: x_int)\nlist(result.as_numpy_iterator())\n[1, 2, 3]\n \n# Each element is a dictionary mapping strings to `tf.Tensor` objects.\nelements =  ([{\"a\": 1, \"b\": \"foo\"},\n              {\"a\": 2, \"b\": \"bar\"},\n              {\"a\": 3, \"b\": \"baz\"}])\ndataset = tf.data.Dataset.from_generator(\n    lambda: elements, {\"a\": tf.int32, \"b\": tf.string})\n# `map_func` takes a single argument of type `dict` with the same keys\n# as the elements.\nresult = dataset.map(lambda d: str(d[\"a\"]) + d[\"b\"])\n The value or values returned by map_func determine the structure of each element in the returned dataset. \ndataset = tf.data.Dataset.range(3)\n# `map_func` returns two `tf.Tensor` objects.\ndef g(x):\n  return tf.constant(37.0), tf.constant([\"Foo\", \"Bar\", \"Baz\"])\nresult = dataset.map(g)\nresult.element_spec\n(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(3,), dtype=tf.string, name=None))\n# Python primitives, lists, and NumPy arrays are implicitly converted to\n# `tf.Tensor`.\ndef h(x):\n  return 37.0, [\"Foo\", \"Bar\"], np.array([1.0, 2.0], dtype=np.float64)\nresult = dataset.map(h)\nresult.element_spec\n(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(2,), dtype=tf.string, name=None), TensorSpec(shape=(2,), dtype=tf.float64, name=None))\n# `map_func` can return nested structures.\ndef i(x):\n  return (37.0, [42, 16]), \"foo\"\nresult = dataset.map(i)\nresult.element_spec\n((TensorSpec(shape=(), dtype=tf.float32, name=None),\n  TensorSpec(shape=(2,), dtype=tf.int32, name=None)),\n TensorSpec(shape=(), dtype=tf.string, name=None))\n map_func can accept as arguments and return any type of dataset element. Note that irrespective of the context in which map_func is defined (eager vs. graph), tf.data traces the function and executes it as a graph. To use Python code inside of the function you have a few options: 1) Rely on AutoGraph to convert Python code into an equivalent graph computation. The downside of this approach is that AutoGraph can convert some but not all Python code. 2) Use tf.py_function, which allows you to write arbitrary Python code but will generally result in worse performance than 1). For example: \nd = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n# transform a string tensor to upper case string using a Python function\ndef upper_case_fn(t: tf.Tensor):\n  return t.numpy().decode('utf-8').upper()\nd = d.map(lambda x: tf.py_function(func=upper_case_fn,\n          inp=[x], Tout=tf.string))\nlist(d.as_numpy_iterator())\n[b'HELLO', b'WORLD']\n 3) Use tf.numpy_function, which also allows you to write arbitrary Python code. Note that tf.py_function accepts tf.Tensor whereas tf.numpy_function accepts numpy arrays and returns only numpy arrays. For example: \nd = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\ndef upper_case_fn(t: np.ndarray):\n  return t.decode('utf-8').upper()\nd = d.map(lambda x: tf.numpy_function(func=upper_case_fn,\n          inp=[x], Tout=tf.string))\nlist(d.as_numpy_iterator())\n[b'HELLO', b'WORLD']\n Note that the use of tf.numpy_function and tf.py_function in general precludes the possibility of executing user-defined transformations in parallel (because of Python GIL). Performance can often be improved by setting num_parallel_calls so that map will use multiple threads to process elements. If deterministic order isn't required, it can also improve performance to set deterministic=False. \ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\ndataset = dataset.map(lambda x: x + 1,\n    num_parallel_calls=tf.data.AUTOTUNE,\n    deterministic=False)\n\n \n\n\n Args\n  map_func   A function mapping a dataset element to another dataset element.  \n  num_parallel_calls   (Optional.) A tf.int32 scalar tf.Tensor, representing the number elements to process asynchronously in parallel. If not specified, elements will be processed sequentially. If the value tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available CPU.  \n  deterministic   (Optional.) A boolean controlling whether determinism should be traded for performance by allowing elements to be produced out of order. If deterministic is None, the tf.data.Options.experimental_deterministic dataset option (True by default) is used to decide whether to produce elements deterministically.   \n \n\n\n Returns\n  Dataset   A Dataset.    map_with_legacy_function View source \nmap_with_legacy_function(\n    map_func, num_parallel_calls=None, deterministic=None\n)\n Maps map_func across the elements of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use `tf.data.Dataset.map()\nNote: This is an escape hatch for existing uses of map that do not work with V2 functions. New uses are strongly discouraged and existing uses should migrate to map as this method will be removed in V2.\n\n \n\n\n Args\n  map_func   A function mapping a nested structure of tensors (having shapes and types defined by self.output_shapes and self.output_types) to another nested structure of tensors.  \n  num_parallel_calls   (Optional.) A tf.int32 scalar tf.Tensor, representing the number elements to process asynchronously in parallel. If not specified, elements will be processed sequentially. If the value tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available CPU.  \n  deterministic   (Optional.) A boolean controlling whether determinism should be traded for performance by allowing elements to be produced out of order. If deterministic is None, the tf.data.Options.experimental_deterministic dataset option (True by default) is used to decide whether to produce elements deterministically.   \n \n\n\n Returns\n  Dataset   A Dataset.    options View source \noptions()\n Returns the options for this dataset and its inputs.\n \n\n\n Returns   A tf.data.Options object representing the dataset options.  \n padded_batch View source \npadded_batch(\n    batch_size, padded_shapes=None, padding_values=None, drop_remainder=False\n)\n Combines consecutive elements of this dataset into padded batches. This transformation combines multiple consecutive elements of the input dataset into a single element. Like tf.data.Dataset.batch, the components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced. Unlike tf.data.Dataset.batch, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in padded_shapes. The padded_shapes argument determines the resulting shape for each dimension of each component in an output element:  If the dimension is a constant, the component will be padded out to that length in that dimension. If the dimension is unknown, the component will be padded out to the maximum length of all elements in that dimension.  \nA = (tf.data.Dataset\n     .range(1, 5, output_type=tf.int32)\n     .map(lambda x: tf.fill([x], x)))\n# Pad to the smallest per-batch size that fits all elements.\nB = A.padded_batch(2)\nfor element in B.as_numpy_iterator():\n  print(element)\n[[1 0]\n [2 2]]\n[[3 3 3 0]\n [4 4 4 4]]\n# Pad to a fixed size.\nC = A.padded_batch(2, padded_shapes=5)\nfor element in C.as_numpy_iterator():\n  print(element)\n[[1 0 0 0 0]\n [2 2 0 0 0]]\n[[3 3 3 0 0]\n [4 4 4 4 0]]\n# Pad with a custom value.\nD = A.padded_batch(2, padded_shapes=5, padding_values=-1)\nfor element in D.as_numpy_iterator():\n  print(element)\n[[ 1 -1 -1 -1 -1]\n [ 2  2 -1 -1 -1]]\n[[ 3  3  3 -1 -1]\n [ 4  4  4  4 -1]]\n# Components of nested elements can be padded independently.\nelements = [([1, 2, 3], [10]),\n            ([4, 5], [11, 12])]\ndataset = tf.data.Dataset.from_generator(\n    lambda: iter(elements), (tf.int32, tf.int32))\n# Pad the first component of the tuple to length 4, and the second\n# component to the smallest size that fits.\ndataset = dataset.padded_batch(2,\n    padded_shapes=([4], [None]),\n    padding_values=(-1, 100))\nlist(dataset.as_numpy_iterator())\n[(array([[ 1,  2,  3, -1], [ 4,  5, -1, -1]], dtype=int32),\n  array([[ 10, 100], [ 11,  12]], dtype=int32))]\n# Pad with a single value and multiple components.\nE = tf.data.Dataset.zip((A, A)).padded_batch(2, padding_values=-1)\nfor element in E.as_numpy_iterator():\n  print(element)\n(array([[ 1, -1],\n       [ 2,  2]], dtype=int32), array([[ 1, -1],\n       [ 2,  2]], dtype=int32))\n(array([[ 3,  3,  3, -1],\n       [ 4,  4,  4,  4]], dtype=int32), array([[ 3,  3,  3, -1],\n       [ 4,  4,  4,  4]], dtype=int32))\n See also tf.data.experimental.dense_to_sparse_batch, which combines elements that may have different shapes into a tf.sparse.SparseTensor.\n \n\n\n Args\n  batch_size   A tf.int64 scalar tf.Tensor, representing the number of consecutive elements of this dataset to combine in a single batch.  \n  padded_shapes   (Optional.) A nested structure of tf.TensorShape or tf.int64 vector tensor-like objects representing the shape to which the respective component of each input element should be padded prior to batching. Any unknown dimensions will be padded to the maximum size of that dimension in each batch. If unset, all dimensions of all components are padded to the maximum size in the batch. padded_shapes must be set if any component has an unknown rank.  \n  padding_values   (Optional.) A nested structure of scalar-shaped tf.Tensor, representing the padding values to use for the respective components. None represents that the nested structure should be padded with default values. Defaults are 0 for numeric types and the empty string for string types. The padding_values should have the same structure as the input dataset. If padding_values is a single element and the input dataset has multiple components, then the same padding_values will be used to pad every component of the dataset. If padding_values is a scalar, then its value will be broadcasted to match the shape of each component.  \n  drop_remainder   (Optional.) A tf.bool scalar tf.Tensor, representing whether the last batch should be dropped in the case it has fewer than batch_size elements; the default behavior is not to drop the smaller batch.   \n \n\n\n Returns\n  Dataset   A Dataset.   \n \n\n\n Raises\n  ValueError   If a component has an unknown rank, and the padded_shapes argument is not set.    prefetch View source \nprefetch(\n    buffer_size\n)\n Creates a Dataset that prefetches elements from this dataset. Most dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements. \nNote: Like other Dataset methods, prefetch operates on the elements of the input dataset. It has no concept of examples vs. batches. examples.prefetch(2) will prefetch two elements (2 examples), while examples.batch(20).prefetch(2) will prefetch 2 elements (2 batches, of 20 examples each).\n \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.prefetch(2)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2]\n\n \n\n\n Args\n  buffer_size   A tf.int64 scalar tf.Tensor, representing the maximum number of elements that will be buffered when prefetching.   \n \n\n\n Returns\n  Dataset   A Dataset.    range View source \n@staticmethod\nrange(\n    *args, **kwargs\n)\n Creates a Dataset of a step-separated range of values. \nlist(Dataset.range(5).as_numpy_iterator())\n[0, 1, 2, 3, 4]\nlist(Dataset.range(2, 5).as_numpy_iterator())\n[2, 3, 4]\nlist(Dataset.range(1, 5, 2).as_numpy_iterator())\n[1, 3]\nlist(Dataset.range(1, 5, -2).as_numpy_iterator())\n[]\nlist(Dataset.range(5, 1).as_numpy_iterator())\n[]\nlist(Dataset.range(5, 1, -2).as_numpy_iterator())\n[5, 3]\nlist(Dataset.range(2, 5, output_type=tf.int32).as_numpy_iterator())\n[2, 3, 4]\nlist(Dataset.range(1, 5, 2, output_type=tf.float32).as_numpy_iterator())\n[1.0, 3.0]\n\n \n\n\n Args\n  *args   follows the same semantics as python's xrange. len(args) == 1 -> start = 0, stop = args[0], step = 1. len(args) == 2 -> start = args[0], stop = args[1], step = 1. len(args) == 3 -> start = args[0], stop = args[1], step = args[2].  \n  **kwargs    output_type: Its expected dtype. (Optional, default: tf.int64). \n\n  \n \n\n\n Returns\n  Dataset   A RangeDataset.   \n \n\n\n Raises\n  ValueError   if len(args) == 0.    reduce View source \nreduce(\n    initial_state, reduce_func\n)\n Reduces the input dataset to a single element. The transformation calls reduce_func successively on every element of the input dataset until the dataset is exhausted, aggregating information in its internal state. The initial_state argument is used for the initial state and the final state is returned as the result. \ntf.data.Dataset.range(5).reduce(np.int64(0), lambda x, _: x + 1).numpy()\n5\ntf.data.Dataset.range(5).reduce(np.int64(0), lambda x, y: x + y).numpy()\n10\n\n \n\n\n Args\n  initial_state   An element representing the initial state of the transformation.  \n  reduce_func   A function that maps (old_state, input_element) to new_state. It must take two arguments and return a new element The structure of new_state must match the structure of initial_state.   \n \n\n\n Returns   A dataset element corresponding to the final state of the transformation.  \n repeat View source \nrepeat(\n    count=None\n)\n Repeats this dataset so each original value is seen count times. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.repeat(3)\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 1, 2, 3, 1, 2, 3]\n \nNote: If this dataset is a function of global state (e.g. a random number generator), then different repetitions may produce different elements.\n\n \n\n\n Args\n  count   (Optional.) A tf.int64 scalar tf.Tensor, representing the number of times the dataset should be repeated. The default behavior (if count is None or -1) is for the dataset be repeated indefinitely.   \n \n\n\n Returns\n  Dataset   A Dataset.    shard View source \nshard(\n    num_shards, index\n)\n Creates a Dataset that includes only 1/num_shards of this dataset. shard is deterministic. The Dataset produced by A.shard(n, i) will contain all elements of A whose index mod n = i. \nA = tf.data.Dataset.range(10)\nB = A.shard(num_shards=3, index=0)\nlist(B.as_numpy_iterator())\n[0, 3, 6, 9]\nC = A.shard(num_shards=3, index=1)\nlist(C.as_numpy_iterator())\n[1, 4, 7]\nD = A.shard(num_shards=3, index=2)\nlist(D.as_numpy_iterator())\n[2, 5, 8]\n This dataset operator is very useful when running distributed training, as it allows each worker to read a unique subset. When reading a single input file, you can shard elements as follows: d = tf.data.TFRecordDataset(input_file)\nd = d.shard(num_workers, worker_index)\nd = d.repeat(num_epochs)\nd = d.shuffle(shuffle_buffer_size)\nd = d.map(parser_fn, num_parallel_calls=num_map_threads)\n Important caveats:  Be sure to shard before you use any randomizing operator (such as shuffle). Generally it is best if the shard operator is used early in the dataset pipeline. For example, when reading from a set of TFRecord files, shard before converting the dataset to input samples. This avoids reading every file on every worker. The following is an example of an efficient sharding strategy within a complete pipeline:  d = Dataset.list_files(pattern)\nd = d.shard(num_workers, worker_index)\nd = d.repeat(num_epochs)\nd = d.shuffle(shuffle_buffer_size)\nd = d.interleave(tf.data.TFRecordDataset,\n                 cycle_length=num_readers, block_length=1)\nd = d.map(parser_fn, num_parallel_calls=num_map_threads)\n\n \n\n\n Args\n  num_shards   A tf.int64 scalar tf.Tensor, representing the number of shards operating in parallel.  \n  index   A tf.int64 scalar tf.Tensor, representing the worker index.   \n \n\n\n Returns\n  Dataset   A Dataset.   \n \n\n\n Raises\n  InvalidArgumentError   if num_shards or index are illegal values. \nNote: error checking is done on a best-effort basis, and errors aren't guaranteed to be caught upon dataset creation. (e.g. providing in a placeholder tensor bypasses the early checking, and will instead result in an error during a session.run call.) \n\n   shuffle View source \nshuffle(\n    buffer_size, seed=None, reshuffle_each_iteration=None\n)\n Randomly shuffles the elements of this dataset. This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required. For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then shuffle will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer. reshuffle_each_iteration controls whether the shuffle order should be different for each epoch. In TF 1.X, the idiomatic way to create epochs was through the repeat transformation: \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=True)\ndataset = dataset.repeat(2)  # doctest: +SKIP\n[1, 0, 2, 1, 2, 0]\n \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=False)\ndataset = dataset.repeat(2)  # doctest: +SKIP\n[1, 0, 2, 1, 0, 2]\n In TF 2.0, tf.data.Dataset objects are Python iterables which makes it possible to also create epochs through Python iteration: \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=True)\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 0, 2]\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 2, 0]\n \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=False)\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 0, 2]\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 0, 2]\n\n \n\n\n Args\n  buffer_size   A tf.int64 scalar tf.Tensor, representing the number of elements from this dataset from which the new dataset will sample.  \n  seed   (Optional.) A tf.int64 scalar tf.Tensor, representing the random seed that will be used to create the distribution. See tf.random.set_seed for behavior.  \n  reshuffle_each_iteration   (Optional.) A boolean, which if true indicates that the dataset should be pseudorandomly reshuffled each time it is iterated over. (Defaults to True.)   \n \n\n\n Returns\n  Dataset   A Dataset.    skip View source \nskip(\n    count\n)\n Creates a Dataset that skips count elements from this dataset. \ndataset = tf.data.Dataset.range(10)\ndataset = dataset.skip(7)\nlist(dataset.as_numpy_iterator())\n[7, 8, 9]\n\n \n\n\n Args\n  count   A tf.int64 scalar tf.Tensor, representing the number of elements of this dataset that should be skipped to form the new dataset. If count is greater than the size of this dataset, the new dataset will contain no elements. If count is -1, skips the entire dataset.   \n \n\n\n Returns\n  Dataset   A Dataset.    take View source \ntake(\n    count\n)\n Creates a Dataset with at most count elements from this dataset. \ndataset = tf.data.Dataset.range(10)\ndataset = dataset.take(3)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2]\n\n \n\n\n Args\n  count   A tf.int64 scalar tf.Tensor, representing the number of elements of this dataset that should be taken to form the new dataset. If count is -1, or if count is greater than the size of this dataset, the new dataset will contain all elements of this dataset.   \n \n\n\n Returns\n  Dataset   A Dataset.    unbatch View source \nunbatch()\n Splits elements of a dataset into multiple elements. For example, if elements of the dataset are shaped [B, a0, a1, ...], where B may vary for each input element, then for each element in the dataset, the unbatched dataset will contain B consecutive elements of shape [a0, a1, ...]. \nelements = [ [1, 2, 3], [1, 2], [1, 2, 3, 4] ]\ndataset = tf.data.Dataset.from_generator(lambda: elements, tf.int64)\ndataset = dataset.unbatch()\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 1, 2, 1, 2, 3, 4]\n \nNote: unbatch requires a data copy to slice up the batched tensor into smaller, unbatched tensors. When optimizing performance, try to avoid unnecessary usage of unbatch.\n\n \n\n\n Returns   A Dataset.  \n window View source \nwindow(\n    size, shift=None, stride=1, drop_remainder=False\n)\n Combines (nests of) input elements into a dataset of (nests of) windows. A \"window\" is a finite dataset of flat elements of size size (or possibly fewer if there are not enough input elements to fill the window and drop_remainder evaluates to False). The shift argument determines the number of input elements by which the window moves on each iteration. If windows and elements are both numbered starting at 0, the first element in window k will be element k * shift of the input dataset. In particular, the first element of the first window will always be the first element of the input dataset. The stride argument determines the stride of the input elements, and the shift argument determines the shift of the window. For example: \ndataset = tf.data.Dataset.range(7).window(2)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 1]\n[2, 3]\n[4, 5]\n[6]\ndataset = tf.data.Dataset.range(7).window(3, 2, 1, True)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 1, 2]\n[2, 3, 4]\n[4, 5, 6]\ndataset = tf.data.Dataset.range(7).window(3, 1, 2, True)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 2, 4]\n[1, 3, 5]\n[2, 4, 6]\n Note that when the window transformation is applied to a dataset of nested elements, it produces a dataset of nested windows. \nnested = ([1, 2, 3, 4], [5, 6, 7, 8])\ndataset = tf.data.Dataset.from_tensor_slices(nested).window(2)\nfor window in dataset:\n  def to_numpy(ds):\n    return list(ds.as_numpy_iterator())\n  print(tuple(to_numpy(component) for component in window))\n([1, 2], [5, 6])\n([3, 4], [7, 8])\n \ndataset = tf.data.Dataset.from_tensor_slices({'a': [1, 2, 3, 4]})\ndataset = dataset.window(2)\nfor window in dataset:\n  def to_numpy(ds):\n    return list(ds.as_numpy_iterator())\n  print({'a': to_numpy(window['a'])})\n{'a': [1, 2]}\n{'a': [3, 4]}\n\n \n\n\n Args\n  size   A tf.int64 scalar tf.Tensor, representing the number of elements of the input dataset to combine into a window. Must be positive.  \n  shift   (Optional.) A tf.int64 scalar tf.Tensor, representing the number of input elements by which the window moves in each iteration. Defaults to size. Must be positive.  \n  stride   (Optional.) A tf.int64 scalar tf.Tensor, representing the stride of the input elements in the sliding window. Must be positive. The default value of 1 means \"retain every input element\".  \n  drop_remainder   (Optional.) A tf.bool scalar tf.Tensor, representing whether the last windows should be dropped if their size is smaller than size.   \n \n\n\n Returns\n  Dataset   A Dataset of (nests of) windows -- a finite datasets of flat elements created from the (nests of) input elements.    with_options View source \nwith_options(\n    options\n)\n Returns a new tf.data.Dataset with the given options set. The options are \"global\" in the sense they apply to the entire dataset. If options are set multiple times, they are merged as long as different options do not use different non-default values. \nds = tf.data.Dataset.range(5)\nds = ds.interleave(lambda x: tf.data.Dataset.range(5),\n                   cycle_length=3,\n                   num_parallel_calls=3)\noptions = tf.data.Options()\n# This will make the interleave order non-deterministic.\noptions.experimental_deterministic = False\nds = ds.with_options(options)\n\n \n\n\n Args\n  options   A tf.data.Options that identifies the options the use.   \n \n\n\n Returns\n  Dataset   A Dataset with the given options.   \n \n\n\n Raises\n  ValueError   when an option is set more than once to a non-default value    zip View source \n@staticmethod\nzip(\n    datasets\n)\n Creates a Dataset by zipping together the given datasets. This method has similar semantics to the built-in zip() function in Python, with the main difference being that the datasets argument can be an arbitrary nested structure of Dataset objects. \n# The nested structure of the `datasets` argument determines the\n# structure of elements in the resulting dataset.\na = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\nb = tf.data.Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]\nds = tf.data.Dataset.zip((a, b))\nlist(ds.as_numpy_iterator())\n[(1, 4), (2, 5), (3, 6)]\nds = tf.data.Dataset.zip((b, a))\nlist(ds.as_numpy_iterator())\n[(4, 1), (5, 2), (6, 3)]\n\n# The `datasets` argument may contain an arbitrary number of datasets.\nc = tf.data.Dataset.range(7, 13).batch(2)  # ==> [ [7, 8],\n                                           #       [9, 10],\n                                           #       [11, 12] ]\nds = tf.data.Dataset.zip((a, b, c))\nfor element in ds.as_numpy_iterator():\n  print(element)\n(1, 4, array([7, 8]))\n(2, 5, array([ 9, 10]))\n(3, 6, array([11, 12]))\n\n# The number of elements in the resulting dataset is the same as\n# the size of the smallest dataset in `datasets`.\nd = tf.data.Dataset.range(13, 15)  # ==> [ 13, 14 ]\nds = tf.data.Dataset.zip((a, d))\nlist(ds.as_numpy_iterator())\n[(1, 13), (2, 14)]\n\n \n\n\n Args\n  datasets   A nested structure of datasets.   \n \n\n\n Returns\n  Dataset   A Dataset.    __bool__ View source \n__bool__()\n __iter__ View source \n__iter__()\n Creates an iterator for elements of this dataset. The returned iterator implements the Python Iterator protocol.\n \n\n\n Returns   An tf.data.Iterator for the elements of this dataset.  \n\n \n\n\n Raises\n  RuntimeError   If not inside of tf.function and not executing eagerly.    __len__ View source \n__len__()\n Returns the length of the dataset if it is known and finite. This method requires that you are running in eager mode, and that the length of the dataset is known and non-infinite. When the length may be unknown or infinite, or if you are running in graph mode, use tf.data.Dataset.cardinality instead.\n \n\n\n Returns   An integer representing the length of the dataset.  \n\n \n\n\n Raises\n  RuntimeError   If the dataset length is unknown or infinite, or if eager execution is not enabled.    __nonzero__ View source \n__nonzero__()\n  \n"}, {"name": "tf.compat.v1.data.TFRecordDataset", "path": "compat/v1/data/tfrecorddataset", "type": "tf.compat", "text": "tf.compat.v1.data.TFRecordDataset A Dataset comprising records from one or more TFRecord files. Inherits From: Dataset, Dataset \ntf.compat.v1.data.TFRecordDataset(\n    filenames, compression_type=None, buffer_size=None, num_parallel_reads=None\n)\n\n \n\n\n Args\n  filenames   A tf.string tensor or tf.data.Dataset containing one or more filenames.  \n  compression_type   (Optional.) A tf.string scalar evaluating to one of \"\" (no compression), \"ZLIB\", or \"GZIP\".  \n  buffer_size   (Optional.) A tf.int64 scalar representing the number of bytes in the read buffer. If your input pipeline is I/O bottlenecked, consider setting this parameter to a value 1-100 MBs. If None, a sensible default for both local and remote file systems is used.  \n  num_parallel_reads   (Optional.) A tf.int64 scalar representing the number of files to read in parallel. If greater than one, the records of files read in parallel are outputted in an interleaved order. If your input pipeline is I/O bottlenecked, consider setting this parameter to a value greater than one to parallelize the I/O. If None, files will be read sequentially.   \n \n\n\n Raises\n  TypeError   If any argument does not have the expected type.  \n  ValueError   If any argument does not have the expected shape.   \n \n\n\n Attributes\n  element_spec   The type specification of an element of this dataset. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset.element_spec\nTensorSpec(shape=(), dtype=tf.int32, name=None)\n\n \n  output_classes   Returns the class of each component of an element of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.compat.v1.data.get_output_classes(dataset). \n \n  output_shapes   Returns the shape of each component of an element of this dataset. (deprecated)Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.compat.v1.data.get_output_shapes(dataset). \n \n  output_types   Returns the type of each component of an element of this dataset. (deprecated)Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.compat.v1.data.get_output_types(dataset). \n   Methods apply View source \napply(\n    transformation_func\n)\n Applies a transformation function to this dataset. apply enables chaining of custom Dataset transformations, which are represented as functions that take one Dataset argument and return a transformed Dataset. \ndataset = tf.data.Dataset.range(100)\ndef dataset_fn(ds):\n  return ds.filter(lambda x: x < 5)\ndataset = dataset.apply(dataset_fn)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2, 3, 4]\n\n \n\n\n Args\n  transformation_func   A function that takes one Dataset argument and returns a Dataset.   \n \n\n\n Returns\n  Dataset   The Dataset returned by applying transformation_func to this dataset.    as_numpy_iterator View source \nas_numpy_iterator()\n Returns an iterator which converts all elements of the dataset to numpy. Use as_numpy_iterator to inspect the content of your dataset. To see element shapes and types, print dataset elements directly instead of using as_numpy_iterator. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nfor element in dataset:\n  print(element)\ntf.Tensor(1, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(3, shape=(), dtype=int32)\n This method requires that you are running in eager mode and the dataset's element_spec contains only TensorSpec components. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nfor element in dataset.as_numpy_iterator():\n  print(element)\n1\n2\n3\n \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nprint(list(dataset.as_numpy_iterator()))\n[1, 2, 3]\n as_numpy_iterator() will preserve the nested structure of dataset elements. \ndataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),\n                                              'b': [5, 6]})\nlist(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5},\n                                      {'a': (2, 4), 'b': 6}]\nTrue\n\n \n\n\n Returns   An iterable over the elements of the dataset, with their tensors converted to numpy arrays.  \n\n \n\n\n Raises\n  TypeError   if an element contains a non-Tensor value.  \n  RuntimeError   if eager execution is not enabled.    batch View source \nbatch(\n    batch_size, drop_remainder=False\n)\n Combines consecutive elements of this dataset into batches. \ndataset = tf.data.Dataset.range(8)\ndataset = dataset.batch(3)\nlist(dataset.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5]), array([6, 7])]\n \ndataset = tf.data.Dataset.range(8)\ndataset = dataset.batch(3, drop_remainder=True)\nlist(dataset.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5])]\n The components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced.\n \n\n\n Args\n  batch_size   A tf.int64 scalar tf.Tensor, representing the number of consecutive elements of this dataset to combine in a single batch.  \n  drop_remainder   (Optional.) A tf.bool scalar tf.Tensor, representing whether the last batch should be dropped in the case it has fewer than batch_size elements; the default behavior is not to drop the smaller batch.   \n \n\n\n Returns\n  Dataset   A Dataset.    cache View source \ncache(\n    filename=''\n)\n Caches the elements in this dataset. The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data. \nNote: For the cache to be finalized, the input dataset must be iterated through in its entirety. Otherwise, subsequent iterations will not use cached data.\n \ndataset = tf.data.Dataset.range(5)\ndataset = dataset.map(lambda x: x**2)\ndataset = dataset.cache()\n# The first time reading through the data will generate the data using\n# `range` and `map`.\nlist(dataset.as_numpy_iterator())\n[0, 1, 4, 9, 16]\n# Subsequent iterations read from the cache.\nlist(dataset.as_numpy_iterator())\n[0, 1, 4, 9, 16]\n When caching to a file, the cached data will persist across runs. Even the first iteration through the data will read from the cache file. Changing the input pipeline before the call to .cache() will have no effect until the cache file is removed or the filename is changed. \ndataset = tf.data.Dataset.range(5)\ndataset = dataset.cache(\"/path/to/file\")  # doctest: +SKIP\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[0, 1, 2, 3, 4]\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.cache(\"/path/to/file\")  # Same file! # doctest: +SKIP\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[0, 1, 2, 3, 4]\n \nNote: cache will produce exactly the same elements during each iteration through the dataset. If you wish to randomize the iteration order, make sure to call shuffle after calling cache.\n\n \n\n\n Args\n  filename   A tf.string scalar tf.Tensor, representing the name of a directory on the filesystem to use for caching elements in this Dataset. If a filename is not provided, the dataset will be cached in memory.   \n \n\n\n Returns\n  Dataset   A Dataset.    cardinality View source \ncardinality()\n Returns the cardinality of the dataset, if known. cardinality may return tf.data.INFINITE_CARDINALITY if the dataset contains an infinite number of elements or tf.data.UNKNOWN_CARDINALITY if the analysis fails to determine the number of elements in the dataset (e.g. when the dataset source is a file). \ndataset = tf.data.Dataset.range(42)\nprint(dataset.cardinality().numpy())\n42\ndataset = dataset.repeat()\ncardinality = dataset.cardinality()\nprint((cardinality == tf.data.INFINITE_CARDINALITY).numpy())\nTrue\ndataset = dataset.filter(lambda x: True)\ncardinality = dataset.cardinality()\nprint((cardinality == tf.data.UNKNOWN_CARDINALITY).numpy())\nTrue\n\n \n\n\n Returns   A scalar tf.int64 Tensor representing the cardinality of the dataset. If the cardinality is infinite or unknown, cardinality returns the named constants tf.data.INFINITE_CARDINALITY and tf.data.UNKNOWN_CARDINALITY respectively.  \n concatenate View source \nconcatenate(\n    dataset\n)\n Creates a Dataset by concatenating the given dataset with this dataset. \na = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\nb = tf.data.Dataset.range(4, 8)  # ==> [ 4, 5, 6, 7 ]\nds = a.concatenate(b)\nlist(ds.as_numpy_iterator())\n[1, 2, 3, 4, 5, 6, 7]\n# The input dataset and dataset to be concatenated should have the same\n# nested structures and output types.\nc = tf.data.Dataset.zip((a, b))\na.concatenate(c)\nTraceback (most recent call last):\nTypeError: Two datasets to concatenate have different types\n<dtype: 'int64'> and (tf.int64, tf.int64)\nd = tf.data.Dataset.from_tensor_slices([\"a\", \"b\", \"c\"])\na.concatenate(d)\nTraceback (most recent call last):\nTypeError: Two datasets to concatenate have different types\n<dtype: 'int64'> and <dtype: 'string'>\n\n \n\n\n Args\n  dataset   Dataset to be concatenated.   \n \n\n\n Returns\n  Dataset   A Dataset.    enumerate View source \nenumerate(\n    start=0\n)\n Enumerates the elements of this dataset. It is similar to python's enumerate. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.enumerate(start=5)\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(5, 1)\n(6, 2)\n(7, 3)\n \n# The nested structure of the input dataset determines the structure of\n# elements in the resulting dataset.\ndataset = tf.data.Dataset.from_tensor_slices([(7, 8), (9, 10)])\ndataset = dataset.enumerate()\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(0, array([7, 8], dtype=int32))\n(1, array([ 9, 10], dtype=int32))\n\n \n\n\n Args\n  start   A tf.int64 scalar tf.Tensor, representing the start value for enumeration.   \n \n\n\n Returns\n  Dataset   A Dataset.    filter View source \nfilter(\n    predicate\n)\n Filters this dataset according to predicate. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.filter(lambda x: x < 3)\nlist(dataset.as_numpy_iterator())\n[1, 2]\n# `tf.math.equal(x, y)` is required for equality comparison\ndef filter_fn(x):\n  return tf.math.equal(x, 1)\ndataset = dataset.filter(filter_fn)\nlist(dataset.as_numpy_iterator())\n[1]\n\n \n\n\n Args\n  predicate   A function mapping a dataset element to a boolean.   \n \n\n\n Returns\n  Dataset   The Dataset containing the elements of this dataset for which predicate is True.    filter_with_legacy_function View source \nfilter_with_legacy_function(\n    predicate\n)\n Filters this dataset according to predicate. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use `tf.data.Dataset.filter()\nNote: This is an escape hatch for existing uses of filter that do not work with V2 functions. New uses are strongly discouraged and existing uses should migrate to filter as this method will be removed in V2.\n\n \n\n\n Args\n  predicate   A function mapping a nested structure of tensors (having shapes and types defined by self.output_shapes and self.output_types) to a scalar tf.bool tensor.   \n \n\n\n Returns\n  Dataset   The Dataset containing the elements of this dataset for which predicate is True.    flat_map View source \nflat_map(\n    map_func\n)\n Maps map_func across this dataset and flattens the result. Use flat_map if you want to make sure that the order of your dataset stays the same. For example, to flatten a dataset of batches into a dataset of their elements: \ndataset = tf.data.Dataset.from_tensor_slices(\n               [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ndataset = dataset.flat_map(lambda x: Dataset.from_tensor_slices(x))\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n tf.data.Dataset.interleave() is a generalization of flat_map, since flat_map produces the same output as tf.data.Dataset.interleave(cycle_length=1)\n \n\n\n Args\n  map_func   A function mapping a dataset element to a dataset.   \n \n\n\n Returns\n  Dataset   A Dataset.    from_generator View source \n@staticmethod\nfrom_generator(\n    generator, output_types=None, output_shapes=None, args=None,\n    output_signature=None\n)\n Creates a Dataset whose elements are generated by generator. (deprecated arguments) Warning: SOME ARGUMENTS ARE DEPRECATED: (output_shapes, output_types). They will be removed in a future version. Instructions for updating: Use output_signature instead The generator argument must be a callable object that returns an object that supports the iter() protocol (e.g. a generator function). The elements generated by generator must be compatible with either the given output_signature argument or with the given output_types and (optionally) output_shapes arguments, whichiver was specified. The recommended way to call from_generator is to use the output_signature argument. In this case the output will be assumed to consist of objects with the classes, shapes and types defined by tf.TypeSpec objects from output_signature argument: \ndef gen():\n  ragged_tensor = tf.ragged.constant([[1, 2], [3]])\n  yield 42, ragged_tensor\n\ndataset = tf.data.Dataset.from_generator(\n     gen,\n     output_signature=(\n         tf.TensorSpec(shape=(), dtype=tf.int32),\n         tf.RaggedTensorSpec(shape=(2, None), dtype=tf.int32)))\n\nlist(dataset.take(1))\n[(<tf.Tensor: shape=(), dtype=int32, numpy=42>,\n<tf.RaggedTensor [[1, 2], [3]]>)]\n There is also a deprecated way to call from_generator by either with output_types argument alone or together with output_shapes argument. In this case the output of the function will be assumed to consist of tf.Tensor objects with with the types defined by output_types and with the shapes which are either unknown or defined by output_shapes. \nNote: The current implementation of Dataset.from_generator() uses tf.numpy_function and inherits the same constraints. In particular, it requires the dataset and iterator related operations to be placed on a device in the same process as the Python program that called Dataset.from_generator(). The body of generator will not be serialized in a GraphDef, and you should not use this method if you need to serialize your model and restore it in a different environment.\n\n\nNote: If generator depends on mutable global variables or other external state, be aware that the runtime may invoke generator multiple times (in order to support repeating the Dataset) and at any time between the call to Dataset.from_generator() and the production of the first element from the generator. Mutating global variables or external state can cause undefined behavior, and we recommend that you explicitly cache any external state in generator before calling Dataset.from_generator().\n\n \n\n\n Args\n  generator   A callable object that returns an object that supports the iter() protocol. If args is not specified, generator must take no arguments; otherwise it must take as many arguments as there are values in args.  \n  output_types   (Optional.) A nested structure of tf.DType objects corresponding to each component of an element yielded by generator.  \n  output_shapes   (Optional.) A nested structure of tf.TensorShape objects corresponding to each component of an element yielded by generator.  \n  args   (Optional.) A tuple of tf.Tensor objects that will be evaluated and passed to generator as NumPy-array arguments.  \n  output_signature   (Optional.) A nested structure of tf.TypeSpec objects corresponding to each component of an element yielded by generator.   \n \n\n\n Returns\n  Dataset   A Dataset.    from_sparse_tensor_slices View source \n@staticmethod\nfrom_sparse_tensor_slices(\n    sparse_tensor\n)\n Splits each rank-N tf.sparse.SparseTensor in this dataset row-wise. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.data.Dataset.from_tensor_slices().\n \n\n\n Args\n  sparse_tensor   A tf.sparse.SparseTensor.   \n \n\n\n Returns\n  Dataset   A Dataset of rank-(N-1) sparse tensors.    from_tensor_slices View source \n@staticmethod\nfrom_tensor_slices(\n    tensors\n)\n Creates a Dataset whose elements are slices of the given tensors. The given tensors are sliced along their first dimension. This operation preserves the structure of the input tensors, removing the first dimension of each tensor and using it as the dataset dimension. All input tensors must have the same size in their first dimensions. \n# Slicing a 1D tensor produces scalar tensor elements.\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nlist(dataset.as_numpy_iterator())\n[1, 2, 3]\n \n# Slicing a 2D tensor produces 1D tensor elements.\ndataset = tf.data.Dataset.from_tensor_slices([[1, 2], [3, 4]])\nlist(dataset.as_numpy_iterator())\n[array([1, 2], dtype=int32), array([3, 4], dtype=int32)]\n \n# Slicing a tuple of 1D tensors produces tuple elements containing\n# scalar tensors.\ndataset = tf.data.Dataset.from_tensor_slices(([1, 2], [3, 4], [5, 6]))\nlist(dataset.as_numpy_iterator())\n[(1, 3, 5), (2, 4, 6)]\n \n# Dictionary structure is also preserved.\ndataset = tf.data.Dataset.from_tensor_slices({\"a\": [1, 2], \"b\": [3, 4]})\nlist(dataset.as_numpy_iterator()) == [{'a': 1, 'b': 3},\n                                      {'a': 2, 'b': 4}]\nTrue\n \n# Two tensors can be combined into one Dataset object.\nfeatures = tf.constant([[1, 3], [2, 1], [3, 3]]) # ==> 3x2 tensor\nlabels = tf.constant(['A', 'B', 'A']) # ==> 3x1 tensor\ndataset = Dataset.from_tensor_slices((features, labels))\n# Both the features and the labels tensors can be converted\n# to a Dataset object separately and combined after.\nfeatures_dataset = Dataset.from_tensor_slices(features)\nlabels_dataset = Dataset.from_tensor_slices(labels)\ndataset = Dataset.zip((features_dataset, labels_dataset))\n# A batched feature and label set can be converted to a Dataset\n# in similar fashion.\nbatched_features = tf.constant([[[1, 3], [2, 3]],\n                                [[2, 1], [1, 2]],\n                                [[3, 3], [3, 2]]], shape=(3, 2, 2))\nbatched_labels = tf.constant([['A', 'A'],\n                              ['B', 'B'],\n                              ['A', 'B']], shape=(3, 2, 1))\ndataset = Dataset.from_tensor_slices((batched_features, batched_labels))\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(array([[1, 3],\n       [2, 3]], dtype=int32), array([[b'A'],\n       [b'A']], dtype=object))\n(array([[2, 1],\n       [1, 2]], dtype=int32), array([[b'B'],\n       [b'B']], dtype=object))\n(array([[3, 3],\n       [3, 2]], dtype=int32), array([[b'A'],\n       [b'B']], dtype=object))\n Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.\n \n\n\n Args\n  tensors   A dataset element, with each component having the same size in the first dimension.   \n \n\n\n Returns\n  Dataset   A Dataset.    from_tensors View source \n@staticmethod\nfrom_tensors(\n    tensors\n)\n Creates a Dataset with a single element, comprising the given tensors. from_tensors produces a dataset containing only a single element. To slice the input tensor into multiple elements, use from_tensor_slices instead. \ndataset = tf.data.Dataset.from_tensors([1, 2, 3])\nlist(dataset.as_numpy_iterator())\n[array([1, 2, 3], dtype=int32)]\ndataset = tf.data.Dataset.from_tensors(([1, 2, 3], 'A'))\nlist(dataset.as_numpy_iterator())\n[(array([1, 2, 3], dtype=int32), b'A')]\n \n# You can use `from_tensors` to produce a dataset which repeats\n# the same example many times.\nexample = tf.constant([1,2,3])\ndataset = tf.data.Dataset.from_tensors(example).repeat(2)\nlist(dataset.as_numpy_iterator())\n[array([1, 2, 3], dtype=int32), array([1, 2, 3], dtype=int32)]\n Note that if tensors contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more tf.constant operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.\n \n\n\n Args\n  tensors   A dataset element.   \n \n\n\n Returns\n  Dataset   A Dataset.    interleave View source \ninterleave(\n    map_func, cycle_length=None, block_length=None, num_parallel_calls=None,\n    deterministic=None\n)\n Maps map_func across this dataset, and interleaves the results. For example, you can use Dataset.interleave() to process many input files concurrently: \n# Preprocess 4 files concurrently, and interleave blocks of 16 records\n# from each file.\nfilenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n             \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\ndataset = tf.data.Dataset.from_tensor_slices(filenames)\ndef parse_fn(filename):\n  return tf.data.Dataset.range(10)\ndataset = dataset.interleave(lambda x:\n    tf.data.TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n    cycle_length=4, block_length=16)\n The cycle_length and block_length arguments control the order in which elements are produced. cycle_length controls the number of input elements that are processed concurrently. If you set cycle_length to 1, this transformation will handle one input element at a time, and will produce identical results to tf.data.Dataset.flat_map. In general, this transformation will apply map_func to cycle_length input elements, open iterators on the returned Dataset objects, and cycle through them producing block_length consecutive elements from each iterator, and consuming the next input element each time it reaches the end of an iterator. For example: \ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n# NOTE: New lines indicate \"block\" boundaries.\ndataset = dataset.interleave(\n    lambda x: Dataset.from_tensors(x).repeat(6),\n    cycle_length=2, block_length=4)\nlist(dataset.as_numpy_iterator())\n[1, 1, 1, 1,\n 2, 2, 2, 2,\n 1, 1,\n 2, 2,\n 3, 3, 3, 3,\n 4, 4, 4, 4,\n 3, 3,\n 4, 4,\n 5, 5, 5, 5,\n 5, 5]\n \nNote: The order of elements yielded by this transformation is deterministic, as long as map_func is a pure function and deterministic=True. If map_func contains any stateful operations, the order in which that state is accessed is undefined.\n Performance can often be improved by setting num_parallel_calls so that interleave will use multiple threads to fetch elements. If determinism isn't required, it can also improve performance to set deterministic=False. \nfilenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n             \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\ndataset = tf.data.Dataset.from_tensor_slices(filenames)\ndataset = dataset.interleave(lambda x: tf.data.TFRecordDataset(x),\n    cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE,\n    deterministic=False)\n\n \n\n\n Args\n  map_func   A function mapping a dataset element to a dataset.  \n  cycle_length   (Optional.) The number of input elements that will be processed concurrently. If not set, the tf.data runtime decides what it should be based on available CPU. If num_parallel_calls is set to tf.data.AUTOTUNE, the cycle_length argument identifies the maximum degree of parallelism.  \n  block_length   (Optional.) The number of consecutive elements to produce from each input element before cycling to another input element. If not set, defaults to 1.  \n  num_parallel_calls   (Optional.) If specified, the implementation creates a threadpool, which is used to fetch inputs from cycle elements asynchronously and in parallel. The default behavior is to fetch inputs from cycle elements synchronously with no parallelism. If the value tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available CPU.  \n  deterministic   (Optional.) A boolean controlling whether determinism should be traded for performance by allowing elements to be produced out of order. If deterministic is None, the tf.data.Options.experimental_deterministic dataset option (True by default) is used to decide whether to produce elements deterministically.   \n \n\n\n Returns\n  Dataset   A Dataset.    list_files View source \n@staticmethod\nlist_files(\n    file_pattern, shuffle=None, seed=None\n)\n A dataset of all files matching one or more glob patterns. The file_pattern argument should be a small number of glob patterns. If your filenames have already been globbed, use Dataset.from_tensor_slices(filenames) instead, as re-globbing every filename with list_files may result in poor performance with remote storage systems. \nNote: The default behavior of this method is to return filenames in a non-deterministic random shuffled order. Pass a seed or shuffle=False to get results in a deterministic order.\n Example: If we had the following files on our filesystem:  /path/to/dir/a.txt /path/to/dir/b.py /path/to/dir/c.py  If we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:  /path/to/dir/b.py /path/to/dir/c.py \n \n\n\n Args\n  file_pattern   A string, a list of strings, or a tf.Tensor of string type (scalar or vector), representing the filename glob (i.e. shell wildcard) pattern(s) that will be matched.  \n  shuffle   (Optional.) If True, the file names will be shuffled randomly. Defaults to True.  \n  seed   (Optional.) A tf.int64 scalar tf.Tensor, representing the random seed that will be used to create the distribution. See tf.random.set_seed for behavior.   \n \n\n\n Returns\n  Dataset   A Dataset of strings corresponding to file names.    make_initializable_iterator View source \nmake_initializable_iterator(\n    shared_name=None\n)\n Creates an iterator for elements of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through tf.compat.v1. In all other situations -- namely, eager mode and inside tf.function -- you can consume dataset elements using for elem in dataset: ... or by explicitly creating iterator via iterator = iter(dataset) and fetching its elements via values = next(iterator). Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use tf.compat.v1.data.make_initializable_iterator(dataset) to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\nNote: The returned iterator will be in an uninitialized state, and you must run the iterator.initializer operation before using it:\n\n# Building graph ...\ndataset = ...\niterator = dataset.make_initializable_iterator()\nnext_value = iterator.get_next()  # This is a Tensor.\n\n# ... from within a session ...\nsess.run(iterator.initializer)\ntry:\n  while True:\n    value = sess.run(next_value)\n    ...\nexcept tf.errors.OutOfRangeError:\n    pass\n\n \n\n\n Args\n  shared_name   (Optional.) If non-empty, the returned iterator will be shared under the given name across multiple sessions that share the same devices (e.g. when using a remote server).   \n \n\n\n Returns   A tf.data.Iterator for elements of this dataset.  \n\n \n\n\n Raises\n  RuntimeError   If eager execution is enabled.    make_one_shot_iterator View source \nmake_one_shot_iterator()\n Creates an iterator for elements of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through tf.compat.v1. In all other situations -- namely, eager mode and inside tf.function -- you can consume dataset elements using for elem in dataset: ... or by explicitly creating iterator via iterator = iter(dataset) and fetching its elements via values = next(iterator). Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use tf.compat.v1.data.make_one_shot_iterator(dataset) to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\nNote: The returned iterator will be initialized automatically. A \"one-shot\" iterator does not currently support re-initialization. For that see make_initializable_iterator.\n Example: # Building graph ...\ndataset = ...\nnext_value = dataset.make_one_shot_iterator().get_next()\n\n# ... from within a session ...\ntry:\n  while True:\n    value = sess.run(next_value)\n    ...\nexcept tf.errors.OutOfRangeError:\n    pass\n\n \n\n\n Returns   An tf.data.Iterator for elements of this dataset.  \n map View source \nmap(\n    map_func, num_parallel_calls=None, deterministic=None\n)\n Maps map_func across the elements of this dataset. This transformation applies map_func to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. map_func can be used to change both the values and the structure of a dataset's elements. For example, adding 1 to each element, or projecting a subset of element components. \ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\ndataset = dataset.map(lambda x: x + 1)\nlist(dataset.as_numpy_iterator())\n[2, 3, 4, 5, 6]\n The input signature of map_func is determined by the structure of each element in this dataset. \ndataset = Dataset.range(5)\n# `map_func` takes a single argument of type `tf.Tensor` with the same\n# shape and dtype.\nresult = dataset.map(lambda x: x + 1)\n \n# Each element is a tuple containing two `tf.Tensor` objects.\nelements = [(1, \"foo\"), (2, \"bar\"), (3, \"baz\")]\ndataset = tf.data.Dataset.from_generator(\n    lambda: elements, (tf.int32, tf.string))\n# `map_func` takes two arguments of type `tf.Tensor`. This function\n# projects out just the first component.\nresult = dataset.map(lambda x_int, y_str: x_int)\nlist(result.as_numpy_iterator())\n[1, 2, 3]\n \n# Each element is a dictionary mapping strings to `tf.Tensor` objects.\nelements =  ([{\"a\": 1, \"b\": \"foo\"},\n              {\"a\": 2, \"b\": \"bar\"},\n              {\"a\": 3, \"b\": \"baz\"}])\ndataset = tf.data.Dataset.from_generator(\n    lambda: elements, {\"a\": tf.int32, \"b\": tf.string})\n# `map_func` takes a single argument of type `dict` with the same keys\n# as the elements.\nresult = dataset.map(lambda d: str(d[\"a\"]) + d[\"b\"])\n The value or values returned by map_func determine the structure of each element in the returned dataset. \ndataset = tf.data.Dataset.range(3)\n# `map_func` returns two `tf.Tensor` objects.\ndef g(x):\n  return tf.constant(37.0), tf.constant([\"Foo\", \"Bar\", \"Baz\"])\nresult = dataset.map(g)\nresult.element_spec\n(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(3,), dtype=tf.string, name=None))\n# Python primitives, lists, and NumPy arrays are implicitly converted to\n# `tf.Tensor`.\ndef h(x):\n  return 37.0, [\"Foo\", \"Bar\"], np.array([1.0, 2.0], dtype=np.float64)\nresult = dataset.map(h)\nresult.element_spec\n(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(2,), dtype=tf.string, name=None), TensorSpec(shape=(2,), dtype=tf.float64, name=None))\n# `map_func` can return nested structures.\ndef i(x):\n  return (37.0, [42, 16]), \"foo\"\nresult = dataset.map(i)\nresult.element_spec\n((TensorSpec(shape=(), dtype=tf.float32, name=None),\n  TensorSpec(shape=(2,), dtype=tf.int32, name=None)),\n TensorSpec(shape=(), dtype=tf.string, name=None))\n map_func can accept as arguments and return any type of dataset element. Note that irrespective of the context in which map_func is defined (eager vs. graph), tf.data traces the function and executes it as a graph. To use Python code inside of the function you have a few options: 1) Rely on AutoGraph to convert Python code into an equivalent graph computation. The downside of this approach is that AutoGraph can convert some but not all Python code. 2) Use tf.py_function, which allows you to write arbitrary Python code but will generally result in worse performance than 1). For example: \nd = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n# transform a string tensor to upper case string using a Python function\ndef upper_case_fn(t: tf.Tensor):\n  return t.numpy().decode('utf-8').upper()\nd = d.map(lambda x: tf.py_function(func=upper_case_fn,\n          inp=[x], Tout=tf.string))\nlist(d.as_numpy_iterator())\n[b'HELLO', b'WORLD']\n 3) Use tf.numpy_function, which also allows you to write arbitrary Python code. Note that tf.py_function accepts tf.Tensor whereas tf.numpy_function accepts numpy arrays and returns only numpy arrays. For example: \nd = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\ndef upper_case_fn(t: np.ndarray):\n  return t.decode('utf-8').upper()\nd = d.map(lambda x: tf.numpy_function(func=upper_case_fn,\n          inp=[x], Tout=tf.string))\nlist(d.as_numpy_iterator())\n[b'HELLO', b'WORLD']\n Note that the use of tf.numpy_function and tf.py_function in general precludes the possibility of executing user-defined transformations in parallel (because of Python GIL). Performance can often be improved by setting num_parallel_calls so that map will use multiple threads to process elements. If deterministic order isn't required, it can also improve performance to set deterministic=False. \ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\ndataset = dataset.map(lambda x: x + 1,\n    num_parallel_calls=tf.data.AUTOTUNE,\n    deterministic=False)\n\n \n\n\n Args\n  map_func   A function mapping a dataset element to another dataset element.  \n  num_parallel_calls   (Optional.) A tf.int32 scalar tf.Tensor, representing the number elements to process asynchronously in parallel. If not specified, elements will be processed sequentially. If the value tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available CPU.  \n  deterministic   (Optional.) A boolean controlling whether determinism should be traded for performance by allowing elements to be produced out of order. If deterministic is None, the tf.data.Options.experimental_deterministic dataset option (True by default) is used to decide whether to produce elements deterministically.   \n \n\n\n Returns\n  Dataset   A Dataset.    map_with_legacy_function View source \nmap_with_legacy_function(\n    map_func, num_parallel_calls=None, deterministic=None\n)\n Maps map_func across the elements of this dataset. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use `tf.data.Dataset.map()\nNote: This is an escape hatch for existing uses of map that do not work with V2 functions. New uses are strongly discouraged and existing uses should migrate to map as this method will be removed in V2.\n\n \n\n\n Args\n  map_func   A function mapping a nested structure of tensors (having shapes and types defined by self.output_shapes and self.output_types) to another nested structure of tensors.  \n  num_parallel_calls   (Optional.) A tf.int32 scalar tf.Tensor, representing the number elements to process asynchronously in parallel. If not specified, elements will be processed sequentially. If the value tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available CPU.  \n  deterministic   (Optional.) A boolean controlling whether determinism should be traded for performance by allowing elements to be produced out of order. If deterministic is None, the tf.data.Options.experimental_deterministic dataset option (True by default) is used to decide whether to produce elements deterministically.   \n \n\n\n Returns\n  Dataset   A Dataset.    options View source \noptions()\n Returns the options for this dataset and its inputs.\n \n\n\n Returns   A tf.data.Options object representing the dataset options.  \n padded_batch View source \npadded_batch(\n    batch_size, padded_shapes=None, padding_values=None, drop_remainder=False\n)\n Combines consecutive elements of this dataset into padded batches. This transformation combines multiple consecutive elements of the input dataset into a single element. Like tf.data.Dataset.batch, the components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced. Unlike tf.data.Dataset.batch, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in padded_shapes. The padded_shapes argument determines the resulting shape for each dimension of each component in an output element:  If the dimension is a constant, the component will be padded out to that length in that dimension. If the dimension is unknown, the component will be padded out to the maximum length of all elements in that dimension.  \nA = (tf.data.Dataset\n     .range(1, 5, output_type=tf.int32)\n     .map(lambda x: tf.fill([x], x)))\n# Pad to the smallest per-batch size that fits all elements.\nB = A.padded_batch(2)\nfor element in B.as_numpy_iterator():\n  print(element)\n[[1 0]\n [2 2]]\n[[3 3 3 0]\n [4 4 4 4]]\n# Pad to a fixed size.\nC = A.padded_batch(2, padded_shapes=5)\nfor element in C.as_numpy_iterator():\n  print(element)\n[[1 0 0 0 0]\n [2 2 0 0 0]]\n[[3 3 3 0 0]\n [4 4 4 4 0]]\n# Pad with a custom value.\nD = A.padded_batch(2, padded_shapes=5, padding_values=-1)\nfor element in D.as_numpy_iterator():\n  print(element)\n[[ 1 -1 -1 -1 -1]\n [ 2  2 -1 -1 -1]]\n[[ 3  3  3 -1 -1]\n [ 4  4  4  4 -1]]\n# Components of nested elements can be padded independently.\nelements = [([1, 2, 3], [10]),\n            ([4, 5], [11, 12])]\ndataset = tf.data.Dataset.from_generator(\n    lambda: iter(elements), (tf.int32, tf.int32))\n# Pad the first component of the tuple to length 4, and the second\n# component to the smallest size that fits.\ndataset = dataset.padded_batch(2,\n    padded_shapes=([4], [None]),\n    padding_values=(-1, 100))\nlist(dataset.as_numpy_iterator())\n[(array([[ 1,  2,  3, -1], [ 4,  5, -1, -1]], dtype=int32),\n  array([[ 10, 100], [ 11,  12]], dtype=int32))]\n# Pad with a single value and multiple components.\nE = tf.data.Dataset.zip((A, A)).padded_batch(2, padding_values=-1)\nfor element in E.as_numpy_iterator():\n  print(element)\n(array([[ 1, -1],\n       [ 2,  2]], dtype=int32), array([[ 1, -1],\n       [ 2,  2]], dtype=int32))\n(array([[ 3,  3,  3, -1],\n       [ 4,  4,  4,  4]], dtype=int32), array([[ 3,  3,  3, -1],\n       [ 4,  4,  4,  4]], dtype=int32))\n See also tf.data.experimental.dense_to_sparse_batch, which combines elements that may have different shapes into a tf.sparse.SparseTensor.\n \n\n\n Args\n  batch_size   A tf.int64 scalar tf.Tensor, representing the number of consecutive elements of this dataset to combine in a single batch.  \n  padded_shapes   (Optional.) A nested structure of tf.TensorShape or tf.int64 vector tensor-like objects representing the shape to which the respective component of each input element should be padded prior to batching. Any unknown dimensions will be padded to the maximum size of that dimension in each batch. If unset, all dimensions of all components are padded to the maximum size in the batch. padded_shapes must be set if any component has an unknown rank.  \n  padding_values   (Optional.) A nested structure of scalar-shaped tf.Tensor, representing the padding values to use for the respective components. None represents that the nested structure should be padded with default values. Defaults are 0 for numeric types and the empty string for string types. The padding_values should have the same structure as the input dataset. If padding_values is a single element and the input dataset has multiple components, then the same padding_values will be used to pad every component of the dataset. If padding_values is a scalar, then its value will be broadcasted to match the shape of each component.  \n  drop_remainder   (Optional.) A tf.bool scalar tf.Tensor, representing whether the last batch should be dropped in the case it has fewer than batch_size elements; the default behavior is not to drop the smaller batch.   \n \n\n\n Returns\n  Dataset   A Dataset.   \n \n\n\n Raises\n  ValueError   If a component has an unknown rank, and the padded_shapes argument is not set.    prefetch View source \nprefetch(\n    buffer_size\n)\n Creates a Dataset that prefetches elements from this dataset. Most dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements. \nNote: Like other Dataset methods, prefetch operates on the elements of the input dataset. It has no concept of examples vs. batches. examples.prefetch(2) will prefetch two elements (2 examples), while examples.batch(20).prefetch(2) will prefetch 2 elements (2 batches, of 20 examples each).\n \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.prefetch(2)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2]\n\n \n\n\n Args\n  buffer_size   A tf.int64 scalar tf.Tensor, representing the maximum number of elements that will be buffered when prefetching.   \n \n\n\n Returns\n  Dataset   A Dataset.    range View source \n@staticmethod\nrange(\n    *args, **kwargs\n)\n Creates a Dataset of a step-separated range of values. \nlist(Dataset.range(5).as_numpy_iterator())\n[0, 1, 2, 3, 4]\nlist(Dataset.range(2, 5).as_numpy_iterator())\n[2, 3, 4]\nlist(Dataset.range(1, 5, 2).as_numpy_iterator())\n[1, 3]\nlist(Dataset.range(1, 5, -2).as_numpy_iterator())\n[]\nlist(Dataset.range(5, 1).as_numpy_iterator())\n[]\nlist(Dataset.range(5, 1, -2).as_numpy_iterator())\n[5, 3]\nlist(Dataset.range(2, 5, output_type=tf.int32).as_numpy_iterator())\n[2, 3, 4]\nlist(Dataset.range(1, 5, 2, output_type=tf.float32).as_numpy_iterator())\n[1.0, 3.0]\n\n \n\n\n Args\n  *args   follows the same semantics as python's xrange. len(args) == 1 -> start = 0, stop = args[0], step = 1. len(args) == 2 -> start = args[0], stop = args[1], step = 1. len(args) == 3 -> start = args[0], stop = args[1], step = args[2].  \n  **kwargs    output_type: Its expected dtype. (Optional, default: tf.int64). \n\n  \n \n\n\n Returns\n  Dataset   A RangeDataset.   \n \n\n\n Raises\n  ValueError   if len(args) == 0.    reduce View source \nreduce(\n    initial_state, reduce_func\n)\n Reduces the input dataset to a single element. The transformation calls reduce_func successively on every element of the input dataset until the dataset is exhausted, aggregating information in its internal state. The initial_state argument is used for the initial state and the final state is returned as the result. \ntf.data.Dataset.range(5).reduce(np.int64(0), lambda x, _: x + 1).numpy()\n5\ntf.data.Dataset.range(5).reduce(np.int64(0), lambda x, y: x + y).numpy()\n10\n\n \n\n\n Args\n  initial_state   An element representing the initial state of the transformation.  \n  reduce_func   A function that maps (old_state, input_element) to new_state. It must take two arguments and return a new element The structure of new_state must match the structure of initial_state.   \n \n\n\n Returns   A dataset element corresponding to the final state of the transformation.  \n repeat View source \nrepeat(\n    count=None\n)\n Repeats this dataset so each original value is seen count times. \ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.repeat(3)\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 1, 2, 3, 1, 2, 3]\n \nNote: If this dataset is a function of global state (e.g. a random number generator), then different repetitions may produce different elements.\n\n \n\n\n Args\n  count   (Optional.) A tf.int64 scalar tf.Tensor, representing the number of times the dataset should be repeated. The default behavior (if count is None or -1) is for the dataset be repeated indefinitely.   \n \n\n\n Returns\n  Dataset   A Dataset.    shard View source \nshard(\n    num_shards, index\n)\n Creates a Dataset that includes only 1/num_shards of this dataset. shard is deterministic. The Dataset produced by A.shard(n, i) will contain all elements of A whose index mod n = i. \nA = tf.data.Dataset.range(10)\nB = A.shard(num_shards=3, index=0)\nlist(B.as_numpy_iterator())\n[0, 3, 6, 9]\nC = A.shard(num_shards=3, index=1)\nlist(C.as_numpy_iterator())\n[1, 4, 7]\nD = A.shard(num_shards=3, index=2)\nlist(D.as_numpy_iterator())\n[2, 5, 8]\n This dataset operator is very useful when running distributed training, as it allows each worker to read a unique subset. When reading a single input file, you can shard elements as follows: d = tf.data.TFRecordDataset(input_file)\nd = d.shard(num_workers, worker_index)\nd = d.repeat(num_epochs)\nd = d.shuffle(shuffle_buffer_size)\nd = d.map(parser_fn, num_parallel_calls=num_map_threads)\n Important caveats:  Be sure to shard before you use any randomizing operator (such as shuffle). Generally it is best if the shard operator is used early in the dataset pipeline. For example, when reading from a set of TFRecord files, shard before converting the dataset to input samples. This avoids reading every file on every worker. The following is an example of an efficient sharding strategy within a complete pipeline:  d = Dataset.list_files(pattern)\nd = d.shard(num_workers, worker_index)\nd = d.repeat(num_epochs)\nd = d.shuffle(shuffle_buffer_size)\nd = d.interleave(tf.data.TFRecordDataset,\n                 cycle_length=num_readers, block_length=1)\nd = d.map(parser_fn, num_parallel_calls=num_map_threads)\n\n \n\n\n Args\n  num_shards   A tf.int64 scalar tf.Tensor, representing the number of shards operating in parallel.  \n  index   A tf.int64 scalar tf.Tensor, representing the worker index.   \n \n\n\n Returns\n  Dataset   A Dataset.   \n \n\n\n Raises\n  InvalidArgumentError   if num_shards or index are illegal values. \nNote: error checking is done on a best-effort basis, and errors aren't guaranteed to be caught upon dataset creation. (e.g. providing in a placeholder tensor bypasses the early checking, and will instead result in an error during a session.run call.) \n\n   shuffle View source \nshuffle(\n    buffer_size, seed=None, reshuffle_each_iteration=None\n)\n Randomly shuffles the elements of this dataset. This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required. For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then shuffle will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer. reshuffle_each_iteration controls whether the shuffle order should be different for each epoch. In TF 1.X, the idiomatic way to create epochs was through the repeat transformation: \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=True)\ndataset = dataset.repeat(2)  # doctest: +SKIP\n[1, 0, 2, 1, 2, 0]\n \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=False)\ndataset = dataset.repeat(2)  # doctest: +SKIP\n[1, 0, 2, 1, 0, 2]\n In TF 2.0, tf.data.Dataset objects are Python iterables which makes it possible to also create epochs through Python iteration: \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=True)\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 0, 2]\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 2, 0]\n \ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=False)\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 0, 2]\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 0, 2]\n\n \n\n\n Args\n  buffer_size   A tf.int64 scalar tf.Tensor, representing the number of elements from this dataset from which the new dataset will sample.  \n  seed   (Optional.) A tf.int64 scalar tf.Tensor, representing the random seed that will be used to create the distribution. See tf.random.set_seed for behavior.  \n  reshuffle_each_iteration   (Optional.) A boolean, which if true indicates that the dataset should be pseudorandomly reshuffled each time it is iterated over. (Defaults to True.)   \n \n\n\n Returns\n  Dataset   A Dataset.    skip View source \nskip(\n    count\n)\n Creates a Dataset that skips count elements from this dataset. \ndataset = tf.data.Dataset.range(10)\ndataset = dataset.skip(7)\nlist(dataset.as_numpy_iterator())\n[7, 8, 9]\n\n \n\n\n Args\n  count   A tf.int64 scalar tf.Tensor, representing the number of elements of this dataset that should be skipped to form the new dataset. If count is greater than the size of this dataset, the new dataset will contain no elements. If count is -1, skips the entire dataset.   \n \n\n\n Returns\n  Dataset   A Dataset.    take View source \ntake(\n    count\n)\n Creates a Dataset with at most count elements from this dataset. \ndataset = tf.data.Dataset.range(10)\ndataset = dataset.take(3)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2]\n\n \n\n\n Args\n  count   A tf.int64 scalar tf.Tensor, representing the number of elements of this dataset that should be taken to form the new dataset. If count is -1, or if count is greater than the size of this dataset, the new dataset will contain all elements of this dataset.   \n \n\n\n Returns\n  Dataset   A Dataset.    unbatch View source \nunbatch()\n Splits elements of a dataset into multiple elements. For example, if elements of the dataset are shaped [B, a0, a1, ...], where B may vary for each input element, then for each element in the dataset, the unbatched dataset will contain B consecutive elements of shape [a0, a1, ...]. \nelements = [ [1, 2, 3], [1, 2], [1, 2, 3, 4] ]\ndataset = tf.data.Dataset.from_generator(lambda: elements, tf.int64)\ndataset = dataset.unbatch()\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 1, 2, 1, 2, 3, 4]\n \nNote: unbatch requires a data copy to slice up the batched tensor into smaller, unbatched tensors. When optimizing performance, try to avoid unnecessary usage of unbatch.\n\n \n\n\n Returns   A Dataset.  \n window View source \nwindow(\n    size, shift=None, stride=1, drop_remainder=False\n)\n Combines (nests of) input elements into a dataset of (nests of) windows. A \"window\" is a finite dataset of flat elements of size size (or possibly fewer if there are not enough input elements to fill the window and drop_remainder evaluates to False). The shift argument determines the number of input elements by which the window moves on each iteration. If windows and elements are both numbered starting at 0, the first element in window k will be element k * shift of the input dataset. In particular, the first element of the first window will always be the first element of the input dataset. The stride argument determines the stride of the input elements, and the shift argument determines the shift of the window. For example: \ndataset = tf.data.Dataset.range(7).window(2)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 1]\n[2, 3]\n[4, 5]\n[6]\ndataset = tf.data.Dataset.range(7).window(3, 2, 1, True)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 1, 2]\n[2, 3, 4]\n[4, 5, 6]\ndataset = tf.data.Dataset.range(7).window(3, 1, 2, True)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 2, 4]\n[1, 3, 5]\n[2, 4, 6]\n Note that when the window transformation is applied to a dataset of nested elements, it produces a dataset of nested windows. \nnested = ([1, 2, 3, 4], [5, 6, 7, 8])\ndataset = tf.data.Dataset.from_tensor_slices(nested).window(2)\nfor window in dataset:\n  def to_numpy(ds):\n    return list(ds.as_numpy_iterator())\n  print(tuple(to_numpy(component) for component in window))\n([1, 2], [5, 6])\n([3, 4], [7, 8])\n \ndataset = tf.data.Dataset.from_tensor_slices({'a': [1, 2, 3, 4]})\ndataset = dataset.window(2)\nfor window in dataset:\n  def to_numpy(ds):\n    return list(ds.as_numpy_iterator())\n  print({'a': to_numpy(window['a'])})\n{'a': [1, 2]}\n{'a': [3, 4]}\n\n \n\n\n Args\n  size   A tf.int64 scalar tf.Tensor, representing the number of elements of the input dataset to combine into a window. Must be positive.  \n  shift   (Optional.) A tf.int64 scalar tf.Tensor, representing the number of input elements by which the window moves in each iteration. Defaults to size. Must be positive.  \n  stride   (Optional.) A tf.int64 scalar tf.Tensor, representing the stride of the input elements in the sliding window. Must be positive. The default value of 1 means \"retain every input element\".  \n  drop_remainder   (Optional.) A tf.bool scalar tf.Tensor, representing whether the last windows should be dropped if their size is smaller than size.   \n \n\n\n Returns\n  Dataset   A Dataset of (nests of) windows -- a finite datasets of flat elements created from the (nests of) input elements.    with_options View source \nwith_options(\n    options\n)\n Returns a new tf.data.Dataset with the given options set. The options are \"global\" in the sense they apply to the entire dataset. If options are set multiple times, they are merged as long as different options do not use different non-default values. \nds = tf.data.Dataset.range(5)\nds = ds.interleave(lambda x: tf.data.Dataset.range(5),\n                   cycle_length=3,\n                   num_parallel_calls=3)\noptions = tf.data.Options()\n# This will make the interleave order non-deterministic.\noptions.experimental_deterministic = False\nds = ds.with_options(options)\n\n \n\n\n Args\n  options   A tf.data.Options that identifies the options the use.   \n \n\n\n Returns\n  Dataset   A Dataset with the given options.   \n \n\n\n Raises\n  ValueError   when an option is set more than once to a non-default value    zip View source \n@staticmethod\nzip(\n    datasets\n)\n Creates a Dataset by zipping together the given datasets. This method has similar semantics to the built-in zip() function in Python, with the main difference being that the datasets argument can be an arbitrary nested structure of Dataset objects. \n# The nested structure of the `datasets` argument determines the\n# structure of elements in the resulting dataset.\na = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\nb = tf.data.Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]\nds = tf.data.Dataset.zip((a, b))\nlist(ds.as_numpy_iterator())\n[(1, 4), (2, 5), (3, 6)]\nds = tf.data.Dataset.zip((b, a))\nlist(ds.as_numpy_iterator())\n[(4, 1), (5, 2), (6, 3)]\n\n# The `datasets` argument may contain an arbitrary number of datasets.\nc = tf.data.Dataset.range(7, 13).batch(2)  # ==> [ [7, 8],\n                                           #       [9, 10],\n                                           #       [11, 12] ]\nds = tf.data.Dataset.zip((a, b, c))\nfor element in ds.as_numpy_iterator():\n  print(element)\n(1, 4, array([7, 8]))\n(2, 5, array([ 9, 10]))\n(3, 6, array([11, 12]))\n\n# The number of elements in the resulting dataset is the same as\n# the size of the smallest dataset in `datasets`.\nd = tf.data.Dataset.range(13, 15)  # ==> [ 13, 14 ]\nds = tf.data.Dataset.zip((a, d))\nlist(ds.as_numpy_iterator())\n[(1, 13), (2, 14)]\n\n \n\n\n Args\n  datasets   A nested structure of datasets.   \n \n\n\n Returns\n  Dataset   A Dataset.    __bool__ View source \n__bool__()\n __iter__ View source \n__iter__()\n Creates an iterator for elements of this dataset. The returned iterator implements the Python Iterator protocol.\n \n\n\n Returns   An tf.data.Iterator for the elements of this dataset.  \n\n \n\n\n Raises\n  RuntimeError   If not inside of tf.function and not executing eagerly.    __len__ View source \n__len__()\n Returns the length of the dataset if it is known and finite. This method requires that you are running in eager mode, and that the length of the dataset is known and non-infinite. When the length may be unknown or infinite, or if you are running in graph mode, use tf.data.Dataset.cardinality instead.\n \n\n\n Returns   An integer representing the length of the dataset.  \n\n \n\n\n Raises\n  RuntimeError   If the dataset length is unknown or infinite, or if eager execution is not enabled.    __nonzero__ View source \n__nonzero__()\n  \n"}, {"name": "tf.compat.v1.debugging", "path": "compat/v1/debugging", "type": "tf.compat", "text": "Module: tf.compat.v1.debugging Public API for tf.debugging namespace. Modules experimental module: Public API for tf.debugging.experimental namespace. Functions Assert(...): Asserts that the given condition is true. assert_all_finite(...): Assert that the tensor does not contain any NaN's or Inf's. assert_equal(...): Assert the condition x == y holds element-wise. assert_greater(...): Assert the condition x > y holds element-wise. assert_greater_equal(...): Assert the condition x >= y holds element-wise. assert_integer(...): Assert that x is of integer dtype. assert_less(...): Assert the condition x < y holds element-wise. assert_less_equal(...): Assert the condition x <= y holds element-wise. assert_near(...): Assert the condition x and y are close element-wise. assert_negative(...): Assert the condition x < 0 holds element-wise. assert_non_negative(...): Assert the condition x >= 0 holds element-wise. assert_non_positive(...): Assert the condition x <= 0 holds element-wise. assert_none_equal(...): Assert the condition x != y holds element-wise. assert_positive(...): Assert the condition x > 0 holds element-wise. assert_proper_iterable(...): Static assert that values is a \"proper\" iterable. assert_rank(...): Assert x has rank equal to rank. assert_rank_at_least(...): Assert x has rank equal to rank or higher. assert_rank_in(...): Assert x has rank in ranks. assert_same_float_dtype(...): Validate and return float type based on tensors and dtype. assert_scalar(...): Asserts that the given tensor is a scalar (i.e. zero-dimensional). assert_shapes(...): Assert tensor shapes and dimension size relationships between tensors. assert_type(...): Statically asserts that the given Tensor is of the specified type. check_numerics(...): Checks a tensor for NaN and Inf values. disable_check_numerics(...): Disable the eager/graph unified numerics checking mechanism. enable_check_numerics(...): Enable tensor numerics checking in an eager/graph unified fashion. get_log_device_placement(...): Get if device placements are logged. is_finite(...): Returns which elements of x are finite. is_inf(...): Returns which elements of x are Inf. is_nan(...): Returns which elements of x are NaN. is_non_decreasing(...): Returns True if x is non-decreasing. is_numeric_tensor(...): Returns True if the elements of tensor are numbers. is_strictly_increasing(...): Returns True if x is strictly increasing. set_log_device_placement(...): Set if device placements should be logged.  \n"}, {"name": "tf.compat.v1.debugging.assert_shapes", "path": "compat/v1/debugging/assert_shapes", "type": "tf.compat", "text": "tf.compat.v1.debugging.assert_shapes Assert tensor shapes and dimension size relationships between tensors. \ntf.compat.v1.debugging.assert_shapes(\n    shapes, data=None, summarize=None, message=None, name=None\n)\n This Op checks that a collection of tensors shape relationships satisfies given constraints. Example: \nn = 10\nq = 3\nd = 7\nx = tf.zeros([n,q])\ny = tf.ones([n,d])\nparam = tf.Variable([1.0, 2.0, 3.0])\nscalar = 1.0\ntf.debugging.assert_shapes([\n (x, ('N', 'Q')),\n (y, ('N', 'D')),\n (param, ('Q',)),\n (scalar, ()),\n])\n \ntf.debugging.assert_shapes([\n  (x, ('N', 'D')),\n  (y, ('N', 'D'))\n])\nTraceback (most recent call last):\n\nValueError: ...\n Example of adding a dependency to an operation: with tf.control_dependencies([tf.assert_shapes(shapes)]):\n  output = tf.matmul(x, y, transpose_a=True)\n If x, y, param or scalar does not have a shape that satisfies all specified constraints, message, as well as the first summarize entries of the first encountered violating tensor are printed, and InvalidArgumentError is raised. Size entries in the specified shapes are checked against other entries by their hash, except:  a size entry is interpreted as an explicit size if it can be parsed as an integer primitive. a size entry is interpreted as any size if it is None or '.'.  If the first entry of a shape is ... (type Ellipsis) or '*' that indicates a variable number of outer dimensions of unspecified size, i.e. the constraint applies to the inner-most dimensions only. Scalar tensors and specified shapes of length zero (excluding the 'inner-most' prefix) are both treated as having a single dimension of size one.\n \n\n\n Args\n  shapes   A list of (Tensor, shape) tuples, wherein shape is the expected shape of Tensor. See the example code above. The shape must be an iterable. Each element of the iterable can be either a concrete integer value or a string that abstractly represents the dimension. For example,  \n('N', 'Q') specifies a 2D shape wherein the first and second dimensions of shape may or may not be equal. \n('N', 'N', 'Q') specifies a 3D shape wherein the first and second dimensions are equal. \n(1, 'N') specifies a 2D shape wherein the first dimension is exactly 1 and the second dimension can be any value. Note that the abstract dimension letters take effect across different tuple elements of the list. For example, tf.debugging.assert_shapes([(x, ('N', 'A')), (y, ('N', 'B'))] asserts that both x and y are rank-2 tensors and their first dimensions are equal (N). shape can also be a tf.TensorShape. \n\n \n  data   The tensors to print out if the condition is False. Defaults to error message and first few entries of the violating tensor.  \n  summarize   Print this many entries of the tensor.  \n  message   A string to prefix to the default message.  \n  name   A name for this operation (optional). Defaults to \"assert_shapes\".   \n \n\n\n Returns   Op raising InvalidArgumentError unless all shape constraints are satisfied. If static checks determine all constraints are satisfied, a no_op is returned.  \n\n \n\n\n Raises\n  ValueError   If static checks determine any shape constraint is violated.     \n"}, {"name": "tf.compat.v1.debugging.experimental", "path": "compat/v1/debugging/experimental", "type": "tf.compat", "text": "Module: tf.compat.v1.debugging.experimental Public API for tf.debugging.experimental namespace. Functions disable_dump_debug_info(...): Disable the currently-enabled debugging dumping. enable_dump_debug_info(...): Enable dumping debugging information from a TensorFlow program.  \n"}, {"name": "tf.compat.v1.decode_csv", "path": "compat/v1/decode_csv", "type": "tf.compat", "text": "tf.compat.v1.decode_csv Convert CSV records to tensors. Each column maps to one tensor.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.io.decode_csv  \ntf.compat.v1.decode_csv(\n    records, record_defaults, field_delim=',', use_quote_delim=True,\n    name=None, na_value='', select_cols=None\n)\n RFC 4180 format is expected for the CSV records. (https://tools.ietf.org/html/rfc4180) Note that we allow leading and trailing spaces with int or float field.\n \n\n\n Args\n  records   A Tensor of type string. Each string is a record/row in the csv and all records should have the same format.  \n  record_defaults   A list of Tensor objects with specific types. Acceptable types are float32, float64, int32, int64, string. One tensor per column of the input record, with either a scalar default value for that column or an empty vector if the column is required.  \n  field_delim   An optional string. Defaults to \",\". char delimiter to separate fields in a record.  \n  use_quote_delim   An optional bool. Defaults to True. If false, treats double quotation marks as regular characters inside of the string fields (ignoring RFC 4180, Section 2, Bullet 5).  \n  name   A name for the operation (optional).  \n  na_value   Additional string to recognize as NA/NaN.  \n  select_cols   Optional sorted list of column indices to select. If specified, only this subset of columns will be parsed and returned.   \n \n\n\n Returns   A list of Tensor objects. Has the same type as record_defaults. Each tensor will have the same shape as records.  \n\n \n\n\n Raises\n  ValueError   If any of the arguments is malformed.     \n"}, {"name": "tf.compat.v1.decode_raw", "path": "compat/v1/decode_raw", "type": "tf.compat", "text": "tf.compat.v1.decode_raw Convert raw byte strings into tensors. (deprecated arguments)  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.io.decode_raw  \ntf.compat.v1.decode_raw(\n    input_bytes=None, out_type=None, little_endian=True, name=None, bytes=None\n)\n Warning: SOME ARGUMENTS ARE DEPRECATED: (bytes). They will be removed in a future version. Instructions for updating: bytes is deprecated, use input_bytes instead\n \n\n\n Args\n  input_bytes   Each element of the input Tensor is converted to an array of bytes.  \n  out_type   DType of the output. Acceptable types are half, float, double, int32, uint16, uint8, int16, int8, int64.  \n  little_endian   Whether the input_bytes data is in little-endian format. Data will be converted into host byte order if necessary.  \n  name   A name for the operation (optional).  \n  bytes   Deprecated parameter. Use input_bytes instead.   \n \n\n\n Returns   A Tensor object storing the decoded bytes.  \n  \n"}, {"name": "tf.compat.v1.delete_session_tensor", "path": "compat/v1/delete_session_tensor", "type": "tf.compat", "text": "tf.compat.v1.delete_session_tensor Delete the tensor for the given tensor handle. \ntf.compat.v1.delete_session_tensor(\n    handle, name=None\n)\n This is EXPERIMENTAL and subject to change. Delete the tensor of a given tensor handle. The tensor is produced in a previous run() and stored in the state of the session.\n \n\n\n Args\n  handle   The string representation of a persistent tensor handle.  \n  name   Optional name prefix for the return tensor.   \n \n\n\n Returns   A pair of graph elements. The first is a placeholder for feeding a tensor handle and the second is a deletion operation.  \n  \n"}, {"name": "tf.compat.v1.depth_to_space", "path": "compat/v1/depth_to_space", "type": "tf.compat", "text": "tf.compat.v1.depth_to_space DepthToSpace for tensors of type T.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.nn.depth_to_space  \ntf.compat.v1.depth_to_space(\n    input, block_size, name=None, data_format='NHWC'\n)\n Rearranges data from depth into blocks of spatial data. This is the reverse transformation of SpaceToDepth. More specifically, this op outputs a copy of the input tensor where values from the depth dimension are moved in spatial blocks to the height and width dimensions. The attr block_size indicates the input block size and how the data is moved.  Chunks of data of size block_size * block_size from depth are rearranged into non-overlapping blocks of size block_size x block_size\n The width the output tensor is input_depth * block_size, whereas the height is input_height * block_size. The Y, X coordinates within each block of the output image are determined by the high order component of the input channel index. The depth of the input tensor must be divisible by block_size * block_size.  The data_format attr specifies the layout of the input and output tensors with the following options: \"NHWC\": [ batch, height, width, channels ] \"NCHW\": [ batch, channels, height, width ] \"NCHW_VECT_C\": qint8 [ batch, channels / 4, height, width, 4 ] It is useful to consider the operation as transforming a 6-D Tensor. e.g. for data_format = NHWC, Each element in the input tensor can be specified via 6 coordinates, ordered by decreasing memory layout significance as: n,iY,iX,bY,bX,oC (where n=batch index, iX, iY means X or Y coordinates within the input image, bX, bY means coordinates within the output block, oC means output channels). The output would be the input transposed to the following layout: n,iY,bY,iX,bX,oC This operation is useful for resizing the activations between convolutions (but keeping all data), e.g. instead of pooling. It is also useful for training purely convolutional models. For example, given an input of shape [1, 1, 1, 4], data_format = \"NHWC\" and block_size = 2: x = [[[[1, 2, 3, 4]]]]\n\n This operation will output a tensor of shape [1, 2, 2, 1]: [[[[1], [2]],\n  [[3], [4]]]]\n Here, the input has a batch of 1 and each batch element has shape [1, 1, 4], the corresponding output will have 2x2 elements and will have a depth of 1 channel (1 = 4 / (block_size * block_size)). The output element shape is [2, 2, 1]. For an input tensor with larger depth, here of shape [1, 1, 1, 12], e.g. x = [[[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]]]]\n This operation, for block size of 2, will return the following tensor of shape [1, 2, 2, 3] [[[[1, 2, 3], [4, 5, 6]],\n  [[7, 8, 9], [10, 11, 12]]]]\n\n Similarly, for the following input of shape [1 2 2 4], and a block size of 2: x =  [[[[1, 2, 3, 4],\n       [5, 6, 7, 8]],\n      [[9, 10, 11, 12],\n       [13, 14, 15, 16]]]]\n the operator will return the following tensor of shape [1 4 4 1]: x = [[[ [1],   [2],  [5],  [6]],\n      [ [3],   [4],  [7],  [8]],\n      [ [9],  [10], [13],  [14]],\n      [ [11], [12], [15],  [16]]]]\n\n\n \n\n\n Args\n  input   A Tensor.  \n  block_size   An int that is >= 2. The size of the spatial block, same as in Space2Depth.  \n  data_format   An optional string from: \"NHWC\", \"NCHW\", \"NCHW_VECT_C\". Defaults to \"NHWC\".  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as input.  \n  \n"}, {"name": "tf.compat.v1.device", "path": "compat/v1/device", "type": "tf.compat", "text": "tf.compat.v1.device Wrapper for Graph.device() using the default graph. \ntf.compat.v1.device(\n    device_name_or_function\n)\n See tf.Graph.device for more details.\n \n\n\n Args\n  device_name_or_function   The device name or function to use in the context.   \n \n\n\n Returns   A context manager that specifies the default device to use for newly created ops.  \n\n \n\n\n Raises\n  RuntimeError   If eager execution is enabled and a function is passed in.     \n"}, {"name": "tf.compat.v1.DeviceSpec", "path": "compat/v1/devicespec", "type": "tf.compat", "text": "tf.compat.v1.DeviceSpec Represents a (possibly partial) specification for a TensorFlow device. Inherits From: DeviceSpec \ntf.compat.v1.DeviceSpec(\n    job=None, replica=None, task=None, device_type=None, device_index=None\n)\n DeviceSpecs are used throughout TensorFlow to describe where state is stored and computations occur. Using DeviceSpec allows you to parse device spec strings to verify their validity, merge them or compose them programmatically. Example: # Place the operations on device \"GPU:0\" in the \"ps\" job.\ndevice_spec = DeviceSpec(job=\"ps\", device_type=\"GPU\", device_index=0)\nwith tf.device(device_spec.to_string()):\n  # Both my_var and squared_var will be placed on /job:ps/device:GPU:0.\n  my_var = tf.Variable(..., name=\"my_variable\")\n  squared_var = tf.square(my_var)\n With eager execution disabled (by default in TensorFlow 1.x and by calling disable_eager_execution() in TensorFlow 2.x), the following syntax can be used: tf.compat.v1.disable_eager_execution()\n\n# Same as previous\ndevice_spec = DeviceSpec(job=\"ps\", device_type=\"GPU\", device_index=0)\n# No need of .to_string() method.\nwith tf.device(device_spec):\n  my_var = tf.Variable(..., name=\"my_variable\")\n  squared_var = tf.square(my_var)\n ```\n\nIf a `DeviceSpec` is partially specified, it will be merged with other\n`DeviceSpec`s according to the scope in which it is defined. `DeviceSpec`\ncomponents defined in inner scopes take precedence over those defined in\nouter scopes.\n\n```python\ngpu0_spec = DeviceSpec(job=\"ps\", device_type=\"GPU\", device_index=0)\nwith tf.device(DeviceSpec(job=\"train\").to_string()):\n  with tf.device(gpu0_spec.to_string()):\n    # Nodes created here will be assigned to /job:ps/device:GPU:0.\n  with tf.device(DeviceSpec(device_type=\"GPU\", device_index=1).to_string()):\n    # Nodes created here will be assigned to /job:train/device:GPU:1.\n A DeviceSpec consists of 5 components -- each of which is optionally specified:  Job: The job name. Replica: The replica index. Task: The task index. Device type: The device type string (e.g. \"CPU\" or \"GPU\"). Device index: The device index. \n \n\n\n Args\n  job   string. Optional job name.  \n  replica   int. Optional replica index.  \n  task   int. Optional task index.  \n  device_type   Optional device type string (e.g. \"CPU\" or \"GPU\")  \n  device_index   int. Optional device index. If left unspecified, device represents 'any' device_index.   \n \n\n\n Attributes\n  device_index  \n \n  device_type  \n \n  job  \n \n  replica  \n \n  task  \n   Methods from_string View source \n@classmethod\nfrom_string(\n    spec\n)\n Construct a DeviceSpec from a string.\n \n\n\n Args\n  spec   a string of the form /job:/replica:/task:/device:CPU: or /job:/replica:/task:/device:GPU: as cpu and gpu are mutually exclusive. All entries are optional. \n  \n \n\n\n Returns   A DeviceSpec.  \n make_merged_spec View source \nmake_merged_spec(\n    dev\n)\n Returns a new DeviceSpec which incorporates dev. When combining specs, dev will take precedence over the current spec. So for instance: first_spec = tf.DeviceSpec(job=0, device_type=\"CPU\")\nsecond_spec = tf.DeviceSpec(device_type=\"GPU\")\ncombined_spec = first_spec.make_merged_spec(second_spec)\n is equivalent to: combined_spec = tf.DeviceSpec(job=0, device_type=\"GPU\")\n\n \n\n\n Args\n  dev   a DeviceSpec   \n \n\n\n Returns   A new DeviceSpec which combines self and dev  \n merge_from View source \nmerge_from(\n    dev\n)\n Merge the properties of \"dev\" into this DeviceSpec. \nNote: Will be removed in TensorFlow 2.x since DeviceSpecs will become immutable.\n\n \n\n\n Args\n  dev   a DeviceSpec.    parse_from_string View source \nparse_from_string(\n    spec\n)\n Parse a DeviceSpec name into its components. 2.x behavior change: In TensorFlow 1.x, this function mutates its own state and returns itself. In 2.x, DeviceSpecs are immutable, and this function will return a DeviceSpec which contains the spec. Recommended: ```\n# my_spec and my_updated_spec are unrelated.\nmy_spec = tf.DeviceSpec.from_string(\"/CPU:0\")\nmy_updated_spec = tf.DeviceSpec.from_string(\"/GPU:0\")\nwith tf.device(my_updated_spec):\n  ...\n```\n Will work in 1.x and 2.x (though deprecated in 2.x): ```\nmy_spec = tf.DeviceSpec.from_string(\"/CPU:0\")\nmy_updated_spec = my_spec.parse_from_string(\"/GPU:0\")\nwith tf.device(my_updated_spec):\n  ...\n```\n Will NOT work in 2.x: ```\nmy_spec = tf.DeviceSpec.from_string(\"/CPU:0\")\nmy_spec.parse_from_string(\"/GPU:0\")  # <== Will not update my_spec\nwith tf.device(my_spec):\n  ...\n```\n In general, DeviceSpec.from_string should completely replace DeviceSpec.parse_from_string, and DeviceSpec.replace should completely replace setting attributes directly.\n \n\n\n Args\n  spec   an optional string of the form /job:/replica:/task:/device:CPU: or /job:/replica:/task:/device:GPU: as cpu and gpu are mutually exclusive. All entries are optional. \n  \n \n\n\n Returns   The DeviceSpec.  \n\n \n\n\n Raises\n  ValueError   if the spec was not valid.    replace View source \nreplace(\n    **kwargs\n)\n Convenience method for making a new DeviceSpec by overriding fields. For instance: my_spec = DeviceSpec=(job=\"my_job\", device=\"CPU\")\nmy_updated_spec = my_spec.replace(device=\"GPU\")\nmy_other_spec = my_spec.replace(device=None)\n\n \n\n\n Args\n  **kwargs   This method takes the same args as the DeviceSpec constructor   \n \n\n\n Returns   A DeviceSpec with the fields specified in kwargs overridden.  \n to_string View source \nto_string()\n Return a string representation of this DeviceSpec.\n \n\n\n Returns   a string of the form /job:/replica:/task:/device::. \n \n __eq__ View source \n__eq__(\n    other\n)\n Checks if the other DeviceSpec is same as the current instance, eg have same value for all the internal fields.\n \n\n\n Args\n  other   Another DeviceSpec   \n \n\n\n Returns   Return True if other is also a DeviceSpec instance and has same value as the current instance. Return False otherwise.  \n  \n"}, {"name": "tf.compat.v1.Dimension", "path": "compat/v1/dimension", "type": "tf.compat", "text": "tf.compat.v1.Dimension Represents the value of one dimension in a TensorShape. \ntf.compat.v1.Dimension(\n    value\n)\n\n \n\n\n Attributes\n  value   The value of this dimension, or None if it is unknown.    Methods assert_is_compatible_with View source \nassert_is_compatible_with(\n    other\n)\n Raises an exception if other is not compatible with this Dimension.\n \n\n\n Args\n  other   Another Dimension.   \n \n\n\n Raises\n  ValueError   If self and other are not compatible (see is_compatible_with).    is_compatible_with View source \nis_compatible_with(\n    other\n)\n Returns true if other is compatible with this Dimension. Two known Dimensions are compatible if they have the same value. An unknown Dimension is compatible with all other Dimensions.\n \n\n\n Args\n  other   Another Dimension.   \n \n\n\n Returns   True if this Dimension and other are compatible.  \n merge_with View source \nmerge_with(\n    other\n)\n Returns a Dimension that combines the information in self and other. Dimensions are combined as follows: tf.compat.v1.Dimension(n)   .merge_with(tf.compat.v1.Dimension(n))     ==\ntf.compat.v1.Dimension(n)\ntf.compat.v1.Dimension(n)   .merge_with(tf.compat.v1.Dimension(None))  ==\ntf.compat.v1.Dimension(n)\ntf.compat.v1.Dimension(None).merge_with(tf.compat.v1.Dimension(n))     ==\ntf.compat.v1.Dimension(n)\n# equivalent to tf.compat.v1.Dimension(None)\ntf.compat.v1.Dimension(None).merge_with(tf.compat.v1.Dimension(None))\n\n# raises ValueError for n != m\ntf.compat.v1.Dimension(n)   .merge_with(tf.compat.v1.Dimension(m))\n\n \n\n\n Args\n  other   Another Dimension.   \n \n\n\n Returns   A Dimension containing the combined information of self and other.  \n\n \n\n\n Raises\n  ValueError   If self and other are not compatible (see is_compatible_with).    __add__ View source \n__add__(\n    other\n)\n Returns the sum of self and other. Dimensions are summed as follows: tf.compat.v1.Dimension(m)    + tf.compat.v1.Dimension(n)     ==\ntf.compat.v1.Dimension(m + n)\ntf.compat.v1.Dimension(m)    + tf.compat.v1.Dimension(None)  # equiv. to\ntf.compat.v1.Dimension(None)\ntf.compat.v1.Dimension(None) + tf.compat.v1.Dimension(n)     # equiv. to\ntf.compat.v1.Dimension(None)\ntf.compat.v1.Dimension(None) + tf.compat.v1.Dimension(None)  # equiv. to\ntf.compat.v1.Dimension(None)\n\n \n\n\n Args\n  other   Another Dimension, or a value accepted by as_dimension.   \n \n\n\n Returns   A Dimension whose value is the sum of self and other.  \n __div__ View source \n__div__(\n    other\n)\n DEPRECATED: Use __floordiv__ via x // y instead. This function exists only for backwards compatibility purposes; new code should use __floordiv__ via the syntax x // y. Using x // y communicates clearly that the result rounds down, and is forward compatible to Python 3.\n \n\n\n Args\n  other   Another Dimension.   \n \n\n\n Returns   A Dimension whose value is the integer quotient of self and other.  \n __eq__ View source \n__eq__(\n    other\n)\n Returns true if other has the same known value as this Dimension. __floordiv__ View source \n__floordiv__(\n    other\n)\n Returns the quotient of self and other rounded down. Dimensions are divided as follows: tf.compat.v1.Dimension(m)    // tf.compat.v1.Dimension(n)     ==\ntf.compat.v1.Dimension(m // n)\ntf.compat.v1.Dimension(m)    // tf.compat.v1.Dimension(None)  # equiv. to\ntf.compat.v1.Dimension(None)\ntf.compat.v1.Dimension(None) // tf.compat.v1.Dimension(n)     # equiv. to\ntf.compat.v1.Dimension(None)\ntf.compat.v1.Dimension(None) // tf.compat.v1.Dimension(None)  # equiv. to\ntf.compat.v1.Dimension(None)\n\n \n\n\n Args\n  other   Another Dimension, or a value accepted by as_dimension.   \n \n\n\n Returns   A Dimension whose value is the integer quotient of self and other.  \n __ge__ View source \n__ge__(\n    other\n)\n Returns True if self is known to be greater than or equal to other. Dimensions are compared as follows: (tf.compat.v1.Dimension(m)    >= tf.compat.v1.Dimension(n))    == (m >= n)\n(tf.compat.v1.Dimension(m)    >= tf.compat.v1.Dimension(None)) == None\n(tf.compat.v1.Dimension(None) >= tf.compat.v1.Dimension(n))    == None\n(tf.compat.v1.Dimension(None) >= tf.compat.v1.Dimension(None)) == None\n\n \n\n\n Args\n  other   Another Dimension.   \n \n\n\n Returns   The value of self.value >= other.value if both are known, otherwise None.  \n __gt__ View source \n__gt__(\n    other\n)\n Returns True if self is known to be greater than other. Dimensions are compared as follows: (tf.compat.v1.Dimension(m)    > tf.compat.v1.Dimension(n))    == (m > n)\n(tf.compat.v1.Dimension(m)    > tf.compat.v1.Dimension(None)) == None\n(tf.compat.v1.Dimension(None) > tf.compat.v1.Dimension(n))    == None\n(tf.compat.v1.Dimension(None) > tf.compat.v1.Dimension(None)) == None\n\n \n\n\n Args\n  other   Another Dimension.   \n \n\n\n Returns   The value of self.value > other.value if both are known, otherwise None.  \n __le__ View source \n__le__(\n    other\n)\n Returns True if self is known to be less than or equal to other. Dimensions are compared as follows: (tf.compat.v1.Dimension(m)    <= tf.compat.v1.Dimension(n))    == (m <= n)\n(tf.compat.v1.Dimension(m)    <= tf.compat.v1.Dimension(None)) == None\n(tf.compat.v1.Dimension(None) <= tf.compat.v1.Dimension(n))    == None\n(tf.compat.v1.Dimension(None) <= tf.compat.v1.Dimension(None)) == None\n\n \n\n\n Args\n  other   Another Dimension.   \n \n\n\n Returns   The value of self.value <= other.value if both are known, otherwise None.  \n __lt__ View source \n__lt__(\n    other\n)\n Returns True if self is known to be less than other. Dimensions are compared as follows: (tf.compat.v1.Dimension(m)    < tf.compat.v1.Dimension(n))    == (m < n)\n(tf.compat.v1.Dimension(m)    < tf.compat.v1.Dimension(None)) == None\n(tf.compat.v1.Dimension(None) < tf.compat.v1.Dimension(n))    == None\n(tf.compat.v1.Dimension(None) < tf.compat.v1.Dimension(None)) == None\n\n \n\n\n Args\n  other   Another Dimension.   \n \n\n\n Returns   The value of self.value < other.value if both are known, otherwise None.  \n __mod__ View source \n__mod__(\n    other\n)\n Returns self modulo other. Dimension modulo are computed as follows: tf.compat.v1.Dimension(m)    % tf.compat.v1.Dimension(n)     ==\ntf.compat.v1.Dimension(m % n)\ntf.compat.v1.Dimension(m)    % tf.compat.v1.Dimension(None)  # equiv. to\ntf.compat.v1.Dimension(None)\ntf.compat.v1.Dimension(None) % tf.compat.v1.Dimension(n)     # equiv. to\ntf.compat.v1.Dimension(None)\ntf.compat.v1.Dimension(None) % tf.compat.v1.Dimension(None)  # equiv. to\ntf.compat.v1.Dimension(None)\n\n \n\n\n Args\n  other   Another Dimension, or a value accepted by as_dimension.   \n \n\n\n Returns   A Dimension whose value is self modulo other.  \n __mul__ View source \n__mul__(\n    other\n)\n Returns the product of self and other. Dimensions are summed as follows: tf.compat.v1.Dimension(m)    * tf.compat.v1.Dimension(n)     ==\ntf.compat.v1.Dimension(m * n)\ntf.compat.v1.Dimension(m)    * tf.compat.v1.Dimension(None)  # equiv. to\ntf.compat.v1.Dimension(None)\ntf.compat.v1.Dimension(None) * tf.compat.v1.Dimension(n)     # equiv. to\ntf.compat.v1.Dimension(None)\ntf.compat.v1.Dimension(None) * tf.compat.v1.Dimension(None)  # equiv. to\ntf.compat.v1.Dimension(None)\n\n \n\n\n Args\n  other   Another Dimension, or a value accepted by as_dimension.   \n \n\n\n Returns   A Dimension whose value is the product of self and other.  \n __ne__ View source \n__ne__(\n    other\n)\n Returns true if other has a different known value from self. __radd__ View source \n__radd__(\n    other\n)\n Returns the sum of other and self.\n \n\n\n Args\n  other   Another Dimension, or a value accepted by as_dimension.   \n \n\n\n Returns   A Dimension whose value is the sum of self and other.  \n __rdiv__ View source \n__rdiv__(\n    other\n)\n Use __floordiv__ via x // y instead. This function exists only to have a better error message. Instead of: TypeError: unsupported operand type(s) for /: 'int' and 'Dimension', this function will explicitly call for usage of // instead.\n \n\n\n Args\n  other   Another Dimension.   \n \n\n\n Raises   TypeError.  \n __rfloordiv__ View source \n__rfloordiv__(\n    other\n)\n Returns the quotient of other and self rounded down.\n \n\n\n Args\n  other   Another Dimension, or a value accepted by as_dimension.   \n \n\n\n Returns   A Dimension whose value is the integer quotient of self and other.  \n __rmod__ View source \n__rmod__(\n    other\n)\n Returns other modulo self.\n \n\n\n Args\n  other   Another Dimension, or a value accepted by as_dimension.   \n \n\n\n Returns   A Dimension whose value is other modulo self.  \n __rmul__ View source \n__rmul__(\n    other\n)\n Returns the product of self and other.\n \n\n\n Args\n  other   Another Dimension, or a value accepted by as_dimension.   \n \n\n\n Returns   A Dimension whose value is the product of self and other.  \n __rsub__ View source \n__rsub__(\n    other\n)\n Returns the subtraction of self from other.\n \n\n\n Args\n  other   Another Dimension, or a value accepted by as_dimension.   \n \n\n\n Returns   A Dimension whose value is the subtraction of self from other.  \n __rtruediv__ View source \n__rtruediv__(\n    other\n)\n Use __floordiv__ via x // y instead. This function exists only to have a better error message. Instead of: TypeError: unsupported operand type(s) for /: 'int' and 'Dimension', this function will explicitly call for usage of // instead.\n \n\n\n Args\n  other   Another Dimension.   \n \n\n\n Raises   TypeError.  \n __sub__ View source \n__sub__(\n    other\n)\n Returns the subtraction of other from self. Dimensions are subtracted as follows: tf.compat.v1.Dimension(m)    - tf.compat.v1.Dimension(n)     ==\ntf.compat.v1.Dimension(m - n)\ntf.compat.v1.Dimension(m)    - tf.compat.v1.Dimension(None)  # equiv. to\ntf.compat.v1.Dimension(None)\ntf.compat.v1.Dimension(None) - tf.compat.v1.Dimension(n)     # equiv. to\ntf.compat.v1.Dimension(None)\ntf.compat.v1.Dimension(None) - tf.compat.v1.Dimension(None)  # equiv. to\ntf.compat.v1.Dimension(None)\n\n \n\n\n Args\n  other   Another Dimension, or a value accepted by as_dimension.   \n \n\n\n Returns   A Dimension whose value is the subtraction of other from self.  \n __truediv__ View source \n__truediv__(\n    other\n)\n Use __floordiv__ via x // y instead. This function exists only to have a better error message. Instead of: TypeError: unsupported operand type(s) for /: 'Dimension' and 'int', this function will explicitly call for usage of // instead.\n \n\n\n Args\n  other   Another Dimension.   \n \n\n\n Raises   TypeError.  \n  \n"}, {"name": "tf.compat.v1.disable_control_flow_v2", "path": "compat/v1/disable_control_flow_v2", "type": "tf.compat", "text": "tf.compat.v1.disable_control_flow_v2 Opts out of control flow v2. \ntf.compat.v1.disable_control_flow_v2()\n \nNote: v2 control flow is always enabled inside of tf.function. Calling this function has no effect in that case.\n If your code needs tf.disable_control_flow_v2() to be called to work properly please file a bug.  \n"}, {"name": "tf.compat.v1.disable_eager_execution", "path": "compat/v1/disable_eager_execution", "type": "tf.compat", "text": "tf.compat.v1.disable_eager_execution Disables eager execution. \ntf.compat.v1.disable_eager_execution()\n This function can only be called before any Graphs, Ops, or Tensors have been created. It can be used at the beginning of the program for complex migration projects from TensorFlow 1.x to 2.x.  \n"}, {"name": "tf.compat.v1.disable_resource_variables", "path": "compat/v1/disable_resource_variables", "type": "tf.compat", "text": "tf.compat.v1.disable_resource_variables Opts out of resource variables. (deprecated) \ntf.compat.v1.disable_resource_variables()\n Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: non-resource variables are not supported in the long term If your code needs tf.disable_resource_variables() to be called to work properly please file a bug.  \n"}, {"name": "tf.compat.v1.disable_tensor_equality", "path": "compat/v1/disable_tensor_equality", "type": "tf.compat", "text": "tf.compat.v1.disable_tensor_equality Compare Tensors by their id and be hashable. \ntf.compat.v1.disable_tensor_equality()\n This is a legacy behaviour of TensorFlow and is highly discouraged.  \n"}, {"name": "tf.compat.v1.disable_v2_behavior", "path": "compat/v1/disable_v2_behavior", "type": "tf.compat", "text": "tf.compat.v1.disable_v2_behavior Disables TensorFlow 2.x behaviors. \ntf.compat.v1.disable_v2_behavior()\n This function can be called at the beginning of the program (before Tensors, Graphs or other structures have been created, and before devices have been initialized. It switches all global behaviors that are different between TensorFlow 1.x and 2.x to behave as intended for 1.x. User can call this function to disable 2.x behavior during complex migrations.  \n"}, {"name": "tf.compat.v1.disable_v2_tensorshape", "path": "compat/v1/disable_v2_tensorshape", "type": "tf.compat", "text": "tf.compat.v1.disable_v2_tensorshape Disables the V2 TensorShape behavior and reverts to V1 behavior. \ntf.compat.v1.disable_v2_tensorshape()\n See docstring for enable_v2_tensorshape for details about the new behavior.  \n"}, {"name": "tf.compat.v1.distribute", "path": "compat/v1/distribute", "type": "tf.compat", "text": "Module: tf.compat.v1.distribute Library for running a computation across multiple devices. The intent of this library is that you can write an algorithm in a stylized way and it will be usable with a variety of different tf.distribute.Strategy implementations. Each descendant will implement a different strategy for distributing the algorithm across multiple devices/machines. Furthermore, these changes can be hidden inside the specific layers and other library classes that need special treatment to run in a distributed setting, so that most users' model definition code can run unchanged. The tf.distribute.Strategy API works the same way with eager and graph execution. Guides  TensorFlow v2.x TensorFlow v1.x  Tutorials  \nDistributed Training Tutorials The tutorials cover how to use tf.distribute.Strategy to do distributed training with native Keras APIs, custom training loops, and Esitmator APIs. They also cover how to save/load model when using tf.distribute.Strategy.\n  Glossary  \nData parallelism is where we run multiple copies of the model on different slices of the input data. This is in contrast to model parallelism where we divide up a single copy of a model across multiple devices. Note: we only support data parallelism for now, but hope to add support for model parallelism in the future. A device is a CPU or accelerator (e.g. GPUs, TPUs) on some machine that TensorFlow can run operations on (see e.g. tf.device). You may have multiple devices on a single machine, or be connected to devices on multiple machines. Devices used to run computations are called worker devices. Devices used to store variables are parameter devices. For some strategies, such as tf.distribute.MirroredStrategy, the worker and parameter devices will be the same (see mirrored variables below). For others they will be different. For example, tf.distribute.experimental.CentralStorageStrategy puts the variables on a single device (which may be a worker device or may be the CPU), and tf.distribute.experimental.ParameterServerStrategy puts the variables on separate machines called parameter servers (see below). A replica is one copy of the model, running on one slice of the input data. Right now each replica is executed on its own worker device, but once we add support for model parallelism a replica may span multiple worker devices. A host is the CPU device on a machine with worker devices, typically used for running input pipelines. A worker is defined to be the physical machine(s) containing the physical devices (e.g. GPUs, TPUs) on which the replicated computation is executed. A worker may contain one or more replicas, but contains at least one replica. Typically one worker will correspond to one machine, but in the case of very large models with model parallelism, one worker may span multiple machines. We typically run one input pipeline per worker, feeding all the replicas on that worker. \nSynchronous, or more commonly sync, training is where the updates from each replica are aggregated together before updating the model variables. This is in contrast to asynchronous, or async training, where each replica updates the model variables independently. You may also have replicas partitioned into groups which are in sync within each group but async between groups. Parameter servers: These are machines that hold a single copy of parameters/variables, used by some strategies (right now just tf.distribute.experimental.ParameterServerStrategy). All replicas that want to operate on a variable retrieve it at the beginning of a step and send an update to be applied at the end of the step. These can in priniciple support either sync or async training, but right now we only have support for async training with parameter servers. Compare to tf.distribute.experimental.CentralStorageStrategy, which puts all variables on a single device on the same machine (and does sync training), and tf.distribute.MirroredStrategy, which mirrors variables to multiple devices (see below). \nReplica context vs. Cross-replica context vs Update context A replica context applies when you execute the computation function that was called with strategy.run. Conceptually, you're in replica context when executing the computation function that is being replicated. An update context is entered in a tf.distribute.StrategyExtended.update call. An cross-replica context is entered when you enter a strategy.scope. This is useful for calling tf.distribute.Strategy methods which operate across the replicas (like reduce_to()). By default you start in a replica context (the \"default single replica context\") and then some methods can switch you back and forth.\n \nDistributed value: Distributed value is represented by the base class tf.distribute.DistributedValues. tf.distribute.DistributedValues is useful to represent values on multiple devices, and it contains a map from replica id to values. Two representative kinds of tf.distribute.DistributedValues are \"PerReplica\" and \"Mirrored\" values. \"PerReplica\" values exist on the worker devices, with a different value for each replica. They are produced by iterating through a distributed dataset returned by tf.distribute.Strategy.experimental_distribute_dataset and tf.distribute.Strategy.distribute_datasets_from_function. They are also the typical result returned by tf.distribute.Strategy.run. \"Mirrored\" values are like \"PerReplica\" values, except we know that the value on all replicas are the same. We can safely read a \"Mirrored\" value in a cross-replica context by using the value on any replica.\n Unwrapping and merging: Consider calling a function fn on multiple replicas, like strategy.run(fn, args=[w]) with an argument w that is a tf.distribute.DistributedValues. This means w will have a map taking replica id 0 to w0, replica id 1 to w1, etc. strategy.run() unwraps w before calling fn, so it calls fn(w0) on device d0, fn(w1) on device d1, etc. It then merges the return values from fn(), which leads to one common object if the returned values are the same object from every replica, or a DistributedValues object otherwise. Reductions and all-reduce: A reduction is a method of aggregating multiple values into one value, like \"sum\" or \"mean\". If a strategy is doing sync training, we will perform a reduction on the gradients to a parameter from all replicas before applying the update. All-reduce is an algorithm for performing a reduction on values from multiple devices and making the result available on all of those devices. Mirrored variables: These are variables that are created on multiple devices, where we keep the variables in sync by applying the same updates to every copy. Mirrored variables are created with tf.Variable(...synchronization=tf.VariableSynchronization.ON_WRITE...). Normally they are only used in synchronous training. \nSyncOnRead variables SyncOnRead variables are created by tf.Variable(...synchronization=tf.VariableSynchronization.ON_READ...), and they are created on multiple devices. In replica context, each component variable on the local replica can perform reads and writes without synchronization with each other. When the SyncOnRead variable is read in cross-replica context, the values from component variables are aggregated and returned. SyncOnRead variables bring a lot of custom configuration difficulty to the underlying logic, so we do not encourage users to instantiate and use SyncOnRead variable on their own. We have mainly used SyncOnRead variables for use cases such as batch norm and metrics. For performance reasons, we often don't need to keep these statistics in sync every step and they can be accumulated on each replica independently. The only time we want to sync them is reporting or checkpointing, which typically happens in cross-replica context. SyncOnRead variables are also often used by advanced users who want to control when variable values are aggregated. For example, users sometimes want to maintain gradients independently on each replica for a couple of steps without aggregation.\n \nDistribute-aware layers Layers are generally called in a replica context, except when defining a Keras functional model. tf.distribute.in_cross_replica_context will let you determine which case you are in. If in a replica context, the tf.distribute.get_replica_context function will return the default replica context outside a strategy scope, None within a strategy scope, and a tf.distribute.ReplicaContext object inside a strategy scope and within a tf.distribute.Strategy.run function. The ReplicaContext object has an all_reduce method for aggregating across all replicas.\n  Note that we provide a default version of tf.distribute.Strategy that is used when no other strategy is in scope, that provides the same API with reasonable default behavior. Modules cluster_resolver module: Library imports for ClusterResolvers. experimental module: Public API for tf.distribute.experimental namespace. Classes class CrossDeviceOps: Base class for cross-device reduction and broadcasting algorithms. class HierarchicalCopyAllReduce: Hierarchical copy all-reduce implementation of CrossDeviceOps. class InputContext: A class wrapping information needed by an input function. class InputReplicationMode: Replication mode for input function. class MirroredStrategy: Synchronous training across multiple replicas on one machine. class NcclAllReduce: NCCL all-reduce implementation of CrossDeviceOps. class OneDeviceStrategy: A distribution strategy for running on a single device. class ReduceOp: Indicates how a set of values should be reduced. class ReductionToOneDevice: A CrossDeviceOps implementation that copies values to one device to reduce. class ReplicaContext: A class with a collection of APIs that can be called in a replica context. class RunOptions: Run options for strategy.run. class Server: An in-process TensorFlow server, for use in distributed training. class Strategy: A list of devices with a state & compute distribution policy. class StrategyExtended: Additional APIs for algorithms that need to be distribution-aware. Functions experimental_set_strategy(...): Set a tf.distribute.Strategy as current without with strategy.scope(). get_loss_reduction(...): tf.distribute.ReduceOp corresponding to the last loss reduction. get_replica_context(...): Returns the current tf.distribute.ReplicaContext or None. get_strategy(...): Returns the current tf.distribute.Strategy object. has_strategy(...): Return if there is a current non-default tf.distribute.Strategy. in_cross_replica_context(...): Returns True if in a cross-replica context.  \n"}, {"name": "tf.compat.v1.distribute.cluster_resolver", "path": "compat/v1/distribute/cluster_resolver", "type": "tf.compat", "text": "Module: tf.compat.v1.distribute.cluster_resolver Library imports for ClusterResolvers. This library contains all implementations of ClusterResolvers. ClusterResolvers are a way of specifying cluster information for distributed execution. Built on top of existing ClusterSpec framework, ClusterResolvers are a way for TensorFlow to communicate with various cluster management systems (e.g. GCE, AWS, etc...). Classes class ClusterResolver: Abstract class for all implementations of ClusterResolvers. class GCEClusterResolver: ClusterResolver for Google Compute Engine. class KubernetesClusterResolver: ClusterResolver for Kubernetes. class SimpleClusterResolver: Simple implementation of ClusterResolver that accepts all attributes. class SlurmClusterResolver: ClusterResolver for system with Slurm workload manager. class TFConfigClusterResolver: Implementation of a ClusterResolver which reads the TF_CONFIG EnvVar. class TPUClusterResolver: Cluster Resolver for Google Cloud TPUs. class UnionResolver: Performs a union on underlying ClusterResolvers.  \n"}, {"name": "tf.compat.v1.distribute.experimental", "path": "compat/v1/distribute/experimental", "type": "tf.compat", "text": "Module: tf.compat.v1.distribute.experimental Public API for tf.distribute.experimental namespace. Classes class CentralStorageStrategy: A one-machine strategy that puts all variables on a single device. class CollectiveCommunication: Cross device communication implementation. class CollectiveHints: Hints for collective operations like AllReduce. class CommunicationImplementation: Cross device communication implementation. class CommunicationOptions: Options for cross device communications like All-reduce. class MultiWorkerMirroredStrategy: A distribution strategy for synchronous training on multiple workers. class ParameterServerStrategy: An asynchronous multi-worker parameter server tf.distribute strategy. class TPUStrategy: TPU distribution strategy implementation.  \n"}, {"name": "tf.compat.v1.distribute.experimental.CentralStorageStrategy", "path": "compat/v1/distribute/experimental/centralstoragestrategy", "type": "tf.compat", "text": "tf.compat.v1.distribute.experimental.CentralStorageStrategy A one-machine strategy that puts all variables on a single device. Inherits From: Strategy \ntf.compat.v1.distribute.experimental.CentralStorageStrategy(\n    compute_devices=None, parameter_device=None\n)\n Variables are assigned to local CPU or the only GPU. If there is more than one GPU, compute operations (other than variable update operations) will be replicated across all GPUs. For Example: strategy = tf.distribute.experimental.CentralStorageStrategy()\n# Create a dataset\nds = tf.data.Dataset.range(5).batch(2)\n# Distribute that dataset\ndist_dataset = strategy.experimental_distribute_dataset(ds)\n\nwith strategy.scope():\n  @tf.function\n  def train_step(val):\n    return val + 1\n\n  # Iterate over the distributed dataset\n  for x in dist_dataset:\n    # process dataset elements\n    strategy.run(train_step, args=(x,))\n\n \n\n\n Attributes\n  cluster_resolver   Returns the cluster resolver associated with this strategy. In general, when using a multi-worker tf.distribute strategy such as tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy(), there is a tf.distribute.cluster_resolver.ClusterResolver associated with the strategy used, and such an instance is returned by this property. Strategies that intend to have an associated tf.distribute.cluster_resolver.ClusterResolver must set the relevant attribute, or override this property; otherwise, None is returned by default. Those strategies should also provide information regarding what is returned by this property. Single-worker strategies usually do not have a tf.distribute.cluster_resolver.ClusterResolver, and in those cases this property will return None. The tf.distribute.cluster_resolver.ClusterResolver may be useful when the user needs to access information such as the cluster spec, task type or task id. For example, \nos.environ['TF_CONFIG'] = json.dumps({\n'cluster': {\n'worker': [\"localhost:12345\", \"localhost:23456\"],\n'ps': [\"localhost:34567\"]\n},\n'task': {'type': 'worker', 'index': 0}\n})\n\n# This implicitly uses TF_CONFIG for the cluster and current task info.\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n\n...\n\nif strategy.cluster_resolver.task_type == 'worker':\n# Perform something that's only applicable on workers. Since we set this\n# as a worker above, this block will run on this particular instance.\nelif strategy.cluster_resolver.task_type == 'ps':\n# Perform something that's only applicable on parameter servers. Since we\n# set this as a worker above, this block will not run on this particular\n# instance.\n For more information, please see tf.distribute.cluster_resolver.ClusterResolver's API docstring. \n \n  extended   tf.distribute.StrategyExtended with additional methods.  \n  num_replicas_in_sync   Returns number of replicas over which gradients are aggregated.    Methods distribute_datasets_from_function View source \ndistribute_datasets_from_function(\n    dataset_fn, options=None\n)\n Distributes tf.data.Dataset instances created by calls to dataset_fn. The argument dataset_fn that users pass in is an input function that has a tf.distribute.InputContext argument and returns a tf.data.Dataset instance. It is expected that the returned dataset from dataset_fn is already batched by per-replica batch size (i.e. global batch size divided by the number of replicas in sync) and sharded. tf.distribute.Strategy.distribute_datasets_from_function does not batch or shard the tf.data.Dataset instance returned from the input function. dataset_fn will be called on the CPU device of each of the workers and each generates a dataset where every replica on that worker will dequeue one batch of inputs (i.e. if a worker has two replicas, two batches will be dequeued from the Dataset every step). This method can be used for several purposes. First, it allows you to specify your own batching and sharding logic. (In contrast, tf.distribute.experimental_distribute_dataset does batching and sharding for you.) For example, where experimental_distribute_dataset is unable to shard the input files, this method might be used to manually shard the dataset (avoiding the slow fallback behavior in experimental_distribute_dataset). In cases where the dataset is infinite, this sharding can be done by creating dataset replicas that differ only in their random seed. The dataset_fn should take an tf.distribute.InputContext instance where information about batching and input replication can be accessed. You can use element_spec property of the tf.distribute.DistributedDataset returned by this API to query the tf.TypeSpec of the elements returned by the iterator. This can be used to set the input_signature property of a tf.function. Follow tf.distribute.DistributedDataset.element_spec to see an example. Key Point: The tf.data.Dataset returned by dataset_fn should have a per-replica batch size, unlike experimental_distribute_dataset, which uses the global batch size. This may be computed using input_context.get_per_replica_batch_size.\nNote: If you are using TPUStrategy, the order in which the data is processed by the workers when using tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function is not guaranteed. This is typically required if you are using tf.distribute to scale prediction. You can however insert an index for each element in the batch and order outputs accordingly. Refer to this snippet for an example of how to order outputs.\n\n\nNote: Stateful dataset transformations are currently not supported with tf.distribute.experimental_distribute_dataset or tf.distribute.distribute_datasets_from_function. Any stateful ops that the dataset may have are currently ignored. For example, if your dataset has a map_fn that uses tf.random.uniform to rotate an image, then you have a dataset graph that depends on state (i.e the random seed) on the local machine where the python process is being executed.\n For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input). If you are interested in last partial batch handling, read this section.\n \n\n\n Args\n  dataset_fn   A function taking a tf.distribute.InputContext instance and returning a tf.data.Dataset.  \n  options   tf.distribute.InputOptions used to control options on how this dataset is distributed.   \n \n\n\n Returns   A tf.distribute.DistributedDataset.  \n experimental_distribute_dataset View source \nexperimental_distribute_dataset(\n    dataset, options=None\n)\n Creates tf.distribute.DistributedDataset from tf.data.Dataset. The returned tf.distribute.DistributedDataset can be iterated over similar to regular datasets. NOTE: The user cannot add any more transformations to a tf.distribute.DistributedDataset. You can only create an iterator or examine the tf.TypeSpec of the data generated by it. See API docs of tf.distribute.DistributedDataset to learn more. The following is an example: \nglobal_batch_size = 2\n# Passing the devices is optional.\nstrategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\n# Create a dataset\ndataset = tf.data.Dataset.range(4).batch(global_batch_size)\n# Distribute that dataset\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\n@tf.function\ndef replica_fn(input):\n  return input*2\nresult = []\n# Iterate over the `tf.distribute.DistributedDataset`\nfor x in dist_dataset:\n  # process dataset elements\n  result.append(strategy.run(replica_fn, args=(x,)))\nprint(result)\n[PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>\n}, PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([4])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([6])>\n}]\n Three key actions happending under the hood of this method are batching, sharding, and prefetching. In the code snippet above, dataset is batched by global_batch_size, and calling experimental_distribute_dataset on it rebatches dataset to a new batch size that is equal to the global batch size divided by the number of replicas in sync. We iterate through it using a Pythonic for loop. x is a tf.distribute.DistributedValues containing data for all replicas, and each replica gets data of the new batch size. tf.distribute.Strategy.run will take care of feeding the right per-replica data in x to the right replica_fn executed on each replica. Sharding contains autosharding across multiple workers and within every worker. First, in multi-worker distributed training (i.e. when you use tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy), autosharding a dataset over a set of workers means that each worker is assigned a subset of the entire dataset (if the right tf.data.experimental.AutoShardPolicy is set). This is to ensure that at each step, a global batch size of non-overlapping dataset elements will be processed by each worker. Autosharding has a couple of different options that can be specified using tf.data.experimental.DistributeOptions. Then, sharding within each worker means the method will split the data among all the worker devices (if more than one a present). This will happen regardless of multi-worker autosharding. \nNote: for autosharding across multiple workers, the default mode is tf.data.experimental.AutoShardPolicy.AUTO. This mode will attempt to shard the input dataset by files if the dataset is being created out of reader datasets (e.g. tf.data.TFRecordDataset, tf.data.TextLineDataset, etc.) or otherwise shard the dataset by data, where each of the workers will read the entire dataset and only process the shard assigned to it. However, if you have less than one input file per worker, we suggest that you disable dataset autosharding across workers by setting the tf.data.experimental.DistributeOptions.auto_shard_policy to be tf.data.experimental.AutoShardPolicy.OFF.\n By default, this method adds a prefetch transformation at the end of the user provided tf.data.Dataset instance. The argument to the prefetch transformation which is buffer_size is equal to the number of replicas in sync. If the above batch splitting and dataset sharding logic is undesirable, please use tf.distribute.Strategy.distribute_datasets_from_function instead, which does not do any automatic batching or sharding for you. \nNote: If you are using TPUStrategy, the order in which the data is processed by the workers when using tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function is not guaranteed. This is typically required if you are using tf.distribute to scale prediction. You can however insert an index for each element in the batch and order outputs accordingly. Refer to this snippet for an example of how to order outputs.\n\n\nNote: Stateful dataset transformations are currently not supported with tf.distribute.experimental_distribute_dataset or tf.distribute.distribute_datasets_from_function. Any stateful ops that the dataset may have are currently ignored. For example, if your dataset has a map_fn that uses tf.random.uniform to rotate an image, then you have a dataset graph that depends on state (i.e the random seed) on the local machine where the python process is being executed.\n For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input. If you are interested in last partial batch handling, read this section.\n \n\n\n Args\n  dataset   tf.data.Dataset that will be sharded across all replicas using the rules stated above.  \n  options   tf.distribute.InputOptions used to control options on how this dataset is distributed.   \n \n\n\n Returns   A tf.distribute.DistributedDataset.  \n experimental_local_results View source \nexperimental_local_results(\n    value\n)\n Returns the list of all local per-replica values contained in value. \nNote: This only returns values on the worker initiated by this client. When using a tf.distribute.Strategy like tf.distribute.experimental.MultiWorkerMirroredStrategy, each worker will be its own client, and this function will only return values computed on that worker.\n\n \n\n\n Args\n  value   A value returned by experimental_run(), run(), extended.call_for_each_replica(), or a variable created in scope.   \n \n\n\n Returns   A tuple of values contained in value. If value represents a single value, this returns (value,).  \n experimental_make_numpy_dataset View source \nexperimental_make_numpy_dataset(\n    numpy_input, session=None\n)\n Makes a tf.data.Dataset for input provided via a numpy array. This avoids adding numpy_input as a large constant in the graph, and copies the data to the machine or machines that will be processing the input. Note that you will likely need to use tf.distribute.Strategy.experimental_distribute_dataset with the returned dataset to further distribute it with the strategy. Example: numpy_input = np.ones([10], dtype=np.float32)\ndataset = strategy.experimental_make_numpy_dataset(numpy_input)\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\n\n \n\n\n Args\n  numpy_input   A nest of NumPy input arrays that will be converted into a dataset. Note that lists of Numpy arrays are stacked, as that is normal tf.data.Dataset behavior.  \n  session   (TensorFlow v1.x graph execution only) A session used for initialization.   \n \n\n\n Returns   A tf.data.Dataset representing numpy_input.  \n experimental_run View source \nexperimental_run(\n    fn, input_iterator=None\n)\n Runs ops in fn on each replica, with inputs from input_iterator. DEPRECATED: This method is not available in TF 2.x. Please switch to using run instead. When eager execution is enabled, executes ops specified by fn on each replica. Otherwise, builds a graph to execute the ops on each replica. Each replica will take a single, different input from the inputs provided by one get_next call on the input iterator. fn may call tf.distribute.get_replica_context() to access members such as replica_id_in_sync_group. Key Point: Depending on the tf.distribute.Strategy implementation being used, and whether eager execution is enabled, fn may be called one or more times (once for each replica).\n \n\n\n Args\n  fn   The function to run. The inputs to the function must match the outputs of input_iterator.get_next(). The output must be a tf.nest of Tensors.  \n  input_iterator   (Optional) input iterator from which the inputs are taken.   \n \n\n\n Returns   Merged return value of fn across replicas. The structure of the return value is the same as the return value from fn. Each element in the structure can either be PerReplica (if the values are unsynchronized), Mirrored (if the values are kept in sync), or Tensor (if running on a single replica).  \n make_dataset_iterator View source \nmake_dataset_iterator(\n    dataset\n)\n Makes an iterator for input provided via dataset. DEPRECATED: This method is not available in TF 2.x. Data from the given dataset will be distributed evenly across all the compute replicas. We will assume that the input dataset is batched by the global batch size. With this assumption, we will make a best effort to divide each batch across all the replicas (one or more workers). If this effort fails, an error will be thrown, and the user should instead use make_input_fn_iterator which provides more control to the user, and does not try to divide a batch across replicas. The user could also use make_input_fn_iterator if they want to customize which input is fed to which replica/worker etc.\n \n\n\n Args\n  dataset   tf.data.Dataset that will be distributed evenly across all replicas.   \n \n\n\n Returns   An tf.distribute.InputIterator which returns inputs for each step of the computation. User should call initialize on the returned iterator.  \n make_input_fn_iterator View source \nmake_input_fn_iterator(\n    input_fn, replication_mode=tf.distribute.InputReplicationMode.PER_WORKER\n)\n Returns an iterator split across replicas created from an input function. DEPRECATED: This method is not available in TF 2.x. The input_fn should take an tf.distribute.InputContext object where information about batching and input sharding can be accessed: def input_fn(input_context):\n  batch_size = input_context.get_per_replica_batch_size(global_batch_size)\n  d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)\n  return d.shard(input_context.num_input_pipelines,\n                 input_context.input_pipeline_id)\nwith strategy.scope():\n  iterator = strategy.make_input_fn_iterator(input_fn)\n  replica_results = strategy.experimental_run(replica_fn, iterator)\n The tf.data.Dataset returned by input_fn should have a per-replica batch size, which may be computed using input_context.get_per_replica_batch_size.\n \n\n\n Args\n  input_fn   A function taking a tf.distribute.InputContext object and returning a tf.data.Dataset.  \n  replication_mode   an enum value of tf.distribute.InputReplicationMode. Only PER_WORKER is supported currently, which means there will be a single call to input_fn per worker. Replicas will dequeue from the local tf.data.Dataset on their worker.   \n \n\n\n Returns   An iterator object that should first be .initialize()-ed. It may then either be passed to strategy.experimental_run() or you can iterator.get_next() to get the next value to pass to strategy.extended.call_for_each_replica().  \n reduce View source \nreduce(\n    reduce_op, value, axis=None\n)\n Reduce value across replicas and return result on current device. \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndef step_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\n\nper_replica_result = strategy.run(step_fn)\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\ntotal\n<tf.Tensor: shape=(), dtype=int32, numpy=1>\n To see how this would look with multiple replicas, consider the same example with MirroredStrategy with 2 GPUs: strategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\ndef step_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\n\nper_replica_result = strategy.run(step_fn)\n# Check devices on which per replica result is:\nstrategy.experimental_local_results(per_replica_result)[0].device\n# /job:localhost/replica:0/task:0/device:GPU:0\nstrategy.experimental_local_results(per_replica_result)[1].device\n# /job:localhost/replica:0/task:0/device:GPU:1\n\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\n# Check device on which reduced result is:\ntotal.device\n# /job:localhost/replica:0/task:0/device:CPU:0\n\n This API is typically used for aggregating the results returned from different replicas, for reporting etc. For example, loss computed from different replicas can be averaged using this API before printing. \nNote: The result is copied to the \"current\" device - which would typically be the CPU of the worker on which the program is running. For TPUStrategy, it is the first TPU host. For multi client MultiWorkerMirroredStrategy, this is CPU of each worker.\n There are a number of different tf.distribute APIs for reducing values across replicas:  \ntf.distribute.ReplicaContext.all_reduce: This differs from Strategy.reduce in that it is for replica context and does not copy the results to the host device. all_reduce should be typically used for reductions inside the training step such as gradients. \ntf.distribute.StrategyExtended.reduce_to and tf.distribute.StrategyExtended.batch_reduce_to: These APIs are more advanced versions of Strategy.reduce as they allow customizing the destination of the result. They are also called in cross replica context.  What should axis be? Given a per-replica value returned by run, say a per-example loss, the batch will be divided across all the replicas. This function allows you to aggregate across replicas and optionally also across batch elements by specifying the axis parameter accordingly. For example, if you have a global batch size of 8 and 2 replicas, values for examples [0, 1, 2, 3] will be on replica 0 and [4, 5, 6, 7] will be on replica 1. With axis=None, reduce will aggregate only across replicas, returning [0+4, 1+5, 2+6, 3+7]. This is useful when each replica is computing a scalar or some other value that doesn't have a \"batch\" dimension (like a gradient or loss). strategy.reduce(\"sum\", per_replica_result, axis=None)\n Sometimes, you will want to aggregate across both the global batch and all replicas. You can get this behavior by specifying the batch dimension as the axis, typically axis=0. In this case it would return a scalar 0+1+2+3+4+5+6+7. strategy.reduce(\"sum\", per_replica_result, axis=0)\n If there is a last partial batch, you will need to specify an axis so that the resulting shape is consistent across replicas. So if the last batch has size 6 and it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch unless you specify axis=0. If you specify tf.distribute.ReduceOp.MEAN, using axis=0 will use the correct denominator of 6. Contrast this with computing reduce_mean to get a scalar value on each replica and this function to average those means, which will weigh some values 1/8 and others 1/4.\n \n\n\n Args\n  reduce_op   a tf.distribute.ReduceOp value specifying how values should be combined. Allows using string representation of the enum such as \"SUM\", \"MEAN\".  \n  value   a tf.distribute.DistributedValues instance, e.g. returned by Strategy.run, to be combined into a single tensor. It can also be a regular tensor when used with OneDeviceStrategy or default strategy.  \n  axis   specifies the dimension to reduce along within each replica's tensor. Should typically be set to the batch dimension, or None to only reduce across replicas (e.g. if the tensor has no batch dimension).   \n \n\n\n Returns   A Tensor.  \n run View source \nrun(\n    fn, args=(), kwargs=None, options=None\n)\n Invokes fn on each replica, with the given arguments. This method is the primary way to distribute your computation with a tf.distribute object. It invokes fn on each replica. If args or kwargs have tf.distribute.DistributedValues, such as those produced by a tf.distribute.DistributedDataset from tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function, when fn is executed on a particular replica, it will be executed with the component of tf.distribute.DistributedValues that correspond to that replica. fn is invoked under a replica context. fn may call tf.distribute.get_replica_context() to access members such as all_reduce. Please see the module-level docstring of tf.distribute for the concept of replica context. All arguments in args or kwargs should either be Python values of a nested structure of tensors, e.g. a list of tensors, in which case args and kwargs will be passed to the fn invoked on each replica. Or args or kwargs can be tf.distribute.DistributedValues containing tensors or composite tensors, i.e. tf.compat.v1.TensorInfo.CompositeTensor, in which case each fn call will get the component of a tf.distribute.DistributedValues corresponding to its replica. Key Point: Depending on the implementation of tf.distribute.Strategy and whether eager execution is enabled, fn may be called one or more times. If fn is annotated with tf.function or tf.distribute.Strategy.run is called inside a tf.function (eager execution is disabled inside a tf.function by default), fn is called once per replica to generate a Tensorflow graph, which will then be reused for execution with new inputs. Otherwise, if eager execution is enabled, fn will be called once per replica every step just like regular python code. Example usage:  Constant tensor input.  \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ntensor_input = tf.constant(3.0)\n@tf.function\ndef replica_fn(input):\n  return input*2.0\nresult = strategy.run(replica_fn, args=(tensor_input,))\nresult\nPerReplica:{\n  0: <tf.Tensor: shape=(), dtype=float32, numpy=6.0>,\n  1: <tf.Tensor: shape=(), dtype=float32, numpy=6.0>\n}\n  DistributedValues input.  \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n@tf.function\ndef run():\n  def value_fn(value_context):\n    return value_context.num_replicas_in_sync\n  distributed_values = (\n    strategy.experimental_distribute_values_from_function(\n      value_fn))\n  def replica_fn2(input):\n    return input*2\n  return strategy.run(replica_fn2, args=(distributed_values,))\nresult = run()\nresult\n<tf.Tensor: shape=(), dtype=int32, numpy=4>\n  Use tf.distribute.ReplicaContext to allreduce values.  \nstrategy = tf.distribute.MirroredStrategy([\"gpu:0\", \"gpu:1\"])\n@tf.function\ndef run():\n   def value_fn(value_context):\n     return tf.constant(value_context.replica_id_in_sync_group)\n   distributed_values = (\n       strategy.experimental_distribute_values_from_function(\n           value_fn))\n   def replica_fn(input):\n     return tf.distribute.get_replica_context().all_reduce(\"sum\", input)\n   return strategy.run(replica_fn, args=(distributed_values,))\nresult = run()\nresult\nPerReplica:{\n  0: <tf.Tensor: shape=(), dtype=int32, numpy=1>,\n  1: <tf.Tensor: shape=(), dtype=int32, numpy=1>\n}\n\n \n\n\n Args\n  fn   The function to run on each replica.  \n  args   Optional positional arguments to fn. Its element can be a Python value, a tensor or a tf.distribute.DistributedValues.  \n  kwargs   Optional keyword arguments to fn. Its element can be a Python value, a tensor or a tf.distribute.DistributedValues.  \n  options   An optional instance of tf.distribute.RunOptions specifying the options to run fn.   \n \n\n\n Returns   Merged return value of fn across replicas. The structure of the return value is the same as the return value from fn. Each element in the structure can either be tf.distribute.DistributedValues, Tensor objects, or Tensors (for example, if running on a single replica).  \n scope View source \nscope()\n Context manager to make the strategy current and distribute variables. This method returns a context manager, and is used as follows: \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n# Variable created inside scope:\nwith strategy.scope():\n  mirrored_variable = tf.Variable(1.)\nmirrored_variable\nMirroredVariable:{\n  0: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>,\n  1: <tf.Variable 'Variable/replica_1:0' shape=() dtype=float32, numpy=1.0>\n}\n# Variable created outside scope:\nregular_variable = tf.Variable(1.)\nregular_variable\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\n What happens when Strategy.scope is entered?  \nstrategy is installed in the global context as the \"current\" strategy. Inside this scope, tf.distribute.get_strategy() will now return this strategy. Outside this scope, it returns the default no-op strategy. Entering the scope also enters the \"cross-replica context\". See tf.distribute.StrategyExtended for an explanation on cross-replica and replica contexts. Variable creation inside scope is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like MirroredStrategy, TPUStrategy and MultiWorkerMiroredStrategy create variables replicated on each replica, whereas ParameterServerStrategy creates variables on the parameter servers. This is done using a custom tf.variable_creator_scope. In some strategies, a default device scope may also be entered: in MultiWorkerMiroredStrategy, a default device scope of \"/CPU:0\" is entered on each worker.  \nNote: Entering a scope does not automatically distribute a computation, except in the case of high level training framework like keras model.fit. If you're not using model.fit, you need to use strategy.run API to explicitly distribute that computation. See an example in the custom training loop tutorial.\n What should be in scope and what should be outside? There are a number of requirements on what needs to happen inside the scope. However, in places where we have information about which strategy is in use, we often enter the scope for the user, so they don't have to do it explicitly (i.e. calling those either inside or outside the scope is OK).  Anything that creates variables that should be distributed variables must be in strategy.scope. This can be either by directly putting it in scope, or relying on another API like strategy.run or model.fit to enter it for you. Any variable that is created outside scope will not be distributed and may have performance implications. Common things that create variables in TF: models, optimizers, metrics. These should always be created inside the scope. Another source of variable creation can be a checkpoint restore - when variables are created lazily. Note that any variable created inside a strategy captures the strategy information. So reading and writing to these variables outside the strategy.scope can also work seamlessly, without the user having to enter the scope. Some strategy APIs (such as strategy.run and strategy.reduce) which require to be in a strategy's scope, enter the scope for you automatically, which means when using those APIs you don't need to enter the scope yourself. When a tf.keras.Model is created inside a strategy.scope, we capture this information. When high level training frameworks methods such as model.compile, model.fit etc are then called on this model, we automatically enter the scope, as well as use this strategy to distribute the training etc. See detailed example in distributed keras tutorial. Note that simply calling the model(..) is not impacted - only high level training framework APIs are. model.compile, model.fit, model.evaluate, model.predict and model.save can all be called inside or outside the scope. The following can be either inside or outside the scope:  Creating the input datasets Defining tf.functions that represent your training step Saving APIs such as tf.saved_model.save. Loading creates variables, so that should go inside the scope if you want to train the model in a distributed way. Checkpoint saving. As mentioned above - checkpoint.restore may sometimes need to be inside scope if it creates variables. \n \n \n\n\n Returns   A context manager.  \n update_config_proto View source \nupdate_config_proto(\n    config_proto\n)\n Returns a copy of config_proto modified for use with this strategy. DEPRECATED: This method is not available in TF 2.x. The updated config has something needed to run a strategy, e.g. configuration to run collective ops, or device filters to improve distributed training performance.\n \n\n\n Args\n  config_proto   a tf.ConfigProto object.   \n \n\n\n Returns   The updated copy of the config_proto.  \n  \n"}, {"name": "tf.compat.v1.distribute.experimental.MultiWorkerMirroredStrategy", "path": "compat/v1/distribute/experimental/multiworkermirroredstrategy", "type": "tf.compat", "text": "tf.compat.v1.distribute.experimental.MultiWorkerMirroredStrategy A distribution strategy for synchronous training on multiple workers. Inherits From: Strategy \ntf.compat.v1.distribute.experimental.MultiWorkerMirroredStrategy(\n    communication=tf.distribute.experimental.CollectiveCommunication.AUTO,\n    cluster_resolver=None\n)\n This strategy implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to tf.distribute.MirroredStrategy, it replicates all variables and computations to each local device. The difference is that it uses a distributed collective implementation (e.g. all-reduce), so that multiple workers can work together. You need to launch your program on each worker and configure cluster_resolver correctly. For example, if you are using tf.distribute.cluster_resolver.TFConfigClusterResolver, each worker needs to have its corresponding task_type and task_id set in the TF_CONFIG environment variable. An example TF_CONFIG on worker-0 of a two worker cluster is: TF_CONFIG = '{\"cluster\": {\"worker\": [\"localhost:12345\", \"localhost:23456\"]}, \"task\": {\"type\": \"worker\", \"index\": 0} }'\n Your program runs on each worker as-is. Note that collectives require each worker to participate. All tf.distribute and non tf.distribute API may use collectives internally, e.g. checkpointing and saving since reading a tf.Variable with tf.VariableSynchronization.ON_READ all-reduces the value. Therefore it's recommended to run exactly the same program on each worker. Dispatching based on task_type or task_id of the worker is error-prone. cluster_resolver.num_accelerators() determines the number of GPUs the strategy uses. If it's zero, the strategy uses the CPU. All workers need to use the same number of devices, otherwise the behavior is undefined. This strategy is not intended for TPU. Use tf.distribute.TPUStrategy instead. After setting up TF_CONFIG, using this strategy is similar to using tf.distribute.MirroredStrategy and tf.distribute.TPUStrategy. strategy = tf.distribute.MultiWorkerMirroredStrategy()\n\nwith strategy.scope():\n  model = tf.keras.Sequential([\n    tf.keras.layers.Dense(2, input_shape=(5,)),\n  ])\n  optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n\ndef dataset_fn(ctx):\n  x = np.random.random((2, 5)).astype(np.float32)\n  y = np.random.randint(2, size=(2, 1))\n  dataset = tf.data.Dataset.from_tensor_slices((x, y))\n  return dataset.repeat().batch(1, drop_remainder=True)\ndist_dataset = strategy.distribute_datasets_from_function(dataset_fn)\n\nmodel.compile()\nmodel.fit(dist_dataset)\n You can also write your own training loop: @tf.function\ndef train_step(iterator):\n\n  def step_fn(inputs):\n    features, labels = inputs\n    with tf.GradientTape() as tape:\n      logits = model(features, training=True)\n      loss = tf.keras.losses.sparse_categorical_crossentropy(\n          labels, logits)\n\n    grads = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n  strategy.run(step_fn, args=(next(iterator),))\n\nfor _ in range(NUM_STEP):\n  train_step(iterator)\n See Multi-worker training with Keras for a detailed tutorial. Saving You need to save and checkpoint on all workers instead of just one. This is because variables whose synchronization=ON_READ triggers aggregation during saving. It's recommended to save to a different path on each worker to avoid race conditions. Each worker saves the same thing. See Multi-worker training with Keras tutorial for examples. Known Issues  \ntf.distribute.cluster_resolver.TFConfigClusterResolver does not return the correct number of accelerators. The strategy uses all available GPUs if cluster_resolver is tf.distribute.cluster_resolver.TFConfigClusterResolver or None. In eager mode, the strategy needs to be created before calling any other Tensorflow API. \n \n\n\n Attributes\n  cluster_resolver   Returns the cluster resolver associated with this strategy. In general, when using a multi-worker tf.distribute strategy such as tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy(), there is a tf.distribute.cluster_resolver.ClusterResolver associated with the strategy used, and such an instance is returned by this property. Strategies that intend to have an associated tf.distribute.cluster_resolver.ClusterResolver must set the relevant attribute, or override this property; otherwise, None is returned by default. Those strategies should also provide information regarding what is returned by this property. Single-worker strategies usually do not have a tf.distribute.cluster_resolver.ClusterResolver, and in those cases this property will return None. The tf.distribute.cluster_resolver.ClusterResolver may be useful when the user needs to access information such as the cluster spec, task type or task id. For example, \nos.environ['TF_CONFIG'] = json.dumps({\n'cluster': {\n'worker': [\"localhost:12345\", \"localhost:23456\"],\n'ps': [\"localhost:34567\"]\n},\n'task': {'type': 'worker', 'index': 0}\n})\n\n# This implicitly uses TF_CONFIG for the cluster and current task info.\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n\n...\n\nif strategy.cluster_resolver.task_type == 'worker':\n# Perform something that's only applicable on workers. Since we set this\n# as a worker above, this block will run on this particular instance.\nelif strategy.cluster_resolver.task_type == 'ps':\n# Perform something that's only applicable on parameter servers. Since we\n# set this as a worker above, this block will not run on this particular\n# instance.\n For more information, please see tf.distribute.cluster_resolver.ClusterResolver's API docstring. \n \n  extended   tf.distribute.StrategyExtended with additional methods.  \n  num_replicas_in_sync   Returns number of replicas over which gradients are aggregated.    Methods distribute_datasets_from_function View source \ndistribute_datasets_from_function(\n    dataset_fn, options=None\n)\n Distributes tf.data.Dataset instances created by calls to dataset_fn. The argument dataset_fn that users pass in is an input function that has a tf.distribute.InputContext argument and returns a tf.data.Dataset instance. It is expected that the returned dataset from dataset_fn is already batched by per-replica batch size (i.e. global batch size divided by the number of replicas in sync) and sharded. tf.distribute.Strategy.distribute_datasets_from_function does not batch or shard the tf.data.Dataset instance returned from the input function. dataset_fn will be called on the CPU device of each of the workers and each generates a dataset where every replica on that worker will dequeue one batch of inputs (i.e. if a worker has two replicas, two batches will be dequeued from the Dataset every step). This method can be used for several purposes. First, it allows you to specify your own batching and sharding logic. (In contrast, tf.distribute.experimental_distribute_dataset does batching and sharding for you.) For example, where experimental_distribute_dataset is unable to shard the input files, this method might be used to manually shard the dataset (avoiding the slow fallback behavior in experimental_distribute_dataset). In cases where the dataset is infinite, this sharding can be done by creating dataset replicas that differ only in their random seed. The dataset_fn should take an tf.distribute.InputContext instance where information about batching and input replication can be accessed. You can use element_spec property of the tf.distribute.DistributedDataset returned by this API to query the tf.TypeSpec of the elements returned by the iterator. This can be used to set the input_signature property of a tf.function. Follow tf.distribute.DistributedDataset.element_spec to see an example. Key Point: The tf.data.Dataset returned by dataset_fn should have a per-replica batch size, unlike experimental_distribute_dataset, which uses the global batch size. This may be computed using input_context.get_per_replica_batch_size.\nNote: If you are using TPUStrategy, the order in which the data is processed by the workers when using tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function is not guaranteed. This is typically required if you are using tf.distribute to scale prediction. You can however insert an index for each element in the batch and order outputs accordingly. Refer to this snippet for an example of how to order outputs.\n\n\nNote: Stateful dataset transformations are currently not supported with tf.distribute.experimental_distribute_dataset or tf.distribute.distribute_datasets_from_function. Any stateful ops that the dataset may have are currently ignored. For example, if your dataset has a map_fn that uses tf.random.uniform to rotate an image, then you have a dataset graph that depends on state (i.e the random seed) on the local machine where the python process is being executed.\n For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input). If you are interested in last partial batch handling, read this section.\n \n\n\n Args\n  dataset_fn   A function taking a tf.distribute.InputContext instance and returning a tf.data.Dataset.  \n  options   tf.distribute.InputOptions used to control options on how this dataset is distributed.   \n \n\n\n Returns   A tf.distribute.DistributedDataset.  \n experimental_distribute_dataset View source \nexperimental_distribute_dataset(\n    dataset, options=None\n)\n Creates tf.distribute.DistributedDataset from tf.data.Dataset. The returned tf.distribute.DistributedDataset can be iterated over similar to regular datasets. NOTE: The user cannot add any more transformations to a tf.distribute.DistributedDataset. You can only create an iterator or examine the tf.TypeSpec of the data generated by it. See API docs of tf.distribute.DistributedDataset to learn more. The following is an example: \nglobal_batch_size = 2\n# Passing the devices is optional.\nstrategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\n# Create a dataset\ndataset = tf.data.Dataset.range(4).batch(global_batch_size)\n# Distribute that dataset\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\n@tf.function\ndef replica_fn(input):\n  return input*2\nresult = []\n# Iterate over the `tf.distribute.DistributedDataset`\nfor x in dist_dataset:\n  # process dataset elements\n  result.append(strategy.run(replica_fn, args=(x,)))\nprint(result)\n[PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>\n}, PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([4])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([6])>\n}]\n Three key actions happending under the hood of this method are batching, sharding, and prefetching. In the code snippet above, dataset is batched by global_batch_size, and calling experimental_distribute_dataset on it rebatches dataset to a new batch size that is equal to the global batch size divided by the number of replicas in sync. We iterate through it using a Pythonic for loop. x is a tf.distribute.DistributedValues containing data for all replicas, and each replica gets data of the new batch size. tf.distribute.Strategy.run will take care of feeding the right per-replica data in x to the right replica_fn executed on each replica. Sharding contains autosharding across multiple workers and within every worker. First, in multi-worker distributed training (i.e. when you use tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy), autosharding a dataset over a set of workers means that each worker is assigned a subset of the entire dataset (if the right tf.data.experimental.AutoShardPolicy is set). This is to ensure that at each step, a global batch size of non-overlapping dataset elements will be processed by each worker. Autosharding has a couple of different options that can be specified using tf.data.experimental.DistributeOptions. Then, sharding within each worker means the method will split the data among all the worker devices (if more than one a present). This will happen regardless of multi-worker autosharding. \nNote: for autosharding across multiple workers, the default mode is tf.data.experimental.AutoShardPolicy.AUTO. This mode will attempt to shard the input dataset by files if the dataset is being created out of reader datasets (e.g. tf.data.TFRecordDataset, tf.data.TextLineDataset, etc.) or otherwise shard the dataset by data, where each of the workers will read the entire dataset and only process the shard assigned to it. However, if you have less than one input file per worker, we suggest that you disable dataset autosharding across workers by setting the tf.data.experimental.DistributeOptions.auto_shard_policy to be tf.data.experimental.AutoShardPolicy.OFF.\n By default, this method adds a prefetch transformation at the end of the user provided tf.data.Dataset instance. The argument to the prefetch transformation which is buffer_size is equal to the number of replicas in sync. If the above batch splitting and dataset sharding logic is undesirable, please use tf.distribute.Strategy.distribute_datasets_from_function instead, which does not do any automatic batching or sharding for you. \nNote: If you are using TPUStrategy, the order in which the data is processed by the workers when using tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function is not guaranteed. This is typically required if you are using tf.distribute to scale prediction. You can however insert an index for each element in the batch and order outputs accordingly. Refer to this snippet for an example of how to order outputs.\n\n\nNote: Stateful dataset transformations are currently not supported with tf.distribute.experimental_distribute_dataset or tf.distribute.distribute_datasets_from_function. Any stateful ops that the dataset may have are currently ignored. For example, if your dataset has a map_fn that uses tf.random.uniform to rotate an image, then you have a dataset graph that depends on state (i.e the random seed) on the local machine where the python process is being executed.\n For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input. If you are interested in last partial batch handling, read this section.\n \n\n\n Args\n  dataset   tf.data.Dataset that will be sharded across all replicas using the rules stated above.  \n  options   tf.distribute.InputOptions used to control options on how this dataset is distributed.   \n \n\n\n Returns   A tf.distribute.DistributedDataset.  \n experimental_local_results View source \nexperimental_local_results(\n    value\n)\n Returns the list of all local per-replica values contained in value. \nNote: This only returns values on the worker initiated by this client. When using a tf.distribute.Strategy like tf.distribute.experimental.MultiWorkerMirroredStrategy, each worker will be its own client, and this function will only return values computed on that worker.\n\n \n\n\n Args\n  value   A value returned by experimental_run(), run(), extended.call_for_each_replica(), or a variable created in scope.   \n \n\n\n Returns   A tuple of values contained in value. If value represents a single value, this returns (value,).  \n experimental_make_numpy_dataset View source \nexperimental_make_numpy_dataset(\n    numpy_input, session=None\n)\n Makes a tf.data.Dataset for input provided via a numpy array. This avoids adding numpy_input as a large constant in the graph, and copies the data to the machine or machines that will be processing the input. Note that you will likely need to use tf.distribute.Strategy.experimental_distribute_dataset with the returned dataset to further distribute it with the strategy. Example: numpy_input = np.ones([10], dtype=np.float32)\ndataset = strategy.experimental_make_numpy_dataset(numpy_input)\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\n\n \n\n\n Args\n  numpy_input   A nest of NumPy input arrays that will be converted into a dataset. Note that lists of Numpy arrays are stacked, as that is normal tf.data.Dataset behavior.  \n  session   (TensorFlow v1.x graph execution only) A session used for initialization.   \n \n\n\n Returns   A tf.data.Dataset representing numpy_input.  \n experimental_run View source \nexperimental_run(\n    fn, input_iterator=None\n)\n Runs ops in fn on each replica, with inputs from input_iterator. DEPRECATED: This method is not available in TF 2.x. Please switch to using run instead. When eager execution is enabled, executes ops specified by fn on each replica. Otherwise, builds a graph to execute the ops on each replica. Each replica will take a single, different input from the inputs provided by one get_next call on the input iterator. fn may call tf.distribute.get_replica_context() to access members such as replica_id_in_sync_group. Key Point: Depending on the tf.distribute.Strategy implementation being used, and whether eager execution is enabled, fn may be called one or more times (once for each replica).\n \n\n\n Args\n  fn   The function to run. The inputs to the function must match the outputs of input_iterator.get_next(). The output must be a tf.nest of Tensors.  \n  input_iterator   (Optional) input iterator from which the inputs are taken.   \n \n\n\n Returns   Merged return value of fn across replicas. The structure of the return value is the same as the return value from fn. Each element in the structure can either be PerReplica (if the values are unsynchronized), Mirrored (if the values are kept in sync), or Tensor (if running on a single replica).  \n make_dataset_iterator View source \nmake_dataset_iterator(\n    dataset\n)\n Makes an iterator for input provided via dataset. DEPRECATED: This method is not available in TF 2.x. Data from the given dataset will be distributed evenly across all the compute replicas. We will assume that the input dataset is batched by the global batch size. With this assumption, we will make a best effort to divide each batch across all the replicas (one or more workers). If this effort fails, an error will be thrown, and the user should instead use make_input_fn_iterator which provides more control to the user, and does not try to divide a batch across replicas. The user could also use make_input_fn_iterator if they want to customize which input is fed to which replica/worker etc.\n \n\n\n Args\n  dataset   tf.data.Dataset that will be distributed evenly across all replicas.   \n \n\n\n Returns   An tf.distribute.InputIterator which returns inputs for each step of the computation. User should call initialize on the returned iterator.  \n make_input_fn_iterator View source \nmake_input_fn_iterator(\n    input_fn, replication_mode=tf.distribute.InputReplicationMode.PER_WORKER\n)\n Returns an iterator split across replicas created from an input function. DEPRECATED: This method is not available in TF 2.x. The input_fn should take an tf.distribute.InputContext object where information about batching and input sharding can be accessed: def input_fn(input_context):\n  batch_size = input_context.get_per_replica_batch_size(global_batch_size)\n  d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)\n  return d.shard(input_context.num_input_pipelines,\n                 input_context.input_pipeline_id)\nwith strategy.scope():\n  iterator = strategy.make_input_fn_iterator(input_fn)\n  replica_results = strategy.experimental_run(replica_fn, iterator)\n The tf.data.Dataset returned by input_fn should have a per-replica batch size, which may be computed using input_context.get_per_replica_batch_size.\n \n\n\n Args\n  input_fn   A function taking a tf.distribute.InputContext object and returning a tf.data.Dataset.  \n  replication_mode   an enum value of tf.distribute.InputReplicationMode. Only PER_WORKER is supported currently, which means there will be a single call to input_fn per worker. Replicas will dequeue from the local tf.data.Dataset on their worker.   \n \n\n\n Returns   An iterator object that should first be .initialize()-ed. It may then either be passed to strategy.experimental_run() or you can iterator.get_next() to get the next value to pass to strategy.extended.call_for_each_replica().  \n reduce View source \nreduce(\n    reduce_op, value, axis=None\n)\n Reduce value across replicas and return result on current device. \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndef step_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\n\nper_replica_result = strategy.run(step_fn)\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\ntotal\n<tf.Tensor: shape=(), dtype=int32, numpy=1>\n To see how this would look with multiple replicas, consider the same example with MirroredStrategy with 2 GPUs: strategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\ndef step_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\n\nper_replica_result = strategy.run(step_fn)\n# Check devices on which per replica result is:\nstrategy.experimental_local_results(per_replica_result)[0].device\n# /job:localhost/replica:0/task:0/device:GPU:0\nstrategy.experimental_local_results(per_replica_result)[1].device\n# /job:localhost/replica:0/task:0/device:GPU:1\n\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\n# Check device on which reduced result is:\ntotal.device\n# /job:localhost/replica:0/task:0/device:CPU:0\n\n This API is typically used for aggregating the results returned from different replicas, for reporting etc. For example, loss computed from different replicas can be averaged using this API before printing. \nNote: The result is copied to the \"current\" device - which would typically be the CPU of the worker on which the program is running. For TPUStrategy, it is the first TPU host. For multi client MultiWorkerMirroredStrategy, this is CPU of each worker.\n There are a number of different tf.distribute APIs for reducing values across replicas:  \ntf.distribute.ReplicaContext.all_reduce: This differs from Strategy.reduce in that it is for replica context and does not copy the results to the host device. all_reduce should be typically used for reductions inside the training step such as gradients. \ntf.distribute.StrategyExtended.reduce_to and tf.distribute.StrategyExtended.batch_reduce_to: These APIs are more advanced versions of Strategy.reduce as they allow customizing the destination of the result. They are also called in cross replica context.  What should axis be? Given a per-replica value returned by run, say a per-example loss, the batch will be divided across all the replicas. This function allows you to aggregate across replicas and optionally also across batch elements by specifying the axis parameter accordingly. For example, if you have a global batch size of 8 and 2 replicas, values for examples [0, 1, 2, 3] will be on replica 0 and [4, 5, 6, 7] will be on replica 1. With axis=None, reduce will aggregate only across replicas, returning [0+4, 1+5, 2+6, 3+7]. This is useful when each replica is computing a scalar or some other value that doesn't have a \"batch\" dimension (like a gradient or loss). strategy.reduce(\"sum\", per_replica_result, axis=None)\n Sometimes, you will want to aggregate across both the global batch and all replicas. You can get this behavior by specifying the batch dimension as the axis, typically axis=0. In this case it would return a scalar 0+1+2+3+4+5+6+7. strategy.reduce(\"sum\", per_replica_result, axis=0)\n If there is a last partial batch, you will need to specify an axis so that the resulting shape is consistent across replicas. So if the last batch has size 6 and it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch unless you specify axis=0. If you specify tf.distribute.ReduceOp.MEAN, using axis=0 will use the correct denominator of 6. Contrast this with computing reduce_mean to get a scalar value on each replica and this function to average those means, which will weigh some values 1/8 and others 1/4.\n \n\n\n Args\n  reduce_op   a tf.distribute.ReduceOp value specifying how values should be combined. Allows using string representation of the enum such as \"SUM\", \"MEAN\".  \n  value   a tf.distribute.DistributedValues instance, e.g. returned by Strategy.run, to be combined into a single tensor. It can also be a regular tensor when used with OneDeviceStrategy or default strategy.  \n  axis   specifies the dimension to reduce along within each replica's tensor. Should typically be set to the batch dimension, or None to only reduce across replicas (e.g. if the tensor has no batch dimension).   \n \n\n\n Returns   A Tensor.  \n run View source \nrun(\n    fn, args=(), kwargs=None, options=None\n)\n Invokes fn on each replica, with the given arguments. This method is the primary way to distribute your computation with a tf.distribute object. It invokes fn on each replica. If args or kwargs have tf.distribute.DistributedValues, such as those produced by a tf.distribute.DistributedDataset from tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function, when fn is executed on a particular replica, it will be executed with the component of tf.distribute.DistributedValues that correspond to that replica. fn is invoked under a replica context. fn may call tf.distribute.get_replica_context() to access members such as all_reduce. Please see the module-level docstring of tf.distribute for the concept of replica context. All arguments in args or kwargs should either be Python values of a nested structure of tensors, e.g. a list of tensors, in which case args and kwargs will be passed to the fn invoked on each replica. Or args or kwargs can be tf.distribute.DistributedValues containing tensors or composite tensors, i.e. tf.compat.v1.TensorInfo.CompositeTensor, in which case each fn call will get the component of a tf.distribute.DistributedValues corresponding to its replica. Key Point: Depending on the implementation of tf.distribute.Strategy and whether eager execution is enabled, fn may be called one or more times. If fn is annotated with tf.function or tf.distribute.Strategy.run is called inside a tf.function (eager execution is disabled inside a tf.function by default), fn is called once per replica to generate a Tensorflow graph, which will then be reused for execution with new inputs. Otherwise, if eager execution is enabled, fn will be called once per replica every step just like regular python code. Example usage:  Constant tensor input.  \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ntensor_input = tf.constant(3.0)\n@tf.function\ndef replica_fn(input):\n  return input*2.0\nresult = strategy.run(replica_fn, args=(tensor_input,))\nresult\nPerReplica:{\n  0: <tf.Tensor: shape=(), dtype=float32, numpy=6.0>,\n  1: <tf.Tensor: shape=(), dtype=float32, numpy=6.0>\n}\n  DistributedValues input.  \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n@tf.function\ndef run():\n  def value_fn(value_context):\n    return value_context.num_replicas_in_sync\n  distributed_values = (\n    strategy.experimental_distribute_values_from_function(\n      value_fn))\n  def replica_fn2(input):\n    return input*2\n  return strategy.run(replica_fn2, args=(distributed_values,))\nresult = run()\nresult\n<tf.Tensor: shape=(), dtype=int32, numpy=4>\n  Use tf.distribute.ReplicaContext to allreduce values.  \nstrategy = tf.distribute.MirroredStrategy([\"gpu:0\", \"gpu:1\"])\n@tf.function\ndef run():\n   def value_fn(value_context):\n     return tf.constant(value_context.replica_id_in_sync_group)\n   distributed_values = (\n       strategy.experimental_distribute_values_from_function(\n           value_fn))\n   def replica_fn(input):\n     return tf.distribute.get_replica_context().all_reduce(\"sum\", input)\n   return strategy.run(replica_fn, args=(distributed_values,))\nresult = run()\nresult\nPerReplica:{\n  0: <tf.Tensor: shape=(), dtype=int32, numpy=1>,\n  1: <tf.Tensor: shape=(), dtype=int32, numpy=1>\n}\n\n \n\n\n Args\n  fn   The function to run on each replica.  \n  args   Optional positional arguments to fn. Its element can be a Python value, a tensor or a tf.distribute.DistributedValues.  \n  kwargs   Optional keyword arguments to fn. Its element can be a Python value, a tensor or a tf.distribute.DistributedValues.  \n  options   An optional instance of tf.distribute.RunOptions specifying the options to run fn.   \n \n\n\n Returns   Merged return value of fn across replicas. The structure of the return value is the same as the return value from fn. Each element in the structure can either be tf.distribute.DistributedValues, Tensor objects, or Tensors (for example, if running on a single replica).  \n scope View source \nscope()\n Context manager to make the strategy current and distribute variables. This method returns a context manager, and is used as follows: \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n# Variable created inside scope:\nwith strategy.scope():\n  mirrored_variable = tf.Variable(1.)\nmirrored_variable\nMirroredVariable:{\n  0: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>,\n  1: <tf.Variable 'Variable/replica_1:0' shape=() dtype=float32, numpy=1.0>\n}\n# Variable created outside scope:\nregular_variable = tf.Variable(1.)\nregular_variable\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\n What happens when Strategy.scope is entered?  \nstrategy is installed in the global context as the \"current\" strategy. Inside this scope, tf.distribute.get_strategy() will now return this strategy. Outside this scope, it returns the default no-op strategy. Entering the scope also enters the \"cross-replica context\". See tf.distribute.StrategyExtended for an explanation on cross-replica and replica contexts. Variable creation inside scope is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like MirroredStrategy, TPUStrategy and MultiWorkerMiroredStrategy create variables replicated on each replica, whereas ParameterServerStrategy creates variables on the parameter servers. This is done using a custom tf.variable_creator_scope. In some strategies, a default device scope may also be entered: in MultiWorkerMiroredStrategy, a default device scope of \"/CPU:0\" is entered on each worker.  \nNote: Entering a scope does not automatically distribute a computation, except in the case of high level training framework like keras model.fit. If you're not using model.fit, you need to use strategy.run API to explicitly distribute that computation. See an example in the custom training loop tutorial.\n What should be in scope and what should be outside? There are a number of requirements on what needs to happen inside the scope. However, in places where we have information about which strategy is in use, we often enter the scope for the user, so they don't have to do it explicitly (i.e. calling those either inside or outside the scope is OK).  Anything that creates variables that should be distributed variables must be in strategy.scope. This can be either by directly putting it in scope, or relying on another API like strategy.run or model.fit to enter it for you. Any variable that is created outside scope will not be distributed and may have performance implications. Common things that create variables in TF: models, optimizers, metrics. These should always be created inside the scope. Another source of variable creation can be a checkpoint restore - when variables are created lazily. Note that any variable created inside a strategy captures the strategy information. So reading and writing to these variables outside the strategy.scope can also work seamlessly, without the user having to enter the scope. Some strategy APIs (such as strategy.run and strategy.reduce) which require to be in a strategy's scope, enter the scope for you automatically, which means when using those APIs you don't need to enter the scope yourself. When a tf.keras.Model is created inside a strategy.scope, we capture this information. When high level training frameworks methods such as model.compile, model.fit etc are then called on this model, we automatically enter the scope, as well as use this strategy to distribute the training etc. See detailed example in distributed keras tutorial. Note that simply calling the model(..) is not impacted - only high level training framework APIs are. model.compile, model.fit, model.evaluate, model.predict and model.save can all be called inside or outside the scope. The following can be either inside or outside the scope:  Creating the input datasets Defining tf.functions that represent your training step Saving APIs such as tf.saved_model.save. Loading creates variables, so that should go inside the scope if you want to train the model in a distributed way. Checkpoint saving. As mentioned above - checkpoint.restore may sometimes need to be inside scope if it creates variables. \n \n \n\n\n Returns   A context manager.  \n update_config_proto View source \nupdate_config_proto(\n    config_proto\n)\n Returns a copy of config_proto modified for use with this strategy. DEPRECATED: This method is not available in TF 2.x. The updated config has something needed to run a strategy, e.g. configuration to run collective ops, or device filters to improve distributed training performance.\n \n\n\n Args\n  config_proto   a tf.ConfigProto object.   \n \n\n\n Returns   The updated copy of the config_proto.  \n  \n"}, {"name": "tf.compat.v1.distribute.experimental.ParameterServerStrategy", "path": "compat/v1/distribute/experimental/parameterserverstrategy", "type": "tf.compat", "text": "tf.compat.v1.distribute.experimental.ParameterServerStrategy An asynchronous multi-worker parameter server tf.distribute strategy. Inherits From: Strategy \ntf.compat.v1.distribute.experimental.ParameterServerStrategy(\n    cluster_resolver=None\n)\n This strategy requires two roles: workers and parameter servers. Variables and updates to those variables will be assigned to parameter servers and other operations are assigned to workers. When each worker has more than one GPU, operations will be replicated on all GPUs. Even though operations may be replicated, variables are not and each worker shares a common view for which parameter server a variable is assigned to. By default it uses TFConfigClusterResolver to detect configurations for multi-worker training. This requires a 'TF_CONFIG' environment variable and the 'TF_CONFIG' must have a cluster spec. This class assumes each worker is running the same code independently, but parameter servers are running a standard server. This means that while each worker will synchronously compute a single gradient update across all GPUs, updates between workers proceed asynchronously. Operations that occur only on the first replica (such as incrementing the global step), will occur on the first replica of every worker. It is expected to call call_for_each_replica(fn, ...) for any operations which potentially can be replicated across replicas (i.e. multiple GPUs) even if there is only CPU or one GPU. When defining the fn, extra caution needs to be taken: 1) It is generally not recommended to open a device scope under the strategy's scope. A device scope (i.e. calling tf.device) will be merged with or override the device for operations but will not change the device for variables. 2) It is also not recommended to open a colocation scope (i.e. calling tf.compat.v1.colocate_with) under the strategy's scope. For colocating variables, use strategy.extended.colocate_vars_with instead. Colocation of ops will possibly create device assignment conflicts. \nNote: This strategy only works with the Estimator API. Pass an instance of this strategy to the experimental_distribute argument when you create the RunConfig. This instance of RunConfig should then be passed to the Estimator instance on which train_and_evaluate is called.\n For Example: strategy = tf.distribute.experimental.ParameterServerStrategy()\nrun_config = tf.estimator.RunConfig(\n    experimental_distribute.train_distribute=strategy)\nestimator = tf.estimator.Estimator(config=run_config)\ntf.estimator.train_and_evaluate(estimator,...)\n\n \n\n\n Args\n  cluster_resolver   Optional tf.distribute.cluster_resolver.ClusterResolver object. Defaults to a tf.distribute.cluster_resolver.TFConfigClusterResolver.   \n \n\n\n Attributes\n  cluster_resolver   Returns the cluster resolver associated with this strategy. In general, when using a multi-worker tf.distribute strategy such as tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy(), there is a tf.distribute.cluster_resolver.ClusterResolver associated with the strategy used, and such an instance is returned by this property. Strategies that intend to have an associated tf.distribute.cluster_resolver.ClusterResolver must set the relevant attribute, or override this property; otherwise, None is returned by default. Those strategies should also provide information regarding what is returned by this property. Single-worker strategies usually do not have a tf.distribute.cluster_resolver.ClusterResolver, and in those cases this property will return None. The tf.distribute.cluster_resolver.ClusterResolver may be useful when the user needs to access information such as the cluster spec, task type or task id. For example, \nos.environ['TF_CONFIG'] = json.dumps({\n'cluster': {\n'worker': [\"localhost:12345\", \"localhost:23456\"],\n'ps': [\"localhost:34567\"]\n},\n'task': {'type': 'worker', 'index': 0}\n})\n\n# This implicitly uses TF_CONFIG for the cluster and current task info.\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n\n...\n\nif strategy.cluster_resolver.task_type == 'worker':\n# Perform something that's only applicable on workers. Since we set this\n# as a worker above, this block will run on this particular instance.\nelif strategy.cluster_resolver.task_type == 'ps':\n# Perform something that's only applicable on parameter servers. Since we\n# set this as a worker above, this block will not run on this particular\n# instance.\n For more information, please see tf.distribute.cluster_resolver.ClusterResolver's API docstring. \n \n  extended   tf.distribute.StrategyExtended with additional methods.  \n  num_replicas_in_sync   Returns number of replicas over which gradients are aggregated.    Methods distribute_datasets_from_function View source \ndistribute_datasets_from_function(\n    dataset_fn, options=None\n)\n Distributes tf.data.Dataset instances created by calls to dataset_fn. The argument dataset_fn that users pass in is an input function that has a tf.distribute.InputContext argument and returns a tf.data.Dataset instance. It is expected that the returned dataset from dataset_fn is already batched by per-replica batch size (i.e. global batch size divided by the number of replicas in sync) and sharded. tf.distribute.Strategy.distribute_datasets_from_function does not batch or shard the tf.data.Dataset instance returned from the input function. dataset_fn will be called on the CPU device of each of the workers and each generates a dataset where every replica on that worker will dequeue one batch of inputs (i.e. if a worker has two replicas, two batches will be dequeued from the Dataset every step). This method can be used for several purposes. First, it allows you to specify your own batching and sharding logic. (In contrast, tf.distribute.experimental_distribute_dataset does batching and sharding for you.) For example, where experimental_distribute_dataset is unable to shard the input files, this method might be used to manually shard the dataset (avoiding the slow fallback behavior in experimental_distribute_dataset). In cases where the dataset is infinite, this sharding can be done by creating dataset replicas that differ only in their random seed. The dataset_fn should take an tf.distribute.InputContext instance where information about batching and input replication can be accessed. You can use element_spec property of the tf.distribute.DistributedDataset returned by this API to query the tf.TypeSpec of the elements returned by the iterator. This can be used to set the input_signature property of a tf.function. Follow tf.distribute.DistributedDataset.element_spec to see an example. Key Point: The tf.data.Dataset returned by dataset_fn should have a per-replica batch size, unlike experimental_distribute_dataset, which uses the global batch size. This may be computed using input_context.get_per_replica_batch_size.\nNote: If you are using TPUStrategy, the order in which the data is processed by the workers when using tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function is not guaranteed. This is typically required if you are using tf.distribute to scale prediction. You can however insert an index for each element in the batch and order outputs accordingly. Refer to this snippet for an example of how to order outputs.\n\n\nNote: Stateful dataset transformations are currently not supported with tf.distribute.experimental_distribute_dataset or tf.distribute.distribute_datasets_from_function. Any stateful ops that the dataset may have are currently ignored. For example, if your dataset has a map_fn that uses tf.random.uniform to rotate an image, then you have a dataset graph that depends on state (i.e the random seed) on the local machine where the python process is being executed.\n For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input). If you are interested in last partial batch handling, read this section.\n \n\n\n Args\n  dataset_fn   A function taking a tf.distribute.InputContext instance and returning a tf.data.Dataset.  \n  options   tf.distribute.InputOptions used to control options on how this dataset is distributed.   \n \n\n\n Returns   A tf.distribute.DistributedDataset.  \n experimental_distribute_dataset View source \nexperimental_distribute_dataset(\n    dataset, options=None\n)\n Creates tf.distribute.DistributedDataset from tf.data.Dataset. The returned tf.distribute.DistributedDataset can be iterated over similar to regular datasets. NOTE: The user cannot add any more transformations to a tf.distribute.DistributedDataset. You can only create an iterator or examine the tf.TypeSpec of the data generated by it. See API docs of tf.distribute.DistributedDataset to learn more. The following is an example: \nglobal_batch_size = 2\n# Passing the devices is optional.\nstrategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\n# Create a dataset\ndataset = tf.data.Dataset.range(4).batch(global_batch_size)\n# Distribute that dataset\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\n@tf.function\ndef replica_fn(input):\n  return input*2\nresult = []\n# Iterate over the `tf.distribute.DistributedDataset`\nfor x in dist_dataset:\n  # process dataset elements\n  result.append(strategy.run(replica_fn, args=(x,)))\nprint(result)\n[PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>\n}, PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([4])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([6])>\n}]\n Three key actions happending under the hood of this method are batching, sharding, and prefetching. In the code snippet above, dataset is batched by global_batch_size, and calling experimental_distribute_dataset on it rebatches dataset to a new batch size that is equal to the global batch size divided by the number of replicas in sync. We iterate through it using a Pythonic for loop. x is a tf.distribute.DistributedValues containing data for all replicas, and each replica gets data of the new batch size. tf.distribute.Strategy.run will take care of feeding the right per-replica data in x to the right replica_fn executed on each replica. Sharding contains autosharding across multiple workers and within every worker. First, in multi-worker distributed training (i.e. when you use tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy), autosharding a dataset over a set of workers means that each worker is assigned a subset of the entire dataset (if the right tf.data.experimental.AutoShardPolicy is set). This is to ensure that at each step, a global batch size of non-overlapping dataset elements will be processed by each worker. Autosharding has a couple of different options that can be specified using tf.data.experimental.DistributeOptions. Then, sharding within each worker means the method will split the data among all the worker devices (if more than one a present). This will happen regardless of multi-worker autosharding. \nNote: for autosharding across multiple workers, the default mode is tf.data.experimental.AutoShardPolicy.AUTO. This mode will attempt to shard the input dataset by files if the dataset is being created out of reader datasets (e.g. tf.data.TFRecordDataset, tf.data.TextLineDataset, etc.) or otherwise shard the dataset by data, where each of the workers will read the entire dataset and only process the shard assigned to it. However, if you have less than one input file per worker, we suggest that you disable dataset autosharding across workers by setting the tf.data.experimental.DistributeOptions.auto_shard_policy to be tf.data.experimental.AutoShardPolicy.OFF.\n By default, this method adds a prefetch transformation at the end of the user provided tf.data.Dataset instance. The argument to the prefetch transformation which is buffer_size is equal to the number of replicas in sync. If the above batch splitting and dataset sharding logic is undesirable, please use tf.distribute.Strategy.distribute_datasets_from_function instead, which does not do any automatic batching or sharding for you. \nNote: If you are using TPUStrategy, the order in which the data is processed by the workers when using tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function is not guaranteed. This is typically required if you are using tf.distribute to scale prediction. You can however insert an index for each element in the batch and order outputs accordingly. Refer to this snippet for an example of how to order outputs.\n\n\nNote: Stateful dataset transformations are currently not supported with tf.distribute.experimental_distribute_dataset or tf.distribute.distribute_datasets_from_function. Any stateful ops that the dataset may have are currently ignored. For example, if your dataset has a map_fn that uses tf.random.uniform to rotate an image, then you have a dataset graph that depends on state (i.e the random seed) on the local machine where the python process is being executed.\n For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input. If you are interested in last partial batch handling, read this section.\n \n\n\n Args\n  dataset   tf.data.Dataset that will be sharded across all replicas using the rules stated above.  \n  options   tf.distribute.InputOptions used to control options on how this dataset is distributed.   \n \n\n\n Returns   A tf.distribute.DistributedDataset.  \n experimental_local_results View source \nexperimental_local_results(\n    value\n)\n Returns the list of all local per-replica values contained in value. \nNote: This only returns values on the worker initiated by this client. When using a tf.distribute.Strategy like tf.distribute.experimental.MultiWorkerMirroredStrategy, each worker will be its own client, and this function will only return values computed on that worker.\n\n \n\n\n Args\n  value   A value returned by experimental_run(), run(), extended.call_for_each_replica(), or a variable created in scope.   \n \n\n\n Returns   A tuple of values contained in value. If value represents a single value, this returns (value,).  \n experimental_make_numpy_dataset View source \nexperimental_make_numpy_dataset(\n    numpy_input, session=None\n)\n Makes a tf.data.Dataset for input provided via a numpy array. This avoids adding numpy_input as a large constant in the graph, and copies the data to the machine or machines that will be processing the input. Note that you will likely need to use tf.distribute.Strategy.experimental_distribute_dataset with the returned dataset to further distribute it with the strategy. Example: numpy_input = np.ones([10], dtype=np.float32)\ndataset = strategy.experimental_make_numpy_dataset(numpy_input)\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\n\n \n\n\n Args\n  numpy_input   A nest of NumPy input arrays that will be converted into a dataset. Note that lists of Numpy arrays are stacked, as that is normal tf.data.Dataset behavior.  \n  session   (TensorFlow v1.x graph execution only) A session used for initialization.   \n \n\n\n Returns   A tf.data.Dataset representing numpy_input.  \n experimental_run View source \nexperimental_run(\n    fn, input_iterator=None\n)\n Runs ops in fn on each replica, with inputs from input_iterator. DEPRECATED: This method is not available in TF 2.x. Please switch to using run instead. When eager execution is enabled, executes ops specified by fn on each replica. Otherwise, builds a graph to execute the ops on each replica. Each replica will take a single, different input from the inputs provided by one get_next call on the input iterator. fn may call tf.distribute.get_replica_context() to access members such as replica_id_in_sync_group. Key Point: Depending on the tf.distribute.Strategy implementation being used, and whether eager execution is enabled, fn may be called one or more times (once for each replica).\n \n\n\n Args\n  fn   The function to run. The inputs to the function must match the outputs of input_iterator.get_next(). The output must be a tf.nest of Tensors.  \n  input_iterator   (Optional) input iterator from which the inputs are taken.   \n \n\n\n Returns   Merged return value of fn across replicas. The structure of the return value is the same as the return value from fn. Each element in the structure can either be PerReplica (if the values are unsynchronized), Mirrored (if the values are kept in sync), or Tensor (if running on a single replica).  \n make_dataset_iterator View source \nmake_dataset_iterator(\n    dataset\n)\n Makes an iterator for input provided via dataset. DEPRECATED: This method is not available in TF 2.x. Data from the given dataset will be distributed evenly across all the compute replicas. We will assume that the input dataset is batched by the global batch size. With this assumption, we will make a best effort to divide each batch across all the replicas (one or more workers). If this effort fails, an error will be thrown, and the user should instead use make_input_fn_iterator which provides more control to the user, and does not try to divide a batch across replicas. The user could also use make_input_fn_iterator if they want to customize which input is fed to which replica/worker etc.\n \n\n\n Args\n  dataset   tf.data.Dataset that will be distributed evenly across all replicas.   \n \n\n\n Returns   An tf.distribute.InputIterator which returns inputs for each step of the computation. User should call initialize on the returned iterator.  \n make_input_fn_iterator View source \nmake_input_fn_iterator(\n    input_fn, replication_mode=tf.distribute.InputReplicationMode.PER_WORKER\n)\n Returns an iterator split across replicas created from an input function. DEPRECATED: This method is not available in TF 2.x. The input_fn should take an tf.distribute.InputContext object where information about batching and input sharding can be accessed: def input_fn(input_context):\n  batch_size = input_context.get_per_replica_batch_size(global_batch_size)\n  d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)\n  return d.shard(input_context.num_input_pipelines,\n                 input_context.input_pipeline_id)\nwith strategy.scope():\n  iterator = strategy.make_input_fn_iterator(input_fn)\n  replica_results = strategy.experimental_run(replica_fn, iterator)\n The tf.data.Dataset returned by input_fn should have a per-replica batch size, which may be computed using input_context.get_per_replica_batch_size.\n \n\n\n Args\n  input_fn   A function taking a tf.distribute.InputContext object and returning a tf.data.Dataset.  \n  replication_mode   an enum value of tf.distribute.InputReplicationMode. Only PER_WORKER is supported currently, which means there will be a single call to input_fn per worker. Replicas will dequeue from the local tf.data.Dataset on their worker.   \n \n\n\n Returns   An iterator object that should first be .initialize()-ed. It may then either be passed to strategy.experimental_run() or you can iterator.get_next() to get the next value to pass to strategy.extended.call_for_each_replica().  \n reduce View source \nreduce(\n    reduce_op, value, axis=None\n)\n Reduce value across replicas and return result on current device. \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndef step_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\n\nper_replica_result = strategy.run(step_fn)\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\ntotal\n<tf.Tensor: shape=(), dtype=int32, numpy=1>\n To see how this would look with multiple replicas, consider the same example with MirroredStrategy with 2 GPUs: strategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\ndef step_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\n\nper_replica_result = strategy.run(step_fn)\n# Check devices on which per replica result is:\nstrategy.experimental_local_results(per_replica_result)[0].device\n# /job:localhost/replica:0/task:0/device:GPU:0\nstrategy.experimental_local_results(per_replica_result)[1].device\n# /job:localhost/replica:0/task:0/device:GPU:1\n\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\n# Check device on which reduced result is:\ntotal.device\n# /job:localhost/replica:0/task:0/device:CPU:0\n\n This API is typically used for aggregating the results returned from different replicas, for reporting etc. For example, loss computed from different replicas can be averaged using this API before printing. \nNote: The result is copied to the \"current\" device - which would typically be the CPU of the worker on which the program is running. For TPUStrategy, it is the first TPU host. For multi client MultiWorkerMirroredStrategy, this is CPU of each worker.\n There are a number of different tf.distribute APIs for reducing values across replicas:  \ntf.distribute.ReplicaContext.all_reduce: This differs from Strategy.reduce in that it is for replica context and does not copy the results to the host device. all_reduce should be typically used for reductions inside the training step such as gradients. \ntf.distribute.StrategyExtended.reduce_to and tf.distribute.StrategyExtended.batch_reduce_to: These APIs are more advanced versions of Strategy.reduce as they allow customizing the destination of the result. They are also called in cross replica context.  What should axis be? Given a per-replica value returned by run, say a per-example loss, the batch will be divided across all the replicas. This function allows you to aggregate across replicas and optionally also across batch elements by specifying the axis parameter accordingly. For example, if you have a global batch size of 8 and 2 replicas, values for examples [0, 1, 2, 3] will be on replica 0 and [4, 5, 6, 7] will be on replica 1. With axis=None, reduce will aggregate only across replicas, returning [0+4, 1+5, 2+6, 3+7]. This is useful when each replica is computing a scalar or some other value that doesn't have a \"batch\" dimension (like a gradient or loss). strategy.reduce(\"sum\", per_replica_result, axis=None)\n Sometimes, you will want to aggregate across both the global batch and all replicas. You can get this behavior by specifying the batch dimension as the axis, typically axis=0. In this case it would return a scalar 0+1+2+3+4+5+6+7. strategy.reduce(\"sum\", per_replica_result, axis=0)\n If there is a last partial batch, you will need to specify an axis so that the resulting shape is consistent across replicas. So if the last batch has size 6 and it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch unless you specify axis=0. If you specify tf.distribute.ReduceOp.MEAN, using axis=0 will use the correct denominator of 6. Contrast this with computing reduce_mean to get a scalar value on each replica and this function to average those means, which will weigh some values 1/8 and others 1/4.\n \n\n\n Args\n  reduce_op   a tf.distribute.ReduceOp value specifying how values should be combined. Allows using string representation of the enum such as \"SUM\", \"MEAN\".  \n  value   a tf.distribute.DistributedValues instance, e.g. returned by Strategy.run, to be combined into a single tensor. It can also be a regular tensor when used with OneDeviceStrategy or default strategy.  \n  axis   specifies the dimension to reduce along within each replica's tensor. Should typically be set to the batch dimension, or None to only reduce across replicas (e.g. if the tensor has no batch dimension).   \n \n\n\n Returns   A Tensor.  \n run View source \nrun(\n    fn, args=(), kwargs=None, options=None\n)\n Invokes fn on each replica, with the given arguments. This method is the primary way to distribute your computation with a tf.distribute object. It invokes fn on each replica. If args or kwargs have tf.distribute.DistributedValues, such as those produced by a tf.distribute.DistributedDataset from tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function, when fn is executed on a particular replica, it will be executed with the component of tf.distribute.DistributedValues that correspond to that replica. fn is invoked under a replica context. fn may call tf.distribute.get_replica_context() to access members such as all_reduce. Please see the module-level docstring of tf.distribute for the concept of replica context. All arguments in args or kwargs should either be Python values of a nested structure of tensors, e.g. a list of tensors, in which case args and kwargs will be passed to the fn invoked on each replica. Or args or kwargs can be tf.distribute.DistributedValues containing tensors or composite tensors, i.e. tf.compat.v1.TensorInfo.CompositeTensor, in which case each fn call will get the component of a tf.distribute.DistributedValues corresponding to its replica. Key Point: Depending on the implementation of tf.distribute.Strategy and whether eager execution is enabled, fn may be called one or more times. If fn is annotated with tf.function or tf.distribute.Strategy.run is called inside a tf.function (eager execution is disabled inside a tf.function by default), fn is called once per replica to generate a Tensorflow graph, which will then be reused for execution with new inputs. Otherwise, if eager execution is enabled, fn will be called once per replica every step just like regular python code. Example usage:  Constant tensor input.  \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ntensor_input = tf.constant(3.0)\n@tf.function\ndef replica_fn(input):\n  return input*2.0\nresult = strategy.run(replica_fn, args=(tensor_input,))\nresult\nPerReplica:{\n  0: <tf.Tensor: shape=(), dtype=float32, numpy=6.0>,\n  1: <tf.Tensor: shape=(), dtype=float32, numpy=6.0>\n}\n  DistributedValues input.  \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n@tf.function\ndef run():\n  def value_fn(value_context):\n    return value_context.num_replicas_in_sync\n  distributed_values = (\n    strategy.experimental_distribute_values_from_function(\n      value_fn))\n  def replica_fn2(input):\n    return input*2\n  return strategy.run(replica_fn2, args=(distributed_values,))\nresult = run()\nresult\n<tf.Tensor: shape=(), dtype=int32, numpy=4>\n  Use tf.distribute.ReplicaContext to allreduce values.  \nstrategy = tf.distribute.MirroredStrategy([\"gpu:0\", \"gpu:1\"])\n@tf.function\ndef run():\n   def value_fn(value_context):\n     return tf.constant(value_context.replica_id_in_sync_group)\n   distributed_values = (\n       strategy.experimental_distribute_values_from_function(\n           value_fn))\n   def replica_fn(input):\n     return tf.distribute.get_replica_context().all_reduce(\"sum\", input)\n   return strategy.run(replica_fn, args=(distributed_values,))\nresult = run()\nresult\nPerReplica:{\n  0: <tf.Tensor: shape=(), dtype=int32, numpy=1>,\n  1: <tf.Tensor: shape=(), dtype=int32, numpy=1>\n}\n\n \n\n\n Args\n  fn   The function to run on each replica.  \n  args   Optional positional arguments to fn. Its element can be a Python value, a tensor or a tf.distribute.DistributedValues.  \n  kwargs   Optional keyword arguments to fn. Its element can be a Python value, a tensor or a tf.distribute.DistributedValues.  \n  options   An optional instance of tf.distribute.RunOptions specifying the options to run fn.   \n \n\n\n Returns   Merged return value of fn across replicas. The structure of the return value is the same as the return value from fn. Each element in the structure can either be tf.distribute.DistributedValues, Tensor objects, or Tensors (for example, if running on a single replica).  \n scope View source \nscope()\n Context manager to make the strategy current and distribute variables. This method returns a context manager, and is used as follows: \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n# Variable created inside scope:\nwith strategy.scope():\n  mirrored_variable = tf.Variable(1.)\nmirrored_variable\nMirroredVariable:{\n  0: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>,\n  1: <tf.Variable 'Variable/replica_1:0' shape=() dtype=float32, numpy=1.0>\n}\n# Variable created outside scope:\nregular_variable = tf.Variable(1.)\nregular_variable\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\n What happens when Strategy.scope is entered?  \nstrategy is installed in the global context as the \"current\" strategy. Inside this scope, tf.distribute.get_strategy() will now return this strategy. Outside this scope, it returns the default no-op strategy. Entering the scope also enters the \"cross-replica context\". See tf.distribute.StrategyExtended for an explanation on cross-replica and replica contexts. Variable creation inside scope is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like MirroredStrategy, TPUStrategy and MultiWorkerMiroredStrategy create variables replicated on each replica, whereas ParameterServerStrategy creates variables on the parameter servers. This is done using a custom tf.variable_creator_scope. In some strategies, a default device scope may also be entered: in MultiWorkerMiroredStrategy, a default device scope of \"/CPU:0\" is entered on each worker.  \nNote: Entering a scope does not automatically distribute a computation, except in the case of high level training framework like keras model.fit. If you're not using model.fit, you need to use strategy.run API to explicitly distribute that computation. See an example in the custom training loop tutorial.\n What should be in scope and what should be outside? There are a number of requirements on what needs to happen inside the scope. However, in places where we have information about which strategy is in use, we often enter the scope for the user, so they don't have to do it explicitly (i.e. calling those either inside or outside the scope is OK).  Anything that creates variables that should be distributed variables must be in strategy.scope. This can be either by directly putting it in scope, or relying on another API like strategy.run or model.fit to enter it for you. Any variable that is created outside scope will not be distributed and may have performance implications. Common things that create variables in TF: models, optimizers, metrics. These should always be created inside the scope. Another source of variable creation can be a checkpoint restore - when variables are created lazily. Note that any variable created inside a strategy captures the strategy information. So reading and writing to these variables outside the strategy.scope can also work seamlessly, without the user having to enter the scope. Some strategy APIs (such as strategy.run and strategy.reduce) which require to be in a strategy's scope, enter the scope for you automatically, which means when using those APIs you don't need to enter the scope yourself. When a tf.keras.Model is created inside a strategy.scope, we capture this information. When high level training frameworks methods such as model.compile, model.fit etc are then called on this model, we automatically enter the scope, as well as use this strategy to distribute the training etc. See detailed example in distributed keras tutorial. Note that simply calling the model(..) is not impacted - only high level training framework APIs are. model.compile, model.fit, model.evaluate, model.predict and model.save can all be called inside or outside the scope. The following can be either inside or outside the scope:  Creating the input datasets Defining tf.functions that represent your training step Saving APIs such as tf.saved_model.save. Loading creates variables, so that should go inside the scope if you want to train the model in a distributed way. Checkpoint saving. As mentioned above - checkpoint.restore may sometimes need to be inside scope if it creates variables. \n \n \n\n\n Returns   A context manager.  \n update_config_proto View source \nupdate_config_proto(\n    config_proto\n)\n Returns a copy of config_proto modified for use with this strategy. DEPRECATED: This method is not available in TF 2.x. The updated config has something needed to run a strategy, e.g. configuration to run collective ops, or device filters to improve distributed training performance.\n \n\n\n Args\n  config_proto   a tf.ConfigProto object.   \n \n\n\n Returns   The updated copy of the config_proto.  \n  \n"}, {"name": "tf.compat.v1.distribute.experimental.TPUStrategy", "path": "compat/v1/distribute/experimental/tpustrategy", "type": "tf.compat", "text": "tf.compat.v1.distribute.experimental.TPUStrategy TPU distribution strategy implementation. Inherits From: Strategy \ntf.compat.v1.distribute.experimental.TPUStrategy(\n    tpu_cluster_resolver=None, steps_per_run=None, device_assignment=None\n)\n\n \n\n\n Args\n  tpu_cluster_resolver   A tf.distribute.cluster_resolver.TPUClusterResolver, which provides information about the TPU cluster.  \n  steps_per_run   Number of steps to run on device before returning to the host. Note that this can have side-effects on performance, hooks, metrics, summaries etc. This parameter is only used when Distribution Strategy is used with estimator or keras.  \n  device_assignment   Optional tf.tpu.experimental.DeviceAssignment to specify the placement of replicas on the TPU cluster. Currently only supports the usecase of using a single core within a TPU cluster.   \n \n\n\n Attributes\n  cluster_resolver   Returns the cluster resolver associated with this strategy. In general, when using a multi-worker tf.distribute strategy such as tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy(), there is a tf.distribute.cluster_resolver.ClusterResolver associated with the strategy used, and such an instance is returned by this property. Strategies that intend to have an associated tf.distribute.cluster_resolver.ClusterResolver must set the relevant attribute, or override this property; otherwise, None is returned by default. Those strategies should also provide information regarding what is returned by this property. Single-worker strategies usually do not have a tf.distribute.cluster_resolver.ClusterResolver, and in those cases this property will return None. The tf.distribute.cluster_resolver.ClusterResolver may be useful when the user needs to access information such as the cluster spec, task type or task id. For example, \nos.environ['TF_CONFIG'] = json.dumps({\n'cluster': {\n'worker': [\"localhost:12345\", \"localhost:23456\"],\n'ps': [\"localhost:34567\"]\n},\n'task': {'type': 'worker', 'index': 0}\n})\n\n# This implicitly uses TF_CONFIG for the cluster and current task info.\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n\n...\n\nif strategy.cluster_resolver.task_type == 'worker':\n# Perform something that's only applicable on workers. Since we set this\n# as a worker above, this block will run on this particular instance.\nelif strategy.cluster_resolver.task_type == 'ps':\n# Perform something that's only applicable on parameter servers. Since we\n# set this as a worker above, this block will not run on this particular\n# instance.\n For more information, please see tf.distribute.cluster_resolver.ClusterResolver's API docstring. \n \n  extended   tf.distribute.StrategyExtended with additional methods.  \n  num_replicas_in_sync   Returns number of replicas over which gradients are aggregated.  \n  steps_per_run   DEPRECATED: use .extended.steps_per_run instead.    Methods distribute_datasets_from_function View source \ndistribute_datasets_from_function(\n    dataset_fn, options=None\n)\n Distributes tf.data.Dataset instances created by calls to dataset_fn. The argument dataset_fn that users pass in is an input function that has a tf.distribute.InputContext argument and returns a tf.data.Dataset instance. It is expected that the returned dataset from dataset_fn is already batched by per-replica batch size (i.e. global batch size divided by the number of replicas in sync) and sharded. tf.distribute.Strategy.distribute_datasets_from_function does not batch or shard the tf.data.Dataset instance returned from the input function. dataset_fn will be called on the CPU device of each of the workers and each generates a dataset where every replica on that worker will dequeue one batch of inputs (i.e. if a worker has two replicas, two batches will be dequeued from the Dataset every step). This method can be used for several purposes. First, it allows you to specify your own batching and sharding logic. (In contrast, tf.distribute.experimental_distribute_dataset does batching and sharding for you.) For example, where experimental_distribute_dataset is unable to shard the input files, this method might be used to manually shard the dataset (avoiding the slow fallback behavior in experimental_distribute_dataset). In cases where the dataset is infinite, this sharding can be done by creating dataset replicas that differ only in their random seed. The dataset_fn should take an tf.distribute.InputContext instance where information about batching and input replication can be accessed. You can use element_spec property of the tf.distribute.DistributedDataset returned by this API to query the tf.TypeSpec of the elements returned by the iterator. This can be used to set the input_signature property of a tf.function. Follow tf.distribute.DistributedDataset.element_spec to see an example. Key Point: The tf.data.Dataset returned by dataset_fn should have a per-replica batch size, unlike experimental_distribute_dataset, which uses the global batch size. This may be computed using input_context.get_per_replica_batch_size.\nNote: If you are using TPUStrategy, the order in which the data is processed by the workers when using tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function is not guaranteed. This is typically required if you are using tf.distribute to scale prediction. You can however insert an index for each element in the batch and order outputs accordingly. Refer to this snippet for an example of how to order outputs.\n\n\nNote: Stateful dataset transformations are currently not supported with tf.distribute.experimental_distribute_dataset or tf.distribute.distribute_datasets_from_function. Any stateful ops that the dataset may have are currently ignored. For example, if your dataset has a map_fn that uses tf.random.uniform to rotate an image, then you have a dataset graph that depends on state (i.e the random seed) on the local machine where the python process is being executed.\n For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input). If you are interested in last partial batch handling, read this section.\n \n\n\n Args\n  dataset_fn   A function taking a tf.distribute.InputContext instance and returning a tf.data.Dataset.  \n  options   tf.distribute.InputOptions used to control options on how this dataset is distributed.   \n \n\n\n Returns   A tf.distribute.DistributedDataset.  \n experimental_distribute_dataset View source \nexperimental_distribute_dataset(\n    dataset, options=None\n)\n Creates tf.distribute.DistributedDataset from tf.data.Dataset. The returned tf.distribute.DistributedDataset can be iterated over similar to regular datasets. NOTE: The user cannot add any more transformations to a tf.distribute.DistributedDataset. You can only create an iterator or examine the tf.TypeSpec of the data generated by it. See API docs of tf.distribute.DistributedDataset to learn more. The following is an example: \nglobal_batch_size = 2\n# Passing the devices is optional.\nstrategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\n# Create a dataset\ndataset = tf.data.Dataset.range(4).batch(global_batch_size)\n# Distribute that dataset\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\n@tf.function\ndef replica_fn(input):\n  return input*2\nresult = []\n# Iterate over the `tf.distribute.DistributedDataset`\nfor x in dist_dataset:\n  # process dataset elements\n  result.append(strategy.run(replica_fn, args=(x,)))\nprint(result)\n[PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>\n}, PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([4])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([6])>\n}]\n Three key actions happending under the hood of this method are batching, sharding, and prefetching. In the code snippet above, dataset is batched by global_batch_size, and calling experimental_distribute_dataset on it rebatches dataset to a new batch size that is equal to the global batch size divided by the number of replicas in sync. We iterate through it using a Pythonic for loop. x is a tf.distribute.DistributedValues containing data for all replicas, and each replica gets data of the new batch size. tf.distribute.Strategy.run will take care of feeding the right per-replica data in x to the right replica_fn executed on each replica. Sharding contains autosharding across multiple workers and within every worker. First, in multi-worker distributed training (i.e. when you use tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy), autosharding a dataset over a set of workers means that each worker is assigned a subset of the entire dataset (if the right tf.data.experimental.AutoShardPolicy is set). This is to ensure that at each step, a global batch size of non-overlapping dataset elements will be processed by each worker. Autosharding has a couple of different options that can be specified using tf.data.experimental.DistributeOptions. Then, sharding within each worker means the method will split the data among all the worker devices (if more than one a present). This will happen regardless of multi-worker autosharding. \nNote: for autosharding across multiple workers, the default mode is tf.data.experimental.AutoShardPolicy.AUTO. This mode will attempt to shard the input dataset by files if the dataset is being created out of reader datasets (e.g. tf.data.TFRecordDataset, tf.data.TextLineDataset, etc.) or otherwise shard the dataset by data, where each of the workers will read the entire dataset and only process the shard assigned to it. However, if you have less than one input file per worker, we suggest that you disable dataset autosharding across workers by setting the tf.data.experimental.DistributeOptions.auto_shard_policy to be tf.data.experimental.AutoShardPolicy.OFF.\n By default, this method adds a prefetch transformation at the end of the user provided tf.data.Dataset instance. The argument to the prefetch transformation which is buffer_size is equal to the number of replicas in sync. If the above batch splitting and dataset sharding logic is undesirable, please use tf.distribute.Strategy.distribute_datasets_from_function instead, which does not do any automatic batching or sharding for you. \nNote: If you are using TPUStrategy, the order in which the data is processed by the workers when using tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function is not guaranteed. This is typically required if you are using tf.distribute to scale prediction. You can however insert an index for each element in the batch and order outputs accordingly. Refer to this snippet for an example of how to order outputs.\n\n\nNote: Stateful dataset transformations are currently not supported with tf.distribute.experimental_distribute_dataset or tf.distribute.distribute_datasets_from_function. Any stateful ops that the dataset may have are currently ignored. For example, if your dataset has a map_fn that uses tf.random.uniform to rotate an image, then you have a dataset graph that depends on state (i.e the random seed) on the local machine where the python process is being executed.\n For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input. If you are interested in last partial batch handling, read this section.\n \n\n\n Args\n  dataset   tf.data.Dataset that will be sharded across all replicas using the rules stated above.  \n  options   tf.distribute.InputOptions used to control options on how this dataset is distributed.   \n \n\n\n Returns   A tf.distribute.DistributedDataset.  \n experimental_local_results View source \nexperimental_local_results(\n    value\n)\n Returns the list of all local per-replica values contained in value. \nNote: This only returns values on the worker initiated by this client. When using a tf.distribute.Strategy like tf.distribute.experimental.MultiWorkerMirroredStrategy, each worker will be its own client, and this function will only return values computed on that worker.\n\n \n\n\n Args\n  value   A value returned by experimental_run(), run(), extended.call_for_each_replica(), or a variable created in scope.   \n \n\n\n Returns   A tuple of values contained in value. If value represents a single value, this returns (value,).  \n experimental_make_numpy_dataset View source \nexperimental_make_numpy_dataset(\n    numpy_input, session=None\n)\n Makes a tf.data.Dataset for input provided via a numpy array. This avoids adding numpy_input as a large constant in the graph, and copies the data to the machine or machines that will be processing the input. Note that you will likely need to use tf.distribute.Strategy.experimental_distribute_dataset with the returned dataset to further distribute it with the strategy. Example: numpy_input = np.ones([10], dtype=np.float32)\ndataset = strategy.experimental_make_numpy_dataset(numpy_input)\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\n\n \n\n\n Args\n  numpy_input   A nest of NumPy input arrays that will be converted into a dataset. Note that lists of Numpy arrays are stacked, as that is normal tf.data.Dataset behavior.  \n  session   (TensorFlow v1.x graph execution only) A session used for initialization.   \n \n\n\n Returns   A tf.data.Dataset representing numpy_input.  \n experimental_run View source \nexperimental_run(\n    fn, input_iterator=None\n)\n Runs ops in fn on each replica, with inputs from input_iterator. DEPRECATED: This method is not available in TF 2.x. Please switch to using run instead. When eager execution is enabled, executes ops specified by fn on each replica. Otherwise, builds a graph to execute the ops on each replica. Each replica will take a single, different input from the inputs provided by one get_next call on the input iterator. fn may call tf.distribute.get_replica_context() to access members such as replica_id_in_sync_group. Key Point: Depending on the tf.distribute.Strategy implementation being used, and whether eager execution is enabled, fn may be called one or more times (once for each replica).\n \n\n\n Args\n  fn   The function to run. The inputs to the function must match the outputs of input_iterator.get_next(). The output must be a tf.nest of Tensors.  \n  input_iterator   (Optional) input iterator from which the inputs are taken.   \n \n\n\n Returns   Merged return value of fn across replicas. The structure of the return value is the same as the return value from fn. Each element in the structure can either be PerReplica (if the values are unsynchronized), Mirrored (if the values are kept in sync), or Tensor (if running on a single replica).  \n make_dataset_iterator View source \nmake_dataset_iterator(\n    dataset\n)\n Makes an iterator for input provided via dataset. DEPRECATED: This method is not available in TF 2.x. Data from the given dataset will be distributed evenly across all the compute replicas. We will assume that the input dataset is batched by the global batch size. With this assumption, we will make a best effort to divide each batch across all the replicas (one or more workers). If this effort fails, an error will be thrown, and the user should instead use make_input_fn_iterator which provides more control to the user, and does not try to divide a batch across replicas. The user could also use make_input_fn_iterator if they want to customize which input is fed to which replica/worker etc.\n \n\n\n Args\n  dataset   tf.data.Dataset that will be distributed evenly across all replicas.   \n \n\n\n Returns   An tf.distribute.InputIterator which returns inputs for each step of the computation. User should call initialize on the returned iterator.  \n make_input_fn_iterator View source \nmake_input_fn_iterator(\n    input_fn, replication_mode=tf.distribute.InputReplicationMode.PER_WORKER\n)\n Returns an iterator split across replicas created from an input function. DEPRECATED: This method is not available in TF 2.x. The input_fn should take an tf.distribute.InputContext object where information about batching and input sharding can be accessed: def input_fn(input_context):\n  batch_size = input_context.get_per_replica_batch_size(global_batch_size)\n  d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)\n  return d.shard(input_context.num_input_pipelines,\n                 input_context.input_pipeline_id)\nwith strategy.scope():\n  iterator = strategy.make_input_fn_iterator(input_fn)\n  replica_results = strategy.experimental_run(replica_fn, iterator)\n The tf.data.Dataset returned by input_fn should have a per-replica batch size, which may be computed using input_context.get_per_replica_batch_size.\n \n\n\n Args\n  input_fn   A function taking a tf.distribute.InputContext object and returning a tf.data.Dataset.  \n  replication_mode   an enum value of tf.distribute.InputReplicationMode. Only PER_WORKER is supported currently, which means there will be a single call to input_fn per worker. Replicas will dequeue from the local tf.data.Dataset on their worker.   \n \n\n\n Returns   An iterator object that should first be .initialize()-ed. It may then either be passed to strategy.experimental_run() or you can iterator.get_next() to get the next value to pass to strategy.extended.call_for_each_replica().  \n reduce View source \nreduce(\n    reduce_op, value, axis=None\n)\n Reduce value across replicas and return result on current device. \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndef step_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\n\nper_replica_result = strategy.run(step_fn)\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\ntotal\n<tf.Tensor: shape=(), dtype=int32, numpy=1>\n To see how this would look with multiple replicas, consider the same example with MirroredStrategy with 2 GPUs: strategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\ndef step_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\n\nper_replica_result = strategy.run(step_fn)\n# Check devices on which per replica result is:\nstrategy.experimental_local_results(per_replica_result)[0].device\n# /job:localhost/replica:0/task:0/device:GPU:0\nstrategy.experimental_local_results(per_replica_result)[1].device\n# /job:localhost/replica:0/task:0/device:GPU:1\n\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\n# Check device on which reduced result is:\ntotal.device\n# /job:localhost/replica:0/task:0/device:CPU:0\n\n This API is typically used for aggregating the results returned from different replicas, for reporting etc. For example, loss computed from different replicas can be averaged using this API before printing. \nNote: The result is copied to the \"current\" device - which would typically be the CPU of the worker on which the program is running. For TPUStrategy, it is the first TPU host. For multi client MultiWorkerMirroredStrategy, this is CPU of each worker.\n There are a number of different tf.distribute APIs for reducing values across replicas:  \ntf.distribute.ReplicaContext.all_reduce: This differs from Strategy.reduce in that it is for replica context and does not copy the results to the host device. all_reduce should be typically used for reductions inside the training step such as gradients. \ntf.distribute.StrategyExtended.reduce_to and tf.distribute.StrategyExtended.batch_reduce_to: These APIs are more advanced versions of Strategy.reduce as they allow customizing the destination of the result. They are also called in cross replica context.  What should axis be? Given a per-replica value returned by run, say a per-example loss, the batch will be divided across all the replicas. This function allows you to aggregate across replicas and optionally also across batch elements by specifying the axis parameter accordingly. For example, if you have a global batch size of 8 and 2 replicas, values for examples [0, 1, 2, 3] will be on replica 0 and [4, 5, 6, 7] will be on replica 1. With axis=None, reduce will aggregate only across replicas, returning [0+4, 1+5, 2+6, 3+7]. This is useful when each replica is computing a scalar or some other value that doesn't have a \"batch\" dimension (like a gradient or loss). strategy.reduce(\"sum\", per_replica_result, axis=None)\n Sometimes, you will want to aggregate across both the global batch and all replicas. You can get this behavior by specifying the batch dimension as the axis, typically axis=0. In this case it would return a scalar 0+1+2+3+4+5+6+7. strategy.reduce(\"sum\", per_replica_result, axis=0)\n If there is a last partial batch, you will need to specify an axis so that the resulting shape is consistent across replicas. So if the last batch has size 6 and it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch unless you specify axis=0. If you specify tf.distribute.ReduceOp.MEAN, using axis=0 will use the correct denominator of 6. Contrast this with computing reduce_mean to get a scalar value on each replica and this function to average those means, which will weigh some values 1/8 and others 1/4.\n \n\n\n Args\n  reduce_op   a tf.distribute.ReduceOp value specifying how values should be combined. Allows using string representation of the enum such as \"SUM\", \"MEAN\".  \n  value   a tf.distribute.DistributedValues instance, e.g. returned by Strategy.run, to be combined into a single tensor. It can also be a regular tensor when used with OneDeviceStrategy or default strategy.  \n  axis   specifies the dimension to reduce along within each replica's tensor. Should typically be set to the batch dimension, or None to only reduce across replicas (e.g. if the tensor has no batch dimension).   \n \n\n\n Returns   A Tensor.  \n run View source \nrun(\n    fn, args=(), kwargs=None, options=None\n)\n Run fn on each replica, with the given arguments. Executes ops specified by fn on each replica. If args or kwargs have \"per-replica\" values, such as those produced by a \"distributed Dataset\", when fn is executed on a particular replica, it will be executed with the component of those \"per-replica\" values that correspond to that replica. fn may call tf.distribute.get_replica_context() to access members such as all_reduce. All arguments in args or kwargs should either be nest of tensors or per-replica objects containing tensors or composite tensors. Users can pass strategy specific options to options argument. An example to enable bucketizing dynamic shapes in TPUStrategy.run is: \nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\ntf.config.experimental_connect_to_cluster(resolver)\ntf.tpu.experimental.initialize_tpu_system(resolver)\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\n \noptions = tf.distribute.RunOptions(\n    experimental_bucketizing_dynamic_shape=True)\n \ndataset = tf.data.Dataset.range(\n   strategy.num_replicas_in_sync, output_type=dtypes.float32).batch(\n       strategy.num_replicas_in_sync, drop_remainder=True)\ninput_iterator = iter(strategy.experimental_distribute_dataset(dataset))\n \n@tf.function()\ndef step_fn(inputs):\n output = tf.reduce_sum(inputs)\n return output\n \nstrategy.run(step_fn, args=(next(input_iterator),), options=options)\n\n \n\n\n Args\n  fn   The function to run. The output must be a tf.nest of Tensors.  \n  args   (Optional) Positional arguments to fn.  \n  kwargs   (Optional) Keyword arguments to fn.  \n  options   (Optional) An instance of tf.distribute.RunOptions specifying the options to run fn.   \n \n\n\n Returns   Merged return value of fn across replicas. The structure of the return value is the same as the return value from fn. Each element in the structure can either be \"per-replica\" Tensor objects or Tensors (for example, if running on a single replica).  \n scope View source \nscope()\n Context manager to make the strategy current and distribute variables. This method returns a context manager, and is used as follows: \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n# Variable created inside scope:\nwith strategy.scope():\n  mirrored_variable = tf.Variable(1.)\nmirrored_variable\nMirroredVariable:{\n  0: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>,\n  1: <tf.Variable 'Variable/replica_1:0' shape=() dtype=float32, numpy=1.0>\n}\n# Variable created outside scope:\nregular_variable = tf.Variable(1.)\nregular_variable\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\n What happens when Strategy.scope is entered?  \nstrategy is installed in the global context as the \"current\" strategy. Inside this scope, tf.distribute.get_strategy() will now return this strategy. Outside this scope, it returns the default no-op strategy. Entering the scope also enters the \"cross-replica context\". See tf.distribute.StrategyExtended for an explanation on cross-replica and replica contexts. Variable creation inside scope is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like MirroredStrategy, TPUStrategy and MultiWorkerMiroredStrategy create variables replicated on each replica, whereas ParameterServerStrategy creates variables on the parameter servers. This is done using a custom tf.variable_creator_scope. In some strategies, a default device scope may also be entered: in MultiWorkerMiroredStrategy, a default device scope of \"/CPU:0\" is entered on each worker.  \nNote: Entering a scope does not automatically distribute a computation, except in the case of high level training framework like keras model.fit. If you're not using model.fit, you need to use strategy.run API to explicitly distribute that computation. See an example in the custom training loop tutorial.\n What should be in scope and what should be outside? There are a number of requirements on what needs to happen inside the scope. However, in places where we have information about which strategy is in use, we often enter the scope for the user, so they don't have to do it explicitly (i.e. calling those either inside or outside the scope is OK).  Anything that creates variables that should be distributed variables must be in strategy.scope. This can be either by directly putting it in scope, or relying on another API like strategy.run or model.fit to enter it for you. Any variable that is created outside scope will not be distributed and may have performance implications. Common things that create variables in TF: models, optimizers, metrics. These should always be created inside the scope. Another source of variable creation can be a checkpoint restore - when variables are created lazily. Note that any variable created inside a strategy captures the strategy information. So reading and writing to these variables outside the strategy.scope can also work seamlessly, without the user having to enter the scope. Some strategy APIs (such as strategy.run and strategy.reduce) which require to be in a strategy's scope, enter the scope for you automatically, which means when using those APIs you don't need to enter the scope yourself. When a tf.keras.Model is created inside a strategy.scope, we capture this information. When high level training frameworks methods such as model.compile, model.fit etc are then called on this model, we automatically enter the scope, as well as use this strategy to distribute the training etc. See detailed example in distributed keras tutorial. Note that simply calling the model(..) is not impacted - only high level training framework APIs are. model.compile, model.fit, model.evaluate, model.predict and model.save can all be called inside or outside the scope. The following can be either inside or outside the scope:  Creating the input datasets Defining tf.functions that represent your training step Saving APIs such as tf.saved_model.save. Loading creates variables, so that should go inside the scope if you want to train the model in a distributed way. Checkpoint saving. As mentioned above - checkpoint.restore may sometimes need to be inside scope if it creates variables. \n \n \n\n\n Returns   A context manager.  \n update_config_proto View source \nupdate_config_proto(\n    config_proto\n)\n Returns a copy of config_proto modified for use with this strategy. DEPRECATED: This method is not available in TF 2.x. The updated config has something needed to run a strategy, e.g. configuration to run collective ops, or device filters to improve distributed training performance.\n \n\n\n Args\n  config_proto   a tf.ConfigProto object.   \n \n\n\n Returns   The updated copy of the config_proto.  \n  \n"}, {"name": "tf.compat.v1.distribute.get_loss_reduction", "path": "compat/v1/distribute/get_loss_reduction", "type": "tf.compat", "text": "tf.compat.v1.distribute.get_loss_reduction tf.distribute.ReduceOp corresponding to the last loss reduction. \ntf.compat.v1.distribute.get_loss_reduction()\n This is used to decide whether loss should be scaled in optimizer (used only for estimator + v1 optimizer use case).\n \n\n\n Returns   tf.distribute.ReduceOp corresponding to the last loss reduction for estimator and v1 optimizer use case. tf.distribute.ReduceOp.SUM otherwise.  \n  \n"}, {"name": "tf.compat.v1.distribute.MirroredStrategy", "path": "compat/v1/distribute/mirroredstrategy", "type": "tf.compat", "text": "tf.compat.v1.distribute.MirroredStrategy Synchronous training across multiple replicas on one machine. Inherits From: Strategy \ntf.compat.v1.distribute.MirroredStrategy(\n    devices=None, cross_device_ops=None\n)\n This strategy is typically used for training on one machine with multiple GPUs. For TPUs, use tf.distribute.TPUStrategy. To use MirroredStrategy with multiple workers, please refer to tf.distribute.experimental.MultiWorkerMirroredStrategy. For example, a variable created under a MirroredStrategy is a MirroredVariable. If no devices are specified in the constructor argument of the strategy then it will use all the available GPUs. If no GPUs are found, it will use the available CPUs. Note that TensorFlow treats all CPUs on a machine as a single device, and uses threads internally for parallelism. \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\nwith strategy.scope():\n  x = tf.Variable(1.)\nx\nMirroredVariable:{\n  0: <tf.Variable ... shape=() dtype=float32, numpy=1.0>,\n  1: <tf.Variable ... shape=() dtype=float32, numpy=1.0>\n}\n While using distribution strategies, all the variable creation should be done within the strategy's scope. This will replicate the variables across all the replicas and keep them in sync using an all-reduce algorithm. Variables created inside a MirroredStrategy which is wrapped with a tf.function are still MirroredVariables. \nx = []\n@tf.function  # Wrap the function with tf.function.\ndef create_variable():\n  if not x:\n    x.append(tf.Variable(1.))\n  return x[0]\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\nwith strategy.scope():\n  _ = create_variable()\n  print(x[0])\nMirroredVariable:{\n  0: <tf.Variable ... shape=() dtype=float32, numpy=1.0>,\n  1: <tf.Variable ... shape=() dtype=float32, numpy=1.0>\n}\n experimental_distribute_dataset can be used to distribute the dataset across the replicas when writing your own training loop. If you are using .fit and .compile methods available in tf.keras, then tf.keras will handle the distribution for you. For example: my_strategy = tf.distribute.MirroredStrategy()\nwith my_strategy.scope():\n  @tf.function\n  def distribute_train_epoch(dataset):\n    def replica_fn(input):\n      # process input and return result\n      return result\n\n    total_result = 0\n    for x in dataset:\n      per_replica_result = my_strategy.run(replica_fn, args=(x,))\n      total_result += my_strategy.reduce(tf.distribute.ReduceOp.SUM,\n                                         per_replica_result, axis=None)\n    return total_result\n\n  dist_dataset = my_strategy.experimental_distribute_dataset(dataset)\n  for _ in range(EPOCHS):\n    train_result = distribute_train_epoch(dist_dataset)\n\n \n\n\n Args\n  devices   a list of device strings such as ['/gpu:0', '/gpu:1']. If None, all available GPUs are used. If no GPUs are found, CPU is used.  \n  cross_device_ops   optional, a descedant of CrossDeviceOps. If this is not set, NcclAllReduce() will be used by default. One would customize this if NCCL isn't available or if a special implementation that exploits the particular hardware is available.   \n \n\n\n Attributes\n  cluster_resolver   Returns the cluster resolver associated with this strategy. In general, when using a multi-worker tf.distribute strategy such as tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy(), there is a tf.distribute.cluster_resolver.ClusterResolver associated with the strategy used, and such an instance is returned by this property. Strategies that intend to have an associated tf.distribute.cluster_resolver.ClusterResolver must set the relevant attribute, or override this property; otherwise, None is returned by default. Those strategies should also provide information regarding what is returned by this property. Single-worker strategies usually do not have a tf.distribute.cluster_resolver.ClusterResolver, and in those cases this property will return None. The tf.distribute.cluster_resolver.ClusterResolver may be useful when the user needs to access information such as the cluster spec, task type or task id. For example, \nos.environ['TF_CONFIG'] = json.dumps({\n'cluster': {\n'worker': [\"localhost:12345\", \"localhost:23456\"],\n'ps': [\"localhost:34567\"]\n},\n'task': {'type': 'worker', 'index': 0}\n})\n\n# This implicitly uses TF_CONFIG for the cluster and current task info.\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n\n...\n\nif strategy.cluster_resolver.task_type == 'worker':\n# Perform something that's only applicable on workers. Since we set this\n# as a worker above, this block will run on this particular instance.\nelif strategy.cluster_resolver.task_type == 'ps':\n# Perform something that's only applicable on parameter servers. Since we\n# set this as a worker above, this block will not run on this particular\n# instance.\n For more information, please see tf.distribute.cluster_resolver.ClusterResolver's API docstring. \n \n  extended   tf.distribute.StrategyExtended with additional methods.  \n  num_replicas_in_sync   Returns number of replicas over which gradients are aggregated.    Methods distribute_datasets_from_function View source \ndistribute_datasets_from_function(\n    dataset_fn, options=None\n)\n Distributes tf.data.Dataset instances created by calls to dataset_fn. The argument dataset_fn that users pass in is an input function that has a tf.distribute.InputContext argument and returns a tf.data.Dataset instance. It is expected that the returned dataset from dataset_fn is already batched by per-replica batch size (i.e. global batch size divided by the number of replicas in sync) and sharded. tf.distribute.Strategy.distribute_datasets_from_function does not batch or shard the tf.data.Dataset instance returned from the input function. dataset_fn will be called on the CPU device of each of the workers and each generates a dataset where every replica on that worker will dequeue one batch of inputs (i.e. if a worker has two replicas, two batches will be dequeued from the Dataset every step). This method can be used for several purposes. First, it allows you to specify your own batching and sharding logic. (In contrast, tf.distribute.experimental_distribute_dataset does batching and sharding for you.) For example, where experimental_distribute_dataset is unable to shard the input files, this method might be used to manually shard the dataset (avoiding the slow fallback behavior in experimental_distribute_dataset). In cases where the dataset is infinite, this sharding can be done by creating dataset replicas that differ only in their random seed. The dataset_fn should take an tf.distribute.InputContext instance where information about batching and input replication can be accessed. You can use element_spec property of the tf.distribute.DistributedDataset returned by this API to query the tf.TypeSpec of the elements returned by the iterator. This can be used to set the input_signature property of a tf.function. Follow tf.distribute.DistributedDataset.element_spec to see an example. Key Point: The tf.data.Dataset returned by dataset_fn should have a per-replica batch size, unlike experimental_distribute_dataset, which uses the global batch size. This may be computed using input_context.get_per_replica_batch_size.\nNote: If you are using TPUStrategy, the order in which the data is processed by the workers when using tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function is not guaranteed. This is typically required if you are using tf.distribute to scale prediction. You can however insert an index for each element in the batch and order outputs accordingly. Refer to this snippet for an example of how to order outputs.\n\n\nNote: Stateful dataset transformations are currently not supported with tf.distribute.experimental_distribute_dataset or tf.distribute.distribute_datasets_from_function. Any stateful ops that the dataset may have are currently ignored. For example, if your dataset has a map_fn that uses tf.random.uniform to rotate an image, then you have a dataset graph that depends on state (i.e the random seed) on the local machine where the python process is being executed.\n For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input). If you are interested in last partial batch handling, read this section.\n \n\n\n Args\n  dataset_fn   A function taking a tf.distribute.InputContext instance and returning a tf.data.Dataset.  \n  options   tf.distribute.InputOptions used to control options on how this dataset is distributed.   \n \n\n\n Returns   A tf.distribute.DistributedDataset.  \n experimental_distribute_dataset View source \nexperimental_distribute_dataset(\n    dataset, options=None\n)\n Creates tf.distribute.DistributedDataset from tf.data.Dataset. The returned tf.distribute.DistributedDataset can be iterated over similar to regular datasets. NOTE: The user cannot add any more transformations to a tf.distribute.DistributedDataset. You can only create an iterator or examine the tf.TypeSpec of the data generated by it. See API docs of tf.distribute.DistributedDataset to learn more. The following is an example: \nglobal_batch_size = 2\n# Passing the devices is optional.\nstrategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\n# Create a dataset\ndataset = tf.data.Dataset.range(4).batch(global_batch_size)\n# Distribute that dataset\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\n@tf.function\ndef replica_fn(input):\n  return input*2\nresult = []\n# Iterate over the `tf.distribute.DistributedDataset`\nfor x in dist_dataset:\n  # process dataset elements\n  result.append(strategy.run(replica_fn, args=(x,)))\nprint(result)\n[PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>\n}, PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([4])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([6])>\n}]\n Three key actions happending under the hood of this method are batching, sharding, and prefetching. In the code snippet above, dataset is batched by global_batch_size, and calling experimental_distribute_dataset on it rebatches dataset to a new batch size that is equal to the global batch size divided by the number of replicas in sync. We iterate through it using a Pythonic for loop. x is a tf.distribute.DistributedValues containing data for all replicas, and each replica gets data of the new batch size. tf.distribute.Strategy.run will take care of feeding the right per-replica data in x to the right replica_fn executed on each replica. Sharding contains autosharding across multiple workers and within every worker. First, in multi-worker distributed training (i.e. when you use tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy), autosharding a dataset over a set of workers means that each worker is assigned a subset of the entire dataset (if the right tf.data.experimental.AutoShardPolicy is set). This is to ensure that at each step, a global batch size of non-overlapping dataset elements will be processed by each worker. Autosharding has a couple of different options that can be specified using tf.data.experimental.DistributeOptions. Then, sharding within each worker means the method will split the data among all the worker devices (if more than one a present). This will happen regardless of multi-worker autosharding. \nNote: for autosharding across multiple workers, the default mode is tf.data.experimental.AutoShardPolicy.AUTO. This mode will attempt to shard the input dataset by files if the dataset is being created out of reader datasets (e.g. tf.data.TFRecordDataset, tf.data.TextLineDataset, etc.) or otherwise shard the dataset by data, where each of the workers will read the entire dataset and only process the shard assigned to it. However, if you have less than one input file per worker, we suggest that you disable dataset autosharding across workers by setting the tf.data.experimental.DistributeOptions.auto_shard_policy to be tf.data.experimental.AutoShardPolicy.OFF.\n By default, this method adds a prefetch transformation at the end of the user provided tf.data.Dataset instance. The argument to the prefetch transformation which is buffer_size is equal to the number of replicas in sync. If the above batch splitting and dataset sharding logic is undesirable, please use tf.distribute.Strategy.distribute_datasets_from_function instead, which does not do any automatic batching or sharding for you. \nNote: If you are using TPUStrategy, the order in which the data is processed by the workers when using tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function is not guaranteed. This is typically required if you are using tf.distribute to scale prediction. You can however insert an index for each element in the batch and order outputs accordingly. Refer to this snippet for an example of how to order outputs.\n\n\nNote: Stateful dataset transformations are currently not supported with tf.distribute.experimental_distribute_dataset or tf.distribute.distribute_datasets_from_function. Any stateful ops that the dataset may have are currently ignored. For example, if your dataset has a map_fn that uses tf.random.uniform to rotate an image, then you have a dataset graph that depends on state (i.e the random seed) on the local machine where the python process is being executed.\n For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input. If you are interested in last partial batch handling, read this section.\n \n\n\n Args\n  dataset   tf.data.Dataset that will be sharded across all replicas using the rules stated above.  \n  options   tf.distribute.InputOptions used to control options on how this dataset is distributed.   \n \n\n\n Returns   A tf.distribute.DistributedDataset.  \n experimental_local_results View source \nexperimental_local_results(\n    value\n)\n Returns the list of all local per-replica values contained in value. \nNote: This only returns values on the worker initiated by this client. When using a tf.distribute.Strategy like tf.distribute.experimental.MultiWorkerMirroredStrategy, each worker will be its own client, and this function will only return values computed on that worker.\n\n \n\n\n Args\n  value   A value returned by experimental_run(), run(), extended.call_for_each_replica(), or a variable created in scope.   \n \n\n\n Returns   A tuple of values contained in value. If value represents a single value, this returns (value,).  \n experimental_make_numpy_dataset View source \nexperimental_make_numpy_dataset(\n    numpy_input, session=None\n)\n Makes a tf.data.Dataset for input provided via a numpy array. This avoids adding numpy_input as a large constant in the graph, and copies the data to the machine or machines that will be processing the input. Note that you will likely need to use tf.distribute.Strategy.experimental_distribute_dataset with the returned dataset to further distribute it with the strategy. Example: numpy_input = np.ones([10], dtype=np.float32)\ndataset = strategy.experimental_make_numpy_dataset(numpy_input)\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\n\n \n\n\n Args\n  numpy_input   A nest of NumPy input arrays that will be converted into a dataset. Note that lists of Numpy arrays are stacked, as that is normal tf.data.Dataset behavior.  \n  session   (TensorFlow v1.x graph execution only) A session used for initialization.   \n \n\n\n Returns   A tf.data.Dataset representing numpy_input.  \n experimental_run View source \nexperimental_run(\n    fn, input_iterator=None\n)\n Runs ops in fn on each replica, with inputs from input_iterator. DEPRECATED: This method is not available in TF 2.x. Please switch to using run instead. When eager execution is enabled, executes ops specified by fn on each replica. Otherwise, builds a graph to execute the ops on each replica. Each replica will take a single, different input from the inputs provided by one get_next call on the input iterator. fn may call tf.distribute.get_replica_context() to access members such as replica_id_in_sync_group. Key Point: Depending on the tf.distribute.Strategy implementation being used, and whether eager execution is enabled, fn may be called one or more times (once for each replica).\n \n\n\n Args\n  fn   The function to run. The inputs to the function must match the outputs of input_iterator.get_next(). The output must be a tf.nest of Tensors.  \n  input_iterator   (Optional) input iterator from which the inputs are taken.   \n \n\n\n Returns   Merged return value of fn across replicas. The structure of the return value is the same as the return value from fn. Each element in the structure can either be PerReplica (if the values are unsynchronized), Mirrored (if the values are kept in sync), or Tensor (if running on a single replica).  \n make_dataset_iterator View source \nmake_dataset_iterator(\n    dataset\n)\n Makes an iterator for input provided via dataset. DEPRECATED: This method is not available in TF 2.x. Data from the given dataset will be distributed evenly across all the compute replicas. We will assume that the input dataset is batched by the global batch size. With this assumption, we will make a best effort to divide each batch across all the replicas (one or more workers). If this effort fails, an error will be thrown, and the user should instead use make_input_fn_iterator which provides more control to the user, and does not try to divide a batch across replicas. The user could also use make_input_fn_iterator if they want to customize which input is fed to which replica/worker etc.\n \n\n\n Args\n  dataset   tf.data.Dataset that will be distributed evenly across all replicas.   \n \n\n\n Returns   An tf.distribute.InputIterator which returns inputs for each step of the computation. User should call initialize on the returned iterator.  \n make_input_fn_iterator View source \nmake_input_fn_iterator(\n    input_fn, replication_mode=tf.distribute.InputReplicationMode.PER_WORKER\n)\n Returns an iterator split across replicas created from an input function. DEPRECATED: This method is not available in TF 2.x. The input_fn should take an tf.distribute.InputContext object where information about batching and input sharding can be accessed: def input_fn(input_context):\n  batch_size = input_context.get_per_replica_batch_size(global_batch_size)\n  d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)\n  return d.shard(input_context.num_input_pipelines,\n                 input_context.input_pipeline_id)\nwith strategy.scope():\n  iterator = strategy.make_input_fn_iterator(input_fn)\n  replica_results = strategy.experimental_run(replica_fn, iterator)\n The tf.data.Dataset returned by input_fn should have a per-replica batch size, which may be computed using input_context.get_per_replica_batch_size.\n \n\n\n Args\n  input_fn   A function taking a tf.distribute.InputContext object and returning a tf.data.Dataset.  \n  replication_mode   an enum value of tf.distribute.InputReplicationMode. Only PER_WORKER is supported currently, which means there will be a single call to input_fn per worker. Replicas will dequeue from the local tf.data.Dataset on their worker.   \n \n\n\n Returns   An iterator object that should first be .initialize()-ed. It may then either be passed to strategy.experimental_run() or you can iterator.get_next() to get the next value to pass to strategy.extended.call_for_each_replica().  \n reduce View source \nreduce(\n    reduce_op, value, axis=None\n)\n Reduce value across replicas and return result on current device. \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndef step_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\n\nper_replica_result = strategy.run(step_fn)\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\ntotal\n<tf.Tensor: shape=(), dtype=int32, numpy=1>\n To see how this would look with multiple replicas, consider the same example with MirroredStrategy with 2 GPUs: strategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\ndef step_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\n\nper_replica_result = strategy.run(step_fn)\n# Check devices on which per replica result is:\nstrategy.experimental_local_results(per_replica_result)[0].device\n# /job:localhost/replica:0/task:0/device:GPU:0\nstrategy.experimental_local_results(per_replica_result)[1].device\n# /job:localhost/replica:0/task:0/device:GPU:1\n\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\n# Check device on which reduced result is:\ntotal.device\n# /job:localhost/replica:0/task:0/device:CPU:0\n\n This API is typically used for aggregating the results returned from different replicas, for reporting etc. For example, loss computed from different replicas can be averaged using this API before printing. \nNote: The result is copied to the \"current\" device - which would typically be the CPU of the worker on which the program is running. For TPUStrategy, it is the first TPU host. For multi client MultiWorkerMirroredStrategy, this is CPU of each worker.\n There are a number of different tf.distribute APIs for reducing values across replicas:  \ntf.distribute.ReplicaContext.all_reduce: This differs from Strategy.reduce in that it is for replica context and does not copy the results to the host device. all_reduce should be typically used for reductions inside the training step such as gradients. \ntf.distribute.StrategyExtended.reduce_to and tf.distribute.StrategyExtended.batch_reduce_to: These APIs are more advanced versions of Strategy.reduce as they allow customizing the destination of the result. They are also called in cross replica context.  What should axis be? Given a per-replica value returned by run, say a per-example loss, the batch will be divided across all the replicas. This function allows you to aggregate across replicas and optionally also across batch elements by specifying the axis parameter accordingly. For example, if you have a global batch size of 8 and 2 replicas, values for examples [0, 1, 2, 3] will be on replica 0 and [4, 5, 6, 7] will be on replica 1. With axis=None, reduce will aggregate only across replicas, returning [0+4, 1+5, 2+6, 3+7]. This is useful when each replica is computing a scalar or some other value that doesn't have a \"batch\" dimension (like a gradient or loss). strategy.reduce(\"sum\", per_replica_result, axis=None)\n Sometimes, you will want to aggregate across both the global batch and all replicas. You can get this behavior by specifying the batch dimension as the axis, typically axis=0. In this case it would return a scalar 0+1+2+3+4+5+6+7. strategy.reduce(\"sum\", per_replica_result, axis=0)\n If there is a last partial batch, you will need to specify an axis so that the resulting shape is consistent across replicas. So if the last batch has size 6 and it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch unless you specify axis=0. If you specify tf.distribute.ReduceOp.MEAN, using axis=0 will use the correct denominator of 6. Contrast this with computing reduce_mean to get a scalar value on each replica and this function to average those means, which will weigh some values 1/8 and others 1/4.\n \n\n\n Args\n  reduce_op   a tf.distribute.ReduceOp value specifying how values should be combined. Allows using string representation of the enum such as \"SUM\", \"MEAN\".  \n  value   a tf.distribute.DistributedValues instance, e.g. returned by Strategy.run, to be combined into a single tensor. It can also be a regular tensor when used with OneDeviceStrategy or default strategy.  \n  axis   specifies the dimension to reduce along within each replica's tensor. Should typically be set to the batch dimension, or None to only reduce across replicas (e.g. if the tensor has no batch dimension).   \n \n\n\n Returns   A Tensor.  \n run View source \nrun(\n    fn, args=(), kwargs=None, options=None\n)\n Invokes fn on each replica, with the given arguments. This method is the primary way to distribute your computation with a tf.distribute object. It invokes fn on each replica. If args or kwargs have tf.distribute.DistributedValues, such as those produced by a tf.distribute.DistributedDataset from tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function, when fn is executed on a particular replica, it will be executed with the component of tf.distribute.DistributedValues that correspond to that replica. fn is invoked under a replica context. fn may call tf.distribute.get_replica_context() to access members such as all_reduce. Please see the module-level docstring of tf.distribute for the concept of replica context. All arguments in args or kwargs should either be Python values of a nested structure of tensors, e.g. a list of tensors, in which case args and kwargs will be passed to the fn invoked on each replica. Or args or kwargs can be tf.distribute.DistributedValues containing tensors or composite tensors, i.e. tf.compat.v1.TensorInfo.CompositeTensor, in which case each fn call will get the component of a tf.distribute.DistributedValues corresponding to its replica. Key Point: Depending on the implementation of tf.distribute.Strategy and whether eager execution is enabled, fn may be called one or more times. If fn is annotated with tf.function or tf.distribute.Strategy.run is called inside a tf.function (eager execution is disabled inside a tf.function by default), fn is called once per replica to generate a Tensorflow graph, which will then be reused for execution with new inputs. Otherwise, if eager execution is enabled, fn will be called once per replica every step just like regular python code. Example usage:  Constant tensor input.  \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ntensor_input = tf.constant(3.0)\n@tf.function\ndef replica_fn(input):\n  return input*2.0\nresult = strategy.run(replica_fn, args=(tensor_input,))\nresult\nPerReplica:{\n  0: <tf.Tensor: shape=(), dtype=float32, numpy=6.0>,\n  1: <tf.Tensor: shape=(), dtype=float32, numpy=6.0>\n}\n  DistributedValues input.  \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n@tf.function\ndef run():\n  def value_fn(value_context):\n    return value_context.num_replicas_in_sync\n  distributed_values = (\n    strategy.experimental_distribute_values_from_function(\n      value_fn))\n  def replica_fn2(input):\n    return input*2\n  return strategy.run(replica_fn2, args=(distributed_values,))\nresult = run()\nresult\n<tf.Tensor: shape=(), dtype=int32, numpy=4>\n  Use tf.distribute.ReplicaContext to allreduce values.  \nstrategy = tf.distribute.MirroredStrategy([\"gpu:0\", \"gpu:1\"])\n@tf.function\ndef run():\n   def value_fn(value_context):\n     return tf.constant(value_context.replica_id_in_sync_group)\n   distributed_values = (\n       strategy.experimental_distribute_values_from_function(\n           value_fn))\n   def replica_fn(input):\n     return tf.distribute.get_replica_context().all_reduce(\"sum\", input)\n   return strategy.run(replica_fn, args=(distributed_values,))\nresult = run()\nresult\nPerReplica:{\n  0: <tf.Tensor: shape=(), dtype=int32, numpy=1>,\n  1: <tf.Tensor: shape=(), dtype=int32, numpy=1>\n}\n\n \n\n\n Args\n  fn   The function to run on each replica.  \n  args   Optional positional arguments to fn. Its element can be a Python value, a tensor or a tf.distribute.DistributedValues.  \n  kwargs   Optional keyword arguments to fn. Its element can be a Python value, a tensor or a tf.distribute.DistributedValues.  \n  options   An optional instance of tf.distribute.RunOptions specifying the options to run fn.   \n \n\n\n Returns   Merged return value of fn across replicas. The structure of the return value is the same as the return value from fn. Each element in the structure can either be tf.distribute.DistributedValues, Tensor objects, or Tensors (for example, if running on a single replica).  \n scope View source \nscope()\n Context manager to make the strategy current and distribute variables. This method returns a context manager, and is used as follows: \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n# Variable created inside scope:\nwith strategy.scope():\n  mirrored_variable = tf.Variable(1.)\nmirrored_variable\nMirroredVariable:{\n  0: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>,\n  1: <tf.Variable 'Variable/replica_1:0' shape=() dtype=float32, numpy=1.0>\n}\n# Variable created outside scope:\nregular_variable = tf.Variable(1.)\nregular_variable\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\n What happens when Strategy.scope is entered?  \nstrategy is installed in the global context as the \"current\" strategy. Inside this scope, tf.distribute.get_strategy() will now return this strategy. Outside this scope, it returns the default no-op strategy. Entering the scope also enters the \"cross-replica context\". See tf.distribute.StrategyExtended for an explanation on cross-replica and replica contexts. Variable creation inside scope is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like MirroredStrategy, TPUStrategy and MultiWorkerMiroredStrategy create variables replicated on each replica, whereas ParameterServerStrategy creates variables on the parameter servers. This is done using a custom tf.variable_creator_scope. In some strategies, a default device scope may also be entered: in MultiWorkerMiroredStrategy, a default device scope of \"/CPU:0\" is entered on each worker.  \nNote: Entering a scope does not automatically distribute a computation, except in the case of high level training framework like keras model.fit. If you're not using model.fit, you need to use strategy.run API to explicitly distribute that computation. See an example in the custom training loop tutorial.\n What should be in scope and what should be outside? There are a number of requirements on what needs to happen inside the scope. However, in places where we have information about which strategy is in use, we often enter the scope for the user, so they don't have to do it explicitly (i.e. calling those either inside or outside the scope is OK).  Anything that creates variables that should be distributed variables must be in strategy.scope. This can be either by directly putting it in scope, or relying on another API like strategy.run or model.fit to enter it for you. Any variable that is created outside scope will not be distributed and may have performance implications. Common things that create variables in TF: models, optimizers, metrics. These should always be created inside the scope. Another source of variable creation can be a checkpoint restore - when variables are created lazily. Note that any variable created inside a strategy captures the strategy information. So reading and writing to these variables outside the strategy.scope can also work seamlessly, without the user having to enter the scope. Some strategy APIs (such as strategy.run and strategy.reduce) which require to be in a strategy's scope, enter the scope for you automatically, which means when using those APIs you don't need to enter the scope yourself. When a tf.keras.Model is created inside a strategy.scope, we capture this information. When high level training frameworks methods such as model.compile, model.fit etc are then called on this model, we automatically enter the scope, as well as use this strategy to distribute the training etc. See detailed example in distributed keras tutorial. Note that simply calling the model(..) is not impacted - only high level training framework APIs are. model.compile, model.fit, model.evaluate, model.predict and model.save can all be called inside or outside the scope. The following can be either inside or outside the scope:  Creating the input datasets Defining tf.functions that represent your training step Saving APIs such as tf.saved_model.save. Loading creates variables, so that should go inside the scope if you want to train the model in a distributed way. Checkpoint saving. As mentioned above - checkpoint.restore may sometimes need to be inside scope if it creates variables. \n \n \n\n\n Returns   A context manager.  \n update_config_proto View source \nupdate_config_proto(\n    config_proto\n)\n Returns a copy of config_proto modified for use with this strategy. DEPRECATED: This method is not available in TF 2.x. The updated config has something needed to run a strategy, e.g. configuration to run collective ops, or device filters to improve distributed training performance.\n \n\n\n Args\n  config_proto   a tf.ConfigProto object.   \n \n\n\n Returns   The updated copy of the config_proto.  \n  \n"}, {"name": "tf.compat.v1.distribute.OneDeviceStrategy", "path": "compat/v1/distribute/onedevicestrategy", "type": "tf.compat", "text": "tf.compat.v1.distribute.OneDeviceStrategy A distribution strategy for running on a single device. Inherits From: Strategy \ntf.compat.v1.distribute.OneDeviceStrategy(\n    device\n)\n Using this strategy will place any variables created in its scope on the specified device. Input distributed through this strategy will be prefetched to the specified device. Moreover, any functions called via strategy.run will also be placed on the specified device as well. Typical usage of this strategy could be testing your code with the tf.distribute.Strategy API before switching to other strategies which actually distribute to multiple devices/machines. For example: tf.enable_eager_execution()\nstrategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n\nwith strategy.scope():\n  v = tf.Variable(1.0)\n  print(v.device)  # /job:localhost/replica:0/task:0/device:GPU:0\n\ndef step_fn(x):\n  return x * 2\n\nresult = 0\nfor i in range(10):\n  result += strategy.run(step_fn, args=(i,))\nprint(result)  # 90\n\n \n\n\n Args\n  device   Device string identifier for the device on which the variables should be placed. See class docs for more details on how the device is used. Examples: \"/cpu:0\", \"/gpu:0\", \"/device:CPU:0\", \"/device:GPU:0\"   \n \n\n\n Attributes\n  cluster_resolver   Returns the cluster resolver associated with this strategy. In general, when using a multi-worker tf.distribute strategy such as tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy(), there is a tf.distribute.cluster_resolver.ClusterResolver associated with the strategy used, and such an instance is returned by this property. Strategies that intend to have an associated tf.distribute.cluster_resolver.ClusterResolver must set the relevant attribute, or override this property; otherwise, None is returned by default. Those strategies should also provide information regarding what is returned by this property. Single-worker strategies usually do not have a tf.distribute.cluster_resolver.ClusterResolver, and in those cases this property will return None. The tf.distribute.cluster_resolver.ClusterResolver may be useful when the user needs to access information such as the cluster spec, task type or task id. For example, \nos.environ['TF_CONFIG'] = json.dumps({\n'cluster': {\n'worker': [\"localhost:12345\", \"localhost:23456\"],\n'ps': [\"localhost:34567\"]\n},\n'task': {'type': 'worker', 'index': 0}\n})\n\n# This implicitly uses TF_CONFIG for the cluster and current task info.\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n\n...\n\nif strategy.cluster_resolver.task_type == 'worker':\n# Perform something that's only applicable on workers. Since we set this\n# as a worker above, this block will run on this particular instance.\nelif strategy.cluster_resolver.task_type == 'ps':\n# Perform something that's only applicable on parameter servers. Since we\n# set this as a worker above, this block will not run on this particular\n# instance.\n For more information, please see tf.distribute.cluster_resolver.ClusterResolver's API docstring. \n \n  extended   tf.distribute.StrategyExtended with additional methods.  \n  num_replicas_in_sync   Returns number of replicas over which gradients are aggregated.    Methods distribute_datasets_from_function View source \ndistribute_datasets_from_function(\n    dataset_fn, options=None\n)\n Distributes tf.data.Dataset instances created by calls to dataset_fn. The argument dataset_fn that users pass in is an input function that has a tf.distribute.InputContext argument and returns a tf.data.Dataset instance. It is expected that the returned dataset from dataset_fn is already batched by per-replica batch size (i.e. global batch size divided by the number of replicas in sync) and sharded. tf.distribute.Strategy.distribute_datasets_from_function does not batch or shard the tf.data.Dataset instance returned from the input function. dataset_fn will be called on the CPU device of each of the workers and each generates a dataset where every replica on that worker will dequeue one batch of inputs (i.e. if a worker has two replicas, two batches will be dequeued from the Dataset every step). This method can be used for several purposes. First, it allows you to specify your own batching and sharding logic. (In contrast, tf.distribute.experimental_distribute_dataset does batching and sharding for you.) For example, where experimental_distribute_dataset is unable to shard the input files, this method might be used to manually shard the dataset (avoiding the slow fallback behavior in experimental_distribute_dataset). In cases where the dataset is infinite, this sharding can be done by creating dataset replicas that differ only in their random seed. The dataset_fn should take an tf.distribute.InputContext instance where information about batching and input replication can be accessed. You can use element_spec property of the tf.distribute.DistributedDataset returned by this API to query the tf.TypeSpec of the elements returned by the iterator. This can be used to set the input_signature property of a tf.function. Follow tf.distribute.DistributedDataset.element_spec to see an example. Key Point: The tf.data.Dataset returned by dataset_fn should have a per-replica batch size, unlike experimental_distribute_dataset, which uses the global batch size. This may be computed using input_context.get_per_replica_batch_size.\nNote: If you are using TPUStrategy, the order in which the data is processed by the workers when using tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function is not guaranteed. This is typically required if you are using tf.distribute to scale prediction. You can however insert an index for each element in the batch and order outputs accordingly. Refer to this snippet for an example of how to order outputs.\n\n\nNote: Stateful dataset transformations are currently not supported with tf.distribute.experimental_distribute_dataset or tf.distribute.distribute_datasets_from_function. Any stateful ops that the dataset may have are currently ignored. For example, if your dataset has a map_fn that uses tf.random.uniform to rotate an image, then you have a dataset graph that depends on state (i.e the random seed) on the local machine where the python process is being executed.\n For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input). If you are interested in last partial batch handling, read this section.\n \n\n\n Args\n  dataset_fn   A function taking a tf.distribute.InputContext instance and returning a tf.data.Dataset.  \n  options   tf.distribute.InputOptions used to control options on how this dataset is distributed.   \n \n\n\n Returns   A tf.distribute.DistributedDataset.  \n experimental_distribute_dataset View source \nexperimental_distribute_dataset(\n    dataset, options=None\n)\n Creates tf.distribute.DistributedDataset from tf.data.Dataset. The returned tf.distribute.DistributedDataset can be iterated over similar to regular datasets. NOTE: The user cannot add any more transformations to a tf.distribute.DistributedDataset. You can only create an iterator or examine the tf.TypeSpec of the data generated by it. See API docs of tf.distribute.DistributedDataset to learn more. The following is an example: \nglobal_batch_size = 2\n# Passing the devices is optional.\nstrategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\n# Create a dataset\ndataset = tf.data.Dataset.range(4).batch(global_batch_size)\n# Distribute that dataset\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\n@tf.function\ndef replica_fn(input):\n  return input*2\nresult = []\n# Iterate over the `tf.distribute.DistributedDataset`\nfor x in dist_dataset:\n  # process dataset elements\n  result.append(strategy.run(replica_fn, args=(x,)))\nprint(result)\n[PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>\n}, PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([4])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([6])>\n}]\n Three key actions happending under the hood of this method are batching, sharding, and prefetching. In the code snippet above, dataset is batched by global_batch_size, and calling experimental_distribute_dataset on it rebatches dataset to a new batch size that is equal to the global batch size divided by the number of replicas in sync. We iterate through it using a Pythonic for loop. x is a tf.distribute.DistributedValues containing data for all replicas, and each replica gets data of the new batch size. tf.distribute.Strategy.run will take care of feeding the right per-replica data in x to the right replica_fn executed on each replica. Sharding contains autosharding across multiple workers and within every worker. First, in multi-worker distributed training (i.e. when you use tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy), autosharding a dataset over a set of workers means that each worker is assigned a subset of the entire dataset (if the right tf.data.experimental.AutoShardPolicy is set). This is to ensure that at each step, a global batch size of non-overlapping dataset elements will be processed by each worker. Autosharding has a couple of different options that can be specified using tf.data.experimental.DistributeOptions. Then, sharding within each worker means the method will split the data among all the worker devices (if more than one a present). This will happen regardless of multi-worker autosharding. \nNote: for autosharding across multiple workers, the default mode is tf.data.experimental.AutoShardPolicy.AUTO. This mode will attempt to shard the input dataset by files if the dataset is being created out of reader datasets (e.g. tf.data.TFRecordDataset, tf.data.TextLineDataset, etc.) or otherwise shard the dataset by data, where each of the workers will read the entire dataset and only process the shard assigned to it. However, if you have less than one input file per worker, we suggest that you disable dataset autosharding across workers by setting the tf.data.experimental.DistributeOptions.auto_shard_policy to be tf.data.experimental.AutoShardPolicy.OFF.\n By default, this method adds a prefetch transformation at the end of the user provided tf.data.Dataset instance. The argument to the prefetch transformation which is buffer_size is equal to the number of replicas in sync. If the above batch splitting and dataset sharding logic is undesirable, please use tf.distribute.Strategy.distribute_datasets_from_function instead, which does not do any automatic batching or sharding for you. \nNote: If you are using TPUStrategy, the order in which the data is processed by the workers when using tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function is not guaranteed. This is typically required if you are using tf.distribute to scale prediction. You can however insert an index for each element in the batch and order outputs accordingly. Refer to this snippet for an example of how to order outputs.\n\n\nNote: Stateful dataset transformations are currently not supported with tf.distribute.experimental_distribute_dataset or tf.distribute.distribute_datasets_from_function. Any stateful ops that the dataset may have are currently ignored. For example, if your dataset has a map_fn that uses tf.random.uniform to rotate an image, then you have a dataset graph that depends on state (i.e the random seed) on the local machine where the python process is being executed.\n For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input. If you are interested in last partial batch handling, read this section.\n \n\n\n Args\n  dataset   tf.data.Dataset that will be sharded across all replicas using the rules stated above.  \n  options   tf.distribute.InputOptions used to control options on how this dataset is distributed.   \n \n\n\n Returns   A tf.distribute.DistributedDataset.  \n experimental_local_results View source \nexperimental_local_results(\n    value\n)\n Returns the list of all local per-replica values contained in value. \nNote: This only returns values on the worker initiated by this client. When using a tf.distribute.Strategy like tf.distribute.experimental.MultiWorkerMirroredStrategy, each worker will be its own client, and this function will only return values computed on that worker.\n\n \n\n\n Args\n  value   A value returned by experimental_run(), run(), extended.call_for_each_replica(), or a variable created in scope.   \n \n\n\n Returns   A tuple of values contained in value. If value represents a single value, this returns (value,).  \n experimental_make_numpy_dataset View source \nexperimental_make_numpy_dataset(\n    numpy_input, session=None\n)\n Makes a tf.data.Dataset for input provided via a numpy array. This avoids adding numpy_input as a large constant in the graph, and copies the data to the machine or machines that will be processing the input. Note that you will likely need to use tf.distribute.Strategy.experimental_distribute_dataset with the returned dataset to further distribute it with the strategy. Example: numpy_input = np.ones([10], dtype=np.float32)\ndataset = strategy.experimental_make_numpy_dataset(numpy_input)\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\n\n \n\n\n Args\n  numpy_input   A nest of NumPy input arrays that will be converted into a dataset. Note that lists of Numpy arrays are stacked, as that is normal tf.data.Dataset behavior.  \n  session   (TensorFlow v1.x graph execution only) A session used for initialization.   \n \n\n\n Returns   A tf.data.Dataset representing numpy_input.  \n experimental_run View source \nexperimental_run(\n    fn, input_iterator=None\n)\n Runs ops in fn on each replica, with inputs from input_iterator. DEPRECATED: This method is not available in TF 2.x. Please switch to using run instead. When eager execution is enabled, executes ops specified by fn on each replica. Otherwise, builds a graph to execute the ops on each replica. Each replica will take a single, different input from the inputs provided by one get_next call on the input iterator. fn may call tf.distribute.get_replica_context() to access members such as replica_id_in_sync_group. Key Point: Depending on the tf.distribute.Strategy implementation being used, and whether eager execution is enabled, fn may be called one or more times (once for each replica).\n \n\n\n Args\n  fn   The function to run. The inputs to the function must match the outputs of input_iterator.get_next(). The output must be a tf.nest of Tensors.  \n  input_iterator   (Optional) input iterator from which the inputs are taken.   \n \n\n\n Returns   Merged return value of fn across replicas. The structure of the return value is the same as the return value from fn. Each element in the structure can either be PerReplica (if the values are unsynchronized), Mirrored (if the values are kept in sync), or Tensor (if running on a single replica).  \n make_dataset_iterator View source \nmake_dataset_iterator(\n    dataset\n)\n Makes an iterator for input provided via dataset. DEPRECATED: This method is not available in TF 2.x. Data from the given dataset will be distributed evenly across all the compute replicas. We will assume that the input dataset is batched by the global batch size. With this assumption, we will make a best effort to divide each batch across all the replicas (one or more workers). If this effort fails, an error will be thrown, and the user should instead use make_input_fn_iterator which provides more control to the user, and does not try to divide a batch across replicas. The user could also use make_input_fn_iterator if they want to customize which input is fed to which replica/worker etc.\n \n\n\n Args\n  dataset   tf.data.Dataset that will be distributed evenly across all replicas.   \n \n\n\n Returns   An tf.distribute.InputIterator which returns inputs for each step of the computation. User should call initialize on the returned iterator.  \n make_input_fn_iterator View source \nmake_input_fn_iterator(\n    input_fn, replication_mode=tf.distribute.InputReplicationMode.PER_WORKER\n)\n Returns an iterator split across replicas created from an input function. DEPRECATED: This method is not available in TF 2.x. The input_fn should take an tf.distribute.InputContext object where information about batching and input sharding can be accessed: def input_fn(input_context):\n  batch_size = input_context.get_per_replica_batch_size(global_batch_size)\n  d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)\n  return d.shard(input_context.num_input_pipelines,\n                 input_context.input_pipeline_id)\nwith strategy.scope():\n  iterator = strategy.make_input_fn_iterator(input_fn)\n  replica_results = strategy.experimental_run(replica_fn, iterator)\n The tf.data.Dataset returned by input_fn should have a per-replica batch size, which may be computed using input_context.get_per_replica_batch_size.\n \n\n\n Args\n  input_fn   A function taking a tf.distribute.InputContext object and returning a tf.data.Dataset.  \n  replication_mode   an enum value of tf.distribute.InputReplicationMode. Only PER_WORKER is supported currently, which means there will be a single call to input_fn per worker. Replicas will dequeue from the local tf.data.Dataset on their worker.   \n \n\n\n Returns   An iterator object that should first be .initialize()-ed. It may then either be passed to strategy.experimental_run() or you can iterator.get_next() to get the next value to pass to strategy.extended.call_for_each_replica().  \n reduce View source \nreduce(\n    reduce_op, value, axis=None\n)\n Reduce value across replicas and return result on current device. \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndef step_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\n\nper_replica_result = strategy.run(step_fn)\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\ntotal\n<tf.Tensor: shape=(), dtype=int32, numpy=1>\n To see how this would look with multiple replicas, consider the same example with MirroredStrategy with 2 GPUs: strategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\ndef step_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\n\nper_replica_result = strategy.run(step_fn)\n# Check devices on which per replica result is:\nstrategy.experimental_local_results(per_replica_result)[0].device\n# /job:localhost/replica:0/task:0/device:GPU:0\nstrategy.experimental_local_results(per_replica_result)[1].device\n# /job:localhost/replica:0/task:0/device:GPU:1\n\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\n# Check device on which reduced result is:\ntotal.device\n# /job:localhost/replica:0/task:0/device:CPU:0\n\n This API is typically used for aggregating the results returned from different replicas, for reporting etc. For example, loss computed from different replicas can be averaged using this API before printing. \nNote: The result is copied to the \"current\" device - which would typically be the CPU of the worker on which the program is running. For TPUStrategy, it is the first TPU host. For multi client MultiWorkerMirroredStrategy, this is CPU of each worker.\n There are a number of different tf.distribute APIs for reducing values across replicas:  \ntf.distribute.ReplicaContext.all_reduce: This differs from Strategy.reduce in that it is for replica context and does not copy the results to the host device. all_reduce should be typically used for reductions inside the training step such as gradients. \ntf.distribute.StrategyExtended.reduce_to and tf.distribute.StrategyExtended.batch_reduce_to: These APIs are more advanced versions of Strategy.reduce as they allow customizing the destination of the result. They are also called in cross replica context.  What should axis be? Given a per-replica value returned by run, say a per-example loss, the batch will be divided across all the replicas. This function allows you to aggregate across replicas and optionally also across batch elements by specifying the axis parameter accordingly. For example, if you have a global batch size of 8 and 2 replicas, values for examples [0, 1, 2, 3] will be on replica 0 and [4, 5, 6, 7] will be on replica 1. With axis=None, reduce will aggregate only across replicas, returning [0+4, 1+5, 2+6, 3+7]. This is useful when each replica is computing a scalar or some other value that doesn't have a \"batch\" dimension (like a gradient or loss). strategy.reduce(\"sum\", per_replica_result, axis=None)\n Sometimes, you will want to aggregate across both the global batch and all replicas. You can get this behavior by specifying the batch dimension as the axis, typically axis=0. In this case it would return a scalar 0+1+2+3+4+5+6+7. strategy.reduce(\"sum\", per_replica_result, axis=0)\n If there is a last partial batch, you will need to specify an axis so that the resulting shape is consistent across replicas. So if the last batch has size 6 and it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch unless you specify axis=0. If you specify tf.distribute.ReduceOp.MEAN, using axis=0 will use the correct denominator of 6. Contrast this with computing reduce_mean to get a scalar value on each replica and this function to average those means, which will weigh some values 1/8 and others 1/4.\n \n\n\n Args\n  reduce_op   a tf.distribute.ReduceOp value specifying how values should be combined. Allows using string representation of the enum such as \"SUM\", \"MEAN\".  \n  value   a tf.distribute.DistributedValues instance, e.g. returned by Strategy.run, to be combined into a single tensor. It can also be a regular tensor when used with OneDeviceStrategy or default strategy.  \n  axis   specifies the dimension to reduce along within each replica's tensor. Should typically be set to the batch dimension, or None to only reduce across replicas (e.g. if the tensor has no batch dimension).   \n \n\n\n Returns   A Tensor.  \n run View source \nrun(\n    fn, args=(), kwargs=None, options=None\n)\n Invokes fn on each replica, with the given arguments. This method is the primary way to distribute your computation with a tf.distribute object. It invokes fn on each replica. If args or kwargs have tf.distribute.DistributedValues, such as those produced by a tf.distribute.DistributedDataset from tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function, when fn is executed on a particular replica, it will be executed with the component of tf.distribute.DistributedValues that correspond to that replica. fn is invoked under a replica context. fn may call tf.distribute.get_replica_context() to access members such as all_reduce. Please see the module-level docstring of tf.distribute for the concept of replica context. All arguments in args or kwargs should either be Python values of a nested structure of tensors, e.g. a list of tensors, in which case args and kwargs will be passed to the fn invoked on each replica. Or args or kwargs can be tf.distribute.DistributedValues containing tensors or composite tensors, i.e. tf.compat.v1.TensorInfo.CompositeTensor, in which case each fn call will get the component of a tf.distribute.DistributedValues corresponding to its replica. Key Point: Depending on the implementation of tf.distribute.Strategy and whether eager execution is enabled, fn may be called one or more times. If fn is annotated with tf.function or tf.distribute.Strategy.run is called inside a tf.function (eager execution is disabled inside a tf.function by default), fn is called once per replica to generate a Tensorflow graph, which will then be reused for execution with new inputs. Otherwise, if eager execution is enabled, fn will be called once per replica every step just like regular python code. Example usage:  Constant tensor input.  \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ntensor_input = tf.constant(3.0)\n@tf.function\ndef replica_fn(input):\n  return input*2.0\nresult = strategy.run(replica_fn, args=(tensor_input,))\nresult\nPerReplica:{\n  0: <tf.Tensor: shape=(), dtype=float32, numpy=6.0>,\n  1: <tf.Tensor: shape=(), dtype=float32, numpy=6.0>\n}\n  DistributedValues input.  \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n@tf.function\ndef run():\n  def value_fn(value_context):\n    return value_context.num_replicas_in_sync\n  distributed_values = (\n    strategy.experimental_distribute_values_from_function(\n      value_fn))\n  def replica_fn2(input):\n    return input*2\n  return strategy.run(replica_fn2, args=(distributed_values,))\nresult = run()\nresult\n<tf.Tensor: shape=(), dtype=int32, numpy=4>\n  Use tf.distribute.ReplicaContext to allreduce values.  \nstrategy = tf.distribute.MirroredStrategy([\"gpu:0\", \"gpu:1\"])\n@tf.function\ndef run():\n   def value_fn(value_context):\n     return tf.constant(value_context.replica_id_in_sync_group)\n   distributed_values = (\n       strategy.experimental_distribute_values_from_function(\n           value_fn))\n   def replica_fn(input):\n     return tf.distribute.get_replica_context().all_reduce(\"sum\", input)\n   return strategy.run(replica_fn, args=(distributed_values,))\nresult = run()\nresult\nPerReplica:{\n  0: <tf.Tensor: shape=(), dtype=int32, numpy=1>,\n  1: <tf.Tensor: shape=(), dtype=int32, numpy=1>\n}\n\n \n\n\n Args\n  fn   The function to run on each replica.  \n  args   Optional positional arguments to fn. Its element can be a Python value, a tensor or a tf.distribute.DistributedValues.  \n  kwargs   Optional keyword arguments to fn. Its element can be a Python value, a tensor or a tf.distribute.DistributedValues.  \n  options   An optional instance of tf.distribute.RunOptions specifying the options to run fn.   \n \n\n\n Returns   Merged return value of fn across replicas. The structure of the return value is the same as the return value from fn. Each element in the structure can either be tf.distribute.DistributedValues, Tensor objects, or Tensors (for example, if running on a single replica).  \n scope View source \nscope()\n Context manager to make the strategy current and distribute variables. This method returns a context manager, and is used as follows: \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n# Variable created inside scope:\nwith strategy.scope():\n  mirrored_variable = tf.Variable(1.)\nmirrored_variable\nMirroredVariable:{\n  0: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>,\n  1: <tf.Variable 'Variable/replica_1:0' shape=() dtype=float32, numpy=1.0>\n}\n# Variable created outside scope:\nregular_variable = tf.Variable(1.)\nregular_variable\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\n What happens when Strategy.scope is entered?  \nstrategy is installed in the global context as the \"current\" strategy. Inside this scope, tf.distribute.get_strategy() will now return this strategy. Outside this scope, it returns the default no-op strategy. Entering the scope also enters the \"cross-replica context\". See tf.distribute.StrategyExtended for an explanation on cross-replica and replica contexts. Variable creation inside scope is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like MirroredStrategy, TPUStrategy and MultiWorkerMiroredStrategy create variables replicated on each replica, whereas ParameterServerStrategy creates variables on the parameter servers. This is done using a custom tf.variable_creator_scope. In some strategies, a default device scope may also be entered: in MultiWorkerMiroredStrategy, a default device scope of \"/CPU:0\" is entered on each worker.  \nNote: Entering a scope does not automatically distribute a computation, except in the case of high level training framework like keras model.fit. If you're not using model.fit, you need to use strategy.run API to explicitly distribute that computation. See an example in the custom training loop tutorial.\n What should be in scope and what should be outside? There are a number of requirements on what needs to happen inside the scope. However, in places where we have information about which strategy is in use, we often enter the scope for the user, so they don't have to do it explicitly (i.e. calling those either inside or outside the scope is OK).  Anything that creates variables that should be distributed variables must be in strategy.scope. This can be either by directly putting it in scope, or relying on another API like strategy.run or model.fit to enter it for you. Any variable that is created outside scope will not be distributed and may have performance implications. Common things that create variables in TF: models, optimizers, metrics. These should always be created inside the scope. Another source of variable creation can be a checkpoint restore - when variables are created lazily. Note that any variable created inside a strategy captures the strategy information. So reading and writing to these variables outside the strategy.scope can also work seamlessly, without the user having to enter the scope. Some strategy APIs (such as strategy.run and strategy.reduce) which require to be in a strategy's scope, enter the scope for you automatically, which means when using those APIs you don't need to enter the scope yourself. When a tf.keras.Model is created inside a strategy.scope, we capture this information. When high level training frameworks methods such as model.compile, model.fit etc are then called on this model, we automatically enter the scope, as well as use this strategy to distribute the training etc. See detailed example in distributed keras tutorial. Note that simply calling the model(..) is not impacted - only high level training framework APIs are. model.compile, model.fit, model.evaluate, model.predict and model.save can all be called inside or outside the scope. The following can be either inside or outside the scope:  Creating the input datasets Defining tf.functions that represent your training step Saving APIs such as tf.saved_model.save. Loading creates variables, so that should go inside the scope if you want to train the model in a distributed way. Checkpoint saving. As mentioned above - checkpoint.restore may sometimes need to be inside scope if it creates variables. \n \n \n\n\n Returns   A context manager.  \n update_config_proto View source \nupdate_config_proto(\n    config_proto\n)\n Returns a copy of config_proto modified for use with this strategy. DEPRECATED: This method is not available in TF 2.x. The updated config has something needed to run a strategy, e.g. configuration to run collective ops, or device filters to improve distributed training performance.\n \n\n\n Args\n  config_proto   a tf.ConfigProto object.   \n \n\n\n Returns   The updated copy of the config_proto.  \n  \n"}, {"name": "tf.compat.v1.distribute.ReplicaContext", "path": "compat/v1/distribute/replicacontext", "type": "tf.compat", "text": "tf.compat.v1.distribute.ReplicaContext A class with a collection of APIs that can be called in a replica context. \ntf.compat.v1.distribute.ReplicaContext(\n    strategy, replica_id_in_sync_group\n)\n You can use tf.distribute.get_replica_context to get an instance of ReplicaContext, which can only be called inside the function passed to tf.distribute.Strategy.run. \nstrategy = tf.distribute.MirroredStrategy(['GPU:0', 'GPU:1'])\ndef func():\n  replica_context = tf.distribute.get_replica_context()\n  return replica_context.replica_id_in_sync_group\nstrategy.run(func)\nPerReplica:{\n  0: <tf.Tensor: shape=(), dtype=int32, numpy=0>,\n  1: <tf.Tensor: shape=(), dtype=int32, numpy=1>\n}\n\n \n\n\n Args\n  strategy   A tf.distribute.Strategy.  \n  replica_id_in_sync_group   An integer, a Tensor or None. Prefer an integer whenever possible to avoid issues with nested tf.function. It accepts a Tensor only to be compatible with tpu.replicate.   \n \n\n\n Attributes\n  devices   Returns the devices this replica is to be executed on, as a tuple of strings. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please avoid relying on devices property.\nNote: For tf.distribute.MirroredStrategy and tf.distribute.experimental.MultiWorkerMirroredStrategy, this returns a nested list of device strings, e.g, [[\"GPU:0\"]]. \n\n \n  num_replicas_in_sync   Returns number of replicas that are kept in sync.  \n  replica_id_in_sync_group   Returns the id of the replica. This identifies the replica among all replicas that are kept in sync. The value of the replica id can range from 0 to tf.distribute.ReplicaContext.num_replicas_in_sync - 1. \nNote: This is not guaranteed to be the same ID as the XLA replica ID use for low-level operations such as collective_permute. \n\n \n  strategy   The current tf.distribute.Strategy object.    Methods all_reduce View source \nall_reduce(\n    reduce_op, value, options=None\n)\n All-reduces value across all replicas. \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndef step_fn():\n  ctx = tf.distribute.get_replica_context()\n  value = tf.identity(1.)\n  return ctx.all_reduce(tf.distribute.ReduceOp.SUM, value)\nstrategy.experimental_local_results(strategy.run(step_fn))\n(<tf.Tensor: shape=(), dtype=float32, numpy=2.0>,\n <tf.Tensor: shape=(), dtype=float32, numpy=2.0>)\n It supports batched operations. You can pass a list of values and it attempts to batch them when possible. You can also specify options to indicate the desired batching behavior, e.g. batch the values into multiple packs so that they can better overlap with computations. \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndef step_fn():\n  ctx = tf.distribute.get_replica_context()\n  value1 = tf.identity(1.)\n  value2 = tf.identity(2.)\n  return ctx.all_reduce(tf.distribute.ReduceOp.SUM, [value1, value2])\nstrategy.experimental_local_results(strategy.run(step_fn))\n([PerReplica:{\n  0: <tf.Tensor: shape=(), dtype=float32, numpy=2.0>,\n  1: <tf.Tensor: shape=(), dtype=float32, numpy=2.0>\n}, PerReplica:{\n  0: <tf.Tensor: shape=(), dtype=float32, numpy=4.0>,\n  1: <tf.Tensor: shape=(), dtype=float32, numpy=4.0>\n}],)\n Note that all replicas need to participate in the all-reduce, otherwise this operation hangs. Note that if there're multiple all-reduces, they need to execute in the same order on all replicas. Dispatching all-reduce based on conditions is usually error-prone. This API currently can only be called in the replica context. Other variants to reduce values across replicas are:  \ntf.distribute.StrategyExtended.reduce_to: the reduce and all-reduce API in the cross-replica context. \ntf.distribute.StrategyExtended.batch_reduce_to: the batched reduce and all-reduce API in the cross-replica context. \ntf.distribute.Strategy.reduce: a more convenient method to reduce to the host in cross-replica context. \n \n\n\n Args\n  reduce_op   a tf.distribute.ReduceOp value specifying how values should be combined. Allows using string representation of the enum such as \"SUM\", \"MEAN\".  \n  value   a nested structure of tf.Tensor which tf.nest.flatten accepts. The structure and the shapes of the tf.Tensor need to be same on all replicas.  \n  options   a tf.distribute.experimental.CommunicationOptions. Options to perform collective operations. This overrides the default options if the tf.distribute.Strategy takes one in the constructor. See tf.distribute.experimental.CommunicationOptions for details of the options.   \n \n\n\n Returns   A nested structure of tf.Tensor with the reduced values. The structure is the same as value.  \n merge_call View source \nmerge_call(\n    merge_fn, args=(), kwargs=None\n)\n Merge args across replicas and run merge_fn in a cross-replica context. This allows communication and coordination when there are multiple calls to the step_fn triggered by a call to strategy.run(step_fn, ...). See tf.distribute.Strategy.run for an explanation. If not inside a distributed scope, this is equivalent to: strategy = tf.distribute.get_strategy()\nwith cross-replica-context(strategy):\n  return merge_fn(strategy, *args, **kwargs)\n\n \n\n\n Args\n  merge_fn   Function that joins arguments from threads that are given as PerReplica. It accepts tf.distribute.Strategy object as the first argument.  \n  args   List or tuple with positional per-thread arguments for merge_fn.  \n  kwargs   Dict with keyword per-thread arguments for merge_fn.   \n \n\n\n Returns   The return value of merge_fn, except for PerReplica values which are unpacked.  \n  \n"}, {"name": "tf.compat.v1.distribute.Strategy", "path": "compat/v1/distribute/strategy", "type": "tf.compat", "text": "tf.compat.v1.distribute.Strategy A list of devices with a state & compute distribution policy. \ntf.compat.v1.distribute.Strategy(\n    extended\n)\n See the guide for overview and examples. \nNote: Not all tf.distribute.Strategy implementations currently support TensorFlow's partitioned variables (where a single variable is split across multiple devices) at this time.\n\n \n\n\n Attributes\n  cluster_resolver   Returns the cluster resolver associated with this strategy. In general, when using a multi-worker tf.distribute strategy such as tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy(), there is a tf.distribute.cluster_resolver.ClusterResolver associated with the strategy used, and such an instance is returned by this property. Strategies that intend to have an associated tf.distribute.cluster_resolver.ClusterResolver must set the relevant attribute, or override this property; otherwise, None is returned by default. Those strategies should also provide information regarding what is returned by this property. Single-worker strategies usually do not have a tf.distribute.cluster_resolver.ClusterResolver, and in those cases this property will return None. The tf.distribute.cluster_resolver.ClusterResolver may be useful when the user needs to access information such as the cluster spec, task type or task id. For example, \nos.environ['TF_CONFIG'] = json.dumps({\n'cluster': {\n'worker': [\"localhost:12345\", \"localhost:23456\"],\n'ps': [\"localhost:34567\"]\n},\n'task': {'type': 'worker', 'index': 0}\n})\n\n# This implicitly uses TF_CONFIG for the cluster and current task info.\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n\n...\n\nif strategy.cluster_resolver.task_type == 'worker':\n# Perform something that's only applicable on workers. Since we set this\n# as a worker above, this block will run on this particular instance.\nelif strategy.cluster_resolver.task_type == 'ps':\n# Perform something that's only applicable on parameter servers. Since we\n# set this as a worker above, this block will not run on this particular\n# instance.\n For more information, please see tf.distribute.cluster_resolver.ClusterResolver's API docstring. \n \n  extended   tf.distribute.StrategyExtended with additional methods.  \n  num_replicas_in_sync   Returns number of replicas over which gradients are aggregated.    Methods distribute_datasets_from_function View source \ndistribute_datasets_from_function(\n    dataset_fn, options=None\n)\n Distributes tf.data.Dataset instances created by calls to dataset_fn. The argument dataset_fn that users pass in is an input function that has a tf.distribute.InputContext argument and returns a tf.data.Dataset instance. It is expected that the returned dataset from dataset_fn is already batched by per-replica batch size (i.e. global batch size divided by the number of replicas in sync) and sharded. tf.distribute.Strategy.distribute_datasets_from_function does not batch or shard the tf.data.Dataset instance returned from the input function. dataset_fn will be called on the CPU device of each of the workers and each generates a dataset where every replica on that worker will dequeue one batch of inputs (i.e. if a worker has two replicas, two batches will be dequeued from the Dataset every step). This method can be used for several purposes. First, it allows you to specify your own batching and sharding logic. (In contrast, tf.distribute.experimental_distribute_dataset does batching and sharding for you.) For example, where experimental_distribute_dataset is unable to shard the input files, this method might be used to manually shard the dataset (avoiding the slow fallback behavior in experimental_distribute_dataset). In cases where the dataset is infinite, this sharding can be done by creating dataset replicas that differ only in their random seed. The dataset_fn should take an tf.distribute.InputContext instance where information about batching and input replication can be accessed. You can use element_spec property of the tf.distribute.DistributedDataset returned by this API to query the tf.TypeSpec of the elements returned by the iterator. This can be used to set the input_signature property of a tf.function. Follow tf.distribute.DistributedDataset.element_spec to see an example. Key Point: The tf.data.Dataset returned by dataset_fn should have a per-replica batch size, unlike experimental_distribute_dataset, which uses the global batch size. This may be computed using input_context.get_per_replica_batch_size.\nNote: If you are using TPUStrategy, the order in which the data is processed by the workers when using tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function is not guaranteed. This is typically required if you are using tf.distribute to scale prediction. You can however insert an index for each element in the batch and order outputs accordingly. Refer to this snippet for an example of how to order outputs.\n\n\nNote: Stateful dataset transformations are currently not supported with tf.distribute.experimental_distribute_dataset or tf.distribute.distribute_datasets_from_function. Any stateful ops that the dataset may have are currently ignored. For example, if your dataset has a map_fn that uses tf.random.uniform to rotate an image, then you have a dataset graph that depends on state (i.e the random seed) on the local machine where the python process is being executed.\n For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input). If you are interested in last partial batch handling, read this section.\n \n\n\n Args\n  dataset_fn   A function taking a tf.distribute.InputContext instance and returning a tf.data.Dataset.  \n  options   tf.distribute.InputOptions used to control options on how this dataset is distributed.   \n \n\n\n Returns   A tf.distribute.DistributedDataset.  \n experimental_distribute_dataset View source \nexperimental_distribute_dataset(\n    dataset, options=None\n)\n Creates tf.distribute.DistributedDataset from tf.data.Dataset. The returned tf.distribute.DistributedDataset can be iterated over similar to regular datasets. NOTE: The user cannot add any more transformations to a tf.distribute.DistributedDataset. You can only create an iterator or examine the tf.TypeSpec of the data generated by it. See API docs of tf.distribute.DistributedDataset to learn more. The following is an example: \nglobal_batch_size = 2\n# Passing the devices is optional.\nstrategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\n# Create a dataset\ndataset = tf.data.Dataset.range(4).batch(global_batch_size)\n# Distribute that dataset\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\n@tf.function\ndef replica_fn(input):\n  return input*2\nresult = []\n# Iterate over the `tf.distribute.DistributedDataset`\nfor x in dist_dataset:\n  # process dataset elements\n  result.append(strategy.run(replica_fn, args=(x,)))\nprint(result)\n[PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>\n}, PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([4])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([6])>\n}]\n Three key actions happending under the hood of this method are batching, sharding, and prefetching. In the code snippet above, dataset is batched by global_batch_size, and calling experimental_distribute_dataset on it rebatches dataset to a new batch size that is equal to the global batch size divided by the number of replicas in sync. We iterate through it using a Pythonic for loop. x is a tf.distribute.DistributedValues containing data for all replicas, and each replica gets data of the new batch size. tf.distribute.Strategy.run will take care of feeding the right per-replica data in x to the right replica_fn executed on each replica. Sharding contains autosharding across multiple workers and within every worker. First, in multi-worker distributed training (i.e. when you use tf.distribute.experimental.MultiWorkerMirroredStrategy or tf.distribute.TPUStrategy), autosharding a dataset over a set of workers means that each worker is assigned a subset of the entire dataset (if the right tf.data.experimental.AutoShardPolicy is set). This is to ensure that at each step, a global batch size of non-overlapping dataset elements will be processed by each worker. Autosharding has a couple of different options that can be specified using tf.data.experimental.DistributeOptions. Then, sharding within each worker means the method will split the data among all the worker devices (if more than one a present). This will happen regardless of multi-worker autosharding. \nNote: for autosharding across multiple workers, the default mode is tf.data.experimental.AutoShardPolicy.AUTO. This mode will attempt to shard the input dataset by files if the dataset is being created out of reader datasets (e.g. tf.data.TFRecordDataset, tf.data.TextLineDataset, etc.) or otherwise shard the dataset by data, where each of the workers will read the entire dataset and only process the shard assigned to it. However, if you have less than one input file per worker, we suggest that you disable dataset autosharding across workers by setting the tf.data.experimental.DistributeOptions.auto_shard_policy to be tf.data.experimental.AutoShardPolicy.OFF.\n By default, this method adds a prefetch transformation at the end of the user provided tf.data.Dataset instance. The argument to the prefetch transformation which is buffer_size is equal to the number of replicas in sync. If the above batch splitting and dataset sharding logic is undesirable, please use tf.distribute.Strategy.distribute_datasets_from_function instead, which does not do any automatic batching or sharding for you. \nNote: If you are using TPUStrategy, the order in which the data is processed by the workers when using tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function is not guaranteed. This is typically required if you are using tf.distribute to scale prediction. You can however insert an index for each element in the batch and order outputs accordingly. Refer to this snippet for an example of how to order outputs.\n\n\nNote: Stateful dataset transformations are currently not supported with tf.distribute.experimental_distribute_dataset or tf.distribute.distribute_datasets_from_function. Any stateful ops that the dataset may have are currently ignored. For example, if your dataset has a map_fn that uses tf.random.uniform to rotate an image, then you have a dataset graph that depends on state (i.e the random seed) on the local machine where the python process is being executed.\n For a tutorial on more usage and properties of this method, refer to the tutorial on distributed input. If you are interested in last partial batch handling, read this section.\n \n\n\n Args\n  dataset   tf.data.Dataset that will be sharded across all replicas using the rules stated above.  \n  options   tf.distribute.InputOptions used to control options on how this dataset is distributed.   \n \n\n\n Returns   A tf.distribute.DistributedDataset.  \n experimental_local_results View source \nexperimental_local_results(\n    value\n)\n Returns the list of all local per-replica values contained in value. \nNote: This only returns values on the worker initiated by this client. When using a tf.distribute.Strategy like tf.distribute.experimental.MultiWorkerMirroredStrategy, each worker will be its own client, and this function will only return values computed on that worker.\n\n \n\n\n Args\n  value   A value returned by experimental_run(), run(), extended.call_for_each_replica(), or a variable created in scope.   \n \n\n\n Returns   A tuple of values contained in value. If value represents a single value, this returns (value,).  \n experimental_make_numpy_dataset View source \nexperimental_make_numpy_dataset(\n    numpy_input, session=None\n)\n Makes a tf.data.Dataset for input provided via a numpy array. This avoids adding numpy_input as a large constant in the graph, and copies the data to the machine or machines that will be processing the input. Note that you will likely need to use tf.distribute.Strategy.experimental_distribute_dataset with the returned dataset to further distribute it with the strategy. Example: numpy_input = np.ones([10], dtype=np.float32)\ndataset = strategy.experimental_make_numpy_dataset(numpy_input)\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\n\n \n\n\n Args\n  numpy_input   A nest of NumPy input arrays that will be converted into a dataset. Note that lists of Numpy arrays are stacked, as that is normal tf.data.Dataset behavior.  \n  session   (TensorFlow v1.x graph execution only) A session used for initialization.   \n \n\n\n Returns   A tf.data.Dataset representing numpy_input.  \n experimental_run View source \nexperimental_run(\n    fn, input_iterator=None\n)\n Runs ops in fn on each replica, with inputs from input_iterator. DEPRECATED: This method is not available in TF 2.x. Please switch to using run instead. When eager execution is enabled, executes ops specified by fn on each replica. Otherwise, builds a graph to execute the ops on each replica. Each replica will take a single, different input from the inputs provided by one get_next call on the input iterator. fn may call tf.distribute.get_replica_context() to access members such as replica_id_in_sync_group. Key Point: Depending on the tf.distribute.Strategy implementation being used, and whether eager execution is enabled, fn may be called one or more times (once for each replica).\n \n\n\n Args\n  fn   The function to run. The inputs to the function must match the outputs of input_iterator.get_next(). The output must be a tf.nest of Tensors.  \n  input_iterator   (Optional) input iterator from which the inputs are taken.   \n \n\n\n Returns   Merged return value of fn across replicas. The structure of the return value is the same as the return value from fn. Each element in the structure can either be PerReplica (if the values are unsynchronized), Mirrored (if the values are kept in sync), or Tensor (if running on a single replica).  \n make_dataset_iterator View source \nmake_dataset_iterator(\n    dataset\n)\n Makes an iterator for input provided via dataset. DEPRECATED: This method is not available in TF 2.x. Data from the given dataset will be distributed evenly across all the compute replicas. We will assume that the input dataset is batched by the global batch size. With this assumption, we will make a best effort to divide each batch across all the replicas (one or more workers). If this effort fails, an error will be thrown, and the user should instead use make_input_fn_iterator which provides more control to the user, and does not try to divide a batch across replicas. The user could also use make_input_fn_iterator if they want to customize which input is fed to which replica/worker etc.\n \n\n\n Args\n  dataset   tf.data.Dataset that will be distributed evenly across all replicas.   \n \n\n\n Returns   An tf.distribute.InputIterator which returns inputs for each step of the computation. User should call initialize on the returned iterator.  \n make_input_fn_iterator View source \nmake_input_fn_iterator(\n    input_fn, replication_mode=tf.distribute.InputReplicationMode.PER_WORKER\n)\n Returns an iterator split across replicas created from an input function. DEPRECATED: This method is not available in TF 2.x. The input_fn should take an tf.distribute.InputContext object where information about batching and input sharding can be accessed: def input_fn(input_context):\n  batch_size = input_context.get_per_replica_batch_size(global_batch_size)\n  d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)\n  return d.shard(input_context.num_input_pipelines,\n                 input_context.input_pipeline_id)\nwith strategy.scope():\n  iterator = strategy.make_input_fn_iterator(input_fn)\n  replica_results = strategy.experimental_run(replica_fn, iterator)\n The tf.data.Dataset returned by input_fn should have a per-replica batch size, which may be computed using input_context.get_per_replica_batch_size.\n \n\n\n Args\n  input_fn   A function taking a tf.distribute.InputContext object and returning a tf.data.Dataset.  \n  replication_mode   an enum value of tf.distribute.InputReplicationMode. Only PER_WORKER is supported currently, which means there will be a single call to input_fn per worker. Replicas will dequeue from the local tf.data.Dataset on their worker.   \n \n\n\n Returns   An iterator object that should first be .initialize()-ed. It may then either be passed to strategy.experimental_run() or you can iterator.get_next() to get the next value to pass to strategy.extended.call_for_each_replica().  \n reduce View source \nreduce(\n    reduce_op, value, axis=None\n)\n Reduce value across replicas and return result on current device. \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndef step_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\n\nper_replica_result = strategy.run(step_fn)\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\ntotal\n<tf.Tensor: shape=(), dtype=int32, numpy=1>\n To see how this would look with multiple replicas, consider the same example with MirroredStrategy with 2 GPUs: strategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\ndef step_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\n\nper_replica_result = strategy.run(step_fn)\n# Check devices on which per replica result is:\nstrategy.experimental_local_results(per_replica_result)[0].device\n# /job:localhost/replica:0/task:0/device:GPU:0\nstrategy.experimental_local_results(per_replica_result)[1].device\n# /job:localhost/replica:0/task:0/device:GPU:1\n\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\n# Check device on which reduced result is:\ntotal.device\n# /job:localhost/replica:0/task:0/device:CPU:0\n\n This API is typically used for aggregating the results returned from different replicas, for reporting etc. For example, loss computed from different replicas can be averaged using this API before printing. \nNote: The result is copied to the \"current\" device - which would typically be the CPU of the worker on which the program is running. For TPUStrategy, it is the first TPU host. For multi client MultiWorkerMirroredStrategy, this is CPU of each worker.\n There are a number of different tf.distribute APIs for reducing values across replicas:  \ntf.distribute.ReplicaContext.all_reduce: This differs from Strategy.reduce in that it is for replica context and does not copy the results to the host device. all_reduce should be typically used for reductions inside the training step such as gradients. \ntf.distribute.StrategyExtended.reduce_to and tf.distribute.StrategyExtended.batch_reduce_to: These APIs are more advanced versions of Strategy.reduce as they allow customizing the destination of the result. They are also called in cross replica context.  What should axis be? Given a per-replica value returned by run, say a per-example loss, the batch will be divided across all the replicas. This function allows you to aggregate across replicas and optionally also across batch elements by specifying the axis parameter accordingly. For example, if you have a global batch size of 8 and 2 replicas, values for examples [0, 1, 2, 3] will be on replica 0 and [4, 5, 6, 7] will be on replica 1. With axis=None, reduce will aggregate only across replicas, returning [0+4, 1+5, 2+6, 3+7]. This is useful when each replica is computing a scalar or some other value that doesn't have a \"batch\" dimension (like a gradient or loss). strategy.reduce(\"sum\", per_replica_result, axis=None)\n Sometimes, you will want to aggregate across both the global batch and all replicas. You can get this behavior by specifying the batch dimension as the axis, typically axis=0. In this case it would return a scalar 0+1+2+3+4+5+6+7. strategy.reduce(\"sum\", per_replica_result, axis=0)\n If there is a last partial batch, you will need to specify an axis so that the resulting shape is consistent across replicas. So if the last batch has size 6 and it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch unless you specify axis=0. If you specify tf.distribute.ReduceOp.MEAN, using axis=0 will use the correct denominator of 6. Contrast this with computing reduce_mean to get a scalar value on each replica and this function to average those means, which will weigh some values 1/8 and others 1/4.\n \n\n\n Args\n  reduce_op   a tf.distribute.ReduceOp value specifying how values should be combined. Allows using string representation of the enum such as \"SUM\", \"MEAN\".  \n  value   a tf.distribute.DistributedValues instance, e.g. returned by Strategy.run, to be combined into a single tensor. It can also be a regular tensor when used with OneDeviceStrategy or default strategy.  \n  axis   specifies the dimension to reduce along within each replica's tensor. Should typically be set to the batch dimension, or None to only reduce across replicas (e.g. if the tensor has no batch dimension).   \n \n\n\n Returns   A Tensor.  \n run View source \nrun(\n    fn, args=(), kwargs=None, options=None\n)\n Invokes fn on each replica, with the given arguments. This method is the primary way to distribute your computation with a tf.distribute object. It invokes fn on each replica. If args or kwargs have tf.distribute.DistributedValues, such as those produced by a tf.distribute.DistributedDataset from tf.distribute.Strategy.experimental_distribute_dataset or tf.distribute.Strategy.distribute_datasets_from_function, when fn is executed on a particular replica, it will be executed with the component of tf.distribute.DistributedValues that correspond to that replica. fn is invoked under a replica context. fn may call tf.distribute.get_replica_context() to access members such as all_reduce. Please see the module-level docstring of tf.distribute for the concept of replica context. All arguments in args or kwargs should either be Python values of a nested structure of tensors, e.g. a list of tensors, in which case args and kwargs will be passed to the fn invoked on each replica. Or args or kwargs can be tf.distribute.DistributedValues containing tensors or composite tensors, i.e. tf.compat.v1.TensorInfo.CompositeTensor, in which case each fn call will get the component of a tf.distribute.DistributedValues corresponding to its replica. Key Point: Depending on the implementation of tf.distribute.Strategy and whether eager execution is enabled, fn may be called one or more times. If fn is annotated with tf.function or tf.distribute.Strategy.run is called inside a tf.function (eager execution is disabled inside a tf.function by default), fn is called once per replica to generate a Tensorflow graph, which will then be reused for execution with new inputs. Otherwise, if eager execution is enabled, fn will be called once per replica every step just like regular python code. Example usage:  Constant tensor input.  \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ntensor_input = tf.constant(3.0)\n@tf.function\ndef replica_fn(input):\n  return input*2.0\nresult = strategy.run(replica_fn, args=(tensor_input,))\nresult\nPerReplica:{\n  0: <tf.Tensor: shape=(), dtype=float32, numpy=6.0>,\n  1: <tf.Tensor: shape=(), dtype=float32, numpy=6.0>\n}\n  DistributedValues input.  \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n@tf.function\ndef run():\n  def value_fn(value_context):\n    return value_context.num_replicas_in_sync\n  distributed_values = (\n    strategy.experimental_distribute_values_from_function(\n      value_fn))\n  def replica_fn2(input):\n    return input*2\n  return strategy.run(replica_fn2, args=(distributed_values,))\nresult = run()\nresult\n<tf.Tensor: shape=(), dtype=int32, numpy=4>\n  Use tf.distribute.ReplicaContext to allreduce values.  \nstrategy = tf.distribute.MirroredStrategy([\"gpu:0\", \"gpu:1\"])\n@tf.function\ndef run():\n   def value_fn(value_context):\n     return tf.constant(value_context.replica_id_in_sync_group)\n   distributed_values = (\n       strategy.experimental_distribute_values_from_function(\n           value_fn))\n   def replica_fn(input):\n     return tf.distribute.get_replica_context().all_reduce(\"sum\", input)\n   return strategy.run(replica_fn, args=(distributed_values,))\nresult = run()\nresult\nPerReplica:{\n  0: <tf.Tensor: shape=(), dtype=int32, numpy=1>,\n  1: <tf.Tensor: shape=(), dtype=int32, numpy=1>\n}\n\n \n\n\n Args\n  fn   The function to run on each replica.  \n  args   Optional positional arguments to fn. Its element can be a Python value, a tensor or a tf.distribute.DistributedValues.  \n  kwargs   Optional keyword arguments to fn. Its element can be a Python value, a tensor or a tf.distribute.DistributedValues.  \n  options   An optional instance of tf.distribute.RunOptions specifying the options to run fn.   \n \n\n\n Returns   Merged return value of fn across replicas. The structure of the return value is the same as the return value from fn. Each element in the structure can either be tf.distribute.DistributedValues, Tensor objects, or Tensors (for example, if running on a single replica).  \n scope View source \nscope()\n Context manager to make the strategy current and distribute variables. This method returns a context manager, and is used as follows: \nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n# Variable created inside scope:\nwith strategy.scope():\n  mirrored_variable = tf.Variable(1.)\nmirrored_variable\nMirroredVariable:{\n  0: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>,\n  1: <tf.Variable 'Variable/replica_1:0' shape=() dtype=float32, numpy=1.0>\n}\n# Variable created outside scope:\nregular_variable = tf.Variable(1.)\nregular_variable\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\n What happens when Strategy.scope is entered?  \nstrategy is installed in the global context as the \"current\" strategy. Inside this scope, tf.distribute.get_strategy() will now return this strategy. Outside this scope, it returns the default no-op strategy. Entering the scope also enters the \"cross-replica context\". See tf.distribute.StrategyExtended for an explanation on cross-replica and replica contexts. Variable creation inside scope is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like MirroredStrategy, TPUStrategy and MultiWorkerMiroredStrategy create variables replicated on each replica, whereas ParameterServerStrategy creates variables on the parameter servers. This is done using a custom tf.variable_creator_scope. In some strategies, a default device scope may also be entered: in MultiWorkerMiroredStrategy, a default device scope of \"/CPU:0\" is entered on each worker.  \nNote: Entering a scope does not automatically distribute a computation, except in the case of high level training framework like keras model.fit. If you're not using model.fit, you need to use strategy.run API to explicitly distribute that computation. See an example in the custom training loop tutorial.\n What should be in scope and what should be outside? There are a number of requirements on what needs to happen inside the scope. However, in places where we have information about which strategy is in use, we often enter the scope for the user, so they don't have to do it explicitly (i.e. calling those either inside or outside the scope is OK).  Anything that creates variables that should be distributed variables must be in strategy.scope. This can be either by directly putting it in scope, or relying on another API like strategy.run or model.fit to enter it for you. Any variable that is created outside scope will not be distributed and may have performance implications. Common things that create variables in TF: models, optimizers, metrics. These should always be created inside the scope. Another source of variable creation can be a checkpoint restore - when variables are created lazily. Note that any variable created inside a strategy captures the strategy information. So reading and writing to these variables outside the strategy.scope can also work seamlessly, without the user having to enter the scope. Some strategy APIs (such as strategy.run and strategy.reduce) which require to be in a strategy's scope, enter the scope for you automatically, which means when using those APIs you don't need to enter the scope yourself. When a tf.keras.Model is created inside a strategy.scope, we capture this information. When high level training frameworks methods such as model.compile, model.fit etc are then called on this model, we automatically enter the scope, as well as use this strategy to distribute the training etc. See detailed example in distributed keras tutorial. Note that simply calling the model(..) is not impacted - only high level training framework APIs are. model.compile, model.fit, model.evaluate, model.predict and model.save can all be called inside or outside the scope. The following can be either inside or outside the scope:  Creating the input datasets Defining tf.functions that represent your training step Saving APIs such as tf.saved_model.save. Loading creates variables, so that should go inside the scope if you want to train the model in a distributed way. Checkpoint saving. As mentioned above - checkpoint.restore may sometimes need to be inside scope if it creates variables. \n \n \n\n\n Returns   A context manager.  \n update_config_proto View source \nupdate_config_proto(\n    config_proto\n)\n Returns a copy of config_proto modified for use with this strategy. DEPRECATED: This method is not available in TF 2.x. The updated config has something needed to run a strategy, e.g. configuration to run collective ops, or device filters to improve distributed training performance.\n \n\n\n Args\n  config_proto   a tf.ConfigProto object.   \n \n\n\n Returns   The updated copy of the config_proto.  \n  \n"}, {"name": "tf.compat.v1.distribute.StrategyExtended", "path": "compat/v1/distribute/strategyextended", "type": "tf.compat", "text": "tf.compat.v1.distribute.StrategyExtended Additional APIs for algorithms that need to be distribution-aware. Inherits From: StrategyExtended \ntf.compat.v1.distribute.StrategyExtended(\n    container_strategy\n)\n \nNote: For most usage of tf.distribute.Strategy, there should be no need to call these methods, since TensorFlow libraries (such as optimizers) already call these methods when needed on your behalf.\n Some common use cases of functions on this page:  Locality  tf.distribute.DistributedValues can have the same locality as a distributed variable, which leads to a mirrored value residing on the same devices as the variable (as opposed to the compute devices). Such values may be passed to a call to tf.distribute.StrategyExtended.update to update the value of a variable. You may use tf.distribute.StrategyExtended.colocate_vars_with to give a variable the same locality as another variable. You may convert a \"PerReplica\" value to a variable's locality by using tf.distribute.StrategyExtended.reduce_to or tf.distribute.StrategyExtended.batch_reduce_to.  How to update a distributed variable  A distributed variable is variables created on multiple devices. As discussed in the glossary, mirrored variable and SyncOnRead variable are two examples. The standard pattern for updating distributed variables is to:  In your function passed to tf.distribute.Strategy.run, compute a list of (update, variable) pairs. For example, the update might be a gradient of the loss with respect to the variable. Switch to cross-replica mode by calling tf.distribute.get_replica_context().merge_call() with the updates and variables as arguments. Call tf.distribute.StrategyExtended.reduce_to(VariableAggregation.SUM, t, v) (for one variable) or tf.distribute.StrategyExtended.batch_reduce_to (for a list of variables) to sum the updates. Call tf.distribute.StrategyExtended.update(v) for each variable to update its value.  Steps 2 through 4 are done automatically by class tf.keras.optimizers.Optimizer if you call its tf.keras.optimizers.Optimizer.apply_gradients method in a replica context. In fact, a higher-level solution to update a distributed variable is by calling assign on the variable as you would do to a regular tf.Variable. You can call the method in both replica context and cross-replica context. For a mirrored variable, calling assign in replica context requires you to specify the aggregation type in the variable constructor. In that case, the context switching and sync described in steps 2 through 4 are handled for you. If you call assign on mirrored variable in cross-replica context, you can only assign a single value or assign values from another mirrored variable or a mirrored tf.distribute.DistributedValues. For a SyncOnRead variable, in replica context, you can simply call assign on it and no aggregation happens under the hood. In cross-replica context, you can only assign a single value to a SyncOnRead variable. One example case is restoring from a checkpoint: if the aggregation type of the variable is tf.VariableAggregation.SUM, it is assumed that replica values were added before checkpointing, so at the time of restoring, the value is divided by the number of replicas and then assigned to each replica; if the aggregation type is tf.VariableAggregation.MEAN, the value is assigned to each replica directly.\n \n\n\n Attributes\n  experimental_between_graph   Whether the strategy uses between-graph replication or not. This is expected to return a constant value that will not be changed throughout its life cycle. \n \n  experimental_require_static_shapes   Returns True if static shape is required; False otherwise.  \n  experimental_should_init   Whether initialization is needed.  \n  parameter_devices   Returns the tuple of all devices used to place variables.  \n  should_checkpoint   Whether checkpointing is needed.  \n  should_save_summary   Whether saving summaries is needed.  \n  worker_devices   Returns the tuple of all devices used to for compute replica execution.    Methods batch_reduce_to View source \nbatch_reduce_to(\n    reduce_op, value_destination_pairs, options=None\n)\n Combine multiple reduce_to calls into one for faster execution. Similar to reduce_to, but accepts a list of (value, destinations) pairs. It's more efficient than reduce each value separately. This API currently can only be called in cross-replica context. Other variants to reduce values across replicas are:  \ntf.distribute.StrategyExtended.reduce_to: the non-batch version of this API. \ntf.distribute.ReplicaContext.all_reduce: the counterpart of this API in replica context. It supports both batched and non-batched all-reduce. \ntf.distribute.Strategy.reduce: a more convenient method to reduce to the host in cross-replica context.  See reduce_to for more information. \n@tf.function\ndef step_fn(var):\n\n  def merge_fn(strategy, value, var):\n    # All-reduce the value. Note that `value` here is a\n    # `tf.distribute.DistributedValues`.\n    reduced = strategy.extended.batch_reduce_to(\n        tf.distribute.ReduceOp.SUM, [(value, var)])[0]\n    strategy.extended.update(var, lambda var, value: var.assign(value),\n        args=(reduced,))\n\n  value = tf.identity(1.)\n  tf.distribute.get_replica_context().merge_call(merge_fn,\n    args=(value, var))\n\ndef run(strategy):\n  with strategy.scope():\n    v = tf.Variable(0.)\n    strategy.run(step_fn, args=(v,))\n    return v\n\nrun(tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"]))\nMirroredVariable:{\n  0: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0>,\n  1: <tf.Variable 'Variable/replica_1:0' shape=() dtype=float32, numpy=2.0>\n}\nrun(tf.distribute.experimental.CentralStorageStrategy(\n    compute_devices=[\"GPU:0\", \"GPU:1\"], parameter_device=\"CPU:0\"))\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0>\nrun(tf.distribute.OneDeviceStrategy(\"GPU:0\"))\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\n\n \n\n\n Args\n  reduce_op   a tf.distribute.ReduceOp value specifying how values should be combined. Allows using string representation of the enum such as \"SUM\", \"MEAN\".  \n  value_destination_pairs   a sequence of (value, destinations) pairs. See tf.distribute.Strategy.reduce_to for descriptions.  \n  options   a tf.distribute.experimental.CommunicationOptions. Options to perform collective operations. This overrides the default options if the tf.distribute.Strategy takes one in the constructor. See tf.distribute.experimental.CommunicationOptions for details of the options.   \n \n\n\n Returns   A list of reduced values, one per pair in value_destination_pairs.  \n broadcast_to View source \nbroadcast_to(\n    tensor, destinations\n)\n Mirror a tensor on one device to all worker devices.\n \n\n\n Args\n  tensor   A Tensor value to broadcast.  \n  destinations   A mirrored variable or device string specifying the destination devices to copy tensor to.   \n \n\n\n Returns   A value mirrored to destinations devices.  \n call_for_each_replica View source \ncall_for_each_replica(\n    fn, args=(), kwargs=None\n)\n Run fn once per replica. fn may call tf.get_replica_context() to access methods such as replica_id_in_sync_group and merge_call(). merge_call() is used to communicate between the replicas and re-enter the cross-replica context. All replicas pause their execution having encountered a merge_call() call. After that the merge_fn-function is executed. Its results are then unwrapped and given back to each replica call. After that execution resumes until fn is complete or encounters another merge_call(). Example: # Called once in \"cross-replica\" context.\ndef merge_fn(distribution, three_plus_replica_id):\n  # sum the values across replicas\n  return sum(distribution.experimental_local_results(three_plus_replica_id))\n\n# Called once per replica in `distribution`, in a \"replica\" context.\ndef fn(three):\n  replica_ctx = tf.get_replica_context()\n  v = three + replica_ctx.replica_id_in_sync_group\n  # Computes the sum of the `v` values across all replicas.\n  s = replica_ctx.merge_call(merge_fn, args=(v,))\n  return s + v\n\nwith distribution.scope():\n  # in \"cross-replica\" context\n  ...\n  merged_results = distribution.run(fn, args=[3])\n  # merged_results has the values from every replica execution of `fn`.\n  # This statement prints a list:\n  print(distribution.experimental_local_results(merged_results))\n\n \n\n\n Args\n  fn   function to run (will be run once per replica).  \n  args   Tuple or list with positional arguments for fn.  \n  kwargs   Dict with keyword arguments for fn.   \n \n\n\n Returns   Merged return value of fn across all replicas.  \n colocate_vars_with View source \ncolocate_vars_with(\n    colocate_with_variable\n)\n Scope that controls which devices variables will be created on. No operations should be added to the graph inside this scope, it should only be used when creating variables (some implementations work by changing variable creation, others work by using a tf.compat.v1.colocate_with() scope). This may only be used inside self.scope(). Example usage: with strategy.scope():\n  var1 = tf.Variable(...)\n  with strategy.extended.colocate_vars_with(var1):\n    # var2 and var3 will be created on the same device(s) as var1\n    var2 = tf.Variable(...)\n    var3 = tf.Variable(...)\n\n  def fn(v1, v2, v3):\n    # operates on v1 from var1, v2 from var2, and v3 from var3\n\n  # `fn` runs on every device `var1` is on, `var2` and `var3` will be there\n  # too.\n  strategy.extended.update(var1, fn, args=(var2, var3))\n\n \n\n\n Args\n  colocate_with_variable   A variable created in this strategy's scope(). Variables created while in the returned context manager will be on the same set of devices as colocate_with_variable.   \n \n\n\n Returns   A context manager.  \n experimental_make_numpy_dataset View source \nexperimental_make_numpy_dataset(\n    numpy_input, session=None\n)\n Makes a dataset for input provided via a numpy array. This avoids adding numpy_input as a large constant in the graph, and copies the data to the machine or machines that will be processing the input.\n \n\n\n Args\n  numpy_input   A nest of NumPy input arrays that will be distributed evenly across all replicas. Note that lists of Numpy arrays are stacked, as that is normal tf.data.Dataset behavior.  \n  session   (TensorFlow v1.x graph execution only) A session used for initialization.   \n \n\n\n Returns   A tf.data.Dataset representing numpy_input.  \n experimental_run_steps_on_iterator View source \nexperimental_run_steps_on_iterator(\n    fn, iterator, iterations=1, initial_loop_values=None\n)\n DEPRECATED: please use run instead. Run fn with input from iterator for iterations times. This method can be used to run a step function for training a number of times using input from a dataset.\n \n\n\n Args\n  fn   function to run using this distribution strategy. The function must have the following signature: def fn(context, inputs). context is an instance of MultiStepContext that will be passed when fn is run. context can be used to specify the outputs to be returned from fn by calling context.set_last_step_output. It can also be used to capture non tensor outputs by context.set_non_tensor_output. See MultiStepContext documentation for more information. inputs will have same type/structure as iterator.get_next(). Typically, fn will use call_for_each_replica method of the strategy to distribute the computation over multiple replicas.  \n  iterator   Iterator of a dataset that represents the input for fn. The caller is responsible for initializing the iterator as needed.  \n  iterations   (Optional) Number of iterations that fn should be run. Defaults to 1.  \n  initial_loop_values   (Optional) Initial values to be passed into the loop that runs fn. Defaults to None. initial_loop_values argument when we have a mechanism to infer the outputs of fn.   \n \n\n\n Returns   Returns the MultiStepContext object which has the following properties, among other things:  run_op: An op that runs fn iterations times. last_step_outputs: A dictionary containing tensors set using context.set_last_step_output. Evaluating this returns the value of the tensors after the last iteration. non_tensor_outputs: A dictionary containing anything that was set by fn by calling context.set_non_tensor_output. \n\n \n non_slot_devices View source \nnon_slot_devices(\n    var_list\n)\n Device(s) for non-slot variables. DEPRECATED: TF 1.x ONLY. This method returns non-slot devices where non-slot variables are placed. Users can create non-slot variables on these devices by using a block: with tf.distribute.StrategyExtended.colocate_vars_with(tf.distribute.StrategyExtended.non_slot_devices(...)):\n  ...\n\n \n\n\n Args\n  var_list   The list of variables being optimized, needed with the default tf.distribute.Strategy.   \n \n\n\n Returns   A sequence of devices for non-slot variables.  \n read_var View source \nread_var(\n    v\n)\n Reads the value of a variable. Returns the aggregate value of a replica-local variable, or the (read-only) value of any other variable.\n \n\n\n Args\n  v   A variable allocated within the scope of this tf.distribute.Strategy.   \n \n\n\n Returns   A tensor representing the value of v, aggregated across replicas if necessary.  \n reduce_to View source \nreduce_to(\n    reduce_op, value, destinations, options=None\n)\n Combine (via e.g. sum or mean) values across replicas. reduce_to aggregates tf.distribute.DistributedValues and distributed variables. It supports both dense values and tf.IndexedSlices. This API currently can only be called in cross-replica context. Other variants to reduce values across replicas are:  \ntf.distribute.StrategyExtended.batch_reduce_to: the batch version of this API. \ntf.distribute.ReplicaContext.all_reduce: the counterpart of this API in replica context. It supports both batched and non-batched all-reduce. \ntf.distribute.Strategy.reduce: a more convenient method to reduce to the host in cross-replica context.  destinations specifies where to reduce the value to, e.g. \"GPU:0\". You can also pass in a Tensor, and the destinations will be the device of that tensor. For all-reduce, pass the same to value and destinations. It can be used in tf.distribute.ReplicaContext.merge_call to write code that works for all tf.distribute.Strategy. \n@tf.function\ndef step_fn(var):\n\n  def merge_fn(strategy, value, var):\n    # All-reduce the value. Note that `value` here is a\n    # `tf.distribute.DistributedValues`.\n    reduced = strategy.extended.reduce_to(tf.distribute.ReduceOp.SUM,\n        value, destinations=var)\n    strategy.extended.update(var, lambda var, value: var.assign(value),\n        args=(reduced,))\n\n  value = tf.identity(1.)\n  tf.distribute.get_replica_context().merge_call(merge_fn,\n    args=(value, var))\n\ndef run(strategy):\n  with strategy.scope():\n    v = tf.Variable(0.)\n    strategy.run(step_fn, args=(v,))\n    return v\n\nrun(tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"]))\nMirroredVariable:{\n  0: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0>,\n  1: <tf.Variable 'Variable/replica_1:0' shape=() dtype=float32, numpy=2.0>\n}\nrun(tf.distribute.experimental.CentralStorageStrategy(\n    compute_devices=[\"GPU:0\", \"GPU:1\"], parameter_device=\"CPU:0\"))\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0>\nrun(tf.distribute.OneDeviceStrategy(\"GPU:0\"))\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\n\n \n\n\n Args\n  reduce_op   a tf.distribute.ReduceOp value specifying how values should be combined. Allows using string representation of the enum such as \"SUM\", \"MEAN\".  \n  value   a tf.distribute.DistributedValues, or a tf.Tensor like object.  \n  destinations   a tf.distribute.DistributedValues, a tf.Variable, a tf.Tensor alike object, or a device string. It specifies the devices to reduce to. To perform an all-reduce, pass the same to value and destinations. Note that if it's a tf.Variable, the value is reduced to the devices of that variable, and this method doesn't update the variable.  \n  options   a tf.distribute.experimental.CommunicationOptions. Options to perform collective operations. This overrides the default options if the tf.distribute.Strategy takes one in the constructor. See tf.distribute.experimental.CommunicationOptions for details of the options.   \n \n\n\n Returns   A tensor or value reduced to destinations.  \n update View source \nupdate(\n    var, fn, args=(), kwargs=None, group=True\n)\n Run fn to update var using inputs mirrored to the same devices. tf.distribute.StrategyExtended.update takes a distributed variable var to be updated, an update function fn, and args and kwargs for fn. It applies fn to each component variable of var and passes corresponding values from args and kwargs. Neither args nor kwargs may contain per-replica values. If they contain mirrored values, they will be unwrapped before calling fn. For example, fn can be assign_add and args can be a mirrored DistributedValues where each component contains the value to be added to this mirrored variable var. Calling update will call assign_add on each component variable of var with the corresponding tensor value on that device. Example usage: strategy = tf.distribute.MirroredStrategy(['GPU:0', 'GPU:1']) # With 2\ndevices\nwith strategy.scope():\n  v = tf.Variable(5.0, aggregation=tf.VariableAggregation.SUM)\ndef update_fn(v):\n  return v.assign(1.0)\nresult = strategy.extended.update(v, update_fn)\n# result is\n# Mirrored:{\n#  0: tf.Tensor(1.0, shape=(), dtype=float32),\n#  1: tf.Tensor(1.0, shape=(), dtype=float32)\n# }\n If var is mirrored across multiple devices, then this method implements logic as following: results = {}\nfor device, v in var:\n  with tf.device(device):\n    # args and kwargs will be unwrapped if they are mirrored.\n    results[device] = fn(v, *args, **kwargs)\nreturn merged(results)\n Otherwise, this method returns fn(var, *args, **kwargs) colocated with var.\n \n\n\n Args\n  var   Variable, possibly mirrored to multiple devices, to operate on.  \n  fn   Function to call. Should take the variable as the first argument.  \n  args   Tuple or list. Additional positional arguments to pass to fn().  \n  kwargs   Dict with keyword arguments to pass to fn().  \n  group   Boolean. Defaults to True. If False, the return value will be unwrapped.   \n \n\n\n Returns   By default, the merged return value of fn across all replicas. The merged result has dependencies to make sure that if it is evaluated at all, the side effects (updates) will happen on every replica. If instead \"group=False\" is specified, this function will return a nest of lists where each list has an element per replica, and the caller is responsible for ensuring all elements are executed.  \n update_non_slot View source \nupdate_non_slot(\n    colocate_with, fn, args=(), kwargs=None, group=True\n)\n Runs fn(*args, **kwargs) on colocate_with devices. Used to update non-slot variables. DEPRECATED: TF 1.x ONLY.\n \n\n\n Args\n  colocate_with   Devices returned by non_slot_devices().  \n  fn   Function to execute.  \n  args   Tuple or list. Positional arguments to pass to fn().  \n  kwargs   Dict with keyword arguments to pass to fn().  \n  group   Boolean. Defaults to True. If False, the return value will be unwrapped.   \n \n\n\n Returns   Return value of fn, possibly merged across devices.  \n value_container View source \nvalue_container(\n    value\n)\n Returns the container that this per-replica value belongs to.\n \n\n\n Args\n  value   A value returned by run() or a variable created in scope().   \n \n\n\n Returns   A container that value belongs to. If value does not belong to any container (including the case of container having been destroyed), returns the value itself. value in experimental_local_results(value_container(value)) will always be true.  \n variable_created_in_scope View source \nvariable_created_in_scope(\n    v\n)\n Tests whether v was created while this strategy scope was active. Variables created inside the strategy scope are \"owned\" by it: \nstrategy = tf.distribute.MirroredStrategy()\nwith strategy.scope():\n  v = tf.Variable(1.)\nstrategy.extended.variable_created_in_scope(v)\nTrue\n Variables created outside the strategy are not owned by it: \nstrategy = tf.distribute.MirroredStrategy()\nv = tf.Variable(1.)\nstrategy.extended.variable_created_in_scope(v)\nFalse\n\n \n\n\n Args\n  v   A tf.Variable instance.   \n \n\n\n Returns   True if v was created inside the scope, False if not.  \n  \n"}, {"name": "tf.compat.v1.distributions", "path": "compat/v1/distributions", "type": "tf.compat", "text": "Module: tf.compat.v1.distributions Core module for TensorFlow distribution objects and helpers. Classes class Bernoulli: Bernoulli distribution. class Beta: Beta distribution. class Categorical: Categorical distribution. class Dirichlet: Dirichlet distribution. class DirichletMultinomial: Dirichlet-Multinomial compound distribution. class Distribution: A generic probability distribution base class. class Exponential: Exponential distribution. class Gamma: Gamma distribution. class Laplace: The Laplace distribution with location loc and scale parameters. class Multinomial: Multinomial distribution. class Normal: The Normal distribution with location loc and scale parameters. class RegisterKL: Decorator to register a KL divergence implementation function. class ReparameterizationType: Instances of this class represent how sampling is reparameterized. class StudentT: Student's t-distribution. class Uniform: Uniform distribution with low and high parameters. Functions kl_divergence(...): Get the KL-divergence KL(distribution_a || distribution_b). (deprecated)\n \n\n\n Other Members\n  FULLY_REPARAMETERIZED   tf.compat.v1.distributions.ReparameterizationType  \n  NOT_REPARAMETERIZED   tf.compat.v1.distributions.ReparameterizationType     \n"}, {"name": "tf.compat.v1.distributions.Bernoulli", "path": "compat/v1/distributions/bernoulli", "type": "tf.compat", "text": "tf.compat.v1.distributions.Bernoulli Bernoulli distribution. Inherits From: Distribution \ntf.compat.v1.distributions.Bernoulli(\n    logits=None, probs=None, dtype=tf.dtypes.int32, validate_args=False,\n    allow_nan_stats=True, name='Bernoulli'\n)\n The Bernoulli distribution with probs parameter, i.e., the probability of a 1 outcome (vs a 0 outcome).\n \n\n\n Args\n  logits   An N-D Tensor representing the log-odds of a 1 event. Each entry in the Tensor parametrizes an independent Bernoulli distribution where the probability of an event is sigmoid(logits). Only one of logits or probs should be passed in.  \n  probs   An N-D Tensor representing the probability of a 1 event. Each entry in the Tensor parameterizes an independent Bernoulli distribution. Only one of logits or probs should be passed in.  \n  dtype   The type of the event samples. Default: int32.  \n  validate_args   Python bool, default False. When True distribution parameters are checked for validity despite possibly degrading runtime performance. When False invalid inputs may silently render incorrect outputs.  \n  allow_nan_stats   Python bool, default True. When True, statistics (e.g., mean, mode, variance) use the value \"NaN\" to indicate the result is undefined. When False, an exception is raised if one or more of the statistic's batch members are undefined.  \n  name   Python str name prefixed to Ops created by this class.   \n \n\n\n Raises\n  ValueError   If p and logits are passed, or if neither are passed.   \n \n\n\n Attributes\n  allow_nan_stats   Python bool describing behavior when a stat is undefined. Stats return +/- infinity when it makes sense. E.g., the variance of a Cauchy distribution is infinity. However, sometimes the statistic is undefined, e.g., if a distribution's pdf does not achieve a maximum within the support of the distribution, the mode is undefined. If the mean is undefined, then by definition the variance is undefined. E.g. the mean for Student's T for df = 1 is undefined (no clear way to say it is either + or - infinity), so the variance = E[(X - mean)**2] is also undefined. \n \n  batch_shape   Shape of a single sample from a single event index as a TensorShape. May be partially defined or unknown. The batch dimensions are indexes into independent, non-identical parameterizations of this distribution. \n \n  dtype   The DType of Tensors handled by this Distribution.  \n  event_shape   Shape of a single sample from a single batch as a TensorShape. May be partially defined or unknown. \n \n  logits   Log-odds of a 1 outcome (vs 0).  \n  name   Name prepended to all ops created by this Distribution.  \n  parameters   Dictionary of parameters used to instantiate this Distribution.  \n  probs   Probability of a 1 outcome (vs 0).  \n  reparameterization_type   Describes how samples from the distribution are reparameterized. Currently this is one of the static instances distributions.FULLY_REPARAMETERIZED or distributions.NOT_REPARAMETERIZED. \n \n  validate_args   Python bool indicating possibly expensive checks are enabled.    Methods batch_shape_tensor View source \nbatch_shape_tensor(\n    name='batch_shape_tensor'\n)\n Shape of a single sample from a single event index as a 1-D Tensor. The batch dimensions are indexes into independent, non-identical parameterizations of this distribution.\n \n\n\n Args\n  name   name to give to the op   \n \n\n\n Returns\n  batch_shape   Tensor.    cdf View source \ncdf(\n    value, name='cdf'\n)\n Cumulative distribution function. Given random variable X, the cumulative distribution function cdf is: cdf(x) := P[X <= x]\n\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  cdf   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    copy View source \ncopy(\n    **override_parameters_kwargs\n)\n Creates a deep copy of the distribution. \nNote: the copy distribution may continue to depend on the original initialization arguments.\n\n \n\n\n Args\n  **override_parameters_kwargs   String/value dictionary of initialization arguments to override with new values.   \n \n\n\n Returns\n  distribution   A new instance of type(self) initialized from the union of self.parameters and override_parameters_kwargs, i.e., dict(self.parameters, **override_parameters_kwargs).    covariance View source \ncovariance(\n    name='covariance'\n)\n Covariance. Covariance is (possibly) defined only for non-scalar-event distributions. For example, for a length-k, vector-valued distribution, it is calculated as, Cov[i, j] = Covariance(X_i, X_j) = E[(X_i - E[X_i]) (X_j - E[X_j])]\n where Cov is a (batch of) k x k matrix, 0 <= (i, j) < k, and E denotes expectation. Alternatively, for non-vector, multivariate distributions (e.g., matrix-valued, Wishart), Covariance shall return a (batch of) matrices under some vectorization of the events, i.e., Cov[i, j] = Covariance(Vec(X)_i, Vec(X)_j) = [as above]\n where Cov is a (batch of) k' x k' matrices, 0 <= (i, j) < k' = reduce_prod(event_shape), and Vec is some function mapping indices of this distribution's event dimensions to indices of a length-k' vector.\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  covariance   Floating-point Tensor with shape [B1, ..., Bn, k', k'] where the first n dimensions are batch coordinates and k' = reduce_prod(self.event_shape).    cross_entropy View source \ncross_entropy(\n    other, name='cross_entropy'\n)\n Computes the (Shannon) cross entropy. Denote this distribution (self) by P and the other distribution by Q. Assuming P, Q are absolutely continuous with respect to one another and permit densities p(x) dr(x) and q(x) dr(x), (Shanon) cross entropy is defined as: H[P, Q] = E_p[-log q(X)] = -int_F p(x) log q(x) dr(x)\n where F denotes the support of the random variable X ~ P.\n \n\n\n Args\n  other   tfp.distributions.Distribution instance.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  cross_entropy   self.dtype Tensor with shape [B1, ..., Bn] representing n different calculations of (Shanon) cross entropy.    entropy View source \nentropy(\n    name='entropy'\n)\n Shannon entropy in nats. event_shape_tensor View source \nevent_shape_tensor(\n    name='event_shape_tensor'\n)\n Shape of a single sample from a single batch as a 1-D int32 Tensor.\n \n\n\n Args\n  name   name to give to the op   \n \n\n\n Returns\n  event_shape   Tensor.    is_scalar_batch View source \nis_scalar_batch(\n    name='is_scalar_batch'\n)\n Indicates that batch_shape == [].\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  is_scalar_batch   bool scalar Tensor.    is_scalar_event View source \nis_scalar_event(\n    name='is_scalar_event'\n)\n Indicates that event_shape == [].\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  is_scalar_event   bool scalar Tensor.    kl_divergence View source \nkl_divergence(\n    other, name='kl_divergence'\n)\n Computes the Kullback--Leibler divergence. Denote this distribution (self) by p and the other distribution by q. Assuming p, q are absolutely continuous with respect to reference measure r, the KL divergence is defined as: KL[p, q] = E_p[log(p(X)/q(X))]\n         = -int_F p(x) log q(x) dr(x) + int_F p(x) log p(x) dr(x)\n         = H[p, q] - H[p]\n where F denotes the support of the random variable X ~ p, H[., .] denotes (Shanon) cross entropy, and H[.] denotes (Shanon) entropy.\n \n\n\n Args\n  other   tfp.distributions.Distribution instance.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  kl_divergence   self.dtype Tensor with shape [B1, ..., Bn] representing n different calculations of the Kullback-Leibler divergence.    log_cdf View source \nlog_cdf(\n    value, name='log_cdf'\n)\n Log cumulative distribution function. Given random variable X, the cumulative distribution function cdf is: log_cdf(x) := Log[ P[X <= x] ]\n Often, a numerical approximation can be used for log_cdf(x) that yields a more accurate answer than simply taking the logarithm of the cdf when x << -1.\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  logcdf   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    log_prob View source \nlog_prob(\n    value, name='log_prob'\n)\n Log probability density/mass function.\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  log_prob   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    log_survival_function View source \nlog_survival_function(\n    value, name='log_survival_function'\n)\n Log survival function. Given random variable X, the survival function is defined: log_survival_function(x) = Log[ P[X > x] ]\n                         = Log[ 1 - P[X <= x] ]\n                         = Log[ 1 - cdf(x) ]\n Typically, different numerical approximations can be used for the log survival function, which are more accurate than 1 - cdf(x) when x >> 1.\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns   Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.  \n mean View source \nmean(\n    name='mean'\n)\n Mean. mode View source \nmode(\n    name='mode'\n)\n Mode. Additional documentation from Bernoulli: Returns 1 if prob > 0.5 and 0 otherwise. param_shapes View source \n@classmethod\nparam_shapes(\n    sample_shape, name='DistributionParamShapes'\n)\n Shapes of parameters given the desired shape of a call to sample(). This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample(). Subclasses should override class method _param_shapes.\n \n\n\n Args\n  sample_shape   Tensor or python list/tuple. Desired shape of a call to sample().  \n  name   name to prepend ops with.   \n \n\n\n Returns   dict of parameter name to Tensor shapes.  \n param_static_shapes View source \n@classmethod\nparam_static_shapes(\n    sample_shape\n)\n param_shapes with static (i.e. TensorShape) shapes. This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample(). Assumes that the sample's shape is known statically. Subclasses should override class method _param_shapes to return constant-valued tensors when constant values are fed.\n \n\n\n Args\n  sample_shape   TensorShape or python list/tuple. Desired shape of a call to sample().   \n \n\n\n Returns   dict of parameter name to TensorShape.  \n\n \n\n\n Raises\n  ValueError   if sample_shape is a TensorShape and is not fully defined.    prob View source \nprob(\n    value, name='prob'\n)\n Probability density/mass function.\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  prob   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    quantile View source \nquantile(\n    value, name='quantile'\n)\n Quantile function. Aka \"inverse cdf\" or \"percent point function\". Given random variable X and p in [0, 1], the quantile is: quantile(p) := x such that P[X <= x] == p\n\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  quantile   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    sample View source \nsample(\n    sample_shape=(), seed=None, name='sample'\n)\n Generate samples of the specified shape. Note that a call to sample() without arguments will generate a single sample.\n \n\n\n Args\n  sample_shape   0D or 1D int32 Tensor. Shape of the generated samples.  \n  seed   Python integer seed for RNG  \n  name   name to give to the op.   \n \n\n\n Returns\n  samples   a Tensor with prepended dimensions sample_shape.    stddev View source \nstddev(\n    name='stddev'\n)\n Standard deviation. Standard deviation is defined as, stddev = E[(X - E[X])**2]**0.5\n where X is the random variable associated with this distribution, E denotes expectation, and stddev.shape = batch_shape + event_shape.\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  stddev   Floating-point Tensor with shape identical to batch_shape + event_shape, i.e., the same shape as self.mean().    survival_function View source \nsurvival_function(\n    value, name='survival_function'\n)\n Survival function. Given random variable X, the survival function is defined: survival_function(x) = P[X > x]\n                     = 1 - P[X <= x]\n                     = 1 - cdf(x).\n\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns   Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.  \n variance View source \nvariance(\n    name='variance'\n)\n Variance. Variance is defined as, Var = E[(X - E[X])**2]\n where X is the random variable associated with this distribution, E denotes expectation, and Var.shape = batch_shape + event_shape.\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  variance   Floating-point Tensor with shape identical to batch_shape + event_shape, i.e., the same shape as self.mean().     \n"}, {"name": "tf.compat.v1.distributions.Beta", "path": "compat/v1/distributions/beta", "type": "tf.compat", "text": "tf.compat.v1.distributions.Beta Beta distribution. Inherits From: Distribution \ntf.compat.v1.distributions.Beta(\n    concentration1=None, concentration0=None, validate_args=False,\n    allow_nan_stats=True, name='Beta'\n)\n The Beta distribution is defined over the (0, 1) interval using parameters concentration1 (aka \"alpha\") and concentration0 (aka \"beta\"). Mathematical Details The probability density function (pdf) is, pdf(x; alpha, beta) = x**(alpha - 1) (1 - x)**(beta - 1) / Z\nZ = Gamma(alpha) Gamma(beta) / Gamma(alpha + beta)\n where:  \nconcentration1 = alpha, \nconcentration0 = beta, \nZ is the normalization constant, and, \nGamma is the gamma function.  The concentration parameters represent mean total counts of a 1 or a 0, i.e., concentration1 = alpha = mean * total_concentration\nconcentration0 = beta  = (1. - mean) * total_concentration\n where mean in (0, 1) and total_concentration is a positive real number representing a mean total_count = concentration1 + concentration0. Distribution parameters are automatically broadcast in all functions; see examples for details. Warning: The samples can be zero due to finite precision. This happens more often when some of the concentrations are very small. Make sure to round the samples to np.finfo(dtype).tiny before computing the density. Samples of this distribution are reparameterized (pathwise differentiable). The derivatives are computed using the approach described in (Figurnov et al., 2018). Examples import tensorflow_probability as tfp\ntfd = tfp.distributions\n\n# Create a batch of three Beta distributions.\nalpha = [1, 2, 3]\nbeta = [1, 2, 3]\ndist = tfd.Beta(alpha, beta)\n\ndist.sample([4, 5])  # Shape [4, 5, 3]\n\n# `x` has three batch entries, each with two samples.\nx = [[.1, .4, .5],\n     [.2, .3, .5]]\n# Calculate the probability of each pair of samples under the corresponding\n# distribution in `dist`.\ndist.prob(x)         # Shape [2, 3]\n\n# Create batch_shape=[2, 3] via parameter broadcast:\nalpha = [[1.], [2]]      # Shape [2, 1]\nbeta = [3., 4, 5]        # Shape [3]\ndist = tfd.Beta(alpha, beta)\n\n# alpha broadcast as: [[1., 1, 1,],\n#                      [2, 2, 2]]\n# beta broadcast as:  [[3., 4, 5],\n#                      [3, 4, 5]]\n# batch_Shape [2, 3]\ndist.sample([4, 5])  # Shape [4, 5, 2, 3]\n\nx = [.2, .3, .5]\n# x will be broadcast as [[.2, .3, .5],\n#                         [.2, .3, .5]],\n# thus matching batch_shape [2, 3].\ndist.prob(x)         # Shape [2, 3]\n Compute the gradients of samples w.r.t. the parameters: alpha = tf.constant(1.0)\nbeta = tf.constant(2.0)\ndist = tfd.Beta(alpha, beta)\nsamples = dist.sample(5)  # Shape [5]\nloss = tf.reduce_mean(tf.square(samples))  # Arbitrary loss function\n# Unbiased stochastic gradients of the loss function\ngrads = tf.gradients(loss, [alpha, beta])\n References: Implicit Reparameterization Gradients: Figurnov et al., 2018 (pdf)\n \n\n\n Args\n  concentration1   Positive floating-point Tensor indicating mean number of successes; aka \"alpha\". Implies self.dtype and self.batch_shape, i.e., concentration1.shape = [N1, N2, ..., Nm] = self.batch_shape.  \n  concentration0   Positive floating-point Tensor indicating mean number of failures; aka \"beta\". Otherwise has same semantics as concentration1.  \n  validate_args   Python bool, default False. When True distribution parameters are checked for validity despite possibly degrading runtime performance. When False invalid inputs may silently render incorrect outputs.  \n  allow_nan_stats   Python bool, default True. When True, statistics (e.g., mean, mode, variance) use the value \"NaN\" to indicate the result is undefined. When False, an exception is raised if one or more of the statistic's batch members are undefined.  \n  name   Python str name prefixed to Ops created by this class.   \n \n\n\n Attributes\n  allow_nan_stats   Python bool describing behavior when a stat is undefined. Stats return +/- infinity when it makes sense. E.g., the variance of a Cauchy distribution is infinity. However, sometimes the statistic is undefined, e.g., if a distribution's pdf does not achieve a maximum within the support of the distribution, the mode is undefined. If the mean is undefined, then by definition the variance is undefined. E.g. the mean for Student's T for df = 1 is undefined (no clear way to say it is either + or - infinity), so the variance = E[(X - mean)**2] is also undefined. \n \n  batch_shape   Shape of a single sample from a single event index as a TensorShape. May be partially defined or unknown. The batch dimensions are indexes into independent, non-identical parameterizations of this distribution. \n \n  concentration0   Concentration parameter associated with a 0 outcome.  \n  concentration1   Concentration parameter associated with a 1 outcome.  \n  dtype   The DType of Tensors handled by this Distribution.  \n  event_shape   Shape of a single sample from a single batch as a TensorShape. May be partially defined or unknown. \n \n  name   Name prepended to all ops created by this Distribution.  \n  parameters   Dictionary of parameters used to instantiate this Distribution.  \n  reparameterization_type   Describes how samples from the distribution are reparameterized. Currently this is one of the static instances distributions.FULLY_REPARAMETERIZED or distributions.NOT_REPARAMETERIZED. \n \n  total_concentration   Sum of concentration parameters.  \n  validate_args   Python bool indicating possibly expensive checks are enabled.    Methods batch_shape_tensor View source \nbatch_shape_tensor(\n    name='batch_shape_tensor'\n)\n Shape of a single sample from a single event index as a 1-D Tensor. The batch dimensions are indexes into independent, non-identical parameterizations of this distribution.\n \n\n\n Args\n  name   name to give to the op   \n \n\n\n Returns\n  batch_shape   Tensor.    cdf View source \ncdf(\n    value, name='cdf'\n)\n Cumulative distribution function. Given random variable X, the cumulative distribution function cdf is: cdf(x) := P[X <= x]\n Additional documentation from Beta: \nNote: x must have dtype self.dtype and be in [0, 1]. It must have a shape compatible with self.batch_shape().\n\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  cdf   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    copy View source \ncopy(\n    **override_parameters_kwargs\n)\n Creates a deep copy of the distribution. \nNote: the copy distribution may continue to depend on the original initialization arguments.\n\n \n\n\n Args\n  **override_parameters_kwargs   String/value dictionary of initialization arguments to override with new values.   \n \n\n\n Returns\n  distribution   A new instance of type(self) initialized from the union of self.parameters and override_parameters_kwargs, i.e., dict(self.parameters, **override_parameters_kwargs).    covariance View source \ncovariance(\n    name='covariance'\n)\n Covariance. Covariance is (possibly) defined only for non-scalar-event distributions. For example, for a length-k, vector-valued distribution, it is calculated as, Cov[i, j] = Covariance(X_i, X_j) = E[(X_i - E[X_i]) (X_j - E[X_j])]\n where Cov is a (batch of) k x k matrix, 0 <= (i, j) < k, and E denotes expectation. Alternatively, for non-vector, multivariate distributions (e.g., matrix-valued, Wishart), Covariance shall return a (batch of) matrices under some vectorization of the events, i.e., Cov[i, j] = Covariance(Vec(X)_i, Vec(X)_j) = [as above]\n where Cov is a (batch of) k' x k' matrices, 0 <= (i, j) < k' = reduce_prod(event_shape), and Vec is some function mapping indices of this distribution's event dimensions to indices of a length-k' vector.\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  covariance   Floating-point Tensor with shape [B1, ..., Bn, k', k'] where the first n dimensions are batch coordinates and k' = reduce_prod(self.event_shape).    cross_entropy View source \ncross_entropy(\n    other, name='cross_entropy'\n)\n Computes the (Shannon) cross entropy. Denote this distribution (self) by P and the other distribution by Q. Assuming P, Q are absolutely continuous with respect to one another and permit densities p(x) dr(x) and q(x) dr(x), (Shanon) cross entropy is defined as: H[P, Q] = E_p[-log q(X)] = -int_F p(x) log q(x) dr(x)\n where F denotes the support of the random variable X ~ P.\n \n\n\n Args\n  other   tfp.distributions.Distribution instance.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  cross_entropy   self.dtype Tensor with shape [B1, ..., Bn] representing n different calculations of (Shanon) cross entropy.    entropy View source \nentropy(\n    name='entropy'\n)\n Shannon entropy in nats. event_shape_tensor View source \nevent_shape_tensor(\n    name='event_shape_tensor'\n)\n Shape of a single sample from a single batch as a 1-D int32 Tensor.\n \n\n\n Args\n  name   name to give to the op   \n \n\n\n Returns\n  event_shape   Tensor.    is_scalar_batch View source \nis_scalar_batch(\n    name='is_scalar_batch'\n)\n Indicates that batch_shape == [].\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  is_scalar_batch   bool scalar Tensor.    is_scalar_event View source \nis_scalar_event(\n    name='is_scalar_event'\n)\n Indicates that event_shape == [].\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  is_scalar_event   bool scalar Tensor.    kl_divergence View source \nkl_divergence(\n    other, name='kl_divergence'\n)\n Computes the Kullback--Leibler divergence. Denote this distribution (self) by p and the other distribution by q. Assuming p, q are absolutely continuous with respect to reference measure r, the KL divergence is defined as: KL[p, q] = E_p[log(p(X)/q(X))]\n         = -int_F p(x) log q(x) dr(x) + int_F p(x) log p(x) dr(x)\n         = H[p, q] - H[p]\n where F denotes the support of the random variable X ~ p, H[., .] denotes (Shanon) cross entropy, and H[.] denotes (Shanon) entropy.\n \n\n\n Args\n  other   tfp.distributions.Distribution instance.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  kl_divergence   self.dtype Tensor with shape [B1, ..., Bn] representing n different calculations of the Kullback-Leibler divergence.    log_cdf View source \nlog_cdf(\n    value, name='log_cdf'\n)\n Log cumulative distribution function. Given random variable X, the cumulative distribution function cdf is: log_cdf(x) := Log[ P[X <= x] ]\n Often, a numerical approximation can be used for log_cdf(x) that yields a more accurate answer than simply taking the logarithm of the cdf when x << -1. Additional documentation from Beta: \nNote: x must have dtype self.dtype and be in [0, 1]. It must have a shape compatible with self.batch_shape().\n\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  logcdf   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    log_prob View source \nlog_prob(\n    value, name='log_prob'\n)\n Log probability density/mass function. Additional documentation from Beta: \nNote: x must have dtype self.dtype and be in [0, 1]. It must have a shape compatible with self.batch_shape().\n\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  log_prob   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    log_survival_function View source \nlog_survival_function(\n    value, name='log_survival_function'\n)\n Log survival function. Given random variable X, the survival function is defined: log_survival_function(x) = Log[ P[X > x] ]\n                         = Log[ 1 - P[X <= x] ]\n                         = Log[ 1 - cdf(x) ]\n Typically, different numerical approximations can be used for the log survival function, which are more accurate than 1 - cdf(x) when x >> 1.\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns   Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.  \n mean View source \nmean(\n    name='mean'\n)\n Mean. mode View source \nmode(\n    name='mode'\n)\n Mode. Additional documentation from Beta: \nNote: The mode is undefined when concentration1 <= 1 or concentration0 <= 1. If self.allow_nan_stats is True, NaN is used for undefined modes. If self.allow_nan_stats is False an exception is raised when one or more modes are undefined.\n param_shapes View source \n@classmethod\nparam_shapes(\n    sample_shape, name='DistributionParamShapes'\n)\n Shapes of parameters given the desired shape of a call to sample(). This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample(). Subclasses should override class method _param_shapes.\n \n\n\n Args\n  sample_shape   Tensor or python list/tuple. Desired shape of a call to sample().  \n  name   name to prepend ops with.   \n \n\n\n Returns   dict of parameter name to Tensor shapes.  \n param_static_shapes View source \n@classmethod\nparam_static_shapes(\n    sample_shape\n)\n param_shapes with static (i.e. TensorShape) shapes. This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample(). Assumes that the sample's shape is known statically. Subclasses should override class method _param_shapes to return constant-valued tensors when constant values are fed.\n \n\n\n Args\n  sample_shape   TensorShape or python list/tuple. Desired shape of a call to sample().   \n \n\n\n Returns   dict of parameter name to TensorShape.  \n\n \n\n\n Raises\n  ValueError   if sample_shape is a TensorShape and is not fully defined.    prob View source \nprob(\n    value, name='prob'\n)\n Probability density/mass function. Additional documentation from Beta: \nNote: x must have dtype self.dtype and be in [0, 1]. It must have a shape compatible with self.batch_shape().\n\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  prob   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    quantile View source \nquantile(\n    value, name='quantile'\n)\n Quantile function. Aka \"inverse cdf\" or \"percent point function\". Given random variable X and p in [0, 1], the quantile is: quantile(p) := x such that P[X <= x] == p\n\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  quantile   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    sample View source \nsample(\n    sample_shape=(), seed=None, name='sample'\n)\n Generate samples of the specified shape. Note that a call to sample() without arguments will generate a single sample.\n \n\n\n Args\n  sample_shape   0D or 1D int32 Tensor. Shape of the generated samples.  \n  seed   Python integer seed for RNG  \n  name   name to give to the op.   \n \n\n\n Returns\n  samples   a Tensor with prepended dimensions sample_shape.    stddev View source \nstddev(\n    name='stddev'\n)\n Standard deviation. Standard deviation is defined as, stddev = E[(X - E[X])**2]**0.5\n where X is the random variable associated with this distribution, E denotes expectation, and stddev.shape = batch_shape + event_shape.\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  stddev   Floating-point Tensor with shape identical to batch_shape + event_shape, i.e., the same shape as self.mean().    survival_function View source \nsurvival_function(\n    value, name='survival_function'\n)\n Survival function. Given random variable X, the survival function is defined: survival_function(x) = P[X > x]\n                     = 1 - P[X <= x]\n                     = 1 - cdf(x).\n\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns   Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.  \n variance View source \nvariance(\n    name='variance'\n)\n Variance. Variance is defined as, Var = E[(X - E[X])**2]\n where X is the random variable associated with this distribution, E denotes expectation, and Var.shape = batch_shape + event_shape.\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  variance   Floating-point Tensor with shape identical to batch_shape + event_shape, i.e., the same shape as self.mean().     \n"}, {"name": "tf.compat.v1.distributions.Categorical", "path": "compat/v1/distributions/categorical", "type": "tf.compat", "text": "tf.compat.v1.distributions.Categorical Categorical distribution. Inherits From: Distribution \ntf.compat.v1.distributions.Categorical(\n    logits=None, probs=None, dtype=tf.dtypes.int32, validate_args=False,\n    allow_nan_stats=True, name='Categorical'\n)\n The Categorical distribution is parameterized by either probabilities or log-probabilities of a set of K classes. It is defined over the integers {0, 1, ..., K}. The Categorical distribution is closely related to the OneHotCategorical and Multinomial distributions. The Categorical distribution can be intuited as generating samples according to argmax{ OneHotCategorical(probs) } itself being identical to argmax{ Multinomial(probs, total_count=1) }. Mathematical Details The probability mass function (pmf) is, pmf(k; pi) = prod_j pi_j**[k == j]\n Pitfalls The number of classes, K, must not exceed:  the largest integer representable by self.dtype, i.e., 2**(mantissa_bits+1) (IEEE 754), the maximum Tensor index, i.e., 2**31-1.  In other words, K <= min(2**31-1, {\n  tf.float16: 2**11,\n  tf.float32: 2**24,\n  tf.float64: 2**53 }[param.dtype])\n\n\nNote: This condition is validated only when self.validate_args = True.\n Examples Creates a 3-class distribution with the 2nd class being most likely. dist = Categorical(probs=[0.1, 0.5, 0.4])\nn = 1e4\nempirical_prob = tf.cast(\n    tf.histogram_fixed_width(\n      dist.sample(int(n)),\n      [0., 2],\n      nbins=3),\n    dtype=tf.float32) / n\n# ==> array([ 0.1005,  0.5037,  0.3958], dtype=float32)\n Creates a 3-class distribution with the 2nd class being most likely. Parameterized by logits rather than probabilities. dist = Categorical(logits=np.log([0.1, 0.5, 0.4])\nn = 1e4\nempirical_prob = tf.cast(\n    tf.histogram_fixed_width(\n      dist.sample(int(n)),\n      [0., 2],\n      nbins=3),\n    dtype=tf.float32) / n\n# ==> array([0.1045,  0.5047, 0.3908], dtype=float32)\n Creates a 3-class distribution with the 3rd class being most likely. The distribution functions can be evaluated on counts. # counts is a scalar.\np = [0.1, 0.4, 0.5]\ndist = Categorical(probs=p)\ndist.prob(0)  # Shape []\n\n# p will be broadcast to [[0.1, 0.4, 0.5], [0.1, 0.4, 0.5]] to match counts.\ncounts = [1, 0]\ndist.prob(counts)  # Shape [2]\n\n# p will be broadcast to shape [3, 5, 7, 3] to match counts.\ncounts = [[...]] # Shape [5, 7, 3]\ndist.prob(counts)  # Shape [5, 7, 3]\n\n \n\n\n Args\n  logits   An N-D Tensor, N >= 1, representing the log probabilities of a set of Categorical distributions. The first N - 1 dimensions index into a batch of independent distributions and the last dimension represents a vector of logits for each class. Only one of logits or probs should be passed in.  \n  probs   An N-D Tensor, N >= 1, representing the probabilities of a set of Categorical distributions. The first N - 1 dimensions index into a batch of independent distributions and the last dimension represents a vector of probabilities for each class. Only one of logits or probs should be passed in.  \n  dtype   The type of the event samples (default: int32).  \n  validate_args   Python bool, default False. When True distribution parameters are checked for validity despite possibly degrading runtime performance. When False invalid inputs may silently render incorrect outputs.  \n  allow_nan_stats   Python bool, default True. When True, statistics (e.g., mean, mode, variance) use the value \"NaN\" to indicate the result is undefined. When False, an exception is raised if one or more of the statistic's batch members are undefined.  \n  name   Python str name prefixed to Ops created by this class.   \n \n\n\n Attributes\n  allow_nan_stats   Python bool describing behavior when a stat is undefined. Stats return +/- infinity when it makes sense. E.g., the variance of a Cauchy distribution is infinity. However, sometimes the statistic is undefined, e.g., if a distribution's pdf does not achieve a maximum within the support of the distribution, the mode is undefined. If the mean is undefined, then by definition the variance is undefined. E.g. the mean for Student's T for df = 1 is undefined (no clear way to say it is either + or - infinity), so the variance = E[(X - mean)**2] is also undefined. \n \n  batch_shape   Shape of a single sample from a single event index as a TensorShape. May be partially defined or unknown. The batch dimensions are indexes into independent, non-identical parameterizations of this distribution. \n \n  dtype   The DType of Tensors handled by this Distribution.  \n  event_shape   Shape of a single sample from a single batch as a TensorShape. May be partially defined or unknown. \n \n  event_size   Scalar int32 tensor: the number of classes.  \n  logits   Vector of coordinatewise logits.  \n  name   Name prepended to all ops created by this Distribution.  \n  parameters   Dictionary of parameters used to instantiate this Distribution.  \n  probs   Vector of coordinatewise probabilities.  \n  reparameterization_type   Describes how samples from the distribution are reparameterized. Currently this is one of the static instances distributions.FULLY_REPARAMETERIZED or distributions.NOT_REPARAMETERIZED. \n \n  validate_args   Python bool indicating possibly expensive checks are enabled.    Methods batch_shape_tensor View source \nbatch_shape_tensor(\n    name='batch_shape_tensor'\n)\n Shape of a single sample from a single event index as a 1-D Tensor. The batch dimensions are indexes into independent, non-identical parameterizations of this distribution.\n \n\n\n Args\n  name   name to give to the op   \n \n\n\n Returns\n  batch_shape   Tensor.    cdf View source \ncdf(\n    value, name='cdf'\n)\n Cumulative distribution function. Given random variable X, the cumulative distribution function cdf is: cdf(x) := P[X <= x]\n\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  cdf   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    copy View source \ncopy(\n    **override_parameters_kwargs\n)\n Creates a deep copy of the distribution. \nNote: the copy distribution may continue to depend on the original initialization arguments.\n\n \n\n\n Args\n  **override_parameters_kwargs   String/value dictionary of initialization arguments to override with new values.   \n \n\n\n Returns\n  distribution   A new instance of type(self) initialized from the union of self.parameters and override_parameters_kwargs, i.e., dict(self.parameters, **override_parameters_kwargs).    covariance View source \ncovariance(\n    name='covariance'\n)\n Covariance. Covariance is (possibly) defined only for non-scalar-event distributions. For example, for a length-k, vector-valued distribution, it is calculated as, Cov[i, j] = Covariance(X_i, X_j) = E[(X_i - E[X_i]) (X_j - E[X_j])]\n where Cov is a (batch of) k x k matrix, 0 <= (i, j) < k, and E denotes expectation. Alternatively, for non-vector, multivariate distributions (e.g., matrix-valued, Wishart), Covariance shall return a (batch of) matrices under some vectorization of the events, i.e., Cov[i, j] = Covariance(Vec(X)_i, Vec(X)_j) = [as above]\n where Cov is a (batch of) k' x k' matrices, 0 <= (i, j) < k' = reduce_prod(event_shape), and Vec is some function mapping indices of this distribution's event dimensions to indices of a length-k' vector.\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  covariance   Floating-point Tensor with shape [B1, ..., Bn, k', k'] where the first n dimensions are batch coordinates and k' = reduce_prod(self.event_shape).    cross_entropy View source \ncross_entropy(\n    other, name='cross_entropy'\n)\n Computes the (Shannon) cross entropy. Denote this distribution (self) by P and the other distribution by Q. Assuming P, Q are absolutely continuous with respect to one another and permit densities p(x) dr(x) and q(x) dr(x), (Shanon) cross entropy is defined as: H[P, Q] = E_p[-log q(X)] = -int_F p(x) log q(x) dr(x)\n where F denotes the support of the random variable X ~ P.\n \n\n\n Args\n  other   tfp.distributions.Distribution instance.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  cross_entropy   self.dtype Tensor with shape [B1, ..., Bn] representing n different calculations of (Shanon) cross entropy.    entropy View source \nentropy(\n    name='entropy'\n)\n Shannon entropy in nats. event_shape_tensor View source \nevent_shape_tensor(\n    name='event_shape_tensor'\n)\n Shape of a single sample from a single batch as a 1-D int32 Tensor.\n \n\n\n Args\n  name   name to give to the op   \n \n\n\n Returns\n  event_shape   Tensor.    is_scalar_batch View source \nis_scalar_batch(\n    name='is_scalar_batch'\n)\n Indicates that batch_shape == [].\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  is_scalar_batch   bool scalar Tensor.    is_scalar_event View source \nis_scalar_event(\n    name='is_scalar_event'\n)\n Indicates that event_shape == [].\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  is_scalar_event   bool scalar Tensor.    kl_divergence View source \nkl_divergence(\n    other, name='kl_divergence'\n)\n Computes the Kullback--Leibler divergence. Denote this distribution (self) by p and the other distribution by q. Assuming p, q are absolutely continuous with respect to reference measure r, the KL divergence is defined as: KL[p, q] = E_p[log(p(X)/q(X))]\n         = -int_F p(x) log q(x) dr(x) + int_F p(x) log p(x) dr(x)\n         = H[p, q] - H[p]\n where F denotes the support of the random variable X ~ p, H[., .] denotes (Shanon) cross entropy, and H[.] denotes (Shanon) entropy.\n \n\n\n Args\n  other   tfp.distributions.Distribution instance.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  kl_divergence   self.dtype Tensor with shape [B1, ..., Bn] representing n different calculations of the Kullback-Leibler divergence.    log_cdf View source \nlog_cdf(\n    value, name='log_cdf'\n)\n Log cumulative distribution function. Given random variable X, the cumulative distribution function cdf is: log_cdf(x) := Log[ P[X <= x] ]\n Often, a numerical approximation can be used for log_cdf(x) that yields a more accurate answer than simply taking the logarithm of the cdf when x << -1.\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  logcdf   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    log_prob View source \nlog_prob(\n    value, name='log_prob'\n)\n Log probability density/mass function.\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  log_prob   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    log_survival_function View source \nlog_survival_function(\n    value, name='log_survival_function'\n)\n Log survival function. Given random variable X, the survival function is defined: log_survival_function(x) = Log[ P[X > x] ]\n                         = Log[ 1 - P[X <= x] ]\n                         = Log[ 1 - cdf(x) ]\n Typically, different numerical approximations can be used for the log survival function, which are more accurate than 1 - cdf(x) when x >> 1.\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns   Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.  \n mean View source \nmean(\n    name='mean'\n)\n Mean. mode View source \nmode(\n    name='mode'\n)\n Mode. param_shapes View source \n@classmethod\nparam_shapes(\n    sample_shape, name='DistributionParamShapes'\n)\n Shapes of parameters given the desired shape of a call to sample(). This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample(). Subclasses should override class method _param_shapes.\n \n\n\n Args\n  sample_shape   Tensor or python list/tuple. Desired shape of a call to sample().  \n  name   name to prepend ops with.   \n \n\n\n Returns   dict of parameter name to Tensor shapes.  \n param_static_shapes View source \n@classmethod\nparam_static_shapes(\n    sample_shape\n)\n param_shapes with static (i.e. TensorShape) shapes. This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample(). Assumes that the sample's shape is known statically. Subclasses should override class method _param_shapes to return constant-valued tensors when constant values are fed.\n \n\n\n Args\n  sample_shape   TensorShape or python list/tuple. Desired shape of a call to sample().   \n \n\n\n Returns   dict of parameter name to TensorShape.  \n\n \n\n\n Raises\n  ValueError   if sample_shape is a TensorShape and is not fully defined.    prob View source \nprob(\n    value, name='prob'\n)\n Probability density/mass function.\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  prob   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    quantile View source \nquantile(\n    value, name='quantile'\n)\n Quantile function. Aka \"inverse cdf\" or \"percent point function\". Given random variable X and p in [0, 1], the quantile is: quantile(p) := x such that P[X <= x] == p\n\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  quantile   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    sample View source \nsample(\n    sample_shape=(), seed=None, name='sample'\n)\n Generate samples of the specified shape. Note that a call to sample() without arguments will generate a single sample.\n \n\n\n Args\n  sample_shape   0D or 1D int32 Tensor. Shape of the generated samples.  \n  seed   Python integer seed for RNG  \n  name   name to give to the op.   \n \n\n\n Returns\n  samples   a Tensor with prepended dimensions sample_shape.    stddev View source \nstddev(\n    name='stddev'\n)\n Standard deviation. Standard deviation is defined as, stddev = E[(X - E[X])**2]**0.5\n where X is the random variable associated with this distribution, E denotes expectation, and stddev.shape = batch_shape + event_shape.\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  stddev   Floating-point Tensor with shape identical to batch_shape + event_shape, i.e., the same shape as self.mean().    survival_function View source \nsurvival_function(\n    value, name='survival_function'\n)\n Survival function. Given random variable X, the survival function is defined: survival_function(x) = P[X > x]\n                     = 1 - P[X <= x]\n                     = 1 - cdf(x).\n\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns   Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.  \n variance View source \nvariance(\n    name='variance'\n)\n Variance. Variance is defined as, Var = E[(X - E[X])**2]\n where X is the random variable associated with this distribution, E denotes expectation, and Var.shape = batch_shape + event_shape.\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  variance   Floating-point Tensor with shape identical to batch_shape + event_shape, i.e., the same shape as self.mean().     \n"}, {"name": "tf.compat.v1.distributions.Dirichlet", "path": "compat/v1/distributions/dirichlet", "type": "tf.compat", "text": "tf.compat.v1.distributions.Dirichlet Dirichlet distribution. Inherits From: Distribution \ntf.compat.v1.distributions.Dirichlet(\n    concentration, validate_args=False, allow_nan_stats=True,\n    name='Dirichlet'\n)\n The Dirichlet distribution is defined over the (k-1)-simplex using a positive, length-k vector concentration (k > 1). The Dirichlet is identically the Beta distribution when k = 2. Mathematical Details The Dirichlet is a distribution over the open (k-1)-simplex, i.e., S^{k-1} = { (x_0, ..., x_{k-1}) in R^k : sum_j x_j = 1 and all_j x_j > 0 }.\n The probability density function (pdf) is, pdf(x; alpha) = prod_j x_j**(alpha_j - 1) / Z\nZ = prod_j Gamma(alpha_j) / Gamma(sum_j alpha_j)\n where:  \nx in S^{k-1}, i.e., the (k-1)-simplex, \nconcentration = alpha = [alpha_0, ..., alpha_{k-1}], alpha_j > 0, \nZ is the normalization constant aka the multivariate beta function, and, \nGamma is the gamma function.  The concentration represents mean total counts of class occurrence, i.e., concentration = alpha = mean * total_concentration\n where mean in S^{k-1} and total_concentration is a positive real number representing a mean total count. Distribution parameters are automatically broadcast in all functions; see examples for details. Warning: Some components of the samples can be zero due to finite precision. This happens more often when some of the concentrations are very small. Make sure to round the samples to np.finfo(dtype).tiny before computing the density. Samples of this distribution are reparameterized (pathwise differentiable). The derivatives are computed using the approach described in (Figurnov et al., 2018). Examples import tensorflow_probability as tfp\ntfd = tfp.distributions\n\n# Create a single trivariate Dirichlet, with the 3rd class being three times\n# more frequent than the first. I.e., batch_shape=[], event_shape=[3].\nalpha = [1., 2, 3]\ndist = tfd.Dirichlet(alpha)\n\ndist.sample([4, 5])  # shape: [4, 5, 3]\n\n# x has one sample, one batch, three classes:\nx = [.2, .3, .5]   # shape: [3]\ndist.prob(x)       # shape: []\n\n# x has two samples from one batch:\nx = [[.1, .4, .5],\n     [.2, .3, .5]]\ndist.prob(x)         # shape: [2]\n\n# alpha will be broadcast to shape [5, 7, 3] to match x.\nx = [[...]]   # shape: [5, 7, 3]\ndist.prob(x)  # shape: [5, 7]\n\n# Create batch_shape=[2], event_shape=[3]:\nalpha = [[1., 2, 3],\n         [4, 5, 6]]   # shape: [2, 3]\ndist = tfd.Dirichlet(alpha)\n\ndist.sample([4, 5])  # shape: [4, 5, 2, 3]\n\nx = [.2, .3, .5]\n# x will be broadcast as [[.2, .3, .5],\n#                         [.2, .3, .5]],\n# thus matching batch_shape [2, 3].\ndist.prob(x)         # shape: [2]\n Compute the gradients of samples w.r.t. the parameters: alpha = tf.constant([1.0, 2.0, 3.0])\ndist = tfd.Dirichlet(alpha)\nsamples = dist.sample(5)  # Shape [5, 3]\nloss = tf.reduce_mean(tf.square(samples))  # Arbitrary loss function\n# Unbiased stochastic gradients of the loss function\ngrads = tf.gradients(loss, alpha)\n References: Implicit Reparameterization Gradients: Figurnov et al., 2018 (pdf)\n \n\n\n Args\n  concentration   Positive floating-point Tensor indicating mean number of class occurrences; aka \"alpha\". Implies self.dtype, and self.batch_shape, self.event_shape, i.e., if concentration.shape = [N1, N2, ..., Nm, k] then batch_shape = [N1, N2, ..., Nm] and event_shape = [k].  \n  validate_args   Python bool, default False. When True distribution parameters are checked for validity despite possibly degrading runtime performance. When False invalid inputs may silently render incorrect outputs.  \n  allow_nan_stats   Python bool, default True. When True, statistics (e.g., mean, mode, variance) use the value \"NaN\" to indicate the result is undefined. When False, an exception is raised if one or more of the statistic's batch members are undefined.  \n  name   Python str name prefixed to Ops created by this class.   \n \n\n\n Attributes\n  allow_nan_stats   Python bool describing behavior when a stat is undefined. Stats return +/- infinity when it makes sense. E.g., the variance of a Cauchy distribution is infinity. However, sometimes the statistic is undefined, e.g., if a distribution's pdf does not achieve a maximum within the support of the distribution, the mode is undefined. If the mean is undefined, then by definition the variance is undefined. E.g. the mean for Student's T for df = 1 is undefined (no clear way to say it is either + or - infinity), so the variance = E[(X - mean)**2] is also undefined. \n \n  batch_shape   Shape of a single sample from a single event index as a TensorShape. May be partially defined or unknown. The batch dimensions are indexes into independent, non-identical parameterizations of this distribution. \n \n  concentration   Concentration parameter; expected counts for that coordinate.  \n  dtype   The DType of Tensors handled by this Distribution.  \n  event_shape   Shape of a single sample from a single batch as a TensorShape. May be partially defined or unknown. \n \n  name   Name prepended to all ops created by this Distribution.  \n  parameters   Dictionary of parameters used to instantiate this Distribution.  \n  reparameterization_type   Describes how samples from the distribution are reparameterized. Currently this is one of the static instances distributions.FULLY_REPARAMETERIZED or distributions.NOT_REPARAMETERIZED. \n \n  total_concentration   Sum of last dim of concentration parameter.  \n  validate_args   Python bool indicating possibly expensive checks are enabled.    Methods batch_shape_tensor View source \nbatch_shape_tensor(\n    name='batch_shape_tensor'\n)\n Shape of a single sample from a single event index as a 1-D Tensor. The batch dimensions are indexes into independent, non-identical parameterizations of this distribution.\n \n\n\n Args\n  name   name to give to the op   \n \n\n\n Returns\n  batch_shape   Tensor.    cdf View source \ncdf(\n    value, name='cdf'\n)\n Cumulative distribution function. Given random variable X, the cumulative distribution function cdf is: cdf(x) := P[X <= x]\n\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  cdf   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    copy View source \ncopy(\n    **override_parameters_kwargs\n)\n Creates a deep copy of the distribution. \nNote: the copy distribution may continue to depend on the original initialization arguments.\n\n \n\n\n Args\n  **override_parameters_kwargs   String/value dictionary of initialization arguments to override with new values.   \n \n\n\n Returns\n  distribution   A new instance of type(self) initialized from the union of self.parameters and override_parameters_kwargs, i.e., dict(self.parameters, **override_parameters_kwargs).    covariance View source \ncovariance(\n    name='covariance'\n)\n Covariance. Covariance is (possibly) defined only for non-scalar-event distributions. For example, for a length-k, vector-valued distribution, it is calculated as, Cov[i, j] = Covariance(X_i, X_j) = E[(X_i - E[X_i]) (X_j - E[X_j])]\n where Cov is a (batch of) k x k matrix, 0 <= (i, j) < k, and E denotes expectation. Alternatively, for non-vector, multivariate distributions (e.g., matrix-valued, Wishart), Covariance shall return a (batch of) matrices under some vectorization of the events, i.e., Cov[i, j] = Covariance(Vec(X)_i, Vec(X)_j) = [as above]\n where Cov is a (batch of) k' x k' matrices, 0 <= (i, j) < k' = reduce_prod(event_shape), and Vec is some function mapping indices of this distribution's event dimensions to indices of a length-k' vector.\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  covariance   Floating-point Tensor with shape [B1, ..., Bn, k', k'] where the first n dimensions are batch coordinates and k' = reduce_prod(self.event_shape).    cross_entropy View source \ncross_entropy(\n    other, name='cross_entropy'\n)\n Computes the (Shannon) cross entropy. Denote this distribution (self) by P and the other distribution by Q. Assuming P, Q are absolutely continuous with respect to one another and permit densities p(x) dr(x) and q(x) dr(x), (Shanon) cross entropy is defined as: H[P, Q] = E_p[-log q(X)] = -int_F p(x) log q(x) dr(x)\n where F denotes the support of the random variable X ~ P.\n \n\n\n Args\n  other   tfp.distributions.Distribution instance.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  cross_entropy   self.dtype Tensor with shape [B1, ..., Bn] representing n different calculations of (Shanon) cross entropy.    entropy View source \nentropy(\n    name='entropy'\n)\n Shannon entropy in nats. event_shape_tensor View source \nevent_shape_tensor(\n    name='event_shape_tensor'\n)\n Shape of a single sample from a single batch as a 1-D int32 Tensor.\n \n\n\n Args\n  name   name to give to the op   \n \n\n\n Returns\n  event_shape   Tensor.    is_scalar_batch View source \nis_scalar_batch(\n    name='is_scalar_batch'\n)\n Indicates that batch_shape == [].\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  is_scalar_batch   bool scalar Tensor.    is_scalar_event View source \nis_scalar_event(\n    name='is_scalar_event'\n)\n Indicates that event_shape == [].\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  is_scalar_event   bool scalar Tensor.    kl_divergence View source \nkl_divergence(\n    other, name='kl_divergence'\n)\n Computes the Kullback--Leibler divergence. Denote this distribution (self) by p and the other distribution by q. Assuming p, q are absolutely continuous with respect to reference measure r, the KL divergence is defined as: KL[p, q] = E_p[log(p(X)/q(X))]\n         = -int_F p(x) log q(x) dr(x) + int_F p(x) log p(x) dr(x)\n         = H[p, q] - H[p]\n where F denotes the support of the random variable X ~ p, H[., .] denotes (Shanon) cross entropy, and H[.] denotes (Shanon) entropy.\n \n\n\n Args\n  other   tfp.distributions.Distribution instance.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  kl_divergence   self.dtype Tensor with shape [B1, ..., Bn] representing n different calculations of the Kullback-Leibler divergence.    log_cdf View source \nlog_cdf(\n    value, name='log_cdf'\n)\n Log cumulative distribution function. Given random variable X, the cumulative distribution function cdf is: log_cdf(x) := Log[ P[X <= x] ]\n Often, a numerical approximation can be used for log_cdf(x) that yields a more accurate answer than simply taking the logarithm of the cdf when x << -1.\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  logcdf   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    log_prob View source \nlog_prob(\n    value, name='log_prob'\n)\n Log probability density/mass function. Additional documentation from Dirichlet: \nNote: value must be a non-negative tensor with dtype self.dtype and be in the (self.event_shape() - 1)-simplex, i.e., tf.reduce_sum(value, -1) = 1. It must have a shape compatible with self.batch_shape() + self.event_shape().\n\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  log_prob   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    log_survival_function View source \nlog_survival_function(\n    value, name='log_survival_function'\n)\n Log survival function. Given random variable X, the survival function is defined: log_survival_function(x) = Log[ P[X > x] ]\n                         = Log[ 1 - P[X <= x] ]\n                         = Log[ 1 - cdf(x) ]\n Typically, different numerical approximations can be used for the log survival function, which are more accurate than 1 - cdf(x) when x >> 1.\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns   Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.  \n mean View source \nmean(\n    name='mean'\n)\n Mean. mode View source \nmode(\n    name='mode'\n)\n Mode. Additional documentation from Dirichlet: \nNote: The mode is undefined when any concentration <= 1. If self.allow_nan_stats is True, NaN is used for undefined modes. If self.allow_nan_stats is False an exception is raised when one or more modes are undefined.\n param_shapes View source \n@classmethod\nparam_shapes(\n    sample_shape, name='DistributionParamShapes'\n)\n Shapes of parameters given the desired shape of a call to sample(). This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample(). Subclasses should override class method _param_shapes.\n \n\n\n Args\n  sample_shape   Tensor or python list/tuple. Desired shape of a call to sample().  \n  name   name to prepend ops with.   \n \n\n\n Returns   dict of parameter name to Tensor shapes.  \n param_static_shapes View source \n@classmethod\nparam_static_shapes(\n    sample_shape\n)\n param_shapes with static (i.e. TensorShape) shapes. This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample(). Assumes that the sample's shape is known statically. Subclasses should override class method _param_shapes to return constant-valued tensors when constant values are fed.\n \n\n\n Args\n  sample_shape   TensorShape or python list/tuple. Desired shape of a call to sample().   \n \n\n\n Returns   dict of parameter name to TensorShape.  \n\n \n\n\n Raises\n  ValueError   if sample_shape is a TensorShape and is not fully defined.    prob View source \nprob(\n    value, name='prob'\n)\n Probability density/mass function. Additional documentation from Dirichlet: \nNote: value must be a non-negative tensor with dtype self.dtype and be in the (self.event_shape() - 1)-simplex, i.e., tf.reduce_sum(value, -1) = 1. It must have a shape compatible with self.batch_shape() + self.event_shape().\n\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  prob   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    quantile View source \nquantile(\n    value, name='quantile'\n)\n Quantile function. Aka \"inverse cdf\" or \"percent point function\". Given random variable X and p in [0, 1], the quantile is: quantile(p) := x such that P[X <= x] == p\n\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  quantile   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    sample View source \nsample(\n    sample_shape=(), seed=None, name='sample'\n)\n Generate samples of the specified shape. Note that a call to sample() without arguments will generate a single sample.\n \n\n\n Args\n  sample_shape   0D or 1D int32 Tensor. Shape of the generated samples.  \n  seed   Python integer seed for RNG  \n  name   name to give to the op.   \n \n\n\n Returns\n  samples   a Tensor with prepended dimensions sample_shape.    stddev View source \nstddev(\n    name='stddev'\n)\n Standard deviation. Standard deviation is defined as, stddev = E[(X - E[X])**2]**0.5\n where X is the random variable associated with this distribution, E denotes expectation, and stddev.shape = batch_shape + event_shape.\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  stddev   Floating-point Tensor with shape identical to batch_shape + event_shape, i.e., the same shape as self.mean().    survival_function View source \nsurvival_function(\n    value, name='survival_function'\n)\n Survival function. Given random variable X, the survival function is defined: survival_function(x) = P[X > x]\n                     = 1 - P[X <= x]\n                     = 1 - cdf(x).\n\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns   Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.  \n variance View source \nvariance(\n    name='variance'\n)\n Variance. Variance is defined as, Var = E[(X - E[X])**2]\n where X is the random variable associated with this distribution, E denotes expectation, and Var.shape = batch_shape + event_shape.\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  variance   Floating-point Tensor with shape identical to batch_shape + event_shape, i.e., the same shape as self.mean().     \n"}, {"name": "tf.compat.v1.distributions.DirichletMultinomial", "path": "compat/v1/distributions/dirichletmultinomial", "type": "tf.compat", "text": "tf.compat.v1.distributions.DirichletMultinomial Dirichlet-Multinomial compound distribution. Inherits From: Distribution \ntf.compat.v1.distributions.DirichletMultinomial(\n    total_count, concentration, validate_args=False, allow_nan_stats=True,\n    name='DirichletMultinomial'\n)\n The Dirichlet-Multinomial distribution is parameterized by a (batch of) length-K concentration vectors (K > 1) and a total_count number of trials, i.e., the number of trials per draw from the DirichletMultinomial. It is defined over a (batch of) length-K vector counts such that tf.reduce_sum(counts, -1) = total_count. The Dirichlet-Multinomial is identically the Beta-Binomial distribution when K = 2. Mathematical Details The Dirichlet-Multinomial is a distribution over K-class counts, i.e., a length-K vector of non-negative integer counts = n = [n_0, ..., n_{K-1}]. The probability mass function (pmf) is, pmf(n; alpha, N) = Beta(alpha + n) / (prod_j n_j!) / Z\nZ = Beta(alpha) / N!\n where:  \nconcentration = alpha = [alpha_0, ..., alpha_{K-1}], alpha_j > 0, \ntotal_count = N, N a positive integer, \nN! is N factorial, and, \nBeta(x) = prod_j Gamma(x_j) / Gamma(sum_j x_j) is the multivariate beta function, and, \nGamma is the gamma function.  Dirichlet-Multinomial is a compound distribution, i.e., its samples are generated as follows.  Choose class probabilities: probs = [p_0,...,p_{K-1}] ~ Dir(concentration)\n Draw integers: counts = [n_0,...,n_{K-1}] ~ Multinomial(total_count, probs)\n  The last concentration dimension parametrizes a single Dirichlet-Multinomial distribution. When calling distribution functions (e.g., dist.prob(counts)), concentration, total_count and counts are broadcast to the same shape. The last dimension of counts corresponds single Dirichlet-Multinomial distributions. Distribution parameters are automatically broadcast in all functions; see examples for details. Pitfalls The number of classes, K, must not exceed:  the largest integer representable by self.dtype, i.e., 2**(mantissa_bits+1) (IEE754), the maximum Tensor index, i.e., 2**31-1.  In other words, K <= min(2**31-1, {\n  tf.float16: 2**11,\n  tf.float32: 2**24,\n  tf.float64: 2**53 }[param.dtype])\n\n\nNote: This condition is validated only when self.validate_args = True.\n Examples alpha = [1., 2., 3.]\nn = 2.\ndist = DirichletMultinomial(n, alpha)\n Creates a 3-class distribution, with the 3rd class is most likely to be drawn. The distribution functions can be evaluated on counts. # counts same shape as alpha.\ncounts = [0., 0., 2.]\ndist.prob(counts)  # Shape []\n\n# alpha will be broadcast to [[1., 2., 3.], [1., 2., 3.]] to match counts.\ncounts = [[1., 1., 0.], [1., 0., 1.]]\ndist.prob(counts)  # Shape [2]\n\n# alpha will be broadcast to shape [5, 7, 3] to match counts.\ncounts = [[...]]  # Shape [5, 7, 3]\ndist.prob(counts)  # Shape [5, 7]\n Creates a 2-batch of 3-class distributions. alpha = [[1., 2., 3.], [4., 5., 6.]]  # Shape [2, 3]\nn = [3., 3.]\ndist = DirichletMultinomial(n, alpha)\n\n# counts will be broadcast to [[2., 1., 0.], [2., 1., 0.]] to match alpha.\ncounts = [2., 1., 0.]\ndist.prob(counts)  # Shape [2]\n\n \n\n\n Args\n  total_count   Non-negative floating point tensor, whose dtype is the same as concentration. The shape is broadcastable to [N1,..., Nm] with m >= 0. Defines this as a batch of N1 x ... x Nm different Dirichlet multinomial distributions. Its components should be equal to integer values.  \n  concentration   Positive floating point tensor, whose dtype is the same as n with shape broadcastable to [N1,..., Nm, K] m >= 0. Defines this as a batch of N1 x ... x Nm different K class Dirichlet multinomial distributions.  \n  validate_args   Python bool, default False. When True distribution parameters are checked for validity despite possibly degrading runtime performance. When False invalid inputs may silently render incorrect outputs.  \n  allow_nan_stats   Python bool, default True. When True, statistics (e.g., mean, mode, variance) use the value \"NaN\" to indicate the result is undefined. When False, an exception is raised if one or more of the statistic's batch members are undefined.  \n  name   Python str name prefixed to Ops created by this class.   \n \n\n\n Attributes\n  allow_nan_stats   Python bool describing behavior when a stat is undefined. Stats return +/- infinity when it makes sense. E.g., the variance of a Cauchy distribution is infinity. However, sometimes the statistic is undefined, e.g., if a distribution's pdf does not achieve a maximum within the support of the distribution, the mode is undefined. If the mean is undefined, then by definition the variance is undefined. E.g. the mean for Student's T for df = 1 is undefined (no clear way to say it is either + or - infinity), so the variance = E[(X - mean)**2] is also undefined. \n \n  batch_shape   Shape of a single sample from a single event index as a TensorShape. May be partially defined or unknown. The batch dimensions are indexes into independent, non-identical parameterizations of this distribution. \n \n  concentration   Concentration parameter; expected prior counts for that coordinate.  \n  dtype   The DType of Tensors handled by this Distribution.  \n  event_shape   Shape of a single sample from a single batch as a TensorShape. May be partially defined or unknown. \n \n  name   Name prepended to all ops created by this Distribution.  \n  parameters   Dictionary of parameters used to instantiate this Distribution.  \n  reparameterization_type   Describes how samples from the distribution are reparameterized. Currently this is one of the static instances distributions.FULLY_REPARAMETERIZED or distributions.NOT_REPARAMETERIZED. \n \n  total_concentration   Sum of last dim of concentration parameter.  \n  total_count   Number of trials used to construct a sample.  \n  validate_args   Python bool indicating possibly expensive checks are enabled.    Methods batch_shape_tensor View source \nbatch_shape_tensor(\n    name='batch_shape_tensor'\n)\n Shape of a single sample from a single event index as a 1-D Tensor. The batch dimensions are indexes into independent, non-identical parameterizations of this distribution.\n \n\n\n Args\n  name   name to give to the op   \n \n\n\n Returns\n  batch_shape   Tensor.    cdf View source \ncdf(\n    value, name='cdf'\n)\n Cumulative distribution function. Given random variable X, the cumulative distribution function cdf is: cdf(x) := P[X <= x]\n\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  cdf   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    copy View source \ncopy(\n    **override_parameters_kwargs\n)\n Creates a deep copy of the distribution. \nNote: the copy distribution may continue to depend on the original initialization arguments.\n\n \n\n\n Args\n  **override_parameters_kwargs   String/value dictionary of initialization arguments to override with new values.   \n \n\n\n Returns\n  distribution   A new instance of type(self) initialized from the union of self.parameters and override_parameters_kwargs, i.e., dict(self.parameters, **override_parameters_kwargs).    covariance View source \ncovariance(\n    name='covariance'\n)\n Covariance. Covariance is (possibly) defined only for non-scalar-event distributions. For example, for a length-k, vector-valued distribution, it is calculated as, Cov[i, j] = Covariance(X_i, X_j) = E[(X_i - E[X_i]) (X_j - E[X_j])]\n where Cov is a (batch of) k x k matrix, 0 <= (i, j) < k, and E denotes expectation. Alternatively, for non-vector, multivariate distributions (e.g., matrix-valued, Wishart), Covariance shall return a (batch of) matrices under some vectorization of the events, i.e., Cov[i, j] = Covariance(Vec(X)_i, Vec(X)_j) = [as above]\n where Cov is a (batch of) k' x k' matrices, 0 <= (i, j) < k' = reduce_prod(event_shape), and Vec is some function mapping indices of this distribution's event dimensions to indices of a length-k' vector. Additional documentation from DirichletMultinomial: The covariance for each batch member is defined as the following: Var(X_j) = n * alpha_j / alpha_0 * (1 - alpha_j / alpha_0) *\n(n + alpha_0) / (1 + alpha_0)\n where concentration = alpha and total_concentration = alpha_0 = sum_j alpha_j. The covariance between elements in a batch is defined as: Cov(X_i, X_j) = -n * alpha_i * alpha_j / alpha_0 ** 2 *\n(n + alpha_0) / (1 + alpha_0)\n\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  covariance   Floating-point Tensor with shape [B1, ..., Bn, k', k'] where the first n dimensions are batch coordinates and k' = reduce_prod(self.event_shape).    cross_entropy View source \ncross_entropy(\n    other, name='cross_entropy'\n)\n Computes the (Shannon) cross entropy. Denote this distribution (self) by P and the other distribution by Q. Assuming P, Q are absolutely continuous with respect to one another and permit densities p(x) dr(x) and q(x) dr(x), (Shanon) cross entropy is defined as: H[P, Q] = E_p[-log q(X)] = -int_F p(x) log q(x) dr(x)\n where F denotes the support of the random variable X ~ P.\n \n\n\n Args\n  other   tfp.distributions.Distribution instance.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  cross_entropy   self.dtype Tensor with shape [B1, ..., Bn] representing n different calculations of (Shanon) cross entropy.    entropy View source \nentropy(\n    name='entropy'\n)\n Shannon entropy in nats. event_shape_tensor View source \nevent_shape_tensor(\n    name='event_shape_tensor'\n)\n Shape of a single sample from a single batch as a 1-D int32 Tensor.\n \n\n\n Args\n  name   name to give to the op   \n \n\n\n Returns\n  event_shape   Tensor.    is_scalar_batch View source \nis_scalar_batch(\n    name='is_scalar_batch'\n)\n Indicates that batch_shape == [].\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  is_scalar_batch   bool scalar Tensor.    is_scalar_event View source \nis_scalar_event(\n    name='is_scalar_event'\n)\n Indicates that event_shape == [].\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  is_scalar_event   bool scalar Tensor.    kl_divergence View source \nkl_divergence(\n    other, name='kl_divergence'\n)\n Computes the Kullback--Leibler divergence. Denote this distribution (self) by p and the other distribution by q. Assuming p, q are absolutely continuous with respect to reference measure r, the KL divergence is defined as: KL[p, q] = E_p[log(p(X)/q(X))]\n         = -int_F p(x) log q(x) dr(x) + int_F p(x) log p(x) dr(x)\n         = H[p, q] - H[p]\n where F denotes the support of the random variable X ~ p, H[., .] denotes (Shanon) cross entropy, and H[.] denotes (Shanon) entropy.\n \n\n\n Args\n  other   tfp.distributions.Distribution instance.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  kl_divergence   self.dtype Tensor with shape [B1, ..., Bn] representing n different calculations of the Kullback-Leibler divergence.    log_cdf View source \nlog_cdf(\n    value, name='log_cdf'\n)\n Log cumulative distribution function. Given random variable X, the cumulative distribution function cdf is: log_cdf(x) := Log[ P[X <= x] ]\n Often, a numerical approximation can be used for log_cdf(x) that yields a more accurate answer than simply taking the logarithm of the cdf when x << -1.\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  logcdf   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    log_prob View source \nlog_prob(\n    value, name='log_prob'\n)\n Log probability density/mass function. Additional documentation from DirichletMultinomial: For each batch of counts, value = [n_0, ..., n_{K-1}], P[value] is the probability that after sampling self.total_count draws from this Dirichlet-Multinomial distribution, the number of draws falling in class j is n_j. Since this definition is exchangeable; different sequences have the same counts so the probability includes a combinatorial coefficient. \nNote: value must be a non-negative tensor with dtype self.dtype, have no fractional components, and such that tf.reduce_sum(value, -1) = self.total_count. Its shape must be broadcastable with self.concentration and self.total_count.\n\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  log_prob   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    log_survival_function View source \nlog_survival_function(\n    value, name='log_survival_function'\n)\n Log survival function. Given random variable X, the survival function is defined: log_survival_function(x) = Log[ P[X > x] ]\n                         = Log[ 1 - P[X <= x] ]\n                         = Log[ 1 - cdf(x) ]\n Typically, different numerical approximations can be used for the log survival function, which are more accurate than 1 - cdf(x) when x >> 1.\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns   Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.  \n mean View source \nmean(\n    name='mean'\n)\n Mean. mode View source \nmode(\n    name='mode'\n)\n Mode. param_shapes View source \n@classmethod\nparam_shapes(\n    sample_shape, name='DistributionParamShapes'\n)\n Shapes of parameters given the desired shape of a call to sample(). This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample(). Subclasses should override class method _param_shapes.\n \n\n\n Args\n  sample_shape   Tensor or python list/tuple. Desired shape of a call to sample().  \n  name   name to prepend ops with.   \n \n\n\n Returns   dict of parameter name to Tensor shapes.  \n param_static_shapes View source \n@classmethod\nparam_static_shapes(\n    sample_shape\n)\n param_shapes with static (i.e. TensorShape) shapes. This is a class method that describes what key/value arguments are required to instantiate the given Distribution so that a particular shape is returned for that instance's call to sample(). Assumes that the sample's shape is known statically. Subclasses should override class method _param_shapes to return constant-valued tensors when constant values are fed.\n \n\n\n Args\n  sample_shape   TensorShape or python list/tuple. Desired shape of a call to sample().   \n \n\n\n Returns   dict of parameter name to TensorShape.  \n\n \n\n\n Raises\n  ValueError   if sample_shape is a TensorShape and is not fully defined.    prob View source \nprob(\n    value, name='prob'\n)\n Probability density/mass function. Additional documentation from DirichletMultinomial: For each batch of counts, value = [n_0, ..., n_{K-1}], P[value] is the probability that after sampling self.total_count draws from this Dirichlet-Multinomial distribution, the number of draws falling in class j is n_j. Since this definition is exchangeable; different sequences have the same counts so the probability includes a combinatorial coefficient. \nNote: value must be a non-negative tensor with dtype self.dtype, have no fractional components, and such that tf.reduce_sum(value, -1) = self.total_count. Its shape must be broadcastable with self.concentration and self.total_count.\n\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  prob   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    quantile View source \nquantile(\n    value, name='quantile'\n)\n Quantile function. Aka \"inverse cdf\" or \"percent point function\". Given random variable X and p in [0, 1], the quantile is: quantile(p) := x such that P[X <= x] == p\n\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  quantile   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    sample View source \nsample(\n    sample_shape=(), seed=None, name='sample'\n)\n Generate samples of the specified shape. Note that a call to sample() without arguments will generate a single sample.\n \n\n\n Args\n  sample_shape   0D or 1D int32 Tensor. Shape of the generated samples.  \n  seed   Python integer seed for RNG  \n  name   name to give to the op.   \n \n\n\n Returns\n  samples   a Tensor with prepended dimensions sample_shape.    stddev View source \nstddev(\n    name='stddev'\n)\n Standard deviation. Standard deviation is defined as, stddev = E[(X - E[X])**2]**0.5\n where X is the random variable associated with this distribution, E denotes expectation, and stddev.shape = batch_shape + event_shape.\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  stddev   Floating-point Tensor with shape identical to batch_shape + event_shape, i.e., the same shape as self.mean().    survival_function View source \nsurvival_function(\n    value, name='survival_function'\n)\n Survival function. Given random variable X, the survival function is defined: survival_function(x) = P[X > x]\n                     = 1 - P[X <= x]\n                     = 1 - cdf(x).\n\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns   Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.  \n variance View source \nvariance(\n    name='variance'\n)\n Variance. Variance is defined as, Var = E[(X - E[X])**2]\n where X is the random variable associated with this distribution, E denotes expectation, and Var.shape = batch_shape + event_shape.\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  variance   Floating-point Tensor with shape identical to batch_shape + event_shape, i.e., the same shape as self.mean().     \n"}, {"name": "tf.compat.v1.distributions.Distribution", "path": "compat/v1/distributions/distribution", "type": "tf.compat", "text": "tf.compat.v1.distributions.Distribution A generic probability distribution base class. \ntf.compat.v1.distributions.Distribution(\n    dtype, reparameterization_type, validate_args, allow_nan_stats, parameters=None,\n    graph_parents=None, name=None\n)\n Distribution is a base class for constructing and organizing properties (e.g., mean, variance) of random variables (e.g, Bernoulli, Gaussian). Subclassing Subclasses are expected to implement a leading-underscore version of the same-named function. The argument signature should be identical except for the omission of name=\"...\". For example, to enable log_prob(value, name=\"log_prob\") a subclass should implement _log_prob(value). Subclasses can append to public-level docstrings by providing docstrings for their method specializations. For example: @util.AppendDocstring(\"Some other details.\")\ndef _log_prob(self, value):\n  ...\n would add the string \"Some other details.\" to the log_prob function docstring. This is implemented as a simple decorator to avoid python linter complaining about missing Args/Returns/Raises sections in the partial docstrings. Broadcasting, batching, and shapes All distributions support batches of independent distributions of that type. The batch shape is determined by broadcasting together the parameters. The shape of arguments to __init__, cdf, log_cdf, prob, and log_prob reflect this broadcasting, as does the return value of sample and sample_n. sample_n_shape = [n] + batch_shape + event_shape, where sample_n_shape is the shape of the Tensor returned from sample_n, n is the number of samples, batch_shape defines how many independent distributions there are, and event_shape defines the shape of samples from each of those independent distributions. Samples are independent along the batch_shape dimensions, but not necessarily so along the event_shape dimensions (depending on the particulars of the underlying distribution). Using the Uniform distribution as an example: minval = 3.0\nmaxval = [[4.0, 6.0],\n          [10.0, 12.0]]\n\n# Broadcasting:\n# This instance represents 4 Uniform distributions. Each has a lower bound at\n# 3.0 as the `minval` parameter was broadcasted to match `maxval`'s shape.\nu = Uniform(minval, maxval)\n\n# `event_shape` is `TensorShape([])`.\nevent_shape = u.event_shape\n# `event_shape_t` is a `Tensor` which will evaluate to [].\nevent_shape_t = u.event_shape_tensor()\n\n# Sampling returns a sample per distribution. `samples` has shape\n# [5, 2, 2], which is [n] + batch_shape + event_shape, where n=5,\n# batch_shape=[2, 2], and event_shape=[].\nsamples = u.sample_n(5)\n\n# The broadcasting holds across methods. Here we use `cdf` as an example. The\n# same holds for `log_cdf` and the likelihood functions.\n\n# `cum_prob` has shape [2, 2] as the `value` argument was broadcasted to the\n# shape of the `Uniform` instance.\ncum_prob_broadcast = u.cdf(4.0)\n\n# `cum_prob`'s shape is [2, 2], one per distribution. No broadcasting\n# occurred.\ncum_prob_per_dist = u.cdf([[4.0, 5.0],\n                           [6.0, 7.0]])\n\n# INVALID as the `value` argument is not broadcastable to the distribution's\n# shape.\ncum_prob_invalid = u.cdf([4.0, 5.0, 6.0])\n Shapes There are three important concepts associated with TensorFlow Distributions shapes:  Event shape describes the shape of a single draw from the distribution; it may be dependent across dimensions. For scalar distributions, the event shape is []. For a 5-dimensional MultivariateNormal, the event shape is [5]. Batch shape describes independent, not identically distributed draws, aka a \"collection\" or \"bunch\" of distributions. Sample shape describes independent, identically distributed draws of batches from the distribution family.  The event shape and the batch shape are properties of a Distribution object, whereas the sample shape is associated with a specific call to sample or log_prob. For detailed usage examples of TensorFlow Distributions shapes, see this tutorial Parameter values leading to undefined statistics or distributions. Some distributions do not have well-defined statistics for all initialization parameter values. For example, the beta distribution is parameterized by positive real numbers concentration1 and concentration0, and does not have well-defined mode if concentration1 < 1 or concentration0 < 1. The user is given the option of raising an exception or returning NaN. a = tf.exp(tf.matmul(logits, weights_a))\nb = tf.exp(tf.matmul(logits, weights_b))\n\n# Will raise exception if ANY batch member has a < 1 or b < 1.\ndist = distributions.beta(a, b, allow_nan_stats=False)\nmode = dist.mode().eval()\n\n# Will return NaN for batch members with either a < 1 or b < 1.\ndist = distributions.beta(a, b, allow_nan_stats=True)  # Default behavior\nmode = dist.mode().eval()\n In all cases, an exception is raised if invalid parameters are passed, e.g. # Will raise an exception if any Op is run.\nnegative_a = -1.0 * a  # beta distribution by definition has a > 0.\ndist = distributions.beta(negative_a, b, allow_nan_stats=True)\ndist.mean().eval()\n\n \n\n\n Args\n  dtype   The type of the event samples. None implies no type-enforcement.  \n  reparameterization_type   Instance of ReparameterizationType. If distributions.FULLY_REPARAMETERIZED, this Distribution can be reparameterized in terms of some standard distribution with a function whose Jacobian is constant for the support of the standard distribution. If distributions.NOT_REPARAMETERIZED, then no such reparameterization is available.  \n  validate_args   Python bool, default False. When True distribution parameters are checked for validity despite possibly degrading runtime performance. When False invalid inputs may silently render incorrect outputs.  \n  allow_nan_stats   Python bool, default True. When True, statistics (e.g., mean, mode, variance) use the value \"NaN\" to indicate the result is undefined. When False, an exception is raised if one or more of the statistic's batch members are undefined.  \n  parameters   Python dict of parameters used to instantiate this Distribution.  \n  graph_parents   Python list of graph prerequisites of this Distribution.  \n  name   Python str name prefixed to Ops created by this class. Default: subclass name.   \n \n\n\n Raises\n  ValueError   if any member of graph_parents is None or not a Tensor.   \n \n\n\n Attributes\n  allow_nan_stats   Python bool describing behavior when a stat is undefined. Stats return +/- infinity when it makes sense. E.g., the variance of a Cauchy distribution is infinity. However, sometimes the statistic is undefined, e.g., if a distribution's pdf does not achieve a maximum within the support of the distribution, the mode is undefined. If the mean is undefined, then by definition the variance is undefined. E.g. the mean for Student's T for df = 1 is undefined (no clear way to say it is either + or - infinity), so the variance = E[(X - mean)**2] is also undefined. \n \n  batch_shape   Shape of a single sample from a single event index as a TensorShape. May be partially defined or unknown. The batch dimensions are indexes into independent, non-identical parameterizations of this distribution. \n \n  dtype   The DType of Tensors handled by this Distribution.  \n  event_shape   Shape of a single sample from a single batch as a TensorShape. May be partially defined or unknown. \n \n  name   Name prepended to all ops created by this Distribution.  \n  parameters   Dictionary of parameters used to instantiate this Distribution.  \n  reparameterization_type   Describes how samples from the distribution are reparameterized. Currently this is one of the static instances distributions.FULLY_REPARAMETERIZED or distributions.NOT_REPARAMETERIZED. \n \n  validate_args   Python bool indicating possibly expensive checks are enabled.    Methods batch_shape_tensor View source \nbatch_shape_tensor(\n    name='batch_shape_tensor'\n)\n Shape of a single sample from a single event index as a 1-D Tensor. The batch dimensions are indexes into independent, non-identical parameterizations of this distribution.\n \n\n\n Args\n  name   name to give to the op   \n \n\n\n Returns\n  batch_shape   Tensor.    cdf View source \ncdf(\n    value, name='cdf'\n)\n Cumulative distribution function. Given random variable X, the cumulative distribution function cdf is: cdf(x) := P[X <= x]\n\n \n\n\n Args\n  value   float or double Tensor.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  cdf   a Tensor of shape sample_shape(x) + self.batch_shape with values of type self.dtype.    copy View source \ncopy(\n    **override_parameters_kwargs\n)\n Creates a deep copy of the distribution. \nNote: the copy distribution may continue to depend on the original initialization arguments.\n\n \n\n\n Args\n  **override_parameters_kwargs   String/value dictionary of initialization arguments to override with new values.   \n \n\n\n Returns\n  distribution   A new instance of type(self) initialized from the union of self.parameters and override_parameters_kwargs, i.e., dict(self.parameters, **override_parameters_kwargs).    covariance View source \ncovariance(\n    name='covariance'\n)\n Covariance. Covariance is (possibly) defined only for non-scalar-event distributions. For example, for a length-k, vector-valued distribution, it is calculated as, Cov[i, j] = Covariance(X_i, X_j) = E[(X_i - E[X_i]) (X_j - E[X_j])]\n where Cov is a (batch of) k x k matrix, 0 <= (i, j) < k, and E denotes expectation. Alternatively, for non-vector, multivariate distributions (e.g., matrix-valued, Wishart), Covariance shall return a (batch of) matrices under some vectorization of the events, i.e., Cov[i, j] = Covariance(Vec(X)_i, Vec(X)_j) = [as above]\n where Cov is a (batch of) k' x k' matrices, 0 <= (i, j) < k' = reduce_prod(event_shape), and Vec is some function mapping indices of this distribution's event dimensions to indices of a length-k' vector.\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  covariance   Floating-point Tensor with shape [B1, ..., Bn, k', k'] where the first n dimensions are batch coordinates and k' = reduce_prod(self.event_shape).    cross_entropy View source \ncross_entropy(\n    other, name='cross_entropy'\n)\n Computes the (Shannon) cross entropy. Denote this distribution (self) by P and the other distribution by Q. Assuming P, Q are absolutely continuous with respect to one another and permit densities p(x) dr(x) and q(x) dr(x), (Shanon) cross entropy is defined as: H[P, Q] = E_p[-log q(X)] = -int_F p(x) log q(x) dr(x)\n where F denotes the support of the random variable X ~ P.\n \n\n\n Args\n  other   tfp.distributions.Distribution instance.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  cross_entropy   self.dtype Tensor with shape [B1, ..., Bn] representing n different calculations of (Shanon) cross entropy.    entropy View source \nentropy(\n    name='entropy'\n)\n Shannon entropy in nats. event_shape_tensor View source \nevent_shape_tensor(\n    name='event_shape_tensor'\n)\n Shape of a single sample from a single batch as a 1-D int32 Tensor.\n \n\n\n Args\n  name   name to give to the op   \n \n\n\n Returns\n  event_shape   Tensor.    is_scalar_batch View source \nis_scalar_batch(\n    name='is_scalar_batch'\n)\n Indicates that batch_shape == [].\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  is_scalar_batch   bool scalar Tensor.    is_scalar_event View source \nis_scalar_event(\n    name='is_scalar_event'\n)\n Indicates that event_shape == [].\n \n\n\n Args\n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  is_scalar_event   bool scalar Tensor.    kl_divergence View source \nkl_divergence(\n    other, name='kl_divergence'\n)\n Computes the Kullback--Leibler divergence. Denote this distribution (self) by p and the other distribution by q. Assuming p, q are absolutely continuous with respect to reference measure r, the KL divergence is defined as: KL[p, q] = E_p[log(p(X)/q(X))]\n         = -int_F p(x) log q(x) dr(x) + int_F p(x) log p(x) dr(x)\n         = H[p, q] - H[p]\n where F denotes the support of the random variable X ~ p, H[., .] denotes (Shanon) cross entropy, and H[.] denotes (Shanon) entropy.\n \n\n\n Args\n  other   tfp.distributions.Distribution instance.  \n  name   Python str prepended to names of ops created by this function.   \n \n\n\n Returns\n  kl_divergence   self.dtype Tensor with shape