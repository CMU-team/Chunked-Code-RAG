{"task_id":3283984,"prompt":"def f_3283984():\n\treturn ","suffix":"","canonical_solution":"bytes.fromhex('4a4b4c').decode('utf-8')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == \"JKL\"\n"],"entry_point":"f_3283984","intent":"decode a hex string '4a4b4c' to UTF-8.","library":[],"docs":[]}
{"task_id":3844801,"prompt":"def f_3844801(myList):\n\treturn ","suffix":"","canonical_solution":"all(x == myList[0] for x in myList)","test_start":"\ndef check(candidate):","test":["\n    assert candidate([1,2,3]) == False\n","\n    assert candidate([1,1,1,1,1,1]) == True\n","\n    assert candidate([1]) == True\n","\n    assert candidate(['k','k','k','k','k']) == True\n","\n    assert candidate([None,'%$#ga',3]) == False\n"],"entry_point":"f_3844801","intent":"check if all elements in list `myList` are identical","library":[],"docs":[]}
{"task_id":4302166,"prompt":"def f_4302166():\n\treturn ","suffix":"","canonical_solution":"'%*s : %*s' % (20, 'Python', 20, 'Very Good')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == '              Python :            Very Good'\n"],"entry_point":"f_4302166","intent":"format number of spaces between strings `Python`, `:` and `Very Good` to be `20`","library":[],"docs":[]}
{"task_id":7555335,"prompt":"def f_7555335(d):\n\treturn ","suffix":"","canonical_solution":"d.decode('cp1251').encode('utf8')","test_start":"\ndef check(candidate):","test":["\n    assert candidate('hello world!'.encode('cp1251')) == b'hello world!'\n","\n    assert candidate('%*(^O*'.encode('cp1251')) == b'%*(^O*'\n","\n    assert candidate(''.encode('cp1251')) == b''\n","\n    assert candidate('hello world!'.encode('cp1251')) != 'hello world!'\n"],"entry_point":"f_7555335","intent":"convert a string `d` from CP-1251 to UTF-8","library":[],"docs":[]}
{"task_id":2544710,"prompt":"def f_2544710(kwargs):\n\treturn ","suffix":"","canonical_solution":"{k: v for k, v in list(kwargs.items()) if v is not None}","test_start":"\ndef check(candidate):","test":["\n    assert candidate({i: None for i in range(10)}) == {}\n","\n    assert candidate({i: min(i,4) for i in range(6)}) == {0:0,1:1,2:2,3:3,4:4,5:4}\n","\n    assert candidate({'abc': 'abc'})['abc'] == 'abc'\n","\n    assert candidate({'x': None, 'yy': 234}) == {'yy': 234}\n"],"entry_point":"f_2544710","intent":"get rid of None values in dictionary `kwargs`","library":[],"docs":[]}
{"task_id":2544710,"prompt":"def f_2544710(kwargs):\n\treturn ","suffix":"","canonical_solution":"dict((k, v) for k, v in kwargs.items() if v is not None)","test_start":"\ndef check(candidate):","test":["\n    assert candidate({i: None for i in range(10)}) == {}\n","\n    assert candidate({i: min(i,4) for i in range(6)}) == {0:0,1:1,2:2,3:3,4:4,5:4}\n","\n    assert candidate({'abc': 'abc'})['abc'] == 'abc'\n","\n    assert candidate({'x': None, 'yy': 234}) == {'yy': 234}\n"],"entry_point":"f_2544710","intent":"get rid of None values in dictionary `kwargs`","library":[],"docs":[]}
{"task_id":14971373,"prompt":"def f_14971373():\n\treturn ","suffix":"","canonical_solution":"subprocess.check_output('ps -ef | grep something | wc -l', shell=True)","test_start":"\nimport subprocess\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    output = b'  PID TTY          TIME CMD\\n  226 pts\/1    00:00:00 bash\\n  285 pts\/1    00:00:00 python3\\n  352 pts\/1    00:00:00 ps\\n'\n    subprocess.check_output = Mock(return_value = output)\n    assert candidate() == output\n"],"entry_point":"f_14971373","intent":"capture final output of a chain of system commands `ps -ef | grep something | wc -l`","library":["subprocess"],"docs":[{"text":"winreg.KEY_EXECUTE  \nEquivalent to KEY_READ.","title":"python.library.winreg#winreg.KEY_EXECUTE"},{"text":"output  \nOutput of the child process if it was captured by run() or check_output(). Otherwise, None.","title":"python.library.subprocess#subprocess.CalledProcessError.output"},{"text":"output  \nOutput of the child process if it was captured by run() or check_output(). Otherwise, None.","title":"python.library.subprocess#subprocess.TimeoutExpired.output"},{"text":"stderr  \nStderr output of the child process if it was captured by run(). Otherwise, None.","title":"python.library.subprocess#subprocess.TimeoutExpired.stderr"},{"text":"cmd  \nCommand that was used to spawn the child process.","title":"python.library.subprocess#subprocess.TimeoutExpired.cmd"},{"text":"Cmd.lastcmd  \nThe last nonempty command prefix seen.","title":"python.library.cmd#cmd.Cmd.lastcmd"},{"text":"report()  \nPrint (to sys.stdout) a comparison between a and b.","title":"python.library.filecmp#filecmp.dircmp.report"},{"text":"stderr  \nStderr output of the child process if it was captured by run(). Otherwise, None.","title":"python.library.subprocess#subprocess.CalledProcessError.stderr"},{"text":"NNTP.last()  \nSend a LAST command. Return as for stat().","title":"python.library.nntplib#nntplib.NNTP.last"},{"text":"window.clrtoeol()  \nErase from cursor to the end of the line.","title":"python.library.curses#curses.window.clrtoeol"}]}
{"task_id":6726636,"prompt":"def f_6726636():\n\treturn ","suffix":"","canonical_solution":"\"\"\"\"\"\".join(['a', 'b', 'c'])","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == \"abc\"\n","\n    assert candidate() == 'a' + 'b' + 'c'\n"],"entry_point":"f_6726636","intent":"concatenate a list of strings `['a', 'b', 'c']`","library":[],"docs":[]}
{"task_id":18079563,"prompt":"def f_18079563(s1, s2):\n\treturn ","suffix":"","canonical_solution":"pd.Series(list(set(s1).intersection(set(s2))))","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    x1, x2 = pd.Series([1,2]), pd.Series([1,3])\n    assert candidate(x1, x2).equals(pd.Series([1]))\n","\n    x1, x2 = pd.Series([1,2]), pd.Series([1,3, 10, 4, 5, 9])\n    assert candidate(x1, x2).equals(pd.Series([1]))\n","\n    x1, x2 = pd.Series([1,2]), pd.Series([1,2, 10])\n    assert candidate(x1, x2).equals(pd.Series([1, 2]))\n"],"entry_point":"f_18079563","intent":"find intersection data between series `s1` and series `s2`","library":["pandas"],"docs":[{"text":"intersection()","title":"django.ref.contrib.gis.gdal#django.contrib.gis.gdal.OGRGeometry.intersection"},{"text":"class Intersection(expr1, expr2, **extra)","title":"django.ref.contrib.gis.functions#django.contrib.gis.db.models.functions.Intersection"},{"text":"intersection(*other_qs)","title":"django.ref.models.querysets#django.db.models.query.QuerySet.intersection"},{"text":"intersection(*others)  \nset & other & ...  \nReturn a new set with elements common to the set and all others.","title":"python.library.stdtypes#frozenset.intersection"},{"text":"intersection_update(*others)  \nset &= other & ...  \nUpdate the set, keeping only elements found in it and all others.","title":"python.library.stdtypes#frozenset.intersection_update"},{"text":"intersects(other)","title":"django.ref.contrib.gis.gdal#django.contrib.gis.gdal.OGRGeometry.intersects"},{"text":"intersects(other)","title":"django.ref.contrib.gis.geos#django.contrib.gis.geos.PreparedGeometry.intersects"},{"text":"pandas.tseries.offsets.BQuarterBegin.startingMonth   BQuarterBegin.startingMonth","title":"pandas.reference.api.pandas.tseries.offsets.bquarterbegin.startingmonth"},{"text":"pandas.tseries.offsets.BYearEnd.normalize   BYearEnd.normalize","title":"pandas.reference.api.pandas.tseries.offsets.byearend.normalize"},{"text":"pandas.tseries.offsets.FY5253Quarter.normalize   FY5253Quarter.normalize","title":"pandas.reference.api.pandas.tseries.offsets.fy5253quarter.normalize"}]}
{"task_id":8315209,"prompt":"def f_8315209(client):\n\t","suffix":"\n\treturn ","canonical_solution":"client.send('HTTP\/1.0 200 OK\\r\\n')","test_start":"\nimport socket\nfrom unittest.mock import Mock\nimport mock\n\ndef check(candidate):","test":["\n    with mock.patch('socket.socket') as mock_socket:\n        mock_socket.return_value.recv.return_value = ''\n        mock_socket.bind(('', 8080))\n        mock_socket.listen(5)\n        mock_socket.accept = Mock(return_value = mock_socket)\n        mock_socket.send = Mock()\n        try:\n            candidate(mock_socket)\n        except:\n            assert False\n"],"entry_point":"f_8315209","intent":"sending http headers to `client`","library":["socket"],"docs":[{"text":"HTTPConnection.send(data)  \nSend data to the server. This should be used directly only after the endheaders() method has been called and before getresponse() is called.","title":"python.library.http.client#http.client.HTTPConnection.send"},{"text":"HTTPResponse.getheaders()  \nReturn a list of (header, value) tuples.","title":"python.library.http.client#http.client.HTTPResponse.getheaders"},{"text":"HTTPResponse.headers  \nHeaders of the response in the form of an email.message.EmailMessage instance.","title":"python.library.http.client#http.client.HTTPResponse.headers"},{"text":"HTTPResponse.info()  \n Deprecated since version 3.9: Deprecated in favor of headers.","title":"python.library.http.client#http.client.HTTPResponse.info"},{"text":"flush_headers()  \nFinally send the headers to the output stream and flush the internal headers buffer.  New in version 3.3.","title":"python.library.http.server#http.server.BaseHTTPRequestHandler.flush_headers"},{"text":"exception http.client.UnknownProtocol  \nA subclass of HTTPException.","title":"python.library.http.client#http.client.UnknownProtocol"},{"text":"HTTPResponse.status  \nStatus code returned by server.","title":"python.library.http.client#http.client.HTTPResponse.status"},{"text":"exception http.client.CannotSendHeader  \nA subclass of ImproperConnectionState.","title":"python.library.http.client#http.client.CannotSendHeader"},{"text":"headers  \nThe headers received with the request.","title":"flask.api.index#flask.Request.headers"},{"text":"exception http.client.UnknownTransferEncoding  \nA subclass of HTTPException.","title":"python.library.http.client#http.client.UnknownTransferEncoding"}]}
{"task_id":26153795,"prompt":"def f_26153795(when):\n\treturn ","suffix":"","canonical_solution":"datetime.datetime.strptime(when, '%Y-%m-%d').date()","test_start":"\nimport datetime\n\ndef check(candidate):","test":["\n    assert candidate('2013-05-07') == datetime.date(2013, 5, 7)\n","\n    assert candidate('2000-02-29') == datetime.date(2000, 2, 29)\n","\n    assert candidate('1990-01-08') == datetime.date(1990, 1, 8)\n","\n    assert candidate('1990-1-08') == datetime.date(1990, 1, 8)\n","\n    assert candidate('1990-1-8') == datetime.date(1990, 1, 8)\n","\n    assert candidate('1990-01-8') == datetime.date(1990, 1, 8)\n"],"entry_point":"f_26153795","intent":"Format a datetime string `when` to extract date only","library":["datetime"],"docs":[{"text":"date_format  \nSimilar to DateInput.format","title":"django.ref.forms.widgets#django.forms.SplitDateTimeWidget.date_format"},{"text":"date_attrs","title":"django.ref.forms.widgets#django.forms.SplitDateTimeWidget.date_attrs"},{"text":"pandas.Timestamp.fromisoformat   Timestamp.fromisoformat()\n \nstring -> datetime from datetime.isoformat() output","title":"pandas.reference.api.pandas.timestamp.fromisoformat"},{"text":"date.month  \nBetween 1 and 12 inclusive.","title":"python.library.datetime#datetime.date.month"},{"text":"time_format  \nSimilar to TimeInput.format","title":"django.ref.forms.widgets#django.forms.SplitDateTimeWidget.time_format"},{"text":"date.__str__()  \nFor a date d, str(d) is equivalent to d.isoformat().","title":"python.library.datetime#datetime.date.__str__"},{"text":"time.__str__()  \nFor a time t, str(t) is equivalent to t.isoformat().","title":"python.library.datetime#datetime.time.__str__"},{"text":"datetime.date()  \nReturn date object with same year, month and day.","title":"python.library.datetime#datetime.datetime.date"},{"text":"locale.ERA_D_FMT  \nGet a format string for time.strftime() to represent a date in a locale-specific era-based way.","title":"python.library.locale#locale.ERA_D_FMT"},{"text":"parse_date(value)  \nParses a string and returns a datetime.date.","title":"django.ref.utils#django.utils.dateparse.parse_date"}]}
{"task_id":172439,"prompt":"def f_172439(inputString):\n\treturn ","suffix":"","canonical_solution":"inputString.split('\\n')","test_start":"\ndef check(candidate):","test":["\n    assert candidate('line a\\nfollows by line b\t...bye\\n') ==         ['line a', 'follows by line b\t...bye', '']\n","\n    assert candidate('no new line in this sentence. ') == ['no new line in this sentence. ']\n","\n    assert candidate('a\tbfs hhhdf\tsfdas') == ['a\tbfs hhhdf\tsfdas']\n","\n    assert candidate('') == ['']\n"],"entry_point":"f_172439","intent":"split a multi-line string `inputString` into separate strings","library":[],"docs":[]}
{"task_id":172439,"prompt":"def f_172439():\n\treturn ","suffix":"","canonical_solution":"' a \\n b \\r\\n c '.split('\\n')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == [' a ', ' b \\r', ' c ']\n"],"entry_point":"f_172439","intent":"Split a multi-line string ` a \\n b \\r\\n c ` by new line character `\\n`","library":[],"docs":[]}
{"task_id":13954222,"prompt":"def f_13954222(b):\n\treturn ","suffix":"","canonical_solution":"\"\"\":\"\"\".join(str(x) for x in b)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(['x','y','zzz']) == 'x:y:zzz'\n","\n    assert candidate(['111','22','3']) == '111:22:3'\n","\n    assert candidate(['']) == ''\n","\n    assert candidate([':',':']) == ':::'\n","\n    assert candidate([',','#','#$%']) == ',:#:#$%'\n","\n    assert candidate(['a','b','c']) != 'abc'\n"],"entry_point":"f_13954222","intent":"concatenate elements of list `b` by a colon \":\"","library":[],"docs":[]}
{"task_id":13567345,"prompt":"def f_13567345(a):\n\treturn ","suffix":"","canonical_solution":"a.sum(axis=1)","test_start":"\nimport numpy as np \n\ndef check(candidate):","test":["\n    a1 = np.array([[i for i in range(3)] for j in range(5)])\n    assert np.array_equal(candidate(a1), np.array([3, 3, 3, 3, 3]))\n","\n    a2 = np.array([[i+j for i in range(3)] for j in range(5)])\n    assert np.array_equal(candidate(a2), np.array([ 3,  6,  9, 12, 15]))\n","\n    a3 = np.array([[i*j for i in range(3)] for j in range(5)])\n    assert np.array_equal(candidate(a3), np.array([ 0,  3,  6,  9, 12]))\n"],"entry_point":"f_13567345","intent":"Calculate sum over all rows of 2D numpy array `a`","library":["numpy"],"docs":[{"text":"numpy.recarray.sum method   recarray.sum(axis=None, dtype=None, out=None, keepdims=False, initial=0, where=True)\n \nReturn the sum of the array elements over the given axis. Refer to numpy.sum for full documentation.  See also  numpy.sum\n\nequivalent function","title":"numpy.reference.generated.numpy.recarray.sum"},{"text":"numpy.ndarray.__pos__ method   ndarray.__pos__(\/)\n \n+self","title":"numpy.reference.generated.numpy.ndarray.__pos__"},{"text":"pandas.tseries.offsets.Tick.apply   Tick.apply()","title":"pandas.reference.api.pandas.tseries.offsets.tick.apply"},{"text":"cumsum_(dim, dtype=None) \u2192 Tensor  \nIn-place version of cumsum()","title":"torch.tensors#torch.Tensor.cumsum_"},{"text":"numpy.recarray.trace method   recarray.trace(offset=0, axis1=0, axis2=1, dtype=None, out=None)\n \nReturn the sum along diagonals of the array. Refer to numpy.trace for full documentation.  See also  numpy.trace\n\nequivalent function","title":"numpy.reference.generated.numpy.recarray.trace"},{"text":"numpy.ndarray.sum method   ndarray.sum(axis=None, dtype=None, out=None, keepdims=False, initial=0, where=True)\n \nReturn the sum of the array elements over the given axis. Refer to numpy.sum for full documentation.  See also  numpy.sum\n\nequivalent function","title":"numpy.reference.generated.numpy.ndarray.sum"},{"text":"pandas.tseries.offsets.Nano.apply   Nano.apply()","title":"pandas.reference.api.pandas.tseries.offsets.nano.apply"},{"text":"numpy.float64[source]\n \nalias of numpy.double","title":"numpy.reference.arrays.scalars#numpy.float64"},{"text":"numpy.trace   numpy.trace(a, offset=0, axis1=0, axis2=1, dtype=None, out=None)[source]\n \nReturn the sum along diagonals of the array. If a is 2-D, the sum along its diagonal with the given offset is returned, i.e., the sum of elements a[i,i+offset] for all i. If a has more than two dimensions, then the axes specified by axis1 and axis2 are used to determine the 2-D sub-arrays whose traces are returned. The shape of the resulting array is the same as that of a with axis1 and axis2 removed.  Parameters \n \naarray_like\n\n\nInput array, from which the diagonals are taken.  \noffsetint, optional\n\n\nOffset of the diagonal from the main diagonal. Can be both positive and negative. Defaults to 0.  \naxis1, axis2int, optional\n\n\nAxes to be used as the first and second axis of the 2-D sub-arrays from which the diagonals should be taken. Defaults are the first two axes of a.  \ndtypedtype, optional\n\n\nDetermines the data-type of the returned array and of the accumulator where the elements are summed. If dtype has the value None and a is of integer type of precision less than the default integer precision, then the default integer precision is used. Otherwise, the precision is the same as that of a.  \noutndarray, optional\n\n\nArray into which the output is placed. Its type is preserved and it must be of the right shape to hold the output.    Returns \n \nsum_along_diagonalsndarray\n\n\nIf a is 2-D, the sum along the diagonal is returned. If a has larger dimensions, then an array of sums along diagonals is returned.      See also  \ndiag, diagonal, diagflat\n\n  Examples >>> np.trace(np.eye(3))\n3.0\n>>> a = np.arange(8).reshape((2,2,2))\n>>> np.trace(a)\narray([6, 8])\n >>> a = np.arange(24).reshape((2,2,2,3))\n>>> np.trace(a).shape\n(2, 3)","title":"numpy.reference.generated.numpy.trace"},{"text":"numpy.ma.trace   ma.trace(self, offset=0, axis1=0, axis2=1, dtype=None, out=None) a.trace(offset=0, axis1=0, axis2=1, dtype=None, out=None) = <numpy.ma.core._frommethod object>\n  Return the sum along diagonals of the array. Refer to numpy.trace for full documentation.  See also  numpy.trace\n\nequivalent function","title":"numpy.reference.generated.numpy.ma.trace"}]}
{"task_id":29784889,"prompt":"def f_29784889():\n\t","suffix":"\n\treturn ","canonical_solution":"warnings.simplefilter('always')","test_start":"\nimport warnings \n\ndef check(candidate):","test":["\n    candidate() \n    assert any([(wf[0] == 'always') for wf in warnings.filters])\n"],"entry_point":"f_29784889","intent":"enable warnings using action 'always'","library":["warnings"],"docs":[{"text":"class Warning(msg, hint=None obj=None, id=None)","title":"django.ref.checks#django.core.checks.Warning"},{"text":"exception Warning  \nBase class for warning categories.","title":"python.library.exceptions#Warning"},{"text":"exception UserWarning  \nBase class for warnings generated by user code.","title":"python.library.exceptions#UserWarning"},{"text":"tf.compat.v1.logging.warning \ntf.compat.v1.logging.warning(\n    msg, *args, **kwargs\n)","title":"tensorflow.compat.v1.logging.warning"},{"text":"exception SyntaxWarning  \nBase class for warnings about dubious syntax.","title":"python.library.exceptions#SyntaxWarning"},{"text":"tf.compat.v1.logging.warn \ntf.compat.v1.logging.warn(\n    msg, *args, **kwargs\n)","title":"tensorflow.compat.v1.logging.warn"},{"text":"SESSION_COOKIE_HTTPONLY  \nBrowsers will not allow JavaScript access to cookies marked as \u201cHTTP only\u201d for security. Default: True","title":"flask.config.index#SESSION_COOKIE_HTTPONLY"},{"text":"exception RuntimeWarning  \nBase class for warnings about dubious runtime behavior.","title":"python.library.exceptions#RuntimeWarning"},{"text":"tkinter.messagebox.showwarning(title=None, message=None, **options)  \ntkinter.messagebox.showerror(title=None, message=None, **options)","title":"python.library.tkinter.messagebox#tkinter.messagebox.showwarning"},{"text":"exception sqlite3.Warning  \nA subclass of Exception.","title":"python.library.sqlite3#sqlite3.Warning"}]}
{"task_id":13550423,"prompt":"def f_13550423(l):\n\treturn ","suffix":"","canonical_solution":"' '.join(map(str, l))","test_start":"\ndef check(candidate):","test":["\n    assert candidate(['x','y','zzz']) == 'x y zzz'\n","\n    assert candidate(['111','22','3']) == '111 22 3'\n","\n    assert candidate(['']) == ''\n","\n    assert candidate([':',':']) == ': :'\n","\n    assert candidate([',','#','#$%']) == ', # #$%'\n","\n    assert candidate(['a','b','c']) != 'abc'\n"],"entry_point":"f_13550423","intent":"concatenate items of list `l` with a space ' '","library":[],"docs":[]}
{"task_id":698223,"prompt":"def f_698223():\n\treturn ","suffix":"","canonical_solution":"time.strptime('30\/03\/09 16:31:32.123', '%d\/%m\/%y %H:%M:%S.%f')","test_start":"\nimport time \n\ndef check(candidate):","test":["\n    answer = time.strptime('30\/03\/09 16:31:32.123', '%d\/%m\/%y %H:%M:%S.%f')\n    assert candidate() == answer\n    false_1 = time.strptime('30\/03\/09 17:31:32.123', '%d\/%m\/%y %H:%M:%S.%f')\n    assert candidate() != false_1\n    false_2 = time.strptime('20\/03\/09 17:31:32.123', '%d\/%m\/%y %H:%M:%S.%f')\n    assert candidate() != false_2\n"],"entry_point":"f_698223","intent":"parse a time string '30\/03\/09 16:31:32.123' containing milliseconds in it","library":["time"],"docs":[{"text":"decode(string)  \nAccept a string as the instance\u2019s new time value.","title":"python.library.xmlrpc.client#xmlrpc.client.DateTime.decode"},{"text":"pandas.Timestamp.microsecond   Timestamp.microsecond","title":"pandas.reference.api.pandas.timestamp.microsecond"},{"text":"parse_time(value)  \nParses a string and returns a datetime.time. UTC offsets aren\u2019t supported; if value describes one, the result is None.","title":"django.ref.utils#django.utils.dateparse.parse_time"},{"text":"pandas.Timestamp.nanosecond   Timestamp.nanosecond","title":"pandas.reference.api.pandas.timestamp.nanosecond"},{"text":"pandas.Timestamp.freq   Timestamp.freq","title":"pandas.reference.api.pandas.timestamp.freq"},{"text":"pandas.Timestamp.min   Timestamp.min=Timestamp('1677-09-21 00:12:43.145224193')","title":"pandas.reference.api.pandas.timestamp.min"},{"text":"time.microsecond  \nIn range(1000000).","title":"python.library.datetime#datetime.time.microsecond"},{"text":"pandas.tseries.offsets.Minute.freqstr   Minute.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.minute.freqstr"},{"text":"pandas.Timestamp.fromisoformat   Timestamp.fromisoformat()\n \nstring -> datetime from datetime.isoformat() output","title":"pandas.reference.api.pandas.timestamp.fromisoformat"},{"text":"time_format  \nSimilar to TimeInput.format","title":"django.ref.forms.widgets#django.forms.SplitDateTimeWidget.time_format"}]}
{"task_id":6633523,"prompt":"def f_6633523(my_string):\n\t","suffix":"\n\treturn my_float","canonical_solution":"my_float = float(my_string.replace(',', ''))","test_start":"\ndef check(candidate):","test":["\n    assert (candidate('1,234.00') - 1234.0) < 1e-6\n","\n    assert (candidate('0.00') - 0.00) < 1e-6\n","\n    assert (candidate('1,000,000.00') - 1000000.00) < 1e-6\n","\n    assert (candidate('1,000,000.00') - 999999.98) > 1e-6\n","\n    assert (candidate('1') - 1.00) < 1e-6\n"],"entry_point":"f_6633523","intent":"convert a string `my_string` with dot and comma into a float number `my_float`","library":[],"docs":[]}
{"task_id":6633523,"prompt":"def f_6633523():\n\treturn ","suffix":"","canonical_solution":"float('123,456.908'.replace(',', ''))","test_start":"\ndef check(candidate):","test":["\n    assert (candidate() - 123456.908) < 1e-6\n    assert (candidate() - 123456.9) > 1e-6\n    assert (candidate() - 1234.908) > 1e-6\n    assert type(candidate()) == float\n    assert int(candidate()) == 123456\n"],"entry_point":"f_6633523","intent":"convert a string `123,456.908` with dot and comma into a floating number","library":[],"docs":[]}
{"task_id":3108285,"prompt":"def f_3108285():\n\t","suffix":"\n\treturn ","canonical_solution":"sys.path.append('\/path\/to\/whatever')","test_start":"\nimport sys \n\ndef check(candidate):","test":["\n    original_paths = [sp for sp in sys.path]\n    candidate()\n    assert '\/path\/to\/whatever' in sys.path\n"],"entry_point":"f_3108285","intent":"set python path '\/path\/to\/whatever' in python script","library":["sys"],"docs":[{"text":"MOVETO=1","title":"matplotlib.path_api#matplotlib.path.Path.MOVETO"},{"text":"token.SLASH  \nToken value for \"\/\".","title":"python.library.token#token.SLASH"},{"text":"path(route, view, kwargs=None, name=None)","title":"django.ref.urls#django.urls.path"},{"text":"LINETO=2","title":"matplotlib.path_api#matplotlib.path.Path.LINETO"},{"text":"tkinter.filedialog.askdirectory(**options)","title":"python.library.dialog#tkinter.filedialog.askdirectory"},{"text":"app_import_path  \nOptionally the import path for the Flask application.","title":"flask.api.index#flask.cli.ScriptInfo.app_import_path"},{"text":"re_path(route, view, kwargs=None, name=None)","title":"django.ref.urls#django.urls.re_path"},{"text":"Path.chmod(mode)  \nChange the file mode and permissions, like os.chmod(): >>> p = Path('setup.py')\n>>> p.stat().st_mode\n33277\n>>> p.chmod(0o444)\n>>> p.stat().st_mode\n33060","title":"python.library.pathlib#pathlib.Path.chmod"},{"text":"token.DOUBLESLASH  \nToken value for \"\/\/\".","title":"python.library.token#token.DOUBLESLASH"},{"text":"test.support.unix_shell  \nPath for shell if not on Windows; otherwise None.","title":"python.library.test#test.support.unix_shell"}]}
{"task_id":2195340,"prompt":"def f_2195340():\n\treturn ","suffix":"","canonical_solution":"re.split('(\\\\W+)', 'Words, words, words.')","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate() == ['Words', ', ', 'words', ', ', 'words', '.', '']\n    assert candidate() == ['Words', ', '] + ['words', ', ', 'words', '.', '']\n"],"entry_point":"f_2195340","intent":"split string 'Words, words, words.' using a regex '(\\\\W+)'","library":["re"],"docs":[{"text":"re.split(pattern, string, maxsplit=0, flags=0)  \nSplit string by the occurrences of pattern. If capturing parentheses are used in pattern, then the text of all groups in the pattern are also returned as part of the resulting list. If maxsplit is nonzero, at most maxsplit splits occur, and the remainder of the string is returned as the final element of the list. >>> re.split(r'\\W+', 'Words, words, words.')\n['Words', 'words', 'words', '']\n>>> re.split(r'(\\W+)', 'Words, words, words.')\n['Words', ', ', 'words', ', ', 'words', '.', '']\n>>> re.split(r'\\W+', 'Words, words, words.', 1)\n['Words', 'words, words.']\n>>> re.split('[a-f]+', '0a3B9', flags=re.IGNORECASE)\n['0', '3', '9']\n If there are capturing groups in the separator and it matches at the start of the string, the result will start with an empty string. The same holds for the end of the string: >>> re.split(r'(\\W+)', '...words, words...')\n['', '...', 'words', ', ', 'words', '...', '']\n That way, separator components are always found at the same relative indices within the result list. Empty matches for the pattern split the string only when not adjacent to a previous empty match. >>> re.split(r'\\b', 'Words, words, words.')\n['', 'Words', ', ', 'words', ', ', 'words', '.']\n>>> re.split(r'\\W*', '...words...')\n['', '', 'w', 'o', 'r', 'd', 's', '', '']\n>>> re.split(r'(\\W*)', '...words...')\n['', '...', '', '', 'w', '', 'o', '', 'r', '', 'd', '', 's', '...', '', '', '']\n  Changed in version 3.1: Added the optional flags argument.   Changed in version 3.7: Added support of splitting on a pattern that could match an empty string.","title":"python.library.re#re.split"},{"text":"Pattern.split(string, maxsplit=0)  \nIdentical to the split() function, using the compiled pattern.","title":"python.library.re#re.Pattern.split"},{"text":"str.split(sep=None, maxsplit=-1)  \nReturn a list of the words in the string, using sep as the delimiter string. If maxsplit is given, at most maxsplit splits are done (thus, the list will have at most maxsplit+1 elements). If maxsplit is not specified or -1, then there is no limit on the number of splits (all possible splits are made). If sep is given, consecutive delimiters are not grouped together and are deemed to delimit empty strings (for example, '1,,2'.split(',') returns ['1', '', '2']). The sep argument may consist of multiple characters (for example, '1<>2<>3'.split('<>') returns ['1', '2', '3']). Splitting an empty string with a specified separator returns ['']. For example: >>> '1,2,3'.split(',')\n['1', '2', '3']\n>>> '1,2,3'.split(',', maxsplit=1)\n['1', '2,3']\n>>> '1,2,,3,'.split(',')\n['1', '2', '', '3', '']\n If sep is not specified or is None, a different splitting algorithm is applied: runs of consecutive whitespace are regarded as a single separator, and the result will contain no empty strings at the start or end if the string has leading or trailing whitespace. Consequently, splitting an empty string or a string consisting of just whitespace with a None separator returns []. For example: >>> '1 2 3'.split()\n['1', '2', '3']\n>>> '1 2 3'.split(maxsplit=1)\n['1', '2 3']\n>>> '   1   2   3   '.split()\n['1', '2', '3']","title":"python.library.stdtypes#str.split"},{"text":"pattern  \nThe regular expression pattern.","title":"python.library.re#re.error.pattern"},{"text":"exception tokenize.TokenError  \nRaised when either a docstring or expression that may be split over several lines is not completed anywhere in the file, for example: \"\"\"Beginning of\ndocstring\n or: [1,\n 2,\n 3","title":"python.library.tokenize#tokenize.TokenError"},{"text":"gettext.lngettext(singular, plural, n)","title":"python.library.gettext#gettext.lngettext"},{"text":"numpy.char.split   char.split(a, sep=None, maxsplit=None)[source]\n \nFor each element in a, return a list of the words in the string, using sep as the delimiter string. Calls str.split element-wise.  Parameters \n \naarray_like of str or unicode\n\n\nsepstr or unicode, optional\n\n\nIf sep is not specified or None, any whitespace string is a separator.  \nmaxsplitint, optional\n\n\nIf maxsplit is given, at most maxsplit splits are done.    Returns \n \noutndarray\n\n\nArray of list objects      See also  \nstr.split, rsplit","title":"numpy.reference.generated.numpy.char.split"},{"text":"numpy.char.chararray.split method   char.chararray.split(sep=None, maxsplit=None)[source]\n \nFor each element in self, return a list of the words in the string, using sep as the delimiter string.  See also  char.split","title":"numpy.reference.generated.numpy.char.chararray.split"},{"text":"numpy.char.chararray.rsplit method   char.chararray.rsplit(sep=None, maxsplit=None)[source]\n \nFor each element in self, return a list of the words in the string, using sep as the delimiter string.  See also  char.rsplit","title":"numpy.reference.generated.numpy.char.chararray.rsplit"},{"text":"numpy.chararray.split method   chararray.split(sep=None, maxsplit=None)[source]\n \nFor each element in self, return a list of the words in the string, using sep as the delimiter string.  See also  char.split","title":"numpy.reference.generated.numpy.chararray.split"}]}
{"task_id":17977584,"prompt":"def f_17977584():\n\treturn ","suffix":"","canonical_solution":"open('Output.txt', 'a')","test_start":"\ndef check(candidate):","test":["\n    f = candidate()\n    assert str(f.__class__) == \"<class '_io.TextIOWrapper'>\"\n    assert f.name == 'Output.txt'\n    assert f.mode == 'a'\n"],"entry_point":"f_17977584","intent":"open a file `Output.txt` in append mode","library":[],"docs":[]}
{"task_id":22676,"prompt":"def f_22676():\n\treturn ","suffix":"","canonical_solution":"urllib.request.urlretrieve('https:\/\/github.com\/zorazrw\/multilingual-conala\/blob\/master\/dataset\/test\/es_test.json', 'mp3.mp3')","test_start":"\nimport urllib \n\ndef check(candidate):","test":["\n    results = candidate()\n    assert len(results) == 2\n    assert results[0] == \"mp3.mp3\"\n    assert results[1].values()[0] == \"GitHub.com\"\n"],"entry_point":"f_22676","intent":"download a file \"http:\/\/www.example.com\/songs\/mp3.mp3\" over HTTP and save to \"mp3.mp3\"","library":["urllib"],"docs":[{"text":"exception http.client.UnimplementedFileMode  \nA subclass of HTTPException.","title":"python.library.http.client#http.client.UnimplementedFileMode"},{"text":"rfile  \nAn io.BufferedIOBase input stream, ready to read from the start of the optional input data.","title":"python.library.http.server#http.server.BaseHTTPRequestHandler.rfile"},{"text":"FieldFile.url","title":"django.ref.models.fields#django.db.models.fields.files.FieldFile.url"},{"text":"class urllib.request.FTPHandler  \nOpen FTP URLs.","title":"python.library.urllib.request#urllib.request.FTPHandler"},{"text":"HttpRequest.readline()","title":"django.ref.request-response#django.http.HttpRequest.readline"},{"text":"HttpRequest.readlines()","title":"django.ref.request-response#django.http.HttpRequest.readlines"},{"text":"_save(name, content)","title":"django.howto.custom-file-storage#django.core.files.storage._save"},{"text":"HttpRequest.read(size=None)","title":"django.ref.request-response#django.http.HttpRequest.read"},{"text":"HTTPResponse.fileno()  \nReturn the fileno of the underlying socket.","title":"python.library.http.client#http.client.HTTPResponse.fileno"},{"text":"urls.staticfiles_urlpatterns()","title":"django.ref.contrib.staticfiles#django.contrib.staticfiles.urls.staticfiles_urlpatterns"}]}
{"task_id":22676,"prompt":"def f_22676(url):\n\t","suffix":"\n\treturn html","canonical_solution":"html = urllib.request.urlopen(url).read()","test_start":"\nimport urllib \n\ndef check(candidate):","test":["\n    html = candidate(\"https:\/\/github.com\/zorazrw\/multilingual-conala\/blob\/master\/dataset\/test\/es_test.json\")\n    assert b\"zorazrw\/multilingual-conala\" in html\n"],"entry_point":"f_22676","intent":"download a file 'http:\/\/www.example.com\/' over HTTP","library":["urllib"],"docs":[{"text":"exception http.client.UnimplementedFileMode  \nA subclass of HTTPException.","title":"python.library.http.client#http.client.UnimplementedFileMode"},{"text":"HTTPResponse.fileno()  \nReturn the fileno of the underlying socket.","title":"python.library.http.client#http.client.HTTPResponse.fileno"},{"text":"HTTPResponse.geturl()  \n Deprecated since version 3.9: Deprecated in favor of url.","title":"python.library.http.client#http.client.HTTPResponse.geturl"},{"text":"rfile  \nAn io.BufferedIOBase input stream, ready to read from the start of the optional input data.","title":"python.library.http.server#http.server.BaseHTTPRequestHandler.rfile"},{"text":"HttpRequest.readline()","title":"django.ref.request-response#django.http.HttpRequest.readline"},{"text":"class urllib.request.FileHandler  \nOpen local files.","title":"python.library.urllib.request#urllib.request.FileHandler"},{"text":"class urllib.request.FTPHandler  \nOpen FTP URLs.","title":"python.library.urllib.request#urllib.request.FTPHandler"},{"text":"HttpRequest.read(size=None)","title":"django.ref.request-response#django.http.HttpRequest.read"},{"text":"HttpRequest.readlines()","title":"django.ref.request-response#django.http.HttpRequest.readlines"},{"text":"exception http.client.UnknownProtocol  \nA subclass of HTTPException.","title":"python.library.http.client#http.client.UnknownProtocol"}]}
{"task_id":22676,"prompt":"def f_22676(url):\n\treturn ","suffix":"","canonical_solution":"requests.get(url)","test_start":"\nimport requests \n\ndef check(candidate):","test":["\n    assert candidate(\"https:\/\/github.com\/\").url == \"https:\/\/github.com\/\"\n","\n    assert candidate(\"https:\/\/google.com\/\").url == \"https:\/\/www.google.com\/\"\n"],"entry_point":"f_22676","intent":"download a file `url` over HTTP","library":["requests"],"docs":[]}
{"task_id":22676,"prompt":"def f_22676(url):\n\t","suffix":"\n\treturn ","canonical_solution":"\n\tresponse = requests.get(url, stream=True)\n\twith open('10MB', 'wb') as handle:\n\t\tfor data in response.iter_content():\n\t\t\thandle.write(data)\n\t","test_start":"\nimport requests \n\ndef check(candidate):","test":["\n    candidate(\"https:\/\/github.com\/\")\n    with open(\"10MB\", 'rb') as fr: \n        all_data = [data for data in fr]\n    assert all_data[: 2] == [b'\\n', b'\\n']\n"],"entry_point":"f_22676","intent":"download a file `url` over HTTP and save to \"10MB\"","library":["requests"],"docs":[]}
{"task_id":15405636,"prompt":"def f_15405636(parser):\n\treturn ","suffix":"","canonical_solution":"parser.add_argument('--version', action='version', version='%(prog)s 2.0')","test_start":"\nimport argparse \n\ndef check(candidate):","test":["\n    parser = argparse.ArgumentParser()\n    output = candidate(parser)\n    assert output.option_strings == ['--version']\n    assert output.dest == 'version'\n    assert output.nargs == 0\n"],"entry_point":"f_15405636","intent":"argparse add argument with flag '--version' and version action of '%(prog)s 2.0' to parser `parser`","library":["argparse"],"docs":[{"text":"class argparse.Action(option_strings, dest, nargs=None, const=None, default=None, type=None, choices=None, required=False, help=None, metavar=None)","title":"python.library.argparse#argparse.Action"},{"text":"ArgumentParser.add_argument(name or flags...[, action][, nargs][, const][, default][, type][, choices][, required][, help][, metavar][, dest])  \nDefine how a single command-line argument should be parsed. Each parameter has its own more detailed description below, but in short they are:  \nname or flags - Either a name or a list of option strings, e.g. foo or -f, --foo. \naction - The basic type of action to be taken when this argument is encountered at the command line. \nnargs - The number of command-line arguments that should be consumed. \nconst - A constant value required by some action and nargs selections. \ndefault - The value produced if the argument is absent from the command line and if it is absent from the namespace object. \ntype - The type to which the command-line argument should be converted. \nchoices - A container of the allowable values for the argument. \nrequired - Whether or not the command-line option may be omitted (optionals only). \nhelp - A brief description of what the argument does. \nmetavar - A name for the argument in usage messages. \ndest - The name of the attribute to be added to the object returned by parse_args().","title":"python.library.argparse#argparse.ArgumentParser.add_argument"},{"text":"ArgumentParser.parse_intermixed_args(args=None, namespace=None)","title":"python.library.argparse#argparse.ArgumentParser.parse_intermixed_args"},{"text":"ArgumentParser.parse_known_args(args=None, namespace=None)","title":"python.library.argparse#argparse.ArgumentParser.parse_known_args"},{"text":"ArgumentParser.add_mutually_exclusive_group(required=False)  \nCreate a mutually exclusive group. argparse will make sure that only one of the arguments in the mutually exclusive group was present on the command line: >>> parser = argparse.ArgumentParser(prog='PROG')\n>>> group = parser.add_mutually_exclusive_group()\n>>> group.add_argument('--foo', action='store_true')\n>>> group.add_argument('--bar', action='store_false')\n>>> parser.parse_args(['--foo'])\nNamespace(bar=True, foo=True)\n>>> parser.parse_args(['--bar'])\nNamespace(bar=False, foo=False)\n>>> parser.parse_args(['--foo', '--bar'])\nusage: PROG [-h] [--foo | --bar]\nPROG: error: argument --bar: not allowed with argument --foo\n The add_mutually_exclusive_group() method also accepts a required argument, to indicate that at least one of the mutually exclusive arguments is required: >>> parser = argparse.ArgumentParser(prog='PROG')\n>>> group = parser.add_mutually_exclusive_group(required=True)\n>>> group.add_argument('--foo', action='store_true')\n>>> group.add_argument('--bar', action='store_false')\n>>> parser.parse_args([])\nusage: PROG [-h] (--foo | --bar)\nPROG: error: one of the arguments --foo --bar is required\n Note that currently mutually exclusive argument groups do not support the title and description arguments of add_argument_group().","title":"python.library.argparse#argparse.ArgumentParser.add_mutually_exclusive_group"},{"text":"class argparse.RawDescriptionHelpFormatter  \nclass argparse.RawTextHelpFormatter  \nclass argparse.ArgumentDefaultsHelpFormatter  \nclass argparse.MetavarTypeHelpFormatter","title":"python.library.argparse#argparse.RawTextHelpFormatter"},{"text":"ArgumentParser.error(message)  \nThis method prints a usage message including the message to the standard error and terminates the program with a status code of 2.","title":"python.library.argparse#argparse.ArgumentParser.error"},{"text":"ArgumentParser.add_argument_group(title=None, description=None)  \nBy default, ArgumentParser groups command-line arguments into \u201cpositional arguments\u201d and \u201coptional arguments\u201d when displaying help messages. When there is a better conceptual grouping of arguments than this default one, appropriate groups can be created using the add_argument_group() method: >>> parser = argparse.ArgumentParser(prog='PROG', add_help=False)\n>>> group = parser.add_argument_group('group')\n>>> group.add_argument('--foo', help='foo help')\n>>> group.add_argument('bar', help='bar help')\n>>> parser.print_help()\nusage: PROG [--foo FOO] bar\n\ngroup:\n  bar    bar help\n  --foo FOO  foo help\n The add_argument_group() method returns an argument group object which has an add_argument() method just like a regular ArgumentParser. When an argument is added to the group, the parser treats it just like a normal argument, but displays the argument in a separate group for help messages. The add_argument_group() method accepts title and description arguments which can be used to customize this display: >>> parser = argparse.ArgumentParser(prog='PROG', add_help=False)\n>>> group1 = parser.add_argument_group('group1', 'group1 description')\n>>> group1.add_argument('foo', help='foo help')\n>>> group2 = parser.add_argument_group('group2', 'group2 description')\n>>> group2.add_argument('--bar', help='bar help')\n>>> parser.print_help()\nusage: PROG [--bar BAR] foo\n\ngroup1:\n  group1 description\n\n  foo    foo help\n\ngroup2:\n  group2 description\n\n  --bar BAR  bar help\n Note that any arguments not in your user-defined groups will end up back in the usual \u201cpositional arguments\u201d and \u201coptional arguments\u201d sections.","title":"python.library.argparse#argparse.ArgumentParser.add_argument_group"},{"text":"ArgumentParser.add_subparsers([title][, description][, prog][, parser_class][, action][, option_string][, dest][, required][, help][, metavar])  \nMany programs split up their functionality into a number of sub-commands, for example, the svn program can invoke sub-commands like svn\ncheckout, svn update, and svn commit. Splitting up functionality this way can be a particularly good idea when a program performs several different functions which require different kinds of command-line arguments. ArgumentParser supports the creation of such sub-commands with the add_subparsers() method. The add_subparsers() method is normally called with no arguments and returns a special action object. This object has a single method, add_parser(), which takes a command name and any ArgumentParser constructor arguments, and returns an ArgumentParser object that can be modified as usual. Description of parameters:  title - title for the sub-parser group in help output; by default \u201csubcommands\u201d if description is provided, otherwise uses title for positional arguments description - description for the sub-parser group in help output, by default None\n prog - usage information that will be displayed with sub-command help, by default the name of the program and any positional arguments before the subparser argument parser_class - class which will be used to create sub-parser instances, by default the class of the current parser (e.g. ArgumentParser) \naction - the basic type of action to be taken when this argument is encountered at the command line \ndest - name of the attribute under which sub-command name will be stored; by default None and no value is stored \nrequired - Whether or not a subcommand must be provided, by default False (added in 3.7) \nhelp - help for sub-parser group in help output, by default None\n \nmetavar - string presenting available sub-commands in help; by default it is None and presents sub-commands in form {cmd1, cmd2, ..}  Some example usage: >>> # create the top-level parser\n>>> parser = argparse.ArgumentParser(prog='PROG')\n>>> parser.add_argument('--foo', action='store_true', help='foo help')\n>>> subparsers = parser.add_subparsers(help='sub-command help')\n>>>\n>>> # create the parser for the \"a\" command\n>>> parser_a = subparsers.add_parser('a', help='a help')\n>>> parser_a.add_argument('bar', type=int, help='bar help')\n>>>\n>>> # create the parser for the \"b\" command\n>>> parser_b = subparsers.add_parser('b', help='b help')\n>>> parser_b.add_argument('--baz', choices='XYZ', help='baz help')\n>>>\n>>> # parse some argument lists\n>>> parser.parse_args(['a', '12'])\nNamespace(bar=12, foo=False)\n>>> parser.parse_args(['--foo', 'b', '--baz', 'Z'])\nNamespace(baz='Z', foo=True)\n Note that the object returned by parse_args() will only contain attributes for the main parser and the subparser that was selected by the command line (and not any other subparsers). So in the example above, when the a command is specified, only the foo and bar attributes are present, and when the b command is specified, only the foo and baz attributes are present. Similarly, when a help message is requested from a subparser, only the help for that particular parser will be printed. The help message will not include parent parser or sibling parser messages. (A help message for each subparser command, however, can be given by supplying the help= argument to add_parser() as above.) >>> parser.parse_args(['--help'])\nusage: PROG [-h] [--foo] {a,b} ...\n\npositional arguments:\n  {a,b}   sub-command help\n    a     a help\n    b     b help\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --foo   foo help\n\n>>> parser.parse_args(['a', '--help'])\nusage: PROG a [-h] bar\n\npositional arguments:\n  bar     bar help\n\noptional arguments:\n  -h, --help  show this help message and exit\n\n>>> parser.parse_args(['b', '--help'])\nusage: PROG b [-h] [--baz {X,Y,Z}]\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --baz {X,Y,Z}  baz help\n The add_subparsers() method also supports title and description keyword arguments. When either is present, the subparser\u2019s commands will appear in their own group in the help output. For example: >>> parser = argparse.ArgumentParser()\n>>> subparsers = parser.add_subparsers(title='subcommands',\n...                                    description='valid subcommands',\n...                                    help='additional help')\n>>> subparsers.add_parser('foo')\n>>> subparsers.add_parser('bar')\n>>> parser.parse_args(['-h'])\nusage:  [-h] {foo,bar} ...\n\noptional arguments:\n  -h, --help  show this help message and exit\n\nsubcommands:\n  valid subcommands\n\n  {foo,bar}   additional help\n Furthermore, add_parser supports an additional aliases argument, which allows multiple strings to refer to the same subparser. This example, like svn, aliases co as a shorthand for checkout: >>> parser = argparse.ArgumentParser()\n>>> subparsers = parser.add_subparsers()\n>>> checkout = subparsers.add_parser('checkout', aliases=['co'])\n>>> checkout.add_argument('foo')\n>>> parser.parse_args(['co', 'bar'])\nNamespace(foo='bar')\n One particularly effective way of handling sub-commands is to combine the use of the add_subparsers() method with calls to set_defaults() so that each subparser knows which Python function it should execute. For example: >>> # sub-command functions\n>>> def foo(args):\n...     print(args.x * args.y)\n...\n>>> def bar(args):\n...     print('((%s))' % args.z)\n...\n>>> # create the top-level parser\n>>> parser = argparse.ArgumentParser()\n>>> subparsers = parser.add_subparsers()\n>>>\n>>> # create the parser for the \"foo\" command\n>>> parser_foo = subparsers.add_parser('foo')\n>>> parser_foo.add_argument('-x', type=int, default=1)\n>>> parser_foo.add_argument('y', type=float)\n>>> parser_foo.set_defaults(func=foo)\n>>>\n>>> # create the parser for the \"bar\" command\n>>> parser_bar = subparsers.add_parser('bar')\n>>> parser_bar.add_argument('z')\n>>> parser_bar.set_defaults(func=bar)\n>>>\n>>> # parse the args and call whatever function was selected\n>>> args = parser.parse_args('foo 1 -x 2'.split())\n>>> args.func(args)\n2.0\n>>>\n>>> # parse the args and call whatever function was selected\n>>> args = parser.parse_args('bar XYZYX'.split())\n>>> args.func(args)\n((XYZYX))\n This way, you can let parse_args() do the job of calling the appropriate function after argument parsing is complete. Associating functions with actions like this is typically the easiest way to handle the different actions for each of your subparsers. However, if it is necessary to check the name of the subparser that was invoked, the dest keyword argument to the add_subparsers() call will work: >>> parser = argparse.ArgumentParser()\n>>> subparsers = parser.add_subparsers(dest='subparser_name')\n>>> subparser1 = subparsers.add_parser('1')\n>>> subparser1.add_argument('-x')\n>>> subparser2 = subparsers.add_parser('2')\n>>> subparser2.add_argument('y')\n>>> parser.parse_args(['2', 'frobble'])\nNamespace(subparser_name='2', y='frobble')\n  Changed in version 3.7: New required keyword argument.","title":"python.library.argparse#argparse.ArgumentParser.add_subparsers"},{"text":"class argparse.RawDescriptionHelpFormatter  \nclass argparse.RawTextHelpFormatter  \nclass argparse.ArgumentDefaultsHelpFormatter  \nclass argparse.MetavarTypeHelpFormatter","title":"python.library.argparse#argparse.ArgumentDefaultsHelpFormatter"}]}
{"task_id":17665809,"prompt":"def f_17665809(d):\n\treturn ","suffix":"","canonical_solution":"{i: d[i] for i in d if i != 'c'}","test_start":"\ndef check(candidate):","test":["\n    assert candidate({'a': 1 , 'b': 2, 'c': 3}) == {'a': 1 , 'b': 2}\n","\n    assert candidate({'c': None}) == {}\n","\n    assert candidate({'a': 1 , 'b': 2, 'c': 3}) != {'a': 1 , 'b': 2, 'c': 3}\n","\n    assert candidate({'c': 1, 'cc': 2, 'ccc':3}) == {'cc': 2, 'ccc':3}\n","\n    assert 'c' not in candidate({'c':i for i in range(10)})\n"],"entry_point":"f_17665809","intent":"remove key 'c' from dictionary `d`","library":[],"docs":[]}
{"task_id":41861705,"prompt":"def f_41861705(split_df, csv_df):\n\treturn ","suffix":"","canonical_solution":"pd.merge(split_df, csv_df, on=['key'], suffixes=('_left', '_right'))","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    split_df = pd.DataFrame({'key': ['foo', 'bar'], 'value': [1, 2]})\n    csv_df = pd.DataFrame({'key': ['foo', 'baz'], 'value': [3, 4]})\n    result = pd.DataFrame({'key': ['foo'], 'value_left': [1],'value_right': [3]})\n    assert all(candidate(csv_df, split_df) == result)\n"],"entry_point":"f_41861705","intent":"Create new DataFrame object by merging columns \"key\" of  dataframes `split_df` and `csv_df` and rename the columns from dataframes `split_df` and `csv_df` with suffix `_left` and `_right` respectively","library":["pandas"],"docs":[{"text":"pandas.merge_ordered   pandas.merge_ordered(left, right, on=None, left_on=None, right_on=None, left_by=None, right_by=None, fill_method=None, suffixes=('_x', '_y'), how='outer')[source]\n \nPerform a merge for ordered data with optional filling\/interpolation. Designed for ordered data like time series data. Optionally perform group-wise merge (see examples).  Parameters \n \nleft:DataFrame\n\n\nright:DataFrame\n\n\non:label or list\n\n\nField names to join on. Must be found in both DataFrames.  \nleft_on:label or list, or array-like\n\n\nField names to join on in left DataFrame. Can be a vector or list of vectors of the length of the DataFrame to use a particular vector as the join key instead of columns.  \nright_on:label or list, or array-like\n\n\nField names to join on in right DataFrame or vector\/list of vectors per left_on docs.  \nleft_by:column name or list of column names\n\n\nGroup left DataFrame by group columns and merge piece by piece with right DataFrame.  \nright_by:column name or list of column names\n\n\nGroup right DataFrame by group columns and merge piece by piece with left DataFrame.  \nfill_method:{\u2018ffill\u2019, None}, default None\n\n\nInterpolation method for data.  \nsuffixes:list-like, default is (\u201c_x\u201d, \u201c_y\u201d)\n\n\nA length-2 sequence where each element is optionally a string indicating the suffix to add to overlapping column names in left and right respectively. Pass a value of None instead of a string to indicate that the column name from left or right should be left as-is, with no suffix. At least one of the values must not be None.  Changed in version 0.25.0.   \nhow:{\u2018left\u2019, \u2018right\u2019, \u2018outer\u2019, \u2018inner\u2019}, default \u2018outer\u2019\n\n\n left: use only keys from left frame (SQL: left outer join) right: use only keys from right frame (SQL: right outer join) outer: use union of keys from both frames (SQL: full outer join) inner: use intersection of keys from both frames (SQL: inner join).     Returns \n DataFrame\n\nThe merged DataFrame output type will the be same as \u2018left\u2019, if it is a subclass of DataFrame.      See also  merge\n\nMerge with a database-style join.  merge_asof\n\nMerge on nearest keys.    Examples \n>>> df1 = pd.DataFrame(\n...     {\n...         \"key\": [\"a\", \"c\", \"e\", \"a\", \"c\", \"e\"],\n...         \"lvalue\": [1, 2, 3, 1, 2, 3],\n...         \"group\": [\"a\", \"a\", \"a\", \"b\", \"b\", \"b\"]\n...     }\n... )\n>>> df1\n      key  lvalue group\n0   a       1     a\n1   c       2     a\n2   e       3     a\n3   a       1     b\n4   c       2     b\n5   e       3     b\n  \n>>> df2 = pd.DataFrame({\"key\": [\"b\", \"c\", \"d\"], \"rvalue\": [1, 2, 3]})\n>>> df2\n      key  rvalue\n0   b       1\n1   c       2\n2   d       3\n  \n>>> merge_ordered(df1, df2, fill_method=\"ffill\", left_by=\"group\")\n  key  lvalue group  rvalue\n0   a       1     a     NaN\n1   b       1     a     1.0\n2   c       2     a     2.0\n3   d       2     a     3.0\n4   e       3     a     3.0\n5   a       1     b     NaN\n6   b       1     b     1.0\n7   c       2     b     2.0\n8   d       2     b     3.0\n9   e       3     b     3.0","title":"pandas.reference.api.pandas.merge_ordered"},{"text":"pandas.DataFrame.merge   DataFrame.merge(right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)[source]\n \nMerge DataFrame or named Series objects with a database-style join. A named Series object is treated as a DataFrame with a single named column. The join is done on columns or indexes. If joining columns on columns, the DataFrame indexes will be ignored. Otherwise if joining indexes on indexes or indexes on a column or columns, the index will be passed on. When performing a cross merge, no column specifications to merge on are allowed.  Warning If both key columns contain rows where the key is a null value, those rows will be matched against each other. This is different from usual SQL join behaviour and can lead to unexpected results.   Parameters \n \nright:DataFrame or named Series\n\n\nObject to merge with.  \nhow:{\u2018left\u2019, \u2018right\u2019, \u2018outer\u2019, \u2018inner\u2019, \u2018cross\u2019}, default \u2018inner\u2019\n\n\nType of merge to be performed.  left: use only keys from left frame, similar to a SQL left outer join; preserve key order. right: use only keys from right frame, similar to a SQL right outer join; preserve key order. outer: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically. inner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys. \ncross: creates the cartesian product from both frames, preserves the order of the left keys.  New in version 1.2.0.     \non:label or list\n\n\nColumn or index level names to join on. These must be found in both DataFrames. If on is None and not merging on indexes then this defaults to the intersection of the columns in both DataFrames.  \nleft_on:label or list, or array-like\n\n\nColumn or index level names to join on in the left DataFrame. Can also be an array or list of arrays of the length of the left DataFrame. These arrays are treated as if they are columns.  \nright_on:label or list, or array-like\n\n\nColumn or index level names to join on in the right DataFrame. Can also be an array or list of arrays of the length of the right DataFrame. These arrays are treated as if they are columns.  \nleft_index:bool, default False\n\n\nUse the index from the left DataFrame as the join key(s). If it is a MultiIndex, the number of keys in the other DataFrame (either the index or a number of columns) must match the number of levels.  \nright_index:bool, default False\n\n\nUse the index from the right DataFrame as the join key. Same caveats as left_index.  \nsort:bool, default False\n\n\nSort the join keys lexicographically in the result DataFrame. If False, the order of the join keys depends on the join type (how keyword).  \nsuffixes:list-like, default is (\u201c_x\u201d, \u201c_y\u201d)\n\n\nA length-2 sequence where each element is optionally a string indicating the suffix to add to overlapping column names in left and right respectively. Pass a value of None instead of a string to indicate that the column name from left or right should be left as-is, with no suffix. At least one of the values must not be None.  \ncopy:bool, default True\n\n\nIf False, avoid copy if possible.  \nindicator:bool or str, default False\n\n\nIf True, adds a column to the output DataFrame called \u201c_merge\u201d with information on the source of each row. The column can be given a different name by providing a string argument. The column will have a Categorical type with the value of \u201cleft_only\u201d for observations whose merge key only appears in the left DataFrame, \u201cright_only\u201d for observations whose merge key only appears in the right DataFrame, and \u201cboth\u201d if the observation\u2019s merge key is found in both DataFrames.  \nvalidate:str, optional\n\n\nIf specified, checks if merge is of specified type.  \u201cone_to_one\u201d or \u201c1:1\u201d: check if merge keys are unique in both left and right datasets. \u201cone_to_many\u201d or \u201c1:m\u201d: check if merge keys are unique in left dataset. \u201cmany_to_one\u201d or \u201cm:1\u201d: check if merge keys are unique in right dataset. \u201cmany_to_many\u201d or \u201cm:m\u201d: allowed, but does not result in checks.     Returns \n DataFrame\n\nA DataFrame of the two merged objects.      See also  merge_ordered\n\nMerge with optional filling\/interpolation.  merge_asof\n\nMerge on nearest keys.  DataFrame.join\n\nSimilar method using indices.    Notes Support for specifying index levels as the on, left_on, and right_on parameters was added in version 0.23.0 Support for merging named Series objects was added in version 0.24.0 Examples \n>>> df1 = pd.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [1, 2, 3, 5]})\n>>> df2 = pd.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [5, 6, 7, 8]})\n>>> df1\n    lkey value\n0   foo      1\n1   bar      2\n2   baz      3\n3   foo      5\n>>> df2\n    rkey value\n0   foo      5\n1   bar      6\n2   baz      7\n3   foo      8\n  Merge df1 and df2 on the lkey and rkey columns. The value columns have the default suffixes, _x and _y, appended. \n>>> df1.merge(df2, left_on='lkey', right_on='rkey')\n  lkey  value_x rkey  value_y\n0  foo        1  foo        5\n1  foo        1  foo        8\n2  foo        5  foo        5\n3  foo        5  foo        8\n4  bar        2  bar        6\n5  baz        3  baz        7\n  Merge DataFrames df1 and df2 with specified left and right suffixes appended to any overlapping columns. \n>>> df1.merge(df2, left_on='lkey', right_on='rkey',\n...           suffixes=('_left', '_right'))\n  lkey  value_left rkey  value_right\n0  foo           1  foo            5\n1  foo           1  foo            8\n2  foo           5  foo            5\n3  foo           5  foo            8\n4  bar           2  bar            6\n5  baz           3  baz            7\n  Merge DataFrames df1 and df2, but raise an exception if the DataFrames have any overlapping columns. \n>>> df1.merge(df2, left_on='lkey', right_on='rkey', suffixes=(False, False))\nTraceback (most recent call last):\n...\nValueError: columns overlap but no suffix specified:\n    Index(['value'], dtype='object')\n  \n>>> df1 = pd.DataFrame({'a': ['foo', 'bar'], 'b': [1, 2]})\n>>> df2 = pd.DataFrame({'a': ['foo', 'baz'], 'c': [3, 4]})\n>>> df1\n      a  b\n0   foo  1\n1   bar  2\n>>> df2\n      a  c\n0   foo  3\n1   baz  4\n  \n>>> df1.merge(df2, how='inner', on='a')\n      a  b  c\n0   foo  1  3\n  \n>>> df1.merge(df2, how='left', on='a')\n      a  b  c\n0   foo  1  3.0\n1   bar  2  NaN\n  \n>>> df1 = pd.DataFrame({'left': ['foo', 'bar']})\n>>> df2 = pd.DataFrame({'right': [7, 8]})\n>>> df1\n    left\n0   foo\n1   bar\n>>> df2\n    right\n0   7\n1   8\n  \n>>> df1.merge(df2, how='cross')\n   left  right\n0   foo      7\n1   foo      8\n2   bar      7\n3   bar      8","title":"pandas.reference.api.pandas.dataframe.merge"},{"text":"pandas.merge   pandas.merge(left, right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)[source]\n \nMerge DataFrame or named Series objects with a database-style join. A named Series object is treated as a DataFrame with a single named column. The join is done on columns or indexes. If joining columns on columns, the DataFrame indexes will be ignored. Otherwise if joining indexes on indexes or indexes on a column or columns, the index will be passed on. When performing a cross merge, no column specifications to merge on are allowed.  Warning If both key columns contain rows where the key is a null value, those rows will be matched against each other. This is different from usual SQL join behaviour and can lead to unexpected results.   Parameters \n \nleft:DataFrame\n\n\nright:DataFrame or named Series\n\n\nObject to merge with.  \nhow:{\u2018left\u2019, \u2018right\u2019, \u2018outer\u2019, \u2018inner\u2019, \u2018cross\u2019}, default \u2018inner\u2019\n\n\nType of merge to be performed.  left: use only keys from left frame, similar to a SQL left outer join; preserve key order. right: use only keys from right frame, similar to a SQL right outer join; preserve key order. outer: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically. inner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys. \ncross: creates the cartesian product from both frames, preserves the order of the left keys.  New in version 1.2.0.     \non:label or list\n\n\nColumn or index level names to join on. These must be found in both DataFrames. If on is None and not merging on indexes then this defaults to the intersection of the columns in both DataFrames.  \nleft_on:label or list, or array-like\n\n\nColumn or index level names to join on in the left DataFrame. Can also be an array or list of arrays of the length of the left DataFrame. These arrays are treated as if they are columns.  \nright_on:label or list, or array-like\n\n\nColumn or index level names to join on in the right DataFrame. Can also be an array or list of arrays of the length of the right DataFrame. These arrays are treated as if they are columns.  \nleft_index:bool, default False\n\n\nUse the index from the left DataFrame as the join key(s). If it is a MultiIndex, the number of keys in the other DataFrame (either the index or a number of columns) must match the number of levels.  \nright_index:bool, default False\n\n\nUse the index from the right DataFrame as the join key. Same caveats as left_index.  \nsort:bool, default False\n\n\nSort the join keys lexicographically in the result DataFrame. If False, the order of the join keys depends on the join type (how keyword).  \nsuffixes:list-like, default is (\u201c_x\u201d, \u201c_y\u201d)\n\n\nA length-2 sequence where each element is optionally a string indicating the suffix to add to overlapping column names in left and right respectively. Pass a value of None instead of a string to indicate that the column name from left or right should be left as-is, with no suffix. At least one of the values must not be None.  \ncopy:bool, default True\n\n\nIf False, avoid copy if possible.  \nindicator:bool or str, default False\n\n\nIf True, adds a column to the output DataFrame called \u201c_merge\u201d with information on the source of each row. The column can be given a different name by providing a string argument. The column will have a Categorical type with the value of \u201cleft_only\u201d for observations whose merge key only appears in the left DataFrame, \u201cright_only\u201d for observations whose merge key only appears in the right DataFrame, and \u201cboth\u201d if the observation\u2019s merge key is found in both DataFrames.  \nvalidate:str, optional\n\n\nIf specified, checks if merge is of specified type.  \u201cone_to_one\u201d or \u201c1:1\u201d: check if merge keys are unique in both left and right datasets. \u201cone_to_many\u201d or \u201c1:m\u201d: check if merge keys are unique in left dataset. \u201cmany_to_one\u201d or \u201cm:1\u201d: check if merge keys are unique in right dataset. \u201cmany_to_many\u201d or \u201cm:m\u201d: allowed, but does not result in checks.     Returns \n DataFrame\n\nA DataFrame of the two merged objects.      See also  merge_ordered\n\nMerge with optional filling\/interpolation.  merge_asof\n\nMerge on nearest keys.  DataFrame.join\n\nSimilar method using indices.    Notes Support for specifying index levels as the on, left_on, and right_on parameters was added in version 0.23.0 Support for merging named Series objects was added in version 0.24.0 Examples \n>>> df1 = pd.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [1, 2, 3, 5]})\n>>> df2 = pd.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [5, 6, 7, 8]})\n>>> df1\n    lkey value\n0   foo      1\n1   bar      2\n2   baz      3\n3   foo      5\n>>> df2\n    rkey value\n0   foo      5\n1   bar      6\n2   baz      7\n3   foo      8\n  Merge df1 and df2 on the lkey and rkey columns. The value columns have the default suffixes, _x and _y, appended. \n>>> df1.merge(df2, left_on='lkey', right_on='rkey')\n  lkey  value_x rkey  value_y\n0  foo        1  foo        5\n1  foo        1  foo        8\n2  foo        5  foo        5\n3  foo        5  foo        8\n4  bar        2  bar        6\n5  baz        3  baz        7\n  Merge DataFrames df1 and df2 with specified left and right suffixes appended to any overlapping columns. \n>>> df1.merge(df2, left_on='lkey', right_on='rkey',\n...           suffixes=('_left', '_right'))\n  lkey  value_left rkey  value_right\n0  foo           1  foo            5\n1  foo           1  foo            8\n2  foo           5  foo            5\n3  foo           5  foo            8\n4  bar           2  bar            6\n5  baz           3  baz            7\n  Merge DataFrames df1 and df2, but raise an exception if the DataFrames have any overlapping columns. \n>>> df1.merge(df2, left_on='lkey', right_on='rkey', suffixes=(False, False))\nTraceback (most recent call last):\n...\nValueError: columns overlap but no suffix specified:\n    Index(['value'], dtype='object')\n  \n>>> df1 = pd.DataFrame({'a': ['foo', 'bar'], 'b': [1, 2]})\n>>> df2 = pd.DataFrame({'a': ['foo', 'baz'], 'c': [3, 4]})\n>>> df1\n      a  b\n0   foo  1\n1   bar  2\n>>> df2\n      a  c\n0   foo  3\n1   baz  4\n  \n>>> df1.merge(df2, how='inner', on='a')\n      a  b  c\n0   foo  1  3\n  \n>>> df1.merge(df2, how='left', on='a')\n      a  b  c\n0   foo  1  3.0\n1   bar  2  NaN\n  \n>>> df1 = pd.DataFrame({'left': ['foo', 'bar']})\n>>> df2 = pd.DataFrame({'right': [7, 8]})\n>>> df1\n    left\n0   foo\n1   bar\n>>> df2\n    right\n0   7\n1   8\n  \n>>> df1.merge(df2, how='cross')\n   left  right\n0   foo      7\n1   foo      8\n2   bar      7\n3   bar      8","title":"pandas.reference.api.pandas.merge"},{"text":"Input\/output  Pickling       \nread_pickle(filepath_or_buffer[, ...]) Load pickled pandas object (or any object) from file.  \nDataFrame.to_pickle(path[, compression, ...]) Pickle (serialize) object to file.      Flat file       \nread_table(filepath_or_buffer[, sep, ...]) Read general delimited file into DataFrame.  \nread_csv(filepath_or_buffer[, sep, ...]) Read a comma-separated values (csv) file into DataFrame.  \nDataFrame.to_csv([path_or_buf, sep, na_rep, ...]) Write object to a comma-separated values (csv) file.  \nread_fwf(filepath_or_buffer[, colspecs, ...]) Read a table of fixed-width formatted lines into DataFrame.      Clipboard       \nread_clipboard([sep]) Read text from clipboard and pass to read_csv.  \nDataFrame.to_clipboard([excel, sep]) Copy object to the system clipboard.      Excel       \nread_excel(io[, sheet_name, header, names, ...]) Read an Excel file into a pandas DataFrame.  \nDataFrame.to_excel(excel_writer[, ...]) Write object to an Excel sheet.  \nExcelFile.parse([sheet_name, header, names, ...]) Parse specified sheet(s) into a DataFrame.          \nStyler.to_excel(excel_writer[, sheet_name, ...]) Write Styler to an Excel sheet.          \nExcelWriter(path[, engine, date_format, ...]) Class for writing DataFrame objects into excel sheets.      JSON       \nread_json([path_or_buf, orient, typ, dtype, ...]) Convert a JSON string to pandas object.  \njson_normalize(data[, record_path, meta, ...]) Normalize semi-structured JSON data into a flat table.  \nDataFrame.to_json([path_or_buf, orient, ...]) Convert the object to a JSON string.          \nbuild_table_schema(data[, index, ...]) Create a Table schema from data.      HTML       \nread_html(io[, match, flavor, header, ...]) Read HTML tables into a list of DataFrame objects.  \nDataFrame.to_html([buf, columns, col_space, ...]) Render a DataFrame as an HTML table.          \nStyler.to_html([buf, table_uuid, ...]) Write Styler to a file, buffer or string in HTML-CSS format.      XML       \nread_xml(path_or_buffer[, xpath, ...]) Read XML document into a DataFrame object.  \nDataFrame.to_xml([path_or_buffer, index, ...]) Render a DataFrame to an XML document.      Latex       \nDataFrame.to_latex([buf, columns, ...]) Render object to a LaTeX tabular, longtable, or nested table.          \nStyler.to_latex([buf, column_format, ...]) Write Styler to a file, buffer or string in LaTeX format.      HDFStore: PyTables (HDF5)       \nread_hdf(path_or_buf[, key, mode, errors, ...]) Read from the store, close it if we opened it.  \nHDFStore.put(key, value[, format, index, ...]) Store object in HDFStore.  \nHDFStore.append(key, value[, format, axes, ...]) Append to Table in file.  \nHDFStore.get(key) Retrieve pandas object stored in file.  \nHDFStore.select(key[, where, start, stop, ...]) Retrieve pandas object stored in file, optionally based on where criteria.  \nHDFStore.info() Print detailed information on the store.  \nHDFStore.keys([include]) Return a list of keys corresponding to objects stored in HDFStore.  \nHDFStore.groups() Return a list of all the top-level nodes.  \nHDFStore.walk([where]) Walk the pytables group hierarchy for pandas objects.     Warning One can store a subclass of DataFrame or Series to HDF5, but the type of the subclass is lost upon storing.    Feather       \nread_feather(path[, columns, use_threads, ...]) Load a feather-format object from the file path.  \nDataFrame.to_feather(path, **kwargs) Write a DataFrame to the binary Feather format.      Parquet       \nread_parquet(path[, engine, columns, ...]) Load a parquet object from the file path, returning a DataFrame.  \nDataFrame.to_parquet([path, engine, ...]) Write a DataFrame to the binary parquet format.      ORC       \nread_orc(path[, columns]) Load an ORC object from the file path, returning a DataFrame.      SAS       \nread_sas(filepath_or_buffer[, format, ...]) Read SAS files stored as either XPORT or SAS7BDAT format files.      SPSS       \nread_spss(path[, usecols, convert_categoricals]) Load an SPSS file from the file path, returning a DataFrame.      SQL       \nread_sql_table(table_name, con[, schema, ...]) Read SQL database table into a DataFrame.  \nread_sql_query(sql, con[, index_col, ...]) Read SQL query into a DataFrame.  \nread_sql(sql, con[, index_col, ...]) Read SQL query or database table into a DataFrame.  \nDataFrame.to_sql(name, con[, schema, ...]) Write records stored in a DataFrame to a SQL database.      Google BigQuery       \nread_gbq(query[, project_id, index_col, ...]) Load data from Google BigQuery.      STATA       \nread_stata(filepath_or_buffer[, ...]) Read Stata file into DataFrame.  \nDataFrame.to_stata(path[, convert_dates, ...]) Export DataFrame object to Stata dta format.          \nStataReader.data_label Return data label of Stata file.  \nStataReader.value_labels() Return a dict, associating each variable name a dict, associating each value its corresponding label.  \nStataReader.variable_labels() Return variable labels as a dict, associating each variable name with corresponding label.  \nStataWriter.write_file() Export DataFrame object to Stata dta format.","title":"pandas.reference.io"},{"text":"pandas.DataFrame.join   DataFrame.join(other, on=None, how='left', lsuffix='', rsuffix='', sort=False)[source]\n \nJoin columns of another DataFrame. Join columns with other DataFrame either on index or on a key column. Efficiently join multiple DataFrame objects by index at once by passing a list.  Parameters \n \nother:DataFrame, Series, or list of DataFrame\n\n\nIndex should be similar to one of the columns in this one. If a Series is passed, its name attribute must be set, and that will be used as the column name in the resulting joined DataFrame.  \non:str, list of str, or array-like, optional\n\n\nColumn or index level name(s) in the caller to join on the index in other, otherwise joins index-on-index. If multiple values given, the other DataFrame must have a MultiIndex. Can pass an array as the join key if it is not already contained in the calling DataFrame. Like an Excel VLOOKUP operation.  \nhow:{\u2018left\u2019, \u2018right\u2019, \u2018outer\u2019, \u2018inner\u2019}, default \u2018left\u2019\n\n\nHow to handle the operation of the two objects.  left: use calling frame\u2019s index (or column if on is specified) right: use other\u2019s index. outer: form union of calling frame\u2019s index (or column if on is specified) with other\u2019s index, and sort it. lexicographically. inner: form intersection of calling frame\u2019s index (or column if on is specified) with other\u2019s index, preserving the order of the calling\u2019s one. \ncross: creates the cartesian product from both frames, preserves the order of the left keys.  New in version 1.2.0.     \nlsuffix:str, default \u2018\u2019\n\n\nSuffix to use from left frame\u2019s overlapping columns.  \nrsuffix:str, default \u2018\u2019\n\n\nSuffix to use from right frame\u2019s overlapping columns.  \nsort:bool, default False\n\n\nOrder result DataFrame lexicographically by the join key. If False, the order of the join key depends on the join type (how keyword).    Returns \n DataFrame\n\nA dataframe containing columns from both the caller and other.      See also  DataFrame.merge\n\nFor column(s)-on-column(s) operations.    Notes Parameters on, lsuffix, and rsuffix are not supported when passing a list of DataFrame objects. Support for specifying index levels as the on parameter was added in version 0.23.0. Examples \n>>> df = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'],\n...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})\n  \n>>> df\n  key   A\n0  K0  A0\n1  K1  A1\n2  K2  A2\n3  K3  A3\n4  K4  A4\n5  K5  A5\n  \n>>> other = pd.DataFrame({'key': ['K0', 'K1', 'K2'],\n...                       'B': ['B0', 'B1', 'B2']})\n  \n>>> other\n  key   B\n0  K0  B0\n1  K1  B1\n2  K2  B2\n  Join DataFrames using their indexes. \n>>> df.join(other, lsuffix='_caller', rsuffix='_other')\n  key_caller   A key_other    B\n0         K0  A0        K0   B0\n1         K1  A1        K1   B1\n2         K2  A2        K2   B2\n3         K3  A3       NaN  NaN\n4         K4  A4       NaN  NaN\n5         K5  A5       NaN  NaN\n  If we want to join using the key columns, we need to set key to be the index in both df and other. The joined DataFrame will have key as its index. \n>>> df.set_index('key').join(other.set_index('key'))\n      A    B\nkey\nK0   A0   B0\nK1   A1   B1\nK2   A2   B2\nK3   A3  NaN\nK4   A4  NaN\nK5   A5  NaN\n  Another option to join using the key columns is to use the on parameter. DataFrame.join always uses other\u2019s index but we can use any column in df. This method preserves the original DataFrame\u2019s index in the result. \n>>> df.join(other.set_index('key'), on='key')\n  key   A    B\n0  K0  A0   B0\n1  K1  A1   B1\n2  K2  A2   B2\n3  K3  A3  NaN\n4  K4  A4  NaN\n5  K5  A5  NaN\n  Using non-unique key values shows how they are matched. \n>>> df = pd.DataFrame({'key': ['K0', 'K1', 'K1', 'K3', 'K0', 'K1'],\n...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})\n  \n>>> df\n  key   A\n0  K0  A0\n1  K1  A1\n2  K1  A2\n3  K3  A3\n4  K0  A4\n5  K1  A5\n  \n>>> df.join(other.set_index('key'), on='key')\n  key   A    B\n0  K0  A0   B0\n1  K1  A1   B1\n2  K1  A2   B1\n3  K3  A3  NaN\n4  K0  A4   B0\n5  K1  A5   B1","title":"pandas.reference.api.pandas.dataframe.join"},{"text":"DataFrame  Constructor       \nDataFrame([data, index, columns, dtype, copy]) Two-dimensional, size-mutable, potentially heterogeneous tabular data.      Attributes and underlying data Axes       \nDataFrame.index The index (row labels) of the DataFrame.  \nDataFrame.columns The column labels of the DataFrame.          \nDataFrame.dtypes Return the dtypes in the DataFrame.  \nDataFrame.info([verbose, buf, max_cols, ...]) Print a concise summary of a DataFrame.  \nDataFrame.select_dtypes([include, exclude]) Return a subset of the DataFrame's columns based on the column dtypes.  \nDataFrame.values Return a Numpy representation of the DataFrame.  \nDataFrame.axes Return a list representing the axes of the DataFrame.  \nDataFrame.ndim Return an int representing the number of axes \/ array dimensions.  \nDataFrame.size Return an int representing the number of elements in this object.  \nDataFrame.shape Return a tuple representing the dimensionality of the DataFrame.  \nDataFrame.memory_usage([index, deep]) Return the memory usage of each column in bytes.  \nDataFrame.empty Indicator whether Series\/DataFrame is empty.  \nDataFrame.set_flags(*[, copy, ...]) Return a new object with updated flags.      Conversion       \nDataFrame.astype(dtype[, copy, errors]) Cast a pandas object to a specified dtype dtype.  \nDataFrame.convert_dtypes([infer_objects, ...]) Convert columns to best possible dtypes using dtypes supporting pd.NA.  \nDataFrame.infer_objects() Attempt to infer better dtypes for object columns.  \nDataFrame.copy([deep]) Make a copy of this object's indices and data.  \nDataFrame.bool() Return the bool of a single element Series or DataFrame.      Indexing, iteration       \nDataFrame.head([n]) Return the first n rows.  \nDataFrame.at Access a single value for a row\/column label pair.  \nDataFrame.iat Access a single value for a row\/column pair by integer position.  \nDataFrame.loc Access a group of rows and columns by label(s) or a boolean array.  \nDataFrame.iloc Purely integer-location based indexing for selection by position.  \nDataFrame.insert(loc, column, value[, ...]) Insert column into DataFrame at specified location.  \nDataFrame.__iter__() Iterate over info axis.  \nDataFrame.items() Iterate over (column name, Series) pairs.  \nDataFrame.iteritems() Iterate over (column name, Series) pairs.  \nDataFrame.keys() Get the 'info axis' (see Indexing for more).  \nDataFrame.iterrows() Iterate over DataFrame rows as (index, Series) pairs.  \nDataFrame.itertuples([index, name]) Iterate over DataFrame rows as namedtuples.  \nDataFrame.lookup(row_labels, col_labels) (DEPRECATED) Label-based \"fancy indexing\" function for DataFrame.  \nDataFrame.pop(item) Return item and drop from frame.  \nDataFrame.tail([n]) Return the last n rows.  \nDataFrame.xs(key[, axis, level, drop_level]) Return cross-section from the Series\/DataFrame.  \nDataFrame.get(key[, default]) Get item from object for given key (ex: DataFrame column).  \nDataFrame.isin(values) Whether each element in the DataFrame is contained in values.  \nDataFrame.where(cond[, other, inplace, ...]) Replace values where the condition is False.  \nDataFrame.mask(cond[, other, inplace, axis, ...]) Replace values where the condition is True.  \nDataFrame.query(expr[, inplace]) Query the columns of a DataFrame with a boolean expression.    For more information on .at, .iat, .loc, and .iloc, see the indexing documentation.   Binary operator functions       \nDataFrame.add(other[, axis, level, fill_value]) Get Addition of dataframe and other, element-wise (binary operator add).  \nDataFrame.sub(other[, axis, level, fill_value]) Get Subtraction of dataframe and other, element-wise (binary operator sub).  \nDataFrame.mul(other[, axis, level, fill_value]) Get Multiplication of dataframe and other, element-wise (binary operator mul).  \nDataFrame.div(other[, axis, level, fill_value]) Get Floating division of dataframe and other, element-wise (binary operator truediv).  \nDataFrame.truediv(other[, axis, level, ...]) Get Floating division of dataframe and other, element-wise (binary operator truediv).  \nDataFrame.floordiv(other[, axis, level, ...]) Get Integer division of dataframe and other, element-wise (binary operator floordiv).  \nDataFrame.mod(other[, axis, level, fill_value]) Get Modulo of dataframe and other, element-wise (binary operator mod).  \nDataFrame.pow(other[, axis, level, fill_value]) Get Exponential power of dataframe and other, element-wise (binary operator pow).  \nDataFrame.dot(other) Compute the matrix multiplication between the DataFrame and other.  \nDataFrame.radd(other[, axis, level, fill_value]) Get Addition of dataframe and other, element-wise (binary operator radd).  \nDataFrame.rsub(other[, axis, level, fill_value]) Get Subtraction of dataframe and other, element-wise (binary operator rsub).  \nDataFrame.rmul(other[, axis, level, fill_value]) Get Multiplication of dataframe and other, element-wise (binary operator rmul).  \nDataFrame.rdiv(other[, axis, level, fill_value]) Get Floating division of dataframe and other, element-wise (binary operator rtruediv).  \nDataFrame.rtruediv(other[, axis, level, ...]) Get Floating division of dataframe and other, element-wise (binary operator rtruediv).  \nDataFrame.rfloordiv(other[, axis, level, ...]) Get Integer division of dataframe and other, element-wise (binary operator rfloordiv).  \nDataFrame.rmod(other[, axis, level, fill_value]) Get Modulo of dataframe and other, element-wise (binary operator rmod).  \nDataFrame.rpow(other[, axis, level, fill_value]) Get Exponential power of dataframe and other, element-wise (binary operator rpow).  \nDataFrame.lt(other[, axis, level]) Get Less than of dataframe and other, element-wise (binary operator lt).  \nDataFrame.gt(other[, axis, level]) Get Greater than of dataframe and other, element-wise (binary operator gt).  \nDataFrame.le(other[, axis, level]) Get Less than or equal to of dataframe and other, element-wise (binary operator le).  \nDataFrame.ge(other[, axis, level]) Get Greater than or equal to of dataframe and other, element-wise (binary operator ge).  \nDataFrame.ne(other[, axis, level]) Get Not equal to of dataframe and other, element-wise (binary operator ne).  \nDataFrame.eq(other[, axis, level]) Get Equal to of dataframe and other, element-wise (binary operator eq).  \nDataFrame.combine(other, func[, fill_value, ...]) Perform column-wise combine with another DataFrame.  \nDataFrame.combine_first(other) Update null elements with value in the same location in other.      Function application, GroupBy & window       \nDataFrame.apply(func[, axis, raw, ...]) Apply a function along an axis of the DataFrame.  \nDataFrame.applymap(func[, na_action]) Apply a function to a Dataframe elementwise.  \nDataFrame.pipe(func, *args, **kwargs) Apply chainable functions that expect Series or DataFrames.  \nDataFrame.agg([func, axis]) Aggregate using one or more operations over the specified axis.  \nDataFrame.aggregate([func, axis]) Aggregate using one or more operations over the specified axis.  \nDataFrame.transform(func[, axis]) Call func on self producing a DataFrame with the same axis shape as self.  \nDataFrame.groupby([by, axis, level, ...]) Group DataFrame using a mapper or by a Series of columns.  \nDataFrame.rolling(window[, min_periods, ...]) Provide rolling window calculations.  \nDataFrame.expanding([min_periods, center, ...]) Provide expanding window calculations.  \nDataFrame.ewm([com, span, halflife, alpha, ...]) Provide exponentially weighted (EW) calculations.      Computations \/ descriptive stats       \nDataFrame.abs() Return a Series\/DataFrame with absolute numeric value of each element.  \nDataFrame.all([axis, bool_only, skipna, level]) Return whether all elements are True, potentially over an axis.  \nDataFrame.any([axis, bool_only, skipna, level]) Return whether any element is True, potentially over an axis.  \nDataFrame.clip([lower, upper, axis, inplace]) Trim values at input threshold(s).  \nDataFrame.corr([method, min_periods]) Compute pairwise correlation of columns, excluding NA\/null values.  \nDataFrame.corrwith(other[, axis, drop, method]) Compute pairwise correlation.  \nDataFrame.count([axis, level, numeric_only]) Count non-NA cells for each column or row.  \nDataFrame.cov([min_periods, ddof]) Compute pairwise covariance of columns, excluding NA\/null values.  \nDataFrame.cummax([axis, skipna]) Return cumulative maximum over a DataFrame or Series axis.  \nDataFrame.cummin([axis, skipna]) Return cumulative minimum over a DataFrame or Series axis.  \nDataFrame.cumprod([axis, skipna]) Return cumulative product over a DataFrame or Series axis.  \nDataFrame.cumsum([axis, skipna]) Return cumulative sum over a DataFrame or Series axis.  \nDataFrame.describe([percentiles, include, ...]) Generate descriptive statistics.  \nDataFrame.diff([periods, axis]) First discrete difference of element.  \nDataFrame.eval(expr[, inplace]) Evaluate a string describing operations on DataFrame columns.  \nDataFrame.kurt([axis, skipna, level, ...]) Return unbiased kurtosis over requested axis.  \nDataFrame.kurtosis([axis, skipna, level, ...]) Return unbiased kurtosis over requested axis.  \nDataFrame.mad([axis, skipna, level]) Return the mean absolute deviation of the values over the requested axis.  \nDataFrame.max([axis, skipna, level, ...]) Return the maximum of the values over the requested axis.  \nDataFrame.mean([axis, skipna, level, ...]) Return the mean of the values over the requested axis.  \nDataFrame.median([axis, skipna, level, ...]) Return the median of the values over the requested axis.  \nDataFrame.min([axis, skipna, level, ...]) Return the minimum of the values over the requested axis.  \nDataFrame.mode([axis, numeric_only, dropna]) Get the mode(s) of each element along the selected axis.  \nDataFrame.pct_change([periods, fill_method, ...]) Percentage change between the current and a prior element.  \nDataFrame.prod([axis, skipna, level, ...]) Return the product of the values over the requested axis.  \nDataFrame.product([axis, skipna, level, ...]) Return the product of the values over the requested axis.  \nDataFrame.quantile([q, axis, numeric_only, ...]) Return values at the given quantile over requested axis.  \nDataFrame.rank([axis, method, numeric_only, ...]) Compute numerical data ranks (1 through n) along axis.  \nDataFrame.round([decimals]) Round a DataFrame to a variable number of decimal places.  \nDataFrame.sem([axis, skipna, level, ddof, ...]) Return unbiased standard error of the mean over requested axis.  \nDataFrame.skew([axis, skipna, level, ...]) Return unbiased skew over requested axis.  \nDataFrame.sum([axis, skipna, level, ...]) Return the sum of the values over the requested axis.  \nDataFrame.std([axis, skipna, level, ddof, ...]) Return sample standard deviation over requested axis.  \nDataFrame.var([axis, skipna, level, ddof, ...]) Return unbiased variance over requested axis.  \nDataFrame.nunique([axis, dropna]) Count number of distinct elements in specified axis.  \nDataFrame.value_counts([subset, normalize, ...]) Return a Series containing counts of unique rows in the DataFrame.      Reindexing \/ selection \/ label manipulation       \nDataFrame.add_prefix(prefix) Prefix labels with string prefix.  \nDataFrame.add_suffix(suffix) Suffix labels with string suffix.  \nDataFrame.align(other[, join, axis, level, ...]) Align two objects on their axes with the specified join method.  \nDataFrame.at_time(time[, asof, axis]) Select values at particular time of day (e.g., 9:30AM).  \nDataFrame.between_time(start_time, end_time) Select values between particular times of the day (e.g., 9:00-9:30 AM).  \nDataFrame.drop([labels, axis, index, ...]) Drop specified labels from rows or columns.  \nDataFrame.drop_duplicates([subset, keep, ...]) Return DataFrame with duplicate rows removed.  \nDataFrame.duplicated([subset, keep]) Return boolean Series denoting duplicate rows.  \nDataFrame.equals(other) Test whether two objects contain the same elements.  \nDataFrame.filter([items, like, regex, axis]) Subset the dataframe rows or columns according to the specified index labels.  \nDataFrame.first(offset) Select initial periods of time series data based on a date offset.  \nDataFrame.head([n]) Return the first n rows.  \nDataFrame.idxmax([axis, skipna]) Return index of first occurrence of maximum over requested axis.  \nDataFrame.idxmin([axis, skipna]) Return index of first occurrence of minimum over requested axis.  \nDataFrame.last(offset) Select final periods of time series data based on a date offset.  \nDataFrame.reindex([labels, index, columns, ...]) Conform Series\/DataFrame to new index with optional filling logic.  \nDataFrame.reindex_like(other[, method, ...]) Return an object with matching indices as other object.  \nDataFrame.rename([mapper, index, columns, ...]) Alter axes labels.  \nDataFrame.rename_axis([mapper, index, ...]) Set the name of the axis for the index or columns.  \nDataFrame.reset_index([level, drop, ...]) Reset the index, or a level of it.  \nDataFrame.sample([n, frac, replace, ...]) Return a random sample of items from an axis of object.  \nDataFrame.set_axis(labels[, axis, inplace]) Assign desired index to given axis.  \nDataFrame.set_index(keys[, drop, append, ...]) Set the DataFrame index using existing columns.  \nDataFrame.tail([n]) Return the last n rows.  \nDataFrame.take(indices[, axis, is_copy]) Return the elements in the given positional indices along an axis.  \nDataFrame.truncate([before, after, axis, copy]) Truncate a Series or DataFrame before and after some index value.      Missing data handling       \nDataFrame.backfill([axis, inplace, limit, ...]) Synonym for DataFrame.fillna() with method='bfill'.  \nDataFrame.bfill([axis, inplace, limit, downcast]) Synonym for DataFrame.fillna() with method='bfill'.  \nDataFrame.dropna([axis, how, thresh, ...]) Remove missing values.  \nDataFrame.ffill([axis, inplace, limit, downcast]) Synonym for DataFrame.fillna() with method='ffill'.  \nDataFrame.fillna([value, method, axis, ...]) Fill NA\/NaN values using the specified method.  \nDataFrame.interpolate([method, axis, limit, ...]) Fill NaN values using an interpolation method.  \nDataFrame.isna() Detect missing values.  \nDataFrame.isnull() DataFrame.isnull is an alias for DataFrame.isna.  \nDataFrame.notna() Detect existing (non-missing) values.  \nDataFrame.notnull() DataFrame.notnull is an alias for DataFrame.notna.  \nDataFrame.pad([axis, inplace, limit, downcast]) Synonym for DataFrame.fillna() with method='ffill'.  \nDataFrame.replace([to_replace, value, ...]) Replace values given in to_replace with value.      Reshaping, sorting, transposing       \nDataFrame.droplevel(level[, axis]) Return Series\/DataFrame with requested index \/ column level(s) removed.  \nDataFrame.pivot([index, columns, values]) Return reshaped DataFrame organized by given index \/ column values.  \nDataFrame.pivot_table([values, index, ...]) Create a spreadsheet-style pivot table as a DataFrame.  \nDataFrame.reorder_levels(order[, axis]) Rearrange index levels using input order.  \nDataFrame.sort_values(by[, axis, ascending, ...]) Sort by the values along either axis.  \nDataFrame.sort_index([axis, level, ...]) Sort object by labels (along an axis).  \nDataFrame.nlargest(n, columns[, keep]) Return the first n rows ordered by columns in descending order.  \nDataFrame.nsmallest(n, columns[, keep]) Return the first n rows ordered by columns in ascending order.  \nDataFrame.swaplevel([i, j, axis]) Swap levels i and j in a MultiIndex.  \nDataFrame.stack([level, dropna]) Stack the prescribed level(s) from columns to index.  \nDataFrame.unstack([level, fill_value]) Pivot a level of the (necessarily hierarchical) index labels.  \nDataFrame.swapaxes(axis1, axis2[, copy]) Interchange axes and swap values axes appropriately.  \nDataFrame.melt([id_vars, value_vars, ...]) Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.  \nDataFrame.explode(column[, ignore_index]) Transform each element of a list-like to a row, replicating index values.  \nDataFrame.squeeze([axis]) Squeeze 1 dimensional axis objects into scalars.  \nDataFrame.to_xarray() Return an xarray object from the pandas object.  \nDataFrame.T   \nDataFrame.transpose(*args[, copy]) Transpose index and columns.      Combining \/ comparing \/ joining \/ merging       \nDataFrame.append(other[, ignore_index, ...]) Append rows of other to the end of caller, returning a new object.  \nDataFrame.assign(**kwargs) Assign new columns to a DataFrame.  \nDataFrame.compare(other[, align_axis, ...]) Compare to another DataFrame and show the differences.  \nDataFrame.join(other[, on, how, lsuffix, ...]) Join columns of another DataFrame.  \nDataFrame.merge(right[, how, on, left_on, ...]) Merge DataFrame or named Series objects with a database-style join.  \nDataFrame.update(other[, join, overwrite, ...]) Modify in place using non-NA values from another DataFrame.      Time Series-related       \nDataFrame.asfreq(freq[, method, how, ...]) Convert time series to specified frequency.  \nDataFrame.asof(where[, subset]) Return the last row(s) without any NaNs before where.  \nDataFrame.shift([periods, freq, axis, ...]) Shift index by desired number of periods with an optional time freq.  \nDataFrame.slice_shift([periods, axis]) (DEPRECATED) Equivalent to shift without copying data.  \nDataFrame.tshift([periods, freq, axis]) (DEPRECATED) Shift the time index, using the index's frequency if available.  \nDataFrame.first_valid_index() Return index for first non-NA value or None, if no NA value is found.  \nDataFrame.last_valid_index() Return index for last non-NA value or None, if no NA value is found.  \nDataFrame.resample(rule[, axis, closed, ...]) Resample time-series data.  \nDataFrame.to_period([freq, axis, copy]) Convert DataFrame from DatetimeIndex to PeriodIndex.  \nDataFrame.to_timestamp([freq, how, axis, copy]) Cast to DatetimeIndex of timestamps, at beginning of period.  \nDataFrame.tz_convert(tz[, axis, level, copy]) Convert tz-aware axis to target time zone.  \nDataFrame.tz_localize(tz[, axis, level, ...]) Localize tz-naive index of a Series or DataFrame to target time zone.      Flags Flags refer to attributes of the pandas object. Properties of the dataset (like the date is was recorded, the URL it was accessed from, etc.) should be stored in DataFrame.attrs.       \nFlags(obj, *, allows_duplicate_labels) Flags that apply to pandas objects.      Metadata DataFrame.attrs is a dictionary for storing global metadata for this DataFrame.  Warning DataFrame.attrs is considered experimental and may change without warning.        \nDataFrame.attrs Dictionary of global attributes of this dataset.      Plotting DataFrame.plot is both a callable method and a namespace attribute for specific plotting methods of the form DataFrame.plot.<kind>.       \nDataFrame.plot([x, y, kind, ax, ....]) DataFrame plotting accessor and method          \nDataFrame.plot.area([x, y]) Draw a stacked area plot.  \nDataFrame.plot.bar([x, y]) Vertical bar plot.  \nDataFrame.plot.barh([x, y]) Make a horizontal bar plot.  \nDataFrame.plot.box([by]) Make a box plot of the DataFrame columns.  \nDataFrame.plot.density([bw_method, ind]) Generate Kernel Density Estimate plot using Gaussian kernels.  \nDataFrame.plot.hexbin(x, y[, C, ...]) Generate a hexagonal binning plot.  \nDataFrame.plot.hist([by, bins]) Draw one histogram of the DataFrame's columns.  \nDataFrame.plot.kde([bw_method, ind]) Generate Kernel Density Estimate plot using Gaussian kernels.  \nDataFrame.plot.line([x, y]) Plot Series or DataFrame as lines.  \nDataFrame.plot.pie(**kwargs) Generate a pie plot.  \nDataFrame.plot.scatter(x, y[, s, c]) Create a scatter plot with varying marker point size and color.          \nDataFrame.boxplot([column, by, ax, ...]) Make a box plot from DataFrame columns.  \nDataFrame.hist([column, by, grid, ...]) Make a histogram of the DataFrame's columns.      Sparse accessor Sparse-dtype specific methods and attributes are provided under the DataFrame.sparse accessor.       \nDataFrame.sparse.density Ratio of non-sparse points to total (dense) data points.          \nDataFrame.sparse.from_spmatrix(data[, ...]) Create a new DataFrame from a scipy sparse matrix.  \nDataFrame.sparse.to_coo() Return the contents of the frame as a sparse SciPy COO matrix.  \nDataFrame.sparse.to_dense() Convert a DataFrame with sparse values to dense.      Serialization \/ IO \/ conversion       \nDataFrame.from_dict(data[, orient, dtype, ...]) Construct DataFrame from dict of array-like or dicts.  \nDataFrame.from_records(data[, index, ...]) Convert structured or record ndarray to DataFrame.  \nDataFrame.to_parquet([path, engine, ...]) Write a DataFrame to the binary parquet format.  \nDataFrame.to_pickle(path[, compression, ...]) Pickle (serialize) object to file.  \nDataFrame.to_csv([path_or_buf, sep, na_rep, ...]) Write object to a comma-separated values (csv) file.  \nDataFrame.to_hdf(path_or_buf, key[, mode, ...]) Write the contained data to an HDF5 file using HDFStore.  \nDataFrame.to_sql(name, con[, schema, ...]) Write records stored in a DataFrame to a SQL database.  \nDataFrame.to_dict([orient, into]) Convert the DataFrame to a dictionary.  \nDataFrame.to_excel(excel_writer[, ...]) Write object to an Excel sheet.  \nDataFrame.to_json([path_or_buf, orient, ...]) Convert the object to a JSON string.  \nDataFrame.to_html([buf, columns, col_space, ...]) Render a DataFrame as an HTML table.  \nDataFrame.to_feather(path, **kwargs) Write a DataFrame to the binary Feather format.  \nDataFrame.to_latex([buf, columns, ...]) Render object to a LaTeX tabular, longtable, or nested table.  \nDataFrame.to_stata(path[, convert_dates, ...]) Export DataFrame object to Stata dta format.  \nDataFrame.to_gbq(destination_table[, ...]) Write a DataFrame to a Google BigQuery table.  \nDataFrame.to_records([index, column_dtypes, ...]) Convert DataFrame to a NumPy record array.  \nDataFrame.to_string([buf, columns, ...]) Render a DataFrame to a console-friendly tabular output.  \nDataFrame.to_clipboard([excel, sep]) Copy object to the system clipboard.  \nDataFrame.to_markdown([buf, mode, index, ...]) Print DataFrame in Markdown-friendly format.  \nDataFrame.style Returns a Styler object.","title":"pandas.reference.frame"},{"text":"pandas.merge_asof   pandas.merge_asof(left, right, on=None, left_on=None, right_on=None, left_index=False, right_index=False, by=None, left_by=None, right_by=None, suffixes=('_x', '_y'), tolerance=None, allow_exact_matches=True, direction='backward')[source]\n \nPerform a merge by key distance. This is similar to a left-join except that we match on nearest key rather than equal keys. Both DataFrames must be sorted by the key. For each row in the left DataFrame:  \n A \u201cbackward\u201d search selects the last row in the right DataFrame whose \u2018on\u2019 key is less than or equal to the left\u2019s key. A \u201cforward\u201d search selects the first row in the right DataFrame whose \u2018on\u2019 key is greater than or equal to the left\u2019s key. A \u201cnearest\u201d search selects the row in the right DataFrame whose \u2018on\u2019 key is closest in absolute distance to the left\u2019s key.  \n The default is \u201cbackward\u201d and is compatible in versions below 0.20.0. The direction parameter was added in version 0.20.0 and introduces \u201cforward\u201d and \u201cnearest\u201d. Optionally match on equivalent keys with \u2018by\u2019 before searching with \u2018on\u2019.  Parameters \n \nleft:DataFrame or named Series\n\n\nright:DataFrame or named Series\n\n\non:label\n\n\nField name to join on. Must be found in both DataFrames. The data MUST be ordered. Furthermore this must be a numeric column, such as datetimelike, integer, or float. On or left_on\/right_on must be given.  \nleft_on:label\n\n\nField name to join on in left DataFrame.  \nright_on:label\n\n\nField name to join on in right DataFrame.  \nleft_index:bool\n\n\nUse the index of the left DataFrame as the join key.  \nright_index:bool\n\n\nUse the index of the right DataFrame as the join key.  \nby:column name or list of column names\n\n\nMatch on these columns before performing merge operation.  \nleft_by:column name\n\n\nField names to match on in the left DataFrame.  \nright_by:column name\n\n\nField names to match on in the right DataFrame.  \nsuffixes:2-length sequence (tuple, list, \u2026)\n\n\nSuffix to apply to overlapping column names in the left and right side, respectively.  \ntolerance:int or Timedelta, optional, default None\n\n\nSelect asof tolerance within this range; must be compatible with the merge index.  \nallow_exact_matches:bool, default True\n\n\n If True, allow matching with the same \u2018on\u2019 value (i.e. less-than-or-equal-to \/ greater-than-or-equal-to) If False, don\u2019t match the same \u2018on\u2019 value (i.e., strictly less-than \/ strictly greater-than).   \ndirection:\u2018backward\u2019 (default), \u2018forward\u2019, or \u2018nearest\u2019\n\n\nWhether to search for prior, subsequent, or closest matches.    Returns \n \nmerged:DataFrame\n\n    See also  merge\n\nMerge with a database-style join.  merge_ordered\n\nMerge with optional filling\/interpolation.    Examples \n>>> left = pd.DataFrame({\"a\": [1, 5, 10], \"left_val\": [\"a\", \"b\", \"c\"]})\n>>> left\n    a left_val\n0   1        a\n1   5        b\n2  10        c\n  \n>>> right = pd.DataFrame({\"a\": [1, 2, 3, 6, 7], \"right_val\": [1, 2, 3, 6, 7]})\n>>> right\n   a  right_val\n0  1          1\n1  2          2\n2  3          3\n3  6          6\n4  7          7\n  \n>>> pd.merge_asof(left, right, on=\"a\")\n    a left_val  right_val\n0   1        a          1\n1   5        b          3\n2  10        c          7\n  \n>>> pd.merge_asof(left, right, on=\"a\", allow_exact_matches=False)\n    a left_val  right_val\n0   1        a        NaN\n1   5        b        3.0\n2  10        c        7.0\n  \n>>> pd.merge_asof(left, right, on=\"a\", direction=\"forward\")\n    a left_val  right_val\n0   1        a        1.0\n1   5        b        6.0\n2  10        c        NaN\n  \n>>> pd.merge_asof(left, right, on=\"a\", direction=\"nearest\")\n    a left_val  right_val\n0   1        a          1\n1   5        b          6\n2  10        c          7\n  We can use indexed DataFrames as well. \n>>> left = pd.DataFrame({\"left_val\": [\"a\", \"b\", \"c\"]}, index=[1, 5, 10])\n>>> left\n   left_val\n1         a\n5         b\n10        c\n  \n>>> right = pd.DataFrame({\"right_val\": [1, 2, 3, 6, 7]}, index=[1, 2, 3, 6, 7])\n>>> right\n   right_val\n1          1\n2          2\n3          3\n6          6\n7          7\n  \n>>> pd.merge_asof(left, right, left_index=True, right_index=True)\n   left_val  right_val\n1         a          1\n5         b          3\n10        c          7\n  Here is a real-world times-series example \n>>> quotes = pd.DataFrame(\n...     {\n...         \"time\": [\n...             pd.Timestamp(\"2016-05-25 13:30:00.023\"),\n...             pd.Timestamp(\"2016-05-25 13:30:00.023\"),\n...             pd.Timestamp(\"2016-05-25 13:30:00.030\"),\n...             pd.Timestamp(\"2016-05-25 13:30:00.041\"),\n...             pd.Timestamp(\"2016-05-25 13:30:00.048\"),\n...             pd.Timestamp(\"2016-05-25 13:30:00.049\"),\n...             pd.Timestamp(\"2016-05-25 13:30:00.072\"),\n...             pd.Timestamp(\"2016-05-25 13:30:00.075\")\n...         ],\n...         \"ticker\": [\n...                \"GOOG\",\n...                \"MSFT\",\n...                \"MSFT\",\n...                \"MSFT\",\n...                \"GOOG\",\n...                \"AAPL\",\n...                \"GOOG\",\n...                \"MSFT\"\n...            ],\n...            \"bid\": [720.50, 51.95, 51.97, 51.99, 720.50, 97.99, 720.50, 52.01],\n...            \"ask\": [720.93, 51.96, 51.98, 52.00, 720.93, 98.01, 720.88, 52.03]\n...     }\n... )\n>>> quotes\n                     time ticker     bid     ask\n0 2016-05-25 13:30:00.023   GOOG  720.50  720.93\n1 2016-05-25 13:30:00.023   MSFT   51.95   51.96\n2 2016-05-25 13:30:00.030   MSFT   51.97   51.98\n3 2016-05-25 13:30:00.041   MSFT   51.99   52.00\n4 2016-05-25 13:30:00.048   GOOG  720.50  720.93\n5 2016-05-25 13:30:00.049   AAPL   97.99   98.01\n6 2016-05-25 13:30:00.072   GOOG  720.50  720.88\n7 2016-05-25 13:30:00.075   MSFT   52.01   52.03\n  \n>>> trades = pd.DataFrame(\n...        {\n...            \"time\": [\n...                pd.Timestamp(\"2016-05-25 13:30:00.023\"),\n...                pd.Timestamp(\"2016-05-25 13:30:00.038\"),\n...                pd.Timestamp(\"2016-05-25 13:30:00.048\"),\n...                pd.Timestamp(\"2016-05-25 13:30:00.048\"),\n...                pd.Timestamp(\"2016-05-25 13:30:00.048\")\n...            ],\n...            \"ticker\": [\"MSFT\", \"MSFT\", \"GOOG\", \"GOOG\", \"AAPL\"],\n...            \"price\": [51.95, 51.95, 720.77, 720.92, 98.0],\n...            \"quantity\": [75, 155, 100, 100, 100]\n...        }\n...    )\n>>> trades\n                     time ticker   price  quantity\n0 2016-05-25 13:30:00.023   MSFT   51.95        75\n1 2016-05-25 13:30:00.038   MSFT   51.95       155\n2 2016-05-25 13:30:00.048   GOOG  720.77       100\n3 2016-05-25 13:30:00.048   GOOG  720.92       100\n4 2016-05-25 13:30:00.048   AAPL   98.00       100\n  By default we are taking the asof of the quotes \n>>> pd.merge_asof(trades, quotes, on=\"time\", by=\"ticker\")\n                     time ticker   price  quantity     bid     ask\n0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96\n1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.98\n2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93\n3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93\n4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\n  We only asof within 2ms between the quote time and the trade time \n>>> pd.merge_asof(\n...     trades, quotes, on=\"time\", by=\"ticker\", tolerance=pd.Timedelta(\"2ms\")\n... )\n                     time ticker   price  quantity     bid     ask\n0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96\n1 2016-05-25 13:30:00.038   MSFT   51.95       155     NaN     NaN\n2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93\n3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93\n4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\n  We only asof within 10ms between the quote time and the trade time and we exclude exact matches on time. However prior data will propagate forward \n>>> pd.merge_asof(\n...     trades,\n...     quotes,\n...     on=\"time\",\n...     by=\"ticker\",\n...     tolerance=pd.Timedelta(\"10ms\"),\n...     allow_exact_matches=False\n... )\n                     time ticker   price  quantity     bid     ask\n0 2016-05-25 13:30:00.023   MSFT   51.95        75     NaN     NaN\n1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.98\n2 2016-05-25 13:30:00.048   GOOG  720.77       100     NaN     NaN\n3 2016-05-25 13:30:00.048   GOOG  720.92       100     NaN     NaN\n4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN","title":"pandas.reference.api.pandas.merge_asof"},{"text":"pandas.DataFrame.T   propertyDataFrame.T","title":"pandas.reference.api.pandas.dataframe.t"},{"text":"pandas.tseries.offsets.BYearEnd.name   BYearEnd.name","title":"pandas.reference.api.pandas.tseries.offsets.byearend.name"},{"text":"pandas.DataFrame.rename   DataFrame.rename(mapper=None, *, index=None, columns=None, axis=None, copy=True, inplace=False, level=None, errors='ignore')[source]\n \nAlter axes labels. Function \/ dict values must be unique (1-to-1). Labels not contained in a dict \/ Series will be left as-is. Extra labels listed don\u2019t throw an error. See the user guide for more.  Parameters \n \nmapper:dict-like or function\n\n\nDict-like or function transformations to apply to that axis\u2019 values. Use either mapper and axis to specify the axis to target with mapper, or index and columns.  \nindex:dict-like or function\n\n\nAlternative to specifying axis (mapper, axis=0 is equivalent to index=mapper).  \ncolumns:dict-like or function\n\n\nAlternative to specifying axis (mapper, axis=1 is equivalent to columns=mapper).  \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019}, default 0\n\n\nAxis to target with mapper. Can be either the axis name (\u2018index\u2019, \u2018columns\u2019) or number (0, 1). The default is \u2018index\u2019.  \ncopy:bool, default True\n\n\nAlso copy underlying data.  \ninplace:bool, default False\n\n\nWhether to return a new DataFrame. If True then value of copy is ignored.  \nlevel:int or level name, default None\n\n\nIn case of a MultiIndex, only rename labels in the specified level.  \nerrors:{\u2018ignore\u2019, \u2018raise\u2019}, default \u2018ignore\u2019\n\n\nIf \u2018raise\u2019, raise a KeyError when a dict-like mapper, index, or columns contains labels that are not present in the Index being transformed. If \u2018ignore\u2019, existing keys will be renamed and extra keys will be ignored.    Returns \n DataFrame or None\n\nDataFrame with the renamed axis labels or None if inplace=True.    Raises \n KeyError\n\nIf any of the labels is not found in the selected axis and \u201cerrors=\u2019raise\u2019\u201d.      See also  DataFrame.rename_axis\n\nSet the name of the axis.    Examples DataFrame.rename supports two calling conventions  (index=index_mapper, columns=columns_mapper, ...) (mapper, axis={'index', 'columns'}, ...)  We highly recommend using keyword arguments to clarify your intent. Rename columns using a mapping: \n>>> df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n>>> df.rename(columns={\"A\": \"a\", \"B\": \"c\"})\n   a  c\n0  1  4\n1  2  5\n2  3  6\n  Rename index using a mapping: \n>>> df.rename(index={0: \"x\", 1: \"y\", 2: \"z\"})\n   A  B\nx  1  4\ny  2  5\nz  3  6\n  Cast index labels to a different type: \n>>> df.index\nRangeIndex(start=0, stop=3, step=1)\n>>> df.rename(index=str).index\nIndex(['0', '1', '2'], dtype='object')\n  \n>>> df.rename(columns={\"A\": \"a\", \"B\": \"b\", \"C\": \"c\"}, errors=\"raise\")\nTraceback (most recent call last):\nKeyError: ['C'] not found in axis\n  Using axis-style parameters: \n>>> df.rename(str.lower, axis='columns')\n   a  b\n0  1  4\n1  2  5\n2  3  6\n  \n>>> df.rename({1: 2, 2: 4}, axis='index')\n   A  B\n0  1  4\n2  2  5\n4  3  6","title":"pandas.reference.api.pandas.dataframe.rename"}]}
{"task_id":10697757,"prompt":"def f_10697757(s):\n\treturn ","suffix":"","canonical_solution":"s.split(' ', 4)","test_start":"\ndef check(candidate):","test":["\n    assert candidate('1 0 A10B 100 Description: This is a description with spaces') ==         ['1', '0', 'A10B', '100', 'Description: This is a description with spaces']\n","\n    assert candidate('this-is-a-continuous-sequence') == ['this-is-a-continuous-sequence']\n","\n    assert candidate('') == ['']\n","\n    assert candidate('\t') == ['\t']\n"],"entry_point":"f_10697757","intent":"Split a string `s` by space with `4` splits","library":[],"docs":[]}
{"task_id":16344756,"prompt":"def f_16344756(app):\n\treturn ","suffix":"","canonical_solution":"app.run(debug=True)","test_start":"\nfrom flask import Flask\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    Flask = Mock()\n    app = Flask('mai')\n    try:\n        candidate(app)\n    except:\n        return False\n"],"entry_point":"f_16344756","intent":"enable debug mode on Flask application `app`","library":["flask"],"docs":[{"text":"DEBUG  \nWhether debug mode is enabled. When using flask run to start the development server, an interactive debugger will be shown for unhandled exceptions, and the server will be reloaded when code changes. The debug attribute maps to this config key. This is enabled when ENV is 'development' and is overridden by the FLASK_DEBUG environment variable. It may not behave as expected if set in code. Do not enable debug mode when deploying in production. Default: True if ENV is 'development', or False otherwise.","title":"flask.config.index#DEBUG"},{"text":"run(host=None, port=None, debug=None, load_dotenv=True, **options)  \nRuns the application on a local development server. Do not use run() in a production setting. It is not intended to meet security and performance requirements for a production server. Instead, see Deployment Options for WSGI server recommendations. If the debug flag is set the server will automatically reload for code changes and show a debugger in case an exception happened. If you want to run the application in debug mode, but disable the code execution on the interactive debugger, you can pass use_evalex=False as parameter. This will keep the debugger\u2019s traceback screen active, but disable code execution. It is not recommended to use this function for development with automatic reloading as this is badly supported. Instead you should be using the flask command line script\u2019s run support.  Keep in Mind Flask will suppress any server error with a generic error page unless it is in debug mode. As such to enable just the interactive debugger without the code reloading, you have to invoke run() with debug=True and use_reloader=False. Setting use_debugger to True without being in debug mode won\u2019t catch any exceptions because there won\u2019t be any to catch.   Parameters \n \nhost (Optional[str]) \u2013 the hostname to listen on. Set this to '0.0.0.0' to have the server available externally as well. Defaults to '127.0.0.1' or the host in the SERVER_NAME config variable if present. \nport (Optional[int]) \u2013 the port of the webserver. Defaults to 5000 or the port defined in the SERVER_NAME config variable if present. \ndebug (Optional[bool]) \u2013 if given, enable or disable debug mode. See debug. \nload_dotenv (bool) \u2013 Load the nearest .env and .flaskenv files to set environment variables. Will also change the working directory to the directory containing the first file found. \noptions (Any) \u2013 the options to be forwarded to the underlying Werkzeug server. See werkzeug.serving.run_simple() for more information.   Return type \nNone    Changelog Changed in version 1.0: If installed, python-dotenv will be used to load environment variables from .env and .flaskenv files. If set, the FLASK_ENV and FLASK_DEBUG environment variables will override env and debug. Threaded mode is enabled by default.   Changed in version 0.10: The default port is now picked from the SERVER_NAME variable.","title":"flask.api.index#flask.Flask.run"},{"text":"app  \na reference to the current application","title":"flask.api.index#flask.blueprints.BlueprintSetupState.app"},{"text":"TEMPLATES_AUTO_RELOAD  \nReload templates when they are changed. If not set, it will be enabled in debug mode. Default: None","title":"flask.config.index#TEMPLATES_AUTO_RELOAD"},{"text":"debug()","title":"django.ref.templates.api#django.template.context_processors.debug"},{"text":"PROPAGATE_EXCEPTIONS  \nExceptions are re-raised rather than being handled by the app\u2019s error handlers. If not set, this is implicitly true if TESTING or DEBUG is enabled. Default: None","title":"flask.config.index#PROPAGATE_EXCEPTIONS"},{"text":"TRAP_HTTP_EXCEPTIONS  \nIf there is no handler for an HTTPException-type exception, re-raise it to be handled by the interactive debugger instead of returning it as a simple error response. Default: False","title":"flask.config.index#TRAP_HTTP_EXCEPTIONS"},{"text":"JSONIFY_PRETTYPRINT_REGULAR  \njsonify responses will be output with newlines, spaces, and indentation for easier reading by humans. Always enabled in debug mode. Default: False","title":"flask.config.index#JSONIFY_PRETTYPRINT_REGULAR"},{"text":"SESSION_COOKIE_HTTPONLY  \nBrowsers will not allow JavaScript access to cookies marked as \u201cHTTP only\u201d for security. Default: True","title":"flask.config.index#SESSION_COOKIE_HTTPONLY"},{"text":"Logging Flask uses standard Python logging. Messages about your Flask application are logged with app.logger, which takes the same name as app.name. This logger can also be used to log your own messages. @app.route('\/login', methods=['POST'])\ndef login():\n    user = get_user(request.form['username'])\n\n    if user.check_password(request.form['password']):\n        login_user(user)\n        app.logger.info('%s logged in successfully', user.username)\n        return redirect(url_for('index'))\n    else:\n        app.logger.info('%s failed to log in', user.username)\n        abort(401)\n If you don\u2019t configure logging, Python\u2019s default log level is usually \u2018warning\u2019. Nothing below the configured level will be visible. Basic Configuration When you want to configure logging for your project, you should do it as soon as possible when the program starts. If app.logger is accessed before logging is configured, it will add a default handler. If possible, configure logging before creating the application object. This example uses dictConfig() to create a logging configuration similar to Flask\u2019s default, except for all logs: from logging.config import dictConfig\n\ndictConfig({\n    'version': 1,\n    'formatters': {'default': {\n        'format': '[%(asctime)s] %(levelname)s in %(module)s: %(message)s',\n    }},\n    'handlers': {'wsgi': {\n        'class': 'logging.StreamHandler',\n        'stream': 'ext:\/\/flask.logging.wsgi_errors_stream',\n        'formatter': 'default'\n    }},\n    'root': {\n        'level': 'INFO',\n        'handlers': ['wsgi']\n    }\n})\n\napp = Flask(__name__)\n Default Configuration If you do not configure logging yourself, Flask will add a StreamHandler to app.logger automatically. During requests, it will write to the stream specified by the WSGI server in environ['wsgi.errors'] (which is usually sys.stderr). Outside a request, it will log to sys.stderr. Removing the Default Handler If you configured logging after accessing app.logger, and need to remove the default handler, you can import and remove it: from flask.logging import default_handler\n\napp.logger.removeHandler(default_handler)\n Email Errors to Admins When running the application on a remote server for production, you probably won\u2019t be looking at the log messages very often. The WSGI server will probably send log messages to a file, and you\u2019ll only check that file if a user tells you something went wrong. To be proactive about discovering and fixing bugs, you can configure a logging.handlers.SMTPHandler to send an email when errors and higher are logged. import logging\nfrom logging.handlers import SMTPHandler\n\nmail_handler = SMTPHandler(\n    mailhost='127.0.0.1',\n    fromaddr='server-error@example.com',\n    toaddrs=['admin@example.com'],\n    subject='Application Error'\n)\nmail_handler.setLevel(logging.ERROR)\nmail_handler.setFormatter(logging.Formatter(\n    '[%(asctime)s] %(levelname)s in %(module)s: %(message)s'\n))\n\nif not app.debug:\n    app.logger.addHandler(mail_handler)\n This requires that you have an SMTP server set up on the same server. See the Python docs for more information about configuring the handler. Injecting Request Information Seeing more information about the request, such as the IP address, may help debugging some errors. You can subclass logging.Formatter to inject your own fields that can be used in messages. You can change the formatter for Flask\u2019s default handler, the mail handler defined above, or any other handler. from flask import has_request_context, request\nfrom flask.logging import default_handler\n\nclass RequestFormatter(logging.Formatter):\n    def format(self, record):\n        if has_request_context():\n            record.url = request.url\n            record.remote_addr = request.remote_addr\n        else:\n            record.url = None\n            record.remote_addr = None\n\n        return super().format(record)\n\nformatter = RequestFormatter(\n    '[%(asctime)s] %(remote_addr)s requested %(url)s\\n'\n    '%(levelname)s in %(module)s: %(message)s'\n)\ndefault_handler.setFormatter(formatter)\nmail_handler.setFormatter(formatter)\n Other Libraries Other libraries may use logging extensively, and you want to see relevant messages from those logs too. The simplest way to do this is to add handlers to the root logger instead of only the app logger. from flask.logging import default_handler\n\nroot = logging.getLogger()\nroot.addHandler(default_handler)\nroot.addHandler(mail_handler)\n Depending on your project, it may be more useful to configure each logger you care about separately, instead of configuring only the root logger. for logger in (\n    app.logger,\n    logging.getLogger('sqlalchemy'),\n    logging.getLogger('other_package'),\n):\n    logger.addHandler(default_handler)\n    logger.addHandler(mail_handler)\n Werkzeug Werkzeug logs basic request\/response information to the 'werkzeug' logger. If the root logger has no handlers configured, Werkzeug adds a StreamHandler to its logger. Flask Extensions Depending on the situation, an extension may choose to log to app.logger or its own named logger. Consult each extension\u2019s documentation for details.","title":"flask.logging.index"}]}
{"task_id":40133826,"prompt":"def f_40133826(mylist):\n\t","suffix":"\n\treturn ","canonical_solution":"pickle.dump(mylist, open('save.txt', 'wb'))","test_start":"\nimport pickle\n\ndef check(candidate):","test":["\n    candidate([i for i in range(10)])\n    data = pickle.load(open('save.txt', 'rb'))\n    assert data == [i for i in range(10)]\n","\n    candidate([\"hello\", \"world\", \"!\"])\n    data = pickle.load(open('save.txt', 'rb'))\n    assert data == [\"hello\", \"world\", \"!\"]\n"],"entry_point":"f_40133826","intent":"python save list `mylist` to file object 'save.txt'","library":["pickle"],"docs":[{"text":"array.tofile(f)  \nWrite all items (as machine values) to the file object f.","title":"python.library.array#array.array.tofile"},{"text":"_save(name, content)","title":"django.howto.custom-file-storage#django.core.files.storage._save"},{"text":"array.tolist()  \nConvert the array to an ordinary list with the same items.","title":"python.library.array#array.array.tolist"},{"text":"exception copy.Error  \nRaised for module specific errors.","title":"python.library.copy#copy.Error"},{"text":"tolist()  \nReturns a list containing the elements of this storage","title":"torch.storage#torch.FloatStorage.tolist"},{"text":"emit(record)  \nOutputs the record to the file.","title":"python.library.logging.handlers#logging.FileHandler.emit"},{"text":"fileobj  \nFile object registered.","title":"python.library.selectors#selectors.SelectorKey.fileobj"},{"text":"array.fromfile(f, n)  \nRead n items (as machine values) from the file object f and append them to the end of the array. If less than n items are available, EOFError is raised, but the items that were available are still inserted into the array.","title":"python.library.array#array.array.fromfile"},{"text":"save_to_buffer(self: torch._C.ScriptFunction, _extra_files: Dict[str, str] = {}) \u2192 bytes","title":"torch.generated.torch.jit.scriptfunction#torch.jit.ScriptFunction.save_to_buffer"},{"text":"save(self: torch._C.ScriptFunction, filename: str, _extra_files: Dict[str, str] = {}) \u2192 None","title":"torch.generated.torch.jit.scriptfunction#torch.jit.ScriptFunction.save"}]}
{"task_id":4490961,"prompt":"def f_4490961(P, T):\n\treturn ","suffix":"","canonical_solution":"scipy.tensordot(P, T, axes=[1, 1]).swapaxes(0, 1)","test_start":"\nimport scipy\nimport numpy as np\n\ndef check(candidate):","test":["\n    P = np.array([[6, 2, 7], [1, 1, 8], [8, 7, 1], [9, 6, 4], [2, 1, 1]])\n    T = np.array([[[9, 7, 2, 3], [9, 6, 8, 2], [6, 6, 2, 8]],\n                  [[4, 5, 5, 3], [1, 8, 3, 5], [2, 8, 1, 6]]])\n    result = np.array([[[114,  96,  42,  78], [ 66,  61,  26,  69], [141, 104,  74,  46], [159, 123,  74,  71],  [ 33,  26,  14,  16]], \n                      [[ 40, 102,  43,  70], [ 21,  77,  16,  56], [ 41, 104,  62,  65], [ 50, 125,  67,  81], [ 11,  26,  14,  17]]])\n    assert np.array_equal(candidate(P, T), result)\n"],"entry_point":"f_4490961","intent":"Multiply a matrix `P` with a 3d tensor `T` in scipy","library":["numpy","scipy"],"docs":[]}
{"task_id":2173087,"prompt":"def f_2173087():\n\treturn ","suffix":"","canonical_solution":"numpy.zeros((3, 3, 3))","test_start":"\nimport numpy \nimport numpy as np\n\ndef check(candidate):","test":["\n    result = np.array([[[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]],\n                          [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]],\n                          [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]])\n    assert np.array_equal(candidate(), result)\n"],"entry_point":"f_2173087","intent":"Create 3d array of zeroes of size `(3,3,3)`","library":["numpy"],"docs":[{"text":"zorder=0","title":"matplotlib.collections_api#matplotlib.collections.QuadMesh.zorder"},{"text":"matplotlib.axes.Axes.zorder   Axes.zorder=0","title":"matplotlib._as_gen.matplotlib.axes.axes.zorder"},{"text":"zorder=0","title":"matplotlib.collections_api#matplotlib.collections.TriMesh.zorder"},{"text":"numpy.ndarray.__bool__ method   ndarray.__bool__(\/)\n \nself != 0","title":"numpy.reference.generated.numpy.ndarray.__bool__"},{"text":"numpy.generic.__array__ method   generic.__array__()\n \nsc.__array__(dtype) return 0-dim array from scalar with specified dtype","title":"numpy.reference.generated.numpy.generic.__array__"},{"text":"numpy.ndarray.__pos__ method   ndarray.__pos__(\/)\n \n+self","title":"numpy.reference.generated.numpy.ndarray.__pos__"},{"text":"zorder=3","title":"matplotlib.offsetbox_api#matplotlib.offsetbox.AnnotationBbox.zorder"},{"text":"zorder=2","title":"matplotlib._as_gen.matplotlib.lines.line2d#matplotlib.lines.Line2D.zorder"},{"text":"zorder=0","title":"matplotlib.collections_api#matplotlib.collections.Collection.zorder"},{"text":"numpy.ndarray.__complex__ method   ndarray.__complex__()","title":"numpy.reference.generated.numpy.ndarray.__complex__"}]}
{"task_id":6266727,"prompt":"def f_6266727(content):\n\treturn ","suffix":"","canonical_solution":"\"\"\" \"\"\".join(content.split(' ')[:-1])","test_start":"\ndef check(candidate):","test":["\n    assert candidate('test') == ''\n","\n    assert candidate('this is an example content') == 'this is an example'\n","\n    assert candidate('  ') == ' '\n","\n    assert candidate('') == ''\n","\n    assert candidate('blank and tab\t') == 'blank and'\n"],"entry_point":"f_6266727","intent":"cut off the last word of a sentence `content`","library":[],"docs":[]}
{"task_id":30385151,"prompt":"def f_30385151(x):\n\t","suffix":"\n\treturn x","canonical_solution":"x = np.asarray(x).reshape(1, -1)[(0), :]","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    assert all(candidate(1.) == np.asarray(1.))\n","\n    assert all(candidate(123) == np.asarray(123))\n","\n    assert all(candidate('a') == np.asarray('a'))\n","\n    assert all(candidate(False) == np.asarray(False))\n"],"entry_point":"f_30385151","intent":"convert scalar `x` to array","library":["numpy"],"docs":[{"text":"numpy.generic.__array_wrap__ method   generic.__array_wrap__()\n \nsc.__array_wrap__(obj) return scalar from array","title":"numpy.reference.generated.numpy.generic.__array_wrap__"},{"text":"numpy.generic.__array__ method   generic.__array__()\n \nsc.__array__(dtype) return 0-dim array from scalar with specified dtype","title":"numpy.reference.generated.numpy.generic.__array__"},{"text":"numpy.float64[source]\n \nalias of numpy.double","title":"numpy.reference.arrays.scalars#numpy.float64"},{"text":"numpy.string_[source]\n \nalias of numpy.bytes_","title":"numpy.reference.arrays.scalars#numpy.string_"},{"text":"numpy.float32[source]\n \nalias of numpy.single","title":"numpy.reference.arrays.scalars#numpy.float32"},{"text":"numpy.unicode_[source]\n \nalias of numpy.str_","title":"numpy.reference.arrays.scalars#numpy.unicode_"},{"text":"numpy.complex64[source]\n \nalias of numpy.csingle","title":"numpy.reference.arrays.scalars#numpy.complex64"},{"text":"numpy.clongfloat[source]\n \nalias of numpy.clongdouble","title":"numpy.reference.arrays.scalars#numpy.clongfloat"},{"text":"numpy.longcomplex[source]\n \nalias of numpy.clongdouble","title":"numpy.reference.arrays.scalars#numpy.longcomplex"},{"text":"numpy.complex_[source]\n \nalias of numpy.cdouble","title":"numpy.reference.arrays.scalars#numpy.complex_"}]}
{"task_id":15856127,"prompt":"def f_15856127(L):\n\treturn ","suffix":"","canonical_solution":"sum(sum(i) if isinstance(i, list) else i for i in L)","test_start":"\ndef check(candidate):","test":["\n    assert candidate([1,2,3,4]) == 10\n","\n    assert candidate([[1],[2],[3],[4]]) == 10\n","\n    assert candidate([1,1,1,1]) == 4\n","\n    assert candidate([1,[2,3],[4]]) == 10\n","\n    assert candidate([]) == 0\n","\n    assert candidate([[], []]) == 0\n"],"entry_point":"f_15856127","intent":"sum all elements of nested list `L`","library":[],"docs":[]}
{"task_id":1592158,"prompt":"def f_1592158():\n\treturn ","suffix":"","canonical_solution":"struct.unpack('!f', bytes.fromhex('470FC614'))[0]","test_start":"\nimport struct \n\ndef check(candidate):","test":["\n    assert (candidate() - 36806.078125) < 1e-6\n","\n    assert (candidate() - 32806.079125) > 1e-6\n"],"entry_point":"f_1592158","intent":"convert hex string '470FC614' to a float number","library":["struct"],"docs":[{"text":"string.hexdigits  \nThe string '0123456789abcdefABCDEF'.","title":"python.library.string#string.hexdigits"},{"text":"classmethod float.fromhex(s)  \nClass method to return the float represented by a hexadecimal string s. The string s may have leading and trailing whitespace.","title":"python.library.stdtypes#float.fromhex"},{"text":"float()  \nCasts this storage to float type","title":"torch.storage#torch.FloatStorage.float"},{"text":"float.hex()  \nReturn a representation of a floating-point number as a hexadecimal string. For finite floating-point numbers, this representation will always include a leading 0x and a trailing p and exponent.","title":"python.library.stdtypes#float.hex"},{"text":"char()  \nCasts this storage to char type","title":"torch.storage#torch.FloatStorage.char"},{"text":"float()  \nCasts all floating point parameters and buffers to float datatype.  Returns \nself  Return type \nModule","title":"torch.generated.torch.nn.unflatten#torch.nn.Unflatten.float"},{"text":"int()  \nCasts this storage to int type","title":"torch.storage#torch.FloatStorage.int"},{"text":"radix()  \nJust returns 10, as this is Decimal, :)","title":"python.library.decimal#decimal.Context.radix"},{"text":"complex_float()  \nCasts this storage to complex float type","title":"torch.storage#torch.FloatStorage.complex_float"},{"text":"byte()  \nCasts this storage to byte type","title":"torch.storage#torch.FloatStorage.byte"}]}
{"task_id":5010536,"prompt":"def f_5010536(my_dict):\n\t","suffix":"\n\treturn my_dict","canonical_solution":"my_dict.update((x, y * 2) for x, y in list(my_dict.items()))","test_start":"\ndef check(candidate):","test":["\n    assert candidate({'a': [1], 'b': 4.9}) == {'a': [1, 1], 'b': 9.8}\n","\n    assert candidate({1:1}) == {1:2}\n","\n    assert candidate({(1,2):[1]}) == {(1,2):[1,1]}\n","\n    assert candidate({'asd':0}) == {'asd':0}\n","\n    assert candidate({}) == {}\n"],"entry_point":"f_5010536","intent":"Multiple each value by `2` for all keys in a dictionary `my_dict`","library":[],"docs":[]}
{"task_id":13745648,"prompt":"def f_13745648():\n\treturn ","suffix":"","canonical_solution":"subprocess.call('sleep.sh', shell=True)","test_start":"\nimport subprocess \nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    subprocess.call = Mock()\n    try:\n        candidate()\n    except:\n        assert False\n"],"entry_point":"f_13745648","intent":"running bash script 'sleep.sh'","library":["subprocess"],"docs":[{"text":"curses.napms(ms)  \nSleep for ms milliseconds.","title":"python.library.curses#curses.napms"},{"text":"cmd  \nCommand that was used to spawn the child process.","title":"python.library.subprocess#subprocess.TimeoutExpired.cmd"},{"text":"subprocess.SW_HIDE  \nHides the window. Another window will be activated.","title":"python.library.subprocess#subprocess.SW_HIDE"},{"text":"signal.SIGCONT  \nContinue the process if it is currently stopped Availability: Unix.","title":"python.library.signal#signal.SIGCONT"},{"text":"run(cmd)  \nProfile the cmd via exec().","title":"python.library.profile#profile.Profile.run"},{"text":"os.SCHED_IDLE  \nScheduling policy for extremely low priority background tasks.","title":"python.library.os#os.SCHED_IDLE"},{"text":"window.nodelay(flag)  \nIf flag is True, getch() will be non-blocking.","title":"python.library.curses#curses.window.nodelay"},{"text":"test.support.unix_shell  \nPath for shell if not on Windows; otherwise None.","title":"python.library.test#test.support.unix_shell"},{"text":"curses.delay_output(ms)  \nInsert an ms millisecond pause in output.","title":"python.library.curses#curses.delay_output"},{"text":"stdout  \nAlias for output, for symmetry with stderr.","title":"python.library.subprocess#subprocess.TimeoutExpired.stdout"}]}
{"task_id":44778,"prompt":"def f_44778(l):\n\treturn ","suffix":"","canonical_solution":"\"\"\",\"\"\".join(l)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(['a','b','c']) == 'a,b,c'\n","\n    assert candidate(['a','b']) == 'a,b'\n","\n    assert candidate([',',',',',']) == ',,,,,'\n","\n    assert candidate([' ','  ','c']) == ' ,  ,c'\n","\n    assert candidate([]) == ''\n"],"entry_point":"f_44778","intent":"Join elements of list `l` with a comma `,`","library":[],"docs":[]}
{"task_id":44778,"prompt":"def f_44778(myList):\n\t","suffix":"\n\treturn myList","canonical_solution":"myList = ','.join(map(str, myList))","test_start":"\ndef check(candidate):","test":["\n    assert candidate([1,2,3]) == '1,2,3'\n","\n    assert candidate([1,2,'a']) == '1,2,a'\n","\n    assert candidate([]) == ''\n","\n    assert candidate(['frg',3253]) == 'frg,3253'\n"],"entry_point":"f_44778","intent":"make a comma-separated string from a list `myList`","library":[],"docs":[]}
{"task_id":7286365,"prompt":"def f_7286365():\n\treturn ","suffix":"","canonical_solution":"list(reversed(list(range(10))))","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == [9,8,7,6,5,4,3,2,1,0]\n","\n    assert len(candidate()) == 10\n","\n    assert min(candidate()) == 0\n","\n    assert type(candidate()) == list\n","\n    assert type(candidate()[-2]) == int\n"],"entry_point":"f_7286365","intent":"reverse the list that contains 1 to 10","library":[],"docs":[]}
{"task_id":18454570,"prompt":"def f_18454570():\n\treturn ","suffix":"","canonical_solution":"'lamp, bag, mirror'.replace('bag,', '')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == 'lamp,  mirror'\n    assert type(candidate()) == str\n    assert len(candidate()) == 13\n    assert candidate().startswith('lamp')\n"],"entry_point":"f_18454570","intent":"remove substring 'bag,' from a string 'lamp, bag, mirror'","library":[],"docs":[]}
{"task_id":4357787,"prompt":"def f_4357787(s):\n\treturn ","suffix":"","canonical_solution":"\"\"\".\"\"\".join(s.split('.')[::-1])","test_start":"\ndef check(candidate):","test":["\n    assert candidate('apple.orange.red.green.yellow') == 'yellow.green.red.orange.apple'\n","\n    assert candidate('apple') == 'apple'\n","\n    assert candidate('apple.orange') == 'orange.apple'\n","\n    assert candidate('123.456') == '456.123'\n","\n    assert candidate('.') == '.'\n"],"entry_point":"f_4357787","intent":"Reverse the order of words, delimited by `.`, in string `s`","library":[],"docs":[]}
{"task_id":21787496,"prompt":"def f_21787496(s):\n\treturn ","suffix":"","canonical_solution":"datetime.datetime.fromtimestamp(s).strftime('%Y-%m-%d %H:%M:%S.%f')","test_start":"\nimport time\nimport datetime\n\ndef check(candidate): ","test":["\n    assert candidate(1236472) == '1970-01-15 07:27:52.000000'\n","\n    assert candidate(0) == '1970-01-01 00:00:00.000000'\n","\n    assert candidate(5.3) == '1970-01-01 00:00:05.300000'\n"],"entry_point":"f_21787496","intent":"convert epoch time represented as milliseconds `s` to string using format '%Y-%m-%d %H:%M:%S.%f'","library":["datetime","time"],"docs":[{"text":"st_atime_ns  \nTime of most recent access expressed in nanoseconds as an integer.","title":"python.library.os#os.stat_result.st_atime_ns"},{"text":"pandas.tseries.offsets.Milli.freqstr   Milli.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.milli.freqstr"},{"text":"st_mtime_ns  \nTime of most recent content modification expressed in nanoseconds as an integer.","title":"python.library.os#os.stat_result.st_mtime_ns"},{"text":"pandas.Timestamp.nanosecond   Timestamp.nanosecond","title":"pandas.reference.api.pandas.timestamp.nanosecond"},{"text":"pandas.Timestamp.microsecond   Timestamp.microsecond","title":"pandas.reference.api.pandas.timestamp.microsecond"},{"text":"time.microsecond  \nIn range(1000000).","title":"python.library.datetime#datetime.time.microsecond"},{"text":"pandas.tseries.offsets.Minute.freqstr   Minute.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.minute.freqstr"},{"text":"pandas.Timestamp.fromisoformat   Timestamp.fromisoformat()\n \nstring -> datetime from datetime.isoformat() output","title":"pandas.reference.api.pandas.timestamp.fromisoformat"},{"text":"datetime.microsecond  \nIn range(1000000).","title":"python.library.datetime#datetime.datetime.microsecond"},{"text":"pandas.tseries.offsets.Nano.freqstr   Nano.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.nano.freqstr"}]}
{"task_id":21787496,"prompt":"def f_21787496():\n\treturn ","suffix":"","canonical_solution":"time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(1236472051807 \/ 1000.0))","test_start":"\nimport time\n\ndef check(candidate): ","test":["\n    assert candidate() == '2009-03-08 00:27:31'\n"],"entry_point":"f_21787496","intent":"parse milliseconds epoch time '1236472051807' to format '%Y-%m-%d %H:%M:%S'","library":["time"],"docs":[{"text":"time.microsecond  \nIn range(1000000).","title":"python.library.datetime#datetime.time.microsecond"},{"text":"datetime.microsecond  \nIn range(1000000).","title":"python.library.datetime#datetime.datetime.microsecond"},{"text":"st_atime_ns  \nTime of most recent access expressed in nanoseconds as an integer.","title":"python.library.os#os.stat_result.st_atime_ns"},{"text":"pandas.Timestamp.microsecond   Timestamp.microsecond","title":"pandas.reference.api.pandas.timestamp.microsecond"},{"text":"pandas.tseries.offsets.Milli.freqstr   Milli.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.milli.freqstr"},{"text":"st_mtime_ns  \nTime of most recent content modification expressed in nanoseconds as an integer.","title":"python.library.os#os.stat_result.st_mtime_ns"},{"text":"pandas.Timestamp.nanosecond   Timestamp.nanosecond","title":"pandas.reference.api.pandas.timestamp.nanosecond"},{"text":"pandas.tseries.offsets.Milli.n   Milli.n","title":"pandas.reference.api.pandas.tseries.offsets.milli.n"},{"text":"pandas.Timestamp.min   Timestamp.min=Timestamp('1677-09-21 00:12:43.145224193')","title":"pandas.reference.api.pandas.timestamp.min"},{"text":"pandas.Timestamp.fromisoformat   Timestamp.fromisoformat()\n \nstring -> datetime from datetime.isoformat() output","title":"pandas.reference.api.pandas.timestamp.fromisoformat"}]}
{"task_id":20573459,"prompt":"def f_20573459():\n\treturn ","suffix":"","canonical_solution":"(datetime.datetime.now() - datetime.timedelta(days=7)).date()","test_start":"\nimport datetime\n\ndef check(candidate): ","test":["\n    assert datetime.datetime.now().date() - candidate() < datetime.timedelta(days = 7, seconds = 1)\n","\n    assert datetime.datetime.now().date() - candidate() >= datetime.timedelta(days = 7)\n"],"entry_point":"f_20573459","intent":"get the date 7 days before the current date","library":["datetime"],"docs":[{"text":"pandas.Timestamp.day   Timestamp.day","title":"pandas.reference.api.pandas.timestamp.day"},{"text":"date.month  \nBetween 1 and 12 inclusive.","title":"python.library.datetime#datetime.date.month"},{"text":"date_attrs","title":"django.ref.forms.widgets#django.forms.SplitDateTimeWidget.date_attrs"},{"text":"classmethod date.today()  \nReturn the current local date. This is equivalent to date.fromtimestamp(time.time()).","title":"python.library.datetime#datetime.date.today"},{"text":"datetime.date()  \nReturn date object with same year, month and day.","title":"python.library.datetime#datetime.datetime.date"},{"text":"get_date()  \nReturn the delivery date of the message as a floating-point number representing seconds since the epoch.","title":"python.library.mailbox#mailbox.MaildirMessage.get_date"},{"text":"get_offset()[source]","title":"matplotlib.dates_api#matplotlib.dates.ConciseDateFormatter.get_offset"},{"text":"pandas.tseries.offsets.Day.nanos   Day.nanos","title":"pandas.reference.api.pandas.tseries.offsets.day.nanos"},{"text":"date_format  \nSimilar to DateInput.format","title":"django.ref.forms.widgets#django.forms.SplitDateTimeWidget.date_format"},{"text":"pandas.tseries.offsets.Day.apply   Day.apply()","title":"pandas.reference.api.pandas.tseries.offsets.day.apply"}]}
{"task_id":15352457,"prompt":"def f_15352457(column, data):\n\treturn ","suffix":"","canonical_solution":"sum(row[column] for row in data)","test_start":"\ndef check(candidate): ","test":["\n    assert candidate(1, [[1,2,3], [4,5,6]]) == 7\n","\n    assert candidate(0, [[1,1,1], [0,1,1]]) == 1\n","\n    assert candidate(5, [[1,1,1,1,1,2], [0,1,1,1,1,1,1,1,1,1,1]]) == 3\n","\n    assert candidate(0, [[1],[2],[3],[4]]) == 10\n"],"entry_point":"f_15352457","intent":"sum elements at index `column` of each list in list `data`","library":[],"docs":[]}
{"task_id":15352457,"prompt":"def f_15352457(array):\n\treturn ","suffix":"","canonical_solution":"[sum(row[i] for row in array) for i in range(len(array[0]))]","test_start":"\ndef check(candidate): ","test":["\n    assert candidate([[1,2,3], [4,5,6]]) == [5, 7, 9]\n","\n    assert candidate([[1,1,1], [0,1,1]]) == [1, 2, 2]\n","\n    assert candidate([[1,1,1,1,1,2], [0,1,1,1,1,1,1,1,1,1,1]]) == [1, 2, 2, 2, 2, 3]\n","\n    assert candidate([[1],[2],[3],[4]]) == [10]\n"],"entry_point":"f_15352457","intent":"sum columns of a list `array`","library":[],"docs":[]}
{"task_id":23164058,"prompt":"def f_23164058():\n\treturn ","suffix":"","canonical_solution":"base64.b64encode(bytes('your string', 'utf-8'))","test_start":"\nimport base64\n\ndef check(candidate): ","test":["\n    assert candidate() == b'eW91ciBzdHJpbmc='\n"],"entry_point":"f_23164058","intent":"encode binary string 'your string' to base64 code","library":["base64"],"docs":[{"text":"base64.b16encode(s)  \nEncode the bytes-like object s using Base16 and return the encoded bytes.","title":"python.library.base64#base64.b16encode"},{"text":"winreg.REG_BINARY  \nBinary data in any form.","title":"python.library.winreg#winreg.REG_BINARY"},{"text":"base64.b32encode(s)  \nEncode the bytes-like object s using Base32 and return the encoded bytes.","title":"python.library.base64#base64.b32encode"},{"text":"binascii.a2b_base64(string)  \nConvert a block of base64 data back to binary and return the binary data. More than one line may be passed at a time.","title":"python.library.binascii#binascii.a2b_base64"},{"text":"numpy.string_[source]\n \nalias of numpy.bytes_","title":"numpy.reference.arrays.scalars#numpy.string_"},{"text":"base64.standard_b64encode(s)  \nEncode bytes-like object s using the standard Base64 alphabet and return the encoded bytes.","title":"python.library.base64#base64.standard_b64encode"},{"text":"HttpResponse.content  \nA bytestring representing the content, encoded from a string if necessary.","title":"django.ref.request-response#django.http.HttpResponse.content"},{"text":"base64.encode(input, output)  \nEncode the contents of the binary input file and write the resulting base64 encoded data to the output file. input and output must be file objects. input will be read until input.read() returns an empty bytes object. encode() inserts a newline character (b'\\n') after every 76 bytes of the output, as well as ensuring that the output always ends with a newline, as per RFC 2045 (MIME).","title":"python.library.base64#base64.encode"},{"text":"compressed","title":"python.library.ipaddress#ipaddress.IPv4Address.compressed"},{"text":"urlsafe_base64_encode(s) [source]\n \nEncodes a bytestring to a base64 string for use in URLs, stripping any trailing equal signs.","title":"django.ref.utils#django.utils.http.urlsafe_base64_encode"}]}
{"task_id":11533274,"prompt":"def f_11533274(dicts):\n\treturn ","suffix":"","canonical_solution":"dict((k, [d[k] for d in dicts]) for k in dicts[0])","test_start":"\ndef check(candidate): ","test":["\n    assert candidate([{'cat': 1, 'dog': 3}, {'cat' : 2, 'dog': ['happy']}]) ==         {'cat': [1, 2], 'dog': [3, ['happy']]}\n","\n    assert candidate([{'cat': 1}, {'cat' : 2}]) != {'cat': 3}\n"],"entry_point":"f_11533274","intent":"combine list of dictionaries `dicts` with the same keys in each list to a single dictionary","library":[],"docs":[]}
{"task_id":11533274,"prompt":"def f_11533274(dicts):\n\treturn ","suffix":"","canonical_solution":"{k: [d[k] for d in dicts] for k in dicts[0]}","test_start":"\ndef check(candidate): ","test":["\n    assert candidate([{'cat': 1, 'dog': 3}, {'cat' : 2, 'dog': ['happy']}]) ==         {'cat': [1, 2], 'dog': [3, ['happy']]}\n","\n    assert candidate([{'cat': 1}, {'cat' : 2}]) != {'cat': 3}\n"],"entry_point":"f_11533274","intent":"Merge a nested dictionary `dicts` into a flat dictionary by concatenating nested values with the same key `k`","library":[],"docs":[]}
{"task_id":14026704,"prompt":"def f_14026704(request):\n\treturn ","suffix":"","canonical_solution":"request.args['myParam']","test_start":"\nimport multidict\n\nclass Request:\n        def __init__(self, args):\n            self.args = args\n\ndef check(candidate): ","test":["\n    args = multidict.MultiDict([('myParam' , 'popeye')])\n    request = Request(args)\n    assert candidate(request) == 'popeye'\n"],"entry_point":"f_14026704","intent":"get the url parameter 'myParam' in a Flask view","library":["multidict"],"docs":[]}
{"task_id":11236006,"prompt":"def f_11236006(mylist):\n\treturn ","suffix":"","canonical_solution":"[k for k, v in list(Counter(mylist).items()) if v > 1]","test_start":"\nfrom collections import Counter\n\ndef check(candidate):","test":["\n    assert candidate([1,3,2,2,1,4]) == [1, 2]\n","\n    assert candidate([1,3,2,2,1,4]) != [3,4]\n","\n    assert candidate([]) == []\n","\n    assert candidate([1,1,1,1,1]) == [1]\n","\n    assert candidate([1.,1.,1.]) == [1.]\n"],"entry_point":"f_11236006","intent":"identify duplicate values in list `mylist`","library":["collections"],"docs":[{"text":"pandas.Index.has_duplicates   propertyIndex.has_duplicates\n \nCheck if the Index has duplicate values.  Returns \n bool\n\nWhether or not the Index has duplicate values.     Examples \n>>> idx = pd.Index([1, 5, 7, 7])\n>>> idx.has_duplicates\nTrue\n  \n>>> idx = pd.Index([1, 5, 7])\n>>> idx.has_duplicates\nFalse\n  \n>>> idx = pd.Index([\"Watermelon\", \"Orange\", \"Apple\",\n...                 \"Watermelon\"]).astype(\"category\")\n>>> idx.has_duplicates\nTrue\n  \n>>> idx = pd.Index([\"Orange\", \"Apple\",\n...                 \"Watermelon\"]).astype(\"category\")\n>>> idx.has_duplicates\nFalse","title":"pandas.reference.api.pandas.index.has_duplicates"},{"text":"numpy.lib.recfunctions.find_duplicates(a, key=None, ignoremask=True, return_index=False)[source]\n \nFind the duplicates in a structured array along a given key  Parameters \n \naarray-like\n\n\nInput array  \nkey{string, None}, optional\n\n\nName of the fields along which to check the duplicates. If None, the search is performed by records  \nignoremask{True, False}, optional\n\n\nWhether masked data should be discarded or considered as duplicates.  \nreturn_index{False, True}, optional\n\n\nWhether to return the indices of the duplicated values.     Examples >>> from numpy.lib import recfunctions as rfn\n>>> ndtype = [('a', int)]\n>>> a = np.ma.array([1, 1, 1, 2, 2, 3, 3],\n...         mask=[0, 0, 1, 0, 0, 0, 1]).view(ndtype)\n>>> rfn.find_duplicates(a, ignoremask=True, return_index=True)\n(masked_array(data=[(1,), (1,), (2,), (2,)],\n             mask=[(False,), (False,), (False,), (False,)],\n       fill_value=(999999,),\n            dtype=[('a', '<i8')]), array([0, 1, 3, 4]))","title":"numpy.user.basics.rec#numpy.lib.recfunctions.find_duplicates"},{"text":"pandas.tseries.offsets.Second.copy   Second.copy()","title":"pandas.reference.api.pandas.tseries.offsets.second.copy"},{"text":"array.tolist()  \nConvert the array to an ordinary list with the same items.","title":"python.library.array#array.array.tolist"},{"text":"copy()  \nReturn a duplicate of the context.","title":"python.library.decimal#decimal.Context.copy"},{"text":"pandas.Index.is_unique   Index.is_unique\n \nReturn if the index has unique values.","title":"pandas.reference.api.pandas.index.is_unique"},{"text":"remove(value)  \nRemove the first occurrence of value. If not found, raises a ValueError.","title":"python.library.collections#collections.deque.remove"},{"text":"@enum.unique","title":"python.library.enum#enum.unique"},{"text":"pandas.MultiIndex.codes   propertyMultiIndex.codes","title":"pandas.reference.api.pandas.multiindex.codes"},{"text":"pandas.tseries.offsets.BQuarterEnd.copy   BQuarterEnd.copy()","title":"pandas.reference.api.pandas.tseries.offsets.bquarterend.copy"}]}
{"task_id":20211942,"prompt":"def f_20211942(db):\n\treturn ","suffix":"","canonical_solution":"db.execute(\"INSERT INTO present VALUES('test2', ?, 10)\", (None,))","test_start":"\nimport sqlite3\n\ndef check(candidate):","test":["\n    sqliteConnection = sqlite3.connect('dev.db')\n    db = sqliteConnection.cursor()\n    print(\"Database created and Successfully Connected to SQLite\")\n    db.execute(\"CREATE TABLE present (name VARCHAR(5), age INTEGER, height INTEGER)\")\n    try:\n        candidate(db)\n    except:\n        assert False\n"],"entry_point":"f_20211942","intent":"Insert a 'None' value into a SQLite3 table.","library":["sqlite3"],"docs":[{"text":"none()","title":"django.ref.models.querysets#django.db.models.query.QuerySet.none"},{"text":"exception sqlite3.Warning  \nA subclass of Exception.","title":"python.library.sqlite3#sqlite3.Warning"},{"text":"winreg.REG_NONE  \nNo defined value type.","title":"python.library.winreg#winreg.REG_NONE"},{"text":"commit(using=None)","title":"django.topics.db.transactions#django.db.transaction.commit"},{"text":"kevent.udata  \nUser defined value.","title":"python.library.select#select.kevent.udata"},{"text":"numpy.dtype.type attribute   dtype.type = None","title":"numpy.reference.generated.numpy.dtype.type"},{"text":"step  \nThe value of the step parameter (or 1 if the parameter was not supplied)","title":"python.library.stdtypes#range.step"},{"text":"colno  \nThe column corresponding to pos (may be None).","title":"python.library.re#re.error.colno"},{"text":"executemany(sql, seq_of_parameters)  \nExecutes a parameterized SQL command against all parameter sequences or mappings found in the sequence seq_of_parameters. The sqlite3 module also allows using an iterator yielding parameters instead of a sequence. import sqlite3\n\nclass IterChars:\n    def __init__(self):\n        self.count = ord('a')\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.count > ord('z'):\n            raise StopIteration\n        self.count += 1\n        return (chr(self.count - 1),) # this is a 1-tuple\n\ncon = sqlite3.connect(\":memory:\")\ncur = con.cursor()\ncur.execute(\"create table characters(c)\")\n\ntheIter = IterChars()\ncur.executemany(\"insert into characters(c) values (?)\", theIter)\n\ncur.execute(\"select c from characters\")\nprint(cur.fetchall())\n\ncon.close()\n Here\u2019s a shorter example using a generator: import sqlite3\nimport string\n\ndef char_generator():\n    for c in string.ascii_lowercase:\n        yield (c,)\n\ncon = sqlite3.connect(\":memory:\")\ncur = con.cursor()\ncur.execute(\"create table characters(c)\")\n\ncur.executemany(\"insert into characters(c) values (?)\", char_generator())\n\ncur.execute(\"select c from characters\")\nprint(cur.fetchall())\n\ncon.close()","title":"python.library.sqlite3#sqlite3.Cursor.executemany"},{"text":"fetchone()  \nFetches the next row of a query result set, returning a single sequence, or None when no more data is available.","title":"python.library.sqlite3#sqlite3.Cursor.fetchone"}]}
{"task_id":406121,"prompt":"def f_406121(list_of_menuitems):\n\treturn ","suffix":"","canonical_solution":"[image for menuitem in list_of_menuitems for image in menuitem]","test_start":"\nfrom collections import Counter\n\ndef check(candidate):","test":["\n    assert candidate([[1,2],[3,4,5]]) == [1,2,3,4,5]\n","\n    assert candidate([[],[]]) == []\n","\n    assert candidate([[1,1,1], []]) == [1,1,1]\n","\n    assert candidate([['1'],['2']]) == ['1','2']\n"],"entry_point":"f_406121","intent":"flatten list `list_of_menuitems`","library":["collections"],"docs":[]}
{"task_id":4741537,"prompt":"def f_4741537(a, b):\n\t","suffix":"\n\treturn a","canonical_solution":"a.extend(b)","test_start":"\ndef check(candidate):","test":["\n    assert candidate([1, 2, 2, 3], {4, 5, 2}) == [1, 2, 2, 3, 2, 4, 5]\n","\n    assert candidate([], {4,5,2}) == [2,4,5]\n","\n    assert candidate([1,2,3,4],{2}) == [1,2,3,4,2]\n","\n    assert candidate([1], {'a'}) == [1, 'a']\n"],"entry_point":"f_4741537","intent":"append elements of a set `b` to a list `a`","library":[],"docs":[]}
{"task_id":15851568,"prompt":"def f_15851568(x):\n\treturn ","suffix":"","canonical_solution":"x.rpartition('-')[0]","test_start":"\ndef check(candidate):","test":["\n    assert candidate('djhajhdjk-dadwqd-dahdjkahsk') == 'djhajhdjk-dadwqd'\n","\n    assert candidate('\/-\/') == '\/'\n","\n    assert candidate('---') == '--'\n","\n    assert candidate('') == ''\n"],"entry_point":"f_15851568","intent":"Split a string `x` by last occurrence of character `-`","library":[],"docs":[]}
{"task_id":15851568,"prompt":"def f_15851568(x):\n\treturn ","suffix":"","canonical_solution":"x.rsplit('-', 1)[0]","test_start":"\ndef check(candidate):","test":["\n    assert candidate('2022-03-01') == '2022-03'\n","\n    assert candidate('2020-2022') == '2020'\n"],"entry_point":"f_15851568","intent":"get the last part of a string before the character '-'","library":[],"docs":[]}
{"task_id":17438096,"prompt":"def f_17438096(filename, ftp):\n\t","suffix":"\n\treturn ","canonical_solution":"ftp.storlines('STOR ' + filename, open(filename, 'r'))","test_start":"\nimport ftplib\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    ftplib.FTP = Mock()\n    ftp = ftplib.FTP(\"10.10.10.10\")\n    ftp.storlines = Mock()\n    file_name = 'readme.txt'\n    with open (file_name, 'a') as f:\n        f.write('apple')\n    candidate(file_name, ftp)\n"],"entry_point":"f_17438096","intent":"upload file using FTP","library":["ftplib"],"docs":[{"text":"class urllib.request.FTPHandler  \nOpen FTP URLs.","title":"python.library.urllib.request#urllib.request.FTPHandler"},{"text":"class UploadedFile","title":"django.ref.files.uploads#django.core.files.uploadedfile.UploadedFile"},{"text":"FTP.mkd(pathname)  \nCreate a new directory on the server.","title":"python.library.ftplib#ftplib.FTP.mkd"},{"text":"FTP.abort()  \nAbort a file transfer that is in progress. Using this does not always work, but it\u2019s worth a try.","title":"python.library.ftplib#ftplib.FTP.abort"},{"text":"FTPHandler.ftp_open(req)  \nOpen the FTP file indicated by req. The login is always done with empty username and password.","title":"python.library.urllib.request#urllib.request.FTPHandler.ftp_open"},{"text":"exception http.client.UnimplementedFileMode  \nA subclass of HTTPException.","title":"python.library.http.client#http.client.UnimplementedFileMode"},{"text":"rfile  \nAn io.BufferedIOBase input stream, ready to read from the start of the optional input data.","title":"python.library.http.server#http.server.BaseHTTPRequestHandler.rfile"},{"text":"FTP.delete(filename)  \nRemove the file named filename from the server. If successful, returns the text of the response, otherwise raises error_perm on permission errors or error_reply on other errors.","title":"python.library.ftplib#ftplib.FTP.delete"},{"text":"class FileField(upload_to=None, max_length=100, **options)","title":"django.ref.models.fields#django.db.models.FileField"},{"text":"class TemporaryFileUploadHandler","title":"django.ref.files.uploads#django.core.files.uploadhandler.TemporaryFileUploadHandler"}]}
{"task_id":28742436,"prompt":"def f_28742436():\n\treturn ","suffix":"","canonical_solution":"np.maximum([2, 3, 4], [1, 5, 2])","test_start":"\nimport numpy as np \n\ndef check(candidate):","test":["\n    assert all(candidate() == np.array([2, 5, 4]))\n"],"entry_point":"f_28742436","intent":"create array containing the maximum value of respective elements of array `[2, 3, 4]` and array `[1, 5, 2]`","library":["numpy"],"docs":[{"text":"numpy.recarray.max method   recarray.max(axis=None, out=None, keepdims=False, initial=<no value>, where=True)\n \nReturn the maximum along a given axis. Refer to numpy.amax for full documentation.  See also  numpy.amax\n\nequivalent function","title":"numpy.reference.generated.numpy.recarray.max"},{"text":"max(x, y)  \nCompares two values numerically and returns the maximum.","title":"python.library.decimal#decimal.Context.max"},{"text":"maximum(other) \u2192 Tensor  \nSee torch.maximum()","title":"torch.tensors#torch.Tensor.maximum"},{"text":"numpy.ndarray.max method   ndarray.max(axis=None, out=None, keepdims=False, initial=<no value>, where=True)\n \nReturn the maximum along a given axis. Refer to numpy.amax for full documentation.  See also  numpy.amax\n\nequivalent function","title":"numpy.reference.generated.numpy.ndarray.max"},{"text":"numpy.maximum   numpy.maximum(x1, x2, \/, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj]) = <ufunc 'maximum'>\n \nElement-wise maximum of array elements. Compare two arrays and returns a new array containing the element-wise maxima. If one of the elements being compared is a NaN, then that element is returned. If both elements are NaNs then the first is returned. The latter distinction is important for complex NaNs, which are defined as at least one of the real or imaginary parts being a NaN. The net effect is that NaNs are propagated.  Parameters \n \nx1, x2array_like\n\n\nThe arrays holding the elements to be compared. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).  \noutndarray, None, or tuple of ndarray and None, optional\n\n\nA location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.  \nwherearray_like, optional\n\n\nThis condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.  **kwargs\n\nFor other keyword-only arguments, see the ufunc docs.    Returns \n \nyndarray or scalar\n\n\nThe maximum of x1 and x2, element-wise. This is a scalar if both x1 and x2 are scalars.      See also  minimum\n\nElement-wise minimum of two arrays, propagates NaNs.  fmax\n\nElement-wise maximum of two arrays, ignores NaNs.  amax\n\nThe maximum value of an array along a given axis, propagates NaNs.  nanmax\n\nThe maximum value of an array along a given axis, ignores NaNs.  \nfmin, amin, nanmin\n\n  Notes The maximum is equivalent to np.where(x1 >= x2, x1, x2) when neither x1 nor x2 are nans, but it is faster and does proper broadcasting. Examples >>> np.maximum([2, 3, 4], [1, 5, 2])\narray([2, 5, 4])\n >>> np.maximum(np.eye(2), [0.5, 2]) # broadcasting\narray([[ 1. ,  2. ],\n       [ 0.5,  2. ]])\n >>> np.maximum([np.nan, 0, np.nan], [0, np.nan, np.nan])\narray([nan, nan, nan])\n>>> np.maximum(np.Inf, 1)\ninf","title":"numpy.reference.generated.numpy.maximum"},{"text":"decimal.MAX_EMAX","title":"python.library.decimal#decimal.MAX_EMAX"},{"text":"max(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)  \nSee torch.max()","title":"torch.tensors#torch.Tensor.max"},{"text":"numpy.recarray.argmax method   recarray.argmax(axis=None, out=None)\n \nReturn indices of the maximum values along the given axis. Refer to numpy.argmax for full documentation.  See also  numpy.argmax\n\nequivalent function","title":"numpy.reference.generated.numpy.recarray.argmax"},{"text":"decimal.MAX_PREC","title":"python.library.decimal#decimal.MAX_PREC"},{"text":"numpy.record.max method   record.max()\n \nScalar method identical to the corresponding array attribute. Please see ndarray.max.","title":"numpy.reference.generated.numpy.record.max"}]}
{"task_id":34280147,"prompt":"def f_34280147(l):\n\treturn ","suffix":"","canonical_solution":"l[3:] + l[:3]","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"my-string\") == \"stringmy-\"\n","\n    assert candidate(\"my \") == \"my \"\n","\n    assert candidate(\"n;ho0-4w606[q\") == \"o0-4w606[qn;h\"\n"],"entry_point":"f_34280147","intent":"print a list `l` and move first 3 elements to the end of the list","library":[],"docs":[]}
{"task_id":4172131,"prompt":"def f_4172131():\n\treturn ","suffix":"","canonical_solution":"[int(1000 * random.random()) for i in range(10000)]","test_start":"\nimport random\n\ndef check(candidate):","test":["\n    result = candidate()\n    assert isinstance(result, list)\n    assert all([isinstance(item, int) for item in result])\n"],"entry_point":"f_4172131","intent":"create a random list of integers","library":["random"],"docs":[{"text":"uuid.uuid4()  \nGenerate a random UUID.","title":"python.library.uuid#uuid.uuid4"},{"text":"random.randint(a, b)  \nReturn a random integer N such that a <= N <= b. Alias for randrange(a, b+1).","title":"python.library.random#random.randint"},{"text":"random.random()  \nReturn the next random floating point number in the range [0.0, 1.0).","title":"python.library.random#random.random"},{"text":"secrets.randbits(k)  \nReturn an int with k random bits.","title":"python.library.secrets#secrets.randbits"},{"text":"secrets.randbelow(n)  \nReturn a random int in the range [0, n).","title":"python.library.secrets#secrets.randbelow"},{"text":"class Random(**extra)","title":"django.ref.models.database-functions#django.db.models.functions.Random"},{"text":"array.tolist()  \nConvert the array to an ordinary list with the same items.","title":"python.library.array#array.array.tolist"},{"text":"class RandomUUID","title":"django.ref.contrib.postgres.functions#django.contrib.postgres.functions.RandomUUID"},{"text":"numpy.random.RandomState.random_integers method   random.RandomState.random_integers(low, high=None, size=None)\n \nRandom integers of type np.int_ between low and high, inclusive. Return random integers of type np.int_ from the \u201cdiscrete uniform\u201d distribution in the closed interval [low, high]. If high is None (the default), then results are from [1, low]. The np.int_ type translates to the C long integer type and its precision is platform dependent. This function has been deprecated. Use randint instead.  Deprecated since version 1.11.0.   Parameters \n \nlowint\n\n\nLowest (signed) integer to be drawn from the distribution (unless high=None, in which case this parameter is the highest such integer).  \nhighint, optional\n\n\nIf provided, the largest (signed) integer to be drawn from the distribution (see above for behavior if high=None).  \nsizeint or tuple of ints, optional\n\n\nOutput shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.    Returns \n \noutint or ndarray of ints\n\n\nsize-shaped array of random integers from the appropriate distribution, or a single such random int if size not provided.      See also  randint\n\nSimilar to random_integers, only for the half-open interval [low, high), and 0 is the lowest value if high is omitted.    Notes To sample from N evenly spaced floating-point numbers between a and b, use: a + (b - a) * (np.random.random_integers(N) - 1) \/ (N - 1.)\n Examples >>> np.random.random_integers(5)\n4 # random\n>>> type(np.random.random_integers(5))\n<class 'numpy.int64'>\n>>> np.random.random_integers(5, size=(3,2))\narray([[5, 4], # random\n       [3, 3],\n       [4, 5]])\n Choose five random numbers from the set of five evenly-spaced numbers between 0 and 2.5, inclusive (i.e., from the set \\({0, 5\/8, 10\/8, 15\/8, 20\/8}\\)): >>> 2.5 * (np.random.random_integers(5, size=(5,)) - 1) \/ 4.\narray([ 0.625,  1.25 ,  0.625,  0.625,  2.5  ]) # random\n Roll two six sided dice 1000 times and sum the results: >>> d1 = np.random.random_integers(1, 6, 1000)\n>>> d2 = np.random.random_integers(1, 6, 1000)\n>>> dsums = d1 + d2\n Display results as a histogram: >>> import matplotlib.pyplot as plt\n>>> count, bins, ignored = plt.hist(dsums, 11, density=True)\n>>> plt.show()","title":"numpy.reference.random.generated.numpy.random.randomstate.random_integers"},{"text":"random.choice(seq)  \nReturn a random element from the non-empty sequence seq. If seq is empty, raises IndexError.","title":"python.library.random#random.choice"}]}
{"task_id":6677332,"prompt":"def f_6677332():\n\treturn ","suffix":"","canonical_solution":"datetime.datetime.now().strftime('%H:%M:%S.%f')","test_start":"\nimport datetime\n\ndef check(candidate):","test":["\n    time_now = datetime.datetime.now().strftime('%H:%M:%S.%f')\n    assert candidate().split('.')[0] == time_now.split('.')[0]\n"],"entry_point":"f_6677332","intent":"Using %f with strftime() in Python to get microseconds","library":["datetime"],"docs":[{"text":"pandas.Timestamp.microsecond   Timestamp.microsecond","title":"pandas.reference.api.pandas.timestamp.microsecond"},{"text":"time.microsecond  \nIn range(1000000).","title":"python.library.datetime#datetime.time.microsecond"},{"text":"datetime.microsecond  \nIn range(1000000).","title":"python.library.datetime#datetime.datetime.microsecond"},{"text":"st_atime_ns  \nTime of most recent access expressed in nanoseconds as an integer.","title":"python.library.os#os.stat_result.st_atime_ns"},{"text":"pandas.Timestamp.nanosecond   Timestamp.nanosecond","title":"pandas.reference.api.pandas.timestamp.nanosecond"},{"text":"pandas.tseries.offsets.Micro.freqstr   Micro.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.micro.freqstr"},{"text":"st_mtime_ns  \nTime of most recent content modification expressed in nanoseconds as an integer.","title":"python.library.os#os.stat_result.st_mtime_ns"},{"text":"pandas.Timestamp.freq   Timestamp.freq","title":"pandas.reference.api.pandas.timestamp.freq"},{"text":"st_atime  \nTime of most recent access expressed in seconds.","title":"python.library.os#os.stat_result.st_atime"},{"text":"pandas.tseries.offsets.Minute.freqstr   Minute.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.minute.freqstr"}]}
{"task_id":15325182,"prompt":"def f_15325182(df):\n\treturn ","suffix":"","canonical_solution":"df.b.str.contains('^f')","test_start":"\nimport pandas as pd \n\ndef check(candidate):","test":["\n    df = pd.DataFrame([[1, 'fat'], [2, 'hip'], [3, 'foo']], columns = ['a', 'b'])\n    expected = [True, False, True]\n    actual = candidate(df)\n    for i in range (0, len(expected)):\n        assert expected[i] == actual[i]\n"],"entry_point":"f_15325182","intent":"filter rows in pandas starting with alphabet 'f' using regular expression.","library":["pandas"],"docs":[{"text":"pandas.DataFrame.filter   DataFrame.filter(items=None, like=None, regex=None, axis=None)[source]\n \nSubset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index.  Parameters \n \nitems:list-like\n\n\nKeep labels from axis which are in items.  \nlike:str\n\n\nKeep labels from axis for which \u201clike in label == True\u201d.  \nregex:str (regular expression)\n\n\nKeep labels from axis for which re.search(regex, label) == True.  \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default None\n\n\nThe axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, \u2018index\u2019 for Series, \u2018columns\u2019 for DataFrame.    Returns \n same type as input object\n    See also  DataFrame.loc\n\nAccess a group of rows and columns by label(s) or a boolean array.    Notes The items, like, and regex parameters are enforced to be mutually exclusive. axis defaults to the info axis that is used when indexing with []. Examples \n>>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),\n...                   index=['mouse', 'rabbit'],\n...                   columns=['one', 'two', 'three'])\n>>> df\n        one  two  three\nmouse     1    2      3\nrabbit    4    5      6\n  \n>>> # select columns by name\n>>> df.filter(items=['one', 'three'])\n         one  three\nmouse     1      3\nrabbit    4      6\n  \n>>> # select columns by regular expression\n>>> df.filter(regex='e$', axis=1)\n         one  three\nmouse     1      3\nrabbit    4      6\n  \n>>> # select rows containing 'bbi'\n>>> df.filter(like='bbi', axis=0)\n         one  two  three\nrabbit    4    5      6","title":"pandas.reference.api.pandas.dataframe.filter"},{"text":"str.rindex(sub[, start[, end]])  \nLike rfind() but raises ValueError when the substring sub is not found.","title":"python.library.stdtypes#str.rindex"},{"text":"pandas.Timestamp.freq   Timestamp.freq","title":"pandas.reference.api.pandas.timestamp.freq"},{"text":"pandas.Series.str.findall   Series.str.findall(pat, flags=0)[source]\n \nFind all occurrences of pattern or regular expression in the Series\/Index. Equivalent to applying re.findall() to all the elements in the Series\/Index.  Parameters \n \npat:str\n\n\nPattern or regular expression.  \nflags:int, default 0\n\n\nFlags from re module, e.g. re.IGNORECASE (default is 0, which means no flags).    Returns \n Series\/Index of lists of strings\n\nAll non-overlapping matches of pattern or regular expression in each string of this Series\/Index.      See also  count\n\nCount occurrences of pattern or regular expression in each string of the Series\/Index.  extractall\n\nFor each string in the Series, extract groups from all matches of regular expression and return a DataFrame with one row for each match and one column for each group.  re.findall\n\nThe equivalent re function to all non-overlapping matches of pattern or regular expression in string, as a list of strings.    Examples \n>>> s = pd.Series(['Lion', 'Monkey', 'Rabbit'])\n  The search for the pattern \u2018Monkey\u2019 returns one match: \n>>> s.str.findall('Monkey')\n0          []\n1    [Monkey]\n2          []\ndtype: object\n  On the other hand, the search for the pattern \u2018MONKEY\u2019 doesn\u2019t return any match: \n>>> s.str.findall('MONKEY')\n0    []\n1    []\n2    []\ndtype: object\n  Flags can be added to the pattern or regular expression. For instance, to find the pattern \u2018MONKEY\u2019 ignoring the case: \n>>> import re\n>>> s.str.findall('MONKEY', flags=re.IGNORECASE)\n0          []\n1    [Monkey]\n2          []\ndtype: object\n  When the pattern matches more than one string in the Series, all matches are returned: \n>>> s.str.findall('on')\n0    [on]\n1    [on]\n2      []\ndtype: object\n  Regular expressions are supported too. For instance, the search for all the strings ending with the word \u2018on\u2019 is shown next: \n>>> s.str.findall('on$')\n0    [on]\n1      []\n2      []\ndtype: object\n  If the pattern is found more than once in the same string, then a list of multiple strings is returned: \n>>> s.str.findall('b')\n0        []\n1        []\n2    [b, b]\ndtype: object","title":"pandas.reference.api.pandas.series.str.findall"},{"text":"pandas.Series.filter   Series.filter(items=None, like=None, regex=None, axis=None)[source]\n \nSubset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index.  Parameters \n \nitems:list-like\n\n\nKeep labels from axis which are in items.  \nlike:str\n\n\nKeep labels from axis for which \u201clike in label == True\u201d.  \nregex:str (regular expression)\n\n\nKeep labels from axis for which re.search(regex, label) == True.  \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default None\n\n\nThe axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, \u2018index\u2019 for Series, \u2018columns\u2019 for DataFrame.    Returns \n same type as input object\n    See also  DataFrame.loc\n\nAccess a group of rows and columns by label(s) or a boolean array.    Notes The items, like, and regex parameters are enforced to be mutually exclusive. axis defaults to the info axis that is used when indexing with []. Examples \n>>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),\n...                   index=['mouse', 'rabbit'],\n...                   columns=['one', 'two', 'three'])\n>>> df\n        one  two  three\nmouse     1    2      3\nrabbit    4    5      6\n  \n>>> # select columns by name\n>>> df.filter(items=['one', 'three'])\n         one  three\nmouse     1      3\nrabbit    4      6\n  \n>>> # select columns by regular expression\n>>> df.filter(regex='e$', axis=1)\n         one  three\nmouse     1      3\nrabbit    4      6\n  \n>>> # select rows containing 'bbi'\n>>> df.filter(like='bbi', axis=0)\n         one  two  three\nrabbit    4    5      6","title":"pandas.reference.api.pandas.series.filter"},{"text":"re.purge()  \nClear the regular expression cache.","title":"python.library.re#re.purge"},{"text":"pandas.Timedelta.freq   Timedelta.freq","title":"pandas.reference.api.pandas.timedelta.freq"},{"text":"pattern  \nThe regular expression pattern.","title":"python.library.re#re.error.pattern"},{"text":"fnmatch.filter(names, pattern)  \nConstruct a list from those elements of the iterable names that match pattern. It is the same as [n for n in names if fnmatch(n, pattern)], but implemented more efficiently.","title":"python.library.fnmatch#fnmatch.filter"},{"text":"pandas.tseries.offsets.BusinessMonthEnd.freqstr   BusinessMonthEnd.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.businessmonthend.freqstr"}]}
{"task_id":583557,"prompt":"def f_583557(tab):\n\treturn ","suffix":"","canonical_solution":"'\\n'.join('\\t'.join(str(col) for col in row) for row in tab)","test_start":"\ndef check(candidate):","test":["\n    assert candidate([[1,2,3],[4,5,6]]) == \"1\\t2\\t3\\n4\\t5\\t6\"\n","\n    assert candidate([[1, 'x' ,3],[4.4,5,\"six\"]]) == \"1\\tx\\t3\\n4.4\\t5\\tsix\"\n","\n    assert candidate([]) == \"\"\n","\n    assert candidate([[],[],[]]) == \"\\n\\n\"\n"],"entry_point":"f_583557","intent":"print a 2 dimensional list `tab` as a table with delimiters","library":[],"docs":[]}
{"task_id":38535931,"prompt":"def f_38535931(df, tuples):\n\treturn ","suffix":"","canonical_solution":"df.set_index(list('BC')).drop(tuples, errors='ignore').reset_index()","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame([[3, 4], [4, 5], [-1, -2]], columns = ['B', 'C'])\n    tuples = [(3, 4), (-1, -2)]\n    expected = pd.DataFrame([[4, 5]], columns = ['B', 'C'])\n    actual = candidate(df, tuples)\n    assert pd.DataFrame.equals(actual, expected)\n"],"entry_point":"f_38535931","intent":"pandas: delete rows in dataframe `df` based on multiple columns values","library":["pandas"],"docs":[{"text":"pandas.MultiIndex.codes   propertyMultiIndex.codes","title":"pandas.reference.api.pandas.multiindex.codes"},{"text":"pandas.MultiIndex.levels   MultiIndex.levels","title":"pandas.reference.api.pandas.multiindex.levels"},{"text":"DataFrame  Constructor       \nDataFrame([data, index, columns, dtype, copy]) Two-dimensional, size-mutable, potentially heterogeneous tabular data.      Attributes and underlying data Axes       \nDataFrame.index The index (row labels) of the DataFrame.  \nDataFrame.columns The column labels of the DataFrame.          \nDataFrame.dtypes Return the dtypes in the DataFrame.  \nDataFrame.info([verbose, buf, max_cols, ...]) Print a concise summary of a DataFrame.  \nDataFrame.select_dtypes([include, exclude]) Return a subset of the DataFrame's columns based on the column dtypes.  \nDataFrame.values Return a Numpy representation of the DataFrame.  \nDataFrame.axes Return a list representing the axes of the DataFrame.  \nDataFrame.ndim Return an int representing the number of axes \/ array dimensions.  \nDataFrame.size Return an int representing the number of elements in this object.  \nDataFrame.shape Return a tuple representing the dimensionality of the DataFrame.  \nDataFrame.memory_usage([index, deep]) Return the memory usage of each column in bytes.  \nDataFrame.empty Indicator whether Series\/DataFrame is empty.  \nDataFrame.set_flags(*[, copy, ...]) Return a new object with updated flags.      Conversion       \nDataFrame.astype(dtype[, copy, errors]) Cast a pandas object to a specified dtype dtype.  \nDataFrame.convert_dtypes([infer_objects, ...]) Convert columns to best possible dtypes using dtypes supporting pd.NA.  \nDataFrame.infer_objects() Attempt to infer better dtypes for object columns.  \nDataFrame.copy([deep]) Make a copy of this object's indices and data.  \nDataFrame.bool() Return the bool of a single element Series or DataFrame.      Indexing, iteration       \nDataFrame.head([n]) Return the first n rows.  \nDataFrame.at Access a single value for a row\/column label pair.  \nDataFrame.iat Access a single value for a row\/column pair by integer position.  \nDataFrame.loc Access a group of rows and columns by label(s) or a boolean array.  \nDataFrame.iloc Purely integer-location based indexing for selection by position.  \nDataFrame.insert(loc, column, value[, ...]) Insert column into DataFrame at specified location.  \nDataFrame.__iter__() Iterate over info axis.  \nDataFrame.items() Iterate over (column name, Series) pairs.  \nDataFrame.iteritems() Iterate over (column name, Series) pairs.  \nDataFrame.keys() Get the 'info axis' (see Indexing for more).  \nDataFrame.iterrows() Iterate over DataFrame rows as (index, Series) pairs.  \nDataFrame.itertuples([index, name]) Iterate over DataFrame rows as namedtuples.  \nDataFrame.lookup(row_labels, col_labels) (DEPRECATED) Label-based \"fancy indexing\" function for DataFrame.  \nDataFrame.pop(item) Return item and drop from frame.  \nDataFrame.tail([n]) Return the last n rows.  \nDataFrame.xs(key[, axis, level, drop_level]) Return cross-section from the Series\/DataFrame.  \nDataFrame.get(key[, default]) Get item from object for given key (ex: DataFrame column).  \nDataFrame.isin(values) Whether each element in the DataFrame is contained in values.  \nDataFrame.where(cond[, other, inplace, ...]) Replace values where the condition is False.  \nDataFrame.mask(cond[, other, inplace, axis, ...]) Replace values where the condition is True.  \nDataFrame.query(expr[, inplace]) Query the columns of a DataFrame with a boolean expression.    For more information on .at, .iat, .loc, and .iloc, see the indexing documentation.   Binary operator functions       \nDataFrame.add(other[, axis, level, fill_value]) Get Addition of dataframe and other, element-wise (binary operator add).  \nDataFrame.sub(other[, axis, level, fill_value]) Get Subtraction of dataframe and other, element-wise (binary operator sub).  \nDataFrame.mul(other[, axis, level, fill_value]) Get Multiplication of dataframe and other, element-wise (binary operator mul).  \nDataFrame.div(other[, axis, level, fill_value]) Get Floating division of dataframe and other, element-wise (binary operator truediv).  \nDataFrame.truediv(other[, axis, level, ...]) Get Floating division of dataframe and other, element-wise (binary operator truediv).  \nDataFrame.floordiv(other[, axis, level, ...]) Get Integer division of dataframe and other, element-wise (binary operator floordiv).  \nDataFrame.mod(other[, axis, level, fill_value]) Get Modulo of dataframe and other, element-wise (binary operator mod).  \nDataFrame.pow(other[, axis, level, fill_value]) Get Exponential power of dataframe and other, element-wise (binary operator pow).  \nDataFrame.dot(other) Compute the matrix multiplication between the DataFrame and other.  \nDataFrame.radd(other[, axis, level, fill_value]) Get Addition of dataframe and other, element-wise (binary operator radd).  \nDataFrame.rsub(other[, axis, level, fill_value]) Get Subtraction of dataframe and other, element-wise (binary operator rsub).  \nDataFrame.rmul(other[, axis, level, fill_value]) Get Multiplication of dataframe and other, element-wise (binary operator rmul).  \nDataFrame.rdiv(other[, axis, level, fill_value]) Get Floating division of dataframe and other, element-wise (binary operator rtruediv).  \nDataFrame.rtruediv(other[, axis, level, ...]) Get Floating division of dataframe and other, element-wise (binary operator rtruediv).  \nDataFrame.rfloordiv(other[, axis, level, ...]) Get Integer division of dataframe and other, element-wise (binary operator rfloordiv).  \nDataFrame.rmod(other[, axis, level, fill_value]) Get Modulo of dataframe and other, element-wise (binary operator rmod).  \nDataFrame.rpow(other[, axis, level, fill_value]) Get Exponential power of dataframe and other, element-wise (binary operator rpow).  \nDataFrame.lt(other[, axis, level]) Get Less than of dataframe and other, element-wise (binary operator lt).  \nDataFrame.gt(other[, axis, level]) Get Greater than of dataframe and other, element-wise (binary operator gt).  \nDataFrame.le(other[, axis, level]) Get Less than or equal to of dataframe and other, element-wise (binary operator le).  \nDataFrame.ge(other[, axis, level]) Get Greater than or equal to of dataframe and other, element-wise (binary operator ge).  \nDataFrame.ne(other[, axis, level]) Get Not equal to of dataframe and other, element-wise (binary operator ne).  \nDataFrame.eq(other[, axis, level]) Get Equal to of dataframe and other, element-wise (binary operator eq).  \nDataFrame.combine(other, func[, fill_value, ...]) Perform column-wise combine with another DataFrame.  \nDataFrame.combine_first(other) Update null elements with value in the same location in other.      Function application, GroupBy & window       \nDataFrame.apply(func[, axis, raw, ...]) Apply a function along an axis of the DataFrame.  \nDataFrame.applymap(func[, na_action]) Apply a function to a Dataframe elementwise.  \nDataFrame.pipe(func, *args, **kwargs) Apply chainable functions that expect Series or DataFrames.  \nDataFrame.agg([func, axis]) Aggregate using one or more operations over the specified axis.  \nDataFrame.aggregate([func, axis]) Aggregate using one or more operations over the specified axis.  \nDataFrame.transform(func[, axis]) Call func on self producing a DataFrame with the same axis shape as self.  \nDataFrame.groupby([by, axis, level, ...]) Group DataFrame using a mapper or by a Series of columns.  \nDataFrame.rolling(window[, min_periods, ...]) Provide rolling window calculations.  \nDataFrame.expanding([min_periods, center, ...]) Provide expanding window calculations.  \nDataFrame.ewm([com, span, halflife, alpha, ...]) Provide exponentially weighted (EW) calculations.      Computations \/ descriptive stats       \nDataFrame.abs() Return a Series\/DataFrame with absolute numeric value of each element.  \nDataFrame.all([axis, bool_only, skipna, level]) Return whether all elements are True, potentially over an axis.  \nDataFrame.any([axis, bool_only, skipna, level]) Return whether any element is True, potentially over an axis.  \nDataFrame.clip([lower, upper, axis, inplace]) Trim values at input threshold(s).  \nDataFrame.corr([method, min_periods]) Compute pairwise correlation of columns, excluding NA\/null values.  \nDataFrame.corrwith(other[, axis, drop, method]) Compute pairwise correlation.  \nDataFrame.count([axis, level, numeric_only]) Count non-NA cells for each column or row.  \nDataFrame.cov([min_periods, ddof]) Compute pairwise covariance of columns, excluding NA\/null values.  \nDataFrame.cummax([axis, skipna]) Return cumulative maximum over a DataFrame or Series axis.  \nDataFrame.cummin([axis, skipna]) Return cumulative minimum over a DataFrame or Series axis.  \nDataFrame.cumprod([axis, skipna]) Return cumulative product over a DataFrame or Series axis.  \nDataFrame.cumsum([axis, skipna]) Return cumulative sum over a DataFrame or Series axis.  \nDataFrame.describe([percentiles, include, ...]) Generate descriptive statistics.  \nDataFrame.diff([periods, axis]) First discrete difference of element.  \nDataFrame.eval(expr[, inplace]) Evaluate a string describing operations on DataFrame columns.  \nDataFrame.kurt([axis, skipna, level, ...]) Return unbiased kurtosis over requested axis.  \nDataFrame.kurtosis([axis, skipna, level, ...]) Return unbiased kurtosis over requested axis.  \nDataFrame.mad([axis, skipna, level]) Return the mean absolute deviation of the values over the requested axis.  \nDataFrame.max([axis, skipna, level, ...]) Return the maximum of the values over the requested axis.  \nDataFrame.mean([axis, skipna, level, ...]) Return the mean of the values over the requested axis.  \nDataFrame.median([axis, skipna, level, ...]) Return the median of the values over the requested axis.  \nDataFrame.min([axis, skipna, level, ...]) Return the minimum of the values over the requested axis.  \nDataFrame.mode([axis, numeric_only, dropna]) Get the mode(s) of each element along the selected axis.  \nDataFrame.pct_change([periods, fill_method, ...]) Percentage change between the current and a prior element.  \nDataFrame.prod([axis, skipna, level, ...]) Return the product of the values over the requested axis.  \nDataFrame.product([axis, skipna, level, ...]) Return the product of the values over the requested axis.  \nDataFrame.quantile([q, axis, numeric_only, ...]) Return values at the given quantile over requested axis.  \nDataFrame.rank([axis, method, numeric_only, ...]) Compute numerical data ranks (1 through n) along axis.  \nDataFrame.round([decimals]) Round a DataFrame to a variable number of decimal places.  \nDataFrame.sem([axis, skipna, level, ddof, ...]) Return unbiased standard error of the mean over requested axis.  \nDataFrame.skew([axis, skipna, level, ...]) Return unbiased skew over requested axis.  \nDataFrame.sum([axis, skipna, level, ...]) Return the sum of the values over the requested axis.  \nDataFrame.std([axis, skipna, level, ddof, ...]) Return sample standard deviation over requested axis.  \nDataFrame.var([axis, skipna, level, ddof, ...]) Return unbiased variance over requested axis.  \nDataFrame.nunique([axis, dropna]) Count number of distinct elements in specified axis.  \nDataFrame.value_counts([subset, normalize, ...]) Return a Series containing counts of unique rows in the DataFrame.      Reindexing \/ selection \/ label manipulation       \nDataFrame.add_prefix(prefix) Prefix labels with string prefix.  \nDataFrame.add_suffix(suffix) Suffix labels with string suffix.  \nDataFrame.align(other[, join, axis, level, ...]) Align two objects on their axes with the specified join method.  \nDataFrame.at_time(time[, asof, axis]) Select values at particular time of day (e.g., 9:30AM).  \nDataFrame.between_time(start_time, end_time) Select values between particular times of the day (e.g., 9:00-9:30 AM).  \nDataFrame.drop([labels, axis, index, ...]) Drop specified labels from rows or columns.  \nDataFrame.drop_duplicates([subset, keep, ...]) Return DataFrame with duplicate rows removed.  \nDataFrame.duplicated([subset, keep]) Return boolean Series denoting duplicate rows.  \nDataFrame.equals(other) Test whether two objects contain the same elements.  \nDataFrame.filter([items, like, regex, axis]) Subset the dataframe rows or columns according to the specified index labels.  \nDataFrame.first(offset) Select initial periods of time series data based on a date offset.  \nDataFrame.head([n]) Return the first n rows.  \nDataFrame.idxmax([axis, skipna]) Return index of first occurrence of maximum over requested axis.  \nDataFrame.idxmin([axis, skipna]) Return index of first occurrence of minimum over requested axis.  \nDataFrame.last(offset) Select final periods of time series data based on a date offset.  \nDataFrame.reindex([labels, index, columns, ...]) Conform Series\/DataFrame to new index with optional filling logic.  \nDataFrame.reindex_like(other[, method, ...]) Return an object with matching indices as other object.  \nDataFrame.rename([mapper, index, columns, ...]) Alter axes labels.  \nDataFrame.rename_axis([mapper, index, ...]) Set the name of the axis for the index or columns.  \nDataFrame.reset_index([level, drop, ...]) Reset the index, or a level of it.  \nDataFrame.sample([n, frac, replace, ...]) Return a random sample of items from an axis of object.  \nDataFrame.set_axis(labels[, axis, inplace]) Assign desired index to given axis.  \nDataFrame.set_index(keys[, drop, append, ...]) Set the DataFrame index using existing columns.  \nDataFrame.tail([n]) Return the last n rows.  \nDataFrame.take(indices[, axis, is_copy]) Return the elements in the given positional indices along an axis.  \nDataFrame.truncate([before, after, axis, copy]) Truncate a Series or DataFrame before and after some index value.      Missing data handling       \nDataFrame.backfill([axis, inplace, limit, ...]) Synonym for DataFrame.fillna() with method='bfill'.  \nDataFrame.bfill([axis, inplace, limit, downcast]) Synonym for DataFrame.fillna() with method='bfill'.  \nDataFrame.dropna([axis, how, thresh, ...]) Remove missing values.  \nDataFrame.ffill([axis, inplace, limit, downcast]) Synonym for DataFrame.fillna() with method='ffill'.  \nDataFrame.fillna([value, method, axis, ...]) Fill NA\/NaN values using the specified method.  \nDataFrame.interpolate([method, axis, limit, ...]) Fill NaN values using an interpolation method.  \nDataFrame.isna() Detect missing values.  \nDataFrame.isnull() DataFrame.isnull is an alias for DataFrame.isna.  \nDataFrame.notna() Detect existing (non-missing) values.  \nDataFrame.notnull() DataFrame.notnull is an alias for DataFrame.notna.  \nDataFrame.pad([axis, inplace, limit, downcast]) Synonym for DataFrame.fillna() with method='ffill'.  \nDataFrame.replace([to_replace, value, ...]) Replace values given in to_replace with value.      Reshaping, sorting, transposing       \nDataFrame.droplevel(level[, axis]) Return Series\/DataFrame with requested index \/ column level(s) removed.  \nDataFrame.pivot([index, columns, values]) Return reshaped DataFrame organized by given index \/ column values.  \nDataFrame.pivot_table([values, index, ...]) Create a spreadsheet-style pivot table as a DataFrame.  \nDataFrame.reorder_levels(order[, axis]) Rearrange index levels using input order.  \nDataFrame.sort_values(by[, axis, ascending, ...]) Sort by the values along either axis.  \nDataFrame.sort_index([axis, level, ...]) Sort object by labels (along an axis).  \nDataFrame.nlargest(n, columns[, keep]) Return the first n rows ordered by columns in descending order.  \nDataFrame.nsmallest(n, columns[, keep]) Return the first n rows ordered by columns in ascending order.  \nDataFrame.swaplevel([i, j, axis]) Swap levels i and j in a MultiIndex.  \nDataFrame.stack([level, dropna]) Stack the prescribed level(s) from columns to index.  \nDataFrame.unstack([level, fill_value]) Pivot a level of the (necessarily hierarchical) index labels.  \nDataFrame.swapaxes(axis1, axis2[, copy]) Interchange axes and swap values axes appropriately.  \nDataFrame.melt([id_vars, value_vars, ...]) Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.  \nDataFrame.explode(column[, ignore_index]) Transform each element of a list-like to a row, replicating index values.  \nDataFrame.squeeze([axis]) Squeeze 1 dimensional axis objects into scalars.  \nDataFrame.to_xarray() Return an xarray object from the pandas object.  \nDataFrame.T   \nDataFrame.transpose(*args[, copy]) Transpose index and columns.      Combining \/ comparing \/ joining \/ merging       \nDataFrame.append(other[, ignore_index, ...]) Append rows of other to the end of caller, returning a new object.  \nDataFrame.assign(**kwargs) Assign new columns to a DataFrame.  \nDataFrame.compare(other[, align_axis, ...]) Compare to another DataFrame and show the differences.  \nDataFrame.join(other[, on, how, lsuffix, ...]) Join columns of another DataFrame.  \nDataFrame.merge(right[, how, on, left_on, ...]) Merge DataFrame or named Series objects with a database-style join.  \nDataFrame.update(other[, join, overwrite, ...]) Modify in place using non-NA values from another DataFrame.      Time Series-related       \nDataFrame.asfreq(freq[, method, how, ...]) Convert time series to specified frequency.  \nDataFrame.asof(where[, subset]) Return the last row(s) without any NaNs before where.  \nDataFrame.shift([periods, freq, axis, ...]) Shift index by desired number of periods with an optional time freq.  \nDataFrame.slice_shift([periods, axis]) (DEPRECATED) Equivalent to shift without copying data.  \nDataFrame.tshift([periods, freq, axis]) (DEPRECATED) Shift the time index, using the index's frequency if available.  \nDataFrame.first_valid_index() Return index for first non-NA value or None, if no NA value is found.  \nDataFrame.last_valid_index() Return index for last non-NA value or None, if no NA value is found.  \nDataFrame.resample(rule[, axis, closed, ...]) Resample time-series data.  \nDataFrame.to_period([freq, axis, copy]) Convert DataFrame from DatetimeIndex to PeriodIndex.  \nDataFrame.to_timestamp([freq, how, axis, copy]) Cast to DatetimeIndex of timestamps, at beginning of period.  \nDataFrame.tz_convert(tz[, axis, level, copy]) Convert tz-aware axis to target time zone.  \nDataFrame.tz_localize(tz[, axis, level, ...]) Localize tz-naive index of a Series or DataFrame to target time zone.      Flags Flags refer to attributes of the pandas object. Properties of the dataset (like the date is was recorded, the URL it was accessed from, etc.) should be stored in DataFrame.attrs.       \nFlags(obj, *, allows_duplicate_labels) Flags that apply to pandas objects.      Metadata DataFrame.attrs is a dictionary for storing global metadata for this DataFrame.  Warning DataFrame.attrs is considered experimental and may change without warning.        \nDataFrame.attrs Dictionary of global attributes of this dataset.      Plotting DataFrame.plot is both a callable method and a namespace attribute for specific plotting methods of the form DataFrame.plot.<kind>.       \nDataFrame.plot([x, y, kind, ax, ....]) DataFrame plotting accessor and method          \nDataFrame.plot.area([x, y]) Draw a stacked area plot.  \nDataFrame.plot.bar([x, y]) Vertical bar plot.  \nDataFrame.plot.barh([x, y]) Make a horizontal bar plot.  \nDataFrame.plot.box([by]) Make a box plot of the DataFrame columns.  \nDataFrame.plot.density([bw_method, ind]) Generate Kernel Density Estimate plot using Gaussian kernels.  \nDataFrame.plot.hexbin(x, y[, C, ...]) Generate a hexagonal binning plot.  \nDataFrame.plot.hist([by, bins]) Draw one histogram of the DataFrame's columns.  \nDataFrame.plot.kde([bw_method, ind]) Generate Kernel Density Estimate plot using Gaussian kernels.  \nDataFrame.plot.line([x, y]) Plot Series or DataFrame as lines.  \nDataFrame.plot.pie(**kwargs) Generate a pie plot.  \nDataFrame.plot.scatter(x, y[, s, c]) Create a scatter plot with varying marker point size and color.          \nDataFrame.boxplot([column, by, ax, ...]) Make a box plot from DataFrame columns.  \nDataFrame.hist([column, by, grid, ...]) Make a histogram of the DataFrame's columns.      Sparse accessor Sparse-dtype specific methods and attributes are provided under the DataFrame.sparse accessor.       \nDataFrame.sparse.density Ratio of non-sparse points to total (dense) data points.          \nDataFrame.sparse.from_spmatrix(data[, ...]) Create a new DataFrame from a scipy sparse matrix.  \nDataFrame.sparse.to_coo() Return the contents of the frame as a sparse SciPy COO matrix.  \nDataFrame.sparse.to_dense() Convert a DataFrame with sparse values to dense.      Serialization \/ IO \/ conversion       \nDataFrame.from_dict(data[, orient, dtype, ...]) Construct DataFrame from dict of array-like or dicts.  \nDataFrame.from_records(data[, index, ...]) Convert structured or record ndarray to DataFrame.  \nDataFrame.to_parquet([path, engine, ...]) Write a DataFrame to the binary parquet format.  \nDataFrame.to_pickle(path[, compression, ...]) Pickle (serialize) object to file.  \nDataFrame.to_csv([path_or_buf, sep, na_rep, ...]) Write object to a comma-separated values (csv) file.  \nDataFrame.to_hdf(path_or_buf, key[, mode, ...]) Write the contained data to an HDF5 file using HDFStore.  \nDataFrame.to_sql(name, con[, schema, ...]) Write records stored in a DataFrame to a SQL database.  \nDataFrame.to_dict([orient, into]) Convert the DataFrame to a dictionary.  \nDataFrame.to_excel(excel_writer[, ...]) Write object to an Excel sheet.  \nDataFrame.to_json([path_or_buf, orient, ...]) Convert the object to a JSON string.  \nDataFrame.to_html([buf, columns, col_space, ...]) Render a DataFrame as an HTML table.  \nDataFrame.to_feather(path, **kwargs) Write a DataFrame to the binary Feather format.  \nDataFrame.to_latex([buf, columns, ...]) Render object to a LaTeX tabular, longtable, or nested table.  \nDataFrame.to_stata(path[, convert_dates, ...]) Export DataFrame object to Stata dta format.  \nDataFrame.to_gbq(destination_table[, ...]) Write a DataFrame to a Google BigQuery table.  \nDataFrame.to_records([index, column_dtypes, ...]) Convert DataFrame to a NumPy record array.  \nDataFrame.to_string([buf, columns, ...]) Render a DataFrame to a console-friendly tabular output.  \nDataFrame.to_clipboard([excel, sep]) Copy object to the system clipboard.  \nDataFrame.to_markdown([buf, mode, index, ...]) Print DataFrame in Markdown-friendly format.  \nDataFrame.style Returns a Styler object.","title":"pandas.reference.frame"},{"text":"pandas.DataFrame.drop_duplicates   DataFrame.drop_duplicates(subset=None, keep='first', inplace=False, ignore_index=False)[source]\n \nReturn DataFrame with duplicate rows removed. Considering certain columns is optional. Indexes, including time indexes are ignored.  Parameters \n \nsubset:column label or sequence of labels, optional\n\n\nOnly consider certain columns for identifying duplicates, by default use all of the columns.  \nkeep:{\u2018first\u2019, \u2018last\u2019, False}, default \u2018first\u2019\n\n\nDetermines which duplicates (if any) to keep. - first : Drop duplicates except for the first occurrence. - last : Drop duplicates except for the last occurrence. - False : Drop all duplicates.  \ninplace:bool, default False\n\n\nWhether to drop duplicates in place or to return a copy.  \nignore_index:bool, default False\n\n\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.  New in version 1.0.0.     Returns \n DataFrame or None\n\nDataFrame with duplicates removed or None if inplace=True.      See also  DataFrame.value_counts\n\nCount unique combinations of columns.    Examples Consider dataset containing ramen rating. \n>>> df = pd.DataFrame({\n...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n...     'rating': [4, 4, 3.5, 15, 5]\n... })\n>>> df\n    brand style  rating\n0  Yum Yum   cup     4.0\n1  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n3  Indomie  pack    15.0\n4  Indomie  pack     5.0\n  By default, it removes duplicate rows based on all columns. \n>>> df.drop_duplicates()\n    brand style  rating\n0  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n3  Indomie  pack    15.0\n4  Indomie  pack     5.0\n  To remove duplicates on specific column(s), use subset. \n>>> df.drop_duplicates(subset=['brand'])\n    brand style  rating\n0  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n  To remove duplicates and keep last occurrences, use keep. \n>>> df.drop_duplicates(subset=['brand', 'style'], keep='last')\n    brand style  rating\n1  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n4  Indomie  pack     5.0","title":"pandas.reference.api.pandas.dataframe.drop_duplicates"},{"text":"pandas.DataFrame.drop   DataFrame.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')[source]\n \nDrop specified labels from rows or columns. Remove rows or columns by specifying label names and corresponding axis, or by specifying directly index or column names. When using a multi-index, labels on different levels can be removed by specifying the level. See the user guide <advanced.shown_levels> for more information about the now unused levels.  Parameters \n \nlabels:single label or list-like\n\n\nIndex or column labels to drop. A tuple will be used as a single label and not treated as a list-like.  \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019}, default 0\n\n\nWhether to drop labels from the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).  \nindex:single label or list-like\n\n\nAlternative to specifying axis (labels, axis=0 is equivalent to index=labels).  \ncolumns:single label or list-like\n\n\nAlternative to specifying axis (labels, axis=1 is equivalent to columns=labels).  \nlevel:int or level name, optional\n\n\nFor MultiIndex, level from which the labels will be removed.  \ninplace:bool, default False\n\n\nIf False, return a copy. Otherwise, do operation inplace and return None.  \nerrors:{\u2018ignore\u2019, \u2018raise\u2019}, default \u2018raise\u2019\n\n\nIf \u2018ignore\u2019, suppress error and only existing labels are dropped.    Returns \n DataFrame or None\n\nDataFrame without the removed index or column labels or None if inplace=True.    Raises \n KeyError\n\nIf any of the labels is not found in the selected axis.      See also  DataFrame.loc\n\nLabel-location based indexer for selection by label.  DataFrame.dropna\n\nReturn DataFrame with labels on given axis omitted where (all or any) data are missing.  DataFrame.drop_duplicates\n\nReturn DataFrame with duplicate rows removed, optionally only considering certain columns.  Series.drop\n\nReturn Series with specified index labels removed.    Examples \n>>> df = pd.DataFrame(np.arange(12).reshape(3, 4),\n...                   columns=['A', 'B', 'C', 'D'])\n>>> df\n   A  B   C   D\n0  0  1   2   3\n1  4  5   6   7\n2  8  9  10  11\n  Drop columns \n>>> df.drop(['B', 'C'], axis=1)\n   A   D\n0  0   3\n1  4   7\n2  8  11\n  \n>>> df.drop(columns=['B', 'C'])\n   A   D\n0  0   3\n1  4   7\n2  8  11\n  Drop a row by index \n>>> df.drop([0, 1])\n   A  B   C   D\n2  8  9  10  11\n  Drop columns and\/or rows of MultiIndex DataFrame \n>>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],\n...                              ['speed', 'weight', 'length']],\n...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n>>> df = pd.DataFrame(index=midx, columns=['big', 'small'],\n...                   data=[[45, 30], [200, 100], [1.5, 1], [30, 20],\n...                         [250, 150], [1.5, 0.8], [320, 250],\n...                         [1, 0.8], [0.3, 0.2]])\n>>> df\n                big     small\nlama    speed   45.0    30.0\n        weight  200.0   100.0\n        length  1.5     1.0\ncow     speed   30.0    20.0\n        weight  250.0   150.0\n        length  1.5     0.8\nfalcon  speed   320.0   250.0\n        weight  1.0     0.8\n        length  0.3     0.2\n  Drop a specific index combination from the MultiIndex DataFrame, i.e., drop the combination 'falcon' and 'weight', which deletes only the corresponding row \n>>> df.drop(index=('falcon', 'weight'))\n                big     small\nlama    speed   45.0    30.0\n        weight  200.0   100.0\n        length  1.5     1.0\ncow     speed   30.0    20.0\n        weight  250.0   150.0\n        length  1.5     0.8\nfalcon  speed   320.0   250.0\n        length  0.3     0.2\n  \n>>> df.drop(index='cow', columns='small')\n                big\nlama    speed   45.0\n        weight  200.0\n        length  1.5\nfalcon  speed   320.0\n        weight  1.0\n        length  0.3\n  \n>>> df.drop(index='length', level=1)\n                big     small\nlama    speed   45.0    30.0\n        weight  200.0   100.0\ncow     speed   30.0    20.0\n        weight  250.0   150.0\nfalcon  speed   320.0   250.0\n        weight  1.0     0.8","title":"pandas.reference.api.pandas.dataframe.drop"},{"text":"pandas.CategoricalIndex.remove_categories   CategoricalIndex.remove_categories(*args, **kwargs)[source]\n \nRemove the specified categories. removals must be included in the old categories. Values which were in the removed categories will be set to NaN  Parameters \n \nremovals:category or list of categories\n\n\nThe categories which should be removed.  \ninplace:bool, default False\n\n\nWhether or not to remove the categories inplace or return a copy of this categorical with removed categories.  Deprecated since version 1.3.0.     Returns \n \ncat:Categorical or None\n\n\nCategorical with removed categories or None if inplace=True.    Raises \n ValueError\n\nIf the removals are not contained in the categories      See also  rename_categories\n\nRename categories.  reorder_categories\n\nReorder categories.  add_categories\n\nAdd new categories.  remove_unused_categories\n\nRemove categories which are not used.  set_categories\n\nSet the categories to the specified ones.    Examples \n>>> c = pd.Categorical(['a', 'c', 'b', 'c', 'd'])\n>>> c\n['a', 'c', 'b', 'c', 'd']\nCategories (4, object): ['a', 'b', 'c', 'd']\n  \n>>> c.remove_categories(['d', 'a'])\n[NaN, 'c', 'b', 'c', NaN]\nCategories (2, object): ['b', 'c']","title":"pandas.reference.api.pandas.categoricalindex.remove_categories"},{"text":"pandas.DataFrame.droplevel   DataFrame.droplevel(level, axis=0)[source]\n \nReturn Series\/DataFrame with requested index \/ column level(s) removed.  Parameters \n \nlevel:int, str, or list-like\n\n\nIf a string is given, must be the name of a level If list-like, elements must be names or positional indexes of levels.  \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019}, default 0\n\n\nAxis along which the level(s) is removed:  0 or \u2018index\u2019: remove level(s) in column. 1 or \u2018columns\u2019: remove level(s) in row.     Returns \n Series\/DataFrame\n\nSeries\/DataFrame with requested index \/ column level(s) removed.     Examples \n>>> df = pd.DataFrame([\n...     [1, 2, 3, 4],\n...     [5, 6, 7, 8],\n...     [9, 10, 11, 12]\n... ]).set_index([0, 1]).rename_axis(['a', 'b'])\n  \n>>> df.columns = pd.MultiIndex.from_tuples([\n...     ('c', 'e'), ('d', 'f')\n... ], names=['level_1', 'level_2'])\n  \n>>> df\nlevel_1   c   d\nlevel_2   e   f\na b\n1 2      3   4\n5 6      7   8\n9 10    11  12\n  \n>>> df.droplevel('a')\nlevel_1   c   d\nlevel_2   e   f\nb\n2        3   4\n6        7   8\n10      11  12\n  \n>>> df.droplevel('level_2', axis=1)\nlevel_1   c   d\na b\n1 2      3   4\n5 6      7   8\n9 10    11  12","title":"pandas.reference.api.pandas.dataframe.droplevel"},{"text":"pandas.core.groupby.DataFrameGroupBy.nunique   DataFrameGroupBy.nunique(dropna=True)[source]\n \nReturn DataFrame with counts of unique elements in each position.  Parameters \n \ndropna:bool, default True\n\n\nDon\u2019t include NaN in the counts.    Returns \n nunique: DataFrame\n   Examples \n>>> df = pd.DataFrame({'id': ['spam', 'egg', 'egg', 'spam',\n...                           'ham', 'ham'],\n...                    'value1': [1, 5, 5, 2, 5, 5],\n...                    'value2': list('abbaxy')})\n>>> df\n     id  value1 value2\n0  spam       1      a\n1   egg       5      b\n2   egg       5      b\n3  spam       2      a\n4   ham       5      x\n5   ham       5      y\n  \n>>> df.groupby('id').nunique()\n      value1  value2\nid\negg        1       1\nham        1       2\nspam       2       1\n  Check for rows with the same id but conflicting values: \n>>> df.groupby('id').filter(lambda g: (g.nunique() > 1).any())\n     id  value1 value2\n0  spam       1      a\n3  spam       2      a\n4   ham       5      x\n5   ham       5      y","title":"pandas.reference.api.pandas.core.groupby.dataframegroupby.nunique"},{"text":"clear() \u2192 None. Remove all items from D.","title":"werkzeug.datastructures.index#werkzeug.datastructures.MultiDict.clear"},{"text":"pandas.Index.delete   Index.delete(loc)[source]\n \nMake new Index with passed location(-s) deleted.  Parameters \n \nloc:int or list of int\n\n\nLocation of item(-s) which will be deleted. Use a list of locations to delete more than one value at the same time.    Returns \n Index\n\nWill be same type as self, except for RangeIndex.      See also  numpy.delete\n\nDelete any rows and column from NumPy array (ndarray).    Examples \n>>> idx = pd.Index(['a', 'b', 'c'])\n>>> idx.delete(1)\nIndex(['a', 'c'], dtype='object')\n  \n>>> idx = pd.Index(['a', 'b', 'c'])\n>>> idx.delete([0, 2])\nIndex(['b'], dtype='object')","title":"pandas.reference.api.pandas.index.delete"}]}
{"task_id":13945749,"prompt":"def f_13945749(goals, penalties):\n\treturn ","suffix":"","canonical_solution":"\"\"\"({:d} goals, ${:d})\"\"\".format(goals, penalties)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(0, 0) == \"(0 goals, $0)\"\n","\n    assert candidate(123, 2) == \"(123 goals, $2)\"\n"],"entry_point":"f_13945749","intent":"format the variables `goals` and `penalties` using string formatting","library":[],"docs":[]}
{"task_id":13945749,"prompt":"def f_13945749(goals, penalties):\n\treturn ","suffix":"","canonical_solution":"\"\"\"({} goals, ${})\"\"\".format(goals, penalties)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(0, 0) == \"(0 goals, $0)\"\n","\n    assert candidate(123, \"???\") == \"(123 goals, $???)\"\n","\n    assert candidate(\"x\", 0.0) == \"(x goals, $0.0)\"\n"],"entry_point":"f_13945749","intent":"format string \"({} goals, ${})\" with variables `goals` and `penalties`","library":[],"docs":[]}
{"task_id":18524642,"prompt":"def f_18524642(L):\n\treturn ","suffix":"","canonical_solution":"[int(''.join(str(d) for d in x)) for x in L]","test_start":"\ndef check(candidate):","test":["\n    assert candidate([[1,2], [2,3,4], [1,0,0]]) == [12,234,100]\n","\n    assert candidate([[1], [2], [3]]) == [1,2,3]\n"],"entry_point":"f_18524642","intent":"convert list of lists `L` to list of integers","library":[],"docs":[]}
{"task_id":18524642,"prompt":"def f_18524642(L):\n\t","suffix":"\n\treturn L","canonical_solution":"L = [int(''.join([str(y) for y in x])) for x in L]","test_start":"\ndef check(candidate):","test":["\n    assert candidate([[1,2], [2,3,4], [1,0,0]]) == [12,234,100]\n","\n    assert candidate([[1], [2], [3]]) == [1,2,3]\n","\n    assert candidate([[1, 0], [0, 2], [3], [0, 0, 0, 0]]) == [10,2,3, 0]\n"],"entry_point":"f_18524642","intent":"convert a list of lists `L` to list of integers","library":[],"docs":[]}
{"task_id":7138686,"prompt":"def f_7138686(lines, myfile):\n\t","suffix":"\n\treturn ","canonical_solution":"myfile.write('\\n'.join(lines))","test_start":"\ndef check(candidate):","test":["\n    with open('tmp.txt', 'w') as myfile:\n        candidate([\"first\", \"second\", \"third\"], myfile)\n    with open('tmp.txt', 'r') as fr: \n        lines = fr.readlines()\n    assert lines == [\"first\\n\", \"second\\n\", \"third\"]\n"],"entry_point":"f_7138686","intent":"write the elements of list `lines` concatenated by special character '\\n' to file `myfile`","library":[],"docs":[]}
{"task_id":17238587,"prompt":"def f_17238587(text):\n\t","suffix":"\n\treturn text","canonical_solution":"text = re.sub('\\\\b(\\\\w+)( \\\\1\\\\b)+', '\\\\1', text)","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    assert candidate(\"text\") == \"text\"\n","\n    assert candidate(\"text text\") == \"text\"\n","\n    assert candidate(\"texttext\") == \"texttext\"\n","\n    assert candidate(\"text and text\") == \"text and text\"\n"],"entry_point":"f_17238587","intent":"Remove duplicate words from a string `text` using regex","library":["re"],"docs":[{"text":"re.purge()  \nClear the regular expression cache.","title":"python.library.re#re.purge"},{"text":"pattern  \nThe regular expression pattern.","title":"python.library.re#re.error.pattern"},{"text":"gettext.lngettext(singular, plural, n)","title":"python.library.gettext#gettext.lngettext"},{"text":"re.split(pattern, string, maxsplit=0, flags=0)  \nSplit string by the occurrences of pattern. If capturing parentheses are used in pattern, then the text of all groups in the pattern are also returned as part of the resulting list. If maxsplit is nonzero, at most maxsplit splits occur, and the remainder of the string is returned as the final element of the list. >>> re.split(r'\\W+', 'Words, words, words.')\n['Words', 'words', 'words', '']\n>>> re.split(r'(\\W+)', 'Words, words, words.')\n['Words', ', ', 'words', ', ', 'words', '.', '']\n>>> re.split(r'\\W+', 'Words, words, words.', 1)\n['Words', 'words, words.']\n>>> re.split('[a-f]+', '0a3B9', flags=re.IGNORECASE)\n['0', '3', '9']\n If there are capturing groups in the separator and it matches at the start of the string, the result will start with an empty string. The same holds for the end of the string: >>> re.split(r'(\\W+)', '...words, words...')\n['', '...', 'words', ', ', 'words', '...', '']\n That way, separator components are always found at the same relative indices within the result list. Empty matches for the pattern split the string only when not adjacent to a previous empty match. >>> re.split(r'\\b', 'Words, words, words.')\n['', 'Words', ', ', 'words', ', ', 'words', '.']\n>>> re.split(r'\\W*', '...words...')\n['', '', 'w', 'o', 'r', 'd', 's', '', '']\n>>> re.split(r'(\\W*)', '...words...')\n['', '...', '', '', 'w', '', 'o', '', 'r', '', 'd', '', 's', '...', '', '', '']\n  Changed in version 3.1: Added the optional flags argument.   Changed in version 3.7: Added support of splitting on a pattern that could match an empty string.","title":"python.library.re#re.split"},{"text":"array.remove(x)  \nRemove the first occurrence of x from the array.","title":"python.library.array#array.array.remove"},{"text":"re.search(pattern, string, flags=0)  \nScan through string looking for the first location where the regular expression pattern produces a match, and return a corresponding match object. Return None if no position in the string matches the pattern; note that this is different from finding a zero-length match at some point in the string.","title":"python.library.re#re.search"},{"text":"gettext.npgettext(context, singular, plural, n)","title":"python.library.gettext#gettext.npgettext"},{"text":"remove(value)  \nRemove the first occurrence of value. If not found, raises a ValueError.","title":"python.library.collections#collections.deque.remove"},{"text":"Match.string  \nThe string passed to match() or search().","title":"python.library.re#re.Match.string"},{"text":"exception xml.dom.SyntaxErr  \nRaised when an invalid or illegal string is specified.","title":"python.library.xml.dom#xml.dom.SyntaxErr"}]}
{"task_id":26053849,"prompt":"def f_26053849(df):\n\treturn ","suffix":"","canonical_solution":"df.astype(bool).sum(axis=1)","test_start":"\nimport pandas as pd \n\ndef check(candidate):","test":["\n    df1 = pd.DataFrame([[0,0,0], [0,1,0], [1,1,1]])\n    assert candidate(df1).to_list() == [0, 1, 3]\n","\n    df2 = pd.DataFrame([[0,0,0], [0,2,0], [1,10,8.9]])\n    assert candidate(df1).to_list() == [0, 1, 3]\n","\n    df2 = pd.DataFrame([[0,0.0,0], [0,2.0,0], [1,10,8.9]])\n    assert candidate(df1).to_list() == [0, 1, 3]\n","\n    df = df = pd.DataFrame([[4, 0, 0], [1, 0, 1]])\n    expected = [1, 2]\n    actual = candidate(df)\n    for i in range(0, len(expected)):\n        assert expected[i] == actual[i]\n"],"entry_point":"f_26053849","intent":"count non zero values in each column in pandas data frame `df`","library":["pandas"],"docs":[{"text":"count_nonzero(dim=None) \u2192 Tensor  \nSee torch.count_nonzero()","title":"torch.tensors#torch.Tensor.count_nonzero"},{"text":"pandas.DataFrame.count   DataFrame.count(axis=0, level=None, numeric_only=False)[source]\n \nCount non-NA cells for each column or row. The values None, NaN, NaT, and optionally numpy.inf (depending on pandas.options.mode.use_inf_as_na) are considered NA.  Parameters \n \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019}, default 0\n\n\nIf 0 or \u2018index\u2019 counts are generated for each column. If 1 or \u2018columns\u2019 counts are generated for each row.  \nlevel:int or str, optional\n\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a DataFrame. A str specifies the level name.  \nnumeric_only:bool, default False\n\n\nInclude only float, int or boolean data.    Returns \n Series or DataFrame\n\nFor each column\/row the number of non-NA\/null entries. If level is specified returns a DataFrame.      See also  Series.count\n\nNumber of non-NA elements in a Series.  DataFrame.value_counts\n\nCount unique combinations of columns.  DataFrame.shape\n\nNumber of DataFrame rows and columns (including NA elements).  DataFrame.isna\n\nBoolean same-sized DataFrame showing places of NA elements.    Examples Constructing DataFrame from a dictionary: \n>>> df = pd.DataFrame({\"Person\":\n...                    [\"John\", \"Myla\", \"Lewis\", \"John\", \"Myla\"],\n...                    \"Age\": [24., np.nan, 21., 33, 26],\n...                    \"Single\": [False, True, True, True, False]})\n>>> df\n   Person   Age  Single\n0    John  24.0   False\n1    Myla   NaN    True\n2   Lewis  21.0    True\n3    John  33.0    True\n4    Myla  26.0   False\n  Notice the uncounted NA values: \n>>> df.count()\nPerson    5\nAge       4\nSingle    5\ndtype: int64\n  Counts for each row: \n>>> df.count(axis='columns')\n0    3\n1    2\n2    3\n3    3\n4    3\ndtype: int64","title":"pandas.reference.api.pandas.dataframe.count"},{"text":"pandas.Timestamp.freq   Timestamp.freq","title":"pandas.reference.api.pandas.timestamp.freq"},{"text":"property df","title":"torch.distributions#torch.distributions.chi2.Chi2.df"},{"text":"pandas.Period.freq   Period.freq","title":"pandas.reference.api.pandas.period.freq"},{"text":"total_count: int = None","title":"torch.distributions#torch.distributions.multinomial.Multinomial.total_count"},{"text":"pandas.Series.dt.freq   Series.dt.freq","title":"pandas.reference.api.pandas.series.dt.freq"},{"text":"colno  \nThe column corresponding to pos (may be None).","title":"python.library.re#re.error.colno"},{"text":"pandas.Timedelta.freq   Timedelta.freq","title":"pandas.reference.api.pandas.timedelta.freq"},{"text":"is_zero(x)  \nReturns True if x is a zero; otherwise returns False.","title":"python.library.decimal#decimal.Context.is_zero"}]}
{"task_id":15534223,"prompt":"def f_15534223():\n\treturn ","suffix":"","canonical_solution":"re.search('(?<!Distillr)\\\\\\\\AcroTray\\\\.exe', 'C:\\\\SomeDir\\\\AcroTray.exe')","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    result = candidate()\n    assert result.span() == (10, 23)\n    assert result.string == \"C:\\SomeDir\\AcroTray.exe\"\n"],"entry_point":"f_15534223","intent":"search for string that matches regular expression pattern '(?<!Distillr)\\\\\\\\AcroTray\\\\.exe' in string 'C:\\\\SomeDir\\\\AcroTray.exe'","library":["re"],"docs":[{"text":"pattern  \nThe regular expression pattern.","title":"python.library.re#re.error.pattern"},{"text":"winreg.REG_SZ  \nA null-terminated string.","title":"python.library.winreg#winreg.REG_SZ"},{"text":"winreg.REG_EXPAND_SZ  \nNull-terminated string containing references to environment variables (%PATH%).","title":"python.library.winreg#winreg.REG_EXPAND_SZ"},{"text":"re.purge()  \nClear the regular expression cache.","title":"python.library.re#re.purge"},{"text":"match  \nA regular expression pattern; only files with names matching this expression will be allowed as choices.","title":"django.ref.forms.fields#django.forms.FilePathField.match"},{"text":"glob.escape(pathname)  \nEscape all special characters ('?', '*' and '['). This is useful if you want to match an arbitrary literal string that may have special characters in it. Special characters in drive\/UNC sharepoints are not escaped, e.g. on Windows escape('\/\/?\/c:\/Quo vadis?.txt') returns '\/\/?\/c:\/Quo vadis[?].txt'.  New in version 3.4.","title":"python.library.glob#glob.escape"},{"text":"Pattern.search(string[, pos[, endpos]])  \nScan through string looking for the first location where this regular expression produces a match, and return a corresponding match object. Return None if no position in the string matches the pattern; note that this is different from finding a zero-length match at some point in the string. The optional second parameter pos gives an index in the string where the search is to start; it defaults to 0. This is not completely equivalent to slicing the string; the '^' pattern character matches at the real beginning of the string and at positions just after a newline, but not necessarily at the index where the search is to start. The optional parameter endpos limits how far the string will be searched; it will be as if the string is endpos characters long, so only the characters from pos to endpos - 1 will be searched for a match. If endpos is less than pos, no match will be found; otherwise, if rx is a compiled regular expression object, rx.search(string, 0, 50) is equivalent to rx.search(string[:50], 0). >>> pattern = re.compile(\"d\")\n>>> pattern.search(\"dog\")     # Match at index 0\n<re.Match object; span=(0, 1), match='d'>\n>>> pattern.search(\"dog\", 1)  # No match; search doesn't include the \"d\"","title":"python.library.re#re.Pattern.search"},{"text":"re.search(pattern, string, flags=0)  \nScan through string looking for the first location where the regular expression pattern produces a match, and return a corresponding match object. Return None if no position in the string matches the pattern; note that this is different from finding a zero-length match at some point in the string.","title":"python.library.re#re.search"},{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"PurePath.match(pattern)  \nMatch this path against the provided glob-style pattern. Return True if matching is successful, False otherwise. If pattern is relative, the path can be either relative or absolute, and matching is done from the right: >>> PurePath('a\/b.py').match('*.py')\nTrue\n>>> PurePath('\/a\/b\/c.py').match('b\/*.py')\nTrue\n>>> PurePath('\/a\/b\/c.py').match('a\/*.py')\nFalse\n If pattern is absolute, the path must be absolute, and the whole path must match: >>> PurePath('\/a.py').match('\/*.py')\nTrue\n>>> PurePath('a\/b.py').match('\/*.py')\nFalse\n As with other methods, case-sensitivity follows platform defaults: >>> PurePosixPath('b.py').match('*.PY')\nFalse\n>>> PureWindowsPath('b.py').match('*.PY')\nTrue","title":"python.library.pathlib#pathlib.PurePath.match"}]}
{"task_id":5453026,"prompt":"def f_5453026():\n\treturn ","suffix":"","canonical_solution":"\"\"\"QH QD JC KD JS\"\"\".split()","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == [\"QH\", \"QD\", \"JC\", \"KD\", \"JS\"]\n"],"entry_point":"f_5453026","intent":"split string 'QH QD JC KD JS' into a list on white spaces","library":[],"docs":[]}
{"task_id":18168684,"prompt":"def f_18168684(line):\n\treturn ","suffix":"","canonical_solution":"re.search('>.*<', line).group(0)","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    assert candidate(\"hahhdsf>0.0<;sgnd\") == \">0.0<\"\n","\n    assert candidate(\"hahhdsf>2.34<;xbnfm\") == \">2.34<\"\n"],"entry_point":"f_18168684","intent":"search for occurrences of regex pattern '>.*<' in xml string `line`","library":["re"],"docs":[{"text":"pattern  \nThe regular expression pattern.","title":"python.library.re#re.error.pattern"},{"text":"exception xml.dom.SyntaxErr  \nRaised when an invalid or illegal string is specified.","title":"python.library.xml.dom#xml.dom.SyntaxErr"},{"text":"xml.sax.handler.property_xml_string","title":"python.library.xml.sax.handler#xml.sax.handler.property_xml_string"},{"text":"Attr.prefix  \nThe part of the name preceding the colon if there is one, else the empty string.","title":"python.library.xml.dom#xml.dom.Attr.prefix"},{"text":"exception xml.dom.InuseAttributeErr  \nRaised when an attempt is made to insert an Attr node that is already present elsewhere in the document.","title":"python.library.xml.dom#xml.dom.InuseAttributeErr"},{"text":"xml.sax.handler.property_dom_node","title":"python.library.xml.sax.handler#xml.sax.handler.property_dom_node"},{"text":"exception xml.dom.IndexSizeErr  \nRaised when an index or size parameter to a method is negative or exceeds the allowed values.","title":"python.library.xml.dom#xml.dom.IndexSizeErr"},{"text":"str.rindex(sub[, start[, end]])  \nLike rfind() but raises ValueError when the substring sub is not found.","title":"python.library.stdtypes#str.rindex"},{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"xml.sax.handler.feature_string_interning","title":"python.library.xml.sax.handler#xml.sax.handler.feature_string_interning"}]}
{"task_id":4914277,"prompt":"def f_4914277(filename):\n\treturn ","suffix":"","canonical_solution":"open(filename, 'w').close()","test_start":"\ndef check(candidate):","test":["\n    filename = 'tmp.txt'\n    with open(filename, 'w') as fw: fw.write(\"hello world!\")\n    with open(filename, 'r') as fr: \n        lines = fr.readlines()\n        assert len(lines) == 1 and lines[0] == \"hello world!\"\n    candidate(filename)\n    with open(filename, 'r') as fr: \n        lines = fr.readlines()\n        assert len(lines) == 0\n"],"entry_point":"f_4914277","intent":"erase all the contents of a file `filename`","library":[],"docs":[]}
{"task_id":19068269,"prompt":"def f_19068269(string_date):\n\treturn ","suffix":"","canonical_solution":"datetime.datetime.strptime(string_date, '%Y-%m-%d %H:%M:%S.%f')","test_start":"\nimport datetime \n\ndef check(candidate):","test":["\n    assert candidate('2022-10-22 11:59:59.20') == datetime.datetime(2022, 10, 22, 11, 59, 59, 200000)\n","\n    assert candidate('2000-01-01 11:59:59.20') == datetime.datetime(2000, 1, 1, 11, 59, 59, 200000)\n","\n    assert candidate('1990-09-09 09:59:59.24') == datetime.datetime(1990, 9, 9, 9, 59, 59, 240000)\n","\n    d = candidate('2022-12-14 07:06:00.25')\n    assert d == datetime.datetime(2022, 12, 14, 7, 6, 0, 250000)\n"],"entry_point":"f_19068269","intent":"convert a string `string_date` into datetime using the format '%Y-%m-%d %H:%M:%S.%f'","library":["datetime"],"docs":[{"text":"date.__str__()  \nFor a date d, str(d) is equivalent to d.isoformat().","title":"python.library.datetime#datetime.date.__str__"},{"text":"datetime.__str__()  \nFor a datetime instance d, str(d) is equivalent to d.isoformat(' ').","title":"python.library.datetime#datetime.datetime.__str__"},{"text":"time.__str__()  \nFor a time t, str(t) is equivalent to t.isoformat().","title":"python.library.datetime#datetime.time.__str__"},{"text":"pandas.Timestamp.fromisoformat   Timestamp.fromisoformat()\n \nstring -> datetime from datetime.isoformat() output","title":"pandas.reference.api.pandas.timestamp.fromisoformat"},{"text":"date_format  \nSimilar to DateInput.format","title":"django.ref.forms.widgets#django.forms.SplitDateTimeWidget.date_format"},{"text":"datetime.date()  \nReturn date object with same year, month and day.","title":"python.library.datetime#datetime.datetime.date"},{"text":"as_datetime()","title":"django.ref.contrib.gis.gdal#django.contrib.gis.gdal.Field.as_datetime"},{"text":"locale.D_FMT  \nGet a string that can be used as a format string for time.strftime() to represent a date in a locale-specific way.","title":"python.library.locale#locale.D_FMT"},{"text":"parse_date(value)  \nParses a string and returns a datetime.date.","title":"django.ref.utils#django.utils.dateparse.parse_date"},{"text":"classmethod datetime.strptime(date_string, format)  \nReturn a datetime corresponding to date_string, parsed according to format. This is equivalent to: datetime(*(time.strptime(date_string, format)[0:6]))\n ValueError is raised if the date_string and format can\u2019t be parsed by time.strptime() or if it returns a value which isn\u2019t a time tuple. For a complete list of formatting directives, see strftime() and strptime() Behavior.","title":"python.library.datetime#datetime.datetime.strptime"}]}
{"task_id":20683167,"prompt":"def f_20683167(thelist):\n\treturn ","suffix":"","canonical_solution":"[index for index, item in enumerate(thelist) if item[0] == '332']","test_start":"\ndef check(candidate):","test":["\n    assert candidate([[0,1,2], ['a','bb','ccc'], ['332',33,2], [33,22,332]]) == [2]\n","\n    assert candidate([[0,1,2], ['332'], ['332'], ['332']]) == [1,2,3]\n","\n    assert candidate([[0,1,2], [332], [332], [332]]) == []\n"],"entry_point":"f_20683167","intent":"find the index of a list with the first element equal to '332' within the list of lists `thelist`","library":[],"docs":[]}
{"task_id":30693804,"prompt":"def f_30693804(text):\n\treturn ","suffix":"","canonical_solution":"re.sub('[^\\\\sa-zA-Z0-9]', '', text).lower().strip()","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate('ABjfK329r0&&*#5t') == 'abjfk329r05t'\n","\n    assert candidate('jseguwphegoi339yup h') == 'jseguwphegoi339yup h'\n","\n    assert candidate('   ') == ''\n"],"entry_point":"f_30693804","intent":"lower a string `text` and remove non-alphanumeric characters aside from space","library":["re"],"docs":[{"text":"string.ascii_lowercase  \nThe lowercase letters 'abcdefghijklmnopqrstuvwxyz'. This value is not locale-dependent and will not change.","title":"python.library.string#string.ascii_lowercase"},{"text":"numpy.chararray.lower method   chararray.lower()[source]\n \nReturn an array with the elements of self converted to lowercase.  See also  char.lower","title":"numpy.reference.generated.numpy.chararray.lower"},{"text":"numpy.char.chararray.lower method   char.chararray.lower()[source]\n \nReturn an array with the elements of self converted to lowercase.  See also  char.lower","title":"numpy.reference.generated.numpy.char.chararray.lower"},{"text":"curses.ascii.islower(c)  \nChecks for an ASCII lower-case character.","title":"python.library.curses.ascii#curses.ascii.islower"},{"text":"token.LESS  \nToken value for \"<\".","title":"python.library.token#token.LESS"},{"text":"gettext.lngettext(singular, plural, n)","title":"python.library.gettext#gettext.lngettext"},{"text":"str.lower()  \nReturn a copy of the string with all the cased characters 4 converted to lowercase. The lowercasing algorithm used is described in section 3.13 of the Unicode Standard.","title":"python.library.stdtypes#str.lower"},{"text":"str.islower()  \nReturn True if all cased characters 4 in the string are lowercase and there is at least one cased character, False otherwise.","title":"python.library.stdtypes#str.islower"},{"text":"re.purge()  \nClear the regular expression cache.","title":"python.library.re#re.purge"},{"text":"window.delch([y, x])  \nDelete any character at (y, x).","title":"python.library.curses#curses.window.delch"}]}
{"task_id":30693804,"prompt":"def f_30693804(text):\n\treturn ","suffix":"","canonical_solution":"re.sub('(?!\\\\s)[\\\\W_]', '', text).lower().strip()","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate('ABjfK329r0&&*#5t') == 'abjfk329r05t'\n","\n    assert candidate('jseguwphegoi339yup h') == 'jseguwphegoi339yup h'\n","\n    assert candidate('   ') == ''\n"],"entry_point":"f_30693804","intent":"remove all non-alphanumeric characters except space from a string `text` and lower it","library":["re"],"docs":[{"text":"string.ascii_lowercase  \nThe lowercase letters 'abcdefghijklmnopqrstuvwxyz'. This value is not locale-dependent and will not change.","title":"python.library.string#string.ascii_lowercase"},{"text":"re.purge()  \nClear the regular expression cache.","title":"python.library.re#re.purge"},{"text":"numpy.chararray.strip method   chararray.strip(chars=None)[source]\n \nFor each element in self, return a copy with the leading and trailing characters removed.  See also  char.strip","title":"numpy.reference.generated.numpy.chararray.strip"},{"text":"curses.ascii.islower(c)  \nChecks for an ASCII lower-case character.","title":"python.library.curses.ascii#curses.ascii.islower"},{"text":"numpy.chararray.lower method   chararray.lower()[source]\n \nReturn an array with the elements of self converted to lowercase.  See also  char.lower","title":"numpy.reference.generated.numpy.chararray.lower"},{"text":"str.islower()  \nReturn True if all cased characters 4 in the string are lowercase and there is at least one cased character, False otherwise.","title":"python.library.stdtypes#str.islower"},{"text":"window.delch([y, x])  \nDelete any character at (y, x).","title":"python.library.curses#curses.window.delch"},{"text":"gettext.lngettext(singular, plural, n)","title":"python.library.gettext#gettext.lngettext"},{"text":"numpy.char.chararray.strip method   char.chararray.strip(chars=None)[source]\n \nFor each element in self, return a copy with the leading and trailing characters removed.  See also  char.strip","title":"numpy.reference.generated.numpy.char.chararray.strip"},{"text":"token.LESS  \nToken value for \"<\".","title":"python.library.token#token.LESS"}]}
{"task_id":17138464,"prompt":"def f_17138464(x, y):\n\treturn ","suffix":"","canonical_solution":"plt.plot(x, y, label='H\\u2082O')","test_start":"\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef check(candidate):","test":["\n    pic = candidate(np.array([1,2,3]),np.array([4,5,6]))[0]\n    assert pic.get_label() == 'H\u2082O'\n    x, y = pic.get_data()\n    assert all(x == np.array([1,2,3]))\n    assert all(y == np.array([4,5,6]))\n","\n    pic = candidate(np.array([6, 7, 899]),np.array([0, 1, 245]))[0]\n    assert pic.get_label() == 'H\u2082O'\n    x, y = pic.get_data()\n    assert all(x == np.array([6, 7, 899]))\n    assert all(y == np.array([0, 1, 245]))\n"],"entry_point":"f_17138464","intent":"subscript text 'H20' with '2' as subscripted in matplotlib labels for arrays 'x' and 'y'.","library":["matplotlib","numpy"],"docs":[{"text":"matplotlib.axis.Tick.set_label2   Tick.set_label2(s)[source]\n \nSet the label2 text.  Parameters \n \nsstr","title":"matplotlib._as_gen.matplotlib.axis.tick.set_label2"},{"text":"zorder=3","title":"matplotlib.text_api#matplotlib.text.Text.zorder"},{"text":"toggle_label(b)[source]","title":"matplotlib._as_gen.mpl_toolkits.axes_grid1.axes_grid.cbaraxesbase#mpl_toolkits.axes_grid1.axes_grid.CbarAxesBase.toggle_label"},{"text":"matplotlib.axes.Axes.set_ylabel   Axes.set_ylabel(ylabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)[source]\n \nSet the label for the y-axis.  Parameters \n \nylabelstr\n\n\nThe label text.  \nlabelpadfloat, default: rcParams[\"axes.labelpad\"] (default: 4.0)\n\n\nSpacing in points from the Axes bounding box including ticks and tick labels. If None, the previous value is left as is.  \nloc{'bottom', 'center', 'top'}, default: rcParams[\"yaxis.labellocation\"] (default: 'center')\n\n\nThe label position. This is a high-level alternative for passing parameters y and horizontalalignment.    Other Parameters \n \n**kwargsText properties\n\n\nText properties control the appearance of the label.      See also  text\n\nDocuments the properties supported by Text.    \n  Examples using matplotlib.axes.Axes.set_ylabel\n \n   Bar Label Demo   \n\n   Stacked bar chart   \n\n   Grouped bar chart with labels   \n\n   CSD Demo   \n\n   Fill Between and Alpha   \n\n   Hatch-filled histograms   \n\n   Hat graph   \n\n   Psd Demo   \n\n   Scatter Demo2   \n\n   Stackplots and streamgraphs   \n\n   Contourf Demo   \n\n   Creating annotated heatmaps   \n\n   Tricontour Demo   \n\n   Tripcolor Demo   \n\n   Triplot Demo   \n\n   Aligning Labels   \n\n   Axes Demo   \n\n   Axis Label Position   \n\n   Resizing axes with constrained layout   \n\n   Resizing axes with tight layout   \n\n   Figure labels: suptitle, supxlabel, supylabel   \n\n   Invert Axes   \n\n   Secondary Axis   \n\n   Figure subfigures   \n\n   Multiple subplots   \n\n   Plots with different scales   \n\n   Box plots with custom fill colors   \n\n   Boxplots   \n\n   Box plot vs. violin plot comparison   \n\n   Violin plot customization   \n\n   Using histograms to plot a cumulative distribution   \n\n   Some features of the histogram (hist) function   \n\n   Producing multiple histograms side by side   \n\n   Using accented text in matplotlib   \n\n   Date tick labels   \n\n   Legend Demo   \n\n   Mathtext   \n\n   Multiline   \n\n   Rendering math equations using TeX   \n\n   Simple axes labels   \n\n   Text Commands   \n\n   Color Demo   \n\n   Line, Poly and RegularPoly Collection with autoscaling   \n\n   Ellipse Collection   \n\n   Dark background style sheet   \n\n   Make Room For Ylabel Using Axesgrid   \n\n   Parasite Simple   \n\n   Parasite Axes demo   \n\n   Parasite axis demo   \n\n   Ticklabel alignment   \n\n   Simple Axis Direction03   \n\n   Simple Axisline   \n\n   Anatomy of a figure   \n\n   XKCD   \n\n   Pythonic Matplotlib   \n\n   Plot 2D data on 3D plot   \n\n   Create 2D bar graphs in different planes   \n\n   3D errorbars   \n\n   Lorenz Attractor   \n\n   2D and 3D Axes in same Figure   \n\n   Automatic Text Offsetting   \n\n   3D scatterplot   \n\n   3D surface with polar coordinates   \n\n   Text annotations in 3D   \n\n   Log Bar   \n\n   Symlog Demo   \n\n   MRI With EEG   \n\n   Topographic hillshading   \n\n   Multiple Yaxis With Spines   \n\n   Basic Usage   \n\n   Artist tutorial   \n\n   Constrained Layout Guide   \n\n   Tight Layout guide   \n\n   Arranging multiple Axes in a Figure   \n\n   Choosing Colormaps in Matplotlib   \n\n   Text in Matplotlib Plots","title":"matplotlib._as_gen.matplotlib.axes.axes.set_ylabel"},{"text":"matplotlib.pyplot.ylabel   matplotlib.pyplot.ylabel(ylabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)[source]\n \nSet the label for the y-axis.  Parameters \n \nylabelstr\n\n\nThe label text.  \nlabelpadfloat, default: rcParams[\"axes.labelpad\"] (default: 4.0)\n\n\nSpacing in points from the Axes bounding box including ticks and tick labels. If None, the previous value is left as is.  \nloc{'bottom', 'center', 'top'}, default: rcParams[\"yaxis.labellocation\"] (default: 'center')\n\n\nThe label position. This is a high-level alternative for passing parameters y and horizontalalignment.    Other Parameters \n \n**kwargsText properties\n\n\nText properties control the appearance of the label.      See also  text\n\nDocuments the properties supported by Text.    \n  Examples using matplotlib.pyplot.ylabel\n \n   Scatter Symbol   \n\n   Multiple subplots   \n\n   Controlling style of text and labels using a dictionary   \n\n   Pyplot Mathtext   \n\n   Pyplot Simple   \n\n   Pyplot Text   \n\n   Solarized Light stylesheet   \n\n   Findobj Demo   \n\n   Table Demo   \n\n   Custom scale   \n\n   Basic Usage   \n\n   Pyplot tutorial","title":"matplotlib._as_gen.matplotlib.pyplot.ylabel"},{"text":"matplotlib.pyplot.text   matplotlib.pyplot.text(x, y, s, fontdict=None, **kwargs)[source]\n \nAdd text to the Axes. Add the text s to the Axes at location x, y in data coordinates.  Parameters \n \nx, yfloat\n\n\nThe position to place the text. By default, this is in data coordinates. The coordinate system can be changed using the transform parameter.  \nsstr\n\n\nThe text.  \nfontdictdict, default: None\n\n\nA dictionary to override the default text properties. If fontdict is None, the defaults are determined by rcParams.    Returns \n Text\n\nThe created Text instance.    Other Parameters \n \n**kwargsText properties.\n\n\nOther miscellaneous text parameters.   \nProperty Description   \nagg_filter a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array  \nalpha scalar or None  \nanimated bool  \nbackgroundcolor color  \nbbox dict with properties for patches.FancyBboxPatch  \nclip_box unknown  \nclip_on unknown  \nclip_path unknown  \ncolor or c color  \nfigure Figure  \nfontfamily or family {FONTNAME, 'serif', 'sans-serif', 'cursive', 'fantasy', 'monospace'}  \nfontproperties or font or font_properties font_manager.FontProperties or str or pathlib.Path  \nfontsize or size float or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}  \nfontstretch or stretch {a numeric value in range 0-1000, 'ultra-condensed', 'extra-condensed', 'condensed', 'semi-condensed', 'normal', 'semi-expanded', 'expanded', 'extra-expanded', 'ultra-expanded'}  \nfontstyle or style {'normal', 'italic', 'oblique'}  \nfontvariant or variant {'normal', 'small-caps'}  \nfontweight or weight {a numeric value in range 0-1000, 'ultralight', 'light', 'normal', 'regular', 'book', 'medium', 'roman', 'semibold', 'demibold', 'demi', 'bold', 'heavy', 'extra bold', 'black'}  \ngid str  \nhorizontalalignment or ha {'center', 'right', 'left'}  \nin_layout bool  \nlabel object  \nlinespacing float (multiple of font size)  \nmath_fontfamily str  \nmultialignment or ma {'left', 'right', 'center'}  \nparse_math bool  \npath_effects AbstractPathEffect  \npicker None or bool or float or callable  \nposition (float, float)  \nrasterized bool  \nrotation float or {'vertical', 'horizontal'}  \nrotation_mode {None, 'default', 'anchor'}  \nsketch_params (scale: float, length: float, randomness: float)  \nsnap bool or None  \ntext object  \ntransform Transform  \ntransform_rotates_text bool  \nurl str  \nusetex bool or None  \nverticalalignment or va {'center', 'top', 'bottom', 'baseline', 'center_baseline'}  \nvisible bool  \nwrap bool  \nx float  \ny float  \nzorder float       Examples Individual keyword arguments can be used to override any given parameter: >>> text(x, y, s, fontsize=12)\n The default transform specifies that text is in data coords, alternatively, you can specify text in axis coords ((0, 0) is lower-left and (1, 1) is upper-right). The example below places text in the center of the Axes: >>> text(0.5, 0.5, 'matplotlib', horizontalalignment='center',\n...      verticalalignment='center', transform=ax.transAxes)\n You can put a rectangular box around the text instance (e.g., to set a background color) by using the keyword bbox. bbox is a dictionary of Rectangle properties. For example: >>> text(x, y, s, bbox=dict(facecolor='red', alpha=0.5))\n \n  Examples using matplotlib.pyplot.text\n \n   Figure size in different units   \n\n   Auto-wrapping text   \n\n   Styling text boxes   \n\n   Controlling style of text and labels using a dictionary   \n\n   Pyplot Mathtext   \n\n   Pyplot Text   \n\n   Reference for Matplotlib artists   \n\n   Close Event   \n\n   transforms.offset_copy   \n\n   Pyplot tutorial   \n\n   Path effects guide   \n\n   Text properties and layout   \n\n   Annotations","title":"matplotlib._as_gen.matplotlib.pyplot.text"},{"text":"matplotlib.pyplot.xticks   matplotlib.pyplot.xticks(ticks=None, labels=None, **kwargs)[source]\n \nGet or set the current tick locations and labels of the x-axis. Pass no arguments to return the current values without modifying them.  Parameters \n \nticksarray-like, optional\n\n\nThe list of xtick locations. Passing an empty list removes all xticks.  \nlabelsarray-like, optional\n\n\nThe labels to place at the given ticks locations. This argument can only be passed if ticks is passed as well.  **kwargs\n\nText properties can be used to control the appearance of the labels.    Returns \n locs\n\nThe list of xtick locations.  labels\n\nThe list of xlabel Text objects.     Notes Calling this function with no arguments (e.g. xticks()) is the pyplot equivalent of calling get_xticks and get_xticklabels on the current axes. Calling this function with arguments is the pyplot equivalent of calling set_xticks and set_xticklabels on the current axes. Examples >>> locs, labels = xticks()  # Get the current locations and labels.\n>>> xticks(np.arange(0, 1, step=0.2))  # Set label locations.\n>>> xticks(np.arange(3), ['Tom', 'Dick', 'Sue'])  # Set text labels.\n>>> xticks([0, 1, 2], ['January', 'February', 'March'],\n...        rotation=20)  # Set text labels and properties.\n>>> xticks([])  # Disable xticks.\n \n  Examples using matplotlib.pyplot.xticks\n \n   Secondary Axis   \n\n   Table Demo   \n\n   Rotating custom tick labels","title":"matplotlib._as_gen.matplotlib.pyplot.xticks"},{"text":"texname","title":"matplotlib.dviread#matplotlib.dviread.DviFont.texname"},{"text":"matplotlib.pyplot.annotate   matplotlib.pyplot.annotate(text, xy, *args, **kwargs)[source]\n \nAnnotate the point xy with text text. In the simplest form, the text is placed at xy. Optionally, the text can be displayed in another position xytext. An arrow pointing from the text to the annotated point xy can then be added by defining arrowprops.  Parameters \n \ntextstr\n\n\nThe text of the annotation.  \nxy(float, float)\n\n\nThe point (x, y) to annotate. The coordinate system is determined by xycoords.  \nxytext(float, float), default: xy\n\n\nThe position (x, y) to place the text at. The coordinate system is determined by textcoords.  \nxycoordsstr or Artist or Transform or callable or (float, float), default: 'data'\n\n\nThe coordinate system that xy is given in. The following types of values are supported:  \nOne of the following strings:   \nValue Description   \n'figure points' Points from the lower left of the figure  \n'figure pixels' Pixels from the lower left of the figure  \n'figure fraction' Fraction of figure from lower left  \n'subfigure points' Points from the lower left of the subfigure  \n'subfigure pixels' Pixels from the lower left of the subfigure  \n'subfigure fraction' Fraction of subfigure from lower left  \n'axes points' Points from lower left corner of axes  \n'axes pixels' Pixels from lower left corner of axes  \n'axes fraction' Fraction of axes from lower left  \n'data' Use the coordinate system of the object being annotated (default)  \n'polar' (theta, r) if not native 'data' coordinates   Note that 'subfigure pixels' and 'figure pixels' are the same for the parent figure, so users who want code that is usable in a subfigure can use 'subfigure pixels'.  An Artist: xy is interpreted as a fraction of the artist's Bbox. E.g. (0, 0) would be the lower left corner of the bounding box and (0.5, 1) would be the center top of the bounding box. A Transform to transform xy to screen coordinates. \nA function with one of the following signatures: def transform(renderer) -> Bbox\ndef transform(renderer) -> Transform\n where renderer is a RendererBase subclass. The result of the function is interpreted like the Artist and Transform cases above.  A tuple (xcoords, ycoords) specifying separate coordinate systems for x and y. xcoords and ycoords must each be of one of the above described types.  See Advanced Annotations for more details.  \ntextcoordsstr or Artist or Transform or callable or (float, float), default: value of xycoords\n\n\nThe coordinate system that xytext is given in. All xycoords values are valid as well as the following strings:   \nValue Description   \n'offset points' Offset (in points) from the xy value  \n'offset pixels' Offset (in pixels) from the xy value    \narrowpropsdict, optional\n\n\nThe properties used to draw a FancyArrowPatch arrow between the positions xy and xytext. Defaults to None, i.e. no arrow is drawn. For historical reasons there are two different ways to specify arrows, \"simple\" and \"fancy\": Simple arrow: If arrowprops does not contain the key 'arrowstyle' the allowed keys are:   \nKey Description   \nwidth The width of the arrow in points  \nheadwidth The width of the base of the arrow head in points  \nheadlength The length of the arrow head in points  \nshrink Fraction of total length to shrink from both ends  \n? Any key to matplotlib.patches.FancyArrowPatch   The arrow is attached to the edge of the text box, the exact position (corners or centers) depending on where it's pointing to. Fancy arrow: This is used if 'arrowstyle' is provided in the arrowprops. Valid keys are the following FancyArrowPatch parameters:   \nKey Description   \narrowstyle the arrow style  \nconnectionstyle the connection style  \nrelpos see below; default is (0.5, 0.5)  \npatchA default is bounding box of the text  \npatchB default is None  \nshrinkA default is 2 points  \nshrinkB default is 2 points  \nmutation_scale default is text size (in points)  \nmutation_aspect default is 1.  \n? any key for matplotlib.patches.PathPatch   The exact starting point position of the arrow is defined by relpos. It's a tuple of relative coordinates of the text box, where (0, 0) is the lower left corner and (1, 1) is the upper right corner. Values <0 and >1 are supported and specify points outside the text box. By default (0.5, 0.5) the starting point is centered in the text box.  \nannotation_clipbool or None, default: None\n\n\nWhether to draw the annotation when the annotation point xy is outside the axes area.  If True, the annotation will only be drawn when xy is within the axes. If False, the annotation will always be drawn. If None, the annotation will only be drawn when xy is within the axes and xycoords is 'data'.   **kwargs\n\nAdditional kwargs are passed to Text.    Returns \n Annotation\n    See also  Advanced Annotations\n  \n  Examples using matplotlib.pyplot.annotate\n \n   Pyplot tutorial   \n\n   Annotations","title":"matplotlib._as_gen.matplotlib.pyplot.annotate"},{"text":"text(x, y, s, fontdict=None, **kwargs)[source]\n \nAdd text to figure.  Parameters \n \nx, yfloat\n\n\nThe position to place the text. By default, this is in figure coordinates, floats in [0, 1]. The coordinate system can be changed using the transform keyword.  \nsstr\n\n\nThe text string.  \nfontdictdict, optional\n\n\nA dictionary to override the default text properties. If not given, the defaults are determined by rcParams[\"font.*\"]. Properties passed as kwargs override the corresponding ones given in fontdict.    Returns \n Text\n  Other Parameters \n \n**kwargsText properties\n\n\nOther miscellaneous text parameters.   \nProperty Description   \nagg_filter a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array  \nalpha scalar or None  \nanimated bool  \nbackgroundcolor color  \nbbox dict with properties for patches.FancyBboxPatch  \nclip_box unknown  \nclip_on unknown  \nclip_path unknown  \ncolor or c color  \nfigure Figure  \nfontfamily or family {FONTNAME, 'serif', 'sans-serif', 'cursive', 'fantasy', 'monospace'}  \nfontproperties or font or font_properties font_manager.FontProperties or str or pathlib.Path  \nfontsize or size float or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}  \nfontstretch or stretch {a numeric value in range 0-1000, 'ultra-condensed', 'extra-condensed', 'condensed', 'semi-condensed', 'normal', 'semi-expanded', 'expanded', 'extra-expanded', 'ultra-expanded'}  \nfontstyle or style {'normal', 'italic', 'oblique'}  \nfontvariant or variant {'normal', 'small-caps'}  \nfontweight or weight {a numeric value in range 0-1000, 'ultralight', 'light', 'normal', 'regular', 'book', 'medium', 'roman', 'semibold', 'demibold', 'demi', 'bold', 'heavy', 'extra bold', 'black'}  \ngid str  \nhorizontalalignment or ha {'center', 'right', 'left'}  \nin_layout bool  \nlabel object  \nlinespacing float (multiple of font size)  \nmath_fontfamily str  \nmultialignment or ma {'left', 'right', 'center'}  \nparse_math bool  \npath_effects AbstractPathEffect  \npicker None or bool or float or callable  \nposition (float, float)  \nrasterized bool  \nrotation float or {'vertical', 'horizontal'}  \nrotation_mode {None, 'default', 'anchor'}  \nsketch_params (scale: float, length: float, randomness: float)  \nsnap bool or None  \ntext object  \ntransform Transform  \ntransform_rotates_text bool  \nurl str  \nusetex bool or None  \nverticalalignment or va {'center', 'top', 'bottom', 'baseline', 'center_baseline'}  \nvisible bool  \nwrap bool  \nx float  \ny float  \nzorder float        See also  Axes.text\npyplot.text","title":"matplotlib.figure_api#matplotlib.figure.SubFigure.text"}]}
{"task_id":17138464,"prompt":"def f_17138464(x, y):\n\treturn ","suffix":"","canonical_solution":"plt.plot(x, y, label='$H_2O$')","test_start":"\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef check(candidate):","test":["\n    pic = candidate(np.array([1,2,3]),np.array([4,5,6]))[0]\n    assert pic.get_label() == '$H_2O$'\n    x, y = pic.get_data()\n    assert all(x == np.array([1,2,3]))\n    assert all(y == np.array([4,5,6]))\n","\n    pic = candidate(np.array([6, 7, 899]),np.array([0, 1, 245]))[0]\n    assert pic.get_label() == '$H_2O$'\n    x, y = pic.get_data()\n    assert all(x == np.array([6, 7, 899]))\n    assert all(y == np.array([0, 1, 245]))\n"],"entry_point":"f_17138464","intent":"subscript text 'H20' with '2' as subscripted in matplotlib labels for arrays 'x' and 'y'.","library":["matplotlib","numpy"],"docs":[{"text":"matplotlib.axis.Tick.set_label2   Tick.set_label2(s)[source]\n \nSet the label2 text.  Parameters \n \nsstr","title":"matplotlib._as_gen.matplotlib.axis.tick.set_label2"},{"text":"zorder=3","title":"matplotlib.text_api#matplotlib.text.Text.zorder"},{"text":"toggle_label(b)[source]","title":"matplotlib._as_gen.mpl_toolkits.axes_grid1.axes_grid.cbaraxesbase#mpl_toolkits.axes_grid1.axes_grid.CbarAxesBase.toggle_label"},{"text":"matplotlib.axes.Axes.set_ylabel   Axes.set_ylabel(ylabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)[source]\n \nSet the label for the y-axis.  Parameters \n \nylabelstr\n\n\nThe label text.  \nlabelpadfloat, default: rcParams[\"axes.labelpad\"] (default: 4.0)\n\n\nSpacing in points from the Axes bounding box including ticks and tick labels. If None, the previous value is left as is.  \nloc{'bottom', 'center', 'top'}, default: rcParams[\"yaxis.labellocation\"] (default: 'center')\n\n\nThe label position. This is a high-level alternative for passing parameters y and horizontalalignment.    Other Parameters \n \n**kwargsText properties\n\n\nText properties control the appearance of the label.      See also  text\n\nDocuments the properties supported by Text.    \n  Examples using matplotlib.axes.Axes.set_ylabel\n \n   Bar Label Demo   \n\n   Stacked bar chart   \n\n   Grouped bar chart with labels   \n\n   CSD Demo   \n\n   Fill Between and Alpha   \n\n   Hatch-filled histograms   \n\n   Hat graph   \n\n   Psd Demo   \n\n   Scatter Demo2   \n\n   Stackplots and streamgraphs   \n\n   Contourf Demo   \n\n   Creating annotated heatmaps   \n\n   Tricontour Demo   \n\n   Tripcolor Demo   \n\n   Triplot Demo   \n\n   Aligning Labels   \n\n   Axes Demo   \n\n   Axis Label Position   \n\n   Resizing axes with constrained layout   \n\n   Resizing axes with tight layout   \n\n   Figure labels: suptitle, supxlabel, supylabel   \n\n   Invert Axes   \n\n   Secondary Axis   \n\n   Figure subfigures   \n\n   Multiple subplots   \n\n   Plots with different scales   \n\n   Box plots with custom fill colors   \n\n   Boxplots   \n\n   Box plot vs. violin plot comparison   \n\n   Violin plot customization   \n\n   Using histograms to plot a cumulative distribution   \n\n   Some features of the histogram (hist) function   \n\n   Producing multiple histograms side by side   \n\n   Using accented text in matplotlib   \n\n   Date tick labels   \n\n   Legend Demo   \n\n   Mathtext   \n\n   Multiline   \n\n   Rendering math equations using TeX   \n\n   Simple axes labels   \n\n   Text Commands   \n\n   Color Demo   \n\n   Line, Poly and RegularPoly Collection with autoscaling   \n\n   Ellipse Collection   \n\n   Dark background style sheet   \n\n   Make Room For Ylabel Using Axesgrid   \n\n   Parasite Simple   \n\n   Parasite Axes demo   \n\n   Parasite axis demo   \n\n   Ticklabel alignment   \n\n   Simple Axis Direction03   \n\n   Simple Axisline   \n\n   Anatomy of a figure   \n\n   XKCD   \n\n   Pythonic Matplotlib   \n\n   Plot 2D data on 3D plot   \n\n   Create 2D bar graphs in different planes   \n\n   3D errorbars   \n\n   Lorenz Attractor   \n\n   2D and 3D Axes in same Figure   \n\n   Automatic Text Offsetting   \n\n   3D scatterplot   \n\n   3D surface with polar coordinates   \n\n   Text annotations in 3D   \n\n   Log Bar   \n\n   Symlog Demo   \n\n   MRI With EEG   \n\n   Topographic hillshading   \n\n   Multiple Yaxis With Spines   \n\n   Basic Usage   \n\n   Artist tutorial   \n\n   Constrained Layout Guide   \n\n   Tight Layout guide   \n\n   Arranging multiple Axes in a Figure   \n\n   Choosing Colormaps in Matplotlib   \n\n   Text in Matplotlib Plots","title":"matplotlib._as_gen.matplotlib.axes.axes.set_ylabel"},{"text":"matplotlib.pyplot.ylabel   matplotlib.pyplot.ylabel(ylabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)[source]\n \nSet the label for the y-axis.  Parameters \n \nylabelstr\n\n\nThe label text.  \nlabelpadfloat, default: rcParams[\"axes.labelpad\"] (default: 4.0)\n\n\nSpacing in points from the Axes bounding box including ticks and tick labels. If None, the previous value is left as is.  \nloc{'bottom', 'center', 'top'}, default: rcParams[\"yaxis.labellocation\"] (default: 'center')\n\n\nThe label position. This is a high-level alternative for passing parameters y and horizontalalignment.    Other Parameters \n \n**kwargsText properties\n\n\nText properties control the appearance of the label.      See also  text\n\nDocuments the properties supported by Text.    \n  Examples using matplotlib.pyplot.ylabel\n \n   Scatter Symbol   \n\n   Multiple subplots   \n\n   Controlling style of text and labels using a dictionary   \n\n   Pyplot Mathtext   \n\n   Pyplot Simple   \n\n   Pyplot Text   \n\n   Solarized Light stylesheet   \n\n   Findobj Demo   \n\n   Table Demo   \n\n   Custom scale   \n\n   Basic Usage   \n\n   Pyplot tutorial","title":"matplotlib._as_gen.matplotlib.pyplot.ylabel"},{"text":"matplotlib.pyplot.text   matplotlib.pyplot.text(x, y, s, fontdict=None, **kwargs)[source]\n \nAdd text to the Axes. Add the text s to the Axes at location x, y in data coordinates.  Parameters \n \nx, yfloat\n\n\nThe position to place the text. By default, this is in data coordinates. The coordinate system can be changed using the transform parameter.  \nsstr\n\n\nThe text.  \nfontdictdict, default: None\n\n\nA dictionary to override the default text properties. If fontdict is None, the defaults are determined by rcParams.    Returns \n Text\n\nThe created Text instance.    Other Parameters \n \n**kwargsText properties.\n\n\nOther miscellaneous text parameters.   \nProperty Description   \nagg_filter a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array  \nalpha scalar or None  \nanimated bool  \nbackgroundcolor color  \nbbox dict with properties for patches.FancyBboxPatch  \nclip_box unknown  \nclip_on unknown  \nclip_path unknown  \ncolor or c color  \nfigure Figure  \nfontfamily or family {FONTNAME, 'serif', 'sans-serif', 'cursive', 'fantasy', 'monospace'}  \nfontproperties or font or font_properties font_manager.FontProperties or str or pathlib.Path  \nfontsize or size float or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}  \nfontstretch or stretch {a numeric value in range 0-1000, 'ultra-condensed', 'extra-condensed', 'condensed', 'semi-condensed', 'normal', 'semi-expanded', 'expanded', 'extra-expanded', 'ultra-expanded'}  \nfontstyle or style {'normal', 'italic', 'oblique'}  \nfontvariant or variant {'normal', 'small-caps'}  \nfontweight or weight {a numeric value in range 0-1000, 'ultralight', 'light', 'normal', 'regular', 'book', 'medium', 'roman', 'semibold', 'demibold', 'demi', 'bold', 'heavy', 'extra bold', 'black'}  \ngid str  \nhorizontalalignment or ha {'center', 'right', 'left'}  \nin_layout bool  \nlabel object  \nlinespacing float (multiple of font size)  \nmath_fontfamily str  \nmultialignment or ma {'left', 'right', 'center'}  \nparse_math bool  \npath_effects AbstractPathEffect  \npicker None or bool or float or callable  \nposition (float, float)  \nrasterized bool  \nrotation float or {'vertical', 'horizontal'}  \nrotation_mode {None, 'default', 'anchor'}  \nsketch_params (scale: float, length: float, randomness: float)  \nsnap bool or None  \ntext object  \ntransform Transform  \ntransform_rotates_text bool  \nurl str  \nusetex bool or None  \nverticalalignment or va {'center', 'top', 'bottom', 'baseline', 'center_baseline'}  \nvisible bool  \nwrap bool  \nx float  \ny float  \nzorder float       Examples Individual keyword arguments can be used to override any given parameter: >>> text(x, y, s, fontsize=12)\n The default transform specifies that text is in data coords, alternatively, you can specify text in axis coords ((0, 0) is lower-left and (1, 1) is upper-right). The example below places text in the center of the Axes: >>> text(0.5, 0.5, 'matplotlib', horizontalalignment='center',\n...      verticalalignment='center', transform=ax.transAxes)\n You can put a rectangular box around the text instance (e.g., to set a background color) by using the keyword bbox. bbox is a dictionary of Rectangle properties. For example: >>> text(x, y, s, bbox=dict(facecolor='red', alpha=0.5))\n \n  Examples using matplotlib.pyplot.text\n \n   Figure size in different units   \n\n   Auto-wrapping text   \n\n   Styling text boxes   \n\n   Controlling style of text and labels using a dictionary   \n\n   Pyplot Mathtext   \n\n   Pyplot Text   \n\n   Reference for Matplotlib artists   \n\n   Close Event   \n\n   transforms.offset_copy   \n\n   Pyplot tutorial   \n\n   Path effects guide   \n\n   Text properties and layout   \n\n   Annotations","title":"matplotlib._as_gen.matplotlib.pyplot.text"},{"text":"matplotlib.pyplot.xticks   matplotlib.pyplot.xticks(ticks=None, labels=None, **kwargs)[source]\n \nGet or set the current tick locations and labels of the x-axis. Pass no arguments to return the current values without modifying them.  Parameters \n \nticksarray-like, optional\n\n\nThe list of xtick locations. Passing an empty list removes all xticks.  \nlabelsarray-like, optional\n\n\nThe labels to place at the given ticks locations. This argument can only be passed if ticks is passed as well.  **kwargs\n\nText properties can be used to control the appearance of the labels.    Returns \n locs\n\nThe list of xtick locations.  labels\n\nThe list of xlabel Text objects.     Notes Calling this function with no arguments (e.g. xticks()) is the pyplot equivalent of calling get_xticks and get_xticklabels on the current axes. Calling this function with arguments is the pyplot equivalent of calling set_xticks and set_xticklabels on the current axes. Examples >>> locs, labels = xticks()  # Get the current locations and labels.\n>>> xticks(np.arange(0, 1, step=0.2))  # Set label locations.\n>>> xticks(np.arange(3), ['Tom', 'Dick', 'Sue'])  # Set text labels.\n>>> xticks([0, 1, 2], ['January', 'February', 'March'],\n...        rotation=20)  # Set text labels and properties.\n>>> xticks([])  # Disable xticks.\n \n  Examples using matplotlib.pyplot.xticks\n \n   Secondary Axis   \n\n   Table Demo   \n\n   Rotating custom tick labels","title":"matplotlib._as_gen.matplotlib.pyplot.xticks"},{"text":"texname","title":"matplotlib.dviread#matplotlib.dviread.DviFont.texname"},{"text":"matplotlib.pyplot.annotate   matplotlib.pyplot.annotate(text, xy, *args, **kwargs)[source]\n \nAnnotate the point xy with text text. In the simplest form, the text is placed at xy. Optionally, the text can be displayed in another position xytext. An arrow pointing from the text to the annotated point xy can then be added by defining arrowprops.  Parameters \n \ntextstr\n\n\nThe text of the annotation.  \nxy(float, float)\n\n\nThe point (x, y) to annotate. The coordinate system is determined by xycoords.  \nxytext(float, float), default: xy\n\n\nThe position (x, y) to place the text at. The coordinate system is determined by textcoords.  \nxycoordsstr or Artist or Transform or callable or (float, float), default: 'data'\n\n\nThe coordinate system that xy is given in. The following types of values are supported:  \nOne of the following strings:   \nValue Description   \n'figure points' Points from the lower left of the figure  \n'figure pixels' Pixels from the lower left of the figure  \n'figure fraction' Fraction of figure from lower left  \n'subfigure points' Points from the lower left of the subfigure  \n'subfigure pixels' Pixels from the lower left of the subfigure  \n'subfigure fraction' Fraction of subfigure from lower left  \n'axes points' Points from lower left corner of axes  \n'axes pixels' Pixels from lower left corner of axes  \n'axes fraction' Fraction of axes from lower left  \n'data' Use the coordinate system of the object being annotated (default)  \n'polar' (theta, r) if not native 'data' coordinates   Note that 'subfigure pixels' and 'figure pixels' are the same for the parent figure, so users who want code that is usable in a subfigure can use 'subfigure pixels'.  An Artist: xy is interpreted as a fraction of the artist's Bbox. E.g. (0, 0) would be the lower left corner of the bounding box and (0.5, 1) would be the center top of the bounding box. A Transform to transform xy to screen coordinates. \nA function with one of the following signatures: def transform(renderer) -> Bbox\ndef transform(renderer) -> Transform\n where renderer is a RendererBase subclass. The result of the function is interpreted like the Artist and Transform cases above.  A tuple (xcoords, ycoords) specifying separate coordinate systems for x and y. xcoords and ycoords must each be of one of the above described types.  See Advanced Annotations for more details.  \ntextcoordsstr or Artist or Transform or callable or (float, float), default: value of xycoords\n\n\nThe coordinate system that xytext is given in. All xycoords values are valid as well as the following strings:   \nValue Description   \n'offset points' Offset (in points) from the xy value  \n'offset pixels' Offset (in pixels) from the xy value    \narrowpropsdict, optional\n\n\nThe properties used to draw a FancyArrowPatch arrow between the positions xy and xytext. Defaults to None, i.e. no arrow is drawn. For historical reasons there are two different ways to specify arrows, \"simple\" and \"fancy\": Simple arrow: If arrowprops does not contain the key 'arrowstyle' the allowed keys are:   \nKey Description   \nwidth The width of the arrow in points  \nheadwidth The width of the base of the arrow head in points  \nheadlength The length of the arrow head in points  \nshrink Fraction of total length to shrink from both ends  \n? Any key to matplotlib.patches.FancyArrowPatch   The arrow is attached to the edge of the text box, the exact position (corners or centers) depending on where it's pointing to. Fancy arrow: This is used if 'arrowstyle' is provided in the arrowprops. Valid keys are the following FancyArrowPatch parameters:   \nKey Description   \narrowstyle the arrow style  \nconnectionstyle the connection style  \nrelpos see below; default is (0.5, 0.5)  \npatchA default is bounding box of the text  \npatchB default is None  \nshrinkA default is 2 points  \nshrinkB default is 2 points  \nmutation_scale default is text size (in points)  \nmutation_aspect default is 1.  \n? any key for matplotlib.patches.PathPatch   The exact starting point position of the arrow is defined by relpos. It's a tuple of relative coordinates of the text box, where (0, 0) is the lower left corner and (1, 1) is the upper right corner. Values <0 and >1 are supported and specify points outside the text box. By default (0.5, 0.5) the starting point is centered in the text box.  \nannotation_clipbool or None, default: None\n\n\nWhether to draw the annotation when the annotation point xy is outside the axes area.  If True, the annotation will only be drawn when xy is within the axes. If False, the annotation will always be drawn. If None, the annotation will only be drawn when xy is within the axes and xycoords is 'data'.   **kwargs\n\nAdditional kwargs are passed to Text.    Returns \n Annotation\n    See also  Advanced Annotations\n  \n  Examples using matplotlib.pyplot.annotate\n \n   Pyplot tutorial   \n\n   Annotations","title":"matplotlib._as_gen.matplotlib.pyplot.annotate"},{"text":"text(x, y, s, fontdict=None, **kwargs)[source]\n \nAdd text to figure.  Parameters \n \nx, yfloat\n\n\nThe position to place the text. By default, this is in figure coordinates, floats in [0, 1]. The coordinate system can be changed using the transform keyword.  \nsstr\n\n\nThe text string.  \nfontdictdict, optional\n\n\nA dictionary to override the default text properties. If not given, the defaults are determined by rcParams[\"font.*\"]. Properties passed as kwargs override the corresponding ones given in fontdict.    Returns \n Text\n  Other Parameters \n \n**kwargsText properties\n\n\nOther miscellaneous text parameters.   \nProperty Description   \nagg_filter a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array  \nalpha scalar or None  \nanimated bool  \nbackgroundcolor color  \nbbox dict with properties for patches.FancyBboxPatch  \nclip_box unknown  \nclip_on unknown  \nclip_path unknown  \ncolor or c color  \nfigure Figure  \nfontfamily or family {FONTNAME, 'serif', 'sans-serif', 'cursive', 'fantasy', 'monospace'}  \nfontproperties or font or font_properties font_manager.FontProperties or str or pathlib.Path  \nfontsize or size float or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}  \nfontstretch or stretch {a numeric value in range 0-1000, 'ultra-condensed', 'extra-condensed', 'condensed', 'semi-condensed', 'normal', 'semi-expanded', 'expanded', 'extra-expanded', 'ultra-expanded'}  \nfontstyle or style {'normal', 'italic', 'oblique'}  \nfontvariant or variant {'normal', 'small-caps'}  \nfontweight or weight {a numeric value in range 0-1000, 'ultralight', 'light', 'normal', 'regular', 'book', 'medium', 'roman', 'semibold', 'demibold', 'demi', 'bold', 'heavy', 'extra bold', 'black'}  \ngid str  \nhorizontalalignment or ha {'center', 'right', 'left'}  \nin_layout bool  \nlabel object  \nlinespacing float (multiple of font size)  \nmath_fontfamily str  \nmultialignment or ma {'left', 'right', 'center'}  \nparse_math bool  \npath_effects AbstractPathEffect  \npicker None or bool or float or callable  \nposition (float, float)  \nrasterized bool  \nrotation float or {'vertical', 'horizontal'}  \nrotation_mode {None, 'default', 'anchor'}  \nsketch_params (scale: float, length: float, randomness: float)  \nsnap bool or None  \ntext object  \ntransform Transform  \ntransform_rotates_text bool  \nurl str  \nusetex bool or None  \nverticalalignment or va {'center', 'top', 'bottom', 'baseline', 'center_baseline'}  \nvisible bool  \nwrap bool  \nx float  \ny float  \nzorder float        See also  Axes.text\npyplot.text","title":"matplotlib.figure_api#matplotlib.figure.SubFigure.text"}]}
{"task_id":9138112,"prompt":"def f_9138112(mylist):\n\treturn ","suffix":"","canonical_solution":"[x for x in mylist if len(x) == 3]","test_start":"\ndef check(candidate):","test":["\n    assert candidate([[1,2,3], 'abc', [345,53], 'avsvasf']) == [[1,2,3], 'abc']\n","\n    assert candidate([[435,654.4,45,2],[34,34,757,65,32423]]) == []\n"],"entry_point":"f_9138112","intent":"loop over a list `mylist` if sublists length equals 3","library":[],"docs":[]}
{"task_id":1807026,"prompt":"def f_1807026():\n\t","suffix":"\n\treturn lst","canonical_solution":"lst = [Object() for _ in range(100)]","test_start":"\nclass Object(): \n    def __init__(self): \n        self.name = \"object\"\n\ndef check(candidate):","test":["\n    lst = candidate()\n    assert all([x.name == \"object\" for x in lst])\n"],"entry_point":"f_1807026","intent":"initialize a list `lst` of 100 objects Object()","library":[],"docs":[]}
{"task_id":13793321,"prompt":"def f_13793321(df1, df2):\n\treturn ","suffix":"","canonical_solution":"df1.merge(df2, on='Date_Time')","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df1 = pd.DataFrame([[1, 2, 3]], columns=[\"Date\", \"Time\", \"Date_Time\"])\n    df2 = pd.DataFrame([[1, 3],[4, 9]], columns=[\"Name\", \"Date_Time\"])\n    assert candidate(df1, df2).to_dict() == {'Date': {0: 1}, 'Time': {0: 2}, 'Date_Time': {0: 3}, 'Name': {0: 1}}\n"],"entry_point":"f_13793321","intent":"joining data from dataframe `df1` with data from dataframe `df2` based on matching values of column 'Date_Time' in both dataframes","library":["pandas"],"docs":[{"text":"pandas.Series.dt.start_time   Series.dt.start_time","title":"pandas.reference.api.pandas.series.dt.start_time"},{"text":"pandas.Timestamp.fold   Timestamp.fold","title":"pandas.reference.api.pandas.timestamp.fold"},{"text":"pandas.Timedelta.freq   Timedelta.freq","title":"pandas.reference.api.pandas.timedelta.freq"},{"text":"pandas.tseries.offsets.BYearEnd.copy   BYearEnd.copy()","title":"pandas.reference.api.pandas.tseries.offsets.byearend.copy"},{"text":"pandas.Timedelta.is_populated   Timedelta.is_populated","title":"pandas.reference.api.pandas.timedelta.is_populated"},{"text":"pandas.tseries.offsets.DateOffset.copy   DateOffset.copy()","title":"pandas.reference.api.pandas.tseries.offsets.dateoffset.copy"},{"text":"pandas.Timestamp.freq   Timestamp.freq","title":"pandas.reference.api.pandas.timestamp.freq"},{"text":"pandas.Series.dt.end_time   Series.dt.end_time","title":"pandas.reference.api.pandas.series.dt.end_time"},{"text":"pandas.tseries.offsets.Hour.copy   Hour.copy()","title":"pandas.reference.api.pandas.tseries.offsets.hour.copy"},{"text":"pandas.Timestamp.second   Timestamp.second","title":"pandas.reference.api.pandas.timestamp.second"}]}
{"task_id":3367288,"prompt":"def f_3367288(str1):\n\treturn ","suffix":"","canonical_solution":"'first string is: %s, second one is: %s' % (str1, 'geo.tif')","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"s001\") == \"first string is: s001, second one is: geo.tif\"\n","\n    assert candidate(\"\") == \"first string is: , second one is: geo.tif\"\n","\n    assert candidate(\"  \") == \"first string is:   , second one is: geo.tif\"\n"],"entry_point":"f_3367288","intent":"use `%s` operator to print variable values `str1` inside a string","library":[],"docs":[]}
{"task_id":3475251,"prompt":"def f_3475251():\n\treturn ","suffix":"","canonical_solution":"[x.strip() for x in '2.MATCHES $$TEXT$$ STRING'.split('$$TEXT$$')]","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == ['2.MATCHES', 'STRING']\n"],"entry_point":"f_3475251","intent":"Split a string '2.MATCHES $$TEXT$$ STRING' by a delimiter '$$TEXT$$'","library":[],"docs":[]}
{"task_id":273192,"prompt":"def f_273192(directory):\n\t","suffix":"\n\treturn ","canonical_solution":"if (not os.path.exists(directory)):\n\t    os.makedirs(directory)","test_start":"\nimport os \n\ndef check(candidate):","test":["\n    candidate(\"hello\")\n    assert os.path.exists(\"hello\")\n","\n    candidate(\"_some_dir\")\n    assert os.path.exists(\"_some_dir\")\n"],"entry_point":"f_273192","intent":"check if directory `directory ` exists and create it if necessary","library":["os"],"docs":[{"text":"stat.S_IFDIR  \nDirectory.","title":"python.library.stat#stat.S_IFDIR"},{"text":"exception FileExistsError  \nRaised when trying to create a file or directory which already exists. Corresponds to errno EEXIST.","title":"python.library.exceptions#FileExistsError"},{"text":"TarInfo.isdir()  \nReturn True if it is a directory.","title":"python.library.tarfile#tarfile.TarInfo.isdir"},{"text":"stat.S_ISDIR(mode)  \nReturn non-zero if the mode is from a directory.","title":"python.library.stat#stat.S_ISDIR"},{"text":"FTP.mkd(pathname)  \nCreate a new directory on the server.","title":"python.library.ftplib#ftplib.FTP.mkd"},{"text":"Path.is_dir()  \nReturn True if the current context references a directory.","title":"python.library.zipfile#zipfile.Path.is_dir"},{"text":"right  \nThe directory b.","title":"python.library.filecmp#filecmp.dircmp.right"},{"text":"Path.exists()  \nReturn True if the current context references a file or directory in the zip file.","title":"python.library.zipfile#zipfile.Path.exists"},{"text":"os.path.isdir(path)  \nReturn True if path is an existing directory. This follows symbolic links, so both islink() and isdir() can be true for the same path.  Changed in version 3.6: Accepts a path-like object.","title":"python.library.os.path#os.path.isdir"},{"text":"exception FileNotFoundError  \nRaised when a file or directory is requested but doesn\u2019t exist. Corresponds to errno ENOENT.","title":"python.library.exceptions#FileNotFoundError"}]}
{"task_id":273192,"prompt":"def f_273192(path):\n\t","suffix":"\n\treturn ","canonical_solution":"try:\n\t    os.makedirs(path)\n\texcept OSError:\n\t    if (not os.path.isdir(path)):\n\t        raise","test_start":"\nimport os \n\ndef check(candidate):","test":["\n    candidate(\"hello\")\n    assert os.path.exists(\"hello\")\n","\n    candidate(\"_some_dir\")\n    assert os.path.exists(\"_some_dir\")\n"],"entry_point":"f_273192","intent":"check if a directory `path` exists and create it if necessary","library":["os"],"docs":[{"text":"exception FileExistsError  \nRaised when trying to create a file or directory which already exists. Corresponds to errno EEXIST.","title":"python.library.exceptions#FileExistsError"},{"text":"Path.exists()  \nReturn True if the current context references a file or directory in the zip file.","title":"python.library.zipfile#zipfile.Path.exists"},{"text":"Path.exists()  \nWhether the path points to an existing file or directory: >>> Path('.').exists()\nTrue\n>>> Path('setup.py').exists()\nTrue\n>>> Path('\/etc').exists()\nTrue\n>>> Path('nonexistentfile').exists()\nFalse\n  Note If the path points to a symlink, exists() returns whether the symlink points to an existing file or directory.","title":"python.library.pathlib#pathlib.Path.exists"},{"text":"FTP.mkd(pathname)  \nCreate a new directory on the server.","title":"python.library.ftplib#ftplib.FTP.mkd"},{"text":"stat.S_IFDIR  \nDirectory.","title":"python.library.stat#stat.S_IFDIR"},{"text":"Path.mkdir(mode=0o777, parents=False, exist_ok=False)  \nCreate a new directory at this given path. If mode is given, it is combined with the process\u2019 umask value to determine the file mode and access flags. If the path already exists, FileExistsError is raised. If parents is true, any missing parents of this path are created as needed; they are created with the default permissions without taking mode into account (mimicking the POSIX mkdir -p command). If parents is false (the default), a missing parent raises FileNotFoundError. If exist_ok is false (the default), FileExistsError is raised if the target directory already exists. If exist_ok is true, FileExistsError exceptions will be ignored (same behavior as the POSIX mkdir -p command), but only if the last path component is not an existing non-directory file.  Changed in version 3.5: The exist_ok parameter was added.","title":"python.library.pathlib#pathlib.Path.mkdir"},{"text":"Path.is_dir()  \nReturn True if the current context references a directory.","title":"python.library.zipfile#zipfile.Path.is_dir"},{"text":"os.path.isdir(path)  \nReturn True if path is an existing directory. This follows symbolic links, so both islink() and isdir() can be true for the same path.  Changed in version 3.6: Accepts a path-like object.","title":"python.library.os.path#os.path.isdir"},{"text":"os.path.exists(path)  \nReturn True if path refers to an existing path or an open file descriptor. Returns False for broken symbolic links. On some platforms, this function may return False if permission is not granted to execute os.stat() on the requested file, even if the path physically exists.  Changed in version 3.3: path can now be an integer: True is returned if it is an open file descriptor, False otherwise.   Changed in version 3.6: Accepts a path-like object.","title":"python.library.os.path#os.path.exists"},{"text":"TarInfo.isdir()  \nReturn True if it is a directory.","title":"python.library.tarfile#tarfile.TarInfo.isdir"}]}
{"task_id":273192,"prompt":"def f_273192(path):\n\t","suffix":"\n\treturn ","canonical_solution":"try:\n\t    os.makedirs(path)\n\texcept OSError as exception:\n\t    if (exception.errno != errno.EEXIST):\n\t        raise","test_start":"\nimport os \n\ndef check(candidate):","test":["\n    candidate(\"hello\")\n    assert os.path.exists(\"hello\")\n","\n    candidate(\"_some_dir\")\n    assert os.path.exists(\"_some_dir\")\n"],"entry_point":"f_273192","intent":"check if a directory `path` exists and create it if necessary","library":["os"],"docs":[{"text":"exception FileExistsError  \nRaised when trying to create a file or directory which already exists. Corresponds to errno EEXIST.","title":"python.library.exceptions#FileExistsError"},{"text":"Path.exists()  \nReturn True if the current context references a file or directory in the zip file.","title":"python.library.zipfile#zipfile.Path.exists"},{"text":"Path.exists()  \nWhether the path points to an existing file or directory: >>> Path('.').exists()\nTrue\n>>> Path('setup.py').exists()\nTrue\n>>> Path('\/etc').exists()\nTrue\n>>> Path('nonexistentfile').exists()\nFalse\n  Note If the path points to a symlink, exists() returns whether the symlink points to an existing file or directory.","title":"python.library.pathlib#pathlib.Path.exists"},{"text":"FTP.mkd(pathname)  \nCreate a new directory on the server.","title":"python.library.ftplib#ftplib.FTP.mkd"},{"text":"stat.S_IFDIR  \nDirectory.","title":"python.library.stat#stat.S_IFDIR"},{"text":"Path.mkdir(mode=0o777, parents=False, exist_ok=False)  \nCreate a new directory at this given path. If mode is given, it is combined with the process\u2019 umask value to determine the file mode and access flags. If the path already exists, FileExistsError is raised. If parents is true, any missing parents of this path are created as needed; they are created with the default permissions without taking mode into account (mimicking the POSIX mkdir -p command). If parents is false (the default), a missing parent raises FileNotFoundError. If exist_ok is false (the default), FileExistsError is raised if the target directory already exists. If exist_ok is true, FileExistsError exceptions will be ignored (same behavior as the POSIX mkdir -p command), but only if the last path component is not an existing non-directory file.  Changed in version 3.5: The exist_ok parameter was added.","title":"python.library.pathlib#pathlib.Path.mkdir"},{"text":"Path.is_dir()  \nReturn True if the current context references a directory.","title":"python.library.zipfile#zipfile.Path.is_dir"},{"text":"os.path.isdir(path)  \nReturn True if path is an existing directory. This follows symbolic links, so both islink() and isdir() can be true for the same path.  Changed in version 3.6: Accepts a path-like object.","title":"python.library.os.path#os.path.isdir"},{"text":"os.path.exists(path)  \nReturn True if path refers to an existing path or an open file descriptor. Returns False for broken symbolic links. On some platforms, this function may return False if permission is not granted to execute os.stat() on the requested file, even if the path physically exists.  Changed in version 3.3: path can now be an integer: True is returned if it is an open file descriptor, False otherwise.   Changed in version 3.6: Accepts a path-like object.","title":"python.library.os.path#os.path.exists"},{"text":"TarInfo.isdir()  \nReturn True if it is a directory.","title":"python.library.tarfile#tarfile.TarInfo.isdir"}]}
{"task_id":18785032,"prompt":"def f_18785032(text):\n\treturn ","suffix":"","canonical_solution":"re.sub('\\\\bH3\\\\b', 'H1', text)","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate(\"hello world and H3\") == \"hello world and H1\"\n","\n    assert candidate(\"hello world and H1\") == \"hello world and H1\"\n","\n    assert candidate(\"hello world!\") == \"hello world!\"\n"],"entry_point":"f_18785032","intent":"Replace a separate word 'H3' by 'H1' in a string 'text'","library":["re"],"docs":[{"text":"gettext.lngettext(singular, plural, n)","title":"python.library.gettext#gettext.lngettext"},{"text":"gettext.npgettext(context, singular, plural, n)","title":"python.library.gettext#gettext.npgettext"},{"text":"str.replace(old, new[, count])  \nReturn a copy of the string with all occurrences of substring old replaced by new. If the optional argument count is given, only the first count occurrences are replaced.","title":"python.library.stdtypes#str.replace"},{"text":"gettext.dpgettext(domain, context, message)","title":"python.library.gettext#gettext.dpgettext"},{"text":"locale.dgettext(domain, msg)","title":"python.library.locale#locale.dgettext"},{"text":"numpy.char.replace   char.replace(a, old, new, count=None)[source]\n \nFor each element in a, return a copy of the string with all occurrences of substring old replaced by new. Calls str.replace element-wise.  Parameters \n \naarray-like of str or unicode\n\n\nold, newstr or unicode\n\n\ncountint, optional\n\n\nIf the optional argument count is given, only the first count occurrences are replaced.    Returns \n \noutndarray\n\n\nOutput array of str or unicode, depending on input type      See also  str.replace","title":"numpy.reference.generated.numpy.char.replace"},{"text":"lgettext(message)","title":"python.library.gettext#gettext.GNUTranslations.lgettext"},{"text":"class Replace(expression, text, replacement=Value(''), **extra)","title":"django.ref.models.database-functions#django.db.models.functions.Replace"},{"text":"numpy.chararray.replace method   chararray.replace(old, new, count=None)[source]\n \nFor each element in self, return a copy of the string with all occurrences of substring old replaced by new.  See also  char.replace","title":"numpy.reference.generated.numpy.chararray.replace"},{"text":"numpy.char.chararray.replace method   char.chararray.replace(old, new, count=None)[source]\n \nFor each element in self, return a copy of the string with all occurrences of substring old replaced by new.  See also  char.replace","title":"numpy.reference.generated.numpy.char.chararray.replace"}]}
{"task_id":1450897,"prompt":"def f_1450897():\n\treturn ","suffix":"","canonical_solution":"re.sub('\\\\D', '', 'aas30dsa20')","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate() == \"3020\"\n"],"entry_point":"f_1450897","intent":"substitute ASCII letters in string 'aas30dsa20' with empty string ''","library":["re"],"docs":[{"text":"string.ascii_lowercase  \nThe lowercase letters 'abcdefghijklmnopqrstuvwxyz'. This value is not locale-dependent and will not change.","title":"python.library.string#string.ascii_lowercase"},{"text":"numpy.unicode_[source]\n \nalias of numpy.str_","title":"numpy.reference.arrays.scalars#numpy.unicode_"},{"text":"winreg.REG_SZ  \nA null-terminated string.","title":"python.library.winreg#winreg.REG_SZ"},{"text":"bytes.isalpha()  \nbytearray.isalpha()  \nReturn True if all bytes in the sequence are alphabetic ASCII characters and the sequence is not empty, False otherwise. Alphabetic ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'. For example: >>> b'ABCabc'.isalpha()\nTrue\n>>> b'ABCabc1'.isalpha()\nFalse","title":"python.library.stdtypes#bytes.isalpha"},{"text":"numpy.string_[source]\n \nalias of numpy.bytes_","title":"numpy.reference.arrays.scalars#numpy.string_"},{"text":"string.ascii_uppercase  \nThe uppercase letters 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'. This value is not locale-dependent and will not change.","title":"python.library.string#string.ascii_uppercase"},{"text":"encoding\n \nAlias for field number 3","title":"matplotlib.dviread#matplotlib.dviread.PsFont.encoding"},{"text":"str.isascii()  \nReturn True if the string is empty or all characters in the string are ASCII, False otherwise. ASCII characters have code points in the range U+0000-U+007F.  New in version 3.7.","title":"python.library.stdtypes#str.isascii"},{"text":"as_string()","title":"django.ref.contrib.gis.gdal#django.contrib.gis.gdal.Field.as_string"},{"text":"test.support.TESTFN_UNICODE  \nSet to a non-ASCII name for a temporary file.","title":"python.library.test#test.support.TESTFN_UNICODE"}]}
{"task_id":1450897,"prompt":"def f_1450897():\n\treturn ","suffix":"","canonical_solution":"\"\"\"\"\"\".join([x for x in 'aas30dsa20' if x.isdigit()])","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == \"3020\"\n"],"entry_point":"f_1450897","intent":"get digits only from a string `aas30dsa20` using lambda function","library":[],"docs":[]}
{"task_id":14435268,"prompt":"def f_14435268(soup):\n\treturn ","suffix":"","canonical_solution":"soup.find('name').string","test_start":"\nfrom bs4 import BeautifulSoup\n\ndef check(candidate):","test":["\n    content = \"<contact><name>LastName<\/name><lastName>FirstName<\/lastName><phone>+90 333 12345<\/phone><\/contact>\"\n    soup = BeautifulSoup(content)\n    assert candidate(soup) == \"LastName\"\n","\n    content = \"<name>hello world!<\/name>\"\n    soup = BeautifulSoup(content)\n    assert candidate(soup) == \"hello world!\"\n"],"entry_point":"f_14435268","intent":"access a tag called \"name\" in beautifulsoup `soup`","library":["bs4"],"docs":[]}
{"task_id":20180210,"prompt":"def f_20180210(A, B):\n\treturn ","suffix":"","canonical_solution":"np.concatenate((A, B))","test_start":"\nimport numpy as np \n\ndef check(candidate):","test":["\n    A = np.array([1,2])\n    B = np.array([3,4])\n    assert np.allclose(candidate(A, B), np.array([1,2,3,4]))\n","\n    A = np.array([[1,2]])\n    B = np.array([[3,4]])\n    assert np.allclose(candidate(A, B), np.array([[1,2],[3,4]]))\n","\n    A = np.array([[1],[2]])\n    B = np.array([[3],[4]])\n    assert np.allclose(candidate(A, B), np.array([[1],[2],[3],[4]]))\n","\n    a = np.array([[1, 3, 4], [4, 5, 6], [6, 0, -1]])\n    b = np.array([[5, 6, 1], [0, 2, -1], [9, 4, 1]])\n    expected = np.array([[ 1, 3, 4], [ 4, 5, 6],\n        [ 6, 0, -1], [ 5, 6, 1], [ 0, 2, -1], [ 9, 4, 1]])\n    assert np.array_equal(candidate(a, b), expected)\n"],"entry_point":"f_20180210","intent":"Create new matrix object  by concatenating data from matrix A and matrix B","library":["numpy"],"docs":[{"text":"numpy.matrix.data attribute   matrix.data\n \nPython buffer object pointing to the start of the array\u2019s data.","title":"numpy.reference.generated.numpy.matrix.data"},{"text":"matrix_power(n) \u2192 Tensor  \nSee torch.matrix_power()","title":"torch.tensors#torch.Tensor.matrix_power"},{"text":"concat_matrix=b'cm'[source]","title":"matplotlib.backend_pdf_api#matplotlib.backends.backend_pdf.Op.concat_matrix"},{"text":"bmm(batch2) \u2192 Tensor  \nSee torch.bmm()","title":"torch.tensors#torch.Tensor.bmm"},{"text":"numpy.complex_[source]\n \nalias of numpy.cdouble","title":"numpy.reference.arrays.scalars#numpy.complex_"},{"text":"numpy.matrix.dumps method   matrix.dumps()\n \nReturns the pickle of the array as a string. pickle.loads will convert the string back to an array.  Parameters \n None","title":"numpy.reference.generated.numpy.matrix.dumps"},{"text":"matmul(tensor2) \u2192 Tensor  \nSee torch.matmul()","title":"torch.tensors#torch.Tensor.matmul"},{"text":"numpy.record.base attribute   record.base\n \nbase object","title":"numpy.reference.generated.numpy.record.base"},{"text":"transpose(dim0, dim1) \u2192 Tensor  \nSee torch.transpose()","title":"torch.tensors#torch.Tensor.transpose"},{"text":"common_files  \nFiles in both a and b.","title":"python.library.filecmp#filecmp.dircmp.common_files"}]}
{"task_id":20180210,"prompt":"def f_20180210(A, B):\n\treturn ","suffix":"","canonical_solution":"np.vstack((A, B))","test_start":"\nimport numpy as np \n\ndef check(candidate):","test":["\n    A = np.array([1,2])\n    B = np.array([3,4])\n    assert np.allclose(candidate(A, B), np.array([[1,2],[3,4]]))\n","\n    A = np.array([[1,2]])\n    B = np.array([[3,4]])\n    assert np.allclose(candidate(A, B), np.array([[1,2],[3,4]]))\n","\n    A = np.array([[1],[2]])\n    B = np.array([[3],[4]])\n    assert np.allclose(candidate(A, B), np.array([[1],[2],[3],[4]]))\n","\n    a = np.array([[1, 3, 4], [4, 5, 6], [6, 0, -1]])\n    b = np.array([[5, 6, 1], [0, 2, -1], [9, 4, 1]])\n    expected = np.array([[ 1, 3, 4], [ 4, 5, 6],\n        [ 6, 0, -1], [ 5, 6, 1], [ 0, 2, -1], [ 9, 4, 1]])\n    assert np.array_equal(candidate(a, b), expected)\n"],"entry_point":"f_20180210","intent":"concat two matrices `A` and `B` in numpy","library":["numpy"],"docs":[{"text":"concat_matrix=b'cm'[source]","title":"matplotlib.backend_pdf_api#matplotlib.backends.backend_pdf.Op.concat_matrix"},{"text":"operator.concat(a, b)  \noperator.__concat__(a, b)  \nReturn a + b for a and b sequences.","title":"python.library.operator#operator.concat"},{"text":"numpy.complex_[source]\n \nalias of numpy.cdouble","title":"numpy.reference.arrays.scalars#numpy.complex_"},{"text":"common_files  \nFiles in both a and b.","title":"python.library.filecmp#filecmp.dircmp.common_files"},{"text":"operator.concat(a, b)  \noperator.__concat__(a, b)  \nReturn a + b for a and b sequences.","title":"python.library.operator#operator.__concat__"},{"text":"common_dirs  \nSubdirectories in both a and b.","title":"python.library.filecmp#filecmp.dircmp.common_dirs"},{"text":"numpy.longcomplex[source]\n \nalias of numpy.clongdouble","title":"numpy.reference.arrays.scalars#numpy.longcomplex"},{"text":"numpy.clongfloat[source]\n \nalias of numpy.clongdouble","title":"numpy.reference.arrays.scalars#numpy.clongfloat"},{"text":"numpy.singlecomplex[source]\n \nalias of numpy.csingle","title":"numpy.reference.arrays.scalars#numpy.singlecomplex"},{"text":"numpy.float32[source]\n \nalias of numpy.single","title":"numpy.reference.arrays.scalars#numpy.float32"}]}
{"task_id":2011048,"prompt":"def f_2011048(filepath):\n\treturn ","suffix":"","canonical_solution":"os.stat(filepath).st_size","test_start":"\nimport os\n\ndef check(candidate):","test":["\n    with open(\"tmp.txt\", 'w') as fw: fw.write(\"hello world!\")\n    assert candidate(\"tmp.txt\") == 12\n","\n    with open(\"tmp.txt\", 'w') as fw: fw.write(\"\")\n    assert candidate(\"tmp.txt\") == 0\n","\n    with open(\"tmp.txt\", 'w') as fw: fw.write('\\n')\n    assert candidate(\"tmp.txt\") == 1\n","\n    filename = 'o.txt'\n    with open (filename, 'w') as f:\n        f.write('a')\n    assert candidate(filename) == 1\n"],"entry_point":"f_2011048","intent":"Get the characters count in a file `filepath`","library":["os"],"docs":[{"text":"stat.ST_SIZE  \nSize in bytes of a plain file; amount of data waiting on some special files.","title":"python.library.stat#stat.ST_SIZE"},{"text":"test.support.TESTFN_NONASCII  \nSet to a filename containing the FS_NONASCII character.","title":"python.library.test#test.support.TESTFN_NONASCII"},{"text":"lines  \nHeight of the terminal window in characters.","title":"python.library.os#os.terminal_size.lines"},{"text":"test.support.TESTFN_ENCODING  \nSet to sys.getfilesystemencoding().","title":"python.library.test#test.support.TESTFN_ENCODING"},{"text":"stat.S_ISREG(mode)  \nReturn non-zero if the mode is from a regular file.","title":"python.library.stat#stat.S_ISREG"},{"text":"stat.S_IFREG  \nRegular file.","title":"python.library.stat#stat.S_IFREG"},{"text":"st_rsize  \nReal size of the file.","title":"python.library.os#os.stat_result.st_rsize"},{"text":"stat.ST_INO  \nInode number.","title":"python.library.stat#stat.ST_INO"},{"text":"st_fstype  \nString that uniquely identifies the type of the filesystem that contains the file.","title":"python.library.os#os.stat_result.st_fstype"},{"text":"st_type  \nFile type.","title":"python.library.os#os.stat_result.st_type"}]}
{"task_id":2600191,"prompt":"def f_2600191(l):\n\treturn ","suffix":"","canonical_solution":"l.count('a')","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"123456asf\") == 1\n","\n    assert candidate(\"123456gyjnccfgsf\") == 0\n","\n    assert candidate(\"aA\"*10) == 10\n"],"entry_point":"f_2600191","intent":"count the occurrences of item \"a\" in list `l`","library":[],"docs":[]}
{"task_id":2600191,"prompt":"def f_2600191(l):\n\treturn ","suffix":"","canonical_solution":"Counter(l)","test_start":"\nfrom collections import Counter \n\ndef check(candidate):","test":["\n    assert dict(candidate(\"123456asf\")) == {'1': 1, '2': 1, '3': 1, '4': 1, '5': 1, '6': 1, 'a': 1, 's': 1, 'f': 1}\n","\n    assert candidate(\"123456gyjnccfgsf\") == {'1': 1,'2': 1,'3': 1,'4': 1,'5': 1,'6': 1,'g': 2,'y': 1,'j': 1,'n': 1,'c': 2,'f': 2,'s': 1}\n","\n    assert candidate(\"aA\"*10) == {'a': 10, 'A': 10}\n","\n    y = candidate([1, 6])\n    assert y[1] == 1\n    assert y[6] == 1\n"],"entry_point":"f_2600191","intent":"count the occurrences of items in list `l`","library":["collections"],"docs":[{"text":"array.count(x)  \nReturn the number of occurrences of x in the array.","title":"python.library.array#array.array.count"},{"text":"operator.countOf(a, b)  \nReturn the number of occurrences of b in a.","title":"python.library.operator#operator.countOf"},{"text":"count(value)  \nReturns the number of occurrences of value.","title":"python.library.multiprocessing.shared_memory#multiprocessing.shared_memory.ShareableList.count"},{"text":"array.tolist()  \nConvert the array to an ordinary list with the same items.","title":"python.library.array#array.array.tolist"},{"text":"array.remove(x)  \nRemove the first occurrence of x from the array.","title":"python.library.array#array.array.remove"},{"text":"NodeList.length  \nThe number of nodes in the sequence.","title":"python.library.xml.dom#xml.dom.NodeList.length"},{"text":"str.count(sub[, start[, end]])  \nReturn the number of non-overlapping occurrences of substring sub in the range [start, end]. Optional arguments start and end are interpreted as in slice notation.","title":"python.library.stdtypes#str.count"},{"text":"NodeList.item(i)  \nReturn the i\u2019th item from the sequence, if there is one, or None. The index i is not allowed to be less than zero or greater than or equal to the length of the sequence.","title":"python.library.xml.dom#xml.dom.NodeList.item"},{"text":"elements()  \nReturn an iterator over elements repeating each as many times as its count. Elements are returned in the order first encountered. If an element\u2019s count is less than one, elements() will ignore it. >>> c = Counter(a=4, b=2, c=0, d=-2)\n>>> sorted(c.elements())\n['a', 'a', 'a', 'a', 'b', 'b']","title":"python.library.collections#collections.Counter.elements"},{"text":"dis.haslocal  \nSequence of bytecodes that access a local variable.","title":"python.library.dis#dis.haslocal"}]}
{"task_id":2600191,"prompt":"def f_2600191(l):\n\treturn ","suffix":"","canonical_solution":"[[x, l.count(x)] for x in set(l)]","test_start":"\nfrom collections import Counter \n\ndef check(candidate):","test":["\n    assert sorted(candidate(\"123456asf\")) == [['1', 1],['2', 1],['3', 1],['4', 1],['5', 1],['6', 1],['a', 1],['f', 1],['s', 1]]\n","\n    assert sorted(candidate(\"aA\"*10)) == [['A', 10], ['a', 10]]\n"],"entry_point":"f_2600191","intent":"count the occurrences of items in list `l`","library":["collections"],"docs":[{"text":"array.count(x)  \nReturn the number of occurrences of x in the array.","title":"python.library.array#array.array.count"},{"text":"operator.countOf(a, b)  \nReturn the number of occurrences of b in a.","title":"python.library.operator#operator.countOf"},{"text":"count(value)  \nReturns the number of occurrences of value.","title":"python.library.multiprocessing.shared_memory#multiprocessing.shared_memory.ShareableList.count"},{"text":"array.tolist()  \nConvert the array to an ordinary list with the same items.","title":"python.library.array#array.array.tolist"},{"text":"array.remove(x)  \nRemove the first occurrence of x from the array.","title":"python.library.array#array.array.remove"},{"text":"NodeList.length  \nThe number of nodes in the sequence.","title":"python.library.xml.dom#xml.dom.NodeList.length"},{"text":"str.count(sub[, start[, end]])  \nReturn the number of non-overlapping occurrences of substring sub in the range [start, end]. Optional arguments start and end are interpreted as in slice notation.","title":"python.library.stdtypes#str.count"},{"text":"NodeList.item(i)  \nReturn the i\u2019th item from the sequence, if there is one, or None. The index i is not allowed to be less than zero or greater than or equal to the length of the sequence.","title":"python.library.xml.dom#xml.dom.NodeList.item"},{"text":"elements()  \nReturn an iterator over elements repeating each as many times as its count. Elements are returned in the order first encountered. If an element\u2019s count is less than one, elements() will ignore it. >>> c = Counter(a=4, b=2, c=0, d=-2)\n>>> sorted(c.elements())\n['a', 'a', 'a', 'a', 'b', 'b']","title":"python.library.collections#collections.Counter.elements"},{"text":"dis.haslocal  \nSequence of bytecodes that access a local variable.","title":"python.library.dis#dis.haslocal"}]}
{"task_id":2600191,"prompt":"def f_2600191(l):\n\treturn ","suffix":"","canonical_solution":"dict(((x, l.count(x)) for x in set(l)))","test_start":"\nfrom collections import Counter \n\ndef check(candidate):","test":["\n    assert candidate(\"123456asf\") == {'4': 1, 'a': 1, '1': 1, 's': 1, '6': 1, 'f': 1, '3': 1, '5': 1, '2': 1}\n","\n    assert candidate(\"aA\"*10) == {'A': 10, 'a': 10}\n","\n    assert candidate([1, 6]) == {1: 1, 6: 1}\n"],"entry_point":"f_2600191","intent":"count the occurrences of items in list `l`","library":["collections"],"docs":[]}
{"task_id":2600191,"prompt":"def f_2600191(l):\n\treturn ","suffix":"","canonical_solution":"l.count('b')","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"123456abbbsf\") == 3\n","\n    assert candidate(\"123456gyjnccfgsf\") == 0\n","\n    assert candidate(\"Ab\"*10) == 10\n"],"entry_point":"f_2600191","intent":"count the occurrences of item \"b\" in list `l`","library":[],"docs":[]}
{"task_id":12842997,"prompt":"def f_12842997(srcfile, dstdir):\n\t","suffix":"\n\treturn ","canonical_solution":"shutil.copy(srcfile, dstdir)","test_start":"\nimport shutil\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    shutil.copy = Mock()\n    try:\n        candidate('opera.txt', '\/')\n    except:\n        return False \n"],"entry_point":"f_12842997","intent":"copy file `srcfile` to directory `dstdir`","library":["shutil"],"docs":[{"text":"right  \nThe directory b.","title":"python.library.filecmp#filecmp.dircmp.right"},{"text":"shutil.move(src, dst, copy_function=copy2)  \nRecursively move a file or directory (src) to another location (dst) and return the destination. If the destination is an existing directory, then src is moved inside that directory. If the destination already exists but is not a directory, it may be overwritten depending on os.rename() semantics. If the destination is on the current filesystem, then os.rename() is used. Otherwise, src is copied to dst using copy_function and then removed. In case of symlinks, a new symlink pointing to the target of src will be created in or as dst and src will be removed. If copy_function is given, it must be a callable that takes two arguments src and dst, and will be used to copy src to dst if os.rename() cannot be used. If the source is a directory, copytree() is called, passing it the copy_function(). The default copy_function is copy2(). Using copy() as the copy_function allows the move to succeed when it is not possible to also copy the metadata, at the expense of not copying any of the metadata. Raises an auditing event shutil.move with arguments src, dst.  Changed in version 3.3: Added explicit symlink handling for foreign filesystems, thus adapting it to the behavior of GNU\u2019s mv. Now returns dst.   Changed in version 3.5: Added the copy_function keyword argument.   Changed in version 3.8: Platform-specific fast-copy syscalls may be used internally in order to copy the file more efficiently. See Platform-dependent efficient copy operations section.   Changed in version 3.9: Accepts a path-like object for both src and dst.","title":"python.library.shutil#shutil.move"},{"text":"shutil.copy(src, dst, *, follow_symlinks=True)  \nCopies the file src to the file or directory dst. src and dst should be path-like objects or strings. If dst specifies a directory, the file will be copied into dst using the base filename from src. Returns the path to the newly created file. If follow_symlinks is false, and src is a symbolic link, dst will be created as a symbolic link. If follow_symlinks is true and src is a symbolic link, dst will be a copy of the file src refers to. copy() copies the file data and the file\u2019s permission mode (see os.chmod()). Other metadata, like the file\u2019s creation and modification times, is not preserved. To preserve all file metadata from the original, use copy2() instead. Raises an auditing event shutil.copyfile with arguments src, dst. Raises an auditing event shutil.copymode with arguments src, dst.  Changed in version 3.3: Added follow_symlinks argument. Now returns path to the newly created file.   Changed in version 3.8: Platform-specific fast-copy syscalls may be used internally in order to copy the file more efficiently. See Platform-dependent efficient copy operations section.","title":"python.library.shutil#shutil.copy"},{"text":"copy_()","title":"torch.storage#torch.FloatStorage.copy_"},{"text":"shutil.copyfile(src, dst, *, follow_symlinks=True)  \nCopy the contents (no metadata) of the file named src to a file named dst and return dst in the most efficient way possible. src and dst are path-like objects or path names given as strings. dst must be the complete target file name; look at copy() for a copy that accepts a target directory path. If src and dst specify the same file, SameFileError is raised. The destination location must be writable; otherwise, an OSError exception will be raised. If dst already exists, it will be replaced. Special files such as character or block devices and pipes cannot be copied with this function. If follow_symlinks is false and src is a symbolic link, a new symbolic link will be created instead of copying the file src points to. Raises an auditing event shutil.copyfile with arguments src, dst.  Changed in version 3.3: IOError used to be raised instead of OSError. Added follow_symlinks argument. Now returns dst.   Changed in version 3.4: Raise SameFileError instead of Error. Since the former is a subclass of the latter, this change is backward compatible.   Changed in version 3.8: Platform-specific fast-copy syscalls may be used internally in order to copy the file more efficiently. See Platform-dependent efficient copy operations section.","title":"python.library.shutil#shutil.copyfile"},{"text":"left  \nThe directory a.","title":"python.library.filecmp#filecmp.dircmp.left"},{"text":"tf.io.gfile.copy     View source on GitHub    Copies data from src to dst.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.io.gfile.copy  \ntf.io.gfile.copy(\n    src, dst, overwrite=False\n)\n\n \n\n\n Args\n  src   string, name of the file whose contents need to be copied  \n  dst   string, name of the file to which to copy to  \n  overwrite   boolean, if false it's an error for dst to be occupied by an existing file.   \n \n\n\n Raises\n  errors.OpError   If the operation fails.","title":"tensorflow.io.gfile.copy"},{"text":"os.replace(src, dst, *, src_dir_fd=None, dst_dir_fd=None)  \nRename the file or directory src to dst. If dst is a directory, OSError will be raised. If dst exists and is a file, it will be replaced silently if the user has permission. The operation may fail if src and dst are on different filesystems. If successful, the renaming will be an atomic operation (this is a POSIX requirement). This function can support specifying src_dir_fd and\/or dst_dir_fd to supply paths relative to directory descriptors. Raises an auditing event os.rename with arguments src, dst, src_dir_fd, dst_dir_fd.  New in version 3.3.   Changed in version 3.6: Accepts a path-like object for src and dst.","title":"python.library.os#os.replace"},{"text":"shutil.copytree(src, dst, symlinks=False, ignore=None, copy_function=copy2, ignore_dangling_symlinks=False, dirs_exist_ok=False)  \nRecursively copy an entire directory tree rooted at src to a directory named dst and return the destination directory. dirs_exist_ok dictates whether to raise an exception in case dst or any missing parent directory already exists. Permissions and times of directories are copied with copystat(), individual files are copied using copy2(). If symlinks is true, symbolic links in the source tree are represented as symbolic links in the new tree and the metadata of the original links will be copied as far as the platform allows; if false or omitted, the contents and metadata of the linked files are copied to the new tree. When symlinks is false, if the file pointed by the symlink doesn\u2019t exist, an exception will be added in the list of errors raised in an Error exception at the end of the copy process. You can set the optional ignore_dangling_symlinks flag to true if you want to silence this exception. Notice that this option has no effect on platforms that don\u2019t support os.symlink(). If ignore is given, it must be a callable that will receive as its arguments the directory being visited by copytree(), and a list of its contents, as returned by os.listdir(). Since copytree() is called recursively, the ignore callable will be called once for each directory that is copied. The callable must return a sequence of directory and file names relative to the current directory (i.e. a subset of the items in its second argument); these names will then be ignored in the copy process. ignore_patterns() can be used to create such a callable that ignores names based on glob-style patterns. If exception(s) occur, an Error is raised with a list of reasons. If copy_function is given, it must be a callable that will be used to copy each file. It will be called with the source path and the destination path as arguments. By default, copy2() is used, but any function that supports the same signature (like copy()) can be used. Raises an auditing event shutil.copytree with arguments src, dst.  Changed in version 3.3: Copy metadata when symlinks is false. Now returns dst.   Changed in version 3.2: Added the copy_function argument to be able to provide a custom copy function. Added the ignore_dangling_symlinks argument to silent dangling symlinks errors when symlinks is false.   Changed in version 3.8: Platform-specific fast-copy syscalls may be used internally in order to copy the file more efficiently. See Platform-dependent efficient copy operations section.   New in version 3.8: The dirs_exist_ok parameter.","title":"python.library.shutil#shutil.copytree"},{"text":"os.copy_file_range(src, dst, count, offset_src=None, offset_dst=None)  \nCopy count bytes from file descriptor src, starting from offset offset_src, to file descriptor dst, starting from offset offset_dst. If offset_src is None, then src is read from the current position; respectively for offset_dst. The files pointed by src and dst must reside in the same filesystem, otherwise an OSError is raised with errno set to errno.EXDEV. This copy is done without the additional cost of transferring data from the kernel to user space and then back into the kernel. Additionally, some filesystems could implement extra optimizations. The copy is done as if both files are opened as binary. The return value is the amount of bytes copied. This could be less than the amount requested. Availability: Linux kernel >= 4.5 or glibc >= 2.27.  New in version 3.8.","title":"python.library.os#os.copy_file_range"}]}
{"task_id":1555968,"prompt":"def f_1555968(x):\n\treturn ","suffix":"","canonical_solution":"max(k for k, v in x.items() if v != 0)","test_start":"\ndef check(candidate):","test":["\n    assert candidate({'a': 1, 'b': 2, 'c': 2000}) == 'c'\n","\n    assert candidate({'a': 0., 'b': 0, 'c': 200.02}) == 'c'\n","\n    assert candidate({'key1': -100, 'key2': 0.}) == 'key1'\n","\n    x = {1:\"g\", 2:\"a\", 5:\"er\", -4:\"dr\"}\n    assert candidate(x) == 5\n"],"entry_point":"f_1555968","intent":"find the key associated with the largest value in dictionary `x` whilst key is non-zero value","library":[],"docs":[]}
{"task_id":17021863,"prompt":"def f_17021863(file):\n\t","suffix":"\n\treturn ","canonical_solution":"file.seek(0)","test_start":"\ndef check(candidate):","test":["\n    with open ('a.txt', 'w') as f:\n        f.write('kangaroo\\nkoala\\noxford\\n')\n    f = open('a.txt', 'r')\n    f.read()\n    candidate(f)\n    assert f.readline() == 'kangaroo\\n'\n"],"entry_point":"f_17021863","intent":"Put the curser at beginning of the file","library":[],"docs":[]}
{"task_id":38152389,"prompt":"def f_38152389(df):\n\t","suffix":"\n\treturn df","canonical_solution":"df['c'] = np.where(df['a'].isnull, df['b'], df['a'])","test_start":"\nimport numpy as np \nimport pandas as pd \n\ndef check(candidate):","test":["\n    df = pd.DataFrame({'a': [1,2,3], 'b': [0,0,0]})\n    assert np.allclose(candidate(df), pd.DataFrame({'a': [1,2,3], 'b': [0,0,0], 'c': [0,0,0]}))\n","\n    df = pd.DataFrame({'a': [0,2,3], 'b': [4,5,6]})\n    assert np.allclose(candidate(df), pd.DataFrame({'a': [0,2,3], 'b': [4,5,6], 'c': [4,5,6]}))\n"],"entry_point":"f_38152389","intent":"combine values from column 'b' and column 'a' of dataframe `df`  into column 'c' of datafram `df`","library":["numpy","pandas"],"docs":[{"text":"pandas.DataFrame.combine_first   DataFrame.combine_first(other)[source]\n \nUpdate null elements with value in the same location in other. Combine two DataFrame objects by filling null values in one DataFrame with non-null values from other DataFrame. The row and column indexes of the resulting DataFrame will be the union of the two.  Parameters \n \nother:DataFrame\n\n\nProvided DataFrame to use to fill null values.    Returns \n DataFrame\n\nThe result of combining the provided DataFrame with the other object.      See also  DataFrame.combine\n\nPerform series-wise operation on two DataFrames using a given function.    Examples \n>>> df1 = pd.DataFrame({'A': [None, 0], 'B': [None, 4]})\n>>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n>>> df1.combine_first(df2)\n     A    B\n0  1.0  3.0\n1  0.0  4.0\n  Null values still persist if the location of that null value does not exist in other \n>>> df1 = pd.DataFrame({'A': [None, 0], 'B': [4, None]})\n>>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1]}, index=[1, 2])\n>>> df1.combine_first(df2)\n     A    B    C\n0  NaN  4.0  NaN\n1  0.0  3.0  1.0\n2  NaN  3.0  1.0","title":"pandas.reference.api.pandas.dataframe.combine_first"},{"text":"pandas.DataFrame.combine   DataFrame.combine(other, func, fill_value=None, overwrite=True)[source]\n \nPerform column-wise combine with another DataFrame. Combines a DataFrame with other DataFrame using func to element-wise combine columns. The row and column indexes of the resulting DataFrame will be the union of the two.  Parameters \n \nother:DataFrame\n\n\nThe DataFrame to merge column-wise.  \nfunc:function\n\n\nFunction that takes two series as inputs and return a Series or a scalar. Used to merge the two dataframes column by columns.  \nfill_value:scalar value, default None\n\n\nThe value to fill NaNs with prior to passing any column to the merge func.  \noverwrite:bool, default True\n\n\nIf True, columns in self that do not exist in other will be overwritten with NaNs.    Returns \n DataFrame\n\nCombination of the provided DataFrames.      See also  DataFrame.combine_first\n\nCombine two DataFrame objects and default to non-null values in frame calling the method.    Examples Combine using a simple function that chooses the smaller column. \n>>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]})\n>>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n>>> take_smaller = lambda s1, s2: s1 if s1.sum() < s2.sum() else s2\n>>> df1.combine(df2, take_smaller)\n   A  B\n0  0  3\n1  0  3\n  Example using a true element-wise combine function. \n>>> df1 = pd.DataFrame({'A': [5, 0], 'B': [2, 4]})\n>>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n>>> df1.combine(df2, np.minimum)\n   A  B\n0  1  2\n1  0  3\n  Using fill_value fills Nones prior to passing the column to the merge function. \n>>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]})\n>>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n>>> df1.combine(df2, take_smaller, fill_value=-5)\n   A    B\n0  0 -5.0\n1  0  4.0\n  However, if the same element in both dataframes is None, that None is preserved \n>>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]})\n>>> df2 = pd.DataFrame({'A': [1, 1], 'B': [None, 3]})\n>>> df1.combine(df2, take_smaller, fill_value=-5)\n    A    B\n0  0 -5.0\n1  0  3.0\n  Example that demonstrates the use of overwrite and behavior when the axis differ between the dataframes. \n>>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]})\n>>> df2 = pd.DataFrame({'B': [3, 3], 'C': [-10, 1], }, index=[1, 2])\n>>> df1.combine(df2, take_smaller)\n     A    B     C\n0  NaN  NaN   NaN\n1  NaN  3.0 -10.0\n2  NaN  3.0   1.0\n  \n>>> df1.combine(df2, take_smaller, overwrite=False)\n     A    B     C\n0  0.0  NaN   NaN\n1  0.0  3.0 -10.0\n2  NaN  3.0   1.0\n  Demonstrating the preference of the passed in dataframe. \n>>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1], }, index=[1, 2])\n>>> df2.combine(df1, take_smaller)\n   A    B   C\n0  0.0  NaN NaN\n1  0.0  3.0 NaN\n2  NaN  3.0 NaN\n  \n>>> df2.combine(df1, take_smaller, overwrite=False)\n     A    B   C\n0  0.0  NaN NaN\n1  0.0  3.0 1.0\n2  NaN  3.0 1.0","title":"pandas.reference.api.pandas.dataframe.combine"},{"text":"pandas.Timestamp.fold   Timestamp.fold","title":"pandas.reference.api.pandas.timestamp.fold"},{"text":"colno  \nThe column corresponding to pos (may be None).","title":"python.library.re#re.error.colno"},{"text":"pandas.Timedelta.freq   Timedelta.freq","title":"pandas.reference.api.pandas.timedelta.freq"},{"text":"colno  \nThe column corresponding to pos.","title":"python.library.json#json.JSONDecodeError.colno"},{"text":"pandas.Timestamp.freq   Timestamp.freq","title":"pandas.reference.api.pandas.timestamp.freq"},{"text":"pandas.Series.dt.freq   Series.dt.freq","title":"pandas.reference.api.pandas.series.dt.freq"},{"text":"pandas.IntervalIndex.mid   IntervalIndex.mid","title":"pandas.reference.api.pandas.intervalindex.mid"},{"text":"pandas.tseries.offsets.BQuarterBegin.normalize   BQuarterBegin.normalize","title":"pandas.reference.api.pandas.tseries.offsets.bquarterbegin.normalize"}]}
{"task_id":4175686,"prompt":"def f_4175686(d):\n\t","suffix":"\n\treturn d","canonical_solution":"del d['ele']","test_start":"\ndef check(candidate):","test":["\n    assert candidate({\"ale\":1, \"ele\": 2}) == {\"ale\": 1}\n"],"entry_point":"f_4175686","intent":"remove key 'ele' from dictionary `d`","library":[],"docs":[]}
{"task_id":11574195,"prompt":"def f_11574195():\n\treturn ","suffix":"","canonical_solution":"['it'] + ['was'] + ['annoying']","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == ['it', 'was', 'annoying']\n"],"entry_point":"f_11574195","intent":"merge list `['it']` and list `['was']` and list `['annoying']` into one list","library":[],"docs":[]}
{"task_id":587647,"prompt":"def f_587647(x):\n\treturn ","suffix":"","canonical_solution":"str(int(x) + 1).zfill(len(x))","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"001\") == \"002\"\n","\n    assert candidate(\"100\") == \"101\"\n"],"entry_point":"f_587647","intent":"increment a value with leading zeroes in a number `x`","library":[],"docs":[]}
{"task_id":17315881,"prompt":"def f_17315881(df):\n\treturn ","suffix":"","canonical_solution":"all(df.index[:-1] <= df.index[1:])","test_start":"\nimport pandas as pd \n\ndef check(candidate):","test":["\n    df1 = pd.DataFrame({'a': [1,2], 'bb': [0,2]})\n    assert candidate(df1) == True\n","\n    df2 = pd.DataFrame({'a': [1,2,3,4,5], 'bb': [0,3,5,7,9]})\n    shuffled = df2.sample(frac=3, replace=True)\n    assert candidate(shuffled) == False\n","\n    df = pd.DataFrame([[1, 2], [5, 4]], columns=['a', 'b'])\n    assert candidate(df)\n"],"entry_point":"f_17315881","intent":"check if a pandas dataframe `df`'s index is sorted","library":["pandas"],"docs":[{"text":"pandas.Index.sort   finalIndex.sort(*args, **kwargs)[source]\n \nUse sort_values instead.","title":"pandas.reference.api.pandas.index.sort"},{"text":"pandas.Timedelta.is_populated   Timedelta.is_populated","title":"pandas.reference.api.pandas.timedelta.is_populated"},{"text":"pandas.Index.sort_values   Index.sort_values(return_indexer=False, ascending=True, na_position='last', key=None)[source]\n \nReturn a sorted copy of the index. Return a sorted copy of the index, and optionally return the indices that sorted the index itself.  Parameters \n \nreturn_indexer:bool, default False\n\n\nShould the indices that would sort the index be returned.  \nascending:bool, default True\n\n\nShould the index values be sorted in an ascending order.  \nna_position:{\u2018first\u2019 or \u2018last\u2019}, default \u2018last\u2019\n\n\nArgument \u2018first\u2019 puts NaNs at the beginning, \u2018last\u2019 puts NaNs at the end.  New in version 1.2.0.   \nkey:callable, optional\n\n\nIf not None, apply the key function to the index values before sorting. This is similar to the key argument in the builtin sorted() function, with the notable difference that this key function should be vectorized. It should expect an Index and return an Index of the same shape.  New in version 1.1.0.     Returns \n \nsorted_index:pandas.Index\n\n\nSorted copy of the index.  \nindexer:numpy.ndarray, optional\n\n\nThe indices that the index itself was sorted by.      See also  Series.sort_values\n\nSort values of a Series.  DataFrame.sort_values\n\nSort values in a DataFrame.    Examples \n>>> idx = pd.Index([10, 100, 1, 1000])\n>>> idx\nInt64Index([10, 100, 1, 1000], dtype='int64')\n  Sort values in ascending order (default behavior). \n>>> idx.sort_values()\nInt64Index([1, 10, 100, 1000], dtype='int64')\n  Sort values in descending order, and also get the indices idx was sorted by. \n>>> idx.sort_values(ascending=False, return_indexer=True)\n(Int64Index([1000, 100, 10, 1], dtype='int64'), array([3, 1, 0, 2]))","title":"pandas.reference.api.pandas.index.sort_values"},{"text":"pandas.tseries.offsets.BYearEnd.is_anchored   BYearEnd.is_anchored()","title":"pandas.reference.api.pandas.tseries.offsets.byearend.is_anchored"},{"text":"pandas.IntervalIndex.left   IntervalIndex.left","title":"pandas.reference.api.pandas.intervalindex.left"},{"text":"pandas.DataFrame.index   DataFrame.index\n \nThe index (row labels) of the DataFrame.","title":"pandas.reference.api.pandas.dataframe.index"},{"text":"pandas.tseries.offsets.Tick.is_anchored   Tick.is_anchored()","title":"pandas.reference.api.pandas.tseries.offsets.tick.is_anchored"},{"text":"pandas.Index.argsort   Index.argsort(*args, **kwargs)[source]\n \nReturn the integer indices that would sort the index.  Parameters \n *args\n\nPassed to numpy.ndarray.argsort.  **kwargs\n\nPassed to numpy.ndarray.argsort.    Returns \n np.ndarray[np.intp]\n\nInteger indices that would sort the index if used as an indexer.      See also  numpy.argsort\n\nSimilar method for NumPy arrays.  Index.sort_values\n\nReturn sorted copy of Index.    Examples \n>>> idx = pd.Index(['b', 'a', 'd', 'c'])\n>>> idx\nIndex(['b', 'a', 'd', 'c'], dtype='object')\n  \n>>> order = idx.argsort()\n>>> order\narray([1, 0, 3, 2])\n  \n>>> idx[order]\nIndex(['a', 'b', 'c', 'd'], dtype='object')","title":"pandas.reference.api.pandas.index.argsort"},{"text":"pandas.tseries.offsets.Second.is_anchored   Second.is_anchored()","title":"pandas.reference.api.pandas.tseries.offsets.second.is_anchored"},{"text":"pandas.tseries.offsets.BQuarterEnd.is_anchored   BQuarterEnd.is_anchored()","title":"pandas.reference.api.pandas.tseries.offsets.bquarterend.is_anchored"}]}
{"task_id":16296643,"prompt":"def f_16296643(t):\n\treturn ","suffix":"","canonical_solution":"list(t)","test_start":"\ndef check(candidate):","test":["\n    assert candidate((0, 1, 2)) == [0,1,2]\n","\n    assert candidate(('a', [], 100)) == ['a', [], 100]\n"],"entry_point":"f_16296643","intent":"Convert tuple `t` to list","library":[],"docs":[]}
{"task_id":16296643,"prompt":"def f_16296643(t):\n\treturn ","suffix":"","canonical_solution":"tuple(t)","test_start":"\ndef check(candidate):","test":["\n    assert candidate([0,1,2]) == (0, 1, 2)\n","\n    assert candidate(['a', [], 100]) == ('a', [], 100)\n"],"entry_point":"f_16296643","intent":"Convert list `t` to tuple","library":[],"docs":[]}
{"task_id":16296643,"prompt":"def f_16296643(level1):\n\t","suffix":"\n\treturn level1","canonical_solution":"level1 = map(list, level1)","test_start":"\ndef check(candidate):","test":["\n    t = ((1, 2), (3, 4))\n    t = candidate(t)\n    assert list(t) == [[1, 2], [3, 4]]\n"],"entry_point":"f_16296643","intent":"Convert tuple `level1` to list","library":[],"docs":[]}
{"task_id":3880399,"prompt":"def f_3880399(dataobject, logFile):\n\treturn ","suffix":"","canonical_solution":"pprint.pprint(dataobject, logFile)","test_start":"\nimport pprint \n\ndef check(candidate):","test":["\n    f = open('kkk.txt', 'w')\n    candidate('hello', f)\n    f.close()\n    with open('kkk.txt', 'r') as f:\n        assert 'hello' in f.readline()\n"],"entry_point":"f_3880399","intent":"send the output of pprint object `dataobject` to file `logFile`","library":["pprint"],"docs":[{"text":"emit(record)  \nOutputs the record to the file.","title":"python.library.logging.handlers#logging.FileHandler.emit"},{"text":"array.tofile(f)  \nWrite all items (as machine values) to the file object f.","title":"python.library.array#array.array.tofile"},{"text":"PrettyPrinter.pprint(object)  \nPrint the formatted representation of object on the configured stream, followed by a newline.","title":"python.library.pprint#pprint.PrettyPrinter.pprint"},{"text":"writeObject(object, contents)[source]","title":"matplotlib.backend_pdf_api#matplotlib.backends.backend_pdf.PdfFile.writeObject"},{"text":"write(object)  \nWrites the object\u2019s contents encoded to the stream.","title":"python.library.codecs#codecs.StreamWriter.write"},{"text":"file","title":"matplotlib.backend_pdf_api#matplotlib.backends.backend_pdf.Stream.file"},{"text":"output(*data)[source]","title":"matplotlib.backend_pdf_api#matplotlib.backends.backend_pdf.PdfFile.output"},{"text":"bpprint(out=None)  \nPrint the output of bpformat() to the file out, or if it is None, to standard output.","title":"python.library.bdb#bdb.Breakpoint.bpprint"},{"text":"pprint.pprint(object, stream=None, indent=1, width=80, depth=None, *, compact=False, sort_dicts=True)  \nPrints the formatted representation of object on stream, followed by a newline. If stream is None, sys.stdout is used. This may be used in the interactive interpreter instead of the print() function for inspecting values (you can even reassign print = pprint.pprint for use within a scope). indent, width, depth, compact and sort_dicts will be passed to the PrettyPrinter constructor as formatting parameters.  Changed in version 3.4: Added the compact parameter.   Changed in version 3.8: Added the sort_dicts parameter. >>> import pprint\n>>> stuff = ['spam', 'eggs', 'lumberjack', 'knights', 'ni']\n>>> stuff.insert(0, stuff)\n>>> pprint.pprint(stuff)\n[<Recursion on list with id=...>,\n 'spam',\n 'eggs',\n 'lumberjack',\n 'knights',\n 'ni']","title":"python.library.pprint#pprint.pprint"},{"text":"numpy.record.pprint method   record.pprint()[source]\n \nPretty-print all fields.","title":"numpy.reference.generated.numpy.record.pprint"}]}
{"task_id":21800169,"prompt":"def f_21800169(df):\n\treturn ","suffix":"","canonical_solution":"df.loc[df['BoolCol']]","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame([[True, 2, 3], [False, 5, 6]], columns = ['BoolCol', 'a', 'b'])\n    y = candidate(df)\n    assert y['a'][0] == 2\n    assert y['b'][0] == 3\n"],"entry_point":"f_21800169","intent":"get index of rows in column 'BoolCol'","library":["pandas"],"docs":[{"text":"pandas.Index.names   propertyIndex.names","title":"pandas.reference.api.pandas.index.names"},{"text":"pandas.DataFrame.index   DataFrame.index\n \nThe index (row labels) of the DataFrame.","title":"pandas.reference.api.pandas.dataframe.index"},{"text":"numpy.nditer.index attribute   nditer.index","title":"numpy.reference.generated.numpy.nditer.index"},{"text":"pandas.Index.nlevels   propertyIndex.nlevels\n \nNumber of levels.","title":"pandas.reference.api.pandas.index.nlevels"},{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"pandas.Index.view   Index.view(cls=None)[source]","title":"pandas.reference.api.pandas.index.view"},{"text":"pandas.Index.T   propertyIndex.T\n \nReturn the transpose, which is by definition self.","title":"pandas.reference.api.pandas.index.t"},{"text":"pandas.Index.empty   propertyIndex.empty","title":"pandas.reference.api.pandas.index.empty"},{"text":"operator.indexOf(a, b)  \nReturn the index of the first of occurrence of b in a.","title":"python.library.operator#operator.indexOf"},{"text":"pandas.IntervalIndex.left   IntervalIndex.left","title":"pandas.reference.api.pandas.intervalindex.left"}]}
{"task_id":21800169,"prompt":"def f_21800169(df):\n\treturn ","suffix":"","canonical_solution":"df.iloc[np.flatnonzero(df['BoolCol'])]","test_start":"\nimport numpy as np\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame([[True, 2, 3], [False, 5, 6]], columns = ['BoolCol', 'a', 'b'])\n    y = candidate(df)\n    assert y['a'][0] == 2\n    assert y['b'][0] == 3\n"],"entry_point":"f_21800169","intent":"Create a list containing the indexes of rows where the value of column 'BoolCol' in dataframe `df` are equal to True","library":["numpy","pandas"],"docs":[{"text":"pandas.DataFrame.index   DataFrame.index\n \nThe index (row labels) of the DataFrame.","title":"pandas.reference.api.pandas.dataframe.index"},{"text":"pandas.Index.names   propertyIndex.names","title":"pandas.reference.api.pandas.index.names"},{"text":"pandas.IntervalIndex.left   IntervalIndex.left","title":"pandas.reference.api.pandas.intervalindex.left"},{"text":"pandas.Index.is_boolean   finalIndex.is_boolean()[source]\n \nCheck if the Index only consists of booleans.  Returns \n bool\n\nWhether or not the Index only consists of booleans.      See also  is_integer\n\nCheck if the Index only consists of integers.  is_floating\n\nCheck if the Index is a floating type.  is_numeric\n\nCheck if the Index only consists of numeric data.  is_object\n\nCheck if the Index is of the object dtype.  is_categorical\n\nCheck if the Index holds categorical data.  is_interval\n\nCheck if the Index holds Interval objects.  is_mixed\n\nCheck if the Index holds data with mixed data types.    Examples \n>>> idx = pd.Index([True, False, True])\n>>> idx.is_boolean()\nTrue\n  \n>>> idx = pd.Index([\"True\", \"False\", \"True\"])\n>>> idx.is_boolean()\nFalse\n  \n>>> idx = pd.Index([True, False, \"True\"])\n>>> idx.is_boolean()\nFalse","title":"pandas.reference.api.pandas.index.is_boolean"},{"text":"pandas.DataFrame.isin   DataFrame.isin(values)[source]\n \nWhether each element in the DataFrame is contained in values.  Parameters \n \nvalues:iterable, Series, DataFrame or dict\n\n\nThe result will only be true at a location if all the labels match. If values is a Series, that\u2019s the index. If values is a dict, the keys must be the column names, which must match. If values is a DataFrame, then both the index and column labels must match.    Returns \n DataFrame\n\nDataFrame of booleans showing whether each element in the DataFrame is contained in values.      See also  DataFrame.eq\n\nEquality test for DataFrame.  Series.isin\n\nEquivalent method on Series.  Series.str.contains\n\nTest if pattern or regex is contained within a string of a Series or Index.    Examples \n>>> df = pd.DataFrame({'num_legs': [2, 4], 'num_wings': [2, 0]},\n...                   index=['falcon', 'dog'])\n>>> df\n        num_legs  num_wings\nfalcon         2          2\ndog            4          0\n  When values is a list check whether every value in the DataFrame is present in the list (which animals have 0 or 2 legs or wings) \n>>> df.isin([0, 2])\n        num_legs  num_wings\nfalcon      True       True\ndog        False       True\n  To check if values is not in the DataFrame, use the ~ operator: \n>>> ~df.isin([0, 2])\n        num_legs  num_wings\nfalcon     False      False\ndog         True      False\n  When values is a dict, we can pass values to check for each column separately: \n>>> df.isin({'num_wings': [0, 3]})\n        num_legs  num_wings\nfalcon     False      False\ndog        False       True\n  When values is a Series or DataFrame the index and column must match. Note that \u2018falcon\u2019 does not match based on the number of legs in other. \n>>> other = pd.DataFrame({'num_legs': [8, 3], 'num_wings': [0, 2]},\n...                      index=['spider', 'falcon'])\n>>> df.isin(other)\n        num_legs  num_wings\nfalcon     False       True\ndog        False      False","title":"pandas.reference.api.pandas.dataframe.isin"},{"text":"pandas.Index.has_duplicates   propertyIndex.has_duplicates\n \nCheck if the Index has duplicate values.  Returns \n bool\n\nWhether or not the Index has duplicate values.     Examples \n>>> idx = pd.Index([1, 5, 7, 7])\n>>> idx.has_duplicates\nTrue\n  \n>>> idx = pd.Index([1, 5, 7])\n>>> idx.has_duplicates\nFalse\n  \n>>> idx = pd.Index([\"Watermelon\", \"Orange\", \"Apple\",\n...                 \"Watermelon\"]).astype(\"category\")\n>>> idx.has_duplicates\nTrue\n  \n>>> idx = pd.Index([\"Orange\", \"Apple\",\n...                 \"Watermelon\"]).astype(\"category\")\n>>> idx.has_duplicates\nFalse","title":"pandas.reference.api.pandas.index.has_duplicates"},{"text":"pandas.CategoricalIndex.equals   CategoricalIndex.equals(other)[source]\n \nDetermine if two CategoricalIndex objects contain the same elements.  Returns \n bool\n\nIf two CategoricalIndex objects have equal elements True, otherwise False.","title":"pandas.reference.api.pandas.categoricalindex.equals"},{"text":"pandas.Index.isin   Index.isin(values, level=None)[source]\n \nReturn a boolean array where the index values are in values. Compute boolean array of whether each index value is found in the passed set of values. The length of the returned boolean array matches the length of the index.  Parameters \n \nvalues:set or list-like\n\n\nSought values.  \nlevel:str or int, optional\n\n\nName or position of the index level to use (if the index is a MultiIndex).    Returns \n np.ndarray[bool]\n\nNumPy array of boolean values.      See also  Series.isin\n\nSame for Series.  DataFrame.isin\n\nSame method for DataFrames.    Notes In the case of MultiIndex you must either specify values as a list-like object containing tuples that are the same length as the number of levels, or specify level. Otherwise it will raise a ValueError. If level is specified:  if it is the name of one and only one index level, use that level; otherwise it should be a number indicating level position.  Examples \n>>> idx = pd.Index([1,2,3])\n>>> idx\nInt64Index([1, 2, 3], dtype='int64')\n  Check whether each index value in a list of values. \n>>> idx.isin([1, 4])\narray([ True, False, False])\n  \n>>> midx = pd.MultiIndex.from_arrays([[1,2,3],\n...                                  ['red', 'blue', 'green']],\n...                                  names=('number', 'color'))\n>>> midx\nMultiIndex([(1,   'red'),\n            (2,  'blue'),\n            (3, 'green')],\n           names=['number', 'color'])\n  Check whether the strings in the \u2018color\u2019 level of the MultiIndex are in a list of colors. \n>>> midx.isin(['red', 'orange', 'yellow'], level='color')\narray([ True, False, False])\n  To check across the levels of a MultiIndex, pass a list of tuples: \n>>> midx.isin([(1, 'red'), (3, 'red')])\narray([ True, False, False])\n  For a DatetimeIndex, string values in values are converted to Timestamps. \n>>> dates = ['2000-03-11', '2000-03-12', '2000-03-13']\n>>> dti = pd.to_datetime(dates)\n>>> dti\nDatetimeIndex(['2000-03-11', '2000-03-12', '2000-03-13'],\ndtype='datetime64[ns]', freq=None)\n  \n>>> dti.isin(['2000-03-11'])\narray([ True, False, False])","title":"pandas.reference.api.pandas.index.isin"},{"text":"pandas.Index.any   Index.any(*args, **kwargs)[source]\n \nReturn whether any element is Truthy.  Parameters \n *args\n\nRequired for compatibility with numpy.  **kwargs\n\nRequired for compatibility with numpy.    Returns \n \nany:bool or array-like (if axis is specified)\n\n\nA single element array-like may be converted to bool.      See also  Index.all\n\nReturn whether all elements are True.  Series.all\n\nReturn whether all elements are True.    Notes Not a Number (NaN), positive infinity and negative infinity evaluate to True because these are not equal to zero. Examples \n>>> index = pd.Index([0, 1, 2])\n>>> index.any()\nTrue\n  \n>>> index = pd.Index([0, 0, 0])\n>>> index.any()\nFalse","title":"pandas.reference.api.pandas.index.any"},{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"}]}
{"task_id":21800169,"prompt":"def f_21800169(df):\n\treturn ","suffix":"","canonical_solution":"df[df['BoolCol'] == True].index.tolist()","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame([[True, 2, 3], [False, 5, 6]], columns = ['BoolCol', 'a', 'b'])\n    y = candidate(df)\n    assert y == [0]\n"],"entry_point":"f_21800169","intent":"from dataframe `df` get list of indexes of rows where column 'BoolCol' values match True","library":["pandas"],"docs":[{"text":"pandas.Index.names   propertyIndex.names","title":"pandas.reference.api.pandas.index.names"},{"text":"pandas.DataFrame.index   DataFrame.index\n \nThe index (row labels) of the DataFrame.","title":"pandas.reference.api.pandas.dataframe.index"},{"text":"pandas.DataFrame.isin   DataFrame.isin(values)[source]\n \nWhether each element in the DataFrame is contained in values.  Parameters \n \nvalues:iterable, Series, DataFrame or dict\n\n\nThe result will only be true at a location if all the labels match. If values is a Series, that\u2019s the index. If values is a dict, the keys must be the column names, which must match. If values is a DataFrame, then both the index and column labels must match.    Returns \n DataFrame\n\nDataFrame of booleans showing whether each element in the DataFrame is contained in values.      See also  DataFrame.eq\n\nEquality test for DataFrame.  Series.isin\n\nEquivalent method on Series.  Series.str.contains\n\nTest if pattern or regex is contained within a string of a Series or Index.    Examples \n>>> df = pd.DataFrame({'num_legs': [2, 4], 'num_wings': [2, 0]},\n...                   index=['falcon', 'dog'])\n>>> df\n        num_legs  num_wings\nfalcon         2          2\ndog            4          0\n  When values is a list check whether every value in the DataFrame is present in the list (which animals have 0 or 2 legs or wings) \n>>> df.isin([0, 2])\n        num_legs  num_wings\nfalcon      True       True\ndog        False       True\n  To check if values is not in the DataFrame, use the ~ operator: \n>>> ~df.isin([0, 2])\n        num_legs  num_wings\nfalcon     False      False\ndog         True      False\n  When values is a dict, we can pass values to check for each column separately: \n>>> df.isin({'num_wings': [0, 3]})\n        num_legs  num_wings\nfalcon     False      False\ndog        False       True\n  When values is a Series or DataFrame the index and column must match. Note that \u2018falcon\u2019 does not match based on the number of legs in other. \n>>> other = pd.DataFrame({'num_legs': [8, 3], 'num_wings': [0, 2]},\n...                      index=['spider', 'falcon'])\n>>> df.isin(other)\n        num_legs  num_wings\nfalcon     False       True\ndog        False      False","title":"pandas.reference.api.pandas.dataframe.isin"},{"text":"pandas.MultiIndex.codes   propertyMultiIndex.codes","title":"pandas.reference.api.pandas.multiindex.codes"},{"text":"pandas.IntervalIndex.left   IntervalIndex.left","title":"pandas.reference.api.pandas.intervalindex.left"},{"text":"pandas.Index.isin   Index.isin(values, level=None)[source]\n \nReturn a boolean array where the index values are in values. Compute boolean array of whether each index value is found in the passed set of values. The length of the returned boolean array matches the length of the index.  Parameters \n \nvalues:set or list-like\n\n\nSought values.  \nlevel:str or int, optional\n\n\nName or position of the index level to use (if the index is a MultiIndex).    Returns \n np.ndarray[bool]\n\nNumPy array of boolean values.      See also  Series.isin\n\nSame for Series.  DataFrame.isin\n\nSame method for DataFrames.    Notes In the case of MultiIndex you must either specify values as a list-like object containing tuples that are the same length as the number of levels, or specify level. Otherwise it will raise a ValueError. If level is specified:  if it is the name of one and only one index level, use that level; otherwise it should be a number indicating level position.  Examples \n>>> idx = pd.Index([1,2,3])\n>>> idx\nInt64Index([1, 2, 3], dtype='int64')\n  Check whether each index value in a list of values. \n>>> idx.isin([1, 4])\narray([ True, False, False])\n  \n>>> midx = pd.MultiIndex.from_arrays([[1,2,3],\n...                                  ['red', 'blue', 'green']],\n...                                  names=('number', 'color'))\n>>> midx\nMultiIndex([(1,   'red'),\n            (2,  'blue'),\n            (3, 'green')],\n           names=['number', 'color'])\n  Check whether the strings in the \u2018color\u2019 level of the MultiIndex are in a list of colors. \n>>> midx.isin(['red', 'orange', 'yellow'], level='color')\narray([ True, False, False])\n  To check across the levels of a MultiIndex, pass a list of tuples: \n>>> midx.isin([(1, 'red'), (3, 'red')])\narray([ True, False, False])\n  For a DatetimeIndex, string values in values are converted to Timestamps. \n>>> dates = ['2000-03-11', '2000-03-12', '2000-03-13']\n>>> dti = pd.to_datetime(dates)\n>>> dti\nDatetimeIndex(['2000-03-11', '2000-03-12', '2000-03-13'],\ndtype='datetime64[ns]', freq=None)\n  \n>>> dti.isin(['2000-03-11'])\narray([ True, False, False])","title":"pandas.reference.api.pandas.index.isin"},{"text":"pandas.Index.view   Index.view(cls=None)[source]","title":"pandas.reference.api.pandas.index.view"},{"text":"pandas.Index.nlevels   propertyIndex.nlevels\n \nNumber of levels.","title":"pandas.reference.api.pandas.index.nlevels"},{"text":"pandas.Index.is_boolean   finalIndex.is_boolean()[source]\n \nCheck if the Index only consists of booleans.  Returns \n bool\n\nWhether or not the Index only consists of booleans.      See also  is_integer\n\nCheck if the Index only consists of integers.  is_floating\n\nCheck if the Index is a floating type.  is_numeric\n\nCheck if the Index only consists of numeric data.  is_object\n\nCheck if the Index is of the object dtype.  is_categorical\n\nCheck if the Index holds categorical data.  is_interval\n\nCheck if the Index holds Interval objects.  is_mixed\n\nCheck if the Index holds data with mixed data types.    Examples \n>>> idx = pd.Index([True, False, True])\n>>> idx.is_boolean()\nTrue\n  \n>>> idx = pd.Index([\"True\", \"False\", \"True\"])\n>>> idx.is_boolean()\nFalse\n  \n>>> idx = pd.Index([True, False, \"True\"])\n>>> idx.is_boolean()\nFalse","title":"pandas.reference.api.pandas.index.is_boolean"},{"text":"dis.hascompare  \nSequence of bytecodes of Boolean operations.","title":"python.library.dis#dis.hascompare"}]}
{"task_id":21800169,"prompt":"def f_21800169(df):\n\treturn ","suffix":"","canonical_solution":"df[df['BoolCol']].index.tolist()","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame([[True, 2, 3], [False, 5, 6]], columns = ['BoolCol', 'a', 'b'])\n    y = candidate(df)\n    assert y == [0]\n"],"entry_point":"f_21800169","intent":"get index of rows in dataframe `df` which column 'BoolCol' matches value True","library":["pandas"],"docs":[{"text":"pandas.DataFrame.index   DataFrame.index\n \nThe index (row labels) of the DataFrame.","title":"pandas.reference.api.pandas.dataframe.index"},{"text":"pandas.DataFrame.bool   DataFrame.bool()[source]\n \nReturn the bool of a single element Series or DataFrame. This must be a boolean scalar value, either True or False. It will raise a ValueError if the Series or DataFrame does not have exactly 1 element, or that element is not boolean (integer values 0 and 1 will also raise an exception).  Returns \n bool\n\nThe value in the Series or DataFrame.      See also  Series.astype\n\nChange the data type of a Series, including to boolean.  DataFrame.astype\n\nChange the data type of a DataFrame, including to boolean.  numpy.bool_\n\nNumPy boolean data type, used by pandas for boolean values.    Examples The method will only work for single element objects with a boolean value: \n>>> pd.Series([True]).bool()\nTrue\n>>> pd.Series([False]).bool()\nFalse\n  \n>>> pd.DataFrame({'col': [True]}).bool()\nTrue\n>>> pd.DataFrame({'col': [False]}).bool()\nFalse","title":"pandas.reference.api.pandas.dataframe.bool"},{"text":"pandas.DataFrame.isin   DataFrame.isin(values)[source]\n \nWhether each element in the DataFrame is contained in values.  Parameters \n \nvalues:iterable, Series, DataFrame or dict\n\n\nThe result will only be true at a location if all the labels match. If values is a Series, that\u2019s the index. If values is a dict, the keys must be the column names, which must match. If values is a DataFrame, then both the index and column labels must match.    Returns \n DataFrame\n\nDataFrame of booleans showing whether each element in the DataFrame is contained in values.      See also  DataFrame.eq\n\nEquality test for DataFrame.  Series.isin\n\nEquivalent method on Series.  Series.str.contains\n\nTest if pattern or regex is contained within a string of a Series or Index.    Examples \n>>> df = pd.DataFrame({'num_legs': [2, 4], 'num_wings': [2, 0]},\n...                   index=['falcon', 'dog'])\n>>> df\n        num_legs  num_wings\nfalcon         2          2\ndog            4          0\n  When values is a list check whether every value in the DataFrame is present in the list (which animals have 0 or 2 legs or wings) \n>>> df.isin([0, 2])\n        num_legs  num_wings\nfalcon      True       True\ndog        False       True\n  To check if values is not in the DataFrame, use the ~ operator: \n>>> ~df.isin([0, 2])\n        num_legs  num_wings\nfalcon     False      False\ndog         True      False\n  When values is a dict, we can pass values to check for each column separately: \n>>> df.isin({'num_wings': [0, 3]})\n        num_legs  num_wings\nfalcon     False      False\ndog        False       True\n  When values is a Series or DataFrame the index and column must match. Note that \u2018falcon\u2019 does not match based on the number of legs in other. \n>>> other = pd.DataFrame({'num_legs': [8, 3], 'num_wings': [0, 2]},\n...                      index=['spider', 'falcon'])\n>>> df.isin(other)\n        num_legs  num_wings\nfalcon     False       True\ndog        False      False","title":"pandas.reference.api.pandas.dataframe.isin"},{"text":"pandas.Index.names   propertyIndex.names","title":"pandas.reference.api.pandas.index.names"},{"text":"pandas.IntervalIndex.left   IntervalIndex.left","title":"pandas.reference.api.pandas.intervalindex.left"},{"text":"pandas.Index.is_boolean   finalIndex.is_boolean()[source]\n \nCheck if the Index only consists of booleans.  Returns \n bool\n\nWhether or not the Index only consists of booleans.      See also  is_integer\n\nCheck if the Index only consists of integers.  is_floating\n\nCheck if the Index is a floating type.  is_numeric\n\nCheck if the Index only consists of numeric data.  is_object\n\nCheck if the Index is of the object dtype.  is_categorical\n\nCheck if the Index holds categorical data.  is_interval\n\nCheck if the Index holds Interval objects.  is_mixed\n\nCheck if the Index holds data with mixed data types.    Examples \n>>> idx = pd.Index([True, False, True])\n>>> idx.is_boolean()\nTrue\n  \n>>> idx = pd.Index([\"True\", \"False\", \"True\"])\n>>> idx.is_boolean()\nFalse\n  \n>>> idx = pd.Index([True, False, \"True\"])\n>>> idx.is_boolean()\nFalse","title":"pandas.reference.api.pandas.index.is_boolean"},{"text":"pandas.api.types.is_bool   pandas.api.types.is_bool()\n \nReturn True if given object is boolean.  Returns \n bool","title":"pandas.reference.api.pandas.api.types.is_bool"},{"text":"pandas.Index.is_unique   Index.is_unique\n \nReturn if the index has unique values.","title":"pandas.reference.api.pandas.index.is_unique"},{"text":"pandas.Index.has_duplicates   propertyIndex.has_duplicates\n \nCheck if the Index has duplicate values.  Returns \n bool\n\nWhether or not the Index has duplicate values.     Examples \n>>> idx = pd.Index([1, 5, 7, 7])\n>>> idx.has_duplicates\nTrue\n  \n>>> idx = pd.Index([1, 5, 7])\n>>> idx.has_duplicates\nFalse\n  \n>>> idx = pd.Index([\"Watermelon\", \"Orange\", \"Apple\",\n...                 \"Watermelon\"]).astype(\"category\")\n>>> idx.has_duplicates\nTrue\n  \n>>> idx = pd.Index([\"Orange\", \"Apple\",\n...                 \"Watermelon\"]).astype(\"category\")\n>>> idx.has_duplicates\nFalse","title":"pandas.reference.api.pandas.index.has_duplicates"},{"text":"pandas.Index.any   Index.any(*args, **kwargs)[source]\n \nReturn whether any element is Truthy.  Parameters \n *args\n\nRequired for compatibility with numpy.  **kwargs\n\nRequired for compatibility with numpy.    Returns \n \nany:bool or array-like (if axis is specified)\n\n\nA single element array-like may be converted to bool.      See also  Index.all\n\nReturn whether all elements are True.  Series.all\n\nReturn whether all elements are True.    Notes Not a Number (NaN), positive infinity and negative infinity evaluate to True because these are not equal to zero. Examples \n>>> index = pd.Index([0, 1, 2])\n>>> index.any()\nTrue\n  \n>>> index = pd.Index([0, 0, 0])\n>>> index.any()\nFalse","title":"pandas.reference.api.pandas.index.any"}]}
{"task_id":299446,"prompt":"def f_299446(owd):\n\t","suffix":"\n\treturn ","canonical_solution":"os.chdir(owd)","test_start":"\nimport os\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    os.chdir = Mock()\n    try:\n        candidate('\/')\n    except:\n        assert False\n"],"entry_point":"f_299446","intent":"change working directory to the directory `owd`","library":["os"],"docs":[{"text":"os.getcwd()  \nReturn a string representing the current working directory.","title":"python.library.os#os.getcwd"},{"text":"left  \nThe directory a.","title":"python.library.filecmp#filecmp.dircmp.left"},{"text":"right  \nThe directory b.","title":"python.library.filecmp#filecmp.dircmp.right"},{"text":"stat.S_IFDIR  \nDirectory.","title":"python.library.stat#stat.S_IFDIR"},{"text":"FTP.cwd(pathname)  \nSet the current directory on the server.","title":"python.library.ftplib#ftplib.FTP.cwd"},{"text":"outdim","title":"django.ref.contrib.gis.geos#django.contrib.gis.geos.WKBWriter.outdim"},{"text":"test.support.TEST_HOME_DIR  \nSet to the top level directory for the test package.","title":"python.library.test#test.support.TEST_HOME_DIR"},{"text":"FTP.mkd(pathname)  \nCreate a new directory on the server.","title":"python.library.ftplib#ftplib.FTP.mkd"},{"text":"test.support.TEST_SUPPORT_DIR  \nSet to the top level directory that contains test.support.","title":"python.library.test#test.support.TEST_SUPPORT_DIR"},{"text":"Path.chmod(mode)  \nChange the file mode and permissions, like os.chmod(): >>> p = Path('setup.py')\n>>> p.stat().st_mode\n33277\n>>> p.chmod(0o444)\n>>> p.stat().st_mode\n33060","title":"python.library.pathlib#pathlib.Path.chmod"}]}
{"task_id":14695134,"prompt":"def f_14695134(c, testfield):\n\t","suffix":"\n\treturn ","canonical_solution":"c.execute(\"INSERT INTO test VALUES (?, 'bar')\", (testfield,))","test_start":"\nimport sqlite3\n\ndef check(candidate):","test":["\n    conn = sqlite3.connect('dev.db')\n    cur = conn.cursor()\n    cur.execute(\"CREATE TABLE test (x VARCHAR(10), y VARCHAR(10))\")\n    candidate(cur, 'kang')\n    cur.execute(\"SELECT * FROM test\")\n    rows = cur.fetchall()\n    assert len(rows) == 1\n"],"entry_point":"f_14695134","intent":"insert data from a string `testfield` to sqlite db `c`","library":["sqlite3"],"docs":[{"text":"as_string()","title":"django.ref.contrib.gis.gdal#django.contrib.gis.gdal.Field.as_string"},{"text":"executemany(sql, seq_of_parameters)  \nExecutes a parameterized SQL command against all parameter sequences or mappings found in the sequence seq_of_parameters. The sqlite3 module also allows using an iterator yielding parameters instead of a sequence. import sqlite3\n\nclass IterChars:\n    def __init__(self):\n        self.count = ord('a')\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.count > ord('z'):\n            raise StopIteration\n        self.count += 1\n        return (chr(self.count - 1),) # this is a 1-tuple\n\ncon = sqlite3.connect(\":memory:\")\ncur = con.cursor()\ncur.execute(\"create table characters(c)\")\n\ntheIter = IterChars()\ncur.executemany(\"insert into characters(c) values (?)\", theIter)\n\ncur.execute(\"select c from characters\")\nprint(cur.fetchall())\n\ncon.close()\n Here\u2019s a shorter example using a generator: import sqlite3\nimport string\n\ndef char_generator():\n    for c in string.ascii_lowercase:\n        yield (c,)\n\ncon = sqlite3.connect(\":memory:\")\ncur = con.cursor()\ncur.execute(\"create table characters(c)\")\n\ncur.executemany(\"insert into characters(c) values (?)\", char_generator())\n\ncur.execute(\"select c from characters\")\nprint(cur.fetchall())\n\ncon.close()","title":"python.library.sqlite3#sqlite3.Cursor.executemany"},{"text":"exception sqlite3.Warning  \nA subclass of Exception.","title":"python.library.sqlite3#sqlite3.Warning"},{"text":"exception sqlite3.DatabaseError  \nException raised for errors that are related to the database.","title":"python.library.sqlite3#sqlite3.DatabaseError"},{"text":"char()  \nCasts this storage to char type","title":"torch.storage#torch.FloatStorage.char"},{"text":"kevent.udata  \nUser defined value.","title":"python.library.select#select.kevent.udata"},{"text":"data_ptr()","title":"torch.storage#torch.FloatStorage.data_ptr"},{"text":"Record.GetString(field)  \nReturn the value of field as a string where possible. field must be an integer.","title":"python.library.msilib#msilib.Record.GetString"},{"text":"class sqlite3.Cursor  \nA Cursor instance has the following attributes and methods.  \nexecute(sql[, parameters])  \nExecutes an SQL statement. Values may be bound to the statement using placeholders. execute() will only execute a single SQL statement. If you try to execute more than one statement with it, it will raise a Warning. Use executescript() if you want to execute multiple SQL statements with one call. \n  \nexecutemany(sql, seq_of_parameters)  \nExecutes a parameterized SQL command against all parameter sequences or mappings found in the sequence seq_of_parameters. The sqlite3 module also allows using an iterator yielding parameters instead of a sequence. import sqlite3\n\nclass IterChars:\n    def __init__(self):\n        self.count = ord('a')\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.count > ord('z'):\n            raise StopIteration\n        self.count += 1\n        return (chr(self.count - 1),) # this is a 1-tuple\n\ncon = sqlite3.connect(\":memory:\")\ncur = con.cursor()\ncur.execute(\"create table characters(c)\")\n\ntheIter = IterChars()\ncur.executemany(\"insert into characters(c) values (?)\", theIter)\n\ncur.execute(\"select c from characters\")\nprint(cur.fetchall())\n\ncon.close()\n Here\u2019s a shorter example using a generator: import sqlite3\nimport string\n\ndef char_generator():\n    for c in string.ascii_lowercase:\n        yield (c,)\n\ncon = sqlite3.connect(\":memory:\")\ncur = con.cursor()\ncur.execute(\"create table characters(c)\")\n\ncur.executemany(\"insert into characters(c) values (?)\", char_generator())\n\ncur.execute(\"select c from characters\")\nprint(cur.fetchall())\n\ncon.close()\n \n  \nexecutescript(sql_script)  \nThis is a nonstandard convenience method for executing multiple SQL statements at once. It issues a COMMIT statement first, then executes the SQL script it gets as a parameter. sql_script can be an instance of str. Example: import sqlite3\n\ncon = sqlite3.connect(\":memory:\")\ncur = con.cursor()\ncur.executescript(\"\"\"\n    create table person(\n        firstname,\n        lastname,\n        age\n    );\n\n    create table book(\n        title,\n        author,\n        published\n    );\n\n    insert into book(title, author, published)\n    values (\n        'Dirk Gently''s Holistic Detective Agency',\n        'Douglas Adams',\n        1987\n    );\n    \"\"\")\ncon.close()\n \n  \nfetchone()  \nFetches the next row of a query result set, returning a single sequence, or None when no more data is available. \n  \nfetchmany(size=cursor.arraysize)  \nFetches the next set of rows of a query result, returning a list. An empty list is returned when no more rows are available. The number of rows to fetch per call is specified by the size parameter. If it is not given, the cursor\u2019s arraysize determines the number of rows to be fetched. The method should try to fetch as many rows as indicated by the size parameter. If this is not possible due to the specified number of rows not being available, fewer rows may be returned. Note there are performance considerations involved with the size parameter. For optimal performance, it is usually best to use the arraysize attribute. If the size parameter is used, then it is best for it to retain the same value from one fetchmany() call to the next. \n  \nfetchall()  \nFetches all (remaining) rows of a query result, returning a list. Note that the cursor\u2019s arraysize attribute can affect the performance of this operation. An empty list is returned when no rows are available. \n  \nclose()  \nClose the cursor now (rather than whenever __del__ is called). The cursor will be unusable from this point forward; a ProgrammingError exception will be raised if any operation is attempted with the cursor. \n  \nrowcount  \nAlthough the Cursor class of the sqlite3 module implements this attribute, the database engine\u2019s own support for the determination of \u201crows affected\u201d\/\u201drows selected\u201d is quirky. For executemany() statements, the number of modifications are summed up into rowcount. As required by the Python DB API Spec, the rowcount attribute \u201cis -1 in case no executeXX() has been performed on the cursor or the rowcount of the last operation is not determinable by the interface\u201d. This includes SELECT statements because we cannot determine the number of rows a query produced until all rows were fetched. With SQLite versions before 3.6.5, rowcount is set to 0 if you make a DELETE FROM table without any condition. \n  \nlastrowid  \nThis read-only attribute provides the rowid of the last modified row. It is only set if you issued an INSERT or a REPLACE statement using the execute() method. For operations other than INSERT or REPLACE or when executemany() is called, lastrowid is set to None. If the INSERT or REPLACE statement failed to insert the previous successful rowid is returned.  Changed in version 3.6: Added support for the REPLACE statement.  \n  \narraysize  \nRead\/write attribute that controls the number of rows returned by fetchmany(). The default value is 1 which means a single row would be fetched per call. \n  \ndescription  \nThis read-only attribute provides the column names of the last query. To remain compatible with the Python DB API, it returns a 7-tuple for each column where the last six items of each tuple are None. It is set for SELECT statements without any matching rows as well. \n  \nconnection  \nThis read-only attribute provides the SQLite database Connection used by the Cursor object. A Cursor object created by calling con.cursor() will have a connection attribute that refers to con: >>> con = sqlite3.connect(\":memory:\")\n>>> cur = con.cursor()\n>>> cur.connection == con\nTrue","title":"python.library.sqlite3#sqlite3.Cursor"},{"text":"class RawSQL(sql, params, output_field=None)","title":"django.ref.models.expressions#django.db.models.expressions.RawSQL"}]}
{"task_id":24242433,"prompt":"def f_24242433():\n\treturn ","suffix":"","canonical_solution":"b'\\\\x89\\\\n'.decode('unicode_escape')","test_start":"\nimport sqlite3\n\ndef check(candidate):","test":["\n    assert candidate() == '\\x89\\n'\n"],"entry_point":"f_24242433","intent":"decode string \"\\\\x89\\\\n\" into a normal string","library":["sqlite3"],"docs":[]}
{"task_id":24242433,"prompt":"def f_24242433(raw_string):\n\treturn ","suffix":"","canonical_solution":"raw_string.decode('unicode_escape')","test_start":"\ndef check(candidate):","test":["\n    assert candidate(b\"Hello\") == \"Hello\"\n","\n    assert candidate(b\"hello world!\") == \"hello world!\"\n","\n    assert candidate(b\". ?? !!x\") == \". ?? !!x\"\n"],"entry_point":"f_24242433","intent":"convert a raw string `raw_string` into a normal string","library":[],"docs":[]}
{"task_id":24242433,"prompt":"def f_24242433(raw_byte_string):\n\treturn ","suffix":"","canonical_solution":"raw_byte_string.decode('unicode_escape')","test_start":"\ndef check(candidate):","test":["\n    assert candidate(b\"Hello\") == \"Hello\"\n","\n    assert candidate(b\"hello world!\") == \"hello world!\"\n","\n    assert candidate(b\". ?? !!x\") == \". ?? !!x\"\n"],"entry_point":"f_24242433","intent":"convert a raw string `raw_byte_string` into a normal string","library":[],"docs":[]}
{"task_id":22882922,"prompt":"def f_22882922(s):\n\treturn ","suffix":"","canonical_solution":"[m.group(0) for m in re.finditer('(\\\\d)\\\\1*', s)]","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    assert candidate('111234') == ['111', '2', '3', '4']\n"],"entry_point":"f_22882922","intent":"split a string `s` with into all strings of repeated characters","library":["re"],"docs":[{"text":"Pattern.split(string, maxsplit=0)  \nIdentical to the split() function, using the compiled pattern.","title":"python.library.re#re.Pattern.split"},{"text":"re.split(pattern, string, maxsplit=0, flags=0)  \nSplit string by the occurrences of pattern. If capturing parentheses are used in pattern, then the text of all groups in the pattern are also returned as part of the resulting list. If maxsplit is nonzero, at most maxsplit splits occur, and the remainder of the string is returned as the final element of the list. >>> re.split(r'\\W+', 'Words, words, words.')\n['Words', 'words', 'words', '']\n>>> re.split(r'(\\W+)', 'Words, words, words.')\n['Words', ', ', 'words', ', ', 'words', '.', '']\n>>> re.split(r'\\W+', 'Words, words, words.', 1)\n['Words', 'words, words.']\n>>> re.split('[a-f]+', '0a3B9', flags=re.IGNORECASE)\n['0', '3', '9']\n If there are capturing groups in the separator and it matches at the start of the string, the result will start with an empty string. The same holds for the end of the string: >>> re.split(r'(\\W+)', '...words, words...')\n['', '...', 'words', ', ', 'words', '...', '']\n That way, separator components are always found at the same relative indices within the result list. Empty matches for the pattern split the string only when not adjacent to a previous empty match. >>> re.split(r'\\b', 'Words, words, words.')\n['', 'Words', ', ', 'words', ', ', 'words', '.']\n>>> re.split(r'\\W*', '...words...')\n['', '', 'w', 'o', 'r', 'd', 's', '', '']\n>>> re.split(r'(\\W*)', '...words...')\n['', '...', '', '', 'w', '', 'o', '', 'r', '', 'd', '', 's', '...', '', '', '']\n  Changed in version 3.1: Added the optional flags argument.   Changed in version 3.7: Added support of splitting on a pattern that could match an empty string.","title":"python.library.re#re.split"},{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"pattern  \nThe regular expression pattern.","title":"python.library.re#re.error.pattern"},{"text":"str.rindex(sub[, start[, end]])  \nLike rfind() but raises ValueError when the substring sub is not found.","title":"python.library.stdtypes#str.rindex"},{"text":"numpy.char.chararray.rsplit method   char.chararray.rsplit(sep=None, maxsplit=None)[source]\n \nFor each element in self, return a list of the words in the string, using sep as the delimiter string.  See also  char.rsplit","title":"numpy.reference.generated.numpy.char.chararray.rsplit"},{"text":"numpy.char.split   char.split(a, sep=None, maxsplit=None)[source]\n \nFor each element in a, return a list of the words in the string, using sep as the delimiter string. Calls str.split element-wise.  Parameters \n \naarray_like of str or unicode\n\n\nsepstr or unicode, optional\n\n\nIf sep is not specified or None, any whitespace string is a separator.  \nmaxsplitint, optional\n\n\nIf maxsplit is given, at most maxsplit splits are done.    Returns \n \noutndarray\n\n\nArray of list objects      See also  \nstr.split, rsplit","title":"numpy.reference.generated.numpy.char.split"},{"text":"numpy.char.chararray.dumps method   char.chararray.dumps()\n \nReturns the pickle of the array as a string. pickle.loads will convert the string back to an array.  Parameters \n None","title":"numpy.reference.generated.numpy.char.chararray.dumps"},{"text":"numpy.string_[source]\n \nalias of numpy.bytes_","title":"numpy.reference.arrays.scalars#numpy.string_"},{"text":"numpy.chararray.rstrip method   chararray.rstrip(chars=None)[source]\n \nFor each element in self, return a copy with the trailing characters removed.  See also  char.rstrip","title":"numpy.reference.generated.numpy.chararray.rstrip"}]}
{"task_id":4143502,"prompt":"def f_4143502():\n\treturn ","suffix":"","canonical_solution":"plt.scatter(np.random.randn(100), np.random.randn(100), facecolors='none')","test_start":"\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef check(candidate):","test":["\n    assert 'matplotlib' in str(type(candidate()))\n"],"entry_point":"f_4143502","intent":"scatter a plot with x, y position of `np.random.randn(100)` and face color equal to none","library":["matplotlib","numpy"],"docs":[{"text":"matplotlib.pyplot.scatter   matplotlib.pyplot.scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, *, edgecolors=None, plotnonfinite=False, data=None, **kwargs)[source]\n \nA scatter plot of y vs. x with varying marker size and\/or color.  Parameters \n \nx, yfloat or array-like, shape (n, )\n\n\nThe data positions.  \nsfloat or array-like, shape (n, ), optional\n\n\nThe marker size in points**2. Default is rcParams['lines.markersize'] ** 2.  \ncarray-like or list of colors or color, optional\n\n\nThe marker colors. Possible values:  A scalar or sequence of n numbers to be mapped to colors using cmap and norm. A 2D array in which the rows are RGB or RGBA. A sequence of colors of length n. A single color format string.  Note that c should not be a single numeric RGB or RGBA sequence because that is indistinguishable from an array of values to be colormapped. If you want to specify the same RGB or RGBA value for all points, use a 2D array with a single row. Otherwise, value- matching will have precedence in case of a size matching with x and y. If you wish to specify a single color for all points prefer the color keyword argument. Defaults to None. In that case the marker color is determined by the value of color, facecolor or facecolors. In case those are not specified or None, the marker color is determined by the next color of the Axes' current \"shape and fill\" color cycle. This cycle defaults to rcParams[\"axes.prop_cycle\"] (default: cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])).  \nmarkerMarkerStyle, default: rcParams[\"scatter.marker\"] (default: 'o')\n\n\nThe marker style. marker can be either an instance of the class or the text shorthand for a particular marker. See matplotlib.markers for more information about marker styles.  \ncmapstr or Colormap, default: rcParams[\"image.cmap\"] (default: 'viridis')\n\n\nA Colormap instance or registered colormap name. cmap is only used if c is an array of floats.  \nnormNormalize, default: None\n\n\nIf c is an array of floats, norm is used to scale the color data, c, in the range 0 to 1, in order to map into the colormap cmap. If None, use the default colors.Normalize.  \nvmin, vmaxfloat, default: None\n\n\nvmin and vmax are used in conjunction with the default norm to map the color array c to the colormap cmap. If None, the respective min and max of the color array is used. It is an error to use vmin\/vmax when norm is given.  \nalphafloat, default: None\n\n\nThe alpha blending value, between 0 (transparent) and 1 (opaque).  \nlinewidthsfloat or array-like, default: rcParams[\"lines.linewidth\"] (default: 1.5)\n\n\nThe linewidth of the marker edges. Note: The default edgecolors is 'face'. You may want to change this as well.  \nedgecolors{'face', 'none', None} or color or sequence of color, default: rcParams[\"scatter.edgecolors\"] (default: 'face')\n\n\nThe edge color of the marker. Possible values:  'face': The edge color will always be the same as the face color. 'none': No patch boundary will be drawn. A color or sequence of colors.  For non-filled markers, edgecolors is ignored. Instead, the color is determined like with 'face', i.e. from c, colors, or facecolors.  \nplotnonfinitebool, default: False\n\n\nWhether to plot points with nonfinite c (i.e. inf, -inf or nan). If True the points are drawn with the bad colormap color (see Colormap.set_bad).    Returns \n PathCollection\n  Other Parameters \n \ndataindexable object, optional\n\n\nIf given, the following parameters also accept a string s, which is interpreted as data[s] (unless this raises an exception): x, y, s, linewidths, edgecolors, c, facecolor, facecolors, color  \n**kwargsCollection properties\n\n    See also  plot\n\nTo plot scatter plots when markers are identical in size and color.    Notes  The plot function will be faster for scatterplots where markers don't vary in size or color. Any or all of x, y, s, and c may be masked arrays, in which case all masks will be combined and only unmasked points will be plotted. Fundamentally, scatter works with 1D arrays; x, y, s, and c may be input as N-D arrays, but within scatter they will be flattened. The exception is c, which will be flattened only if its size matches the size of x and y.  \n  Examples using matplotlib.pyplot.scatter\n \n   Scatter Masked   \n\n   Scatter Symbol   \n\n   Scatter plot   \n\n   Hyperlinks   \n\n   Pyplot tutorial","title":"matplotlib._as_gen.matplotlib.pyplot.scatter"},{"text":"quiver_doc=\"\\nPlot a 2D field of arrows.\\n\\nCall signature::\\n\\n quiver([X, Y], U, V, [C], **kw)\\n\\n*X*, *Y* define the arrow locations, *U*, *V* define the arrow directions, and\\n*C* optionally sets the color.\\n\\nEach arrow is internally represented by a filled polygon with a default edge\\nlinewidth of 0. As a result, an arrow is rather a filled area, not a line with\\na head, and `.PolyCollection` properties like *linewidth*, *linestyle*,\\n*facecolor*, etc. act accordingly.\\n\\n**Arrow size**\\n\\nThe default settings auto-scales the length of the arrows to a reasonable size.\\nTo change this behavior see the *scale* and *scale_units* parameters.\\n\\n**Arrow shape**\\n\\nThe defaults give a slightly swept-back arrow; to make the head a\\ntriangle, make *headaxislength* the same as *headlength*. To make the\\narrow more pointed, reduce *headwidth* or increase *headlength* and\\n*headaxislength*. To make the head smaller relative to the shaft,\\nscale down all the head parameters. You will probably do best to leave\\nminshaft alone.\\n\\n**Arrow outline**\\n\\n*linewidths* and *edgecolors* can be used to customize the arrow\\noutlines.\\n\\nParameters\\n----------\\nX, Y : 1D or 2D array-like, optional\\n The x and y coordinates of the arrow locations.\\n\\n If not given, they will be generated as a uniform integer meshgrid based\\n on the dimensions of *U* and *V*.\\n\\n If *X* and *Y* are 1D but *U*, *V* are 2D, *X*, *Y* are expanded to 2D\\n using ``X, Y = np.meshgrid(X, Y)``. In this case ``len(X)`` and ``len(Y)``\\n must match the column and row dimensions of *U* and *V*.\\n\\nU, V : 1D or 2D array-like\\n The x and y direction components of the arrow vectors.\\n\\n They must have the same number of elements, matching the number of arrow\\n locations. *U* and *V* may be masked. Only locations unmasked in\\n *U*, *V*, and *C* will be drawn.\\n\\nC : 1D or 2D array-like, optional\\n Numeric data that defines the arrow colors by colormapping via *norm* and\\n *cmap*.\\n\\n This does not support explicit colors. If you want to set colors directly,\\n use *color* instead. The size of *C* must match the number of arrow\\n locations.\\n\\nunits : {'width', 'height', 'dots', 'inches', 'x', 'y', 'xy'}, default: 'width'\\n The arrow dimensions (except for *length*) are measured in multiples of\\n this unit.\\n\\n The following values are supported:\\n\\n - 'width', 'height': The width or height of the axis.\\n - 'dots', 'inches': Pixels or inches based on the figure dpi.\\n - 'x', 'y', 'xy': *X*, *Y* or :math:`\\\\sqrt{X^2 + Y^2}` in data units.\\n\\n The arrows scale differently depending on the units. For\\n 'x' or 'y', the arrows get larger as one zooms in; for other\\n units, the arrow size is independent of the zoom state. For\\n 'width or 'height', the arrow size increases with the width and\\n height of the axes, respectively, when the window is resized;\\n for 'dots' or 'inches', resizing does not change the arrows.\\n\\nangles : {'uv', 'xy'} or array-like, default: 'uv'\\n Method for determining the angle of the arrows.\\n\\n - 'uv': The arrow axis aspect ratio is 1 so that\\n if *U* == *V* the orientation of the arrow on the plot is 45 degrees\\n counter-clockwise from the horizontal axis (positive to the right).\\n\\n Use this if the arrows symbolize a quantity that is not based on\\n *X*, *Y* data coordinates.\\n\\n - 'xy': Arrows point from (x, y) to (x+u, y+v).\\n Use this for plotting a gradient field, for example.\\n\\n - Alternatively, arbitrary angles may be specified explicitly as an array\\n of values in degrees, counter-clockwise from the horizontal axis.\\n\\n In this case *U*, *V* is only used to determine the length of the\\n arrows.\\n\\n Note: inverting a data axis will correspondingly invert the\\n arrows only with ``angles='xy'``.\\n\\nscale : float, optional\\n Number of data units per arrow length unit, e.g., m\/s per plot width; a\\n smaller scale parameter makes the arrow longer. Default is *None*.\\n\\n If *None*, a simple autoscaling algorithm is used, based on the average\\n vector length and the number of vectors. The arrow length unit is given by\\n the *scale_units* parameter.\\n\\nscale_units : {'width', 'height', 'dots', 'inches', 'x', 'y', 'xy'}, optional\\n If the *scale* kwarg is *None*, the arrow length unit. Default is *None*.\\n\\n e.g. *scale_units* is 'inches', *scale* is 2.0, and ``(u, v) = (1, 0)``,\\n then the vector will be 0.5 inches long.\\n\\n If *scale_units* is 'width' or 'height', then the vector will be half the\\n width\/height of the axes.\\n\\n If *scale_units* is 'x' then the vector will be 0.5 x-axis\\n units. To plot vectors in the x-y plane, with u and v having\\n the same units as x and y, use\\n ``angles='xy', scale_units='xy', scale=1``.\\n\\nwidth : float, optional\\n Shaft width in arrow units; default depends on choice of units,\\n above, and number of vectors; a typical starting value is about\\n 0.005 times the width of the plot.\\n\\nheadwidth : float, default: 3\\n Head width as multiple of shaft width.\\n\\nheadlength : float, default: 5\\n Head length as multiple of shaft width.\\n\\nheadaxislength : float, default: 4.5\\n Head length at shaft intersection.\\n\\nminshaft : float, default: 1\\n Length below which arrow scales, in units of head length. Do not\\n set this to less than 1, or small arrows will look terrible!\\n\\nminlength : float, default: 1\\n Minimum length as a multiple of shaft width; if an arrow length\\n is less than this, plot a dot (hexagon) of this diameter instead.\\n\\npivot : {'tail', 'mid', 'middle', 'tip'}, default: 'tail'\\n The part of the arrow that is anchored to the *X*, *Y* grid. The arrow\\n rotates about this point.\\n\\n 'mid' is a synonym for 'middle'.\\n\\ncolor : color or color sequence, optional\\n Explicit color(s) for the arrows. If *C* has been set, *color* has no\\n effect.\\n\\n This is a synonym for the `.PolyCollection` *facecolor* parameter.\\n\\nOther Parameters\\n----------------\\ndata : indexable object, optional\\n DATA_PARAMETER_PLACEHOLDER\\n\\n**kwargs : `~matplotlib.collections.PolyCollection` properties, optional\\n All other keyword arguments are passed on to `.PolyCollection`:\\n\\n \\n .. table::\\n :class: property-table\\n\\n ================================================================================================= =====================================================================================================\\n Property Description \\n ================================================================================================= =====================================================================================================\\n :meth:`agg_filter <matplotlib.artist.Artist.set_agg_filter>` a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\\n :meth:`alpha <matplotlib.collections.Collection.set_alpha>` array-like or scalar or None \\n :meth:`animated <matplotlib.artist.Artist.set_animated>` bool \\n :meth:`antialiased <matplotlib.collections.Collection.set_antialiased>` or aa or antialiaseds bool or list of bools \\n :meth:`array <matplotlib.cm.ScalarMappable.set_array>` array-like or None \\n :meth:`capstyle <matplotlib.collections.Collection.set_capstyle>` `.CapStyle` or {'butt', 'projecting', 'round'} \\n :meth:`clim <matplotlib.cm.ScalarMappable.set_clim>` (vmin: float, vmax: float) \\n :meth:`clip_box <matplotlib.artist.Artist.set_clip_box>` `.Bbox` \\n :meth:`clip_on <matplotlib.artist.Artist.set_clip_on>` bool \\n :meth:`clip_path <matplotlib.artist.Artist.set_clip_path>` Patch or (Path, Transform) or None \\n :meth:`cmap <matplotlib.cm.ScalarMappable.set_cmap>` `.Colormap` or str or None \\n :meth:`color <matplotlib.collections.Collection.set_color>` color or list of rgba tuples \\n :meth:`edgecolor <matplotlib.collections.Collection.set_edgecolor>` or ec or edgecolors color or list of colors or 'face' \\n :meth:`facecolor <matplotlib.collections.Collection.set_facecolor>` or facecolors or fc color or list of colors \\n :meth:`figure <matplotlib.artist.Artist.set_figure>` `.Figure` \\n :meth:`gid <matplotlib.artist.Artist.set_gid>` str \\n :meth:`hatch <matplotlib.collections.Collection.set_hatch>` {'\/', '\\\\\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'} \\n :meth:`in_layout <matplotlib.artist.Artist.set_in_layout>` bool \\n :meth:`joinstyle <matplotlib.collections.Collection.set_joinstyle>` `.JoinStyle` or {'miter', 'round', 'bevel'} \\n :meth:`label <matplotlib.artist.Artist.set_label>` object \\n :meth:`linestyle <matplotlib.collections.Collection.set_linestyle>` or dashes or linestyles or ls str or tuple or list thereof \\n :meth:`linewidth <matplotlib.collections.Collection.set_linewidth>` or linewidths or lw float or list of floats \\n :meth:`norm <matplotlib.cm.ScalarMappable.set_norm>` `.Normalize` or None \\n :meth:`offset_transform <matplotlib.collections.Collection.set_offset_transform>` `.Transform` \\n :meth:`offsets <matplotlib.collections.Collection.set_offsets>` (N, 2) or (2,) array-like \\n :meth:`path_effects <matplotlib.artist.Artist.set_path_effects>` `.AbstractPathEffect` \\n :meth:`paths <matplotlib.collections.PolyCollection.set_verts>` list of array-like \\n :meth:`picker <matplotlib.artist.Artist.set_picker>` None or bool or float or callable \\n :meth:`pickradius <matplotlib.collections.Collection.set_pickradius>` float \\n :meth:`rasterized <matplotlib.artist.Artist.set_rasterized>` bool \\n :meth:`sizes <matplotlib.collections._CollectionWithSizes.set_sizes>` ndarray or None \\n :meth:`sketch_params <matplotlib.artist.Artist.set_sketch_params>` (scale: float, length: float, randomness: float) \\n :meth:`snap <matplotlib.artist.Artist.set_snap>` bool or None \\n :meth:`transform <matplotlib.artist.Artist.set_transform>` `.Transform` \\n :meth:`url <matplotlib.artist.Artist.set_url>` str \\n :meth:`urls <matplotlib.collections.Collection.set_urls>` list of str or None \\n :meth:`verts <matplotlib.collections.PolyCollection.set_verts>` list of array-like \\n :meth:`verts_and_codes <matplotlib.collections.PolyCollection.set_verts_and_codes>` unknown \\n :meth:`visible <matplotlib.artist.Artist.set_visible>` bool \\n :meth:`zorder <matplotlib.artist.Artist.set_zorder>` float \\n ================================================================================================= =====================================================================================================\\n\\n\\nReturns\\n-------\\n`~matplotlib.quiver.Quiver`\\n\\nSee Also\\n--------\\n.Axes.quiverkey : Add a key to a quiver plot.\\n\"","title":"matplotlib._as_gen.matplotlib.quiver.quiver#matplotlib.quiver.Quiver.quiver_doc"},{"text":"__hash__=None","title":"matplotlib.transformations#matplotlib.transforms.AffineBase.__hash__"},{"text":"__hash__=None","title":"matplotlib.transformations#matplotlib.transforms.TransformWrapper.__hash__"},{"text":"matplotlib.axes.Axes.scatter   Axes.scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, *, edgecolors=None, plotnonfinite=False, data=None, **kwargs)[source]\n \nA scatter plot of y vs. x with varying marker size and\/or color.  Parameters \n \nx, yfloat or array-like, shape (n, )\n\n\nThe data positions.  \nsfloat or array-like, shape (n, ), optional\n\n\nThe marker size in points**2. Default is rcParams['lines.markersize'] ** 2.  \ncarray-like or list of colors or color, optional\n\n\nThe marker colors. Possible values:  A scalar or sequence of n numbers to be mapped to colors using cmap and norm. A 2D array in which the rows are RGB or RGBA. A sequence of colors of length n. A single color format string.  Note that c should not be a single numeric RGB or RGBA sequence because that is indistinguishable from an array of values to be colormapped. If you want to specify the same RGB or RGBA value for all points, use a 2D array with a single row. Otherwise, value- matching will have precedence in case of a size matching with x and y. If you wish to specify a single color for all points prefer the color keyword argument. Defaults to None. In that case the marker color is determined by the value of color, facecolor or facecolors. In case those are not specified or None, the marker color is determined by the next color of the Axes' current \"shape and fill\" color cycle. This cycle defaults to rcParams[\"axes.prop_cycle\"] (default: cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])).  \nmarkerMarkerStyle, default: rcParams[\"scatter.marker\"] (default: 'o')\n\n\nThe marker style. marker can be either an instance of the class or the text shorthand for a particular marker. See matplotlib.markers for more information about marker styles.  \ncmapstr or Colormap, default: rcParams[\"image.cmap\"] (default: 'viridis')\n\n\nA Colormap instance or registered colormap name. cmap is only used if c is an array of floats.  \nnormNormalize, default: None\n\n\nIf c is an array of floats, norm is used to scale the color data, c, in the range 0 to 1, in order to map into the colormap cmap. If None, use the default colors.Normalize.  \nvmin, vmaxfloat, default: None\n\n\nvmin and vmax are used in conjunction with the default norm to map the color array c to the colormap cmap. If None, the respective min and max of the color array is used. It is an error to use vmin\/vmax when norm is given.  \nalphafloat, default: None\n\n\nThe alpha blending value, between 0 (transparent) and 1 (opaque).  \nlinewidthsfloat or array-like, default: rcParams[\"lines.linewidth\"] (default: 1.5)\n\n\nThe linewidth of the marker edges. Note: The default edgecolors is 'face'. You may want to change this as well.  \nedgecolors{'face', 'none', None} or color or sequence of color, default: rcParams[\"scatter.edgecolors\"] (default: 'face')\n\n\nThe edge color of the marker. Possible values:  'face': The edge color will always be the same as the face color. 'none': No patch boundary will be drawn. A color or sequence of colors.  For non-filled markers, edgecolors is ignored. Instead, the color is determined like with 'face', i.e. from c, colors, or facecolors.  \nplotnonfinitebool, default: False\n\n\nWhether to plot points with nonfinite c (i.e. inf, -inf or nan). If True the points are drawn with the bad colormap color (see Colormap.set_bad).    Returns \n PathCollection\n  Other Parameters \n \ndataindexable object, optional\n\n\nIf given, the following parameters also accept a string s, which is interpreted as data[s] (unless this raises an exception): x, y, s, linewidths, edgecolors, c, facecolor, facecolors, color  \n**kwargsCollection properties\n\n    See also  plot\n\nTo plot scatter plots when markers are identical in size and color.    Notes  The plot function will be faster for scatterplots where markers don't vary in size or color. Any or all of x, y, s, and c may be masked arrays, in which case all masks will be combined and only unmasked points will be plotted. Fundamentally, scatter works with 1D arrays; x, y, s, and c may be input as N-D arrays, but within scatter they will be flattened. The exception is c, which will be flattened only if its size matches the size of x and y.  \n  Examples using matplotlib.axes.Axes.scatter\n \n   Scatter Custom Symbol   \n\n   Scatter Demo2   \n\n   Scatter plot with histograms   \n\n   Scatter plot with pie chart markers   \n\n   Scatter plots with a legend   \n\n   Advanced quiver and quiverkey functions   \n\n   Axes box aspect   \n\n   Axis Label Position   \n\n   Plot a confidence ellipse of a two-dimensional dataset   \n\n   Violin plot customization   \n\n   Scatter plot on polar axis   \n\n   Legend Demo   \n\n   Scatter Histogram (Locatable Axes)   \n\n   mpl_toolkits.axisartist.floating_axes features   \n\n   Rain simulation   \n\n   Zoom Window   \n\n   Plotting with keywords   \n\n   Zorder Demo   \n\n   Plot 2D data on 3D plot   \n\n   3D scatterplot   \n\n   Automatically setting tick positions   \n\n   Unit handling   \n\n   Annotate Text Arrow   \n\n   Polygon Selector   \n\n   Basic Usage   \n\n   Choosing Colormaps in Matplotlib   \n\n   scatter(x, y)","title":"matplotlib._as_gen.matplotlib.axes.axes.scatter"},{"text":"scatter(dim, index, src) \u2192 Tensor  \nOut-of-place version of torch.Tensor.scatter_()","title":"torch.tensors#torch.Tensor.scatter"},{"text":"get_facecolor()[source]","title":"matplotlib.collections_api#matplotlib.collections.TriMesh.get_facecolor"},{"text":"get_facecolor()[source]","title":"matplotlib.collections_api#matplotlib.collections.PathCollection.get_facecolor"},{"text":"inverse(value)[source]","title":"matplotlib._as_gen.matplotlib.colors.normalize#matplotlib.colors.Normalize.inverse"},{"text":"get_facecolor()[source]","title":"matplotlib.collections_api#matplotlib.collections.Collection.get_facecolor"}]}
{"task_id":4143502,"prompt":"def f_4143502():\n\treturn ","suffix":"","canonical_solution":"plt.plot(np.random.randn(100), np.random.randn(100), 'o', mfc='none')","test_start":"\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef check(candidate):","test":["\n    assert 'matplotlib' in str(type(candidate()[0]))\n"],"entry_point":"f_4143502","intent":"do a scatter plot with empty circles","library":["matplotlib","numpy"],"docs":[{"text":"__hash__=None","title":"matplotlib.transformations#matplotlib.transforms.TransformWrapper.__hash__"},{"text":"__hash__=None","title":"matplotlib.transformations#matplotlib.transforms.AffineBase.__hash__"},{"text":"quiver_doc=\"\\nPlot a 2D field of arrows.\\n\\nCall signature::\\n\\n quiver([X, Y], U, V, [C], **kw)\\n\\n*X*, *Y* define the arrow locations, *U*, *V* define the arrow directions, and\\n*C* optionally sets the color.\\n\\nEach arrow is internally represented by a filled polygon with a default edge\\nlinewidth of 0. As a result, an arrow is rather a filled area, not a line with\\na head, and `.PolyCollection` properties like *linewidth*, *linestyle*,\\n*facecolor*, etc. act accordingly.\\n\\n**Arrow size**\\n\\nThe default settings auto-scales the length of the arrows to a reasonable size.\\nTo change this behavior see the *scale* and *scale_units* parameters.\\n\\n**Arrow shape**\\n\\nThe defaults give a slightly swept-back arrow; to make the head a\\ntriangle, make *headaxislength* the same as *headlength*. To make the\\narrow more pointed, reduce *headwidth* or increase *headlength* and\\n*headaxislength*. To make the head smaller relative to the shaft,\\nscale down all the head parameters. You will probably do best to leave\\nminshaft alone.\\n\\n**Arrow outline**\\n\\n*linewidths* and *edgecolors* can be used to customize the arrow\\noutlines.\\n\\nParameters\\n----------\\nX, Y : 1D or 2D array-like, optional\\n The x and y coordinates of the arrow locations.\\n\\n If not given, they will be generated as a uniform integer meshgrid based\\n on the dimensions of *U* and *V*.\\n\\n If *X* and *Y* are 1D but *U*, *V* are 2D, *X*, *Y* are expanded to 2D\\n using ``X, Y = np.meshgrid(X, Y)``. In this case ``len(X)`` and ``len(Y)``\\n must match the column and row dimensions of *U* and *V*.\\n\\nU, V : 1D or 2D array-like\\n The x and y direction components of the arrow vectors.\\n\\n They must have the same number of elements, matching the number of arrow\\n locations. *U* and *V* may be masked. Only locations unmasked in\\n *U*, *V*, and *C* will be drawn.\\n\\nC : 1D or 2D array-like, optional\\n Numeric data that defines the arrow colors by colormapping via *norm* and\\n *cmap*.\\n\\n This does not support explicit colors. If you want to set colors directly,\\n use *color* instead. The size of *C* must match the number of arrow\\n locations.\\n\\nunits : {'width', 'height', 'dots', 'inches', 'x', 'y', 'xy'}, default: 'width'\\n The arrow dimensions (except for *length*) are measured in multiples of\\n this unit.\\n\\n The following values are supported:\\n\\n - 'width', 'height': The width or height of the axis.\\n - 'dots', 'inches': Pixels or inches based on the figure dpi.\\n - 'x', 'y', 'xy': *X*, *Y* or :math:`\\\\sqrt{X^2 + Y^2}` in data units.\\n\\n The arrows scale differently depending on the units. For\\n 'x' or 'y', the arrows get larger as one zooms in; for other\\n units, the arrow size is independent of the zoom state. For\\n 'width or 'height', the arrow size increases with the width and\\n height of the axes, respectively, when the window is resized;\\n for 'dots' or 'inches', resizing does not change the arrows.\\n\\nangles : {'uv', 'xy'} or array-like, default: 'uv'\\n Method for determining the angle of the arrows.\\n\\n - 'uv': The arrow axis aspect ratio is 1 so that\\n if *U* == *V* the orientation of the arrow on the plot is 45 degrees\\n counter-clockwise from the horizontal axis (positive to the right).\\n\\n Use this if the arrows symbolize a quantity that is not based on\\n *X*, *Y* data coordinates.\\n\\n - 'xy': Arrows point from (x, y) to (x+u, y+v).\\n Use this for plotting a gradient field, for example.\\n\\n - Alternatively, arbitrary angles may be specified explicitly as an array\\n of values in degrees, counter-clockwise from the horizontal axis.\\n\\n In this case *U*, *V* is only used to determine the length of the\\n arrows.\\n\\n Note: inverting a data axis will correspondingly invert the\\n arrows only with ``angles='xy'``.\\n\\nscale : float, optional\\n Number of data units per arrow length unit, e.g., m\/s per plot width; a\\n smaller scale parameter makes the arrow longer. Default is *None*.\\n\\n If *None*, a simple autoscaling algorithm is used, based on the average\\n vector length and the number of vectors. The arrow length unit is given by\\n the *scale_units* parameter.\\n\\nscale_units : {'width', 'height', 'dots', 'inches', 'x', 'y', 'xy'}, optional\\n If the *scale* kwarg is *None*, the arrow length unit. Default is *None*.\\n\\n e.g. *scale_units* is 'inches', *scale* is 2.0, and ``(u, v) = (1, 0)``,\\n then the vector will be 0.5 inches long.\\n\\n If *scale_units* is 'width' or 'height', then the vector will be half the\\n width\/height of the axes.\\n\\n If *scale_units* is 'x' then the vector will be 0.5 x-axis\\n units. To plot vectors in the x-y plane, with u and v having\\n the same units as x and y, use\\n ``angles='xy', scale_units='xy', scale=1``.\\n\\nwidth : float, optional\\n Shaft width in arrow units; default depends on choice of units,\\n above, and number of vectors; a typical starting value is about\\n 0.005 times the width of the plot.\\n\\nheadwidth : float, default: 3\\n Head width as multiple of shaft width.\\n\\nheadlength : float, default: 5\\n Head length as multiple of shaft width.\\n\\nheadaxislength : float, default: 4.5\\n Head length at shaft intersection.\\n\\nminshaft : float, default: 1\\n Length below which arrow scales, in units of head length. Do not\\n set this to less than 1, or small arrows will look terrible!\\n\\nminlength : float, default: 1\\n Minimum length as a multiple of shaft width; if an arrow length\\n is less than this, plot a dot (hexagon) of this diameter instead.\\n\\npivot : {'tail', 'mid', 'middle', 'tip'}, default: 'tail'\\n The part of the arrow that is anchored to the *X*, *Y* grid. The arrow\\n rotates about this point.\\n\\n 'mid' is a synonym for 'middle'.\\n\\ncolor : color or color sequence, optional\\n Explicit color(s) for the arrows. If *C* has been set, *color* has no\\n effect.\\n\\n This is a synonym for the `.PolyCollection` *facecolor* parameter.\\n\\nOther Parameters\\n----------------\\ndata : indexable object, optional\\n DATA_PARAMETER_PLACEHOLDER\\n\\n**kwargs : `~matplotlib.collections.PolyCollection` properties, optional\\n All other keyword arguments are passed on to `.PolyCollection`:\\n\\n \\n .. table::\\n :class: property-table\\n\\n ================================================================================================= =====================================================================================================\\n Property Description \\n ================================================================================================= =====================================================================================================\\n :meth:`agg_filter <matplotlib.artist.Artist.set_agg_filter>` a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\\n :meth:`alpha <matplotlib.collections.Collection.set_alpha>` array-like or scalar or None \\n :meth:`animated <matplotlib.artist.Artist.set_animated>` bool \\n :meth:`antialiased <matplotlib.collections.Collection.set_antialiased>` or aa or antialiaseds bool or list of bools \\n :meth:`array <matplotlib.cm.ScalarMappable.set_array>` array-like or None \\n :meth:`capstyle <matplotlib.collections.Collection.set_capstyle>` `.CapStyle` or {'butt', 'projecting', 'round'} \\n :meth:`clim <matplotlib.cm.ScalarMappable.set_clim>` (vmin: float, vmax: float) \\n :meth:`clip_box <matplotlib.artist.Artist.set_clip_box>` `.Bbox` \\n :meth:`clip_on <matplotlib.artist.Artist.set_clip_on>` bool \\n :meth:`clip_path <matplotlib.artist.Artist.set_clip_path>` Patch or (Path, Transform) or None \\n :meth:`cmap <matplotlib.cm.ScalarMappable.set_cmap>` `.Colormap` or str or None \\n :meth:`color <matplotlib.collections.Collection.set_color>` color or list of rgba tuples \\n :meth:`edgecolor <matplotlib.collections.Collection.set_edgecolor>` or ec or edgecolors color or list of colors or 'face' \\n :meth:`facecolor <matplotlib.collections.Collection.set_facecolor>` or facecolors or fc color or list of colors \\n :meth:`figure <matplotlib.artist.Artist.set_figure>` `.Figure` \\n :meth:`gid <matplotlib.artist.Artist.set_gid>` str \\n :meth:`hatch <matplotlib.collections.Collection.set_hatch>` {'\/', '\\\\\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'} \\n :meth:`in_layout <matplotlib.artist.Artist.set_in_layout>` bool \\n :meth:`joinstyle <matplotlib.collections.Collection.set_joinstyle>` `.JoinStyle` or {'miter', 'round', 'bevel'} \\n :meth:`label <matplotlib.artist.Artist.set_label>` object \\n :meth:`linestyle <matplotlib.collections.Collection.set_linestyle>` or dashes or linestyles or ls str or tuple or list thereof \\n :meth:`linewidth <matplotlib.collections.Collection.set_linewidth>` or linewidths or lw float or list of floats \\n :meth:`norm <matplotlib.cm.ScalarMappable.set_norm>` `.Normalize` or None \\n :meth:`offset_transform <matplotlib.collections.Collection.set_offset_transform>` `.Transform` \\n :meth:`offsets <matplotlib.collections.Collection.set_offsets>` (N, 2) or (2,) array-like \\n :meth:`path_effects <matplotlib.artist.Artist.set_path_effects>` `.AbstractPathEffect` \\n :meth:`paths <matplotlib.collections.PolyCollection.set_verts>` list of array-like \\n :meth:`picker <matplotlib.artist.Artist.set_picker>` None or bool or float or callable \\n :meth:`pickradius <matplotlib.collections.Collection.set_pickradius>` float \\n :meth:`rasterized <matplotlib.artist.Artist.set_rasterized>` bool \\n :meth:`sizes <matplotlib.collections._CollectionWithSizes.set_sizes>` ndarray or None \\n :meth:`sketch_params <matplotlib.artist.Artist.set_sketch_params>` (scale: float, length: float, randomness: float) \\n :meth:`snap <matplotlib.artist.Artist.set_snap>` bool or None \\n :meth:`transform <matplotlib.artist.Artist.set_transform>` `.Transform` \\n :meth:`url <matplotlib.artist.Artist.set_url>` str \\n :meth:`urls <matplotlib.collections.Collection.set_urls>` list of str or None \\n :meth:`verts <matplotlib.collections.PolyCollection.set_verts>` list of array-like \\n :meth:`verts_and_codes <matplotlib.collections.PolyCollection.set_verts_and_codes>` unknown \\n :meth:`visible <matplotlib.artist.Artist.set_visible>` bool \\n :meth:`zorder <matplotlib.artist.Artist.set_zorder>` float \\n ================================================================================================= =====================================================================================================\\n\\n\\nReturns\\n-------\\n`~matplotlib.quiver.Quiver`\\n\\nSee Also\\n--------\\n.Axes.quiverkey : Add a key to a quiver plot.\\n\"","title":"matplotlib._as_gen.matplotlib.quiver.quiver#matplotlib.quiver.Quiver.quiver_doc"},{"text":"zorder=0","title":"matplotlib.collections_api#matplotlib.collections.CircleCollection.zorder"},{"text":"__hash__=None","title":"matplotlib.transformations#matplotlib.transforms.CompositeGenericTransform.__hash__"},{"text":"barbs_doc='\\nPlot a 2D field of barbs.\\n\\nCall signature::\\n\\n barbs([X, Y], U, V, [C], **kw)\\n\\nWhere *X*, *Y* define the barb locations, *U*, *V* define the barb\\ndirections, and *C* optionally sets the color.\\n\\nAll arguments may be 1D or 2D. *U*, *V*, *C* may be masked arrays, but masked\\n*X*, *Y* are not supported at present.\\n\\nBarbs are traditionally used in meteorology as a way to plot the speed\\nand direction of wind observations, but can technically be used to\\nplot any two dimensional vector quantity. As opposed to arrows, which\\ngive vector magnitude by the length of the arrow, the barbs give more\\nquantitative information about the vector magnitude by putting slanted\\nlines or a triangle for various increments in magnitude, as show\\nschematically below::\\n\\n : \/\\\\ \\\\\\n : \/ \\\\ \\\\\\n : \/ \\\\ \\\\ \\\\\\n : \/ \\\\ \\\\ \\\\\\n : ------------------------------\\n\\nThe largest increment is given by a triangle (or \"flag\"). After those\\ncome full lines (barbs). The smallest increment is a half line. There\\nis only, of course, ever at most 1 half line. If the magnitude is\\nsmall and only needs a single half-line and no full lines or\\ntriangles, the half-line is offset from the end of the barb so that it\\ncan be easily distinguished from barbs with a single full line. The\\nmagnitude for the barb shown above would nominally be 65, using the\\nstandard increments of 50, 10, and 5.\\n\\nSee also https:\/\/en.wikipedia.org\/wiki\/Wind_barb.\\n\\nParameters\\n----------\\nX, Y : 1D or 2D array-like, optional\\n The x and y coordinates of the barb locations. See *pivot* for how the\\n barbs are drawn to the x, y positions.\\n\\n If not given, they will be generated as a uniform integer meshgrid based\\n on the dimensions of *U* and *V*.\\n\\n If *X* and *Y* are 1D but *U*, *V* are 2D, *X*, *Y* are expanded to 2D\\n using ``X, Y = np.meshgrid(X, Y)``. In this case ``len(X)`` and ``len(Y)``\\n must match the column and row dimensions of *U* and *V*.\\n\\nU, V : 1D or 2D array-like\\n The x and y components of the barb shaft.\\n\\nC : 1D or 2D array-like, optional\\n Numeric data that defines the barb colors by colormapping via *norm* and\\n *cmap*.\\n\\n This does not support explicit colors. If you want to set colors directly,\\n use *barbcolor* instead.\\n\\nlength : float, default: 7\\n Length of the barb in points; the other parts of the barb\\n are scaled against this.\\n\\npivot : {\\'tip\\', \\'middle\\'} or float, default: \\'tip\\'\\n The part of the arrow that is anchored to the *X*, *Y* grid. The barb\\n rotates about this point. This can also be a number, which shifts the\\n start of the barb that many points away from grid point.\\n\\nbarbcolor : color or color sequence\\n The color of all parts of the barb except for the flags. This parameter\\n is analogous to the *edgecolor* parameter for polygons, which can be used\\n instead. However this parameter will override facecolor.\\n\\nflagcolor : color or color sequence\\n The color of any flags on the barb. This parameter is analogous to the\\n *facecolor* parameter for polygons, which can be used instead. However,\\n this parameter will override facecolor. If this is not set (and *C* has\\n not either) then *flagcolor* will be set to match *barbcolor* so that the\\n barb has a uniform color. If *C* has been set, *flagcolor* has no effect.\\n\\nsizes : dict, optional\\n A dictionary of coefficients specifying the ratio of a given\\n feature to the length of the barb. Only those values one wishes to\\n override need to be included. These features include:\\n\\n - \\'spacing\\' - space between features (flags, full\/half barbs)\\n - \\'height\\' - height (distance from shaft to top) of a flag or full barb\\n - \\'width\\' - width of a flag, twice the width of a full barb\\n - \\'emptybarb\\' - radius of the circle used for low magnitudes\\n\\nfill_empty : bool, default: False\\n Whether the empty barbs (circles) that are drawn should be filled with\\n the flag color. If they are not filled, the center is transparent.\\n\\nrounding : bool, default: True\\n Whether the vector magnitude should be rounded when allocating barb\\n components. If True, the magnitude is rounded to the nearest multiple\\n of the half-barb increment. If False, the magnitude is simply truncated\\n to the next lowest multiple.\\n\\nbarb_increments : dict, optional\\n A dictionary of increments specifying values to associate with\\n different parts of the barb. Only those values one wishes to\\n override need to be included.\\n\\n - \\'half\\' - half barbs (Default is 5)\\n - \\'full\\' - full barbs (Default is 10)\\n - \\'flag\\' - flags (default is 50)\\n\\nflip_barb : bool or array-like of bool, default: False\\n Whether the lines and flags should point opposite to normal.\\n Normal behavior is for the barbs and lines to point right (comes from wind\\n barbs having these features point towards low pressure in the Northern\\n Hemisphere).\\n\\n A single value is applied to all barbs. Individual barbs can be flipped by\\n passing a bool array of the same size as *U* and *V*.\\n\\nReturns\\n-------\\nbarbs : `~matplotlib.quiver.Barbs`\\n\\nOther Parameters\\n----------------\\ndata : indexable object, optional\\n DATA_PARAMETER_PLACEHOLDER\\n\\n**kwargs\\n The barbs can further be customized using `.PolyCollection` keyword\\n arguments:\\n\\n \\n .. table::\\n :class: property-table\\n\\n ================================================================================================= =====================================================================================================\\n Property Description \\n ================================================================================================= =====================================================================================================\\n :meth:`agg_filter <matplotlib.artist.Artist.set_agg_filter>` a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\\n :meth:`alpha <matplotlib.collections.Collection.set_alpha>` array-like or scalar or None \\n :meth:`animated <matplotlib.artist.Artist.set_animated>` bool \\n :meth:`antialiased <matplotlib.collections.Collection.set_antialiased>` or aa or antialiaseds bool or list of bools \\n :meth:`array <matplotlib.cm.ScalarMappable.set_array>` array-like or None \\n :meth:`capstyle <matplotlib.collections.Collection.set_capstyle>` `.CapStyle` or {\\'butt\\', \\'projecting\\', \\'round\\'} \\n :meth:`clim <matplotlib.cm.ScalarMappable.set_clim>` (vmin: float, vmax: float) \\n :meth:`clip_box <matplotlib.artist.Artist.set_clip_box>` `.Bbox` \\n :meth:`clip_on <matplotlib.artist.Artist.set_clip_on>` bool \\n :meth:`clip_path <matplotlib.artist.Artist.set_clip_path>` Patch or (Path, Transform) or None \\n :meth:`cmap <matplotlib.cm.ScalarMappable.set_cmap>` `.Colormap` or str or None \\n :meth:`color <matplotlib.collections.Collection.set_color>` color or list of rgba tuples \\n :meth:`edgecolor <matplotlib.collections.Collection.set_edgecolor>` or ec or edgecolors color or list of colors or \\'face\\' \\n :meth:`facecolor <matplotlib.collections.Collection.set_facecolor>` or facecolors or fc color or list of colors \\n :meth:`figure <matplotlib.artist.Artist.set_figure>` `.Figure` \\n :meth:`gid <matplotlib.artist.Artist.set_gid>` str \\n :meth:`hatch <matplotlib.collections.Collection.set_hatch>` {\\'\/\\', \\'\\\\\\\\\\', \\'|\\', \\'-\\', \\'+\\', \\'x\\', \\'o\\', \\'O\\', \\'.\\', \\'*\\'} \\n :meth:`in_layout <matplotlib.artist.Artist.set_in_layout>` bool \\n :meth:`joinstyle <matplotlib.collections.Collection.set_joinstyle>` `.JoinStyle` or {\\'miter\\', \\'round\\', \\'bevel\\'} \\n :meth:`label <matplotlib.artist.Artist.set_label>` object \\n :meth:`linestyle <matplotlib.collections.Collection.set_linestyle>` or dashes or linestyles or ls str or tuple or list thereof \\n :meth:`linewidth <matplotlib.collections.Collection.set_linewidth>` or linewidths or lw float or list of floats \\n :meth:`norm <matplotlib.cm.ScalarMappable.set_norm>` `.Normalize` or None \\n :meth:`offset_transform <matplotlib.collections.Collection.set_offset_transform>` `.Transform` \\n :meth:`offsets <matplotlib.collections.Collection.set_offsets>` (N, 2) or (2,) array-like \\n :meth:`path_effects <matplotlib.artist.Artist.set_path_effects>` `.AbstractPathEffect` \\n :meth:`paths <matplotlib.collections.PolyCollection.set_verts>` list of array-like \\n :meth:`picker <matplotlib.artist.Artist.set_picker>` None or bool or float or callable \\n :meth:`pickradius <matplotlib.collections.Collection.set_pickradius>` float \\n :meth:`rasterized <matplotlib.artist.Artist.set_rasterized>` bool \\n :meth:`sizes <matplotlib.collections._CollectionWithSizes.set_sizes>` ndarray or None \\n :meth:`sketch_params <matplotlib.artist.Artist.set_sketch_params>` (scale: float, length: float, randomness: float) \\n :meth:`snap <matplotlib.artist.Artist.set_snap>` bool or None \\n :meth:`transform <matplotlib.artist.Artist.set_transform>` `.Transform` \\n :meth:`url <matplotlib.artist.Artist.set_url>` str \\n :meth:`urls <matplotlib.collections.Collection.set_urls>` list of str or None \\n :meth:`verts <matplotlib.collections.PolyCollection.set_verts>` list of array-like \\n :meth:`verts_and_codes <matplotlib.collections.PolyCollection.set_verts_and_codes>` unknown \\n :meth:`visible <matplotlib.artist.Artist.set_visible>` bool \\n :meth:`zorder <matplotlib.artist.Artist.set_zorder>` float \\n ================================================================================================= =====================================================================================================\\n\\n'","title":"matplotlib._as_gen.matplotlib.quiver.barbs#matplotlib.quiver.Barbs.barbs_doc"},{"text":"axis=None","title":"matplotlib.ticker_api#matplotlib.ticker.TickHelper.axis"},{"text":"widths","title":"matplotlib.dviread#matplotlib.dviread.DviFont.widths"},{"text":"matplotlib.axes.Axes.zorder   Axes.zorder=0","title":"matplotlib._as_gen.matplotlib.axes.axes.zorder"},{"text":"matplotlib.artist.Artist.zorder   Artist.zorder=0","title":"matplotlib._as_gen.matplotlib.artist.artist.zorder"}]}
{"task_id":32063985,"prompt":"def f_32063985(soup):\n\treturn ","suffix":"","canonical_solution":"soup.find('div', id='main-content').decompose()","test_start":"\nfrom bs4 import BeautifulSoup\n\ndef check(candidate):","test":["\n    markup = \"<a>This is not div <div>This is div 1<\/div><div id='main-content'>This is div 2<\/div><\/a>\"\n    soup = BeautifulSoup(markup,\"html.parser\")\n    candidate(soup)\n    assert str(soup) == '<a>This is not div <div>This is div 1<\/div><\/a>'\n"],"entry_point":"f_32063985","intent":"remove a div from `soup` with a id `main-content` using beautifulsoup","library":["bs4"],"docs":[]}
{"task_id":27975069,"prompt":"def f_27975069(df):\n\treturn ","suffix":"","canonical_solution":"df[df['ids'].str.contains('ball')]","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    f = pd.DataFrame([[\"ball1\", 1, 2], [\"hall\", 5, 4]], columns = ['ids', 'x', 'y'])\n    f1 = candidate(f)\n    assert f1['x'][0] == 1\n    assert f1['y'][0] == 2\n"],"entry_point":"f_27975069","intent":"filter rows of datafram `df` containing key word `ball` in column `ids`","library":["pandas"],"docs":[{"text":"kevent.data  \nFilter specific data.","title":"python.library.select#select.kevent.data"},{"text":"pandas.MultiIndex.codes   propertyMultiIndex.codes","title":"pandas.reference.api.pandas.multiindex.codes"},{"text":"pandas.DataFrame.filter   DataFrame.filter(items=None, like=None, regex=None, axis=None)[source]\n \nSubset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index.  Parameters \n \nitems:list-like\n\n\nKeep labels from axis which are in items.  \nlike:str\n\n\nKeep labels from axis for which \u201clike in label == True\u201d.  \nregex:str (regular expression)\n\n\nKeep labels from axis for which re.search(regex, label) == True.  \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default None\n\n\nThe axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, \u2018index\u2019 for Series, \u2018columns\u2019 for DataFrame.    Returns \n same type as input object\n    See also  DataFrame.loc\n\nAccess a group of rows and columns by label(s) or a boolean array.    Notes The items, like, and regex parameters are enforced to be mutually exclusive. axis defaults to the info axis that is used when indexing with []. Examples \n>>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),\n...                   index=['mouse', 'rabbit'],\n...                   columns=['one', 'two', 'three'])\n>>> df\n        one  two  three\nmouse     1    2      3\nrabbit    4    5      6\n  \n>>> # select columns by name\n>>> df.filter(items=['one', 'three'])\n         one  three\nmouse     1      3\nrabbit    4      6\n  \n>>> # select columns by regular expression\n>>> df.filter(regex='e$', axis=1)\n         one  three\nmouse     1      3\nrabbit    4      6\n  \n>>> # select rows containing 'bbi'\n>>> df.filter(like='bbi', axis=0)\n         one  two  three\nrabbit    4    5      6","title":"pandas.reference.api.pandas.dataframe.filter"},{"text":"pandas.Index.names   propertyIndex.names","title":"pandas.reference.api.pandas.index.names"},{"text":"colno  \nThe column corresponding to pos (may be None).","title":"python.library.re#re.error.colno"},{"text":"property df","title":"torch.distributions#torch.distributions.chi2.Chi2.df"},{"text":"pandas.Series.filter   Series.filter(items=None, like=None, regex=None, axis=None)[source]\n \nSubset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index.  Parameters \n \nitems:list-like\n\n\nKeep labels from axis which are in items.  \nlike:str\n\n\nKeep labels from axis for which \u201clike in label == True\u201d.  \nregex:str (regular expression)\n\n\nKeep labels from axis for which re.search(regex, label) == True.  \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default None\n\n\nThe axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, \u2018index\u2019 for Series, \u2018columns\u2019 for DataFrame.    Returns \n same type as input object\n    See also  DataFrame.loc\n\nAccess a group of rows and columns by label(s) or a boolean array.    Notes The items, like, and regex parameters are enforced to be mutually exclusive. axis defaults to the info axis that is used when indexing with []. Examples \n>>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),\n...                   index=['mouse', 'rabbit'],\n...                   columns=['one', 'two', 'three'])\n>>> df\n        one  two  three\nmouse     1    2      3\nrabbit    4    5      6\n  \n>>> # select columns by name\n>>> df.filter(items=['one', 'three'])\n         one  three\nmouse     1      3\nrabbit    4      6\n  \n>>> # select columns by regular expression\n>>> df.filter(regex='e$', axis=1)\n         one  three\nmouse     1      3\nrabbit    4      6\n  \n>>> # select rows containing 'bbi'\n>>> df.filter(like='bbi', axis=0)\n         one  two  three\nrabbit    4    5      6","title":"pandas.reference.api.pandas.series.filter"},{"text":"pandas.tseries.offsets.Nano.nanos   Nano.nanos","title":"pandas.reference.api.pandas.tseries.offsets.nano.nanos"},{"text":"colno  \nThe column corresponding to pos.","title":"python.library.json#json.JSONDecodeError.colno"},{"text":"contains(other)","title":"django.ref.contrib.gis.gdal#django.contrib.gis.gdal.OGRGeometry.contains"}]}
{"task_id":20461165,"prompt":"def f_20461165(df):\n\treturn ","suffix":"","canonical_solution":"df.reset_index(level=0, inplace=True)","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame([[384, 593], [781, 123]], columns = ['gi', 'ptt_loc'])\n    candidate(df)\n    assert df['index'][0] == 0\n    assert df['index'][1] == 1\n"],"entry_point":"f_20461165","intent":"convert index at level 0 into a column in dataframe `df`","library":["pandas"],"docs":[{"text":"pandas.MultiIndex.levels   MultiIndex.levels","title":"pandas.reference.api.pandas.multiindex.levels"},{"text":"pandas.Index.empty   propertyIndex.empty","title":"pandas.reference.api.pandas.index.empty"},{"text":"pandas.DataFrame.to_records   DataFrame.to_records(index=True, column_dtypes=None, index_dtypes=None)[source]\n \nConvert DataFrame to a NumPy record array. Index will be included as the first field of the record array if requested.  Parameters \n \nindex:bool, default True\n\n\nInclude index in resulting record array, stored in \u2018index\u2019 field or using the index label, if set.  \ncolumn_dtypes:str, type, dict, default None\n\n\nIf a string or type, the data type to store all columns. If a dictionary, a mapping of column names and indices (zero-indexed) to specific data types.  \nindex_dtypes:str, type, dict, default None\n\n\nIf a string or type, the data type to store all index levels. If a dictionary, a mapping of index level names and indices (zero-indexed) to specific data types. This mapping is applied only if index=True.    Returns \n numpy.recarray\n\nNumPy ndarray with the DataFrame labels as fields and each row of the DataFrame as entries.      See also  DataFrame.from_records\n\nConvert structured or record ndarray to DataFrame.  numpy.recarray\n\nAn ndarray that allows field access using attributes, analogous to typed columns in a spreadsheet.    Examples \n>>> df = pd.DataFrame({'A': [1, 2], 'B': [0.5, 0.75]},\n...                   index=['a', 'b'])\n>>> df\n   A     B\na  1  0.50\nb  2  0.75\n>>> df.to_records()\nrec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n          dtype=[('index', 'O'), ('A', '<i8'), ('B', '<f8')])\n  If the DataFrame index has no label then the recarray field name is set to \u2018index\u2019. If the index has a label then this is used as the field name: \n>>> df.index = df.index.rename(\"I\")\n>>> df.to_records()\nrec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n          dtype=[('I', 'O'), ('A', '<i8'), ('B', '<f8')])\n  The index can be excluded from the record array: \n>>> df.to_records(index=False)\nrec.array([(1, 0.5 ), (2, 0.75)],\n          dtype=[('A', '<i8'), ('B', '<f8')])\n  Data types can be specified for the columns: \n>>> df.to_records(column_dtypes={\"A\": \"int32\"})\nrec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n          dtype=[('I', 'O'), ('A', '<i4'), ('B', '<f8')])\n  As well as for the index: \n>>> df.to_records(index_dtypes=\"<S2\")\nrec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)],\n          dtype=[('I', 'S2'), ('A', '<i8'), ('B', '<f8')])\n  \n>>> index_dtypes = f\"<S{df.index.str.len().max()}\"\n>>> df.to_records(index_dtypes=index_dtypes)\nrec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)],\n          dtype=[('I', 'S1'), ('A', '<i8'), ('B', '<f8')])","title":"pandas.reference.api.pandas.dataframe.to_records"},{"text":"pandas.Index.view   Index.view(cls=None)[source]","title":"pandas.reference.api.pandas.index.view"},{"text":"pandas.Index.nlevels   propertyIndex.nlevels\n \nNumber of levels.","title":"pandas.reference.api.pandas.index.nlevels"},{"text":"pandas.Index.to_frame   Index.to_frame(index=True, name=NoDefault.no_default)[source]\n \nCreate a DataFrame with a column containing the Index.  Parameters \n \nindex:bool, default True\n\n\nSet the index of the returned DataFrame as the original Index.  \nname:object, default None\n\n\nThe passed name should substitute for the index name (if it has one).    Returns \n DataFrame\n\nDataFrame containing the original Index data.      See also  Index.to_series\n\nConvert an Index to a Series.  Series.to_frame\n\nConvert Series to DataFrame.    Examples \n>>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n>>> idx.to_frame()\n       animal\nanimal\nAnt       Ant\nBear     Bear\nCow       Cow\n  By default, the original Index is reused. To enforce a new Index: \n>>> idx.to_frame(index=False)\n    animal\n0   Ant\n1  Bear\n2   Cow\n  To override the name of the resulting column, specify name: \n>>> idx.to_frame(index=False, name='zoo')\n    zoo\n0   Ant\n1  Bear\n2   Cow","title":"pandas.reference.api.pandas.index.to_frame"},{"text":"pandas.IntervalIndex.left   IntervalIndex.left","title":"pandas.reference.api.pandas.intervalindex.left"},{"text":"pandas.Index.names   propertyIndex.names","title":"pandas.reference.api.pandas.index.names"},{"text":"colno  \nThe column corresponding to pos (may be None).","title":"python.library.re#re.error.colno"},{"text":"pandas.IntervalIndex.mid   IntervalIndex.mid","title":"pandas.reference.api.pandas.intervalindex.mid"}]}
{"task_id":20461165,"prompt":"def f_20461165(df):\n\t","suffix":"\n\treturn ","canonical_solution":"df['index1'] = df.index","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame([[384, 593], [781, 123]], columns = ['gi', 'ptt_loc'])\n    candidate(df)\n    assert df['index1'][0] == 0\n    assert df['index1'][1] == 1\n"],"entry_point":"f_20461165","intent":"Add indexes in a data frame `df` to a column `index1`","library":["pandas"],"docs":[{"text":"pandas.DataFrame.index   DataFrame.index\n \nThe index (row labels) of the DataFrame.","title":"pandas.reference.api.pandas.dataframe.index"},{"text":"pandas.tseries.offsets.BQuarterEnd.apply_index   BQuarterEnd.apply_index(other)","title":"pandas.reference.api.pandas.tseries.offsets.bquarterend.apply_index"},{"text":"pandas.Index.names   propertyIndex.names","title":"pandas.reference.api.pandas.index.names"},{"text":"pandas.tseries.offsets.BYearEnd.apply_index   BYearEnd.apply_index(other)","title":"pandas.reference.api.pandas.tseries.offsets.byearend.apply_index"},{"text":"pandas.tseries.offsets.Day.apply_index   Day.apply_index(other)","title":"pandas.reference.api.pandas.tseries.offsets.day.apply_index"},{"text":"pandas.tseries.offsets.Week.apply_index   Week.apply_index(other)","title":"pandas.reference.api.pandas.tseries.offsets.week.apply_index"},{"text":"pandas.tseries.offsets.BYearBegin.apply_index   BYearBegin.apply_index(other)","title":"pandas.reference.api.pandas.tseries.offsets.byearbegin.apply_index"},{"text":"pandas.tseries.offsets.Tick.apply_index   Tick.apply_index(other)","title":"pandas.reference.api.pandas.tseries.offsets.tick.apply_index"},{"text":"pandas.tseries.offsets.YearEnd.apply_index   YearEnd.apply_index(other)","title":"pandas.reference.api.pandas.tseries.offsets.yearend.apply_index"},{"text":"pandas.tseries.offsets.BQuarterBegin.apply_index   BQuarterBegin.apply_index(other)","title":"pandas.reference.api.pandas.tseries.offsets.bquarterbegin.apply_index"}]}
{"task_id":20461165,"prompt":"def f_20461165(df):\n\treturn ","suffix":"","canonical_solution":"df.reset_index(level=['tick', 'obs'])","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame([['2016-09-13', 'C', 2, 0.0139], ['2016-07-17', 'A', 2, 0.5577]], columns = ['tick', 'tag', 'obs', 'val'])\n    df = df.set_index(['tick', 'tag', 'obs'])\n    df = candidate(df)\n    assert df['tick']['C'] == '2016-09-13'\n"],"entry_point":"f_20461165","intent":"convert pandas index in a dataframe `df` to columns","library":["pandas"],"docs":[{"text":"pandas.DataFrame.columns   DataFrame.columns\n \nThe column labels of the DataFrame.","title":"pandas.reference.api.pandas.dataframe.columns"},{"text":"pandas.Index.T   propertyIndex.T\n \nReturn the transpose, which is by definition self.","title":"pandas.reference.api.pandas.index.t"},{"text":"pandas.DataFrame.to_records   DataFrame.to_records(index=True, column_dtypes=None, index_dtypes=None)[source]\n \nConvert DataFrame to a NumPy record array. Index will be included as the first field of the record array if requested.  Parameters \n \nindex:bool, default True\n\n\nInclude index in resulting record array, stored in \u2018index\u2019 field or using the index label, if set.  \ncolumn_dtypes:str, type, dict, default None\n\n\nIf a string or type, the data type to store all columns. If a dictionary, a mapping of column names and indices (zero-indexed) to specific data types.  \nindex_dtypes:str, type, dict, default None\n\n\nIf a string or type, the data type to store all index levels. If a dictionary, a mapping of index level names and indices (zero-indexed) to specific data types. This mapping is applied only if index=True.    Returns \n numpy.recarray\n\nNumPy ndarray with the DataFrame labels as fields and each row of the DataFrame as entries.      See also  DataFrame.from_records\n\nConvert structured or record ndarray to DataFrame.  numpy.recarray\n\nAn ndarray that allows field access using attributes, analogous to typed columns in a spreadsheet.    Examples \n>>> df = pd.DataFrame({'A': [1, 2], 'B': [0.5, 0.75]},\n...                   index=['a', 'b'])\n>>> df\n   A     B\na  1  0.50\nb  2  0.75\n>>> df.to_records()\nrec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n          dtype=[('index', 'O'), ('A', '<i8'), ('B', '<f8')])\n  If the DataFrame index has no label then the recarray field name is set to \u2018index\u2019. If the index has a label then this is used as the field name: \n>>> df.index = df.index.rename(\"I\")\n>>> df.to_records()\nrec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n          dtype=[('I', 'O'), ('A', '<i8'), ('B', '<f8')])\n  The index can be excluded from the record array: \n>>> df.to_records(index=False)\nrec.array([(1, 0.5 ), (2, 0.75)],\n          dtype=[('A', '<i8'), ('B', '<f8')])\n  Data types can be specified for the columns: \n>>> df.to_records(column_dtypes={\"A\": \"int32\"})\nrec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n          dtype=[('I', 'O'), ('A', '<i4'), ('B', '<f8')])\n  As well as for the index: \n>>> df.to_records(index_dtypes=\"<S2\")\nrec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)],\n          dtype=[('I', 'S2'), ('A', '<i8'), ('B', '<f8')])\n  \n>>> index_dtypes = f\"<S{df.index.str.len().max()}\"\n>>> df.to_records(index_dtypes=index_dtypes)\nrec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)],\n          dtype=[('I', 'S1'), ('A', '<i8'), ('B', '<f8')])","title":"pandas.reference.api.pandas.dataframe.to_records"},{"text":"pandas.DataFrame.index   DataFrame.index\n \nThe index (row labels) of the DataFrame.","title":"pandas.reference.api.pandas.dataframe.index"},{"text":"pandas.Index.names   propertyIndex.names","title":"pandas.reference.api.pandas.index.names"},{"text":"pandas.Index.transpose   Index.transpose(*args, **kwargs)[source]\n \nReturn the transpose, which is by definition self.  Returns \n %(klass)s","title":"pandas.reference.api.pandas.index.transpose"},{"text":"DataFrame  Constructor       \nDataFrame([data, index, columns, dtype, copy]) Two-dimensional, size-mutable, potentially heterogeneous tabular data.      Attributes and underlying data Axes       \nDataFrame.index The index (row labels) of the DataFrame.  \nDataFrame.columns The column labels of the DataFrame.          \nDataFrame.dtypes Return the dtypes in the DataFrame.  \nDataFrame.info([verbose, buf, max_cols, ...]) Print a concise summary of a DataFrame.  \nDataFrame.select_dtypes([include, exclude]) Return a subset of the DataFrame's columns based on the column dtypes.  \nDataFrame.values Return a Numpy representation of the DataFrame.  \nDataFrame.axes Return a list representing the axes of the DataFrame.  \nDataFrame.ndim Return an int representing the number of axes \/ array dimensions.  \nDataFrame.size Return an int representing the number of elements in this object.  \nDataFrame.shape Return a tuple representing the dimensionality of the DataFrame.  \nDataFrame.memory_usage([index, deep]) Return the memory usage of each column in bytes.  \nDataFrame.empty Indicator whether Series\/DataFrame is empty.  \nDataFrame.set_flags(*[, copy, ...]) Return a new object with updated flags.      Conversion       \nDataFrame.astype(dtype[, copy, errors]) Cast a pandas object to a specified dtype dtype.  \nDataFrame.convert_dtypes([infer_objects, ...]) Convert columns to best possible dtypes using dtypes supporting pd.NA.  \nDataFrame.infer_objects() Attempt to infer better dtypes for object columns.  \nDataFrame.copy([deep]) Make a copy of this object's indices and data.  \nDataFrame.bool() Return the bool of a single element Series or DataFrame.      Indexing, iteration       \nDataFrame.head([n]) Return the first n rows.  \nDataFrame.at Access a single value for a row\/column label pair.  \nDataFrame.iat Access a single value for a row\/column pair by integer position.  \nDataFrame.loc Access a group of rows and columns by label(s) or a boolean array.  \nDataFrame.iloc Purely integer-location based indexing for selection by position.  \nDataFrame.insert(loc, column, value[, ...]) Insert column into DataFrame at specified location.  \nDataFrame.__iter__() Iterate over info axis.  \nDataFrame.items() Iterate over (column name, Series) pairs.  \nDataFrame.iteritems() Iterate over (column name, Series) pairs.  \nDataFrame.keys() Get the 'info axis' (see Indexing for more).  \nDataFrame.iterrows() Iterate over DataFrame rows as (index, Series) pairs.  \nDataFrame.itertuples([index, name]) Iterate over DataFrame rows as namedtuples.  \nDataFrame.lookup(row_labels, col_labels) (DEPRECATED) Label-based \"fancy indexing\" function for DataFrame.  \nDataFrame.pop(item) Return item and drop from frame.  \nDataFrame.tail([n]) Return the last n rows.  \nDataFrame.xs(key[, axis, level, drop_level]) Return cross-section from the Series\/DataFrame.  \nDataFrame.get(key[, default]) Get item from object for given key (ex: DataFrame column).  \nDataFrame.isin(values) Whether each element in the DataFrame is contained in values.  \nDataFrame.where(cond[, other, inplace, ...]) Replace values where the condition is False.  \nDataFrame.mask(cond[, other, inplace, axis, ...]) Replace values where the condition is True.  \nDataFrame.query(expr[, inplace]) Query the columns of a DataFrame with a boolean expression.    For more information on .at, .iat, .loc, and .iloc, see the indexing documentation.   Binary operator functions       \nDataFrame.add(other[, axis, level, fill_value]) Get Addition of dataframe and other, element-wise (binary operator add).  \nDataFrame.sub(other[, axis, level, fill_value]) Get Subtraction of dataframe and other, element-wise (binary operator sub).  \nDataFrame.mul(other[, axis, level, fill_value]) Get Multiplication of dataframe and other, element-wise (binary operator mul).  \nDataFrame.div(other[, axis, level, fill_value]) Get Floating division of dataframe and other, element-wise (binary operator truediv).  \nDataFrame.truediv(other[, axis, level, ...]) Get Floating division of dataframe and other, element-wise (binary operator truediv).  \nDataFrame.floordiv(other[, axis, level, ...]) Get Integer division of dataframe and other, element-wise (binary operator floordiv).  \nDataFrame.mod(other[, axis, level, fill_value]) Get Modulo of dataframe and other, element-wise (binary operator mod).  \nDataFrame.pow(other[, axis, level, fill_value]) Get Exponential power of dataframe and other, element-wise (binary operator pow).  \nDataFrame.dot(other) Compute the matrix multiplication between the DataFrame and other.  \nDataFrame.radd(other[, axis, level, fill_value]) Get Addition of dataframe and other, element-wise (binary operator radd).  \nDataFrame.rsub(other[, axis, level, fill_value]) Get Subtraction of dataframe and other, element-wise (binary operator rsub).  \nDataFrame.rmul(other[, axis, level, fill_value]) Get Multiplication of dataframe and other, element-wise (binary operator rmul).  \nDataFrame.rdiv(other[, axis, level, fill_value]) Get Floating division of dataframe and other, element-wise (binary operator rtruediv).  \nDataFrame.rtruediv(other[, axis, level, ...]) Get Floating division of dataframe and other, element-wise (binary operator rtruediv).  \nDataFrame.rfloordiv(other[, axis, level, ...]) Get Integer division of dataframe and other, element-wise (binary operator rfloordiv).  \nDataFrame.rmod(other[, axis, level, fill_value]) Get Modulo of dataframe and other, element-wise (binary operator rmod).  \nDataFrame.rpow(other[, axis, level, fill_value]) Get Exponential power of dataframe and other, element-wise (binary operator rpow).  \nDataFrame.lt(other[, axis, level]) Get Less than of dataframe and other, element-wise (binary operator lt).  \nDataFrame.gt(other[, axis, level]) Get Greater than of dataframe and other, element-wise (binary operator gt).  \nDataFrame.le(other[, axis, level]) Get Less than or equal to of dataframe and other, element-wise (binary operator le).  \nDataFrame.ge(other[, axis, level]) Get Greater than or equal to of dataframe and other, element-wise (binary operator ge).  \nDataFrame.ne(other[, axis, level]) Get Not equal to of dataframe and other, element-wise (binary operator ne).  \nDataFrame.eq(other[, axis, level]) Get Equal to of dataframe and other, element-wise (binary operator eq).  \nDataFrame.combine(other, func[, fill_value, ...]) Perform column-wise combine with another DataFrame.  \nDataFrame.combine_first(other) Update null elements with value in the same location in other.      Function application, GroupBy & window       \nDataFrame.apply(func[, axis, raw, ...]) Apply a function along an axis of the DataFrame.  \nDataFrame.applymap(func[, na_action]) Apply a function to a Dataframe elementwise.  \nDataFrame.pipe(func, *args, **kwargs) Apply chainable functions that expect Series or DataFrames.  \nDataFrame.agg([func, axis]) Aggregate using one or more operations over the specified axis.  \nDataFrame.aggregate([func, axis]) Aggregate using one or more operations over the specified axis.  \nDataFrame.transform(func[, axis]) Call func on self producing a DataFrame with the same axis shape as self.  \nDataFrame.groupby([by, axis, level, ...]) Group DataFrame using a mapper or by a Series of columns.  \nDataFrame.rolling(window[, min_periods, ...]) Provide rolling window calculations.  \nDataFrame.expanding([min_periods, center, ...]) Provide expanding window calculations.  \nDataFrame.ewm([com, span, halflife, alpha, ...]) Provide exponentially weighted (EW) calculations.      Computations \/ descriptive stats       \nDataFrame.abs() Return a Series\/DataFrame with absolute numeric value of each element.  \nDataFrame.all([axis, bool_only, skipna, level]) Return whether all elements are True, potentially over an axis.  \nDataFrame.any([axis, bool_only, skipna, level]) Return whether any element is True, potentially over an axis.  \nDataFrame.clip([lower, upper, axis, inplace]) Trim values at input threshold(s).  \nDataFrame.corr([method, min_periods]) Compute pairwise correlation of columns, excluding NA\/null values.  \nDataFrame.corrwith(other[, axis, drop, method]) Compute pairwise correlation.  \nDataFrame.count([axis, level, numeric_only]) Count non-NA cells for each column or row.  \nDataFrame.cov([min_periods, ddof]) Compute pairwise covariance of columns, excluding NA\/null values.  \nDataFrame.cummax([axis, skipna]) Return cumulative maximum over a DataFrame or Series axis.  \nDataFrame.cummin([axis, skipna]) Return cumulative minimum over a DataFrame or Series axis.  \nDataFrame.cumprod([axis, skipna]) Return cumulative product over a DataFrame or Series axis.  \nDataFrame.cumsum([axis, skipna]) Return cumulative sum over a DataFrame or Series axis.  \nDataFrame.describe([percentiles, include, ...]) Generate descriptive statistics.  \nDataFrame.diff([periods, axis]) First discrete difference of element.  \nDataFrame.eval(expr[, inplace]) Evaluate a string describing operations on DataFrame columns.  \nDataFrame.kurt([axis, skipna, level, ...]) Return unbiased kurtosis over requested axis.  \nDataFrame.kurtosis([axis, skipna, level, ...]) Return unbiased kurtosis over requested axis.  \nDataFrame.mad([axis, skipna, level]) Return the mean absolute deviation of the values over the requested axis.  \nDataFrame.max([axis, skipna, level, ...]) Return the maximum of the values over the requested axis.  \nDataFrame.mean([axis, skipna, level, ...]) Return the mean of the values over the requested axis.  \nDataFrame.median([axis, skipna, level, ...]) Return the median of the values over the requested axis.  \nDataFrame.min([axis, skipna, level, ...]) Return the minimum of the values over the requested axis.  \nDataFrame.mode([axis, numeric_only, dropna]) Get the mode(s) of each element along the selected axis.  \nDataFrame.pct_change([periods, fill_method, ...]) Percentage change between the current and a prior element.  \nDataFrame.prod([axis, skipna, level, ...]) Return the product of the values over the requested axis.  \nDataFrame.product([axis, skipna, level, ...]) Return the product of the values over the requested axis.  \nDataFrame.quantile([q, axis, numeric_only, ...]) Return values at the given quantile over requested axis.  \nDataFrame.rank([axis, method, numeric_only, ...]) Compute numerical data ranks (1 through n) along axis.  \nDataFrame.round([decimals]) Round a DataFrame to a variable number of decimal places.  \nDataFrame.sem([axis, skipna, level, ddof, ...]) Return unbiased standard error of the mean over requested axis.  \nDataFrame.skew([axis, skipna, level, ...]) Return unbiased skew over requested axis.  \nDataFrame.sum([axis, skipna, level, ...]) Return the sum of the values over the requested axis.  \nDataFrame.std([axis, skipna, level, ddof, ...]) Return sample standard deviation over requested axis.  \nDataFrame.var([axis, skipna, level, ddof, ...]) Return unbiased variance over requested axis.  \nDataFrame.nunique([axis, dropna]) Count number of distinct elements in specified axis.  \nDataFrame.value_counts([subset, normalize, ...]) Return a Series containing counts of unique rows in the DataFrame.      Reindexing \/ selection \/ label manipulation       \nDataFrame.add_prefix(prefix) Prefix labels with string prefix.  \nDataFrame.add_suffix(suffix) Suffix labels with string suffix.  \nDataFrame.align(other[, join, axis, level, ...]) Align two objects on their axes with the specified join method.  \nDataFrame.at_time(time[, asof, axis]) Select values at particular time of day (e.g., 9:30AM).  \nDataFrame.between_time(start_time, end_time) Select values between particular times of the day (e.g., 9:00-9:30 AM).  \nDataFrame.drop([labels, axis, index, ...]) Drop specified labels from rows or columns.  \nDataFrame.drop_duplicates([subset, keep, ...]) Return DataFrame with duplicate rows removed.  \nDataFrame.duplicated([subset, keep]) Return boolean Series denoting duplicate rows.  \nDataFrame.equals(other) Test whether two objects contain the same elements.  \nDataFrame.filter([items, like, regex, axis]) Subset the dataframe rows or columns according to the specified index labels.  \nDataFrame.first(offset) Select initial periods of time series data based on a date offset.  \nDataFrame.head([n]) Return the first n rows.  \nDataFrame.idxmax([axis, skipna]) Return index of first occurrence of maximum over requested axis.  \nDataFrame.idxmin([axis, skipna]) Return index of first occurrence of minimum over requested axis.  \nDataFrame.last(offset) Select final periods of time series data based on a date offset.  \nDataFrame.reindex([labels, index, columns, ...]) Conform Series\/DataFrame to new index with optional filling logic.  \nDataFrame.reindex_like(other[, method, ...]) Return an object with matching indices as other object.  \nDataFrame.rename([mapper, index, columns, ...]) Alter axes labels.  \nDataFrame.rename_axis([mapper, index, ...]) Set the name of the axis for the index or columns.  \nDataFrame.reset_index([level, drop, ...]) Reset the index, or a level of it.  \nDataFrame.sample([n, frac, replace, ...]) Return a random sample of items from an axis of object.  \nDataFrame.set_axis(labels[, axis, inplace]) Assign desired index to given axis.  \nDataFrame.set_index(keys[, drop, append, ...]) Set the DataFrame index using existing columns.  \nDataFrame.tail([n]) Return the last n rows.  \nDataFrame.take(indices[, axis, is_copy]) Return the elements in the given positional indices along an axis.  \nDataFrame.truncate([before, after, axis, copy]) Truncate a Series or DataFrame before and after some index value.      Missing data handling       \nDataFrame.backfill([axis, inplace, limit, ...]) Synonym for DataFrame.fillna() with method='bfill'.  \nDataFrame.bfill([axis, inplace, limit, downcast]) Synonym for DataFrame.fillna() with method='bfill'.  \nDataFrame.dropna([axis, how, thresh, ...]) Remove missing values.  \nDataFrame.ffill([axis, inplace, limit, downcast]) Synonym for DataFrame.fillna() with method='ffill'.  \nDataFrame.fillna([value, method, axis, ...]) Fill NA\/NaN values using the specified method.  \nDataFrame.interpolate([method, axis, limit, ...]) Fill NaN values using an interpolation method.  \nDataFrame.isna() Detect missing values.  \nDataFrame.isnull() DataFrame.isnull is an alias for DataFrame.isna.  \nDataFrame.notna() Detect existing (non-missing) values.  \nDataFrame.notnull() DataFrame.notnull is an alias for DataFrame.notna.  \nDataFrame.pad([axis, inplace, limit, downcast]) Synonym for DataFrame.fillna() with method='ffill'.  \nDataFrame.replace([to_replace, value, ...]) Replace values given in to_replace with value.      Reshaping, sorting, transposing       \nDataFrame.droplevel(level[, axis]) Return Series\/DataFrame with requested index \/ column level(s) removed.  \nDataFrame.pivot([index, columns, values]) Return reshaped DataFrame organized by given index \/ column values.  \nDataFrame.pivot_table([values, index, ...]) Create a spreadsheet-style pivot table as a DataFrame.  \nDataFrame.reorder_levels(order[, axis]) Rearrange index levels using input order.  \nDataFrame.sort_values(by[, axis, ascending, ...]) Sort by the values along either axis.  \nDataFrame.sort_index([axis, level, ...]) Sort object by labels (along an axis).  \nDataFrame.nlargest(n, columns[, keep]) Return the first n rows ordered by columns in descending order.  \nDataFrame.nsmallest(n, columns[, keep]) Return the first n rows ordered by columns in ascending order.  \nDataFrame.swaplevel([i, j, axis]) Swap levels i and j in a MultiIndex.  \nDataFrame.stack([level, dropna]) Stack the prescribed level(s) from columns to index.  \nDataFrame.unstack([level, fill_value]) Pivot a level of the (necessarily hierarchical) index labels.  \nDataFrame.swapaxes(axis1, axis2[, copy]) Interchange axes and swap values axes appropriately.  \nDataFrame.melt([id_vars, value_vars, ...]) Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.  \nDataFrame.explode(column[, ignore_index]) Transform each element of a list-like to a row, replicating index values.  \nDataFrame.squeeze([axis]) Squeeze 1 dimensional axis objects into scalars.  \nDataFrame.to_xarray() Return an xarray object from the pandas object.  \nDataFrame.T   \nDataFrame.transpose(*args[, copy]) Transpose index and columns.      Combining \/ comparing \/ joining \/ merging       \nDataFrame.append(other[, ignore_index, ...]) Append rows of other to the end of caller, returning a new object.  \nDataFrame.assign(**kwargs) Assign new columns to a DataFrame.  \nDataFrame.compare(other[, align_axis, ...]) Compare to another DataFrame and show the differences.  \nDataFrame.join(other[, on, how, lsuffix, ...]) Join columns of another DataFrame.  \nDataFrame.merge(right[, how, on, left_on, ...]) Merge DataFrame or named Series objects with a database-style join.  \nDataFrame.update(other[, join, overwrite, ...]) Modify in place using non-NA values from another DataFrame.      Time Series-related       \nDataFrame.asfreq(freq[, method, how, ...]) Convert time series to specified frequency.  \nDataFrame.asof(where[, subset]) Return the last row(s) without any NaNs before where.  \nDataFrame.shift([periods, freq, axis, ...]) Shift index by desired number of periods with an optional time freq.  \nDataFrame.slice_shift([periods, axis]) (DEPRECATED) Equivalent to shift without copying data.  \nDataFrame.tshift([periods, freq, axis]) (DEPRECATED) Shift the time index, using the index's frequency if available.  \nDataFrame.first_valid_index() Return index for first non-NA value or None, if no NA value is found.  \nDataFrame.last_valid_index() Return index for last non-NA value or None, if no NA value is found.  \nDataFrame.resample(rule[, axis, closed, ...]) Resample time-series data.  \nDataFrame.to_period([freq, axis, copy]) Convert DataFrame from DatetimeIndex to PeriodIndex.  \nDataFrame.to_timestamp([freq, how, axis, copy]) Cast to DatetimeIndex of timestamps, at beginning of period.  \nDataFrame.tz_convert(tz[, axis, level, copy]) Convert tz-aware axis to target time zone.  \nDataFrame.tz_localize(tz[, axis, level, ...]) Localize tz-naive index of a Series or DataFrame to target time zone.      Flags Flags refer to attributes of the pandas object. Properties of the dataset (like the date is was recorded, the URL it was accessed from, etc.) should be stored in DataFrame.attrs.       \nFlags(obj, *, allows_duplicate_labels) Flags that apply to pandas objects.      Metadata DataFrame.attrs is a dictionary for storing global metadata for this DataFrame.  Warning DataFrame.attrs is considered experimental and may change without warning.        \nDataFrame.attrs Dictionary of global attributes of this dataset.      Plotting DataFrame.plot is both a callable method and a namespace attribute for specific plotting methods of the form DataFrame.plot.<kind>.       \nDataFrame.plot([x, y, kind, ax, ....]) DataFrame plotting accessor and method          \nDataFrame.plot.area([x, y]) Draw a stacked area plot.  \nDataFrame.plot.bar([x, y]) Vertical bar plot.  \nDataFrame.plot.barh([x, y]) Make a horizontal bar plot.  \nDataFrame.plot.box([by]) Make a box plot of the DataFrame columns.  \nDataFrame.plot.density([bw_method, ind]) Generate Kernel Density Estimate plot using Gaussian kernels.  \nDataFrame.plot.hexbin(x, y[, C, ...]) Generate a hexagonal binning plot.  \nDataFrame.plot.hist([by, bins]) Draw one histogram of the DataFrame's columns.  \nDataFrame.plot.kde([bw_method, ind]) Generate Kernel Density Estimate plot using Gaussian kernels.  \nDataFrame.plot.line([x, y]) Plot Series or DataFrame as lines.  \nDataFrame.plot.pie(**kwargs) Generate a pie plot.  \nDataFrame.plot.scatter(x, y[, s, c]) Create a scatter plot with varying marker point size and color.          \nDataFrame.boxplot([column, by, ax, ...]) Make a box plot from DataFrame columns.  \nDataFrame.hist([column, by, grid, ...]) Make a histogram of the DataFrame's columns.      Sparse accessor Sparse-dtype specific methods and attributes are provided under the DataFrame.sparse accessor.       \nDataFrame.sparse.density Ratio of non-sparse points to total (dense) data points.          \nDataFrame.sparse.from_spmatrix(data[, ...]) Create a new DataFrame from a scipy sparse matrix.  \nDataFrame.sparse.to_coo() Return the contents of the frame as a sparse SciPy COO matrix.  \nDataFrame.sparse.to_dense() Convert a DataFrame with sparse values to dense.      Serialization \/ IO \/ conversion       \nDataFrame.from_dict(data[, orient, dtype, ...]) Construct DataFrame from dict of array-like or dicts.  \nDataFrame.from_records(data[, index, ...]) Convert structured or record ndarray to DataFrame.  \nDataFrame.to_parquet([path, engine, ...]) Write a DataFrame to the binary parquet format.  \nDataFrame.to_pickle(path[, compression, ...]) Pickle (serialize) object to file.  \nDataFrame.to_csv([path_or_buf, sep, na_rep, ...]) Write object to a comma-separated values (csv) file.  \nDataFrame.to_hdf(path_or_buf, key[, mode, ...]) Write the contained data to an HDF5 file using HDFStore.  \nDataFrame.to_sql(name, con[, schema, ...]) Write records stored in a DataFrame to a SQL database.  \nDataFrame.to_dict([orient, into]) Convert the DataFrame to a dictionary.  \nDataFrame.to_excel(excel_writer[, ...]) Write object to an Excel sheet.  \nDataFrame.to_json([path_or_buf, orient, ...]) Convert the object to a JSON string.  \nDataFrame.to_html([buf, columns, col_space, ...]) Render a DataFrame as an HTML table.  \nDataFrame.to_feather(path, **kwargs) Write a DataFrame to the binary Feather format.  \nDataFrame.to_latex([buf, columns, ...]) Render object to a LaTeX tabular, longtable, or nested table.  \nDataFrame.to_stata(path[, convert_dates, ...]) Export DataFrame object to Stata dta format.  \nDataFrame.to_gbq(destination_table[, ...]) Write a DataFrame to a Google BigQuery table.  \nDataFrame.to_records([index, column_dtypes, ...]) Convert DataFrame to a NumPy record array.  \nDataFrame.to_string([buf, columns, ...]) Render a DataFrame to a console-friendly tabular output.  \nDataFrame.to_clipboard([excel, sep]) Copy object to the system clipboard.  \nDataFrame.to_markdown([buf, mode, index, ...]) Print DataFrame in Markdown-friendly format.  \nDataFrame.style Returns a Styler object.","title":"pandas.reference.frame"},{"text":"pandas.Timestamp.fold   Timestamp.fold","title":"pandas.reference.api.pandas.timestamp.fold"},{"text":"pandas.DataFrame.pivot   DataFrame.pivot(index=None, columns=None, values=None)[source]\n \nReturn reshaped DataFrame organized by given index \/ column values. Reshape data (produce a \u201cpivot\u201d table) based on column values. Uses unique values from specified index \/ columns to form axes of the resulting DataFrame. This function does not support data aggregation, multiple values will result in a MultiIndex in the columns. See the User Guide for more on reshaping.  Parameters \n \nindex:str or object or a list of str, optional\n\n\nColumn to use to make new frame\u2019s index. If None, uses existing index.  Changed in version 1.1.0: Also accept list of index names.   \ncolumns:str or object or a list of str\n\n\nColumn to use to make new frame\u2019s columns.  Changed in version 1.1.0: Also accept list of columns names.   \nvalues:str, object or a list of the previous, optional\n\n\nColumn(s) to use for populating new frame\u2019s values. If not specified, all remaining columns will be used and the result will have hierarchically indexed columns.    Returns \n DataFrame\n\nReturns reshaped DataFrame.    Raises \n ValueError:\n\nWhen there are any index, columns combinations with multiple values. DataFrame.pivot_table when you need to aggregate.      See also  DataFrame.pivot_table\n\nGeneralization of pivot that can handle duplicate values for one index\/column pair.  DataFrame.unstack\n\nPivot based on the index values instead of a column.  wide_to_long\n\nWide panel to long format. Less flexible but more user-friendly than melt.    Notes For finer-tuned control, see hierarchical indexing documentation along with the related stack\/unstack methods. Examples \n>>> df = pd.DataFrame({'foo': ['one', 'one', 'one', 'two', 'two',\n...                            'two'],\n...                    'bar': ['A', 'B', 'C', 'A', 'B', 'C'],\n...                    'baz': [1, 2, 3, 4, 5, 6],\n...                    'zoo': ['x', 'y', 'z', 'q', 'w', 't']})\n>>> df\n    foo   bar  baz  zoo\n0   one   A    1    x\n1   one   B    2    y\n2   one   C    3    z\n3   two   A    4    q\n4   two   B    5    w\n5   two   C    6    t\n  \n>>> df.pivot(index='foo', columns='bar', values='baz')\nbar  A   B   C\nfoo\none  1   2   3\ntwo  4   5   6\n  \n>>> df.pivot(index='foo', columns='bar')['baz']\nbar  A   B   C\nfoo\none  1   2   3\ntwo  4   5   6\n  \n>>> df.pivot(index='foo', columns='bar', values=['baz', 'zoo'])\n      baz       zoo\nbar   A  B  C   A  B  C\nfoo\none   1  2  3   x  y  z\ntwo   4  5  6   q  w  t\n  You could also assign a list of column names or a list of index names. \n>>> df = pd.DataFrame({\n...        \"lev1\": [1, 1, 1, 2, 2, 2],\n...        \"lev2\": [1, 1, 2, 1, 1, 2],\n...        \"lev3\": [1, 2, 1, 2, 1, 2],\n...        \"lev4\": [1, 2, 3, 4, 5, 6],\n...        \"values\": [0, 1, 2, 3, 4, 5]})\n>>> df\n    lev1 lev2 lev3 lev4 values\n0   1    1    1    1    0\n1   1    1    2    2    1\n2   1    2    1    3    2\n3   2    1    2    4    3\n4   2    1    1    5    4\n5   2    2    2    6    5\n  \n>>> df.pivot(index=\"lev1\", columns=[\"lev2\", \"lev3\"],values=\"values\")\nlev2    1         2\nlev3    1    2    1    2\nlev1\n1     0.0  1.0  2.0  NaN\n2     4.0  3.0  NaN  5.0\n  \n>>> df.pivot(index=[\"lev1\", \"lev2\"], columns=[\"lev3\"],values=\"values\")\n      lev3    1    2\nlev1  lev2\n   1     1  0.0  1.0\n         2  2.0  NaN\n   2     1  4.0  3.0\n         2  NaN  5.0\n  A ValueError is raised if there are any duplicates. \n>>> df = pd.DataFrame({\"foo\": ['one', 'one', 'two', 'two'],\n...                    \"bar\": ['A', 'A', 'B', 'C'],\n...                    \"baz\": [1, 2, 3, 4]})\n>>> df\n   foo bar  baz\n0  one   A    1\n1  one   A    2\n2  two   B    3\n3  two   C    4\n  Notice that the first two rows are the same for our index and columns arguments. \n>>> df.pivot(index='foo', columns='bar', values='baz')\nTraceback (most recent call last):\n   ...\nValueError: Index contains duplicate entries, cannot reshape","title":"pandas.reference.api.pandas.dataframe.pivot"},{"text":"pandas.IntervalIndex.left   IntervalIndex.left","title":"pandas.reference.api.pandas.intervalindex.left"}]}
{"task_id":4685571,"prompt":"def f_4685571(b):\n\treturn ","suffix":"","canonical_solution":"[x[::-1] for x in b]","test_start":"\ndef check(candidate):","test":["\n    b = [('spam',0), ('eggs',1)]\n    b1 = candidate(b)\n    assert b1 == [(0, 'spam'), (1, 'eggs')]\n"],"entry_point":"f_4685571","intent":"Get reverse of list items from list 'b' using extended slicing","library":[],"docs":[]}
{"task_id":17960441,"prompt":"def f_17960441(a, b):\n\treturn ","suffix":"","canonical_solution":"np.array([zip(x, y) for x, y in zip(a, b)])","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    a = np.array([[9, 8], [7, 6]])\n    b = np.array([[7, 1], [5, 2]])\n    c = candidate(a, b)\n    expected = [(9, 7), (8, 1)]\n    ctr = 0\n    for i in c[0]:\n        assert i == expected[ctr]\n        ctr += 1\n"],"entry_point":"f_17960441","intent":"join each element in array `a` with element at the same index in array `b` as a tuple","library":["numpy"],"docs":[{"text":"numpy.clongfloat[source]\n \nalias of numpy.clongdouble","title":"numpy.reference.arrays.scalars#numpy.clongfloat"},{"text":"numpy.complex_[source]\n \nalias of numpy.cdouble","title":"numpy.reference.arrays.scalars#numpy.complex_"},{"text":"numpy.distutils.misc_util.dot_join(*args)[source]","title":"numpy.reference.distutils.misc_util#numpy.distutils.misc_util.dot_join"},{"text":"numpy.longcomplex[source]\n \nalias of numpy.clongdouble","title":"numpy.reference.arrays.scalars#numpy.longcomplex"},{"text":"numpy.lib.recfunctions.join_by(key, r1, r2, jointype='inner', r1postfix='1', r2postfix='2', defaults=None, usemask=True, asrecarray=False)[source]\n \nJoin arrays r1 and r2 on key key. The key should be either a string or a sequence of string corresponding to the fields used to join the array. An exception is raised if the key field cannot be found in the two input arrays. Neither r1 nor r2 should have any duplicates along key: the presence of duplicates will make the output quite unreliable. Note that duplicates are not looked for by the algorithm.  Parameters \n \nkey{string, sequence}\n\n\nA string or a sequence of strings corresponding to the fields used for comparison.  \nr1, r2arrays\n\n\nStructured arrays.  \njointype{\u2018inner\u2019, \u2018outer\u2019, \u2018leftouter\u2019}, optional\n\n\nIf \u2018inner\u2019, returns the elements common to both r1 and r2. If \u2018outer\u2019, returns the common elements as well as the elements of r1 not in r2 and the elements of not in r2. If \u2018leftouter\u2019, returns the common elements and the elements of r1 not in r2.  \nr1postfixstring, optional\n\n\nString appended to the names of the fields of r1 that are present in r2 but absent of the key.  \nr2postfixstring, optional\n\n\nString appended to the names of the fields of r2 that are present in r1 but absent of the key.  \ndefaults{dictionary}, optional\n\n\nDictionary mapping field names to the corresponding default values.  \nusemask{True, False}, optional\n\n\nWhether to return a MaskedArray (or MaskedRecords is asrecarray==True) or a ndarray.  \nasrecarray{False, True}, optional\n\n\nWhether to return a recarray (or MaskedRecords if usemask==True) or just a flexible-type ndarray.     Notes  The output is sorted along the key. A temporary array is formed by dropping the fields not in the key for the two arrays and concatenating the result. This array is then sorted, and the common entries selected. The output is constructed by filling the fields with the selected entries. Matching is not preserved if there are some duplicates\u2026","title":"numpy.user.basics.rec#numpy.lib.recfunctions.join_by"},{"text":"array.tolist()  \nConvert the array to an ordinary list with the same items.","title":"python.library.array#array.array.tolist"},{"text":"union(*others)  \nset | other | ...  \nReturn a new set with elements from the set and all others.","title":"python.library.stdtypes#frozenset.union"},{"text":"common_files  \nFiles in both a and b.","title":"python.library.filecmp#filecmp.dircmp.common_files"},{"text":"pandas.tseries.offsets.BQuarterEnd.copy   BQuarterEnd.copy()","title":"pandas.reference.api.pandas.tseries.offsets.bquarterend.copy"},{"text":"numpy.chararray.join method   chararray.join(seq)[source]\n \nReturn a string which is the concatenation of the strings in the sequence seq.  See also  char.join","title":"numpy.reference.generated.numpy.chararray.join"}]}
{"task_id":17960441,"prompt":"def f_17960441(a, b):\n\treturn ","suffix":"","canonical_solution":"np.array(list(zip(a.ravel(),b.ravel())), dtype=('i4,i4')).reshape(a.shape)","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    a = np.array([[9, 8], [7, 6]])\n    b = np.array([[7, 1], [5, 2]])\n    c = candidate(a, b)\n    e = np.array([[(9, 7), (8, 1)], [(7, 5), (6, 2)]], dtype=[('f0', '<i4'), ('f1', '<i4')])\n    assert np.array_equal(c, e)\n"],"entry_point":"f_17960441","intent":"zip two 2-d arrays `a` and `b`","library":["numpy"],"docs":[{"text":"common_files  \nFiles in both a and b.","title":"python.library.filecmp#filecmp.dircmp.common_files"},{"text":"common_dirs  \nSubdirectories in both a and b.","title":"python.library.filecmp#filecmp.dircmp.common_dirs"},{"text":"common  \nFiles and subdirectories in both a and b.","title":"python.library.filecmp#filecmp.dircmp.common"},{"text":"report_full_closure()  \nPrint a comparison between a and b and common subdirectories (recursively).","title":"python.library.filecmp#filecmp.dircmp.report_full_closure"},{"text":"pandas.tseries.offsets.BQuarterEnd.copy   BQuarterEnd.copy()","title":"pandas.reference.api.pandas.tseries.offsets.bquarterend.copy"},{"text":"pandas.tseries.offsets.BQuarterBegin.copy   BQuarterBegin.copy()","title":"pandas.reference.api.pandas.tseries.offsets.bquarterbegin.copy"},{"text":"pandas.tseries.offsets.Second.copy   Second.copy()","title":"pandas.reference.api.pandas.tseries.offsets.second.copy"},{"text":"numpy.complex_[source]\n \nalias of numpy.cdouble","title":"numpy.reference.arrays.scalars#numpy.complex_"},{"text":"report_partial_closure()  \nPrint a comparison between a and b and common immediate subdirectories.","title":"python.library.filecmp#filecmp.dircmp.report_partial_closure"},{"text":"numpy.complex64[source]\n \nalias of numpy.csingle","title":"numpy.reference.arrays.scalars#numpy.complex64"}]}
{"task_id":438684,"prompt":"def f_438684(list_of_ints):\n\treturn ","suffix":"","canonical_solution":"\"\"\",\"\"\".join([str(i) for i in list_of_ints])","test_start":"\ndef check(candidate):","test":["\n    list_of_ints = [8, 7, 6]\n    assert candidate(list_of_ints) == '8,7,6'\n","\n    list_of_ints = [0, 1, 6]\n    assert candidate(list_of_ints) == '0,1,6'\n"],"entry_point":"f_438684","intent":"convert list `list_of_ints` into a comma separated string","library":[],"docs":[]}
{"task_id":8519922,"prompt":"def f_8519922(url, DATA, HEADERS_DICT, username, password):\n\treturn ","suffix":"","canonical_solution":"requests.post(url, data=DATA, headers=HEADERS_DICT, auth=(username, password))","test_start":"\nimport requests\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    url='https:\/\/www.google.com'\n    HEADERS_DICT = {'Accept':'text\/json'}\n    requests.post = Mock()\n    try:\n        candidate(url, \"{'name': 'abc'}\", HEADERS_DICT, 'admin', 'admin123')\n    except:\n        assert False\n"],"entry_point":"f_8519922","intent":"Send a post request with raw data `DATA` and basic authentication with `username` and `password`","library":["requests"],"docs":[]}
{"task_id":26443308,"prompt":"def f_26443308():\n\treturn ","suffix":"","canonical_solution":"'abcd}def}'.rfind('}')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == 8\n"],"entry_point":"f_26443308","intent":"Find last occurrence of character '}' in string \"abcd}def}\"","library":[],"docs":[]}
{"task_id":22365172,"prompt":"def f_22365172():\n\treturn ","suffix":"","canonical_solution":"[item for item in [1, 2, 3]]","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == [1,2,3]\n"],"entry_point":"f_22365172","intent":"Iterate ove list `[1, 2, 3]` using list comprehension","library":[],"docs":[]}
{"task_id":12300912,"prompt":"def f_12300912(d):\n\treturn ","suffix":"","canonical_solution":"[(x['x'], x['y']) for x in d]","test_start":"\ndef check(candidate):","test":["\n    data = [{'x': 1, 'y': 10}, {'x': 3, 'y': 15}, {'x': 2, 'y': 1}]\n    res = candidate(data)\n    assert res == [(1, 10), (3, 15), (2, 1)]\n"],"entry_point":"f_12300912","intent":"extract all the values with keys 'x' and 'y' from a list of dictionaries `d` to list of tuples","library":[],"docs":[]}
{"task_id":678236,"prompt":"def f_678236():\n\treturn ","suffix":"","canonical_solution":"os.path.splitext(os.path.basename('hemanth.txt'))[0]","test_start":"\nimport os \n\ndef check(candidate):","test":["\n    assert candidate() == \"hemanth\"\n"],"entry_point":"f_678236","intent":"get the filename without the extension from file 'hemanth.txt'","library":["os"],"docs":[{"text":"filename  \nThe name of the file the syntax error occurred in.","title":"python.library.exceptions#SyntaxError.filename"},{"text":"get_filename(fullname)  \nReturns path.  New in version 3.4.","title":"python.library.importlib#importlib.machinery.ExtensionFileLoader.get_filename"},{"text":"fileinput.filename()  \nReturn the name of the file currently being read. Before the first line has been read, returns None.","title":"python.library.fileinput#fileinput.filename"},{"text":"abstractmethod get_filename(fullname)  \nReturns path.","title":"python.library.importlib#importlib.abc.FileLoader.get_filename"},{"text":"filename\n \nAlias for field number 4","title":"matplotlib.dviread#matplotlib.dviread.PsFont.filename"},{"text":"filename  \nFilename (str).","title":"python.library.tracemalloc#tracemalloc.Frame.filename"},{"text":"ZipInfo.filename  \nName of the file in the archive.","title":"python.library.zipfile#zipfile.ZipInfo.filename"},{"text":"ZipFile.filename  \nName of the ZIP file.","title":"python.library.zipfile#zipfile.ZipFile.filename"},{"text":"os.extsep  \nThe character which separates the base filename from the extension; for example, the '.' in os.py. Also available via os.path.","title":"python.library.os#os.extsep"},{"text":"property filename","title":"skimage.api.skimage.io#skimage.io.MultiImage.filename"}]}
{"task_id":7895449,"prompt":"def f_7895449():\n\treturn ","suffix":"","canonical_solution":"sum([['A', 'B', 'C'], ['D', 'E', 'F'], ['G', 'H', 'I']], [])","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I']\n"],"entry_point":"f_7895449","intent":"create a list containing flattened list `[['A', 'B', 'C'], ['D', 'E', 'F'], ['G', 'H', 'I']]`","library":[],"docs":[]}
{"task_id":31617845,"prompt":"def f_31617845(df):\n\t","suffix":"\n\treturn df","canonical_solution":"df = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame([67, 68, 69, 70, 99, 100, 101, 102], columns = ['closing_price'])\n    assert candidate(df).shape[0] == 3\n"],"entry_point":"f_31617845","intent":"select rows in a dataframe `df` column 'closing_price' between two values 99 and 101","library":["pandas"],"docs":[]}
{"task_id":25698710,"prompt":"def f_25698710(df):\n\treturn ","suffix":"","canonical_solution":"df.replace({'\\n': '<br>'}, regex=True)","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame(['klm\\npqr', 'wxy\\njkl'], columns = ['val'])\n    expected = pd.DataFrame(['klm<br>pqr', 'wxy<br>jkl'], columns = ['val'])\n    assert pd.DataFrame.equals(candidate(df), expected)\n"],"entry_point":"f_25698710","intent":"replace all occurences of newlines `\\n` with `<br>` in dataframe `df`","library":["pandas"],"docs":[{"text":"token.NEWLINE","title":"python.library.token#token.NEWLINE"},{"text":"re.purge()  \nClear the regular expression cache.","title":"python.library.re#re.purge"},{"text":"token.N_TOKENS","title":"python.library.token#token.N_TOKENS"},{"text":"pattern  \nThe regular expression pattern.","title":"python.library.re#re.error.pattern"},{"text":"str.rindex(sub[, start[, end]])  \nLike rfind() but raises ValueError when the substring sub is not found.","title":"python.library.stdtypes#str.rindex"},{"text":"pandas.tseries.offsets.BYearBegin.freqstr   BYearBegin.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.byearbegin.freqstr"},{"text":"pandas.tseries.offsets.BYearEnd.freqstr   BYearEnd.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.byearend.freqstr"},{"text":"re.S  \nre.DOTALL  \nMake the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).","title":"python.library.re#re.S"},{"text":"pandas.DataFrame.replace   DataFrame.replace(to_replace=None, value=NoDefault.no_default, inplace=False, limit=None, regex=False, method=NoDefault.no_default)[source]\n \nReplace values given in to_replace with value. Values of the DataFrame are replaced with other values dynamically. This differs from updating with .loc or .iloc, which require you to specify a location to update with some value.  Parameters \n \nto_replace:str, regex, list, dict, Series, int, float, or None\n\n\nHow to find the values that will be replaced.  \nnumeric, str or regex:  \n numeric: numeric values equal to to_replace will be replaced with value str: string exactly matching to_replace will be replaced with value regex: regexs matching to_replace will be replaced with value  \n  \nlist of str, regex, or numeric:  \n First, if to_replace and value are both lists, they must be the same length. Second, if regex=True then all of the strings in both lists will be interpreted as regexs otherwise they will match directly. This doesn\u2019t matter much for value since there are only a few possible substitution regexes you can use. str, regex and numeric rules apply as above.  \n  \ndict:  \n Dicts can be used to specify different replacement values for different existing values. For example, {'a': 'b', 'y': 'z'} replaces the value \u2018a\u2019 with \u2018b\u2019 and \u2018y\u2019 with \u2018z\u2019. To use a dict in this way the value parameter should be None. For a DataFrame a dict can specify that different values should be replaced in different columns. For example, {'a': 1, 'b': 'z'} looks for the value 1 in column \u2018a\u2019 and the value \u2018z\u2019 in column \u2018b\u2019 and replaces these values with whatever is specified in value. The value parameter should not be None in this case. You can treat this as a special case of passing two lists except that you are specifying the column to search in. For a DataFrame nested dictionaries, e.g., {'a': {'b': np.nan}}, are read as follows: look in column \u2018a\u2019 for the value \u2018b\u2019 and replace it with NaN. The value parameter should be None to use a nested dict in this way. You can nest regular expressions as well. Note that column names (the top-level dictionary keys in a nested dictionary) cannot be regular expressions.  \n  \nNone:  \n This means that the regex argument must be a string, compiled regular expression, or list, dict, ndarray or Series of such elements. If value is also None then this must be a nested dictionary or Series.  \n   See the examples section for examples of each of these.  \nvalue:scalar, dict, list, str, regex, default None\n\n\nValue to replace any values matching to_replace with. For a DataFrame a dict of values can be used to specify which value to use for each column (columns not in the dict will not be filled). Regular expressions, strings and lists or dicts of such objects are also allowed.  \ninplace:bool, default False\n\n\nIf True, performs operation inplace and returns None.  \nlimit:int, default None\n\n\nMaximum size gap to forward or backward fill.  \nregex:bool or same types as to_replace, default False\n\n\nWhether to interpret to_replace and\/or value as regular expressions. If this is True then to_replace must be a string. Alternatively, this could be a regular expression or a list, dict, or array of regular expressions in which case to_replace must be None.  \nmethod:{\u2018pad\u2019, \u2018ffill\u2019, \u2018bfill\u2019, None}\n\n\nThe method to use when for replacement, when to_replace is a scalar, list or tuple and value is None.  Changed in version 0.23.0: Added to DataFrame.     Returns \n DataFrame\n\nObject after replacement.    Raises \n AssertionError\n\n If regex is not a bool and to_replace is not None.   TypeError\n\n If to_replace is not a scalar, array-like, dict, or None If to_replace is a dict and value is not a list, dict, ndarray, or Series If to_replace is None and regex is not compilable into a regular expression or is a list, dict, ndarray, or Series. When replacing multiple bool or datetime64 objects and the arguments to to_replace does not match the type of the value being replaced   ValueError\n\n If a list or an ndarray is passed to to_replace and value but they are not the same length.       See also  DataFrame.fillna\n\nFill NA values.  DataFrame.where\n\nReplace values based on boolean condition.  Series.str.replace\n\nSimple string replacement.    Notes  Regex substitution is performed under the hood with re.sub. The rules for substitution for re.sub are the same. Regular expressions will only substitute on strings, meaning you cannot provide, for example, a regular expression matching floating point numbers and expect the columns in your frame that have a numeric dtype to be matched. However, if those floating point numbers are strings, then you can do this. This method has a lot of options. You are encouraged to experiment and play with this method to gain intuition about how it works. When dict is used as the to_replace value, it is like key(s) in the dict are the to_replace part and value(s) in the dict are the value parameter.  Examples Scalar `to_replace` and `value` \n>>> s = pd.Series([1, 2, 3, 4, 5])\n>>> s.replace(1, 5)\n0    5\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n  \n>>> df = pd.DataFrame({'A': [0, 1, 2, 3, 4],\n...                    'B': [5, 6, 7, 8, 9],\n...                    'C': ['a', 'b', 'c', 'd', 'e']})\n>>> df.replace(0, 5)\n    A  B  C\n0  5  5  a\n1  1  6  b\n2  2  7  c\n3  3  8  d\n4  4  9  e\n  List-like `to_replace` \n>>> df.replace([0, 1, 2, 3], 4)\n    A  B  C\n0  4  5  a\n1  4  6  b\n2  4  7  c\n3  4  8  d\n4  4  9  e\n  \n>>> df.replace([0, 1, 2, 3], [4, 3, 2, 1])\n    A  B  C\n0  4  5  a\n1  3  6  b\n2  2  7  c\n3  1  8  d\n4  4  9  e\n  \n>>> s.replace([1, 2], method='bfill')\n0    3\n1    3\n2    3\n3    4\n4    5\ndtype: int64\n  dict-like `to_replace` \n>>> df.replace({0: 10, 1: 100})\n        A  B  C\n0   10  5  a\n1  100  6  b\n2    2  7  c\n3    3  8  d\n4    4  9  e\n  \n>>> df.replace({'A': 0, 'B': 5}, 100)\n        A    B  C\n0  100  100  a\n1    1    6  b\n2    2    7  c\n3    3    8  d\n4    4    9  e\n  \n>>> df.replace({'A': {0: 100, 4: 400}})\n        A  B  C\n0  100  5  a\n1    1  6  b\n2    2  7  c\n3    3  8  d\n4  400  9  e\n  Regular expression `to_replace` \n>>> df = pd.DataFrame({'A': ['bat', 'foo', 'bait'],\n...                    'B': ['abc', 'bar', 'xyz']})\n>>> df.replace(to_replace=r'^ba.$', value='new', regex=True)\n        A    B\n0   new  abc\n1   foo  new\n2  bait  xyz\n  \n>>> df.replace({'A': r'^ba.$'}, {'A': 'new'}, regex=True)\n        A    B\n0   new  abc\n1   foo  bar\n2  bait  xyz\n  \n>>> df.replace(regex=r'^ba.$', value='new')\n        A    B\n0   new  abc\n1   foo  new\n2  bait  xyz\n  \n>>> df.replace(regex={r'^ba.$': 'new', 'foo': 'xyz'})\n        A    B\n0   new  abc\n1   xyz  new\n2  bait  xyz\n  \n>>> df.replace(regex=[r'^ba.$', 'foo'], value='new')\n        A    B\n0   new  abc\n1   new  new\n2  bait  xyz\n  Compare the behavior of s.replace({'a': None}) and s.replace('a', None) to understand the peculiarities of the to_replace parameter: \n>>> s = pd.Series([10, 'a', 'a', 'b', 'a'])\n  When one uses a dict as the to_replace value, it is like the value(s) in the dict are equal to the value parameter. s.replace({'a': None}) is equivalent to s.replace(to_replace={'a': None}, value=None, method=None): \n>>> s.replace({'a': None})\n0      10\n1    None\n2    None\n3       b\n4    None\ndtype: object\n  When value is not explicitly passed and to_replace is a scalar, list or tuple, replace uses the method parameter (default \u2018pad\u2019) to do the replacement. So this is why the \u2018a\u2019 values are being replaced by 10 in rows 1 and 2 and \u2018b\u2019 in row 4 in this case. \n>>> s.replace('a')\n0    10\n1    10\n2    10\n3     b\n4     b\ndtype: object\n  On the other hand, if None is explicitly passed for value, it will be respected: \n>>> s.replace('a', None)\n0      10\n1    None\n2    None\n3       b\n4    None\ndtype: object\n   \n Changed in version 1.4.0: Previously the explicit None was silently ignored.","title":"pandas.reference.api.pandas.dataframe.replace"},{"text":"pandas.tseries.offsets.BQuarterEnd.freqstr   BQuarterEnd.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.bquarterend.freqstr"}]}
{"task_id":25698710,"prompt":"def f_25698710(df):\n\treturn ","suffix":"","canonical_solution":"df.replace({'\\n': '<br>'}, regex=True)","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame(['klm\\npqr', 'wxy\\njkl'], columns = ['val'])\n    expected = pd.DataFrame(['klm<br>pqr', 'wxy<br>jkl'], columns = ['val'])\n    assert pd.DataFrame.equals(candidate(df), expected)\n"],"entry_point":"f_25698710","intent":"replace all occurrences of a string `\\n` by string `<br>` in a pandas data frame `df`","library":["pandas"],"docs":[{"text":"pandas.tseries.offsets.BYearBegin.freqstr   BYearBegin.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.byearbegin.freqstr"},{"text":"pandas.tseries.offsets.BYearEnd.freqstr   BYearEnd.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.byearend.freqstr"},{"text":"pandas.tseries.offsets.BQuarterEnd.freqstr   BQuarterEnd.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.bquarterend.freqstr"},{"text":"str.rindex(sub[, start[, end]])  \nLike rfind() but raises ValueError when the substring sub is not found.","title":"python.library.stdtypes#str.rindex"},{"text":"pandas.tseries.offsets.BQuarterBegin.freqstr   BQuarterBegin.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.bquarterbegin.freqstr"},{"text":"pandas.DataFrame.replace   DataFrame.replace(to_replace=None, value=NoDefault.no_default, inplace=False, limit=None, regex=False, method=NoDefault.no_default)[source]\n \nReplace values given in to_replace with value. Values of the DataFrame are replaced with other values dynamically. This differs from updating with .loc or .iloc, which require you to specify a location to update with some value.  Parameters \n \nto_replace:str, regex, list, dict, Series, int, float, or None\n\n\nHow to find the values that will be replaced.  \nnumeric, str or regex:  \n numeric: numeric values equal to to_replace will be replaced with value str: string exactly matching to_replace will be replaced with value regex: regexs matching to_replace will be replaced with value  \n  \nlist of str, regex, or numeric:  \n First, if to_replace and value are both lists, they must be the same length. Second, if regex=True then all of the strings in both lists will be interpreted as regexs otherwise they will match directly. This doesn\u2019t matter much for value since there are only a few possible substitution regexes you can use. str, regex and numeric rules apply as above.  \n  \ndict:  \n Dicts can be used to specify different replacement values for different existing values. For example, {'a': 'b', 'y': 'z'} replaces the value \u2018a\u2019 with \u2018b\u2019 and \u2018y\u2019 with \u2018z\u2019. To use a dict in this way the value parameter should be None. For a DataFrame a dict can specify that different values should be replaced in different columns. For example, {'a': 1, 'b': 'z'} looks for the value 1 in column \u2018a\u2019 and the value \u2018z\u2019 in column \u2018b\u2019 and replaces these values with whatever is specified in value. The value parameter should not be None in this case. You can treat this as a special case of passing two lists except that you are specifying the column to search in. For a DataFrame nested dictionaries, e.g., {'a': {'b': np.nan}}, are read as follows: look in column \u2018a\u2019 for the value \u2018b\u2019 and replace it with NaN. The value parameter should be None to use a nested dict in this way. You can nest regular expressions as well. Note that column names (the top-level dictionary keys in a nested dictionary) cannot be regular expressions.  \n  \nNone:  \n This means that the regex argument must be a string, compiled regular expression, or list, dict, ndarray or Series of such elements. If value is also None then this must be a nested dictionary or Series.  \n   See the examples section for examples of each of these.  \nvalue:scalar, dict, list, str, regex, default None\n\n\nValue to replace any values matching to_replace with. For a DataFrame a dict of values can be used to specify which value to use for each column (columns not in the dict will not be filled). Regular expressions, strings and lists or dicts of such objects are also allowed.  \ninplace:bool, default False\n\n\nIf True, performs operation inplace and returns None.  \nlimit:int, default None\n\n\nMaximum size gap to forward or backward fill.  \nregex:bool or same types as to_replace, default False\n\n\nWhether to interpret to_replace and\/or value as regular expressions. If this is True then to_replace must be a string. Alternatively, this could be a regular expression or a list, dict, or array of regular expressions in which case to_replace must be None.  \nmethod:{\u2018pad\u2019, \u2018ffill\u2019, \u2018bfill\u2019, None}\n\n\nThe method to use when for replacement, when to_replace is a scalar, list or tuple and value is None.  Changed in version 0.23.0: Added to DataFrame.     Returns \n DataFrame\n\nObject after replacement.    Raises \n AssertionError\n\n If regex is not a bool and to_replace is not None.   TypeError\n\n If to_replace is not a scalar, array-like, dict, or None If to_replace is a dict and value is not a list, dict, ndarray, or Series If to_replace is None and regex is not compilable into a regular expression or is a list, dict, ndarray, or Series. When replacing multiple bool or datetime64 objects and the arguments to to_replace does not match the type of the value being replaced   ValueError\n\n If a list or an ndarray is passed to to_replace and value but they are not the same length.       See also  DataFrame.fillna\n\nFill NA values.  DataFrame.where\n\nReplace values based on boolean condition.  Series.str.replace\n\nSimple string replacement.    Notes  Regex substitution is performed under the hood with re.sub. The rules for substitution for re.sub are the same. Regular expressions will only substitute on strings, meaning you cannot provide, for example, a regular expression matching floating point numbers and expect the columns in your frame that have a numeric dtype to be matched. However, if those floating point numbers are strings, then you can do this. This method has a lot of options. You are encouraged to experiment and play with this method to gain intuition about how it works. When dict is used as the to_replace value, it is like key(s) in the dict are the to_replace part and value(s) in the dict are the value parameter.  Examples Scalar `to_replace` and `value` \n>>> s = pd.Series([1, 2, 3, 4, 5])\n>>> s.replace(1, 5)\n0    5\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n  \n>>> df = pd.DataFrame({'A': [0, 1, 2, 3, 4],\n...                    'B': [5, 6, 7, 8, 9],\n...                    'C': ['a', 'b', 'c', 'd', 'e']})\n>>> df.replace(0, 5)\n    A  B  C\n0  5  5  a\n1  1  6  b\n2  2  7  c\n3  3  8  d\n4  4  9  e\n  List-like `to_replace` \n>>> df.replace([0, 1, 2, 3], 4)\n    A  B  C\n0  4  5  a\n1  4  6  b\n2  4  7  c\n3  4  8  d\n4  4  9  e\n  \n>>> df.replace([0, 1, 2, 3], [4, 3, 2, 1])\n    A  B  C\n0  4  5  a\n1  3  6  b\n2  2  7  c\n3  1  8  d\n4  4  9  e\n  \n>>> s.replace([1, 2], method='bfill')\n0    3\n1    3\n2    3\n3    4\n4    5\ndtype: int64\n  dict-like `to_replace` \n>>> df.replace({0: 10, 1: 100})\n        A  B  C\n0   10  5  a\n1  100  6  b\n2    2  7  c\n3    3  8  d\n4    4  9  e\n  \n>>> df.replace({'A': 0, 'B': 5}, 100)\n        A    B  C\n0  100  100  a\n1    1    6  b\n2    2    7  c\n3    3    8  d\n4    4    9  e\n  \n>>> df.replace({'A': {0: 100, 4: 400}})\n        A  B  C\n0  100  5  a\n1    1  6  b\n2    2  7  c\n3    3  8  d\n4  400  9  e\n  Regular expression `to_replace` \n>>> df = pd.DataFrame({'A': ['bat', 'foo', 'bait'],\n...                    'B': ['abc', 'bar', 'xyz']})\n>>> df.replace(to_replace=r'^ba.$', value='new', regex=True)\n        A    B\n0   new  abc\n1   foo  new\n2  bait  xyz\n  \n>>> df.replace({'A': r'^ba.$'}, {'A': 'new'}, regex=True)\n        A    B\n0   new  abc\n1   foo  bar\n2  bait  xyz\n  \n>>> df.replace(regex=r'^ba.$', value='new')\n        A    B\n0   new  abc\n1   foo  new\n2  bait  xyz\n  \n>>> df.replace(regex={r'^ba.$': 'new', 'foo': 'xyz'})\n        A    B\n0   new  abc\n1   xyz  new\n2  bait  xyz\n  \n>>> df.replace(regex=[r'^ba.$', 'foo'], value='new')\n        A    B\n0   new  abc\n1   new  new\n2  bait  xyz\n  Compare the behavior of s.replace({'a': None}) and s.replace('a', None) to understand the peculiarities of the to_replace parameter: \n>>> s = pd.Series([10, 'a', 'a', 'b', 'a'])\n  When one uses a dict as the to_replace value, it is like the value(s) in the dict are equal to the value parameter. s.replace({'a': None}) is equivalent to s.replace(to_replace={'a': None}, value=None, method=None): \n>>> s.replace({'a': None})\n0      10\n1    None\n2    None\n3       b\n4    None\ndtype: object\n  When value is not explicitly passed and to_replace is a scalar, list or tuple, replace uses the method parameter (default \u2018pad\u2019) to do the replacement. So this is why the \u2018a\u2019 values are being replaced by 10 in rows 1 and 2 and \u2018b\u2019 in row 4 in this case. \n>>> s.replace('a')\n0    10\n1    10\n2    10\n3     b\n4     b\ndtype: object\n  On the other hand, if None is explicitly passed for value, it will be respected: \n>>> s.replace('a', None)\n0      10\n1    None\n2    None\n3       b\n4    None\ndtype: object\n   \n Changed in version 1.4.0: Previously the explicit None was silently ignored.","title":"pandas.reference.api.pandas.dataframe.replace"},{"text":"pandas.Period.freq   Period.freq","title":"pandas.reference.api.pandas.period.freq"},{"text":"pandas.tseries.offsets.Nano.freqstr   Nano.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.nano.freqstr"},{"text":"pandas.Series.dt.freq   Series.dt.freq","title":"pandas.reference.api.pandas.series.dt.freq"},{"text":"pandas.tseries.offsets.BusinessMonthEnd.freqstr   BusinessMonthEnd.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.businessmonthend.freqstr"}]}
{"task_id":41923858,"prompt":"def f_41923858(word):\n\treturn ","suffix":"","canonical_solution":"[(x + y) for x, y in zip(word, word[1:])]","test_start":"\ndef check(candidate):","test":["\n    assert candidate('abcdef') == ['ab', 'bc', 'cd', 'de', 'ef']\n","\n    assert candidate([\"hello\", \"world\", \"!\"]) == [\"helloworld\", \"world!\"]\n"],"entry_point":"f_41923858","intent":"create a list containing each two adjacent letters in string `word` as its elements","library":[],"docs":[]}
{"task_id":41923858,"prompt":"def f_41923858(word):\n\treturn ","suffix":"","canonical_solution":"list(map(lambda x, y: x + y, word[:-1], word[1:]))","test_start":"\ndef check(candidate):","test":["\n    assert candidate('abcdef') == ['ab', 'bc', 'cd', 'de', 'ef']\n"],"entry_point":"f_41923858","intent":"Get a list of pairs from a string `word` using lambda function","library":[],"docs":[]}
{"task_id":9760588,"prompt":"def f_9760588(myString):\n\treturn ","suffix":"","canonical_solution":"re.findall('(https?:\/\/[^\\\\s]+)', myString)","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate(\"This is a link http:\/\/www.google.com\") == [\"http:\/\/www.google.com\"]\n","\n    assert candidate(\"Please refer to the website: http:\/\/www.google.com\") == [\"http:\/\/www.google.com\"]\n"],"entry_point":"f_9760588","intent":"extract a url from a string `myString`","library":["re"],"docs":[{"text":"urllib.parse.unwrap(url)  \nExtract the url from a wrapped URL (that is, a string formatted as <URL:scheme:\/\/host\/path>, <scheme:\/\/host\/path>, URL:scheme:\/\/host\/path or scheme:\/\/host\/path). If url is not a wrapped URL, it is returned without changes.","title":"python.library.urllib.parse#urllib.parse.unwrap"},{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"token.STRING","title":"python.library.token#token.STRING"},{"text":"gettext.ldgettext(domain, message)","title":"python.library.gettext#gettext.ldgettext"},{"text":"uuid.NAMESPACE_URL  \nWhen this namespace is specified, the name string is a URL.","title":"python.library.uuid#uuid.NAMESPACE_URL"},{"text":"token.SLASH  \nToken value for \"\/\".","title":"python.library.token#token.SLASH"},{"text":"exception xml.dom.SyntaxErr  \nRaised when an invalid or illegal string is specified.","title":"python.library.xml.dom#xml.dom.SyntaxErr"},{"text":"str.rindex(sub[, start[, end]])  \nLike rfind() but raises ValueError when the substring sub is not found.","title":"python.library.stdtypes#str.rindex"},{"text":"gettext.dpgettext(domain, context, message)","title":"python.library.gettext#gettext.dpgettext"},{"text":"resolve(path, urlconf=None)","title":"django.ref.urlresolvers#django.urls.resolve"}]}
{"task_id":9760588,"prompt":"def f_9760588(myString):\n\treturn ","suffix":"","canonical_solution":"re.search('(?P<url>https?:\/\/[^\\\\s]+)', myString).group('url')","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate(\"This is a link http:\/\/www.google.com\") == \"http:\/\/www.google.com\"\n","\n    assert candidate(\"Please refer to the website: http:\/\/www.google.com\") == \"http:\/\/www.google.com\"\n"],"entry_point":"f_9760588","intent":"extract a url from a string `myString`","library":["re"],"docs":[{"text":"urllib.parse.unwrap(url)  \nExtract the url from a wrapped URL (that is, a string formatted as <URL:scheme:\/\/host\/path>, <scheme:\/\/host\/path>, URL:scheme:\/\/host\/path or scheme:\/\/host\/path). If url is not a wrapped URL, it is returned without changes.","title":"python.library.urllib.parse#urllib.parse.unwrap"},{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"token.STRING","title":"python.library.token#token.STRING"},{"text":"gettext.ldgettext(domain, message)","title":"python.library.gettext#gettext.ldgettext"},{"text":"uuid.NAMESPACE_URL  \nWhen this namespace is specified, the name string is a URL.","title":"python.library.uuid#uuid.NAMESPACE_URL"},{"text":"token.SLASH  \nToken value for \"\/\".","title":"python.library.token#token.SLASH"},{"text":"exception xml.dom.SyntaxErr  \nRaised when an invalid or illegal string is specified.","title":"python.library.xml.dom#xml.dom.SyntaxErr"},{"text":"str.rindex(sub[, start[, end]])  \nLike rfind() but raises ValueError when the substring sub is not found.","title":"python.library.stdtypes#str.rindex"},{"text":"gettext.dpgettext(domain, context, message)","title":"python.library.gettext#gettext.dpgettext"},{"text":"resolve(path, urlconf=None)","title":"django.ref.urlresolvers#django.urls.resolve"}]}
{"task_id":5843518,"prompt":"def f_5843518(mystring):\n\treturn ","suffix":"","canonical_solution":"re.sub('[^A-Za-z0-9]+', '', mystring)","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate('Special $#! characters   spaces 888323') == 'Specialcharactersspaces888323'\n"],"entry_point":"f_5843518","intent":"remove all special characters, punctuation and spaces from a string `mystring` using regex","library":["re"],"docs":[{"text":"re.purge()  \nClear the regular expression cache.","title":"python.library.re#re.purge"},{"text":"pattern  \nThe regular expression pattern.","title":"python.library.re#re.error.pattern"},{"text":"winreg.REG_SZ  \nA null-terminated string.","title":"python.library.winreg#winreg.REG_SZ"},{"text":"str.lstrip([chars])  \nReturn a copy of the string with leading characters removed. The chars argument is a string specifying the set of characters to be removed. If omitted or None, the chars argument defaults to removing whitespace. The chars argument is not a prefix; rather, all combinations of its values are stripped: >>> '   spacious   '.lstrip()\n'spacious   '\n>>> 'www.example.com'.lstrip('cmowz.')\n'example.com'\n See str.removeprefix() for a method that will remove a single prefix string rather than all of a set of characters. For example: >>> 'Arthur: three!'.lstrip('Arthur: ')\n'ee!'\n>>> 'Arthur: three!'.removeprefix('Arthur: ')\n'three!'","title":"python.library.stdtypes#str.lstrip"},{"text":"token.STRING","title":"python.library.token#token.STRING"},{"text":"str.strip([chars])  \nReturn a copy of the string with the leading and trailing characters removed. The chars argument is a string specifying the set of characters to be removed. If omitted or None, the chars argument defaults to removing whitespace. The chars argument is not a prefix or suffix; rather, all combinations of its values are stripped: >>> '   spacious   '.strip()\n'spacious'\n>>> 'www.example.com'.strip('cmowz.')\n'example'\n The outermost leading and trailing chars argument values are stripped from the string. Characters are removed from the leading end until reaching a string character that is not contained in the set of characters in chars. A similar action takes place on the trailing end. For example: >>> comment_string = '#....... Section 3.2.1 Issue #32 .......'\n>>> comment_string.strip('.#! ')\n'Section 3.2.1 Issue #32'","title":"python.library.stdtypes#str.strip"},{"text":"re.S  \nre.DOTALL  \nMake the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).","title":"python.library.re#re.S"},{"text":"exception xml.dom.SyntaxErr  \nRaised when an invalid or illegal string is specified.","title":"python.library.xml.dom#xml.dom.SyntaxErr"},{"text":"token.SLASH  \nToken value for \"\/\".","title":"python.library.token#token.SLASH"},{"text":"str.rstrip([chars])  \nReturn a copy of the string with trailing characters removed. The chars argument is a string specifying the set of characters to be removed. If omitted or None, the chars argument defaults to removing whitespace. The chars argument is not a suffix; rather, all combinations of its values are stripped: >>> '   spacious   '.rstrip()\n'   spacious'\n>>> 'mississippi'.rstrip('ipz')\n'mississ'\n See str.removesuffix() for a method that will remove a single suffix string rather than all of a set of characters. For example: >>> 'Monty Python'.rstrip(' Python')\n'M'\n>>> 'Monty Python'.removesuffix(' Python')\n'Monty'","title":"python.library.stdtypes#str.rstrip"}]}
{"task_id":36674519,"prompt":"def f_36674519():\n\treturn ","suffix":"","canonical_solution":"pd.date_range('2016-01-01', freq='WOM-2FRI', periods=13)","test_start":"\nimport pandas as pd\nimport datetime\n\ndef check(candidate):","test":["\n    actual = candidate() \n    expected = [[2016, 1, 8], [2016, 2, 12],\n                [2016, 3, 11], [2016, 4, 8],\n                [2016, 5, 13], [2016, 6, 10],\n                [2016, 7, 8], [2016, 8, 12],\n                [2016, 9, 9], [2016, 10, 14],\n                [2016, 11, 11], [2016, 12, 9],\n                [2017, 1, 13]]\n    for i in range(0, len(expected)):\n        d = datetime.date(expected[i][0], expected[i][1], expected[i][2])\n        assert d == actual[i].date()\n"],"entry_point":"f_36674519","intent":"create a DatetimeIndex containing 13 periods of the second friday of each month starting from date '2016-01-01'","library":["datetime","pandas"],"docs":[{"text":"pandas.PeriodIndex.start_time   propertyPeriodIndex.start_time","title":"pandas.reference.api.pandas.periodindex.start_time"},{"text":"date.month  \nBetween 1 and 12 inclusive.","title":"python.library.datetime#datetime.date.month"},{"text":"pandas.tseries.offsets.FY5253Quarter.weekday   FY5253Quarter.weekday","title":"pandas.reference.api.pandas.tseries.offsets.fy5253quarter.weekday"},{"text":"datetime.month  \nBetween 1 and 12 inclusive.","title":"python.library.datetime#datetime.datetime.month"},{"text":"pandas.tseries.offsets.BQuarterEnd.startingMonth   BQuarterEnd.startingMonth","title":"pandas.reference.api.pandas.tseries.offsets.bquarterend.startingmonth"},{"text":"pandas.tseries.offsets.FY5253Quarter.startingMonth   FY5253Quarter.startingMonth","title":"pandas.reference.api.pandas.tseries.offsets.fy5253quarter.startingmonth"},{"text":"pandas.PeriodIndex.qyear   propertyPeriodIndex.qyear","title":"pandas.reference.api.pandas.periodindex.qyear"},{"text":"pandas.PeriodIndex.end_time   propertyPeriodIndex.end_time","title":"pandas.reference.api.pandas.periodindex.end_time"},{"text":"pandas.tseries.offsets.WeekOfMonth.weekday   WeekOfMonth.weekday","title":"pandas.reference.api.pandas.tseries.offsets.weekofmonth.weekday"},{"text":"pandas.tseries.offsets.BQuarterBegin.startingMonth   BQuarterBegin.startingMonth","title":"pandas.reference.api.pandas.tseries.offsets.bquarterbegin.startingmonth"}]}
{"task_id":508657,"prompt":"def f_508657():\n\t","suffix":"\n\treturn matrix","canonical_solution":"matrix = [['a', 'b'], ['c', 'd'], ['e', 'f']]","test_start":"\ndef check(candidate):","test":["\n    matrix = candidate()\n    assert len(matrix) == 3\n    assert all([len(row)==2 for row in matrix])\n"],"entry_point":"f_508657","intent":"Create multidimensional array `matrix` with 3 rows and 2 columns in python","library":[],"docs":[]}
{"task_id":1007481,"prompt":"def f_1007481(mystring):\n\treturn ","suffix":"","canonical_solution":"mystring.replace(' ', '_')","test_start":"\ndef check(candidate):","test":["\n    assert candidate(' ') == '_'\n","\n    assert candidate(' _ ') == '___'\n","\n    assert candidate('') == ''\n","\n    assert candidate('123123') == '123123'\n","\n    assert candidate('\\_ ') == '\\__'\n"],"entry_point":"f_1007481","intent":"replace spaces with underscore in string `mystring`","library":[],"docs":[]}
{"task_id":1249786,"prompt":"def f_1249786(my_string):\n\treturn ","suffix":"","canonical_solution":"\"\"\" \"\"\".join(my_string.split())","test_start":"\ndef check(candidate):","test":["\n    assert candidate('hello   world ') == 'hello world'\n","\n    assert candidate('') == ''\n","\n    assert candidate('    ') == ''\n","\n    assert candidate('  hello') == 'hello'\n","\n    assert candidate(' h  e  l  l  o   ') == 'h e l l o'\n"],"entry_point":"f_1249786","intent":"split string `my_string` on white spaces","library":[],"docs":[]}
{"task_id":4444923,"prompt":"def f_4444923(filename):\n\treturn ","suffix":"","canonical_solution":"os.path.splitext(filename)[0]","test_start":"\nimport os\n\ndef check(candidate):","test":["\n    assert candidate('\/Users\/test\/hello.txt') == '\/Users\/test\/hello'\n","\n    assert candidate('hello.txt') == 'hello'\n","\n    assert candidate('hello') == 'hello'\n","\n    assert candidate('.gitignore') == '.gitignore'\n"],"entry_point":"f_4444923","intent":"get filename without extension from file `filename`","library":["os"],"docs":[{"text":"get_filename(fullname)  \nReturns path.  New in version 3.4.","title":"python.library.importlib#importlib.machinery.ExtensionFileLoader.get_filename"},{"text":"fileinput.filename()  \nReturn the name of the file currently being read. Before the first line has been read, returns None.","title":"python.library.fileinput#fileinput.filename"},{"text":"filename  \nThe name of the file the syntax error occurred in.","title":"python.library.exceptions#SyntaxError.filename"},{"text":"filename  \nFilename (str).","title":"python.library.tracemalloc#tracemalloc.Frame.filename"},{"text":"property filename","title":"skimage.api.skimage.io#skimage.io.MultiImage.filename"},{"text":"abstractmethod get_filename(fullname)  \nReturns path.","title":"python.library.importlib#importlib.abc.FileLoader.get_filename"},{"text":"ZipInfo.filename  \nName of the file in the archive.","title":"python.library.zipfile#zipfile.ZipInfo.filename"},{"text":"ZipFile.filename  \nName of the ZIP file.","title":"python.library.zipfile#zipfile.ZipFile.filename"},{"text":"filename\n \nAlias for field number 4","title":"matplotlib.dviread#matplotlib.dviread.PsFont.filename"},{"text":"os.extsep  \nThe character which separates the base filename from the extension; for example, the '.' in os.py. Also available via os.path.","title":"python.library.os#os.extsep"}]}
{"task_id":13728486,"prompt":"def f_13728486(l):\n\treturn ","suffix":"","canonical_solution":"[sum(l[:i]) for i, _ in enumerate(l)]","test_start":"\ndef check(candidate):","test":["\n    assert candidate([1,2,3]) == [0,1,3]\n","\n    assert candidate([]) == []\n","\n    assert candidate([1]) == [0]\n"],"entry_point":"f_13728486","intent":"get a list containing the sum of each element `i` in list `l` plus the previous elements","library":[],"docs":[]}
{"task_id":9743134,"prompt":"def f_9743134():\n\treturn ","suffix":"","canonical_solution":"\"\"\"Docs\/src\/Scripts\/temp\"\"\".replace('\/', '\/\\x00\/').split('\\x00')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == ['Docs\/', '\/src\/', '\/Scripts\/', '\/temp']\n","\n    assert candidate() != ['Docs', 'src', 'Scripts', 'temp']\n"],"entry_point":"f_9743134","intent":"split a string `Docs\/src\/Scripts\/temp` by `\/` keeping `\/` in the result","library":[],"docs":[]}
{"task_id":20546419,"prompt":"def f_20546419(r):\n\treturn ","suffix":"","canonical_solution":"np.random.shuffle(np.transpose(r))","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    a1 = np.array([[ 1, 20], [ 2, 30]])\n    candidate(a1)\n    assert np.array_equal(a1, np.array([[ 1, 20],[ 2, 30]])) or np.array_equal(a1, np.array([[ 20, 1], [ 30, 2]]))\n","\n    a2 = np.array([[ 1], [ 2]])\n    candidate(a2)                       \n    assert np.array_equal(a2,np.array([[ 1], [ 2]]) )\n","\n    a3 = np.array([[ 1,2,3]])\n    candidate(a3)\n    assert np.array_equal(a3,np.array([[ 1,2,3]])) or np.array_equal(a3,np.array([[ 2,1,3]]))           or np.array_equal(a3,np.array([[ 1,3,2]]))            or np.array_equal(a3,np.array([[3,2,1]])) or np.array_equal(a3,np.array([[3,1,2]]))            or np.array_equal(a3,np.array([[2,3,1]])) \n","\n    a4 = np.zeros(shape=(5,2))\n    candidate(a4)\n    assert np.array_equal(a4, np.zeros(shape=(5,2)))\n"],"entry_point":"f_20546419","intent":"shuffle columns of an numpy array 'r'","library":["numpy"],"docs":[{"text":"sklearn.utils.shuffle  \nsklearn.utils.shuffle(*arrays, random_state=None, n_samples=None) [source]\n \nShuffle arrays or sparse matrices in a consistent way. This is a convenience alias to resample(*arrays, replace=False) to do random permutations of the collections.  Parameters \n \n*arrayssequence of indexable data-structures \n\nIndexable data-structures can be arrays, lists, dataframes or scipy sparse matrices with consistent first dimension.  \nrandom_stateint, RandomState instance or None, default=None \n\nDetermines random number generation for shuffling the data. Pass an int for reproducible results across multiple function calls. See Glossary.  \nn_samplesint, default=None \n\nNumber of samples to generate. If left to None this is automatically set to the first dimension of the arrays. It should not be larger than the length of arrays.    Returns \n \nshuffled_arrayssequence of indexable data-structures \n\nSequence of shuffled copies of the collections. The original arrays are not impacted.      See also  \nresample\n\n  Examples It is possible to mix sparse and dense arrays in the same run: >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])\n>>> y = np.array([0, 1, 2])\n\n>>> from scipy.sparse import coo_matrix\n>>> X_sparse = coo_matrix(X)\n\n>>> from sklearn.utils import shuffle\n>>> X, X_sparse, y = shuffle(X, X_sparse, y, random_state=0)\n>>> X\narray([[0., 0.],\n       [2., 1.],\n       [1., 0.]])\n\n>>> X_sparse\n<3x2 sparse matrix of type '<... 'numpy.float64'>'\n    with 3 stored elements in Compressed Sparse Row format>\n\n>>> X_sparse.toarray()\narray([[0., 0.],\n       [2., 1.],\n       [1., 0.]])\n\n>>> y\narray([2, 1, 0])\n\n>>> shuffle(y, n_samples=2, random_state=0)\narray([0, 1])\n \n Examples using sklearn.utils.shuffle\n \n  Color Quantization using K-Means  \n\n  Empirical evaluation of the impact of k-means initialization  \n\n  Combine predictors using stacking  \n\n  Model Complexity Influence  \n\n  Prediction Latency  \n\n  Early stopping of Stochastic Gradient Descent  \n\n  Approximate nearest neighbors in TSNE  \n\n  Effect of varying threshold for self-training","title":"sklearn.modules.generated.sklearn.utils.shuffle"},{"text":"sklearn.utils.shuffle(*arrays, random_state=None, n_samples=None) [source]\n \nShuffle arrays or sparse matrices in a consistent way. This is a convenience alias to resample(*arrays, replace=False) to do random permutations of the collections.  Parameters \n \n*arrayssequence of indexable data-structures \n\nIndexable data-structures can be arrays, lists, dataframes or scipy sparse matrices with consistent first dimension.  \nrandom_stateint, RandomState instance or None, default=None \n\nDetermines random number generation for shuffling the data. Pass an int for reproducible results across multiple function calls. See Glossary.  \nn_samplesint, default=None \n\nNumber of samples to generate. If left to None this is automatically set to the first dimension of the arrays. It should not be larger than the length of arrays.    Returns \n \nshuffled_arrayssequence of indexable data-structures \n\nSequence of shuffled copies of the collections. The original arrays are not impacted.      See also  \nresample\n\n  Examples It is possible to mix sparse and dense arrays in the same run: >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])\n>>> y = np.array([0, 1, 2])\n\n>>> from scipy.sparse import coo_matrix\n>>> X_sparse = coo_matrix(X)\n\n>>> from sklearn.utils import shuffle\n>>> X, X_sparse, y = shuffle(X, X_sparse, y, random_state=0)\n>>> X\narray([[0., 0.],\n       [2., 1.],\n       [1., 0.]])\n\n>>> X_sparse\n<3x2 sparse matrix of type '<... 'numpy.float64'>'\n    with 3 stored elements in Compressed Sparse Row format>\n\n>>> X_sparse.toarray()\narray([[0., 0.],\n       [2., 1.],\n       [1., 0.]])\n\n>>> y\narray([2, 1, 0])\n\n>>> shuffle(y, n_samples=2, random_state=0)\narray([0, 1])","title":"sklearn.modules.generated.sklearn.utils.shuffle#sklearn.utils.shuffle"},{"text":"numpy.random.shuffle   random.shuffle(x)\n \nModify a sequence in-place by shuffling its contents. This function only shuffles the array along the first axis of a multi-dimensional array. The order of sub-arrays is changed but their contents remains the same.  Note New code should use the shuffle method of a default_rng() instance instead; please see the Quick Start.   Parameters \n \nxndarray or MutableSequence\n\n\nThe array, list or mutable sequence to be shuffled.    Returns \n None\n    See also  Generator.shuffle\n\nwhich should be used for new code.    Examples >>> arr = np.arange(10)\n>>> np.random.shuffle(arr)\n>>> arr\n[1 7 5 2 9 4 3 6 0 8] # random\n Multi-dimensional arrays are only shuffled along the first axis: >>> arr = np.arange(9).reshape((3, 3))\n>>> np.random.shuffle(arr)\n>>> arr\narray([[3, 4, 5], # random\n       [6, 7, 8],\n       [0, 1, 2]])","title":"numpy.reference.random.generated.numpy.random.shuffle"},{"text":"numpy.random.Generator.shuffle method   random.Generator.shuffle(x, axis=0)\n \nModify an array or sequence in-place by shuffling its contents. The order of sub-arrays is changed but their contents remains the same.  Parameters \n \nxndarray or MutableSequence\n\n\nThe array, list or mutable sequence to be shuffled.  \naxisint, optional\n\n\nThe axis which x is shuffled along. Default is 0. It is only supported on ndarray objects.    Returns \n None\n   Examples >>> rng = np.random.default_rng()\n>>> arr = np.arange(10)\n>>> rng.shuffle(arr)\n>>> arr\n[1 7 5 2 9 4 3 6 0 8] # random\n >>> arr = np.arange(9).reshape((3, 3))\n>>> rng.shuffle(arr)\n>>> arr\narray([[3, 4, 5], # random\n       [6, 7, 8],\n       [0, 1, 2]])\n >>> arr = np.arange(9).reshape((3, 3))\n>>> rng.shuffle(arr, axis=1)\n>>> arr\narray([[2, 0, 1], # random\n       [5, 3, 4],\n       [8, 6, 7]])","title":"numpy.reference.random.generated.numpy.random.generator.shuffle"},{"text":"numpy.random.RandomState.shuffle method   random.RandomState.shuffle(x)\n \nModify a sequence in-place by shuffling its contents. This function only shuffles the array along the first axis of a multi-dimensional array. The order of sub-arrays is changed but their contents remains the same.  Note New code should use the shuffle method of a default_rng() instance instead; please see the Quick Start.   Parameters \n \nxndarray or MutableSequence\n\n\nThe array, list or mutable sequence to be shuffled.    Returns \n None\n    See also  Generator.shuffle\n\nwhich should be used for new code.    Examples >>> arr = np.arange(10)\n>>> np.random.shuffle(arr)\n>>> arr\n[1 7 5 2 9 4 3 6 0 8] # random\n Multi-dimensional arrays are only shuffled along the first axis: >>> arr = np.arange(9).reshape((3, 3))\n>>> np.random.shuffle(arr)\n>>> arr\narray([[3, 4, 5], # random\n       [6, 7, 8],\n       [0, 1, 2]])","title":"numpy.reference.random.generated.numpy.random.randomstate.shuffle"},{"text":"numpy.random.Generator.permuted method   random.Generator.permuted(x, axis=None, out=None)\n \nRandomly permute x along axis axis. Unlike shuffle, each slice along the given axis is shuffled independently of the others.  Parameters \n \nxarray_like, at least one-dimensional\n\n\nArray to be shuffled.  \naxisint, optional\n\n\nSlices of x in this axis are shuffled. Each slice is shuffled independently of the others. If axis is None, the flattened array is shuffled.  \noutndarray, optional\n\n\nIf given, this is the destinaton of the shuffled array. If out is None, a shuffled copy of the array is returned.    Returns \n ndarray\n\nIf out is None, a shuffled copy of x is returned. Otherwise, the shuffled array is stored in out, and out is returned      See also  shuffle\npermutation\n  Examples Create a numpy.random.Generator instance: >>> rng = np.random.default_rng()\n Create a test array: >>> x = np.arange(24).reshape(3, 8)\n>>> x\narray([[ 0,  1,  2,  3,  4,  5,  6,  7],\n       [ 8,  9, 10, 11, 12, 13, 14, 15],\n       [16, 17, 18, 19, 20, 21, 22, 23]])\n Shuffle the rows of x: >>> y = rng.permuted(x, axis=1)\n>>> y\narray([[ 4,  3,  6,  7,  1,  2,  5,  0],  # random\n       [15, 10, 14,  9, 12, 11,  8, 13],\n       [17, 16, 20, 21, 18, 22, 23, 19]])\n x has not been modified: >>> x\narray([[ 0,  1,  2,  3,  4,  5,  6,  7],\n       [ 8,  9, 10, 11, 12, 13, 14, 15],\n       [16, 17, 18, 19, 20, 21, 22, 23]])\n To shuffle the rows of x in-place, pass x as the out parameter: >>> y = rng.permuted(x, axis=1, out=x)\n>>> x\narray([[ 3,  0,  4,  7,  1,  6,  2,  5],  # random\n       [ 8, 14, 13,  9, 12, 11, 15, 10],\n       [17, 18, 16, 22, 19, 23, 20, 21]])\n Note that when the out parameter is given, the return value is out: >>> y is x\nTrue","title":"numpy.reference.random.generated.numpy.random.generator.permuted"},{"text":"array.reverse()  \nReverse the order of the items in the array.","title":"python.library.array#array.array.reverse"},{"text":"swapdims(dim0, dim1) \u2192 Tensor  \nSee torch.swapdims()","title":"torch.tensors#torch.Tensor.swapdims"},{"text":"flip(dims) \u2192 Tensor  \nSee torch.flip()","title":"torch.tensors#torch.Tensor.flip"},{"text":"numpy.r_   numpy.r_ = <numpy.lib.index_tricks.RClass object>\n \nTranslates slice objects to concatenation along the first axis. This is a simple way to build up arrays quickly. There are two use cases.  If the index expression contains comma separated arrays, then stack them along their first axis. If the index expression contains slice notation or scalars then create a 1-D array with a range indicated by the slice notation.  If slice notation is used, the syntax start:stop:step is equivalent to np.arange(start, stop, step) inside of the brackets. However, if step is an imaginary number (i.e. 100j) then its integer portion is interpreted as a number-of-points desired and the start and stop are inclusive. In other words start:stop:stepj is interpreted as np.linspace(start, stop, step, endpoint=1) inside of the brackets. After expansion of slice notation, all comma separated sequences are concatenated together. Optional character strings placed as the first element of the index expression can be used to change the output. The strings \u2018r\u2019 or \u2018c\u2019 result in matrix output. If the result is 1-D and \u2018r\u2019 is specified a 1 x N (row) matrix is produced. If the result is 1-D and \u2018c\u2019 is specified, then a N x 1 (column) matrix is produced. If the result is 2-D then both provide the same matrix result. A string integer specifies which axis to stack multiple comma separated arrays along. A string of two comma-separated integers allows indication of the minimum number of dimensions to force each entry into as the second integer (the axis to concatenate along is still the first integer). A string with three comma-separated integers allows specification of the axis to concatenate along, the minimum number of dimensions to force the entries to, and which axis should contain the start of the arrays which are less than the specified number of dimensions. In other words the third integer allows you to specify where the 1\u2019s should be placed in the shape of the arrays that have their shapes upgraded. By default, they are placed in the front of the shape tuple. The third argument allows you to specify where the start of the array should be instead. Thus, a third argument of \u20180\u2019 would place the 1\u2019s at the end of the array shape. Negative integers specify where in the new shape tuple the last dimension of upgraded arrays should be placed, so the default is \u2018-1\u2019.  Parameters \n Not a function, so takes no parameters\n  Returns \n A concatenated ndarray or matrix.\n    See also  concatenate\n\nJoin a sequence of arrays along an existing axis.  c_\n\nTranslates slice objects to concatenation along the second axis.    Examples >>> np.r_[np.array([1,2,3]), 0, 0, np.array([4,5,6])]\narray([1, 2, 3, ..., 4, 5, 6])\n>>> np.r_[-1:1:6j, [0]*3, 5, 6]\narray([-1. , -0.6, -0.2,  0.2,  0.6,  1. ,  0. ,  0. ,  0. ,  5. ,  6. ])\n String integers specify the axis to concatenate along or the minimum number of dimensions to force entries into. >>> a = np.array([[0, 1, 2], [3, 4, 5]])\n>>> np.r_['-1', a, a] # concatenate along last axis\narray([[0, 1, 2, 0, 1, 2],\n       [3, 4, 5, 3, 4, 5]])\n>>> np.r_['0,2', [1,2,3], [4,5,6]] # concatenate along first axis, dim>=2\narray([[1, 2, 3],\n       [4, 5, 6]])\n >>> np.r_['0,2,0', [1,2,3], [4,5,6]]\narray([[1],\n       [2],\n       [3],\n       [4],\n       [5],\n       [6]])\n>>> np.r_['1,2,0', [1,2,3], [4,5,6]]\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n Using \u2018r\u2019 or \u2018c\u2019 as a first string argument creates a matrix. >>> np.r_['r',[1,2,3], [4,5,6]]\nmatrix([[1, 2, 3, 4, 5, 6]])","title":"numpy.reference.generated.numpy.r_"}]}
{"task_id":32675861,"prompt":"def f_32675861(df):\n\t","suffix":"\n\treturn df","canonical_solution":"df['D'] = df['B']","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df_1 = pd.DataFrame({'A': [1,2,3], 'B': ['a', 'b', 'c']})\n    candidate(df_1)\n    assert (df_1['D'] == df_1['B']).all()\n","\n    df_2 = pd.DataFrame({'A': [1,2,3], 'B': [1, 'A', 'B']})\n    candidate(df_2)\n    assert (df_2['D'] == df_2['B']).all()\n","\n    df_3 = pd.DataFrame({'B': [1]})\n    candidate(df_3)\n    assert df_3['D'][0] == 1\n","\n    df_4 = pd.DataFrame({'B': []})\n    candidate(df_4)\n    assert len(df_4['D']) == 0\n"],"entry_point":"f_32675861","intent":"copy all values in a column 'B' to a new column 'D' in a pandas data frame 'df'","library":["pandas"],"docs":[]}
{"task_id":14227561,"prompt":"def f_14227561(data):\n\treturn ","suffix":"","canonical_solution":"list(data['A']['B'].values())[0]['maindata'][0]['Info']","test_start":"\nimport json\n\ndef check(candidate):","test":["\n    s1 = '{\"A\":{\"B\":{\"unknown\":{\"1\":\"F\",\"maindata\":[{\"Info\":\"TEXT\"}]}}}}'\n    data = json.loads(s1)\n    assert candidate(data) == 'TEXT'\n","\n    s2 = '{\"A\":{\"B\":{\"sample1\":{\"1\":\"F\",\"maindata\":[{\"Info\":\"TEXT!\"}]}}}}'\n    data = json.loads(s2)\n    assert candidate(data) == 'TEXT!'\n","\n    s3 = '{\"A\":{\"B\":{\"sample_weird_un\":{\"1\":\"F\",\"maindata\":[{\"Info\":\"!\"}]}}}}'\n    data = json.loads(s3)\n    assert candidate(data) == '!'\n","\n    s4 = '{\"A\":{\"B\":{\"sample_weird_un\":{\"1\":\"F\",\"maindata\":[{\"Info\":\"\"}]}}}}'\n    data = json.loads(s4)\n    assert candidate(data) == ''\n"],"entry_point":"f_14227561","intent":"find a value within nested json 'data' where the key inside another key 'B' is unknown.","library":["json"],"docs":[]}
{"task_id":14858916,"prompt":"def f_14858916(string, predicate):\n\treturn ","suffix":"","canonical_solution":"all(predicate(x) for x in string)","test_start":"\ndef check(candidate):","test":["\n    def predicate(x):\n        if x == 'a':\n            return True\n        else:\n            return False\n    assert candidate('aab', predicate) == False\n","\n    def predicate(x):\n        if x == 'a':\n            return True\n        else:\n            return False\n    assert candidate('aa', predicate) == True\n","\n    def predicate(x):\n        if x == 'a':\n            return True\n        else:\n            return False\n    assert candidate('', predicate) == True\n","\n    def predicate(x):\n        if x.islower():\n            return True\n        else:\n            return False\n    assert candidate('abc', predicate) == True\n","\n    def predicate(x):\n        if x.islower():\n            return True\n        else:\n            return False\n    assert candidate('Ab', predicate) == False\n","\n    def predicate(x):\n        if x.islower():\n            return True\n        else:\n            return False\n    assert candidate('ABCD', predicate) == False\n"],"entry_point":"f_14858916","intent":"check characters of string `string` are true predication of function `predicate`","library":[],"docs":[]}
{"task_id":574236,"prompt":"def f_574236():\n\treturn ","suffix":"","canonical_solution":"os.statvfs('\/').f_files - os.statvfs('\/').f_ffree","test_start":"\nimport os \n\ndef check(candidate):","test":["\n    assert candidate() == (os.statvfs('\/').f_files - os.statvfs('\/').f_ffree)\n"],"entry_point":"f_574236","intent":"determine number of files on a drive with python","library":["os"],"docs":[{"text":"ZipInfo.volume  \nVolume number of file header.","title":"python.library.zipfile#zipfile.ZipInfo.volume"},{"text":"st_nlink  \nNumber of hard links.","title":"python.library.os#os.stat_result.st_nlink"},{"text":"test.support.fd_count()  \nCount the number of open file descriptors.","title":"python.library.test#test.support.fd_count"},{"text":"stat.ST_SIZE  \nSize in bytes of a plain file; amount of data waiting on some special files.","title":"python.library.stat#stat.ST_SIZE"},{"text":"st_rsize  \nReal size of the file.","title":"python.library.os#os.stat_result.st_rsize"},{"text":"stat.ST_NLINK  \nNumber of links to the inode.","title":"python.library.stat#stat.ST_NLINK"},{"text":"st_gen  \nFile generation number.","title":"python.library.os#os.stat_result.st_gen"},{"text":"st_gid  \nGroup identifier of the file owner.","title":"python.library.os#os.stat_result.st_gid"},{"text":"st_uid  \nUser identifier of the file owner.","title":"python.library.os#os.stat_result.st_uid"},{"text":"ZipInfo.file_size  \nSize of the uncompressed file.","title":"python.library.zipfile#zipfile.ZipInfo.file_size"}]}
{"task_id":7011291,"prompt":"def f_7011291(cursor):\n\treturn ","suffix":"","canonical_solution":"cursor.fetchone()[0]","test_start":"\nimport sqlite3\n\ndef check(candidate):","test":["\n    conn = sqlite3.connect('main')\n    cursor = conn.cursor()\n    cursor.execute(\"CREATE TABLE student (name VARCHAR(10))\")\n    cursor.execute(\"INSERT INTO student VALUES('abc')\")\n    cursor.execute(\"SELECT * FROM student\")\n    assert candidate(cursor) == 'abc'\n"],"entry_point":"f_7011291","intent":"how to get a single result from a SQLite query from `cursor`","library":["sqlite3"],"docs":[{"text":"fetchone()  \nFetches the next row of a query result set, returning a single sequence, or None when no more data is available.","title":"python.library.sqlite3#sqlite3.Cursor.fetchone"},{"text":"class sqlite3.Cursor  \nA Cursor instance has the following attributes and methods.  \nexecute(sql[, parameters])  \nExecutes an SQL statement. Values may be bound to the statement using placeholders. execute() will only execute a single SQL statement. If you try to execute more than one statement with it, it will raise a Warning. Use executescript() if you want to execute multiple SQL statements with one call. \n  \nexecutemany(sql, seq_of_parameters)  \nExecutes a parameterized SQL command against all parameter sequences or mappings found in the sequence seq_of_parameters. The sqlite3 module also allows using an iterator yielding parameters instead of a sequence. import sqlite3\n\nclass IterChars:\n    def __init__(self):\n        self.count = ord('a')\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.count > ord('z'):\n            raise StopIteration\n        self.count += 1\n        return (chr(self.count - 1),) # this is a 1-tuple\n\ncon = sqlite3.connect(\":memory:\")\ncur = con.cursor()\ncur.execute(\"create table characters(c)\")\n\ntheIter = IterChars()\ncur.executemany(\"insert into characters(c) values (?)\", theIter)\n\ncur.execute(\"select c from characters\")\nprint(cur.fetchall())\n\ncon.close()\n Here\u2019s a shorter example using a generator: import sqlite3\nimport string\n\ndef char_generator():\n    for c in string.ascii_lowercase:\n        yield (c,)\n\ncon = sqlite3.connect(\":memory:\")\ncur = con.cursor()\ncur.execute(\"create table characters(c)\")\n\ncur.executemany(\"insert into characters(c) values (?)\", char_generator())\n\ncur.execute(\"select c from characters\")\nprint(cur.fetchall())\n\ncon.close()\n \n  \nexecutescript(sql_script)  \nThis is a nonstandard convenience method for executing multiple SQL statements at once. It issues a COMMIT statement first, then executes the SQL script it gets as a parameter. sql_script can be an instance of str. Example: import sqlite3\n\ncon = sqlite3.connect(\":memory:\")\ncur = con.cursor()\ncur.executescript(\"\"\"\n    create table person(\n        firstname,\n        lastname,\n        age\n    );\n\n    create table book(\n        title,\n        author,\n        published\n    );\n\n    insert into book(title, author, published)\n    values (\n        'Dirk Gently''s Holistic Detective Agency',\n        'Douglas Adams',\n        1987\n    );\n    \"\"\")\ncon.close()\n \n  \nfetchone()  \nFetches the next row of a query result set, returning a single sequence, or None when no more data is available. \n  \nfetchmany(size=cursor.arraysize)  \nFetches the next set of rows of a query result, returning a list. An empty list is returned when no more rows are available. The number of rows to fetch per call is specified by the size parameter. If it is not given, the cursor\u2019s arraysize determines the number of rows to be fetched. The method should try to fetch as many rows as indicated by the size parameter. If this is not possible due to the specified number of rows not being available, fewer rows may be returned. Note there are performance considerations involved with the size parameter. For optimal performance, it is usually best to use the arraysize attribute. If the size parameter is used, then it is best for it to retain the same value from one fetchmany() call to the next. \n  \nfetchall()  \nFetches all (remaining) rows of a query result, returning a list. Note that the cursor\u2019s arraysize attribute can affect the performance of this operation. An empty list is returned when no rows are available. \n  \nclose()  \nClose the cursor now (rather than whenever __del__ is called). The cursor will be unusable from this point forward; a ProgrammingError exception will be raised if any operation is attempted with the cursor. \n  \nrowcount  \nAlthough the Cursor class of the sqlite3 module implements this attribute, the database engine\u2019s own support for the determination of \u201crows affected\u201d\/\u201drows selected\u201d is quirky. For executemany() statements, the number of modifications are summed up into rowcount. As required by the Python DB API Spec, the rowcount attribute \u201cis -1 in case no executeXX() has been performed on the cursor or the rowcount of the last operation is not determinable by the interface\u201d. This includes SELECT statements because we cannot determine the number of rows a query produced until all rows were fetched. With SQLite versions before 3.6.5, rowcount is set to 0 if you make a DELETE FROM table without any condition. \n  \nlastrowid  \nThis read-only attribute provides the rowid of the last modified row. It is only set if you issued an INSERT or a REPLACE statement using the execute() method. For operations other than INSERT or REPLACE or when executemany() is called, lastrowid is set to None. If the INSERT or REPLACE statement failed to insert the previous successful rowid is returned.  Changed in version 3.6: Added support for the REPLACE statement.  \n  \narraysize  \nRead\/write attribute that controls the number of rows returned by fetchmany(). The default value is 1 which means a single row would be fetched per call. \n  \ndescription  \nThis read-only attribute provides the column names of the last query. To remain compatible with the Python DB API, it returns a 7-tuple for each column where the last six items of each tuple are None. It is set for SELECT statements without any matching rows as well. \n  \nconnection  \nThis read-only attribute provides the SQLite database Connection used by the Cursor object. A Cursor object created by calling con.cursor() will have a connection attribute that refers to con: >>> con = sqlite3.connect(\":memory:\")\n>>> cur = con.cursor()\n>>> cur.connection == con\nTrue","title":"python.library.sqlite3#sqlite3.Cursor"},{"text":"execute(sql[, parameters])  \nThis is a nonstandard shortcut that creates a cursor object by calling the cursor() method, calls the cursor\u2019s execute() method with the parameters given, and returns the cursor.","title":"python.library.sqlite3#sqlite3.Connection.execute"},{"text":"execute(sql[, parameters])  \nExecutes an SQL statement. Values may be bound to the statement using placeholders. execute() will only execute a single SQL statement. If you try to execute more than one statement with it, it will raise a Warning. Use executescript() if you want to execute multiple SQL statements with one call.","title":"python.library.sqlite3#sqlite3.Cursor.execute"},{"text":"arraysize  \nRead\/write attribute that controls the number of rows returned by fetchmany(). The default value is 1 which means a single row would be fetched per call.","title":"python.library.sqlite3#sqlite3.Cursor.arraysize"},{"text":"row_factory  \nYou can change this attribute to a callable that accepts the cursor and the original row as a tuple and will return the real result row. This way, you can implement more advanced ways of returning results, such as returning an object that can also access columns by name. Example: import sqlite3\n\ndef dict_factory(cursor, row):\n    d = {}\n    for idx, col in enumerate(cursor.description):\n        d[col[0]] = row[idx]\n    return d\n\ncon = sqlite3.connect(\":memory:\")\ncon.row_factory = dict_factory\ncur = con.cursor()\ncur.execute(\"select 1 as a\")\nprint(cur.fetchone()[\"a\"])\n\ncon.close()\n If returning a tuple doesn\u2019t suffice and you want name-based access to columns, you should consider setting row_factory to the highly-optimized sqlite3.Row type. Row provides both index-based and case-insensitive name-based access to columns with almost no memory overhead. It will probably be better than your own custom dictionary-based approach or even a db_row based solution.","title":"python.library.sqlite3#sqlite3.Connection.row_factory"},{"text":"fetchall()  \nFetches all (remaining) rows of a query result, returning a list. Note that the cursor\u2019s arraysize attribute can affect the performance of this operation. An empty list is returned when no rows are available.","title":"python.library.sqlite3#sqlite3.Cursor.fetchall"},{"text":"cursor(factory=Cursor)  \nThe cursor method accepts a single optional parameter factory. If supplied, this must be a callable returning an instance of Cursor or its subclasses.","title":"python.library.sqlite3#sqlite3.Connection.cursor"},{"text":"exception sqlite3.Warning  \nA subclass of Exception.","title":"python.library.sqlite3#sqlite3.Warning"},{"text":"executemany(sql, seq_of_parameters)  \nExecutes a parameterized SQL command against all parameter sequences or mappings found in the sequence seq_of_parameters. The sqlite3 module also allows using an iterator yielding parameters instead of a sequence. import sqlite3\n\nclass IterChars:\n    def __init__(self):\n        self.count = ord('a')\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.count > ord('z'):\n            raise StopIteration\n        self.count += 1\n        return (chr(self.count - 1),) # this is a 1-tuple\n\ncon = sqlite3.connect(\":memory:\")\ncur = con.cursor()\ncur.execute(\"create table characters(c)\")\n\ntheIter = IterChars()\ncur.executemany(\"insert into characters(c) values (?)\", theIter)\n\ncur.execute(\"select c from characters\")\nprint(cur.fetchall())\n\ncon.close()\n Here\u2019s a shorter example using a generator: import sqlite3\nimport string\n\ndef char_generator():\n    for c in string.ascii_lowercase:\n        yield (c,)\n\ncon = sqlite3.connect(\":memory:\")\ncur = con.cursor()\ncur.execute(\"create table characters(c)\")\n\ncur.executemany(\"insert into characters(c) values (?)\", char_generator())\n\ncur.execute(\"select c from characters\")\nprint(cur.fetchall())\n\ncon.close()","title":"python.library.sqlite3#sqlite3.Cursor.executemany"}]}
{"task_id":6378889,"prompt":"def f_6378889(user_input):\n\t","suffix":"\n\treturn user_list","canonical_solution":"user_list = [int(number) for number in user_input.split(',')]","test_start":"\ndef check(candidate):","test":["\n    assert candidate('0') == [0]\n","\n    assert candidate('12') == [12]\n","\n    assert candidate('12,33,223') == [12, 33, 223]\n"],"entry_point":"f_6378889","intent":"convert string `user_input` into a list of integers `user_list`","library":[],"docs":[]}
{"task_id":6378889,"prompt":"def f_6378889(user):\n\treturn ","suffix":"","canonical_solution":"[int(s) for s in user.split(',')]","test_start":"\ndef check(candidate):","test":["\n    assert candidate('0') == [0]\n","\n    assert candidate('12') == [12]\n","\n    assert candidate('12,33,223') == [12, 33, 223]\n"],"entry_point":"f_6378889","intent":"Get a list of integers by splitting  a string `user` with comma","library":[],"docs":[]}
{"task_id":5212870,"prompt":"def f_5212870(list):\n\treturn ","suffix":"","canonical_solution":"sorted(list, key=lambda x: (x[0], -x[1]))","test_start":"\ndef check(candidate):","test":["\n    list = [(9, 0), (9, 1), (9, -1), (8, 5), (4, 5)]\n    assert candidate(list) == [(4, 5), (8, 5), (9, 1), (9, 0), (9, -1)]\n"],"entry_point":"f_5212870","intent":"Sorting a Python list `list` by the first item ascending and last item descending","library":[],"docs":[]}
{"task_id":403421,"prompt":"def f_403421(ut, cmpfun):\n\t","suffix":"\n\treturn ut","canonical_solution":"ut.sort(key=cmpfun, reverse=True)","test_start":"\ndef check(candidate):","test":["\n    assert candidate([], lambda x: x) == []\n","\n    assert candidate(['a', 'b', 'c'], lambda x: x) == ['c', 'b', 'a']\n","\n    assert candidate([2, 1, 3], lambda x: -x) == [1, 2, 3]\n"],"entry_point":"f_403421","intent":"sort a list of objects `ut`, based on a function `cmpfun` in descending order","library":[],"docs":[]}
{"task_id":403421,"prompt":"def f_403421(ut):\n\t","suffix":"\n\treturn ut","canonical_solution":"ut.sort(key=lambda x: x.count, reverse=True)","test_start":"\nclass Tag: \n    def __init__(self, name, count): \n        self.name = name \n        self.count = count \n\n    def __str__(self):\n        return f\"[{self.name}]-[{self.count}]\"\n\ndef check(candidate):","test":["\n    result = candidate([Tag(\"red\", 1), Tag(\"blue\", 22), Tag(\"black\", 0)])\n    assert (result[0].name == \"blue\") and (result[0].count == 22)\n    assert (result[1].name == \"red\") and (result[1].count == 1)\n    assert (result[2].name == \"black\") and (result[2].count == 0)\n"],"entry_point":"f_403421","intent":"reverse list `ut` based on the `count` attribute of each object","library":[],"docs":[]}
{"task_id":3944876,"prompt":"def f_3944876(i):\n\treturn ","suffix":"","canonical_solution":"'ME' + str(i)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(100) == \"ME100\"\n","\n    assert candidate(0.22) == \"ME0.22\"\n","\n    assert candidate(\"text\") == \"MEtext\"\n"],"entry_point":"f_3944876","intent":"cast an int `i` to a string and concat to string 'ME'","library":[],"docs":[]}
{"task_id":40903174,"prompt":"def f_40903174(df):\n\treturn ","suffix":"","canonical_solution":"df.sort_values(['System_num', 'Dis'])","test_start":"\nimport pandas as pd \n\ndef check(candidate):","test":["\n    df1 = pd.DataFrame([[6, 1, 1], [5, 1, 1], [4, 1, 1], [3, 2, 1], [2, 2, 1], [1, 2, 1]], columns = ['Dis', 'System_num', 'Energy'])\n    df_ans1 = pd.DataFrame([[4, 1, 1], [5, 1, 1], [6, 1, 1], [1, 2, 1], [2, 2, 1], [3, 2, 1]], columns = ['Dis', 'System_num', 'Energy'])\n    assert (df_ans1.equals(candidate(df1).reset_index(drop = True))) == True\n","\n    df2 = pd.DataFrame([[6, 3, 1], [5, 2, 1], [4, 1, 1]], columns = ['Dis', 'System_num', 'Energy'])\n    df_ans2 = pd.DataFrame([[4, 1, 1], [5, 2, 1], [6, 3, 1]], columns = ['Dis', 'System_num', 'Energy'])\n    assert (df_ans2.equals(candidate(df2).reset_index(drop = True))) == True\n","\n    df3 = pd.DataFrame([[1, 3, 1], [3, 3, 1], [2, 3, 1], [6, 1, 1], [4, 1, 1], [5, 2, 1], [3, 2, 1]], columns = ['Dis', 'System_num', 'Energy'])\n    df_ans3 = pd.DataFrame([[4, 1,1], [6, 1, 1], [3, 2, 1], [5, 2, 1], [1, 3, 1], [2, 3, 1], [3, 3, 1]], columns = ['Dis', 'System_num', 'Energy'])\n    assert (df_ans3.equals(candidate(df3).reset_index(drop = True))) == True \n","\n    df4 = pd.DataFrame([[1, 2, 3], [1, 2, 3], [4, 1, 3]], columns = ['Dis', 'System_num', 'Energy'])\n    df_ans4 = pd.DataFrame([[1, 2, 3], [1, 2, 3], [4, 1, 3]])\n    assert (df_ans4.equals(candidate(df4).reset_index(drop = True))) == False\n"],"entry_point":"f_40903174","intent":"Sorting data in Pandas DataFrame `df` with columns 'System_num' and 'Dis'","library":["pandas"],"docs":[{"text":"pandas.Index.sort   finalIndex.sort(*args, **kwargs)[source]\n \nUse sort_values instead.","title":"pandas.reference.api.pandas.index.sort"},{"text":"pandas.DataFrame.sort_values   DataFrame.sort_values(by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last', ignore_index=False, key=None)[source]\n \nSort by the values along either axis.  Parameters \n \nby:str or list of str\n\n\nName or list of names to sort by.  if axis is 0 or \u2018index\u2019 then by may contain index levels and\/or column labels. if axis is 1 or \u2018columns\u2019 then by may contain column levels and\/or index labels.   \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019}, default 0\n\n\nAxis to be sorted.  \nascending:bool or list of bool, default True\n\n\nSort ascending vs. descending. Specify list for multiple sort orders. If this is a list of bools, must match the length of the by.  \ninplace:bool, default False\n\n\nIf True, perform operation in-place.  \nkind:{\u2018quicksort\u2019, \u2018mergesort\u2019, \u2018heapsort\u2019, \u2018stable\u2019}, default \u2018quicksort\u2019\n\n\nChoice of sorting algorithm. See also numpy.sort() for more information. mergesort and stable are the only stable algorithms. For DataFrames, this option is only applied when sorting on a single column or label.  \nna_position:{\u2018first\u2019, \u2018last\u2019}, default \u2018last\u2019\n\n\nPuts NaNs at the beginning if first; last puts NaNs at the end.  \nignore_index:bool, default False\n\n\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.  New in version 1.0.0.   \nkey:callable, optional\n\n\nApply the key function to the values before sorting. This is similar to the key argument in the builtin sorted() function, with the notable difference that this key function should be vectorized. It should expect a Series and return a Series with the same shape as the input. It will be applied to each column in by independently.  New in version 1.1.0.     Returns \n DataFrame or None\n\nDataFrame with sorted values or None if inplace=True.      See also  DataFrame.sort_index\n\nSort a DataFrame by the index.  Series.sort_values\n\nSimilar method for a Series.    Examples \n>>> df = pd.DataFrame({\n...     'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],\n...     'col2': [2, 1, 9, 8, 7, 4],\n...     'col3': [0, 1, 9, 4, 2, 3],\n...     'col4': ['a', 'B', 'c', 'D', 'e', 'F']\n... })\n>>> df\n  col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\n  Sort by col1 \n>>> df.sort_values(by=['col1'])\n  col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n5    C     4     3    F\n4    D     7     2    e\n3  NaN     8     4    D\n  Sort by multiple columns \n>>> df.sort_values(by=['col1', 'col2'])\n  col1  col2  col3 col4\n1    A     1     1    B\n0    A     2     0    a\n2    B     9     9    c\n5    C     4     3    F\n4    D     7     2    e\n3  NaN     8     4    D\n  Sort Descending \n>>> df.sort_values(by='col1', ascending=False)\n  col1  col2  col3 col4\n4    D     7     2    e\n5    C     4     3    F\n2    B     9     9    c\n0    A     2     0    a\n1    A     1     1    B\n3  NaN     8     4    D\n  Putting NAs first \n>>> df.sort_values(by='col1', ascending=False, na_position='first')\n  col1  col2  col3 col4\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\n2    B     9     9    c\n0    A     2     0    a\n1    A     1     1    B\n  Sorting with a key function \n>>> df.sort_values(by='col4', key=lambda col: col.str.lower())\n   col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\n  Natural sort with the key argument, using the natsort <https:\/\/github.com\/SethMMorton\/natsort> package. \n>>> df = pd.DataFrame({\n...    \"time\": ['0hr', '128hr', '72hr', '48hr', '96hr'],\n...    \"value\": [10, 20, 30, 40, 50]\n... })\n>>> df\n    time  value\n0    0hr     10\n1  128hr     20\n2   72hr     30\n3   48hr     40\n4   96hr     50\n>>> from natsort import index_natsorted\n>>> df.sort_values(\n...    by=\"time\",\n...    key=lambda x: np.argsort(index_natsorted(df[\"time\"]))\n... )\n    time  value\n0    0hr     10\n3   48hr     40\n2   72hr     30\n4   96hr     50\n1  128hr     20","title":"pandas.reference.api.pandas.dataframe.sort_values"},{"text":"pandas.Index.argsort   Index.argsort(*args, **kwargs)[source]\n \nReturn the integer indices that would sort the index.  Parameters \n *args\n\nPassed to numpy.ndarray.argsort.  **kwargs\n\nPassed to numpy.ndarray.argsort.    Returns \n np.ndarray[np.intp]\n\nInteger indices that would sort the index if used as an indexer.      See also  numpy.argsort\n\nSimilar method for NumPy arrays.  Index.sort_values\n\nReturn sorted copy of Index.    Examples \n>>> idx = pd.Index(['b', 'a', 'd', 'c'])\n>>> idx\nIndex(['b', 'a', 'd', 'c'], dtype='object')\n  \n>>> order = idx.argsort()\n>>> order\narray([1, 0, 3, 2])\n  \n>>> idx[order]\nIndex(['a', 'b', 'c', 'd'], dtype='object')","title":"pandas.reference.api.pandas.index.argsort"},{"text":"pandas.DataFrame.nsmallest   DataFrame.nsmallest(n, columns, keep='first')[source]\n \nReturn the first n rows ordered by columns in ascending order. Return the first n rows with the smallest values in columns, in ascending order. The columns that are not specified are returned as well, but not used for ordering. This method is equivalent to df.sort_values(columns, ascending=True).head(n), but more performant.  Parameters \n \nn:int\n\n\nNumber of items to retrieve.  \ncolumns:list or str\n\n\nColumn name or names to order by.  \nkeep:{\u2018first\u2019, \u2018last\u2019, \u2018all\u2019}, default \u2018first\u2019\n\n\nWhere there are duplicate values:  first : take the first occurrence. last : take the last occurrence. all : do not drop any duplicates, even it means selecting more than n items.     Returns \n DataFrame\n    See also  DataFrame.nlargest\n\nReturn the first n rows ordered by columns in descending order.  DataFrame.sort_values\n\nSort DataFrame by the values.  DataFrame.head\n\nReturn the first n rows without re-ordering.    Examples \n>>> df = pd.DataFrame({'population': [59000000, 65000000, 434000,\n...                                   434000, 434000, 337000, 337000,\n...                                   11300, 11300],\n...                    'GDP': [1937894, 2583560 , 12011, 4520, 12128,\n...                            17036, 182, 38, 311],\n...                    'alpha-2': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\",\n...                                \"IS\", \"NR\", \"TV\", \"AI\"]},\n...                   index=[\"Italy\", \"France\", \"Malta\",\n...                          \"Maldives\", \"Brunei\", \"Iceland\",\n...                          \"Nauru\", \"Tuvalu\", \"Anguilla\"])\n>>> df\n          population      GDP alpha-2\nItaly       59000000  1937894      IT\nFrance      65000000  2583560      FR\nMalta         434000    12011      MT\nMaldives      434000     4520      MV\nBrunei        434000    12128      BN\nIceland       337000    17036      IS\nNauru         337000      182      NR\nTuvalu         11300       38      TV\nAnguilla       11300      311      AI\n  In the following example, we will use nsmallest to select the three rows having the smallest values in column \u201cpopulation\u201d. \n>>> df.nsmallest(3, 'population')\n          population    GDP alpha-2\nTuvalu         11300     38      TV\nAnguilla       11300    311      AI\nIceland       337000  17036      IS\n  When using keep='last', ties are resolved in reverse order: \n>>> df.nsmallest(3, 'population', keep='last')\n          population  GDP alpha-2\nAnguilla       11300  311      AI\nTuvalu         11300   38      TV\nNauru         337000  182      NR\n  When using keep='all', all duplicate items are maintained: \n>>> df.nsmallest(3, 'population', keep='all')\n          population    GDP alpha-2\nTuvalu         11300     38      TV\nAnguilla       11300    311      AI\nIceland       337000  17036      IS\nNauru         337000    182      NR\n  To order by the smallest values in column \u201cpopulation\u201d and then \u201cGDP\u201d, we can specify multiple columns like in the next example. \n>>> df.nsmallest(3, ['population', 'GDP'])\n          population  GDP alpha-2\nTuvalu         11300   38      TV\nAnguilla       11300  311      AI\nNauru         337000  182      NR","title":"pandas.reference.api.pandas.dataframe.nsmallest"},{"text":"colno  \nThe column corresponding to pos.","title":"python.library.json#json.JSONDecodeError.colno"},{"text":"pandas.DataFrame.sort_index   DataFrame.sort_index(axis=0, level=None, ascending=True, inplace=False, kind='quicksort', na_position='last', sort_remaining=True, ignore_index=False, key=None)[source]\n \nSort object by labels (along an axis). Returns a new DataFrame sorted by label if inplace argument is False, otherwise updates the original DataFrame and returns None.  Parameters \n \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019}, default 0\n\n\nThe axis along which to sort. The value 0 identifies the rows, and 1 identifies the columns.  \nlevel:int or level name or list of ints or list of level names\n\n\nIf not None, sort on values in specified index level(s).  \nascending:bool or list-like of bools, default True\n\n\nSort ascending vs. descending. When the index is a MultiIndex the sort direction can be controlled for each level individually.  \ninplace:bool, default False\n\n\nIf True, perform operation in-place.  \nkind:{\u2018quicksort\u2019, \u2018mergesort\u2019, \u2018heapsort\u2019, \u2018stable\u2019}, default \u2018quicksort\u2019\n\n\nChoice of sorting algorithm. See also numpy.sort() for more information. mergesort and stable are the only stable algorithms. For DataFrames, this option is only applied when sorting on a single column or label.  \nna_position:{\u2018first\u2019, \u2018last\u2019}, default \u2018last\u2019\n\n\nPuts NaNs at the beginning if first; last puts NaNs at the end. Not implemented for MultiIndex.  \nsort_remaining:bool, default True\n\n\nIf True and sorting by level and index is multilevel, sort by other levels too (in order) after sorting by specified level.  \nignore_index:bool, default False\n\n\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.  New in version 1.0.0.   \nkey:callable, optional\n\n\nIf not None, apply the key function to the index values before sorting. This is similar to the key argument in the builtin sorted() function, with the notable difference that this key function should be vectorized. It should expect an Index and return an Index of the same shape. For MultiIndex inputs, the key is applied per level.  New in version 1.1.0.     Returns \n DataFrame or None\n\nThe original DataFrame sorted by the labels or None if inplace=True.      See also  Series.sort_index\n\nSort Series by the index.  DataFrame.sort_values\n\nSort DataFrame by the value.  Series.sort_values\n\nSort Series by the value.    Examples \n>>> df = pd.DataFrame([1, 2, 3, 4, 5], index=[100, 29, 234, 1, 150],\n...                   columns=['A'])\n>>> df.sort_index()\n     A\n1    4\n29   2\n100  1\n150  5\n234  3\n  By default, it sorts in ascending order, to sort in descending order, use ascending=False \n>>> df.sort_index(ascending=False)\n     A\n234  3\n150  5\n100  1\n29   2\n1    4\n  A key function can be specified which is applied to the index before sorting. For a MultiIndex this is applied to each level separately. \n>>> df = pd.DataFrame({\"a\": [1, 2, 3, 4]}, index=['A', 'b', 'C', 'd'])\n>>> df.sort_index(key=lambda x: x.str.lower())\n   a\nA  1\nb  2\nC  3\nd  4","title":"pandas.reference.api.pandas.dataframe.sort_index"},{"text":"pandas.Series.sort_values   Series.sort_values(axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last', ignore_index=False, key=None)[source]\n \nSort by the values. Sort a Series in ascending or descending order by some criterion.  Parameters \n \naxis:{0 or \u2018index\u2019}, default 0\n\n\nAxis to direct sorting. The value \u2018index\u2019 is accepted for compatibility with DataFrame.sort_values.  \nascending:bool or list of bools, default True\n\n\nIf True, sort values in ascending order, otherwise descending.  \ninplace:bool, default False\n\n\nIf True, perform operation in-place.  \nkind:{\u2018quicksort\u2019, \u2018mergesort\u2019, \u2018heapsort\u2019, \u2018stable\u2019}, default \u2018quicksort\u2019\n\n\nChoice of sorting algorithm. See also numpy.sort() for more information. \u2018mergesort\u2019 and \u2018stable\u2019 are the only stable algorithms.  \nna_position:{\u2018first\u2019 or \u2018last\u2019}, default \u2018last\u2019\n\n\nArgument \u2018first\u2019 puts NaNs at the beginning, \u2018last\u2019 puts NaNs at the end.  \nignore_index:bool, default False\n\n\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.  New in version 1.0.0.   \nkey:callable, optional\n\n\nIf not None, apply the key function to the series values before sorting. This is similar to the key argument in the builtin sorted() function, with the notable difference that this key function should be vectorized. It should expect a Series and return an array-like.  New in version 1.1.0.     Returns \n Series or None\n\nSeries ordered by values or None if inplace=True.      See also  Series.sort_index\n\nSort by the Series indices.  DataFrame.sort_values\n\nSort DataFrame by the values along either axis.  DataFrame.sort_index\n\nSort DataFrame by indices.    Examples \n>>> s = pd.Series([np.nan, 1, 3, 10, 5])\n>>> s\n0     NaN\n1     1.0\n2     3.0\n3     10.0\n4     5.0\ndtype: float64\n  Sort values ascending order (default behaviour) \n>>> s.sort_values(ascending=True)\n1     1.0\n2     3.0\n4     5.0\n3    10.0\n0     NaN\ndtype: float64\n  Sort values descending order \n>>> s.sort_values(ascending=False)\n3    10.0\n4     5.0\n2     3.0\n1     1.0\n0     NaN\ndtype: float64\n  Sort values inplace \n>>> s.sort_values(ascending=False, inplace=True)\n>>> s\n3    10.0\n4     5.0\n2     3.0\n1     1.0\n0     NaN\ndtype: float64\n  Sort values putting NAs first \n>>> s.sort_values(na_position='first')\n0     NaN\n1     1.0\n2     3.0\n4     5.0\n3    10.0\ndtype: float64\n  Sort a series of strings \n>>> s = pd.Series(['z', 'b', 'd', 'a', 'c'])\n>>> s\n0    z\n1    b\n2    d\n3    a\n4    c\ndtype: object\n  \n>>> s.sort_values()\n3    a\n1    b\n4    c\n2    d\n0    z\ndtype: object\n  Sort using a key function. Your key function will be given the Series of values and should return an array-like. \n>>> s = pd.Series(['a', 'B', 'c', 'D', 'e'])\n>>> s.sort_values()\n1    B\n3    D\n0    a\n2    c\n4    e\ndtype: object\n>>> s.sort_values(key=lambda x: x.str.lower())\n0    a\n1    B\n2    c\n3    D\n4    e\ndtype: object\n  NumPy ufuncs work well here. For example, we can sort by the sin of the value \n>>> s = pd.Series([-4, -2, 0, 2, 4])\n>>> s.sort_values(key=np.sin)\n1   -2\n4    4\n2    0\n0   -4\n3    2\ndtype: int64\n  More complicated user-defined functions can be used, as long as they expect a Series and return an array-like \n>>> s.sort_values(key=lambda x: (np.tan(x.cumsum())))\n0   -4\n3    2\n4    4\n1   -2\n2    0\ndtype: int64","title":"pandas.reference.api.pandas.series.sort_values"},{"text":"colno  \nThe column corresponding to pos (may be None).","title":"python.library.re#re.error.colno"},{"text":"pandas.Index.sort_values   Index.sort_values(return_indexer=False, ascending=True, na_position='last', key=None)[source]\n \nReturn a sorted copy of the index. Return a sorted copy of the index, and optionally return the indices that sorted the index itself.  Parameters \n \nreturn_indexer:bool, default False\n\n\nShould the indices that would sort the index be returned.  \nascending:bool, default True\n\n\nShould the index values be sorted in an ascending order.  \nna_position:{\u2018first\u2019 or \u2018last\u2019}, default \u2018last\u2019\n\n\nArgument \u2018first\u2019 puts NaNs at the beginning, \u2018last\u2019 puts NaNs at the end.  New in version 1.2.0.   \nkey:callable, optional\n\n\nIf not None, apply the key function to the index values before sorting. This is similar to the key argument in the builtin sorted() function, with the notable difference that this key function should be vectorized. It should expect an Index and return an Index of the same shape.  New in version 1.1.0.     Returns \n \nsorted_index:pandas.Index\n\n\nSorted copy of the index.  \nindexer:numpy.ndarray, optional\n\n\nThe indices that the index itself was sorted by.      See also  Series.sort_values\n\nSort values of a Series.  DataFrame.sort_values\n\nSort values in a DataFrame.    Examples \n>>> idx = pd.Index([10, 100, 1, 1000])\n>>> idx\nInt64Index([10, 100, 1, 1000], dtype='int64')\n  Sort values in ascending order (default behavior). \n>>> idx.sort_values()\nInt64Index([1, 10, 100, 1000], dtype='int64')\n  Sort values in descending order, and also get the indices idx was sorted by. \n>>> idx.sort_values(ascending=False, return_indexer=True)\n(Int64Index([1000, 100, 10, 1], dtype='int64'), array([3, 1, 0, 2]))","title":"pandas.reference.api.pandas.index.sort_values"},{"text":"pandas.tseries.offsets.Nano.n   Nano.n","title":"pandas.reference.api.pandas.tseries.offsets.nano.n"}]}
{"task_id":4454298,"prompt":"def f_4454298(infile, outfile):\n\t","suffix":"\n\treturn ","canonical_solution":"open(outfile, 'w').write('#test firstline\\n' + open(infile).read())","test_start":"\nimport filecmp\n\ndef check(candidate): ","test":["\n    open('test1.txt', 'w').write('test1')\n    candidate('test1.txt', 'test1_out.txt')\n    open('test1_ans.txt', 'w').write('#test firstline\\ntest1')\n    assert filecmp.cmp('test1_out.txt', 'test1_ans.txt') == True\n","\n    open('test2.txt', 'w').write('\\ntest2\\n')\n    candidate('test2.txt', 'test2_out.txt')\n    open('test2_ans.txt', 'w').write('#test firstline\\n\\ntest2\\n')\n    assert filecmp.cmp('test2_out.txt', 'test2_ans.txt') == True\n","\n    open('test3.txt', 'w').write(' \\n \\n')\n    candidate('test3.txt', 'test3_out.txt')\n    open('test3_ans.txt', 'w').write('#test firstline\\n \\n \\n')\n    assert filecmp.cmp('test3_out.txt', 'test3_ans.txt') == True\n","\n    open('test4.txt', 'w').write('hello')\n    candidate('test4.txt', 'test4_out.txt')\n    open('test4_ans.txt', 'w').write('hello')\n    assert filecmp.cmp('test4_out.txt', 'test4_ans.txt') == False\n"],"entry_point":"f_4454298","intent":"prepend the line '#test firstline\\n' to the contents of file 'infile' and save as the file 'outfile'","library":["filecmp"],"docs":[]}
{"task_id":19729928,"prompt":"def f_19729928(l):\n\t","suffix":"\n\treturn l","canonical_solution":"l.sort(key=lambda t: len(t[1]), reverse=True)","test_start":"\ndef check(candidate): ","test":["\n    assert candidate([(\"a\", [1]), (\"b\", [1,2]), (\"c\", [1,2,3])]) ==         [(\"c\", [1,2,3]), (\"b\", [1,2]), (\"a\", [1])]\n","\n    assert candidate([(\"a\", [1]), (\"b\", [2]), (\"c\", [1,2,3])]) ==         [(\"c\", [1,2,3]), (\"a\", [1]), (\"b\", [2])]\n","\n    assert candidate([(\"a\", [1]), (\"b\", [2]), (\"c\", [3])]) ==         [(\"a\", [1]), (\"b\", [2]), (\"c\", [3])]\n"],"entry_point":"f_19729928","intent":"sort a list `l` by length of value in tuple","library":[],"docs":[]}
{"task_id":31371879,"prompt":"def f_31371879(s):\n\treturn ","suffix":"","canonical_solution":"re.findall('\\\\b(\\\\w+)d\\\\b', s)","test_start":"\nimport re\n\ndef check(candidate): ","test":["\n    assert candidate(\"this is good\") == [\"goo\"]\n","\n    assert candidate(\"this is interesting\") == []\n","\n    assert candidate(\"good bad dd\") == [\"goo\", \"ba\", \"d\"]\n"],"entry_point":"f_31371879","intent":"split string `s` by words that ends with 'd'","library":["re"],"docs":[{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"str.rindex(sub[, start[, end]])  \nLike rfind() but raises ValueError when the substring sub is not found.","title":"python.library.stdtypes#str.rindex"},{"text":"re.split(pattern, string, maxsplit=0, flags=0)  \nSplit string by the occurrences of pattern. If capturing parentheses are used in pattern, then the text of all groups in the pattern are also returned as part of the resulting list. If maxsplit is nonzero, at most maxsplit splits occur, and the remainder of the string is returned as the final element of the list. >>> re.split(r'\\W+', 'Words, words, words.')\n['Words', 'words', 'words', '']\n>>> re.split(r'(\\W+)', 'Words, words, words.')\n['Words', ', ', 'words', ', ', 'words', '.', '']\n>>> re.split(r'\\W+', 'Words, words, words.', 1)\n['Words', 'words, words.']\n>>> re.split('[a-f]+', '0a3B9', flags=re.IGNORECASE)\n['0', '3', '9']\n If there are capturing groups in the separator and it matches at the start of the string, the result will start with an empty string. The same holds for the end of the string: >>> re.split(r'(\\W+)', '...words, words...')\n['', '...', 'words', ', ', 'words', '...', '']\n That way, separator components are always found at the same relative indices within the result list. Empty matches for the pattern split the string only when not adjacent to a previous empty match. >>> re.split(r'\\b', 'Words, words, words.')\n['', 'Words', ', ', 'words', ', ', 'words', '.']\n>>> re.split(r'\\W*', '...words...')\n['', '', 'w', 'o', 'r', 'd', 's', '', '']\n>>> re.split(r'(\\W*)', '...words...')\n['', '...', '', '', 'w', '', 'o', '', 'r', '', 'd', '', 's', '...', '', '', '']\n  Changed in version 3.1: Added the optional flags argument.   Changed in version 3.7: Added support of splitting on a pattern that could match an empty string.","title":"python.library.re#re.split"},{"text":"str.split(sep=None, maxsplit=-1)  \nReturn a list of the words in the string, using sep as the delimiter string. If maxsplit is given, at most maxsplit splits are done (thus, the list will have at most maxsplit+1 elements). If maxsplit is not specified or -1, then there is no limit on the number of splits (all possible splits are made). If sep is given, consecutive delimiters are not grouped together and are deemed to delimit empty strings (for example, '1,,2'.split(',') returns ['1', '', '2']). The sep argument may consist of multiple characters (for example, '1<>2<>3'.split('<>') returns ['1', '2', '3']). Splitting an empty string with a specified separator returns ['']. For example: >>> '1,2,3'.split(',')\n['1', '2', '3']\n>>> '1,2,3'.split(',', maxsplit=1)\n['1', '2,3']\n>>> '1,2,,3,'.split(',')\n['1', '2', '', '3', '']\n If sep is not specified or is None, a different splitting algorithm is applied: runs of consecutive whitespace are regarded as a single separator, and the result will contain no empty strings at the start or end if the string has leading or trailing whitespace. Consequently, splitting an empty string or a string consisting of just whitespace with a None separator returns []. For example: >>> '1 2 3'.split()\n['1', '2', '3']\n>>> '1 2 3'.split(maxsplit=1)\n['1', '2 3']\n>>> '   1   2   3   '.split()\n['1', '2', '3']","title":"python.library.stdtypes#str.split"},{"text":"numpy.char.split   char.split(a, sep=None, maxsplit=None)[source]\n \nFor each element in a, return a list of the words in the string, using sep as the delimiter string. Calls str.split element-wise.  Parameters \n \naarray_like of str or unicode\n\n\nsepstr or unicode, optional\n\n\nIf sep is not specified or None, any whitespace string is a separator.  \nmaxsplitint, optional\n\n\nIf maxsplit is given, at most maxsplit splits are done.    Returns \n \noutndarray\n\n\nArray of list objects      See also  \nstr.split, rsplit","title":"numpy.reference.generated.numpy.char.split"},{"text":"Pattern.split(string, maxsplit=0)  \nIdentical to the split() function, using the compiled pattern.","title":"python.library.re#re.Pattern.split"},{"text":"numpy.char.chararray.split method   char.chararray.split(sep=None, maxsplit=None)[source]\n \nFor each element in self, return a list of the words in the string, using sep as the delimiter string.  See also  char.split","title":"numpy.reference.generated.numpy.char.chararray.split"},{"text":"str.rsplit(sep=None, maxsplit=-1)  \nReturn a list of the words in the string, using sep as the delimiter string. If maxsplit is given, at most maxsplit splits are done, the rightmost ones. If sep is not specified or None, any whitespace string is a separator. Except for splitting from the right, rsplit() behaves like split() which is described in detail below.","title":"python.library.stdtypes#str.rsplit"},{"text":"numpy.chararray.split method   chararray.split(sep=None, maxsplit=None)[source]\n \nFor each element in self, return a list of the words in the string, using sep as the delimiter string.  See also  char.split","title":"numpy.reference.generated.numpy.chararray.split"},{"text":"numpy.char.chararray.rsplit method   char.chararray.rsplit(sep=None, maxsplit=None)[source]\n \nFor each element in self, return a list of the words in the string, using sep as the delimiter string.  See also  char.rsplit","title":"numpy.reference.generated.numpy.char.chararray.rsplit"}]}
{"task_id":9012008,"prompt":"def f_9012008():\n\treturn ","suffix":"","canonical_solution":"bool(re.search('ba[rzd]', 'foobarrrr'))","test_start":"\nimport re\n\ndef check(candidate): ","test":["\n    assert candidate() == True\n"],"entry_point":"f_9012008","intent":"return `True` if string `foobarrrr` contains regex `ba[rzd]`","library":["re"],"docs":[{"text":"pattern  \nThe regular expression pattern.","title":"python.library.re#re.error.pattern"},{"text":"winreg.REG_SZ  \nA null-terminated string.","title":"python.library.winreg#winreg.REG_SZ"},{"text":"re.purge()  \nClear the regular expression cache.","title":"python.library.re#re.purge"},{"text":"str.rindex(sub[, start[, end]])  \nLike rfind() but raises ValueError when the substring sub is not found.","title":"python.library.stdtypes#str.rindex"},{"text":"str.isupper()  \nReturn True if all cased characters 4 in the string are uppercase and there is at least one cased character, False otherwise. >>> 'BANANA'.isupper()\nTrue\n>>> 'banana'.isupper()\nFalse\n>>> 'baNana'.isupper()\nFalse\n>>> ' '.isupper()\nFalse","title":"python.library.stdtypes#str.isupper"},{"text":"isatty()  \nReturns False.","title":"python.library.chunk#chunk.Chunk.isatty"},{"text":"exception xml.dom.SyntaxErr  \nRaised when an invalid or illegal string is specified.","title":"python.library.xml.dom#xml.dom.SyntaxErr"},{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"token.RBRACE  \nToken value for \"}\".","title":"python.library.token#token.RBRACE"},{"text":"stat.S_ISREG(mode)  \nReturn non-zero if the mode is from a regular file.","title":"python.library.stat#stat.S_ISREG"}]}
{"task_id":7961363,"prompt":"def f_7961363(t):\n\treturn ","suffix":"","canonical_solution":"list(set(t))","test_start":"\ndef check(candidate): ","test":["\n    assert candidate([1,2,3]) == [1,2,3]\n","\n    assert candidate([1,1,1,1,1,1,1,1,1,1]) == [1] \n","\n    assert candidate([1,2,2,2,2,2,3,3,3,3,3]) == [1,2,3]\n","\n    assert (candidate([1, '1']) == [1, '1']) or (candidate([1, '1']) == ['1', 1])\n","\n    assert candidate([1.0, 1]) == [1.0] \n","\n    assert candidate([]) == [] \n","\n    assert candidate([None]) == [None] \n"],"entry_point":"f_7961363","intent":"Removing duplicates in list `t`","library":[],"docs":[]}
{"task_id":7961363,"prompt":"def f_7961363(source_list):\n\treturn ","suffix":"","canonical_solution":"list(set(source_list))","test_start":"\ndef check(candidate): ","test":["\n    assert candidate([1,2,3]) == [1,2,3]\n","\n    assert candidate([1,1,1,1,1,1,1,1,1,1]) == [1] \n","\n    assert candidate([1,2,2,2,2,2,3,3,3,3,3]) == [1,2,3]\n","\n    assert (candidate([1, '1']) == [1, '1']) or (candidate([1, '1']) == ['1', 1])\n","\n    assert candidate([1.0, 1]) == [1.0] \n","\n    assert candidate([]) == [] \n","\n    assert candidate([None]) == [None] \n"],"entry_point":"f_7961363","intent":"Removing duplicates in list `source_list`","library":[],"docs":[]}
{"task_id":7961363,"prompt":"def f_7961363():\n\treturn ","suffix":"","canonical_solution":"list(OrderedDict.fromkeys('abracadabra'))","test_start":"\nfrom collections import OrderedDict\n\ndef check(candidate):","test":["\n    assert candidate() == ['a', 'b', 'r', 'c', 'd']\n"],"entry_point":"f_7961363","intent":"Removing duplicates in list `abracadabra`","library":["collections"],"docs":[{"text":"remove(value)  \nRemove the first occurrence of value. If not found, raises a ValueError.","title":"python.library.collections#collections.deque.remove"},{"text":"array.remove(x)  \nRemove the first occurrence of x from the array.","title":"python.library.array#array.array.remove"},{"text":"most_common([n])  \nReturn a list of the n most common elements and their counts from the most common to the least. If n is omitted or None, most_common() returns all elements in the counter. Elements with equal counts are ordered in the order first encountered: >>> Counter('abracadabra').most_common(3)\n[('a', 5), ('b', 2), ('r', 2)]","title":"python.library.collections#collections.Counter.most_common"},{"text":"clear()  \nRemove all elements from the deque leaving it with length 0.","title":"python.library.collections#collections.deque.clear"},{"text":"clear()  \nRemove all elements from the set.","title":"python.library.stdtypes#frozenset.clear"},{"text":"re.purge()  \nClear the regular expression cache.","title":"python.library.re#re.purge"},{"text":"token.N_TOKENS","title":"python.library.token#token.N_TOKENS"},{"text":"copy()  \nReturn a duplicate of the context.","title":"python.library.decimal#decimal.Context.copy"},{"text":"clear()  \nRemove all items from the dictionary.","title":"python.library.stdtypes#dict.clear"},{"text":"numpy.lib.recfunctions.find_duplicates(a, key=None, ignoremask=True, return_index=False)[source]\n \nFind the duplicates in a structured array along a given key  Parameters \n \naarray-like\n\n\nInput array  \nkey{string, None}, optional\n\n\nName of the fields along which to check the duplicates. If None, the search is performed by records  \nignoremask{True, False}, optional\n\n\nWhether masked data should be discarded or considered as duplicates.  \nreturn_index{False, True}, optional\n\n\nWhether to return the indices of the duplicated values.     Examples >>> from numpy.lib import recfunctions as rfn\n>>> ndtype = [('a', int)]\n>>> a = np.ma.array([1, 1, 1, 2, 2, 3, 3],\n...         mask=[0, 0, 1, 0, 0, 0, 1]).view(ndtype)\n>>> rfn.find_duplicates(a, ignoremask=True, return_index=True)\n(masked_array(data=[(1,), (1,), (2,), (2,)],\n             mask=[(False,), (False,), (False,), (False,)],\n       fill_value=(999999,),\n            dtype=[('a', '<i8')]), array([0, 1, 3, 4]))","title":"numpy.user.basics.rec#numpy.lib.recfunctions.find_duplicates"}]}
{"task_id":5183533,"prompt":"def f_5183533(a):\n\treturn ","suffix":"","canonical_solution":"numpy.array(a).reshape(-1).tolist()","test_start":"\nimport numpy\n\ndef check(candidate):","test":["\n    assert candidate([[1,2,3],[4,5,6]]) == [1,2,3,4,5,6]\n","\n    assert candidate(['a', 'aa', 'abc']) == ['a', 'aa', 'abc']\n"],"entry_point":"f_5183533","intent":"Convert array `a` into a list","library":["numpy"],"docs":[{"text":"array.tolist()  \nConvert the array to an ordinary list with the same items.","title":"python.library.array#array.array.tolist"},{"text":"tolist()  \nReturns a list containing the elements of this storage","title":"torch.storage#torch.FloatStorage.tolist"},{"text":"array.reverse()  \nReverse the order of the items in the array.","title":"python.library.array#array.array.reverse"},{"text":"array.tofile(f)  \nWrite all items (as machine values) to the file object f.","title":"python.library.array#array.array.tofile"},{"text":"numpy.recarray.dumps method   recarray.dumps()\n \nReturns the pickle of the array as a string. pickle.loads will convert the string back to an array.  Parameters \n None","title":"numpy.reference.generated.numpy.recarray.dumps"},{"text":"array.fromlist(list)  \nAppend items from the list. This is equivalent to for x in list:\na.append(x) except that if there is a type error, the array is unchanged.","title":"python.library.array#array.array.fromlist"},{"text":"numpy.distutils.misc_util.as_list(seq)[source]","title":"numpy.reference.distutils.misc_util#numpy.distutils.misc_util.as_list"},{"text":"array.typecodes  \nA string with all available type codes.","title":"python.library.array#array.typecodes"},{"text":"numpy.matrix.dumps method   matrix.dumps()\n \nReturns the pickle of the array as a string. pickle.loads will convert the string back to an array.  Parameters \n None","title":"numpy.reference.generated.numpy.matrix.dumps"},{"text":"numpy.chararray.dumps method   chararray.dumps()\n \nReturns the pickle of the array as a string. pickle.loads will convert the string back to an array.  Parameters \n None","title":"numpy.reference.generated.numpy.chararray.dumps"}]}
{"task_id":5183533,"prompt":"def f_5183533(a):\n\treturn ","suffix":"","canonical_solution":"numpy.array(a)[0].tolist()","test_start":"\nimport numpy\n\ndef check(candidate):","test":["\n    assert candidate([[1,2,3],[4,5,6]]) == [1,2,3]\n","\n    assert candidate(['a', 'aa', 'abc']) == 'a'\n"],"entry_point":"f_5183533","intent":"Convert the first row of numpy matrix `a` to a list","library":["numpy"],"docs":[{"text":"array.tolist()  \nConvert the array to an ordinary list with the same items.","title":"python.library.array#array.array.tolist"},{"text":"numpy.matrix.dumps method   matrix.dumps()\n \nReturns the pickle of the array as a string. pickle.loads will convert the string back to an array.  Parameters \n None","title":"numpy.reference.generated.numpy.matrix.dumps"},{"text":"numpy.distutils.misc_util.as_list(seq)[source]","title":"numpy.reference.distutils.misc_util#numpy.distutils.misc_util.as_list"},{"text":"numpy.recarray.dumps method   recarray.dumps()\n \nReturns the pickle of the array as a string. pickle.loads will convert the string back to an array.  Parameters \n None","title":"numpy.reference.generated.numpy.recarray.dumps"},{"text":"numpy.matrix.getA method   matrix.getA()[source]\n \nReturn self as an ndarray object. Equivalent to np.asarray(self).  Parameters \n None\n  Returns \n \nretndarray\n\n\nself as an ndarray     Examples >>> x = np.matrix(np.arange(12).reshape((3,4))); x\nmatrix([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])\n>>> x.getA()\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])","title":"numpy.reference.generated.numpy.matrix.geta"},{"text":"array.tofile(f)  \nWrite all items (as machine values) to the file object f.","title":"python.library.array#array.array.tofile"},{"text":"numpy.matrix.tolist method   matrix.tolist()[source]\n \nReturn the matrix as a (possibly nested) list. See ndarray.tolist for full documentation.  See also  ndarray.tolist\n  Examples >>> x = np.matrix(np.arange(12).reshape((3,4))); x\nmatrix([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])\n>>> x.tolist()\n[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]]","title":"numpy.reference.generated.numpy.matrix.tolist"},{"text":"numpy.matrix.data attribute   matrix.data\n \nPython buffer object pointing to the start of the array\u2019s data.","title":"numpy.reference.generated.numpy.matrix.data"},{"text":"tolist()  \nReturns a list containing the elements of this storage","title":"torch.storage#torch.FloatStorage.tolist"},{"text":"numpy.ndarray.dumps method   ndarray.dumps()\n \nReturns the pickle of the array as a string. pickle.loads will convert the string back to an array.  Parameters \n None","title":"numpy.reference.generated.numpy.ndarray.dumps"}]}
{"task_id":5999747,"prompt":"def f_5999747(soup):\n\treturn ","suffix":"","canonical_solution":"soup.find(text='Address:').findNext('td').contents[0]","test_start":"\nfrom bs4 import BeautifulSoup\n\ndef check(candidate):","test":["\n    assert candidate(BeautifulSoup(\"<td><b>Address:<\/b><\/td><td>My home address<\/td>\")) == \"My home address\"\n","\n    assert candidate(BeautifulSoup(\"<td><b>Address:<\/b><\/td><td>This is my home address<\/td><td>Not my home address<\/td>\")) == \"This is my home address\"\n","\n    assert candidate(BeautifulSoup(\"<td><b>Address:<\/b><\/td><td>My home address<li>My home address in a list<\/li><\/td>\")) == \"My home address\"\n"],"entry_point":"f_5999747","intent":"In `soup`, get the content of the sibling of the `td`  tag with text content `Address:`","library":["bs4"],"docs":[]}
{"task_id":4284648,"prompt":"def f_4284648(l):\n\treturn ","suffix":"","canonical_solution":"\"\"\" \"\"\".join([('%d@%d' % t) for t in l])","test_start":"\ndef check(candidate):","test":["\n    assert candidate([(1, 2), (3, 4)]) == \"1@2 3@4\"\n","\n    assert candidate([(10, 11), (12, 13)]) == \"10@11 12@13\"\n","\n    assert candidate([(10.2, 11.4), (12.14, 13.13)]) == \"10@11 12@13\"\n"],"entry_point":"f_4284648","intent":"convert elements of each tuple in list `l` into a string  separated by character `@`","library":[],"docs":[]}
{"task_id":4284648,"prompt":"def f_4284648(l):\n\treturn ","suffix":"","canonical_solution":"\"\"\" \"\"\".join([('%d@%d' % (t[0], t[1])) for t in l])","test_start":"\ndef check(candidate):","test":["\n    assert candidate([(1, 2), (3, 4)]) == \"1@2 3@4\"\n","\n    assert candidate([(10, 11), (12, 13)]) == \"10@11 12@13\"\n","\n    assert candidate([(10.2, 11.4), (12.14, 13.13)]) == \"10@11 12@13\"\n"],"entry_point":"f_4284648","intent":"convert each tuple in list `l` to a string with '@' separating the tuples' elements","library":[],"docs":[]}
{"task_id":29696641,"prompt":"def f_29696641(teststr):\n\treturn ","suffix":"","canonical_solution":"[i for i in teststr if re.search('\\\\d+[xX]', i)]","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate(['1 FirstString', '2x Sec String', '3rd String', 'x forString', '5X fifth']) == ['2x Sec String', '5X fifth']\n","\n    assert candidate(['1x', '2', '3X', '4x random', '5X random']) == ['1x', '3X', '4x random', '5X random']\n","\n    assert candidate(['1x', '2', '3X', '4xrandom', '5Xrandom']) == ['1x', '3X', '4xrandom', '5Xrandom']\n"],"entry_point":"f_29696641","intent":"Get all matches with regex pattern `\\\\d+[xX]` in list of string `teststr`","library":["re"],"docs":[{"text":"pattern  \nThe regular expression pattern.","title":"python.library.re#re.error.pattern"},{"text":"re.split(pattern, string, maxsplit=0, flags=0)  \nSplit string by the occurrences of pattern. If capturing parentheses are used in pattern, then the text of all groups in the pattern are also returned as part of the resulting list. If maxsplit is nonzero, at most maxsplit splits occur, and the remainder of the string is returned as the final element of the list. >>> re.split(r'\\W+', 'Words, words, words.')\n['Words', 'words', 'words', '']\n>>> re.split(r'(\\W+)', 'Words, words, words.')\n['Words', ', ', 'words', ', ', 'words', '.', '']\n>>> re.split(r'\\W+', 'Words, words, words.', 1)\n['Words', 'words, words.']\n>>> re.split('[a-f]+', '0a3B9', flags=re.IGNORECASE)\n['0', '3', '9']\n If there are capturing groups in the separator and it matches at the start of the string, the result will start with an empty string. The same holds for the end of the string: >>> re.split(r'(\\W+)', '...words, words...')\n['', '...', 'words', ', ', 'words', '...', '']\n That way, separator components are always found at the same relative indices within the result list. Empty matches for the pattern split the string only when not adjacent to a previous empty match. >>> re.split(r'\\b', 'Words, words, words.')\n['', 'Words', ', ', 'words', ', ', 'words', '.']\n>>> re.split(r'\\W*', '...words...')\n['', '', 'w', 'o', 'r', 'd', 's', '', '']\n>>> re.split(r'(\\W*)', '...words...')\n['', '...', '', '', 'w', '', 'o', '', 'r', '', 'd', '', 's', '...', '', '', '']\n  Changed in version 3.1: Added the optional flags argument.   Changed in version 3.7: Added support of splitting on a pattern that could match an empty string.","title":"python.library.re#re.split"},{"text":"re.findall(pattern, string, flags=0)  \nReturn all non-overlapping matches of pattern in string, as a list of strings. The string is scanned left-to-right, and matches are returned in the order found. If one or more groups are present in the pattern, return a list of groups; this will be a list of tuples if the pattern has more than one group. Empty matches are included in the result.  Changed in version 3.7: Non-empty matches can now start just after a previous empty match.","title":"python.library.re#re.findall"},{"text":"fnmatch.translate(pattern)  \nReturn the shell-style pattern converted to a regular expression for using with re.match(). Example: >>> import fnmatch, re\n>>>\n>>> regex = fnmatch.translate('*.txt')\n>>> regex\n'(?s:.*\\\\.txt)\\\\Z'\n>>> reobj = re.compile(regex)\n>>> reobj.match('foobar.txt')\n<re.Match object; span=(0, 10), match='foobar.txt'>","title":"python.library.fnmatch#fnmatch.translate"},{"text":"re.purge()  \nClear the regular expression cache.","title":"python.library.re#re.purge"},{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"Pattern.search(string[, pos[, endpos]])  \nScan through string looking for the first location where this regular expression produces a match, and return a corresponding match object. Return None if no position in the string matches the pattern; note that this is different from finding a zero-length match at some point in the string. The optional second parameter pos gives an index in the string where the search is to start; it defaults to 0. This is not completely equivalent to slicing the string; the '^' pattern character matches at the real beginning of the string and at positions just after a newline, but not necessarily at the index where the search is to start. The optional parameter endpos limits how far the string will be searched; it will be as if the string is endpos characters long, so only the characters from pos to endpos - 1 will be searched for a match. If endpos is less than pos, no match will be found; otherwise, if rx is a compiled regular expression object, rx.search(string, 0, 50) is equivalent to rx.search(string[:50], 0). >>> pattern = re.compile(\"d\")\n>>> pattern.search(\"dog\")     # Match at index 0\n<re.Match object; span=(0, 1), match='d'>\n>>> pattern.search(\"dog\", 1)  # No match; search doesn't include the \"d\"","title":"python.library.re#re.Pattern.search"},{"text":"str.rindex(sub[, start[, end]])  \nLike rfind() but raises ValueError when the substring sub is not found.","title":"python.library.stdtypes#str.rindex"},{"text":"re.search(pattern, string, flags=0)  \nScan through string looking for the first location where the regular expression pattern produces a match, and return a corresponding match object. Return None if no position in the string matches the pattern; note that this is different from finding a zero-length match at some point in the string.","title":"python.library.re#re.search"},{"text":"test.support.set_match_tests(patterns)  \nDefine match test with regular expression patterns.","title":"python.library.test#test.support.set_match_tests"}]}
{"task_id":15315452,"prompt":"def f_15315452(df):\n\treturn ","suffix":"","canonical_solution":"df['A'][(df['B'] > 50) & (df['C'] == 900)]","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame({'A': [7, 7, 4, 4, 7, 7, 3, 9, 6, 3], 'B': [20, 80, 90, 30, 80, 60, 80, 40, 40 ,10], 'C': [300, 700, 100, 900, 200, 800, 900, 100, 100, 600]})\n    assert candidate(df).to_dict() == {6: 3}\n","\n    df1 = pd.DataFrame({'A': [9, 9, 5, 8, 7, 9, 2, 2, 5, 7], 'B': [40, 70, 70, 80, 50, 30, 80, 80, 80, 70], 'C': [300, 700, 900, 900, 200, 900, 700, 400, 300, 800]})\n    assert candidate(df1).to_dict() == {2: 5, 3: 8}\n","\n    df2 = pd.DataFrame({'A': [3, 4, 5, 6], 'B': [-10, 50, 20, 10], 'C': [900, 800, 900, 900]})\n    assert candidate(df2).to_dict() == {}\n"],"entry_point":"f_15315452","intent":"select values from column 'A' for which corresponding values in column 'B' will be greater than 50, and in column 'C' - equal 900 in dataframe `df`","library":["pandas"],"docs":[]}
{"task_id":4642501,"prompt":"def f_4642501(o):\n\treturn ","suffix":"","canonical_solution":"sorted(o.items())","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    assert candidate({1:\"abc\", 5:\"klm\", 2:\"pqr\"}) == [(1, \"abc\"), (2, \"pqr\"), (5, \"klm\")]\n","\n    assert candidate({4.221:\"uwv\", -1.009:\"pow\"}) == [(-1.009, 'pow'), (4.221, 'uwv')]\n","\n    assert candidate({\"as2q\":\"piqr\", \"#wwq\":\"say\", \"Rwc\":\"koala\", \"35\":\"kangaroo\"}) == [('#wwq', 'say'), ('35', 'kangaroo'), ('Rwc', 'koala'), ('as2q', 'piqr')]\n"],"entry_point":"f_4642501","intent":"Sort dictionary `o` in ascending order based on its keys and items","library":["pandas"],"docs":[]}
{"task_id":4642501,"prompt":"def f_4642501(d):\n\treturn ","suffix":"","canonical_solution":"sorted(d)","test_start":"\ndef check(candidate):","test":["\n    assert candidate({1:\"abc\", 5:\"klm\", 2:\"pqr\"}) == [1, 2, 5]\n","\n    assert candidate({4.221:\"uwv\", -1.009:\"pow\"}) == [-1.009, 4.221]\n","\n    assert candidate({\"as2q\":\"piqr\", \"#wwq\":\"say\", \"Rwc\":\"koala\", \"35\":\"kangaroo\"}) == ['#wwq', '35', 'Rwc', 'as2q']\n"],"entry_point":"f_4642501","intent":"get sorted list of keys of dict `d`","library":[],"docs":[]}
{"task_id":4642501,"prompt":"def f_4642501(d):\n\treturn ","suffix":"","canonical_solution":"sorted(d.items())","test_start":"\ndef check(candidate):","test":["\n    d = {'a': [1, 2, 3], 'c': ['one', 'two'], 'b': ['blah', 'bhasdf', 'asdf'], 'd': ['asdf', 'wer', 'asdf', 'zxcv']}\n    assert candidate(d) == [('a', [1, 2, 3]), ('b', ['blah', 'bhasdf', 'asdf']), ('c', ['one', 'two']), ('d', ['asdf', 'wer', 'asdf', 'zxcv'])]\n"],"entry_point":"f_4642501","intent":"sort dictionaries `d` by keys","library":[],"docs":[]}
{"task_id":642154,"prompt":"def f_642154():\n\treturn ","suffix":"","canonical_solution":"int('1')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == 1\n","\n    assert candidate() + 1 == 2\n"],"entry_point":"f_642154","intent":"convert string \"1\" into integer","library":[],"docs":[]}
{"task_id":642154,"prompt":"def f_642154(T1):\n\treturn ","suffix":"","canonical_solution":"[list(map(int, x)) for x in T1]","test_start":"\ndef check(candidate):","test":["\n    T1 = (('13', '17', '18', '21', '32'), ('07', '11', '13', '14', '28'), ('01', '05', '06', '08', '15', '16'))\n    assert candidate(T1) == [[13, 17, 18, 21, 32], [7, 11, 13, 14, 28], [1, 5, 6, 8, 15, 16]]\n"],"entry_point":"f_642154","intent":"convert items in `T1` to integers","library":[],"docs":[]}
{"task_id":3777301,"prompt":"def f_3777301():\n\t","suffix":"\n\treturn ","canonical_solution":"subprocess.call(['.\/test.sh'])","test_start":"\nimport subprocess\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    subprocess.call = Mock()\n    try:\n        candidate()\n    except:\n        assert False\n"],"entry_point":"f_3777301","intent":"call a shell script `.\/test.sh` using subprocess","library":["subprocess"],"docs":[{"text":"test.support.unix_shell  \nPath for shell if not on Windows; otherwise None.","title":"python.library.test#test.support.unix_shell"},{"text":"test.support.script_helper.kill_python(p)  \nRun the given subprocess.Popen process until completion and return stdout.","title":"python.library.test#test.support.script_helper.kill_python"},{"text":"cmd  \nCommand that was used to spawn the child process.","title":"python.library.subprocess#subprocess.CalledProcessError.cmd"},{"text":"cmd  \nCommand that was used to spawn the child process.","title":"python.library.subprocess#subprocess.TimeoutExpired.cmd"},{"text":"test.support.script_helper.spawn_python(*args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, **kw)  \nRun a Python subprocess with the given arguments. kw is extra keyword args to pass to subprocess.Popen(). Returns a subprocess.Popen object.","title":"python.library.test#test.support.script_helper.spawn_python"},{"text":"stdout  \nAlias for output, for symmetry with stderr.","title":"python.library.subprocess#subprocess.CalledProcessError.stdout"},{"text":"unittest.mock.FILTER_DIR","title":"python.library.unittest.mock#unittest.mock.FILTER_DIR"},{"text":"test.support.TEST_HOME_DIR  \nSet to the top level directory for the test package.","title":"python.library.test#test.support.TEST_HOME_DIR"},{"text":"run(cmd)  \nProfile the cmd via exec().","title":"python.library.profile#profile.Profile.run"},{"text":"stat.S_IXGRP  \nGroup has execute permission.","title":"python.library.stat#stat.S_IXGRP"}]}
{"task_id":3777301,"prompt":"def f_3777301():\n\t","suffix":"\n\treturn ","canonical_solution":"subprocess.call(['notepad'])","test_start":"\nimport subprocess\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    subprocess.call = Mock()\n    try:\n        candidate()\n    except:\n        assert False\n"],"entry_point":"f_3777301","intent":"call a shell script `notepad` using subprocess","library":["subprocess"],"docs":[{"text":"cmd  \nCommand that was used to spawn the child process.","title":"python.library.subprocess#subprocess.CalledProcessError.cmd"},{"text":"cmd  \nCommand that was used to spawn the child process.","title":"python.library.subprocess#subprocess.TimeoutExpired.cmd"},{"text":"stdout  \nAlias for output, for symmetry with stderr.","title":"python.library.subprocess#subprocess.CalledProcessError.stdout"},{"text":"test.support.script_helper.kill_python(p)  \nRun the given subprocess.Popen process until completion and return stdout.","title":"python.library.test#test.support.script_helper.kill_python"},{"text":"Cmd.prompt  \nThe prompt issued to solicit input.","title":"python.library.cmd#cmd.Cmd.prompt"},{"text":"stdin  \nStandard input stream (StreamWriter) or None if the process was created with stdin=None.","title":"python.library.asyncio-subprocess#asyncio.asyncio.subprocess.Process.stdin"},{"text":"run(cmd)  \nProfile the cmd via exec().","title":"python.library.profile#profile.Profile.run"},{"text":"test.support.unix_shell  \nPath for shell if not on Windows; otherwise None.","title":"python.library.test#test.support.unix_shell"},{"text":"subprocess.SW_HIDE  \nHides the window. Another window will be activated.","title":"python.library.subprocess#subprocess.SW_HIDE"},{"text":"stat.S_IFREG  \nRegular file.","title":"python.library.stat#stat.S_IFREG"}]}
{"task_id":7946798,"prompt":"def f_7946798(l1, l2):\n\treturn ","suffix":"","canonical_solution":"[val for pair in zip(l1, l2) for val in pair]","test_start":"\ndef check(candidate):","test":["\n    assert candidate([1,2,3], [10,20,30]) == [1,10,2,20,3,30]\n","\n    assert candidate([1,2,3], ['c','b','a']) == [1,'c',2,'b',3,'a']\n","\n    assert candidate([1,2,3], ['c','b']) == [1,'c',2,'b']\n"],"entry_point":"f_7946798","intent":"combine lists `l1` and `l2`  by alternating their elements","library":[],"docs":[]}
{"task_id":8908287,"prompt":"def f_8908287():\n\treturn ","suffix":"","canonical_solution":"base64.b64encode(b'data to be encoded')","test_start":"\nimport base64\n\ndef check(candidate):","test":["\n    assert candidate() == b'ZGF0YSB0byBiZSBlbmNvZGVk'\n"],"entry_point":"f_8908287","intent":"encode string 'data to be encoded'","library":["base64"],"docs":[{"text":"HttpResponse.content  \nA bytestring representing the content, encoded from a string if necessary.","title":"django.ref.request-response#django.http.HttpResponse.content"},{"text":"numpy.chararray.encode method   chararray.encode(encoding=None, errors=None)[source]\n \nCalls str.encode element-wise.  See also  char.encode","title":"numpy.reference.generated.numpy.chararray.encode"},{"text":"numpy.char.chararray.encode method   char.chararray.encode(encoding=None, errors=None)[source]\n \nCalls str.encode element-wise.  See also  char.encode","title":"numpy.reference.generated.numpy.char.chararray.encode"},{"text":"Text.data  \nThe content of the text node as a string.","title":"python.library.xml.dom#xml.dom.Text.data"},{"text":"numpy.string_[source]\n \nalias of numpy.bytes_","title":"numpy.reference.arrays.scalars#numpy.string_"},{"text":"as_string()","title":"django.ref.contrib.gis.gdal#django.contrib.gis.gdal.Field.as_string"},{"text":"encoding  \nThe name of the encoding used to decode the stream\u2019s bytes into strings, and to encode strings into bytes.","title":"python.library.io#io.TextIOBase.encoding"},{"text":"body_encode(string)  \nBody-encode the string string. The type of encoding (base64 or quoted-printable) will be based on the body_encoding attribute.","title":"python.library.email.charset#email.charset.Charset.body_encode"},{"text":"errors  \nThe error setting of the decoder or encoder.","title":"python.library.io#io.TextIOBase.errors"},{"text":"encode_netloc()  \nEncodes the netloc part to an ASCII safe URL as bytes.  Return type \nstr","title":"werkzeug.urls.index#werkzeug.urls.BaseURL.encode_netloc"}]}
{"task_id":8908287,"prompt":"def f_8908287():\n\treturn ","suffix":"","canonical_solution":"'data to be encoded'.encode('ascii')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == b'data to be encoded'\n"],"entry_point":"f_8908287","intent":"encode a string `data to be encoded` to `ascii` encoding","library":[],"docs":[]}
{"task_id":7856296,"prompt":"def f_7856296():\n\treturn ","suffix":"","canonical_solution":"list(csv.reader(open('text.txt', 'r'), delimiter='\\t'))","test_start":"\nimport csv \n\ndef check(candidate):","test":["\n    with open('text.txt', 'w', newline='') as csvfile:\n        spamwriter = csv.writer(csvfile, delimiter='\t')\n        spamwriter.writerow(['Spam', 'Lovely Spam', 'Wonderful Spam'])\n        spamwriter.writerow(['hello', 'world', '!'])\n\n    assert candidate() == [['Spam', 'Lovely Spam', 'Wonderful Spam'], ['hello', 'world', '!']]\n"],"entry_point":"f_7856296","intent":"parse tab-delimited CSV file 'text.txt' into a list","library":["csv"],"docs":[{"text":"exception csv.Error  \nRaised by any of the functions when an error is detected.","title":"python.library.csv#csv.Error"},{"text":"gettext.npgettext(context, singular, plural, n)","title":"python.library.gettext#gettext.npgettext"},{"text":"gettext.lngettext(singular, plural, n)","title":"python.library.gettext#gettext.lngettext"},{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"filename\n \nAlias for field number 4","title":"matplotlib.dviread#matplotlib.dviread.PsFont.filename"},{"text":"class csv.excel_tab  \nThe excel_tab class defines the usual properties of an Excel-generated TAB-delimited file. It is registered with the dialect name 'excel-tab'.","title":"python.library.csv#csv.excel_tab"},{"text":"end  \nThe index after the last invalid data in object.","title":"python.library.exceptions#UnicodeError.end"},{"text":"start  \nThe first index of invalid data in object.","title":"python.library.exceptions#UnicodeError.start"},{"text":"tabs()  \nReturns a list of windows managed by the notebook.","title":"python.library.tkinter.ttk#tkinter.ttk.Notebook.tabs"},{"text":"Dialect.skipinitialspace  \nWhen True, whitespace immediately following the delimiter is ignored. The default is False.","title":"python.library.csv#csv.Dialect.skipinitialspace"}]}
{"task_id":9035479,"prompt":"def f_9035479(my_object, my_str):\n\treturn ","suffix":"","canonical_solution":"getattr(my_object, my_str)","test_start":"\ndef check(candidate):","test":["\n    class Student:\n        id = 9\n        name = \"abc\"\n        grade = 97.08\n\n    s = Student()\n    \n    assert candidate(s, \"name\") == \"abc\"\n","\n    class Student:\n        id = 9\n        name = \"abc\"\n        grade = 97.08\n\n    s = Student()\n    \n    assert (candidate(s, \"grade\") - 97.08) < 1e-6\n","\n    class Student:\n        id = 9\n        name = \"abc\"\n        grade = 97.08\n\n    s = Student()\n    \n    assert (candidate(s, \"grade\") - 97.07) > 1e-6\n","\n    class Student:\n        id = 9\n        name = \"abc\"\n        grade = 97.08\n\n    s = Student()\n    \n    assert candidate(s, \"id\") == 9\n"],"entry_point":"f_9035479","intent":"Get attribute `my_str` of object `my_object`","library":[],"docs":[]}
{"task_id":5558418,"prompt":"def f_5558418(LD):\n\treturn ","suffix":"","canonical_solution":"dict(zip(LD[0], zip(*[list(d.values()) for d in LD])))","test_start":"\nimport collections\n\ndef check(candidate):","test":["\n    employees = [{'name' : 'apple', 'id': 60}, {'name' : 'orange', 'id': 65}]\n    exp_result = {'name': ('apple', 'orange'), 'id': (60, 65)}\n    actual_result = candidate(employees)\n    for key in actual_result:\n        assert collections.Counter(list(exp_result[key])) == collections.Counter(list(actual_result[key]))\n"],"entry_point":"f_5558418","intent":"group a list of dicts `LD` into one dict by key","library":["collections"],"docs":[]}
{"task_id":638048,"prompt":"def f_638048(list_of_pairs):\n\treturn ","suffix":"","canonical_solution":"sum([pair[0] for pair in list_of_pairs])","test_start":"\ndef check(candidate):","test":["\n    assert candidate([(5, 9), (-1, -2), (4, 2)]) == 8\n"],"entry_point":"f_638048","intent":"sum the first value in each tuple in a list of tuples `list_of_pairs` in python","library":[],"docs":[]}
{"task_id":14950260,"prompt":"def f_14950260():\n\treturn ","suffix":"","canonical_solution":"ast.literal_eval(\"{'code1':1,'code2':1}\")","test_start":"\nimport ast\n\ndef check(candidate):","test":["\n    d = candidate()\n    exp_result = {'code1' : 1, 'code2': 1}\n    for key in d:\n        if key not in exp_result:\n            assert False\n        else:\n            assert d[key] == exp_result[key]\n"],"entry_point":"f_14950260","intent":"convert unicode string u\"{'code1':1,'code2':1}\" into dictionary","library":["ast"],"docs":[{"text":"dis.opmap  \nDictionary mapping operation names to bytecodes.","title":"python.library.dis#dis.opmap"},{"text":"start  \nThe first index of invalid data in object.","title":"python.library.exceptions#UnicodeError.start"},{"text":"html.entities.codepoint2name  \nA dictionary that maps Unicode code points to HTML entity names.","title":"python.library.html.entities#html.entities.codepoint2name"},{"text":"html.entities.name2codepoint  \nA dictionary that maps HTML entity names to the Unicode code points.","title":"python.library.html.entities#html.entities.name2codepoint"},{"text":"numpy.unicode_[source]\n \nalias of numpy.str_","title":"numpy.reference.arrays.scalars#numpy.unicode_"},{"text":"object  \nThe object the codec was attempting to encode or decode.","title":"python.library.exceptions#UnicodeError.object"},{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"reason  \nA string describing the specific codec error.","title":"python.library.exceptions#UnicodeError.reason"},{"text":"end  \nThe index after the last invalid data in object.","title":"python.library.exceptions#UnicodeError.end"},{"text":"token.LBRACE  \nToken value for \"{\".","title":"python.library.token#token.LBRACE"}]}
{"task_id":11416772,"prompt":"def f_11416772(mystring):\n\treturn ","suffix":"","canonical_solution":"[word for word in mystring.split() if word.startswith('$')]","test_start":"\ndef check(candidate):","test":["\n    str = \"$abc def $efg $hij klm $\"\n    exp_result = ['$abc', '$efg', '$hij', '$']\n    assert sorted(candidate(str)) == sorted(exp_result)\n"],"entry_point":"f_11416772","intent":"find all words in a string `mystring` that start with the `$` sign","library":[],"docs":[]}
{"task_id":11331982,"prompt":"def f_11331982(text):\n\t","suffix":"\n\treturn text","canonical_solution":"text = re.sub('^https?:\\\\\/\\\\\/.*[\\\\r\\\\n]*', '', text, flags=re.MULTILINE)","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate(\"https:\/\/www.wikipedia.org\/ click at\") == \"\"\n"],"entry_point":"f_11331982","intent":"remove any url within string `text`","library":["re"],"docs":[{"text":"re.purge()  \nClear the regular expression cache.","title":"python.library.re#re.purge"},{"text":"BaseHandler.close()  \nRemove any parents.","title":"python.library.urllib.request#urllib.request.BaseHandler.close"},{"text":"re_path(route, view, kwargs=None, name=None)","title":"django.ref.urls#django.urls.re_path"},{"text":"exception xml.dom.SyntaxErr  \nRaised when an invalid or illegal string is specified.","title":"python.library.xml.dom#xml.dom.SyntaxErr"},{"text":"gettext.ldgettext(domain, message)","title":"python.library.gettext#gettext.ldgettext"},{"text":"strip_tags(value)  \nTries to remove anything that looks like an HTML tag from the string, that is anything contained within <>. Absolutely NO guarantee is provided about the resulting string being HTML safe. So NEVER mark safe the result of a strip_tag call without escaping it first, for example with escape(). For example: strip_tags(value)\n If value is \"<b>Joel<\/b> <button>is<\/button> a <span>slug<\/span>\" the return value will be \"Joel is a slug\". If you are looking for a more robust solution, take a look at the bleach Python library.","title":"django.ref.utils#django.utils.html.strip_tags"},{"text":"gettext.dpgettext(domain, context, message)","title":"python.library.gettext#gettext.dpgettext"},{"text":"window.clrtoeol()  \nErase from cursor to the end of the line.","title":"python.library.curses#curses.window.clrtoeol"},{"text":"Match.string  \nThe string passed to match() or search().","title":"python.library.re#re.Match.string"},{"text":"clear()  \nRemove the payload and all of the headers.","title":"python.library.email.message#email.message.EmailMessage.clear"}]}
{"task_id":34945274,"prompt":"def f_34945274(A):\n\treturn ","suffix":"","canonical_solution":"np.where(np.in1d(A, [1, 3, 4]).reshape(A.shape), A, 0)","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    A = np.array([[6, 8, 1, 3, 4], [-1, 0, 3, 5, 1]])\n    B = np.array([[0, 0, 1, 3, 4], [0, 0, 3, 0, 1]])\n    assert np.array_equal(candidate(A), B)\n"],"entry_point":"f_34945274","intent":"replace all elements in array `A` that are not present in array `[1, 3, 4]` with zeros","library":["numpy"],"docs":[{"text":"array.remove(x)  \nRemove the first occurrence of x from the array.","title":"python.library.array#array.array.remove"},{"text":"numpy.ma.MaskedArray.__array_priority__ attribute   ma.MaskedArray.__array_priority__ = 15","title":"numpy.reference.generated.numpy.ma.maskedarray.__array_priority__"},{"text":"numpy.chararray.replace method   chararray.replace(old, new, count=None)[source]\n \nFor each element in self, return a copy of the string with all occurrences of substring old replaced by new.  See also  char.replace","title":"numpy.reference.generated.numpy.chararray.replace"},{"text":"array.tolist()  \nConvert the array to an ordinary list with the same items.","title":"python.library.array#array.array.tolist"},{"text":"numpy.char.chararray.replace method   char.chararray.replace(old, new, count=None)[source]\n \nFor each element in self, return a copy of the string with all occurrences of substring old replaced by new.  See also  char.replace","title":"numpy.reference.generated.numpy.char.chararray.replace"},{"text":"numpy.ma.masked_array   numpy.ma.masked_array[source]\n \nalias of numpy.ma.core.MaskedArray","title":"numpy.reference.generated.numpy.ma.masked_array"},{"text":"numpy.float16[source]\n \nalias of numpy.half","title":"numpy.reference.arrays.scalars#numpy.float16"},{"text":"numpy.recarray.squeeze method   recarray.squeeze(axis=None)\n \nRemove axes of length one from a. Refer to numpy.squeeze for full documentation.  See also  numpy.squeeze\n\nequivalent function","title":"numpy.reference.generated.numpy.recarray.squeeze"},{"text":"numpy.chararray.squeeze method   chararray.squeeze(axis=None)\n \nRemove axes of length one from a. Refer to numpy.squeeze for full documentation.  See also  numpy.squeeze\n\nequivalent function","title":"numpy.reference.generated.numpy.chararray.squeeze"},{"text":"__hash__=None","title":"matplotlib.transformations#matplotlib.transforms.AffineBase.__hash__"}]}
{"task_id":15819980,"prompt":"def f_15819980(a):\n\treturn ","suffix":"","canonical_solution":"np.mean(a, axis=1)","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    A = np.array([[6, 8, 1, 3, 4], [-1, 0, 3, 5, 1]])\n    B = np.array([4.4, 1.6])\n    assert np.array_equal(candidate(A), B)\n"],"entry_point":"f_15819980","intent":"calculate mean across dimension in a 2d array `a`","library":["numpy"],"docs":[{"text":"mean(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)  \nSee torch.mean()","title":"torch.tensors#torch.Tensor.mean"},{"text":"ndim  \nAlias for dim()","title":"torch.tensors#torch.Tensor.ndim"},{"text":"ndimension() \u2192 int  \nAlias for dim()","title":"torch.tensors#torch.Tensor.ndimension"},{"text":"property mean","title":"torch.distributions#torch.distributions.multivariate_normal.MultivariateNormal.mean"},{"text":"numpy.recarray.mean method   recarray.mean(axis=None, dtype=None, out=None, keepdims=False, *, where=True)\n \nReturns the average of the array elements along given axis. Refer to numpy.mean for full documentation.  See also  numpy.mean\n\nequivalent function","title":"numpy.reference.generated.numpy.recarray.mean"},{"text":"tf.experimental.numpy.ndim TensorFlow variant of NumPy's ndim. \ntf.experimental.numpy.ndim(\n    a\n)","title":"tensorflow.experimental.numpy.ndim"},{"text":"transpose(dim0, dim1) \u2192 Tensor  \nSee torch.transpose()","title":"torch.tensors#torch.Tensor.transpose"},{"text":"property mean","title":"torch.distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean"},{"text":"numpy.ndarray.mean method   ndarray.mean(axis=None, dtype=None, out=None, keepdims=False, *, where=True)\n \nReturns the average of the array elements along given axis. Refer to numpy.mean for full documentation.  See also  numpy.mean\n\nequivalent function","title":"numpy.reference.generated.numpy.ndarray.mean"},{"text":"diagonal(offset=0, dim1=0, dim2=1) \u2192 Tensor  \nSee torch.diagonal()","title":"torch.tensors#torch.Tensor.diagonal"}]}
{"task_id":19894365,"prompt":"def f_19894365():\n\treturn ","suffix":"","canonical_solution":"subprocess.call(['\/usr\/bin\/Rscript', '--vanilla', '\/pathto\/MyrScript.r'])","test_start":"\nfrom unittest.mock import Mock\nimport subprocess\n\ndef check(candidate):","test":["\n    subprocess.call = Mock(return_value = 0)\n    assert candidate() == 0\n"],"entry_point":"f_19894365","intent":"running r script '\/pathto\/MyrScript.r' from python","library":["subprocess"],"docs":[{"text":"stat.S_IXUSR  \nOwner has execute permission.","title":"python.library.stat#stat.S_IXUSR"},{"text":"LINETO=2","title":"matplotlib.path_api#matplotlib.path.Path.LINETO"},{"text":"MOVETO=1","title":"matplotlib.path_api#matplotlib.path.Path.MOVETO"},{"text":"stat.S_IXGRP  \nGroup has execute permission.","title":"python.library.stat#stat.S_IXGRP"},{"text":"run_script(pathname)  \nAnalyze the contents of the pathname file, which must contain Python code.","title":"python.library.modulefinder#modulefinder.ModuleFinder.run_script"},{"text":"stat.S_IRUSR  \nOwner has read permission.","title":"python.library.stat#stat.S_IRUSR"},{"text":"stat.S_IRGRP  \nGroup has read permission.","title":"python.library.stat#stat.S_IRGRP"},{"text":"stat.S_IWUSR  \nOwner has write permission.","title":"python.library.stat#stat.S_IWUSR"},{"text":"stat.S_IWOTH  \nOthers have write permission.","title":"python.library.stat#stat.S_IWOTH"},{"text":"stat.S_IFREG  \nRegular file.","title":"python.library.stat#stat.S_IFREG"}]}
{"task_id":19894365,"prompt":"def f_19894365():\n\treturn ","suffix":"","canonical_solution":"subprocess.call('\/usr\/bin\/Rscript --vanilla \/pathto\/MyrScript.r', shell=True)","test_start":"\nfrom unittest.mock import Mock\nimport subprocess\n\ndef check(candidate):","test":["\n    subprocess.call = Mock(return_value = 0)\n    assert candidate() == 0\n"],"entry_point":"f_19894365","intent":"run r script '\/usr\/bin\/Rscript --vanilla \/pathto\/MyrScript.r'","library":["subprocess"],"docs":[{"text":"stat.S_IFREG  \nRegular file.","title":"python.library.stat#stat.S_IFREG"},{"text":"stat.S_IXUSR  \nOwner has execute permission.","title":"python.library.stat#stat.S_IXUSR"},{"text":"stat.S_IXGRP  \nGroup has execute permission.","title":"python.library.stat#stat.S_IXGRP"},{"text":"run_script(pathname)  \nAnalyze the contents of the pathname file, which must contain Python code.","title":"python.library.modulefinder#modulefinder.ModuleFinder.run_script"},{"text":"run(cmd)  \nProfile the cmd via exec().","title":"python.library.profile#profile.Profile.run"},{"text":"run(test)  \nRun test and return the result.","title":"python.library.test#test.support.BasicTestRunner.run"},{"text":"rsqrt() \u2192 Tensor  \nSee torch.rsqrt()","title":"torch.tensors#torch.Tensor.rsqrt"},{"text":"stat.S_IWUSR  \nOwner has write permission.","title":"python.library.stat#stat.S_IWUSR"},{"text":"stat.S_IRUSR  \nOwner has read permission.","title":"python.library.stat#stat.S_IRUSR"},{"text":"test.support.unix_shell  \nPath for shell if not on Windows; otherwise None.","title":"python.library.test#test.support.unix_shell"}]}
{"task_id":33058590,"prompt":"def f_33058590(df):\n\treturn ","suffix":"","canonical_solution":"df.fillna(df.mean(axis=0))","test_start":"\nimport pandas as pd\nimport numpy as np\n\ndef check(candidate):","test":["\n    df = pd.DataFrame([[1,2,3],[4,5,6],[7.0,np.nan,9.0]], columns=[\"c1\",\"c2\",\"c3\"]) \n    res = pd.DataFrame([[1,2,3],[4,5,6],[7.0,3.5,9.0]], columns=[\"c1\",\"c2\",\"c3\"])\n    assert candidate(df).equals(res)\n"],"entry_point":"f_33058590","intent":"replacing nan in the dataframe `df` with row average","library":["numpy","pandas"],"docs":[{"text":"pandas.tseries.offsets.BYearEnd.normalize   BYearEnd.normalize","title":"pandas.reference.api.pandas.tseries.offsets.byearend.normalize"},{"text":"pandas.tseries.offsets.BYearBegin.normalize   BYearBegin.normalize","title":"pandas.reference.api.pandas.tseries.offsets.byearbegin.normalize"},{"text":"pandas.tseries.offsets.Tick.normalize   Tick.normalize","title":"pandas.reference.api.pandas.tseries.offsets.tick.normalize"},{"text":"pandas.tseries.offsets.BQuarterEnd.normalize   BQuarterEnd.normalize","title":"pandas.reference.api.pandas.tseries.offsets.bquarterend.normalize"},{"text":"pandas.tseries.offsets.Nano.normalize   Nano.normalize","title":"pandas.reference.api.pandas.tseries.offsets.nano.normalize"},{"text":"pandas.tseries.offsets.SemiMonthEnd.normalize   SemiMonthEnd.normalize","title":"pandas.reference.api.pandas.tseries.offsets.semimonthend.normalize"},{"text":"pandas.tseries.offsets.BQuarterBegin.normalize   BQuarterBegin.normalize","title":"pandas.reference.api.pandas.tseries.offsets.bquarterbegin.normalize"},{"text":"pandas.tseries.offsets.BYearEnd.apply   BYearEnd.apply()","title":"pandas.reference.api.pandas.tseries.offsets.byearend.apply"},{"text":"pandas.tseries.offsets.Tick.apply   Tick.apply()","title":"pandas.reference.api.pandas.tseries.offsets.tick.apply"},{"text":"pandas.tseries.offsets.FY5253Quarter.normalize   FY5253Quarter.normalize","title":"pandas.reference.api.pandas.tseries.offsets.fy5253quarter.normalize"}]}
{"task_id":12400256,"prompt":"def f_12400256():\n\treturn ","suffix":"","canonical_solution":"time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(1347517370))","test_start":"\nimport time\n\ndef check(candidate):","test":["\n    assert candidate() == \"2012-09-13 06:22:50\"\n"],"entry_point":"f_12400256","intent":"Convert unix timestamp '1347517370' to formatted string '%Y-%m-%d %H:%M:%S'","library":["time"],"docs":[{"text":"pandas.Timestamp.fromisoformat   Timestamp.fromisoformat()\n \nstring -> datetime from datetime.isoformat() output","title":"pandas.reference.api.pandas.timestamp.fromisoformat"},{"text":"pandas.Timestamp.min   Timestamp.min=Timestamp('1677-09-21 00:12:43.145224193')","title":"pandas.reference.api.pandas.timestamp.min"},{"text":"pandas.Timestamp.max   Timestamp.max=Timestamp('2262-04-11 23:47:16.854775807')","title":"pandas.reference.api.pandas.timestamp.max"},{"text":"pandas.Timestamp.nanosecond   Timestamp.nanosecond","title":"pandas.reference.api.pandas.timestamp.nanosecond"},{"text":"st_mtime_ns  \nTime of most recent content modification expressed in nanoseconds as an integer.","title":"python.library.os#os.stat_result.st_mtime_ns"},{"text":"pandas.Timestamp.microsecond   Timestamp.microsecond","title":"pandas.reference.api.pandas.timestamp.microsecond"},{"text":"st_atime_ns  \nTime of most recent access expressed in nanoseconds as an integer.","title":"python.library.os#os.stat_result.st_atime_ns"},{"text":"pandas.Timestamp.freq   Timestamp.freq","title":"pandas.reference.api.pandas.timestamp.freq"},{"text":"pandas.Timestamp.resolution   Timestamp.resolution=Timedelta('0 days 00:00:00.000000001')","title":"pandas.reference.api.pandas.timestamp.resolution"},{"text":"time.__str__()  \nFor a time t, str(t) is equivalent to t.isoformat().","title":"python.library.datetime#datetime.time.__str__"}]}
{"task_id":23359886,"prompt":"def f_23359886(a):\n\treturn ","suffix":"","canonical_solution":"a[np.where((a[:, (0)] == 0) * (a[:, (1)] == 1))]","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    a = np.array([[ 0,  1,  2], [ 3,  4,  5], [ 6,  7,  8], [ 9, 10, 11], [12, 13, 14]])\n    res = np.array([[0, 1, 2]])\n    assert np.array_equal(candidate(a), res)\n"],"entry_point":"f_23359886","intent":"selecting rows in Numpy ndarray 'a', where the value in the first column is 0 and value in the second column is 1","library":["numpy"],"docs":[{"text":"colno  \nThe column corresponding to pos.","title":"python.library.json#json.JSONDecodeError.colno"},{"text":"numpy.ndarray.__bool__ method   ndarray.__bool__(\/)\n \nself != 0","title":"numpy.reference.generated.numpy.ndarray.__bool__"},{"text":"colno  \nThe column corresponding to pos (may be None).","title":"python.library.re#re.error.colno"},{"text":"numpy.ndarray.nonzero method   ndarray.nonzero()\n \nReturn the indices of the elements that are non-zero. Refer to numpy.nonzero for full documentation.  See also  numpy.nonzero\n\nequivalent function","title":"numpy.reference.generated.numpy.ndarray.nonzero"},{"text":"numpy.chararray.index method   chararray.index(sub, start=0, end=None)[source]\n \nLike find, but raises ValueError when the substring is not found.  See also  char.index","title":"numpy.reference.generated.numpy.chararray.index"},{"text":"numpy.char.chararray.rindex method   char.chararray.rindex(sub, start=0, end=None)[source]\n \nLike rfind, but raises ValueError when the substring sub is not found.  See also  char.rindex","title":"numpy.reference.generated.numpy.char.chararray.rindex"},{"text":"numpy.char.chararray.index method   char.chararray.index(sub, start=0, end=None)[source]\n \nLike find, but raises ValueError when the substring is not found.  See also  char.index","title":"numpy.reference.generated.numpy.char.chararray.index"},{"text":"numpy.chararray.rindex method   chararray.rindex(sub, start=0, end=None)[source]\n \nLike rfind, but raises ValueError when the substring sub is not found.  See also  char.rindex","title":"numpy.reference.generated.numpy.chararray.rindex"},{"text":"pandas.tseries.offsets.Second.copy   Second.copy()","title":"pandas.reference.api.pandas.tseries.offsets.second.copy"},{"text":"pandas.tseries.offsets.Second.apply   Second.apply()","title":"pandas.reference.api.pandas.tseries.offsets.second.apply"}]}
{"task_id":4383082,"prompt":"def f_4383082(words):\n\treturn ","suffix":"","canonical_solution":"re.split(' +', words)","test_start":"\nimport regex as re\n\ndef check(candidate):","test":["\n    s = \"hello world sample text\"\n    res = [\"hello\", \"world\", \"sample\", \"text\"]\n    assert candidate(s) == res\n"],"entry_point":"f_4383082","intent":"separate words delimited by one or more spaces into a list","library":["regex"],"docs":[{"text":"gettext.lngettext(singular, plural, n)","title":"python.library.gettext#gettext.lngettext"},{"text":"re.split(pattern, string, maxsplit=0, flags=0)  \nSplit string by the occurrences of pattern. If capturing parentheses are used in pattern, then the text of all groups in the pattern are also returned as part of the resulting list. If maxsplit is nonzero, at most maxsplit splits occur, and the remainder of the string is returned as the final element of the list. >>> re.split(r'\\W+', 'Words, words, words.')\n['Words', 'words', 'words', '']\n>>> re.split(r'(\\W+)', 'Words, words, words.')\n['Words', ', ', 'words', ', ', 'words', '.', '']\n>>> re.split(r'\\W+', 'Words, words, words.', 1)\n['Words', 'words, words.']\n>>> re.split('[a-f]+', '0a3B9', flags=re.IGNORECASE)\n['0', '3', '9']\n If there are capturing groups in the separator and it matches at the start of the string, the result will start with an empty string. The same holds for the end of the string: >>> re.split(r'(\\W+)', '...words, words...')\n['', '...', 'words', ', ', 'words', '...', '']\n That way, separator components are always found at the same relative indices within the result list. Empty matches for the pattern split the string only when not adjacent to a previous empty match. >>> re.split(r'\\b', 'Words, words, words.')\n['', 'Words', ', ', 'words', ', ', 'words', '.']\n>>> re.split(r'\\W*', '...words...')\n['', '', 'w', 'o', 'r', 'd', 's', '', '']\n>>> re.split(r'(\\W*)', '...words...')\n['', '...', '', '', 'w', '', 'o', '', 'r', '', 'd', '', 's', '...', '', '', '']\n  Changed in version 3.1: Added the optional flags argument.   Changed in version 3.7: Added support of splitting on a pattern that could match an empty string.","title":"python.library.re#re.split"},{"text":"numpy.char.split   char.split(a, sep=None, maxsplit=None)[source]\n \nFor each element in a, return a list of the words in the string, using sep as the delimiter string. Calls str.split element-wise.  Parameters \n \naarray_like of str or unicode\n\n\nsepstr or unicode, optional\n\n\nIf sep is not specified or None, any whitespace string is a separator.  \nmaxsplitint, optional\n\n\nIf maxsplit is given, at most maxsplit splits are done.    Returns \n \noutndarray\n\n\nArray of list objects      See also  \nstr.split, rsplit","title":"numpy.reference.generated.numpy.char.split"},{"text":"str.split(sep=None, maxsplit=-1)  \nReturn a list of the words in the string, using sep as the delimiter string. If maxsplit is given, at most maxsplit splits are done (thus, the list will have at most maxsplit+1 elements). If maxsplit is not specified or -1, then there is no limit on the number of splits (all possible splits are made). If sep is given, consecutive delimiters are not grouped together and are deemed to delimit empty strings (for example, '1,,2'.split(',') returns ['1', '', '2']). The sep argument may consist of multiple characters (for example, '1<>2<>3'.split('<>') returns ['1', '2', '3']). Splitting an empty string with a specified separator returns ['']. For example: >>> '1,2,3'.split(',')\n['1', '2', '3']\n>>> '1,2,3'.split(',', maxsplit=1)\n['1', '2,3']\n>>> '1,2,,3,'.split(',')\n['1', '2', '', '3', '']\n If sep is not specified or is None, a different splitting algorithm is applied: runs of consecutive whitespace are regarded as a single separator, and the result will contain no empty strings at the start or end if the string has leading or trailing whitespace. Consequently, splitting an empty string or a string consisting of just whitespace with a None separator returns []. For example: >>> '1 2 3'.split()\n['1', '2', '3']\n>>> '1 2 3'.split(maxsplit=1)\n['1', '2 3']\n>>> '   1   2   3   '.split()\n['1', '2', '3']","title":"python.library.stdtypes#str.split"},{"text":"re.purge()  \nClear the regular expression cache.","title":"python.library.re#re.purge"},{"text":"numpy.char.chararray.split method   char.chararray.split(sep=None, maxsplit=None)[source]\n \nFor each element in self, return a list of the words in the string, using sep as the delimiter string.  See also  char.split","title":"numpy.reference.generated.numpy.char.chararray.split"},{"text":"numpy.chararray.split method   chararray.split(sep=None, maxsplit=None)[source]\n \nFor each element in self, return a list of the words in the string, using sep as the delimiter string.  See also  char.split","title":"numpy.reference.generated.numpy.chararray.split"},{"text":"token.N_TOKENS","title":"python.library.token#token.N_TOKENS"},{"text":"token.INDENT","title":"python.library.token#token.INDENT"},{"text":"numpy.char.chararray.rsplit method   char.chararray.rsplit(sep=None, maxsplit=None)[source]\n \nFor each element in self, return a list of the words in the string, using sep as the delimiter string.  See also  char.rsplit","title":"numpy.reference.generated.numpy.char.chararray.rsplit"}]}
{"task_id":14637696,"prompt":"def f_14637696(words):\n\treturn ","suffix":"","canonical_solution":"len(max(words, key=len))","test_start":"\ndef check(candidate):","test":["\n    assert candidate([\"hello\", \"world\", \"sample\", \"text\", \"superballer\"]) == 11\n"],"entry_point":"f_14637696","intent":"length of longest element in list `words`","library":[],"docs":[]}
{"task_id":3933478,"prompt":"def f_3933478(result):\n\treturn ","suffix":"","canonical_solution":"result[0]['from_user']","test_start":"\ndef check(candidate):","test":["\n    Contents = [{\"hi\": 7, \"bye\": 4, \"from_user\": 0}, {1: 2, 3: 4, 5: 6}]\n    assert candidate(Contents) == 0\n"],"entry_point":"f_3933478","intent":"get the value associated with unicode key 'from_user' of first dictionary in list `result`","library":[],"docs":[]}
{"task_id":39112645,"prompt":"def f_39112645():\n\treturn ","suffix":"","canonical_solution":"[line.split() for line in open('File.txt')]","test_start":"\ndef check(candidate):","test":["\n    with open('File.txt','w') as fw:\n        fw.write(\"hi hello cat dog\")\n    assert candidate() == [['hi', 'hello', 'cat', 'dog']]\n"],"entry_point":"f_39112645","intent":"Retrieve each line from a file 'File.txt' as a list","library":[],"docs":[]}
{"task_id":1031851,"prompt":"def f_1031851(a):\n\treturn ","suffix":"","canonical_solution":"dict((v, k) for k, v in a.items())","test_start":"\ndef check(candidate):","test":["\n    a = {\"one\": 1, \"two\": 2}\n    assert candidate(a) == {1: \"one\", 2: \"two\"}\n"],"entry_point":"f_1031851","intent":"swap keys with values in a dictionary `a`","library":[],"docs":[]}
{"task_id":8577137,"prompt":"def f_8577137():\n\treturn ","suffix":"","canonical_solution":"open('path\/to\/FILE_NAME.ext', 'w')","test_start":"\nimport os\n\ndef check(candidate):","test":["\n    path1 = os.path.join(\"\", \"path\")\n    os.mkdir(path1)\n    path2 = os.path.join(\"path\", \"to\")\n    os.mkdir(path2)\n    candidate()\n    assert os.path.exists('path\/to\/FILE_NAME.ext')\n"],"entry_point":"f_8577137","intent":"Open a file `path\/to\/FILE_NAME.ext` in write mode","library":["os"],"docs":[{"text":"_open(name, mode='rb')","title":"django.howto.custom-file-storage#django.core.files.storage._open"},{"text":"FieldFile.open(mode='rb')","title":"django.ref.models.fields#django.db.models.fields.files.FieldFile.open"},{"text":"tkinter.filedialog.askdirectory(**options)","title":"python.library.dialog#tkinter.filedialog.askdirectory"},{"text":"Path.open(mode='r', buffering=-1, encoding=None, errors=None, newline=None)  \nOpen the file pointed to by the path, like the built-in open() function does: >>> p = Path('setup.py')\n>>> with p.open() as f:\n...     f.readline()\n...\n'#!\/usr\/bin\/env python3\\n'","title":"python.library.pathlib#pathlib.Path.open"},{"text":"get_alternative_name(file_root, file_ext)","title":"django.howto.custom-file-storage#django.core.files.storage.get_alternative_name"},{"text":"array.tofile(f)  \nWrite all items (as machine values) to the file object f.","title":"python.library.array#array.array.tofile"},{"text":"path  \nPath to the extension module.","title":"python.library.importlib#importlib.machinery.ExtensionFileLoader.path"},{"text":"abstractmethod get_filename(fullname)  \nReturns path.","title":"python.library.importlib#importlib.abc.FileLoader.get_filename"},{"text":"fd  \nUnderlying file descriptor.","title":"python.library.selectors#selectors.SelectorKey.fd"},{"text":"filename  \nThe name of the file the syntax error occurred in.","title":"python.library.exceptions#SyntaxError.filename"}]}
{"task_id":17926273,"prompt":"def f_17926273(df):\n\treturn ","suffix":"","canonical_solution":"df.groupby(['col1', 'col2'])['col3'].nunique().reset_index()","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    data = [[1, 1, 1], [1, 1, 1], [1, 1, 2], [1, 2, 3], [1, 2, 3], [1, 2, 3], \n            [2, 1, 1], [2, 1, 2], [2, 1, 3], [2, 2, 3], [2, 2, 3], [2, 2, 3]]\n    expected = [[1, 1, 2], [1, 2, 1], [2, 1, 3], [2, 2, 1]]\n    df = pd.DataFrame(data, columns = ['col1', 'col2', 'col3'])\n    expected_df = pd.DataFrame(expected, columns = ['col1', 'col2', 'col3'])\n    df1 = candidate(df)\n    assert pd.DataFrame.equals(expected_df, df1)\n"],"entry_point":"f_17926273","intent":"count distinct values in a column 'col3' of a pandas dataframe `df` group by objects in 'col1' and 'col2'","library":["pandas"],"docs":[{"text":"distinct(*fields)","title":"django.ref.models.querysets#django.db.models.query.QuerySet.distinct"},{"text":"DataFrame  Constructor       \nDataFrame([data, index, columns, dtype, copy]) Two-dimensional, size-mutable, potentially heterogeneous tabular data.      Attributes and underlying data Axes       \nDataFrame.index The index (row labels) of the DataFrame.  \nDataFrame.columns The column labels of the DataFrame.          \nDataFrame.dtypes Return the dtypes in the DataFrame.  \nDataFrame.info([verbose, buf, max_cols, ...]) Print a concise summary of a DataFrame.  \nDataFrame.select_dtypes([include, exclude]) Return a subset of the DataFrame's columns based on the column dtypes.  \nDataFrame.values Return a Numpy representation of the DataFrame.  \nDataFrame.axes Return a list representing the axes of the DataFrame.  \nDataFrame.ndim Return an int representing the number of axes \/ array dimensions.  \nDataFrame.size Return an int representing the number of elements in this object.  \nDataFrame.shape Return a tuple representing the dimensionality of the DataFrame.  \nDataFrame.memory_usage([index, deep]) Return the memory usage of each column in bytes.  \nDataFrame.empty Indicator whether Series\/DataFrame is empty.  \nDataFrame.set_flags(*[, copy, ...]) Return a new object with updated flags.      Conversion       \nDataFrame.astype(dtype[, copy, errors]) Cast a pandas object to a specified dtype dtype.  \nDataFrame.convert_dtypes([infer_objects, ...]) Convert columns to best possible dtypes using dtypes supporting pd.NA.  \nDataFrame.infer_objects() Attempt to infer better dtypes for object columns.  \nDataFrame.copy([deep]) Make a copy of this object's indices and data.  \nDataFrame.bool() Return the bool of a single element Series or DataFrame.      Indexing, iteration       \nDataFrame.head([n]) Return the first n rows.  \nDataFrame.at Access a single value for a row\/column label pair.  \nDataFrame.iat Access a single value for a row\/column pair by integer position.  \nDataFrame.loc Access a group of rows and columns by label(s) or a boolean array.  \nDataFrame.iloc Purely integer-location based indexing for selection by position.  \nDataFrame.insert(loc, column, value[, ...]) Insert column into DataFrame at specified location.  \nDataFrame.__iter__() Iterate over info axis.  \nDataFrame.items() Iterate over (column name, Series) pairs.  \nDataFrame.iteritems() Iterate over (column name, Series) pairs.  \nDataFrame.keys() Get the 'info axis' (see Indexing for more).  \nDataFrame.iterrows() Iterate over DataFrame rows as (index, Series) pairs.  \nDataFrame.itertuples([index, name]) Iterate over DataFrame rows as namedtuples.  \nDataFrame.lookup(row_labels, col_labels) (DEPRECATED) Label-based \"fancy indexing\" function for DataFrame.  \nDataFrame.pop(item) Return item and drop from frame.  \nDataFrame.tail([n]) Return the last n rows.  \nDataFrame.xs(key[, axis, level, drop_level]) Return cross-section from the Series\/DataFrame.  \nDataFrame.get(key[, default]) Get item from object for given key (ex: DataFrame column).  \nDataFrame.isin(values) Whether each element in the DataFrame is contained in values.  \nDataFrame.where(cond[, other, inplace, ...]) Replace values where the condition is False.  \nDataFrame.mask(cond[, other, inplace, axis, ...]) Replace values where the condition is True.  \nDataFrame.query(expr[, inplace]) Query the columns of a DataFrame with a boolean expression.    For more information on .at, .iat, .loc, and .iloc, see the indexing documentation.   Binary operator functions       \nDataFrame.add(other[, axis, level, fill_value]) Get Addition of dataframe and other, element-wise (binary operator add).  \nDataFrame.sub(other[, axis, level, fill_value]) Get Subtraction of dataframe and other, element-wise (binary operator sub).  \nDataFrame.mul(other[, axis, level, fill_value]) Get Multiplication of dataframe and other, element-wise (binary operator mul).  \nDataFrame.div(other[, axis, level, fill_value]) Get Floating division of dataframe and other, element-wise (binary operator truediv).  \nDataFrame.truediv(other[, axis, level, ...]) Get Floating division of dataframe and other, element-wise (binary operator truediv).  \nDataFrame.floordiv(other[, axis, level, ...]) Get Integer division of dataframe and other, element-wise (binary operator floordiv).  \nDataFrame.mod(other[, axis, level, fill_value]) Get Modulo of dataframe and other, element-wise (binary operator mod).  \nDataFrame.pow(other[, axis, level, fill_value]) Get Exponential power of dataframe and other, element-wise (binary operator pow).  \nDataFrame.dot(other) Compute the matrix multiplication between the DataFrame and other.  \nDataFrame.radd(other[, axis, level, fill_value]) Get Addition of dataframe and other, element-wise (binary operator radd).  \nDataFrame.rsub(other[, axis, level, fill_value]) Get Subtraction of dataframe and other, element-wise (binary operator rsub).  \nDataFrame.rmul(other[, axis, level, fill_value]) Get Multiplication of dataframe and other, element-wise (binary operator rmul).  \nDataFrame.rdiv(other[, axis, level, fill_value]) Get Floating division of dataframe and other, element-wise (binary operator rtruediv).  \nDataFrame.rtruediv(other[, axis, level, ...]) Get Floating division of dataframe and other, element-wise (binary operator rtruediv).  \nDataFrame.rfloordiv(other[, axis, level, ...]) Get Integer division of dataframe and other, element-wise (binary operator rfloordiv).  \nDataFrame.rmod(other[, axis, level, fill_value]) Get Modulo of dataframe and other, element-wise (binary operator rmod).  \nDataFrame.rpow(other[, axis, level, fill_value]) Get Exponential power of dataframe and other, element-wise (binary operator rpow).  \nDataFrame.lt(other[, axis, level]) Get Less than of dataframe and other, element-wise (binary operator lt).  \nDataFrame.gt(other[, axis, level]) Get Greater than of dataframe and other, element-wise (binary operator gt).  \nDataFrame.le(other[, axis, level]) Get Less than or equal to of dataframe and other, element-wise (binary operator le).  \nDataFrame.ge(other[, axis, level]) Get Greater than or equal to of dataframe and other, element-wise (binary operator ge).  \nDataFrame.ne(other[, axis, level]) Get Not equal to of dataframe and other, element-wise (binary operator ne).  \nDataFrame.eq(other[, axis, level]) Get Equal to of dataframe and other, element-wise (binary operator eq).  \nDataFrame.combine(other, func[, fill_value, ...]) Perform column-wise combine with another DataFrame.  \nDataFrame.combine_first(other) Update null elements with value in the same location in other.      Function application, GroupBy & window       \nDataFrame.apply(func[, axis, raw, ...]) Apply a function along an axis of the DataFrame.  \nDataFrame.applymap(func[, na_action]) Apply a function to a Dataframe elementwise.  \nDataFrame.pipe(func, *args, **kwargs) Apply chainable functions that expect Series or DataFrames.  \nDataFrame.agg([func, axis]) Aggregate using one or more operations over the specified axis.  \nDataFrame.aggregate([func, axis]) Aggregate using one or more operations over the specified axis.  \nDataFrame.transform(func[, axis]) Call func on self producing a DataFrame with the same axis shape as self.  \nDataFrame.groupby([by, axis, level, ...]) Group DataFrame using a mapper or by a Series of columns.  \nDataFrame.rolling(window[, min_periods, ...]) Provide rolling window calculations.  \nDataFrame.expanding([min_periods, center, ...]) Provide expanding window calculations.  \nDataFrame.ewm([com, span, halflife, alpha, ...]) Provide exponentially weighted (EW) calculations.      Computations \/ descriptive stats       \nDataFrame.abs() Return a Series\/DataFrame with absolute numeric value of each element.  \nDataFrame.all([axis, bool_only, skipna, level]) Return whether all elements are True, potentially over an axis.  \nDataFrame.any([axis, bool_only, skipna, level]) Return whether any element is True, potentially over an axis.  \nDataFrame.clip([lower, upper, axis, inplace]) Trim values at input threshold(s).  \nDataFrame.corr([method, min_periods]) Compute pairwise correlation of columns, excluding NA\/null values.  \nDataFrame.corrwith(other[, axis, drop, method]) Compute pairwise correlation.  \nDataFrame.count([axis, level, numeric_only]) Count non-NA cells for each column or row.  \nDataFrame.cov([min_periods, ddof]) Compute pairwise covariance of columns, excluding NA\/null values.  \nDataFrame.cummax([axis, skipna]) Return cumulative maximum over a DataFrame or Series axis.  \nDataFrame.cummin([axis, skipna]) Return cumulative minimum over a DataFrame or Series axis.  \nDataFrame.cumprod([axis, skipna]) Return cumulative product over a DataFrame or Series axis.  \nDataFrame.cumsum([axis, skipna]) Return cumulative sum over a DataFrame or Series axis.  \nDataFrame.describe([percentiles, include, ...]) Generate descriptive statistics.  \nDataFrame.diff([periods, axis]) First discrete difference of element.  \nDataFrame.eval(expr[, inplace]) Evaluate a string describing operations on DataFrame columns.  \nDataFrame.kurt([axis, skipna, level, ...]) Return unbiased kurtosis over requested axis.  \nDataFrame.kurtosis([axis, skipna, level, ...]) Return unbiased kurtosis over requested axis.  \nDataFrame.mad([axis, skipna, level]) Return the mean absolute deviation of the values over the requested axis.  \nDataFrame.max([axis, skipna, level, ...]) Return the maximum of the values over the requested axis.  \nDataFrame.mean([axis, skipna, level, ...]) Return the mean of the values over the requested axis.  \nDataFrame.median([axis, skipna, level, ...]) Return the median of the values over the requested axis.  \nDataFrame.min([axis, skipna, level, ...]) Return the minimum of the values over the requested axis.  \nDataFrame.mode([axis, numeric_only, dropna]) Get the mode(s) of each element along the selected axis.  \nDataFrame.pct_change([periods, fill_method, ...]) Percentage change between the current and a prior element.  \nDataFrame.prod([axis, skipna, level, ...]) Return the product of the values over the requested axis.  \nDataFrame.product([axis, skipna, level, ...]) Return the product of the values over the requested axis.  \nDataFrame.quantile([q, axis, numeric_only, ...]) Return values at the given quantile over requested axis.  \nDataFrame.rank([axis, method, numeric_only, ...]) Compute numerical data ranks (1 through n) along axis.  \nDataFrame.round([decimals]) Round a DataFrame to a variable number of decimal places.  \nDataFrame.sem([axis, skipna, level, ddof, ...]) Return unbiased standard error of the mean over requested axis.  \nDataFrame.skew([axis, skipna, level, ...]) Return unbiased skew over requested axis.  \nDataFrame.sum([axis, skipna, level, ...]) Return the sum of the values over the requested axis.  \nDataFrame.std([axis, skipna, level, ddof, ...]) Return sample standard deviation over requested axis.  \nDataFrame.var([axis, skipna, level, ddof, ...]) Return unbiased variance over requested axis.  \nDataFrame.nunique([axis, dropna]) Count number of distinct elements in specified axis.  \nDataFrame.value_counts([subset, normalize, ...]) Return a Series containing counts of unique rows in the DataFrame.      Reindexing \/ selection \/ label manipulation       \nDataFrame.add_prefix(prefix) Prefix labels with string prefix.  \nDataFrame.add_suffix(suffix) Suffix labels with string suffix.  \nDataFrame.align(other[, join, axis, level, ...]) Align two objects on their axes with the specified join method.  \nDataFrame.at_time(time[, asof, axis]) Select values at particular time of day (e.g., 9:30AM).  \nDataFrame.between_time(start_time, end_time) Select values between particular times of the day (e.g., 9:00-9:30 AM).  \nDataFrame.drop([labels, axis, index, ...]) Drop specified labels from rows or columns.  \nDataFrame.drop_duplicates([subset, keep, ...]) Return DataFrame with duplicate rows removed.  \nDataFrame.duplicated([subset, keep]) Return boolean Series denoting duplicate rows.  \nDataFrame.equals(other) Test whether two objects contain the same elements.  \nDataFrame.filter([items, like, regex, axis]) Subset the dataframe rows or columns according to the specified index labels.  \nDataFrame.first(offset) Select initial periods of time series data based on a date offset.  \nDataFrame.head([n]) Return the first n rows.  \nDataFrame.idxmax([axis, skipna]) Return index of first occurrence of maximum over requested axis.  \nDataFrame.idxmin([axis, skipna]) Return index of first occurrence of minimum over requested axis.  \nDataFrame.last(offset) Select final periods of time series data based on a date offset.  \nDataFrame.reindex([labels, index, columns, ...]) Conform Series\/DataFrame to new index with optional filling logic.  \nDataFrame.reindex_like(other[, method, ...]) Return an object with matching indices as other object.  \nDataFrame.rename([mapper, index, columns, ...]) Alter axes labels.  \nDataFrame.rename_axis([mapper, index, ...]) Set the name of the axis for the index or columns.  \nDataFrame.reset_index([level, drop, ...]) Reset the index, or a level of it.  \nDataFrame.sample([n, frac, replace, ...]) Return a random sample of items from an axis of object.  \nDataFrame.set_axis(labels[, axis, inplace]) Assign desired index to given axis.  \nDataFrame.set_index(keys[, drop, append, ...]) Set the DataFrame index using existing columns.  \nDataFrame.tail([n]) Return the last n rows.  \nDataFrame.take(indices[, axis, is_copy]) Return the elements in the given positional indices along an axis.  \nDataFrame.truncate([before, after, axis, copy]) Truncate a Series or DataFrame before and after some index value.      Missing data handling       \nDataFrame.backfill([axis, inplace, limit, ...]) Synonym for DataFrame.fillna() with method='bfill'.  \nDataFrame.bfill([axis, inplace, limit, downcast]) Synonym for DataFrame.fillna() with method='bfill'.  \nDataFrame.dropna([axis, how, thresh, ...]) Remove missing values.  \nDataFrame.ffill([axis, inplace, limit, downcast]) Synonym for DataFrame.fillna() with method='ffill'.  \nDataFrame.fillna([value, method, axis, ...]) Fill NA\/NaN values using the specified method.  \nDataFrame.interpolate([method, axis, limit, ...]) Fill NaN values using an interpolation method.  \nDataFrame.isna() Detect missing values.  \nDataFrame.isnull() DataFrame.isnull is an alias for DataFrame.isna.  \nDataFrame.notna() Detect existing (non-missing) values.  \nDataFrame.notnull() DataFrame.notnull is an alias for DataFrame.notna.  \nDataFrame.pad([axis, inplace, limit, downcast]) Synonym for DataFrame.fillna() with method='ffill'.  \nDataFrame.replace([to_replace, value, ...]) Replace values given in to_replace with value.      Reshaping, sorting, transposing       \nDataFrame.droplevel(level[, axis]) Return Series\/DataFrame with requested index \/ column level(s) removed.  \nDataFrame.pivot([index, columns, values]) Return reshaped DataFrame organized by given index \/ column values.  \nDataFrame.pivot_table([values, index, ...]) Create a spreadsheet-style pivot table as a DataFrame.  \nDataFrame.reorder_levels(order[, axis]) Rearrange index levels using input order.  \nDataFrame.sort_values(by[, axis, ascending, ...]) Sort by the values along either axis.  \nDataFrame.sort_index([axis, level, ...]) Sort object by labels (along an axis).  \nDataFrame.nlargest(n, columns[, keep]) Return the first n rows ordered by columns in descending order.  \nDataFrame.nsmallest(n, columns[, keep]) Return the first n rows ordered by columns in ascending order.  \nDataFrame.swaplevel([i, j, axis]) Swap levels i and j in a MultiIndex.  \nDataFrame.stack([level, dropna]) Stack the prescribed level(s) from columns to index.  \nDataFrame.unstack([level, fill_value]) Pivot a level of the (necessarily hierarchical) index labels.  \nDataFrame.swapaxes(axis1, axis2[, copy]) Interchange axes and swap values axes appropriately.  \nDataFrame.melt([id_vars, value_vars, ...]) Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.  \nDataFrame.explode(column[, ignore_index]) Transform each element of a list-like to a row, replicating index values.  \nDataFrame.squeeze([axis]) Squeeze 1 dimensional axis objects into scalars.  \nDataFrame.to_xarray() Return an xarray object from the pandas object.  \nDataFrame.T   \nDataFrame.transpose(*args[, copy]) Transpose index and columns.      Combining \/ comparing \/ joining \/ merging       \nDataFrame.append(other[, ignore_index, ...]) Append rows of other to the end of caller, returning a new object.  \nDataFrame.assign(**kwargs) Assign new columns to a DataFrame.  \nDataFrame.compare(other[, align_axis, ...]) Compare to another DataFrame and show the differences.  \nDataFrame.join(other[, on, how, lsuffix, ...]) Join columns of another DataFrame.  \nDataFrame.merge(right[, how, on, left_on, ...]) Merge DataFrame or named Series objects with a database-style join.  \nDataFrame.update(other[, join, overwrite, ...]) Modify in place using non-NA values from another DataFrame.      Time Series-related       \nDataFrame.asfreq(freq[, method, how, ...]) Convert time series to specified frequency.  \nDataFrame.asof(where[, subset]) Return the last row(s) without any NaNs before where.  \nDataFrame.shift([periods, freq, axis, ...]) Shift index by desired number of periods with an optional time freq.  \nDataFrame.slice_shift([periods, axis]) (DEPRECATED) Equivalent to shift without copying data.  \nDataFrame.tshift([periods, freq, axis]) (DEPRECATED) Shift the time index, using the index's frequency if available.  \nDataFrame.first_valid_index() Return index for first non-NA value or None, if no NA value is found.  \nDataFrame.last_valid_index() Return index for last non-NA value or None, if no NA value is found.  \nDataFrame.resample(rule[, axis, closed, ...]) Resample time-series data.  \nDataFrame.to_period([freq, axis, copy]) Convert DataFrame from DatetimeIndex to PeriodIndex.  \nDataFrame.to_timestamp([freq, how, axis, copy]) Cast to DatetimeIndex of timestamps, at beginning of period.  \nDataFrame.tz_convert(tz[, axis, level, copy]) Convert tz-aware axis to target time zone.  \nDataFrame.tz_localize(tz[, axis, level, ...]) Localize tz-naive index of a Series or DataFrame to target time zone.      Flags Flags refer to attributes of the pandas object. Properties of the dataset (like the date is was recorded, the URL it was accessed from, etc.) should be stored in DataFrame.attrs.       \nFlags(obj, *, allows_duplicate_labels) Flags that apply to pandas objects.      Metadata DataFrame.attrs is a dictionary for storing global metadata for this DataFrame.  Warning DataFrame.attrs is considered experimental and may change without warning.        \nDataFrame.attrs Dictionary of global attributes of this dataset.      Plotting DataFrame.plot is both a callable method and a namespace attribute for specific plotting methods of the form DataFrame.plot.<kind>.       \nDataFrame.plot([x, y, kind, ax, ....]) DataFrame plotting accessor and method          \nDataFrame.plot.area([x, y]) Draw a stacked area plot.  \nDataFrame.plot.bar([x, y]) Vertical bar plot.  \nDataFrame.plot.barh([x, y]) Make a horizontal bar plot.  \nDataFrame.plot.box([by]) Make a box plot of the DataFrame columns.  \nDataFrame.plot.density([bw_method, ind]) Generate Kernel Density Estimate plot using Gaussian kernels.  \nDataFrame.plot.hexbin(x, y[, C, ...]) Generate a hexagonal binning plot.  \nDataFrame.plot.hist([by, bins]) Draw one histogram of the DataFrame's columns.  \nDataFrame.plot.kde([bw_method, ind]) Generate Kernel Density Estimate plot using Gaussian kernels.  \nDataFrame.plot.line([x, y]) Plot Series or DataFrame as lines.  \nDataFrame.plot.pie(**kwargs) Generate a pie plot.  \nDataFrame.plot.scatter(x, y[, s, c]) Create a scatter plot with varying marker point size and color.          \nDataFrame.boxplot([column, by, ax, ...]) Make a box plot from DataFrame columns.  \nDataFrame.hist([column, by, grid, ...]) Make a histogram of the DataFrame's columns.      Sparse accessor Sparse-dtype specific methods and attributes are provided under the DataFrame.sparse accessor.       \nDataFrame.sparse.density Ratio of non-sparse points to total (dense) data points.          \nDataFrame.sparse.from_spmatrix(data[, ...]) Create a new DataFrame from a scipy sparse matrix.  \nDataFrame.sparse.to_coo() Return the contents of the frame as a sparse SciPy COO matrix.  \nDataFrame.sparse.to_dense() Convert a DataFrame with sparse values to dense.      Serialization \/ IO \/ conversion       \nDataFrame.from_dict(data[, orient, dtype, ...]) Construct DataFrame from dict of array-like or dicts.  \nDataFrame.from_records(data[, index, ...]) Convert structured or record ndarray to DataFrame.  \nDataFrame.to_parquet([path, engine, ...]) Write a DataFrame to the binary parquet format.  \nDataFrame.to_pickle(path[, compression, ...]) Pickle (serialize) object to file.  \nDataFrame.to_csv([path_or_buf, sep, na_rep, ...]) Write object to a comma-separated values (csv) file.  \nDataFrame.to_hdf(path_or_buf, key[, mode, ...]) Write the contained data to an HDF5 file using HDFStore.  \nDataFrame.to_sql(name, con[, schema, ...]) Write records stored in a DataFrame to a SQL database.  \nDataFrame.to_dict([orient, into]) Convert the DataFrame to a dictionary.  \nDataFrame.to_excel(excel_writer[, ...]) Write object to an Excel sheet.  \nDataFrame.to_json([path_or_buf, orient, ...]) Convert the object to a JSON string.  \nDataFrame.to_html([buf, columns, col_space, ...]) Render a DataFrame as an HTML table.  \nDataFrame.to_feather(path, **kwargs) Write a DataFrame to the binary Feather format.  \nDataFrame.to_latex([buf, columns, ...]) Render object to a LaTeX tabular, longtable, or nested table.  \nDataFrame.to_stata(path[, convert_dates, ...]) Export DataFrame object to Stata dta format.  \nDataFrame.to_gbq(destination_table[, ...]) Write a DataFrame to a Google BigQuery table.  \nDataFrame.to_records([index, column_dtypes, ...]) Convert DataFrame to a NumPy record array.  \nDataFrame.to_string([buf, columns, ...]) Render a DataFrame to a console-friendly tabular output.  \nDataFrame.to_clipboard([excel, sep]) Copy object to the system clipboard.  \nDataFrame.to_markdown([buf, mode, index, ...]) Print DataFrame in Markdown-friendly format.  \nDataFrame.style Returns a Styler object.","title":"pandas.reference.frame"},{"text":"pandas.core.groupby.DataFrameGroupBy.nunique   DataFrameGroupBy.nunique(dropna=True)[source]\n \nReturn DataFrame with counts of unique elements in each position.  Parameters \n \ndropna:bool, default True\n\n\nDon\u2019t include NaN in the counts.    Returns \n nunique: DataFrame\n   Examples \n>>> df = pd.DataFrame({'id': ['spam', 'egg', 'egg', 'spam',\n...                           'ham', 'ham'],\n...                    'value1': [1, 5, 5, 2, 5, 5],\n...                    'value2': list('abbaxy')})\n>>> df\n     id  value1 value2\n0  spam       1      a\n1   egg       5      b\n2   egg       5      b\n3  spam       2      a\n4   ham       5      x\n5   ham       5      y\n  \n>>> df.groupby('id').nunique()\n      value1  value2\nid\negg        1       1\nham        1       2\nspam       2       1\n  Check for rows with the same id but conflicting values: \n>>> df.groupby('id').filter(lambda g: (g.nunique() > 1).any())\n     id  value1 value2\n0  spam       1      a\n3  spam       2      a\n4   ham       5      x\n5   ham       5      y","title":"pandas.reference.api.pandas.core.groupby.dataframegroupby.nunique"},{"text":"pandas.DataFrame.items   DataFrame.items()[source]\n \nIterate over (column name, Series) pairs. Iterates over the DataFrame columns, returning a tuple with the column name and the content as a Series.  Yields \n \nlabel:object\n\n\nThe column names for the DataFrame being iterated over.  \ncontent:Series\n\n\nThe column entries belonging to each label, as a Series.      See also  DataFrame.iterrows\n\nIterate over DataFrame rows as (index, Series) pairs.  DataFrame.itertuples\n\nIterate over DataFrame rows as namedtuples of the values.    Examples \n>>> df = pd.DataFrame({'species': ['bear', 'bear', 'marsupial'],\n...                   'population': [1864, 22000, 80000]},\n...                   index=['panda', 'polar', 'koala'])\n>>> df\n        species   population\npanda   bear      1864\npolar   bear      22000\nkoala   marsupial 80000\n>>> for label, content in df.items():\n...     print(f'label: {label}')\n...     print(f'content: {content}', sep='\\n')\n...\nlabel: species\ncontent:\npanda         bear\npolar         bear\nkoala    marsupial\nName: species, dtype: object\nlabel: population\ncontent:\npanda     1864\npolar    22000\nkoala    80000\nName: population, dtype: int64","title":"pandas.reference.api.pandas.dataframe.items"},{"text":"pandas.DataFrame.drop_duplicates   DataFrame.drop_duplicates(subset=None, keep='first', inplace=False, ignore_index=False)[source]\n \nReturn DataFrame with duplicate rows removed. Considering certain columns is optional. Indexes, including time indexes are ignored.  Parameters \n \nsubset:column label or sequence of labels, optional\n\n\nOnly consider certain columns for identifying duplicates, by default use all of the columns.  \nkeep:{\u2018first\u2019, \u2018last\u2019, False}, default \u2018first\u2019\n\n\nDetermines which duplicates (if any) to keep. - first : Drop duplicates except for the first occurrence. - last : Drop duplicates except for the last occurrence. - False : Drop all duplicates.  \ninplace:bool, default False\n\n\nWhether to drop duplicates in place or to return a copy.  \nignore_index:bool, default False\n\n\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.  New in version 1.0.0.     Returns \n DataFrame or None\n\nDataFrame with duplicates removed or None if inplace=True.      See also  DataFrame.value_counts\n\nCount unique combinations of columns.    Examples Consider dataset containing ramen rating. \n>>> df = pd.DataFrame({\n...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n...     'rating': [4, 4, 3.5, 15, 5]\n... })\n>>> df\n    brand style  rating\n0  Yum Yum   cup     4.0\n1  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n3  Indomie  pack    15.0\n4  Indomie  pack     5.0\n  By default, it removes duplicate rows based on all columns. \n>>> df.drop_duplicates()\n    brand style  rating\n0  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n3  Indomie  pack    15.0\n4  Indomie  pack     5.0\n  To remove duplicates on specific column(s), use subset. \n>>> df.drop_duplicates(subset=['brand'])\n    brand style  rating\n0  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n  To remove duplicates and keep last occurrences, use keep. \n>>> df.drop_duplicates(subset=['brand', 'style'], keep='last')\n    brand style  rating\n1  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n4  Indomie  pack     5.0","title":"pandas.reference.api.pandas.dataframe.drop_duplicates"},{"text":"pandas.core.groupby.DataFrameGroupBy.count   DataFrameGroupBy.count()[source]\n \nCompute count of group, excluding missing values.  Returns \n Series or DataFrame\n\nCount of values within each group.      See also  Series.groupby\n\nApply a function groupby to a Series.  DataFrame.groupby\n\nApply a function groupby to each row or column of a DataFrame.","title":"pandas.reference.api.pandas.core.groupby.dataframegroupby.count"},{"text":"pandas.core.groupby.GroupBy.__iter__   GroupBy.__iter__()[source]\n \nGroupby iterator.  Returns \n Generator yielding sequence of (name, subsetted object)\nfor each group","title":"pandas.reference.api.pandas.core.groupby.groupby.__iter__"},{"text":"pandas.core.groupby.DataFrameGroupBy.cumcount   DataFrameGroupBy.cumcount(ascending=True)[source]\n \nNumber each item in each group from 0 to the length of that group - 1. Essentially this is equivalent to \nself.apply(lambda x: pd.Series(np.arange(len(x)), x.index))\n   Parameters \n \nascending:bool, default True\n\n\nIf False, number in reverse, from length of group - 1 to 0.    Returns \n Series\n\nSequence number of each element within each group.      See also  ngroup\n\nNumber the groups themselves.    Examples \n>>> df = pd.DataFrame([['a'], ['a'], ['a'], ['b'], ['b'], ['a']],\n...                   columns=['A'])\n>>> df\n   A\n0  a\n1  a\n2  a\n3  b\n4  b\n5  a\n>>> df.groupby('A').cumcount()\n0    0\n1    1\n2    2\n3    0\n4    1\n5    3\ndtype: int64\n>>> df.groupby('A').cumcount(ascending=False)\n0    3\n1    2\n2    1\n3    1\n4    0\n5    0\ndtype: int64","title":"pandas.reference.api.pandas.core.groupby.dataframegroupby.cumcount"},{"text":"GroupBy GroupBy objects are returned by groupby calls: pandas.DataFrame.groupby(), pandas.Series.groupby(), etc.  Indexing, iteration       \nGroupBy.__iter__() Groupby iterator.  \nGroupBy.groups Dict {group name -> group labels}.  \nGroupBy.indices Dict {group name -> group indices}.  \nGroupBy.get_group(name[, obj]) Construct DataFrame from group with provided name.          \nGrouper(*args, **kwargs) A Grouper allows the user to specify a groupby instruction for an object.      Function application       \nGroupBy.apply(func, *args, **kwargs) Apply function func group-wise and combine the results together.  \nGroupBy.agg(func, *args, **kwargs)   \nSeriesGroupBy.aggregate([func, engine, ...]) Aggregate using one or more operations over the specified axis.  \nDataFrameGroupBy.aggregate([func, engine, ...]) Aggregate using one or more operations over the specified axis.  \nSeriesGroupBy.transform(func, *args[, ...]) Call function producing a like-indexed Series on each group and return a Series having the same indexes as the original object filled with the transformed values.  \nDataFrameGroupBy.transform(func, *args[, ...]) Call function producing a like-indexed DataFrame on each group and return a DataFrame having the same indexes as the original object filled with the transformed values.  \nGroupBy.pipe(func, *args, **kwargs) Apply a function func with arguments to this GroupBy object and return the function's result.      Computations \/ descriptive stats       \nGroupBy.all([skipna]) Return True if all values in the group are truthful, else False.  \nGroupBy.any([skipna]) Return True if any value in the group is truthful, else False.  \nGroupBy.bfill([limit]) Backward fill the values.  \nGroupBy.backfill([limit]) Backward fill the values.  \nGroupBy.count() Compute count of group, excluding missing values.  \nGroupBy.cumcount([ascending]) Number each item in each group from 0 to the length of that group - 1.  \nGroupBy.cummax([axis]) Cumulative max for each group.  \nGroupBy.cummin([axis]) Cumulative min for each group.  \nGroupBy.cumprod([axis]) Cumulative product for each group.  \nGroupBy.cumsum([axis]) Cumulative sum for each group.  \nGroupBy.ffill([limit]) Forward fill the values.  \nGroupBy.first([numeric_only, min_count]) Compute first of group values.  \nGroupBy.head([n]) Return first n rows of each group.  \nGroupBy.last([numeric_only, min_count]) Compute last of group values.  \nGroupBy.max([numeric_only, min_count]) Compute max of group values.  \nGroupBy.mean([numeric_only, engine, ...]) Compute mean of groups, excluding missing values.  \nGroupBy.median([numeric_only]) Compute median of groups, excluding missing values.  \nGroupBy.min([numeric_only, min_count]) Compute min of group values.  \nGroupBy.ngroup([ascending]) Number each group from 0 to the number of groups - 1.  \nGroupBy.nth(n[, dropna]) Take the nth row from each group if n is an int, otherwise a subset of rows.  \nGroupBy.ohlc() Compute open, high, low and close values of a group, excluding missing values.  \nGroupBy.pad([limit]) Forward fill the values.  \nGroupBy.prod([numeric_only, min_count]) Compute prod of group values.  \nGroupBy.rank([method, ascending, na_option, ...]) Provide the rank of values within each group.  \nGroupBy.pct_change([periods, fill_method, ...]) Calculate pct_change of each value to previous entry in group.  \nGroupBy.size() Compute group sizes.  \nGroupBy.sem([ddof]) Compute standard error of the mean of groups, excluding missing values.  \nGroupBy.std([ddof, engine, engine_kwargs]) Compute standard deviation of groups, excluding missing values.  \nGroupBy.sum([numeric_only, min_count, ...]) Compute sum of group values.  \nGroupBy.var([ddof, engine, engine_kwargs]) Compute variance of groups, excluding missing values.  \nGroupBy.tail([n]) Return last n rows of each group.    The following methods are available in both SeriesGroupBy and DataFrameGroupBy objects, but may differ slightly, usually in that the DataFrameGroupBy version usually permits the specification of an axis argument, and often an argument indicating whether to restrict application to columns of a specific data type.       \nDataFrameGroupBy.all([skipna]) Return True if all values in the group are truthful, else False.  \nDataFrameGroupBy.any([skipna]) Return True if any value in the group is truthful, else False.  \nDataFrameGroupBy.backfill([limit]) Backward fill the values.  \nDataFrameGroupBy.bfill([limit]) Backward fill the values.  \nDataFrameGroupBy.corr Compute pairwise correlation of columns, excluding NA\/null values.  \nDataFrameGroupBy.count() Compute count of group, excluding missing values.  \nDataFrameGroupBy.cov Compute pairwise covariance of columns, excluding NA\/null values.  \nDataFrameGroupBy.cumcount([ascending]) Number each item in each group from 0 to the length of that group - 1.  \nDataFrameGroupBy.cummax([axis]) Cumulative max for each group.  \nDataFrameGroupBy.cummin([axis]) Cumulative min for each group.  \nDataFrameGroupBy.cumprod([axis]) Cumulative product for each group.  \nDataFrameGroupBy.cumsum([axis]) Cumulative sum for each group.  \nDataFrameGroupBy.describe(**kwargs) Generate descriptive statistics.  \nDataFrameGroupBy.diff First discrete difference of element.  \nDataFrameGroupBy.ffill([limit]) Forward fill the values.  \nDataFrameGroupBy.fillna Fill NA\/NaN values using the specified method.  \nDataFrameGroupBy.filter(func[, dropna]) Return a copy of a DataFrame excluding filtered elements.  \nDataFrameGroupBy.hist Make a histogram of the DataFrame's columns.  \nDataFrameGroupBy.idxmax([axis, skipna]) Return index of first occurrence of maximum over requested axis.  \nDataFrameGroupBy.idxmin([axis, skipna]) Return index of first occurrence of minimum over requested axis.  \nDataFrameGroupBy.mad Return the mean absolute deviation of the values over the requested axis.  \nDataFrameGroupBy.nunique([dropna]) Return DataFrame with counts of unique elements in each position.  \nDataFrameGroupBy.pad([limit]) Forward fill the values.  \nDataFrameGroupBy.pct_change([periods, ...]) Calculate pct_change of each value to previous entry in group.  \nDataFrameGroupBy.plot Class implementing the .plot attribute for groupby objects.  \nDataFrameGroupBy.quantile([q, interpolation]) Return group values at the given quantile, a la numpy.percentile.  \nDataFrameGroupBy.rank([method, ascending, ...]) Provide the rank of values within each group.  \nDataFrameGroupBy.resample(rule, *args, **kwargs) Provide resampling when using a TimeGrouper.  \nDataFrameGroupBy.sample([n, frac, replace, ...]) Return a random sample of items from each group.  \nDataFrameGroupBy.shift([periods, freq, ...]) Shift each group by periods observations.  \nDataFrameGroupBy.size() Compute group sizes.  \nDataFrameGroupBy.skew Return unbiased skew over requested axis.  \nDataFrameGroupBy.take Return the elements in the given positional indices along an axis.  \nDataFrameGroupBy.tshift (DEPRECATED) Shift the time index, using the index's frequency if available.  \nDataFrameGroupBy.value_counts([subset, ...]) Return a Series or DataFrame containing counts of unique rows.    The following methods are available only for SeriesGroupBy objects.       \nSeriesGroupBy.hist Draw histogram of the input series using matplotlib.  \nSeriesGroupBy.nlargest([n, keep]) Return the largest n elements.  \nSeriesGroupBy.nsmallest([n, keep]) Return the smallest n elements.  \nSeriesGroupBy.nunique([dropna]) Return number of unique elements in the group.  \nSeriesGroupBy.unique Return unique values of Series object.  \nSeriesGroupBy.value_counts([normalize, ...])   \nSeriesGroupBy.is_monotonic_increasing Alias for is_monotonic.  \nSeriesGroupBy.is_monotonic_decreasing Return boolean if values in the object are monotonic_decreasing.    The following methods are available only for DataFrameGroupBy objects.       \nDataFrameGroupBy.corrwith Compute pairwise correlation.  \nDataFrameGroupBy.boxplot([subplots, column, ...]) Make box plots from DataFrameGroupBy data.","title":"pandas.reference.groupby"},{"text":"pandas.DataFrame.iteritems   DataFrame.iteritems()[source]\n \nIterate over (column name, Series) pairs. Iterates over the DataFrame columns, returning a tuple with the column name and the content as a Series.  Yields \n \nlabel:object\n\n\nThe column names for the DataFrame being iterated over.  \ncontent:Series\n\n\nThe column entries belonging to each label, as a Series.      See also  DataFrame.iterrows\n\nIterate over DataFrame rows as (index, Series) pairs.  DataFrame.itertuples\n\nIterate over DataFrame rows as namedtuples of the values.    Examples \n>>> df = pd.DataFrame({'species': ['bear', 'bear', 'marsupial'],\n...                   'population': [1864, 22000, 80000]},\n...                   index=['panda', 'polar', 'koala'])\n>>> df\n        species   population\npanda   bear      1864\npolar   bear      22000\nkoala   marsupial 80000\n>>> for label, content in df.items():\n...     print(f'label: {label}')\n...     print(f'content: {content}', sep='\\n')\n...\nlabel: species\ncontent:\npanda         bear\npolar         bear\nkoala    marsupial\nName: species, dtype: object\nlabel: population\ncontent:\npanda     1864\npolar    22000\nkoala    80000\nName: population, dtype: int64","title":"pandas.reference.api.pandas.dataframe.iteritems"}]}
{"task_id":3735814,"prompt":"def f_3735814(dict1):\n\treturn ","suffix":"","canonical_solution":"any(key.startswith('EMP$$') for key in dict1)","test_start":"\ndef check(candidate):","test":["\n    assert candidate({'EMP$$': 1, 'EMP$$112': 4}) == True\n","\n    assert candidate({'EMP$$': 1, 'EM$$112': 4}) == True\n","\n    assert candidate({'EMP$33': 0}) == False\n"],"entry_point":"f_3735814","intent":"Check if any key in the dictionary `dict1` starts with the string `EMP$$`","library":[],"docs":[]}
{"task_id":3735814,"prompt":"def f_3735814(dict1):\n\treturn ","suffix":"","canonical_solution":"[value for key, value in list(dict1.items()) if key.startswith('EMP$$')]","test_start":"\ndef check(candidate):","test":["\n    assert sorted(candidate({'EMP$$': 1, 'EMP$$112': 4})) == [1, 4]\n","\n    assert sorted(candidate({'EMP$$': 1, 'EM$$112': 4})) == [1]\n","\n    assert sorted(candidate({'EMP$33': 0})) == []\n"],"entry_point":"f_3735814","intent":"create list of values from dictionary `dict1` that have a key that starts with 'EMP$$'","library":[],"docs":[]}
{"task_id":26097916,"prompt":"def f_26097916(sf):\n\t","suffix":"\n\treturn df","canonical_solution":"df = pd.DataFrame({'email': sf.index, 'list': sf.values})","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    dict = {'email1': [1.0, 5.0, 7.0], 'email2': [4.2, 3.6, -0.9]}\n    sf = pd.Series(dict)\n    k = [['email1', [1.0, 5.0, 7.0]], ['email2', [4.2, 3.6, -0.9]]]\n    df1 = pd.DataFrame(k, columns=['email', 'list'])\n    df2 = candidate(sf)\n    assert pd.DataFrame.equals(df1, df2)\n"],"entry_point":"f_26097916","intent":"convert a pandas series `sf` into a pandas dataframe `df` with columns `email` and `list`","library":["pandas"],"docs":[]}
{"task_id":4048964,"prompt":"def f_4048964(list):\n\treturn ","suffix":"","canonical_solution":"'\\t'.join(map(str, list))","test_start":"\ndef check(candidate):","test":["\n    assert candidate(['hello', 'world', '!']) == 'hello\\tworld\\t!'\n","\n    assert candidate([]) == \"\"\n","\n    assert candidate([\"mconala\"]) == \"mconala\"\n","\n    assert candidate([\"MCoNaLa\"]) == \"MCoNaLa\"\n"],"entry_point":"f_4048964","intent":"concatenate elements of list `list` by tabs `\t`","library":[],"docs":[]}
{"task_id":3182716,"prompt":"def f_3182716():\n\treturn ","suffix":"","canonical_solution":"'\\xd0\\xbf\\xd1\\x80\\xd0\\xb8'.encode('raw_unicode_escape')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == b'\\xd0\\xbf\\xd1\\x80\\xd0\\xb8'\n"],"entry_point":"f_3182716","intent":"print unicode string '\\xd0\\xbf\\xd1\\x80\\xd0\\xb8' with utf-8","library":[],"docs":[]}
{"task_id":3182716,"prompt":"def f_3182716():\n\treturn ","suffix":"","canonical_solution":"'Sopet\\xc3\\xb3n'.encode('latin-1').decode('utf-8')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == \"Sopet\u00f3n\"\n"],"entry_point":"f_3182716","intent":"Encode a latin character in string `Sopet\\xc3\\xb3n` properly","library":[],"docs":[]}
{"task_id":35622945,"prompt":"def f_35622945(s):\n\treturn ","suffix":"","canonical_solution":"re.findall('n(?<=[^n]n)n+(?=[^n])(?i)', s)","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    assert candidate(\"ncnnnne\") == ['nnnn']\n","\n    assert candidate(\"nn\") == []\n","\n    assert candidate(\"ask\") == []\n"],"entry_point":"f_35622945","intent":"regex, find \"n\"s only in the middle of string `s`","library":["re"],"docs":[{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"pattern  \nThe regular expression pattern.","title":"python.library.re#re.error.pattern"},{"text":"re.S  \nre.DOTALL  \nMake the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).","title":"python.library.re#re.S"},{"text":"str.rindex(sub[, start[, end]])  \nLike rfind() but raises ValueError when the substring sub is not found.","title":"python.library.stdtypes#str.rindex"},{"text":"Pattern.search(string[, pos[, endpos]])  \nScan through string looking for the first location where this regular expression produces a match, and return a corresponding match object. Return None if no position in the string matches the pattern; note that this is different from finding a zero-length match at some point in the string. The optional second parameter pos gives an index in the string where the search is to start; it defaults to 0. This is not completely equivalent to slicing the string; the '^' pattern character matches at the real beginning of the string and at positions just after a newline, but not necessarily at the index where the search is to start. The optional parameter endpos limits how far the string will be searched; it will be as if the string is endpos characters long, so only the characters from pos to endpos - 1 will be searched for a match. If endpos is less than pos, no match will be found; otherwise, if rx is a compiled regular expression object, rx.search(string, 0, 50) is equivalent to rx.search(string[:50], 0). >>> pattern = re.compile(\"d\")\n>>> pattern.search(\"dog\")     # Match at index 0\n<re.Match object; span=(0, 1), match='d'>\n>>> pattern.search(\"dog\", 1)  # No match; search doesn't include the \"d\"","title":"python.library.re#re.Pattern.search"},{"text":"re.search(pattern, string, flags=0)  \nScan through string looking for the first location where the regular expression pattern produces a match, and return a corresponding match object. Return None if no position in the string matches the pattern; note that this is different from finding a zero-length match at some point in the string.","title":"python.library.re#re.search"},{"text":"re.S  \nre.DOTALL  \nMake the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).","title":"python.library.re#re.DOTALL"},{"text":"Pattern.match(string[, pos[, endpos]])  \nIf zero or more characters at the beginning of string match this regular expression, return a corresponding match object. Return None if the string does not match the pattern; note that this is different from a zero-length match. The optional pos and endpos parameters have the same meaning as for the search() method. >>> pattern = re.compile(\"o\")\n>>> pattern.match(\"dog\")      # No match as \"o\" is not at the start of \"dog\".\n>>> pattern.match(\"dog\", 1)   # Match as \"o\" is the 2nd character of \"dog\".\n<re.Match object; span=(1, 2), match='o'>\n If you want to locate a match anywhere in string, use search() instead (see also search() vs. match()).","title":"python.library.re#re.Pattern.match"},{"text":"re.split(pattern, string, maxsplit=0, flags=0)  \nSplit string by the occurrences of pattern. If capturing parentheses are used in pattern, then the text of all groups in the pattern are also returned as part of the resulting list. If maxsplit is nonzero, at most maxsplit splits occur, and the remainder of the string is returned as the final element of the list. >>> re.split(r'\\W+', 'Words, words, words.')\n['Words', 'words', 'words', '']\n>>> re.split(r'(\\W+)', 'Words, words, words.')\n['Words', ', ', 'words', ', ', 'words', '.', '']\n>>> re.split(r'\\W+', 'Words, words, words.', 1)\n['Words', 'words, words.']\n>>> re.split('[a-f]+', '0a3B9', flags=re.IGNORECASE)\n['0', '3', '9']\n If there are capturing groups in the separator and it matches at the start of the string, the result will start with an empty string. The same holds for the end of the string: >>> re.split(r'(\\W+)', '...words, words...')\n['', '...', 'words', ', ', 'words', '...', '']\n That way, separator components are always found at the same relative indices within the result list. Empty matches for the pattern split the string only when not adjacent to a previous empty match. >>> re.split(r'\\b', 'Words, words, words.')\n['', 'Words', ', ', 'words', ', ', 'words', '.']\n>>> re.split(r'\\W*', '...words...')\n['', '', 'w', 'o', 'r', 'd', 's', '', '']\n>>> re.split(r'(\\W*)', '...words...')\n['', '...', '', '', 'w', '', 'o', '', 'r', '', 'd', '', 's', '...', '', '', '']\n  Changed in version 3.1: Added the optional flags argument.   Changed in version 3.7: Added support of splitting on a pattern that could match an empty string.","title":"python.library.re#re.split"},{"text":"Pattern.findall(string[, pos[, endpos]])  \nSimilar to the findall() function, using the compiled pattern, but also accepts optional pos and endpos parameters that limit the search region like for search().","title":"python.library.re#re.Pattern.findall"}]}
{"task_id":5306756,"prompt":"def f_5306756():\n\treturn ","suffix":"","canonical_solution":"'{0:.0f}%'.format(1.0 \/ 3 * 100)","test_start":"\ndef check(candidate):","test":["\n    assert(candidate() == \"33%\")\n"],"entry_point":"f_5306756","intent":"display the float `1\/3*100` as a percentage","library":[],"docs":[]}
{"task_id":2878084,"prompt":"def f_2878084(mylist):\n\t","suffix":"\n\treturn mylist","canonical_solution":"mylist.sort(key=lambda x: x['title'])","test_start":"\ndef check(candidate):","test":["\n    input = [\n        {'title':'New York Times', 'title_url':'New_York_Times','id':4}, \n        {'title':'USA Today','title_url':'USA_Today','id':6}, \n        {'title':'Apple News','title_url':'Apple_News','id':2}\n    ]\n    res = [\n        {'title': 'Apple News', 'title_url': 'Apple_News', 'id': 2}, \n        {'title': 'New York Times', 'title_url': 'New_York_Times', 'id': 4},\n        {'title': 'USA Today', 'title_url': 'USA_Today', 'id': 6}\n    ]\n    assert candidate(input) == res\n"],"entry_point":"f_2878084","intent":"sort a list of dictionary `mylist` by the key `title`","library":[],"docs":[]}
{"task_id":2878084,"prompt":"def f_2878084(l):\n\t","suffix":"\n\treturn l","canonical_solution":"l.sort(key=lambda x: x['title'])","test_start":"\ndef check(candidate):","test":["\n    input = [\n        {'title':'New York Times', 'title_url':'New_York_Times','id':4}, \n        {'title':'USA Today','title_url':'USA_Today','id':6}, \n        {'title':'Apple News','title_url':'Apple_News','id':2}\n    ]\n    res = [\n        {'title': 'Apple News', 'title_url': 'Apple_News', 'id': 2}, \n        {'title': 'New York Times', 'title_url': 'New_York_Times', 'id': 4},\n        {'title': 'USA Today', 'title_url': 'USA_Today', 'id': 6}\n    ]\n    assert candidate(input) == res\n"],"entry_point":"f_2878084","intent":"sort a list `l` of dicts by dict value 'title'","library":[],"docs":[]}
{"task_id":2878084,"prompt":"def f_2878084(l):\n\t","suffix":"\n\treturn l","canonical_solution":"l.sort(key=lambda x: (x['title'], x['title_url'], x['id']))","test_start":"\ndef check(candidate):","test":["\n    input = [\n        {'title':'New York Times', 'title_url':'New_York_Times','id':4}, \n        {'title':'USA Today','title_url':'USA_Today','id':6}, \n        {'title':'Apple News','title_url':'Apple_News','id':2}\n    ]\n    res = [\n        {'title': 'Apple News', 'title_url': 'Apple_News', 'id': 2}, \n        {'title': 'New York Times', 'title_url': 'New_York_Times', 'id': 4},\n        {'title': 'USA Today', 'title_url': 'USA_Today', 'id': 6}\n    ]\n    assert candidate(input) == res\n"],"entry_point":"f_2878084","intent":"sort a list of dictionaries by the value of keys 'title', 'title_url', 'id' in ascending order.","library":[],"docs":[]}
{"task_id":9323159,"prompt":"def f_9323159(l1, l2):\n\treturn ","suffix":"","canonical_solution":"heapq.nlargest(10, range(len(l1)), key=lambda i: abs(l1[i] - l2[i]))","test_start":"\nimport heapq\n\ndef check(candidate):","test":["\n    l1 = [99, 86, 90, 70, 86, 95, 56, 98, 80, 81]\n    l2 = [21, 11, 21, 1, 26, 40, 4, 50, 34, 37]\n    res = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n    assert candidate(l1, l2) == res\n"],"entry_point":"f_9323159","intent":"find 10 largest differences between each respective elements of list `l1` and list `l2`","library":["heapq"],"docs":[{"text":"difference()","title":"django.ref.contrib.gis.gdal#django.contrib.gis.gdal.OGRGeometry.difference"},{"text":"max(x, y)  \nCompares two values numerically and returns the maximum.","title":"python.library.decimal#decimal.Context.max"},{"text":"difference(*others)  \nset - other - ...  \nReturn a new set with elements in the set that are not in the others.","title":"python.library.stdtypes#frozenset.difference"},{"text":"sym_difference()","title":"django.ref.contrib.gis.gdal#django.contrib.gis.gdal.OGRGeometry.sym_difference"},{"text":"next_minus(x)  \nReturns the largest representable number smaller than x.","title":"python.library.decimal#decimal.Context.next_minus"},{"text":"decimal.MAX_EMAX","title":"python.library.decimal#decimal.MAX_EMAX"},{"text":"dis.cmp_op  \nSequence of all compare operation names.","title":"python.library.dis#dis.cmp_op"},{"text":"find_longest_match(alo=0, ahi=None, blo=0, bhi=None)  \nFind longest matching block in a[alo:ahi] and b[blo:bhi]. If isjunk was omitted or None, find_longest_match() returns (i, j, k) such that a[i:i+k] is equal to b[j:j+k], where alo\n<= i <= i+k <= ahi and blo <= j <= j+k <= bhi. For all (i', j',\nk') meeting those conditions, the additional conditions k >= k', i\n<= i', and if i == i', j <= j' are also met. In other words, of all maximal matching blocks, return one that starts earliest in a, and of all those maximal matching blocks that start earliest in a, return the one that starts earliest in b. >>> s = SequenceMatcher(None, \" abcd\", \"abcd abcd\")\n>>> s.find_longest_match(0, 5, 0, 9)\nMatch(a=0, b=4, size=5)\n If isjunk was provided, first the longest matching block is determined as above, but with the additional restriction that no junk element appears in the block. Then that block is extended as far as possible by matching (only) junk elements on both sides. So the resulting block never matches on junk except as identical junk happens to be adjacent to an interesting match. Here\u2019s the same example as before, but considering blanks to be junk. That prevents ' abcd' from matching the ' abcd' at the tail end of the second sequence directly. Instead only the 'abcd' can match, and matches the leftmost 'abcd' in the second sequence: >>> s = SequenceMatcher(lambda x: x==\" \", \" abcd\", \"abcd abcd\")\n>>> s.find_longest_match(0, 5, 0, 9)\nMatch(a=1, b=0, size=4)\n If no blocks match, this returns (alo, blo, 0). This method returns a named tuple Match(a, b, size).  Changed in version 3.9: Added default arguments.","title":"python.library.difflib#difflib.SequenceMatcher.find_longest_match"},{"text":"pandas.tseries.offsets.LastWeekOfMonth.normalize   LastWeekOfMonth.normalize","title":"pandas.reference.api.pandas.tseries.offsets.lastweekofmonth.normalize"},{"text":"symmetric_difference_update(other)  \nset ^= other  \nUpdate the set, keeping only elements found in either set, but not in both.","title":"python.library.stdtypes#frozenset.symmetric_difference_update"}]}
{"task_id":29877663,"prompt":"def f_29877663(soup):\n\treturn ","suffix":"","canonical_solution":"soup.find_all('span', {'class': 'starGryB sp'})","test_start":"\nimport bs4\n\ndef check(candidate):","test":["\n    html = '''<span class=\"starBig sp\">4.1<\/span>\n             <span class=\"starGryB sp\">2.9<\/span>\n             <span class=\"sp starGryB\">2.9<\/span>\n             <span class=\"sp starBig\">22<\/span>'''\n    soup = bs4.BeautifulSoup(html, features=\"html5lib\")\n    res = '''[<span class=\"starGryB sp\">2.9<\/span>]'''\n    assert(str(candidate(soup)) == res)\n"],"entry_point":"f_29877663","intent":"BeautifulSoup find all 'span' elements in HTML string `soup` with class of 'starGryB sp'","library":["bs4"],"docs":[]}
{"task_id":24189150,"prompt":"def f_24189150(df, engine):\n\t","suffix":"\n\treturn ","canonical_solution":"df.to_sql('test', engine)","test_start":"\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\ndef check(candidate):","test":["\n    engine = create_engine('sqlite:\/\/', echo=False)\n    df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})\n    candidate(df, engine)\n    result = pd.read_sql('SELECT name FROM test', engine)\n    assert result.equals(df)\n"],"entry_point":"f_24189150","intent":"write records in dataframe `df` to table 'test' in schema 'a_schema' with `engine`","library":["pandas","sqlalchemy"],"docs":[{"text":"pandas.DataFrame.to_sql   DataFrame.to_sql(name, con, schema=None, if_exists='fail', index=True, index_label=None, chunksize=None, dtype=None, method=None)[source]\n \nWrite records stored in a DataFrame to a SQL database. Databases supported by SQLAlchemy [1] are supported. Tables can be newly created, appended to, or overwritten.  Parameters \n \nname:str\n\n\nName of SQL table.  \ncon:sqlalchemy.engine.(Engine or Connection) or sqlite3.Connection\n\n\nUsing SQLAlchemy makes it possible to use any DB supported by that library. Legacy support is provided for sqlite3.Connection objects. The user is responsible for engine disposal and connection closure for the SQLAlchemy connectable See here.  \nschema:str, optional\n\n\nSpecify the schema (if database flavor supports this). If None, use default schema.  \nif_exists:{\u2018fail\u2019, \u2018replace\u2019, \u2018append\u2019}, default \u2018fail\u2019\n\n\nHow to behave if the table already exists.  fail: Raise a ValueError. replace: Drop the table before inserting new values. append: Insert new values to the existing table.   \nindex:bool, default True\n\n\nWrite DataFrame index as a column. Uses index_label as the column name in the table.  \nindex_label:str or sequence, default None\n\n\nColumn label for index column(s). If None is given (default) and index is True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex.  \nchunksize:int, optional\n\n\nSpecify the number of rows in each batch to be written at a time. By default, all rows will be written at once.  \ndtype:dict or scalar, optional\n\n\nSpecifying the datatype for columns. If a dictionary is used, the keys should be the column names and the values should be the SQLAlchemy types or strings for the sqlite3 legacy mode. If a scalar is provided, it will be applied to all columns.  \nmethod:{None, \u2018multi\u2019, callable}, optional\n\n\nControls the SQL insertion clause used:  None : Uses standard SQL INSERT clause (one per row). \u2018multi\u2019: Pass multiple values in a single INSERT clause. callable with signature (pd_table, conn, keys, data_iter).  Details and a sample callable implementation can be found in the section insert method.    Returns \n None or int\n\nNumber of rows affected by to_sql. None is returned if the callable passed into method does not return the number of rows. The number of returned rows affected is the sum of the rowcount attribute of sqlite3.Cursor or SQLAlchemy connectable which may not reflect the exact number of written rows as stipulated in the sqlite3 or SQLAlchemy.  New in version 1.4.0.     Raises \n ValueError\n\nWhen the table already exists and if_exists is \u2018fail\u2019 (the default).      See also  read_sql\n\nRead a DataFrame from a table.    Notes Timezone aware datetime columns will be written as Timestamp with timezone type with SQLAlchemy if supported by the database. Otherwise, the datetimes will be stored as timezone unaware timestamps local to the original timezone. References  1 \nhttps:\/\/docs.sqlalchemy.org  2 \nhttps:\/\/www.python.org\/dev\/peps\/pep-0249\/   Examples Create an in-memory SQLite database. \n>>> from sqlalchemy import create_engine\n>>> engine = create_engine('sqlite:\/\/', echo=False)\n  Create a table from scratch with 3 rows. \n>>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})\n>>> df\n     name\n0  User 1\n1  User 2\n2  User 3\n  \n>>> df.to_sql('users', con=engine)\n3\n>>> engine.execute(\"SELECT * FROM users\").fetchall()\n[(0, 'User 1'), (1, 'User 2'), (2, 'User 3')]\n  An sqlalchemy.engine.Connection can also be passed to con: \n>>> with engine.begin() as connection:\n...     df1 = pd.DataFrame({'name' : ['User 4', 'User 5']})\n...     df1.to_sql('users', con=connection, if_exists='append')\n2\n  This is allowed to support operations that require that the same DBAPI connection is used for the entire operation. \n>>> df2 = pd.DataFrame({'name' : ['User 6', 'User 7']})\n>>> df2.to_sql('users', con=engine, if_exists='append')\n2\n>>> engine.execute(\"SELECT * FROM users\").fetchall()\n[(0, 'User 1'), (1, 'User 2'), (2, 'User 3'),\n (0, 'User 4'), (1, 'User 5'), (0, 'User 6'),\n (1, 'User 7')]\n  Overwrite the table with just df2. \n>>> df2.to_sql('users', con=engine, if_exists='replace',\n...            index_label='id')\n2\n>>> engine.execute(\"SELECT * FROM users\").fetchall()\n[(0, 'User 6'), (1, 'User 7')]\n  Specify the dtype (especially useful for integers with missing values). Notice that while pandas is forced to store the data as floating point, the database supports nullable integers. When fetching the data with Python, we get back integer scalars. \n>>> df = pd.DataFrame({\"A\": [1, None, 2]})\n>>> df\n     A\n0  1.0\n1  NaN\n2  2.0\n  \n>>> from sqlalchemy.types import Integer\n>>> df.to_sql('integers', con=engine, index=False,\n...           dtype={\"A\": Integer()})\n3\n  \n>>> engine.execute(\"SELECT * FROM integers\").fetchall()\n[(1,), (None,), (2,)]","title":"pandas.reference.api.pandas.dataframe.to_sql"},{"text":"BaseDatabaseSchemaEditor.execute(sql, params=())","title":"django.ref.schema-editor#django.db.backends.base.schema.BaseDatabaseSchemaEditor.execute"},{"text":"pandas.Series.to_sql   Series.to_sql(name, con, schema=None, if_exists='fail', index=True, index_label=None, chunksize=None, dtype=None, method=None)[source]\n \nWrite records stored in a DataFrame to a SQL database. Databases supported by SQLAlchemy [1] are supported. Tables can be newly created, appended to, or overwritten.  Parameters \n \nname:str\n\n\nName of SQL table.  \ncon:sqlalchemy.engine.(Engine or Connection) or sqlite3.Connection\n\n\nUsing SQLAlchemy makes it possible to use any DB supported by that library. Legacy support is provided for sqlite3.Connection objects. The user is responsible for engine disposal and connection closure for the SQLAlchemy connectable See here.  \nschema:str, optional\n\n\nSpecify the schema (if database flavor supports this). If None, use default schema.  \nif_exists:{\u2018fail\u2019, \u2018replace\u2019, \u2018append\u2019}, default \u2018fail\u2019\n\n\nHow to behave if the table already exists.  fail: Raise a ValueError. replace: Drop the table before inserting new values. append: Insert new values to the existing table.   \nindex:bool, default True\n\n\nWrite DataFrame index as a column. Uses index_label as the column name in the table.  \nindex_label:str or sequence, default None\n\n\nColumn label for index column(s). If None is given (default) and index is True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex.  \nchunksize:int, optional\n\n\nSpecify the number of rows in each batch to be written at a time. By default, all rows will be written at once.  \ndtype:dict or scalar, optional\n\n\nSpecifying the datatype for columns. If a dictionary is used, the keys should be the column names and the values should be the SQLAlchemy types or strings for the sqlite3 legacy mode. If a scalar is provided, it will be applied to all columns.  \nmethod:{None, \u2018multi\u2019, callable}, optional\n\n\nControls the SQL insertion clause used:  None : Uses standard SQL INSERT clause (one per row). \u2018multi\u2019: Pass multiple values in a single INSERT clause. callable with signature (pd_table, conn, keys, data_iter).  Details and a sample callable implementation can be found in the section insert method.    Returns \n None or int\n\nNumber of rows affected by to_sql. None is returned if the callable passed into method does not return the number of rows. The number of returned rows affected is the sum of the rowcount attribute of sqlite3.Cursor or SQLAlchemy connectable which may not reflect the exact number of written rows as stipulated in the sqlite3 or SQLAlchemy.  New in version 1.4.0.     Raises \n ValueError\n\nWhen the table already exists and if_exists is \u2018fail\u2019 (the default).      See also  read_sql\n\nRead a DataFrame from a table.    Notes Timezone aware datetime columns will be written as Timestamp with timezone type with SQLAlchemy if supported by the database. Otherwise, the datetimes will be stored as timezone unaware timestamps local to the original timezone. References  1 \nhttps:\/\/docs.sqlalchemy.org  2 \nhttps:\/\/www.python.org\/dev\/peps\/pep-0249\/   Examples Create an in-memory SQLite database. \n>>> from sqlalchemy import create_engine\n>>> engine = create_engine('sqlite:\/\/', echo=False)\n  Create a table from scratch with 3 rows. \n>>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})\n>>> df\n     name\n0  User 1\n1  User 2\n2  User 3\n  \n>>> df.to_sql('users', con=engine)\n3\n>>> engine.execute(\"SELECT * FROM users\").fetchall()\n[(0, 'User 1'), (1, 'User 2'), (2, 'User 3')]\n  An sqlalchemy.engine.Connection can also be passed to con: \n>>> with engine.begin() as connection:\n...     df1 = pd.DataFrame({'name' : ['User 4', 'User 5']})\n...     df1.to_sql('users', con=connection, if_exists='append')\n2\n  This is allowed to support operations that require that the same DBAPI connection is used for the entire operation. \n>>> df2 = pd.DataFrame({'name' : ['User 6', 'User 7']})\n>>> df2.to_sql('users', con=engine, if_exists='append')\n2\n>>> engine.execute(\"SELECT * FROM users\").fetchall()\n[(0, 'User 1'), (1, 'User 2'), (2, 'User 3'),\n (0, 'User 4'), (1, 'User 5'), (0, 'User 6'),\n (1, 'User 7')]\n  Overwrite the table with just df2. \n>>> df2.to_sql('users', con=engine, if_exists='replace',\n...            index_label='id')\n2\n>>> engine.execute(\"SELECT * FROM users\").fetchall()\n[(0, 'User 6'), (1, 'User 7')]\n  Specify the dtype (especially useful for integers with missing values). Notice that while pandas is forced to store the data as floating point, the database supports nullable integers. When fetching the data with Python, we get back integer scalars. \n>>> df = pd.DataFrame({\"A\": [1, None, 2]})\n>>> df\n     A\n0  1.0\n1  NaN\n2  2.0\n  \n>>> from sqlalchemy.types import Integer\n>>> df.to_sql('integers', con=engine, index=False,\n...           dtype={\"A\": Integer()})\n3\n  \n>>> engine.execute(\"SELECT * FROM integers\").fetchall()\n[(1,), (None,), (2,)]","title":"pandas.reference.api.pandas.series.to_sql"},{"text":"pandas.DataFrame.T   propertyDataFrame.T","title":"pandas.reference.api.pandas.dataframe.t"},{"text":"numpy.record.base attribute   record.base\n \nbase object","title":"numpy.reference.generated.numpy.record.base"},{"text":"SchemaEditor.connection","title":"django.ref.schema-editor#django.db.backends.base.schema.SchemaEditor.connection"},{"text":"pandas.DataFrame.to_records   DataFrame.to_records(index=True, column_dtypes=None, index_dtypes=None)[source]\n \nConvert DataFrame to a NumPy record array. Index will be included as the first field of the record array if requested.  Parameters \n \nindex:bool, default True\n\n\nInclude index in resulting record array, stored in \u2018index\u2019 field or using the index label, if set.  \ncolumn_dtypes:str, type, dict, default None\n\n\nIf a string or type, the data type to store all columns. If a dictionary, a mapping of column names and indices (zero-indexed) to specific data types.  \nindex_dtypes:str, type, dict, default None\n\n\nIf a string or type, the data type to store all index levels. If a dictionary, a mapping of index level names and indices (zero-indexed) to specific data types. This mapping is applied only if index=True.    Returns \n numpy.recarray\n\nNumPy ndarray with the DataFrame labels as fields and each row of the DataFrame as entries.      See also  DataFrame.from_records\n\nConvert structured or record ndarray to DataFrame.  numpy.recarray\n\nAn ndarray that allows field access using attributes, analogous to typed columns in a spreadsheet.    Examples \n>>> df = pd.DataFrame({'A': [1, 2], 'B': [0.5, 0.75]},\n...                   index=['a', 'b'])\n>>> df\n   A     B\na  1  0.50\nb  2  0.75\n>>> df.to_records()\nrec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n          dtype=[('index', 'O'), ('A', '<i8'), ('B', '<f8')])\n  If the DataFrame index has no label then the recarray field name is set to \u2018index\u2019. If the index has a label then this is used as the field name: \n>>> df.index = df.index.rename(\"I\")\n>>> df.to_records()\nrec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n          dtype=[('I', 'O'), ('A', '<i8'), ('B', '<f8')])\n  The index can be excluded from the record array: \n>>> df.to_records(index=False)\nrec.array([(1, 0.5 ), (2, 0.75)],\n          dtype=[('A', '<i8'), ('B', '<f8')])\n  Data types can be specified for the columns: \n>>> df.to_records(column_dtypes={\"A\": \"int32\"})\nrec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n          dtype=[('I', 'O'), ('A', '<i4'), ('B', '<f8')])\n  As well as for the index: \n>>> df.to_records(index_dtypes=\"<S2\")\nrec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)],\n          dtype=[('I', 'S2'), ('A', '<i8'), ('B', '<f8')])\n  \n>>> index_dtypes = f\"<S{df.index.str.len().max()}\"\n>>> df.to_records(index_dtypes=index_dtypes)\nrec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)],\n          dtype=[('I', 'S1'), ('A', '<i8'), ('B', '<f8')])","title":"pandas.reference.api.pandas.dataframe.to_records"},{"text":"class BaseDatabaseSchemaEditor","title":"django.ref.schema-editor#django.db.backends.base.schema.BaseDatabaseSchemaEditor"},{"text":"pandas.tseries.offsets.Nano.copy   Nano.copy()","title":"pandas.reference.api.pandas.tseries.offsets.nano.copy"},{"text":"property df","title":"torch.distributions#torch.distributions.chi2.Chi2.df"}]}
{"task_id":30766151,"prompt":"def f_30766151(s):\n\treturn ","suffix":"","canonical_solution":"re.sub('[^(){}[\\]]', '', s)","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate(\"(a(vdwvndw){}]\") == \"((){}]\"\n","\n    assert candidate(\"12345\") == \"\"\n"],"entry_point":"f_30766151","intent":"Extract brackets from string `s`","library":["re"],"docs":[{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"str.rindex(sub[, start[, end]])  \nLike rfind() but raises ValueError when the substring sub is not found.","title":"python.library.stdtypes#str.rindex"},{"text":"gettext.lngettext(singular, plural, n)","title":"python.library.gettext#gettext.lngettext"},{"text":"token.DOT  \nToken value for \".\".","title":"python.library.token#token.DOT"},{"text":"winreg.REG_SZ  \nA null-terminated string.","title":"python.library.winreg#winreg.REG_SZ"},{"text":"token.LSQB  \nToken value for \"[\".","title":"python.library.token#token.LSQB"},{"text":"numpy.char.chararray.index method   char.chararray.index(sub, start=0, end=None)[source]\n \nLike find, but raises ValueError when the substring is not found.  See also  char.index","title":"numpy.reference.generated.numpy.char.chararray.index"},{"text":"class ast.JoinedStr(values)  \nAn f-string, comprising a series of FormattedValue and Constant nodes. >>> print(ast.dump(ast.parse('f\"sin({a}) is {sin(a):.3}\"', mode='eval'), indent=4))\nExpression(\n    body=JoinedStr(\n        values=[\n            Constant(value='sin('),\n            FormattedValue(\n                value=Name(id='a', ctx=Load()),\n                conversion=-1),\n            Constant(value=') is '),\n            FormattedValue(\n                value=Call(\n                    func=Name(id='sin', ctx=Load()),\n                    args=[\n                        Name(id='a', ctx=Load())],\n                    keywords=[]),\n                conversion=-1,\n                format_spec=JoinedStr(\n                    values=[\n                        Constant(value='.3')]))]))","title":"python.library.ast#ast.JoinedStr"},{"text":"email.utils.unquote(str)  \nReturn a new string which is an unquoted version of str. If str ends and begins with double quotes, they are stripped off. Likewise if str ends and begins with angle brackets, they are stripped off.","title":"python.library.email.utils#email.utils.unquote"},{"text":"token.SLASH  \nToken value for \"\/\".","title":"python.library.token#token.SLASH"}]}
{"task_id":1143379,"prompt":"def f_1143379(L):\n\treturn ","suffix":"","canonical_solution":"list(dict((x[0], x) for x in L).values())","test_start":"\ndef check(candidate):","test":["\n    L = [['14', '65', 76], ['2', '5', 6], ['7', '12', 33], ['14', '22', 46]]\n    res = [['14', '22', 46], ['2', '5', 6], ['7', '12', 33]]\n    assert(candidate(L) == res)\n","\n    assert candidate([\"a\", \"aa\", \"abc\", \"bac\"]) == [\"abc\", \"bac\"]\n"],"entry_point":"f_1143379","intent":"remove duplicate elements from list 'L'","library":[],"docs":[]}
{"task_id":12330522,"prompt":"def f_12330522(file):\n\treturn ","suffix":"","canonical_solution":"[line.rstrip('\\n') for line in file]","test_start":"\ndef check(candidate):","test":["\n    res = ['1', '2', '3']\n    f = open(\"myfile.txt\", \"a\")\n    f.write(\"1\\n2\\n3\")\n    f.close()\n    f = open(\"myfile.txt\", \"r\")\n    assert candidate(f) == res\n"],"entry_point":"f_12330522","intent":"read a file `file` without newlines","library":[],"docs":[]}
{"task_id":364621,"prompt":"def f_364621(testlist):\n\treturn ","suffix":"","canonical_solution":"[i for (i, x) in enumerate(testlist) if (x == 1)]","test_start":"\ndef check(candidate):","test":["\n    testlist = [1,2,3,5,3,1,2,1,6]\n    assert candidate(testlist) == [0, 5, 7]\n","\n    testlist = [0, -1]\n    assert candidate(testlist) == []\n"],"entry_point":"f_364621","intent":"get the position of item 1 in `testlist`","library":[],"docs":[]}
{"task_id":364621,"prompt":"def f_364621(testlist):\n\treturn ","suffix":"","canonical_solution":"[i for (i, x) in enumerate(testlist) if (x == 1)]","test_start":"\ndef check(candidate):","test":["\n    testlist = [1,2,3,5,3,1,2,1,6]\n    assert candidate(testlist) == [0, 5, 7]\n","\n    testlist = [0, -1]\n    assert candidate(testlist) == []\n"],"entry_point":"f_364621","intent":"get the position of item 1 in `testlist`","library":[],"docs":[]}
{"task_id":364621,"prompt":"def f_364621(testlist, element):\n\treturn ","suffix":"","canonical_solution":"testlist.index(element)","test_start":"\ndef check(candidate):","test":["\n    testlist = [1,2,3,5,3,1,2,1,6]\n    assert candidate(testlist, 1) == 0\n","\n    testlist = [1,2,3,5,3,1,2,1,6]\n    try:\n        candidate(testlist, 14)\n    except:\n        assert True\n"],"entry_point":"f_364621","intent":"get the position of item `element` in list `testlist`","library":[],"docs":[]}
{"task_id":13145368,"prompt":"def f_13145368(lis):\n\treturn ","suffix":"","canonical_solution":"max(lis, key=lambda item: item[1])[0]","test_start":"\ndef check(candidate):","test":["\n    lis = [(101, 153), (255, 827), (361, 961)]\n    assert candidate(lis) == 361\n"],"entry_point":"f_13145368","intent":"find the first element of the tuple with the maximum second element in a list of tuples `lis`","library":[],"docs":[]}
{"task_id":13145368,"prompt":"def f_13145368(lis):\n\treturn ","suffix":"","canonical_solution":"max(lis, key=itemgetter(1))[0]","test_start":"\nfrom operator import itemgetter \n\ndef check(candidate):","test":["\n    lis = [(101, 153), (255, 827), (361, 961)]\n    assert candidate(lis) == 361\n"],"entry_point":"f_13145368","intent":"get the item at index 0 from the tuple that has maximum value at index 1 in list `lis`","library":["operator"],"docs":[{"text":"NodeList.item(i)  \nReturn the i\u2019th item from the sequence, if there is one, or None. The index i is not allowed to be less than zero or greater than or equal to the length of the sequence.","title":"python.library.xml.dom#xml.dom.NodeList.item"},{"text":"operator.getitem(a, b)  \noperator.__getitem__(a, b)  \nReturn the value of a at index b.","title":"python.library.operator#operator.__getitem__"},{"text":"operator.getitem(a, b)  \noperator.__getitem__(a, b)  \nReturn the value of a at index b.","title":"python.library.operator#operator.getitem"},{"text":"index(value)  \nReturns first index position of value. Raises ValueError if value is not present.","title":"python.library.multiprocessing.shared_memory#multiprocessing.shared_memory.ShareableList.index"},{"text":"step  \nThe value of the step parameter (or 1 if the parameter was not supplied)","title":"python.library.stdtypes#range.step"},{"text":"decimal.MAX_PREC","title":"python.library.decimal#decimal.MAX_PREC"},{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"array.index(x)  \nReturn the smallest i such that i is the index of the first occurrence of x in the array.","title":"python.library.array#array.array.index"},{"text":"str.rfind(sub[, start[, end]])  \nReturn the highest index in the string where substring sub is found, such that sub is contained within s[start:end]. Optional arguments start and end are interpreted as in slice notation. Return -1 on failure.","title":"python.library.stdtypes#str.rfind"},{"text":"decimal.MAX_EMAX","title":"python.library.decimal#decimal.MAX_EMAX"}]}
{"task_id":2689189,"prompt":"def f_2689189():\n\t","suffix":"\n\treturn ","canonical_solution":"time.sleep(1)","test_start":"\nimport time\n\ndef check(candidate):","test":["\n    t1 = time.time()\n    candidate()\n    t2 = time.time()\n    assert t2 - t1 > 1\n"],"entry_point":"f_2689189","intent":"Make a delay of 1 second","library":["time"],"docs":[{"text":"curses.delay_output(ms)  \nInsert an ms millisecond pause in output.","title":"python.library.curses#curses.delay_output"},{"text":"timeout  \nTimeout in seconds.","title":"python.library.subprocess#subprocess.TimeoutExpired.timeout"},{"text":"set_step()  \nStop after one line of code.","title":"python.library.bdb#bdb.Bdb.set_step"},{"text":"winsound.SND_ASYNC  \nReturn immediately, allowing sounds to play asynchronously.","title":"python.library.winsound#winsound.SND_ASYNC"},{"text":"pygame.time.delay() \n pause the program for an amount of time delay(milliseconds) -> time  Will pause for a given number of milliseconds. This function will use the processor (rather than sleeping) in order to make the delay more accurate than pygame.time.wait(). This returns the actual number of milliseconds used.","title":"pygame.ref.time#pygame.time.delay"},{"text":"time.microsecond  \nIn range(1000000).","title":"python.library.datetime#datetime.time.microsecond"},{"text":"oss_audio_device.post()  \nTell the driver that there is likely to be a pause in the output, making it possible for the device to handle the pause more intelligently. You might use this after playing a spot sound effect, before waiting for user input, or before doing disk I\/O.","title":"python.library.ossaudiodev#ossaudiodev.oss_audio_device.post"},{"text":"curses.beep()  \nEmit a short attention sound.","title":"python.library.curses#curses.beep"},{"text":"winsound.SND_NOSTOP  \nDo not interrupt sounds currently playing.","title":"python.library.winsound#winsound.SND_NOSTOP"},{"text":"window.nodelay(flag)  \nIf flag is True, getch() will be non-blocking.","title":"python.library.curses#curses.window.nodelay"}]}
{"task_id":12485244,"prompt":"def f_12485244(L):\n\treturn ","suffix":"","canonical_solution":"\"\"\", \"\"\".join('(' + ', '.join(i) + ')' for i in L)","test_start":"\ndef check(candidate):","test":["\n    L = [(\"abc\", \"def\"), (\"hij\", \"klm\")]\n    assert candidate(L) == '(abc, def), (hij, klm)'\n"],"entry_point":"f_12485244","intent":"convert list of tuples `L` to a string","library":[],"docs":[]}
{"task_id":755857,"prompt":"def f_755857():\n\t","suffix":"\n\treturn b","canonical_solution":"b = models.CharField(max_length=7, default='0000000', editable=False)","test_start":"\nfrom django.db import models\n\ndef check(candidate):","test":["\n    assert candidate().get_default() == '0000000'\n"],"entry_point":"f_755857","intent":"Django set default value of field `b` equal to '0000000'","library":["django"],"docs":[{"text":"max_value","title":"django.ref.forms.fields#django.forms.DecimalField.max_value"},{"text":"max_value","title":"django.ref.forms.fields#django.forms.IntegerField.max_value"},{"text":"Field.default","title":"django.ref.models.fields#django.db.models.Field.default"},{"text":"class DecimalField(max_digits=None, decimal_places=None, **options)","title":"django.ref.models.fields#django.db.models.DecimalField"},{"text":"class BigIntegerField(**options)","title":"django.ref.models.fields#django.db.models.BigIntegerField"},{"text":"as_int()","title":"django.ref.contrib.gis.gdal#django.contrib.gis.gdal.Field.as_int"},{"text":"SET_DEFAULT  \nSet the ForeignKey to its default value; a default for the ForeignKey must be set.","title":"django.ref.models.fields#django.db.models.SET_DEFAULT"},{"text":"class IntegerField(**options)","title":"django.ref.models.fields#django.db.models.IntegerField"},{"text":"class SmallIntegerField(**options)","title":"django.ref.models.fields#django.db.models.SmallIntegerField"},{"text":"class Value(value, output_field=None)","title":"django.ref.models.expressions#django.db.models.Value"}]}
{"task_id":16193578,"prompt":"def f_16193578(list5):\n\treturn ","suffix":"","canonical_solution":"sorted(list5, key = lambda x: (degrees(x), x))","test_start":"\nfrom math import degrees\n\ndef check(candidate):","test":["\n    list5 = [4, 1, 2, 3, 9, 5]\n    assert candidate(list5) == [1, 2, 3, 4, 5, 9]\n"],"entry_point":"f_16193578","intent":"Sort lis `list5` in ascending order based on the degrees value of its elements","library":["math"],"docs":[{"text":"zorder=5","title":"matplotlib.offsetbox_api#matplotlib.offsetbox.AnchoredOffsetbox.zorder"},{"text":"zorder=0","title":"matplotlib.collections_api#matplotlib.collections.TriMesh.zorder"},{"text":"zorder=0","title":"matplotlib.collections_api#matplotlib.collections.Collection.zorder"},{"text":"zorder=0","title":"matplotlib.collections_api#matplotlib.collections.BrokenBarHCollection.zorder"},{"text":"zorder=5","title":"matplotlib.legend_api#matplotlib.legend.Legend.zorder"},{"text":"zorder=0","title":"matplotlib.collections_api#matplotlib.collections.LineCollection.zorder"},{"text":"zorder=0","title":"matplotlib.collections_api#matplotlib.collections.RegularPolyCollection.zorder"},{"text":"zorder=0","title":"matplotlib.collections_api#matplotlib.collections.PathCollection.zorder"},{"text":"msort() \u2192 Tensor  \nSee torch.msort()","title":"torch.tensors#torch.Tensor.msort"},{"text":"zorder=0","title":"matplotlib.collections_api#matplotlib.collections.EventCollection.zorder"}]}
{"task_id":16041405,"prompt":"def f_16041405(l):\n\treturn ","suffix":"","canonical_solution":"(n for n in l)","test_start":"\ndef check(candidate):","test":["\n    generator = candidate([1,2,3,5])\n    assert str(type(generator)) == \"<class 'generator'>\"\n    assert [x for x in generator] == [1, 2, 3, 5]\n"],"entry_point":"f_16041405","intent":"convert a list `l` into a generator object","library":[],"docs":[]}
{"task_id":18837607,"prompt":"def f_18837607(oldlist, removelist):\n\treturn ","suffix":"","canonical_solution":"[v for i, v in enumerate(oldlist) if i not in removelist]","test_start":"\ndef check(candidate):","test":["\n    assert candidate([\"asdf\",\"ghjk\",\"qwer\",\"tyui\"], [1,3]) == ['asdf', 'qwer']\n","\n    assert candidate([1,2,3,4,5], [0,4]) == [2,3,4]\n"],"entry_point":"f_18837607","intent":"remove elements from list `oldlist` that have an index number mentioned in list `removelist`","library":[],"docs":[]}
{"task_id":4710067,"prompt":"def f_4710067():\n\treturn ","suffix":"","canonical_solution":"open('yourfile.txt', 'w')","test_start":"\ndef check(candidate):","test":["\n    fw = candidate()\n    assert fw.name == \"yourfile.txt\"\n    assert fw.mode == 'w'\n"],"entry_point":"f_4710067","intent":"Open a file `yourfile.txt` in write mode","library":[],"docs":[]}
{"task_id":7373219,"prompt":"def f_7373219(obj, attr):\n\treturn ","suffix":"","canonical_solution":"getattr(obj, attr)","test_start":"\ndef check(candidate):","test":["\n    class Student:\n        student_id = \"\"\n        student_name = \"\"\n\n        def __init__(self, student_id=101, student_name=\"Adam\"):\n            self.student_id = student_id\n            self.student_name = student_name\n\n    student = Student()\n\n    assert(candidate(student, 'student_name') == \"Adam\")\n    assert(candidate(student, 'student_id') == 101)\n","\n    class Student:\n        student_id = \"\"\n        student_name = \"\"\n\n        def __init__(self, student_id=101, student_name=\"Adam\"):\n            self.student_id = student_id\n            self.student_name = student_name\n\n    student = Student()\n\n    try:\n        value = candidate(student, 'student_none')\n    except: \n        assert True\n"],"entry_point":"f_7373219","intent":"get attribute 'attr' from object `obj`","library":[],"docs":[]}
{"task_id":8171751,"prompt":"def f_8171751():\n\treturn ","suffix":"","canonical_solution":"reduce(lambda a, b: a + b, (('aa',), ('bb',), ('cc',)))","test_start":"\nfrom functools import reduce\n\ndef check(candidate):","test":["\n    assert candidate() == ('aa', 'bb', 'cc')\n"],"entry_point":"f_8171751","intent":"convert tuple of tuples `(('aa',), ('bb',), ('cc',))` to tuple","library":["functools"],"docs":[{"text":"tuple","title":"django.ref.contrib.gis.gdal#django.contrib.gis.gdal.OGRGeometry.tuple"},{"text":"tuple","title":"django.ref.contrib.gis.gdal#django.contrib.gis.gdal.Envelope.tuple"},{"text":"somenamedtuple._replace(**kwargs)  \nReturn a new instance of the named tuple replacing specified fields with new values: >>> p = Point(x=11, y=22)\n>>> p._replace(x=33)\nPoint(x=33, y=22)\n\n>>> for partnum, record in inventory.items():\n...     inventory[partnum] = record._replace(price=newprices[partnum], timestamp=time.now())","title":"python.library.collections#collections.somenamedtuple._replace"},{"text":"token.COMMA  \nToken value for \",\".","title":"python.library.token#token.COMMA"},{"text":"numpy.polynomial.chebyshev.Chebyshev.domain attribute   polynomial.chebyshev.Chebyshev.domain = array([-1, 1])","title":"numpy.reference.generated.numpy.polynomial.chebyshev.chebyshev.domain"},{"text":"token.LPAR  \nToken value for \"(\".","title":"python.library.token#token.LPAR"},{"text":"get_opcodes()  \nReturn list of 5-tuples describing how to turn a into b. Each tuple is of the form (tag, i1, i2, j1, j2). The first tuple has i1 == j1 ==\n0, and remaining tuples have i1 equal to the i2 from the preceding tuple, and, likewise, j1 equal to the previous j2. The tag values are strings, with these meanings:   \nValue Meaning   \n'replace' a[i1:i2] should be replaced by b[j1:j2].  \n'delete' a[i1:i2] should be deleted. Note that j1 == j2 in this case.  \n'insert' b[j1:j2] should be inserted at a[i1:i1]. Note that i1 == i2 in this case.  \n'equal' a[i1:i2] == b[j1:j2] (the sub-sequences are equal).   For example: >>> a = \"qabxcd\"\n>>> b = \"abycdf\"\n>>> s = SequenceMatcher(None, a, b)\n>>> for tag, i1, i2, j1, j2 in s.get_opcodes():\n...     print('{:7}   a[{}:{}] --> b[{}:{}] {!r:>8} --> {!r}'.format(\n...         tag, i1, i2, j1, j2, a[i1:i2], b[j1:j2]))\ndelete    a[0:1] --> b[0:0]      'q' --> ''\nequal     a[1:3] --> b[0:2]     'ab' --> 'ab'\nreplace   a[3:4] --> b[2:3]      'x' --> 'y'\nequal     a[4:6] --> b[3:5]     'cd' --> 'cd'\ninsert    a[6:6] --> b[5:6]       '' --> 'f'","title":"python.library.difflib#difflib.SequenceMatcher.get_opcodes"},{"text":"numpy.polynomial.hermite.Hermite.domain attribute   polynomial.hermite.Hermite.domain = array([-1, 1])","title":"numpy.reference.generated.numpy.polynomial.hermite.hermite.domain"},{"text":"array.tolist()  \nConvert the array to an ordinary list with the same items.","title":"python.library.array#array.array.tolist"},{"text":"numpy.longcomplex[source]\n \nalias of numpy.clongdouble","title":"numpy.reference.arrays.scalars#numpy.longcomplex"}]}
{"task_id":8171751,"prompt":"def f_8171751():\n\treturn ","suffix":"","canonical_solution":"list(map(lambda a: a[0], (('aa',), ('bb',), ('cc',))))","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == ['aa', 'bb', 'cc']\n"],"entry_point":"f_8171751","intent":"convert tuple of tuples `(('aa',), ('bb',), ('cc',))` to list in one line","library":[],"docs":[]}
{"task_id":28986489,"prompt":"def f_28986489(df):\n\t","suffix":"\n\treturn df","canonical_solution":"df['range'].replace(',', '-', inplace=True)","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame({'range' : [\",\", \"(50,290)\", \",,,\"]})\n    res = pd.DataFrame({'range' : [\"-\", \"(50,290)\", \",,,\"]})\n    assert candidate(df).equals(res)\n"],"entry_point":"f_28986489","intent":"replace a characters in a column of a dataframe `df`","library":["pandas"],"docs":[{"text":"pandas.DataFrame.replace   DataFrame.replace(to_replace=None, value=NoDefault.no_default, inplace=False, limit=None, regex=False, method=NoDefault.no_default)[source]\n \nReplace values given in to_replace with value. Values of the DataFrame are replaced with other values dynamically. This differs from updating with .loc or .iloc, which require you to specify a location to update with some value.  Parameters \n \nto_replace:str, regex, list, dict, Series, int, float, or None\n\n\nHow to find the values that will be replaced.  \nnumeric, str or regex:  \n numeric: numeric values equal to to_replace will be replaced with value str: string exactly matching to_replace will be replaced with value regex: regexs matching to_replace will be replaced with value  \n  \nlist of str, regex, or numeric:  \n First, if to_replace and value are both lists, they must be the same length. Second, if regex=True then all of the strings in both lists will be interpreted as regexs otherwise they will match directly. This doesn\u2019t matter much for value since there are only a few possible substitution regexes you can use. str, regex and numeric rules apply as above.  \n  \ndict:  \n Dicts can be used to specify different replacement values for different existing values. For example, {'a': 'b', 'y': 'z'} replaces the value \u2018a\u2019 with \u2018b\u2019 and \u2018y\u2019 with \u2018z\u2019. To use a dict in this way the value parameter should be None. For a DataFrame a dict can specify that different values should be replaced in different columns. For example, {'a': 1, 'b': 'z'} looks for the value 1 in column \u2018a\u2019 and the value \u2018z\u2019 in column \u2018b\u2019 and replaces these values with whatever is specified in value. The value parameter should not be None in this case. You can treat this as a special case of passing two lists except that you are specifying the column to search in. For a DataFrame nested dictionaries, e.g., {'a': {'b': np.nan}}, are read as follows: look in column \u2018a\u2019 for the value \u2018b\u2019 and replace it with NaN. The value parameter should be None to use a nested dict in this way. You can nest regular expressions as well. Note that column names (the top-level dictionary keys in a nested dictionary) cannot be regular expressions.  \n  \nNone:  \n This means that the regex argument must be a string, compiled regular expression, or list, dict, ndarray or Series of such elements. If value is also None then this must be a nested dictionary or Series.  \n   See the examples section for examples of each of these.  \nvalue:scalar, dict, list, str, regex, default None\n\n\nValue to replace any values matching to_replace with. For a DataFrame a dict of values can be used to specify which value to use for each column (columns not in the dict will not be filled). Regular expressions, strings and lists or dicts of such objects are also allowed.  \ninplace:bool, default False\n\n\nIf True, performs operation inplace and returns None.  \nlimit:int, default None\n\n\nMaximum size gap to forward or backward fill.  \nregex:bool or same types as to_replace, default False\n\n\nWhether to interpret to_replace and\/or value as regular expressions. If this is True then to_replace must be a string. Alternatively, this could be a regular expression or a list, dict, or array of regular expressions in which case to_replace must be None.  \nmethod:{\u2018pad\u2019, \u2018ffill\u2019, \u2018bfill\u2019, None}\n\n\nThe method to use when for replacement, when to_replace is a scalar, list or tuple and value is None.  Changed in version 0.23.0: Added to DataFrame.     Returns \n DataFrame\n\nObject after replacement.    Raises \n AssertionError\n\n If regex is not a bool and to_replace is not None.   TypeError\n\n If to_replace is not a scalar, array-like, dict, or None If to_replace is a dict and value is not a list, dict, ndarray, or Series If to_replace is None and regex is not compilable into a regular expression or is a list, dict, ndarray, or Series. When replacing multiple bool or datetime64 objects and the arguments to to_replace does not match the type of the value being replaced   ValueError\n\n If a list or an ndarray is passed to to_replace and value but they are not the same length.       See also  DataFrame.fillna\n\nFill NA values.  DataFrame.where\n\nReplace values based on boolean condition.  Series.str.replace\n\nSimple string replacement.    Notes  Regex substitution is performed under the hood with re.sub. The rules for substitution for re.sub are the same. Regular expressions will only substitute on strings, meaning you cannot provide, for example, a regular expression matching floating point numbers and expect the columns in your frame that have a numeric dtype to be matched. However, if those floating point numbers are strings, then you can do this. This method has a lot of options. You are encouraged to experiment and play with this method to gain intuition about how it works. When dict is used as the to_replace value, it is like key(s) in the dict are the to_replace part and value(s) in the dict are the value parameter.  Examples Scalar `to_replace` and `value` \n>>> s = pd.Series([1, 2, 3, 4, 5])\n>>> s.replace(1, 5)\n0    5\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n  \n>>> df = pd.DataFrame({'A': [0, 1, 2, 3, 4],\n...                    'B': [5, 6, 7, 8, 9],\n...                    'C': ['a', 'b', 'c', 'd', 'e']})\n>>> df.replace(0, 5)\n    A  B  C\n0  5  5  a\n1  1  6  b\n2  2  7  c\n3  3  8  d\n4  4  9  e\n  List-like `to_replace` \n>>> df.replace([0, 1, 2, 3], 4)\n    A  B  C\n0  4  5  a\n1  4  6  b\n2  4  7  c\n3  4  8  d\n4  4  9  e\n  \n>>> df.replace([0, 1, 2, 3], [4, 3, 2, 1])\n    A  B  C\n0  4  5  a\n1  3  6  b\n2  2  7  c\n3  1  8  d\n4  4  9  e\n  \n>>> s.replace([1, 2], method='bfill')\n0    3\n1    3\n2    3\n3    4\n4    5\ndtype: int64\n  dict-like `to_replace` \n>>> df.replace({0: 10, 1: 100})\n        A  B  C\n0   10  5  a\n1  100  6  b\n2    2  7  c\n3    3  8  d\n4    4  9  e\n  \n>>> df.replace({'A': 0, 'B': 5}, 100)\n        A    B  C\n0  100  100  a\n1    1    6  b\n2    2    7  c\n3    3    8  d\n4    4    9  e\n  \n>>> df.replace({'A': {0: 100, 4: 400}})\n        A  B  C\n0  100  5  a\n1    1  6  b\n2    2  7  c\n3    3  8  d\n4  400  9  e\n  Regular expression `to_replace` \n>>> df = pd.DataFrame({'A': ['bat', 'foo', 'bait'],\n...                    'B': ['abc', 'bar', 'xyz']})\n>>> df.replace(to_replace=r'^ba.$', value='new', regex=True)\n        A    B\n0   new  abc\n1   foo  new\n2  bait  xyz\n  \n>>> df.replace({'A': r'^ba.$'}, {'A': 'new'}, regex=True)\n        A    B\n0   new  abc\n1   foo  bar\n2  bait  xyz\n  \n>>> df.replace(regex=r'^ba.$', value='new')\n        A    B\n0   new  abc\n1   foo  new\n2  bait  xyz\n  \n>>> df.replace(regex={r'^ba.$': 'new', 'foo': 'xyz'})\n        A    B\n0   new  abc\n1   xyz  new\n2  bait  xyz\n  \n>>> df.replace(regex=[r'^ba.$', 'foo'], value='new')\n        A    B\n0   new  abc\n1   new  new\n2  bait  xyz\n  Compare the behavior of s.replace({'a': None}) and s.replace('a', None) to understand the peculiarities of the to_replace parameter: \n>>> s = pd.Series([10, 'a', 'a', 'b', 'a'])\n  When one uses a dict as the to_replace value, it is like the value(s) in the dict are equal to the value parameter. s.replace({'a': None}) is equivalent to s.replace(to_replace={'a': None}, value=None, method=None): \n>>> s.replace({'a': None})\n0      10\n1    None\n2    None\n3       b\n4    None\ndtype: object\n  When value is not explicitly passed and to_replace is a scalar, list or tuple, replace uses the method parameter (default \u2018pad\u2019) to do the replacement. So this is why the \u2018a\u2019 values are being replaced by 10 in rows 1 and 2 and \u2018b\u2019 in row 4 in this case. \n>>> s.replace('a')\n0    10\n1    10\n2    10\n3     b\n4     b\ndtype: object\n  On the other hand, if None is explicitly passed for value, it will be respected: \n>>> s.replace('a', None)\n0      10\n1    None\n2    None\n3       b\n4    None\ndtype: object\n   \n Changed in version 1.4.0: Previously the explicit None was silently ignored.","title":"pandas.reference.api.pandas.dataframe.replace"},{"text":"pandas.Series.replace   Series.replace(to_replace=None, value=NoDefault.no_default, inplace=False, limit=None, regex=False, method=NoDefault.no_default)[source]\n \nReplace values given in to_replace with value. Values of the Series are replaced with other values dynamically. This differs from updating with .loc or .iloc, which require you to specify a location to update with some value.  Parameters \n \nto_replace:str, regex, list, dict, Series, int, float, or None\n\n\nHow to find the values that will be replaced.  \nnumeric, str or regex:  \n numeric: numeric values equal to to_replace will be replaced with value str: string exactly matching to_replace will be replaced with value regex: regexs matching to_replace will be replaced with value  \n  \nlist of str, regex, or numeric:  \n First, if to_replace and value are both lists, they must be the same length. Second, if regex=True then all of the strings in both lists will be interpreted as regexs otherwise they will match directly. This doesn\u2019t matter much for value since there are only a few possible substitution regexes you can use. str, regex and numeric rules apply as above.  \n  \ndict:  \n Dicts can be used to specify different replacement values for different existing values. For example, {'a': 'b', 'y': 'z'} replaces the value \u2018a\u2019 with \u2018b\u2019 and \u2018y\u2019 with \u2018z\u2019. To use a dict in this way the value parameter should be None. For a DataFrame a dict can specify that different values should be replaced in different columns. For example, {'a': 1, 'b': 'z'} looks for the value 1 in column \u2018a\u2019 and the value \u2018z\u2019 in column \u2018b\u2019 and replaces these values with whatever is specified in value. The value parameter should not be None in this case. You can treat this as a special case of passing two lists except that you are specifying the column to search in. For a DataFrame nested dictionaries, e.g., {'a': {'b': np.nan}}, are read as follows: look in column \u2018a\u2019 for the value \u2018b\u2019 and replace it with NaN. The value parameter should be None to use a nested dict in this way. You can nest regular expressions as well. Note that column names (the top-level dictionary keys in a nested dictionary) cannot be regular expressions.  \n  \nNone:  \n This means that the regex argument must be a string, compiled regular expression, or list, dict, ndarray or Series of such elements. If value is also None then this must be a nested dictionary or Series.  \n   See the examples section for examples of each of these.  \nvalue:scalar, dict, list, str, regex, default None\n\n\nValue to replace any values matching to_replace with. For a DataFrame a dict of values can be used to specify which value to use for each column (columns not in the dict will not be filled). Regular expressions, strings and lists or dicts of such objects are also allowed.  \ninplace:bool, default False\n\n\nIf True, performs operation inplace and returns None.  \nlimit:int, default None\n\n\nMaximum size gap to forward or backward fill.  \nregex:bool or same types as to_replace, default False\n\n\nWhether to interpret to_replace and\/or value as regular expressions. If this is True then to_replace must be a string. Alternatively, this could be a regular expression or a list, dict, or array of regular expressions in which case to_replace must be None.  \nmethod:{\u2018pad\u2019, \u2018ffill\u2019, \u2018bfill\u2019, None}\n\n\nThe method to use when for replacement, when to_replace is a scalar, list or tuple and value is None.  Changed in version 0.23.0: Added to DataFrame.     Returns \n Series\n\nObject after replacement.    Raises \n AssertionError\n\n If regex is not a bool and to_replace is not None.   TypeError\n\n If to_replace is not a scalar, array-like, dict, or None If to_replace is a dict and value is not a list, dict, ndarray, or Series If to_replace is None and regex is not compilable into a regular expression or is a list, dict, ndarray, or Series. When replacing multiple bool or datetime64 objects and the arguments to to_replace does not match the type of the value being replaced   ValueError\n\n If a list or an ndarray is passed to to_replace and value but they are not the same length.       See also  Series.fillna\n\nFill NA values.  Series.where\n\nReplace values based on boolean condition.  Series.str.replace\n\nSimple string replacement.    Notes  Regex substitution is performed under the hood with re.sub. The rules for substitution for re.sub are the same. Regular expressions will only substitute on strings, meaning you cannot provide, for example, a regular expression matching floating point numbers and expect the columns in your frame that have a numeric dtype to be matched. However, if those floating point numbers are strings, then you can do this. This method has a lot of options. You are encouraged to experiment and play with this method to gain intuition about how it works. When dict is used as the to_replace value, it is like key(s) in the dict are the to_replace part and value(s) in the dict are the value parameter.  Examples Scalar `to_replace` and `value` \n>>> s = pd.Series([1, 2, 3, 4, 5])\n>>> s.replace(1, 5)\n0    5\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n  \n>>> df = pd.DataFrame({'A': [0, 1, 2, 3, 4],\n...                    'B': [5, 6, 7, 8, 9],\n...                    'C': ['a', 'b', 'c', 'd', 'e']})\n>>> df.replace(0, 5)\n    A  B  C\n0  5  5  a\n1  1  6  b\n2  2  7  c\n3  3  8  d\n4  4  9  e\n  List-like `to_replace` \n>>> df.replace([0, 1, 2, 3], 4)\n    A  B  C\n0  4  5  a\n1  4  6  b\n2  4  7  c\n3  4  8  d\n4  4  9  e\n  \n>>> df.replace([0, 1, 2, 3], [4, 3, 2, 1])\n    A  B  C\n0  4  5  a\n1  3  6  b\n2  2  7  c\n3  1  8  d\n4  4  9  e\n  \n>>> s.replace([1, 2], method='bfill')\n0    3\n1    3\n2    3\n3    4\n4    5\ndtype: int64\n  dict-like `to_replace` \n>>> df.replace({0: 10, 1: 100})\n        A  B  C\n0   10  5  a\n1  100  6  b\n2    2  7  c\n3    3  8  d\n4    4  9  e\n  \n>>> df.replace({'A': 0, 'B': 5}, 100)\n        A    B  C\n0  100  100  a\n1    1    6  b\n2    2    7  c\n3    3    8  d\n4    4    9  e\n  \n>>> df.replace({'A': {0: 100, 4: 400}})\n        A  B  C\n0  100  5  a\n1    1  6  b\n2    2  7  c\n3    3  8  d\n4  400  9  e\n  Regular expression `to_replace` \n>>> df = pd.DataFrame({'A': ['bat', 'foo', 'bait'],\n...                    'B': ['abc', 'bar', 'xyz']})\n>>> df.replace(to_replace=r'^ba.$', value='new', regex=True)\n        A    B\n0   new  abc\n1   foo  new\n2  bait  xyz\n  \n>>> df.replace({'A': r'^ba.$'}, {'A': 'new'}, regex=True)\n        A    B\n0   new  abc\n1   foo  bar\n2  bait  xyz\n  \n>>> df.replace(regex=r'^ba.$', value='new')\n        A    B\n0   new  abc\n1   foo  new\n2  bait  xyz\n  \n>>> df.replace(regex={r'^ba.$': 'new', 'foo': 'xyz'})\n        A    B\n0   new  abc\n1   xyz  new\n2  bait  xyz\n  \n>>> df.replace(regex=[r'^ba.$', 'foo'], value='new')\n        A    B\n0   new  abc\n1   new  new\n2  bait  xyz\n  Compare the behavior of s.replace({'a': None}) and s.replace('a', None) to understand the peculiarities of the to_replace parameter: \n>>> s = pd.Series([10, 'a', 'a', 'b', 'a'])\n  When one uses a dict as the to_replace value, it is like the value(s) in the dict are equal to the value parameter. s.replace({'a': None}) is equivalent to s.replace(to_replace={'a': None}, value=None, method=None): \n>>> s.replace({'a': None})\n0      10\n1    None\n2    None\n3       b\n4    None\ndtype: object\n  When value is not explicitly passed and to_replace is a scalar, list or tuple, replace uses the method parameter (default \u2018pad\u2019) to do the replacement. So this is why the \u2018a\u2019 values are being replaced by 10 in rows 1 and 2 and \u2018b\u2019 in row 4 in this case. \n>>> s.replace('a')\n0    10\n1    10\n2    10\n3     b\n4     b\ndtype: object\n  On the other hand, if None is explicitly passed for value, it will be respected: \n>>> s.replace('a', None)\n0      10\n1    None\n2    None\n3       b\n4    None\ndtype: object\n   \n Changed in version 1.4.0: Previously the explicit None was silently ignored.","title":"pandas.reference.api.pandas.series.replace"},{"text":"property df","title":"torch.distributions#torch.distributions.chi2.Chi2.df"},{"text":"colno  \nThe column corresponding to pos (may be None).","title":"python.library.re#re.error.colno"},{"text":"encoding\n \nAlias for field number 3","title":"matplotlib.dviread#matplotlib.dviread.PsFont.encoding"},{"text":"colno  \nThe column corresponding to pos.","title":"python.library.json#json.JSONDecodeError.colno"},{"text":"class Replace(expression, text, replacement=Value(''), **extra)","title":"django.ref.models.database-functions#django.db.models.functions.Replace"},{"text":"window.delch([y, x])  \nDelete any character at (y, x).","title":"python.library.curses#curses.window.delch"},{"text":"numpy.char.chararray.replace method   char.chararray.replace(old, new, count=None)[source]\n \nFor each element in self, return a copy of the string with all occurrences of substring old replaced by new.  See also  char.replace","title":"numpy.reference.generated.numpy.char.chararray.replace"},{"text":"numpy.char.replace   char.replace(a, old, new, count=None)[source]\n \nFor each element in a, return a copy of the string with all occurrences of substring old replaced by new. Calls str.replace element-wise.  Parameters \n \naarray-like of str or unicode\n\n\nold, newstr or unicode\n\n\ncountint, optional\n\n\nIf the optional argument count is given, only the first count occurrences are replaced.    Returns \n \noutndarray\n\n\nOutput array of str or unicode, depending on input type      See also  str.replace","title":"numpy.reference.generated.numpy.char.replace"}]}
{"task_id":19339,"prompt":"def f_19339():\n\treturn ","suffix":"","canonical_solution":"zip(*[('a', 1), ('b', 2), ('c', 3), ('d', 4)])","test_start":"\ndef check(candidate):","test":["\n    assert [a for a in candidate()] == [('a', 'b', 'c', 'd'), (1, 2, 3, 4)]\n"],"entry_point":"f_19339","intent":"unzip the list `[('a', 1), ('b', 2), ('c', 3), ('d', 4)]`","library":[],"docs":[]}
{"task_id":19339,"prompt":"def f_19339(original):\n\treturn ","suffix":"","canonical_solution":"([a for (a, b) in original], [b for (a, b) in original])","test_start":"\ndef check(candidate):","test":["\n    original = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]\n    assert candidate(original) == (['a', 'b', 'c', 'd'], [1, 2, 3, 4])\n","\n    original2 = [([], 1), ([], 2), (5, 3), (6, 4)]\n    assert candidate(original2) == ([[], [], 5, 6], [1, 2, 3, 4])\n"],"entry_point":"f_19339","intent":"unzip list `original`","library":[],"docs":[]}
{"task_id":19339,"prompt":"def f_19339(original):\n\treturn ","suffix":"","canonical_solution":"((a for (a, b) in original), (b for (a, b) in original))","test_start":"\ndef check(candidate):","test":["\n    original = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]\n    result = candidate(original)\n    assert [a for gen in result for a in gen] == ['a','b','c','d',1,2,3,4]\n","\n    original2 = [([], 1), ([], 2), (5, 3), (6, 4)]\n    result2 = candidate(original2)\n    assert [a for gen in result2 for a in gen] == [[], [], 5, 6, 1, 2, 3, 4]\n"],"entry_point":"f_19339","intent":"unzip list `original` and return a generator","library":[],"docs":[]}
{"task_id":19339,"prompt":"def f_19339():\n\treturn ","suffix":"","canonical_solution":"zip(*[('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e',)])","test_start":"\ndef check(candidate):","test":["\n    assert list(candidate()) == [('a', 'b', 'c', 'd', 'e')]\n"],"entry_point":"f_19339","intent":"unzip list `[('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e', )]`","library":[],"docs":[]}
{"task_id":19339,"prompt":"def f_19339():\n\treturn ","suffix":"","canonical_solution":"list(zip_longest(('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e',)))","test_start":"\nfrom itertools import zip_longest\n\ndef check(candidate):","test":["\n    assert(candidate() == [('a', 'b', 'c', 'd', 'e'), (1, 2, 3, 4, None)])\n"],"entry_point":"f_19339","intent":"unzip list `[('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e', )]` and fill empty results with None","library":["itertools"],"docs":[{"text":"ZipInfo.reserved  \nMust be zero.","title":"python.library.zipfile#zipfile.ZipInfo.reserved"},{"text":"ZipInfo.CRC  \nCRC-32 of the uncompressed file.","title":"python.library.zipfile#zipfile.ZipInfo.CRC"},{"text":"ZipFile.namelist()  \nReturn a list of archive members by name.","title":"python.library.zipfile#zipfile.ZipFile.namelist"},{"text":"ZipInfo.extract_version  \nPKZIP version needed to extract archive.","title":"python.library.zipfile#zipfile.ZipInfo.extract_version"},{"text":"numpy.distutils.misc_util.as_list(seq)[source]","title":"numpy.reference.distutils.misc_util#numpy.distutils.misc_util.as_list"},{"text":"numpy.recarray.dumps method   recarray.dumps()\n \nReturns the pickle of the array as a string. pickle.loads will convert the string back to an array.  Parameters \n None","title":"numpy.reference.generated.numpy.recarray.dumps"},{"text":"pandas.Index.empty   propertyIndex.empty","title":"pandas.reference.api.pandas.index.empty"},{"text":"matplotlib.artist.Artist.zorder   Artist.zorder=0","title":"matplotlib._as_gen.matplotlib.artist.artist.zorder"},{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"str.rindex(sub[, start[, end]])  \nLike rfind() but raises ValueError when the substring sub is not found.","title":"python.library.stdtypes#str.rindex"}]}
{"task_id":1960516,"prompt":"def f_1960516():\n\treturn ","suffix":"","canonical_solution":"json.dumps('3.9')","test_start":"\nimport json\n\ndef check(candidate):","test":["\n    data = candidate()\n    assert json.loads(data) == '3.9'\n"],"entry_point":"f_1960516","intent":"encode `Decimal('3.9')` to a JSON string","library":["json"],"docs":[{"text":"lineno  \nThe line corresponding to pos.","title":"python.library.json#json.JSONDecodeError.lineno"},{"text":"decimal.MAX_PREC","title":"python.library.decimal#decimal.MAX_PREC"},{"text":"colno  \nThe column corresponding to pos.","title":"python.library.json#json.JSONDecodeError.colno"},{"text":"decimal.MIN_ETINY","title":"python.library.decimal#decimal.MIN_ETINY"},{"text":"radix()  \nJust returns 10, as this is Decimal, :)","title":"python.library.decimal#decimal.Context.radix"},{"text":"decimal.MIN_EMIN","title":"python.library.decimal#decimal.MIN_EMIN"},{"text":"msg  \nThe unformatted error message.","title":"python.library.json#json.JSONDecodeError.msg"},{"text":"string.digits  \nThe string '0123456789'.","title":"python.library.string#string.digits"},{"text":"to_sci_string(x)  \nConverts a number to a string using scientific notation.","title":"python.library.decimal#decimal.Context.to_sci_string"},{"text":"doc  \nThe JSON document being parsed.","title":"python.library.json#json.JSONDecodeError.doc"}]}
{"task_id":1024847,"prompt":"def f_1024847(d):\n\t","suffix":"\n\treturn d","canonical_solution":"d['mynewkey'] = 'mynewvalue'","test_start":"\ndef check(candidate):","test":["\n    assert candidate({'key': 'value'}) == {'key': 'value', 'mynewkey': 'mynewvalue'}\n"],"entry_point":"f_1024847","intent":"Add key \"mynewkey\" to dictionary `d` with value \"mynewvalue\"","library":[],"docs":[]}
{"task_id":1024847,"prompt":"def f_1024847(data):\n\t","suffix":"\n\treturn data","canonical_solution":"data.update({'a': 1, })","test_start":"\ndef check(candidate):","test":["\n    assert candidate({'key': 'value'}) == {'key': 'value', 'a': 1}\n","\n    assert candidate({'key': 'value', 'a' : 2}) == {'key': 'value', 'a': 1}\n"],"entry_point":"f_1024847","intent":"Add key 'a' to dictionary `data` with value 1","library":[],"docs":[]}
{"task_id":1024847,"prompt":"def f_1024847(data):\n\t","suffix":"\n\treturn data","canonical_solution":"data.update(dict(a=1))","test_start":"\ndef check(candidate):","test":["\n    assert candidate({'key': 'value'}) == {'key': 'value', 'a': 1}\n","\n    assert candidate({'key': 'value', 'a' : 2}) == {'key': 'value', 'a': 1}\n"],"entry_point":"f_1024847","intent":"Add key 'a' to dictionary `data` with value 1","library":[],"docs":[]}
{"task_id":1024847,"prompt":"def f_1024847(data):\n\t","suffix":"\n\treturn data","canonical_solution":"data.update(a=1)","test_start":"\ndef check(candidate):","test":["\n    assert candidate({'key': 'value'}) == {'key': 'value', 'a': 1}\n","\n    assert candidate({'key': 'value', 'a' : 2}) == {'key': 'value', 'a': 1}\n"],"entry_point":"f_1024847","intent":"Add key 'a' to dictionary `data` with value 1","library":[],"docs":[]}
{"task_id":35837346,"prompt":"def f_35837346(matrix):\n\treturn ","suffix":"","canonical_solution":"max([max(i) for i in matrix])","test_start":"\ndef check(candidate):","test":["\n    assert candidate([[1,2,3],[4,5,6],[7,8,9]]) == 9\n","\n    assert candidate([[1.3,2.8],[4.2,10],[7.9,8.1,5]]) == 10\n"],"entry_point":"f_35837346","intent":"find maximal value in matrix `matrix`","library":[],"docs":[]}
{"task_id":20457038,"prompt":"def f_20457038(answer):\n\t","suffix":"\n\treturn answer","canonical_solution":"answer = str(round(answer, 2))","test_start":"\ndef check(candidate):","test":["\n    assert candidate(2.34351) == \"2.34\"\n","\n    assert candidate(99.375) == \"99.38\"\n","\n    assert candidate(4.1) == \"4.1\"\n","\n    assert candidate(3) == \"3\"\n"],"entry_point":"f_20457038","intent":"Round number `answer` to 2 precision after the decimal point","library":[],"docs":[]}
{"task_id":2890896,"prompt":"def f_2890896(s):\n\t","suffix":"\n\treturn ip","canonical_solution":"ip = re.findall('[0-9]+(?:\\\\.[0-9]+){3}', s)","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate(\"<html><head><title>Current IP Check<\/title><\/head><body>Current IP Address: 165.91.15.131<\/body><\/html>\") == [\"165.91.15.131\"]\n","\n    assert candidate(\"<html><head><title>Current IP Check<\/title><\/head><body>Current IP Address: 165.91.15.131 and this is not a IP Address: 165.91.15<\/body><\/html>\") == [\"165.91.15.131\"]\n","\n    assert candidate(\"<html><head><title>Current IP Check<\/title><\/head><body>Current IP Address: 192.168.1.1 & this is another IP address: 192.168.1.2<\/body><\/html>\") == [\"192.168.1.1\", \"192.168.1.2\"]\n"],"entry_point":"f_2890896","intent":"extract ip address `ip` from an html string `s`","library":["re"],"docs":[{"text":"compressed","title":"python.library.ipaddress#ipaddress.IPv4Address.compressed"},{"text":"compressed","title":"python.library.ipaddress#ipaddress.IPv6Address.compressed"},{"text":"ip","title":"python.library.ipaddress#ipaddress.IPv6Interface.ip"},{"text":"exploded","title":"python.library.ipaddress#ipaddress.IPv6Address.exploded"},{"text":"compressed","title":"python.library.ipaddress#ipaddress.IPv4Network.compressed"},{"text":"with_prefixlen","title":"python.library.ipaddress#ipaddress.IPv4Network.with_prefixlen"},{"text":"compressed","title":"python.library.ipaddress#ipaddress.IPv6Network.compressed"},{"text":"with_prefixlen","title":"python.library.ipaddress#ipaddress.IPv6Network.with_prefixlen"},{"text":"prefixlen","title":"python.library.ipaddress#ipaddress.IPv6Network.prefixlen"},{"text":"network_address","title":"python.library.ipaddress#ipaddress.IPv6Network.network_address"}]}
{"task_id":29836836,"prompt":"def f_29836836(df):\n\treturn ","suffix":"","canonical_solution":"df.groupby('A').filter(lambda x: len(x) > 1)","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    assert candidate(pd.DataFrame([[1, 2], [1, 4], [5, 6]], columns=['A', 'B'])).equals(pd.DataFrame([[1, 2], [1, 4]], columns=['A', 'B'])) is True\n","\n    assert candidate(pd.DataFrame([[1, 2], [1, 4], [1, 6]], columns=['A', 'B'])).equals(pd.DataFrame([[1, 2], [1, 4], [1, 6]], columns=['A', 'B'])) is True\n"],"entry_point":"f_29836836","intent":"filter dataframe `df` by values in column `A` that appear more than once","library":["pandas"],"docs":[{"text":"remove(value)  \nRemove the first occurrence of value. If not found, raises a ValueError.","title":"python.library.collections#collections.deque.remove"},{"text":"pandas.MultiIndex.codes   propertyMultiIndex.codes","title":"pandas.reference.api.pandas.multiindex.codes"},{"text":"pandas.Timedelta.is_populated   Timedelta.is_populated","title":"pandas.reference.api.pandas.timedelta.is_populated"},{"text":"pandas.DataFrame.duplicated   DataFrame.duplicated(subset=None, keep='first')[source]\n \nReturn boolean Series denoting duplicate rows. Considering certain columns is optional.  Parameters \n \nsubset:column label or sequence of labels, optional\n\n\nOnly consider certain columns for identifying duplicates, by default use all of the columns.  \nkeep:{\u2018first\u2019, \u2018last\u2019, False}, default \u2018first\u2019\n\n\nDetermines which duplicates (if any) to mark.  first : Mark duplicates as True except for the first occurrence. last : Mark duplicates as True except for the last occurrence. False : Mark all duplicates as True.     Returns \n Series\n\nBoolean series for each duplicated rows.      See also  Index.duplicated\n\nEquivalent method on index.  Series.duplicated\n\nEquivalent method on Series.  Series.drop_duplicates\n\nRemove duplicate values from Series.  DataFrame.drop_duplicates\n\nRemove duplicate values from DataFrame.    Examples Consider dataset containing ramen rating. \n>>> df = pd.DataFrame({\n...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n...     'rating': [4, 4, 3.5, 15, 5]\n... })\n>>> df\n    brand style  rating\n0  Yum Yum   cup     4.0\n1  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n3  Indomie  pack    15.0\n4  Indomie  pack     5.0\n  By default, for each set of duplicated values, the first occurrence is set on False and all others on True. \n>>> df.duplicated()\n0    False\n1     True\n2    False\n3    False\n4    False\ndtype: bool\n  By using \u2018last\u2019, the last occurrence of each set of duplicated values is set on False and all others on True. \n>>> df.duplicated(keep='last')\n0     True\n1    False\n2    False\n3    False\n4    False\ndtype: bool\n  By setting keep on False, all duplicates are True. \n>>> df.duplicated(keep=False)\n0     True\n1     True\n2    False\n3    False\n4    False\ndtype: bool\n  To find duplicates on specific column(s), use subset. \n>>> df.duplicated(subset=['brand'])\n0    False\n1     True\n2    False\n3     True\n4     True\ndtype: bool","title":"pandas.reference.api.pandas.dataframe.duplicated"},{"text":"pandas.Timestamp.second   Timestamp.second","title":"pandas.reference.api.pandas.timestamp.second"},{"text":"pandas.DataFrame.drop_duplicates   DataFrame.drop_duplicates(subset=None, keep='first', inplace=False, ignore_index=False)[source]\n \nReturn DataFrame with duplicate rows removed. Considering certain columns is optional. Indexes, including time indexes are ignored.  Parameters \n \nsubset:column label or sequence of labels, optional\n\n\nOnly consider certain columns for identifying duplicates, by default use all of the columns.  \nkeep:{\u2018first\u2019, \u2018last\u2019, False}, default \u2018first\u2019\n\n\nDetermines which duplicates (if any) to keep. - first : Drop duplicates except for the first occurrence. - last : Drop duplicates except for the last occurrence. - False : Drop all duplicates.  \ninplace:bool, default False\n\n\nWhether to drop duplicates in place or to return a copy.  \nignore_index:bool, default False\n\n\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.  New in version 1.0.0.     Returns \n DataFrame or None\n\nDataFrame with duplicates removed or None if inplace=True.      See also  DataFrame.value_counts\n\nCount unique combinations of columns.    Examples Consider dataset containing ramen rating. \n>>> df = pd.DataFrame({\n...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n...     'rating': [4, 4, 3.5, 15, 5]\n... })\n>>> df\n    brand style  rating\n0  Yum Yum   cup     4.0\n1  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n3  Indomie  pack    15.0\n4  Indomie  pack     5.0\n  By default, it removes duplicate rows based on all columns. \n>>> df.drop_duplicates()\n    brand style  rating\n0  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n3  Indomie  pack    15.0\n4  Indomie  pack     5.0\n  To remove duplicates on specific column(s), use subset. \n>>> df.drop_duplicates(subset=['brand'])\n    brand style  rating\n0  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n  To remove duplicates and keep last occurrences, use keep. \n>>> df.drop_duplicates(subset=['brand', 'style'], keep='last')\n    brand style  rating\n1  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n4  Indomie  pack     5.0","title":"pandas.reference.api.pandas.dataframe.drop_duplicates"},{"text":"pandas.Timedelta.freq   Timedelta.freq","title":"pandas.reference.api.pandas.timedelta.freq"},{"text":"filter(*args, **kwargs)","title":"django.ref.models.querysets#django.db.models.query.QuerySet.filter"},{"text":"pandas.IntervalIndex.mid   IntervalIndex.mid","title":"pandas.reference.api.pandas.intervalindex.mid"},{"text":"pandas.Timestamp.freq   Timestamp.freq","title":"pandas.reference.api.pandas.timestamp.freq"}]}
{"task_id":2545397,"prompt":"def f_2545397(myfile):\n\treturn ","suffix":"","canonical_solution":"[x for x in myfile if x != '']","test_start":"\ndef check(candidate):","test":["\n    with open('.\/tmp.txt', 'w') as fw: \n        for s in [\"hello\", \"world\", \"!!!\"]:\n            fw.write(f\"{s}\\n\")\n\n    with open('.\/tmp.txt', 'r') as myfile:\n        lines = candidate(myfile)\n        assert isinstance(lines, list)\n        assert len(lines) == 3\n        assert lines[0].strip() == \"hello\"\n"],"entry_point":"f_2545397","intent":"append each line in file `myfile` into a list","library":[],"docs":[]}
{"task_id":2545397,"prompt":"def f_2545397():\n\t","suffix":"\n\treturn lst","canonical_solution":"lst = list(map(int, open('filename.txt').readlines()))","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    with open('.\/filename.txt', 'w') as fw: \n        for s in [\"1\", \"2\", \"100\"]:\n            fw.write(f\"{s}\\n\")\n\n    assert candidate() == [1, 2, 100]\n"],"entry_point":"f_2545397","intent":"Get a list of integers `lst` from a file `filename.txt`","library":["pandas"],"docs":[]}
{"task_id":35420052,"prompt":"def f_35420052(plt, mappable, ax3):\n\t","suffix":"\n\treturn plt","canonical_solution":"plt.colorbar(mappable=mappable, cax=ax3)","test_start":"\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom obspy.core.trace import Trace\nfrom obspy.imaging.spectrogram import spectrogram\n\ndef check(candidate):","test":["\n    spl1 = Trace(data=np.arange(0, 10))\n    fig = plt.figure()\n    ax1 = fig.add_axes([0.1, 0.75, 0.7, 0.2]) #[left bottom width height]\n    ax2 = fig.add_axes([0.1, 0.1, 0.7, 0.60], sharex=ax1)\n    ax3 = fig.add_axes([0.83, 0.1, 0.03, 0.6])\n\n    #make time vector\n    t = np.arange(spl1.stats.npts) \/ spl1.stats.sampling_rate\n\n    #plot waveform (top subfigure)    \n    ax1.plot(t, spl1.data, 'k')\n\n    #plot spectrogram (bottom subfigure)\n    spl2 = spl1\n    fig = spl2.spectrogram(show=False, axes=ax2, wlen=10)\n    mappable = ax2.images[0]\n    candidate(plt, mappable, ax3)\n    \n    im=ax2.images\n    assert im[-1].colorbar is not None\n"],"entry_point":"f_35420052","intent":"add color bar with image `mappable` to plot `plt`","library":["matplotlib","numpy","obspy"],"docs":[{"text":"n_rasterize=50","title":"matplotlib.colorbar_api#matplotlib.colorbar.Colorbar.n_rasterize"},{"text":"prop","title":"matplotlib.type1font#matplotlib.type1font.Type1Font.prop"},{"text":"colorbar(mappable, *, ticks=None, **kwargs)[source]","title":"matplotlib._as_gen.mpl_toolkits.axes_grid1.axes_grid.cbaraxesbase#mpl_toolkits.axes_grid1.axes_grid.CbarAxesBase.colorbar"},{"text":"matplotlib.pyplot.colorbar   matplotlib.pyplot.colorbar(mappable=None, cax=None, ax=None, **kw)[source]\n \nAdd a colorbar to a plot.  Parameters \n mappable\n\nThe matplotlib.cm.ScalarMappable (i.e., AxesImage, ContourSet, etc.) described by this colorbar. This argument is mandatory for the Figure.colorbar method but optional for the pyplot.colorbar function, which sets the default to the current image. Note that one can create a ScalarMappable \"on-the-fly\" to generate colorbars not attached to a previously drawn artist, e.g. fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax)\n  \ncaxAxes, optional\n\n\nAxes into which the colorbar will be drawn.  \naxAxes, list of Axes, optional\n\n\nOne or more parent axes from which space for a new colorbar axes will be stolen, if cax is None. This has no effect if cax is set.  \nuse_gridspecbool, optional\n\n\nIf cax is None, a new cax is created as an instance of Axes. If ax is an instance of Subplot and use_gridspec is True, cax is created as an instance of Subplot using the gridspec module.    Returns \n \ncolorbarColorbar\n\n   Notes Additional keyword arguments are of two kinds: axes properties:  locationNone or {'left', 'right', 'top', 'bottom'}\n\n\nThe location, relative to the parent axes, where the colorbar axes is created. It also determines the orientation of the colorbar (colorbars on the left and right are vertical, colorbars at the top and bottom are horizontal). If None, the location will come from the orientation if it is set (vertical colorbars on the right, horizontal ones at the bottom), or default to 'right' if orientation is unset.  orientationNone or {'vertical', 'horizontal'}\n\n\nThe orientation of the colorbar. It is preferable to set the location of the colorbar, as that also determines the orientation; passing incompatible values for location and orientation raises an exception.  fractionfloat, default: 0.15\n\n\nFraction of original axes to use for colorbar.  shrinkfloat, default: 1.0\n\n\nFraction by which to multiply the size of the colorbar.  aspectfloat, default: 20\n\n\nRatio of long to short dimensions.  padfloat, default: 0.05 if vertical, 0.15 if horizontal\n\n\nFraction of original axes between colorbar and new image axes.  anchor(float, float), optional\n\n\nThe anchor point of the colorbar axes. Defaults to (0.0, 0.5) if vertical; (0.5, 1.0) if horizontal.  panchor(float, float), or False, optional\n\n\nThe anchor point of the colorbar parent axes. If False, the parent axes' anchor will be unchanged. Defaults to (1.0, 0.5) if vertical; (0.5, 0.0) if horizontal.   colorbar properties:   \nProperty Description   \nextend {'neither', 'both', 'min', 'max'} If not 'neither', make pointed end(s) for out-of- range values. These are set for a given colormap using the colormap set_under and set_over methods.  \nextendfrac {None, 'auto', length, lengths} If set to None, both the minimum and maximum triangular colorbar extensions with have a length of 5% of the interior colorbar length (this is the default setting). If set to 'auto', makes the triangular colorbar extensions the same lengths as the interior boxes (when spacing is set to 'uniform') or the same lengths as the respective adjacent interior boxes (when spacing is set to 'proportional'). If a scalar, indicates the length of both the minimum and maximum triangular colorbar extensions as a fraction of the interior colorbar length. A two-element sequence of fractions may also be given, indicating the lengths of the minimum and maximum colorbar extensions respectively as a fraction of the interior colorbar length.  \nextendrect bool If False the minimum and maximum colorbar extensions will be triangular (the default). If True the extensions will be rectangular.  \nspacing {'uniform', 'proportional'} Uniform spacing gives each discrete color the same space; proportional makes the space proportional to the data interval.  \nticks None or list of ticks or Locator If None, ticks are determined automatically from the input.  \nformat None or str or Formatter If None, ScalarFormatter is used. If a format string is given, e.g., '%.3f', that is used. An alternative Formatter may be given instead.  \ndrawedges bool Whether to draw lines at color boundaries.  \nlabel str The label on the colorbar's long axis.   The following will probably be useful only in the context of indexed colors (that is, when the mappable has norm=NoNorm()), or other unusual circumstances.   \nProperty Description   \nboundaries None or a sequence  \nvalues None or a sequence which must be of length 1 less than the sequence of boundaries. For each region delimited by adjacent entries in boundaries, the colormapped to the corresponding value in values will be used.   If mappable is a ContourSet, its extend kwarg is included automatically. The shrink kwarg provides a simple way to scale the colorbar with respect to the axes. Note that if cax is specified, it determines the size of the colorbar and shrink and aspect kwargs are ignored. For more precise control, you can manually specify the positions of the axes objects in which the mappable and the colorbar are drawn. In this case, do not use any of the axes properties kwargs. It is known that some vector graphics viewers (svg and pdf) renders white gaps between segments of the colorbar. This is due to bugs in the viewers, not Matplotlib. As a workaround, the colorbar can be rendered with overlapping segments: cbar = colorbar()\ncbar.solids.set_edgecolor(\"face\")\ndraw()\n However this has negative consequences in other circumstances, e.g. with semi-transparent images (alpha < 1) and colorbar extensions; therefore, this workaround is not used by default (see issue #1188). \n  Examples using matplotlib.pyplot.colorbar\n \n   Subplots spacings and margins   \n\n   Ellipse Collection   \n\n   Axes Divider   \n\n   Simple Colorbar   \n\n   Image tutorial   \n\n   Tight Layout guide","title":"matplotlib._as_gen.matplotlib.pyplot.colorbar"},{"text":"colorbar(mappable, cax=None, ax=None, use_gridspec=True, **kw)[source]\n \nAdd a colorbar to a plot.  Parameters \n mappable\n\nThe matplotlib.cm.ScalarMappable (i.e., AxesImage, ContourSet, etc.) described by this colorbar. This argument is mandatory for the Figure.colorbar method but optional for the pyplot.colorbar function, which sets the default to the current image. Note that one can create a ScalarMappable \"on-the-fly\" to generate colorbars not attached to a previously drawn artist, e.g. fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax)\n  \ncaxAxes, optional\n\n\nAxes into which the colorbar will be drawn.  \naxAxes, list of Axes, optional\n\n\nOne or more parent axes from which space for a new colorbar axes will be stolen, if cax is None. This has no effect if cax is set.  \nuse_gridspecbool, optional\n\n\nIf cax is None, a new cax is created as an instance of Axes. If ax is an instance of Subplot and use_gridspec is True, cax is created as an instance of Subplot using the gridspec module.    Returns \n \ncolorbarColorbar\n\n   Notes Additional keyword arguments are of two kinds: axes properties:  locationNone or {'left', 'right', 'top', 'bottom'}\n\n\nThe location, relative to the parent axes, where the colorbar axes is created. It also determines the orientation of the colorbar (colorbars on the left and right are vertical, colorbars at the top and bottom are horizontal). If None, the location will come from the orientation if it is set (vertical colorbars on the right, horizontal ones at the bottom), or default to 'right' if orientation is unset.  orientationNone or {'vertical', 'horizontal'}\n\n\nThe orientation of the colorbar. It is preferable to set the location of the colorbar, as that also determines the orientation; passing incompatible values for location and orientation raises an exception.  fractionfloat, default: 0.15\n\n\nFraction of original axes to use for colorbar.  shrinkfloat, default: 1.0\n\n\nFraction by which to multiply the size of the colorbar.  aspectfloat, default: 20\n\n\nRatio of long to short dimensions.  padfloat, default: 0.05 if vertical, 0.15 if horizontal\n\n\nFraction of original axes between colorbar and new image axes.  anchor(float, float), optional\n\n\nThe anchor point of the colorbar axes. Defaults to (0.0, 0.5) if vertical; (0.5, 1.0) if horizontal.  panchor(float, float), or False, optional\n\n\nThe anchor point of the colorbar parent axes. If False, the parent axes' anchor will be unchanged. Defaults to (1.0, 0.5) if vertical; (0.5, 0.0) if horizontal.   colorbar properties:   \nProperty Description   \nextend {'neither', 'both', 'min', 'max'} If not 'neither', make pointed end(s) for out-of- range values. These are set for a given colormap using the colormap set_under and set_over methods.  \nextendfrac {None, 'auto', length, lengths} If set to None, both the minimum and maximum triangular colorbar extensions with have a length of 5% of the interior colorbar length (this is the default setting). If set to 'auto', makes the triangular colorbar extensions the same lengths as the interior boxes (when spacing is set to 'uniform') or the same lengths as the respective adjacent interior boxes (when spacing is set to 'proportional'). If a scalar, indicates the length of both the minimum and maximum triangular colorbar extensions as a fraction of the interior colorbar length. A two-element sequence of fractions may also be given, indicating the lengths of the minimum and maximum colorbar extensions respectively as a fraction of the interior colorbar length.  \nextendrect bool If False the minimum and maximum colorbar extensions will be triangular (the default). If True the extensions will be rectangular.  \nspacing {'uniform', 'proportional'} Uniform spacing gives each discrete color the same space; proportional makes the space proportional to the data interval.  \nticks None or list of ticks or Locator If None, ticks are determined automatically from the input.  \nformat None or str or Formatter If None, ScalarFormatter is used. If a format string is given, e.g., '%.3f', that is used. An alternative Formatter may be given instead.  \ndrawedges bool Whether to draw lines at color boundaries.  \nlabel str The label on the colorbar's long axis.   The following will probably be useful only in the context of indexed colors (that is, when the mappable has norm=NoNorm()), or other unusual circumstances.   \nProperty Description   \nboundaries None or a sequence  \nvalues None or a sequence which must be of length 1 less than the sequence of boundaries. For each region delimited by adjacent entries in boundaries, the colormapped to the corresponding value in values will be used.   If mappable is a ContourSet, its extend kwarg is included automatically. The shrink kwarg provides a simple way to scale the colorbar with respect to the axes. Note that if cax is specified, it determines the size of the colorbar and shrink and aspect kwargs are ignored. For more precise control, you can manually specify the positions of the axes objects in which the mappable and the colorbar are drawn. In this case, do not use any of the axes properties kwargs. It is known that some vector graphics viewers (svg and pdf) renders white gaps between segments of the colorbar. This is due to bugs in the viewers, not Matplotlib. As a workaround, the colorbar can be rendered with overlapping segments: cbar = colorbar()\ncbar.solids.set_edgecolor(\"face\")\ndraw()\n However this has negative consequences in other circumstances, e.g. with semi-transparent images (alpha < 1) and colorbar extensions; therefore, this workaround is not used by default (see issue #1188).","title":"matplotlib.figure_api#matplotlib.figure.FigureBase.colorbar"},{"text":"colorbar(mappable, cax=None, ax=None, use_gridspec=True, **kw)[source]\n \nAdd a colorbar to a plot.  Parameters \n mappable\n\nThe matplotlib.cm.ScalarMappable (i.e., AxesImage, ContourSet, etc.) described by this colorbar. This argument is mandatory for the Figure.colorbar method but optional for the pyplot.colorbar function, which sets the default to the current image. Note that one can create a ScalarMappable \"on-the-fly\" to generate colorbars not attached to a previously drawn artist, e.g. fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax)\n  \ncaxAxes, optional\n\n\nAxes into which the colorbar will be drawn.  \naxAxes, list of Axes, optional\n\n\nOne or more parent axes from which space for a new colorbar axes will be stolen, if cax is None. This has no effect if cax is set.  \nuse_gridspecbool, optional\n\n\nIf cax is None, a new cax is created as an instance of Axes. If ax is an instance of Subplot and use_gridspec is True, cax is created as an instance of Subplot using the gridspec module.    Returns \n \ncolorbarColorbar\n\n   Notes Additional keyword arguments are of two kinds: axes properties:  locationNone or {'left', 'right', 'top', 'bottom'}\n\n\nThe location, relative to the parent axes, where the colorbar axes is created. It also determines the orientation of the colorbar (colorbars on the left and right are vertical, colorbars at the top and bottom are horizontal). If None, the location will come from the orientation if it is set (vertical colorbars on the right, horizontal ones at the bottom), or default to 'right' if orientation is unset.  orientationNone or {'vertical', 'horizontal'}\n\n\nThe orientation of the colorbar. It is preferable to set the location of the colorbar, as that also determines the orientation; passing incompatible values for location and orientation raises an exception.  fractionfloat, default: 0.15\n\n\nFraction of original axes to use for colorbar.  shrinkfloat, default: 1.0\n\n\nFraction by which to multiply the size of the colorbar.  aspectfloat, default: 20\n\n\nRatio of long to short dimensions.  padfloat, default: 0.05 if vertical, 0.15 if horizontal\n\n\nFraction of original axes between colorbar and new image axes.  anchor(float, float), optional\n\n\nThe anchor point of the colorbar axes. Defaults to (0.0, 0.5) if vertical; (0.5, 1.0) if horizontal.  panchor(float, float), or False, optional\n\n\nThe anchor point of the colorbar parent axes. If False, the parent axes' anchor will be unchanged. Defaults to (1.0, 0.5) if vertical; (0.5, 0.0) if horizontal.   colorbar properties:   \nProperty Description   \nextend {'neither', 'both', 'min', 'max'} If not 'neither', make pointed end(s) for out-of- range values. These are set for a given colormap using the colormap set_under and set_over methods.  \nextendfrac {None, 'auto', length, lengths} If set to None, both the minimum and maximum triangular colorbar extensions with have a length of 5% of the interior colorbar length (this is the default setting). If set to 'auto', makes the triangular colorbar extensions the same lengths as the interior boxes (when spacing is set to 'uniform') or the same lengths as the respective adjacent interior boxes (when spacing is set to 'proportional'). If a scalar, indicates the length of both the minimum and maximum triangular colorbar extensions as a fraction of the interior colorbar length. A two-element sequence of fractions may also be given, indicating the lengths of the minimum and maximum colorbar extensions respectively as a fraction of the interior colorbar length.  \nextendrect bool If False the minimum and maximum colorbar extensions will be triangular (the default). If True the extensions will be rectangular.  \nspacing {'uniform', 'proportional'} Uniform spacing gives each discrete color the same space; proportional makes the space proportional to the data interval.  \nticks None or list of ticks or Locator If None, ticks are determined automatically from the input.  \nformat None or str or Formatter If None, ScalarFormatter is used. If a format string is given, e.g., '%.3f', that is used. An alternative Formatter may be given instead.  \ndrawedges bool Whether to draw lines at color boundaries.  \nlabel str The label on the colorbar's long axis.   The following will probably be useful only in the context of indexed colors (that is, when the mappable has norm=NoNorm()), or other unusual circumstances.   \nProperty Description   \nboundaries None or a sequence  \nvalues None or a sequence which must be of length 1 less than the sequence of boundaries. For each region delimited by adjacent entries in boundaries, the colormapped to the corresponding value in values will be used.   If mappable is a ContourSet, its extend kwarg is included automatically. The shrink kwarg provides a simple way to scale the colorbar with respect to the axes. Note that if cax is specified, it determines the size of the colorbar and shrink and aspect kwargs are ignored. For more precise control, you can manually specify the positions of the axes objects in which the mappable and the colorbar are drawn. In this case, do not use any of the axes properties kwargs. It is known that some vector graphics viewers (svg and pdf) renders white gaps between segments of the colorbar. This is due to bugs in the viewers, not Matplotlib. As a workaround, the colorbar can be rendered with overlapping segments: cbar = colorbar()\ncbar.solids.set_edgecolor(\"face\")\ndraw()\n However this has negative consequences in other circumstances, e.g. with semi-transparent images (alpha < 1) and colorbar extensions; therefore, this workaround is not used by default (see issue #1188).","title":"matplotlib.figure_api#matplotlib.figure.Figure.colorbar"},{"text":"colorbar(mappable, cax=None, ax=None, use_gridspec=True, **kw)[source]\n \nAdd a colorbar to a plot.  Parameters \n mappable\n\nThe matplotlib.cm.ScalarMappable (i.e., AxesImage, ContourSet, etc.) described by this colorbar. This argument is mandatory for the Figure.colorbar method but optional for the pyplot.colorbar function, which sets the default to the current image. Note that one can create a ScalarMappable \"on-the-fly\" to generate colorbars not attached to a previously drawn artist, e.g. fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax)\n  \ncaxAxes, optional\n\n\nAxes into which the colorbar will be drawn.  \naxAxes, list of Axes, optional\n\n\nOne or more parent axes from which space for a new colorbar axes will be stolen, if cax is None. This has no effect if cax is set.  \nuse_gridspecbool, optional\n\n\nIf cax is None, a new cax is created as an instance of Axes. If ax is an instance of Subplot and use_gridspec is True, cax is created as an instance of Subplot using the gridspec module.    Returns \n \ncolorbarColorbar\n\n   Notes Additional keyword arguments are of two kinds: axes properties:  locationNone or {'left', 'right', 'top', 'bottom'}\n\n\nThe location, relative to the parent axes, where the colorbar axes is created. It also determines the orientation of the colorbar (colorbars on the left and right are vertical, colorbars at the top and bottom are horizontal). If None, the location will come from the orientation if it is set (vertical colorbars on the right, horizontal ones at the bottom), or default to 'right' if orientation is unset.  orientationNone or {'vertical', 'horizontal'}\n\n\nThe orientation of the colorbar. It is preferable to set the location of the colorbar, as that also determines the orientation; passing incompatible values for location and orientation raises an exception.  fractionfloat, default: 0.15\n\n\nFraction of original axes to use for colorbar.  shrinkfloat, default: 1.0\n\n\nFraction by which to multiply the size of the colorbar.  aspectfloat, default: 20\n\n\nRatio of long to short dimensions.  padfloat, default: 0.05 if vertical, 0.15 if horizontal\n\n\nFraction of original axes between colorbar and new image axes.  anchor(float, float), optional\n\n\nThe anchor point of the colorbar axes. Defaults to (0.0, 0.5) if vertical; (0.5, 1.0) if horizontal.  panchor(float, float), or False, optional\n\n\nThe anchor point of the colorbar parent axes. If False, the parent axes' anchor will be unchanged. Defaults to (1.0, 0.5) if vertical; (0.5, 0.0) if horizontal.   colorbar properties:   \nProperty Description   \nextend {'neither', 'both', 'min', 'max'} If not 'neither', make pointed end(s) for out-of- range values. These are set for a given colormap using the colormap set_under and set_over methods.  \nextendfrac {None, 'auto', length, lengths} If set to None, both the minimum and maximum triangular colorbar extensions with have a length of 5% of the interior colorbar length (this is the default setting). If set to 'auto', makes the triangular colorbar extensions the same lengths as the interior boxes (when spacing is set to 'uniform') or the same lengths as the respective adjacent interior boxes (when spacing is set to 'proportional'). If a scalar, indicates the length of both the minimum and maximum triangular colorbar extensions as a fraction of the interior colorbar length. A two-element sequence of fractions may also be given, indicating the lengths of the minimum and maximum colorbar extensions respectively as a fraction of the interior colorbar length.  \nextendrect bool If False the minimum and maximum colorbar extensions will be triangular (the default). If True the extensions will be rectangular.  \nspacing {'uniform', 'proportional'} Uniform spacing gives each discrete color the same space; proportional makes the space proportional to the data interval.  \nticks None or list of ticks or Locator If None, ticks are determined automatically from the input.  \nformat None or str or Formatter If None, ScalarFormatter is used. If a format string is given, e.g., '%.3f', that is used. An alternative Formatter may be given instead.  \ndrawedges bool Whether to draw lines at color boundaries.  \nlabel str The label on the colorbar's long axis.   The following will probably be useful only in the context of indexed colors (that is, when the mappable has norm=NoNorm()), or other unusual circumstances.   \nProperty Description   \nboundaries None or a sequence  \nvalues None or a sequence which must be of length 1 less than the sequence of boundaries. For each region delimited by adjacent entries in boundaries, the colormapped to the corresponding value in values will be used.   If mappable is a ContourSet, its extend kwarg is included automatically. The shrink kwarg provides a simple way to scale the colorbar with respect to the axes. Note that if cax is specified, it determines the size of the colorbar and shrink and aspect kwargs are ignored. For more precise control, you can manually specify the positions of the axes objects in which the mappable and the colorbar are drawn. In this case, do not use any of the axes properties kwargs. It is known that some vector graphics viewers (svg and pdf) renders white gaps between segments of the colorbar. This is due to bugs in the viewers, not Matplotlib. As a workaround, the colorbar can be rendered with overlapping segments: cbar = colorbar()\ncbar.solids.set_edgecolor(\"face\")\ndraw()\n However this has negative consequences in other circumstances, e.g. with semi-transparent images (alpha < 1) and colorbar extensions; therefore, this workaround is not used by default (see issue #1188).","title":"matplotlib.figure_api#matplotlib.figure.SubFigure.colorbar"},{"text":"classmatplotlib.colorbar.Colorbar(ax, mappable=None, *, cmap=None, norm=None, alpha=None, values=None, boundaries=None, orientation='vertical', ticklocation='auto', extend=None, spacing='uniform', ticks=None, format=None, drawedges=False, filled=True, extendfrac=None, extendrect=False, label='')[source]\n \nBases: object Draw a colorbar in an existing axes. Typically, colorbars are created using Figure.colorbar or pyplot.colorbar and associated with ScalarMappables (such as an AxesImage generated via imshow). In order to draw a colorbar not associated with other elements in the figure, e.g. when showing a colormap by itself, one can create an empty ScalarMappable, or directly pass cmap and norm instead of mappable to Colorbar. Useful public methods are set_label() and add_lines().  Parameters \n \naxAxes\n\n\nThe Axes instance in which the colorbar is drawn.  \nmappableScalarMappable\n\n\nThe mappable whose colormap and norm will be used. To show the under- and over- value colors, the mappable's norm should be specified as norm = colors.Normalize(clip=False)\n To show the colors versus index instead of on a 0-1 scale, use: norm=colors.NoNorm()\n  \ncmapColormap, default: rcParams[\"image.cmap\"] (default: 'viridis')\n\n\nThe colormap to use. This parameter is ignored, unless mappable is None.  \nnormNormalize\n\n\nThe normalization to use. This parameter is ignored, unless mappable is None.  \nalphafloat\n\n\nThe colorbar transparency between 0 (transparent) and 1 (opaque).  values, boundaries\n\nIf unset, the colormap will be displayed on a 0-1 scale.  \norientation{'vertical', 'horizontal'}\n\n\nticklocation{'auto', 'left', 'right', 'top', 'bottom'}\n\n\nextend{'neither', 'both', 'min', 'max'}\n\n\nspacing{'uniform', 'proportional'}\n\n\nticksLocator or array-like of float\n\n\nformatstr or Formatter\n\n\ndrawedgesbool\n\n\nfilledbool\n\nextendfrac\nextendrec\n\nlabelstr\n\n  Attributes \n \naxAxes\n\n\nThe Axes instance in which the colorbar is drawn.  \nlineslist\n\n\nA list of LineCollection (empty if no lines were drawn).  \ndividersLineCollection\n\n\nA LineCollection (empty if drawedges is False).       add_lines(*args, **kwargs)[source]\n \nDraw lines on the colorbar. The lines are appended to the list lines.  Parameters \n \nlevelsarray-like\n\n\nThe positions of the lines.  \ncolorscolor or list of colors\n\n\nEither a single color applying to all lines or one color value for each line.  \nlinewidthsfloat or array-like\n\n\nEither a single linewidth applying to all lines or one linewidth for each line.  \nerasebool, default: True\n\n\nWhether to remove any previously added lines.     Notes Alternatively, this method can also be called with the signature colorbar.add_lines(contour_set, erase=True), in which case levels, colors, and linewidths are taken from contour_set. \n   drag_pan(button, key, x, y)[source]\n\n   draw_all()[source]\n \nCalculate any free parameters based on the current cmap and norm, and do all the drawing. \n   get_ticks(minor=False)[source]\n \nReturn the ticks as a list of locations.  Parameters \n \nminorboolean, default: False\n\n\nif True return the minor ticks.     \n   minorticks_off()[source]\n \nTurn the minor ticks of the colorbar off. \n   minorticks_on()[source]\n \nTurn on colorbar minor ticks. \n   n_rasterize=50\n\n   propertypatch[source]\n\n   remove()[source]\n \nRemove this colorbar from the figure. If the colorbar was created with use_gridspec=True the previous gridspec is restored. \n   set_alpha(alpha)[source]\n \nSet the transparency between 0 (transparent) and 1 (opaque). If an array is provided, alpha will be set to None to use the transparency values associated with the colormap. \n   set_label(label, *, loc=None, **kwargs)[source]\n \nAdd a label to the long axis of the colorbar.  Parameters \n \nlabelstr\n\n\nThe label text.  \nlocstr, optional\n\n\nThe location of the label.  For horizontal orientation one of {'left', 'center', 'right'} For vertical orientation one of {'bottom', 'center', 'top'}  Defaults to rcParams[\"xaxis.labellocation\"] (default: 'center') or rcParams[\"yaxis.labellocation\"] (default: 'center') depending on the orientation.  **kwargs\n\nKeyword arguments are passed to set_xlabel \/ set_ylabel. Supported keywords are labelpad and Text properties.     \n   set_ticklabels(ticklabels, update_ticks=<deprecated parameter>, *, minor=False, **kwargs)[source]\n \nSet tick labels.  Discouraged The use of this method is discouraged, because of the dependency on tick positions. In most cases, you'll want to use set_ticks(positions, labels=labels) instead. If you are using this method, you should always fix the tick positions before, e.g. by using Colorbar.set_ticks or by explicitly setting a FixedLocator on the long axis of the colorbar. Otherwise, ticks are free to move and the labels may end up in unexpected positions.   Parameters \n \nticklabelssequence of str or of Text\n\n\nTexts for labeling each tick location in the sequence set by Colorbar.set_ticks; the number of labels must match the number of locations.  \nupdate_ticksbool, default: True\n\n This keyword argument is ignored and will be be removed. Deprecated  minorbool\n\n\nIf True, set minor ticks instead of major ticks.    **kwargs\n\nText properties for the labels.     \n   set_ticks(ticks, update_ticks=<deprecated parameter>, labels=None, *, minor=False, **kwargs)[source]\n \nSet tick locations.  Parameters \n \ntickslist of floats\n\n\nList of tick locations.  \nlabelslist of str, optional\n\n\nList of tick labels. If not set, the labels show the data value.  \nminorbool, default: False\n\n\nIf False, set the major ticks; if True, the minor ticks.  **kwargs\n\nText properties for the labels. These take effect only if you pass labels. In other cases, please use tick_params.     \n   update_normal(mappable)[source]\n \nUpdate solid patches, lines, etc. This is meant to be called when the norm of the image or contour plot to which this colorbar belongs changes. If the norm on the mappable is different than before, this resets the locator and formatter for the axis, so if these have been customized, they will need to be customized again. However, if the norm only changes values of vmin, vmax or cmap then the old formatter and locator will be preserved. \n   update_ticks()[source]\n \nSetup the ticks and ticklabels. This should not be needed by users.","title":"matplotlib.colorbar_api#matplotlib.colorbar.Colorbar"},{"text":"matplotlib.colorbar Colorbars are a visualization of the mapping from scalar values to colors. In Matplotlib they are drawn into a dedicated Axes.  Note Colorbars are typically created through Figure.colorbar or its pyplot wrapper pyplot.colorbar, which internally use Colorbar together with make_axes_gridspec (for GridSpec-positioned axes) or make_axes (for non-GridSpec-positioned axes). End-users most likely won't need to directly use this module's API.    classmatplotlib.colorbar.Colorbar(ax, mappable=None, *, cmap=None, norm=None, alpha=None, values=None, boundaries=None, orientation='vertical', ticklocation='auto', extend=None, spacing='uniform', ticks=None, format=None, drawedges=False, filled=True, extendfrac=None, extendrect=False, label='')[source]\n \nBases: object Draw a colorbar in an existing axes. Typically, colorbars are created using Figure.colorbar or pyplot.colorbar and associated with ScalarMappables (such as an AxesImage generated via imshow). In order to draw a colorbar not associated with other elements in the figure, e.g. when showing a colormap by itself, one can create an empty ScalarMappable, or directly pass cmap and norm instead of mappable to Colorbar. Useful public methods are set_label() and add_lines().  Parameters \n \naxAxes\n\n\nThe Axes instance in which the colorbar is drawn.  \nmappableScalarMappable\n\n\nThe mappable whose colormap and norm will be used. To show the under- and over- value colors, the mappable's norm should be specified as norm = colors.Normalize(clip=False)\n To show the colors versus index instead of on a 0-1 scale, use: norm=colors.NoNorm()\n  \ncmapColormap, default: rcParams[\"image.cmap\"] (default: 'viridis')\n\n\nThe colormap to use. This parameter is ignored, unless mappable is None.  \nnormNormalize\n\n\nThe normalization to use. This parameter is ignored, unless mappable is None.  \nalphafloat\n\n\nThe colorbar transparency between 0 (transparent) and 1 (opaque).  values, boundaries\n\nIf unset, the colormap will be displayed on a 0-1 scale.  \norientation{'vertical', 'horizontal'}\n\n\nticklocation{'auto', 'left', 'right', 'top', 'bottom'}\n\n\nextend{'neither', 'both', 'min', 'max'}\n\n\nspacing{'uniform', 'proportional'}\n\n\nticksLocator or array-like of float\n\n\nformatstr or Formatter\n\n\ndrawedgesbool\n\n\nfilledbool\n\nextendfrac\nextendrec\n\nlabelstr\n\n  Attributes \n \naxAxes\n\n\nThe Axes instance in which the colorbar is drawn.  \nlineslist\n\n\nA list of LineCollection (empty if no lines were drawn).  \ndividersLineCollection\n\n\nA LineCollection (empty if drawedges is False).       add_lines(*args, **kwargs)[source]\n \nDraw lines on the colorbar. The lines are appended to the list lines.  Parameters \n \nlevelsarray-like\n\n\nThe positions of the lines.  \ncolorscolor or list of colors\n\n\nEither a single color applying to all lines or one color value for each line.  \nlinewidthsfloat or array-like\n\n\nEither a single linewidth applying to all lines or one linewidth for each line.  \nerasebool, default: True\n\n\nWhether to remove any previously added lines.     Notes Alternatively, this method can also be called with the signature colorbar.add_lines(contour_set, erase=True), in which case levels, colors, and linewidths are taken from contour_set. \n   drag_pan(button, key, x, y)[source]\n\n   draw_all()[source]\n \nCalculate any free parameters based on the current cmap and norm, and do all the drawing. \n   get_ticks(minor=False)[source]\n \nReturn the ticks as a list of locations.  Parameters \n \nminorboolean, default: False\n\n\nif True return the minor ticks.     \n   minorticks_off()[source]\n \nTurn the minor ticks of the colorbar off. \n   minorticks_on()[source]\n \nTurn on colorbar minor ticks. \n   n_rasterize=50\n\n   propertypatch[source]\n\n   remove()[source]\n \nRemove this colorbar from the figure. If the colorbar was created with use_gridspec=True the previous gridspec is restored. \n   set_alpha(alpha)[source]\n \nSet the transparency between 0 (transparent) and 1 (opaque). If an array is provided, alpha will be set to None to use the transparency values associated with the colormap. \n   set_label(label, *, loc=None, **kwargs)[source]\n \nAdd a label to the long axis of the colorbar.  Parameters \n \nlabelstr\n\n\nThe label text.  \nlocstr, optional\n\n\nThe location of the label.  For horizontal orientation one of {'left', 'center', 'right'} For vertical orientation one of {'bottom', 'center', 'top'}  Defaults to rcParams[\"xaxis.labellocation\"] (default: 'center') or rcParams[\"yaxis.labellocation\"] (default: 'center') depending on the orientation.  **kwargs\n\nKeyword arguments are passed to set_xlabel \/ set_ylabel. Supported keywords are labelpad and Text properties.     \n   set_ticklabels(ticklabels, update_ticks=<deprecated parameter>, *, minor=False, **kwargs)[source]\n \nSet tick labels.  Discouraged The use of this method is discouraged, because of the dependency on tick positions. In most cases, you'll want to use set_ticks(positions, labels=labels) instead. If you are using this method, you should always fix the tick positions before, e.g. by using Colorbar.set_ticks or by explicitly setting a FixedLocator on the long axis of the colorbar. Otherwise, ticks are free to move and the labels may end up in unexpected positions.   Parameters \n \nticklabelssequence of str or of Text\n\n\nTexts for labeling each tick location in the sequence set by Colorbar.set_ticks; the number of labels must match the number of locations.  \nupdate_ticksbool, default: True\n\n This keyword argument is ignored and will be be removed. Deprecated  minorbool\n\n\nIf True, set minor ticks instead of major ticks.    **kwargs\n\nText properties for the labels.     \n   set_ticks(ticks, update_ticks=<deprecated parameter>, labels=None, *, minor=False, **kwargs)[source]\n \nSet tick locations.  Parameters \n \ntickslist of floats\n\n\nList of tick locations.  \nlabelslist of str, optional\n\n\nList of tick labels. If not set, the labels show the data value.  \nminorbool, default: False\n\n\nIf False, set the major ticks; if True, the minor ticks.  **kwargs\n\nText properties for the labels. These take effect only if you pass labels. In other cases, please use tick_params.     \n   update_normal(mappable)[source]\n \nUpdate solid patches, lines, etc. This is meant to be called when the norm of the image or contour plot to which this colorbar belongs changes. If the norm on the mappable is different than before, this resets the locator and formatter for the axis, so if these have been customized, they will need to be customized again. However, if the norm only changes values of vmin, vmax or cmap then the old formatter and locator will be preserved. \n   update_ticks()[source]\n \nSetup the ticks and ticklabels. This should not be needed by users. \n \n   matplotlib.colorbar.ColorbarBase[source]\n \nalias of matplotlib.colorbar.Colorbar \n   classmatplotlib.colorbar.ColorbarPatch(ax, mappable=None, *, cmap=None, norm=None, alpha=None, values=None, boundaries=None, orientation='vertical', ticklocation='auto', extend=None, spacing='uniform', ticks=None, format=None, drawedges=False, filled=True, extendfrac=None, extendrect=False, label='')[source]\n \nBases: matplotlib.colorbar.Colorbar [Deprecated] Notes  Deprecated since version 3.4:   \n   matplotlib.colorbar.colorbar_factory(cax, mappable, **kwargs)[source]\n \n[Deprecated] Create a colorbar on the given axes for the given mappable.  Note This is a low-level function to turn an existing axes into a colorbar axes. Typically, you'll want to use colorbar instead, which automatically handles creation and placement of a suitable axes as well.   Parameters \n \ncaxAxes\n\n\nThe Axes to turn into a colorbar.  \nmappableScalarMappable\n\n\nThe mappable to be described by the colorbar.  **kwargs\n\nKeyword arguments are passed to the respective colorbar class.    Returns \n Colorbar\n\nThe created colorbar instance.     Notes  Deprecated since version 3.4.  \n   matplotlib.colorbar.make_axes(parents, location=None, orientation=None, fraction=0.15, shrink=1.0, aspect=20, **kw)[source]\n \nCreate an Axes suitable for a colorbar. The axes is placed in the figure of the parents axes, by resizing and repositioning parents.  Parameters \n \nparentsAxes or list of Axes\n\n\nThe Axes to use as parents for placing the colorbar.  \nlocationNone or {'left', 'right', 'top', 'bottom'}\n\n\nThe location, relative to the parent axes, where the colorbar axes is created. It also determines the orientation of the colorbar (colorbars on the left and right are vertical, colorbars at the top and bottom are horizontal). If None, the location will come from the orientation if it is set (vertical colorbars on the right, horizontal ones at the bottom), or default to 'right' if orientation is unset.  \norientationNone or {'vertical', 'horizontal'}\n\n\nThe orientation of the colorbar. It is preferable to set the location of the colorbar, as that also determines the orientation; passing incompatible values for location and orientation raises an exception.  \nfractionfloat, default: 0.15\n\n\nFraction of original axes to use for colorbar.  \nshrinkfloat, default: 1.0\n\n\nFraction by which to multiply the size of the colorbar.  \naspectfloat, default: 20\n\n\nRatio of long to short dimensions.    Returns \n \ncaxAxes\n\n\nThe child axes.  \nkwdict\n\n\nThe reduced keyword dictionary to be passed when creating the colorbar instance.    Other Parameters \n \npadfloat, default: 0.05 if vertical, 0.15 if horizontal\n\n\nFraction of original axes between colorbar and new image axes.  \nanchor(float, float), optional\n\n\nThe anchor point of the colorbar axes. Defaults to (0.0, 0.5) if vertical; (0.5, 1.0) if horizontal.  \npanchor(float, float), or False, optional\n\n\nThe anchor point of the colorbar parent axes. If False, the parent axes' anchor will be unchanged. Defaults to (1.0, 0.5) if vertical; (0.5, 0.0) if horizontal.     \n   matplotlib.colorbar.make_axes_gridspec(parent, *, location=None, orientation=None, fraction=0.15, shrink=1.0, aspect=20, **kw)[source]\n \nCreate a SubplotBase suitable for a colorbar. The axes is placed in the figure of the parent axes, by resizing and repositioning parent. This function is similar to make_axes. Primary differences are  \nmake_axes_gridspec should only be used with a SubplotBase parent. \nmake_axes creates an Axes; make_axes_gridspec creates a SubplotBase. \nmake_axes updates the position of the parent. make_axes_gridspec replaces the grid_spec attribute of the parent with a new one.  While this function is meant to be compatible with make_axes, there could be some minor differences.  Parameters \n \nparentAxes\n\n\nThe Axes to use as parent for placing the colorbar.  \nlocationNone or {'left', 'right', 'top', 'bottom'}\n\n\nThe location, relative to the parent axes, where the colorbar axes is created. It also determines the orientation of the colorbar (colorbars on the left and right are vertical, colorbars at the top and bottom are horizontal). If None, the location will come from the orientation if it is set (vertical colorbars on the right, horizontal ones at the bottom), or default to 'right' if orientation is unset.  \norientationNone or {'vertical', 'horizontal'}\n\n\nThe orientation of the colorbar. It is preferable to set the location of the colorbar, as that also determines the orientation; passing incompatible values for location and orientation raises an exception.  \nfractionfloat, default: 0.15\n\n\nFraction of original axes to use for colorbar.  \nshrinkfloat, default: 1.0\n\n\nFraction by which to multiply the size of the colorbar.  \naspectfloat, default: 20\n\n\nRatio of long to short dimensions.    Returns \n \ncaxSubplotBase\n\n\nThe child axes.  \nkwdict\n\n\nThe reduced keyword dictionary to be passed when creating the colorbar instance.    Other Parameters \n \npadfloat, default: 0.05 if vertical, 0.15 if horizontal\n\n\nFraction of original axes between colorbar and new image axes.  \nanchor(float, float), optional\n\n\nThe anchor point of the colorbar axes. Defaults to (0.0, 0.5) if vertical; (0.5, 1.0) if horizontal.  \npanchor(float, float), or False, optional\n\n\nThe anchor point of the colorbar parent axes. If False, the parent axes' anchor will be unchanged. Defaults to (1.0, 0.5) if vertical; (0.5, 0.0) if horizontal.","title":"matplotlib.colorbar_api"},{"text":"colorbar_extend\n \nWhen this colormap exists on a scalar mappable and colorbar_extend is not False, colorbar creation will pick up colorbar_extend as the default value for the extend keyword in the matplotlib.colorbar.Colorbar constructor.","title":"matplotlib._as_gen.matplotlib.colors.colormap#matplotlib.colors.Colormap.colorbar_extend"}]}
{"task_id":29903025,"prompt":"def f_29903025(df):\n\treturn ","suffix":"","canonical_solution":"Counter(' '.join(df['text']).split()).most_common(100)","test_start":"\nimport pandas as pd\nfrom collections import Counter\n \ndef check(candidate):","test":["\n    df = pd.DataFrame({\"text\": [\n      'Python is a high-level, general-purpose programming language.', \n      'Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically-typed and garbage-collected.'\n    ]})\n    assert candidate(df) == [('Python', 2),('is', 2),('a', 1),('high-level,', 1),('general-purpose', 1),\n        ('programming', 1),('language.', 1),('Its', 1),('design', 1),('philosophy', 1),('emphasizes', 1),\n        ('code', 1),('readability', 1),('with', 1), ('the', 1),('use', 1),('of', 1),('significant', 1),\n        ('indentation.', 1),('dynamically-typed', 1),('and', 1),('garbage-collected.', 1)]\n"],"entry_point":"f_29903025","intent":"count most frequent 100 words in column 'text' of dataframe `df`","library":["collections","pandas"],"docs":[{"text":"pandas.Period.freq   Period.freq","title":"pandas.reference.api.pandas.period.freq"},{"text":"pandas.DataFrame.items   DataFrame.items()[source]\n \nIterate over (column name, Series) pairs. Iterates over the DataFrame columns, returning a tuple with the column name and the content as a Series.  Yields \n \nlabel:object\n\n\nThe column names for the DataFrame being iterated over.  \ncontent:Series\n\n\nThe column entries belonging to each label, as a Series.      See also  DataFrame.iterrows\n\nIterate over DataFrame rows as (index, Series) pairs.  DataFrame.itertuples\n\nIterate over DataFrame rows as namedtuples of the values.    Examples \n>>> df = pd.DataFrame({'species': ['bear', 'bear', 'marsupial'],\n...                   'population': [1864, 22000, 80000]},\n...                   index=['panda', 'polar', 'koala'])\n>>> df\n        species   population\npanda   bear      1864\npolar   bear      22000\nkoala   marsupial 80000\n>>> for label, content in df.items():\n...     print(f'label: {label}')\n...     print(f'content: {content}', sep='\\n')\n...\nlabel: species\ncontent:\npanda         bear\npolar         bear\nkoala    marsupial\nName: species, dtype: object\nlabel: population\ncontent:\npanda     1864\npolar    22000\nkoala    80000\nName: population, dtype: int64","title":"pandas.reference.api.pandas.dataframe.items"},{"text":"pandas.Period.ordinal   Period.ordinal","title":"pandas.reference.api.pandas.period.ordinal"},{"text":"pandas.Timedelta.freq   Timedelta.freq","title":"pandas.reference.api.pandas.timedelta.freq"},{"text":"pandas.Series.str.count   Series.str.count(pat, flags=0)[source]\n \nCount occurrences of pattern in each string of the Series\/Index. This function is used to count the number of times a particular regex pattern is repeated in each of the string elements of the Series.  Parameters \n \npat:str\n\n\nValid regular expression.  \nflags:int, default 0, meaning no flags\n\n\nFlags for the re module. For a complete list, see here.  **kwargs\n\nFor compatibility with other string methods. Not used.    Returns \n Series or Index\n\nSame type as the calling object containing the integer counts.      See also  re\n\nStandard library module for regular expressions.  str.count\n\nStandard library version, without regular expression support.    Notes Some characters need to be escaped when passing in pat. eg. '$' has a special meaning in regex and must be escaped when finding this literal character. Examples \n>>> s = pd.Series(['A', 'B', 'Aaba', 'Baca', np.nan, 'CABA', 'cat'])\n>>> s.str.count('a')\n0    0.0\n1    0.0\n2    2.0\n3    2.0\n4    NaN\n5    0.0\n6    1.0\ndtype: float64\n  Escape '$' to find the literal dollar sign. \n>>> s = pd.Series(['$', 'B', 'Aab$', '$$ca', 'C$B$', 'cat'])\n>>> s.str.count('\\\\$')\n0    1\n1    0\n2    1\n3    2\n4    2\n5    0\ndtype: int64\n  This is also available on Index \n>>> pd.Index(['A', 'A', 'Aaba', 'cat']).str.count('a')\nInt64Index([0, 0, 2, 1], dtype='int64')","title":"pandas.reference.api.pandas.series.str.count"},{"text":"pandas.DataFrame.iteritems   DataFrame.iteritems()[source]\n \nIterate over (column name, Series) pairs. Iterates over the DataFrame columns, returning a tuple with the column name and the content as a Series.  Yields \n \nlabel:object\n\n\nThe column names for the DataFrame being iterated over.  \ncontent:Series\n\n\nThe column entries belonging to each label, as a Series.      See also  DataFrame.iterrows\n\nIterate over DataFrame rows as (index, Series) pairs.  DataFrame.itertuples\n\nIterate over DataFrame rows as namedtuples of the values.    Examples \n>>> df = pd.DataFrame({'species': ['bear', 'bear', 'marsupial'],\n...                   'population': [1864, 22000, 80000]},\n...                   index=['panda', 'polar', 'koala'])\n>>> df\n        species   population\npanda   bear      1864\npolar   bear      22000\nkoala   marsupial 80000\n>>> for label, content in df.items():\n...     print(f'label: {label}')\n...     print(f'content: {content}', sep='\\n')\n...\nlabel: species\ncontent:\npanda         bear\npolar         bear\nkoala    marsupial\nName: species, dtype: object\nlabel: population\ncontent:\npanda     1864\npolar    22000\nkoala    80000\nName: population, dtype: int64","title":"pandas.reference.api.pandas.dataframe.iteritems"},{"text":"pandas.tseries.offsets.BYearEnd.freqstr   BYearEnd.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.byearend.freqstr"},{"text":"decimal.MAX_PREC","title":"python.library.decimal#decimal.MAX_PREC"},{"text":"pandas.tseries.offsets.Day.freqstr   Day.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.day.freqstr"},{"text":"pandas.Timedelta.max   Timedelta.max=Timedelta('106751 days 23:47:16.854775807')","title":"pandas.reference.api.pandas.timedelta.max"}]}
{"task_id":7378180,"prompt":"def f_7378180():\n\treturn ","suffix":"","canonical_solution":"list(itertools.combinations((1, 2, 3), 2))","test_start":"\nimport itertools\n\ndef check(candidate):","test":["\n    assert candidate() == [(1, 2), (1, 3), (2, 3)]\n"],"entry_point":"f_7378180","intent":"generate all 2-element subsets of tuple `(1, 2, 3)`","library":["itertools"],"docs":[{"text":"zorder=2","title":"matplotlib._as_gen.matplotlib.lines.line2d#matplotlib.lines.Line2D.zorder"},{"text":"itertools.combinations_with_replacement(iterable, r)  \nReturn r length subsequences of elements from the input iterable allowing individual elements to be repeated more than once. The combination tuples are emitted in lexicographic ordering according to the order of the input iterable. So, if the input iterable is sorted, the combination tuples will be produced in sorted order. Elements are treated as unique based on their position, not on their value. So if the input elements are unique, the generated combinations will also be unique. Roughly equivalent to: def combinations_with_replacement(iterable, r):\n    # combinations_with_replacement('ABC', 2) --> AA AB AC BB BC CC\n    pool = tuple(iterable)\n    n = len(pool)\n    if not n and r:\n        return\n    indices = [0] * r\n    yield tuple(pool[i] for i in indices)\n    while True:\n        for i in reversed(range(r)):\n            if indices[i] != n - 1:\n                break\n        else:\n            return\n        indices[i:] = [indices[i] + 1] * (r - i)\n        yield tuple(pool[i] for i in indices)\n The code for combinations_with_replacement() can be also expressed as a subsequence of product() after filtering entries where the elements are not in sorted order (according to their position in the input pool): def combinations_with_replacement(iterable, r):\n    pool = tuple(iterable)\n    n = len(pool)\n    for indices in product(range(n), repeat=r):\n        if sorted(indices) == list(indices):\n            yield tuple(pool[i] for i in indices)\n The number of items returned is (n+r-1)! \/ r! \/ (n-1)! when n > 0.  New in version 3.1.","title":"python.library.itertools#itertools.combinations_with_replacement"},{"text":"numpy.polynomial.hermite.Hermite.domain attribute   polynomial.hermite.Hermite.domain = array([-1, 1])","title":"numpy.reference.generated.numpy.polynomial.hermite.hermite.domain"},{"text":"numpy.polynomial.chebyshev.Chebyshev.domain attribute   polynomial.chebyshev.Chebyshev.domain = array([-1, 1])","title":"numpy.reference.generated.numpy.polynomial.chebyshev.chebyshev.domain"},{"text":"itertools.permutations(iterable, r=None)  \nReturn successive r length permutations of elements in the iterable. If r is not specified or is None, then r defaults to the length of the iterable and all possible full-length permutations are generated. The permutation tuples are emitted in lexicographic ordering according to the order of the input iterable. So, if the input iterable is sorted, the combination tuples will be produced in sorted order. Elements are treated as unique based on their position, not on their value. So if the input elements are unique, there will be no repeat values in each permutation. Roughly equivalent to: def permutations(iterable, r=None):\n    # permutations('ABCD', 2) --> AB AC AD BA BC BD CA CB CD DA DB DC\n    # permutations(range(3)) --> 012 021 102 120 201 210\n    pool = tuple(iterable)\n    n = len(pool)\n    r = n if r is None else r\n    if r > n:\n        return\n    indices = list(range(n))\n    cycles = list(range(n, n-r, -1))\n    yield tuple(pool[i] for i in indices[:r])\n    while n:\n        for i in reversed(range(r)):\n            cycles[i] -= 1\n            if cycles[i] == 0:\n                indices[i:] = indices[i+1:] + indices[i:i+1]\n                cycles[i] = n - i\n            else:\n                j = cycles[i]\n                indices[i], indices[-j] = indices[-j], indices[i]\n                yield tuple(pool[i] for i in indices[:r])\n                break\n        else:\n            return\n The code for permutations() can be also expressed as a subsequence of product(), filtered to exclude entries with repeated elements (those from the same position in the input pool): def permutations(iterable, r=None):\n    pool = tuple(iterable)\n    n = len(pool)\n    r = n if r is None else r\n    for indices in product(range(n), repeat=r):\n        if len(set(indices)) == r:\n            yield tuple(pool[i] for i in indices)\n The number of items returned is n! \/ (n-r)! when 0 <= r <= n or zero when r > n.","title":"python.library.itertools#itertools.permutations"},{"text":"numpy.polynomial.hermite_e.HermiteE.domain attribute   polynomial.hermite_e.HermiteE.domain = array([-1, 1])","title":"numpy.reference.generated.numpy.polynomial.hermite_e.hermitee.domain"},{"text":"union(*others)  \nset | other | ...  \nReturn a new set with elements from the set and all others.","title":"python.library.stdtypes#frozenset.union"},{"text":"numpy.random.SeedSequence.spawn_key attribute   random.SeedSequence.spawn_key","title":"numpy.reference.random.bit_generators.generated.numpy.random.seedsequence.spawn_key"},{"text":"tuple","title":"django.ref.contrib.gis.gdal#django.contrib.gis.gdal.OGRGeometry.tuple"},{"text":"numpy.longcomplex[source]\n \nalias of numpy.clongdouble","title":"numpy.reference.arrays.scalars#numpy.longcomplex"}]}
{"task_id":4530069,"prompt":"def f_4530069():\n\treturn ","suffix":"","canonical_solution":"datetime.now(pytz.utc)","test_start":"\nimport pytz\nimport time\nfrom datetime import datetime, timezone\n\ndef check(candidate):","test":["\n    assert (candidate() - datetime(1970, 1, 1).replace(tzinfo=timezone.utc)).total_seconds() - time.time() <= 1\n"],"entry_point":"f_4530069","intent":"get a value of datetime.today() in the UTC time zone","library":["datetime","pytz","time"],"docs":[{"text":"classmethod date.today()  \nReturn the current local date. This is equivalent to date.fromtimestamp(time.time()).","title":"python.library.datetime#datetime.date.today"},{"text":"timezone.dst(dt)  \nAlways returns None.","title":"python.library.datetime#datetime.timezone.dst"},{"text":"classmethod datetime.now(tz=None)  \nReturn the current local date and time. If optional argument tz is None or not specified, this is like today(), but, if possible, supplies more precision than can be gotten from going through a time.time() timestamp (for example, this may be possible on platforms supplying the C gettimeofday() function). If tz is not None, it must be an instance of a tzinfo subclass, and the current date and time are converted to tz\u2019s time zone. This function is preferred over today() and utcnow().","title":"python.library.datetime#datetime.datetime.now"},{"text":"classmethod datetime.today()  \nReturn the current local datetime, with tzinfo None. Equivalent to: datetime.fromtimestamp(time.time())\n See also now(), fromtimestamp(). This method is functionally equivalent to now(), but without a tz parameter.","title":"python.library.datetime#datetime.datetime.today"},{"text":"utc  \ntzinfo instance that represents UTC.","title":"django.ref.utils#django.utils.timezone.utc"},{"text":"timezone.utc  \nThe UTC timezone, timezone(timedelta(0)).","title":"python.library.datetime#datetime.timezone.utc"},{"text":"get_current_timezone()  \nReturns a tzinfo instance that represents the current time zone.","title":"django.ref.utils#django.utils.timezone.get_current_timezone"},{"text":"classmethod datetime.utcnow()  \nReturn the current UTC date and time, with tzinfo None. This is like now(), but returns the current UTC date and time, as a naive datetime object. An aware current UTC datetime can be obtained by calling datetime.now(timezone.utc). See also now().  Warning Because naive datetime objects are treated by many datetime methods as local times, it is preferred to use aware datetimes to represent times in UTC. As such, the recommended way to create an object representing the current time in UTC is by calling datetime.now(timezone.utc).","title":"python.library.datetime#datetime.datetime.utcnow"},{"text":"now()  \nReturns a datetime that represents the current point in time. Exactly what\u2019s returned depends on the value of USE_TZ:  If USE_TZ is False, this will be a naive datetime (i.e. a datetime without an associated timezone) that represents the current time in the system\u2019s local timezone. If USE_TZ is True, this will be an aware datetime representing the current time in UTC. Note that now() will always return times in UTC regardless of the value of TIME_ZONE; you can use localtime() to get the time in the current time zone.","title":"django.ref.utils#django.utils.timezone.now"},{"text":"datetime.hour  \nIn range(24).","title":"python.library.datetime#datetime.datetime.hour"}]}
{"task_id":4842956,"prompt":"def f_4842956(list1):\n\t","suffix":"\n\treturn list2","canonical_solution":"list2 = [x for x in list1 if x != []]","test_start":"\ndef check(candidate):","test":["\n    assert candidate([[\"a\"], [], [\"b\"]]) == [[\"a\"], [\"b\"]]\n","\n    assert candidate([[], [1,2,3], [], [\"b\"]]) == [[1,2,3], [\"b\"]]\n"],"entry_point":"f_4842956","intent":"Get a new list `list2`by removing empty list from a list of lists `list1`","library":[],"docs":[]}
{"task_id":4842956,"prompt":"def f_4842956(list1):\n\t","suffix":"\n\treturn list2","canonical_solution":"list2 = [x for x in list1 if x]","test_start":"\ndef check(candidate):","test":["\n    assert candidate([[\"a\"], [], [\"b\"]]) == [[\"a\"], [\"b\"]]\n","\n    assert candidate([[], [1,2,3], [], [\"b\"]]) == [[1,2,3], [\"b\"]]\n"],"entry_point":"f_4842956","intent":"Create `list2` to contain the lists from list `list1` excluding the empty lists from `list1`","library":[],"docs":[]}
{"task_id":9262278,"prompt":"def f_9262278(data):\n\treturn ","suffix":"","canonical_solution":"HttpResponse(data, content_type='application\/json')","test_start":"\nimport os\nimport json\nfrom django.http import HttpResponse\nfrom django.conf import settings\nif not settings.configured:\n    settings.configure(DEBUG=True)\n\ndef check(candidate):","test":["\n    if settings.DEBUG:\n        assert candidate(json.dumps({\"Sample-Key\": \"Sample-Value\"})).content == b'{\"Sample-Key\": \"Sample-Value\"}'\n","\n    if settings.DEBUG:\n        assert candidate(json.dumps({\"Sample-Key\": \"Sample-Value\"}))['content-type'] == 'application\/json'\n"],"entry_point":"f_9262278","intent":"Django response with JSON `data`","library":["django","json","os"],"docs":[{"text":"Form.errors.get_json_data(escape_html=False)","title":"django.ref.forms.api#django.forms.Form.errors.get_json_data"},{"text":"HttpResponse.items()  \nActs like dict.items() for HTTP headers on the response.","title":"django.ref.request-response#django.http.HttpResponse.items"},{"text":"class QueryDict","title":"django.ref.request-response#django.http.QueryDict"},{"text":"class HttpResponse","title":"django.ref.request-response#django.http.HttpResponse"},{"text":"HttpResponse.content  \nA bytestring representing the content, encoded from a string if necessary.","title":"django.ref.request-response#django.http.HttpResponse.content"},{"text":"Form.errors.as_json(escape_html=False)","title":"django.ref.forms.api#django.forms.Form.errors.as_json"},{"text":"json","title":"django.ref.contrib.gis.gdal#django.contrib.gis.gdal.OGRGeometry.json"},{"text":"class StreamingHttpResponse","title":"django.ref.request-response#django.http.StreamingHttpResponse"},{"text":"Form.errors.as_data()","title":"django.ref.forms.api#django.forms.Form.errors.as_data"},{"text":"class SimpleTemplateResponse","title":"django.ref.template-response#django.template.response.SimpleTemplateResponse"}]}
{"task_id":17284947,"prompt":"def f_17284947(example_str):\n\treturn ","suffix":"","canonical_solution":"re.findall('(.*?)\\\\[.*?\\\\]', example_str)","test_start":"\nimport re \n\ndef check(candidate):  ","test":["\n    list_elems = candidate(\"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\")\n    assert \"\".join(list_elems).strip() == 'Josie Smith Mugsy Dog Smith'\n"],"entry_point":"f_17284947","intent":"get all text that is not enclosed within square brackets in string `example_str`","library":["re"],"docs":[{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"email.utils.unquote(str)  \nReturn a new string which is an unquoted version of str. If str ends and begins with double quotes, they are stripped off. Likewise if str ends and begins with angle brackets, they are stripped off.","title":"python.library.email.utils#email.utils.unquote"},{"text":"str.rindex(sub[, start[, end]])  \nLike rfind() but raises ValueError when the substring sub is not found.","title":"python.library.stdtypes#str.rindex"},{"text":"gettext.lngettext(singular, plural, n)","title":"python.library.gettext#gettext.lngettext"},{"text":"exception xml.dom.SyntaxErr  \nRaised when an invalid or illegal string is specified.","title":"python.library.xml.dom#xml.dom.SyntaxErr"},{"text":"str.strip([chars])  \nReturn a copy of the string with the leading and trailing characters removed. The chars argument is a string specifying the set of characters to be removed. If omitted or None, the chars argument defaults to removing whitespace. The chars argument is not a prefix or suffix; rather, all combinations of its values are stripped: >>> '   spacious   '.strip()\n'spacious'\n>>> 'www.example.com'.strip('cmowz.')\n'example'\n The outermost leading and trailing chars argument values are stripped from the string. Characters are removed from the leading end until reaching a string character that is not contained in the set of characters in chars. A similar action takes place on the trailing end. For example: >>> comment_string = '#....... Section 3.2.1 Issue #32 .......'\n>>> comment_string.strip('.#! ')\n'Section 3.2.1 Issue #32'","title":"python.library.stdtypes#str.strip"},{"text":"gettext.dpgettext(domain, context, message)","title":"python.library.gettext#gettext.dpgettext"},{"text":"numpy.char.chararray.index method   char.chararray.index(sub, start=0, end=None)[source]\n \nLike find, but raises ValueError when the substring is not found.  See also  char.index","title":"numpy.reference.generated.numpy.char.chararray.index"},{"text":"reason  \nA string describing the specific codec error.","title":"python.library.exceptions#UnicodeError.reason"},{"text":"numpy.chararray.index method   chararray.index(sub, start=0, end=None)[source]\n \nLike find, but raises ValueError when the substring is not found.  See also  char.index","title":"numpy.reference.generated.numpy.chararray.index"}]}
{"task_id":17284947,"prompt":"def f_17284947(example_str):\n\treturn ","suffix":"","canonical_solution":"re.findall('(.*?)(?:\\\\[.*?\\\\]|$)', example_str)","test_start":"\nimport re \n\ndef check(candidate): ","test":["\n    list_elems = candidate(\"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\")\n    assert \"\".join(list_elems).strip() == 'Josie Smith Mugsy Dog Smith'\n"],"entry_point":"f_17284947","intent":"Use a regex to get all text in a string `example_str` that is not surrounded by square brackets","library":["re"],"docs":[{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"pattern  \nThe regular expression pattern.","title":"python.library.re#re.error.pattern"},{"text":"gettext.lngettext(singular, plural, n)","title":"python.library.gettext#gettext.lngettext"},{"text":"gettext.dpgettext(domain, context, message)","title":"python.library.gettext#gettext.dpgettext"},{"text":"Match.string  \nThe string passed to match() or search().","title":"python.library.re#re.Match.string"},{"text":"exception xml.dom.SyntaxErr  \nRaised when an invalid or illegal string is specified.","title":"python.library.xml.dom#xml.dom.SyntaxErr"},{"text":"str.rindex(sub[, start[, end]])  \nLike rfind() but raises ValueError when the substring sub is not found.","title":"python.library.stdtypes#str.rindex"},{"text":"re.S  \nre.DOTALL  \nMake the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).","title":"python.library.re#re.S"},{"text":"fnmatch.translate(pattern)  \nReturn the shell-style pattern converted to a regular expression for using with re.match(). Example: >>> import fnmatch, re\n>>>\n>>> regex = fnmatch.translate('*.txt')\n>>> regex\n'(?s:.*\\\\.txt)\\\\Z'\n>>> reobj = re.compile(regex)\n>>> reobj.match('foobar.txt')\n<re.Match object; span=(0, 10), match='foobar.txt'>","title":"python.library.fnmatch#fnmatch.translate"},{"text":"email.utils.unquote(str)  \nReturn a new string which is an unquoted version of str. If str ends and begins with double quotes, they are stripped off. Likewise if str ends and begins with angle brackets, they are stripped off.","title":"python.library.email.utils#email.utils.unquote"}]}
{"task_id":14182339,"prompt":"def f_14182339():\n\treturn ","suffix":"","canonical_solution":"re.findall('\\\\(.+?\\\\)|\\\\w', '(zyx)bc')","test_start":"\nimport re \n\ndef check(candidate):    ","test":["\n    assert candidate() == ['(zyx)', 'b', 'c']\n"],"entry_point":"f_14182339","intent":"get whatever is between parentheses as a single match, and any char outside as an individual match in string '(zyx)bc'","library":["re"],"docs":[{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"token.SLASHEQUAL  \nToken value for \"\/=\".","title":"python.library.token#token.SLASHEQUAL"},{"text":"token.SLASH  \nToken value for \"\/\".","title":"python.library.token#token.SLASH"},{"text":"token.STAREQUAL  \nToken value for \"*=\".","title":"python.library.token#token.STAREQUAL"},{"text":"token.LBRACE  \nToken value for \"{\".","title":"python.library.token#token.LBRACE"},{"text":"token.COMMA  \nToken value for \",\".","title":"python.library.token#token.COMMA"},{"text":"token.RBRACE  \nToken value for \"}\".","title":"python.library.token#token.RBRACE"},{"text":"str.rindex(sub[, start[, end]])  \nLike rfind() but raises ValueError when the substring sub is not found.","title":"python.library.stdtypes#str.rindex"},{"text":"token.DOUBLESLASH  \nToken value for \"\/\/\".","title":"python.library.token#token.DOUBLESLASH"},{"text":"token.COLONEQUAL  \nToken value for \":=\".","title":"python.library.token#token.COLONEQUAL"}]}
{"task_id":14182339,"prompt":"def f_14182339():\n\treturn ","suffix":"","canonical_solution":"re.findall('\\\\((.*?)\\\\)|(\\\\w)', '(zyx)bc')","test_start":"\nimport re \n\ndef check(candidate): ","test":["\n    assert candidate() == [('zyx', ''), ('', 'b'), ('', 'c')]\n"],"entry_point":"f_14182339","intent":"match regex '\\\\((.*?)\\\\)|(\\\\w)' with string '(zyx)bc'","library":["re"],"docs":[{"text":"pattern  \nThe regular expression pattern.","title":"python.library.re#re.error.pattern"},{"text":"winreg.REG_SZ  \nA null-terminated string.","title":"python.library.winreg#winreg.REG_SZ"},{"text":"fnmatch.translate(pattern)  \nReturn the shell-style pattern converted to a regular expression for using with re.match(). Example: >>> import fnmatch, re\n>>>\n>>> regex = fnmatch.translate('*.txt')\n>>> regex\n'(?s:.*\\\\.txt)\\\\Z'\n>>> reobj = re.compile(regex)\n>>> reobj.match('foobar.txt')\n<re.Match object; span=(0, 10), match='foobar.txt'>","title":"python.library.fnmatch#fnmatch.translate"},{"text":"token.SLASHEQUAL  \nToken value for \"\/=\".","title":"python.library.token#token.SLASHEQUAL"},{"text":"re.S  \nre.DOTALL  \nMake the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).","title":"python.library.re#re.S"},{"text":"winreg.REG_EXPAND_SZ  \nNull-terminated string containing references to environment variables (%PATH%).","title":"python.library.winreg#winreg.REG_EXPAND_SZ"},{"text":"token.SLASH  \nToken value for \"\/\".","title":"python.library.token#token.SLASH"},{"text":"token.DOUBLESLASH  \nToken value for \"\/\/\".","title":"python.library.token#token.DOUBLESLASH"},{"text":"Match.string  \nThe string passed to match() or search().","title":"python.library.re#re.Match.string"},{"text":"re.purge()  \nClear the regular expression cache.","title":"python.library.re#re.purge"}]}
{"task_id":14182339,"prompt":"def f_14182339():\n\treturn ","suffix":"","canonical_solution":"re.findall('\\\\(.*?\\\\)|\\\\w', '(zyx)bc')","test_start":"\nimport re \n\ndef check(candidate): ","test":["\n    assert candidate() == ['(zyx)', 'b', 'c']\n"],"entry_point":"f_14182339","intent":"match multiple regex patterns with the alternation operator `|` in a string `(zyx)bc`","library":["re"],"docs":[{"text":"pattern  \nThe regular expression pattern.","title":"python.library.re#re.error.pattern"},{"text":"token.VBAREQUAL  \nToken value for \"|=\".","title":"python.library.token#token.VBAREQUAL"},{"text":"token.VBAR  \nToken value for \"|\".","title":"python.library.token#token.VBAR"},{"text":"re.purge()  \nClear the regular expression cache.","title":"python.library.re#re.purge"},{"text":"Pattern.search(string[, pos[, endpos]])  \nScan through string looking for the first location where this regular expression produces a match, and return a corresponding match object. Return None if no position in the string matches the pattern; note that this is different from finding a zero-length match at some point in the string. The optional second parameter pos gives an index in the string where the search is to start; it defaults to 0. This is not completely equivalent to slicing the string; the '^' pattern character matches at the real beginning of the string and at positions just after a newline, but not necessarily at the index where the search is to start. The optional parameter endpos limits how far the string will be searched; it will be as if the string is endpos characters long, so only the characters from pos to endpos - 1 will be searched for a match. If endpos is less than pos, no match will be found; otherwise, if rx is a compiled regular expression object, rx.search(string, 0, 50) is equivalent to rx.search(string[:50], 0). >>> pattern = re.compile(\"d\")\n>>> pattern.search(\"dog\")     # Match at index 0\n<re.Match object; span=(0, 1), match='d'>\n>>> pattern.search(\"dog\", 1)  # No match; search doesn't include the \"d\"","title":"python.library.re#re.Pattern.search"},{"text":"fnmatch.translate(pattern)  \nReturn the shell-style pattern converted to a regular expression for using with re.match(). Example: >>> import fnmatch, re\n>>>\n>>> regex = fnmatch.translate('*.txt')\n>>> regex\n'(?s:.*\\\\.txt)\\\\Z'\n>>> reobj = re.compile(regex)\n>>> reobj.match('foobar.txt')\n<re.Match object; span=(0, 10), match='foobar.txt'>","title":"python.library.fnmatch#fnmatch.translate"},{"text":"test.support.set_match_tests(patterns)  \nDefine match test with regular expression patterns.","title":"python.library.test#test.support.set_match_tests"},{"text":"Pattern.match(string[, pos[, endpos]])  \nIf zero or more characters at the beginning of string match this regular expression, return a corresponding match object. Return None if the string does not match the pattern; note that this is different from a zero-length match. The optional pos and endpos parameters have the same meaning as for the search() method. >>> pattern = re.compile(\"o\")\n>>> pattern.match(\"dog\")      # No match as \"o\" is not at the start of \"dog\".\n>>> pattern.match(\"dog\", 1)   # Match as \"o\" is the 2nd character of \"dog\".\n<re.Match object; span=(1, 2), match='o'>\n If you want to locate a match anywhere in string, use search() instead (see also search() vs. match()).","title":"python.library.re#re.Pattern.match"},{"text":"re.search(pattern, string, flags=0)  \nScan through string looking for the first location where the regular expression pattern produces a match, and return a corresponding match object. Return None if no position in the string matches the pattern; note that this is different from finding a zero-length match at some point in the string.","title":"python.library.re#re.search"},{"text":"token.RBRACE  \nToken value for \"}\".","title":"python.library.token#token.RBRACE"}]}
{"task_id":7126916,"prompt":"def f_7126916(elements):\n\t","suffix":"\n\treturn elements","canonical_solution":"elements = ['%{0}%'.format(element) for element in elements]","test_start":"\ndef check(candidate): ","test":["\n    elements = ['abc', 'def', 'ijk', 'mno']\n    assert candidate(elements) == ['%abc%', '%def%', '%ijk%', '%mno%']\n","\n    elements = [1, 2, 3, 4, 500]\n    assert candidate(elements) == ['%1%', '%2%', '%3%', '%4%', '%500%']\n"],"entry_point":"f_7126916","intent":"formate each string cin list `elements` into pattern '%{0}%'","library":[],"docs":[]}
{"task_id":3595685,"prompt":"def f_3595685():\n\treturn ","suffix":"","canonical_solution":"subprocess.Popen(['background-process', 'arguments'])","test_start":"\nimport subprocess\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    subprocess.Popen = Mock(return_value = 0)\n    assert candidate() == 0\n"],"entry_point":"f_3595685","intent":"Open a background process 'background-process' with arguments 'arguments'","library":["subprocess"],"docs":[{"text":"cmd  \nCommand that was used to spawn the child process.","title":"python.library.subprocess#subprocess.TimeoutExpired.cmd"},{"text":"run(cmd)  \nProfile the cmd via exec().","title":"python.library.profile#profile.Profile.run"},{"text":"cmd  \nCommand that was used to spawn the child process.","title":"python.library.subprocess#subprocess.CalledProcessError.cmd"},{"text":"os.SCHED_IDLE  \nScheduling policy for extremely low priority background tasks.","title":"python.library.os#os.SCHED_IDLE"},{"text":"args  \nThe arguments used to launch the process. This may be a list or a string.","title":"python.library.subprocess#subprocess.CompletedProcess.args"},{"text":"subprocess.SW_HIDE  \nHides the window. Another window will be activated.","title":"python.library.subprocess#subprocess.SW_HIDE"},{"text":"signal.SIGCONT  \nContinue the process if it is currently stopped Availability: Unix.","title":"python.library.signal#signal.SIGCONT"},{"text":"stdout  \nAlias for output, for symmetry with stderr.","title":"python.library.subprocess#subprocess.CalledProcessError.stdout"},{"text":"started=False","title":"matplotlib.backend_webagg_api#matplotlib.backends.backend_webagg.WebAggApplication.started"},{"text":"Cmd.prompt  \nThe prompt issued to solicit input.","title":"python.library.cmd#cmd.Cmd.prompt"}]}
{"task_id":18453566,"prompt":"def f_18453566(mydict, mykeys):\n\treturn ","suffix":"","canonical_solution":"[mydict[x] for x in mykeys]","test_start":"\ndef check(candidate):","test":["\n    mydict = {'one': 1, 'two': 2, 'three': 3}\n    mykeys = ['three', 'one']\n    assert candidate(mydict, mykeys) == [3, 1]\n","\n    mydict = {'one': 1.0, 'two': 2.0, 'three': 3.0}\n    mykeys = ['one']\n    assert candidate(mydict, mykeys) == [1.0]\n"],"entry_point":"f_18453566","intent":"get list of values from dictionary 'mydict' w.r.t. list of keys 'mykeys'","library":[],"docs":[]}
{"task_id":12692135,"prompt":"def f_12692135():\n\treturn ","suffix":"","canonical_solution":"dict([('Name', 'Joe'), ('Age', 22)])","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == {'Name': 'Joe', 'Age': 22}\n"],"entry_point":"f_12692135","intent":"convert list `[('Name', 'Joe'), ('Age', 22)]` into a dictionary","library":[],"docs":[]}
{"task_id":14401047,"prompt":"def f_14401047(data):\n\treturn ","suffix":"","canonical_solution":"data.mean(axis=1).reshape(data.shape[0], -1)","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    data = np.array([[1, 2, 3, 4, 4, 3, 7, 1], [6, 2, 3, 4, 3, 4, 4, 1]])\n    expected_res = np.array([[3.125], [3.375]])\n    assert np.array_equal(candidate(data), expected_res)\n"],"entry_point":"f_14401047","intent":"average each two columns of array `data`","library":["numpy"],"docs":[{"text":"pandas.tseries.offsets.Second.normalize   Second.normalize","title":"pandas.reference.api.pandas.tseries.offsets.second.normalize"},{"text":"pandas.tseries.offsets.Second.apply   Second.apply()","title":"pandas.reference.api.pandas.tseries.offsets.second.apply"},{"text":"pandas.tseries.offsets.BYearEnd.normalize   BYearEnd.normalize","title":"pandas.reference.api.pandas.tseries.offsets.byearend.normalize"},{"text":"pandas.tseries.offsets.BQuarterEnd.normalize   BQuarterEnd.normalize","title":"pandas.reference.api.pandas.tseries.offsets.bquarterend.normalize"},{"text":"pandas.tseries.offsets.Micro.apply   Micro.apply()","title":"pandas.reference.api.pandas.tseries.offsets.micro.apply"},{"text":"pandas.tseries.offsets.BQuarterBegin.normalize   BQuarterBegin.normalize","title":"pandas.reference.api.pandas.tseries.offsets.bquarterbegin.normalize"},{"text":"pandas.tseries.offsets.Milli.apply   Milli.apply()","title":"pandas.reference.api.pandas.tseries.offsets.milli.apply"},{"text":"pandas.tseries.offsets.LastWeekOfMonth.normalize   LastWeekOfMonth.normalize","title":"pandas.reference.api.pandas.tseries.offsets.lastweekofmonth.normalize"},{"text":"pandas.tseries.offsets.Micro.normalize   Micro.normalize","title":"pandas.reference.api.pandas.tseries.offsets.micro.normalize"},{"text":"pandas.tseries.offsets.Milli.normalize   Milli.normalize","title":"pandas.reference.api.pandas.tseries.offsets.milli.normalize"}]}
{"task_id":18886596,"prompt":"def f_18886596(s):\n\treturn ","suffix":"","canonical_solution":"s.replace('\"', '\\\"')","test_start":"\ndef check(candidate):","test":["\n    s = 'This sentence has some \"quotes\" in it'\n    assert candidate(s) == 'This sentence has some \\\"quotes\\\" in it'\n"],"entry_point":"f_18886596","intent":"double backslash escape all double quotes in string `s`","library":[],"docs":[]}
{"task_id":5932059,"prompt":"def f_5932059(s):\n\treturn ","suffix":"","canonical_solution":"re.split('(\\\\W+)', s)","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    s = \"this is  a\\nsentence\"\n    assert candidate(s) == ['this', ' ', 'is', '  ', 'a', '\\n', 'sentence']\n"],"entry_point":"f_5932059","intent":"split a string `s` into a list of words and whitespace","library":["re"],"docs":[{"text":"numpy.char.split   char.split(a, sep=None, maxsplit=None)[source]\n \nFor each element in a, return a list of the words in the string, using sep as the delimiter string. Calls str.split element-wise.  Parameters \n \naarray_like of str or unicode\n\n\nsepstr or unicode, optional\n\n\nIf sep is not specified or None, any whitespace string is a separator.  \nmaxsplitint, optional\n\n\nIf maxsplit is given, at most maxsplit splits are done.    Returns \n \noutndarray\n\n\nArray of list objects      See also  \nstr.split, rsplit","title":"numpy.reference.generated.numpy.char.split"},{"text":"re.split(pattern, string, maxsplit=0, flags=0)  \nSplit string by the occurrences of pattern. If capturing parentheses are used in pattern, then the text of all groups in the pattern are also returned as part of the resulting list. If maxsplit is nonzero, at most maxsplit splits occur, and the remainder of the string is returned as the final element of the list. >>> re.split(r'\\W+', 'Words, words, words.')\n['Words', 'words', 'words', '']\n>>> re.split(r'(\\W+)', 'Words, words, words.')\n['Words', ', ', 'words', ', ', 'words', '.', '']\n>>> re.split(r'\\W+', 'Words, words, words.', 1)\n['Words', 'words, words.']\n>>> re.split('[a-f]+', '0a3B9', flags=re.IGNORECASE)\n['0', '3', '9']\n If there are capturing groups in the separator and it matches at the start of the string, the result will start with an empty string. The same holds for the end of the string: >>> re.split(r'(\\W+)', '...words, words...')\n['', '...', 'words', ', ', 'words', '...', '']\n That way, separator components are always found at the same relative indices within the result list. Empty matches for the pattern split the string only when not adjacent to a previous empty match. >>> re.split(r'\\b', 'Words, words, words.')\n['', 'Words', ', ', 'words', ', ', 'words', '.']\n>>> re.split(r'\\W*', '...words...')\n['', '', 'w', 'o', 'r', 'd', 's', '', '']\n>>> re.split(r'(\\W*)', '...words...')\n['', '...', '', '', 'w', '', 'o', '', 'r', '', 'd', '', 's', '...', '', '', '']\n  Changed in version 3.1: Added the optional flags argument.   Changed in version 3.7: Added support of splitting on a pattern that could match an empty string.","title":"python.library.re#re.split"},{"text":"gettext.lngettext(singular, plural, n)","title":"python.library.gettext#gettext.lngettext"},{"text":"str.split(sep=None, maxsplit=-1)  \nReturn a list of the words in the string, using sep as the delimiter string. If maxsplit is given, at most maxsplit splits are done (thus, the list will have at most maxsplit+1 elements). If maxsplit is not specified or -1, then there is no limit on the number of splits (all possible splits are made). If sep is given, consecutive delimiters are not grouped together and are deemed to delimit empty strings (for example, '1,,2'.split(',') returns ['1', '', '2']). The sep argument may consist of multiple characters (for example, '1<>2<>3'.split('<>') returns ['1', '2', '3']). Splitting an empty string with a specified separator returns ['']. For example: >>> '1,2,3'.split(',')\n['1', '2', '3']\n>>> '1,2,3'.split(',', maxsplit=1)\n['1', '2,3']\n>>> '1,2,,3,'.split(',')\n['1', '2', '', '3', '']\n If sep is not specified or is None, a different splitting algorithm is applied: runs of consecutive whitespace are regarded as a single separator, and the result will contain no empty strings at the start or end if the string has leading or trailing whitespace. Consequently, splitting an empty string or a string consisting of just whitespace with a None separator returns []. For example: >>> '1 2 3'.split()\n['1', '2', '3']\n>>> '1 2 3'.split(maxsplit=1)\n['1', '2 3']\n>>> '   1   2   3   '.split()\n['1', '2', '3']","title":"python.library.stdtypes#str.split"},{"text":"str.rsplit(sep=None, maxsplit=-1)  \nReturn a list of the words in the string, using sep as the delimiter string. If maxsplit is given, at most maxsplit splits are done, the rightmost ones. If sep is not specified or None, any whitespace string is a separator. Except for splitting from the right, rsplit() behaves like split() which is described in detail below.","title":"python.library.stdtypes#str.rsplit"},{"text":"numpy.char.rsplit   char.rsplit(a, sep=None, maxsplit=None)[source]\n \nFor each element in a, return a list of the words in the string, using sep as the delimiter string. Calls str.rsplit element-wise. Except for splitting from the right, rsplit behaves like split.  Parameters \n \naarray_like of str or unicode\n\n\nsepstr or unicode, optional\n\n\nIf sep is not specified or None, any whitespace string is a separator.  \nmaxsplitint, optional\n\n\nIf maxsplit is given, at most maxsplit splits are done, the rightmost ones.    Returns \n \noutndarray\n\n\nArray of list objects      See also  \nstr.rsplit, split","title":"numpy.reference.generated.numpy.char.rsplit"},{"text":"numpy.char.chararray.rsplit method   char.chararray.rsplit(sep=None, maxsplit=None)[source]\n \nFor each element in self, return a list of the words in the string, using sep as the delimiter string.  See also  char.rsplit","title":"numpy.reference.generated.numpy.char.chararray.rsplit"},{"text":"string.capwords(s, sep=None)  \nSplit the argument into words using str.split(), capitalize each word using str.capitalize(), and join the capitalized words using str.join(). If the optional second argument sep is absent or None, runs of whitespace characters are replaced by a single space and leading and trailing whitespace are removed, otherwise sep is used to split and join the words.","title":"python.library.string#string.capwords"},{"text":"numpy.char.chararray.split method   char.chararray.split(sep=None, maxsplit=None)[source]\n \nFor each element in self, return a list of the words in the string, using sep as the delimiter string.  See also  char.split","title":"numpy.reference.generated.numpy.char.chararray.split"},{"text":"numpy.chararray.rsplit method   chararray.rsplit(sep=None, maxsplit=None)[source]\n \nFor each element in self, return a list of the words in the string, using sep as the delimiter string.  See also  char.rsplit","title":"numpy.reference.generated.numpy.chararray.rsplit"}]}
{"task_id":9938130,"prompt":"def f_9938130(df):\n\treturn ","suffix":"","canonical_solution":"df.plot(kind='barh', stacked=True)","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    data = [[1, 3], [2, 5], [4, 8]]\n    df = pd.DataFrame(data, columns = ['a', 'b'])\n    assert str(candidate(df)).split('(')[0] == 'AxesSubplot'\n"],"entry_point":"f_9938130","intent":"plotting stacked barplots on a panda data frame `df`","library":["pandas"],"docs":[{"text":"pandas.DataFrame.plot.bar   DataFrame.plot.bar(x=None, y=None, **kwargs)[source]\n \nVertical bar plot. A bar plot is a plot that presents categorical data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.  Parameters \n \nx:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, the index of the DataFrame is used.  \ny:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, all numerical columns are used.  \ncolor:str, array-like, or dict, optional\n\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:  \n A single color string referred to by name, RGB or RGBA code,\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.    \n A sequence of color strings referred to by name, RGB or RGBA\n\ncode, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    \n A dict of the form {column name:color}, so that each column will be\n\n\ncolored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and bars for column b in red.      New in version 1.1.0.   **kwargs\n\nAdditional keyword arguments are documented in DataFrame.plot().    Returns \n matplotlib.axes.Axes or np.ndarray of them\n\nAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  DataFrame.plot.barh\n\nHorizontal bar plot.  DataFrame.plot\n\nMake plots of a DataFrame.  matplotlib.pyplot.bar\n\nMake a bar plot with matplotlib.    Examples Basic plot. \n>>> df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})\n>>> ax = df.plot.bar(x='lab', y='val', rot=0)\n     Plot a whole dataframe to a bar plot. Each column is assigned a distinct color, and each row is nested in a group along the horizontal axis. \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.bar(rot=0)\n     Plot stacked bar charts for the DataFrame \n>>> ax = df.plot.bar(stacked=True)\n     Instead of nesting, the figure can be split by column with subplots=True. In this case, a numpy.ndarray of matplotlib.axes.Axes are returned. \n>>> axes = df.plot.bar(rot=0, subplots=True)\n>>> axes[1].legend(loc=2)  \n     If you don\u2019t like the default colours, you can specify how you\u2019d like each column to be colored. \n>>> axes = df.plot.bar(\n...     rot=0, subplots=True, color={\"speed\": \"red\", \"lifespan\": \"green\"}\n... )\n>>> axes[1].legend(loc=2)  \n     Plot a single column. \n>>> ax = df.plot.bar(y='speed', rot=0)\n     Plot only selected categories for the DataFrame. \n>>> ax = df.plot.bar(x='lifespan', rot=0)","title":"pandas.reference.api.pandas.dataframe.plot.bar"},{"text":"pandas.Series.plot.bar   Series.plot.bar(x=None, y=None, **kwargs)[source]\n \nVertical bar plot. A bar plot is a plot that presents categorical data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.  Parameters \n \nx:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, the index of the DataFrame is used.  \ny:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, all numerical columns are used.  \ncolor:str, array-like, or dict, optional\n\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:  \n A single color string referred to by name, RGB or RGBA code,\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.    \n A sequence of color strings referred to by name, RGB or RGBA\n\ncode, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    \n A dict of the form {column name:color}, so that each column will be\n\n\ncolored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and bars for column b in red.      New in version 1.1.0.   **kwargs\n\nAdditional keyword arguments are documented in DataFrame.plot().    Returns \n matplotlib.axes.Axes or np.ndarray of them\n\nAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  DataFrame.plot.barh\n\nHorizontal bar plot.  DataFrame.plot\n\nMake plots of a DataFrame.  matplotlib.pyplot.bar\n\nMake a bar plot with matplotlib.    Examples Basic plot. \n>>> df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})\n>>> ax = df.plot.bar(x='lab', y='val', rot=0)\n     Plot a whole dataframe to a bar plot. Each column is assigned a distinct color, and each row is nested in a group along the horizontal axis. \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.bar(rot=0)\n     Plot stacked bar charts for the DataFrame \n>>> ax = df.plot.bar(stacked=True)\n     Instead of nesting, the figure can be split by column with subplots=True. In this case, a numpy.ndarray of matplotlib.axes.Axes are returned. \n>>> axes = df.plot.bar(rot=0, subplots=True)\n>>> axes[1].legend(loc=2)  \n     If you don\u2019t like the default colours, you can specify how you\u2019d like each column to be colored. \n>>> axes = df.plot.bar(\n...     rot=0, subplots=True, color={\"speed\": \"red\", \"lifespan\": \"green\"}\n... )\n>>> axes[1].legend(loc=2)  \n     Plot a single column. \n>>> ax = df.plot.bar(y='speed', rot=0)\n     Plot only selected categories for the DataFrame. \n>>> ax = df.plot.bar(x='lifespan', rot=0)","title":"pandas.reference.api.pandas.series.plot.bar"},{"text":"pandas.DataFrame.plot.barh   DataFrame.plot.barh(x=None, y=None, **kwargs)[source]\n \nMake a horizontal bar plot. A horizontal bar plot is a plot that presents quantitative data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.  Parameters \n \nx:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, the index of the DataFrame is used.  \ny:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, all numerical columns are used.  \ncolor:str, array-like, or dict, optional\n\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:  \n A single color string referred to by name, RGB or RGBA code,\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.    \n A sequence of color strings referred to by name, RGB or RGBA\n\ncode, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    \n A dict of the form {column name:color}, so that each column will be\n\n\ncolored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and bars for column b in red.      New in version 1.1.0.   **kwargs\n\nAdditional keyword arguments are documented in DataFrame.plot().    Returns \n matplotlib.axes.Axes or np.ndarray of them\n\nAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  DataFrame.plot.bar\n\nVertical bar plot.  DataFrame.plot\n\nMake plots of DataFrame using matplotlib.  matplotlib.axes.Axes.bar\n\nPlot a vertical bar plot using matplotlib.    Examples Basic example \n>>> df = pd.DataFrame({'lab': ['A', 'B', 'C'], 'val': [10, 30, 20]})\n>>> ax = df.plot.barh(x='lab', y='val')\n     Plot a whole DataFrame to a horizontal bar plot \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh()\n     Plot stacked barh charts for the DataFrame \n>>> ax = df.plot.barh(stacked=True)\n     We can specify colors for each column \n>>> ax = df.plot.barh(color={\"speed\": \"red\", \"lifespan\": \"green\"})\n     Plot a column of the DataFrame to a horizontal bar plot \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh(y='speed')\n     Plot DataFrame versus the desired column \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh(x='lifespan')","title":"pandas.reference.api.pandas.dataframe.plot.barh"},{"text":"barbs_doc='\\nPlot a 2D field of barbs.\\n\\nCall signature::\\n\\n barbs([X, Y], U, V, [C], **kw)\\n\\nWhere *X*, *Y* define the barb locations, *U*, *V* define the barb\\ndirections, and *C* optionally sets the color.\\n\\nAll arguments may be 1D or 2D. *U*, *V*, *C* may be masked arrays, but masked\\n*X*, *Y* are not supported at present.\\n\\nBarbs are traditionally used in meteorology as a way to plot the speed\\nand direction of wind observations, but can technically be used to\\nplot any two dimensional vector quantity. As opposed to arrows, which\\ngive vector magnitude by the length of the arrow, the barbs give more\\nquantitative information about the vector magnitude by putting slanted\\nlines or a triangle for various increments in magnitude, as show\\nschematically below::\\n\\n : \/\\\\ \\\\\\n : \/ \\\\ \\\\\\n : \/ \\\\ \\\\ \\\\\\n : \/ \\\\ \\\\ \\\\\\n : ------------------------------\\n\\nThe largest increment is given by a triangle (or \"flag\"). After those\\ncome full lines (barbs). The smallest increment is a half line. There\\nis only, of course, ever at most 1 half line. If the magnitude is\\nsmall and only needs a single half-line and no full lines or\\ntriangles, the half-line is offset from the end of the barb so that it\\ncan be easily distinguished from barbs with a single full line. The\\nmagnitude for the barb shown above would nominally be 65, using the\\nstandard increments of 50, 10, and 5.\\n\\nSee also https:\/\/en.wikipedia.org\/wiki\/Wind_barb.\\n\\nParameters\\n----------\\nX, Y : 1D or 2D array-like, optional\\n The x and y coordinates of the barb locations. See *pivot* for how the\\n barbs are drawn to the x, y positions.\\n\\n If not given, they will be generated as a uniform integer meshgrid based\\n on the dimensions of *U* and *V*.\\n\\n If *X* and *Y* are 1D but *U*, *V* are 2D, *X*, *Y* are expanded to 2D\\n using ``X, Y = np.meshgrid(X, Y)``. In this case ``len(X)`` and ``len(Y)``\\n must match the column and row dimensions of *U* and *V*.\\n\\nU, V : 1D or 2D array-like\\n The x and y components of the barb shaft.\\n\\nC : 1D or 2D array-like, optional\\n Numeric data that defines the barb colors by colormapping via *norm* and\\n *cmap*.\\n\\n This does not support explicit colors. If you want to set colors directly,\\n use *barbcolor* instead.\\n\\nlength : float, default: 7\\n Length of the barb in points; the other parts of the barb\\n are scaled against this.\\n\\npivot : {\\'tip\\', \\'middle\\'} or float, default: \\'tip\\'\\n The part of the arrow that is anchored to the *X*, *Y* grid. The barb\\n rotates about this point. This can also be a number, which shifts the\\n start of the barb that many points away from grid point.\\n\\nbarbcolor : color or color sequence\\n The color of all parts of the barb except for the flags. This parameter\\n is analogous to the *edgecolor* parameter for polygons, which can be used\\n instead. However this parameter will override facecolor.\\n\\nflagcolor : color or color sequence\\n The color of any flags on the barb. This parameter is analogous to the\\n *facecolor* parameter for polygons, which can be used instead. However,\\n this parameter will override facecolor. If this is not set (and *C* has\\n not either) then *flagcolor* will be set to match *barbcolor* so that the\\n barb has a uniform color. If *C* has been set, *flagcolor* has no effect.\\n\\nsizes : dict, optional\\n A dictionary of coefficients specifying the ratio of a given\\n feature to the length of the barb. Only those values one wishes to\\n override need to be included. These features include:\\n\\n - \\'spacing\\' - space between features (flags, full\/half barbs)\\n - \\'height\\' - height (distance from shaft to top) of a flag or full barb\\n - \\'width\\' - width of a flag, twice the width of a full barb\\n - \\'emptybarb\\' - radius of the circle used for low magnitudes\\n\\nfill_empty : bool, default: False\\n Whether the empty barbs (circles) that are drawn should be filled with\\n the flag color. If they are not filled, the center is transparent.\\n\\nrounding : bool, default: True\\n Whether the vector magnitude should be rounded when allocating barb\\n components. If True, the magnitude is rounded to the nearest multiple\\n of the half-barb increment. If False, the magnitude is simply truncated\\n to the next lowest multiple.\\n\\nbarb_increments : dict, optional\\n A dictionary of increments specifying values to associate with\\n different parts of the barb. Only those values one wishes to\\n override need to be included.\\n\\n - \\'half\\' - half barbs (Default is 5)\\n - \\'full\\' - full barbs (Default is 10)\\n - \\'flag\\' - flags (default is 50)\\n\\nflip_barb : bool or array-like of bool, default: False\\n Whether the lines and flags should point opposite to normal.\\n Normal behavior is for the barbs and lines to point right (comes from wind\\n barbs having these features point towards low pressure in the Northern\\n Hemisphere).\\n\\n A single value is applied to all barbs. Individual barbs can be flipped by\\n passing a bool array of the same size as *U* and *V*.\\n\\nReturns\\n-------\\nbarbs : `~matplotlib.quiver.Barbs`\\n\\nOther Parameters\\n----------------\\ndata : indexable object, optional\\n DATA_PARAMETER_PLACEHOLDER\\n\\n**kwargs\\n The barbs can further be customized using `.PolyCollection` keyword\\n arguments:\\n\\n \\n .. table::\\n :class: property-table\\n\\n ================================================================================================= =====================================================================================================\\n Property Description \\n ================================================================================================= =====================================================================================================\\n :meth:`agg_filter <matplotlib.artist.Artist.set_agg_filter>` a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\\n :meth:`alpha <matplotlib.collections.Collection.set_alpha>` array-like or scalar or None \\n :meth:`animated <matplotlib.artist.Artist.set_animated>` bool \\n :meth:`antialiased <matplotlib.collections.Collection.set_antialiased>` or aa or antialiaseds bool or list of bools \\n :meth:`array <matplotlib.cm.ScalarMappable.set_array>` array-like or None \\n :meth:`capstyle <matplotlib.collections.Collection.set_capstyle>` `.CapStyle` or {\\'butt\\', \\'projecting\\', \\'round\\'} \\n :meth:`clim <matplotlib.cm.ScalarMappable.set_clim>` (vmin: float, vmax: float) \\n :meth:`clip_box <matplotlib.artist.Artist.set_clip_box>` `.Bbox` \\n :meth:`clip_on <matplotlib.artist.Artist.set_clip_on>` bool \\n :meth:`clip_path <matplotlib.artist.Artist.set_clip_path>` Patch or (Path, Transform) or None \\n :meth:`cmap <matplotlib.cm.ScalarMappable.set_cmap>` `.Colormap` or str or None \\n :meth:`color <matplotlib.collections.Collection.set_color>` color or list of rgba tuples \\n :meth:`edgecolor <matplotlib.collections.Collection.set_edgecolor>` or ec or edgecolors color or list of colors or \\'face\\' \\n :meth:`facecolor <matplotlib.collections.Collection.set_facecolor>` or facecolors or fc color or list of colors \\n :meth:`figure <matplotlib.artist.Artist.set_figure>` `.Figure` \\n :meth:`gid <matplotlib.artist.Artist.set_gid>` str \\n :meth:`hatch <matplotlib.collections.Collection.set_hatch>` {\\'\/\\', \\'\\\\\\\\\\', \\'|\\', \\'-\\', \\'+\\', \\'x\\', \\'o\\', \\'O\\', \\'.\\', \\'*\\'} \\n :meth:`in_layout <matplotlib.artist.Artist.set_in_layout>` bool \\n :meth:`joinstyle <matplotlib.collections.Collection.set_joinstyle>` `.JoinStyle` or {\\'miter\\', \\'round\\', \\'bevel\\'} \\n :meth:`label <matplotlib.artist.Artist.set_label>` object \\n :meth:`linestyle <matplotlib.collections.Collection.set_linestyle>` or dashes or linestyles or ls str or tuple or list thereof \\n :meth:`linewidth <matplotlib.collections.Collection.set_linewidth>` or linewidths or lw float or list of floats \\n :meth:`norm <matplotlib.cm.ScalarMappable.set_norm>` `.Normalize` or None \\n :meth:`offset_transform <matplotlib.collections.Collection.set_offset_transform>` `.Transform` \\n :meth:`offsets <matplotlib.collections.Collection.set_offsets>` (N, 2) or (2,) array-like \\n :meth:`path_effects <matplotlib.artist.Artist.set_path_effects>` `.AbstractPathEffect` \\n :meth:`paths <matplotlib.collections.PolyCollection.set_verts>` list of array-like \\n :meth:`picker <matplotlib.artist.Artist.set_picker>` None or bool or float or callable \\n :meth:`pickradius <matplotlib.collections.Collection.set_pickradius>` float \\n :meth:`rasterized <matplotlib.artist.Artist.set_rasterized>` bool \\n :meth:`sizes <matplotlib.collections._CollectionWithSizes.set_sizes>` ndarray or None \\n :meth:`sketch_params <matplotlib.artist.Artist.set_sketch_params>` (scale: float, length: float, randomness: float) \\n :meth:`snap <matplotlib.artist.Artist.set_snap>` bool or None \\n :meth:`transform <matplotlib.artist.Artist.set_transform>` `.Transform` \\n :meth:`url <matplotlib.artist.Artist.set_url>` str \\n :meth:`urls <matplotlib.collections.Collection.set_urls>` list of str or None \\n :meth:`verts <matplotlib.collections.PolyCollection.set_verts>` list of array-like \\n :meth:`verts_and_codes <matplotlib.collections.PolyCollection.set_verts_and_codes>` unknown \\n :meth:`visible <matplotlib.artist.Artist.set_visible>` bool \\n :meth:`zorder <matplotlib.artist.Artist.set_zorder>` float \\n ================================================================================================= =====================================================================================================\\n\\n'","title":"matplotlib._as_gen.matplotlib.quiver.barbs#matplotlib.quiver.Barbs.barbs_doc"},{"text":"pandas.DataFrame.plot.area   DataFrame.plot.area(x=None, y=None, **kwargs)[source]\n \nDraw a stacked area plot. An area plot displays quantitative data visually. This function wraps the matplotlib area function.  Parameters \n \nx:label or position, optional\n\n\nCoordinates for the X axis. By default uses the index.  \ny:label or position, optional\n\n\nColumn to plot. By default uses all columns.  \nstacked:bool, default True\n\n\nArea plots are stacked by default. Set to False to create a unstacked plot.  **kwargs\n\nAdditional keyword arguments are documented in DataFrame.plot().    Returns \n matplotlib.axes.Axes or numpy.ndarray\n\nArea plot, or array of area plots if subplots is True.      See also  DataFrame.plot\n\nMake plots of DataFrame using matplotlib \/ pylab.    Examples Draw an area plot based on basic business metrics: \n>>> df = pd.DataFrame({\n...     'sales': [3, 2, 3, 9, 10, 6],\n...     'signups': [5, 5, 6, 12, 14, 13],\n...     'visits': [20, 42, 28, 62, 81, 50],\n... }, index=pd.date_range(start='2018\/01\/01', end='2018\/07\/01',\n...                        freq='M'))\n>>> ax = df.plot.area()\n     Area plots are stacked by default. To produce an unstacked plot, pass stacked=False: \n>>> ax = df.plot.area(stacked=False)\n     Draw an area plot for a single column: \n>>> ax = df.plot.area(y='sales')\n     Draw with a different x: \n>>> df = pd.DataFrame({\n...     'sales': [3, 2, 3],\n...     'visits': [20, 42, 28],\n...     'day': [1, 2, 3],\n... })\n>>> ax = df.plot.area(x='day')","title":"pandas.reference.api.pandas.dataframe.plot.area"},{"text":"pandas.Series.plot.barh   Series.plot.barh(x=None, y=None, **kwargs)[source]\n \nMake a horizontal bar plot. A horizontal bar plot is a plot that presents quantitative data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.  Parameters \n \nx:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, the index of the DataFrame is used.  \ny:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, all numerical columns are used.  \ncolor:str, array-like, or dict, optional\n\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:  \n A single color string referred to by name, RGB or RGBA code,\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.    \n A sequence of color strings referred to by name, RGB or RGBA\n\ncode, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    \n A dict of the form {column name:color}, so that each column will be\n\n\ncolored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and bars for column b in red.      New in version 1.1.0.   **kwargs\n\nAdditional keyword arguments are documented in DataFrame.plot().    Returns \n matplotlib.axes.Axes or np.ndarray of them\n\nAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  DataFrame.plot.bar\n\nVertical bar plot.  DataFrame.plot\n\nMake plots of DataFrame using matplotlib.  matplotlib.axes.Axes.bar\n\nPlot a vertical bar plot using matplotlib.    Examples Basic example \n>>> df = pd.DataFrame({'lab': ['A', 'B', 'C'], 'val': [10, 30, 20]})\n>>> ax = df.plot.barh(x='lab', y='val')\n     Plot a whole DataFrame to a horizontal bar plot \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh()\n     Plot stacked barh charts for the DataFrame \n>>> ax = df.plot.barh(stacked=True)\n     We can specify colors for each column \n>>> ax = df.plot.barh(color={\"speed\": \"red\", \"lifespan\": \"green\"})\n     Plot a column of the DataFrame to a horizontal bar plot \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh(y='speed')\n     Plot DataFrame versus the desired column \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh(x='lifespan')","title":"pandas.reference.api.pandas.series.plot.barh"},{"text":"n_rasterize=50","title":"matplotlib.colorbar_api#matplotlib.colorbar.Colorbar.n_rasterize"},{"text":"pandas.Series.plot.area   Series.plot.area(x=None, y=None, **kwargs)[source]\n \nDraw a stacked area plot. An area plot displays quantitative data visually. This function wraps the matplotlib area function.  Parameters \n \nx:label or position, optional\n\n\nCoordinates for the X axis. By default uses the index.  \ny:label or position, optional\n\n\nColumn to plot. By default uses all columns.  \nstacked:bool, default True\n\n\nArea plots are stacked by default. Set to False to create a unstacked plot.  **kwargs\n\nAdditional keyword arguments are documented in DataFrame.plot().    Returns \n matplotlib.axes.Axes or numpy.ndarray\n\nArea plot, or array of area plots if subplots is True.      See also  DataFrame.plot\n\nMake plots of DataFrame using matplotlib \/ pylab.    Examples Draw an area plot based on basic business metrics: \n>>> df = pd.DataFrame({\n...     'sales': [3, 2, 3, 9, 10, 6],\n...     'signups': [5, 5, 6, 12, 14, 13],\n...     'visits': [20, 42, 28, 62, 81, 50],\n... }, index=pd.date_range(start='2018\/01\/01', end='2018\/07\/01',\n...                        freq='M'))\n>>> ax = df.plot.area()\n     Area plots are stacked by default. To produce an unstacked plot, pass stacked=False: \n>>> ax = df.plot.area(stacked=False)\n     Draw an area plot for a single column: \n>>> ax = df.plot.area(y='sales')\n     Draw with a different x: \n>>> df = pd.DataFrame({\n...     'sales': [3, 2, 3],\n...     'visits': [20, 42, 28],\n...     'day': [1, 2, 3],\n... })\n>>> ax = df.plot.area(x='day')","title":"pandas.reference.api.pandas.series.plot.area"},{"text":"pandas.tseries.offsets.Tick.apply   Tick.apply()","title":"pandas.reference.api.pandas.tseries.offsets.tick.apply"},{"text":"pandas.Timestamp.fold   Timestamp.fold","title":"pandas.reference.api.pandas.timestamp.fold"}]}
{"task_id":35945473,"prompt":"def f_35945473(myDictionary):\n\treturn ","suffix":"","canonical_solution":"{i[1]: i[0] for i in list(myDictionary.items())}","test_start":"\ndef check(candidate):","test":["\n    assert candidate({'a' : 'b', 'c' : 'd'}) == {'b': 'a', 'd': 'c'}\n"],"entry_point":"f_35945473","intent":"reverse the keys and values in a dictionary `myDictionary`","library":[],"docs":[]}
{"task_id":30729735,"prompt":"def f_30729735(myList):\n\treturn ","suffix":"","canonical_solution":"[i for i, j in enumerate(myList) if 'how' in j.lower() or 'what' in j.lower()]","test_start":"\ndef check(candidate):","test":["\n    assert candidate(['abc', 'how', 'what', 'def']) == [1, 2]\n"],"entry_point":"f_30729735","intent":"finding the index of elements containing substring 'how' and 'what' in a list of strings 'myList'.","library":[],"docs":[]}
{"task_id":1303243,"prompt":"def f_1303243(obj):\n\treturn ","suffix":"","canonical_solution":"isinstance(obj, str)","test_start":"\ndef check(candidate):","test":["\n    assert candidate('python3') == True\n","\n    assert candidate(1.23) == False\n"],"entry_point":"f_1303243","intent":"check if object `obj` is a string","library":[],"docs":[]}
{"task_id":1303243,"prompt":"def f_1303243(o):\n\treturn ","suffix":"","canonical_solution":"isinstance(o, str)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"hello\") == True\n","\n    assert candidate(123) == False\n","\n    assert candidate([]) == False\n","\n    assert candidate({\"aa\", \"v\"}) == False\n","\n    assert candidate(\"123\") == True\n"],"entry_point":"f_1303243","intent":"check if object `o` is a string","library":[],"docs":[]}
{"task_id":1303243,"prompt":"def f_1303243(o):\n\treturn ","suffix":"","canonical_solution":"(type(o) is str)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"hello\") == True\n","\n    assert candidate(123) == False\n","\n    assert candidate([]) == False\n","\n    assert candidate({\"aa\", \"v\"}) == False\n","\n    assert candidate(\"123\") == True\n"],"entry_point":"f_1303243","intent":"check if object `o` is a string","library":[],"docs":[]}
{"task_id":1303243,"prompt":"def f_1303243(obj_to_test):\n\treturn ","suffix":"","canonical_solution":"isinstance(obj_to_test, str)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"hello\") == True\n","\n    assert candidate(123) == False\n","\n    assert candidate([]) == False\n","\n    assert candidate({\"aa\", \"v\"}) == False\n","\n    assert candidate(\"123\") == True\n"],"entry_point":"f_1303243","intent":"check if `obj_to_test` is a string","library":[],"docs":[]}
{"task_id":8177079,"prompt":"def f_8177079(list1, list2):\n\t","suffix":"\n\treturn ","canonical_solution":"list2.extend(list1)","test_start":"\ndef check(candidate):","test":["\n    a, b = [1, 2, 3], [4, 5, 6]\n    candidate(a, b)\n    assert b == [4, 5, 6, 1, 2, 3]\n","\n    a, c = [1, 2, 3], [7, 8, 9]\n    candidate(a, c)\n    assert c == [7, 8, 9, 1, 2, 3] \n","\n    b = [4, 5, 6, 1, 2, 3]\n    c = [7, 8, 9, 1, 2, 3] \n    candidate(b, c)\n    assert c == [7, 8, 9, 1, 2, 3, 4, 5, 6, 1, 2, 3]\n"],"entry_point":"f_8177079","intent":"append list `list1` to `list2`","library":[],"docs":[]}
{"task_id":8177079,"prompt":"def f_8177079(mylog, list1):\n\t","suffix":"\n\treturn ","canonical_solution":"list1.extend(mylog)","test_start":"\ndef check(candidate):","test":["\n    a, b = [1, 2, 3], [4, 5, 6]\n    candidate(a, b)\n    assert b == [4, 5, 6, 1, 2, 3]\n","\n    a, c = [1, 2, 3], [7, 8, 9]\n    candidate(a, c)\n    assert c == [7, 8, 9, 1, 2, 3] \n","\n    b = [4, 5, 6, 1, 2, 3]\n    c = [7, 8, 9, 1, 2, 3] \n    candidate(b, c)\n    assert c == [7, 8, 9, 1, 2, 3, 4, 5, 6, 1, 2, 3]\n"],"entry_point":"f_8177079","intent":"append list `mylog` to `list1`","library":[],"docs":[]}
{"task_id":8177079,"prompt":"def f_8177079(a, c):\n\t","suffix":"\n\treturn ","canonical_solution":"c.extend(a)","test_start":"\ndef check(candidate):","test":["\n    a, b = [1, 2, 3], [4, 5, 6]\n    candidate(a, b)\n    assert b == [4, 5, 6, 1, 2, 3]\n","\n    a, c = [1, 2, 3], [7, 8, 9]\n    candidate(a, c)\n    assert c == [7, 8, 9, 1, 2, 3] \n","\n    b = [4, 5, 6, 1, 2, 3]\n    c = [7, 8, 9, 1, 2, 3] \n    candidate(b, c)\n    assert c == [7, 8, 9, 1, 2, 3, 4, 5, 6, 1, 2, 3]\n"],"entry_point":"f_8177079","intent":"append list `a` to `c`","library":[],"docs":[]}
{"task_id":8177079,"prompt":"def f_8177079(mylog, list1):\n\t","suffix":"\n\treturn ","canonical_solution":"for line in mylog:\n\t    list1.append(line)","test_start":"\ndef check(candidate):","test":["\n    a, b = [1, 2, 3], [4, 5, 6]\n    candidate(a, b)\n    assert b == [4, 5, 6, 1, 2, 3]\n","\n    a, c = [1, 2, 3], [7, 8, 9]\n    candidate(a, c)\n    assert c == [7, 8, 9, 1, 2, 3] \n","\n    b = [4, 5, 6, 1, 2, 3]\n    c = [7, 8, 9, 1, 2, 3] \n    candidate(b, c)\n    assert c == [7, 8, 9, 1, 2, 3, 4, 5, 6, 1, 2, 3]\n"],"entry_point":"f_8177079","intent":"append items in list `mylog` to `list1`","library":[],"docs":[]}
{"task_id":4126227,"prompt":"def f_4126227(a, b):\n\t","suffix":"\n\treturn ","canonical_solution":"b.append((a[0][0], a[0][2]))","test_start":"\ndef check(candidate):","test":["\n    a = [(1,2,3),(4,5,6)]\n    b = [(0,0)]\n    candidate(a, b)\n    assert(b == [(0, 0), (1, 3)])\n"],"entry_point":"f_4126227","intent":"append a tuple of elements from list `a` with indexes '[0][0] [0][2]' to list `b`","library":[],"docs":[]}
{"task_id":34902378,"prompt":"def f_34902378(app):\n\t","suffix":"\n\treturn ","canonical_solution":"app.config['SECRET_KEY'] = 'Your_secret_string'","test_start":"\nfrom flask import Flask\n\ndef check(candidate):","test":["\n    app = Flask(\"test\")\n    candidate(app)\n    assert app.config['SECRET_KEY'] == 'Your_secret_string'\n"],"entry_point":"f_34902378","intent":"Initialize `SECRET_KEY` in flask config with `Your_secret_string `","library":["flask"],"docs":[{"text":"SECRET_KEY  \nA secret key that will be used for securely signing the session cookie and can be used for any other security related needs by extensions or your application. It should be a long random bytes or str. For example, copy the output of this to your config: $ python -c 'import os; print(os.urandom(16))'\nb'_5#y2L\"F4Q8z\\n\\xec]\/'\n Do not reveal the secret key when posting questions or committing code. Default: None","title":"flask.config.index#SECRET_KEY"},{"text":"secret_key  \nIf a secret key is set, cryptographic components can use this to sign cookies and other things. Set this to a complex random value when you want to use the secure cookie for instance. This attribute can also be configured from the config with the SECRET_KEY configuration key. Defaults to None.","title":"flask.api.index#flask.Flask.secret_key"},{"text":"SESSION_COOKIE_HTTPONLY  \nBrowsers will not allow JavaScript access to cookies marked as \u201cHTTP only\u201d for security. Default: True","title":"flask.config.index#SESSION_COOKIE_HTTPONLY"},{"text":"config_class  \nalias of flask.config.Config","title":"flask.api.index#flask.Flask.config_class"},{"text":"class flask.Config(root_path, defaults=None)  \nWorks exactly like a dict but provides ways to fill it from files or special dictionaries. There are two common patterns to populate the config. Either you can fill the config from a config file: app.config.from_pyfile('yourconfig.cfg')\n Or alternatively you can define the configuration options in the module that calls from_object() or provide an import path to a module that should be loaded. It is also possible to tell it to use the same module and with that provide the configuration values just before the call: DEBUG = True\nSECRET_KEY = 'development key'\napp.config.from_object(__name__)\n In both cases (loading from any Python file or loading from modules), only uppercase keys are added to the config. This makes it possible to use lowercase values in the config file for temporary values that are not added to the config or to define the config keys in the same file that implements the application. Probably the most interesting way to load configurations is from an environment variable pointing to a file: app.config.from_envvar('YOURAPPLICATION_SETTINGS')\n In this case before launching the application you have to set this environment variable to the file you want to use. On Linux and OS X use the export statement: export YOURAPPLICATION_SETTINGS='\/path\/to\/config\/file'\n On windows use set instead.  Parameters \n \nroot_path (str) \u2013 path to which files are read relative from. When the config object is created by the application, this is the application\u2019s root_path. \ndefaults (Optional[dict]) \u2013 an optional dictionary of default values   Return type \nNone    \nfrom_envvar(variable_name, silent=False)  \nLoads a configuration from an environment variable pointing to a configuration file. This is basically just a shortcut with nicer error messages for this line of code: app.config.from_pyfile(os.environ['YOURAPPLICATION_SETTINGS'])\n  Parameters \n \nvariable_name (str) \u2013 name of the environment variable \nsilent (bool) \u2013 set to True if you want silent failure for missing files.   Returns \nbool. True if able to load config, False otherwise.  Return type \nbool   \n  \nfrom_file(filename, load, silent=False)  \nUpdate the values in the config from a file that is loaded using the load parameter. The loaded data is passed to the from_mapping() method. import toml\napp.config.from_file(\"config.toml\", load=toml.load)\n  Parameters \n \nfilename (str) \u2013 The path to the data file. This can be an absolute path or relative to the config root path. \nload (Callable[[Reader], Mapping] where Reader implements a read method.) \u2013 A callable that takes a file handle and returns a mapping of loaded data from the file. \nsilent (bool) \u2013 Ignore the file if it doesn\u2019t exist.   Return type \nbool    New in version 2.0.  \n  \nfrom_mapping(mapping=None, **kwargs)  \nUpdates the config like update() ignoring items with non-upper keys.  Changelog New in version 0.11.   Parameters \n \nmapping (Optional[Mapping[str, Any]]) \u2013  \nkwargs (Any) \u2013    Return type \nbool   \n  \nfrom_object(obj)  \nUpdates the values from the given object. An object can be of one of the following two types:  a string: in this case the object with that name will be imported an actual object reference: that object is used directly  Objects are usually either modules or classes. from_object() loads only the uppercase attributes of the module\/class. A dict object will not work with from_object() because the keys of a dict are not attributes of the dict class. Example of module-based configuration: app.config.from_object('yourapplication.default_config')\nfrom yourapplication import default_config\napp.config.from_object(default_config)\n Nothing is done to the object before loading. If the object is a class and has @property attributes, it needs to be instantiated before being passed to this method. You should not use this function to load the actual configuration but rather configuration defaults. The actual config should be loaded with from_pyfile() and ideally from a location not within the package because the package might be installed system wide. See Development \/ Production for an example of class-based configuration using from_object().  Parameters \nobj (Union[object, str]) \u2013 an import name or object  Return type \nNone   \n  \nfrom_pyfile(filename, silent=False)  \nUpdates the values in the config from a Python file. This function behaves as if the file was imported as module with the from_object() function.  Parameters \n \nfilename (str) \u2013 the filename of the config. This can either be an absolute filename or a filename relative to the root path. \nsilent (bool) \u2013 set to True if you want silent failure for missing files.   Return type \nbool    Changelog New in version 0.7: silent parameter.  \n\n  \nget_namespace(namespace, lowercase=True, trim_namespace=True)  \nReturns a dictionary containing a subset of configuration options that match the specified namespace\/prefix. Example usage: app.config['IMAGE_STORE_TYPE'] = 'fs'\napp.config['IMAGE_STORE_PATH'] = '\/var\/app\/images'\napp.config['IMAGE_STORE_BASE_URL'] = 'http:\/\/img.website.com'\nimage_store_config = app.config.get_namespace('IMAGE_STORE_')\n The resulting dictionary image_store_config would look like: {\n    'type': 'fs',\n    'path': '\/var\/app\/images',\n    'base_url': 'http:\/\/img.website.com'\n}\n This is often useful when configuration options map directly to keyword arguments in functions or class constructors.  Parameters \n \nnamespace (str) \u2013 a configuration namespace \nlowercase (bool) \u2013 a flag indicating if the keys of the resulting dictionary should be lowercase \ntrim_namespace (bool) \u2013 a flag indicating if the keys of the resulting dictionary should not include the namespace   Return type \nDict[str, Any]    Changelog New in version 0.11.","title":"flask.api.index#flask.Config"},{"text":"setdefault(key, default=None)  \nInsert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default.  Parameters \n \nkey (str) \u2013  \ndefault (Optional[Any]) \u2013    Return type \nAny","title":"flask.api.index#flask.sessions.SecureCookieSession.setdefault"},{"text":"get(key, default=None)  \nReturn the value for key if key is in the dictionary, else default.  Parameters \n \nkey (str) \u2013  \ndefault (Optional[Any]) \u2013    Return type \nAny","title":"flask.api.index#flask.sessions.SecureCookieSession.get"},{"text":"app  \na reference to the current application","title":"flask.api.index#flask.blueprints.BlueprintSetupState.app"},{"text":"MAX_CONTENT_LENGTH  \nDon\u2019t read more than this many bytes from the incoming request data. If not set and the request does not specify a CONTENT_LENGTH, no data will be read for security. Default: None","title":"flask.config.index#MAX_CONTENT_LENGTH"},{"text":"jinja_environment  \nalias of flask.templating.Environment","title":"flask.api.index#flask.Flask.jinja_environment"}]}
{"task_id":22799300,"prompt":"def f_22799300(out):\n\treturn ","suffix":"","canonical_solution":"pd.DataFrame(out.tolist(), columns=['out-1', 'out-2'], index=out.index)","test_start":"\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\ndef check(candidate):","test":["\n    df = pd.DataFrame(dict(x=np.random.randn(100), y=np.repeat(list(\"abcd\"), 25)))\n    out = df.groupby(\"y\").x.apply(stats.ttest_1samp, 0)\n    test = pd.DataFrame(out.tolist())\n    test.columns = ['out-1', 'out-2']\n    test.index = out.index\n    res = candidate(out)\n    assert(test.equals(res))\n"],"entry_point":"f_22799300","intent":"unpack a series of tuples in pandas `out` into a DataFrame with column names 'out-1' and 'out-2'","library":["numpy","pandas","scipy"],"docs":[{"text":"pandas.tseries.offsets.BYearEnd.normalize   BYearEnd.normalize","title":"pandas.reference.api.pandas.tseries.offsets.byearend.normalize"},{"text":"pandas.tseries.offsets.BYearBegin.normalize   BYearBegin.normalize","title":"pandas.reference.api.pandas.tseries.offsets.byearbegin.normalize"},{"text":"pandas.Timestamp.fold   Timestamp.fold","title":"pandas.reference.api.pandas.timestamp.fold"},{"text":"pandas.tseries.offsets.SemiMonthEnd.normalize   SemiMonthEnd.normalize","title":"pandas.reference.api.pandas.tseries.offsets.semimonthend.normalize"},{"text":"pandas.tseries.offsets.BYearEnd.n   BYearEnd.n","title":"pandas.reference.api.pandas.tseries.offsets.byearend.n"},{"text":"pandas.tseries.offsets.BYearEnd.freqstr   BYearEnd.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.byearend.freqstr"},{"text":"pandas.Series.T   propertySeries.T\n \nReturn the transpose, which is by definition self.","title":"pandas.reference.api.pandas.series.t"},{"text":"pandas.tseries.offsets.Second.name   Second.name","title":"pandas.reference.api.pandas.tseries.offsets.second.name"},{"text":"pandas.tseries.offsets.BYearEnd.name   BYearEnd.name","title":"pandas.reference.api.pandas.tseries.offsets.byearend.name"},{"text":"pandas.MultiIndex.from_tuples   classmethodMultiIndex.from_tuples(tuples, sortorder=None, names=None)[source]\n \nConvert list of tuples to MultiIndex.  Parameters \n \ntuples:list \/ sequence of tuple-likes\n\n\nEach tuple is the index of one row\/column.  \nsortorder:int or None\n\n\nLevel of sortedness (must be lexicographically sorted by that level).  \nnames:list \/ sequence of str, optional\n\n\nNames for the levels in the index.    Returns \n MultiIndex\n    See also  MultiIndex.from_arrays\n\nConvert list of arrays to MultiIndex.  MultiIndex.from_product\n\nMake a MultiIndex from cartesian product of iterables.  MultiIndex.from_frame\n\nMake a MultiIndex from a DataFrame.    Examples \n>>> tuples = [(1, 'red'), (1, 'blue'),\n...           (2, 'red'), (2, 'blue')]\n>>> pd.MultiIndex.from_tuples(tuples, names=('number', 'color'))\nMultiIndex([(1,  'red'),\n            (1, 'blue'),\n            (2,  'red'),\n            (2, 'blue')],\n           names=['number', 'color'])","title":"pandas.reference.api.pandas.multiindex.from_tuples"}]}
{"task_id":1762484,"prompt":"def f_1762484(stocks_list):\n\treturn ","suffix":"","canonical_solution":"[x for x in range(len(stocks_list)) if stocks_list[x] == 'MSFT']","test_start":"\ndef check(candidate):","test":["\n    stocks_list = ['AAPL', 'MSFT', 'GOOG', 'MSFT', 'MSFT']\n    assert(candidate(stocks_list) == [1,3,4])\n","\n    stocks_list = ['AAPL', 'MSXT', 'GOOG', 'MSAT', 'SFT']\n    assert(candidate(stocks_list) == [])\n"],"entry_point":"f_1762484","intent":"find the index of an element 'MSFT' in a list `stocks_list`","library":[],"docs":[]}
{"task_id":3464359,"prompt":"def f_3464359(ax, labels):\n\treturn ","suffix":"","canonical_solution":"ax.set_xticklabels(labels, rotation=45)","test_start":"\nimport matplotlib.pyplot as plt \n\ndef check(candidate):","test":["\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3, 4], [1, 4, 2, 3])\n    ret = candidate(ax, [f\"#{i}\" for i in range(7)])\n    assert [tt.get_rotation() == 45.0 for tt in ret]\n"],"entry_point":"f_3464359","intent":"rotate the xtick `labels` of matplotlib plot `ax` by `45` degrees to make long labels readable","library":["matplotlib"],"docs":[{"text":"get_rotate_label(text)[source]","title":"matplotlib._as_gen.mpl_toolkits.mplot3d.axis3d.axis#mpl_toolkits.mplot3d.axis3d.Axis.get_rotate_label"},{"text":"matplotlib.axis.Axis.set_label_coords   Axis.set_label_coords(x, y, transform=None)[source]\n \nSet the coordinates of the label. By default, the x coordinate of the y label and the y coordinate of the x label are determined by the tick label bounding boxes, but this can lead to poor alignment of multiple labels if there are multiple axes. You can also specify the coordinate system of the label with the transform. If None, the default coordinate system will be the axes coordinate system: (0, 0) is bottom left, (0.5, 0.5) is center, etc.","title":"matplotlib._as_gen.matplotlib.axis.axis.set_label_coords"},{"text":"matplotlib.pyplot.xticks   matplotlib.pyplot.xticks(ticks=None, labels=None, **kwargs)[source]\n \nGet or set the current tick locations and labels of the x-axis. Pass no arguments to return the current values without modifying them.  Parameters \n \nticksarray-like, optional\n\n\nThe list of xtick locations. Passing an empty list removes all xticks.  \nlabelsarray-like, optional\n\n\nThe labels to place at the given ticks locations. This argument can only be passed if ticks is passed as well.  **kwargs\n\nText properties can be used to control the appearance of the labels.    Returns \n locs\n\nThe list of xtick locations.  labels\n\nThe list of xlabel Text objects.     Notes Calling this function with no arguments (e.g. xticks()) is the pyplot equivalent of calling get_xticks and get_xticklabels on the current axes. Calling this function with arguments is the pyplot equivalent of calling set_xticks and set_xticklabels on the current axes. Examples >>> locs, labels = xticks()  # Get the current locations and labels.\n>>> xticks(np.arange(0, 1, step=0.2))  # Set label locations.\n>>> xticks(np.arange(3), ['Tom', 'Dick', 'Sue'])  # Set text labels.\n>>> xticks([0, 1, 2], ['January', 'February', 'March'],\n...        rotation=20)  # Set text labels and properties.\n>>> xticks([])  # Disable xticks.\n \n  Examples using matplotlib.pyplot.xticks\n \n   Secondary Axis   \n\n   Table Demo   \n\n   Rotating custom tick labels","title":"matplotlib._as_gen.matplotlib.pyplot.xticks"},{"text":"widths","title":"matplotlib.dviread#matplotlib.dviread.DviFont.widths"},{"text":"align_xlabels(axs=None)[source]\n \nAlign the xlabels of subplots in the same subplot column if label alignment is being done automatically (i.e. the label position is not manually set). Alignment persists for draw events after this is called. If a label is on the bottom, it is aligned with labels on Axes that also have their label on the bottom and that have the same bottom-most subplot row. If the label is on the top, it is aligned with labels on Axes with the same top-most row.  Parameters \n \naxslist of Axes\n\n\nOptional list of (or ndarray) Axes to align the xlabels. Default is to align all Axes on the figure.      See also  matplotlib.figure.Figure.align_ylabels\nmatplotlib.figure.Figure.align_labels\n  Notes This assumes that axs are from the same GridSpec, so that their SubplotSpec positions correspond to figure positions. Examples Example with rotated xtick labels: fig, axs = plt.subplots(1, 2)\nfor tick in axs[0].get_xticklabels():\n    tick.set_rotation(55)\naxs[0].set_xlabel('XLabel 0')\naxs[1].set_xlabel('XLabel 1')\nfig.align_xlabels()","title":"matplotlib.figure_api#matplotlib.figure.FigureBase.align_xlabels"},{"text":"align_xlabels(axs=None)[source]\n \nAlign the xlabels of subplots in the same subplot column if label alignment is being done automatically (i.e. the label position is not manually set). Alignment persists for draw events after this is called. If a label is on the bottom, it is aligned with labels on Axes that also have their label on the bottom and that have the same bottom-most subplot row. If the label is on the top, it is aligned with labels on Axes with the same top-most row.  Parameters \n \naxslist of Axes\n\n\nOptional list of (or ndarray) Axes to align the xlabels. Default is to align all Axes on the figure.      See also  matplotlib.figure.Figure.align_ylabels\nmatplotlib.figure.Figure.align_labels\n  Notes This assumes that axs are from the same GridSpec, so that their SubplotSpec positions correspond to figure positions. Examples Example with rotated xtick labels: fig, axs = plt.subplots(1, 2)\nfor tick in axs[0].get_xticklabels():\n    tick.set_rotation(55)\naxs[0].set_xlabel('XLabel 0')\naxs[1].set_xlabel('XLabel 1')\nfig.align_xlabels()","title":"matplotlib.figure_api#matplotlib.figure.SubFigure.align_xlabels"},{"text":"align_xlabels(axs=None)[source]\n \nAlign the xlabels of subplots in the same subplot column if label alignment is being done automatically (i.e. the label position is not manually set). Alignment persists for draw events after this is called. If a label is on the bottom, it is aligned with labels on Axes that also have their label on the bottom and that have the same bottom-most subplot row. If the label is on the top, it is aligned with labels on Axes with the same top-most row.  Parameters \n \naxslist of Axes\n\n\nOptional list of (or ndarray) Axes to align the xlabels. Default is to align all Axes on the figure.      See also  matplotlib.figure.Figure.align_ylabels\nmatplotlib.figure.Figure.align_labels\n  Notes This assumes that axs are from the same GridSpec, so that their SubplotSpec positions correspond to figure positions. Examples Example with rotated xtick labels: fig, axs = plt.subplots(1, 2)\nfor tick in axs[0].get_xticklabels():\n    tick.set_rotation(55)\naxs[0].set_xlabel('XLabel 0')\naxs[1].set_xlabel('XLabel 1')\nfig.align_xlabels()","title":"matplotlib.figure_api#matplotlib.figure.Figure.align_xlabels"},{"text":"autofmt_xdate(bottom=0.2, rotation=30, ha='right', which='major')[source]\n \nDate ticklabels often overlap, so it is useful to rotate them and right align them. Also, a common use case is a number of subplots with shared x-axis where the x-axis is date data. The ticklabels are often long, and it helps to rotate them on the bottom subplot and turn them off on other subplots, as well as turn off xlabels.  Parameters \n \nbottomfloat, default: 0.2\n\n\nThe bottom of the subplots for subplots_adjust.  \nrotationfloat, default: 30 degrees\n\n\nThe rotation angle of the xtick labels in degrees.  \nha{'left', 'center', 'right'}, default: 'right'\n\n\nThe horizontal alignment of the xticklabels.  \nwhich{'major', 'minor', 'both'}, default: 'major'\n\n\nSelects which ticklabels to rotate.","title":"matplotlib.figure_api#matplotlib.figure.SubFigure.autofmt_xdate"},{"text":"matplotlib.axis.Tick.set_label2   Tick.set_label2(s)[source]\n \nSet the label2 text.  Parameters \n \nsstr","title":"matplotlib._as_gen.matplotlib.axis.tick.set_label2"},{"text":"autofmt_xdate(bottom=0.2, rotation=30, ha='right', which='major')[source]\n \nDate ticklabels often overlap, so it is useful to rotate them and right align them. Also, a common use case is a number of subplots with shared x-axis where the x-axis is date data. The ticklabels are often long, and it helps to rotate them on the bottom subplot and turn them off on other subplots, as well as turn off xlabels.  Parameters \n \nbottomfloat, default: 0.2\n\n\nThe bottom of the subplots for subplots_adjust.  \nrotationfloat, default: 30 degrees\n\n\nThe rotation angle of the xtick labels in degrees.  \nha{'left', 'center', 'right'}, default: 'right'\n\n\nThe horizontal alignment of the xticklabels.  \nwhich{'major', 'minor', 'both'}, default: 'major'\n\n\nSelects which ticklabels to rotate.","title":"matplotlib.figure_api#matplotlib.figure.Figure.autofmt_xdate"}]}
{"task_id":875968,"prompt":"def f_875968(s):\n\treturn ","suffix":"","canonical_solution":"re.sub('[^\\\\w]', ' ', s)","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    s = \"how much for the maple syrup? $20.99? That's ridiculous!!!\"\n    assert candidate(s) == 'how much for the maple syrup   20 99  That s ridiculous   '\n"],"entry_point":"f_875968","intent":"remove symbols from a string `s`","library":["re"],"docs":[{"text":"str.rstrip([chars])  \nReturn a copy of the string with trailing characters removed. The chars argument is a string specifying the set of characters to be removed. If omitted or None, the chars argument defaults to removing whitespace. The chars argument is not a suffix; rather, all combinations of its values are stripped: >>> '   spacious   '.rstrip()\n'   spacious'\n>>> 'mississippi'.rstrip('ipz')\n'mississ'\n See str.removesuffix() for a method that will remove a single suffix string rather than all of a set of characters. For example: >>> 'Monty Python'.rstrip(' Python')\n'M'\n>>> 'Monty Python'.removesuffix(' Python')\n'Monty'","title":"python.library.stdtypes#str.rstrip"},{"text":"re.purge()  \nClear the regular expression cache.","title":"python.library.re#re.purge"},{"text":"window.delch([y, x])  \nDelete any character at (y, x).","title":"python.library.curses#curses.window.delch"},{"text":"str.strip([chars])  \nReturn a copy of the string with the leading and trailing characters removed. The chars argument is a string specifying the set of characters to be removed. If omitted or None, the chars argument defaults to removing whitespace. The chars argument is not a prefix or suffix; rather, all combinations of its values are stripped: >>> '   spacious   '.strip()\n'spacious'\n>>> 'www.example.com'.strip('cmowz.')\n'example'\n The outermost leading and trailing chars argument values are stripped from the string. Characters are removed from the leading end until reaching a string character that is not contained in the set of characters in chars. A similar action takes place on the trailing end. For example: >>> comment_string = '#....... Section 3.2.1 Issue #32 .......'\n>>> comment_string.strip('.#! ')\n'Section 3.2.1 Issue #32'","title":"python.library.stdtypes#str.strip"},{"text":"str.lstrip([chars])  \nReturn a copy of the string with leading characters removed. The chars argument is a string specifying the set of characters to be removed. If omitted or None, the chars argument defaults to removing whitespace. The chars argument is not a prefix; rather, all combinations of its values are stripped: >>> '   spacious   '.lstrip()\n'spacious   '\n>>> 'www.example.com'.lstrip('cmowz.')\n'example.com'\n See str.removeprefix() for a method that will remove a single prefix string rather than all of a set of characters. For example: >>> 'Arthur: three!'.lstrip('Arthur: ')\n'ee!'\n>>> 'Arthur: three!'.removeprefix('Arthur: ')\n'three!'","title":"python.library.stdtypes#str.lstrip"},{"text":"token.SLASH  \nToken value for \"\/\".","title":"python.library.token#token.SLASH"},{"text":"winreg.REG_SZ  \nA null-terminated string.","title":"python.library.winreg#winreg.REG_SZ"},{"text":"token.LESS  \nToken value for \"<\".","title":"python.library.token#token.LESS"},{"text":"clear()  \nRemove all items from the dictionary.","title":"python.library.stdtypes#dict.clear"},{"text":"token.DOUBLESLASH  \nToken value for \"\/\/\".","title":"python.library.token#token.DOUBLESLASH"}]}
{"task_id":34750084,"prompt":"def f_34750084(s):\n\treturn ","suffix":"","canonical_solution":"re.findall(\"'\\\\\\\\[0-7]{1,3}'\", s)","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate(r\"char x = '\\077';\") == [\"'\\\\077'\"]\n"],"entry_point":"f_34750084","intent":"Find octal characters matches from a string `s` using regex","library":["re"],"docs":[{"text":"pattern  \nThe regular expression pattern.","title":"python.library.re#re.error.pattern"},{"text":"string.octdigits  \nThe string '01234567'.","title":"python.library.string#string.octdigits"},{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"re.S  \nre.DOTALL  \nMake the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).","title":"python.library.re#re.S"},{"text":"str.rindex(sub[, start[, end]])  \nLike rfind() but raises ValueError when the substring sub is not found.","title":"python.library.stdtypes#str.rindex"},{"text":"Pattern.match(string[, pos[, endpos]])  \nIf zero or more characters at the beginning of string match this regular expression, return a corresponding match object. Return None if the string does not match the pattern; note that this is different from a zero-length match. The optional pos and endpos parameters have the same meaning as for the search() method. >>> pattern = re.compile(\"o\")\n>>> pattern.match(\"dog\")      # No match as \"o\" is not at the start of \"dog\".\n>>> pattern.match(\"dog\", 1)   # Match as \"o\" is the 2nd character of \"dog\".\n<re.Match object; span=(1, 2), match='o'>\n If you want to locate a match anywhere in string, use search() instead (see also search() vs. match()).","title":"python.library.re#re.Pattern.match"},{"text":"Pattern.search(string[, pos[, endpos]])  \nScan through string looking for the first location where this regular expression produces a match, and return a corresponding match object. Return None if no position in the string matches the pattern; note that this is different from finding a zero-length match at some point in the string. The optional second parameter pos gives an index in the string where the search is to start; it defaults to 0. This is not completely equivalent to slicing the string; the '^' pattern character matches at the real beginning of the string and at positions just after a newline, but not necessarily at the index where the search is to start. The optional parameter endpos limits how far the string will be searched; it will be as if the string is endpos characters long, so only the characters from pos to endpos - 1 will be searched for a match. If endpos is less than pos, no match will be found; otherwise, if rx is a compiled regular expression object, rx.search(string, 0, 50) is equivalent to rx.search(string[:50], 0). >>> pattern = re.compile(\"d\")\n>>> pattern.search(\"dog\")     # Match at index 0\n<re.Match object; span=(0, 1), match='d'>\n>>> pattern.search(\"dog\", 1)  # No match; search doesn't include the \"d\"","title":"python.library.re#re.Pattern.search"},{"text":"re.search(pattern, string, flags=0)  \nScan through string looking for the first location where the regular expression pattern produces a match, and return a corresponding match object. Return None if no position in the string matches the pattern; note that this is different from finding a zero-length match at some point in the string.","title":"python.library.re#re.search"},{"text":"fnmatch.translate(pattern)  \nReturn the shell-style pattern converted to a regular expression for using with re.match(). Example: >>> import fnmatch, re\n>>>\n>>> regex = fnmatch.translate('*.txt')\n>>> regex\n'(?s:.*\\\\.txt)\\\\Z'\n>>> reobj = re.compile(regex)\n>>> reobj.match('foobar.txt')\n<re.Match object; span=(0, 10), match='foobar.txt'>","title":"python.library.fnmatch#fnmatch.translate"},{"text":"locale.THOUSEP  \nGet the separator character for thousands (groups of three digits).","title":"python.library.locale#locale.THOUSEP"}]}
{"task_id":13209288,"prompt":"def f_13209288(input):\n\treturn ","suffix":"","canonical_solution":"re.split(r'[ ](?=[A-Z]+\\b)', input)","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate('HELLO there HOW are YOU') == ['HELLO there', 'HOW are', 'YOU']\n","\n    assert candidate('hELLO there HoW are YOU') == ['hELLO there HoW are', 'YOU']\n","\n    assert candidate('7 is a NUMBER') == ['7 is a', 'NUMBER']\n","\n    assert candidate('NUMBER 7') == ['NUMBER 7']\n"],"entry_point":"f_13209288","intent":"split string `input` based on occurrences of regex pattern '[ ](?=[A-Z]+\\\\b)'","library":["re"],"docs":[{"text":"Pattern.split(string, maxsplit=0)  \nIdentical to the split() function, using the compiled pattern.","title":"python.library.re#re.Pattern.split"},{"text":"re.split(pattern, string, maxsplit=0, flags=0)  \nSplit string by the occurrences of pattern. If capturing parentheses are used in pattern, then the text of all groups in the pattern are also returned as part of the resulting list. If maxsplit is nonzero, at most maxsplit splits occur, and the remainder of the string is returned as the final element of the list. >>> re.split(r'\\W+', 'Words, words, words.')\n['Words', 'words', 'words', '']\n>>> re.split(r'(\\W+)', 'Words, words, words.')\n['Words', ', ', 'words', ', ', 'words', '.', '']\n>>> re.split(r'\\W+', 'Words, words, words.', 1)\n['Words', 'words, words.']\n>>> re.split('[a-f]+', '0a3B9', flags=re.IGNORECASE)\n['0', '3', '9']\n If there are capturing groups in the separator and it matches at the start of the string, the result will start with an empty string. The same holds for the end of the string: >>> re.split(r'(\\W+)', '...words, words...')\n['', '...', 'words', ', ', 'words', '...', '']\n That way, separator components are always found at the same relative indices within the result list. Empty matches for the pattern split the string only when not adjacent to a previous empty match. >>> re.split(r'\\b', 'Words, words, words.')\n['', 'Words', ', ', 'words', ', ', 'words', '.']\n>>> re.split(r'\\W*', '...words...')\n['', '', 'w', 'o', 'r', 'd', 's', '', '']\n>>> re.split(r'(\\W*)', '...words...')\n['', '...', '', '', 'w', '', 'o', '', 'r', '', 'd', '', 's', '...', '', '', '']\n  Changed in version 3.1: Added the optional flags argument.   Changed in version 3.7: Added support of splitting on a pattern that could match an empty string.","title":"python.library.re#re.split"},{"text":"pattern  \nThe regular expression pattern.","title":"python.library.re#re.error.pattern"},{"text":"re.purge()  \nClear the regular expression cache.","title":"python.library.re#re.purge"},{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"Pattern.search(string[, pos[, endpos]])  \nScan through string looking for the first location where this regular expression produces a match, and return a corresponding match object. Return None if no position in the string matches the pattern; note that this is different from finding a zero-length match at some point in the string. The optional second parameter pos gives an index in the string where the search is to start; it defaults to 0. This is not completely equivalent to slicing the string; the '^' pattern character matches at the real beginning of the string and at positions just after a newline, but not necessarily at the index where the search is to start. The optional parameter endpos limits how far the string will be searched; it will be as if the string is endpos characters long, so only the characters from pos to endpos - 1 will be searched for a match. If endpos is less than pos, no match will be found; otherwise, if rx is a compiled regular expression object, rx.search(string, 0, 50) is equivalent to rx.search(string[:50], 0). >>> pattern = re.compile(\"d\")\n>>> pattern.search(\"dog\")     # Match at index 0\n<re.Match object; span=(0, 1), match='d'>\n>>> pattern.search(\"dog\", 1)  # No match; search doesn't include the \"d\"","title":"python.library.re#re.Pattern.search"},{"text":"fnmatch.translate(pattern)  \nReturn the shell-style pattern converted to a regular expression for using with re.match(). Example: >>> import fnmatch, re\n>>>\n>>> regex = fnmatch.translate('*.txt')\n>>> regex\n'(?s:.*\\\\.txt)\\\\Z'\n>>> reobj = re.compile(regex)\n>>> reobj.match('foobar.txt')\n<re.Match object; span=(0, 10), match='foobar.txt'>","title":"python.library.fnmatch#fnmatch.translate"},{"text":"str.rindex(sub[, start[, end]])  \nLike rfind() but raises ValueError when the substring sub is not found.","title":"python.library.stdtypes#str.rindex"},{"text":"str.split(sep=None, maxsplit=-1)  \nReturn a list of the words in the string, using sep as the delimiter string. If maxsplit is given, at most maxsplit splits are done (thus, the list will have at most maxsplit+1 elements). If maxsplit is not specified or -1, then there is no limit on the number of splits (all possible splits are made). If sep is given, consecutive delimiters are not grouped together and are deemed to delimit empty strings (for example, '1,,2'.split(',') returns ['1', '', '2']). The sep argument may consist of multiple characters (for example, '1<>2<>3'.split('<>') returns ['1', '2', '3']). Splitting an empty string with a specified separator returns ['']. For example: >>> '1,2,3'.split(',')\n['1', '2', '3']\n>>> '1,2,3'.split(',', maxsplit=1)\n['1', '2,3']\n>>> '1,2,,3,'.split(',')\n['1', '2', '', '3', '']\n If sep is not specified or is None, a different splitting algorithm is applied: runs of consecutive whitespace are regarded as a single separator, and the result will contain no empty strings at the start or end if the string has leading or trailing whitespace. Consequently, splitting an empty string or a string consisting of just whitespace with a None separator returns []. For example: >>> '1 2 3'.split()\n['1', '2', '3']\n>>> '1 2 3'.split(maxsplit=1)\n['1', '2 3']\n>>> '   1   2   3   '.split()\n['1', '2', '3']","title":"python.library.stdtypes#str.split"},{"text":"winreg.REG_SZ  \nA null-terminated string.","title":"python.library.winreg#winreg.REG_SZ"}]}
{"task_id":13209288,"prompt":"def f_13209288(input):\n\treturn ","suffix":"","canonical_solution":"re.split('[ ](?=[A-Z])', input)","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate('HELLO there HOW are YOU') == ['HELLO there', 'HOW are', 'YOU']\n","\n    assert candidate('hELLO there HoW are YOU') == ['hELLO there', 'HoW are', 'YOU']\n","\n    assert candidate('7 is a NUMBER') == ['7 is a', 'NUMBER']\n","\n    assert candidate('NUMBER 7') == ['NUMBER 7']\n"],"entry_point":"f_13209288","intent":"Split string `input` at every space followed by an upper-case letter","library":["re"],"docs":[{"text":"re.split(pattern, string, maxsplit=0, flags=0)  \nSplit string by the occurrences of pattern. If capturing parentheses are used in pattern, then the text of all groups in the pattern are also returned as part of the resulting list. If maxsplit is nonzero, at most maxsplit splits occur, and the remainder of the string is returned as the final element of the list. >>> re.split(r'\\W+', 'Words, words, words.')\n['Words', 'words', 'words', '']\n>>> re.split(r'(\\W+)', 'Words, words, words.')\n['Words', ', ', 'words', ', ', 'words', '.', '']\n>>> re.split(r'\\W+', 'Words, words, words.', 1)\n['Words', 'words, words.']\n>>> re.split('[a-f]+', '0a3B9', flags=re.IGNORECASE)\n['0', '3', '9']\n If there are capturing groups in the separator and it matches at the start of the string, the result will start with an empty string. The same holds for the end of the string: >>> re.split(r'(\\W+)', '...words, words...')\n['', '...', 'words', ', ', 'words', '...', '']\n That way, separator components are always found at the same relative indices within the result list. Empty matches for the pattern split the string only when not adjacent to a previous empty match. >>> re.split(r'\\b', 'Words, words, words.')\n['', 'Words', ', ', 'words', ', ', 'words', '.']\n>>> re.split(r'\\W*', '...words...')\n['', '', 'w', 'o', 'r', 'd', 's', '', '']\n>>> re.split(r'(\\W*)', '...words...')\n['', '...', '', '', 'w', '', 'o', '', 'r', '', 'd', '', 's', '...', '', '', '']\n  Changed in version 3.1: Added the optional flags argument.   Changed in version 3.7: Added support of splitting on a pattern that could match an empty string.","title":"python.library.re#re.split"},{"text":"Pattern.split(string, maxsplit=0)  \nIdentical to the split() function, using the compiled pattern.","title":"python.library.re#re.Pattern.split"},{"text":"re.purge()  \nClear the regular expression cache.","title":"python.library.re#re.purge"},{"text":"gettext.lngettext(singular, plural, n)","title":"python.library.gettext#gettext.lngettext"},{"text":"pattern  \nThe regular expression pattern.","title":"python.library.re#re.error.pattern"},{"text":"output  \nA list of str objects with the formatted output of matching messages.","title":"python.library.unittest#unittest.TestCase.output"},{"text":"compressed","title":"python.library.ipaddress#ipaddress.IPv4Address.compressed"},{"text":"Dialect.skipinitialspace  \nWhen True, whitespace immediately following the delimiter is ignored. The default is False.","title":"python.library.csv#csv.Dialect.skipinitialspace"},{"text":"numpy.char.chararray.split method   char.chararray.split(sep=None, maxsplit=None)[source]\n \nFor each element in self, return a list of the words in the string, using sep as the delimiter string.  See also  char.split","title":"numpy.reference.generated.numpy.char.chararray.split"},{"text":"numpy.char.split   char.split(a, sep=None, maxsplit=None)[source]\n \nFor each element in a, return a list of the words in the string, using sep as the delimiter string. Calls str.split element-wise.  Parameters \n \naarray_like of str or unicode\n\n\nsepstr or unicode, optional\n\n\nIf sep is not specified or None, any whitespace string is a separator.  \nmaxsplitint, optional\n\n\nIf maxsplit is given, at most maxsplit splits are done.    Returns \n \noutndarray\n\n\nArray of list objects      See also  \nstr.split, rsplit","title":"numpy.reference.generated.numpy.char.split"}]}
{"task_id":24642040,"prompt":"def f_24642040(url, files, headers, data):\n\treturn ","suffix":"","canonical_solution":"requests.post(url, files=files, headers=headers, data=data)","test_start":"\nimport requests\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    requests.post = Mock()\n    try:\n        candidate('https:\/\/www.google.com', ['a.txt'], {'accept': 'text\/json'}, {'name': 'abc'})\n    except:\n        assert False\n"],"entry_point":"f_24642040","intent":"send multipart encoded file `files` to url `url` with headers `headers` and metadata `data`","library":["requests"],"docs":[]}
{"task_id":4290716,"prompt":"def f_4290716(filename, bytes_):\n\treturn ","suffix":"","canonical_solution":"open(filename, 'wb').write(bytes_)","test_start":"\ndef check(candidate):","test":["\n    bytes_ = b'68 65 6c 6c 6f'\n    candidate(\"tmpfile\", bytes_)\n\n    with open(\"tmpfile\", 'rb') as fr:\n        assert fr.read() == bytes_\n"],"entry_point":"f_4290716","intent":"write bytes `bytes_` to a file `filename` in python 3","library":[],"docs":[]}
{"task_id":33078554,"prompt":"def f_33078554(lst, dct):\n\treturn ","suffix":"","canonical_solution":"[dct[k] for k in lst]","test_start":"\ndef check(candidate):","test":["\n    assert candidate(['c', 'd', 'a', 'b', 'd'], {'a': '3', 'b': '3', 'c': '5', 'd': '3'}) == ['5', '3', '3', '3', '3'] \n","\n    assert candidate(['c', 'd', 'a', 'b', 'd'], {'a': 3, 'b': 3, 'c': 5, 'd': 3}) == [5, 3, 3, 3, 3] \n","\n    assert candidate(['c', 'd', 'a', 'b'], {'a': 3, 'b': 3, 'c': 5, 'd': 3}) == [5, 3, 3, 3]\n"],"entry_point":"f_33078554","intent":"get a list from a list `lst` with values mapped into a dictionary `dct`","library":[],"docs":[]}
{"task_id":15247628,"prompt":"def f_15247628(x):\n\treturn ","suffix":"","canonical_solution":"x['name'][x.duplicated('name')]","test_start":"\nimport pandas as pd \n\ndef check(candidate): ","test":["\n    assert candidate(pd.DataFrame([{'name': 'willy', 'age': 10}, {'name': 'wilson', 'age': 11}, {'name': 'zoe', 'age': 10}])).tolist() == [] \n","\n    assert candidate(pd.DataFrame([{'name': 'willy', 'age': 10}, {'name': 'willy', 'age': 11}, {'name': 'zoe', 'age': 10}])).tolist() == ['willy'] \n","\n    assert candidate(pd.DataFrame([{'name': 'willy', 'age': 11}, {'name': 'willy', 'age': 11}, {'name': 'zoe', 'age': 10}])).tolist() == ['willy'] \n","\n    assert candidate(pd.DataFrame([{'name': 'Willy', 'age': 11}, {'name': 'willy', 'age': 11}, {'name': 'zoe', 'age': 10}])).tolist() == []\n"],"entry_point":"f_15247628","intent":"find duplicate names in column 'name' of the dataframe `x`","library":["pandas"],"docs":[{"text":"pandas.tseries.offsets.Second.name   Second.name","title":"pandas.reference.api.pandas.tseries.offsets.second.name"},{"text":"pandas.Index.names   propertyIndex.names","title":"pandas.reference.api.pandas.index.names"},{"text":"pandas.tseries.offsets.Micro.name   Micro.name","title":"pandas.reference.api.pandas.tseries.offsets.micro.name"},{"text":"pandas.tseries.offsets.Nano.name   Nano.name","title":"pandas.reference.api.pandas.tseries.offsets.nano.name"},{"text":"pandas.tseries.offsets.Day.name   Day.name","title":"pandas.reference.api.pandas.tseries.offsets.day.name"},{"text":"pandas.tseries.offsets.FY5253Quarter.name   FY5253Quarter.name","title":"pandas.reference.api.pandas.tseries.offsets.fy5253quarter.name"},{"text":"pandas.tseries.offsets.BusinessMonthEnd.name   BusinessMonthEnd.name","title":"pandas.reference.api.pandas.tseries.offsets.businessmonthend.name"},{"text":"colno  \nThe column corresponding to pos (may be None).","title":"python.library.re#re.error.colno"},{"text":"pandas.tseries.offsets.Tick.name   Tick.name","title":"pandas.reference.api.pandas.tseries.offsets.tick.name"},{"text":"pandas.Index.has_duplicates   propertyIndex.has_duplicates\n \nCheck if the Index has duplicate values.  Returns \n bool\n\nWhether or not the Index has duplicate values.     Examples \n>>> idx = pd.Index([1, 5, 7, 7])\n>>> idx.has_duplicates\nTrue\n  \n>>> idx = pd.Index([1, 5, 7])\n>>> idx.has_duplicates\nFalse\n  \n>>> idx = pd.Index([\"Watermelon\", \"Orange\", \"Apple\",\n...                 \"Watermelon\"]).astype(\"category\")\n>>> idx.has_duplicates\nTrue\n  \n>>> idx = pd.Index([\"Orange\", \"Apple\",\n...                 \"Watermelon\"]).astype(\"category\")\n>>> idx.has_duplicates\nFalse","title":"pandas.reference.api.pandas.index.has_duplicates"}]}
{"task_id":783897,"prompt":"def f_783897():\n\treturn ","suffix":"","canonical_solution":"round(1.923328437452, 3)","test_start":"\ndef check(candidate): ","test":["\n    assert candidate() == 1.923\n"],"entry_point":"f_783897","intent":"truncate float 1.923328437452 to 3 decimal places","library":[],"docs":[]}
{"task_id":22859493,"prompt":"def f_22859493(li):\n\treturn ","suffix":"","canonical_solution":"sorted(li, key=lambda x: datetime.strptime(x[1], '%d\/%m\/%Y'), reverse=True)","test_start":"\nfrom datetime import datetime\n\ndef check(candidate): ","test":["\n    assert candidate([['name', '01\/03\/2012', 'job'], ['name', '02\/05\/2013', 'job'], ['name', '03\/08\/2014', 'job']]) == [['name', '03\/08\/2014', 'job'], ['name', '02\/05\/2013', 'job'], ['name', '01\/03\/2012', 'job']] \n","\n    assert candidate([['name', '01\/03\/2012', 'job'], ['name', '02\/05\/2012', 'job'], ['name', '03\/08\/2012', 'job']]) == [['name', '03\/08\/2012', 'job'], ['name', '02\/05\/2012', 'job'], ['name', '01\/03\/2012', 'job']] \n","\n    assert candidate([['name', '01\/03\/2012', 'job'], ['name', '02\/03\/2012', 'job'], ['name', '03\/03\/2012', 'job']]) == [['name', '03\/03\/2012', 'job'], ['name', '02\/03\/2012', 'job'], ['name', '01\/03\/2012', 'job']] \n","\n    assert candidate([['name', '03\/03\/2012', 'job'], ['name', '03\/03\/2012', 'job'], ['name', '03\/03\/2012', 'job']]) == [['name', '03\/03\/2012', 'job'], ['name', '03\/03\/2012', 'job'], ['name', '03\/03\/2012', 'job']] \n"],"entry_point":"f_22859493","intent":"sort list `li` in descending order based on the date value in second element of each list in list `li`","library":["datetime"],"docs":[{"text":"pandas.tseries.offsets.Second.apply   Second.apply()","title":"pandas.reference.api.pandas.tseries.offsets.second.apply"},{"text":"pandas.Timestamp.second   Timestamp.second","title":"pandas.reference.api.pandas.timestamp.second"},{"text":"pandas.tseries.offsets.Second.normalize   Second.normalize","title":"pandas.reference.api.pandas.tseries.offsets.second.normalize"},{"text":"pandas.tseries.offsets.Second.copy   Second.copy()","title":"pandas.reference.api.pandas.tseries.offsets.second.copy"},{"text":"time.second  \nIn range(60).","title":"python.library.datetime#datetime.time.second"},{"text":"pandas.tseries.offsets.LastWeekOfMonth.normalize   LastWeekOfMonth.normalize","title":"pandas.reference.api.pandas.tseries.offsets.lastweekofmonth.normalize"},{"text":"pandas.tseries.offsets.Second.n   Second.n","title":"pandas.reference.api.pandas.tseries.offsets.second.n"},{"text":"pandas.tseries.offsets.DateOffset.normalize   DateOffset.normalize","title":"pandas.reference.api.pandas.tseries.offsets.dateoffset.normalize"},{"text":"datetime.second  \nIn range(60).","title":"python.library.datetime#datetime.datetime.second"},{"text":"pandas.tseries.offsets.BYearEnd.apply   BYearEnd.apply()","title":"pandas.reference.api.pandas.tseries.offsets.byearend.apply"}]}
{"task_id":29394552,"prompt":"def f_29394552(ax):\n\t","suffix":"\n\treturn ","canonical_solution":"ax.set_rlabel_position(135)","test_start":"\nimport matplotlib.pyplot as plt \n\ndef check(candidate): ","test":["\n    ax = plt.subplot(111, polar=True)\n    candidate(ax)\n    assert ax.properties()['rlabel_position'] == 135.0\n"],"entry_point":"f_29394552","intent":"place the radial ticks in plot `ax` at 135 degrees","library":["matplotlib"],"docs":[{"text":"barbs_doc='\\nPlot a 2D field of barbs.\\n\\nCall signature::\\n\\n barbs([X, Y], U, V, [C], **kw)\\n\\nWhere *X*, *Y* define the barb locations, *U*, *V* define the barb\\ndirections, and *C* optionally sets the color.\\n\\nAll arguments may be 1D or 2D. *U*, *V*, *C* may be masked arrays, but masked\\n*X*, *Y* are not supported at present.\\n\\nBarbs are traditionally used in meteorology as a way to plot the speed\\nand direction of wind observations, but can technically be used to\\nplot any two dimensional vector quantity. As opposed to arrows, which\\ngive vector magnitude by the length of the arrow, the barbs give more\\nquantitative information about the vector magnitude by putting slanted\\nlines or a triangle for various increments in magnitude, as show\\nschematically below::\\n\\n : \/\\\\ \\\\\\n : \/ \\\\ \\\\\\n : \/ \\\\ \\\\ \\\\\\n : \/ \\\\ \\\\ \\\\\\n : ------------------------------\\n\\nThe largest increment is given by a triangle (or \"flag\"). After those\\ncome full lines (barbs). The smallest increment is a half line. There\\nis only, of course, ever at most 1 half line. If the magnitude is\\nsmall and only needs a single half-line and no full lines or\\ntriangles, the half-line is offset from the end of the barb so that it\\ncan be easily distinguished from barbs with a single full line. The\\nmagnitude for the barb shown above would nominally be 65, using the\\nstandard increments of 50, 10, and 5.\\n\\nSee also https:\/\/en.wikipedia.org\/wiki\/Wind_barb.\\n\\nParameters\\n----------\\nX, Y : 1D or 2D array-like, optional\\n The x and y coordinates of the barb locations. See *pivot* for how the\\n barbs are drawn to the x, y positions.\\n\\n If not given, they will be generated as a uniform integer meshgrid based\\n on the dimensions of *U* and *V*.\\n\\n If *X* and *Y* are 1D but *U*, *V* are 2D, *X*, *Y* are expanded to 2D\\n using ``X, Y = np.meshgrid(X, Y)``. In this case ``len(X)`` and ``len(Y)``\\n must match the column and row dimensions of *U* and *V*.\\n\\nU, V : 1D or 2D array-like\\n The x and y components of the barb shaft.\\n\\nC : 1D or 2D array-like, optional\\n Numeric data that defines the barb colors by colormapping via *norm* and\\n *cmap*.\\n\\n This does not support explicit colors. If you want to set colors directly,\\n use *barbcolor* instead.\\n\\nlength : float, default: 7\\n Length of the barb in points; the other parts of the barb\\n are scaled against this.\\n\\npivot : {\\'tip\\', \\'middle\\'} or float, default: \\'tip\\'\\n The part of the arrow that is anchored to the *X*, *Y* grid. The barb\\n rotates about this point. This can also be a number, which shifts the\\n start of the barb that many points away from grid point.\\n\\nbarbcolor : color or color sequence\\n The color of all parts of the barb except for the flags. This parameter\\n is analogous to the *edgecolor* parameter for polygons, which can be used\\n instead. However this parameter will override facecolor.\\n\\nflagcolor : color or color sequence\\n The color of any flags on the barb. This parameter is analogous to the\\n *facecolor* parameter for polygons, which can be used instead. However,\\n this parameter will override facecolor. If this is not set (and *C* has\\n not either) then *flagcolor* will be set to match *barbcolor* so that the\\n barb has a uniform color. If *C* has been set, *flagcolor* has no effect.\\n\\nsizes : dict, optional\\n A dictionary of coefficients specifying the ratio of a given\\n feature to the length of the barb. Only those values one wishes to\\n override need to be included. These features include:\\n\\n - \\'spacing\\' - space between features (flags, full\/half barbs)\\n - \\'height\\' - height (distance from shaft to top) of a flag or full barb\\n - \\'width\\' - width of a flag, twice the width of a full barb\\n - \\'emptybarb\\' - radius of the circle used for low magnitudes\\n\\nfill_empty : bool, default: False\\n Whether the empty barbs (circles) that are drawn should be filled with\\n the flag color. If they are not filled, the center is transparent.\\n\\nrounding : bool, default: True\\n Whether the vector magnitude should be rounded when allocating barb\\n components. If True, the magnitude is rounded to the nearest multiple\\n of the half-barb increment. If False, the magnitude is simply truncated\\n to the next lowest multiple.\\n\\nbarb_increments : dict, optional\\n A dictionary of increments specifying values to associate with\\n different parts of the barb. Only those values one wishes to\\n override need to be included.\\n\\n - \\'half\\' - half barbs (Default is 5)\\n - \\'full\\' - full barbs (Default is 10)\\n - \\'flag\\' - flags (default is 50)\\n\\nflip_barb : bool or array-like of bool, default: False\\n Whether the lines and flags should point opposite to normal.\\n Normal behavior is for the barbs and lines to point right (comes from wind\\n barbs having these features point towards low pressure in the Northern\\n Hemisphere).\\n\\n A single value is applied to all barbs. Individual barbs can be flipped by\\n passing a bool array of the same size as *U* and *V*.\\n\\nReturns\\n-------\\nbarbs : `~matplotlib.quiver.Barbs`\\n\\nOther Parameters\\n----------------\\ndata : indexable object, optional\\n DATA_PARAMETER_PLACEHOLDER\\n\\n**kwargs\\n The barbs can further be customized using `.PolyCollection` keyword\\n arguments:\\n\\n \\n .. table::\\n :class: property-table\\n\\n ================================================================================================= =====================================================================================================\\n Property Description \\n ================================================================================================= =====================================================================================================\\n :meth:`agg_filter <matplotlib.artist.Artist.set_agg_filter>` a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\\n :meth:`alpha <matplotlib.collections.Collection.set_alpha>` array-like or scalar or None \\n :meth:`animated <matplotlib.artist.Artist.set_animated>` bool \\n :meth:`antialiased <matplotlib.collections.Collection.set_antialiased>` or aa or antialiaseds bool or list of bools \\n :meth:`array <matplotlib.cm.ScalarMappable.set_array>` array-like or None \\n :meth:`capstyle <matplotlib.collections.Collection.set_capstyle>` `.CapStyle` or {\\'butt\\', \\'projecting\\', \\'round\\'} \\n :meth:`clim <matplotlib.cm.ScalarMappable.set_clim>` (vmin: float, vmax: float) \\n :meth:`clip_box <matplotlib.artist.Artist.set_clip_box>` `.Bbox` \\n :meth:`clip_on <matplotlib.artist.Artist.set_clip_on>` bool \\n :meth:`clip_path <matplotlib.artist.Artist.set_clip_path>` Patch or (Path, Transform) or None \\n :meth:`cmap <matplotlib.cm.ScalarMappable.set_cmap>` `.Colormap` or str or None \\n :meth:`color <matplotlib.collections.Collection.set_color>` color or list of rgba tuples \\n :meth:`edgecolor <matplotlib.collections.Collection.set_edgecolor>` or ec or edgecolors color or list of colors or \\'face\\' \\n :meth:`facecolor <matplotlib.collections.Collection.set_facecolor>` or facecolors or fc color or list of colors \\n :meth:`figure <matplotlib.artist.Artist.set_figure>` `.Figure` \\n :meth:`gid <matplotlib.artist.Artist.set_gid>` str \\n :meth:`hatch <matplotlib.collections.Collection.set_hatch>` {\\'\/\\', \\'\\\\\\\\\\', \\'|\\', \\'-\\', \\'+\\', \\'x\\', \\'o\\', \\'O\\', \\'.\\', \\'*\\'} \\n :meth:`in_layout <matplotlib.artist.Artist.set_in_layout>` bool \\n :meth:`joinstyle <matplotlib.collections.Collection.set_joinstyle>` `.JoinStyle` or {\\'miter\\', \\'round\\', \\'bevel\\'} \\n :meth:`label <matplotlib.artist.Artist.set_label>` object \\n :meth:`linestyle <matplotlib.collections.Collection.set_linestyle>` or dashes or linestyles or ls str or tuple or list thereof \\n :meth:`linewidth <matplotlib.collections.Collection.set_linewidth>` or linewidths or lw float or list of floats \\n :meth:`norm <matplotlib.cm.ScalarMappable.set_norm>` `.Normalize` or None \\n :meth:`offset_transform <matplotlib.collections.Collection.set_offset_transform>` `.Transform` \\n :meth:`offsets <matplotlib.collections.Collection.set_offsets>` (N, 2) or (2,) array-like \\n :meth:`path_effects <matplotlib.artist.Artist.set_path_effects>` `.AbstractPathEffect` \\n :meth:`paths <matplotlib.collections.PolyCollection.set_verts>` list of array-like \\n :meth:`picker <matplotlib.artist.Artist.set_picker>` None or bool or float or callable \\n :meth:`pickradius <matplotlib.collections.Collection.set_pickradius>` float \\n :meth:`rasterized <matplotlib.artist.Artist.set_rasterized>` bool \\n :meth:`sizes <matplotlib.collections._CollectionWithSizes.set_sizes>` ndarray or None \\n :meth:`sketch_params <matplotlib.artist.Artist.set_sketch_params>` (scale: float, length: float, randomness: float) \\n :meth:`snap <matplotlib.artist.Artist.set_snap>` bool or None \\n :meth:`transform <matplotlib.artist.Artist.set_transform>` `.Transform` \\n :meth:`url <matplotlib.artist.Artist.set_url>` str \\n :meth:`urls <matplotlib.collections.Collection.set_urls>` list of str or None \\n :meth:`verts <matplotlib.collections.PolyCollection.set_verts>` list of array-like \\n :meth:`verts_and_codes <matplotlib.collections.PolyCollection.set_verts_and_codes>` unknown \\n :meth:`visible <matplotlib.artist.Artist.set_visible>` bool \\n :meth:`zorder <matplotlib.artist.Artist.set_zorder>` float \\n ================================================================================================= =====================================================================================================\\n\\n'","title":"matplotlib._as_gen.matplotlib.quiver.barbs#matplotlib.quiver.Barbs.barbs_doc"},{"text":"axis=None","title":"matplotlib.ticker_api#matplotlib.ticker.TickHelper.axis"},{"text":"quiver_doc=\"\\nPlot a 2D field of arrows.\\n\\nCall signature::\\n\\n quiver([X, Y], U, V, [C], **kw)\\n\\n*X*, *Y* define the arrow locations, *U*, *V* define the arrow directions, and\\n*C* optionally sets the color.\\n\\nEach arrow is internally represented by a filled polygon with a default edge\\nlinewidth of 0. As a result, an arrow is rather a filled area, not a line with\\na head, and `.PolyCollection` properties like *linewidth*, *linestyle*,\\n*facecolor*, etc. act accordingly.\\n\\n**Arrow size**\\n\\nThe default settings auto-scales the length of the arrows to a reasonable size.\\nTo change this behavior see the *scale* and *scale_units* parameters.\\n\\n**Arrow shape**\\n\\nThe defaults give a slightly swept-back arrow; to make the head a\\ntriangle, make *headaxislength* the same as *headlength*. To make the\\narrow more pointed, reduce *headwidth* or increase *headlength* and\\n*headaxislength*. To make the head smaller relative to the shaft,\\nscale down all the head parameters. You will probably do best to leave\\nminshaft alone.\\n\\n**Arrow outline**\\n\\n*linewidths* and *edgecolors* can be used to customize the arrow\\noutlines.\\n\\nParameters\\n----------\\nX, Y : 1D or 2D array-like, optional\\n The x and y coordinates of the arrow locations.\\n\\n If not given, they will be generated as a uniform integer meshgrid based\\n on the dimensions of *U* and *V*.\\n\\n If *X* and *Y* are 1D but *U*, *V* are 2D, *X*, *Y* are expanded to 2D\\n using ``X, Y = np.meshgrid(X, Y)``. In this case ``len(X)`` and ``len(Y)``\\n must match the column and row dimensions of *U* and *V*.\\n\\nU, V : 1D or 2D array-like\\n The x and y direction components of the arrow vectors.\\n\\n They must have the same number of elements, matching the number of arrow\\n locations. *U* and *V* may be masked. Only locations unmasked in\\n *U*, *V*, and *C* will be drawn.\\n\\nC : 1D or 2D array-like, optional\\n Numeric data that defines the arrow colors by colormapping via *norm* and\\n *cmap*.\\n\\n This does not support explicit colors. If you want to set colors directly,\\n use *color* instead. The size of *C* must match the number of arrow\\n locations.\\n\\nunits : {'width', 'height', 'dots', 'inches', 'x', 'y', 'xy'}, default: 'width'\\n The arrow dimensions (except for *length*) are measured in multiples of\\n this unit.\\n\\n The following values are supported:\\n\\n - 'width', 'height': The width or height of the axis.\\n - 'dots', 'inches': Pixels or inches based on the figure dpi.\\n - 'x', 'y', 'xy': *X*, *Y* or :math:`\\\\sqrt{X^2 + Y^2}` in data units.\\n\\n The arrows scale differently depending on the units. For\\n 'x' or 'y', the arrows get larger as one zooms in; for other\\n units, the arrow size is independent of the zoom state. For\\n 'width or 'height', the arrow size increases with the width and\\n height of the axes, respectively, when the window is resized;\\n for 'dots' or 'inches', resizing does not change the arrows.\\n\\nangles : {'uv', 'xy'} or array-like, default: 'uv'\\n Method for determining the angle of the arrows.\\n\\n - 'uv': The arrow axis aspect ratio is 1 so that\\n if *U* == *V* the orientation of the arrow on the plot is 45 degrees\\n counter-clockwise from the horizontal axis (positive to the right).\\n\\n Use this if the arrows symbolize a quantity that is not based on\\n *X*, *Y* data coordinates.\\n\\n - 'xy': Arrows point from (x, y) to (x+u, y+v).\\n Use this for plotting a gradient field, for example.\\n\\n - Alternatively, arbitrary angles may be specified explicitly as an array\\n of values in degrees, counter-clockwise from the horizontal axis.\\n\\n In this case *U*, *V* is only used to determine the length of the\\n arrows.\\n\\n Note: inverting a data axis will correspondingly invert the\\n arrows only with ``angles='xy'``.\\n\\nscale : float, optional\\n Number of data units per arrow length unit, e.g., m\/s per plot width; a\\n smaller scale parameter makes the arrow longer. Default is *None*.\\n\\n If *None*, a simple autoscaling algorithm is used, based on the average\\n vector length and the number of vectors. The arrow length unit is given by\\n the *scale_units* parameter.\\n\\nscale_units : {'width', 'height', 'dots', 'inches', 'x', 'y', 'xy'}, optional\\n If the *scale* kwarg is *None*, the arrow length unit. Default is *None*.\\n\\n e.g. *scale_units* is 'inches', *scale* is 2.0, and ``(u, v) = (1, 0)``,\\n then the vector will be 0.5 inches long.\\n\\n If *scale_units* is 'width' or 'height', then the vector will be half the\\n width\/height of the axes.\\n\\n If *scale_units* is 'x' then the vector will be 0.5 x-axis\\n units. To plot vectors in the x-y plane, with u and v having\\n the same units as x and y, use\\n ``angles='xy', scale_units='xy', scale=1``.\\n\\nwidth : float, optional\\n Shaft width in arrow units; default depends on choice of units,\\n above, and number of vectors; a typical starting value is about\\n 0.005 times the width of the plot.\\n\\nheadwidth : float, default: 3\\n Head width as multiple of shaft width.\\n\\nheadlength : float, default: 5\\n Head length as multiple of shaft width.\\n\\nheadaxislength : float, default: 4.5\\n Head length at shaft intersection.\\n\\nminshaft : float, default: 1\\n Length below which arrow scales, in units of head length. Do not\\n set this to less than 1, or small arrows will look terrible!\\n\\nminlength : float, default: 1\\n Minimum length as a multiple of shaft width; if an arrow length\\n is less than this, plot a dot (hexagon) of this diameter instead.\\n\\npivot : {'tail', 'mid', 'middle', 'tip'}, default: 'tail'\\n The part of the arrow that is anchored to the *X*, *Y* grid. The arrow\\n rotates about this point.\\n\\n 'mid' is a synonym for 'middle'.\\n\\ncolor : color or color sequence, optional\\n Explicit color(s) for the arrows. If *C* has been set, *color* has no\\n effect.\\n\\n This is a synonym for the `.PolyCollection` *facecolor* parameter.\\n\\nOther Parameters\\n----------------\\ndata : indexable object, optional\\n DATA_PARAMETER_PLACEHOLDER\\n\\n**kwargs : `~matplotlib.collections.PolyCollection` properties, optional\\n All other keyword arguments are passed on to `.PolyCollection`:\\n\\n \\n .. table::\\n :class: property-table\\n\\n ================================================================================================= =====================================================================================================\\n Property Description \\n ================================================================================================= =====================================================================================================\\n :meth:`agg_filter <matplotlib.artist.Artist.set_agg_filter>` a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\\n :meth:`alpha <matplotlib.collections.Collection.set_alpha>` array-like or scalar or None \\n :meth:`animated <matplotlib.artist.Artist.set_animated>` bool \\n :meth:`antialiased <matplotlib.collections.Collection.set_antialiased>` or aa or antialiaseds bool or list of bools \\n :meth:`array <matplotlib.cm.ScalarMappable.set_array>` array-like or None \\n :meth:`capstyle <matplotlib.collections.Collection.set_capstyle>` `.CapStyle` or {'butt', 'projecting', 'round'} \\n :meth:`clim <matplotlib.cm.ScalarMappable.set_clim>` (vmin: float, vmax: float) \\n :meth:`clip_box <matplotlib.artist.Artist.set_clip_box>` `.Bbox` \\n :meth:`clip_on <matplotlib.artist.Artist.set_clip_on>` bool \\n :meth:`clip_path <matplotlib.artist.Artist.set_clip_path>` Patch or (Path, Transform) or None \\n :meth:`cmap <matplotlib.cm.ScalarMappable.set_cmap>` `.Colormap` or str or None \\n :meth:`color <matplotlib.collections.Collection.set_color>` color or list of rgba tuples \\n :meth:`edgecolor <matplotlib.collections.Collection.set_edgecolor>` or ec or edgecolors color or list of colors or 'face' \\n :meth:`facecolor <matplotlib.collections.Collection.set_facecolor>` or facecolors or fc color or list of colors \\n :meth:`figure <matplotlib.artist.Artist.set_figure>` `.Figure` \\n :meth:`gid <matplotlib.artist.Artist.set_gid>` str \\n :meth:`hatch <matplotlib.collections.Collection.set_hatch>` {'\/', '\\\\\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'} \\n :meth:`in_layout <matplotlib.artist.Artist.set_in_layout>` bool \\n :meth:`joinstyle <matplotlib.collections.Collection.set_joinstyle>` `.JoinStyle` or {'miter', 'round', 'bevel'} \\n :meth:`label <matplotlib.artist.Artist.set_label>` object \\n :meth:`linestyle <matplotlib.collections.Collection.set_linestyle>` or dashes or linestyles or ls str or tuple or list thereof \\n :meth:`linewidth <matplotlib.collections.Collection.set_linewidth>` or linewidths or lw float or list of floats \\n :meth:`norm <matplotlib.cm.ScalarMappable.set_norm>` `.Normalize` or None \\n :meth:`offset_transform <matplotlib.collections.Collection.set_offset_transform>` `.Transform` \\n :meth:`offsets <matplotlib.collections.Collection.set_offsets>` (N, 2) or (2,) array-like \\n :meth:`path_effects <matplotlib.artist.Artist.set_path_effects>` `.AbstractPathEffect` \\n :meth:`paths <matplotlib.collections.PolyCollection.set_verts>` list of array-like \\n :meth:`picker <matplotlib.artist.Artist.set_picker>` None or bool or float or callable \\n :meth:`pickradius <matplotlib.collections.Collection.set_pickradius>` float \\n :meth:`rasterized <matplotlib.artist.Artist.set_rasterized>` bool \\n :meth:`sizes <matplotlib.collections._CollectionWithSizes.set_sizes>` ndarray or None \\n :meth:`sketch_params <matplotlib.artist.Artist.set_sketch_params>` (scale: float, length: float, randomness: float) \\n :meth:`snap <matplotlib.artist.Artist.set_snap>` bool or None \\n :meth:`transform <matplotlib.artist.Artist.set_transform>` `.Transform` \\n :meth:`url <matplotlib.artist.Artist.set_url>` str \\n :meth:`urls <matplotlib.collections.Collection.set_urls>` list of str or None \\n :meth:`verts <matplotlib.collections.PolyCollection.set_verts>` list of array-like \\n :meth:`verts_and_codes <matplotlib.collections.PolyCollection.set_verts_and_codes>` unknown \\n :meth:`visible <matplotlib.artist.Artist.set_visible>` bool \\n :meth:`zorder <matplotlib.artist.Artist.set_zorder>` float \\n ================================================================================================= =====================================================================================================\\n\\n\\nReturns\\n-------\\n`~matplotlib.quiver.Quiver`\\n\\nSee Also\\n--------\\n.Axes.quiverkey : Add a key to a quiver plot.\\n\"","title":"matplotlib._as_gen.matplotlib.quiver.quiver#matplotlib.quiver.Quiver.quiver_doc"},{"text":"set_axis(axis)[source]","title":"matplotlib.projections_api#matplotlib.projections.polar.RadialLocator.set_axis"},{"text":"pandas.tseries.offsets.Tick.apply   Tick.apply()","title":"pandas.reference.api.pandas.tseries.offsets.tick.apply"},{"text":"decimal.MAX_EMAX","title":"python.library.decimal#decimal.MAX_EMAX"},{"text":"MAXTICKS=1000","title":"matplotlib.ticker_api#matplotlib.ticker.Locator.MAXTICKS"},{"text":"set_axis(axis)[source]","title":"matplotlib.projections_api#matplotlib.projections.polar.PolarAxes.RadialLocator.set_axis"},{"text":"pandas.tseries.offsets.Tick.normalize   Tick.normalize","title":"pandas.reference.api.pandas.tseries.offsets.tick.normalize"},{"text":"matplotlib.axis.Axis.OFFSETTEXTPAD   Axis.OFFSETTEXTPAD=3","title":"matplotlib._as_gen.matplotlib.axis.axis.offsettextpad"}]}
{"task_id":3320406,"prompt":"def f_3320406(my_path):\n\treturn ","suffix":"","canonical_solution":"os.path.isabs(my_path)","test_start":"\nimport os\n\ndef check(candidate): ","test":["\n    assert candidate('.') == False \n","\n    assert candidate('\/') == True \n","\n    assert candidate('\/usr') == True\n"],"entry_point":"f_3320406","intent":"check if path `my_path` is an absolute path","library":["os"],"docs":[{"text":"PurePath.is_absolute()  \nReturn whether the path is absolute or not. A path is considered absolute if it has both a root and (if the flavour allows) a drive: >>> PurePosixPath('\/a\/b').is_absolute()\nTrue\n>>> PurePosixPath('a\/b').is_absolute()\nFalse\n\n>>> PureWindowsPath('c:\/a\/b').is_absolute()\nTrue\n>>> PureWindowsPath('\/a\/b').is_absolute()\nFalse\n>>> PureWindowsPath('c:').is_absolute()\nFalse\n>>> PureWindowsPath('\/\/some\/share').is_absolute()\nTrue","title":"python.library.pathlib#pathlib.PurePath.is_absolute"},{"text":"Path.is_dir()  \nReturn True if the current context references a directory.","title":"python.library.zipfile#zipfile.Path.is_dir"},{"text":"is_package(fullname)  \nReturn True if path appears to be for a package.","title":"python.library.importlib#importlib.machinery.SourceFileLoader.is_package"},{"text":"Path.is_file()  \nReturn True if the current context references a file.","title":"python.library.zipfile#zipfile.Path.is_file"},{"text":"Path.is_dir()  \nReturn True if the path points to a directory (or a symbolic link pointing to a directory), False if it points to another kind of file. False is also returned if the path doesn\u2019t exist or is a broken symlink; other errors (such as permission errors) are propagated.","title":"python.library.pathlib#pathlib.Path.is_dir"},{"text":"PurePath.is_relative_to(*other)  \nReturn whether or not this path is relative to the other path. >>> p = PurePath('\/etc\/passwd')\n>>> p.is_relative_to('\/etc')\nTrue\n>>> p.is_relative_to('\/usr')\nFalse\n  New in version 3.9.","title":"python.library.pathlib#pathlib.PurePath.is_relative_to"},{"text":"is_package(fullname)  \nDetermines if the module is a package based on path.","title":"python.library.importlib#importlib.machinery.SourcelessFileLoader.is_package"},{"text":"stat.S_ISDIR(mode)  \nReturn non-zero if the mode is from a directory.","title":"python.library.stat#stat.S_ISDIR"},{"text":"PurePath.match(pattern)  \nMatch this path against the provided glob-style pattern. Return True if matching is successful, False otherwise. If pattern is relative, the path can be either relative or absolute, and matching is done from the right: >>> PurePath('a\/b.py').match('*.py')\nTrue\n>>> PurePath('\/a\/b\/c.py').match('b\/*.py')\nTrue\n>>> PurePath('\/a\/b\/c.py').match('a\/*.py')\nFalse\n If pattern is absolute, the path must be absolute, and the whole path must match: >>> PurePath('\/a.py').match('\/*.py')\nTrue\n>>> PurePath('a\/b.py').match('\/*.py')\nFalse\n As with other methods, case-sensitivity follows platform defaults: >>> PurePosixPath('b.py').match('*.PY')\nFalse\n>>> PureWindowsPath('b.py').match('*.PY')\nTrue","title":"python.library.pathlib#pathlib.PurePath.match"},{"text":"stat.S_ISREG(mode)  \nReturn non-zero if the mode is from a regular file.","title":"python.library.stat#stat.S_ISREG"}]}
{"task_id":2212433,"prompt":"def f_2212433(yourdict):\n\treturn ","suffix":"","canonical_solution":"len(list(yourdict.keys()))","test_start":"\ndef check(candidate): ","test":["\n    assert candidate({'a': 1, 'b': 2, 'c': 3}) == 3 \n","\n    assert candidate({'a': 2, 'c': 3}) == 2\n"],"entry_point":"f_2212433","intent":"get number of keys in dictionary `yourdict`","library":[],"docs":[]}
{"task_id":2212433,"prompt":"def f_2212433(yourdictfile):\n\treturn ","suffix":"","canonical_solution":"len(set(open(yourdictfile).read().split()))","test_start":"\ndef check(candidate): ","test":["\n    with open('dict.txt', 'w') as fw:\n        for w in [\"apple\", \"banana\", \"tv\", \"apple\", \"phone\"]:\n            fw.write(f\"{w}\\n\")\n    assert candidate('dict.txt') == 4\n"],"entry_point":"f_2212433","intent":"count the number of keys in dictionary `yourdictfile`","library":[],"docs":[]}
{"task_id":20067636,"prompt":"def f_20067636(df):\n\treturn ","suffix":"","canonical_solution":"df.groupby('id').first()","test_start":"\nimport pandas as pd \n\ndef check(candidate): ","test":["\n    df = pd.DataFrame({\n        'id': [1, 1, 1, 2, 2, 3, 3, 3, 3, 4, 4, 5, 6, 6, 6, 7, 7], \n        'value': ['first', 'second', 'second', 'first', 'second', 'first', 'third', 'fourth', 'fifth', 'second', 'fifth', 'first', 'first', 'second', 'third', 'fourth', 'fifth']\n    })\n    assert candidate(df).to_dict() == {'value': {1: 'first', 2: 'first', 3: 'first', 4: 'second', 5: 'first', 6: 'first', 7: 'fourth'}}\n"],"entry_point":"f_20067636","intent":"pandas dataframe `df` get first row of each group by 'id'","library":["pandas"],"docs":[{"text":"pandas.core.groupby.GroupBy.head   finalGroupBy.head(n=5)[source]\n \nReturn first n rows of each group. Similar to .apply(lambda x: x.head(n)), but it returns a subset of rows from the original DataFrame with original index and order preserved (as_index flag is ignored).  Parameters \n \nn:int\n\n\nIf positive: number of entries to include from start of each group. If negative: number of entries to exclude from end of each group.    Returns \n Series or DataFrame\n\nSubset of original Series or DataFrame as determined by n.      See also  Series.groupby\n\nApply a function groupby to a Series.  DataFrame.groupby\n\nApply a function groupby to each row or column of a DataFrame.    Examples \n>>> df = pd.DataFrame([[1, 2], [1, 4], [5, 6]],\n...                   columns=['A', 'B'])\n>>> df.groupby('A').head(1)\n   A  B\n0  1  2\n2  5  6\n>>> df.groupby('A').head(-1)\n   A  B\n0  1  2","title":"pandas.reference.api.pandas.core.groupby.groupby.head"},{"text":"first()","title":"django.ref.models.querysets#django.db.models.query.QuerySet.first"},{"text":"pandas.IntervalIndex.right   IntervalIndex.right","title":"pandas.reference.api.pandas.intervalindex.right"},{"text":"pandas.Timestamp.second   Timestamp.second","title":"pandas.reference.api.pandas.timestamp.second"},{"text":"pandas.DataFrame.index   DataFrame.index\n \nThe index (row labels) of the DataFrame.","title":"pandas.reference.api.pandas.dataframe.index"},{"text":"pandas.core.groupby.GroupBy.tail   finalGroupBy.tail(n=5)[source]\n \nReturn last n rows of each group. Similar to .apply(lambda x: x.tail(n)), but it returns a subset of rows from the original DataFrame with original index and order preserved (as_index flag is ignored).  Parameters \n \nn:int\n\n\nIf positive: number of entries to include from end of each group. If negative: number of entries to exclude from start of each group.    Returns \n Series or DataFrame\n\nSubset of original Series or DataFrame as determined by n.      See also  Series.groupby\n\nApply a function groupby to a Series.  DataFrame.groupby\n\nApply a function groupby to each row or column of a DataFrame.    Examples \n>>> df = pd.DataFrame([['a', 1], ['a', 2], ['b', 1], ['b', 2]],\n...                   columns=['A', 'B'])\n>>> df.groupby('A').tail(1)\n   A  B\n1  a  2\n3  b  2\n>>> df.groupby('A').tail(-1)\n   A  B\n1  a  2\n3  b  2","title":"pandas.reference.api.pandas.core.groupby.groupby.tail"},{"text":"pandas.Timestamp.freq   Timestamp.freq","title":"pandas.reference.api.pandas.timestamp.freq"},{"text":"pandas.tseries.offsets.Tick.copy   Tick.copy()","title":"pandas.reference.api.pandas.tseries.offsets.tick.copy"},{"text":"stat.ST_GID  \nGroup id of the owner.","title":"python.library.stat#stat.ST_GID"},{"text":"pandas.tseries.offsets.BYearEnd.copy   BYearEnd.copy()","title":"pandas.reference.api.pandas.tseries.offsets.byearend.copy"}]}
{"task_id":40924332,"prompt":"def f_40924332(df):\n\treturn ","suffix":"","canonical_solution":"pd.concat([df[0].apply(pd.Series), df[1]], axis=1)","test_start":"\nimport numpy as np\nimport pandas as pd \n\ndef check(callerFunction):","test":["\n    assert callerFunction(pd.DataFrame([[[8, 10, 12], 'A'], [[7, 9, 11], 'B']])).equals(pd.DataFrame([[8,10,12,'A'], [7,9,11,'B']], columns=[0,1,2,1]))\n","\n    assert callerFunction(pd.DataFrame([[[8, 10, 12], 'A'], [[7, 11], 'B']])).equals(pd.DataFrame([[8.0,10.0,12.0,'A'], [7.0,11.0,np.nan,'B']], columns=[0,1,2,1]))\n","\n    assert callerFunction(pd.DataFrame([[[8, 10, 12]], [[7, 9, 11], 'B']])).equals(pd.DataFrame([[8,10,12,None], [7,9,11,'B']], columns=[0,1,2,1]))\n"],"entry_point":"f_40924332","intent":"split a list in first column into multiple columns keeping other columns as well in pandas data frame `df`","library":["numpy","pandas"],"docs":[{"text":"pandas.MultiIndex.from_frame   classmethodMultiIndex.from_frame(df, sortorder=None, names=None)[source]\n \nMake a MultiIndex from a DataFrame.  Parameters \n \ndf:DataFrame\n\n\nDataFrame to be converted to MultiIndex.  \nsortorder:int, optional\n\n\nLevel of sortedness (must be lexicographically sorted by that level).  \nnames:list-like, optional\n\n\nIf no names are provided, use the column names, or tuple of column names if the columns is a MultiIndex. If a sequence, overwrite names with the given sequence.    Returns \n MultiIndex\n\nThe MultiIndex representation of the given DataFrame.      See also  MultiIndex.from_arrays\n\nConvert list of arrays to MultiIndex.  MultiIndex.from_tuples\n\nConvert list of tuples to MultiIndex.  MultiIndex.from_product\n\nMake a MultiIndex from cartesian product of iterables.    Examples \n>>> df = pd.DataFrame([['HI', 'Temp'], ['HI', 'Precip'],\n...                    ['NJ', 'Temp'], ['NJ', 'Precip']],\n...                   columns=['a', 'b'])\n>>> df\n      a       b\n0    HI    Temp\n1    HI  Precip\n2    NJ    Temp\n3    NJ  Precip\n  \n>>> pd.MultiIndex.from_frame(df)\nMultiIndex([('HI',   'Temp'),\n            ('HI', 'Precip'),\n            ('NJ',   'Temp'),\n            ('NJ', 'Precip')],\n           names=['a', 'b'])\n  Using explicit names, instead of the column names \n>>> pd.MultiIndex.from_frame(df, names=['state', 'observation'])\nMultiIndex([('HI',   'Temp'),\n            ('HI', 'Precip'),\n            ('NJ',   'Temp'),\n            ('NJ', 'Precip')],\n           names=['state', 'observation'])","title":"pandas.reference.api.pandas.multiindex.from_frame"},{"text":"pandas.DataFrame.pivot   DataFrame.pivot(index=None, columns=None, values=None)[source]\n \nReturn reshaped DataFrame organized by given index \/ column values. Reshape data (produce a \u201cpivot\u201d table) based on column values. Uses unique values from specified index \/ columns to form axes of the resulting DataFrame. This function does not support data aggregation, multiple values will result in a MultiIndex in the columns. See the User Guide for more on reshaping.  Parameters \n \nindex:str or object or a list of str, optional\n\n\nColumn to use to make new frame\u2019s index. If None, uses existing index.  Changed in version 1.1.0: Also accept list of index names.   \ncolumns:str or object or a list of str\n\n\nColumn to use to make new frame\u2019s columns.  Changed in version 1.1.0: Also accept list of columns names.   \nvalues:str, object or a list of the previous, optional\n\n\nColumn(s) to use for populating new frame\u2019s values. If not specified, all remaining columns will be used and the result will have hierarchically indexed columns.    Returns \n DataFrame\n\nReturns reshaped DataFrame.    Raises \n ValueError:\n\nWhen there are any index, columns combinations with multiple values. DataFrame.pivot_table when you need to aggregate.      See also  DataFrame.pivot_table\n\nGeneralization of pivot that can handle duplicate values for one index\/column pair.  DataFrame.unstack\n\nPivot based on the index values instead of a column.  wide_to_long\n\nWide panel to long format. Less flexible but more user-friendly than melt.    Notes For finer-tuned control, see hierarchical indexing documentation along with the related stack\/unstack methods. Examples \n>>> df = pd.DataFrame({'foo': ['one', 'one', 'one', 'two', 'two',\n...                            'two'],\n...                    'bar': ['A', 'B', 'C', 'A', 'B', 'C'],\n...                    'baz': [1, 2, 3, 4, 5, 6],\n...                    'zoo': ['x', 'y', 'z', 'q', 'w', 't']})\n>>> df\n    foo   bar  baz  zoo\n0   one   A    1    x\n1   one   B    2    y\n2   one   C    3    z\n3   two   A    4    q\n4   two   B    5    w\n5   two   C    6    t\n  \n>>> df.pivot(index='foo', columns='bar', values='baz')\nbar  A   B   C\nfoo\none  1   2   3\ntwo  4   5   6\n  \n>>> df.pivot(index='foo', columns='bar')['baz']\nbar  A   B   C\nfoo\none  1   2   3\ntwo  4   5   6\n  \n>>> df.pivot(index='foo', columns='bar', values=['baz', 'zoo'])\n      baz       zoo\nbar   A  B  C   A  B  C\nfoo\none   1  2  3   x  y  z\ntwo   4  5  6   q  w  t\n  You could also assign a list of column names or a list of index names. \n>>> df = pd.DataFrame({\n...        \"lev1\": [1, 1, 1, 2, 2, 2],\n...        \"lev2\": [1, 1, 2, 1, 1, 2],\n...        \"lev3\": [1, 2, 1, 2, 1, 2],\n...        \"lev4\": [1, 2, 3, 4, 5, 6],\n...        \"values\": [0, 1, 2, 3, 4, 5]})\n>>> df\n    lev1 lev2 lev3 lev4 values\n0   1    1    1    1    0\n1   1    1    2    2    1\n2   1    2    1    3    2\n3   2    1    2    4    3\n4   2    1    1    5    4\n5   2    2    2    6    5\n  \n>>> df.pivot(index=\"lev1\", columns=[\"lev2\", \"lev3\"],values=\"values\")\nlev2    1         2\nlev3    1    2    1    2\nlev1\n1     0.0  1.0  2.0  NaN\n2     4.0  3.0  NaN  5.0\n  \n>>> df.pivot(index=[\"lev1\", \"lev2\"], columns=[\"lev3\"],values=\"values\")\n      lev3    1    2\nlev1  lev2\n   1     1  0.0  1.0\n         2  2.0  NaN\n   2     1  4.0  3.0\n         2  NaN  5.0\n  A ValueError is raised if there are any duplicates. \n>>> df = pd.DataFrame({\"foo\": ['one', 'one', 'two', 'two'],\n...                    \"bar\": ['A', 'A', 'B', 'C'],\n...                    \"baz\": [1, 2, 3, 4]})\n>>> df\n   foo bar  baz\n0  one   A    1\n1  one   A    2\n2  two   B    3\n3  two   C    4\n  Notice that the first two rows are the same for our index and columns arguments. \n>>> df.pivot(index='foo', columns='bar', values='baz')\nTraceback (most recent call last):\n   ...\nValueError: Index contains duplicate entries, cannot reshape","title":"pandas.reference.api.pandas.dataframe.pivot"},{"text":"pandas.DataFrame.columns   DataFrame.columns\n \nThe column labels of the DataFrame.","title":"pandas.reference.api.pandas.dataframe.columns"},{"text":"pandas.pivot   pandas.pivot(data, index=None, columns=None, values=None)[source]\n \nReturn reshaped DataFrame organized by given index \/ column values. Reshape data (produce a \u201cpivot\u201d table) based on column values. Uses unique values from specified index \/ columns to form axes of the resulting DataFrame. This function does not support data aggregation, multiple values will result in a MultiIndex in the columns. See the User Guide for more on reshaping.  Parameters \n \ndata:DataFrame\n\n\nindex:str or object or a list of str, optional\n\n\nColumn to use to make new frame\u2019s index. If None, uses existing index.  Changed in version 1.1.0: Also accept list of index names.   \ncolumns:str or object or a list of str\n\n\nColumn to use to make new frame\u2019s columns.  Changed in version 1.1.0: Also accept list of columns names.   \nvalues:str, object or a list of the previous, optional\n\n\nColumn(s) to use for populating new frame\u2019s values. If not specified, all remaining columns will be used and the result will have hierarchically indexed columns.    Returns \n DataFrame\n\nReturns reshaped DataFrame.    Raises \n ValueError:\n\nWhen there are any index, columns combinations with multiple values. DataFrame.pivot_table when you need to aggregate.      See also  DataFrame.pivot_table\n\nGeneralization of pivot that can handle duplicate values for one index\/column pair.  DataFrame.unstack\n\nPivot based on the index values instead of a column.  wide_to_long\n\nWide panel to long format. Less flexible but more user-friendly than melt.    Notes For finer-tuned control, see hierarchical indexing documentation along with the related stack\/unstack methods. Examples \n>>> df = pd.DataFrame({'foo': ['one', 'one', 'one', 'two', 'two',\n...                            'two'],\n...                    'bar': ['A', 'B', 'C', 'A', 'B', 'C'],\n...                    'baz': [1, 2, 3, 4, 5, 6],\n...                    'zoo': ['x', 'y', 'z', 'q', 'w', 't']})\n>>> df\n    foo   bar  baz  zoo\n0   one   A    1    x\n1   one   B    2    y\n2   one   C    3    z\n3   two   A    4    q\n4   two   B    5    w\n5   two   C    6    t\n  \n>>> df.pivot(index='foo', columns='bar', values='baz')\nbar  A   B   C\nfoo\none  1   2   3\ntwo  4   5   6\n  \n>>> df.pivot(index='foo', columns='bar')['baz']\nbar  A   B   C\nfoo\none  1   2   3\ntwo  4   5   6\n  \n>>> df.pivot(index='foo', columns='bar', values=['baz', 'zoo'])\n      baz       zoo\nbar   A  B  C   A  B  C\nfoo\none   1  2  3   x  y  z\ntwo   4  5  6   q  w  t\n  You could also assign a list of column names or a list of index names. \n>>> df = pd.DataFrame({\n...        \"lev1\": [1, 1, 1, 2, 2, 2],\n...        \"lev2\": [1, 1, 2, 1, 1, 2],\n...        \"lev3\": [1, 2, 1, 2, 1, 2],\n...        \"lev4\": [1, 2, 3, 4, 5, 6],\n...        \"values\": [0, 1, 2, 3, 4, 5]})\n>>> df\n    lev1 lev2 lev3 lev4 values\n0   1    1    1    1    0\n1   1    1    2    2    1\n2   1    2    1    3    2\n3   2    1    2    4    3\n4   2    1    1    5    4\n5   2    2    2    6    5\n  \n>>> df.pivot(index=\"lev1\", columns=[\"lev2\", \"lev3\"],values=\"values\")\nlev2    1         2\nlev3    1    2    1    2\nlev1\n1     0.0  1.0  2.0  NaN\n2     4.0  3.0  NaN  5.0\n  \n>>> df.pivot(index=[\"lev1\", \"lev2\"], columns=[\"lev3\"],values=\"values\")\n      lev3    1    2\nlev1  lev2\n   1     1  0.0  1.0\n         2  2.0  NaN\n   2     1  4.0  3.0\n         2  NaN  5.0\n  A ValueError is raised if there are any duplicates. \n>>> df = pd.DataFrame({\"foo\": ['one', 'one', 'two', 'two'],\n...                    \"bar\": ['A', 'A', 'B', 'C'],\n...                    \"baz\": [1, 2, 3, 4]})\n>>> df\n   foo bar  baz\n0  one   A    1\n1  one   A    2\n2  two   B    3\n3  two   C    4\n  Notice that the first two rows are the same for our index and columns arguments. \n>>> df.pivot(index='foo', columns='bar', values='baz')\nTraceback (most recent call last):\n   ...\nValueError: Index contains duplicate entries, cannot reshape","title":"pandas.reference.api.pandas.pivot"},{"text":"pandas.MultiIndex.codes   propertyMultiIndex.codes","title":"pandas.reference.api.pandas.multiindex.codes"},{"text":"pandas.IntervalIndex.left   IntervalIndex.left","title":"pandas.reference.api.pandas.intervalindex.left"},{"text":"pandas.DataFrame.explode   DataFrame.explode(column, ignore_index=False)[source]\n \nTransform each element of a list-like to a row, replicating index values.  New in version 0.25.0.   Parameters \n \ncolumn:IndexLabel\n\n\nColumn(s) to explode. For multiple columns, specify a non-empty list with each element be str or tuple, and all specified columns their list-like data on same row of the frame must have matching length.  New in version 1.3.0: Multi-column explode   \nignore_index:bool, default False\n\n\nIf True, the resulting index will be labeled 0, 1, \u2026, n - 1.  New in version 1.1.0.     Returns \n DataFrame\n\nExploded lists to rows of the subset columns; index will be duplicated for these rows.    Raises \n ValueError :\n\n If columns of the frame are not unique. If specified columns to explode is empty list. If specified columns to explode have not matching count of elements rowwise in the frame.       See also  DataFrame.unstack\n\nPivot a level of the (necessarily hierarchical) index labels.  DataFrame.melt\n\nUnpivot a DataFrame from wide format to long format.  Series.explode\n\nExplode a DataFrame from list-like columns to long format.    Notes This routine will explode list-likes including lists, tuples, sets, Series, and np.ndarray. The result dtype of the subset rows will be object. Scalars will be returned unchanged, and empty list-likes will result in a np.nan for that row. In addition, the ordering of rows in the output will be non-deterministic when exploding sets. Examples \n>>> df = pd.DataFrame({'A': [[0, 1, 2], 'foo', [], [3, 4]],\n...                    'B': 1,\n...                    'C': [['a', 'b', 'c'], np.nan, [], ['d', 'e']]})\n>>> df\n           A  B          C\n0  [0, 1, 2]  1  [a, b, c]\n1        foo  1        NaN\n2         []  1         []\n3     [3, 4]  1     [d, e]\n  Single-column explode. \n>>> df.explode('A')\n     A  B          C\n0    0  1  [a, b, c]\n0    1  1  [a, b, c]\n0    2  1  [a, b, c]\n1  foo  1        NaN\n2  NaN  1         []\n3    3  1     [d, e]\n3    4  1     [d, e]\n  Multi-column explode. \n>>> df.explode(list('AC'))\n     A  B    C\n0    0  1    a\n0    1  1    b\n0    2  1    c\n1  foo  1  NaN\n2  NaN  1  NaN\n3    3  1    d\n3    4  1    e","title":"pandas.reference.api.pandas.dataframe.explode"},{"text":"pandas.Timestamp.fold   Timestamp.fold","title":"pandas.reference.api.pandas.timestamp.fold"},{"text":"pandas.Series.str.split   Series.str.split(pat=None, n=- 1, expand=False, *, regex=None)[source]\n \nSplit strings around given separator\/delimiter. Splits the string in the Series\/Index from the beginning, at the specified delimiter string.  Parameters \n \npat:str or compiled regex, optional\n\n\nString or regular expression to split on. If not specified, split on whitespace.  \nn:int, default -1 (all)\n\n\nLimit number of splits in output. None, 0 and -1 will be interpreted as return all splits.  \nexpand:bool, default False\n\n\nExpand the split strings into separate columns.  If True, return DataFrame\/MultiIndex expanding dimensionality. If False, return Series\/Index, containing lists of strings.   \nregex:bool, default None\n\n\nDetermines if the passed-in pattern is a regular expression:  If True, assumes the passed-in pattern is a regular expression If False, treats the pattern as a literal string. If None and pat length is 1, treats pat as a literal string. If None and pat length is not 1, treats pat as a regular expression. Cannot be set to False if pat is a compiled regex   New in version 1.4.0.     Returns \n Series, Index, DataFrame or MultiIndex\n\nType matches caller unless expand=True (see Notes).    Raises \n ValueError\n\n if regex is False and pat is a compiled regex       See also  Series.str.split\n\nSplit strings around given separator\/delimiter.  Series.str.rsplit\n\nSplits string around given separator\/delimiter, starting from the right.  Series.str.join\n\nJoin lists contained as elements in the Series\/Index with passed delimiter.  str.split\n\nStandard library version for split.  str.rsplit\n\nStandard library version for rsplit.    Notes The handling of the n keyword depends on the number of found splits:  If found splits > n, make first n splits only If found splits <= n, make all splits If for a certain row the number of found splits < n, append None for padding up to n if expand=True  If using expand=True, Series and Index callers return DataFrame and MultiIndex objects, respectively. Use of regex=False with a pat as a compiled regex will raise an error. Examples \n>>> s = pd.Series(\n...     [\n...         \"this is a regular sentence\",\n...         \"https:\/\/docs.python.org\/3\/tutorial\/index.html\",\n...         np.nan\n...     ]\n... )\n>>> s\n0                       this is a regular sentence\n1    https:\/\/docs.python.org\/3\/tutorial\/index.html\n2                                              NaN\ndtype: object\n  In the default setting, the string is split by whitespace. \n>>> s.str.split()\n0                   [this, is, a, regular, sentence]\n1    [https:\/\/docs.python.org\/3\/tutorial\/index.html]\n2                                                NaN\ndtype: object\n  Without the n parameter, the outputs of rsplit and split are identical. \n>>> s.str.rsplit()\n0                   [this, is, a, regular, sentence]\n1    [https:\/\/docs.python.org\/3\/tutorial\/index.html]\n2                                                NaN\ndtype: object\n  The n parameter can be used to limit the number of splits on the delimiter. The outputs of split and rsplit are different. \n>>> s.str.split(n=2)\n0                     [this, is, a regular sentence]\n1    [https:\/\/docs.python.org\/3\/tutorial\/index.html]\n2                                                NaN\ndtype: object\n  \n>>> s.str.rsplit(n=2)\n0                     [this is a, regular, sentence]\n1    [https:\/\/docs.python.org\/3\/tutorial\/index.html]\n2                                                NaN\ndtype: object\n  The pat parameter can be used to split by other characters. \n>>> s.str.split(pat=\"\/\")\n0                         [this is a regular sentence]\n1    [https:, , docs.python.org, 3, tutorial, index...\n2                                                  NaN\ndtype: object\n  When using expand=True, the split elements will expand out into separate columns. If NaN is present, it is propagated throughout the columns during the split. \n>>> s.str.split(expand=True)\n                                               0     1     2        3         4\n0                                           this    is     a  regular  sentence\n1  https:\/\/docs.python.org\/3\/tutorial\/index.html  None  None     None      None\n2                                            NaN   NaN   NaN      NaN       NaN\n  For slightly more complex use cases like splitting the html document name from a url, a combination of parameter settings can be used. \n>>> s.str.rsplit(\"\/\", n=1, expand=True)\n                                    0           1\n0          this is a regular sentence        None\n1  https:\/\/docs.python.org\/3\/tutorial  index.html\n2                                 NaN         NaN\n  Remember to escape special characters when explicitly using regular expressions. \n>>> s = pd.Series([\"foo and bar plus baz\"])\n>>> s.str.split(r\"and|plus\", expand=True)\n    0   1   2\n0 foo bar baz\n  Regular expressions can be used to handle urls or file names. When pat is a string and regex=None (the default), the given pat is compiled as a regex only if len(pat) != 1. \n>>> s = pd.Series(['foojpgbar.jpg'])\n>>> s.str.split(r\".\", expand=True)\n           0    1\n0  foojpgbar  jpg\n  \n>>> s.str.split(r\"\\.jpg\", expand=True)\n           0 1\n0  foojpgbar\n  When regex=True, pat is interpreted as a regex \n>>> s.str.split(r\"\\.jpg\", regex=True, expand=True)\n           0 1\n0  foojpgbar\n  A compiled regex can be passed as pat \n>>> import re\n>>> s.str.split(re.compile(r\"\\.jpg\"), expand=True)\n           0 1\n0  foojpgbar\n  When regex=False, pat is interpreted as the string itself \n>>> s.str.split(r\"\\.jpg\", regex=False, expand=True)\n               0\n0  foojpgbar.jpg","title":"pandas.reference.api.pandas.series.str.split"},{"text":"pandas.wide_to_long   pandas.wide_to_long(df, stubnames, i, j, sep='', suffix='\\\\d+')[source]\n \nUnpivot a DataFrame from wide to long format. Less flexible but more user-friendly than melt. With stubnames [\u2018A\u2019, \u2018B\u2019], this function expects to find one or more group of columns with format A-suffix1, A-suffix2,\u2026, B-suffix1, B-suffix2,\u2026 You specify what you want to call this suffix in the resulting long format with j (for example j=\u2019year\u2019) Each row of these wide variables are assumed to be uniquely identified by i (can be a single column name or a list of column names) All remaining variables in the data frame are left intact.  Parameters \n \ndf:DataFrame\n\n\nThe wide-format DataFrame.  \nstubnames:str or list-like\n\n\nThe stub name(s). The wide format variables are assumed to start with the stub names.  \ni:str or list-like\n\n\nColumn(s) to use as id variable(s).  \nj:str\n\n\nThe name of the sub-observation variable. What you wish to name your suffix in the long format.  \nsep:str, default \u201c\u201d\n\n\nA character indicating the separation of the variable names in the wide format, to be stripped from the names in the long format. For example, if your column names are A-suffix1, A-suffix2, you can strip the hyphen by specifying sep=\u2019-\u2019.  \nsuffix:str, default \u2018\\d+\u2019\n\n\nA regular expression capturing the wanted suffixes. \u2018\\d+\u2019 captures numeric suffixes. Suffixes with no numbers could be specified with the negated character class \u2018\\D+\u2019. You can also further disambiguate suffixes, for example, if your wide variables are of the form A-one, B-two,.., and you have an unrelated column A-rating, you can ignore the last one by specifying suffix=\u2019(!?one|two)\u2019. When all suffixes are numeric, they are cast to int64\/float64.    Returns \n DataFrame\n\nA DataFrame that contains each stub name as a variable, with new index (i, j).      See also  melt\n\nUnpivot a DataFrame from wide to long format, optionally leaving identifiers set.  pivot\n\nCreate a spreadsheet-style pivot table as a DataFrame.  DataFrame.pivot\n\nPivot without aggregation that can handle non-numeric data.  DataFrame.pivot_table\n\nGeneralization of pivot that can handle duplicate values for one index\/column pair.  DataFrame.unstack\n\nPivot based on the index values instead of a column.    Notes All extra variables are left untouched. This simply uses pandas.melt under the hood, but is hard-coded to \u201cdo the right thing\u201d in a typical case. Examples \n>>> np.random.seed(123)\n>>> df = pd.DataFrame({\"A1970\" : {0 : \"a\", 1 : \"b\", 2 : \"c\"},\n...                    \"A1980\" : {0 : \"d\", 1 : \"e\", 2 : \"f\"},\n...                    \"B1970\" : {0 : 2.5, 1 : 1.2, 2 : .7},\n...                    \"B1980\" : {0 : 3.2, 1 : 1.3, 2 : .1},\n...                    \"X\"     : dict(zip(range(3), np.random.randn(3)))\n...                   })\n>>> df[\"id\"] = df.index\n>>> df\n  A1970 A1980  B1970  B1980         X  id\n0     a     d    2.5    3.2 -1.085631   0\n1     b     e    1.2    1.3  0.997345   1\n2     c     f    0.7    0.1  0.282978   2\n>>> pd.wide_to_long(df, [\"A\", \"B\"], i=\"id\", j=\"year\")\n... \n                X  A    B\nid year\n0  1970 -1.085631  a  2.5\n1  1970  0.997345  b  1.2\n2  1970  0.282978  c  0.7\n0  1980 -1.085631  d  3.2\n1  1980  0.997345  e  1.3\n2  1980  0.282978  f  0.1\n  With multiple id columns \n>>> df = pd.DataFrame({\n...     'famid': [1, 1, 1, 2, 2, 2, 3, 3, 3],\n...     'birth': [1, 2, 3, 1, 2, 3, 1, 2, 3],\n...     'ht1': [2.8, 2.9, 2.2, 2, 1.8, 1.9, 2.2, 2.3, 2.1],\n...     'ht2': [3.4, 3.8, 2.9, 3.2, 2.8, 2.4, 3.3, 3.4, 2.9]\n... })\n>>> df\n   famid  birth  ht1  ht2\n0      1      1  2.8  3.4\n1      1      2  2.9  3.8\n2      1      3  2.2  2.9\n3      2      1  2.0  3.2\n4      2      2  1.8  2.8\n5      2      3  1.9  2.4\n6      3      1  2.2  3.3\n7      3      2  2.3  3.4\n8      3      3  2.1  2.9\n>>> l = pd.wide_to_long(df, stubnames='ht', i=['famid', 'birth'], j='age')\n>>> l\n... \n                  ht\nfamid birth age\n1     1     1    2.8\n            2    3.4\n      2     1    2.9\n            2    3.8\n      3     1    2.2\n            2    2.9\n2     1     1    2.0\n            2    3.2\n      2     1    1.8\n            2    2.8\n      3     1    1.9\n            2    2.4\n3     1     1    2.2\n            2    3.3\n      2     1    2.3\n            2    3.4\n      3     1    2.1\n            2    2.9\n  Going from long back to wide just takes some creative use of unstack \n>>> w = l.unstack()\n>>> w.columns = w.columns.map('{0[0]}{0[1]}'.format)\n>>> w.reset_index()\n   famid  birth  ht1  ht2\n0      1      1  2.8  3.4\n1      1      2  2.9  3.8\n2      1      3  2.2  2.9\n3      2      1  2.0  3.2\n4      2      2  1.8  2.8\n5      2      3  1.9  2.4\n6      3      1  2.2  3.3\n7      3      2  2.3  3.4\n8      3      3  2.1  2.9\n  Less wieldy column names are also handled \n>>> np.random.seed(0)\n>>> df = pd.DataFrame({'A(weekly)-2010': np.random.rand(3),\n...                    'A(weekly)-2011': np.random.rand(3),\n...                    'B(weekly)-2010': np.random.rand(3),\n...                    'B(weekly)-2011': np.random.rand(3),\n...                    'X' : np.random.randint(3, size=3)})\n>>> df['id'] = df.index\n>>> df \n   A(weekly)-2010  A(weekly)-2011  B(weekly)-2010  B(weekly)-2011  X  id\n0        0.548814        0.544883        0.437587        0.383442  0   0\n1        0.715189        0.423655        0.891773        0.791725  1   1\n2        0.602763        0.645894        0.963663        0.528895  1   2\n  \n>>> pd.wide_to_long(df, ['A(weekly)', 'B(weekly)'], i='id',\n...                 j='year', sep='-')\n... \n         X  A(weekly)  B(weekly)\nid year\n0  2010  0   0.548814   0.437587\n1  2010  1   0.715189   0.891773\n2  2010  1   0.602763   0.963663\n0  2011  0   0.544883   0.383442\n1  2011  1   0.423655   0.791725\n2  2011  1   0.645894   0.528895\n  If we have many columns, we could also use a regex to find our stubnames and pass that list on to wide_to_long \n>>> stubnames = sorted(\n...     set([match[0] for match in df.columns.str.findall(\n...         r'[A-B]\\(.*\\)').values if match != []])\n... )\n>>> list(stubnames)\n['A(weekly)', 'B(weekly)']\n  All of the above examples have integers as suffixes. It is possible to have non-integers as suffixes. \n>>> df = pd.DataFrame({\n...     'famid': [1, 1, 1, 2, 2, 2, 3, 3, 3],\n...     'birth': [1, 2, 3, 1, 2, 3, 1, 2, 3],\n...     'ht_one': [2.8, 2.9, 2.2, 2, 1.8, 1.9, 2.2, 2.3, 2.1],\n...     'ht_two': [3.4, 3.8, 2.9, 3.2, 2.8, 2.4, 3.3, 3.4, 2.9]\n... })\n>>> df\n   famid  birth  ht_one  ht_two\n0      1      1     2.8     3.4\n1      1      2     2.9     3.8\n2      1      3     2.2     2.9\n3      2      1     2.0     3.2\n4      2      2     1.8     2.8\n5      2      3     1.9     2.4\n6      3      1     2.2     3.3\n7      3      2     2.3     3.4\n8      3      3     2.1     2.9\n  \n>>> l = pd.wide_to_long(df, stubnames='ht', i=['famid', 'birth'], j='age',\n...                     sep='_', suffix=r'\\w+')\n>>> l\n... \n                  ht\nfamid birth age\n1     1     one  2.8\n            two  3.4\n      2     one  2.9\n            two  3.8\n      3     one  2.2\n            two  2.9\n2     1     one  2.0\n            two  3.2\n      2     one  1.8\n            two  2.8\n      3     one  1.9\n            two  2.4\n3     1     one  2.2\n            two  3.3\n      2     one  2.3\n            two  3.4\n      3     one  2.1\n            two  2.9","title":"pandas.reference.api.pandas.wide_to_long"}]}
{"task_id":30759776,"prompt":"def f_30759776(data):\n\treturn ","suffix":"","canonical_solution":"re.findall('src=\"js\/([^\"]*\\\\bjquery\\\\b[^\"]*)\"', data)","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    data = '<script type=\"text\/javascript\" src=\"js\/jquery-1.9.1.min.js\"\/><script type=\"text\/javascript\" src=\"js\/jquery-migrate-1.2.1.min.js\"\/><script type=\"text\/javascript\" src=\"js\/jquery-ui.min.js\"\/><script type=\"text\/javascript\" src=\"js\/abc_bsub.js\"\/><script type=\"text\/javascript\" src=\"js\/abc_core.js\"\/>            <script type=\"text\/javascript\" src=\"js\/abc_explore.js\"\/><script type=\"text\/javascript\" src=\"js\/abc_qaa.js\"\/>'\n    assert candidate(data) == ['jquery-1.9.1.min.js', 'jquery-migrate-1.2.1.min.js', 'jquery-ui.min.js']\n"],"entry_point":"f_30759776","intent":"extract attributes 'src=\"js\/([^\"]*\\\\bjquery\\\\b[^\"]*)\"' from string `data`","library":["re"],"docs":[{"text":"exception xml.dom.SyntaxErr  \nRaised when an invalid or illegal string is specified.","title":"python.library.xml.dom#xml.dom.SyntaxErr"},{"text":"Element.getAttributeNS(namespaceURI, localName)  \nReturn the value of the attribute named by namespaceURI and localName as a string. If no such attribute exists, an empty string is returned, as if the attribute had no value.","title":"python.library.xml.dom#xml.dom.Element.getAttributeNS"},{"text":"Text.data  \nThe content of the text node as a string.","title":"python.library.xml.dom#xml.dom.Text.data"},{"text":"Form.errors.get_json_data(escape_html=False)","title":"django.ref.forms.api#django.forms.Form.errors.get_json_data"},{"text":"as_string()","title":"django.ref.contrib.gis.gdal#django.contrib.gis.gdal.Field.as_string"},{"text":"dis.hasname  \nSequence of bytecodes that access an attribute by name.","title":"python.library.dis#dis.hasname"},{"text":"xml.sax.handler.property_xml_string","title":"python.library.xml.sax.handler#xml.sax.handler.property_xml_string"},{"text":"exception xml.dom.InuseAttributeErr  \nRaised when an attempt is made to insert an Attr node that is already present elsewhere in the document.","title":"python.library.xml.dom#xml.dom.InuseAttributeErr"},{"text":"Comment.data  \nThe content of the comment as a string. The attribute contains all characters between the leading <!-- and trailing -->, but does not include them.","title":"python.library.xml.dom#xml.dom.Comment.data"},{"text":"token.DOUBLESLASH  \nToken value for \"\/\/\".","title":"python.library.token#token.DOUBLESLASH"}]}
{"task_id":25388796,"prompt":"def f_25388796():\n\treturn ","suffix":"","canonical_solution":"sum(int(float(item)) for item in [_f for _f in ['', '3.4', '', '', '1.0'] if _f])","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == 4\n"],"entry_point":"f_25388796","intent":"Sum integers contained in strings in list `['', '3.4', '', '', '1.0']`","library":[],"docs":[]}
{"task_id":804995,"prompt":"def f_804995():\n\treturn ","suffix":"","canonical_solution":"subprocess.Popen(['c:\\\\Program Files\\\\VMware\\\\VMware Server\\\\vmware-cmd.bat'])","test_start":"\nimport subprocess\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    subprocess.Popen = Mock(return_value = 0)\n    assert candidate() == 0\n"],"entry_point":"f_804995","intent":"Call a subprocess with arguments `c:\\\\Program Files\\\\VMware\\\\VMware Server\\\\vmware-cmd.bat` that may contain spaces","library":["subprocess"],"docs":[{"text":"cmd  \nCommand that was used to spawn the child process.","title":"python.library.subprocess#subprocess.CalledProcessError.cmd"},{"text":"cmd  \nCommand that was used to spawn the child process.","title":"python.library.subprocess#subprocess.TimeoutExpired.cmd"},{"text":"stdout  \nAlias for output, for symmetry with stderr.","title":"python.library.subprocess#subprocess.CalledProcessError.stdout"},{"text":"winreg.REG_EXPAND_SZ  \nNull-terminated string containing references to environment variables (%PATH%).","title":"python.library.winreg#winreg.REG_EXPAND_SZ"},{"text":"Cmd.lastcmd  \nThe last nonempty command prefix seen.","title":"python.library.cmd#cmd.Cmd.lastcmd"},{"text":"Cmd.prompt  \nThe prompt issued to solicit input.","title":"python.library.cmd#cmd.Cmd.prompt"},{"text":"stat.S_IXGRP  \nGroup has execute permission.","title":"python.library.stat#stat.S_IXGRP"},{"text":"stdout  \nAlias for output, for symmetry with stderr.","title":"python.library.subprocess#subprocess.TimeoutExpired.stdout"},{"text":"test.support.unix_shell  \nPath for shell if not on Windows; otherwise None.","title":"python.library.test#test.support.unix_shell"},{"text":"psname\n \nAlias for field number 1","title":"matplotlib.dviread#matplotlib.dviread.PsFont.psname"}]}
{"task_id":26441253,"prompt":"def f_26441253(q):\n\t","suffix":"\n\treturn q","canonical_solution":"for n in [1,3,4,2]: q.put((-n, n))","test_start":"\nfrom queue import PriorityQueue\n\ndef check(candidate):","test":["\n    q = PriorityQueue()\n    q = candidate(q)\n    expected = [4, 3, 2, 1]\n    for i in range(0, len(expected)):\n        assert q.get()[1] == expected[i]\n"],"entry_point":"f_26441253","intent":"reverse a priority queue `q` in python without using classes","library":["queue"],"docs":[{"text":"dis.hasjrel  \nSequence of bytecodes that have a relative jump target.","title":"python.library.dis#dis.hasjrel"},{"text":"dis.hasjabs  \nSequence of bytecodes that have an absolute jump target.","title":"python.library.dis#dis.hasjabs"},{"text":"SimpleQueue.get_nowait()  \nEquivalent to get(False).","title":"python.library.queue#queue.SimpleQueue.get_nowait"},{"text":"Queue.get_nowait()  \nEquivalent to get(False).","title":"python.library.queue#queue.Queue.get_nowait"},{"text":"get()  \nRemove and return an item from the queue.","title":"python.library.multiprocessing#multiprocessing.SimpleQueue.get"},{"text":"get_nowait()  \nEquivalent to get(False).","title":"python.library.multiprocessing#multiprocessing.Queue.get_nowait"},{"text":"offset  \nstart index of operation within bytecode sequence","title":"python.library.dis#dis.Instruction.offset"},{"text":"reverse()","title":"django.ref.models.querysets#django.db.models.query.QuerySet.reverse"},{"text":"dis.hasconst  \nSequence of bytecodes that access a constant.","title":"python.library.dis#dis.hasconst"},{"text":"lineno  \nThe line corresponding to pos (may be None).","title":"python.library.re#re.error.lineno"}]}
{"task_id":18897261,"prompt":"def f_18897261(df):\n\treturn ","suffix":"","canonical_solution":"df['group'].plot(kind='bar', color=['r', 'g', 'b', 'r', 'g', 'b', 'r'])","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame([1, 3, 4, 5, 7, 9], columns = ['group'])\n    a = candidate(df)\n    assert 'AxesSubplot' in str(type(a))\n"],"entry_point":"f_18897261","intent":"make a barplot of data in column `group` of dataframe `df` colour-coded according to list `color`","library":["pandas"],"docs":[{"text":"pandas.DataFrame.plot.bar   DataFrame.plot.bar(x=None, y=None, **kwargs)[source]\n \nVertical bar plot. A bar plot is a plot that presents categorical data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.  Parameters \n \nx:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, the index of the DataFrame is used.  \ny:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, all numerical columns are used.  \ncolor:str, array-like, or dict, optional\n\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:  \n A single color string referred to by name, RGB or RGBA code,\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.    \n A sequence of color strings referred to by name, RGB or RGBA\n\ncode, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    \n A dict of the form {column name:color}, so that each column will be\n\n\ncolored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and bars for column b in red.      New in version 1.1.0.   **kwargs\n\nAdditional keyword arguments are documented in DataFrame.plot().    Returns \n matplotlib.axes.Axes or np.ndarray of them\n\nAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  DataFrame.plot.barh\n\nHorizontal bar plot.  DataFrame.plot\n\nMake plots of a DataFrame.  matplotlib.pyplot.bar\n\nMake a bar plot with matplotlib.    Examples Basic plot. \n>>> df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})\n>>> ax = df.plot.bar(x='lab', y='val', rot=0)\n     Plot a whole dataframe to a bar plot. Each column is assigned a distinct color, and each row is nested in a group along the horizontal axis. \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.bar(rot=0)\n     Plot stacked bar charts for the DataFrame \n>>> ax = df.plot.bar(stacked=True)\n     Instead of nesting, the figure can be split by column with subplots=True. In this case, a numpy.ndarray of matplotlib.axes.Axes are returned. \n>>> axes = df.plot.bar(rot=0, subplots=True)\n>>> axes[1].legend(loc=2)  \n     If you don\u2019t like the default colours, you can specify how you\u2019d like each column to be colored. \n>>> axes = df.plot.bar(\n...     rot=0, subplots=True, color={\"speed\": \"red\", \"lifespan\": \"green\"}\n... )\n>>> axes[1].legend(loc=2)  \n     Plot a single column. \n>>> ax = df.plot.bar(y='speed', rot=0)\n     Plot only selected categories for the DataFrame. \n>>> ax = df.plot.bar(x='lifespan', rot=0)","title":"pandas.reference.api.pandas.dataframe.plot.bar"},{"text":"pandas.DataFrame.plot.barh   DataFrame.plot.barh(x=None, y=None, **kwargs)[source]\n \nMake a horizontal bar plot. A horizontal bar plot is a plot that presents quantitative data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.  Parameters \n \nx:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, the index of the DataFrame is used.  \ny:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, all numerical columns are used.  \ncolor:str, array-like, or dict, optional\n\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:  \n A single color string referred to by name, RGB or RGBA code,\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.    \n A sequence of color strings referred to by name, RGB or RGBA\n\ncode, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    \n A dict of the form {column name:color}, so that each column will be\n\n\ncolored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and bars for column b in red.      New in version 1.1.0.   **kwargs\n\nAdditional keyword arguments are documented in DataFrame.plot().    Returns \n matplotlib.axes.Axes or np.ndarray of them\n\nAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  DataFrame.plot.bar\n\nVertical bar plot.  DataFrame.plot\n\nMake plots of DataFrame using matplotlib.  matplotlib.axes.Axes.bar\n\nPlot a vertical bar plot using matplotlib.    Examples Basic example \n>>> df = pd.DataFrame({'lab': ['A', 'B', 'C'], 'val': [10, 30, 20]})\n>>> ax = df.plot.barh(x='lab', y='val')\n     Plot a whole DataFrame to a horizontal bar plot \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh()\n     Plot stacked barh charts for the DataFrame \n>>> ax = df.plot.barh(stacked=True)\n     We can specify colors for each column \n>>> ax = df.plot.barh(color={\"speed\": \"red\", \"lifespan\": \"green\"})\n     Plot a column of the DataFrame to a horizontal bar plot \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh(y='speed')\n     Plot DataFrame versus the desired column \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh(x='lifespan')","title":"pandas.reference.api.pandas.dataframe.plot.barh"},{"text":"pandas.Series.plot.bar   Series.plot.bar(x=None, y=None, **kwargs)[source]\n \nVertical bar plot. A bar plot is a plot that presents categorical data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.  Parameters \n \nx:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, the index of the DataFrame is used.  \ny:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, all numerical columns are used.  \ncolor:str, array-like, or dict, optional\n\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:  \n A single color string referred to by name, RGB or RGBA code,\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.    \n A sequence of color strings referred to by name, RGB or RGBA\n\ncode, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    \n A dict of the form {column name:color}, so that each column will be\n\n\ncolored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and bars for column b in red.      New in version 1.1.0.   **kwargs\n\nAdditional keyword arguments are documented in DataFrame.plot().    Returns \n matplotlib.axes.Axes or np.ndarray of them\n\nAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  DataFrame.plot.barh\n\nHorizontal bar plot.  DataFrame.plot\n\nMake plots of a DataFrame.  matplotlib.pyplot.bar\n\nMake a bar plot with matplotlib.    Examples Basic plot. \n>>> df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})\n>>> ax = df.plot.bar(x='lab', y='val', rot=0)\n     Plot a whole dataframe to a bar plot. Each column is assigned a distinct color, and each row is nested in a group along the horizontal axis. \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.bar(rot=0)\n     Plot stacked bar charts for the DataFrame \n>>> ax = df.plot.bar(stacked=True)\n     Instead of nesting, the figure can be split by column with subplots=True. In this case, a numpy.ndarray of matplotlib.axes.Axes are returned. \n>>> axes = df.plot.bar(rot=0, subplots=True)\n>>> axes[1].legend(loc=2)  \n     If you don\u2019t like the default colours, you can specify how you\u2019d like each column to be colored. \n>>> axes = df.plot.bar(\n...     rot=0, subplots=True, color={\"speed\": \"red\", \"lifespan\": \"green\"}\n... )\n>>> axes[1].legend(loc=2)  \n     Plot a single column. \n>>> ax = df.plot.bar(y='speed', rot=0)\n     Plot only selected categories for the DataFrame. \n>>> ax = df.plot.bar(x='lifespan', rot=0)","title":"pandas.reference.api.pandas.series.plot.bar"},{"text":"pandas.Series.plot.barh   Series.plot.barh(x=None, y=None, **kwargs)[source]\n \nMake a horizontal bar plot. A horizontal bar plot is a plot that presents quantitative data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.  Parameters \n \nx:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, the index of the DataFrame is used.  \ny:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, all numerical columns are used.  \ncolor:str, array-like, or dict, optional\n\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:  \n A single color string referred to by name, RGB or RGBA code,\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.    \n A sequence of color strings referred to by name, RGB or RGBA\n\ncode, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    \n A dict of the form {column name:color}, so that each column will be\n\n\ncolored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and bars for column b in red.      New in version 1.1.0.   **kwargs\n\nAdditional keyword arguments are documented in DataFrame.plot().    Returns \n matplotlib.axes.Axes or np.ndarray of them\n\nAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  DataFrame.plot.bar\n\nVertical bar plot.  DataFrame.plot\n\nMake plots of DataFrame using matplotlib.  matplotlib.axes.Axes.bar\n\nPlot a vertical bar plot using matplotlib.    Examples Basic example \n>>> df = pd.DataFrame({'lab': ['A', 'B', 'C'], 'val': [10, 30, 20]})\n>>> ax = df.plot.barh(x='lab', y='val')\n     Plot a whole DataFrame to a horizontal bar plot \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh()\n     Plot stacked barh charts for the DataFrame \n>>> ax = df.plot.barh(stacked=True)\n     We can specify colors for each column \n>>> ax = df.plot.barh(color={\"speed\": \"red\", \"lifespan\": \"green\"})\n     Plot a column of the DataFrame to a horizontal bar plot \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh(y='speed')\n     Plot DataFrame versus the desired column \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh(x='lifespan')","title":"pandas.reference.api.pandas.series.plot.barh"},{"text":"pandas.io.formats.style.Styler.bar   Styler.bar(subset=None, axis=0, *, color=None, cmap=None, width=100, height=100, align='mid', vmin=None, vmax=None, props='width: 10em;')[source]\n \nDraw bar chart in the cell backgrounds.  Changed in version 1.4.0.   Parameters \n \nsubset:label, array-like, IndexSlice, optional\n\n\nA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function.  \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default 0\n\n\nApply to each column (axis=0 or 'index'), to each row (axis=1 or 'columns'), or to the entire DataFrame at once with axis=None.  \ncolor:str or 2-tuple\/list\n\n\nIf a str is passed, the color is the same for both negative and positive numbers. If 2-tuple\/list is used, the first element is the color_negative and the second is the color_positive (eg: [\u2018#d65f5f\u2019, \u2018#5fba7d\u2019]).  \ncmap:str, matplotlib.cm.ColorMap\n\n\nA string name of a matplotlib Colormap, or a Colormap object. Cannot be used together with color.  New in version 1.4.0.   \nwidth:float, default 100\n\n\nThe percentage of the cell, measured from the left, in which to draw the bars, in [0, 100].  \nheight:float, default 100\n\n\nThe percentage height of the bar in the cell, centrally aligned, in [0,100].  New in version 1.4.0.   \nalign:str, int, float, callable, default \u2018mid\u2019\n\n\nHow to align the bars within the cells relative to a width adjusted center. If string must be one of:  \u2018left\u2019 : bars are drawn rightwards from the minimum data value. \u2018right\u2019 : bars are drawn leftwards from the maximum data value. \u2018zero\u2019 : a value of zero is located at the center of the cell. \u2018mid\u2019 : a value of (max-min)\/2 is located at the center of the cell, or if all values are negative (positive) the zero is aligned at the right (left) of the cell. \u2018mean\u2019 : the mean value of the data is located at the center of the cell.  If a float or integer is given this will indicate the center of the cell. If a callable should take a 1d or 2d array and return a scalar.  Changed in version 1.4.0.   \nvmin:float, optional\n\n\nMinimum bar value, defining the left hand limit of the bar drawing range, lower values are clipped to vmin. When None (default): the minimum value of the data will be used.  \nvmax:float, optional\n\n\nMaximum bar value, defining the right hand limit of the bar drawing range, higher values are clipped to vmax. When None (default): the maximum value of the data will be used.  \nprops:str, optional\n\n\nThe base CSS of the cell that is extended to add the bar chart. Defaults to \u201cwidth: 10em;\u201d.  New in version 1.4.0.     Returns \n \nself:Styler\n\n   Notes This section of the user guide: Table Visualization gives a number of examples for different settings and color coordination.","title":"pandas.reference.api.pandas.io.formats.style.styler.bar"},{"text":"pandas.DataFrame.plot.line   DataFrame.plot.line(x=None, y=None, **kwargs)[source]\n \nPlot Series or DataFrame as lines. This function is useful to plot lines using DataFrame\u2019s values as coordinates.  Parameters \n \nx:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, the index of the DataFrame is used.  \ny:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, all numerical columns are used.  \ncolor:str, array-like, or dict, optional\n\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:  \n A single color string referred to by name, RGB or RGBA code,\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.    \n A sequence of color strings referred to by name, RGB or RGBA\n\ncode, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s line will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    \n A dict of the form {column name:color}, so that each column will be\n\n\ncolored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color lines for column a in green and lines for column b in red.      New in version 1.1.0.   **kwargs\n\nAdditional keyword arguments are documented in DataFrame.plot().    Returns \n matplotlib.axes.Axes or np.ndarray of them\n\nAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  matplotlib.pyplot.plot\n\nPlot y versus x as lines and\/or markers.    Examples \n>>> s = pd.Series([1, 3, 2])\n>>> s.plot.line()\n<AxesSubplot:ylabel='Density'>\n     The following example shows the populations for some animals over the years. \n>>> df = pd.DataFrame({\n...    'pig': [20, 18, 489, 675, 1776],\n...    'horse': [4, 25, 281, 600, 1900]\n...    }, index=[1990, 1997, 2003, 2009, 2014])\n>>> lines = df.plot.line()\n     An example with subplots, so an array of axes is returned. \n>>> axes = df.plot.line(subplots=True)\n>>> type(axes)\n<class 'numpy.ndarray'>\n     Let\u2019s repeat the same example, but specifying colors for each column (in this case, for each animal). \n>>> axes = df.plot.line(\n...     subplots=True, color={\"pig\": \"pink\", \"horse\": \"#742802\"}\n... )\n     The following example shows the relationship between both populations. \n>>> lines = df.plot.line(x='pig', y='horse')","title":"pandas.reference.api.pandas.dataframe.plot.line"},{"text":"pandas.core.groupby.DataFrameGroupBy.boxplot   DataFrameGroupBy.boxplot(subplots=True, column=None, fontsize=None, rot=0, grid=True, ax=None, figsize=None, layout=None, sharex=False, sharey=True, backend=None, **kwargs)[source]\n \nMake box plots from DataFrameGroupBy data.  Parameters \n \ngrouped:Grouped DataFrame\n\n\nsubplots:bool\n\n\n False - no subplots will be used True - create a subplot for each group.   \ncolumn:column name or list of names, or vector\n\n\nCan be any valid input to groupby.  \nfontsize:int or str\n\n\nrot:label rotation angle\n\n\ngrid:Setting this to True will show the grid\n\n\nax:Matplotlib axis object, default None\n\n\nfigsize:A tuple (width, height) in inches\n\n\nlayout:tuple (optional)\n\n\nThe layout of the plot: (rows, columns).  \nsharex:bool, default False\n\n\nWhether x-axes will be shared among subplots.  \nsharey:bool, default True\n\n\nWhether y-axes will be shared among subplots.  \nbackend:str, default None\n\n\nBackend to use instead of the backend specified in the option plotting.backend. For instance, \u2018matplotlib\u2019. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend.  New in version 1.0.0.   **kwargs\n\nAll other plotting keyword arguments to be passed to matplotlib\u2019s boxplot function.    Returns \n dict of key\/value = group key\/DataFrame.boxplot return value\nor DataFrame.boxplot return value in case subplots=figures=False\n   Examples You can create boxplots for grouped data and show them as separate subplots: \n>>> import itertools\n>>> tuples = [t for t in itertools.product(range(1000), range(4))]\n>>> index = pd.MultiIndex.from_tuples(tuples, names=['lvl0', 'lvl1'])\n>>> data = np.random.randn(len(index),4)\n>>> df = pd.DataFrame(data, columns=list('ABCD'), index=index)\n>>> grouped = df.groupby(level='lvl1')\n>>> grouped.boxplot(rot=45, fontsize=12, figsize=(8,10))  \n     The subplots=False option shows the boxplots in a single figure. \n>>> grouped.boxplot(subplots=False, rot=45, fontsize=12)","title":"pandas.reference.api.pandas.core.groupby.dataframegroupby.boxplot"},{"text":"pandas.Series.plot.line   Series.plot.line(x=None, y=None, **kwargs)[source]\n \nPlot Series or DataFrame as lines. This function is useful to plot lines using DataFrame\u2019s values as coordinates.  Parameters \n \nx:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, the index of the DataFrame is used.  \ny:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, all numerical columns are used.  \ncolor:str, array-like, or dict, optional\n\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:  \n A single color string referred to by name, RGB or RGBA code,\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.    \n A sequence of color strings referred to by name, RGB or RGBA\n\ncode, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s line will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    \n A dict of the form {column name:color}, so that each column will be\n\n\ncolored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color lines for column a in green and lines for column b in red.      New in version 1.1.0.   **kwargs\n\nAdditional keyword arguments are documented in DataFrame.plot().    Returns \n matplotlib.axes.Axes or np.ndarray of them\n\nAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  matplotlib.pyplot.plot\n\nPlot y versus x as lines and\/or markers.    Examples \n>>> s = pd.Series([1, 3, 2])\n>>> s.plot.line()\n<AxesSubplot:ylabel='Density'>\n     The following example shows the populations for some animals over the years. \n>>> df = pd.DataFrame({\n...    'pig': [20, 18, 489, 675, 1776],\n...    'horse': [4, 25, 281, 600, 1900]\n...    }, index=[1990, 1997, 2003, 2009, 2014])\n>>> lines = df.plot.line()\n     An example with subplots, so an array of axes is returned. \n>>> axes = df.plot.line(subplots=True)\n>>> type(axes)\n<class 'numpy.ndarray'>\n     Let\u2019s repeat the same example, but specifying colors for each column (in this case, for each animal). \n>>> axes = df.plot.line(\n...     subplots=True, color={\"pig\": \"pink\", \"horse\": \"#742802\"}\n... )\n     The following example shows the relationship between both populations. \n>>> lines = df.plot.line(x='pig', y='horse')","title":"pandas.reference.api.pandas.series.plot.line"},{"text":"n_rasterize=50","title":"matplotlib.colorbar_api#matplotlib.colorbar.Colorbar.n_rasterize"},{"text":"parts","title":"matplotlib.type1font#matplotlib.type1font.Type1Font.parts"}]}
{"task_id":373194,"prompt":"def f_373194(data):\n\treturn ","suffix":"","canonical_solution":"re.findall('([a-fA-F\\\\d]{32})', data)","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    assert candidate('6f96cfdfe5ccc627cadf24b41725caa4 gorilla') ==         ['6f96cfdfe5ccc627cadf24b41725caa4']\n"],"entry_point":"f_373194","intent":"find all matches of regex pattern '([a-fA-F\\\\d]{32})' in string `data`","library":["re"],"docs":[{"text":"pattern  \nThe regular expression pattern.","title":"python.library.re#re.error.pattern"},{"text":"re.search(pattern, string, flags=0)  \nScan through string looking for the first location where the regular expression pattern produces a match, and return a corresponding match object. Return None if no position in the string matches the pattern; note that this is different from finding a zero-length match at some point in the string.","title":"python.library.re#re.search"},{"text":"fnmatch.translate(pattern)  \nReturn the shell-style pattern converted to a regular expression for using with re.match(). Example: >>> import fnmatch, re\n>>>\n>>> regex = fnmatch.translate('*.txt')\n>>> regex\n'(?s:.*\\\\.txt)\\\\Z'\n>>> reobj = re.compile(regex)\n>>> reobj.match('foobar.txt')\n<re.Match object; span=(0, 10), match='foobar.txt'>","title":"python.library.fnmatch#fnmatch.translate"},{"text":"re.purge()  \nClear the regular expression cache.","title":"python.library.re#re.purge"},{"text":"Pattern.search(string[, pos[, endpos]])  \nScan through string looking for the first location where this regular expression produces a match, and return a corresponding match object. Return None if no position in the string matches the pattern; note that this is different from finding a zero-length match at some point in the string. The optional second parameter pos gives an index in the string where the search is to start; it defaults to 0. This is not completely equivalent to slicing the string; the '^' pattern character matches at the real beginning of the string and at positions just after a newline, but not necessarily at the index where the search is to start. The optional parameter endpos limits how far the string will be searched; it will be as if the string is endpos characters long, so only the characters from pos to endpos - 1 will be searched for a match. If endpos is less than pos, no match will be found; otherwise, if rx is a compiled regular expression object, rx.search(string, 0, 50) is equivalent to rx.search(string[:50], 0). >>> pattern = re.compile(\"d\")\n>>> pattern.search(\"dog\")     # Match at index 0\n<re.Match object; span=(0, 1), match='d'>\n>>> pattern.search(\"dog\", 1)  # No match; search doesn't include the \"d\"","title":"python.library.re#re.Pattern.search"},{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"re.findall(pattern, string, flags=0)  \nReturn all non-overlapping matches of pattern in string, as a list of strings. The string is scanned left-to-right, and matches are returned in the order found. If one or more groups are present in the pattern, return a list of groups; this will be a list of tuples if the pattern has more than one group. Empty matches are included in the result.  Changed in version 3.7: Non-empty matches can now start just after a previous empty match.","title":"python.library.re#re.findall"},{"text":"Pattern.findall(string[, pos[, endpos]])  \nSimilar to the findall() function, using the compiled pattern, but also accepts optional pos and endpos parameters that limit the search region like for search().","title":"python.library.re#re.Pattern.findall"},{"text":"str.rindex(sub[, start[, end]])  \nLike rfind() but raises ValueError when the substring sub is not found.","title":"python.library.stdtypes#str.rindex"},{"text":"re.fullmatch(pattern, string, flags=0)  \nIf the whole string matches the regular expression pattern, return a corresponding match object. Return None if the string does not match the pattern; note that this is different from a zero-length match.  New in version 3.4.","title":"python.library.re#re.fullmatch"}]}
{"task_id":518021,"prompt":"def f_518021(my_list):\n\treturn ","suffix":"","canonical_solution":"len(my_list)","test_start":"\ndef check(candidate):","test":["\n    assert candidate([]) == 0\n","\n    assert candidate([1]) == 1\n","\n    assert candidate([1, 2]) == 2\n"],"entry_point":"f_518021","intent":"Get the length of list `my_list`","library":[],"docs":[]}
{"task_id":518021,"prompt":"def f_518021(l):\n\treturn ","suffix":"","canonical_solution":"len(l)","test_start":"\nimport numpy as np \n\ndef check(candidate):","test":["\n    assert candidate([]) == 0\n","\n    assert candidate(np.array([1])) == 1\n","\n    assert candidate(np.array([1, 2])) == 2\n"],"entry_point":"f_518021","intent":"Getting the length of array `l`","library":["numpy"],"docs":[]}
{"task_id":518021,"prompt":"def f_518021(s):\n\treturn ","suffix":"","canonical_solution":"len(s)","test_start":"\nimport numpy as np \n\ndef check(candidate):","test":["\n    assert candidate([]) == 0\n","\n    assert candidate(np.array([1])) == 1\n","\n    assert candidate(np.array([1, 2])) == 2\n"],"entry_point":"f_518021","intent":"Getting the length of array `s`","library":["numpy"],"docs":[]}
{"task_id":518021,"prompt":"def f_518021(my_tuple):\n\treturn ","suffix":"","canonical_solution":"len(my_tuple)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(()) == 0\n","\n    assert candidate(('aa', 'wfseg', '')) == 3\n","\n    assert candidate(('apple',)) == 1\n"],"entry_point":"f_518021","intent":"Getting the length of `my_tuple`","library":[],"docs":[]}
{"task_id":518021,"prompt":"def f_518021(my_string):\n\treturn ","suffix":"","canonical_solution":"len(my_string)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"sedfgbdjofgljnh\") == 15\n","\n    assert candidate(\"             \") == 13\n","\n    assert candidate(\"vsdh4'cdf'\") == 10\n"],"entry_point":"f_518021","intent":"Getting the length of `my_string`","library":[],"docs":[]}
{"task_id":40452956,"prompt":"def f_40452956():\n\treturn ","suffix":"","canonical_solution":"b'\\\\a'.decode('unicode-escape')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == '\\x07'\n"],"entry_point":"f_40452956","intent":"remove escape character from string \"\\\\a\"","library":[],"docs":[]}
{"task_id":8687018,"prompt":"def f_8687018():\n\treturn ","suffix":"","canonical_solution":"\"\"\"obama\"\"\".replace('a', '%temp%').replace('b', 'a').replace('%temp%', 'b')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == 'oabmb'\n"],"entry_point":"f_8687018","intent":"replace each 'a' with 'b' and each 'b' with 'a' in the string 'obama' in a single pass.","library":[],"docs":[]}
{"task_id":303200,"prompt":"def f_303200():\n\t","suffix":"\n\treturn ","canonical_solution":"shutil.rmtree('\/folder_name')","test_start":"\nimport os\nimport shutil\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    shutil.rmtree = Mock()\n    os.walk = Mock(return_value = [])\n    candidate()\n    assert os.walk('\/') == []\n"],"entry_point":"f_303200","intent":"remove directory tree '\/folder_name'","library":["os","shutil"],"docs":[{"text":"Path.rmdir()  \nRemove this directory. The directory must be empty.","title":"python.library.pathlib#pathlib.Path.rmdir"},{"text":"FTP.rmd(dirname)  \nRemove the directory named dirname on the server.","title":"python.library.ftplib#ftplib.FTP.rmd"},{"text":"left  \nThe directory a.","title":"python.library.filecmp#filecmp.dircmp.left"},{"text":"re_path(route, view, kwargs=None, name=None)","title":"django.ref.urls#django.urls.re_path"},{"text":"test.support.unload(name)  \nDelete name from sys.modules.","title":"python.library.test#test.support.unload"},{"text":"remove_pyc()  \nRemove .pyc files on uninstall.","title":"python.library.msilib#msilib.Directory.remove_pyc"},{"text":"remove_folder(folder)  \nDelete the folder whose name is folder. If the folder contains any messages, a NotEmptyError exception will be raised and the folder will not be deleted.","title":"python.library.mailbox#mailbox.Maildir.remove_folder"},{"text":"right  \nThe directory b.","title":"python.library.filecmp#filecmp.dircmp.right"},{"text":"test.support.rmtree(path)  \nCall shutil.rmtree() on path or call os.lstat() and os.rmdir() to remove a path and its contents. On Windows platforms, this is wrapped with a wait loop that checks for the existence of the files.","title":"python.library.test#test.support.rmtree"},{"text":"path(route, view, kwargs=None, name=None)","title":"django.ref.urls#django.urls.path"}]}
{"task_id":13740672,"prompt":"def f_13740672(data):\n\t","suffix":"\n\treturn data","canonical_solution":"\n    def weekday(i):\n        if i >=1 and i <= 5: return True\n        else: return False\n    data['weekday'] = data['my_dt'].apply(lambda x: weekday(x))\n","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    data = pd.DataFrame([1, 2, 3, 4, 5, 6, 7], columns = ['my_dt'])\n    data = candidate(data)\n    assert data['weekday'][5] == False\n    assert data['weekday'][6] == False\n    for i in range (0, 5):\n        assert data['weekday'][i]\n"],"entry_point":"f_13740672","intent":"create a new column `weekday` in pandas data frame `data` based on the values in column `my_dt`","library":["pandas"],"docs":[{"text":"pandas.tseries.offsets.Week.weekday   Week.weekday","title":"pandas.reference.api.pandas.tseries.offsets.week.weekday"},{"text":"pandas.tseries.offsets.FY5253Quarter.weekday   FY5253Quarter.weekday","title":"pandas.reference.api.pandas.tseries.offsets.fy5253quarter.weekday"},{"text":"pandas.tseries.offsets.WeekOfMonth.weekday   WeekOfMonth.weekday","title":"pandas.reference.api.pandas.tseries.offsets.weekofmonth.weekday"},{"text":"pandas.tseries.offsets.FY5253.weekday   FY5253.weekday","title":"pandas.reference.api.pandas.tseries.offsets.fy5253.weekday"},{"text":"pandas.tseries.offsets.LastWeekOfMonth.weekday   LastWeekOfMonth.weekday","title":"pandas.reference.api.pandas.tseries.offsets.lastweekofmonth.weekday"},{"text":"pandas.tseries.offsets.Day.copy   Day.copy()","title":"pandas.reference.api.pandas.tseries.offsets.day.copy"},{"text":"pandas.tseries.offsets.Week.apply   Week.apply()","title":"pandas.reference.api.pandas.tseries.offsets.week.apply"},{"text":"pandas.Timestamp.day   Timestamp.day","title":"pandas.reference.api.pandas.timestamp.day"},{"text":"pandas.tseries.offsets.Day.apply   Day.apply()","title":"pandas.reference.api.pandas.tseries.offsets.day.apply"},{"text":"pandas.tseries.offsets.Day.name   Day.name","title":"pandas.reference.api.pandas.tseries.offsets.day.name"}]}
{"task_id":20950650,"prompt":"def f_20950650(x):\n\treturn ","suffix":"","canonical_solution":"sorted(x, key=x.get, reverse=True)","test_start":"\nfrom collections import Counter\n\ndef check(candidate):","test":["\n    x = Counter({'blue': 1, 'red': 2, 'green': 3})\n    assert candidate(x) == ['green', 'red', 'blue']\n","\n    x = Counter({'blue': 1.234, 'red': 1.35, 'green': 1.789})\n    assert candidate(x) == ['green', 'red', 'blue']\n","\n    x = Counter({'blue': \"b\", 'red': \"r\", 'green': \"g\"})\n    assert candidate(x) == ['red', 'green', 'blue']\n"],"entry_point":"f_20950650","intent":"reverse sort Counter `x` by values","library":["collections"],"docs":[]}
{"task_id":20950650,"prompt":"def f_20950650(x):\n\treturn ","suffix":"","canonical_solution":"sorted(list(x.items()), key=lambda pair: pair[1], reverse=True)","test_start":"\nfrom collections import Counter\n\ndef check(candidate):","test":["\n    x = Counter({'blue': 1, 'red': 2, 'green': 3})\n    assert candidate(x) == [('green', 3), ('red', 2), ('blue', 1)]\n","\n    x = Counter({'blue': 1.234, 'red': 1.35, 'green': 1.789})\n    assert candidate(x) == [('green', 1.789), ('red', 1.35), ('blue', 1.234)]\n","\n    x = Counter({'blue': \"b\", 'red': \"r\", 'green': \"g\"})\n    assert candidate(x) == [('red', \"r\"), ('green', \"g\"), ('blue', \"b\")]\n"],"entry_point":"f_20950650","intent":"reverse sort counter `x` by value","library":["collections"],"docs":[]}
{"task_id":9775297,"prompt":"def f_9775297(a, b):\n\treturn ","suffix":"","canonical_solution":"np.vstack((a, b))","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    a = np.array([[1, 2, 3], [4, 5, 6]])\n    b = np.array([[9, 8, 7], [6, 5, 4]])\n    assert np.array_equal(candidate(a, b), np.array([[1, 2, 3], [4, 5, 6], [9, 8, 7], [6, 5, 4]]))\n","\n    a = np.array([[1, 2.45, 3], [4, 0.55, 612]])\n    b = np.array([[988, 8, 7], [6, 512, 4]])\n    assert np.array_equal(candidate(a, b), np.array([[1, 2.45, 3], [4, 0.55, 612], [988, 8, 7], [6, 512, 4]]))\n"],"entry_point":"f_9775297","intent":"append a numpy array 'b' to a numpy array 'a'","library":["numpy"],"docs":[{"text":"array.append(x)  \nAppend a new item with value x to the end of the array.","title":"python.library.array#array.array.append"},{"text":"pandas.tseries.offsets.BQuarterEnd.copy   BQuarterEnd.copy()","title":"pandas.reference.api.pandas.tseries.offsets.bquarterend.copy"},{"text":"pandas.tseries.offsets.BQuarterBegin.copy   BQuarterBegin.copy()","title":"pandas.reference.api.pandas.tseries.offsets.bquarterbegin.copy"},{"text":"numpy.complex_[source]\n \nalias of numpy.cdouble","title":"numpy.reference.arrays.scalars#numpy.complex_"},{"text":"numpy.string_[source]\n \nalias of numpy.bytes_","title":"numpy.reference.arrays.scalars#numpy.string_"},{"text":"numpy.clongfloat[source]\n \nalias of numpy.clongdouble","title":"numpy.reference.arrays.scalars#numpy.clongfloat"},{"text":"numpy.distutils.misc_util.dict_append(d, **kws)[source]","title":"numpy.reference.distutils.misc_util#numpy.distutils.misc_util.dict_append"},{"text":"numpy.cfloat[source]\n \nalias of numpy.cdouble","title":"numpy.reference.arrays.scalars#numpy.cfloat"},{"text":"numpy.unicode_[source]\n \nalias of numpy.str_","title":"numpy.reference.arrays.scalars#numpy.unicode_"},{"text":"numpy.float64[source]\n \nalias of numpy.double","title":"numpy.reference.arrays.scalars#numpy.float64"}]}
{"task_id":21887754,"prompt":"def f_21887754(a, b):\n\treturn ","suffix":"","canonical_solution":"np.concatenate((a, b), axis=0)","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    a = np.array([[1, 5, 9], [2, 6, 10]])\n    b = np.array([[3, 7, 11], [4, 8, 12]])\n    assert np.array_equal(candidate(a, b), np.array([[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]]))\n","\n    a = np.array([[1, 2.45, 3], [4, 0.55, 612]])\n    b = np.array([[988, 8, 7], [6, 512, 4]])\n    assert np.array_equal(candidate(a, b), np.array([[1, 2.45, 3], [4, 0.55, 612], [988, 8, 7], [6, 512, 4]]))\n"],"entry_point":"f_21887754","intent":"numpy concatenate two arrays `a` and `b` along the first axis","library":["numpy"],"docs":[{"text":"numpy.concatenate   numpy.concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")\n \nJoin a sequence of arrays along an existing axis.  Parameters \n \na1, a2, \u2026sequence of array_like\n\n\nThe arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  \naxisint, optional\n\n\nThe axis along which the arrays will be joined. If axis is None, arrays are flattened before use. Default is 0.  \noutndarray, optional\n\n\nIf provided, the destination to place the result. The shape must be correct, matching that of what concatenate would have returned if no out argument were specified.  \ndtypestr or dtype\n\n\nIf provided, the destination array will have this dtype. Cannot be provided together with out.  New in version 1.20.0.   \ncasting{\u2018no\u2019, \u2018equiv\u2019, \u2018safe\u2019, \u2018same_kind\u2019, \u2018unsafe\u2019}, optional\n\n\nControls what kind of data casting may occur. Defaults to \u2018same_kind\u2019.  New in version 1.20.0.     Returns \n \nresndarray\n\n\nThe concatenated array.      See also  ma.concatenate\n\nConcatenate function that preserves input masks.  array_split\n\nSplit an array into multiple sub-arrays of equal or near-equal size.  split\n\nSplit array into a list of multiple sub-arrays of equal size.  hsplit\n\nSplit array into multiple sub-arrays horizontally (column wise).  vsplit\n\nSplit array into multiple sub-arrays vertically (row wise).  dsplit\n\nSplit array into multiple sub-arrays along the 3rd axis (depth).  stack\n\nStack a sequence of arrays along a new axis.  block\n\nAssemble arrays from blocks.  hstack\n\nStack arrays in sequence horizontally (column wise).  vstack\n\nStack arrays in sequence vertically (row wise).  dstack\n\nStack arrays in sequence depth wise (along third dimension).  column_stack\n\nStack 1-D arrays as columns into a 2-D array.    Notes When one or more of the arrays to be concatenated is a MaskedArray, this function will return a MaskedArray object instead of an ndarray, but the input masks are not preserved. In cases where a MaskedArray is expected as input, use the ma.concatenate function from the masked array module instead. Examples >>> a = np.array([[1, 2], [3, 4]])\n>>> b = np.array([[5, 6]])\n>>> np.concatenate((a, b), axis=0)\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n>>> np.concatenate((a, b.T), axis=1)\narray([[1, 2, 5],\n       [3, 4, 6]])\n>>> np.concatenate((a, b), axis=None)\narray([1, 2, 3, 4, 5, 6])\n This function will not preserve masking of MaskedArray inputs. >>> a = np.ma.arange(3)\n>>> a[1] = np.ma.masked\n>>> b = np.arange(2, 5)\n>>> a\nmasked_array(data=[0, --, 2],\n             mask=[False,  True, False],\n       fill_value=999999)\n>>> b\narray([2, 3, 4])\n>>> np.concatenate([a, b])\nmasked_array(data=[0, 1, 2, 2, 3, 4],\n             mask=False,\n       fill_value=999999)\n>>> np.ma.concatenate([a, b])\nmasked_array(data=[0, --, 2, 2, 3, 4],\n             mask=[False,  True, False, False, False, False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.concatenate"},{"text":"numpy.ma.concatenate   ma.concatenate(arrays, axis=0)[source]\n \nConcatenate a sequence of arrays along the given axis.  Parameters \n \narrayssequence of array_like\n\n\nThe arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  \naxisint, optional\n\n\nThe axis along which the arrays will be joined. Default is 0.    Returns \n \nresultMaskedArray\n\n\nThe concatenated array with any masked entries preserved.      See also  numpy.concatenate\n\nEquivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma\n>>> a = ma.arange(3)\n>>> a[1] = ma.masked\n>>> b = ma.arange(2, 5)\n>>> a\nmasked_array(data=[0, --, 2],\n             mask=[False,  True, False],\n       fill_value=999999)\n>>> b\nmasked_array(data=[2, 3, 4],\n             mask=False,\n       fill_value=999999)\n>>> ma.concatenate([a, b])\nmasked_array(data=[0, --, 2, 2, 3, 4],\n             mask=[False,  True, False, False, False, False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.ma.concatenate"},{"text":"concat_matrix=b'cm'[source]","title":"matplotlib.backend_pdf_api#matplotlib.backends.backend_pdf.Op.concat_matrix"},{"text":"numpy.clongfloat[source]\n \nalias of numpy.clongdouble","title":"numpy.reference.arrays.scalars#numpy.clongfloat"},{"text":"numpy.c_   numpy.c_ = <numpy.lib.index_tricks.CClass object>\n \nTranslates slice objects to concatenation along the second axis. This is short-hand for np.r_['-1,2,0', index expression], which is useful because of its common occurrence. In particular, arrays will be stacked along their last axis after being upgraded to at least 2-D with 1\u2019s post-pended to the shape (column vectors made out of 1-D arrays).  See also  column_stack\n\nStack 1-D arrays as columns into a 2-D array.  r_\n\nFor more detailed documentation.    Examples >>> np.c_[np.array([1,2,3]), np.array([4,5,6])]\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n>>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]\narray([[1, 2, 3, ..., 4, 5, 6]])","title":"numpy.reference.generated.numpy.c_"},{"text":"numpy.longfloat[source]\n \nalias of numpy.longdouble","title":"numpy.reference.arrays.scalars#numpy.longfloat"},{"text":"pandas.tseries.offsets.Second.copy   Second.copy()","title":"pandas.reference.api.pandas.tseries.offsets.second.copy"},{"text":"numpy.r_   numpy.r_ = <numpy.lib.index_tricks.RClass object>\n \nTranslates slice objects to concatenation along the first axis. This is a simple way to build up arrays quickly. There are two use cases.  If the index expression contains comma separated arrays, then stack them along their first axis. If the index expression contains slice notation or scalars then create a 1-D array with a range indicated by the slice notation.  If slice notation is used, the syntax start:stop:step is equivalent to np.arange(start, stop, step) inside of the brackets. However, if step is an imaginary number (i.e. 100j) then its integer portion is interpreted as a number-of-points desired and the start and stop are inclusive. In other words start:stop:stepj is interpreted as np.linspace(start, stop, step, endpoint=1) inside of the brackets. After expansion of slice notation, all comma separated sequences are concatenated together. Optional character strings placed as the first element of the index expression can be used to change the output. The strings \u2018r\u2019 or \u2018c\u2019 result in matrix output. If the result is 1-D and \u2018r\u2019 is specified a 1 x N (row) matrix is produced. If the result is 1-D and \u2018c\u2019 is specified, then a N x 1 (column) matrix is produced. If the result is 2-D then both provide the same matrix result. A string integer specifies which axis to stack multiple comma separated arrays along. A string of two comma-separated integers allows indication of the minimum number of dimensions to force each entry into as the second integer (the axis to concatenate along is still the first integer). A string with three comma-separated integers allows specification of the axis to concatenate along, the minimum number of dimensions to force the entries to, and which axis should contain the start of the arrays which are less than the specified number of dimensions. In other words the third integer allows you to specify where the 1\u2019s should be placed in the shape of the arrays that have their shapes upgraded. By default, they are placed in the front of the shape tuple. The third argument allows you to specify where the start of the array should be instead. Thus, a third argument of \u20180\u2019 would place the 1\u2019s at the end of the array shape. Negative integers specify where in the new shape tuple the last dimension of upgraded arrays should be placed, so the default is \u2018-1\u2019.  Parameters \n Not a function, so takes no parameters\n  Returns \n A concatenated ndarray or matrix.\n    See also  concatenate\n\nJoin a sequence of arrays along an existing axis.  c_\n\nTranslates slice objects to concatenation along the second axis.    Examples >>> np.r_[np.array([1,2,3]), 0, 0, np.array([4,5,6])]\narray([1, 2, 3, ..., 4, 5, 6])\n>>> np.r_[-1:1:6j, [0]*3, 5, 6]\narray([-1. , -0.6, -0.2,  0.2,  0.6,  1. ,  0. ,  0. ,  0. ,  5. ,  6. ])\n String integers specify the axis to concatenate along or the minimum number of dimensions to force entries into. >>> a = np.array([[0, 1, 2], [3, 4, 5]])\n>>> np.r_['-1', a, a] # concatenate along last axis\narray([[0, 1, 2, 0, 1, 2],\n       [3, 4, 5, 3, 4, 5]])\n>>> np.r_['0,2', [1,2,3], [4,5,6]] # concatenate along first axis, dim>=2\narray([[1, 2, 3],\n       [4, 5, 6]])\n >>> np.r_['0,2,0', [1,2,3], [4,5,6]]\narray([[1],\n       [2],\n       [3],\n       [4],\n       [5],\n       [6]])\n>>> np.r_['1,2,0', [1,2,3], [4,5,6]]\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n Using \u2018r\u2019 or \u2018c\u2019 as a first string argument creates a matrix. >>> np.r_['r',[1,2,3], [4,5,6]]\nmatrix([[1, 2, 3, 4, 5, 6]])","title":"numpy.reference.generated.numpy.r_"},{"text":"numpy.cfloat[source]\n \nalias of numpy.cdouble","title":"numpy.reference.arrays.scalars#numpy.cfloat"},{"text":"pandas.tseries.offsets.BQuarterEnd.copy   BQuarterEnd.copy()","title":"pandas.reference.api.pandas.tseries.offsets.bquarterend.copy"}]}
{"task_id":21887754,"prompt":"def f_21887754(a, b):\n\treturn ","suffix":"","canonical_solution":"np.concatenate((a, b), axis=1)","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    a = np.array([[1, 5, 9], [2, 6, 10]])\n    b = np.array([[3, 7, 11], [4, 8, 12]])\n    assert np.array_equal(candidate(a, b), np.array([[1, 5, 9, 3, 7, 11], [2, 6, 10, 4, 8, 12]]))\n","\n    a = np.array([[1, 2.45, 3], [4, 0.55, 612]])\n    b = np.array([[988, 8, 7], [6, 512, 4]])\n    assert np.array_equal(candidate(a, b), np.array([[1, 2.45, 3, 988, 8, 7], [4, 0.55, 612, 6, 512, 4]]))\n"],"entry_point":"f_21887754","intent":"numpy concatenate two arrays `a` and `b` along the second axis","library":["numpy"],"docs":[{"text":"numpy.concatenate   numpy.concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")\n \nJoin a sequence of arrays along an existing axis.  Parameters \n \na1, a2, \u2026sequence of array_like\n\n\nThe arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  \naxisint, optional\n\n\nThe axis along which the arrays will be joined. If axis is None, arrays are flattened before use. Default is 0.  \noutndarray, optional\n\n\nIf provided, the destination to place the result. The shape must be correct, matching that of what concatenate would have returned if no out argument were specified.  \ndtypestr or dtype\n\n\nIf provided, the destination array will have this dtype. Cannot be provided together with out.  New in version 1.20.0.   \ncasting{\u2018no\u2019, \u2018equiv\u2019, \u2018safe\u2019, \u2018same_kind\u2019, \u2018unsafe\u2019}, optional\n\n\nControls what kind of data casting may occur. Defaults to \u2018same_kind\u2019.  New in version 1.20.0.     Returns \n \nresndarray\n\n\nThe concatenated array.      See also  ma.concatenate\n\nConcatenate function that preserves input masks.  array_split\n\nSplit an array into multiple sub-arrays of equal or near-equal size.  split\n\nSplit array into a list of multiple sub-arrays of equal size.  hsplit\n\nSplit array into multiple sub-arrays horizontally (column wise).  vsplit\n\nSplit array into multiple sub-arrays vertically (row wise).  dsplit\n\nSplit array into multiple sub-arrays along the 3rd axis (depth).  stack\n\nStack a sequence of arrays along a new axis.  block\n\nAssemble arrays from blocks.  hstack\n\nStack arrays in sequence horizontally (column wise).  vstack\n\nStack arrays in sequence vertically (row wise).  dstack\n\nStack arrays in sequence depth wise (along third dimension).  column_stack\n\nStack 1-D arrays as columns into a 2-D array.    Notes When one or more of the arrays to be concatenated is a MaskedArray, this function will return a MaskedArray object instead of an ndarray, but the input masks are not preserved. In cases where a MaskedArray is expected as input, use the ma.concatenate function from the masked array module instead. Examples >>> a = np.array([[1, 2], [3, 4]])\n>>> b = np.array([[5, 6]])\n>>> np.concatenate((a, b), axis=0)\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n>>> np.concatenate((a, b.T), axis=1)\narray([[1, 2, 5],\n       [3, 4, 6]])\n>>> np.concatenate((a, b), axis=None)\narray([1, 2, 3, 4, 5, 6])\n This function will not preserve masking of MaskedArray inputs. >>> a = np.ma.arange(3)\n>>> a[1] = np.ma.masked\n>>> b = np.arange(2, 5)\n>>> a\nmasked_array(data=[0, --, 2],\n             mask=[False,  True, False],\n       fill_value=999999)\n>>> b\narray([2, 3, 4])\n>>> np.concatenate([a, b])\nmasked_array(data=[0, 1, 2, 2, 3, 4],\n             mask=False,\n       fill_value=999999)\n>>> np.ma.concatenate([a, b])\nmasked_array(data=[0, --, 2, 2, 3, 4],\n             mask=[False,  True, False, False, False, False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.concatenate"},{"text":"numpy.ma.concatenate   ma.concatenate(arrays, axis=0)[source]\n \nConcatenate a sequence of arrays along the given axis.  Parameters \n \narrayssequence of array_like\n\n\nThe arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  \naxisint, optional\n\n\nThe axis along which the arrays will be joined. Default is 0.    Returns \n \nresultMaskedArray\n\n\nThe concatenated array with any masked entries preserved.      See also  numpy.concatenate\n\nEquivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma\n>>> a = ma.arange(3)\n>>> a[1] = ma.masked\n>>> b = ma.arange(2, 5)\n>>> a\nmasked_array(data=[0, --, 2],\n             mask=[False,  True, False],\n       fill_value=999999)\n>>> b\nmasked_array(data=[2, 3, 4],\n             mask=False,\n       fill_value=999999)\n>>> ma.concatenate([a, b])\nmasked_array(data=[0, --, 2, 2, 3, 4],\n             mask=[False,  True, False, False, False, False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.ma.concatenate"},{"text":"numpy.clongfloat[source]\n \nalias of numpy.clongdouble","title":"numpy.reference.arrays.scalars#numpy.clongfloat"},{"text":"pandas.tseries.offsets.Second.copy   Second.copy()","title":"pandas.reference.api.pandas.tseries.offsets.second.copy"},{"text":"concat_matrix=b'cm'[source]","title":"matplotlib.backend_pdf_api#matplotlib.backends.backend_pdf.Op.concat_matrix"},{"text":"numpy.c_   numpy.c_ = <numpy.lib.index_tricks.CClass object>\n \nTranslates slice objects to concatenation along the second axis. This is short-hand for np.r_['-1,2,0', index expression], which is useful because of its common occurrence. In particular, arrays will be stacked along their last axis after being upgraded to at least 2-D with 1\u2019s post-pended to the shape (column vectors made out of 1-D arrays).  See also  column_stack\n\nStack 1-D arrays as columns into a 2-D array.  r_\n\nFor more detailed documentation.    Examples >>> np.c_[np.array([1,2,3]), np.array([4,5,6])]\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n>>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]\narray([[1, 2, 3, ..., 4, 5, 6]])","title":"numpy.reference.generated.numpy.c_"},{"text":"numpy.longfloat[source]\n \nalias of numpy.longdouble","title":"numpy.reference.arrays.scalars#numpy.longfloat"},{"text":"pandas.tseries.offsets.BQuarterEnd.copy   BQuarterEnd.copy()","title":"pandas.reference.api.pandas.tseries.offsets.bquarterend.copy"},{"text":"pandas.tseries.offsets.Second.normalize   Second.normalize","title":"pandas.reference.api.pandas.tseries.offsets.second.normalize"},{"text":"pandas.tseries.offsets.BQuarterBegin.copy   BQuarterBegin.copy()","title":"pandas.reference.api.pandas.tseries.offsets.bquarterbegin.copy"}]}
{"task_id":21887754,"prompt":"def f_21887754(a, b):\n\treturn ","suffix":"","canonical_solution":"np.r_[(a[None, :], b[None, :])]","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    a = np.array([[1, 5, 9], [2, 6, 10]])\n    b = np.array([[3, 7, 11], [4, 8, 12]])\n    assert np.array_equal(candidate(a, b), np.array([[[1, 5, 9], [2, 6, 10]], [[3, 7, 11], [4, 8, 12]]]))\n","\n    a = np.array([[1, 2.45, 3], [4, 0.55, 612]])\n    b = np.array([[988, 8, 7], [6, 512, 4]])\n    assert np.array_equal(candidate(a, b), np.array([[[1, 2.45, 3], [4, 0.55, 612]], [[988, 8 , 7], [6, 512, 4]]]))\n"],"entry_point":"f_21887754","intent":"numpy concatenate two arrays `a` and `b` along the first axis","library":["numpy"],"docs":[{"text":"numpy.concatenate   numpy.concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")\n \nJoin a sequence of arrays along an existing axis.  Parameters \n \na1, a2, \u2026sequence of array_like\n\n\nThe arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  \naxisint, optional\n\n\nThe axis along which the arrays will be joined. If axis is None, arrays are flattened before use. Default is 0.  \noutndarray, optional\n\n\nIf provided, the destination to place the result. The shape must be correct, matching that of what concatenate would have returned if no out argument were specified.  \ndtypestr or dtype\n\n\nIf provided, the destination array will have this dtype. Cannot be provided together with out.  New in version 1.20.0.   \ncasting{\u2018no\u2019, \u2018equiv\u2019, \u2018safe\u2019, \u2018same_kind\u2019, \u2018unsafe\u2019}, optional\n\n\nControls what kind of data casting may occur. Defaults to \u2018same_kind\u2019.  New in version 1.20.0.     Returns \n \nresndarray\n\n\nThe concatenated array.      See also  ma.concatenate\n\nConcatenate function that preserves input masks.  array_split\n\nSplit an array into multiple sub-arrays of equal or near-equal size.  split\n\nSplit array into a list of multiple sub-arrays of equal size.  hsplit\n\nSplit array into multiple sub-arrays horizontally (column wise).  vsplit\n\nSplit array into multiple sub-arrays vertically (row wise).  dsplit\n\nSplit array into multiple sub-arrays along the 3rd axis (depth).  stack\n\nStack a sequence of arrays along a new axis.  block\n\nAssemble arrays from blocks.  hstack\n\nStack arrays in sequence horizontally (column wise).  vstack\n\nStack arrays in sequence vertically (row wise).  dstack\n\nStack arrays in sequence depth wise (along third dimension).  column_stack\n\nStack 1-D arrays as columns into a 2-D array.    Notes When one or more of the arrays to be concatenated is a MaskedArray, this function will return a MaskedArray object instead of an ndarray, but the input masks are not preserved. In cases where a MaskedArray is expected as input, use the ma.concatenate function from the masked array module instead. Examples >>> a = np.array([[1, 2], [3, 4]])\n>>> b = np.array([[5, 6]])\n>>> np.concatenate((a, b), axis=0)\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n>>> np.concatenate((a, b.T), axis=1)\narray([[1, 2, 5],\n       [3, 4, 6]])\n>>> np.concatenate((a, b), axis=None)\narray([1, 2, 3, 4, 5, 6])\n This function will not preserve masking of MaskedArray inputs. >>> a = np.ma.arange(3)\n>>> a[1] = np.ma.masked\n>>> b = np.arange(2, 5)\n>>> a\nmasked_array(data=[0, --, 2],\n             mask=[False,  True, False],\n       fill_value=999999)\n>>> b\narray([2, 3, 4])\n>>> np.concatenate([a, b])\nmasked_array(data=[0, 1, 2, 2, 3, 4],\n             mask=False,\n       fill_value=999999)\n>>> np.ma.concatenate([a, b])\nmasked_array(data=[0, --, 2, 2, 3, 4],\n             mask=[False,  True, False, False, False, False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.concatenate"},{"text":"numpy.ma.concatenate   ma.concatenate(arrays, axis=0)[source]\n \nConcatenate a sequence of arrays along the given axis.  Parameters \n \narrayssequence of array_like\n\n\nThe arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  \naxisint, optional\n\n\nThe axis along which the arrays will be joined. Default is 0.    Returns \n \nresultMaskedArray\n\n\nThe concatenated array with any masked entries preserved.      See also  numpy.concatenate\n\nEquivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma\n>>> a = ma.arange(3)\n>>> a[1] = ma.masked\n>>> b = ma.arange(2, 5)\n>>> a\nmasked_array(data=[0, --, 2],\n             mask=[False,  True, False],\n       fill_value=999999)\n>>> b\nmasked_array(data=[2, 3, 4],\n             mask=False,\n       fill_value=999999)\n>>> ma.concatenate([a, b])\nmasked_array(data=[0, --, 2, 2, 3, 4],\n             mask=[False,  True, False, False, False, False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.ma.concatenate"},{"text":"concat_matrix=b'cm'[source]","title":"matplotlib.backend_pdf_api#matplotlib.backends.backend_pdf.Op.concat_matrix"},{"text":"numpy.clongfloat[source]\n \nalias of numpy.clongdouble","title":"numpy.reference.arrays.scalars#numpy.clongfloat"},{"text":"numpy.c_   numpy.c_ = <numpy.lib.index_tricks.CClass object>\n \nTranslates slice objects to concatenation along the second axis. This is short-hand for np.r_['-1,2,0', index expression], which is useful because of its common occurrence. In particular, arrays will be stacked along their last axis after being upgraded to at least 2-D with 1\u2019s post-pended to the shape (column vectors made out of 1-D arrays).  See also  column_stack\n\nStack 1-D arrays as columns into a 2-D array.  r_\n\nFor more detailed documentation.    Examples >>> np.c_[np.array([1,2,3]), np.array([4,5,6])]\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n>>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]\narray([[1, 2, 3, ..., 4, 5, 6]])","title":"numpy.reference.generated.numpy.c_"},{"text":"numpy.longfloat[source]\n \nalias of numpy.longdouble","title":"numpy.reference.arrays.scalars#numpy.longfloat"},{"text":"pandas.tseries.offsets.Second.copy   Second.copy()","title":"pandas.reference.api.pandas.tseries.offsets.second.copy"},{"text":"numpy.r_   numpy.r_ = <numpy.lib.index_tricks.RClass object>\n \nTranslates slice objects to concatenation along the first axis. This is a simple way to build up arrays quickly. There are two use cases.  If the index expression contains comma separated arrays, then stack them along their first axis. If the index expression contains slice notation or scalars then create a 1-D array with a range indicated by the slice notation.  If slice notation is used, the syntax start:stop:step is equivalent to np.arange(start, stop, step) inside of the brackets. However, if step is an imaginary number (i.e. 100j) then its integer portion is interpreted as a number-of-points desired and the start and stop are inclusive. In other words start:stop:stepj is interpreted as np.linspace(start, stop, step, endpoint=1) inside of the brackets. After expansion of slice notation, all comma separated sequences are concatenated together. Optional character strings placed as the first element of the index expression can be used to change the output. The strings \u2018r\u2019 or \u2018c\u2019 result in matrix output. If the result is 1-D and \u2018r\u2019 is specified a 1 x N (row) matrix is produced. If the result is 1-D and \u2018c\u2019 is specified, then a N x 1 (column) matrix is produced. If the result is 2-D then both provide the same matrix result. A string integer specifies which axis to stack multiple comma separated arrays along. A string of two comma-separated integers allows indication of the minimum number of dimensions to force each entry into as the second integer (the axis to concatenate along is still the first integer). A string with three comma-separated integers allows specification of the axis to concatenate along, the minimum number of dimensions to force the entries to, and which axis should contain the start of the arrays which are less than the specified number of dimensions. In other words the third integer allows you to specify where the 1\u2019s should be placed in the shape of the arrays that have their shapes upgraded. By default, they are placed in the front of the shape tuple. The third argument allows you to specify where the start of the array should be instead. Thus, a third argument of \u20180\u2019 would place the 1\u2019s at the end of the array shape. Negative integers specify where in the new shape tuple the last dimension of upgraded arrays should be placed, so the default is \u2018-1\u2019.  Parameters \n Not a function, so takes no parameters\n  Returns \n A concatenated ndarray or matrix.\n    See also  concatenate\n\nJoin a sequence of arrays along an existing axis.  c_\n\nTranslates slice objects to concatenation along the second axis.    Examples >>> np.r_[np.array([1,2,3]), 0, 0, np.array([4,5,6])]\narray([1, 2, 3, ..., 4, 5, 6])\n>>> np.r_[-1:1:6j, [0]*3, 5, 6]\narray([-1. , -0.6, -0.2,  0.2,  0.6,  1. ,  0. ,  0. ,  0. ,  5. ,  6. ])\n String integers specify the axis to concatenate along or the minimum number of dimensions to force entries into. >>> a = np.array([[0, 1, 2], [3, 4, 5]])\n>>> np.r_['-1', a, a] # concatenate along last axis\narray([[0, 1, 2, 0, 1, 2],\n       [3, 4, 5, 3, 4, 5]])\n>>> np.r_['0,2', [1,2,3], [4,5,6]] # concatenate along first axis, dim>=2\narray([[1, 2, 3],\n       [4, 5, 6]])\n >>> np.r_['0,2,0', [1,2,3], [4,5,6]]\narray([[1],\n       [2],\n       [3],\n       [4],\n       [5],\n       [6]])\n>>> np.r_['1,2,0', [1,2,3], [4,5,6]]\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n Using \u2018r\u2019 or \u2018c\u2019 as a first string argument creates a matrix. >>> np.r_['r',[1,2,3], [4,5,6]]\nmatrix([[1, 2, 3, 4, 5, 6]])","title":"numpy.reference.generated.numpy.r_"},{"text":"numpy.cfloat[source]\n \nalias of numpy.cdouble","title":"numpy.reference.arrays.scalars#numpy.cfloat"},{"text":"pandas.tseries.offsets.BQuarterEnd.copy   BQuarterEnd.copy()","title":"pandas.reference.api.pandas.tseries.offsets.bquarterend.copy"}]}
{"task_id":21887754,"prompt":"def f_21887754(a, b):\n\treturn ","suffix":"","canonical_solution":"np.array((a, b))","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    a = np.array([[1, 5, 9], [2, 6, 10]])\n    b = np.array([[3, 7, 11], [4, 8, 12]])\n    assert np.array_equal(candidate(a, b), np.array([[[1, 5, 9], [2, 6, 10]], [[3, 7, 11], [4, 8, 12]]]))\n","\n    a = np.array([[1, 2.45, 3], [4, 0.55, 612]])\n    b = np.array([[988, 8, 7], [6, 512, 4]])\n    assert np.array_equal(candidate(a, b), np.array([[[1, 2.45, 3], [4, 0.55, 612]], [[988, 8 , 7], [6, 512, 4]]]))\n"],"entry_point":"f_21887754","intent":"numpy concatenate two arrays `a` and `b` along the first axis","library":["numpy"],"docs":[{"text":"numpy.concatenate   numpy.concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")\n \nJoin a sequence of arrays along an existing axis.  Parameters \n \na1, a2, \u2026sequence of array_like\n\n\nThe arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  \naxisint, optional\n\n\nThe axis along which the arrays will be joined. If axis is None, arrays are flattened before use. Default is 0.  \noutndarray, optional\n\n\nIf provided, the destination to place the result. The shape must be correct, matching that of what concatenate would have returned if no out argument were specified.  \ndtypestr or dtype\n\n\nIf provided, the destination array will have this dtype. Cannot be provided together with out.  New in version 1.20.0.   \ncasting{\u2018no\u2019, \u2018equiv\u2019, \u2018safe\u2019, \u2018same_kind\u2019, \u2018unsafe\u2019}, optional\n\n\nControls what kind of data casting may occur. Defaults to \u2018same_kind\u2019.  New in version 1.20.0.     Returns \n \nresndarray\n\n\nThe concatenated array.      See also  ma.concatenate\n\nConcatenate function that preserves input masks.  array_split\n\nSplit an array into multiple sub-arrays of equal or near-equal size.  split\n\nSplit array into a list of multiple sub-arrays of equal size.  hsplit\n\nSplit array into multiple sub-arrays horizontally (column wise).  vsplit\n\nSplit array into multiple sub-arrays vertically (row wise).  dsplit\n\nSplit array into multiple sub-arrays along the 3rd axis (depth).  stack\n\nStack a sequence of arrays along a new axis.  block\n\nAssemble arrays from blocks.  hstack\n\nStack arrays in sequence horizontally (column wise).  vstack\n\nStack arrays in sequence vertically (row wise).  dstack\n\nStack arrays in sequence depth wise (along third dimension).  column_stack\n\nStack 1-D arrays as columns into a 2-D array.    Notes When one or more of the arrays to be concatenated is a MaskedArray, this function will return a MaskedArray object instead of an ndarray, but the input masks are not preserved. In cases where a MaskedArray is expected as input, use the ma.concatenate function from the masked array module instead. Examples >>> a = np.array([[1, 2], [3, 4]])\n>>> b = np.array([[5, 6]])\n>>> np.concatenate((a, b), axis=0)\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n>>> np.concatenate((a, b.T), axis=1)\narray([[1, 2, 5],\n       [3, 4, 6]])\n>>> np.concatenate((a, b), axis=None)\narray([1, 2, 3, 4, 5, 6])\n This function will not preserve masking of MaskedArray inputs. >>> a = np.ma.arange(3)\n>>> a[1] = np.ma.masked\n>>> b = np.arange(2, 5)\n>>> a\nmasked_array(data=[0, --, 2],\n             mask=[False,  True, False],\n       fill_value=999999)\n>>> b\narray([2, 3, 4])\n>>> np.concatenate([a, b])\nmasked_array(data=[0, 1, 2, 2, 3, 4],\n             mask=False,\n       fill_value=999999)\n>>> np.ma.concatenate([a, b])\nmasked_array(data=[0, --, 2, 2, 3, 4],\n             mask=[False,  True, False, False, False, False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.concatenate"},{"text":"numpy.ma.concatenate   ma.concatenate(arrays, axis=0)[source]\n \nConcatenate a sequence of arrays along the given axis.  Parameters \n \narrayssequence of array_like\n\n\nThe arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  \naxisint, optional\n\n\nThe axis along which the arrays will be joined. Default is 0.    Returns \n \nresultMaskedArray\n\n\nThe concatenated array with any masked entries preserved.      See also  numpy.concatenate\n\nEquivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma\n>>> a = ma.arange(3)\n>>> a[1] = ma.masked\n>>> b = ma.arange(2, 5)\n>>> a\nmasked_array(data=[0, --, 2],\n             mask=[False,  True, False],\n       fill_value=999999)\n>>> b\nmasked_array(data=[2, 3, 4],\n             mask=False,\n       fill_value=999999)\n>>> ma.concatenate([a, b])\nmasked_array(data=[0, --, 2, 2, 3, 4],\n             mask=[False,  True, False, False, False, False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.ma.concatenate"},{"text":"concat_matrix=b'cm'[source]","title":"matplotlib.backend_pdf_api#matplotlib.backends.backend_pdf.Op.concat_matrix"},{"text":"numpy.clongfloat[source]\n \nalias of numpy.clongdouble","title":"numpy.reference.arrays.scalars#numpy.clongfloat"},{"text":"numpy.c_   numpy.c_ = <numpy.lib.index_tricks.CClass object>\n \nTranslates slice objects to concatenation along the second axis. This is short-hand for np.r_['-1,2,0', index expression], which is useful because of its common occurrence. In particular, arrays will be stacked along their last axis after being upgraded to at least 2-D with 1\u2019s post-pended to the shape (column vectors made out of 1-D arrays).  See also  column_stack\n\nStack 1-D arrays as columns into a 2-D array.  r_\n\nFor more detailed documentation.    Examples >>> np.c_[np.array([1,2,3]), np.array([4,5,6])]\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n>>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]\narray([[1, 2, 3, ..., 4, 5, 6]])","title":"numpy.reference.generated.numpy.c_"},{"text":"numpy.longfloat[source]\n \nalias of numpy.longdouble","title":"numpy.reference.arrays.scalars#numpy.longfloat"},{"text":"pandas.tseries.offsets.Second.copy   Second.copy()","title":"pandas.reference.api.pandas.tseries.offsets.second.copy"},{"text":"numpy.r_   numpy.r_ = <numpy.lib.index_tricks.RClass object>\n \nTranslates slice objects to concatenation along the first axis. This is a simple way to build up arrays quickly. There are two use cases.  If the index expression contains comma separated arrays, then stack them along their first axis. If the index expression contains slice notation or scalars then create a 1-D array with a range indicated by the slice notation.  If slice notation is used, the syntax start:stop:step is equivalent to np.arange(start, stop, step) inside of the brackets. However, if step is an imaginary number (i.e. 100j) then its integer portion is interpreted as a number-of-points desired and the start and stop are inclusive. In other words start:stop:stepj is interpreted as np.linspace(start, stop, step, endpoint=1) inside of the brackets. After expansion of slice notation, all comma separated sequences are concatenated together. Optional character strings placed as the first element of the index expression can be used to change the output. The strings \u2018r\u2019 or \u2018c\u2019 result in matrix output. If the result is 1-D and \u2018r\u2019 is specified a 1 x N (row) matrix is produced. If the result is 1-D and \u2018c\u2019 is specified, then a N x 1 (column) matrix is produced. If the result is 2-D then both provide the same matrix result. A string integer specifies which axis to stack multiple comma separated arrays along. A string of two comma-separated integers allows indication of the minimum number of dimensions to force each entry into as the second integer (the axis to concatenate along is still the first integer). A string with three comma-separated integers allows specification of the axis to concatenate along, the minimum number of dimensions to force the entries to, and which axis should contain the start of the arrays which are less than the specified number of dimensions. In other words the third integer allows you to specify where the 1\u2019s should be placed in the shape of the arrays that have their shapes upgraded. By default, they are placed in the front of the shape tuple. The third argument allows you to specify where the start of the array should be instead. Thus, a third argument of \u20180\u2019 would place the 1\u2019s at the end of the array shape. Negative integers specify where in the new shape tuple the last dimension of upgraded arrays should be placed, so the default is \u2018-1\u2019.  Parameters \n Not a function, so takes no parameters\n  Returns \n A concatenated ndarray or matrix.\n    See also  concatenate\n\nJoin a sequence of arrays along an existing axis.  c_\n\nTranslates slice objects to concatenation along the second axis.    Examples >>> np.r_[np.array([1,2,3]), 0, 0, np.array([4,5,6])]\narray([1, 2, 3, ..., 4, 5, 6])\n>>> np.r_[-1:1:6j, [0]*3, 5, 6]\narray([-1. , -0.6, -0.2,  0.2,  0.6,  1. ,  0. ,  0. ,  0. ,  5. ,  6. ])\n String integers specify the axis to concatenate along or the minimum number of dimensions to force entries into. >>> a = np.array([[0, 1, 2], [3, 4, 5]])\n>>> np.r_['-1', a, a] # concatenate along last axis\narray([[0, 1, 2, 0, 1, 2],\n       [3, 4, 5, 3, 4, 5]])\n>>> np.r_['0,2', [1,2,3], [4,5,6]] # concatenate along first axis, dim>=2\narray([[1, 2, 3],\n       [4, 5, 6]])\n >>> np.r_['0,2,0', [1,2,3], [4,5,6]]\narray([[1],\n       [2],\n       [3],\n       [4],\n       [5],\n       [6]])\n>>> np.r_['1,2,0', [1,2,3], [4,5,6]]\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n Using \u2018r\u2019 or \u2018c\u2019 as a first string argument creates a matrix. >>> np.r_['r',[1,2,3], [4,5,6]]\nmatrix([[1, 2, 3, 4, 5, 6]])","title":"numpy.reference.generated.numpy.r_"},{"text":"numpy.cfloat[source]\n \nalias of numpy.cdouble","title":"numpy.reference.arrays.scalars#numpy.cfloat"},{"text":"pandas.tseries.offsets.BQuarterEnd.copy   BQuarterEnd.copy()","title":"pandas.reference.api.pandas.tseries.offsets.bquarterend.copy"}]}
{"task_id":2805231,"prompt":"def f_2805231():\n\treturn ","suffix":"","canonical_solution":"socket.getaddrinfo('google.com', 80)","test_start":"\nimport socket\n\ndef check(candidate):","test":["\n    res = candidate()\n    assert all([(add[4][1] == 80) for add in res])\n"],"entry_point":"f_2805231","intent":"fetch address information for host 'google.com' ion port 80","library":["socket"],"docs":[{"text":"exploded","title":"python.library.ipaddress#ipaddress.IPv6Address.exploded"},{"text":"Request.origin_req_host  \nThe original host for the request, without port.","title":"python.library.urllib.request#urllib.request.Request.origin_req_host"},{"text":"exploded","title":"python.library.ipaddress#ipaddress.IPv6Network.exploded"},{"text":"network_address","title":"python.library.ipaddress#ipaddress.IPv6Network.network_address"},{"text":"reverse_pointer","title":"python.library.ipaddress#ipaddress.IPv6Address.reverse_pointer"},{"text":"ip","title":"python.library.ipaddress#ipaddress.IPv6Interface.ip"},{"text":"max_prefixlen","title":"python.library.ipaddress#ipaddress.IPv6Address.max_prefixlen"},{"text":"network","title":"python.library.ipaddress#ipaddress.IPv6Interface.network"},{"text":"compressed","title":"python.library.ipaddress#ipaddress.IPv6Address.compressed"},{"text":"hostmask","title":"python.library.ipaddress#ipaddress.IPv6Network.hostmask"}]}
{"task_id":17552997,"prompt":"def f_17552997(df):\n\treturn ","suffix":"","canonical_solution":"df.xs('sat', level='day', drop_level=False)","test_start":"\nimport pandas as pd \n\ndef check(candidate):","test":["\n    df = pd.DataFrame({'year':[2008,2008,2008,2008,2009,2009,2009,2009], \n                      'flavour':['strawberry','strawberry','banana','banana',\n                      'strawberry','strawberry','banana','banana'],\n                      'day':['sat','sun','sat','sun','sat','sun','sat','sun'],\n                      'sales':[10,12,22,23,11,13,23,24]})\n    df = df.set_index(['year','flavour','day'])\n    assert candidate(df).to_dict() == {'sales': {(2008, 'strawberry', 'sat'): 10, (2008, 'banana', 'sat'): 22, (2009, 'strawberry', 'sat'): 11, (2009, 'banana', 'sat'): 23}}\n"],"entry_point":"f_17552997","intent":"add a column 'day' with value 'sat' to dataframe `df`","library":["pandas"],"docs":[{"text":"pandas.Timestamp.day   Timestamp.day","title":"pandas.reference.api.pandas.timestamp.day"},{"text":"pandas.tseries.offsets.Day.name   Day.name","title":"pandas.reference.api.pandas.tseries.offsets.day.name"},{"text":"pandas.tseries.offsets.Day.apply   Day.apply()","title":"pandas.reference.api.pandas.tseries.offsets.day.apply"},{"text":"pandas.tseries.offsets.Day.freqstr   Day.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.day.freqstr"},{"text":"pandas.tseries.offsets.Day.n   Day.n","title":"pandas.reference.api.pandas.tseries.offsets.day.n"},{"text":"pandas.tseries.offsets.Day.onOffset   Day.onOffset()","title":"pandas.reference.api.pandas.tseries.offsets.day.onoffset"},{"text":"pandas.Timedelta.value   Timedelta.value","title":"pandas.reference.api.pandas.timedelta.value"},{"text":"pandas.tseries.offsets.Day.kwds   Day.kwds","title":"pandas.reference.api.pandas.tseries.offsets.day.kwds"},{"text":"pandas.tseries.offsets.FY5253Quarter.weekday   FY5253Quarter.weekday","title":"pandas.reference.api.pandas.tseries.offsets.fy5253quarter.weekday"},{"text":"pandas.tseries.offsets.Day.nanos   Day.nanos","title":"pandas.reference.api.pandas.tseries.offsets.day.nanos"}]}
{"task_id":4356842,"prompt":"def f_4356842():\n\treturn ","suffix":"","canonical_solution":"HttpResponse('Unauthorized', status=401)","test_start":"\nfrom django.http import HttpResponse\nfrom django.conf import settings\n\nif not settings.configured:\n    settings.configure(DEBUG=True)\n\ndef check(candidate):","test":["\n    assert candidate().status_code == 401\n"],"entry_point":"f_4356842","intent":"return a 401 unauthorized in django","library":["django"],"docs":[{"text":"class models.Permission","title":"django.ref.contrib.auth#django.contrib.auth.models.Permission"},{"text":"auth()","title":"django.ref.templates.api#django.contrib.auth.context_processors.auth"},{"text":"class HttpResponseForbidden  \nActs just like HttpResponse but uses a 403 status code.","title":"django.ref.request-response#django.http.HttpResponseForbidden"},{"text":"defaults.permission_denied(request, exception, template_name='403.html')","title":"django.ref.views#django.views.defaults.permission_denied"},{"text":"class HttpResponseBadRequest  \nActs just like HttpResponse but uses a 400 status code.","title":"django.ref.request-response#django.http.HttpResponseBadRequest"},{"text":"HTTPBasicAuthHandler.http_error_401(req, fp, code, msg, hdrs)  \nRetry the request with authentication information, if available.","title":"python.library.urllib.request#urllib.request.HTTPBasicAuthHandler.http_error_401"},{"text":"django.core.signals.got_request_exception","title":"django.ref.signals#django.core.signals.got_request_exception"},{"text":"get_user_permissions(user_obj, obj=None)  \nReturns an empty set.","title":"django.ref.contrib.auth#django.contrib.auth.backends.BaseBackend.get_user_permissions"},{"text":"class models.User","title":"django.ref.contrib.auth#django.contrib.auth.models.User"},{"text":"HTTPDigestAuthHandler.http_error_401(req, fp, code, msg, hdrs)  \nRetry the request with authentication information, if available.","title":"python.library.urllib.request#urllib.request.HTTPDigestAuthHandler.http_error_401"}]}
{"task_id":13598363,"prompt":"def f_13598363():\n\treturn ","suffix":"","canonical_solution":"Flask('test', template_folder='wherever')","test_start":"\nfrom flask import Flask\n\ndef check(candidate):","test":["\n    __name__ == \"test\"\n    assert candidate().template_folder == \"wherever\"\n"],"entry_point":"f_13598363","intent":"Flask set folder 'wherever' as the default template folder","library":["flask"],"docs":[{"text":"jinja_environment  \nalias of flask.templating.Environment","title":"flask.api.index#flask.Flask.jinja_environment"},{"text":"TEMPLATES_AUTO_RELOAD  \nReload templates when they are changed. If not set, it will be enabled in debug mode. Default: None","title":"flask.config.index#TEMPLATES_AUTO_RELOAD"},{"text":"template_folder  \nThe path to the templates folder, relative to root_path, to add to the template loader. None if templates should not be added.","title":"flask.api.index#flask.Flask.template_folder"},{"text":"template_folder  \nThe path to the templates folder, relative to root_path, to add to the template loader. None if templates should not be added.","title":"flask.api.index#flask.Blueprint.template_folder"},{"text":"PREFERRED_URL_SCHEME  \nUse this scheme for generating external URLs when not in a request context. Default: 'http'","title":"flask.config.index#PREFERRED_URL_SCHEME"},{"text":"app  \na reference to the current application","title":"flask.api.index#flask.blueprints.BlueprintSetupState.app"},{"text":"SESSION_COOKIE_HTTPONLY  \nBrowsers will not allow JavaScript access to cookies marked as \u201cHTTP only\u201d for security. Default: True","title":"flask.config.index#SESSION_COOKIE_HTTPONLY"},{"text":"config_class  \nalias of flask.config.Config","title":"flask.api.index#flask.Flask.config_class"},{"text":"app_ctx_globals_class  \nalias of flask.ctx._AppCtxGlobals","title":"flask.api.index#flask.Flask.app_ctx_globals_class"},{"text":"url_rule_class  \nalias of werkzeug.routing.Rule","title":"flask.api.index#flask.Flask.url_rule_class"}]}
{"task_id":3398589,"prompt":"def f_3398589(c2):\n\t","suffix":"\n\treturn c2","canonical_solution":"c2.sort(key=lambda row: row[2])","test_start":"\ndef check(candidate):","test":["\n    c2 = [[14, 25, 46], [1, 22, 53], [7, 8, 9]]\n    candidate(c2)\n    assert c2[0] == [7,8,9]\n","\n    c2 = [[14.343, 25.24, 46], [1, 22, 53.45], [7, 8.65, 9]]\n    candidate(c2)\n    assert c2[0] == [7,8.65,9]\n"],"entry_point":"f_3398589","intent":"sort a list of lists 'c2' such that third row comes first","library":[],"docs":[]}
{"task_id":3398589,"prompt":"def f_3398589(c2):\n\t","suffix":"\n\treturn c2","canonical_solution":"c2.sort(key=lambda row: (row[2], row[1], row[0]))","test_start":"\ndef check(candidate):","test":["\n    c2 = [[14, 25, 46], [1, 22, 53], [7, 8, 9]]\n    candidate(c2)\n    assert c2[0] == [7,8,9]\n","\n    c2 = [[14.343, 25.24, 46], [1, 22, 53.45], [7, 8.65, 9]]\n    candidate(c2)\n    assert c2[0] == [7,8.65,9]\n"],"entry_point":"f_3398589","intent":"sort a list of lists 'c2' in reversed row order","library":[],"docs":[]}
{"task_id":3398589,"prompt":"def f_3398589(c2):\n\t","suffix":"\n\treturn c2","canonical_solution":"c2.sort(key=lambda row: (row[2], row[1]))","test_start":"\ndef check(candidate):","test":["\n    c2 = [[14, 25, 46], [1, 22, 53], [7, 8, 9]]\n    candidate(c2)\n    assert c2[0] == [7,8,9]\n","\n    c2 = [[14.343, 25.24, 46], [1, 22, 53.45], [7, 8.65, 9]]\n    candidate(c2)\n    assert c2[0] == [7,8.65,9]\n"],"entry_point":"f_3398589","intent":"Sorting a list of lists `c2`, each by the third and second row","library":[],"docs":[]}
{"task_id":10960463,"prompt":"def f_10960463():\n\treturn ","suffix":"","canonical_solution":"matplotlib.rc('font', **{'sans-serif': 'Arial', 'family': 'sans-serif'})","test_start":"\nimport matplotlib\n\ndef check(candidate):","test":["\n    try:\n        candidate()\n    except:\n        assert False\n"],"entry_point":"f_10960463","intent":"set font `Arial` to display non-ascii characters in matplotlib","library":["matplotlib"],"docs":[{"text":"prop","title":"matplotlib.type1font#matplotlib.type1font.Type1Font.prop"},{"text":"encoding\n \nAlias for field number 3","title":"matplotlib.dviread#matplotlib.dviread.PsFont.encoding"},{"text":"FONTSIZE=10","title":"matplotlib.table_api#matplotlib.table.Table.FONTSIZE"},{"text":"widths","title":"matplotlib.dviread#matplotlib.dviread.DviFont.widths"},{"text":"FONT_SCALE=100.0","title":"matplotlib.textpath_api#matplotlib.textpath.TextToPath.FONT_SCALE"},{"text":"parts","title":"matplotlib.type1font#matplotlib.type1font.Type1Font.parts"},{"text":"texname","title":"matplotlib.dviread#matplotlib.dviread.DviFont.texname"},{"text":"filename\n \nAlias for field number 4","title":"matplotlib.dviread#matplotlib.dviread.PsFont.filename"},{"text":"test.support.FS_NONASCII  \nA non-ASCII character encodable by os.fsencode().","title":"python.library.test#test.support.FS_NONASCII"},{"text":"texname\n \nAlias for field number 0","title":"matplotlib.dviread#matplotlib.dviread.PsFont.texname"}]}
{"task_id":20576618,"prompt":"def f_20576618(df):\n\treturn ","suffix":"","canonical_solution":"df['date'].apply(lambda x: x.toordinal())","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame(\n        {\n            \"group\": [\"A\", \"A\", \"A\", \"A\", \"A\"],\n            \"date\": pd.to_datetime([\"2020-01-02\", \"2020-01-13\", \"2020-02-01\", \"2020-02-23\", \"2020-03-05\"]),\n            \"value\": [10, 20, 16, 31, 56],\n        })    \n    data_series = candidate(df).tolist()\n    assert data_series[1] == 737437\n","\n    df = pd.DataFrame(\n        {\n            \"group\": [\"A\", \"A\", \"A\", \"A\", \"A\"],\n            \"date\": pd.to_datetime([\"2020-01-02\", \"2020-01-13\", \"2020-02-01\", \"2020-02-23\", \"2020-03-05\"]),\n            \"value\": [10, 20, 16, 31, 56],\n        })    \n    data_series = candidate(df).tolist()\n    assert data_series[1] == 737437\n"],"entry_point":"f_20576618","intent":"Convert  DateTime column 'date' of pandas dataframe 'df' to ordinal","library":["pandas"],"docs":[{"text":"pandas.Period.ordinal   Period.ordinal","title":"pandas.reference.api.pandas.period.ordinal"},{"text":"pandas.Timestamp.day   Timestamp.day","title":"pandas.reference.api.pandas.timestamp.day"},{"text":"pandas.tseries.offsets.DateOffset.normalize   DateOffset.normalize","title":"pandas.reference.api.pandas.tseries.offsets.dateoffset.normalize"},{"text":"pandas.PeriodIndex.start_time   propertyPeriodIndex.start_time","title":"pandas.reference.api.pandas.periodindex.start_time"},{"text":"pandas.Timestamp.fromisoformat   Timestamp.fromisoformat()\n \nstring -> datetime from datetime.isoformat() output","title":"pandas.reference.api.pandas.timestamp.fromisoformat"},{"text":"pandas.DatetimeIndex.dayofyear   propertyDatetimeIndex.dayofyear\n \nThe ordinal day of the year.","title":"pandas.reference.api.pandas.datetimeindex.dayofyear"},{"text":"pandas.Timestamp.fold   Timestamp.fold","title":"pandas.reference.api.pandas.timestamp.fold"},{"text":"classmethod datetime.fromordinal(ordinal)  \nReturn the datetime corresponding to the proleptic Gregorian ordinal, where January 1 of year 1 has ordinal 1. ValueError is raised unless 1\n<= ordinal <= datetime.max.toordinal(). The hour, minute, second and microsecond of the result are all 0, and tzinfo is None.","title":"python.library.datetime#datetime.datetime.fromordinal"},{"text":"pandas.Timestamp.freq   Timestamp.freq","title":"pandas.reference.api.pandas.timestamp.freq"},{"text":"pandas.PeriodIndex.qyear   propertyPeriodIndex.qyear","title":"pandas.reference.api.pandas.periodindex.qyear"}]}
{"task_id":31793195,"prompt":"def f_31793195(df):\n\treturn ","suffix":"","canonical_solution":"df.index.get_loc('bob')","test_start":"\nimport pandas as pd\nimport numpy as np\n\ndef check(candidate):","test":["\n    df = pd.DataFrame(data=np.asarray([[1,2,3],[4,5,6],[7,8,9]]), index=['alice', 'bob', 'charlie'])\n    index = candidate(df)\n    assert index == 1\n"],"entry_point":"f_31793195","intent":"Get the integer location of a key `bob` in a pandas data frame `df`","library":["numpy","pandas"],"docs":[{"text":"pandas.IntervalIndex.left   IntervalIndex.left","title":"pandas.reference.api.pandas.intervalindex.left"},{"text":"pandas.Index.names   propertyIndex.names","title":"pandas.reference.api.pandas.index.names"},{"text":"colno  \nThe column corresponding to pos (may be None).","title":"python.library.re#re.error.colno"},{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"pandas.tseries.offsets.Micro.name   Micro.name","title":"pandas.reference.api.pandas.tseries.offsets.micro.name"},{"text":"pandas.tseries.offsets.FY5253Quarter.name   FY5253Quarter.name","title":"pandas.reference.api.pandas.tseries.offsets.fy5253quarter.name"},{"text":"pandas.tseries.offsets.Nano.name   Nano.name","title":"pandas.reference.api.pandas.tseries.offsets.nano.name"},{"text":"property df","title":"torch.distributions#torch.distributions.chi2.Chi2.df"},{"text":"pandas.tseries.offsets.Day.name   Day.name","title":"pandas.reference.api.pandas.tseries.offsets.day.name"},{"text":"pandas.tseries.offsets.Nano.nanos   Nano.nanos","title":"pandas.reference.api.pandas.tseries.offsets.nano.nanos"}]}
{"task_id":10487278,"prompt":"def f_10487278(my_dict):\n\t","suffix":"\n\treturn my_dict","canonical_solution":"my_dict.update({'third_key': 1})","test_start":"\ndef check(candidate):","test":["\n    my_dict = {'a':1, 'b':2}\n    assert candidate(my_dict) == {'a':1, 'b':2, 'third_key': 1}\n","\n    my_dict = {'c':1, 'd':2}\n    assert candidate(my_dict) == {'c':1, 'd':2, 'third_key': 1}\n"],"entry_point":"f_10487278","intent":"add an item with key 'third_key' and value 1 to an dictionary `my_dict`","library":[],"docs":[]}
{"task_id":10487278,"prompt":"def f_10487278():\n\t","suffix":"\n\treturn my_list","canonical_solution":"my_list = []","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == []\n"],"entry_point":"f_10487278","intent":"declare an array `my_list`","library":[],"docs":[]}
{"task_id":10487278,"prompt":"def f_10487278(my_list):\n\t","suffix":"\n\treturn my_list","canonical_solution":"my_list.append(12)","test_start":"\ndef check(candidate):","test":["\n    assert candidate([1,2]) == [1, 2, 12] \n","\n    assert candidate([5,6]) == [5, 6, 12]\n"],"entry_point":"f_10487278","intent":"Insert item `12` to a list `my_list`","library":[],"docs":[]}
{"task_id":10155684,"prompt":"def f_10155684(myList):\n\t","suffix":"\n\treturn myList","canonical_solution":"myList.insert(0, 'wuggah')","test_start":"\ndef check(candidate):","test":["\n    assert candidate([1,2]) == ['wuggah', 1, 2]\n","\n    assert candidate([]) == ['wuggah'] \n"],"entry_point":"f_10155684","intent":"add an entry 'wuggah' at the beginning of list `myList`","library":[],"docs":[]}
{"task_id":3519125,"prompt":"def f_3519125(hex_str):\n\treturn ","suffix":"","canonical_solution":"bytes.fromhex(hex_str.replace('\\\\x', ''))","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"\\\\xF3\\\\xBE\\\\x80\\\\x80\") == b'\\xf3\\xbe\\x80\\x80'\n"],"entry_point":"f_3519125","intent":"convert a hex-string representation `hex_str` to actual bytes","library":[],"docs":[]}
{"task_id":40144769,"prompt":"def f_40144769(df):\n\treturn ","suffix":"","canonical_solution":"df[df.columns[-1]]","test_start":"\nimport pandas as pd \n\ndef check(candidate):","test":["\n    df = pd.DataFrame([[1, 2, 3],[4,5,6]], columns=[\"a\", \"b\", \"c\"])\n    assert candidate(df).tolist() == [3,6]\n","\n    df = pd.DataFrame([[\"Hello\", \"world!\"],[\"Hi\", \"world!\"]], columns=[\"a\", \"b\"])\n    assert candidate(df).tolist() == [\"world!\", \"world!\"]\n"],"entry_point":"f_40144769","intent":"select the last column of dataframe `df`","library":["pandas"],"docs":[{"text":"last()","title":"django.ref.models.querysets#django.db.models.query.QuerySet.last"},{"text":"pandas.Series.dt.end_time   Series.dt.end_time","title":"pandas.reference.api.pandas.series.dt.end_time"},{"text":"pandas.tseries.offsets.LastWeekOfMonth.freqstr   LastWeekOfMonth.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.lastweekofmonth.freqstr"},{"text":"pandas.PeriodIndex.end_time   propertyPeriodIndex.end_time","title":"pandas.reference.api.pandas.periodindex.end_time"},{"text":"pandas.DataFrame.columns   DataFrame.columns\n \nThe column labels of the DataFrame.","title":"pandas.reference.api.pandas.dataframe.columns"},{"text":"pandas.tseries.offsets.LastWeekOfMonth.week   LastWeekOfMonth.week","title":"pandas.reference.api.pandas.tseries.offsets.lastweekofmonth.week"},{"text":"pandas.tseries.offsets.LastWeekOfMonth.name   LastWeekOfMonth.name","title":"pandas.reference.api.pandas.tseries.offsets.lastweekofmonth.name"},{"text":"pandas.tseries.offsets.LastWeekOfMonth.onOffset   LastWeekOfMonth.onOffset()","title":"pandas.reference.api.pandas.tseries.offsets.lastweekofmonth.onoffset"},{"text":"pandas.tseries.offsets.LastWeekOfMonth.is_year_end   LastWeekOfMonth.is_year_end()","title":"pandas.reference.api.pandas.tseries.offsets.lastweekofmonth.is_year_end"},{"text":"pandas.tseries.offsets.LastWeekOfMonth.nanos   LastWeekOfMonth.nanos","title":"pandas.reference.api.pandas.tseries.offsets.lastweekofmonth.nanos"}]}
{"task_id":30787901,"prompt":"def f_30787901(df):\n\treturn ","suffix":"","canonical_solution":"df.loc[df['Letters'] == 'C', 'Letters'].values[0]","test_start":"\nimport pandas as pd \n\ndef check(candidate):","test":["\n    df = pd.DataFrame([[\"a\", 1],[\"C\", 6]], columns=[\"Letters\", \"Numbers\"])\n    assert candidate(df) == 'C'\n","\n    df = pd.DataFrame([[None, 1],[\"C\", 789]], columns=[\"Letters\", \"Names\"])\n    assert candidate(df) == 'C'\n"],"entry_point":"f_30787901","intent":"get the first value from dataframe `df` where column 'Letters' is equal to 'C'","library":["pandas"],"docs":[{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"colno  \nThe column corresponding to pos (may be None).","title":"python.library.re#re.error.colno"},{"text":"operator.indexOf(a, b)  \nReturn the index of the first of occurrence of b in a.","title":"python.library.operator#operator.indexOf"},{"text":"numpy.char.chararray.index method   char.chararray.index(sub, start=0, end=None)[source]\n \nLike find, but raises ValueError when the substring is not found.  See also  char.index","title":"numpy.reference.generated.numpy.char.chararray.index"},{"text":"token.STAREQUAL  \nToken value for \"*=\".","title":"python.library.token#token.STAREQUAL"},{"text":"pandas.IntervalIndex.right   IntervalIndex.right","title":"pandas.reference.api.pandas.intervalindex.right"},{"text":"colno  \nThe column corresponding to pos.","title":"python.library.json#json.JSONDecodeError.colno"},{"text":"numpy.chararray.index method   chararray.index(sub, start=0, end=None)[source]\n \nLike find, but raises ValueError when the substring is not found.  See also  char.index","title":"numpy.reference.generated.numpy.chararray.index"},{"text":"str.rindex(sub[, start[, end]])  \nLike rfind() but raises ValueError when the substring sub is not found.","title":"python.library.stdtypes#str.rindex"},{"text":"pandas.DataFrame.at   propertyDataFrame.at\n \nAccess a single value for a row\/column label pair. Similar to loc, in that both provide label-based lookups. Use at if you only need to get or set a single value in a DataFrame or Series.  Raises \n KeyError\n\nIf \u2018label\u2019 does not exist in DataFrame.      See also  DataFrame.iat\n\nAccess a single value for a row\/column pair by integer position.  DataFrame.loc\n\nAccess a group of rows and columns by label(s).  Series.at\n\nAccess a single value using a label.    Examples \n>>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],\n...                   index=[4, 5, 6], columns=['A', 'B', 'C'])\n>>> df\n    A   B   C\n4   0   2   3\n5   0   4   1\n6  10  20  30\n  Get value at specified row\/column pair \n>>> df.at[4, 'B']\n2\n  Set value at specified row\/column pair \n>>> df.at[4, 'B'] = 10\n>>> df.at[4, 'B']\n10\n  Get value within a Series \n>>> df.loc[5].at['B']\n4","title":"pandas.reference.api.pandas.dataframe.at"}]}
{"task_id":18730044,"prompt":"def f_18730044():\n\treturn ","suffix":"","canonical_solution":"np.column_stack(([1, 2, 3], [4, 5, 6]))","test_start":"\nimport numpy as np \n\ndef check(candidate):","test":["\n    assert np.all(candidate() == np.array([[1, 4], [2, 5], [3, 6]]))\n"],"entry_point":"f_18730044","intent":"converting two lists `[1, 2, 3]` and `[4, 5, 6]` into a matrix","library":["numpy"],"docs":[{"text":"array.tolist()  \nConvert the array to an ordinary list with the same items.","title":"python.library.array#array.array.tolist"},{"text":"numpy.complex_[source]\n \nalias of numpy.cdouble","title":"numpy.reference.arrays.scalars#numpy.complex_"},{"text":"pandas.MultiIndex.codes   propertyMultiIndex.codes","title":"pandas.reference.api.pandas.multiindex.codes"},{"text":"numpy.matrix.dumps method   matrix.dumps()\n \nReturns the pickle of the array as a string. pickle.loads will convert the string back to an array.  Parameters \n None","title":"numpy.reference.generated.numpy.matrix.dumps"},{"text":"numpy.longcomplex[source]\n \nalias of numpy.clongdouble","title":"numpy.reference.arrays.scalars#numpy.longcomplex"},{"text":"numpy.singlecomplex[source]\n \nalias of numpy.csingle","title":"numpy.reference.arrays.scalars#numpy.singlecomplex"},{"text":"numpy.ndarray.__complex__ method   ndarray.__complex__()","title":"numpy.reference.generated.numpy.ndarray.__complex__"},{"text":"numpy.complex64[source]\n \nalias of numpy.csingle","title":"numpy.reference.arrays.scalars#numpy.complex64"},{"text":"numpy.ndarray.__neg__ method   ndarray.__neg__(\/)\n \n-self","title":"numpy.reference.generated.numpy.ndarray.__neg__"},{"text":"numpy.r_   numpy.r_ = <numpy.lib.index_tricks.RClass object>\n \nTranslates slice objects to concatenation along the first axis. This is a simple way to build up arrays quickly. There are two use cases.  If the index expression contains comma separated arrays, then stack them along their first axis. If the index expression contains slice notation or scalars then create a 1-D array with a range indicated by the slice notation.  If slice notation is used, the syntax start:stop:step is equivalent to np.arange(start, stop, step) inside of the brackets. However, if step is an imaginary number (i.e. 100j) then its integer portion is interpreted as a number-of-points desired and the start and stop are inclusive. In other words start:stop:stepj is interpreted as np.linspace(start, stop, step, endpoint=1) inside of the brackets. After expansion of slice notation, all comma separated sequences are concatenated together. Optional character strings placed as the first element of the index expression can be used to change the output. The strings \u2018r\u2019 or \u2018c\u2019 result in matrix output. If the result is 1-D and \u2018r\u2019 is specified a 1 x N (row) matrix is produced. If the result is 1-D and \u2018c\u2019 is specified, then a N x 1 (column) matrix is produced. If the result is 2-D then both provide the same matrix result. A string integer specifies which axis to stack multiple comma separated arrays along. A string of two comma-separated integers allows indication of the minimum number of dimensions to force each entry into as the second integer (the axis to concatenate along is still the first integer). A string with three comma-separated integers allows specification of the axis to concatenate along, the minimum number of dimensions to force the entries to, and which axis should contain the start of the arrays which are less than the specified number of dimensions. In other words the third integer allows you to specify where the 1\u2019s should be placed in the shape of the arrays that have their shapes upgraded. By default, they are placed in the front of the shape tuple. The third argument allows you to specify where the start of the array should be instead. Thus, a third argument of \u20180\u2019 would place the 1\u2019s at the end of the array shape. Negative integers specify where in the new shape tuple the last dimension of upgraded arrays should be placed, so the default is \u2018-1\u2019.  Parameters \n Not a function, so takes no parameters\n  Returns \n A concatenated ndarray or matrix.\n    See also  concatenate\n\nJoin a sequence of arrays along an existing axis.  c_\n\nTranslates slice objects to concatenation along the second axis.    Examples >>> np.r_[np.array([1,2,3]), 0, 0, np.array([4,5,6])]\narray([1, 2, 3, ..., 4, 5, 6])\n>>> np.r_[-1:1:6j, [0]*3, 5, 6]\narray([-1. , -0.6, -0.2,  0.2,  0.6,  1. ,  0. ,  0. ,  0. ,  5. ,  6. ])\n String integers specify the axis to concatenate along or the minimum number of dimensions to force entries into. >>> a = np.array([[0, 1, 2], [3, 4, 5]])\n>>> np.r_['-1', a, a] # concatenate along last axis\narray([[0, 1, 2, 0, 1, 2],\n       [3, 4, 5, 3, 4, 5]])\n>>> np.r_['0,2', [1,2,3], [4,5,6]] # concatenate along first axis, dim>=2\narray([[1, 2, 3],\n       [4, 5, 6]])\n >>> np.r_['0,2,0', [1,2,3], [4,5,6]]\narray([[1],\n       [2],\n       [3],\n       [4],\n       [5],\n       [6]])\n>>> np.r_['1,2,0', [1,2,3], [4,5,6]]\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n Using \u2018r\u2019 or \u2018c\u2019 as a first string argument creates a matrix. >>> np.r_['r',[1,2,3], [4,5,6]]\nmatrix([[1, 2, 3, 4, 5, 6]])","title":"numpy.reference.generated.numpy.r_"}]}
{"task_id":402504,"prompt":"def f_402504(i):\n\treturn ","suffix":"","canonical_solution":"type(i)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"hello\") is str\n","\n    assert candidate(123) is int\n","\n    assert candidate(\"123\") is str\n","\n    assert candidate(123.4) is float\n"],"entry_point":"f_402504","intent":"get the type of `i`","library":[],"docs":[]}
{"task_id":402504,"prompt":"def f_402504(v):\n\treturn ","suffix":"","canonical_solution":"type(v)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"hello\") is str\n","\n    assert candidate(123) is int\n","\n    assert candidate(\"123\") is str\n","\n    assert candidate(123.4) is float\n"],"entry_point":"f_402504","intent":"determine the type of variable `v`","library":[],"docs":[]}
{"task_id":402504,"prompt":"def f_402504(v):\n\treturn ","suffix":"","canonical_solution":"type(v)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"hello\") is str\n","\n    assert candidate(123) is int\n","\n    assert candidate(\"123\") is str\n","\n    assert candidate(123.4) is float\n"],"entry_point":"f_402504","intent":"determine the type of variable `v`","library":[],"docs":[]}
{"task_id":402504,"prompt":"def f_402504(variable_name):\n\treturn ","suffix":"","canonical_solution":"type(variable_name)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"hello\") is str\n","\n    assert candidate(123) is int\n","\n    assert candidate(\"123\") is str\n","\n    assert candidate(123.4) is float\n"],"entry_point":"f_402504","intent":"get the type of variable `variable_name`","library":[],"docs":[]}
{"task_id":2300756,"prompt":"def f_2300756(g):\n\treturn ","suffix":"","canonical_solution":"next(itertools.islice(g, 5, 5 + 1))","test_start":"\nimport itertools\n\ndef check(candidate):","test":["\n    test = [1, 2, 3, 4, 5, 6, 7]\n    assert(candidate(test) == 6)\n"],"entry_point":"f_2300756","intent":"get the 5th item of a generator `g`","library":["itertools"],"docs":[{"text":"grp.getgrall()  \nReturn a list of all available group entries, in arbitrary order.","title":"python.library.grp#grp.getgrall"},{"text":"__getitem__()","title":"django.ref.contrib.gis.gdal#django.contrib.gis.gdal.OGRGeometry.__getitem__"},{"text":"g \n Gets or sets the green value of the Color. g -> int  The green value of the Color.","title":"pygame.ref.color#pygame.Color.g"},{"text":"zorder=5","title":"matplotlib.legend_api#matplotlib.legend.Legend.zorder"},{"text":"__iter__()","title":"django.ref.contrib.gis.gdal#django.contrib.gis.gdal.OGRGeometry.__iter__"},{"text":"numpy.ndarray.__gt__ method   ndarray.__gt__(value, \/)\n \nReturn self>value.","title":"numpy.reference.generated.numpy.ndarray.__gt__"},{"text":"lgamma() \u2192 Tensor  \nSee torch.lgamma()","title":"torch.tensors#torch.Tensor.lgamma"},{"text":"numpy.nditer.value attribute   nditer.value","title":"numpy.reference.generated.numpy.nditer.value"},{"text":"gettext.lngettext(singular, plural, n)","title":"python.library.gettext#gettext.lngettext"},{"text":"contains(other)","title":"django.ref.contrib.gis.gdal#django.contrib.gis.gdal.OGRGeometry.contains"}]}
{"task_id":20056548,"prompt":"def f_20056548(word):\n\treturn ","suffix":"","canonical_solution":"'\"{}\"'.format(word)","test_start":"\ndef check(candidate):","test":["\n    assert candidate('Some Random Word') == '\"Some Random Word\"'\n"],"entry_point":"f_20056548","intent":"return a string `word` with string format","library":[],"docs":[]}
{"task_id":8546245,"prompt":"def f_8546245(list):\n\treturn ","suffix":"","canonical_solution":"\"\"\" \"\"\".join(list)","test_start":"\ndef check(candidate):","test":["\n    test = ['hello', 'good', 'morning']\n    assert candidate(test) == \"hello good morning\"\n"],"entry_point":"f_8546245","intent":"join a list of strings `list` using a space ' '","library":[],"docs":[]}
{"task_id":2276416,"prompt":"def f_2276416():\n\t","suffix":"\n\treturn y","canonical_solution":"y = [[] for n in range(2)]","test_start":"\ndef check(candidate):","test":["\n    assert(candidate() == [[], []])\n"],"entry_point":"f_2276416","intent":"create list `y` containing two empty lists","library":[],"docs":[]}
{"task_id":3925614,"prompt":"def f_3925614(filename):\n\t","suffix":"\n\treturn data","canonical_solution":"data = [line.strip() for line in open(filename, 'r')]","test_start":"\ndef check(candidate):","test":["\n    file1 = open(\"myfile.txt\", \"w\")\n    L = [\"This is Delhi \\n\", \"This is Paris \\n\", \"This is London \\n\"]\n    file1.writelines(L)\n    file1.close()\n    assert candidate('myfile.txt') == ['This is Delhi', 'This is Paris', 'This is London']\n"],"entry_point":"f_3925614","intent":"read a file `filename` into a list `data`","library":[],"docs":[]}
{"task_id":22187233,"prompt":"def f_22187233():\n\treturn ","suffix":"","canonical_solution":"\"\"\"\"\"\".join([char for char in 'it is icy' if char != 'i'])","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == 't s cy'\n"],"entry_point":"f_22187233","intent":"delete all occurrences of character 'i' in string 'it is icy'","library":[],"docs":[]}
{"task_id":22187233,"prompt":"def f_22187233():\n\treturn ","suffix":"","canonical_solution":"re.sub('i', '', 'it is icy')","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    assert candidate() == 't s cy'\n"],"entry_point":"f_22187233","intent":"delete all instances of a character 'i' in a string 'it is icy'","library":["re"],"docs":[{"text":"re.purge()  \nClear the regular expression cache.","title":"python.library.re#re.purge"},{"text":"window.delch([y, x])  \nDelete any character at (y, x).","title":"python.library.curses#curses.window.delch"},{"text":"window.clrtoeol()  \nErase from cursor to the end of the line.","title":"python.library.curses#curses.window.clrtoeol"},{"text":"clear()  \nRemove all elements from the set.","title":"python.library.stdtypes#frozenset.clear"},{"text":"clear()  \nRemove all items from the dictionary.","title":"python.library.stdtypes#dict.clear"},{"text":"end  \nThe index after the last invalid data in object.","title":"python.library.exceptions#UnicodeError.end"},{"text":"numpy.chararray.strip method   chararray.strip(chars=None)[source]\n \nFor each element in self, return a copy with the leading and trailing characters removed.  See also  char.strip","title":"numpy.reference.generated.numpy.chararray.strip"},{"text":"start  \nThe first index of invalid data in object.","title":"python.library.exceptions#UnicodeError.start"},{"text":"InteractiveConsole.resetbuffer()  \nRemove any unhandled source text from the input buffer.","title":"python.library.code#code.InteractiveConsole.resetbuffer"},{"text":"numpy.char.chararray.strip method   char.chararray.strip(chars=None)[source]\n \nFor each element in self, return a copy with the leading and trailing characters removed.  See also  char.strip","title":"numpy.reference.generated.numpy.char.chararray.strip"}]}
{"task_id":22187233,"prompt":"def f_22187233():\n\treturn ","suffix":"","canonical_solution":"\"\"\"it is icy\"\"\".replace('i', '')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == 't s cy'\n"],"entry_point":"f_22187233","intent":"delete all characters \"i\" in string \"it is icy\"","library":[],"docs":[]}
{"task_id":13413590,"prompt":"def f_13413590(df):\n\treturn ","suffix":"","canonical_solution":"df.dropna(subset=[1])","test_start":"\nimport numpy as np\nimport pandas as pd\n\ndef check(candidate):","test":["\n    data = {0:[3.0, 4.0, 2.0], 1:[2.0, 3.0, np.nan], 2:[np.nan, 3.0, np.nan]}\n    df = pd.DataFrame(data)\n    d = {0:[3.0, 4.0], 1:[2.0, 3.0], 2:[np.nan, 3.0]}\n    res = pd.DataFrame(d)\n    assert candidate(df).equals(res)\n"],"entry_point":"f_13413590","intent":"Drop rows of pandas dataframe `df` having NaN in column at index \"1\"","library":["numpy","pandas"],"docs":[{"text":"pandas.DataFrame.drop   DataFrame.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')[source]\n \nDrop specified labels from rows or columns. Remove rows or columns by specifying label names and corresponding axis, or by specifying directly index or column names. When using a multi-index, labels on different levels can be removed by specifying the level. See the user guide <advanced.shown_levels> for more information about the now unused levels.  Parameters \n \nlabels:single label or list-like\n\n\nIndex or column labels to drop. A tuple will be used as a single label and not treated as a list-like.  \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019}, default 0\n\n\nWhether to drop labels from the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).  \nindex:single label or list-like\n\n\nAlternative to specifying axis (labels, axis=0 is equivalent to index=labels).  \ncolumns:single label or list-like\n\n\nAlternative to specifying axis (labels, axis=1 is equivalent to columns=labels).  \nlevel:int or level name, optional\n\n\nFor MultiIndex, level from which the labels will be removed.  \ninplace:bool, default False\n\n\nIf False, return a copy. Otherwise, do operation inplace and return None.  \nerrors:{\u2018ignore\u2019, \u2018raise\u2019}, default \u2018raise\u2019\n\n\nIf \u2018ignore\u2019, suppress error and only existing labels are dropped.    Returns \n DataFrame or None\n\nDataFrame without the removed index or column labels or None if inplace=True.    Raises \n KeyError\n\nIf any of the labels is not found in the selected axis.      See also  DataFrame.loc\n\nLabel-location based indexer for selection by label.  DataFrame.dropna\n\nReturn DataFrame with labels on given axis omitted where (all or any) data are missing.  DataFrame.drop_duplicates\n\nReturn DataFrame with duplicate rows removed, optionally only considering certain columns.  Series.drop\n\nReturn Series with specified index labels removed.    Examples \n>>> df = pd.DataFrame(np.arange(12).reshape(3, 4),\n...                   columns=['A', 'B', 'C', 'D'])\n>>> df\n   A  B   C   D\n0  0  1   2   3\n1  4  5   6   7\n2  8  9  10  11\n  Drop columns \n>>> df.drop(['B', 'C'], axis=1)\n   A   D\n0  0   3\n1  4   7\n2  8  11\n  \n>>> df.drop(columns=['B', 'C'])\n   A   D\n0  0   3\n1  4   7\n2  8  11\n  Drop a row by index \n>>> df.drop([0, 1])\n   A  B   C   D\n2  8  9  10  11\n  Drop columns and\/or rows of MultiIndex DataFrame \n>>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],\n...                              ['speed', 'weight', 'length']],\n...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n>>> df = pd.DataFrame(index=midx, columns=['big', 'small'],\n...                   data=[[45, 30], [200, 100], [1.5, 1], [30, 20],\n...                         [250, 150], [1.5, 0.8], [320, 250],\n...                         [1, 0.8], [0.3, 0.2]])\n>>> df\n                big     small\nlama    speed   45.0    30.0\n        weight  200.0   100.0\n        length  1.5     1.0\ncow     speed   30.0    20.0\n        weight  250.0   150.0\n        length  1.5     0.8\nfalcon  speed   320.0   250.0\n        weight  1.0     0.8\n        length  0.3     0.2\n  Drop a specific index combination from the MultiIndex DataFrame, i.e., drop the combination 'falcon' and 'weight', which deletes only the corresponding row \n>>> df.drop(index=('falcon', 'weight'))\n                big     small\nlama    speed   45.0    30.0\n        weight  200.0   100.0\n        length  1.5     1.0\ncow     speed   30.0    20.0\n        weight  250.0   150.0\n        length  1.5     0.8\nfalcon  speed   320.0   250.0\n        length  0.3     0.2\n  \n>>> df.drop(index='cow', columns='small')\n                big\nlama    speed   45.0\n        weight  200.0\n        length  1.5\nfalcon  speed   320.0\n        weight  1.0\n        length  0.3\n  \n>>> df.drop(index='length', level=1)\n                big     small\nlama    speed   45.0    30.0\n        weight  200.0   100.0\ncow     speed   30.0    20.0\n        weight  250.0   150.0\nfalcon  speed   320.0   250.0\n        weight  1.0     0.8","title":"pandas.reference.api.pandas.dataframe.drop"},{"text":"pandas.core.groupby.DataFrameGroupBy.nunique   DataFrameGroupBy.nunique(dropna=True)[source]\n \nReturn DataFrame with counts of unique elements in each position.  Parameters \n \ndropna:bool, default True\n\n\nDon\u2019t include NaN in the counts.    Returns \n nunique: DataFrame\n   Examples \n>>> df = pd.DataFrame({'id': ['spam', 'egg', 'egg', 'spam',\n...                           'ham', 'ham'],\n...                    'value1': [1, 5, 5, 2, 5, 5],\n...                    'value2': list('abbaxy')})\n>>> df\n     id  value1 value2\n0  spam       1      a\n1   egg       5      b\n2   egg       5      b\n3  spam       2      a\n4   ham       5      x\n5   ham       5      y\n  \n>>> df.groupby('id').nunique()\n      value1  value2\nid\negg        1       1\nham        1       2\nspam       2       1\n  Check for rows with the same id but conflicting values: \n>>> df.groupby('id').filter(lambda g: (g.nunique() > 1).any())\n     id  value1 value2\n0  spam       1      a\n3  spam       2      a\n4   ham       5      x\n5   ham       5      y","title":"pandas.reference.api.pandas.core.groupby.dataframegroupby.nunique"},{"text":"pandas.tseries.offsets.Nano.apply   Nano.apply()","title":"pandas.reference.api.pandas.tseries.offsets.nano.apply"},{"text":"pandas.DataFrame.drop_duplicates   DataFrame.drop_duplicates(subset=None, keep='first', inplace=False, ignore_index=False)[source]\n \nReturn DataFrame with duplicate rows removed. Considering certain columns is optional. Indexes, including time indexes are ignored.  Parameters \n \nsubset:column label or sequence of labels, optional\n\n\nOnly consider certain columns for identifying duplicates, by default use all of the columns.  \nkeep:{\u2018first\u2019, \u2018last\u2019, False}, default \u2018first\u2019\n\n\nDetermines which duplicates (if any) to keep. - first : Drop duplicates except for the first occurrence. - last : Drop duplicates except for the last occurrence. - False : Drop all duplicates.  \ninplace:bool, default False\n\n\nWhether to drop duplicates in place or to return a copy.  \nignore_index:bool, default False\n\n\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.  New in version 1.0.0.     Returns \n DataFrame or None\n\nDataFrame with duplicates removed or None if inplace=True.      See also  DataFrame.value_counts\n\nCount unique combinations of columns.    Examples Consider dataset containing ramen rating. \n>>> df = pd.DataFrame({\n...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n...     'rating': [4, 4, 3.5, 15, 5]\n... })\n>>> df\n    brand style  rating\n0  Yum Yum   cup     4.0\n1  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n3  Indomie  pack    15.0\n4  Indomie  pack     5.0\n  By default, it removes duplicate rows based on all columns. \n>>> df.drop_duplicates()\n    brand style  rating\n0  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n3  Indomie  pack    15.0\n4  Indomie  pack     5.0\n  To remove duplicates on specific column(s), use subset. \n>>> df.drop_duplicates(subset=['brand'])\n    brand style  rating\n0  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n  To remove duplicates and keep last occurrences, use keep. \n>>> df.drop_duplicates(subset=['brand', 'style'], keep='last')\n    brand style  rating\n1  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n4  Indomie  pack     5.0","title":"pandas.reference.api.pandas.dataframe.drop_duplicates"},{"text":"pandas.IntervalIndex.mid   IntervalIndex.mid","title":"pandas.reference.api.pandas.intervalindex.mid"},{"text":"pandas.Index.empty   propertyIndex.empty","title":"pandas.reference.api.pandas.index.empty"},{"text":"pandas.IntervalIndex.left   IntervalIndex.left","title":"pandas.reference.api.pandas.intervalindex.left"},{"text":"pandas.errors.OutOfBoundsDatetime   exceptionpandas.errors.OutOfBoundsDatetime","title":"pandas.reference.api.pandas.errors.outofboundsdatetime"},{"text":"pandas.IntervalIndex.right   IntervalIndex.right","title":"pandas.reference.api.pandas.intervalindex.right"},{"text":"pandas.DataFrame.dropna   DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)[source]\n \nRemove missing values. See the User Guide for more on which values are considered missing, and how to work with missing data.  Parameters \n \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019}, default 0\n\n\nDetermine if rows or columns which contain missing values are removed.  0, or \u2018index\u2019 : Drop rows which contain missing values. 1, or \u2018columns\u2019 : Drop columns which contain missing value.   Changed in version 1.0.0: Pass tuple or list to drop on multiple axes. Only a single axis is allowed.   \nhow:{\u2018any\u2019, \u2018all\u2019}, default \u2018any\u2019\n\n\nDetermine if row or column is removed from DataFrame, when we have at least one NA or all NA.  \u2018any\u2019 : If any NA values are present, drop that row or column. \u2018all\u2019 : If all values are NA, drop that row or column.   \nthresh:int, optional\n\n\nRequire that many non-NA values.  \nsubset:column label or sequence of labels, optional\n\n\nLabels along other axis to consider, e.g. if you are dropping rows these would be a list of columns to include.  \ninplace:bool, default False\n\n\nIf True, do operation inplace and return None.    Returns \n DataFrame or None\n\nDataFrame with NA entries dropped from it or None if inplace=True.      See also  DataFrame.isna\n\nIndicate missing values.  DataFrame.notna\n\nIndicate existing (non-missing) values.  DataFrame.fillna\n\nReplace missing values.  Series.dropna\n\nDrop missing values.  Index.dropna\n\nDrop missing indices.    Examples \n>>> df = pd.DataFrame({\"name\": ['Alfred', 'Batman', 'Catwoman'],\n...                    \"toy\": [np.nan, 'Batmobile', 'Bullwhip'],\n...                    \"born\": [pd.NaT, pd.Timestamp(\"1940-04-25\"),\n...                             pd.NaT]})\n>>> df\n       name        toy       born\n0    Alfred        NaN        NaT\n1    Batman  Batmobile 1940-04-25\n2  Catwoman   Bullwhip        NaT\n  Drop the rows where at least one element is missing. \n>>> df.dropna()\n     name        toy       born\n1  Batman  Batmobile 1940-04-25\n  Drop the columns where at least one element is missing. \n>>> df.dropna(axis='columns')\n       name\n0    Alfred\n1    Batman\n2  Catwoman\n  Drop the rows where all elements are missing. \n>>> df.dropna(how='all')\n       name        toy       born\n0    Alfred        NaN        NaT\n1    Batman  Batmobile 1940-04-25\n2  Catwoman   Bullwhip        NaT\n  Keep only the rows with at least 2 non-NA values. \n>>> df.dropna(thresh=2)\n       name        toy       born\n1    Batman  Batmobile 1940-04-25\n2  Catwoman   Bullwhip        NaT\n  Define in which columns to look for missing values. \n>>> df.dropna(subset=['name', 'toy'])\n       name        toy       born\n1    Batman  Batmobile 1940-04-25\n2  Catwoman   Bullwhip        NaT\n  Keep the DataFrame with valid entries in the same variable. \n>>> df.dropna(inplace=True)\n>>> df\n     name        toy       born\n1  Batman  Batmobile 1940-04-25","title":"pandas.reference.api.pandas.dataframe.dropna"}]}
{"task_id":598398,"prompt":"def f_598398(myList):\n\treturn ","suffix":"","canonical_solution":"[x for x in myList if x.n == 30]","test_start":"\nimport numpy as np\nimport pandas as pd\n\ndef check(candidate):","test":["\n    class Data: \n        def __init__(self, a, n): \n            self.a = a\n            self.n = n\n    \n    myList = [Data(i, 10*(i%4)) for i in range(20)]\n    assert candidate(myList) == [myList[i] for i in [3, 7, 11, 15, 19]]\n"],"entry_point":"f_598398","intent":"get elements from list `myList`, that have a field `n` value 30","library":["numpy","pandas"],"docs":[]}
{"task_id":10351772,"prompt":"def f_10351772(intstringlist):\n\t","suffix":"\n\treturn nums","canonical_solution":"nums = [int(x) for x in intstringlist]","test_start":"\ndef check(candidate):","test":["\n    assert candidate(['1', '2', '3', '4', '5']) == [1, 2, 3, 4, 5]\n","\n    assert candidate(['001', '200', '3', '4', '5']) == [1, 200, 3, 4, 5]\n"],"entry_point":"f_10351772","intent":"converting list of strings `intstringlist` to list of integer `nums`","library":[],"docs":[]}
{"task_id":493386,"prompt":"def f_493386():\n\treturn ","suffix":"","canonical_solution":"sys.stdout.write('.')","test_start":"\nimport sys\n\ndef check(candidate):","test":["\n    assert candidate() == 1\n"],"entry_point":"f_493386","intent":"print \".\" without newline","library":["sys"],"docs":[{"text":"token.NEWLINE","title":"python.library.token#token.NEWLINE"},{"text":"re.S  \nre.DOTALL  \nMake the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).","title":"python.library.re#re.S"},{"text":"token.DOT  \nToken value for \".\".","title":"python.library.token#token.DOT"},{"text":"token.INDENT","title":"python.library.token#token.INDENT"},{"text":"re.S  \nre.DOTALL  \nMake the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).","title":"python.library.re#re.DOTALL"},{"text":"PrettyPrinter.pprint(object)  \nPrint the formatted representation of object on the configured stream, followed by a newline.","title":"python.library.pprint#pprint.PrettyPrinter.pprint"},{"text":"token.COLON  \nToken value for \":\".","title":"python.library.token#token.COLON"},{"text":"msvcrt.putch(char)  \nPrint the byte string char to the console without buffering.","title":"python.library.msvcrt#msvcrt.putch"},{"text":"token.N_TOKENS","title":"python.library.token#token.N_TOKENS"},{"text":"gettext.lngettext(singular, plural, n)","title":"python.library.gettext#gettext.lngettext"}]}
{"task_id":6569528,"prompt":"def f_6569528():\n\treturn ","suffix":"","canonical_solution":"int(round(2.52 * 100))","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == 252\n"],"entry_point":"f_6569528","intent":"round off the float that is the product of `2.52 * 100` and convert it to an int","library":[],"docs":[]}
{"task_id":3964681,"prompt":"def f_3964681():\n\t","suffix":"\n\treturn files","canonical_solution":"\n\tos.chdir('\/mydir')\n\tfiles = [] \n\tfor file in glob.glob('*.txt'):\n\t\tfiles.append(file)\n","test_start":"\nimport os\nimport glob\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    samples = ['abc.txt']\n    os.chdir = Mock()\n    glob.glob = Mock(return_value = samples)\n    assert candidate() == samples\n"],"entry_point":"f_3964681","intent":"Find all files `files` in directory '\/mydir' with extension '.txt'","library":["glob","os"],"docs":[{"text":"right  \nThe directory b.","title":"python.library.filecmp#filecmp.dircmp.right"},{"text":"left  \nThe directory a.","title":"python.library.filecmp#filecmp.dircmp.left"},{"text":"left_only  \nFiles and subdirectories only in a.","title":"python.library.filecmp#filecmp.dircmp.left_only"},{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"common_dirs  \nSubdirectories in both a and b.","title":"python.library.filecmp#filecmp.dircmp.common_dirs"},{"text":"report_full_closure()  \nPrint a comparison between a and b and common subdirectories (recursively).","title":"python.library.filecmp#filecmp.dircmp.report_full_closure"},{"text":"left_list  \nFiles and subdirectories in a, filtered by hide and ignore.","title":"python.library.filecmp#filecmp.dircmp.left_list"},{"text":"path  \nThe path the finder will search in.","title":"python.library.importlib#importlib.machinery.FileFinder.path"},{"text":"stat.S_IFDIR  \nDirectory.","title":"python.library.stat#stat.S_IFDIR"},{"text":"common  \nFiles and subdirectories in both a and b.","title":"python.library.filecmp#filecmp.dircmp.common"}]}
{"task_id":3964681,"prompt":"def f_3964681():\n\treturn ","suffix":"","canonical_solution":"[file for file in os.listdir('\/mydir') if file.endswith('.txt')]","test_start":"\nimport os\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    samples = ['abc.txt', 'f.csv']\n    os.listdir = Mock(return_value = samples)\n    assert candidate() == ['abc.txt']\n"],"entry_point":"f_3964681","intent":"Find all files in directory \"\/mydir\" with extension \".txt\"","library":["os"],"docs":[{"text":"right  \nThe directory b.","title":"python.library.filecmp#filecmp.dircmp.right"},{"text":"left  \nThe directory a.","title":"python.library.filecmp#filecmp.dircmp.left"},{"text":"stat.S_IFDIR  \nDirectory.","title":"python.library.stat#stat.S_IFDIR"},{"text":"left_only  \nFiles and subdirectories only in a.","title":"python.library.filecmp#filecmp.dircmp.left_only"},{"text":"report_full_closure()  \nPrint a comparison between a and b and common subdirectories (recursively).","title":"python.library.filecmp#filecmp.dircmp.report_full_closure"},{"text":"common_dirs  \nSubdirectories in both a and b.","title":"python.library.filecmp#filecmp.dircmp.common_dirs"},{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"stat.S_IFREG  \nRegular file.","title":"python.library.stat#stat.S_IFREG"},{"text":"left_list  \nFiles and subdirectories in a, filtered by hide and ignore.","title":"python.library.filecmp#filecmp.dircmp.left_list"},{"text":"report_partial_closure()  \nPrint a comparison between a and b and common immediate subdirectories.","title":"python.library.filecmp#filecmp.dircmp.report_partial_closure"}]}
{"task_id":3964681,"prompt":"def f_3964681():\n\treturn ","suffix":"","canonical_solution":"[file for (root, dirs, files) in os.walk('\/mydir') for file in files if file.endswith('.txt')]","test_start":"\nimport os\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    name = '\/mydir'\n    samples = [(name, [], ['abc.txt', 'f.csv'])]\n    os.walk = Mock(return_value = samples)\n    assert candidate() == ['abc.txt']\n"],"entry_point":"f_3964681","intent":"Find all files in directory \"\/mydir\" with extension \".txt\"","library":["os"],"docs":[{"text":"right  \nThe directory b.","title":"python.library.filecmp#filecmp.dircmp.right"},{"text":"left  \nThe directory a.","title":"python.library.filecmp#filecmp.dircmp.left"},{"text":"stat.S_IFDIR  \nDirectory.","title":"python.library.stat#stat.S_IFDIR"},{"text":"left_only  \nFiles and subdirectories only in a.","title":"python.library.filecmp#filecmp.dircmp.left_only"},{"text":"report_full_closure()  \nPrint a comparison between a and b and common subdirectories (recursively).","title":"python.library.filecmp#filecmp.dircmp.report_full_closure"},{"text":"common_dirs  \nSubdirectories in both a and b.","title":"python.library.filecmp#filecmp.dircmp.common_dirs"},{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"stat.S_IFREG  \nRegular file.","title":"python.library.stat#stat.S_IFREG"},{"text":"left_list  \nFiles and subdirectories in a, filtered by hide and ignore.","title":"python.library.filecmp#filecmp.dircmp.left_list"},{"text":"report_partial_closure()  \nPrint a comparison between a and b and common immediate subdirectories.","title":"python.library.filecmp#filecmp.dircmp.report_partial_closure"}]}
{"task_id":20865487,"prompt":"def f_20865487(df):\n\treturn ","suffix":"","canonical_solution":"df.plot(legend=False)","test_start":"\nimport os \nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame([1, 2, 3, 4, 5], columns = ['Vals'])\n    res = candidate(df)\n    assert 'AxesSubplot' in str(type(res))\n    assert res.legend_ is None\n"],"entry_point":"f_20865487","intent":"plot dataframe `df` without a legend","library":["os","pandas"],"docs":[{"text":"texname","title":"matplotlib.dviread#matplotlib.dviread.DviFont.texname"},{"text":"texname\n \nAlias for field number 0","title":"matplotlib.dviread#matplotlib.dviread.PsFont.texname"},{"text":"filename\n \nAlias for field number 4","title":"matplotlib.dviread#matplotlib.dviread.PsFont.filename"},{"text":"__hash__=None","title":"matplotlib.transformations#matplotlib.transforms.TransformWrapper.__hash__"},{"text":"axis=None","title":"matplotlib.ticker_api#matplotlib.ticker.TickHelper.axis"},{"text":"psname\n \nAlias for field number 1","title":"matplotlib.dviread#matplotlib.dviread.PsFont.psname"},{"text":"property df","title":"torch.distributions#torch.distributions.chi2.Chi2.df"},{"text":"prop","title":"matplotlib.type1font#matplotlib.type1font.Type1Font.prop"},{"text":"__hash__=None","title":"matplotlib.transformations#matplotlib.transforms.CompositeGenericTransform.__hash__"},{"text":"decrypted","title":"matplotlib.type1font#matplotlib.type1font.Type1Font.decrypted"}]}
{"task_id":13368659,"prompt":"def f_13368659():\n\treturn ","suffix":"","canonical_solution":"['192.168.%d.%d'%(i, j) for i in range(256) for j in range(256)]","test_start":"\ndef check(candidate):","test":["\n    addrs = candidate()\n    assert len(addrs) == 256*256\n    assert addrs == [f'192.168.{i}.{j}' for i in range(256) for j in range(256)]\n"],"entry_point":"f_13368659","intent":"loop through the IP address range \"192.168.x.x\"","library":[],"docs":[]}
{"task_id":4065737,"prompt":"def f_4065737(x):\n\treturn ","suffix":"","canonical_solution":"sum(1 << i for i, b in enumerate(x) if b)","test_start":"\ndef check(candidate):","test":["\n    assert candidate([1,2,3]) == 7\n","\n    assert candidate([1,2,None,3,None]) == 11\n"],"entry_point":"f_4065737","intent":"Sum the corresponding decimal values for binary values of each boolean element in list `x`","library":[],"docs":[]}
{"task_id":8691311,"prompt":"def f_8691311(line1, line2, line3, target):\n\t","suffix":"\n\treturn ","canonical_solution":"target.write('%r\\n%r\\n%r\\n' % (line1, line2, line3))","test_start":"\ndef check(candidate):","test":["\n    file_name = 'abc.txt'\n    lines = ['fgh', 'ijk', 'mnop']\n    f = open(file_name, 'a')\n    candidate(lines[0], lines[1], lines[2], f)\n    f.close()\n    with open(file_name, 'r') as f:\n        f_lines = f.readlines()\n        for i in range (0, len(lines)):\n            assert lines[i] in f_lines[i]\n"],"entry_point":"f_8691311","intent":"write multiple strings `line1`, `line2` and `line3` in one line in a file `target`","library":[],"docs":[]}
{"task_id":10632111,"prompt":"def f_10632111(data):\n\treturn ","suffix":"","canonical_solution":"[y for x in data for y in (x if isinstance(x, list) else [x])]","test_start":"\ndef check(candidate):","test":["\n    data = [[1, 2], [3]]\n    assert candidate(data) == [1, 2, 3]\n","\n    data = [[1, 2], [3], []]\n    assert candidate(data) == [1, 2, 3]\n","\n    data = [1,2,3]\n    assert candidate(data) == [1, 2, 3]\n"],"entry_point":"f_10632111","intent":"Convert list of lists `data` into a flat list","library":[],"docs":[]}
{"task_id":15392730,"prompt":"def f_15392730():\n\treturn ","suffix":"","canonical_solution":"'foo\\nbar'.encode('unicode_escape')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == b'foo\\\\nbar'\n"],"entry_point":"f_15392730","intent":"Print new line character as `\\n` in a string `foo\\nbar`","library":[],"docs":[]}
{"task_id":1010961,"prompt":"def f_1010961(s):\n\treturn ","suffix":"","canonical_solution":"\"\"\"\"\"\".join(s.rsplit(',', 1))","test_start":"\ndef check(candidate):","test":["\n    assert candidate('abc, def, klm') == 'abc, def klm'\n"],"entry_point":"f_1010961","intent":"remove last comma character ',' in string `s`","library":[],"docs":[]}
{"task_id":23855976,"prompt":"def f_23855976(x):\n\treturn ","suffix":"","canonical_solution":"(x[1:] + x[:-1]) \/ 2","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    x = np.array([ 1230.,  1230.,  1227.,  1235.,  1217.,  1153.,  1170.])\n    xm = np.array([1230. , 1228.5, 1231. , 1226. , 1185. , 1161.5])\n    assert np.array_equal(candidate(x), xm)\n"],"entry_point":"f_23855976","intent":"calculate the mean of each element in array `x` with the element previous to it","library":["numpy"],"docs":[]}
{"task_id":23855976,"prompt":"def f_23855976(x):\n\treturn ","suffix":"","canonical_solution":"x[:-1] + (x[1:] - x[:-1]) \/ 2","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    x = np.array([ 1230.,  1230.,  1227.,  1235.,  1217.,  1153.,  1170.])\n    xm = np.array([1230. , 1228.5, 1231. , 1226. , 1185. , 1161.5])\n    assert np.array_equal(candidate(x), xm)\n"],"entry_point":"f_23855976","intent":"get an array of the mean of each two consecutive values in numpy array `x`","library":["numpy"],"docs":[]}
{"task_id":6375343,"prompt":"def f_6375343():\n\t","suffix":"\n\treturn arr","canonical_solution":"arr = numpy.fromiter(codecs.open('new.txt', encoding='utf-8'), dtype='<U2')","test_start":"\nimport numpy\nimport codecs\nimport numpy as np\n\ndef check(candidate):","test":["\n    with open ('new.txt', 'a', encoding='utf-8') as f:\n        f.write('\u091f')\n        f.write('\u091c')\n    arr = candidate()\n    assert arr[0] == '\u091f\u091c'\n"],"entry_point":"f_6375343","intent":"load data containing `utf-8` from file `new.txt` into numpy array `arr`","library":["codecs","numpy"],"docs":[{"text":"numpy.unicode_[source]\n \nalias of numpy.str_","title":"numpy.reference.arrays.scalars#numpy.unicode_"},{"text":"start  \nThe first index of invalid data in object.","title":"python.library.exceptions#UnicodeError.start"},{"text":"numpy.chararray.dumps method   chararray.dumps()\n \nReturns the pickle of the array as a string. pickle.loads will convert the string back to an array.  Parameters \n None","title":"numpy.reference.generated.numpy.chararray.dumps"},{"text":"numpy.char.chararray.dumps method   char.chararray.dumps()\n \nReturns the pickle of the array as a string. pickle.loads will convert the string back to an array.  Parameters \n None","title":"numpy.reference.generated.numpy.char.chararray.dumps"},{"text":"numpy.string_[source]\n \nalias of numpy.bytes_","title":"numpy.reference.arrays.scalars#numpy.string_"},{"text":"end  \nThe index after the last invalid data in object.","title":"python.library.exceptions#UnicodeError.end"},{"text":"class numpy.object_[source]\n \nAny Python object.  Character code \n'O'","title":"numpy.reference.arrays.scalars#numpy.object_"},{"text":"array.fromfile(f, n)  \nRead n items (as machine values) from the file object f and append them to the end of the array. If less than n items are available, EOFError is raised, but the items that were available are still inserted into the array.","title":"python.library.array#array.array.fromfile"},{"text":"numpy.chararray.decode method   chararray.decode(encoding=None, errors=None)[source]\n \nCalls str.decode element-wise.  See also  char.decode","title":"numpy.reference.generated.numpy.chararray.decode"},{"text":"numpy.char.chararray.index method   char.chararray.index(sub, start=0, end=None)[source]\n \nLike find, but raises ValueError when the substring is not found.  See also  char.index","title":"numpy.reference.generated.numpy.char.chararray.index"}]}
{"task_id":1547733,"prompt":"def f_1547733(l):\n\t","suffix":"\n\treturn l","canonical_solution":"l = sorted(l, key=itemgetter('time'), reverse=True)","test_start":"\nfrom operator import itemgetter\n\ndef check(candidate):","test":["\n    l = [ {'time':33}, {'time':11}, {'time':66} ]\n    assert candidate(l) == [{'time':66}, {'time':33}, {'time':11}]\n"],"entry_point":"f_1547733","intent":"reverse sort list of dicts `l` by value for key `time`","library":["operator"],"docs":[{"text":"array.reverse()  \nReverse the order of the items in the array.","title":"python.library.array#array.array.reverse"},{"text":"pandas.Timestamp.fold   Timestamp.fold","title":"pandas.reference.api.pandas.timestamp.fold"},{"text":"pandas.tseries.offsets.Hour.apply   Hour.apply()","title":"pandas.reference.api.pandas.tseries.offsets.hour.apply"},{"text":"pandas.tseries.offsets.Hour.freqstr   Hour.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.hour.freqstr"},{"text":"pandas.tseries.offsets.Hour.copy   Hour.copy()","title":"pandas.reference.api.pandas.tseries.offsets.hour.copy"},{"text":"pandas.tseries.offsets.Day.freqstr   Day.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.day.freqstr"},{"text":"pandas.Timestamp.freq   Timestamp.freq","title":"pandas.reference.api.pandas.timestamp.freq"},{"text":"reverse()","title":"django.ref.models.querysets#django.db.models.query.QuerySet.reverse"},{"text":"pandas.tseries.offsets.BYearEnd.freqstr   BYearEnd.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.byearend.freqstr"},{"text":"pandas.tseries.offsets.Hour.nanos   Hour.nanos","title":"pandas.reference.api.pandas.tseries.offsets.hour.nanos"}]}
{"task_id":1547733,"prompt":"def f_1547733(l):\n\t","suffix":"\n\treturn l","canonical_solution":"l = sorted(l, key=lambda a: a['time'], reverse=True)","test_start":"\ndef check(candidate):","test":["\n    l = [ {'time':33}, {'time':11}, {'time':66} ]\n    assert candidate(l) == [{'time':66}, {'time':33}, {'time':11}]\n"],"entry_point":"f_1547733","intent":"Sort a list of dictionary `l` based on key `time` in descending order","library":[],"docs":[]}
{"task_id":37080612,"prompt":"def f_37080612(df):\n\treturn ","suffix":"","canonical_solution":"df.loc[df[0].str.contains('(Hel|Just)')]","test_start":"\nimport pandas as pd \n\ndef check(candidate):","test":["\n    df = pd.DataFrame([['Hello', 'World'], ['Just', 'Wanted'], ['To', 'Say'], ['I\\'m', 'Tired']])\n    df1 = candidate(df)\n    assert df1[0][0] == 'Hello'\n    assert df1[0][1] == 'Just'\n"],"entry_point":"f_37080612","intent":"get rows of dataframe `df` that match regex '(Hel|Just)'","library":["pandas"],"docs":[{"text":"pandas.Series.str.findall   Series.str.findall(pat, flags=0)[source]\n \nFind all occurrences of pattern or regular expression in the Series\/Index. Equivalent to applying re.findall() to all the elements in the Series\/Index.  Parameters \n \npat:str\n\n\nPattern or regular expression.  \nflags:int, default 0\n\n\nFlags from re module, e.g. re.IGNORECASE (default is 0, which means no flags).    Returns \n Series\/Index of lists of strings\n\nAll non-overlapping matches of pattern or regular expression in each string of this Series\/Index.      See also  count\n\nCount occurrences of pattern or regular expression in each string of the Series\/Index.  extractall\n\nFor each string in the Series, extract groups from all matches of regular expression and return a DataFrame with one row for each match and one column for each group.  re.findall\n\nThe equivalent re function to all non-overlapping matches of pattern or regular expression in string, as a list of strings.    Examples \n>>> s = pd.Series(['Lion', 'Monkey', 'Rabbit'])\n  The search for the pattern \u2018Monkey\u2019 returns one match: \n>>> s.str.findall('Monkey')\n0          []\n1    [Monkey]\n2          []\ndtype: object\n  On the other hand, the search for the pattern \u2018MONKEY\u2019 doesn\u2019t return any match: \n>>> s.str.findall('MONKEY')\n0    []\n1    []\n2    []\ndtype: object\n  Flags can be added to the pattern or regular expression. For instance, to find the pattern \u2018MONKEY\u2019 ignoring the case: \n>>> import re\n>>> s.str.findall('MONKEY', flags=re.IGNORECASE)\n0          []\n1    [Monkey]\n2          []\ndtype: object\n  When the pattern matches more than one string in the Series, all matches are returned: \n>>> s.str.findall('on')\n0    [on]\n1    [on]\n2      []\ndtype: object\n  Regular expressions are supported too. For instance, the search for all the strings ending with the word \u2018on\u2019 is shown next: \n>>> s.str.findall('on$')\n0    [on]\n1      []\n2      []\ndtype: object\n  If the pattern is found more than once in the same string, then a list of multiple strings is returned: \n>>> s.str.findall('b')\n0        []\n1        []\n2    [b, b]\ndtype: object","title":"pandas.reference.api.pandas.series.str.findall"},{"text":"pattern  \nThe regular expression pattern.","title":"python.library.re#re.error.pattern"},{"text":"pandas.Series.str.extractall   Series.str.extractall(pat, flags=0)[source]\n \nExtract capture groups in the regex pat as columns in DataFrame. For each subject string in the Series, extract groups from all matches of regular expression pat. When each subject string in the Series has exactly one match, extractall(pat).xs(0, level=\u2019match\u2019) is the same as extract(pat).  Parameters \n \npat:str\n\n\nRegular expression pattern with capturing groups.  \nflags:int, default 0 (no flags)\n\n\nA re module flag, for example re.IGNORECASE. These allow to modify regular expression matching for things like case, spaces, etc. Multiple flags can be combined with the bitwise OR operator, for example re.IGNORECASE | re.MULTILINE.    Returns \n DataFrame\n\nA DataFrame with one row for each match, and one column for each group. Its rows have a MultiIndex with first levels that come from the subject Series. The last level is named \u2018match\u2019 and indexes the matches in each item of the Series. Any capture group names in regular expression pat will be used for column names; otherwise capture group numbers will be used.      See also  extract\n\nReturns first match only (not all matches).    Examples A pattern with one group will return a DataFrame with one column. Indices with no matches will not appear in the result. \n>>> s = pd.Series([\"a1a2\", \"b1\", \"c1\"], index=[\"A\", \"B\", \"C\"])\n>>> s.str.extractall(r\"[ab](\\d)\")\n        0\nmatch\nA 0      1\n  1      2\nB 0      1\n  Capture group names are used for column names of the result. \n>>> s.str.extractall(r\"[ab](?P<digit>\\d)\")\n        digit\nmatch\nA 0         1\n  1         2\nB 0         1\n  A pattern with two groups will return a DataFrame with two columns. \n>>> s.str.extractall(r\"(?P<letter>[ab])(?P<digit>\\d)\")\n        letter digit\nmatch\nA 0          a     1\n  1          a     2\nB 0          b     1\n  Optional groups that do not match are NaN in the result. \n>>> s.str.extractall(r\"(?P<letter>[ab])?(?P<digit>\\d)\")\n        letter digit\nmatch\nA 0          a     1\n  1          a     2\nB 0          b     1\nC 0        NaN     1","title":"pandas.reference.api.pandas.series.str.extractall"},{"text":"re.purge()  \nClear the regular expression cache.","title":"python.library.re#re.purge"},{"text":"pandas.Series.str.extract   Series.str.extract(pat, flags=0, expand=True)[source]\n \nExtract capture groups in the regex pat as columns in a DataFrame. For each subject string in the Series, extract groups from the first match of regular expression pat.  Parameters \n \npat:str\n\n\nRegular expression pattern with capturing groups.  \nflags:int, default 0 (no flags)\n\n\nFlags from the re module, e.g. re.IGNORECASE, that modify regular expression matching for things like case, spaces, etc. For more details, see re.  \nexpand:bool, default True\n\n\nIf True, return DataFrame with one column per capture group. If False, return a Series\/Index if there is one capture group or DataFrame if there are multiple capture groups.    Returns \n DataFrame or Series or Index\n\nA DataFrame with one row for each subject string, and one column for each group. Any capture group names in regular expression pat will be used for column names; otherwise capture group numbers will be used. The dtype of each result column is always object, even when no match is found. If expand=False and pat has only one capture group, then return a Series (if subject is a Series) or Index (if subject is an Index).      See also  extractall\n\nReturns all matches (not just the first match).    Examples A pattern with two groups will return a DataFrame with two columns. Non-matches will be NaN. \n>>> s = pd.Series(['a1', 'b2', 'c3'])\n>>> s.str.extract(r'([ab])(\\d)')\n    0    1\n0    a    1\n1    b    2\n2  NaN  NaN\n  A pattern may contain optional groups. \n>>> s.str.extract(r'([ab])?(\\d)')\n    0  1\n0    a  1\n1    b  2\n2  NaN  3\n  Named groups will become column names in the result. \n>>> s.str.extract(r'(?P<letter>[ab])(?P<digit>\\d)')\nletter digit\n0      a     1\n1      b     2\n2    NaN   NaN\n  A pattern with one group will return a DataFrame with one column if expand=True. \n>>> s.str.extract(r'[ab](\\d)', expand=True)\n    0\n0    1\n1    2\n2  NaN\n  A pattern with one group will return a Series if expand=False. \n>>> s.str.extract(r'[ab](\\d)', expand=False)\n0      1\n1      2\n2    NaN\ndtype: object","title":"pandas.reference.api.pandas.series.str.extract"},{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"pandas.DataFrame.filter   DataFrame.filter(items=None, like=None, regex=None, axis=None)[source]\n \nSubset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index.  Parameters \n \nitems:list-like\n\n\nKeep labels from axis which are in items.  \nlike:str\n\n\nKeep labels from axis for which \u201clike in label == True\u201d.  \nregex:str (regular expression)\n\n\nKeep labels from axis for which re.search(regex, label) == True.  \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default None\n\n\nThe axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, \u2018index\u2019 for Series, \u2018columns\u2019 for DataFrame.    Returns \n same type as input object\n    See also  DataFrame.loc\n\nAccess a group of rows and columns by label(s) or a boolean array.    Notes The items, like, and regex parameters are enforced to be mutually exclusive. axis defaults to the info axis that is used when indexing with []. Examples \n>>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),\n...                   index=['mouse', 'rabbit'],\n...                   columns=['one', 'two', 'three'])\n>>> df\n        one  two  three\nmouse     1    2      3\nrabbit    4    5      6\n  \n>>> # select columns by name\n>>> df.filter(items=['one', 'three'])\n         one  three\nmouse     1      3\nrabbit    4      6\n  \n>>> # select columns by regular expression\n>>> df.filter(regex='e$', axis=1)\n         one  three\nmouse     1      3\nrabbit    4      6\n  \n>>> # select rows containing 'bbi'\n>>> df.filter(like='bbi', axis=0)\n         one  two  three\nrabbit    4    5      6","title":"pandas.reference.api.pandas.dataframe.filter"},{"text":"str.rindex(sub[, start[, end]])  \nLike rfind() but raises ValueError when the substring sub is not found.","title":"python.library.stdtypes#str.rindex"},{"text":"pandas.Index.names   propertyIndex.names","title":"pandas.reference.api.pandas.index.names"},{"text":"pandas.DataFrame.T   propertyDataFrame.T","title":"pandas.reference.api.pandas.dataframe.t"}]}
{"task_id":14716342,"prompt":"def f_14716342(your_string):\n\treturn ","suffix":"","canonical_solution":"re.search('\\\\[(.*)\\\\]', your_string).group(1)","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate('[uranus]') == 'uranus'\n","\n    assert candidate('hello[world] !') == 'world'\n"],"entry_point":"f_14716342","intent":"find the string in `your_string` between two special characters \"[\" and \"]\"","library":["re"],"docs":[{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"str.rindex(sub[, start[, end]])  \nLike rfind() but raises ValueError when the substring sub is not found.","title":"python.library.stdtypes#str.rindex"},{"text":"numpy.char.chararray.index method   char.chararray.index(sub, start=0, end=None)[source]\n \nLike find, but raises ValueError when the substring is not found.  See also  char.index","title":"numpy.reference.generated.numpy.char.chararray.index"},{"text":"numpy.chararray.index method   chararray.index(sub, start=0, end=None)[source]\n \nLike find, but raises ValueError when the substring is not found.  See also  char.index","title":"numpy.reference.generated.numpy.chararray.index"},{"text":"token.DOUBLESLASH  \nToken value for \"\/\/\".","title":"python.library.token#token.DOUBLESLASH"},{"text":"token.RSQB  \nToken value for \"]\".","title":"python.library.token#token.RSQB"},{"text":"Match.string  \nThe string passed to match() or search().","title":"python.library.re#re.Match.string"},{"text":"pattern  \nThe regular expression pattern.","title":"python.library.re#re.error.pattern"},{"text":"token.SLASH  \nToken value for \"\/\".","title":"python.library.token#token.SLASH"},{"text":"numpy.char.chararray.find method   char.chararray.find(sub, start=0, end=None)[source]\n \nFor each element, return the lowest index in the string where substring sub is found.  See also  char.find","title":"numpy.reference.generated.numpy.char.chararray.find"}]}
{"task_id":18684076,"prompt":"def f_18684076():\n\treturn ","suffix":"","canonical_solution":"[d.strftime('%Y%m%d') for d in pandas.date_range('20130226', '20130302')]","test_start":"\nimport pandas \n\ndef check(candidate):","test":["\n    assert candidate() == ['20130226', '20130227', '20130228', '20130301', '20130302']\n"],"entry_point":"f_18684076","intent":"create a list of date string in 'yyyymmdd' format with Python Pandas from '20130226' to '20130302'","library":["pandas"],"docs":[{"text":"date.__str__()  \nFor a date d, str(d) is equivalent to d.isoformat().","title":"python.library.datetime#datetime.date.__str__"},{"text":"datetime.date()  \nReturn date object with same year, month and day.","title":"python.library.datetime#datetime.datetime.date"},{"text":"date_format  \nSimilar to DateInput.format","title":"django.ref.forms.widgets#django.forms.SplitDateTimeWidget.date_format"},{"text":"pandas.Timestamp.fromisoformat   Timestamp.fromisoformat()\n \nstring -> datetime from datetime.isoformat() output","title":"pandas.reference.api.pandas.timestamp.fromisoformat"},{"text":"date.month  \nBetween 1 and 12 inclusive.","title":"python.library.datetime#datetime.date.month"},{"text":"pandas.tseries.offsets.YearBegin.freqstr   YearBegin.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.yearbegin.freqstr"},{"text":"pandas.Timestamp.day   Timestamp.day","title":"pandas.reference.api.pandas.timestamp.day"},{"text":"pandas.Timestamp.date   Timestamp.date()\n \nReturn date object with same year, month and day.","title":"pandas.reference.api.pandas.timestamp.date"},{"text":"pandas.tseries.offsets.YearEnd.freqstr   YearEnd.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.yearend.freqstr"},{"text":"date.replace(year=self.year, month=self.month, day=self.day)  \nReturn a date with the same value, except for those parameters given new values by whichever keyword arguments are specified. Example: >>> from datetime import date\n>>> d = date(2002, 12, 31)\n>>> d.replace(day=26)\ndatetime.date(2002, 12, 26)","title":"python.library.datetime#datetime.date.replace"}]}
{"task_id":1666700,"prompt":"def f_1666700():\n\treturn ","suffix":"","canonical_solution":"\"\"\"The big brown fox is brown\"\"\".count('brown')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == 2\n"],"entry_point":"f_1666700","intent":"count number of times string 'brown' occurred in string 'The big brown fox is brown'","library":[],"docs":[]}
{"task_id":18979111,"prompt":"def f_18979111(request_body):\n\treturn ","suffix":"","canonical_solution":"json.loads(request_body)","test_start":"\nimport json \n\ndef check(candidate):","test":["\n    x = \"\"\"{\n    \"Name\": \"Jennifer Smith\",\n    \"Contact Number\": 7867567898,\n    \"Email\": \"jen123@gmail.com\",\n    \"Hobbies\":[\"Reading\", \"Sketching\", \"Horse Riding\"]\n    }\"\"\"\n    assert candidate(x) == {'Hobbies': ['Reading', 'Sketching', 'Horse Riding'], 'Name': 'Jennifer Smith', 'Email': 'jen123@gmail.com', 'Contact Number': 7867567898}\n"],"entry_point":"f_18979111","intent":"decode json string `request_body` to python dict","library":["json"],"docs":[{"text":"msg  \nThe unformatted error message.","title":"python.library.json#json.JSONDecodeError.msg"},{"text":"class QueryDict","title":"django.ref.request-response#django.http.QueryDict"},{"text":"Form.errors.get_json_data(escape_html=False)","title":"django.ref.forms.api#django.forms.Form.errors.get_json_data"},{"text":"HttpResponse.items()  \nActs like dict.items() for HTTP headers on the response.","title":"django.ref.request-response#django.http.HttpResponse.items"},{"text":"doc  \nThe JSON document being parsed.","title":"python.library.json#json.JSONDecodeError.doc"},{"text":"json_decoder  \nalias of flask.json.JSONDecoder","title":"flask.api.index#flask.Flask.json_decoder"},{"text":"HttpRequest.readlines()","title":"django.ref.request-response#django.http.HttpRequest.readlines"},{"text":"Form.errors.as_json(escape_html=False)","title":"django.ref.forms.api#django.forms.Form.errors.as_json"},{"text":"HttpRequest.read(size=None)","title":"django.ref.request-response#django.http.HttpRequest.read"},{"text":"HttpResponse.content  \nA bytestring representing the content, encoded from a string if necessary.","title":"django.ref.request-response#django.http.HttpResponse.content"}]}
{"task_id":7243750,"prompt":"def f_7243750(url, file_name):\n\treturn ","suffix":"","canonical_solution":"urllib.request.urlretrieve(url, file_name)","test_start":"\nimport urllib \n\ndef check(candidate):","test":["\n    file_name = 'g.html'\n    candidate('https:\/\/asia.nikkei.com\/Business\/Tech\/Semiconductors\/U.S.-chip-tool-maker-Synopsys-expands-in-Vietnam-amid-China-tech-war', file_name)\n    with open (file_name, 'r') as f:\n        lines = f.readlines()\n        if len(lines) == 0: assert False\n        else: assert True\n"],"entry_point":"f_7243750","intent":"download the file from url `url` and save it under file `file_name`","library":["urllib"],"docs":[{"text":"_save(name, content)","title":"django.howto.custom-file-storage#django.core.files.storage._save"},{"text":"name  \nThe name of the file including the relative path from MEDIA_ROOT.","title":"django.ref.files.file#django.core.files.File.name"},{"text":"FieldFile.url","title":"django.ref.models.fields#django.db.models.fields.files.FieldFile.url"},{"text":"class urllib.request.FileHandler  \nOpen local files.","title":"python.library.urllib.request#urllib.request.FileHandler"},{"text":"FieldFile.save(name, content, save=True)","title":"django.ref.models.fields#django.db.models.fields.files.FieldFile.save"},{"text":"get_available_name(name, max_length=None)","title":"django.howto.custom-file-storage#django.core.files.storage.get_available_name"},{"text":"get_valid_name(name)","title":"django.howto.custom-file-storage#django.core.files.storage.get_valid_name"},{"text":"_open(name, mode='rb')","title":"django.howto.custom-file-storage#django.core.files.storage._open"},{"text":"FieldFile.name","title":"django.ref.models.fields#django.db.models.fields.files.FieldFile.name"},{"text":"views.serve(request, path)","title":"django.ref.contrib.staticfiles#django.contrib.staticfiles.views.serve"}]}
{"task_id":743806,"prompt":"def f_743806(text):\n\treturn ","suffix":"","canonical_solution":"text.split()","test_start":"\ndef check(candidate):","test":["\n    assert candidate('The quick brown fox') == ['The', 'quick', 'brown', 'fox']\n","\n    assert candidate('hello!') == ['hello!']\n","\n    assert candidate('hello world !') == ['hello', 'world', '!']\n"],"entry_point":"f_743806","intent":"split string `text` by space","library":[],"docs":[]}
{"task_id":743806,"prompt":"def f_743806(text):\n\treturn ","suffix":"","canonical_solution":"text.split(',')","test_start":"\ndef check(candidate):","test":["\n    assert candidate('The quick brown fox') == ['The quick brown fox']\n","\n    assert candidate('The,quick,brown,fox') == ['The', 'quick', 'brown', 'fox']\n"],"entry_point":"f_743806","intent":"split string `text` by \",\"","library":[],"docs":[]}
{"task_id":743806,"prompt":"def f_743806(line):\n\treturn ","suffix":"","canonical_solution":"line.split()","test_start":"\ndef check(candidate):","test":["\n    assert candidate('The  quick brown  fox') == ['The', 'quick', 'brown', 'fox']\n"],"entry_point":"f_743806","intent":"Split string `line` into a list by whitespace","library":[],"docs":[]}
{"task_id":35044115,"prompt":"def f_35044115(s):\n\treturn ","suffix":"","canonical_solution":"[re.sub('(?<!\\\\d)\\\\.(?!\\\\d)', ' ', i) for i in s]","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    assert candidate('h.j.k') == ['h', ' ', 'j', ' ', 'k']\n"],"entry_point":"f_35044115","intent":"replace dot characters  '.' associated with ascii letters in list `s` with space ' '","library":["re"],"docs":[{"text":"token.DOT  \nToken value for \".\".","title":"python.library.token#token.DOT"},{"text":"re.S  \nre.DOTALL  \nMake the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).","title":"python.library.re#re.S"},{"text":"re.S  \nre.DOTALL  \nMake the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).","title":"python.library.re#re.DOTALL"},{"text":"numpy.unicode_[source]\n \nalias of numpy.str_","title":"numpy.reference.arrays.scalars#numpy.unicode_"},{"text":"encoding\n \nAlias for field number 3","title":"matplotlib.dviread#matplotlib.dviread.PsFont.encoding"},{"text":"curses.ascii.isprint(c)  \nChecks for any ASCII printable character including space.","title":"python.library.curses.ascii#curses.ascii.isprint"},{"text":"window.delch([y, x])  \nDelete any character at (y, x).","title":"python.library.curses#curses.window.delch"},{"text":"re.purge()  \nClear the regular expression cache.","title":"python.library.re#re.purge"},{"text":"token.SLASH  \nToken value for \"\/\".","title":"python.library.token#token.SLASH"},{"text":"class typing.Sized  \nAn alias to collections.abc.Sized","title":"python.library.typing#typing.Sized"}]}
{"task_id":38388799,"prompt":"def f_38388799(list_of_strings):\n\treturn ","suffix":"","canonical_solution":"sorted(list_of_strings, key=lambda s: s.split(',')[1])","test_start":"\ndef check(candidate):","test":["\n    assert candidate(['parrot, medicine', 'abott, kangaroo', 'sriracha, coriander', 'phone, bottle']) == ['phone, bottle', 'sriracha, coriander', 'abott, kangaroo', 'parrot, medicine']\n","\n    assert candidate(['abott, kangaroo', 'parrot, medicine', 'sriracha, coriander', 'phone, bottle']) == ['phone, bottle', 'sriracha, coriander', 'abott, kangaroo', 'parrot, medicine']\n"],"entry_point":"f_38388799","intent":"sort list `list_of_strings` based on second index of each string `s`","library":[],"docs":[]}
{"task_id":37004138,"prompt":"def f_37004138(lst):\n\treturn ","suffix":"","canonical_solution":"[element for element in lst if isinstance(element, int)]","test_start":"\ndef check(candidate):","test":["\n    lst = [1, \"hello\", \"string\", 2, 4.46]\n    assert candidate(lst) == [1, 2]\n","\n    lst = [\"hello\", \"string\"]\n    assert candidate(lst) == []\n"],"entry_point":"f_37004138","intent":"eliminate non-integer items from list `lst`","library":[],"docs":[]}
{"task_id":37004138,"prompt":"def f_37004138(lst):\n\treturn ","suffix":"","canonical_solution":"[element for element in lst if not isinstance(element, str)]","test_start":"\ndef check(candidate):","test":["\n    lst = [1, \"hello\", \"string\", 2, 4.46]\n    assert candidate(lst) == [1, 2, 4.46]\n","\n    lst = [\"hello\", \"string\"]\n    assert candidate(lst) == []\n"],"entry_point":"f_37004138","intent":"get all the elements except strings from the list 'lst'.","library":[],"docs":[]}
{"task_id":72899,"prompt":"def f_72899(list_to_be_sorted):\n\treturn ","suffix":"","canonical_solution":"sorted(list_to_be_sorted, key=lambda k: k['name'])","test_start":"\ndef check(candidate):","test":["\n    list_to_be_sorted = [{'name': 'Homer', 'age': 39}, {'name': 'Bart', 'age': 10}]\n    assert candidate(list_to_be_sorted) == [{'name': 'Bart', 'age': 10}, {'name': 'Homer', 'age': 39}]\n","\n    list_to_be_sorted = [{'name': 'ABCD'}, {'name': 'AABCD'}]\n    assert candidate(list_to_be_sorted) == [{'name': 'AABCD'}, {'name': 'ABCD'}]\n"],"entry_point":"f_72899","intent":"Sort a list of dictionaries `list_to_be_sorted` by the value of the dictionary key `name`","library":[],"docs":[]}
{"task_id":72899,"prompt":"def f_72899(l):\n\treturn ","suffix":"","canonical_solution":"sorted(l, key=itemgetter('name'), reverse=True)","test_start":"\nfrom operator import itemgetter\n\ndef check(candidate):","test":["\n    list_to_be_sorted = [{'name': 'Homer', 'age': 39}, {'name': 'Bart', 'age': 10}]\n    assert candidate(list_to_be_sorted) == [{'name': 'Homer', 'age': 39}, {'name': 'Bart', 'age': 10}]\n","\n    list_to_be_sorted = [{'name': 'ABCD'}, {'name': 'AABCD'}]\n    assert candidate(list_to_be_sorted) == [{'name': 'ABCD'}, {'name': 'AABCD'}]\n"],"entry_point":"f_72899","intent":"sort a list of dictionaries `l` by values in key `name` in descending order","library":["operator"],"docs":[{"text":"pandas.Index.sort   finalIndex.sort(*args, **kwargs)[source]\n \nUse sort_values instead.","title":"pandas.reference.api.pandas.index.sort"},{"text":"argsort(dim=-1, descending=False) \u2192 LongTensor  \nSee torch.argsort()","title":"torch.tensors#torch.Tensor.argsort"},{"text":"sort(dim=-1, descending=False) -> (Tensor, LongTensor)  \nSee torch.sort()","title":"torch.tensors#torch.Tensor.sort"},{"text":"pandas.Index.names   propertyIndex.names","title":"pandas.reference.api.pandas.index.names"},{"text":"pandas.tseries.offsets.Second.name   Second.name","title":"pandas.reference.api.pandas.tseries.offsets.second.name"},{"text":"order_by(*fields)","title":"django.ref.models.querysets#django.db.models.query.QuerySet.order_by"},{"text":"msort() \u2192 Tensor  \nSee torch.msort()","title":"torch.tensors#torch.Tensor.msort"},{"text":"pandas.tseries.offsets.Day.name   Day.name","title":"pandas.reference.api.pandas.tseries.offsets.day.name"},{"text":"exception KeyError  \nRaised when a mapping (dictionary) key is not found in the set of existing keys.","title":"python.library.exceptions#KeyError"},{"text":"pandas.tseries.offsets.BYearEnd.name   BYearEnd.name","title":"pandas.reference.api.pandas.tseries.offsets.byearend.name"}]}
{"task_id":72899,"prompt":"def f_72899(list_of_dicts):\n\t","suffix":"\n\treturn list_of_dicts","canonical_solution":"list_of_dicts.sort(key=operator.itemgetter('name'))","test_start":"\nimport operator\n\ndef check(candidate):","test":["\n    list_to_be_sorted = [{'name': 'Homer', 'age': 39}, {'name': 'Bart', 'age': 10}]\n    assert candidate(list_to_be_sorted) == [{'name': 'Bart', 'age': 10}, {'name': 'Homer', 'age': 39}]\n","\n    list_to_be_sorted = [{'name': 'ABCD'}, {'name': 'AABCD'}]\n    assert candidate(list_to_be_sorted) == [{'name': 'AABCD'}, {'name': 'ABCD'}]\n"],"entry_point":"f_72899","intent":"sort a list of dictionaries `list_of_dicts` by `name` values of the dictionary","library":["operator"],"docs":[{"text":"pandas.Index.sort   finalIndex.sort(*args, **kwargs)[source]\n \nUse sort_values instead.","title":"pandas.reference.api.pandas.index.sort"},{"text":"pandas.Index.names   propertyIndex.names","title":"pandas.reference.api.pandas.index.names"},{"text":"order_by(*fields)","title":"django.ref.models.querysets#django.db.models.query.QuerySet.order_by"},{"text":"test.support.sortdict(dict)  \nReturn a repr of dict with keys sorted.","title":"python.library.test#test.support.sortdict"},{"text":"pandas.tseries.offsets.Day.name   Day.name","title":"pandas.reference.api.pandas.tseries.offsets.day.name"},{"text":"dis.hasname  \nSequence of bytecodes that access an attribute by name.","title":"python.library.dis#dis.hasname"},{"text":"pandas.tseries.offsets.Second.name   Second.name","title":"pandas.reference.api.pandas.tseries.offsets.second.name"},{"text":"pandas.tseries.offsets.Micro.name   Micro.name","title":"pandas.reference.api.pandas.tseries.offsets.micro.name"},{"text":"msort() \u2192 Tensor  \nSee torch.msort()","title":"torch.tensors#torch.Tensor.msort"},{"text":"pandas.tseries.offsets.BQuarterEnd.name   BQuarterEnd.name","title":"pandas.reference.api.pandas.tseries.offsets.bquarterend.name"}]}
{"task_id":72899,"prompt":"def f_72899(list_of_dicts):\n\t","suffix":"\n\treturn list_of_dicts","canonical_solution":"list_of_dicts.sort(key=operator.itemgetter('age'))","test_start":"\nimport operator\n\ndef check(candidate):","test":["\n    list_to_be_sorted = [{'name': 'Homer', 'age': 39}, {'name': 'Bart', 'age': 10}]\n    assert candidate(list_to_be_sorted) == [{'name': 'Bart', 'age': 10}, {'name': 'Homer', 'age': 39}]\n","\n    list_to_be_sorted = [{'name': 'ABCD', 'age': 10}, {'name': 'AABCD', 'age': 9}]\n    assert candidate(list_to_be_sorted) == [{'name': 'AABCD', 'age': 9}, {'name': 'ABCD', 'age': 10}]\n"],"entry_point":"f_72899","intent":"sort a list of dictionaries `list_of_dicts` by `age` values of the dictionary","library":["operator"],"docs":[{"text":"pandas.Index.sort   finalIndex.sort(*args, **kwargs)[source]\n \nUse sort_values instead.","title":"pandas.reference.api.pandas.index.sort"},{"text":"order_by(*fields)","title":"django.ref.models.querysets#django.db.models.query.QuerySet.order_by"},{"text":"test.support.sortdict(dict)  \nReturn a repr of dict with keys sorted.","title":"python.library.test#test.support.sortdict"},{"text":"pandas.tseries.offsets.YearBegin.kwds   YearBegin.kwds","title":"pandas.reference.api.pandas.tseries.offsets.yearbegin.kwds"},{"text":"JSON_SORT_KEYS  \nSort the keys of JSON objects alphabetically. This is useful for caching because it ensures the data is serialized the same way no matter what Python\u2019s hash seed is. While not recommended, you can disable this for a possible performance improvement at the cost of caching. Default: True","title":"flask.config.index#JSON_SORT_KEYS"},{"text":"pandas.tseries.offsets.YearEnd.kwds   YearEnd.kwds","title":"pandas.reference.api.pandas.tseries.offsets.yearend.kwds"},{"text":"pandas.tseries.offsets.YearBegin.name   YearBegin.name","title":"pandas.reference.api.pandas.tseries.offsets.yearbegin.name"},{"text":"pandas.tseries.offsets.YearBegin.freqstr   YearBegin.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.yearbegin.freqstr"},{"text":"pandas.tseries.offsets.YearBegin.apply   YearBegin.apply()","title":"pandas.reference.api.pandas.tseries.offsets.yearbegin.apply"},{"text":"clear()  \nRemove all items from the dictionary.","title":"python.library.stdtypes#dict.clear"}]}
{"task_id":36402748,"prompt":"def f_36402748(df):\n\treturn ","suffix":"","canonical_solution":"df.groupby('prots').sum().sort_values('scores', ascending=False)","test_start":"\nimport pandas as pd \n\ndef check(candidate):","test":["\n    COLUMN_NAMES = [\"chemicals\", \"prots\", \"scores\"]\n    data = [[\"chemical1\", \"prot1\", 100],[\"chemical2\", \"prot2\", 50],[\"chemical3\", \"prot1\", 120]]\n    df = pd.DataFrame(data, columns = COLUMN_NAMES)\n    assert candidate(df).to_dict() == {'scores': {'prot1': 220, 'prot2': 50}}\n"],"entry_point":"f_36402748","intent":"sort a Dataframe `df` by the total ocurrences in a column 'scores' group by 'prots'","library":["pandas"],"docs":[{"text":"pandas.Index.sort   finalIndex.sort(*args, **kwargs)[source]\n \nUse sort_values instead.","title":"pandas.reference.api.pandas.index.sort"},{"text":"pandas.DataFrame.nlargest   DataFrame.nlargest(n, columns, keep='first')[source]\n \nReturn the first n rows ordered by columns in descending order. Return the first n rows with the largest values in columns, in descending order. The columns that are not specified are returned as well, but not used for ordering. This method is equivalent to df.sort_values(columns, ascending=False).head(n), but more performant.  Parameters \n \nn:int\n\n\nNumber of rows to return.  \ncolumns:label or list of labels\n\n\nColumn label(s) to order by.  \nkeep:{\u2018first\u2019, \u2018last\u2019, \u2018all\u2019}, default \u2018first\u2019\n\n\nWhere there are duplicate values:  first : prioritize the first occurrence(s) last : prioritize the last occurrence(s) all : do not drop any duplicates, even it means selecting more than n items.     Returns \n DataFrame\n\nThe first n rows ordered by the given columns in descending order.      See also  DataFrame.nsmallest\n\nReturn the first n rows ordered by columns in ascending order.  DataFrame.sort_values\n\nSort DataFrame by the values.  DataFrame.head\n\nReturn the first n rows without re-ordering.    Notes This function cannot be used with all column types. For example, when specifying columns with object or category dtypes, TypeError is raised. Examples \n>>> df = pd.DataFrame({'population': [59000000, 65000000, 434000,\n...                                   434000, 434000, 337000, 11300,\n...                                   11300, 11300],\n...                    'GDP': [1937894, 2583560 , 12011, 4520, 12128,\n...                            17036, 182, 38, 311],\n...                    'alpha-2': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\",\n...                                \"IS\", \"NR\", \"TV\", \"AI\"]},\n...                   index=[\"Italy\", \"France\", \"Malta\",\n...                          \"Maldives\", \"Brunei\", \"Iceland\",\n...                          \"Nauru\", \"Tuvalu\", \"Anguilla\"])\n>>> df\n          population      GDP alpha-2\nItaly       59000000  1937894      IT\nFrance      65000000  2583560      FR\nMalta         434000    12011      MT\nMaldives      434000     4520      MV\nBrunei        434000    12128      BN\nIceland       337000    17036      IS\nNauru          11300      182      NR\nTuvalu         11300       38      TV\nAnguilla       11300      311      AI\n  In the following example, we will use nlargest to select the three rows having the largest values in column \u201cpopulation\u201d. \n>>> df.nlargest(3, 'population')\n        population      GDP alpha-2\nFrance    65000000  2583560      FR\nItaly     59000000  1937894      IT\nMalta       434000    12011      MT\n  When using keep='last', ties are resolved in reverse order: \n>>> df.nlargest(3, 'population', keep='last')\n        population      GDP alpha-2\nFrance    65000000  2583560      FR\nItaly     59000000  1937894      IT\nBrunei      434000    12128      BN\n  When using keep='all', all duplicate items are maintained: \n>>> df.nlargest(3, 'population', keep='all')\n          population      GDP alpha-2\nFrance      65000000  2583560      FR\nItaly       59000000  1937894      IT\nMalta         434000    12011      MT\nMaldives      434000     4520      MV\nBrunei        434000    12128      BN\n  To order by the largest values in column \u201cpopulation\u201d and then \u201cGDP\u201d, we can specify multiple columns like in the next example. \n>>> df.nlargest(3, ['population', 'GDP'])\n        population      GDP alpha-2\nFrance    65000000  2583560      FR\nItaly     59000000  1937894      IT\nBrunei      434000    12128      BN","title":"pandas.reference.api.pandas.dataframe.nlargest"},{"text":"pandas.DataFrame.nsmallest   DataFrame.nsmallest(n, columns, keep='first')[source]\n \nReturn the first n rows ordered by columns in ascending order. Return the first n rows with the smallest values in columns, in ascending order. The columns that are not specified are returned as well, but not used for ordering. This method is equivalent to df.sort_values(columns, ascending=True).head(n), but more performant.  Parameters \n \nn:int\n\n\nNumber of items to retrieve.  \ncolumns:list or str\n\n\nColumn name or names to order by.  \nkeep:{\u2018first\u2019, \u2018last\u2019, \u2018all\u2019}, default \u2018first\u2019\n\n\nWhere there are duplicate values:  first : take the first occurrence. last : take the last occurrence. all : do not drop any duplicates, even it means selecting more than n items.     Returns \n DataFrame\n    See also  DataFrame.nlargest\n\nReturn the first n rows ordered by columns in descending order.  DataFrame.sort_values\n\nSort DataFrame by the values.  DataFrame.head\n\nReturn the first n rows without re-ordering.    Examples \n>>> df = pd.DataFrame({'population': [59000000, 65000000, 434000,\n...                                   434000, 434000, 337000, 337000,\n...                                   11300, 11300],\n...                    'GDP': [1937894, 2583560 , 12011, 4520, 12128,\n...                            17036, 182, 38, 311],\n...                    'alpha-2': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\",\n...                                \"IS\", \"NR\", \"TV\", \"AI\"]},\n...                   index=[\"Italy\", \"France\", \"Malta\",\n...                          \"Maldives\", \"Brunei\", \"Iceland\",\n...                          \"Nauru\", \"Tuvalu\", \"Anguilla\"])\n>>> df\n          population      GDP alpha-2\nItaly       59000000  1937894      IT\nFrance      65000000  2583560      FR\nMalta         434000    12011      MT\nMaldives      434000     4520      MV\nBrunei        434000    12128      BN\nIceland       337000    17036      IS\nNauru         337000      182      NR\nTuvalu         11300       38      TV\nAnguilla       11300      311      AI\n  In the following example, we will use nsmallest to select the three rows having the smallest values in column \u201cpopulation\u201d. \n>>> df.nsmallest(3, 'population')\n          population    GDP alpha-2\nTuvalu         11300     38      TV\nAnguilla       11300    311      AI\nIceland       337000  17036      IS\n  When using keep='last', ties are resolved in reverse order: \n>>> df.nsmallest(3, 'population', keep='last')\n          population  GDP alpha-2\nAnguilla       11300  311      AI\nTuvalu         11300   38      TV\nNauru         337000  182      NR\n  When using keep='all', all duplicate items are maintained: \n>>> df.nsmallest(3, 'population', keep='all')\n          population    GDP alpha-2\nTuvalu         11300     38      TV\nAnguilla       11300    311      AI\nIceland       337000  17036      IS\nNauru         337000    182      NR\n  To order by the smallest values in column \u201cpopulation\u201d and then \u201cGDP\u201d, we can specify multiple columns like in the next example. \n>>> df.nsmallest(3, ['population', 'GDP'])\n          population  GDP alpha-2\nTuvalu         11300   38      TV\nAnguilla       11300  311      AI\nNauru         337000  182      NR","title":"pandas.reference.api.pandas.dataframe.nsmallest"},{"text":"order_by(*fields)","title":"django.ref.models.querysets#django.db.models.query.QuerySet.order_by"},{"text":"pandas.DataFrame.sort_values   DataFrame.sort_values(by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last', ignore_index=False, key=None)[source]\n \nSort by the values along either axis.  Parameters \n \nby:str or list of str\n\n\nName or list of names to sort by.  if axis is 0 or \u2018index\u2019 then by may contain index levels and\/or column labels. if axis is 1 or \u2018columns\u2019 then by may contain column levels and\/or index labels.   \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019}, default 0\n\n\nAxis to be sorted.  \nascending:bool or list of bool, default True\n\n\nSort ascending vs. descending. Specify list for multiple sort orders. If this is a list of bools, must match the length of the by.  \ninplace:bool, default False\n\n\nIf True, perform operation in-place.  \nkind:{\u2018quicksort\u2019, \u2018mergesort\u2019, \u2018heapsort\u2019, \u2018stable\u2019}, default \u2018quicksort\u2019\n\n\nChoice of sorting algorithm. See also numpy.sort() for more information. mergesort and stable are the only stable algorithms. For DataFrames, this option is only applied when sorting on a single column or label.  \nna_position:{\u2018first\u2019, \u2018last\u2019}, default \u2018last\u2019\n\n\nPuts NaNs at the beginning if first; last puts NaNs at the end.  \nignore_index:bool, default False\n\n\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.  New in version 1.0.0.   \nkey:callable, optional\n\n\nApply the key function to the values before sorting. This is similar to the key argument in the builtin sorted() function, with the notable difference that this key function should be vectorized. It should expect a Series and return a Series with the same shape as the input. It will be applied to each column in by independently.  New in version 1.1.0.     Returns \n DataFrame or None\n\nDataFrame with sorted values or None if inplace=True.      See also  DataFrame.sort_index\n\nSort a DataFrame by the index.  Series.sort_values\n\nSimilar method for a Series.    Examples \n>>> df = pd.DataFrame({\n...     'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],\n...     'col2': [2, 1, 9, 8, 7, 4],\n...     'col3': [0, 1, 9, 4, 2, 3],\n...     'col4': ['a', 'B', 'c', 'D', 'e', 'F']\n... })\n>>> df\n  col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\n  Sort by col1 \n>>> df.sort_values(by=['col1'])\n  col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n5    C     4     3    F\n4    D     7     2    e\n3  NaN     8     4    D\n  Sort by multiple columns \n>>> df.sort_values(by=['col1', 'col2'])\n  col1  col2  col3 col4\n1    A     1     1    B\n0    A     2     0    a\n2    B     9     9    c\n5    C     4     3    F\n4    D     7     2    e\n3  NaN     8     4    D\n  Sort Descending \n>>> df.sort_values(by='col1', ascending=False)\n  col1  col2  col3 col4\n4    D     7     2    e\n5    C     4     3    F\n2    B     9     9    c\n0    A     2     0    a\n1    A     1     1    B\n3  NaN     8     4    D\n  Putting NAs first \n>>> df.sort_values(by='col1', ascending=False, na_position='first')\n  col1  col2  col3 col4\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\n2    B     9     9    c\n0    A     2     0    a\n1    A     1     1    B\n  Sorting with a key function \n>>> df.sort_values(by='col4', key=lambda col: col.str.lower())\n   col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\n  Natural sort with the key argument, using the natsort <https:\/\/github.com\/SethMMorton\/natsort> package. \n>>> df = pd.DataFrame({\n...    \"time\": ['0hr', '128hr', '72hr', '48hr', '96hr'],\n...    \"value\": [10, 20, 30, 40, 50]\n... })\n>>> df\n    time  value\n0    0hr     10\n1  128hr     20\n2   72hr     30\n3   48hr     40\n4   96hr     50\n>>> from natsort import index_natsorted\n>>> df.sort_values(\n...    by=\"time\",\n...    key=lambda x: np.argsort(index_natsorted(df[\"time\"]))\n... )\n    time  value\n0    0hr     10\n3   48hr     40\n2   72hr     30\n4   96hr     50\n1  128hr     20","title":"pandas.reference.api.pandas.dataframe.sort_values"},{"text":"pandas.tseries.offsets.SemiMonthEnd.apply   SemiMonthEnd.apply()","title":"pandas.reference.api.pandas.tseries.offsets.semimonthend.apply"},{"text":"pandas.Period.ordinal   Period.ordinal","title":"pandas.reference.api.pandas.period.ordinal"},{"text":"class Rank(*expressions, **extra)","title":"django.ref.models.database-functions#django.db.models.functions.Rank"},{"text":"pandas.tseries.offsets.Second.apply   Second.apply()","title":"pandas.reference.api.pandas.tseries.offsets.second.apply"},{"text":"pandas.tseries.offsets.Tick.apply   Tick.apply()","title":"pandas.reference.api.pandas.tseries.offsets.tick.apply"}]}
{"task_id":29881993,"prompt":"def f_29881993(trans):\n\treturn ","suffix":"","canonical_solution":"\"\"\",\"\"\".join(trans['category'])","test_start":"\ndef check(candidate):","test":["\n    trans = {'category':[\"hello\", \"world\",\"test\"], 'dummy_key':[\"dummy_val\"]}\n    assert candidate(trans) == \"hello,world,test\"\n"],"entry_point":"f_29881993","intent":"join together with \",\" elements inside a list indexed with 'category' within a dictionary `trans`","library":[],"docs":[]}
{"task_id":34158494,"prompt":"def f_34158494():\n\treturn ","suffix":"","canonical_solution":"\"\"\"\"\"\".join(['A', 'B', 'C', 'D'])","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == 'ABCD'\n"],"entry_point":"f_34158494","intent":"concatenate array of strings `['A', 'B', 'C', 'D']` into a string","library":[],"docs":[]}
{"task_id":12666897,"prompt":"def f_12666897(sents):\n\treturn ","suffix":"","canonical_solution":"[x for x in sents if not x.startswith('@$\\t') and not x.startswith('#')]","test_start":"\ndef check(candidate):","test":["\n    sents = [\"@$\tabcd\", \"#453923\", \"abcd\", \"hello\", \"1\"]\n    assert candidate(sents) == [\"abcd\", \"hello\", \"1\"]\n","\n    sents = [\"@$\tabcd\", \"@$t453923\", \"abcd\", \"hello\", \"1\"]\n    assert candidate(sents) == [\"@$t453923\", \"abcd\", \"hello\", \"1\"]\n","\n    sents = [\"#tabcd\", \"##453923\", \"abcd\", \"hello\", \"1\"]\n    assert candidate(sents) == [\"abcd\", \"hello\", \"1\"] \n"],"entry_point":"f_12666897","intent":"Remove all strings from a list a strings `sents` where the values starts with `@$\\t` or `#`","library":[],"docs":[]}
{"task_id":5944630,"prompt":"def f_5944630(list):\n\t","suffix":"\n\treturn list","canonical_solution":"list.sort(key=lambda item: (item['points'], item['time']))","test_start":"\ndef check(candidate):","test":["\n    list = [\n        {'name':'JOHN', 'points' : 30, 'time' : '0:02:2'},\n        {'name':'KARL','points':50,'time': '0:03:00'},\n        {'name':'TEST','points':20,'time': '0:03:00'}\n    ]\n    assert candidate(list) == [\n        {'name':'TEST','points':20,'time': '0:03:00'}, \n        {'name':'JOHN', 'points' : 30, 'time' : '0:02:2'},\n        {'name':'KARL','points':50,'time': '0:03:00'}\n    ]\n","\n    list = [\n        {'name':'JOHN', 'points' : 30, 'time' : '0:02:2'},\n        {'name':'KARL','points':30,'time': '0:03:00'},\n        {'name':'TEST','points':30,'time': '0:01:01'}\n    ]\n    assert candidate(list) == [\n        {'name':'TEST','points':30,'time': '0:01:01'},\n        {'name':'JOHN', 'points' : 30, 'time' : '0:02:2'},\n        {'name':'KARL','points':30,'time': '0:03:00'}\n    ]\n"],"entry_point":"f_5944630","intent":"sort a list of dictionary `list` first by key `points` and then by `time`","library":[],"docs":[]}
{"task_id":7852855,"prompt":"def f_7852855():\n\treturn ","suffix":"","canonical_solution":"datetime.datetime(1970, 1, 1).second","test_start":"\nimport time\nimport datetime\n\ndef check(candidate):","test":["\n    assert candidate() == 0\n"],"entry_point":"f_7852855","intent":"convert datetime object `(1970, 1, 1)` to seconds","library":["datetime","time"],"docs":[{"text":"time.second  \nIn range(60).","title":"python.library.datetime#datetime.time.second"},{"text":"datetime.second  \nIn range(60).","title":"python.library.datetime#datetime.datetime.second"},{"text":"pandas.Timestamp.second   Timestamp.second","title":"pandas.reference.api.pandas.timestamp.second"},{"text":"time.microsecond  \nIn range(1000000).","title":"python.library.datetime#datetime.time.microsecond"},{"text":"time.minute  \nIn range(60).","title":"python.library.datetime#datetime.time.minute"},{"text":"datetime.microsecond  \nIn range(1000000).","title":"python.library.datetime#datetime.datetime.microsecond"},{"text":"pandas.Timestamp.microsecond   Timestamp.microsecond","title":"pandas.reference.api.pandas.timestamp.microsecond"},{"text":"datetime.minute  \nIn range(60).","title":"python.library.datetime#datetime.datetime.minute"},{"text":"pandas.Timestamp.nanosecond   Timestamp.nanosecond","title":"pandas.reference.api.pandas.timestamp.nanosecond"},{"text":"pandas.Timestamp.freq   Timestamp.freq","title":"pandas.reference.api.pandas.timestamp.freq"}]}
{"task_id":2763750,"prompt":"def f_2763750():\n\treturn ","suffix":"","canonical_solution":"re.sub('(\\\\_a)?\\\\.([^\\\\.]*)$', '_suff.\\\\2', 'long.file.name.jpg')","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    assert candidate() == 'long.file.name_suff.jpg'\n"],"entry_point":"f_2763750","intent":"insert `_suff` before the file extension in `long.file.name.jpg` or replace `_a` with `suff` if it precedes the extension.","library":["re"],"docs":[{"text":"filename\n \nAlias for field number 4","title":"matplotlib.dviread#matplotlib.dviread.PsFont.filename"},{"text":"get_alternative_name(file_root, file_ext)","title":"django.howto.custom-file-storage#django.core.files.storage.get_alternative_name"},{"text":"test.support.TESTFN_NONASCII  \nSet to a filename containing the FS_NONASCII character.","title":"python.library.test#test.support.TESTFN_NONASCII"},{"text":"psname\n \nAlias for field number 1","title":"matplotlib.dviread#matplotlib.dviread.PsFont.psname"},{"text":"test.support.TESTFN_UNICODE  \nSet to a non-ASCII name for a temporary file.","title":"python.library.test#test.support.TESTFN_UNICODE"},{"text":"match  \nA regular expression pattern; only files with names matching this expression will be allowed as choices.","title":"django.ref.forms.fields#django.forms.FilePathField.match"},{"text":"stat.SF_APPEND  \nThe file may only be appended to.","title":"python.library.stat#stat.SF_APPEND"},{"text":"stat.S_IFREG  \nRegular file.","title":"python.library.stat#stat.S_IFREG"},{"text":"property filename","title":"skimage.api.skimage.io#skimage.io.MultiImage.filename"},{"text":"get_filename(fullname)  \nReturns path.  New in version 3.4.","title":"python.library.importlib#importlib.machinery.ExtensionFileLoader.get_filename"}]}
{"task_id":6420361,"prompt":"def f_6420361(module):\n\t","suffix":"\n\treturn ","canonical_solution":"imp.reload(module)","test_start":"\nimport imp\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    imp.reload = Mock()\n    try:\n        candidate('ads')\n        assert True\n    except:\n        assert False\n"],"entry_point":"f_6420361","intent":"reload a module `module`","library":["imp"],"docs":[{"text":"load_module(name=None)","title":"python.library.importlib#importlib.machinery.SourcelessFileLoader.load_module"},{"text":"TEMPLATES_AUTO_RELOAD  \nReload templates when they are changed. If not set, it will be enabled in debug mode. Default: None","title":"flask.config.index#TEMPLATES_AUTO_RELOAD"},{"text":"importlib.reload(module)  \nReload a previously imported module. The argument must be a module object, so it must have been successfully imported before. This is useful if you have edited the module source file using an external editor and want to try out the new version without leaving the Python interpreter. The return value is the module object (which can be different if re-importing causes a different object to be placed in sys.modules). When reload() is executed:  Python module\u2019s code is recompiled and the module-level code re-executed, defining a new set of objects which are bound to names in the module\u2019s dictionary by reusing the loader which originally loaded the module. The init function of extension modules is not called a second time. As with all other objects in Python the old objects are only reclaimed after their reference counts drop to zero. The names in the module namespace are updated to point to any new or changed objects. Other references to the old objects (such as names external to the module) are not rebound to refer to the new objects and must be updated in each namespace where they occur if that is desired.  There are a number of other caveats: When a module is reloaded, its dictionary (containing the module\u2019s global variables) is retained. Redefinitions of names will override the old definitions, so this is generally not a problem. If the new version of a module does not define a name that was defined by the old version, the old definition remains. This feature can be used to the module\u2019s advantage if it maintains a global table or cache of objects \u2014 with a try statement it can test for the table\u2019s presence and skip its initialization if desired: try:\n    cache\nexcept NameError:\n    cache = {}\n It is generally not very useful to reload built-in or dynamically loaded modules. Reloading sys, __main__, builtins and other key modules is not recommended. In many cases extension modules are not designed to be initialized more than once, and may fail in arbitrary ways when reloaded. If a module imports objects from another module using from \u2026 import \u2026, calling reload() for the other module does not redefine the objects imported from it \u2014 one way around this is to re-execute the from statement, another is to use import and qualified names (module.name) instead. If a module instantiates instances of a class, reloading the module that defines the class does not affect the method definitions of the instances \u2014 they continue to use the old class definition. The same is true for derived classes.  New in version 3.4.   Changed in version 3.7: ModuleNotFoundError is raised when the module being reloaded lacks a ModuleSpec.","title":"python.library.importlib#importlib.reload"},{"text":"loader_state","title":"python.library.importlib#importlib.machinery.ModuleSpec.loader_state"},{"text":"cached","title":"python.library.importlib#importlib.machinery.ModuleSpec.cached"},{"text":"loader","title":"python.library.importlib#importlib.machinery.ModuleSpec.loader"},{"text":"exception copy.Error  \nRaised for module specific errors.","title":"python.library.copy#copy.Error"},{"text":"submodule_search_locations","title":"python.library.importlib#importlib.machinery.ModuleSpec.submodule_search_locations"},{"text":"parent","title":"python.library.importlib#importlib.machinery.ModuleSpec.parent"},{"text":"matplotlib.style.reload_library()[source]\n \nReload the style library.","title":"matplotlib.style_api#matplotlib.style.reload_library"}]}
{"task_id":19546911,"prompt":"def f_19546911(number):\n\treturn ","suffix":"","canonical_solution":"struct.unpack('H', struct.pack('h', number))","test_start":"\nimport struct \n\ndef check(candidate):","test":["\n    assert candidate(3) == (3,)\n"],"entry_point":"f_19546911","intent":"Convert integer `number` into an unassigned integer","library":["struct"],"docs":[{"text":"radix()  \nJust returns 10, as this is Decimal, :)","title":"python.library.decimal#decimal.Context.radix"},{"text":"token.NUMBER","title":"python.library.token#token.NUMBER"},{"text":"arg  \nnumeric argument to operation (if any), otherwise None","title":"python.library.dis#dis.Instruction.arg"},{"text":"Record.GetInteger(field)  \nReturn the value of field as an integer where possible. field must be an integer.","title":"python.library.msilib#msilib.Record.GetInteger"},{"text":"int()  \nCasts this storage to int type","title":"torch.storage#torch.FloatStorage.int"},{"text":"numerator  \nAbstract.","title":"python.library.numbers#numbers.Rational.numerator"},{"text":"step  \nThe value of the step parameter (or 1 if the parameter was not supplied)","title":"python.library.stdtypes#range.step"},{"text":"as_int()","title":"django.ref.contrib.gis.gdal#django.contrib.gis.gdal.Field.as_int"},{"text":"to_integral_exact(x)  \nRounds to an integer.","title":"python.library.decimal#decimal.Context.to_integral_exact"},{"text":"string.digits  \nThe string '0123456789'.","title":"python.library.string#string.digits"}]}
{"task_id":9746522,"prompt":"def f_9746522(numlist):\n\t","suffix":"\n\treturn numlist","canonical_solution":"numlist = [float(x) for x in numlist]","test_start":"\ndef check(candidate):","test":["\n    assert candidate([3, 4]) == [3.0, 4.0]\n"],"entry_point":"f_9746522","intent":"convert int values in list `numlist` to float","library":[],"docs":[]}
{"task_id":20107570,"prompt":"def f_20107570(df, filename):\n\t","suffix":"\n\treturn ","canonical_solution":"df.to_csv(filename, index=False)","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    file_name = 'a.csv'\n    df = pd.DataFrame([1, 2, 3], columns = ['Vals'])\n    candidate(df, file_name)\n    with open (file_name, 'r') as f:\n        lines = f.readlines()\n    assert len(lines) == 4\n"],"entry_point":"f_20107570","intent":"write dataframe `df`, excluding index, to a csv file `filename`","library":["pandas"],"docs":[{"text":"pandas.DataFrame.to_csv   DataFrame.to_csv(path_or_buf=None, sep=',', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, mode='w', encoding=None, compression='infer', quoting=None, quotechar='\"', line_terminator=None, chunksize=None, date_format=None, doublequote=True, escapechar=None, decimal='.', errors='strict', storage_options=None)[source]\n \nWrite object to a comma-separated values (csv) file.  Parameters \n \npath_or_buf:str, path object, file-like object, or None, default None\n\n\nString, path object (implementing os.PathLike[str]), or file-like object implementing a write() function. If None, the result is returned as a string. If a non-binary file object is passed, it should be opened with newline=\u2019\u2019, disabling universal newlines. If a binary file object is passed, mode might need to contain a \u2018b\u2019.  Changed in version 1.2.0: Support for binary file objects was introduced.   \nsep:str, default \u2018,\u2019\n\n\nString of length 1. Field delimiter for the output file.  \nna_rep:str, default \u2018\u2019\n\n\nMissing data representation.  \nfloat_format:str, default None\n\n\nFormat string for floating point numbers.  \ncolumns:sequence, optional\n\n\nColumns to write.  \nheader:bool or list of str, default True\n\n\nWrite out the column names. If a list of strings is given it is assumed to be aliases for the column names.  \nindex:bool, default True\n\n\nWrite row names (index).  \nindex_label:str or sequence, or False, default None\n\n\nColumn label for index column(s) if desired. If None is given, and header and index are True, then the index names are used. A sequence should be given if the object uses MultiIndex. If False do not print fields for index names. Use index_label=False for easier importing in R.  \nmode:str\n\n\nPython write mode, default \u2018w\u2019.  \nencoding:str, optional\n\n\nA string representing the encoding to use in the output file, defaults to \u2018utf-8\u2019. encoding is not supported if path_or_buf is a non-binary file object.  \ncompression:str or dict, default \u2018infer\u2019\n\n\nFor on-the-fly compression of the output data. If \u2018infer\u2019 and \u2018%s\u2019 path-like, then detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, or zstandard.ZstdDecompressor, respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.  Changed in version 1.0.0: May now be a dict with key \u2018method\u2019 as compression mode and other entries as additional compression options if compression mode is \u2018zip\u2019.   Changed in version 1.1.0: Passing compression options as keys in dict is supported for compression modes \u2018gzip\u2019, \u2018bz2\u2019, \u2018zstd\u2019, and \u2018zip\u2019.   Changed in version 1.2.0: Compression is supported for binary file objects.   Changed in version 1.2.0: Previous versions forwarded dict entries for \u2018gzip\u2019 to gzip.open instead of gzip.GzipFile which prevented setting mtime.   \nquoting:optional constant from csv module\n\n\nDefaults to csv.QUOTE_MINIMAL. If you have set a float_format then floats are converted to strings and thus csv.QUOTE_NONNUMERIC will treat them as non-numeric.  \nquotechar:str, default \u2018\"\u2019\n\n\nString of length 1. Character used to quote fields.  \nline_terminator:str, optional\n\n\nThe newline character or character sequence to use in the output file. Defaults to os.linesep, which depends on the OS in which this method is called (\u2019\\n\u2019 for linux, \u2018\\r\\n\u2019 for Windows, i.e.).  \nchunksize:int or None\n\n\nRows to write at a time.  \ndate_format:str, default None\n\n\nFormat string for datetime objects.  \ndoublequote:bool, default True\n\n\nControl quoting of quotechar inside a field.  \nescapechar:str, default None\n\n\nString of length 1. Character used to escape sep and quotechar when appropriate.  \ndecimal:str, default \u2018.\u2019\n\n\nCharacter recognized as decimal separator. E.g. use \u2018,\u2019 for European data.  \nerrors:str, default \u2018strict\u2019\n\n\nSpecifies how encoding and decoding errors are to be handled. See the errors argument for open() for a full list of options.  New in version 1.1.0.   \nstorage_options:dict, optional\n\n\nExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3:\/\/\u201d, and \u201cgcs:\/\/\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.  New in version 1.2.0.     Returns \n None or str\n\nIf path_or_buf is None, returns the resulting csv format as a string. Otherwise returns None.      See also  read_csv\n\nLoad a CSV file into a DataFrame.  to_excel\n\nWrite DataFrame to an Excel file.    Examples \n>>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'],\n...                    'mask': ['red', 'purple'],\n...                    'weapon': ['sai', 'bo staff']})\n>>> df.to_csv(index=False)\n'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n'\n  Create \u2018out.zip\u2019 containing \u2018out.csv\u2019 \n>>> compression_opts = dict(method='zip',\n...                         archive_name='out.csv')  \n>>> df.to_csv('out.zip', index=False,\n...           compression=compression_opts)  \n  To write a csv file to a new folder or nested folder you will first need to create it using either Pathlib or os: \n>>> from pathlib import Path  \n>>> filepath = Path('folder\/subfolder\/out.csv')  \n>>> filepath.parent.mkdir(parents=True, exist_ok=True)  \n>>> df.to_csv(filepath)  \n  \n>>> import os  \n>>> os.makedirs('folder\/subfolder', exist_ok=True)  \n>>> df.to_csv('folder\/subfolder\/out.csv')","title":"pandas.reference.api.pandas.dataframe.to_csv"},{"text":"filename\n \nAlias for field number 4","title":"matplotlib.dviread#matplotlib.dviread.PsFont.filename"},{"text":"stat.UF_NODUMP  \nDo not dump the file.","title":"python.library.stat#stat.UF_NODUMP"},{"text":"pandas.Index.names   propertyIndex.names","title":"pandas.reference.api.pandas.index.names"},{"text":"psname\n \nAlias for field number 1","title":"matplotlib.dviread#matplotlib.dviread.PsFont.psname"},{"text":"texname\n \nAlias for field number 0","title":"matplotlib.dviread#matplotlib.dviread.PsFont.texname"},{"text":"pandas.DataFrame.to_excel   DataFrame.to_excel(excel_writer, sheet_name='Sheet1', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, startrow=0, startcol=0, engine=None, merge_cells=True, encoding=None, inf_rep='inf', verbose=True, freeze_panes=None, storage_options=None)[source]\n \nWrite object to an Excel sheet. To write a single object to an Excel .xlsx file it is only necessary to specify a target file name. To write to multiple sheets it is necessary to create an ExcelWriter object with a target file name, and specify a sheet in the file to write to. Multiple sheets may be written to by specifying unique sheet_name. With all data written to the file it is necessary to save the changes. Note that creating an ExcelWriter object with a file name that already exists will result in the contents of the existing file being erased.  Parameters \n \nexcel_writer:path-like, file-like, or ExcelWriter object\n\n\nFile path or existing ExcelWriter.  \nsheet_name:str, default \u2018Sheet1\u2019\n\n\nName of sheet which will contain DataFrame.  \nna_rep:str, default \u2018\u2019\n\n\nMissing data representation.  \nfloat_format:str, optional\n\n\nFormat string for floating point numbers. For example float_format=\"%.2f\" will format 0.1234 to 0.12.  \ncolumns:sequence or list of str, optional\n\n\nColumns to write.  \nheader:bool or list of str, default True\n\n\nWrite out the column names. If a list of string is given it is assumed to be aliases for the column names.  \nindex:bool, default True\n\n\nWrite row names (index).  \nindex_label:str or sequence, optional\n\n\nColumn label for index column(s) if desired. If not specified, and header and index are True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex.  \nstartrow:int, default 0\n\n\nUpper left cell row to dump data frame.  \nstartcol:int, default 0\n\n\nUpper left cell column to dump data frame.  \nengine:str, optional\n\n\nWrite engine to use, \u2018openpyxl\u2019 or \u2018xlsxwriter\u2019. You can also set this via the options io.excel.xlsx.writer, io.excel.xls.writer, and io.excel.xlsm.writer.  Deprecated since version 1.2.0: As the xlwt package is no longer maintained, the xlwt engine will be removed in a future version of pandas.   \nmerge_cells:bool, default True\n\n\nWrite MultiIndex and Hierarchical Rows as merged cells.  \nencoding:str, optional\n\n\nEncoding of the resulting excel file. Only necessary for xlwt, other writers support unicode natively.  \ninf_rep:str, default \u2018inf\u2019\n\n\nRepresentation for infinity (there is no native representation for infinity in Excel).  \nverbose:bool, default True\n\n\nDisplay more information in the error logs.  \nfreeze_panes:tuple of int (length 2), optional\n\n\nSpecifies the one-based bottommost row and rightmost column that is to be frozen.  \nstorage_options:dict, optional\n\n\nExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3:\/\/\u201d, and \u201cgcs:\/\/\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.  New in version 1.2.0.       See also  to_csv\n\nWrite DataFrame to a comma-separated values (csv) file.  ExcelWriter\n\nClass for writing DataFrame objects into excel sheets.  read_excel\n\nRead an Excel file into a pandas DataFrame.  read_csv\n\nRead a comma-separated values (csv) file into DataFrame.    Notes For compatibility with to_csv(), to_excel serializes lists and dicts to strings before writing. Once a workbook has been saved it is not possible to write further data without rewriting the whole workbook. Examples Create, write to and save a workbook: \n>>> df1 = pd.DataFrame([['a', 'b'], ['c', 'd']],\n...                    index=['row 1', 'row 2'],\n...                    columns=['col 1', 'col 2'])\n>>> df1.to_excel(\"output.xlsx\")  \n  To specify the sheet name: \n>>> df1.to_excel(\"output.xlsx\",\n...              sheet_name='Sheet_name_1')  \n  If you wish to write to more than one sheet in the workbook, it is necessary to specify an ExcelWriter object: \n>>> df2 = df1.copy()\n>>> with pd.ExcelWriter('output.xlsx') as writer:  \n...     df1.to_excel(writer, sheet_name='Sheet_name_1')\n...     df2.to_excel(writer, sheet_name='Sheet_name_2')\n  ExcelWriter can also be used to append to an existing Excel file: \n>>> with pd.ExcelWriter('output.xlsx',\n...                     mode='a') as writer:  \n...     df.to_excel(writer, sheet_name='Sheet_name_3')\n  To set the library that is used to write the Excel file, you can pass the engine keyword (the default engine is automatically chosen depending on the file extension): \n>>> df1.to_excel('output1.xlsx', engine='xlsxwriter')","title":"pandas.reference.api.pandas.dataframe.to_excel"},{"text":"pandas.Series.to_csv   Series.to_csv(path_or_buf=None, sep=',', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, mode='w', encoding=None, compression='infer', quoting=None, quotechar='\"', line_terminator=None, chunksize=None, date_format=None, doublequote=True, escapechar=None, decimal='.', errors='strict', storage_options=None)[source]\n \nWrite object to a comma-separated values (csv) file.  Parameters \n \npath_or_buf:str, path object, file-like object, or None, default None\n\n\nString, path object (implementing os.PathLike[str]), or file-like object implementing a write() function. If None, the result is returned as a string. If a non-binary file object is passed, it should be opened with newline=\u2019\u2019, disabling universal newlines. If a binary file object is passed, mode might need to contain a \u2018b\u2019.  Changed in version 1.2.0: Support for binary file objects was introduced.   \nsep:str, default \u2018,\u2019\n\n\nString of length 1. Field delimiter for the output file.  \nna_rep:str, default \u2018\u2019\n\n\nMissing data representation.  \nfloat_format:str, default None\n\n\nFormat string for floating point numbers.  \ncolumns:sequence, optional\n\n\nColumns to write.  \nheader:bool or list of str, default True\n\n\nWrite out the column names. If a list of strings is given it is assumed to be aliases for the column names.  \nindex:bool, default True\n\n\nWrite row names (index).  \nindex_label:str or sequence, or False, default None\n\n\nColumn label for index column(s) if desired. If None is given, and header and index are True, then the index names are used. A sequence should be given if the object uses MultiIndex. If False do not print fields for index names. Use index_label=False for easier importing in R.  \nmode:str\n\n\nPython write mode, default \u2018w\u2019.  \nencoding:str, optional\n\n\nA string representing the encoding to use in the output file, defaults to \u2018utf-8\u2019. encoding is not supported if path_or_buf is a non-binary file object.  \ncompression:str or dict, default \u2018infer\u2019\n\n\nFor on-the-fly compression of the output data. If \u2018infer\u2019 and \u2018%s\u2019 path-like, then detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, or zstandard.ZstdDecompressor, respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.  Changed in version 1.0.0: May now be a dict with key \u2018method\u2019 as compression mode and other entries as additional compression options if compression mode is \u2018zip\u2019.   Changed in version 1.1.0: Passing compression options as keys in dict is supported for compression modes \u2018gzip\u2019, \u2018bz2\u2019, \u2018zstd\u2019, and \u2018zip\u2019.   Changed in version 1.2.0: Compression is supported for binary file objects.   Changed in version 1.2.0: Previous versions forwarded dict entries for \u2018gzip\u2019 to gzip.open instead of gzip.GzipFile which prevented setting mtime.   \nquoting:optional constant from csv module\n\n\nDefaults to csv.QUOTE_MINIMAL. If you have set a float_format then floats are converted to strings and thus csv.QUOTE_NONNUMERIC will treat them as non-numeric.  \nquotechar:str, default \u2018\"\u2019\n\n\nString of length 1. Character used to quote fields.  \nline_terminator:str, optional\n\n\nThe newline character or character sequence to use in the output file. Defaults to os.linesep, which depends on the OS in which this method is called (\u2019\\n\u2019 for linux, \u2018\\r\\n\u2019 for Windows, i.e.).  \nchunksize:int or None\n\n\nRows to write at a time.  \ndate_format:str, default None\n\n\nFormat string for datetime objects.  \ndoublequote:bool, default True\n\n\nControl quoting of quotechar inside a field.  \nescapechar:str, default None\n\n\nString of length 1. Character used to escape sep and quotechar when appropriate.  \ndecimal:str, default \u2018.\u2019\n\n\nCharacter recognized as decimal separator. E.g. use \u2018,\u2019 for European data.  \nerrors:str, default \u2018strict\u2019\n\n\nSpecifies how encoding and decoding errors are to be handled. See the errors argument for open() for a full list of options.  New in version 1.1.0.   \nstorage_options:dict, optional\n\n\nExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3:\/\/\u201d, and \u201cgcs:\/\/\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.  New in version 1.2.0.     Returns \n None or str\n\nIf path_or_buf is None, returns the resulting csv format as a string. Otherwise returns None.      See also  read_csv\n\nLoad a CSV file into a DataFrame.  to_excel\n\nWrite DataFrame to an Excel file.    Examples \n>>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'],\n...                    'mask': ['red', 'purple'],\n...                    'weapon': ['sai', 'bo staff']})\n>>> df.to_csv(index=False)\n'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n'\n  Create \u2018out.zip\u2019 containing \u2018out.csv\u2019 \n>>> compression_opts = dict(method='zip',\n...                         archive_name='out.csv')  \n>>> df.to_csv('out.zip', index=False,\n...           compression=compression_opts)  \n  To write a csv file to a new folder or nested folder you will first need to create it using either Pathlib or os: \n>>> from pathlib import Path  \n>>> filepath = Path('folder\/subfolder\/out.csv')  \n>>> filepath.parent.mkdir(parents=True, exist_ok=True)  \n>>> df.to_csv(filepath)  \n  \n>>> import os  \n>>> os.makedirs('folder\/subfolder', exist_ok=True)  \n>>> df.to_csv('folder\/subfolder\/out.csv')","title":"pandas.reference.api.pandas.series.to_csv"},{"text":"pandas.Index.view   Index.view(cls=None)[source]","title":"pandas.reference.api.pandas.index.view"},{"text":"pandas.DataFrame.to_string   DataFrame.to_string(buf=None, columns=None, col_space=None, header=True, index=True, na_rep='NaN', formatters=None, float_format=None, sparsify=None, index_names=True, justify=None, max_rows=None, max_cols=None, show_dimensions=False, decimal='.', line_width=None, min_rows=None, max_colwidth=None, encoding=None)[source]\n \nRender a DataFrame to a console-friendly tabular output.  Parameters \n \nbuf:str, Path or StringIO-like, optional, default None\n\n\nBuffer to write to. If None, the output is returned as a string.  \ncolumns:sequence, optional, default None\n\n\nThe subset of columns to write. Writes all columns by default.  \ncol_space:int, list or dict of int, optional\n\n\nThe minimum width of each column. If a list of ints is given every integers corresponds with one column. If a dict is given, the key references the column, while the value defines the space to use..  \nheader:bool or sequence of str, optional\n\n\nWrite out the column names. If a list of strings is given, it is assumed to be aliases for the column names.  \nindex:bool, optional, default True\n\n\nWhether to print index (row) labels.  \nna_rep:str, optional, default \u2018NaN\u2019\n\n\nString representation of NaN to use.  \nformatters:list, tuple or dict of one-param. functions, optional\n\n\nFormatter functions to apply to columns\u2019 elements by position or name. The result of each function must be a unicode string. List\/tuple must be of length equal to the number of columns.  \nfloat_format:one-parameter function, optional, default None\n\n\nFormatter function to apply to columns\u2019 elements if they are floats. This function must return a unicode string and will be applied only to the non-NaN elements, with NaN being handled by na_rep.  Changed in version 1.2.0.   \nsparsify:bool, optional, default True\n\n\nSet to False for a DataFrame with a hierarchical index to print every multiindex key at each row.  \nindex_names:bool, optional, default True\n\n\nPrints the names of the indexes.  \njustify:str, default None\n\n\nHow to justify the column labels. If None uses the option from the print configuration (controlled by set_option), \u2018right\u2019 out of the box. Valid values are  left right center justify justify-all start end inherit match-parent initial unset.   \nmax_rows:int, optional\n\n\nMaximum number of rows to display in the console.  \nmax_cols:int, optional\n\n\nMaximum number of columns to display in the console.  \nshow_dimensions:bool, default False\n\n\nDisplay DataFrame dimensions (number of rows by number of columns).  \ndecimal:str, default \u2018.\u2019\n\n\nCharacter recognized as decimal separator, e.g. \u2018,\u2019 in Europe.  \nline_width:int, optional\n\n\nWidth to wrap a line in characters.  \nmin_rows:int, optional\n\n\nThe number of rows to display in the console in a truncated repr (when number of rows is above max_rows).  \nmax_colwidth:int, optional\n\n\nMax width to truncate each column in characters. By default, no limit.  New in version 1.0.0.   \nencoding:str, default \u201cutf-8\u201d\n\n\nSet character encoding.  New in version 1.0.     Returns \n str or None\n\nIf buf is None, returns the result as a string. Otherwise returns None.      See also  to_html\n\nConvert DataFrame to HTML.    Examples \n>>> d = {'col1': [1, 2, 3], 'col2': [4, 5, 6]}\n>>> df = pd.DataFrame(d)\n>>> print(df.to_string())\n   col1  col2\n0     1     4\n1     2     5\n2     3     6","title":"pandas.reference.api.pandas.dataframe.to_string"}]}
{"task_id":8740353,"prompt":"def f_8740353(unescaped):\n\t","suffix":"\n\treturn json_data","canonical_solution":"json_data = json.loads(unescaped)","test_start":"\nimport json \n\ndef check(candidate):","test":["\n    x = \"\"\"{\n    \"Name\": \"Jennifer Smith\",\n    \"Contact Number\": 7867567898,\n    \"Email\": \"jen123@gmail.com\",\n    \"Hobbies\":[\"Reading\", \"Sketching\", \"Horse Riding\"]\n    }\"\"\"\n    assert candidate(x) == {'Hobbies': ['Reading', 'Sketching', 'Horse Riding'], 'Name': 'Jennifer Smith', 'Email': 'jen123@gmail.com', 'Contact Number': 7867567898}\n"],"entry_point":"f_8740353","intent":"convert a urllib unquoted string `unescaped` to a json data `json_data`","library":["json"],"docs":[{"text":"urllib.parse.unquote_to_bytes(string)  \nReplace %xx escapes with their single-octet equivalent, and return a bytes object. string may be either a str or a bytes object. If it is a str, unescaped non-ASCII characters in string are encoded into UTF-8 bytes. Example: unquote_to_bytes('a%26%EF') yields b'a&\\xef'.","title":"python.library.urllib.parse#urllib.parse.unquote_to_bytes"},{"text":"msg  \nThe unformatted error message.","title":"python.library.json#json.JSONDecodeError.msg"},{"text":"unescape()  \nConvert escaped markup back into a text string. This replaces HTML entities with the characters they represent. >>> Markup(\"Main &raquo; <em>About<\/em>\").unescape()\n'Main \u00bb <em>About<\/em>'\n  Return type \nstr","title":"flask.api.index#flask.Markup.unescape"},{"text":"urllib.parse.unquote(string, encoding='utf-8', errors='replace')  \nReplace %xx escapes with their single-character equivalent. The optional encoding and errors parameters specify how to decode percent-encoded sequences into Unicode characters, as accepted by the bytes.decode() method. string may be either a str or a bytes object. encoding defaults to 'utf-8'. errors defaults to 'replace', meaning invalid sequences are replaced by a placeholder character. Example: unquote('\/El%20Ni%C3%B1o\/') yields '\/El Ni\u00f1o\/'.  Changed in version 3.9: string parameter supports bytes and str objects (previously only str).","title":"python.library.urllib.parse#urllib.parse.unquote"},{"text":"Form.errors.get_json_data(escape_html=False)","title":"django.ref.forms.api#django.forms.Form.errors.get_json_data"},{"text":"open_unknown(fullurl, data=None)  \nOverridable interface to open unknown URL types.","title":"python.library.urllib.request#urllib.request.URLopener.open_unknown"},{"text":"doc  \nThe JSON document being parsed.","title":"python.library.json#json.JSONDecodeError.doc"},{"text":"email.utils.unquote(str)  \nReturn a new string which is an unquoted version of str. If str ends and begins with double quotes, they are stripped off. Likewise if str ends and begins with angle brackets, they are stripped off.","title":"python.library.email.utils#email.utils.unquote"},{"text":"Form.errors.as_json(escape_html=False)","title":"django.ref.forms.api#django.forms.Form.errors.as_json"},{"text":"msg  \nThe unformatted error message.","title":"python.library.re#re.error.msg"}]}
{"task_id":5891453,"prompt":"def f_5891453():\n\treturn ","suffix":"","canonical_solution":"[chr(i) for i in range(127)]","test_start":"\ndef check(candidate):","test":["\n    chars = candidate()\n    assert len(chars) == 127\n    assert chars == [chr(i) for i in range(127)]\n"],"entry_point":"f_5891453","intent":"Create a list containing all ascii characters as its elements","library":[],"docs":[]}
{"task_id":18367007,"prompt":"def f_18367007(newFileBytes, newFile):\n\t","suffix":"\n\treturn ","canonical_solution":"newFile.write(struct.pack('5B', *newFileBytes))","test_start":"\nimport struct \n\ndef check(candidate):","test":["\n    newFileBytes = [123, 3, 123, 100, 99]\n    file_name = 'f.txt'\n    newFile = open(file_name, 'wb')\n    candidate(newFileBytes, newFile)\n    newFile.close()\n    with open (file_name, 'rb') as f:\n        lines = f.readlines()\n        assert lines == [b'{\u0003{dc']\n"],"entry_point":"f_18367007","intent":"write `newFileBytes` to a binary file `newFile`","library":["struct"],"docs":[{"text":"winreg.REG_BINARY  \nBinary data in any form.","title":"python.library.winreg#winreg.REG_BINARY"},{"text":"array.tofile(f)  \nWrite all items (as machine values) to the file object f.","title":"python.library.array#array.array.tofile"},{"text":"Path.write_bytes(data)  \nOpen the file pointed to in bytes mode, write data to it, and close the file: >>> p = Path('my_binary_file')\n>>> p.write_bytes(b'Binary file contents')\n20\n>>> p.read_bytes()\nb'Binary file contents'\n An existing file of the same name is overwritten.  New in version 3.5.","title":"python.library.pathlib#pathlib.Path.write_bytes"},{"text":"stat.UF_NODUMP  \nDo not dump the file.","title":"python.library.stat#stat.UF_NODUMP"},{"text":"byte()  \nCasts this storage to byte type","title":"torch.storage#torch.FloatStorage.byte"},{"text":"numpy.string_[source]\n \nalias of numpy.bytes_","title":"numpy.reference.arrays.scalars#numpy.string_"},{"text":"stat.UF_APPEND  \nThe file may only be appended to.","title":"python.library.stat#stat.UF_APPEND"},{"text":"byteorder","title":"django.ref.contrib.gis.geos#django.contrib.gis.geos.WKBWriter.byteorder"},{"text":"Path.read_bytes()  \nRead the current file as bytes.","title":"python.library.zipfile#zipfile.Path.read_bytes"},{"text":"ZipInfo.reserved  \nMust be zero.","title":"python.library.zipfile#zipfile.ZipInfo.reserved"}]}
{"task_id":21805490,"prompt":"def f_21805490(string):\n\treturn ","suffix":"","canonical_solution":"re.sub('^[A-Z0-9]*(?![a-z])', '', string)","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    assert candidate(\"AASKH317298DIUANFProgramming is fun\") == \"Programming is fun\"\n"],"entry_point":"f_21805490","intent":"python regex - check for a capital letter with a following lowercase in string `string`","library":["re"],"docs":[{"text":"str.islower()  \nReturn True if all cased characters 4 in the string are lowercase and there is at least one cased character, False otherwise.","title":"python.library.stdtypes#str.islower"},{"text":"string.ascii_lowercase  \nThe lowercase letters 'abcdefghijklmnopqrstuvwxyz'. This value is not locale-dependent and will not change.","title":"python.library.string#string.ascii_lowercase"},{"text":"str.isupper()  \nReturn True if all cased characters 4 in the string are uppercase and there is at least one cased character, False otherwise. >>> 'BANANA'.isupper()\nTrue\n>>> 'banana'.isupper()\nFalse\n>>> 'baNana'.isupper()\nFalse\n>>> ' '.isupper()\nFalse","title":"python.library.stdtypes#str.isupper"},{"text":"str.istitle()  \nReturn True if the string is a titlecased string and there is at least one character, for example uppercase characters may only follow uncased characters and lowercase characters only cased ones. Return False otherwise.","title":"python.library.stdtypes#str.istitle"},{"text":"pattern  \nThe regular expression pattern.","title":"python.library.re#re.error.pattern"},{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"curses.ascii.islower(c)  \nChecks for an ASCII lower-case character.","title":"python.library.curses.ascii#curses.ascii.islower"},{"text":"string.ascii_uppercase  \nThe uppercase letters 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'. This value is not locale-dependent and will not change.","title":"python.library.string#string.ascii_uppercase"},{"text":"re.search(pattern, string, flags=0)  \nScan through string looking for the first location where the regular expression pattern produces a match, and return a corresponding match object. Return None if no position in the string matches the pattern; note that this is different from finding a zero-length match at some point in the string.","title":"python.library.re#re.search"},{"text":"Pattern.match(string[, pos[, endpos]])  \nIf zero or more characters at the beginning of string match this regular expression, return a corresponding match object. Return None if the string does not match the pattern; note that this is different from a zero-length match. The optional pos and endpos parameters have the same meaning as for the search() method. >>> pattern = re.compile(\"o\")\n>>> pattern.match(\"dog\")      # No match as \"o\" is not at the start of \"dog\".\n>>> pattern.match(\"dog\", 1)   # Match as \"o\" is the 2nd character of \"dog\".\n<re.Match object; span=(1, 2), match='o'>\n If you want to locate a match anywhere in string, use search() instead (see also search() vs. match()).","title":"python.library.re#re.Pattern.match"}]}
{"task_id":16125229,"prompt":"def f_16125229(dict):\n\treturn ","suffix":"","canonical_solution":"list(dict.keys())[-1]","test_start":"\ndef check(candidate):","test":["\n    assert candidate({'t': 1, 'r': 2}) == 'r'\n","\n    assert candidate({'c': 1, 'b': 2, 'a': 1}) == 'a'\n"],"entry_point":"f_16125229","intent":"get the last key of dictionary `dict`","library":[],"docs":[]}
{"task_id":6159900,"prompt":"def f_6159900(f):\n\treturn ","suffix":"","canonical_solution":"print('hi there', file=f)","test_start":"\ndef check(candidate):","test":["\n    file_name = 'a.txt'\n    f = open(file_name, 'w')\n    candidate(f)\n    f.close()\n    with open (file_name, 'r') as f:\n        lines = f.readlines()\n        assert lines[0] == 'hi there\\n'\n"],"entry_point":"f_6159900","intent":"write line \"hi there\" to file `f`","library":[],"docs":[]}
{"task_id":6159900,"prompt":"def f_6159900(myfile):\n\t","suffix":"\n\treturn ","canonical_solution":"\n\tf = open(myfile, 'w')\n\tf.write(\"hi there\\n\")\n\tf.close()\n","test_start":"\ndef check(candidate):","test":["\n    file_name = 'myfile'\n    candidate(file_name)\n    with open (file_name, 'r') as f:\n        lines = f.readlines()\n        assert lines[0] == 'hi there\\n'\n"],"entry_point":"f_6159900","intent":"write line \"hi there\" to file `myfile`","library":[],"docs":[]}
{"task_id":6159900,"prompt":"def f_6159900():\n\t","suffix":"\n\treturn ","canonical_solution":"\n\twith open('somefile.txt', 'a') as the_file: \n\t\tthe_file.write('Hello\\n')\n","test_start":"\ndef check(candidate):","test":["\n    file_name = 'somefile.txt'\n    candidate()\n    with open (file_name, 'r') as f:\n        lines = f.readlines()\n        assert lines[0] == 'Hello\\n'\n"],"entry_point":"f_6159900","intent":"write line \"Hello\" to file `somefile.txt`","library":[],"docs":[]}
{"task_id":19527279,"prompt":"def f_19527279(s):\n\treturn ","suffix":"","canonical_solution":"s.encode('iso-8859-15')","test_start":"\ndef check(candidate):","test":["\n    assert candidate('table') == b'table'\n","\n    assert candidate('hello world!') == b'hello world!'\n"],"entry_point":"f_19527279","intent":"convert unicode string `s` to ascii","library":[],"docs":[]}
{"task_id":356483,"prompt":"def f_356483(text):\n\treturn ","suffix":"","canonical_solution":"re.findall('Test([0-9.]*[0-9]+)', text)","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    assert candidate('Test0.9ssd') == ['0.9']\n","\n    assert candidate('Test0.0 ..2ssd') == ['0.0']\n"],"entry_point":"f_356483","intent":"Find all numbers and dots from a string `text` using regex","library":["re"],"docs":[{"text":"pattern  \nThe regular expression pattern.","title":"python.library.re#re.error.pattern"},{"text":"string.digits  \nThe string '0123456789'.","title":"python.library.string#string.digits"},{"text":"re.purge()  \nClear the regular expression cache.","title":"python.library.re#re.purge"},{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"token.DOT  \nToken value for \".\".","title":"python.library.token#token.DOT"},{"text":"gettext.lngettext(singular, plural, n)","title":"python.library.gettext#gettext.lngettext"},{"text":"gettext.dpgettext(domain, context, message)","title":"python.library.gettext#gettext.dpgettext"},{"text":"re.search(pattern, string, flags=0)  \nScan through string looking for the first location where the regular expression pattern produces a match, and return a corresponding match object. Return None if no position in the string matches the pattern; note that this is different from finding a zero-length match at some point in the string.","title":"python.library.re#re.search"},{"text":"Pattern.findall(string[, pos[, endpos]])  \nSimilar to the findall() function, using the compiled pattern, but also accepts optional pos and endpos parameters that limit the search region like for search().","title":"python.library.re#re.Pattern.findall"},{"text":"Match.string  \nThe string passed to match() or search().","title":"python.library.re#re.Match.string"}]}
{"task_id":38081866,"prompt":"def f_38081866():\n\treturn ","suffix":"","canonical_solution":"os.system('powershell.exe', 'script.ps1')","test_start":"\nimport os\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    os.system = Mock()\n    try:\n        candidate()\n        assert True\n    except:\n        assert False\n"],"entry_point":"f_38081866","intent":"execute script 'script.ps1' using 'powershell.exe' shell","library":["os"],"docs":[{"text":"psname\n \nAlias for field number 1","title":"matplotlib.dviread#matplotlib.dviread.PsFont.psname"},{"text":"test.support.unix_shell  \nPath for shell if not on Windows; otherwise None.","title":"python.library.test#test.support.unix_shell"},{"text":"run(cmd)  \nProfile the cmd via exec().","title":"python.library.profile#profile.Profile.run"},{"text":"winreg.KEY_EXECUTE  \nEquivalent to KEY_READ.","title":"python.library.winreg#winreg.KEY_EXECUTE"},{"text":"stat.S_IXGRP  \nGroup has execute permission.","title":"python.library.stat#stat.S_IXGRP"},{"text":"cmd  \nCommand that was used to spawn the child process.","title":"python.library.subprocess#subprocess.TimeoutExpired.cmd"},{"text":"cmd  \nCommand that was used to spawn the child process.","title":"python.library.subprocess#subprocess.CalledProcessError.cmd"},{"text":"os.execl(path, arg0, arg1, ...)  \nos.execle(path, arg0, arg1, ..., env)  \nos.execlp(file, arg0, arg1, ...)  \nos.execlpe(file, arg0, arg1, ..., env)  \nos.execv(path, args)  \nos.execve(path, args, env)  \nos.execvp(file, args)  \nos.execvpe(file, args, env)  \nThese functions all execute a new program, replacing the current process; they do not return. On Unix, the new executable is loaded into the current process, and will have the same process id as the caller. Errors will be reported as OSError exceptions. The current process is replaced immediately. Open file objects and descriptors are not flushed, so if there may be data buffered on these open files, you should flush them using sys.stdout.flush() or os.fsync() before calling an exec* function. The \u201cl\u201d and \u201cv\u201d variants of the exec* functions differ in how command-line arguments are passed. The \u201cl\u201d variants are perhaps the easiest to work with if the number of parameters is fixed when the code is written; the individual parameters simply become additional parameters to the execl*() functions. The \u201cv\u201d variants are good when the number of parameters is variable, with the arguments being passed in a list or tuple as the args parameter. In either case, the arguments to the child process should start with the name of the command being run, but this is not enforced. The variants which include a \u201cp\u201d near the end (execlp(), execlpe(), execvp(), and execvpe()) will use the PATH environment variable to locate the program file. When the environment is being replaced (using one of the exec*e variants, discussed in the next paragraph), the new environment is used as the source of the PATH variable. The other variants, execl(), execle(), execv(), and execve(), will not use the PATH variable to locate the executable; path must contain an appropriate absolute or relative path. For execle(), execlpe(), execve(), and execvpe() (note that these all end in \u201ce\u201d), the env parameter must be a mapping which is used to define the environment variables for the new process (these are used instead of the current process\u2019 environment); the functions execl(), execlp(), execv(), and execvp() all cause the new process to inherit the environment of the current process. For execve() on some platforms, path may also be specified as an open file descriptor. This functionality may not be supported on your platform; you can check whether or not it is available using os.supports_fd. If it is unavailable, using it will raise a NotImplementedError. Raises an auditing event os.exec with arguments path, args, env. Availability: Unix, Windows.  New in version 3.3: Added support for specifying path as an open file descriptor for execve().   Changed in version 3.6: Accepts a path-like object.","title":"python.library.os#os.execlpe"},{"text":"os.execl(path, arg0, arg1, ...)  \nos.execle(path, arg0, arg1, ..., env)  \nos.execlp(file, arg0, arg1, ...)  \nos.execlpe(file, arg0, arg1, ..., env)  \nos.execv(path, args)  \nos.execve(path, args, env)  \nos.execvp(file, args)  \nos.execvpe(file, args, env)  \nThese functions all execute a new program, replacing the current process; they do not return. On Unix, the new executable is loaded into the current process, and will have the same process id as the caller. Errors will be reported as OSError exceptions. The current process is replaced immediately. Open file objects and descriptors are not flushed, so if there may be data buffered on these open files, you should flush them using sys.stdout.flush() or os.fsync() before calling an exec* function. The \u201cl\u201d and \u201cv\u201d variants of the exec* functions differ in how command-line arguments are passed. The \u201cl\u201d variants are perhaps the easiest to work with if the number of parameters is fixed when the code is written; the individual parameters simply become additional parameters to the execl*() functions. The \u201cv\u201d variants are good when the number of parameters is variable, with the arguments being passed in a list or tuple as the args parameter. In either case, the arguments to the child process should start with the name of the command being run, but this is not enforced. The variants which include a \u201cp\u201d near the end (execlp(), execlpe(), execvp(), and execvpe()) will use the PATH environment variable to locate the program file. When the environment is being replaced (using one of the exec*e variants, discussed in the next paragraph), the new environment is used as the source of the PATH variable. The other variants, execl(), execle(), execv(), and execve(), will not use the PATH variable to locate the executable; path must contain an appropriate absolute or relative path. For execle(), execlpe(), execve(), and execvpe() (note that these all end in \u201ce\u201d), the env parameter must be a mapping which is used to define the environment variables for the new process (these are used instead of the current process\u2019 environment); the functions execl(), execlp(), execv(), and execvp() all cause the new process to inherit the environment of the current process. For execve() on some platforms, path may also be specified as an open file descriptor. This functionality may not be supported on your platform; you can check whether or not it is available using os.supports_fd. If it is unavailable, using it will raise a NotImplementedError. Raises an auditing event os.exec with arguments path, args, env. Availability: Unix, Windows.  New in version 3.3: Added support for specifying path as an open file descriptor for execve().   Changed in version 3.6: Accepts a path-like object.","title":"python.library.os#os.execv"},{"text":"os.execl(path, arg0, arg1, ...)  \nos.execle(path, arg0, arg1, ..., env)  \nos.execlp(file, arg0, arg1, ...)  \nos.execlpe(file, arg0, arg1, ..., env)  \nos.execv(path, args)  \nos.execve(path, args, env)  \nos.execvp(file, args)  \nos.execvpe(file, args, env)  \nThese functions all execute a new program, replacing the current process; they do not return. On Unix, the new executable is loaded into the current process, and will have the same process id as the caller. Errors will be reported as OSError exceptions. The current process is replaced immediately. Open file objects and descriptors are not flushed, so if there may be data buffered on these open files, you should flush them using sys.stdout.flush() or os.fsync() before calling an exec* function. The \u201cl\u201d and \u201cv\u201d variants of the exec* functions differ in how command-line arguments are passed. The \u201cl\u201d variants are perhaps the easiest to work with if the number of parameters is fixed when the code is written; the individual parameters simply become additional parameters to the execl*() functions. The \u201cv\u201d variants are good when the number of parameters is variable, with the arguments being passed in a list or tuple as the args parameter. In either case, the arguments to the child process should start with the name of the command being run, but this is not enforced. The variants which include a \u201cp\u201d near the end (execlp(), execlpe(), execvp(), and execvpe()) will use the PATH environment variable to locate the program file. When the environment is being replaced (using one of the exec*e variants, discussed in the next paragraph), the new environment is used as the source of the PATH variable. The other variants, execl(), execle(), execv(), and execve(), will not use the PATH variable to locate the executable; path must contain an appropriate absolute or relative path. For execle(), execlpe(), execve(), and execvpe() (note that these all end in \u201ce\u201d), the env parameter must be a mapping which is used to define the environment variables for the new process (these are used instead of the current process\u2019 environment); the functions execl(), execlp(), execv(), and execvp() all cause the new process to inherit the environment of the current process. For execve() on some platforms, path may also be specified as an open file descriptor. This functionality may not be supported on your platform; you can check whether or not it is available using os.supports_fd. If it is unavailable, using it will raise a NotImplementedError. Raises an auditing event os.exec with arguments path, args, env. Availability: Unix, Windows.  New in version 3.3: Added support for specifying path as an open file descriptor for execve().   Changed in version 3.6: Accepts a path-like object.","title":"python.library.os#os.execlp"}]}
{"task_id":7349646,"prompt":"def f_7349646(b):\n\t","suffix":"\n\treturn b","canonical_solution":"b.sort(key=lambda x: x[2])","test_start":"\ndef check(candidate):","test":["\n    b = [(1,2,3), (4,5,6), (7,8,0)]\n    assert candidate(b) == [(7,8,0), (1,2,3), (4,5,6)]\n","\n    b = [(1,2,'a'), (4,5,'c'), (7,8,'A')]\n    assert candidate(b) == [(7,8,'A'), (1,2,'a'), (4,5,'c')]\n"],"entry_point":"f_7349646","intent":"Sort a list of tuples `b` by third item in the tuple","library":[],"docs":[]}
{"task_id":10607688,"prompt":"def f_10607688():\n\treturn ","suffix":"","canonical_solution":"datetime.datetime.now()","test_start":"\nimport datetime\n\ndef check(candidate):","test":["\n    y = candidate()\n    assert y.year >= 2022\n"],"entry_point":"f_10607688","intent":"create a datetime with the current date & time","library":["datetime"],"docs":[{"text":"datetime.date()  \nReturn date object with same year, month and day.","title":"python.library.datetime#datetime.datetime.date"},{"text":"pandas.Timestamp.day   Timestamp.day","title":"pandas.reference.api.pandas.timestamp.day"},{"text":"classmethod date.today()  \nReturn the current local date. This is equivalent to date.fromtimestamp(time.time()).","title":"python.library.datetime#datetime.date.today"},{"text":"time.hour  \nIn range(24).","title":"python.library.datetime#datetime.time.hour"},{"text":"pandas.Timestamp.hour   Timestamp.hour","title":"pandas.reference.api.pandas.timestamp.hour"},{"text":"pandas.Timedelta.value   Timedelta.value","title":"pandas.reference.api.pandas.timedelta.value"},{"text":"time_format  \nSimilar to TimeInput.format","title":"django.ref.forms.widgets#django.forms.SplitDateTimeWidget.time_format"},{"text":"pandas.Timestamp.fromisoformat   Timestamp.fromisoformat()\n \nstring -> datetime from datetime.isoformat() output","title":"pandas.reference.api.pandas.timestamp.fromisoformat"},{"text":"datetime.hour  \nIn range(24).","title":"python.library.datetime#datetime.datetime.hour"},{"text":"date_attrs","title":"django.ref.forms.widgets#django.forms.SplitDateTimeWidget.date_attrs"}]}
{"task_id":30843103,"prompt":"def f_30843103(lst):\n\treturn ","suffix":"","canonical_solution":"next(i for i, x in enumerate(lst) if not isinstance(x, bool) and x == 1)","test_start":"\ndef check(candidate):","test":["\n    lst = [True, False, 1, 3]\n    assert candidate(lst) == 2\n"],"entry_point":"f_30843103","intent":"get the index of an integer `1` from a list `lst` if the list also contains boolean items","library":[],"docs":[]}
{"task_id":4918425,"prompt":"def f_4918425(a):\n\t","suffix":"\n\treturn a","canonical_solution":"a[:] = [(x - 13) for x in a]","test_start":"\ndef check(candidate):","test":["\n    a = [14, 15]\n    candidate(a)\n    assert a == [1, 2]\n","\n    a = [float(x) for x in range(13, 20)]\n    candidate(a)\n    assert a == [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n"],"entry_point":"f_4918425","intent":"subtract 13 from every number in a list `a`","library":[],"docs":[]}
{"task_id":17794266,"prompt":"def f_17794266(x):\n\treturn ","suffix":"","canonical_solution":"max(x.min(), x.max(), key=abs)","test_start":"\nimport numpy as np \n\ndef check(candidate):","test":["\n    x = np.matrix([[1, 1], [2, -3]])\n    assert candidate(x) == -3\n"],"entry_point":"f_17794266","intent":"get the highest element in absolute value in a numpy matrix `x`","library":["numpy"],"docs":[{"text":"abs(x)  \nReturns the absolute value of x.","title":"python.library.decimal#decimal.Context.abs"},{"text":"numpy.absolute   numpy.absolute(x, \/, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj]) = <ufunc 'absolute'>\n \nCalculate the absolute value element-wise. np.abs is a shorthand for this function.  Parameters \n \nxarray_like\n\n\nInput array.  \noutndarray, None, or tuple of ndarray and None, optional\n\n\nA location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.  \nwherearray_like, optional\n\n\nThis condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.  **kwargs\n\nFor other keyword-only arguments, see the ufunc docs.    Returns \n \nabsolutendarray\n\n\nAn ndarray containing the absolute value of each element in x. For complex input, a + ib, the absolute value is \\(\\sqrt{ a^2 + b^2 }\\). This is a scalar if x is a scalar.     Examples >>> x = np.array([-1.2, 1.2])\n>>> np.absolute(x)\narray([ 1.2,  1.2])\n>>> np.absolute(1.2 + 1j)\n1.5620499351813308\n Plot the function over [-10, 10]: >>> import matplotlib.pyplot as plt\n >>> x = np.linspace(start=-10, stop=10, num=101)\n>>> plt.plot(x, np.absolute(x))\n>>> plt.show()\n    Plot the function over the complex plane: >>> xx = x + 1j * x[:, np.newaxis]\n>>> plt.imshow(np.abs(xx), extent=[-10, 10, -10, 10], cmap='gray')\n>>> plt.show()\n    The abs function can be used as a shorthand for np.absolute on ndarrays. >>> x = np.array([-1.2, 1.2])\n>>> abs(x)\narray([1.2, 1.2])","title":"numpy.reference.generated.numpy.absolute"},{"text":"math.fabs(x)  \nReturn the absolute value of x.","title":"python.library.math#math.fabs"},{"text":"numpy.ma.MaskedArray.__array_priority__ attribute   ma.MaskedArray.__array_priority__ = 15","title":"numpy.reference.generated.numpy.ma.maskedarray.__array_priority__"},{"text":"decimal.MAX_EMAX","title":"python.library.decimal#decimal.MAX_EMAX"},{"text":"max(x, y)  \nCompares two values numerically and returns the maximum.","title":"python.library.decimal#decimal.Context.max"},{"text":"numpy.ndarray.__abs__ method   ndarray.__abs__(self)","title":"numpy.reference.generated.numpy.ndarray.__abs__"},{"text":"numpy.matrix.max method   matrix.max(axis=None, out=None)[source]\n \nReturn the maximum value along an axis.  Parameters \n See `amax` for complete descriptions\n    See also  \namax, ndarray.max\n\n  Notes This is the same as ndarray.max, but returns a matrix object where ndarray.max would return an ndarray. Examples >>> x = np.matrix(np.arange(12).reshape((3,4))); x\nmatrix([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])\n>>> x.max()\n11\n>>> x.max(0)\nmatrix([[ 8,  9, 10, 11]])\n>>> x.max(1)\nmatrix([[ 3],\n        [ 7],\n        [11]])","title":"numpy.reference.generated.numpy.matrix.max"},{"text":"numpy.nditer.index attribute   nditer.index","title":"numpy.reference.generated.numpy.nditer.index"},{"text":"numpy.record.base attribute   record.base\n \nbase object","title":"numpy.reference.generated.numpy.record.base"}]}
{"task_id":30551576,"prompt":"def f_30551576(s):\n\treturn ","suffix":"","canonical_solution":"re.findall(r'\"(http.*?)\"', s, re.MULTILINE | re.DOTALL)","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    s = (\n      '     [irrelevant javascript code here]'\n      '     sources:[{file:\"http:\/\/url.com\/folder1\/v.html\",label:\"label1\"},'\n      '     {file:\"http:\/\/url.com\/folder2\/v.html\",label:\"label2\"},'\n      '     {file:\"http:\/\/url.com\/folder3\/v.html\",label:\"label3\"}],'\n      '     [irrelevant javascript code here]'\n    )\n    assert candidate(s) == ['http:\/\/url.com\/folder1\/v.html', 'http:\/\/url.com\/folder2\/v.html', 'http:\/\/url.com\/folder3\/v.html']\n","\n    s = (\n      '     [irrelevant javascript code here]'\n      '     [irrelevant python code here]'\n    )\n    assert candidate(s) == []\n"],"entry_point":"f_30551576","intent":"Get all urls within text `s`","library":["re"],"docs":[{"text":"gettext.lngettext(singular, plural, n)","title":"python.library.gettext#gettext.lngettext"},{"text":"str.index(sub[, start[, end]])  \nLike find(), but raise ValueError when the substring is not found.","title":"python.library.stdtypes#str.index"},{"text":"urls.staticfiles_urlpatterns()","title":"django.ref.contrib.staticfiles#django.contrib.staticfiles.urls.staticfiles_urlpatterns"},{"text":"handler500","title":"django.ref.urls#django.conf.urls.handler500"},{"text":"url  \nURL of the resource retrieved, commonly used to determine if a redirect was followed.","title":"python.library.urllib.request#urllib.response.addinfourl.url"},{"text":"urllib.parse.unwrap(url)  \nExtract the url from a wrapped URL (that is, a string formatted as <URL:scheme:\/\/host\/path>, <scheme:\/\/host\/path>, URL:scheme:\/\/host\/path or scheme:\/\/host\/path). If url is not a wrapped URL, it is returned without changes.","title":"python.library.urllib.parse#urllib.parse.unwrap"},{"text":"handler400","title":"django.ref.urls#django.conf.urls.handler400"},{"text":"token.SLASH  \nToken value for \"\/\".","title":"python.library.token#token.SLASH"},{"text":"get_script_prefix()","title":"django.ref.urlresolvers#django.urls.get_script_prefix"},{"text":"handler404","title":"django.ref.urls#django.conf.urls.handler404"}]}
{"task_id":113534,"prompt":"def f_113534(mystring):\n\treturn ","suffix":"","canonical_solution":"mystring.replace(' ', '! !').split('!')","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"This is the string I want to split\") ==       ['This',' ','is',' ','the',' ','string',' ','I',' ','want',' ','to',' ','split']\n"],"entry_point":"f_113534","intent":"split a string `mystring` considering the spaces ' '","library":[],"docs":[]}
{"task_id":5838735,"prompt":"def f_5838735(path):\n\treturn ","suffix":"","canonical_solution":"open(path, 'r')","test_start":"\ndef check(candidate):","test":["\n    with open('tmp.txt', 'w') as fw: fw.write('hello world!')\n    f = candidate('tmp.txt')\n    assert f.name == 'tmp.txt'\n    assert f.mode == 'r'\n"],"entry_point":"f_5838735","intent":"open file `path` with mode 'r'","library":[],"docs":[]}
{"task_id":36003967,"prompt":"def f_36003967(data):\n\treturn ","suffix":"","canonical_solution":"[[sum(item) for item in zip(*items)] for items in zip(*data)]","test_start":"\ndef check(candidate):","test":["\n    data = [[[5, 10, 30, 24, 100], [1, 9, 25, 49, 81]],\n            [[15, 10, 10, 16, 70], [10, 1, 25, 11, 19]],\n            [[34, 20, 10, 10, 30], [9, 20, 25, 30, 80]]]\n    assert candidate(data) == [[54, 40, 50, 50, 200], [20, 30, 75, 90, 180]]\n"],"entry_point":"f_36003967","intent":"sum elements at the same index in list `data`","library":[],"docs":[]}
{"task_id":7635237,"prompt":"def f_7635237(a):\n\treturn ","suffix":"","canonical_solution":"a[:, (np.newaxis)]","test_start":"\nimport numpy as np \n\ndef check(candidate):","test":["\n    data = np.array([[[5, 10, 30, 24, 100], [1, 9, 25, 49, 81]],\n            [[15, 10, 10, 16, 70], [10, 1, 25, 11, 19]],\n            [[34, 20, 10, 10, 30], [9, 20, 25, 30, 80]]])\n    assert candidate(data).tolist() == [[[[  5,  10,  30,  24, 100],\n         [  1,   9,  25,  49,  81]]],\n       [[[ 15,  10,  10,  16,  70],\n         [ 10,   1,  25,  11,  19]]],\n       [[[ 34,  20,  10,  10,  30],\n         [  9,  20,  25,  30,  80]]]]\n"],"entry_point":"f_7635237","intent":"add a new axis to array `a`","library":["numpy"],"docs":[{"text":"array.append(x)  \nAppend a new item with value x to the end of the array.","title":"python.library.array#array.array.append"},{"text":"matplotlib.axis.Axis.OFFSETTEXTPAD   Axis.OFFSETTEXTPAD=3","title":"matplotlib._as_gen.matplotlib.axis.axis.offsettextpad"},{"text":"axis=None","title":"matplotlib.ticker_api#matplotlib.ticker.TickHelper.axis"},{"text":"quiver_doc=\"\\nPlot a 2D field of arrows.\\n\\nCall signature::\\n\\n quiver([X, Y], U, V, [C], **kw)\\n\\n*X*, *Y* define the arrow locations, *U*, *V* define the arrow directions, and\\n*C* optionally sets the color.\\n\\nEach arrow is internally represented by a filled polygon with a default edge\\nlinewidth of 0. As a result, an arrow is rather a filled area, not a line with\\na head, and `.PolyCollection` properties like *linewidth*, *linestyle*,\\n*facecolor*, etc. act accordingly.\\n\\n**Arrow size**\\n\\nThe default settings auto-scales the length of the arrows to a reasonable size.\\nTo change this behavior see the *scale* and *scale_units* parameters.\\n\\n**Arrow shape**\\n\\nThe defaults give a slightly swept-back arrow; to make the head a\\ntriangle, make *headaxislength* the same as *headlength*. To make the\\narrow more pointed, reduce *headwidth* or increase *headlength* and\\n*headaxislength*. To make the head smaller relative to the shaft,\\nscale down all the head parameters. You will probably do best to leave\\nminshaft alone.\\n\\n**Arrow outline**\\n\\n*linewidths* and *edgecolors* can be used to customize the arrow\\noutlines.\\n\\nParameters\\n----------\\nX, Y : 1D or 2D array-like, optional\\n The x and y coordinates of the arrow locations.\\n\\n If not given, they will be generated as a uniform integer meshgrid based\\n on the dimensions of *U* and *V*.\\n\\n If *X* and *Y* are 1D but *U*, *V* are 2D, *X*, *Y* are expanded to 2D\\n using ``X, Y = np.meshgrid(X, Y)``. In this case ``len(X)`` and ``len(Y)``\\n must match the column and row dimensions of *U* and *V*.\\n\\nU, V : 1D or 2D array-like\\n The x and y direction components of the arrow vectors.\\n\\n They must have the same number of elements, matching the number of arrow\\n locations. *U* and *V* may be masked. Only locations unmasked in\\n *U*, *V*, and *C* will be drawn.\\n\\nC : 1D or 2D array-like, optional\\n Numeric data that defines the arrow colors by colormapping via *norm* and\\n *cmap*.\\n\\n This does not support explicit colors. If you want to set colors directly,\\n use *color* instead. The size of *C* must match the number of arrow\\n locations.\\n\\nunits : {'width', 'height', 'dots', 'inches', 'x', 'y', 'xy'}, default: 'width'\\n The arrow dimensions (except for *length*) are measured in multiples of\\n this unit.\\n\\n The following values are supported:\\n\\n - 'width', 'height': The width or height of the axis.\\n - 'dots', 'inches': Pixels or inches based on the figure dpi.\\n - 'x', 'y', 'xy': *X*, *Y* or :math:`\\\\sqrt{X^2 + Y^2}` in data units.\\n\\n The arrows scale differently depending on the units. For\\n 'x' or 'y', the arrows get larger as one zooms in; for other\\n units, the arrow size is independent of the zoom state. For\\n 'width or 'height', the arrow size increases with the width and\\n height of the axes, respectively, when the window is resized;\\n for 'dots' or 'inches', resizing does not change the arrows.\\n\\nangles : {'uv', 'xy'} or array-like, default: 'uv'\\n Method for determining the angle of the arrows.\\n\\n - 'uv': The arrow axis aspect ratio is 1 so that\\n if *U* == *V* the orientation of the arrow on the plot is 45 degrees\\n counter-clockwise from the horizontal axis (positive to the right).\\n\\n Use this if the arrows symbolize a quantity that is not based on\\n *X*, *Y* data coordinates.\\n\\n - 'xy': Arrows point from (x, y) to (x+u, y+v).\\n Use this for plotting a gradient field, for example.\\n\\n - Alternatively, arbitrary angles may be specified explicitly as an array\\n of values in degrees, counter-clockwise from the horizontal axis.\\n\\n In this case *U*, *V* is only used to determine the length of the\\n arrows.\\n\\n Note: inverting a data axis will correspondingly invert the\\n arrows only with ``angles='xy'``.\\n\\nscale : float, optional\\n Number of data units per arrow length unit, e.g., m\/s per plot width; a\\n smaller scale parameter makes the arrow longer. Default is *None*.\\n\\n If *None*, a simple autoscaling algorithm is used, based on the average\\n vector length and the number of vectors. The arrow length unit is given by\\n the *scale_units* parameter.\\n\\nscale_units : {'width', 'height', 'dots', 'inches', 'x', 'y', 'xy'}, optional\\n If the *scale* kwarg is *None*, the arrow length unit. Default is *None*.\\n\\n e.g. *scale_units* is 'inches', *scale* is 2.0, and ``(u, v) = (1, 0)``,\\n then the vector will be 0.5 inches long.\\n\\n If *scale_units* is 'width' or 'height', then the vector will be half the\\n width\/height of the axes.\\n\\n If *scale_units* is 'x' then the vector will be 0.5 x-axis\\n units. To plot vectors in the x-y plane, with u and v having\\n the same units as x and y, use\\n ``angles='xy', scale_units='xy', scale=1``.\\n\\nwidth : float, optional\\n Shaft width in arrow units; default depends on choice of units,\\n above, and number of vectors; a typical starting value is about\\n 0.005 times the width of the plot.\\n\\nheadwidth : float, default: 3\\n Head width as multiple of shaft width.\\n\\nheadlength : float, default: 5\\n Head length as multiple of shaft width.\\n\\nheadaxislength : float, default: 4.5\\n Head length at shaft intersection.\\n\\nminshaft : float, default: 1\\n Length below which arrow scales, in units of head length. Do not\\n set this to less than 1, or small arrows will look terrible!\\n\\nminlength : float, default: 1\\n Minimum length as a multiple of shaft width; if an arrow length\\n is less than this, plot a dot (hexagon) of this diameter instead.\\n\\npivot : {'tail', 'mid', 'middle', 'tip'}, default: 'tail'\\n The part of the arrow that is anchored to the *X*, *Y* grid. The arrow\\n rotates about this point.\\n\\n 'mid' is a synonym for 'middle'.\\n\\ncolor : color or color sequence, optional\\n Explicit color(s) for the arrows. If *C* has been set, *color* has no\\n effect.\\n\\n This is a synonym for the `.PolyCollection` *facecolor* parameter.\\n\\nOther Parameters\\n----------------\\ndata : indexable object, optional\\n DATA_PARAMETER_PLACEHOLDER\\n\\n**kwargs : `~matplotlib.collections.PolyCollection` properties, optional\\n All other keyword arguments are passed on to `.PolyCollection`:\\n\\n \\n .. table::\\n :class: property-table\\n\\n ================================================================================================= =====================================================================================================\\n Property Description \\n ================================================================================================= =====================================================================================================\\n :meth:`agg_filter <matplotlib.artist.Artist.set_agg_filter>` a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\\n :meth:`alpha <matplotlib.collections.Collection.set_alpha>` array-like or scalar or None \\n :meth:`animated <matplotlib.artist.Artist.set_animated>` bool \\n :meth:`antialiased <matplotlib.collections.Collection.set_antialiased>` or aa or antialiaseds bool or list of bools \\n :meth:`array <matplotlib.cm.ScalarMappable.set_array>` array-like or None \\n :meth:`capstyle <matplotlib.collections.Collection.set_capstyle>` `.CapStyle` or {'butt', 'projecting', 'round'} \\n :meth:`clim <matplotlib.cm.ScalarMappable.set_clim>` (vmin: float, vmax: float) \\n :meth:`clip_box <matplotlib.artist.Artist.set_clip_box>` `.Bbox` \\n :meth:`clip_on <matplotlib.artist.Artist.set_clip_on>` bool \\n :meth:`clip_path <matplotlib.artist.Artist.set_clip_path>` Patch or (Path, Transform) or None \\n :meth:`cmap <matplotlib.cm.ScalarMappable.set_cmap>` `.Colormap` or str or None \\n :meth:`color <matplotlib.collections.Collection.set_color>` color or list of rgba tuples \\n :meth:`edgecolor <matplotlib.collections.Collection.set_edgecolor>` or ec or edgecolors color or list of colors or 'face' \\n :meth:`facecolor <matplotlib.collections.Collection.set_facecolor>` or facecolors or fc color or list of colors \\n :meth:`figure <matplotlib.artist.Artist.set_figure>` `.Figure` \\n :meth:`gid <matplotlib.artist.Artist.set_gid>` str \\n :meth:`hatch <matplotlib.collections.Collection.set_hatch>` {'\/', '\\\\\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'} \\n :meth:`in_layout <matplotlib.artist.Artist.set_in_layout>` bool \\n :meth:`joinstyle <matplotlib.collections.Collection.set_joinstyle>` `.JoinStyle` or {'miter', 'round', 'bevel'} \\n :meth:`label <matplotlib.artist.Artist.set_label>` object \\n :meth:`linestyle <matplotlib.collections.Collection.set_linestyle>` or dashes or linestyles or ls str or tuple or list thereof \\n :meth:`linewidth <matplotlib.collections.Collection.set_linewidth>` or linewidths or lw float or list of floats \\n :meth:`norm <matplotlib.cm.ScalarMappable.set_norm>` `.Normalize` or None \\n :meth:`offset_transform <matplotlib.collections.Collection.set_offset_transform>` `.Transform` \\n :meth:`offsets <matplotlib.collections.Collection.set_offsets>` (N, 2) or (2,) array-like \\n :meth:`path_effects <matplotlib.artist.Artist.set_path_effects>` `.AbstractPathEffect` \\n :meth:`paths <matplotlib.collections.PolyCollection.set_verts>` list of array-like \\n :meth:`picker <matplotlib.artist.Artist.set_picker>` None or bool or float or callable \\n :meth:`pickradius <matplotlib.collections.Collection.set_pickradius>` float \\n :meth:`rasterized <matplotlib.artist.Artist.set_rasterized>` bool \\n :meth:`sizes <matplotlib.collections._CollectionWithSizes.set_sizes>` ndarray or None \\n :meth:`sketch_params <matplotlib.artist.Artist.set_sketch_params>` (scale: float, length: float, randomness: float) \\n :meth:`snap <matplotlib.artist.Artist.set_snap>` bool or None \\n :meth:`transform <matplotlib.artist.Artist.set_transform>` `.Transform` \\n :meth:`url <matplotlib.artist.Artist.set_url>` str \\n :meth:`urls <matplotlib.collections.Collection.set_urls>` list of str or None \\n :meth:`verts <matplotlib.collections.PolyCollection.set_verts>` list of array-like \\n :meth:`verts_and_codes <matplotlib.collections.PolyCollection.set_verts_and_codes>` unknown \\n :meth:`visible <matplotlib.artist.Artist.set_visible>` bool \\n :meth:`zorder <matplotlib.artist.Artist.set_zorder>` float \\n ================================================================================================= =====================================================================================================\\n\\n\\nReturns\\n-------\\n`~matplotlib.quiver.Quiver`\\n\\nSee Also\\n--------\\n.Axes.quiverkey : Add a key to a quiver plot.\\n\"","title":"matplotlib._as_gen.matplotlib.quiver.quiver#matplotlib.quiver.Quiver.quiver_doc"},{"text":"add_artist(a)[source]","title":"matplotlib._as_gen.mpl_toolkits.axes_grid1.axes_size.maxextent#mpl_toolkits.axes_grid1.axes_size.MaxExtent.add_artist"},{"text":"matplotlib.axes.Axes.name   Axes.name='rectilinear'","title":"matplotlib._as_gen.matplotlib.axes.axes.name"},{"text":"matplotlib.axes.Axes.zorder   Axes.zorder=0","title":"matplotlib._as_gen.matplotlib.axes.axes.zorder"},{"text":"INVALID_NON_AFFINE=1","title":"matplotlib.transformations#matplotlib.transforms.TransformNode.INVALID_NON_AFFINE"},{"text":"barbs_doc='\\nPlot a 2D field of barbs.\\n\\nCall signature::\\n\\n barbs([X, Y], U, V, [C], **kw)\\n\\nWhere *X*, *Y* define the barb locations, *U*, *V* define the barb\\ndirections, and *C* optionally sets the color.\\n\\nAll arguments may be 1D or 2D. *U*, *V*, *C* may be masked arrays, but masked\\n*X*, *Y* are not supported at present.\\n\\nBarbs are traditionally used in meteorology as a way to plot the speed\\nand direction of wind observations, but can technically be used to\\nplot any two dimensional vector quantity. As opposed to arrows, which\\ngive vector magnitude by the length of the arrow, the barbs give more\\nquantitative information about the vector magnitude by putting slanted\\nlines or a triangle for various increments in magnitude, as show\\nschematically below::\\n\\n : \/\\\\ \\\\\\n : \/ \\\\ \\\\\\n : \/ \\\\ \\\\ \\\\\\n : \/ \\\\ \\\\ \\\\\\n : ------------------------------\\n\\nThe largest increment is given by a triangle (or \"flag\"). After those\\ncome full lines (barbs). The smallest increment is a half line. There\\nis only, of course, ever at most 1 half line. If the magnitude is\\nsmall and only needs a single half-line and no full lines or\\ntriangles, the half-line is offset from the end of the barb so that it\\ncan be easily distinguished from barbs with a single full line. The\\nmagnitude for the barb shown above would nominally be 65, using the\\nstandard increments of 50, 10, and 5.\\n\\nSee also https:\/\/en.wikipedia.org\/wiki\/Wind_barb.\\n\\nParameters\\n----------\\nX, Y : 1D or 2D array-like, optional\\n The x and y coordinates of the barb locations. See *pivot* for how the\\n barbs are drawn to the x, y positions.\\n\\n If not given, they will be generated as a uniform integer meshgrid based\\n on the dimensions of *U* and *V*.\\n\\n If *X* and *Y* are 1D but *U*, *V* are 2D, *X*, *Y* are expanded to 2D\\n using ``X, Y = np.meshgrid(X, Y)``. In this case ``len(X)`` and ``len(Y)``\\n must match the column and row dimensions of *U* and *V*.\\n\\nU, V : 1D or 2D array-like\\n The x and y components of the barb shaft.\\n\\nC : 1D or 2D array-like, optional\\n Numeric data that defines the barb colors by colormapping via *norm* and\\n *cmap*.\\n\\n This does not support explicit colors. If you want to set colors directly,\\n use *barbcolor* instead.\\n\\nlength : float, default: 7\\n Length of the barb in points; the other parts of the barb\\n are scaled against this.\\n\\npivot : {\\'tip\\', \\'middle\\'} or float, default: \\'tip\\'\\n The part of the arrow that is anchored to the *X*, *Y* grid. The barb\\n rotates about this point. This can also be a number, which shifts the\\n start of the barb that many points away from grid point.\\n\\nbarbcolor : color or color sequence\\n The color of all parts of the barb except for the flags. This parameter\\n is analogous to the *edgecolor* parameter for polygons, which can be used\\n instead. However this parameter will override facecolor.\\n\\nflagcolor : color or color sequence\\n The color of any flags on the barb. This parameter is analogous to the\\n *facecolor* parameter for polygons, which can be used instead. However,\\n this parameter will override facecolor. If this is not set (and *C* has\\n not either) then *flagcolor* will be set to match *barbcolor* so that the\\n barb has a uniform color. If *C* has been set, *flagcolor* has no effect.\\n\\nsizes : dict, optional\\n A dictionary of coefficients specifying the ratio of a given\\n feature to the length of the barb. Only those values one wishes to\\n override need to be included. These features include:\\n\\n - \\'spacing\\' - space between features (flags, full\/half barbs)\\n - \\'height\\' - height (distance from shaft to top) of a flag or full barb\\n - \\'width\\' - width of a flag, twice the width of a full barb\\n - \\'emptybarb\\' - radius of the circle used for low magnitudes\\n\\nfill_empty : bool, default: False\\n Whether the empty barbs (circles) that are drawn should be filled with\\n the flag color. If they are not filled, the center is transparent.\\n\\nrounding : bool, default: True\\n Whether the vector magnitude should be rounded when allocating barb\\n components. If True, the magnitude is rounded to the nearest multiple\\n of the half-barb increment. If False, the magnitude is simply truncated\\n to the next lowest multiple.\\n\\nbarb_increments : dict, optional\\n A dictionary of increments specifying values to associate with\\n different parts of the barb. Only those values one wishes to\\n override need to be included.\\n\\n - \\'half\\' - half barbs (Default is 5)\\n - \\'full\\' - full barbs (Default is 10)\\n - \\'flag\\' - flags (default is 50)\\n\\nflip_barb : bool or array-like of bool, default: False\\n Whether the lines and flags should point opposite to normal.\\n Normal behavior is for the barbs and lines to point right (comes from wind\\n barbs having these features point towards low pressure in the Northern\\n Hemisphere).\\n\\n A single value is applied to all barbs. Individual barbs can be flipped by\\n passing a bool array of the same size as *U* and *V*.\\n\\nReturns\\n-------\\nbarbs : `~matplotlib.quiver.Barbs`\\n\\nOther Parameters\\n----------------\\ndata : indexable object, optional\\n DATA_PARAMETER_PLACEHOLDER\\n\\n**kwargs\\n The barbs can further be customized using `.PolyCollection` keyword\\n arguments:\\n\\n \\n .. table::\\n :class: property-table\\n\\n ================================================================================================= =====================================================================================================\\n Property Description \\n ================================================================================================= =====================================================================================================\\n :meth:`agg_filter <matplotlib.artist.Artist.set_agg_filter>` a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\\n :meth:`alpha <matplotlib.collections.Collection.set_alpha>` array-like or scalar or None \\n :meth:`animated <matplotlib.artist.Artist.set_animated>` bool \\n :meth:`antialiased <matplotlib.collections.Collection.set_antialiased>` or aa or antialiaseds bool or list of bools \\n :meth:`array <matplotlib.cm.ScalarMappable.set_array>` array-like or None \\n :meth:`capstyle <matplotlib.collections.Collection.set_capstyle>` `.CapStyle` or {\\'butt\\', \\'projecting\\', \\'round\\'} \\n :meth:`clim <matplotlib.cm.ScalarMappable.set_clim>` (vmin: float, vmax: float) \\n :meth:`clip_box <matplotlib.artist.Artist.set_clip_box>` `.Bbox` \\n :meth:`clip_on <matplotlib.artist.Artist.set_clip_on>` bool \\n :meth:`clip_path <matplotlib.artist.Artist.set_clip_path>` Patch or (Path, Transform) or None \\n :meth:`cmap <matplotlib.cm.ScalarMappable.set_cmap>` `.Colormap` or str or None \\n :meth:`color <matplotlib.collections.Collection.set_color>` color or list of rgba tuples \\n :meth:`edgecolor <matplotlib.collections.Collection.set_edgecolor>` or ec or edgecolors color or list of colors or \\'face\\' \\n :meth:`facecolor <matplotlib.collections.Collection.set_facecolor>` or facecolors or fc color or list of colors \\n :meth:`figure <matplotlib.artist.Artist.set_figure>` `.Figure` \\n :meth:`gid <matplotlib.artist.Artist.set_gid>` str \\n :meth:`hatch <matplotlib.collections.Collection.set_hatch>` {\\'\/\\', \\'\\\\\\\\\\', \\'|\\', \\'-\\', \\'+\\', \\'x\\', \\'o\\', \\'O\\', \\'.\\', \\'*\\'} \\n :meth:`in_layout <matplotlib.artist.Artist.set_in_layout>` bool \\n :meth:`joinstyle <matplotlib.collections.Collection.set_joinstyle>` `.JoinStyle` or {\\'miter\\', \\'round\\', \\'bevel\\'} \\n :meth:`label <matplotlib.artist.Artist.set_label>` object \\n :meth:`linestyle <matplotlib.collections.Collection.set_linestyle>` or dashes or linestyles or ls str or tuple or list thereof \\n :meth:`linewidth <matplotlib.collections.Collection.set_linewidth>` or linewidths or lw float or list of floats \\n :meth:`norm <matplotlib.cm.ScalarMappable.set_norm>` `.Normalize` or None \\n :meth:`offset_transform <matplotlib.collections.Collection.set_offset_transform>` `.Transform` \\n :meth:`offsets <matplotlib.collections.Collection.set_offsets>` (N, 2) or (2,) array-like \\n :meth:`path_effects <matplotlib.artist.Artist.set_path_effects>` `.AbstractPathEffect` \\n :meth:`paths <matplotlib.collections.PolyCollection.set_verts>` list of array-like \\n :meth:`picker <matplotlib.artist.Artist.set_picker>` None or bool or float or callable \\n :meth:`pickradius <matplotlib.collections.Collection.set_pickradius>` float \\n :meth:`rasterized <matplotlib.artist.Artist.set_rasterized>` bool \\n :meth:`sizes <matplotlib.collections._CollectionWithSizes.set_sizes>` ndarray or None \\n :meth:`sketch_params <matplotlib.artist.Artist.set_sketch_params>` (scale: float, length: float, randomness: float) \\n :meth:`snap <matplotlib.artist.Artist.set_snap>` bool or None \\n :meth:`transform <matplotlib.artist.Artist.set_transform>` `.Transform` \\n :meth:`url <matplotlib.artist.Artist.set_url>` str \\n :meth:`urls <matplotlib.collections.Collection.set_urls>` list of str or None \\n :meth:`verts <matplotlib.collections.PolyCollection.set_verts>` list of array-like \\n :meth:`verts_and_codes <matplotlib.collections.PolyCollection.set_verts_and_codes>` unknown \\n :meth:`visible <matplotlib.artist.Artist.set_visible>` bool \\n :meth:`zorder <matplotlib.artist.Artist.set_zorder>` float \\n ================================================================================================= =====================================================================================================\\n\\n'","title":"matplotlib._as_gen.matplotlib.quiver.barbs#matplotlib.quiver.Barbs.barbs_doc"},{"text":"numpy.chararray.squeeze method   chararray.squeeze(axis=None)\n \nRemove axes of length one from a. Refer to numpy.squeeze for full documentation.  See also  numpy.squeeze\n\nequivalent function","title":"numpy.reference.generated.numpy.chararray.squeeze"}]}
