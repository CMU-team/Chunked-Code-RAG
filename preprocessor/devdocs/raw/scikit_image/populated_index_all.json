[{"name": "A crash course on NumPy for images", "path": "user_guide/numpy_images", "type": "Guide", "text": "A crash course on NumPy for images Images in scikit-image are represented by NumPy ndarrays. Hence, many common operations can be achieved using standard NumPy methods for manipulating arrays: >>> from skimage import data\n>>> camera = data.camera()\n>>> type(camera)\n<type 'numpy.ndarray'>\n Retrieving the geometry of the image and the number of pixels: >>> camera.shape\n(512, 512)\n>>> camera.size\n262144\n Retrieving statistical information about image intensity values: >>> camera.min(), camera.max()\n(0, 255)\n>>> camera.mean()\n118.31400299072266\n NumPy arrays representing images can be of different integer or float numerical types. See Image data types and what they mean for more information about these types and how scikit-image treats them. NumPy indexing NumPy indexing can be used both for looking at the pixel values and to modify them: >>> # Get the value of the pixel at the 10th row and 20th column\n>>> camera[10, 20]\n153\n>>> # Set to black the pixel at the 3rd row and 10th column\n>>> camera[3, 10] = 0\n Be careful! In NumPy indexing, the first dimension (camera.shape[0]) corresponds to rows, while the second (camera.shape[1]) corresponds to columns, with the origin (camera[0, 0]) at the top-left corner. This matches matrix/linear algebra notation, but is in contrast to Cartesian (x, y) coordinates. See Coordinate conventions below for more details. Beyond individual pixels, it is possible to access/modify values of whole sets of pixels using the different indexing capabilities of NumPy. Slicing: >>> # Set the first ten lines to \"black\" (0)\n>>> camera[:10] = 0\n Masking (indexing with masks of booleans): >>> mask = camera < 87\n>>> # Set to \"white\" (255) the pixels where mask is True\n>>> camera[mask] = 255\n Fancy indexing (indexing with sets of indices): >>> inds_r = np.arange(len(camera))\n>>> inds_c = 4 * inds_r % len(camera)\n>>> camera[inds_r, inds_c] = 0\n Masks are very useful when you need to select a set of pixels on which to perform the manipulations. The mask can be any boolean array of the same shape as the image (or a shape broadcastable to the image shape). This can be used to define a region of interest, for example, a disk: >>> nrows, ncols = camera.shape\n>>> row, col = np.ogrid[:nrows, :ncols]\n>>> cnt_row, cnt_col = nrows / 2, ncols / 2\n>>> outer_disk_mask = ((row - cnt_row)**2 + (col - cnt_col)**2 >\n...                    (nrows / 2)**2)\n>>> camera[outer_disk_mask] = 0\n  Boolean operations from NumPy can be used to define even more complex masks: >>> lower_half = row > cnt_row\n>>> lower_half_disk = np.logical_and(lower_half, outer_disk_mask)\n>>> camera = data.camera()\n>>> camera[lower_half_disk] = 0\n Color images All of the above remains true for color images. A color image is a NumPy array with an additional trailing dimension for the channels: >>> cat = data.chelsea()\n>>> type(cat)\n<type 'numpy.ndarray'>\n>>> cat.shape\n(300, 451, 3)\n This shows that cat is a 300-by-451 pixel image with three channels (red, green, and blue). As before, we can get and set the pixel values: >>> cat[10, 20]\narray([151, 129, 115], dtype=uint8)\n>>> # Set the pixel at (50th row, 60th column) to \"black\"\n>>> cat[50, 60] = 0\n>>> # set the pixel at (50th row, 61st column) to \"green\"\n>>> cat[50, 61] = [0, 255, 0]  # [red, green, blue]\n We can also use 2D boolean masks for 2D multichannel images, as we did with the grayscale image above: Using a 2D mask on a 2D color image >>> from skimage import data\n>>> cat = data.chelsea()\n>>> reddish = cat[:, :, 0] > 160\n>>> cat[reddish] = [0, 255, 0]\n>>> plt.imshow(cat)\n (Source code, png, pdf)    Coordinate conventions Because scikit-image represents images using NumPy arrays, the coordinate conventions must match. Two-dimensional (2D) grayscale images (such as camera above) are indexed by rows and columns (abbreviated to either (row, col) or (r, c)), with the lowest element (0, 0) at the top-left corner. In various parts of the library, you will also see rr and cc refer to lists of row and column coordinates. We distinguish this convention from (x, y), which commonly denote standard Cartesian coordinates, where x is the horizontal coordinate, y - the vertical one, and the origin is at the bottom left (Matplotlib axes, for example, use this convention). In the case of multichannel images, the last dimension is used for color channels and is denoted by channel or ch. Finally, for volumetric (3D) images, such as videos, magnetic resonance imaging (MRI) scans, confocal microscopy, etc. we refer to the leading dimension as plane, abbreviated as pln or p. These conventions are summarized below:  Dimension name and order conventions in scikit-image  \nImage type Coordinates   \n2D grayscale (row, col)  \n2D multichannel (eg. RGB) (row, col, ch)  \n3D grayscale (pln, row, col)  \n3D multichannel (pln, row, col, ch)   Many functions in scikit-image can operate on 3D images directly: >>> im3d = np.random.rand(100, 1000, 1000)\n>>> from skimage import morphology\n>>> from scipy import ndimage as ndi\n>>> seeds = ndi.label(im3d < 0.1)[0]\n>>> ws = morphology.watershed(im3d, seeds)\n In many cases, however, the third spatial dimension has lower resolution than the other two. Some scikit-image functions provide a spacing keyword argument to help handle this kind of data: >>> from skimage import segmentation\n>>> slics = segmentation.slic(im3d, spacing=[5, 1, 1], multichannel=False)\n Other times, the processing must be done plane-wise. When planes are stacked along the leading dimension (in agreement with our convention), the following syntax can be used: >>> from skimage import filters\n>>> edges = np.empty_like(im3d)\n>>> for pln, image in enumerate(im3d):\n...     # Iterate over the leading dimension\n...     edges[pln] = filters.sobel(image)\n Notes on the order of array dimensions Although the labeling of the axes might seem arbitrary, it can have a significant effect on the speed of operations. This is because modern processors never retrieve just one item from memory, but rather a whole chunk of adjacent items (an operation called prefetching). Therefore, processing of elements that are next to each other in memory is faster than processing them when they are scattered, even if the number of operations is the same: >>> def in_order_multiply(arr, scalar):\n...     for plane in list(range(arr.shape[0])):\n...         arr[plane, :, :] *= scalar\n...\n>>> def out_of_order_multiply(arr, scalar):\n...     for plane in list(range(arr.shape[2])):\n...         arr[:, :, plane] *= scalar\n...\n>>> import time\n>>> im3d = np.random.rand(100, 1024, 1024)\n>>> t0 = time.time(); x = in_order_multiply(im3d, 5); t1 = time.time()\n>>> print(\"%.2f seconds\" % (t1 - t0))  \n0.14 seconds\n>>> s0 = time.time(); x = out_of_order_multiply(im3d, 5); s1 = time.time()\n>>> print(\"%.2f seconds\" % (s1 - s0))  \n1.18 seconds\n>>> print(\"Speedup: %.1fx\" % ((s1 - s0) / (t1 - t0)))  \nSpeedup: 8.6x\n When the last/rightmost dimension becomes even larger the speedup is even more dramatic. It is worth thinking about data locality when developing algorithms. In particular, scikit-image uses C-contiguous arrays by default. When using nested loops, the last/rightmost dimension of the array should be in the innermost loop of the computation. In the example above, the *= numpy operator iterates over all remaining dimensions. A note on the time dimension Although scikit-image does not currently provide functions to work specifically with time-varying 3D data, its compatibility with NumPy arrays allows us to work quite naturally with a 5D array of the shape (t, pln, row, col, ch): >>> for timepoint in image5d:  \n...     # Each timepoint is a 3D multichannel image\n...     do_something_with(timepoint)\n We can then supplement the above table as follows:  Addendum to dimension names and orders in scikit-image  \nImage type coordinates   \n2D color video (t, row, col, ch)  \n3D multichannel video (t, pln, row, col, ch)  \n"}, {"name": "color", "path": "api/skimage.color", "type": "color", "text": "Module: color  \nskimage.color.combine_stains(stains, conv_matrix) Stain to RGB color space conversion.  \nskimage.color.convert_colorspace(arr, \u2026) Convert an image array to a new color space.  \nskimage.color.deltaE_cie76(lab1, lab2) Euclidean distance between two points in Lab color space  \nskimage.color.deltaE_ciede2000(lab1, lab2[, \u2026]) Color difference as given by the CIEDE 2000 standard.  \nskimage.color.deltaE_ciede94(lab1, lab2[, \u2026]) Color difference according to CIEDE 94 standard  \nskimage.color.deltaE_cmc(lab1, lab2[, kL, kC]) Color difference from the CMC l:c standard.  \nskimage.color.gray2rgb(image[, alpha]) Create an RGB representation of a gray-level image.  \nskimage.color.gray2rgba(image[, alpha]) Create a RGBA representation of a gray-level image.  \nskimage.color.grey2rgb(image[, alpha]) Create an RGB representation of a gray-level image.  \nskimage.color.hed2rgb(hed) Haematoxylin-Eosin-DAB (HED) to RGB color space conversion.  \nskimage.color.hsv2rgb(hsv) HSV to RGB color space conversion.  \nskimage.color.lab2lch(lab) CIE-LAB to CIE-LCH color space conversion.  \nskimage.color.lab2rgb(lab[, illuminant, \u2026]) Lab to RGB color space conversion.  \nskimage.color.lab2xyz(lab[, illuminant, \u2026]) CIE-LAB to XYZcolor space conversion.  \nskimage.color.label2rgb(label[, image, \u2026]) Return an RGB image where color-coded labels are painted over the image.  \nskimage.color.lch2lab(lch) CIE-LCH to CIE-LAB color space conversion.  \nskimage.color.rgb2gray(rgb) Compute luminance of an RGB image.  \nskimage.color.rgb2grey(rgb) Compute luminance of an RGB image.  \nskimage.color.rgb2hed(rgb) RGB to Haematoxylin-Eosin-DAB (HED) color space conversion.  \nskimage.color.rgb2hsv(rgb) RGB to HSV color space conversion.  \nskimage.color.rgb2lab(rgb[, illuminant, \u2026]) Conversion from the sRGB color space (IEC 61966-2-1:1999) to the CIE Lab colorspace under the given illuminant and observer.  \nskimage.color.rgb2rgbcie(rgb) RGB to RGB CIE color space conversion.  \nskimage.color.rgb2xyz(rgb) RGB to XYZ color space conversion.  \nskimage.color.rgb2ycbcr(rgb) RGB to YCbCr color space conversion.  \nskimage.color.rgb2ydbdr(rgb) RGB to YDbDr color space conversion.  \nskimage.color.rgb2yiq(rgb) RGB to YIQ color space conversion.  \nskimage.color.rgb2ypbpr(rgb) RGB to YPbPr color space conversion.  \nskimage.color.rgb2yuv(rgb) RGB to YUV color space conversion.  \nskimage.color.rgba2rgb(rgba[, background]) RGBA to RGB conversion using alpha blending [1].  \nskimage.color.rgbcie2rgb(rgbcie) RGB CIE to RGB color space conversion.  \nskimage.color.separate_stains(rgb, conv_matrix) RGB to stain color space conversion.  \nskimage.color.xyz2lab(xyz[, illuminant, \u2026]) XYZ to CIE-LAB color space conversion.  \nskimage.color.xyz2rgb(xyz) XYZ to RGB color space conversion.  \nskimage.color.ycbcr2rgb(ycbcr) YCbCr to RGB color space conversion.  \nskimage.color.ydbdr2rgb(ydbdr) YDbDr to RGB color space conversion.  \nskimage.color.yiq2rgb(yiq) YIQ to RGB color space conversion.  \nskimage.color.ypbpr2rgb(ypbpr) YPbPr to RGB color space conversion.  \nskimage.color.yuv2rgb(yuv) YUV to RGB color space conversion.   combine_stains  \nskimage.color.combine_stains(stains, conv_matrix) [source]\n \nStain to RGB color space conversion.  Parameters \n \nstains(\u2026, 3) array_like \n\nThe image in stain color space. Final dimension denotes channels.  conv_matrix: ndarray\n\nThe stain separation matrix as described by G. Landini [1].    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf stains is not at least 2-D with shape (\u2026, 3).     Notes Stain combination matrices available in the color module and their respective colorspace:  \nrgb_from_hed: Hematoxylin + Eosin + DAB \nrgb_from_hdx: Hematoxylin + DAB \nrgb_from_fgx: Feulgen + Light Green \nrgb_from_bex: Giemsa stain : Methyl Blue + Eosin \nrgb_from_rbd: FastRed + FastBlue + DAB \nrgb_from_gdx: Methyl Green + DAB \nrgb_from_hax: Hematoxylin + AEC \nrgb_from_bro: Blue matrix Anilline Blue + Red matrix Azocarmine + Orange matrix Orange-G \nrgb_from_bpx: Methyl Blue + Ponceau Fuchsin \nrgb_from_ahx: Alcian Blue + Hematoxylin \nrgb_from_hpx: Hematoxylin + PAS  References  \n1  \nhttps://web.archive.org/web/20160624145052/http://www.mecourse.com/landinig/software/cdeconv/cdeconv.html  \n2  \nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution,\u201d Anal. Quant. Cytol. Histol., vol. 23, no. 4, pp. 291\u2013299, Aug. 2001.   Examples >>> from skimage import data\n>>> from skimage.color import (separate_stains, combine_stains,\n...                            hdx_from_rgb, rgb_from_hdx)\n>>> ihc = data.immunohistochemistry()\n>>> ihc_hdx = separate_stains(ihc, hdx_from_rgb)\n>>> ihc_rgb = combine_stains(ihc_hdx, rgb_from_hdx)\n \n convert_colorspace  \nskimage.color.convert_colorspace(arr, fromspace, tospace) [source]\n \nConvert an image array to a new color space.  Valid color spaces are:\n\n\u2018RGB\u2019, \u2018HSV\u2019, \u2018RGB CIE\u2019, \u2018XYZ\u2019, \u2018YUV\u2019, \u2018YIQ\u2019, \u2018YPbPr\u2019, \u2018YCbCr\u2019, \u2018YDbDr\u2019    Parameters \n \narr(\u2026, 3) array_like \n\nThe image to convert. Final dimension denotes channels.  \nfromspacestr \n\nThe color space to convert from. Can be specified in lower case.  \ntospacestr \n\nThe color space to convert to. Can be specified in lower case.    Returns \n \nout(\u2026, 3) ndarray \n\nThe converted image. Same dimensions as input.    Raises \n ValueError\n\nIf fromspace is not a valid color space  ValueError\n\nIf tospace is not a valid color space     Notes Conversion is performed through the \u201ccentral\u201d RGB color space, i.e. conversion from XYZ to HSV is implemented as XYZ -> RGB -> HSV instead of directly. Examples >>> from skimage import data\n>>> img = data.astronaut()\n>>> img_hsv = convert_colorspace(img, 'RGB', 'HSV')\n \n deltaE_cie76  \nskimage.color.deltaE_cie76(lab1, lab2) [source]\n \nEuclidean distance between two points in Lab color space  Parameters \n \nlab1array_like \n\nreference color (Lab colorspace)  \nlab2array_like \n\ncomparison color (Lab colorspace)    Returns \n \ndEarray_like \n\ndistance between colors lab1 and lab2     References  \n1  \nhttps://en.wikipedia.org/wiki/Color_difference  \n2  \nA. R. Robertson, \u201cThe CIE 1976 color-difference formulae,\u201d Color Res. Appl. 2, 7-11 (1977).   \n deltaE_ciede2000  \nskimage.color.deltaE_ciede2000(lab1, lab2, kL=1, kC=1, kH=1) [source]\n \nColor difference as given by the CIEDE 2000 standard. CIEDE 2000 is a major revision of CIDE94. The perceptual calibration is largely based on experience with automotive paint on smooth surfaces.  Parameters \n \nlab1array_like \n\nreference color (Lab colorspace)  \nlab2array_like \n\ncomparison color (Lab colorspace)  \nkLfloat (range), optional \n\nlightness scale factor, 1 for \u201cacceptably close\u201d; 2 for \u201cimperceptible\u201d see deltaE_cmc  \nkCfloat (range), optional \n\nchroma scale factor, usually 1  \nkHfloat (range), optional \n\nhue scale factor, usually 1    Returns \n \ndeltaEarray_like \n\nThe distance between lab1 and lab2     Notes CIEDE 2000 assumes parametric weighting factors for the lightness, chroma, and hue (kL, kC, kH respectively). These default to 1. References  \n1  \nhttps://en.wikipedia.org/wiki/Color_difference  \n2  \nhttp://www.ece.rochester.edu/~gsharma/ciede2000/ciede2000noteCRNA.pdf DOI:10.1364/AO.33.008069  \n3  \nM. Melgosa, J. Quesada, and E. Hita, \u201cUniformity of some recent color metrics tested with an accurate color-difference tolerance dataset,\u201d Appl. Opt. 33, 8069-8077 (1994).   \n deltaE_ciede94  \nskimage.color.deltaE_ciede94(lab1, lab2, kH=1, kC=1, kL=1, k1=0.045, k2=0.015) [source]\n \nColor difference according to CIEDE 94 standard Accommodates perceptual non-uniformities through the use of application specific scale factors (kH, kC, kL, k1, and k2).  Parameters \n \nlab1array_like \n\nreference color (Lab colorspace)  \nlab2array_like \n\ncomparison color (Lab colorspace)  \nkHfloat, optional \n\nHue scale  \nkCfloat, optional \n\nChroma scale  \nkLfloat, optional \n\nLightness scale  \nk1float, optional \n\nfirst scale parameter  \nk2float, optional \n\nsecond scale parameter    Returns \n \ndEarray_like \n\ncolor difference between lab1 and lab2     Notes deltaE_ciede94 is not symmetric with respect to lab1 and lab2. CIEDE94 defines the scales for the lightness, hue, and chroma in terms of the first color. Consequently, the first color should be regarded as the \u201creference\u201d color. kL, k1, k2 depend on the application and default to the values suggested for graphic arts   \nParameter Graphic Arts Textiles   \nkL 1.000 2.000  \nk1 0.045 0.048  \nk2 0.015 0.014   References  \n1  \nhttps://en.wikipedia.org/wiki/Color_difference  \n2  \nhttp://www.brucelindbloom.com/index.html?Eqn_DeltaE_CIE94.html   \n deltaE_cmc  \nskimage.color.deltaE_cmc(lab1, lab2, kL=1, kC=1) [source]\n \nColor difference from the CMC l:c standard. This color difference was developed by the Colour Measurement Committee (CMC) of the Society of Dyers and Colourists (United Kingdom). It is intended for use in the textile industry. The scale factors kL, kC set the weight given to differences in lightness and chroma relative to differences in hue. The usual values are kL=2, kC=1 for \u201cacceptability\u201d and kL=1, kC=1 for \u201cimperceptibility\u201d. Colors with dE > 1 are \u201cdifferent\u201d for the given scale factors.  Parameters \n \nlab1array_like \n\nreference color (Lab colorspace)  \nlab2array_like \n\ncomparison color (Lab colorspace)    Returns \n \ndEarray_like \n\ndistance between colors lab1 and lab2     Notes deltaE_cmc the defines the scales for the lightness, hue, and chroma in terms of the first color. Consequently deltaE_cmc(lab1, lab2) != deltaE_cmc(lab2, lab1) References  \n1  \nhttps://en.wikipedia.org/wiki/Color_difference  \n2  \nhttp://www.brucelindbloom.com/index.html?Eqn_DeltaE_CIE94.html  \n3  \nF. J. J. Clarke, R. McDonald, and B. Rigg, \u201cModification to the JPC79 colour-difference formula,\u201d J. Soc. Dyers Colour. 100, 128-132 (1984).   \n gray2rgb  \nskimage.color.gray2rgb(image, alpha=None) [source]\n \nCreate an RGB representation of a gray-level image.  Parameters \n \nimagearray_like \n\nInput image.  \nalphabool, optional \n\nEnsure that the output image has an alpha layer. If None, alpha layers are passed through but not created.    Returns \n \nrgb(\u2026, 3) ndarray \n\nRGB image. A new dimension of length 3 is added to input image.     Notes If the input is a 1-dimensional image of shape (M, ), the output will be shape (M, 3). \n Examples using skimage.color.gray2rgb\n \n  Tinting gray-scale images   gray2rgba  \nskimage.color.gray2rgba(image, alpha=None) [source]\n \nCreate a RGBA representation of a gray-level image.  Parameters \n \nimagearray_like \n\nInput image.  \nalphaarray_like, optional \n\nAlpha channel of the output image. It may be a scalar or an array that can be broadcast to image. If not specified it is set to the maximum limit corresponding to the image dtype.    Returns \n \nrgbandarray \n\nRGBA image. A new dimension of length 4 is added to input image shape.     \n grey2rgb  \nskimage.color.grey2rgb(image, alpha=None) [source]\n \nCreate an RGB representation of a gray-level image.  Parameters \n \nimagearray_like \n\nInput image.  \nalphabool, optional \n\nEnsure that the output image has an alpha layer. If None, alpha layers are passed through but not created.    Returns \n \nrgb(\u2026, 3) ndarray \n\nRGB image. A new dimension of length 3 is added to input image.     Notes If the input is a 1-dimensional image of shape (M, ), the output will be shape (M, 3). \n hed2rgb  \nskimage.color.hed2rgb(hed) [source]\n \nHaematoxylin-Eosin-DAB (HED) to RGB color space conversion.  Parameters \n \nhed(\u2026, 3) array_like \n\nThe image in the HED color space. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB. Same dimensions as input.    Raises \n ValueError\n\nIf hed is not at least 2-D with shape (\u2026, 3).     References  \n1  \nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution.,\u201d Analytical and quantitative cytology and histology / the International Academy of Cytology [and] American Society of Cytology, vol. 23, no. 4, pp. 291-9, Aug. 2001.   Examples >>> from skimage import data\n>>> from skimage.color import rgb2hed, hed2rgb\n>>> ihc = data.immunohistochemistry()\n>>> ihc_hed = rgb2hed(ihc)\n>>> ihc_rgb = hed2rgb(ihc_hed)\n \n hsv2rgb  \nskimage.color.hsv2rgb(hsv) [source]\n \nHSV to RGB color space conversion.  Parameters \n \nhsv(\u2026, 3) array_like \n\nThe image in HSV format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf hsv is not at least 2-D with shape (\u2026, 3).     Notes Conversion between RGB and HSV color spaces results in some loss of precision, due to integer arithmetic and rounding [1]. References  \n1  \nhttps://en.wikipedia.org/wiki/HSL_and_HSV   Examples >>> from skimage import data\n>>> img = data.astronaut()\n>>> img_hsv = rgb2hsv(img)\n>>> img_rgb = hsv2rgb(img_hsv)\n \n Examples using skimage.color.hsv2rgb\n \n  Tinting gray-scale images  \n\n  Flood Fill   lab2lch  \nskimage.color.lab2lch(lab) [source]\n \nCIE-LAB to CIE-LCH color space conversion. LCH is the cylindrical representation of the LAB (Cartesian) colorspace  Parameters \n \nlab(\u2026, 3) array_like \n\nThe N-D image in CIE-LAB format. The last (N+1-th) dimension must have at least 3 elements, corresponding to the L, a, and b color channels. Subsequent elements are copied.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in LCH format, in a N-D array with same shape as input lab.    Raises \n ValueError\n\nIf lch does not have at least 3 color channels (i.e. l, a, b).     Notes The Hue is expressed as an angle between (0, 2*pi) Examples >>> from skimage import data\n>>> from skimage.color import rgb2lab, lab2lch\n>>> img = data.astronaut()\n>>> img_lab = rgb2lab(img)\n>>> img_lch = lab2lch(img_lab)\n \n lab2rgb  \nskimage.color.lab2rgb(lab, illuminant='D65', observer='2') [source]\n \nLab to RGB color space conversion.  Parameters \n \nlab(\u2026, 3) array_like \n\nThe image in Lab format. Final dimension denotes channels.  \nilluminant{\u201cA\u201d, \u201cD50\u201d, \u201cD55\u201d, \u201cD65\u201d, \u201cD75\u201d, \u201cE\u201d}, optional \n\nThe name of the illuminant (the function is NOT case sensitive).  \nobserver{\u201c2\u201d, \u201c10\u201d}, optional \n\nThe aperture angle of the observer.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf lab is not at least 2-D with shape (\u2026, 3).     Notes This function uses lab2xyz and xyz2rgb. By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref=95.047, y_ref=100., z_ref=108.883. See function get_xyz_coords for a list of supported illuminants. References  \n1  \nhttps://en.wikipedia.org/wiki/Standard_illuminant   \n lab2xyz  \nskimage.color.lab2xyz(lab, illuminant='D65', observer='2') [source]\n \nCIE-LAB to XYZcolor space conversion.  Parameters \n \nlab(\u2026, 3) array_like \n\nThe image in Lab format. Final dimension denotes channels.  \nilluminant{\u201cA\u201d, \u201cD50\u201d, \u201cD55\u201d, \u201cD65\u201d, \u201cD75\u201d, \u201cE\u201d}, optional \n\nThe name of the illuminant (the function is NOT case sensitive).  \nobserver{\u201c2\u201d, \u201c10\u201d}, optional \n\nThe aperture angle of the observer.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in XYZ format. Same dimensions as input.    Raises \n ValueError\n\nIf lab is not at least 2-D with shape (\u2026, 3).  ValueError\n\nIf either the illuminant or the observer angle are not supported or unknown.  UserWarning\n\nIf any of the pixels are invalid (Z < 0).     Notes By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref = 95.047, y_ref = 100., z_ref = 108.883. See function \u2018get_xyz_coords\u2019 for a list of supported illuminants. References  \n1  \nhttp://www.easyrgb.com/index.php?X=MATH&H=07  \n2  \nhttps://en.wikipedia.org/wiki/Lab_color_space   \n label2rgb  \nskimage.color.label2rgb(label, image=None, colors=None, alpha=0.3, bg_label=-1, bg_color=(0, 0, 0), image_alpha=1, kind='overlay') [source]\n \nReturn an RGB image where color-coded labels are painted over the image.  Parameters \n \nlabelarray, shape (M, N) \n\nInteger array of labels with the same shape as image.  \nimagearray, shape (M, N, 3), optional \n\nImage used as underlay for labels. If the input is an RGB image, it\u2019s converted to grayscale before coloring.  \ncolorslist, optional \n\nList of colors. If the number of labels exceeds the number of colors, then the colors are cycled.  \nalphafloat [0, 1], optional \n\nOpacity of colorized labels. Ignored if image is None.  \nbg_labelint, optional \n\nLabel that\u2019s treated as the background. If bg_label is specified, bg_color is None, and kind is overlay, background is not painted by any colors.  \nbg_colorstr or array, optional \n\nBackground color. Must be a name in color_dict or RGB float values between [0, 1].  \nimage_alphafloat [0, 1], optional \n\nOpacity of the image.  \nkindstring, one of {\u2018overlay\u2019, \u2018avg\u2019} \n\nThe kind of color image desired. \u2018overlay\u2019 cycles over defined colors and overlays the colored labels over the original image. \u2018avg\u2019 replaces each labeled segment with its average color, for a stained-class or pastel painting appearance.    Returns \n \nresultarray of float, shape (M, N, 3) \n\nThe result of blending a cycling colormap (colors) for each distinct value in label with the image, at a certain alpha value.     \n Examples using skimage.color.label2rgb\n \n  Segment human cells (in mitosis)   lch2lab  \nskimage.color.lch2lab(lch) [source]\n \nCIE-LCH to CIE-LAB color space conversion. LCH is the cylindrical representation of the LAB (Cartesian) colorspace  Parameters \n \nlch(\u2026, 3) array_like \n\nThe N-D image in CIE-LCH format. The last (N+1-th) dimension must have at least 3 elements, corresponding to the L, a, and b color channels. Subsequent elements are copied.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in LAB format, with same shape as input lch.    Raises \n ValueError\n\nIf lch does not have at least 3 color channels (i.e. l, c, h).     Examples >>> from skimage import data\n>>> from skimage.color import rgb2lab, lch2lab\n>>> img = data.astronaut()\n>>> img_lab = rgb2lab(img)\n>>> img_lch = lab2lch(img_lab)\n>>> img_lab2 = lch2lab(img_lch)\n \n rgb2gray  \nskimage.color.rgb2gray(rgb) [source]\n \nCompute luminance of an RGB image.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \noutndarray \n\nThe luminance image - an array which is the same size as the input array, but with the channel dimension removed.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes The weights used in this conversion are calibrated for contemporary CRT phosphors: Y = 0.2125 R + 0.7154 G + 0.0721 B\n If there is an alpha channel present, it is ignored. References  \n1  \nhttp://poynton.ca/PDFs/ColorFAQ.pdf   Examples >>> from skimage.color import rgb2gray\n>>> from skimage import data\n>>> img = data.astronaut()\n>>> img_gray = rgb2gray(img)\n \n Examples using skimage.color.rgb2gray\n \n  Registration using optical flow  \n\n  Phase Unwrapping   rgb2grey  \nskimage.color.rgb2grey(rgb) [source]\n \nCompute luminance of an RGB image.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \noutndarray \n\nThe luminance image - an array which is the same size as the input array, but with the channel dimension removed.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes The weights used in this conversion are calibrated for contemporary CRT phosphors: Y = 0.2125 R + 0.7154 G + 0.0721 B\n If there is an alpha channel present, it is ignored. References  \n1  \nhttp://poynton.ca/PDFs/ColorFAQ.pdf   Examples >>> from skimage.color import rgb2gray\n>>> from skimage import data\n>>> img = data.astronaut()\n>>> img_gray = rgb2gray(img)\n \n rgb2hed  \nskimage.color.rgb2hed(rgb) [source]\n \nRGB to Haematoxylin-Eosin-DAB (HED) color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in HED format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     References  \n1  \nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution.,\u201d Analytical and quantitative cytology and histology / the International Academy of Cytology [and] American Society of Cytology, vol. 23, no. 4, pp. 291-9, Aug. 2001.   Examples >>> from skimage import data\n>>> from skimage.color import rgb2hed\n>>> ihc = data.immunohistochemistry()\n>>> ihc_hed = rgb2hed(ihc)\n \n rgb2hsv  \nskimage.color.rgb2hsv(rgb) [source]\n \nRGB to HSV color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in HSV format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes Conversion between RGB and HSV color spaces results in some loss of precision, due to integer arithmetic and rounding [1]. References  \n1  \nhttps://en.wikipedia.org/wiki/HSL_and_HSV   Examples >>> from skimage import color\n>>> from skimage import data\n>>> img = data.astronaut()\n>>> img_hsv = color.rgb2hsv(img)\n \n Examples using skimage.color.rgb2hsv\n \n  Tinting gray-scale images  \n\n  Flood Fill   rgb2lab  \nskimage.color.rgb2lab(rgb, illuminant='D65', observer='2') [source]\n \nConversion from the sRGB color space (IEC 61966-2-1:1999) to the CIE Lab colorspace under the given illuminant and observer.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.  \nilluminant{\u201cA\u201d, \u201cD50\u201d, \u201cD55\u201d, \u201cD65\u201d, \u201cD75\u201d, \u201cE\u201d}, optional \n\nThe name of the illuminant (the function is NOT case sensitive).  \nobserver{\u201c2\u201d, \u201c10\u201d}, optional \n\nThe aperture angle of the observer.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in Lab format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes RGB is a device-dependent color space so, if you use this function, be sure that the image you are analyzing has been mapped to the sRGB color space. This function uses rgb2xyz and xyz2lab. By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref=95.047, y_ref=100., z_ref=108.883. See function get_xyz_coords for a list of supported illuminants. References  \n1  \nhttps://en.wikipedia.org/wiki/Standard_illuminant   \n rgb2rgbcie  \nskimage.color.rgb2rgbcie(rgb) [source]\n \nRGB to RGB CIE color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB CIE format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     References  \n1  \nhttps://en.wikipedia.org/wiki/CIE_1931_color_space   Examples >>> from skimage import data\n>>> from skimage.color import rgb2rgbcie\n>>> img = data.astronaut()\n>>> img_rgbcie = rgb2rgbcie(img)\n \n rgb2xyz  \nskimage.color.rgb2xyz(rgb) [source]\n \nRGB to XYZ color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in XYZ format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes The CIE XYZ color space is derived from the CIE RGB color space. Note however that this function converts from sRGB. References  \n1  \nhttps://en.wikipedia.org/wiki/CIE_1931_color_space   Examples >>> from skimage import data\n>>> img = data.astronaut()\n>>> img_xyz = rgb2xyz(img)\n \n rgb2ycbcr  \nskimage.color.rgb2ycbcr(rgb) [source]\n \nRGB to YCbCr color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in YCbCr format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes Y is between 16 and 235. This is the color space commonly used by video codecs; it is sometimes incorrectly called \u201cYUV\u201d. References  \n1  \nhttps://en.wikipedia.org/wiki/YCbCr   \n rgb2ydbdr  \nskimage.color.rgb2ydbdr(rgb) [source]\n \nRGB to YDbDr color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in YDbDr format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes This is the color space commonly used by video codecs. It is also the reversible color transform in JPEG2000. References  \n1  \nhttps://en.wikipedia.org/wiki/YDbDr   \n rgb2yiq  \nskimage.color.rgb2yiq(rgb) [source]\n \nRGB to YIQ color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in YIQ format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     \n rgb2ypbpr  \nskimage.color.rgb2ypbpr(rgb) [source]\n \nRGB to YPbPr color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in YPbPr format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     References  \n1  \nhttps://en.wikipedia.org/wiki/YPbPr   \n rgb2yuv  \nskimage.color.rgb2yuv(rgb) [source]\n \nRGB to YUV color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in YUV format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes Y is between 0 and 1. Use YCbCr instead of YUV for the color space commonly used by video codecs, where Y ranges from 16 to 235. References  \n1  \nhttps://en.wikipedia.org/wiki/YUV   \n rgba2rgb  \nskimage.color.rgba2rgb(rgba, background=(1, 1, 1)) [source]\n \nRGBA to RGB conversion using alpha blending [1].  Parameters \n \nrgba(\u2026, 4) array_like \n\nThe image in RGBA format. Final dimension denotes channels.  \nbackgroundarray_like \n\nThe color of the background to blend the image with (3 floats between 0 to 1 - the RGB value of the background).    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf rgba is not at least 2-D with shape (\u2026, 4).     References  \n1(1,2)  \nhttps://en.wikipedia.org/wiki/Alpha_compositing#Alpha_blending   Examples >>> from skimage import color\n>>> from skimage import data\n>>> img_rgba = data.logo()\n>>> img_rgb = color.rgba2rgb(img_rgba)\n \n rgbcie2rgb  \nskimage.color.rgbcie2rgb(rgbcie) [source]\n \nRGB CIE to RGB color space conversion.  Parameters \n \nrgbcie(\u2026, 3) array_like \n\nThe image in RGB CIE format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf rgbcie is not at least 2-D with shape (\u2026, 3).     References  \n1  \nhttps://en.wikipedia.org/wiki/CIE_1931_color_space   Examples >>> from skimage import data\n>>> from skimage.color import rgb2rgbcie, rgbcie2rgb\n>>> img = data.astronaut()\n>>> img_rgbcie = rgb2rgbcie(img)\n>>> img_rgb = rgbcie2rgb(img_rgbcie)\n \n separate_stains  \nskimage.color.separate_stains(rgb, conv_matrix) [source]\n \nRGB to stain color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.  conv_matrix: ndarray\n\nThe stain separation matrix as described by G. Landini [1].    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in stain color space. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes Stain separation matrices available in the color module and their respective colorspace:  \nhed_from_rgb: Hematoxylin + Eosin + DAB \nhdx_from_rgb: Hematoxylin + DAB \nfgx_from_rgb: Feulgen + Light Green \nbex_from_rgb: Giemsa stain : Methyl Blue + Eosin \nrbd_from_rgb: FastRed + FastBlue + DAB \ngdx_from_rgb: Methyl Green + DAB \nhax_from_rgb: Hematoxylin + AEC \nbro_from_rgb: Blue matrix Anilline Blue + Red matrix Azocarmine + Orange matrix Orange-G \nbpx_from_rgb: Methyl Blue + Ponceau Fuchsin \nahx_from_rgb: Alcian Blue + Hematoxylin \nhpx_from_rgb: Hematoxylin + PAS  This implementation borrows some ideas from DIPlib [2], e.g. the compensation using a small value to avoid log artifacts when calculating the Beer-Lambert law. References  \n1  \nhttps://web.archive.org/web/20160624145052/http://www.mecourse.com/landinig/software/cdeconv/cdeconv.html  \n2  \nhttps://github.com/DIPlib/diplib/  \n3  \nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution,\u201d Anal. Quant. Cytol. Histol., vol. 23, no. 4, pp. 291\u2013299, Aug. 2001.   Examples >>> from skimage import data\n>>> from skimage.color import separate_stains, hdx_from_rgb\n>>> ihc = data.immunohistochemistry()\n>>> ihc_hdx = separate_stains(ihc, hdx_from_rgb)\n \n xyz2lab  \nskimage.color.xyz2lab(xyz, illuminant='D65', observer='2') [source]\n \nXYZ to CIE-LAB color space conversion.  Parameters \n \nxyz(\u2026, 3) array_like \n\nThe image in XYZ format. Final dimension denotes channels.  \nilluminant{\u201cA\u201d, \u201cD50\u201d, \u201cD55\u201d, \u201cD65\u201d, \u201cD75\u201d, \u201cE\u201d}, optional \n\nThe name of the illuminant (the function is NOT case sensitive).  \nobserver{\u201c2\u201d, \u201c10\u201d}, optional \n\nThe aperture angle of the observer.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in CIE-LAB format. Same dimensions as input.    Raises \n ValueError\n\nIf xyz is not at least 2-D with shape (\u2026, 3).  ValueError\n\nIf either the illuminant or the observer angle is unsupported or unknown.     Notes By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref=95.047, y_ref=100., z_ref=108.883. See function get_xyz_coords for a list of supported illuminants. References  \n1  \nhttp://www.easyrgb.com/index.php?X=MATH&H=07  \n2  \nhttps://en.wikipedia.org/wiki/Lab_color_space   Examples >>> from skimage import data\n>>> from skimage.color import rgb2xyz, xyz2lab\n>>> img = data.astronaut()\n>>> img_xyz = rgb2xyz(img)\n>>> img_lab = xyz2lab(img_xyz)\n \n xyz2rgb  \nskimage.color.xyz2rgb(xyz) [source]\n \nXYZ to RGB color space conversion.  Parameters \n \nxyz(\u2026, 3) array_like \n\nThe image in XYZ format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf xyz is not at least 2-D with shape (\u2026, 3).     Notes The CIE XYZ color space is derived from the CIE RGB color space. Note however that this function converts to sRGB. References  \n1  \nhttps://en.wikipedia.org/wiki/CIE_1931_color_space   Examples >>> from skimage import data\n>>> from skimage.color import rgb2xyz, xyz2rgb\n>>> img = data.astronaut()\n>>> img_xyz = rgb2xyz(img)\n>>> img_rgb = xyz2rgb(img_xyz)\n \n ycbcr2rgb  \nskimage.color.ycbcr2rgb(ycbcr) [source]\n \nYCbCr to RGB color space conversion.  Parameters \n \nycbcr(\u2026, 3) array_like \n\nThe image in YCbCr format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf ycbcr is not at least 2-D with shape (\u2026, 3).     Notes Y is between 16 and 235. This is the color space commonly used by video codecs; it is sometimes incorrectly called \u201cYUV\u201d. References  \n1  \nhttps://en.wikipedia.org/wiki/YCbCr   \n ydbdr2rgb  \nskimage.color.ydbdr2rgb(ydbdr) [source]\n \nYDbDr to RGB color space conversion.  Parameters \n \nydbdr(\u2026, 3) array_like \n\nThe image in YDbDr format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf ydbdr is not at least 2-D with shape (\u2026, 3).     Notes This is the color space commonly used by video codecs, also called the reversible color transform in JPEG2000. References  \n1  \nhttps://en.wikipedia.org/wiki/YDbDr   \n yiq2rgb  \nskimage.color.yiq2rgb(yiq) [source]\n \nYIQ to RGB color space conversion.  Parameters \n \nyiq(\u2026, 3) array_like \n\nThe image in YIQ format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf yiq is not at least 2-D with shape (\u2026, 3).     \n ypbpr2rgb  \nskimage.color.ypbpr2rgb(ypbpr) [source]\n \nYPbPr to RGB color space conversion.  Parameters \n \nypbpr(\u2026, 3) array_like \n\nThe image in YPbPr format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf ypbpr is not at least 2-D with shape (\u2026, 3).     References  \n1  \nhttps://en.wikipedia.org/wiki/YPbPr   \n yuv2rgb  \nskimage.color.yuv2rgb(yuv) [source]\n \nYUV to RGB color space conversion.  Parameters \n \nyuv(\u2026, 3) array_like \n\nThe image in YUV format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf yuv is not at least 2-D with shape (\u2026, 3).     References  \n1  \nhttps://en.wikipedia.org/wiki/YUV   \n\n"}, {"name": "color.combine_stains()", "path": "api/skimage.color#skimage.color.combine_stains", "type": "color", "text": " \nskimage.color.combine_stains(stains, conv_matrix) [source]\n \nStain to RGB color space conversion.  Parameters \n \nstains(\u2026, 3) array_like \n\nThe image in stain color space. Final dimension denotes channels.  conv_matrix: ndarray\n\nThe stain separation matrix as described by G. Landini [1].    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf stains is not at least 2-D with shape (\u2026, 3).     Notes Stain combination matrices available in the color module and their respective colorspace:  \nrgb_from_hed: Hematoxylin + Eosin + DAB \nrgb_from_hdx: Hematoxylin + DAB \nrgb_from_fgx: Feulgen + Light Green \nrgb_from_bex: Giemsa stain : Methyl Blue + Eosin \nrgb_from_rbd: FastRed + FastBlue + DAB \nrgb_from_gdx: Methyl Green + DAB \nrgb_from_hax: Hematoxylin + AEC \nrgb_from_bro: Blue matrix Anilline Blue + Red matrix Azocarmine + Orange matrix Orange-G \nrgb_from_bpx: Methyl Blue + Ponceau Fuchsin \nrgb_from_ahx: Alcian Blue + Hematoxylin \nrgb_from_hpx: Hematoxylin + PAS  References  \n1  \nhttps://web.archive.org/web/20160624145052/http://www.mecourse.com/landinig/software/cdeconv/cdeconv.html  \n2  \nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution,\u201d Anal. Quant. Cytol. Histol., vol. 23, no. 4, pp. 291\u2013299, Aug. 2001.   Examples >>> from skimage import data\n>>> from skimage.color import (separate_stains, combine_stains,\n...                            hdx_from_rgb, rgb_from_hdx)\n>>> ihc = data.immunohistochemistry()\n>>> ihc_hdx = separate_stains(ihc, hdx_from_rgb)\n>>> ihc_rgb = combine_stains(ihc_hdx, rgb_from_hdx)\n \n"}, {"name": "color.convert_colorspace()", "path": "api/skimage.color#skimage.color.convert_colorspace", "type": "color", "text": " \nskimage.color.convert_colorspace(arr, fromspace, tospace) [source]\n \nConvert an image array to a new color space.  Valid color spaces are:\n\n\u2018RGB\u2019, \u2018HSV\u2019, \u2018RGB CIE\u2019, \u2018XYZ\u2019, \u2018YUV\u2019, \u2018YIQ\u2019, \u2018YPbPr\u2019, \u2018YCbCr\u2019, \u2018YDbDr\u2019    Parameters \n \narr(\u2026, 3) array_like \n\nThe image to convert. Final dimension denotes channels.  \nfromspacestr \n\nThe color space to convert from. Can be specified in lower case.  \ntospacestr \n\nThe color space to convert to. Can be specified in lower case.    Returns \n \nout(\u2026, 3) ndarray \n\nThe converted image. Same dimensions as input.    Raises \n ValueError\n\nIf fromspace is not a valid color space  ValueError\n\nIf tospace is not a valid color space     Notes Conversion is performed through the \u201ccentral\u201d RGB color space, i.e. conversion from XYZ to HSV is implemented as XYZ -> RGB -> HSV instead of directly. Examples >>> from skimage import data\n>>> img = data.astronaut()\n>>> img_hsv = convert_colorspace(img, 'RGB', 'HSV')\n \n"}, {"name": "color.deltaE_cie76()", "path": "api/skimage.color#skimage.color.deltaE_cie76", "type": "color", "text": " \nskimage.color.deltaE_cie76(lab1, lab2) [source]\n \nEuclidean distance between two points in Lab color space  Parameters \n \nlab1array_like \n\nreference color (Lab colorspace)  \nlab2array_like \n\ncomparison color (Lab colorspace)    Returns \n \ndEarray_like \n\ndistance between colors lab1 and lab2     References  \n1  \nhttps://en.wikipedia.org/wiki/Color_difference  \n2  \nA. R. Robertson, \u201cThe CIE 1976 color-difference formulae,\u201d Color Res. Appl. 2, 7-11 (1977).   \n"}, {"name": "color.deltaE_ciede2000()", "path": "api/skimage.color#skimage.color.deltaE_ciede2000", "type": "color", "text": " \nskimage.color.deltaE_ciede2000(lab1, lab2, kL=1, kC=1, kH=1) [source]\n \nColor difference as given by the CIEDE 2000 standard. CIEDE 2000 is a major revision of CIDE94. The perceptual calibration is largely based on experience with automotive paint on smooth surfaces.  Parameters \n \nlab1array_like \n\nreference color (Lab colorspace)  \nlab2array_like \n\ncomparison color (Lab colorspace)  \nkLfloat (range), optional \n\nlightness scale factor, 1 for \u201cacceptably close\u201d; 2 for \u201cimperceptible\u201d see deltaE_cmc  \nkCfloat (range), optional \n\nchroma scale factor, usually 1  \nkHfloat (range), optional \n\nhue scale factor, usually 1    Returns \n \ndeltaEarray_like \n\nThe distance between lab1 and lab2     Notes CIEDE 2000 assumes parametric weighting factors for the lightness, chroma, and hue (kL, kC, kH respectively). These default to 1. References  \n1  \nhttps://en.wikipedia.org/wiki/Color_difference  \n2  \nhttp://www.ece.rochester.edu/~gsharma/ciede2000/ciede2000noteCRNA.pdf DOI:10.1364/AO.33.008069  \n3  \nM. Melgosa, J. Quesada, and E. Hita, \u201cUniformity of some recent color metrics tested with an accurate color-difference tolerance dataset,\u201d Appl. Opt. 33, 8069-8077 (1994).   \n"}, {"name": "color.deltaE_ciede94()", "path": "api/skimage.color#skimage.color.deltaE_ciede94", "type": "color", "text": " \nskimage.color.deltaE_ciede94(lab1, lab2, kH=1, kC=1, kL=1, k1=0.045, k2=0.015) [source]\n \nColor difference according to CIEDE 94 standard Accommodates perceptual non-uniformities through the use of application specific scale factors (kH, kC, kL, k1, and k2).  Parameters \n \nlab1array_like \n\nreference color (Lab colorspace)  \nlab2array_like \n\ncomparison color (Lab colorspace)  \nkHfloat, optional \n\nHue scale  \nkCfloat, optional \n\nChroma scale  \nkLfloat, optional \n\nLightness scale  \nk1float, optional \n\nfirst scale parameter  \nk2float, optional \n\nsecond scale parameter    Returns \n \ndEarray_like \n\ncolor difference between lab1 and lab2     Notes deltaE_ciede94 is not symmetric with respect to lab1 and lab2. CIEDE94 defines the scales for the lightness, hue, and chroma in terms of the first color. Consequently, the first color should be regarded as the \u201creference\u201d color. kL, k1, k2 depend on the application and default to the values suggested for graphic arts   \nParameter Graphic Arts Textiles   \nkL 1.000 2.000  \nk1 0.045 0.048  \nk2 0.015 0.014   References  \n1  \nhttps://en.wikipedia.org/wiki/Color_difference  \n2  \nhttp://www.brucelindbloom.com/index.html?Eqn_DeltaE_CIE94.html   \n"}, {"name": "color.deltaE_cmc()", "path": "api/skimage.color#skimage.color.deltaE_cmc", "type": "color", "text": " \nskimage.color.deltaE_cmc(lab1, lab2, kL=1, kC=1) [source]\n \nColor difference from the CMC l:c standard. This color difference was developed by the Colour Measurement Committee (CMC) of the Society of Dyers and Colourists (United Kingdom). It is intended for use in the textile industry. The scale factors kL, kC set the weight given to differences in lightness and chroma relative to differences in hue. The usual values are kL=2, kC=1 for \u201cacceptability\u201d and kL=1, kC=1 for \u201cimperceptibility\u201d. Colors with dE > 1 are \u201cdifferent\u201d for the given scale factors.  Parameters \n \nlab1array_like \n\nreference color (Lab colorspace)  \nlab2array_like \n\ncomparison color (Lab colorspace)    Returns \n \ndEarray_like \n\ndistance between colors lab1 and lab2     Notes deltaE_cmc the defines the scales for the lightness, hue, and chroma in terms of the first color. Consequently deltaE_cmc(lab1, lab2) != deltaE_cmc(lab2, lab1) References  \n1  \nhttps://en.wikipedia.org/wiki/Color_difference  \n2  \nhttp://www.brucelindbloom.com/index.html?Eqn_DeltaE_CIE94.html  \n3  \nF. J. J. Clarke, R. McDonald, and B. Rigg, \u201cModification to the JPC79 colour-difference formula,\u201d J. Soc. Dyers Colour. 100, 128-132 (1984).   \n"}, {"name": "color.gray2rgb()", "path": "api/skimage.color#skimage.color.gray2rgb", "type": "color", "text": " \nskimage.color.gray2rgb(image, alpha=None) [source]\n \nCreate an RGB representation of a gray-level image.  Parameters \n \nimagearray_like \n\nInput image.  \nalphabool, optional \n\nEnsure that the output image has an alpha layer. If None, alpha layers are passed through but not created.    Returns \n \nrgb(\u2026, 3) ndarray \n\nRGB image. A new dimension of length 3 is added to input image.     Notes If the input is a 1-dimensional image of shape (M, ), the output will be shape (M, 3). \n"}, {"name": "color.gray2rgba()", "path": "api/skimage.color#skimage.color.gray2rgba", "type": "color", "text": " \nskimage.color.gray2rgba(image, alpha=None) [source]\n \nCreate a RGBA representation of a gray-level image.  Parameters \n \nimagearray_like \n\nInput image.  \nalphaarray_like, optional \n\nAlpha channel of the output image. It may be a scalar or an array that can be broadcast to image. If not specified it is set to the maximum limit corresponding to the image dtype.    Returns \n \nrgbandarray \n\nRGBA image. A new dimension of length 4 is added to input image shape.     \n"}, {"name": "color.grey2rgb()", "path": "api/skimage.color#skimage.color.grey2rgb", "type": "color", "text": " \nskimage.color.grey2rgb(image, alpha=None) [source]\n \nCreate an RGB representation of a gray-level image.  Parameters \n \nimagearray_like \n\nInput image.  \nalphabool, optional \n\nEnsure that the output image has an alpha layer. If None, alpha layers are passed through but not created.    Returns \n \nrgb(\u2026, 3) ndarray \n\nRGB image. A new dimension of length 3 is added to input image.     Notes If the input is a 1-dimensional image of shape (M, ), the output will be shape (M, 3). \n"}, {"name": "color.hed2rgb()", "path": "api/skimage.color#skimage.color.hed2rgb", "type": "color", "text": " \nskimage.color.hed2rgb(hed) [source]\n \nHaematoxylin-Eosin-DAB (HED) to RGB color space conversion.  Parameters \n \nhed(\u2026, 3) array_like \n\nThe image in the HED color space. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB. Same dimensions as input.    Raises \n ValueError\n\nIf hed is not at least 2-D with shape (\u2026, 3).     References  \n1  \nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution.,\u201d Analytical and quantitative cytology and histology / the International Academy of Cytology [and] American Society of Cytology, vol. 23, no. 4, pp. 291-9, Aug. 2001.   Examples >>> from skimage import data\n>>> from skimage.color import rgb2hed, hed2rgb\n>>> ihc = data.immunohistochemistry()\n>>> ihc_hed = rgb2hed(ihc)\n>>> ihc_rgb = hed2rgb(ihc_hed)\n \n"}, {"name": "color.hsv2rgb()", "path": "api/skimage.color#skimage.color.hsv2rgb", "type": "color", "text": " \nskimage.color.hsv2rgb(hsv) [source]\n \nHSV to RGB color space conversion.  Parameters \n \nhsv(\u2026, 3) array_like \n\nThe image in HSV format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf hsv is not at least 2-D with shape (\u2026, 3).     Notes Conversion between RGB and HSV color spaces results in some loss of precision, due to integer arithmetic and rounding [1]. References  \n1  \nhttps://en.wikipedia.org/wiki/HSL_and_HSV   Examples >>> from skimage import data\n>>> img = data.astronaut()\n>>> img_hsv = rgb2hsv(img)\n>>> img_rgb = hsv2rgb(img_hsv)\n \n"}, {"name": "color.lab2lch()", "path": "api/skimage.color#skimage.color.lab2lch", "type": "color", "text": " \nskimage.color.lab2lch(lab) [source]\n \nCIE-LAB to CIE-LCH color space conversion. LCH is the cylindrical representation of the LAB (Cartesian) colorspace  Parameters \n \nlab(\u2026, 3) array_like \n\nThe N-D image in CIE-LAB format. The last (N+1-th) dimension must have at least 3 elements, corresponding to the L, a, and b color channels. Subsequent elements are copied.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in LCH format, in a N-D array with same shape as input lab.    Raises \n ValueError\n\nIf lch does not have at least 3 color channels (i.e. l, a, b).     Notes The Hue is expressed as an angle between (0, 2*pi) Examples >>> from skimage import data\n>>> from skimage.color import rgb2lab, lab2lch\n>>> img = data.astronaut()\n>>> img_lab = rgb2lab(img)\n>>> img_lch = lab2lch(img_lab)\n \n"}, {"name": "color.lab2rgb()", "path": "api/skimage.color#skimage.color.lab2rgb", "type": "color", "text": " \nskimage.color.lab2rgb(lab, illuminant='D65', observer='2') [source]\n \nLab to RGB color space conversion.  Parameters \n \nlab(\u2026, 3) array_like \n\nThe image in Lab format. Final dimension denotes channels.  \nilluminant{\u201cA\u201d, \u201cD50\u201d, \u201cD55\u201d, \u201cD65\u201d, \u201cD75\u201d, \u201cE\u201d}, optional \n\nThe name of the illuminant (the function is NOT case sensitive).  \nobserver{\u201c2\u201d, \u201c10\u201d}, optional \n\nThe aperture angle of the observer.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf lab is not at least 2-D with shape (\u2026, 3).     Notes This function uses lab2xyz and xyz2rgb. By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref=95.047, y_ref=100., z_ref=108.883. See function get_xyz_coords for a list of supported illuminants. References  \n1  \nhttps://en.wikipedia.org/wiki/Standard_illuminant   \n"}, {"name": "color.lab2xyz()", "path": "api/skimage.color#skimage.color.lab2xyz", "type": "color", "text": " \nskimage.color.lab2xyz(lab, illuminant='D65', observer='2') [source]\n \nCIE-LAB to XYZcolor space conversion.  Parameters \n \nlab(\u2026, 3) array_like \n\nThe image in Lab format. Final dimension denotes channels.  \nilluminant{\u201cA\u201d, \u201cD50\u201d, \u201cD55\u201d, \u201cD65\u201d, \u201cD75\u201d, \u201cE\u201d}, optional \n\nThe name of the illuminant (the function is NOT case sensitive).  \nobserver{\u201c2\u201d, \u201c10\u201d}, optional \n\nThe aperture angle of the observer.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in XYZ format. Same dimensions as input.    Raises \n ValueError\n\nIf lab is not at least 2-D with shape (\u2026, 3).  ValueError\n\nIf either the illuminant or the observer angle are not supported or unknown.  UserWarning\n\nIf any of the pixels are invalid (Z < 0).     Notes By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref = 95.047, y_ref = 100., z_ref = 108.883. See function \u2018get_xyz_coords\u2019 for a list of supported illuminants. References  \n1  \nhttp://www.easyrgb.com/index.php?X=MATH&H=07  \n2  \nhttps://en.wikipedia.org/wiki/Lab_color_space   \n"}, {"name": "color.label2rgb()", "path": "api/skimage.color#skimage.color.label2rgb", "type": "color", "text": " \nskimage.color.label2rgb(label, image=None, colors=None, alpha=0.3, bg_label=-1, bg_color=(0, 0, 0), image_alpha=1, kind='overlay') [source]\n \nReturn an RGB image where color-coded labels are painted over the image.  Parameters \n \nlabelarray, shape (M, N) \n\nInteger array of labels with the same shape as image.  \nimagearray, shape (M, N, 3), optional \n\nImage used as underlay for labels. If the input is an RGB image, it\u2019s converted to grayscale before coloring.  \ncolorslist, optional \n\nList of colors. If the number of labels exceeds the number of colors, then the colors are cycled.  \nalphafloat [0, 1], optional \n\nOpacity of colorized labels. Ignored if image is None.  \nbg_labelint, optional \n\nLabel that\u2019s treated as the background. If bg_label is specified, bg_color is None, and kind is overlay, background is not painted by any colors.  \nbg_colorstr or array, optional \n\nBackground color. Must be a name in color_dict or RGB float values between [0, 1].  \nimage_alphafloat [0, 1], optional \n\nOpacity of the image.  \nkindstring, one of {\u2018overlay\u2019, \u2018avg\u2019} \n\nThe kind of color image desired. \u2018overlay\u2019 cycles over defined colors and overlays the colored labels over the original image. \u2018avg\u2019 replaces each labeled segment with its average color, for a stained-class or pastel painting appearance.    Returns \n \nresultarray of float, shape (M, N, 3) \n\nThe result of blending a cycling colormap (colors) for each distinct value in label with the image, at a certain alpha value.     \n"}, {"name": "color.lch2lab()", "path": "api/skimage.color#skimage.color.lch2lab", "type": "color", "text": " \nskimage.color.lch2lab(lch) [source]\n \nCIE-LCH to CIE-LAB color space conversion. LCH is the cylindrical representation of the LAB (Cartesian) colorspace  Parameters \n \nlch(\u2026, 3) array_like \n\nThe N-D image in CIE-LCH format. The last (N+1-th) dimension must have at least 3 elements, corresponding to the L, a, and b color channels. Subsequent elements are copied.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in LAB format, with same shape as input lch.    Raises \n ValueError\n\nIf lch does not have at least 3 color channels (i.e. l, c, h).     Examples >>> from skimage import data\n>>> from skimage.color import rgb2lab, lch2lab\n>>> img = data.astronaut()\n>>> img_lab = rgb2lab(img)\n>>> img_lch = lab2lch(img_lab)\n>>> img_lab2 = lch2lab(img_lch)\n \n"}, {"name": "color.rgb2gray()", "path": "api/skimage.color#skimage.color.rgb2gray", "type": "color", "text": " \nskimage.color.rgb2gray(rgb) [source]\n \nCompute luminance of an RGB image.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \noutndarray \n\nThe luminance image - an array which is the same size as the input array, but with the channel dimension removed.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes The weights used in this conversion are calibrated for contemporary CRT phosphors: Y = 0.2125 R + 0.7154 G + 0.0721 B\n If there is an alpha channel present, it is ignored. References  \n1  \nhttp://poynton.ca/PDFs/ColorFAQ.pdf   Examples >>> from skimage.color import rgb2gray\n>>> from skimage import data\n>>> img = data.astronaut()\n>>> img_gray = rgb2gray(img)\n \n"}, {"name": "color.rgb2grey()", "path": "api/skimage.color#skimage.color.rgb2grey", "type": "color", "text": " \nskimage.color.rgb2grey(rgb) [source]\n \nCompute luminance of an RGB image.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \noutndarray \n\nThe luminance image - an array which is the same size as the input array, but with the channel dimension removed.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes The weights used in this conversion are calibrated for contemporary CRT phosphors: Y = 0.2125 R + 0.7154 G + 0.0721 B\n If there is an alpha channel present, it is ignored. References  \n1  \nhttp://poynton.ca/PDFs/ColorFAQ.pdf   Examples >>> from skimage.color import rgb2gray\n>>> from skimage import data\n>>> img = data.astronaut()\n>>> img_gray = rgb2gray(img)\n \n"}, {"name": "color.rgb2hed()", "path": "api/skimage.color#skimage.color.rgb2hed", "type": "color", "text": " \nskimage.color.rgb2hed(rgb) [source]\n \nRGB to Haematoxylin-Eosin-DAB (HED) color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in HED format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     References  \n1  \nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution.,\u201d Analytical and quantitative cytology and histology / the International Academy of Cytology [and] American Society of Cytology, vol. 23, no. 4, pp. 291-9, Aug. 2001.   Examples >>> from skimage import data\n>>> from skimage.color import rgb2hed\n>>> ihc = data.immunohistochemistry()\n>>> ihc_hed = rgb2hed(ihc)\n \n"}, {"name": "color.rgb2hsv()", "path": "api/skimage.color#skimage.color.rgb2hsv", "type": "color", "text": " \nskimage.color.rgb2hsv(rgb) [source]\n \nRGB to HSV color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in HSV format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes Conversion between RGB and HSV color spaces results in some loss of precision, due to integer arithmetic and rounding [1]. References  \n1  \nhttps://en.wikipedia.org/wiki/HSL_and_HSV   Examples >>> from skimage import color\n>>> from skimage import data\n>>> img = data.astronaut()\n>>> img_hsv = color.rgb2hsv(img)\n \n"}, {"name": "color.rgb2lab()", "path": "api/skimage.color#skimage.color.rgb2lab", "type": "color", "text": " \nskimage.color.rgb2lab(rgb, illuminant='D65', observer='2') [source]\n \nConversion from the sRGB color space (IEC 61966-2-1:1999) to the CIE Lab colorspace under the given illuminant and observer.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.  \nilluminant{\u201cA\u201d, \u201cD50\u201d, \u201cD55\u201d, \u201cD65\u201d, \u201cD75\u201d, \u201cE\u201d}, optional \n\nThe name of the illuminant (the function is NOT case sensitive).  \nobserver{\u201c2\u201d, \u201c10\u201d}, optional \n\nThe aperture angle of the observer.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in Lab format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes RGB is a device-dependent color space so, if you use this function, be sure that the image you are analyzing has been mapped to the sRGB color space. This function uses rgb2xyz and xyz2lab. By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref=95.047, y_ref=100., z_ref=108.883. See function get_xyz_coords for a list of supported illuminants. References  \n1  \nhttps://en.wikipedia.org/wiki/Standard_illuminant   \n"}, {"name": "color.rgb2rgbcie()", "path": "api/skimage.color#skimage.color.rgb2rgbcie", "type": "color", "text": " \nskimage.color.rgb2rgbcie(rgb) [source]\n \nRGB to RGB CIE color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB CIE format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     References  \n1  \nhttps://en.wikipedia.org/wiki/CIE_1931_color_space   Examples >>> from skimage import data\n>>> from skimage.color import rgb2rgbcie\n>>> img = data.astronaut()\n>>> img_rgbcie = rgb2rgbcie(img)\n \n"}, {"name": "color.rgb2xyz()", "path": "api/skimage.color#skimage.color.rgb2xyz", "type": "color", "text": " \nskimage.color.rgb2xyz(rgb) [source]\n \nRGB to XYZ color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in XYZ format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes The CIE XYZ color space is derived from the CIE RGB color space. Note however that this function converts from sRGB. References  \n1  \nhttps://en.wikipedia.org/wiki/CIE_1931_color_space   Examples >>> from skimage import data\n>>> img = data.astronaut()\n>>> img_xyz = rgb2xyz(img)\n \n"}, {"name": "color.rgb2ycbcr()", "path": "api/skimage.color#skimage.color.rgb2ycbcr", "type": "color", "text": " \nskimage.color.rgb2ycbcr(rgb) [source]\n \nRGB to YCbCr color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in YCbCr format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes Y is between 16 and 235. This is the color space commonly used by video codecs; it is sometimes incorrectly called \u201cYUV\u201d. References  \n1  \nhttps://en.wikipedia.org/wiki/YCbCr   \n"}, {"name": "color.rgb2ydbdr()", "path": "api/skimage.color#skimage.color.rgb2ydbdr", "type": "color", "text": " \nskimage.color.rgb2ydbdr(rgb) [source]\n \nRGB to YDbDr color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in YDbDr format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes This is the color space commonly used by video codecs. It is also the reversible color transform in JPEG2000. References  \n1  \nhttps://en.wikipedia.org/wiki/YDbDr   \n"}, {"name": "color.rgb2yiq()", "path": "api/skimage.color#skimage.color.rgb2yiq", "type": "color", "text": " \nskimage.color.rgb2yiq(rgb) [source]\n \nRGB to YIQ color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in YIQ format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     \n"}, {"name": "color.rgb2ypbpr()", "path": "api/skimage.color#skimage.color.rgb2ypbpr", "type": "color", "text": " \nskimage.color.rgb2ypbpr(rgb) [source]\n \nRGB to YPbPr color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in YPbPr format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     References  \n1  \nhttps://en.wikipedia.org/wiki/YPbPr   \n"}, {"name": "color.rgb2yuv()", "path": "api/skimage.color#skimage.color.rgb2yuv", "type": "color", "text": " \nskimage.color.rgb2yuv(rgb) [source]\n \nRGB to YUV color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in YUV format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes Y is between 0 and 1. Use YCbCr instead of YUV for the color space commonly used by video codecs, where Y ranges from 16 to 235. References  \n1  \nhttps://en.wikipedia.org/wiki/YUV   \n"}, {"name": "color.rgba2rgb()", "path": "api/skimage.color#skimage.color.rgba2rgb", "type": "color", "text": " \nskimage.color.rgba2rgb(rgba, background=(1, 1, 1)) [source]\n \nRGBA to RGB conversion using alpha blending [1].  Parameters \n \nrgba(\u2026, 4) array_like \n\nThe image in RGBA format. Final dimension denotes channels.  \nbackgroundarray_like \n\nThe color of the background to blend the image with (3 floats between 0 to 1 - the RGB value of the background).    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf rgba is not at least 2-D with shape (\u2026, 4).     References  \n1(1,2)  \nhttps://en.wikipedia.org/wiki/Alpha_compositing#Alpha_blending   Examples >>> from skimage import color\n>>> from skimage import data\n>>> img_rgba = data.logo()\n>>> img_rgb = color.rgba2rgb(img_rgba)\n \n"}, {"name": "color.rgbcie2rgb()", "path": "api/skimage.color#skimage.color.rgbcie2rgb", "type": "color", "text": " \nskimage.color.rgbcie2rgb(rgbcie) [source]\n \nRGB CIE to RGB color space conversion.  Parameters \n \nrgbcie(\u2026, 3) array_like \n\nThe image in RGB CIE format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf rgbcie is not at least 2-D with shape (\u2026, 3).     References  \n1  \nhttps://en.wikipedia.org/wiki/CIE_1931_color_space   Examples >>> from skimage import data\n>>> from skimage.color import rgb2rgbcie, rgbcie2rgb\n>>> img = data.astronaut()\n>>> img_rgbcie = rgb2rgbcie(img)\n>>> img_rgb = rgbcie2rgb(img_rgbcie)\n \n"}, {"name": "color.separate_stains()", "path": "api/skimage.color#skimage.color.separate_stains", "type": "color", "text": " \nskimage.color.separate_stains(rgb, conv_matrix) [source]\n \nRGB to stain color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.  conv_matrix: ndarray\n\nThe stain separation matrix as described by G. Landini [1].    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in stain color space. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes Stain separation matrices available in the color module and their respective colorspace:  \nhed_from_rgb: Hematoxylin + Eosin + DAB \nhdx_from_rgb: Hematoxylin + DAB \nfgx_from_rgb: Feulgen + Light Green \nbex_from_rgb: Giemsa stain : Methyl Blue + Eosin \nrbd_from_rgb: FastRed + FastBlue + DAB \ngdx_from_rgb: Methyl Green + DAB \nhax_from_rgb: Hematoxylin + AEC \nbro_from_rgb: Blue matrix Anilline Blue + Red matrix Azocarmine + Orange matrix Orange-G \nbpx_from_rgb: Methyl Blue + Ponceau Fuchsin \nahx_from_rgb: Alcian Blue + Hematoxylin \nhpx_from_rgb: Hematoxylin + PAS  This implementation borrows some ideas from DIPlib [2], e.g. the compensation using a small value to avoid log artifacts when calculating the Beer-Lambert law. References  \n1  \nhttps://web.archive.org/web/20160624145052/http://www.mecourse.com/landinig/software/cdeconv/cdeconv.html  \n2  \nhttps://github.com/DIPlib/diplib/  \n3  \nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution,\u201d Anal. Quant. Cytol. Histol., vol. 23, no. 4, pp. 291\u2013299, Aug. 2001.   Examples >>> from skimage import data\n>>> from skimage.color import separate_stains, hdx_from_rgb\n>>> ihc = data.immunohistochemistry()\n>>> ihc_hdx = separate_stains(ihc, hdx_from_rgb)\n \n"}, {"name": "color.xyz2lab()", "path": "api/skimage.color#skimage.color.xyz2lab", "type": "color", "text": " \nskimage.color.xyz2lab(xyz, illuminant='D65', observer='2') [source]\n \nXYZ to CIE-LAB color space conversion.  Parameters \n \nxyz(\u2026, 3) array_like \n\nThe image in XYZ format. Final dimension denotes channels.  \nilluminant{\u201cA\u201d, \u201cD50\u201d, \u201cD55\u201d, \u201cD65\u201d, \u201cD75\u201d, \u201cE\u201d}, optional \n\nThe name of the illuminant (the function is NOT case sensitive).  \nobserver{\u201c2\u201d, \u201c10\u201d}, optional \n\nThe aperture angle of the observer.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in CIE-LAB format. Same dimensions as input.    Raises \n ValueError\n\nIf xyz is not at least 2-D with shape (\u2026, 3).  ValueError\n\nIf either the illuminant or the observer angle is unsupported or unknown.     Notes By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref=95.047, y_ref=100., z_ref=108.883. See function get_xyz_coords for a list of supported illuminants. References  \n1  \nhttp://www.easyrgb.com/index.php?X=MATH&H=07  \n2  \nhttps://en.wikipedia.org/wiki/Lab_color_space   Examples >>> from skimage import data\n>>> from skimage.color import rgb2xyz, xyz2lab\n>>> img = data.astronaut()\n>>> img_xyz = rgb2xyz(img)\n>>> img_lab = xyz2lab(img_xyz)\n \n"}, {"name": "color.xyz2rgb()", "path": "api/skimage.color#skimage.color.xyz2rgb", "type": "color", "text": " \nskimage.color.xyz2rgb(xyz) [source]\n \nXYZ to RGB color space conversion.  Parameters \n \nxyz(\u2026, 3) array_like \n\nThe image in XYZ format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf xyz is not at least 2-D with shape (\u2026, 3).     Notes The CIE XYZ color space is derived from the CIE RGB color space. Note however that this function converts to sRGB. References  \n1  \nhttps://en.wikipedia.org/wiki/CIE_1931_color_space   Examples >>> from skimage import data\n>>> from skimage.color import rgb2xyz, xyz2rgb\n>>> img = data.astronaut()\n>>> img_xyz = rgb2xyz(img)\n>>> img_rgb = xyz2rgb(img_xyz)\n \n"}, {"name": "color.ycbcr2rgb()", "path": "api/skimage.color#skimage.color.ycbcr2rgb", "type": "color", "text": " \nskimage.color.ycbcr2rgb(ycbcr) [source]\n \nYCbCr to RGB color space conversion.  Parameters \n \nycbcr(\u2026, 3) array_like \n\nThe image in YCbCr format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf ycbcr is not at least 2-D with shape (\u2026, 3).     Notes Y is between 16 and 235. This is the color space commonly used by video codecs; it is sometimes incorrectly called \u201cYUV\u201d. References  \n1  \nhttps://en.wikipedia.org/wiki/YCbCr   \n"}, {"name": "color.ydbdr2rgb()", "path": "api/skimage.color#skimage.color.ydbdr2rgb", "type": "color", "text": " \nskimage.color.ydbdr2rgb(ydbdr) [source]\n \nYDbDr to RGB color space conversion.  Parameters \n \nydbdr(\u2026, 3) array_like \n\nThe image in YDbDr format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf ydbdr is not at least 2-D with shape (\u2026, 3).     Notes This is the color space commonly used by video codecs, also called the reversible color transform in JPEG2000. References  \n1  \nhttps://en.wikipedia.org/wiki/YDbDr   \n"}, {"name": "color.yiq2rgb()", "path": "api/skimage.color#skimage.color.yiq2rgb", "type": "color", "text": " \nskimage.color.yiq2rgb(yiq) [source]\n \nYIQ to RGB color space conversion.  Parameters \n \nyiq(\u2026, 3) array_like \n\nThe image in YIQ format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf yiq is not at least 2-D with shape (\u2026, 3).     \n"}, {"name": "color.ypbpr2rgb()", "path": "api/skimage.color#skimage.color.ypbpr2rgb", "type": "color", "text": " \nskimage.color.ypbpr2rgb(ypbpr) [source]\n \nYPbPr to RGB color space conversion.  Parameters \n \nypbpr(\u2026, 3) array_like \n\nThe image in YPbPr format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf ypbpr is not at least 2-D with shape (\u2026, 3).     References  \n1  \nhttps://en.wikipedia.org/wiki/YPbPr   \n"}, {"name": "color.yuv2rgb()", "path": "api/skimage.color#skimage.color.yuv2rgb", "type": "color", "text": " \nskimage.color.yuv2rgb(yuv) [source]\n \nYUV to RGB color space conversion.  Parameters \n \nyuv(\u2026, 3) array_like \n\nThe image in YUV format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf yuv is not at least 2-D with shape (\u2026, 3).     References  \n1  \nhttps://en.wikipedia.org/wiki/YUV   \n"}, {"name": "data", "path": "api/skimage.data", "type": "data", "text": "Module: data Standard test images. For more images, see  http://sipi.usc.edu/database/database.php   \nskimage.data.astronaut() Color image of the astronaut Eileen Collins.  \nskimage.data.binary_blobs([length, \u2026]) Generate synthetic binary image with several rounded blob-like objects.  \nskimage.data.brain() Subset of data from the University of North Carolina Volume Rendering Test Data Set.  \nskimage.data.brick() Brick wall.  \nskimage.data.camera() Gray-level \u201ccamera\u201d image.  \nskimage.data.cat() Chelsea the cat.  \nskimage.data.cell() Cell floating in saline.  \nskimage.data.cells3d() 3D fluorescence microscopy image of cells.  \nskimage.data.checkerboard() Checkerboard image.  \nskimage.data.chelsea() Chelsea the cat.  \nskimage.data.clock() Motion blurred clock.  \nskimage.data.coffee() Coffee cup.  \nskimage.data.coins() Greek coins from Pompeii.  \nskimage.data.colorwheel() Color Wheel.  \nskimage.data.download_all([directory]) Download all datasets for use with scikit-image offline.  \nskimage.data.eagle() A golden eagle.  \nskimage.data.grass() Grass.  \nskimage.data.gravel() Gravel  \nskimage.data.horse() Black and white silhouette of a horse.  \nskimage.data.hubble_deep_field() Hubble eXtreme Deep Field.  \nskimage.data.human_mitosis() Image of human cells undergoing mitosis.  \nskimage.data.immunohistochemistry() Immunohistochemical (IHC) staining with hematoxylin counterstaining.  \nskimage.data.kidney() Mouse kidney tissue.  \nskimage.data.lbp_frontal_face_cascade_filename() Return the path to the XML file containing the weak classifier cascade.  \nskimage.data.lfw_subset() Subset of data from the LFW dataset.  \nskimage.data.lily() Lily of the valley plant stem.  \nskimage.data.logo() Scikit-image logo, a RGBA image.  \nskimage.data.microaneurysms() Gray-level \u201cmicroaneurysms\u201d image.  \nskimage.data.moon() Surface of the moon.  \nskimage.data.page() Scanned page.  \nskimage.data.retina() Human retina.  \nskimage.data.rocket() Launch photo of DSCOVR on Falcon 9 by SpaceX.  \nskimage.data.shepp_logan_phantom() Shepp Logan Phantom.  \nskimage.data.skin() Microscopy image of dermis and epidermis (skin layers).  \nskimage.data.stereo_motorcycle() Rectified stereo image pair with ground-truth disparities.  \nskimage.data.text() Gray-level \u201ctext\u201d image used for corner detection.   astronaut  \nskimage.data.astronaut() [source]\n \nColor image of the astronaut Eileen Collins. Photograph of Eileen Collins, an American astronaut. She was selected as an astronaut in 1992 and first piloted the space shuttle STS-63 in 1995. She retired in 2006 after spending a total of 38 days, 8 hours and 10 minutes in outer space. This image was downloaded from the NASA Great Images database <https://flic.kr/p/r9qvLn>`__. No known copyright restrictions, released into the public domain.  Returns \n \nastronaut(512, 512, 3) uint8 ndarray \n\nAstronaut image.     \n Examples using skimage.data.astronaut\n \n  Flood Fill   binary_blobs  \nskimage.data.binary_blobs(length=512, blob_size_fraction=0.1, n_dim=2, volume_fraction=0.5, seed=None) [source]\n \nGenerate synthetic binary image with several rounded blob-like objects.  Parameters \n \nlengthint, optional \n\nLinear size of output image.  \nblob_size_fractionfloat, optional \n\nTypical linear size of blob, as a fraction of length, should be smaller than 1.  \nn_dimint, optional \n\nNumber of dimensions of output image.  \nvolume_fractionfloat, default 0.5 \n\nFraction of image pixels covered by the blobs (where the output is 1). Should be in [0, 1].  \nseedint, optional \n\nSeed to initialize the random number generator. If None, a random seed from the operating system is used.    Returns \n \nblobsndarray of bools \n\nOutput binary image     Examples >>> from skimage import data\n>>> data.binary_blobs(length=5, blob_size_fraction=0.2, seed=1)\narray([[ True, False,  True,  True,  True],\n       [ True,  True,  True, False,  True],\n       [False,  True, False,  True,  True],\n       [ True, False, False,  True,  True],\n       [ True, False, False, False,  True]])\n>>> blobs = data.binary_blobs(length=256, blob_size_fraction=0.1)\n>>> # Finer structures\n>>> blobs = data.binary_blobs(length=256, blob_size_fraction=0.05)\n>>> # Blobs cover a smaller volume fraction of the image\n>>> blobs = data.binary_blobs(length=256, volume_fraction=0.3)\n \n brain  \nskimage.data.brain() [source]\n \nSubset of data from the University of North Carolina Volume Rendering Test Data Set. The full dataset is available at [1].  Returns \n \nimage(10, 256, 256) uint16 ndarray \n   Notes The 3D volume consists of 10 layers from the larger volume. References  \n1  \nhttps://graphics.stanford.edu/data/voldata/   \n Examples using skimage.data.brain\n \n  Local Histogram Equalization  \n\n  Rank filters   brick  \nskimage.data.brick() [source]\n \nBrick wall.  Returns \n \nbrick(512, 512) uint8 image \n\nA small section of a brick wall.     Notes The original image was downloaded from CC0Textures and licensed under the Creative Commons CC0 License. A perspective transform was then applied to the image, prior to rotating it by 90 degrees, cropping and scaling it to obtain the final image. \n camera  \nskimage.data.camera() [source]\n \nGray-level \u201ccamera\u201d image. Can be used for segmentation and denoising examples.  Returns \n \ncamera(512, 512) uint8 ndarray \n\nCamera image.     Notes No copyright restrictions. CC0 by the photographer (Lav Varshney).  Changed in version 0.18: This image was replaced due to copyright restrictions. For more information, please see [1].  References  \n1  \nhttps://github.com/scikit-image/scikit-image/issues/3927   \n Examples using skimage.data.camera\n \n  Tinting gray-scale images  \n\n  Masked Normalized Cross-Correlation  \n\n  Entropy  \n\n  GLCM Texture Features  \n\n  Multi-Otsu Thresholding  \n\n  Flood Fill  \n\n  Rank filters   cat  \nskimage.data.cat() [source]\n \nChelsea the cat. An example with texture, prominent edges in horizontal and diagonal directions, as well as features of differing scales.  Returns \n \nchelsea(300, 451, 3) uint8 ndarray \n\nChelsea image.     Notes No copyright restrictions. CC0 by the photographer (Stefan van der Walt). \n cell  \nskimage.data.cell() [source]\n \nCell floating in saline. This is a quantitative phase image retrieved from a digital hologram using the Python library qpformat. The image shows a cell with high phase value, above the background phase. Because of a banding pattern artifact in the background, this image is a good test of thresholding algorithms. The pixel spacing is 0.107 \u00b5m. These data were part of a comparison between several refractive index retrieval techniques for spherical objects as part of [1]. This image is CC0, dedicated to the public domain. You may copy, modify, or distribute it without asking permission.  Returns \n \ncell(660, 550) uint8 array \n\nImage of a cell.     References  \n1  \nPaul M\u00fcller, Mirjam Sch\u00fcrmann, Salvatore Girardo, Gheorghe Cojoc, and Jochen Guck. \u201cAccurate evaluation of size and refractive index for spherical objects in quantitative phase imaging.\u201d Optics Express 26(8): 10729-10743 (2018). DOI:10.1364/OE.26.010729   \n cells3d  \nskimage.data.cells3d() [source]\n \n3D fluorescence microscopy image of cells. The returned data is a 3D multichannel array with dimensions provided in (z, c, y, x) order. Each voxel has a size of (0.29 0.26 0.26) micrometer. Channel 0 contains cell membranes, channel 1 contains nuclei.  Returns \n cells3d: (60, 2, 256, 256) uint16 ndarray\n\nThe volumetric images of cells taken with an optical microscope.     Notes The data for this was provided by the Allen Institute for Cell Science. It has been downsampled by a factor of 4 in the row and column dimensions to reduce computational time. The microscope reports the following voxel spacing in microns:  Original voxel size is (0.290, 0.065, 0.065). Scaling factor is (1, 4, 4) in each dimension. After rescaling the voxel size is (0.29 0.26 0.26).  \n Examples using skimage.data.cells3d\n \n  3D adaptive histogram equalization  \n\n  Use rolling-ball algorithm for estimating background intensity  \n\n  Explore 3D images (of cells)   checkerboard  \nskimage.data.checkerboard() [source]\n \nCheckerboard image. Checkerboards are often used in image calibration, since the corner-points are easy to locate. Because of the many parallel edges, they also visualise distortions particularly well.  Returns \n \ncheckerboard(200, 200) uint8 ndarray \n\nCheckerboard image.     \n Examples using skimage.data.checkerboard\n \n  Flood Fill   chelsea  \nskimage.data.chelsea() [source]\n \nChelsea the cat. An example with texture, prominent edges in horizontal and diagonal directions, as well as features of differing scales.  Returns \n \nchelsea(300, 451, 3) uint8 ndarray \n\nChelsea image.     Notes No copyright restrictions. CC0 by the photographer (Stefan van der Walt). \n Examples using skimage.data.chelsea\n \n  Phase Unwrapping  \n\n  Flood Fill   clock  \nskimage.data.clock() [source]\n \nMotion blurred clock. This photograph of a wall clock was taken while moving the camera in an aproximately horizontal direction. It may be used to illustrate inverse filters and deconvolution. Released into the public domain by the photographer (Stefan van der Walt).  Returns \n \nclock(300, 400) uint8 ndarray \n\nClock image.     \n coffee  \nskimage.data.coffee() [source]\n \nCoffee cup. This photograph is courtesy of Pikolo Espresso Bar. It contains several elliptical shapes as well as varying texture (smooth porcelain to course wood grain).  Returns \n \ncoffee(400, 600, 3) uint8 ndarray \n\nCoffee image.     Notes No copyright restrictions. CC0 by the photographer (Rachel Michetti). \n coins  \nskimage.data.coins() [source]\n \nGreek coins from Pompeii. This image shows several coins outlined against a gray background. It is especially useful in, e.g. segmentation tests, where individual objects need to be identified against a background. The background shares enough grey levels with the coins that a simple segmentation is not sufficient.  Returns \n \ncoins(303, 384) uint8 ndarray \n\nCoins image.     Notes This image was downloaded from the Brooklyn Museum Collection. No known copyright restrictions. \n Examples using skimage.data.coins\n \n  Finding local maxima  \n\n  Measure region properties  \n\n  Use rolling-ball algorithm for estimating background intensity   colorwheel  \nskimage.data.colorwheel() [source]\n \nColor Wheel.  Returns \n \ncolorwheel(370, 371, 3) uint8 image \n\nA colorwheel.     \n download_all  \nskimage.data.download_all(directory=None) [source]\n \nDownload all datasets for use with scikit-image offline. Scikit-image datasets are no longer shipped with the library by default. This allows us to use higher quality datasets, while keeping the library download size small. This function requires the installation of an optional dependency, pooch, to download the full dataset. Follow installation instruction found at https://scikit-image.org/docs/stable/install.html Call this function to download all sample images making them available offline on your machine.  Parameters \n directory: path-like, optional\n\nThe directory where the dataset should be stored.    Raises \n ModuleNotFoundError:\n\nIf pooch is not install, this error will be raised.     Notes scikit-image will only search for images stored in the default directory. Only specify the directory if you wish to download the images to your own folder for a particular reason. You can access the location of the default data directory by inspecting the variable skimage.data.data_dir. \n eagle  \nskimage.data.eagle() [source]\n \nA golden eagle. Suitable for examples on segmentation, Hough transforms, and corner detection.  Returns \n \neagle(2019, 1826) uint8 ndarray \n\nEagle image.     Notes No copyright restrictions. CC0 by the photographer (Dayane Machado). \n Examples using skimage.data.eagle\n \n  Markers for watershed transform   grass  \nskimage.data.grass() [source]\n \nGrass.  Returns \n \ngrass(512, 512) uint8 image \n\nSome grass.     Notes The original image was downloaded from DeviantArt and licensed underthe Creative Commons CC0 License. The downloaded image was cropped to include a region of (512, 512) pixels around the top left corner, converted to grayscale, then to uint8 prior to saving the result in PNG format. \n gravel  \nskimage.data.gravel() [source]\n \nGravel  Returns \n \ngravel(512, 512) uint8 image \n\nGrayscale gravel sample.     Notes The original image was downloaded from CC0Textures and licensed under the Creative Commons CC0 License. The downloaded image was then rescaled to (1024, 1024), then the top left (512, 512) pixel region was cropped prior to converting the image to grayscale and uint8 data type. The result was saved using the PNG format. \n horse  \nskimage.data.horse() [source]\n \nBlack and white silhouette of a horse. This image was downloaded from openclipart No copyright restrictions. CC0 given by owner (Andreas Preuss (marauder)).  Returns \n \nhorse(328, 400) bool ndarray \n\nHorse image.     \n hubble_deep_field  \nskimage.data.hubble_deep_field() [source]\n \nHubble eXtreme Deep Field. This photograph contains the Hubble Telescope\u2019s farthest ever view of the universe. It can be useful as an example for multi-scale detection.  Returns \n \nhubble_deep_field(872, 1000, 3) uint8 ndarray \n\nHubble deep field image.     Notes This image was downloaded from HubbleSite. The image was captured by NASA and may be freely used in the public domain. \n human_mitosis  \nskimage.data.human_mitosis() [source]\n \nImage of human cells undergoing mitosis.  Returns \n human_mitosis: (512, 512) uint8 ndimage\n\nData of human cells undergoing mitosis taken during the preperation of the manuscript in [1].     Notes Copyright David Root. Licensed under CC-0 [2]. References  \n1  \nMoffat J, Grueneberg DA, Yang X, Kim SY, Kloepfer AM, Hinkle G, Piqani B, Eisenhaure TM, Luo B, Grenier JK, Carpenter AE, Foo SY, Stewart SA, Stockwell BR, Hacohen N, Hahn WC, Lander ES, Sabatini DM, Root DE (2006) A lentiviral RNAi library for human and mouse genes applied to an arrayed viral high-content screen. Cell, 124(6):1283-98 / :DOI: 10.1016/j.cell.2006.01.040 PMID 16564017  \n2  \nGitHub licensing discussion https://github.com/CellProfiler/examples/issues/41   \n Examples using skimage.data.human_mitosis\n \n  Segment human cells (in mitosis)   immunohistochemistry  \nskimage.data.immunohistochemistry() [source]\n \nImmunohistochemical (IHC) staining with hematoxylin counterstaining. This picture shows colonic glands where the IHC expression of FHL2 protein is revealed with DAB. Hematoxylin counterstaining is applied to enhance the negative parts of the tissue. This image was acquired at the Center for Microscopy And Molecular Imaging (CMMI). No known copyright restrictions.  Returns \n \nimmunohistochemistry(512, 512, 3) uint8 ndarray \n\nImmunohistochemistry image.     \n kidney  \nskimage.data.kidney() [source]\n \nMouse kidney tissue. This biological tissue on a pre-prepared slide was imaged with confocal fluorescence microscopy (Nikon C1 inverted microscope). Image shape is (16, 512, 512, 3). That is 512x512 pixels in X-Y, 16 image slices in Z, and 3 color channels (emission wavelengths 450nm, 515nm, and 605nm, respectively). Real-space voxel size is 1.24 microns in X-Y, and 1.25 microns in Z. Data type is unsigned 16-bit integers.  Returns \n \nkidney(16, 512, 512, 3) uint16 ndarray \n\nKidney 3D multichannel image.     Notes This image was acquired by Genevieve Buckley at Monasoh Micro Imaging in 2018. License: CC0 \n lbp_frontal_face_cascade_filename  \nskimage.data.lbp_frontal_face_cascade_filename() [source]\n \nReturn the path to the XML file containing the weak classifier cascade. These classifiers were trained using LBP features. The file is part of the OpenCV repository [1]. References  \n1  \nOpenCV lbpcascade trained files https://github.com/opencv/opencv/tree/master/data/lbpcascades   \n lfw_subset  \nskimage.data.lfw_subset() [source]\n \nSubset of data from the LFW dataset. This database is a subset of the LFW database containing:  100 faces 100 non-faces  The full dataset is available at [2].  Returns \n \nimages(200, 25, 25) uint8 ndarray \n\n100 first images are faces and subsequent 100 are non-faces.     Notes The faces were randomly selected from the LFW dataset and the non-faces were extracted from the background of the same dataset. The cropped ROIs have been resized to a 25 x 25 pixels. References  \n1  \nHuang, G., Mattar, M., Lee, H., & Learned-Miller, E. G. (2012). Learning to align from scratch. In Advances in Neural Information Processing Systems (pp. 764-772).  \n2  \nhttp://vis-www.cs.umass.edu/lfw/   \n Examples using skimage.data.lfw_subset\n \n  Specific images   lily  \nskimage.data.lily() [source]\n \nLily of the valley plant stem. This plant stem on a pre-prepared slide was imaged with confocal fluorescence microscopy (Nikon C1 inverted microscope). Image shape is (922, 922, 4). That is 922x922 pixels in X-Y, with 4 color channels. Real-space voxel size is 1.24 microns in X-Y. Data type is unsigned 16-bit integers.  Returns \n \nlily(922, 922, 4) uint16 ndarray \n\nLily 2D multichannel image.     Notes This image was acquired by Genevieve Buckley at Monasoh Micro Imaging in 2018. License: CC0 \n logo  \nskimage.data.logo() [source]\n \nScikit-image logo, a RGBA image.  Returns \n \nlogo(500, 500, 4) uint8 ndarray \n\nLogo image.     \n microaneurysms  \nskimage.data.microaneurysms() [source]\n \nGray-level \u201cmicroaneurysms\u201d image. Detail from an image of the retina (green channel). The image is a crop of image 07_dr.JPG from the High-Resolution Fundus (HRF) Image Database: https://www5.cs.fau.de/research/data/fundus-images/  Returns \n \nmicroaneurysms(102, 102) uint8 ndarray \n\nRetina image with lesions.     Notes No copyright restrictions. CC0 given by owner (Andreas Maier). References  \n1  \nBudai, A., Bock, R, Maier, A., Hornegger, J., Michelson, G. (2013). Robust Vessel Segmentation in Fundus Images. International Journal of Biomedical Imaging, vol. 2013, 2013. DOI:10.1155/2013/154860   \n moon  \nskimage.data.moon() [source]\n \nSurface of the moon. This low-contrast image of the surface of the moon is useful for illustrating histogram equalization and contrast stretching.  Returns \n \nmoon(512, 512) uint8 ndarray \n\nMoon image.     \n Examples using skimage.data.moon\n \n  Local Histogram Equalization   page  \nskimage.data.page() [source]\n \nScanned page. This image of printed text is useful for demonstrations requiring uneven background illumination.  Returns \n \npage(191, 384) uint8 ndarray \n\nPage image.     \n Examples using skimage.data.page\n \n  Use rolling-ball algorithm for estimating background intensity  \n\n  Rank filters   retina  \nskimage.data.retina() [source]\n \nHuman retina. This image of a retina is useful for demonstrations requiring circular images.  Returns \n \nretina(1411, 1411, 3) uint8 ndarray \n\nRetina image in RGB.     Notes This image was downloaded from wikimedia. This file is made available under the Creative Commons CC0 1.0 Universal Public Domain Dedication. References  \n1  \nH\u00e4ggstr\u00f6m, Mikael (2014). \u201cMedical gallery of Mikael H\u00e4ggstr\u00f6m 2014\u201d. WikiJournal of Medicine 1 (2). DOI:10.15347/wjm/2014.008. ISSN 2002-4436. Public Domain   \n rocket  \nskimage.data.rocket() [source]\n \nLaunch photo of DSCOVR on Falcon 9 by SpaceX. This is the launch photo of Falcon 9 carrying DSCOVR lifted off from SpaceX\u2019s Launch Complex 40 at Cape Canaveral Air Force Station, FL.  Returns \n \nrocket(427, 640, 3) uint8 ndarray \n\nRocket image.     Notes This image was downloaded from SpaceX Photos. The image was captured by SpaceX and released in the public domain. \n shepp_logan_phantom  \nskimage.data.shepp_logan_phantom() [source]\n \nShepp Logan Phantom.  Returns \n \nphantom(400, 400) float64 image \n\nImage of the Shepp-Logan phantom in grayscale.     References  \n1  \nL. A. Shepp and B. F. Logan, \u201cThe Fourier reconstruction of a head section,\u201d in IEEE Transactions on Nuclear Science, vol. 21, no. 3, pp. 21-43, June 1974. DOI:10.1109/TNS.1974.6499235   \n skin  \nskimage.data.skin() [source]\n \nMicroscopy image of dermis and epidermis (skin layers). Hematoxylin and eosin stained slide at 10x of normal epidermis and dermis with a benign intradermal nevus.  Returns \n \nskin(960, 1280, 3) RGB image of uint8 \n   Notes This image requires an Internet connection the first time it is called, and to have the pooch package installed, in order to fetch the image file from the scikit-image datasets repository. The source of this image is https://en.wikipedia.org/wiki/File:Normal_Epidermis_and_Dermis_with_Intradermal_Nevus_10x.JPG The image was released in the public domain by its author Kilbad. \n Examples using skimage.data.skin\n \n  Trainable segmentation using local features and random forests   stereo_motorcycle  \nskimage.data.stereo_motorcycle() [source]\n \nRectified stereo image pair with ground-truth disparities. The two images are rectified such that every pixel in the left image has its corresponding pixel on the same scanline in the right image. That means that both images are warped such that they have the same orientation but a horizontal spatial offset (baseline). The ground-truth pixel offset in column direction is specified by the included disparity map. The two images are part of the Middlebury 2014 stereo benchmark. The dataset was created by Nera Nesic, Porter Westling, Xi Wang, York Kitajima, Greg Krathwohl, and Daniel Scharstein at Middlebury College. A detailed description of the acquisition process can be found in [1]. The images included here are down-sampled versions of the default exposure images in the benchmark. The images are down-sampled by a factor of 4 using the function skimage.transform.downscale_local_mean. The calibration data in the following and the included ground-truth disparity map are valid for the down-sampled images: Focal length:           994.978px\nPrincipal point x:      311.193px\nPrincipal point y:      254.877px\nPrincipal point dx:      31.086px\nBaseline:               193.001mm\n  Returns \n \nimg_left(500, 741, 3) uint8 ndarray \n\nLeft stereo image.  \nimg_right(500, 741, 3) uint8 ndarray \n\nRight stereo image.  \ndisp(500, 741, 3) float ndarray \n\nGround-truth disparity map, where each value describes the offset in column direction between corresponding pixels in the left and the right stereo images. E.g. the corresponding pixel of img_left[10, 10 + disp[10, 10]] is img_right[10, 10]. NaNs denote pixels in the left image that do not have ground-truth.     Notes The original resolution images, images with different exposure and lighting, and ground-truth depth maps can be found at the Middlebury website [2]. References  \n1  \nD. Scharstein, H. Hirschmueller, Y. Kitajima, G. Krathwohl, N. Nesic, X. Wang, and P. Westling. High-resolution stereo datasets with subpixel-accurate ground truth. In German Conference on Pattern Recognition (GCPR 2014), Muenster, Germany, September 2014.  \n2  \nhttp://vision.middlebury.edu/stereo/data/scenes2014/   \n Examples using skimage.data.stereo_motorcycle\n \n  Specific images  \n\n  Registration using optical flow   text  \nskimage.data.text() [source]\n \nGray-level \u201ctext\u201d image used for corner detection.  Returns \n \ntext(172, 448) uint8 ndarray \n\nText image.     Notes This image was downloaded from Wikipedia <https://en.wikipedia.org/wiki/File:Corner.png>`__. No known copyright restrictions, released into the public domain. \n\n"}, {"name": "Data visualization", "path": "user_guide/visualization", "type": "Guide", "text": "Data visualization Data visualization takes an important place in image processing. Data can be a simple unique 2D image or a more complex with multidimensional aspects: 3D in space, timeslapse, multiple channels. Therefore, the visualization strategy will depend on the data complexity and a range of tools external to scikit-image can be used for this purpose. Historically, scikit-image provided viewer tools but powerful packages are now available and must be preferred. Matplotlib Matplotlib is a library able to generate static plots, which includes image visualization. Plotly Plotly is a plotting library relying on web technologies with interaction capabilities. Mayavi Mayavi can be used to visualize 3D images. Napari Napari is a multi-dimensional image viewer. It\u2019s designed for browsing, annotating, and analyzing large multi-dimensional images.\n"}, {"name": "data.astronaut()", "path": "api/skimage.data#skimage.data.astronaut", "type": "data", "text": " \nskimage.data.astronaut() [source]\n \nColor image of the astronaut Eileen Collins. Photograph of Eileen Collins, an American astronaut. She was selected as an astronaut in 1992 and first piloted the space shuttle STS-63 in 1995. She retired in 2006 after spending a total of 38 days, 8 hours and 10 minutes in outer space. This image was downloaded from the NASA Great Images database <https://flic.kr/p/r9qvLn>`__. No known copyright restrictions, released into the public domain.  Returns \n \nastronaut(512, 512, 3) uint8 ndarray \n\nAstronaut image.     \n"}, {"name": "data.binary_blobs()", "path": "api/skimage.data#skimage.data.binary_blobs", "type": "data", "text": " \nskimage.data.binary_blobs(length=512, blob_size_fraction=0.1, n_dim=2, volume_fraction=0.5, seed=None) [source]\n \nGenerate synthetic binary image with several rounded blob-like objects.  Parameters \n \nlengthint, optional \n\nLinear size of output image.  \nblob_size_fractionfloat, optional \n\nTypical linear size of blob, as a fraction of length, should be smaller than 1.  \nn_dimint, optional \n\nNumber of dimensions of output image.  \nvolume_fractionfloat, default 0.5 \n\nFraction of image pixels covered by the blobs (where the output is 1). Should be in [0, 1].  \nseedint, optional \n\nSeed to initialize the random number generator. If None, a random seed from the operating system is used.    Returns \n \nblobsndarray of bools \n\nOutput binary image     Examples >>> from skimage import data\n>>> data.binary_blobs(length=5, blob_size_fraction=0.2, seed=1)\narray([[ True, False,  True,  True,  True],\n       [ True,  True,  True, False,  True],\n       [False,  True, False,  True,  True],\n       [ True, False, False,  True,  True],\n       [ True, False, False, False,  True]])\n>>> blobs = data.binary_blobs(length=256, blob_size_fraction=0.1)\n>>> # Finer structures\n>>> blobs = data.binary_blobs(length=256, blob_size_fraction=0.05)\n>>> # Blobs cover a smaller volume fraction of the image\n>>> blobs = data.binary_blobs(length=256, volume_fraction=0.3)\n \n"}, {"name": "data.brain()", "path": "api/skimage.data#skimage.data.brain", "type": "data", "text": " \nskimage.data.brain() [source]\n \nSubset of data from the University of North Carolina Volume Rendering Test Data Set. The full dataset is available at [1].  Returns \n \nimage(10, 256, 256) uint16 ndarray \n   Notes The 3D volume consists of 10 layers from the larger volume. References  \n1  \nhttps://graphics.stanford.edu/data/voldata/   \n"}, {"name": "data.brick()", "path": "api/skimage.data#skimage.data.brick", "type": "data", "text": " \nskimage.data.brick() [source]\n \nBrick wall.  Returns \n \nbrick(512, 512) uint8 image \n\nA small section of a brick wall.     Notes The original image was downloaded from CC0Textures and licensed under the Creative Commons CC0 License. A perspective transform was then applied to the image, prior to rotating it by 90 degrees, cropping and scaling it to obtain the final image. \n"}, {"name": "data.camera()", "path": "api/skimage.data#skimage.data.camera", "type": "data", "text": " \nskimage.data.camera() [source]\n \nGray-level \u201ccamera\u201d image. Can be used for segmentation and denoising examples.  Returns \n \ncamera(512, 512) uint8 ndarray \n\nCamera image.     Notes No copyright restrictions. CC0 by the photographer (Lav Varshney).  Changed in version 0.18: This image was replaced due to copyright restrictions. For more information, please see [1].  References  \n1  \nhttps://github.com/scikit-image/scikit-image/issues/3927   \n"}, {"name": "data.cat()", "path": "api/skimage.data#skimage.data.cat", "type": "data", "text": " \nskimage.data.cat() [source]\n \nChelsea the cat. An example with texture, prominent edges in horizontal and diagonal directions, as well as features of differing scales.  Returns \n \nchelsea(300, 451, 3) uint8 ndarray \n\nChelsea image.     Notes No copyright restrictions. CC0 by the photographer (Stefan van der Walt). \n"}, {"name": "data.cell()", "path": "api/skimage.data#skimage.data.cell", "type": "data", "text": " \nskimage.data.cell() [source]\n \nCell floating in saline. This is a quantitative phase image retrieved from a digital hologram using the Python library qpformat. The image shows a cell with high phase value, above the background phase. Because of a banding pattern artifact in the background, this image is a good test of thresholding algorithms. The pixel spacing is 0.107 \u00b5m. These data were part of a comparison between several refractive index retrieval techniques for spherical objects as part of [1]. This image is CC0, dedicated to the public domain. You may copy, modify, or distribute it without asking permission.  Returns \n \ncell(660, 550) uint8 array \n\nImage of a cell.     References  \n1  \nPaul M\u00fcller, Mirjam Sch\u00fcrmann, Salvatore Girardo, Gheorghe Cojoc, and Jochen Guck. \u201cAccurate evaluation of size and refractive index for spherical objects in quantitative phase imaging.\u201d Optics Express 26(8): 10729-10743 (2018). DOI:10.1364/OE.26.010729   \n"}, {"name": "data.cells3d()", "path": "api/skimage.data#skimage.data.cells3d", "type": "data", "text": " \nskimage.data.cells3d() [source]\n \n3D fluorescence microscopy image of cells. The returned data is a 3D multichannel array with dimensions provided in (z, c, y, x) order. Each voxel has a size of (0.29 0.26 0.26) micrometer. Channel 0 contains cell membranes, channel 1 contains nuclei.  Returns \n cells3d: (60, 2, 256, 256) uint16 ndarray\n\nThe volumetric images of cells taken with an optical microscope.     Notes The data for this was provided by the Allen Institute for Cell Science. It has been downsampled by a factor of 4 in the row and column dimensions to reduce computational time. The microscope reports the following voxel spacing in microns:  Original voxel size is (0.290, 0.065, 0.065). Scaling factor is (1, 4, 4) in each dimension. After rescaling the voxel size is (0.29 0.26 0.26).  \n"}, {"name": "data.checkerboard()", "path": "api/skimage.data#skimage.data.checkerboard", "type": "data", "text": " \nskimage.data.checkerboard() [source]\n \nCheckerboard image. Checkerboards are often used in image calibration, since the corner-points are easy to locate. Because of the many parallel edges, they also visualise distortions particularly well.  Returns \n \ncheckerboard(200, 200) uint8 ndarray \n\nCheckerboard image.     \n"}, {"name": "data.chelsea()", "path": "api/skimage.data#skimage.data.chelsea", "type": "data", "text": " \nskimage.data.chelsea() [source]\n \nChelsea the cat. An example with texture, prominent edges in horizontal and diagonal directions, as well as features of differing scales.  Returns \n \nchelsea(300, 451, 3) uint8 ndarray \n\nChelsea image.     Notes No copyright restrictions. CC0 by the photographer (Stefan van der Walt). \n"}, {"name": "data.clock()", "path": "api/skimage.data#skimage.data.clock", "type": "data", "text": " \nskimage.data.clock() [source]\n \nMotion blurred clock. This photograph of a wall clock was taken while moving the camera in an aproximately horizontal direction. It may be used to illustrate inverse filters and deconvolution. Released into the public domain by the photographer (Stefan van der Walt).  Returns \n \nclock(300, 400) uint8 ndarray \n\nClock image.     \n"}, {"name": "data.coffee()", "path": "api/skimage.data#skimage.data.coffee", "type": "data", "text": " \nskimage.data.coffee() [source]\n \nCoffee cup. This photograph is courtesy of Pikolo Espresso Bar. It contains several elliptical shapes as well as varying texture (smooth porcelain to course wood grain).  Returns \n \ncoffee(400, 600, 3) uint8 ndarray \n\nCoffee image.     Notes No copyright restrictions. CC0 by the photographer (Rachel Michetti). \n"}, {"name": "data.coins()", "path": "api/skimage.data#skimage.data.coins", "type": "data", "text": " \nskimage.data.coins() [source]\n \nGreek coins from Pompeii. This image shows several coins outlined against a gray background. It is especially useful in, e.g. segmentation tests, where individual objects need to be identified against a background. The background shares enough grey levels with the coins that a simple segmentation is not sufficient.  Returns \n \ncoins(303, 384) uint8 ndarray \n\nCoins image.     Notes This image was downloaded from the Brooklyn Museum Collection. No known copyright restrictions. \n"}, {"name": "data.colorwheel()", "path": "api/skimage.data#skimage.data.colorwheel", "type": "data", "text": " \nskimage.data.colorwheel() [source]\n \nColor Wheel.  Returns \n \ncolorwheel(370, 371, 3) uint8 image \n\nA colorwheel.     \n"}, {"name": "data.download_all()", "path": "api/skimage.data#skimage.data.download_all", "type": "data", "text": " \nskimage.data.download_all(directory=None) [source]\n \nDownload all datasets for use with scikit-image offline. Scikit-image datasets are no longer shipped with the library by default. This allows us to use higher quality datasets, while keeping the library download size small. This function requires the installation of an optional dependency, pooch, to download the full dataset. Follow installation instruction found at https://scikit-image.org/docs/stable/install.html Call this function to download all sample images making them available offline on your machine.  Parameters \n directory: path-like, optional\n\nThe directory where the dataset should be stored.    Raises \n ModuleNotFoundError:\n\nIf pooch is not install, this error will be raised.     Notes scikit-image will only search for images stored in the default directory. Only specify the directory if you wish to download the images to your own folder for a particular reason. You can access the location of the default data directory by inspecting the variable skimage.data.data_dir. \n"}, {"name": "data.eagle()", "path": "api/skimage.data#skimage.data.eagle", "type": "data", "text": " \nskimage.data.eagle() [source]\n \nA golden eagle. Suitable for examples on segmentation, Hough transforms, and corner detection.  Returns \n \neagle(2019, 1826) uint8 ndarray \n\nEagle image.     Notes No copyright restrictions. CC0 by the photographer (Dayane Machado). \n"}, {"name": "data.grass()", "path": "api/skimage.data#skimage.data.grass", "type": "data", "text": " \nskimage.data.grass() [source]\n \nGrass.  Returns \n \ngrass(512, 512) uint8 image \n\nSome grass.     Notes The original image was downloaded from DeviantArt and licensed underthe Creative Commons CC0 License. The downloaded image was cropped to include a region of (512, 512) pixels around the top left corner, converted to grayscale, then to uint8 prior to saving the result in PNG format. \n"}, {"name": "data.gravel()", "path": "api/skimage.data#skimage.data.gravel", "type": "data", "text": " \nskimage.data.gravel() [source]\n \nGravel  Returns \n \ngravel(512, 512) uint8 image \n\nGrayscale gravel sample.     Notes The original image was downloaded from CC0Textures and licensed under the Creative Commons CC0 License. The downloaded image was then rescaled to (1024, 1024), then the top left (512, 512) pixel region was cropped prior to converting the image to grayscale and uint8 data type. The result was saved using the PNG format. \n"}, {"name": "data.horse()", "path": "api/skimage.data#skimage.data.horse", "type": "data", "text": " \nskimage.data.horse() [source]\n \nBlack and white silhouette of a horse. This image was downloaded from openclipart No copyright restrictions. CC0 given by owner (Andreas Preuss (marauder)).  Returns \n \nhorse(328, 400) bool ndarray \n\nHorse image.     \n"}, {"name": "data.hubble_deep_field()", "path": "api/skimage.data#skimage.data.hubble_deep_field", "type": "data", "text": " \nskimage.data.hubble_deep_field() [source]\n \nHubble eXtreme Deep Field. This photograph contains the Hubble Telescope\u2019s farthest ever view of the universe. It can be useful as an example for multi-scale detection.  Returns \n \nhubble_deep_field(872, 1000, 3) uint8 ndarray \n\nHubble deep field image.     Notes This image was downloaded from HubbleSite. The image was captured by NASA and may be freely used in the public domain. \n"}, {"name": "data.human_mitosis()", "path": "api/skimage.data#skimage.data.human_mitosis", "type": "data", "text": " \nskimage.data.human_mitosis() [source]\n \nImage of human cells undergoing mitosis.  Returns \n human_mitosis: (512, 512) uint8 ndimage\n\nData of human cells undergoing mitosis taken during the preperation of the manuscript in [1].     Notes Copyright David Root. Licensed under CC-0 [2]. References  \n1  \nMoffat J, Grueneberg DA, Yang X, Kim SY, Kloepfer AM, Hinkle G, Piqani B, Eisenhaure TM, Luo B, Grenier JK, Carpenter AE, Foo SY, Stewart SA, Stockwell BR, Hacohen N, Hahn WC, Lander ES, Sabatini DM, Root DE (2006) A lentiviral RNAi library for human and mouse genes applied to an arrayed viral high-content screen. Cell, 124(6):1283-98 / :DOI: 10.1016/j.cell.2006.01.040 PMID 16564017  \n2  \nGitHub licensing discussion https://github.com/CellProfiler/examples/issues/41   \n"}, {"name": "data.immunohistochemistry()", "path": "api/skimage.data#skimage.data.immunohistochemistry", "type": "data", "text": " \nskimage.data.immunohistochemistry() [source]\n \nImmunohistochemical (IHC) staining with hematoxylin counterstaining. This picture shows colonic glands where the IHC expression of FHL2 protein is revealed with DAB. Hematoxylin counterstaining is applied to enhance the negative parts of the tissue. This image was acquired at the Center for Microscopy And Molecular Imaging (CMMI). No known copyright restrictions.  Returns \n \nimmunohistochemistry(512, 512, 3) uint8 ndarray \n\nImmunohistochemistry image.     \n"}, {"name": "data.kidney()", "path": "api/skimage.data#skimage.data.kidney", "type": "data", "text": " \nskimage.data.kidney() [source]\n \nMouse kidney tissue. This biological tissue on a pre-prepared slide was imaged with confocal fluorescence microscopy (Nikon C1 inverted microscope). Image shape is (16, 512, 512, 3). That is 512x512 pixels in X-Y, 16 image slices in Z, and 3 color channels (emission wavelengths 450nm, 515nm, and 605nm, respectively). Real-space voxel size is 1.24 microns in X-Y, and 1.25 microns in Z. Data type is unsigned 16-bit integers.  Returns \n \nkidney(16, 512, 512, 3) uint16 ndarray \n\nKidney 3D multichannel image.     Notes This image was acquired by Genevieve Buckley at Monasoh Micro Imaging in 2018. License: CC0 \n"}, {"name": "data.lbp_frontal_face_cascade_filename()", "path": "api/skimage.data#skimage.data.lbp_frontal_face_cascade_filename", "type": "data", "text": " \nskimage.data.lbp_frontal_face_cascade_filename() [source]\n \nReturn the path to the XML file containing the weak classifier cascade. These classifiers were trained using LBP features. The file is part of the OpenCV repository [1]. References  \n1  \nOpenCV lbpcascade trained files https://github.com/opencv/opencv/tree/master/data/lbpcascades   \n"}, {"name": "data.lfw_subset()", "path": "api/skimage.data#skimage.data.lfw_subset", "type": "data", "text": " \nskimage.data.lfw_subset() [source]\n \nSubset of data from the LFW dataset. This database is a subset of the LFW database containing:  100 faces 100 non-faces  The full dataset is available at [2].  Returns \n \nimages(200, 25, 25) uint8 ndarray \n\n100 first images are faces and subsequent 100 are non-faces.     Notes The faces were randomly selected from the LFW dataset and the non-faces were extracted from the background of the same dataset. The cropped ROIs have been resized to a 25 x 25 pixels. References  \n1  \nHuang, G., Mattar, M., Lee, H., & Learned-Miller, E. G. (2012). Learning to align from scratch. In Advances in Neural Information Processing Systems (pp. 764-772).  \n2  \nhttp://vis-www.cs.umass.edu/lfw/   \n"}, {"name": "data.lily()", "path": "api/skimage.data#skimage.data.lily", "type": "data", "text": " \nskimage.data.lily() [source]\n \nLily of the valley plant stem. This plant stem on a pre-prepared slide was imaged with confocal fluorescence microscopy (Nikon C1 inverted microscope). Image shape is (922, 922, 4). That is 922x922 pixels in X-Y, with 4 color channels. Real-space voxel size is 1.24 microns in X-Y. Data type is unsigned 16-bit integers.  Returns \n \nlily(922, 922, 4) uint16 ndarray \n\nLily 2D multichannel image.     Notes This image was acquired by Genevieve Buckley at Monasoh Micro Imaging in 2018. License: CC0 \n"}, {"name": "data.logo()", "path": "api/skimage.data#skimage.data.logo", "type": "data", "text": " \nskimage.data.logo() [source]\n \nScikit-image logo, a RGBA image.  Returns \n \nlogo(500, 500, 4) uint8 ndarray \n\nLogo image.     \n"}, {"name": "data.microaneurysms()", "path": "api/skimage.data#skimage.data.microaneurysms", "type": "data", "text": " \nskimage.data.microaneurysms() [source]\n \nGray-level \u201cmicroaneurysms\u201d image. Detail from an image of the retina (green channel). The image is a crop of image 07_dr.JPG from the High-Resolution Fundus (HRF) Image Database: https://www5.cs.fau.de/research/data/fundus-images/  Returns \n \nmicroaneurysms(102, 102) uint8 ndarray \n\nRetina image with lesions.     Notes No copyright restrictions. CC0 given by owner (Andreas Maier). References  \n1  \nBudai, A., Bock, R, Maier, A., Hornegger, J., Michelson, G. (2013). Robust Vessel Segmentation in Fundus Images. International Journal of Biomedical Imaging, vol. 2013, 2013. DOI:10.1155/2013/154860   \n"}, {"name": "data.moon()", "path": "api/skimage.data#skimage.data.moon", "type": "data", "text": " \nskimage.data.moon() [source]\n \nSurface of the moon. This low-contrast image of the surface of the moon is useful for illustrating histogram equalization and contrast stretching.  Returns \n \nmoon(512, 512) uint8 ndarray \n\nMoon image.     \n"}, {"name": "data.page()", "path": "api/skimage.data#skimage.data.page", "type": "data", "text": " \nskimage.data.page() [source]\n \nScanned page. This image of printed text is useful for demonstrations requiring uneven background illumination.  Returns \n \npage(191, 384) uint8 ndarray \n\nPage image.     \n"}, {"name": "data.retina()", "path": "api/skimage.data#skimage.data.retina", "type": "data", "text": " \nskimage.data.retina() [source]\n \nHuman retina. This image of a retina is useful for demonstrations requiring circular images.  Returns \n \nretina(1411, 1411, 3) uint8 ndarray \n\nRetina image in RGB.     Notes This image was downloaded from wikimedia. This file is made available under the Creative Commons CC0 1.0 Universal Public Domain Dedication. References  \n1  \nH\u00e4ggstr\u00f6m, Mikael (2014). \u201cMedical gallery of Mikael H\u00e4ggstr\u00f6m 2014\u201d. WikiJournal of Medicine 1 (2). DOI:10.15347/wjm/2014.008. ISSN 2002-4436. Public Domain   \n"}, {"name": "data.rocket()", "path": "api/skimage.data#skimage.data.rocket", "type": "data", "text": " \nskimage.data.rocket() [source]\n \nLaunch photo of DSCOVR on Falcon 9 by SpaceX. This is the launch photo of Falcon 9 carrying DSCOVR lifted off from SpaceX\u2019s Launch Complex 40 at Cape Canaveral Air Force Station, FL.  Returns \n \nrocket(427, 640, 3) uint8 ndarray \n\nRocket image.     Notes This image was downloaded from SpaceX Photos. The image was captured by SpaceX and released in the public domain. \n"}, {"name": "data.shepp_logan_phantom()", "path": "api/skimage.data#skimage.data.shepp_logan_phantom", "type": "data", "text": " \nskimage.data.shepp_logan_phantom() [source]\n \nShepp Logan Phantom.  Returns \n \nphantom(400, 400) float64 image \n\nImage of the Shepp-Logan phantom in grayscale.     References  \n1  \nL. A. Shepp and B. F. Logan, \u201cThe Fourier reconstruction of a head section,\u201d in IEEE Transactions on Nuclear Science, vol. 21, no. 3, pp. 21-43, June 1974. DOI:10.1109/TNS.1974.6499235   \n"}, {"name": "data.skin()", "path": "api/skimage.data#skimage.data.skin", "type": "data", "text": " \nskimage.data.skin() [source]\n \nMicroscopy image of dermis and epidermis (skin layers). Hematoxylin and eosin stained slide at 10x of normal epidermis and dermis with a benign intradermal nevus.  Returns \n \nskin(960, 1280, 3) RGB image of uint8 \n   Notes This image requires an Internet connection the first time it is called, and to have the pooch package installed, in order to fetch the image file from the scikit-image datasets repository. The source of this image is https://en.wikipedia.org/wiki/File:Normal_Epidermis_and_Dermis_with_Intradermal_Nevus_10x.JPG The image was released in the public domain by its author Kilbad. \n"}, {"name": "data.stereo_motorcycle()", "path": "api/skimage.data#skimage.data.stereo_motorcycle", "type": "data", "text": " \nskimage.data.stereo_motorcycle() [source]\n \nRectified stereo image pair with ground-truth disparities. The two images are rectified such that every pixel in the left image has its corresponding pixel on the same scanline in the right image. That means that both images are warped such that they have the same orientation but a horizontal spatial offset (baseline). The ground-truth pixel offset in column direction is specified by the included disparity map. The two images are part of the Middlebury 2014 stereo benchmark. The dataset was created by Nera Nesic, Porter Westling, Xi Wang, York Kitajima, Greg Krathwohl, and Daniel Scharstein at Middlebury College. A detailed description of the acquisition process can be found in [1]. The images included here are down-sampled versions of the default exposure images in the benchmark. The images are down-sampled by a factor of 4 using the function skimage.transform.downscale_local_mean. The calibration data in the following and the included ground-truth disparity map are valid for the down-sampled images: Focal length:           994.978px\nPrincipal point x:      311.193px\nPrincipal point y:      254.877px\nPrincipal point dx:      31.086px\nBaseline:               193.001mm\n  Returns \n \nimg_left(500, 741, 3) uint8 ndarray \n\nLeft stereo image.  \nimg_right(500, 741, 3) uint8 ndarray \n\nRight stereo image.  \ndisp(500, 741, 3) float ndarray \n\nGround-truth disparity map, where each value describes the offset in column direction between corresponding pixels in the left and the right stereo images. E.g. the corresponding pixel of img_left[10, 10 + disp[10, 10]] is img_right[10, 10]. NaNs denote pixels in the left image that do not have ground-truth.     Notes The original resolution images, images with different exposure and lighting, and ground-truth depth maps can be found at the Middlebury website [2]. References  \n1  \nD. Scharstein, H. Hirschmueller, Y. Kitajima, G. Krathwohl, N. Nesic, X. Wang, and P. Westling. High-resolution stereo datasets with subpixel-accurate ground truth. In German Conference on Pattern Recognition (GCPR 2014), Muenster, Germany, September 2014.  \n2  \nhttp://vision.middlebury.edu/stereo/data/scenes2014/   \n"}, {"name": "data.text()", "path": "api/skimage.data#skimage.data.text", "type": "data", "text": " \nskimage.data.text() [source]\n \nGray-level \u201ctext\u201d image used for corner detection.  Returns \n \ntext(172, 448) uint8 ndarray \n\nText image.     Notes This image was downloaded from Wikipedia <https://en.wikipedia.org/wiki/File:Corner.png>`__. No known copyright restrictions, released into the public domain. \n"}, {"name": "draw", "path": "api/skimage.draw", "type": "draw", "text": "Module: draw  \nskimage.draw.bezier_curve(r0, c0, r1, c1, \u2026) Generate Bezier curve coordinates.  \nskimage.draw.circle(r, c, radius[, shape]) Generate coordinates of pixels within circle.  \nskimage.draw.circle_perimeter(r, c, radius) Generate circle perimeter coordinates.  \nskimage.draw.circle_perimeter_aa(r, c, radius) Generate anti-aliased circle perimeter coordinates.  \nskimage.draw.disk(center, radius, *[, shape]) Generate coordinates of pixels within circle.  \nskimage.draw.ellipse(r, c, r_radius, c_radius) Generate coordinates of pixels within ellipse.  \nskimage.draw.ellipse_perimeter(r, c, \u2026[, \u2026]) Generate ellipse perimeter coordinates.  \nskimage.draw.ellipsoid(a, b, c[, spacing, \u2026]) Generates ellipsoid with semimajor axes aligned with grid dimensions on grid with specified spacing.  \nskimage.draw.ellipsoid_stats(a, b, c) Calculates analytical surface area and volume for ellipsoid with semimajor axes aligned with grid dimensions of specified spacing.  \nskimage.draw.line(r0, c0, r1, c1) Generate line pixel coordinates.  \nskimage.draw.line_aa(r0, c0, r1, c1) Generate anti-aliased line pixel coordinates.  \nskimage.draw.line_nd(start, stop, *[, \u2026]) Draw a single-pixel thick line in n dimensions.  \nskimage.draw.polygon(r, c[, shape]) Generate coordinates of pixels within polygon.  \nskimage.draw.polygon2mask(image_shape, polygon) Compute a mask from polygon.  \nskimage.draw.polygon_perimeter(r, c[, \u2026]) Generate polygon perimeter coordinates.  \nskimage.draw.random_shapes(image_shape, \u2026) Generate an image with random shapes, labeled with bounding boxes.  \nskimage.draw.rectangle(start[, end, extent, \u2026]) Generate coordinates of pixels within a rectangle.  \nskimage.draw.rectangle_perimeter(start[, \u2026]) Generate coordinates of pixels that are exactly around a rectangle.  \nskimage.draw.set_color(image, coords, color) Set pixel color in the image at the given coordinates.   bezier_curve  \nskimage.draw.bezier_curve(r0, c0, r1, c1, r2, c2, weight, shape=None) [source]\n \nGenerate Bezier curve coordinates.  Parameters \n \nr0, c0int \n\nCoordinates of the first control point.  \nr1, c1int \n\nCoordinates of the middle control point.  \nr2, c2int \n\nCoordinates of the last control point.  \nweightdouble \n\nMiddle control point weight, it describes the line tension.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for curves that exceed the image size. If None, the full extent of the curve is used.    Returns \n \nrr, cc(N,) ndarray of int \n\nIndices of pixels that belong to the Bezier curve. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Notes The algorithm is the rational quadratic algorithm presented in reference [1]. References  \n1  \nA Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012 http://members.chello.at/easyfilter/Bresenham.pdf   Examples >>> import numpy as np\n>>> from skimage.draw import bezier_curve\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = bezier_curve(1, 5, 5, -2, 8, 8, 2)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n circle  \nskimage.draw.circle(r, c, radius, shape=None) [source]\n \nGenerate coordinates of pixels within circle.  Parameters \n \nr, cdouble \n\nCenter coordinate of disk.  \nradiusdouble \n\nRadius of disk.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for disks that exceed the image size. If None, the full extent of the disk is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.    Returns \n \nrr, ccndarray of int \n\nPixel coordinates of disk. May be used to directly index into an array, e.g. img[rr, cc] = 1.    Warns \n Deprecated:\n\n New in version 0.17: This function is deprecated and will be removed in scikit-image 0.19. Please use the function named disk instead.      \n circle_perimeter  \nskimage.draw.circle_perimeter(r, c, radius, method='bresenham', shape=None) [source]\n \nGenerate circle perimeter coordinates.  Parameters \n \nr, cint \n\nCentre coordinate of circle.  \nradiusint \n\nRadius of circle.  \nmethod{\u2018bresenham\u2019, \u2018andres\u2019}, optional \n\nbresenham : Bresenham method (default) andres : Andres method  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for circles that exceed the image size. If None, the full extent of the circle is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.    Returns \n \nrr, cc(N,) ndarray of int \n\nBresenham and Andres\u2019 method: Indices of pixels that belong to the circle perimeter. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Notes Andres method presents the advantage that concentric circles create a disc whereas Bresenham can make holes. There is also less distortions when Andres circles are rotated. Bresenham method is also known as midpoint circle algorithm. Anti-aliased circle generator is available with circle_perimeter_aa. References  \n1  \nJ.E. Bresenham, \u201cAlgorithm for computer control of a digital plotter\u201d, IBM Systems journal, 4 (1965) 25-30.  \n2  \nE. Andres, \u201cDiscrete circles, rings and spheres\u201d, Computers & Graphics, 18 (1994) 695-706.   Examples >>> from skimage.draw import circle_perimeter\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = circle_perimeter(4, 4, 3)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n circle_perimeter_aa  \nskimage.draw.circle_perimeter_aa(r, c, radius, shape=None) [source]\n \nGenerate anti-aliased circle perimeter coordinates.  Parameters \n \nr, cint \n\nCentre coordinate of circle.  \nradiusint \n\nRadius of circle.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for circles that exceed the image size. If None, the full extent of the circle is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.    Returns \n \nrr, cc, val(N,) ndarray (int, int, float) \n\nIndices of pixels (rr, cc) and intensity values (val). img[rr, cc] = val.     Notes Wu\u2019s method draws anti-aliased circle. This implementation doesn\u2019t use lookup table optimization. Use the function draw.set_color to apply circle_perimeter_aa results to color images. References  \n1  \nX. Wu, \u201cAn efficient antialiasing technique\u201d, In ACM SIGGRAPH Computer Graphics, 25 (1991) 143-152.   Examples >>> from skimage.draw import circle_perimeter_aa\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc, val = circle_perimeter_aa(4, 4, 3)\n>>> img[rr, cc] = val * 255\n>>> img\narray([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,  60, 211, 255, 211,  60,   0,   0,   0],\n       [  0,  60, 194,  43,   0,  43, 194,  60,   0,   0],\n       [  0, 211,  43,   0,   0,   0,  43, 211,   0,   0],\n       [  0, 255,   0,   0,   0,   0,   0, 255,   0,   0],\n       [  0, 211,  43,   0,   0,   0,  43, 211,   0,   0],\n       [  0,  60, 194,  43,   0,  43, 194,  60,   0,   0],\n       [  0,   0,  60, 211, 255, 211,  60,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=uint8)\n >>> from skimage import data, draw\n>>> image = data.chelsea()\n>>> rr, cc, val = draw.circle_perimeter_aa(r=100, c=100, radius=75)\n>>> draw.set_color(image, (rr, cc), [1, 0, 0], alpha=val)\n \n disk  \nskimage.draw.disk(center, radius, *, shape=None) [source]\n \nGenerate coordinates of pixels within circle.  Parameters \n \ncentertuple \n\nCenter coordinate of disk.  \nradiusdouble \n\nRadius of disk.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for disks that exceed the image size. If None, the full extent of the disk is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.    Returns \n \nrr, ccndarray of int \n\nPixel coordinates of disk. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Examples >>> from skimage.draw import disk\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = disk((4, 4), 5)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n ellipse  \nskimage.draw.ellipse(r, c, r_radius, c_radius, shape=None, rotation=0.0) [source]\n \nGenerate coordinates of pixels within ellipse.  Parameters \n \nr, cdouble \n\nCentre coordinate of ellipse.  \nr_radius, c_radiusdouble \n\nMinor and major semi-axes. (r/r_radius)**2 + (c/c_radius)**2 = 1.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for ellipses which exceed the image size. By default the full extent of the ellipse are used. Must be at least length 2. Only the first two values are used to determine the extent.  \nrotationfloat, optional (default 0.) \n\nSet the ellipse rotation (rotation) in range (-PI, PI) in contra clock wise direction, so PI/2 degree means swap ellipse axis    Returns \n \nrr, ccndarray of int \n\nPixel coordinates of ellipse. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Notes The ellipse equation: ((x * cos(alpha) + y * sin(alpha)) / x_radius) ** 2 +\n((x * sin(alpha) - y * cos(alpha)) / y_radius) ** 2 = 1\n Note that the positions of ellipse without specified shape can have also, negative values, as this is correct on the plane. On the other hand using these ellipse positions for an image afterwards may lead to appearing on the other side of image, because image[-1, -1] = image[end-1, end-1] >>> rr, cc = ellipse(1, 2, 3, 6)\n>>> img = np.zeros((6, 12), dtype=np.uint8)\n>>> img[rr, cc] = 1\n>>> img\narray([[1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1]], dtype=uint8)\n Examples >>> from skimage.draw import ellipse\n>>> img = np.zeros((10, 12), dtype=np.uint8)\n>>> rr, cc = ellipse(5, 6, 3, 5, rotation=np.deg2rad(30))\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n Examples using skimage.draw.ellipse\n \n  Masked Normalized Cross-Correlation  \n\n  Measure region properties   ellipse_perimeter  \nskimage.draw.ellipse_perimeter(r, c, r_radius, c_radius, orientation=0, shape=None) [source]\n \nGenerate ellipse perimeter coordinates.  Parameters \n \nr, cint \n\nCentre coordinate of ellipse.  \nr_radius, c_radiusint \n\nMinor and major semi-axes. (r/r_radius)**2 + (c/c_radius)**2 = 1.  \norientationdouble, optional \n\nMajor axis orientation in clockwise direction as radians.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for ellipses that exceed the image size. If None, the full extent of the ellipse is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.    Returns \n \nrr, cc(N,) ndarray of int \n\nIndices of pixels that belong to the ellipse perimeter. May be used to directly index into an array, e.g. img[rr, cc] = 1.     References  \n1  \nA Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012 http://members.chello.at/easyfilter/Bresenham.pdf   Examples >>> from skimage.draw import ellipse_perimeter\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = ellipse_perimeter(5, 5, 3, 4)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n Note that the positions of ellipse without specified shape can have also, negative values, as this is correct on the plane. On the other hand using these ellipse positions for an image afterwards may lead to appearing on the other side of image, because image[-1, -1] = image[end-1, end-1] >>> rr, cc = ellipse_perimeter(2, 3, 4, 5)\n>>> img = np.zeros((9, 12), dtype=np.uint8)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n       [0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]], dtype=uint8)\n \n ellipsoid  \nskimage.draw.ellipsoid(a, b, c, spacing=(1.0, 1.0, 1.0), levelset=False) [source]\n \nGenerates ellipsoid with semimajor axes aligned with grid dimensions on grid with specified spacing.  Parameters \n \nafloat \n\nLength of semimajor axis aligned with x-axis.  \nbfloat \n\nLength of semimajor axis aligned with y-axis.  \ncfloat \n\nLength of semimajor axis aligned with z-axis.  \nspacingtuple of floats, length 3 \n\nSpacing in (x, y, z) spatial dimensions.  \nlevelsetbool \n\nIf True, returns the level set for this ellipsoid (signed level set about zero, with positive denoting interior) as np.float64. False returns a binarized version of said level set.    Returns \n \nellip(N, M, P) array \n\nEllipsoid centered in a correctly sized array for given spacing. Boolean dtype unless levelset=True, in which case a float array is returned with the level set above 0.0 representing the ellipsoid.     \n ellipsoid_stats  \nskimage.draw.ellipsoid_stats(a, b, c) [source]\n \nCalculates analytical surface area and volume for ellipsoid with semimajor axes aligned with grid dimensions of specified spacing.  Parameters \n \nafloat \n\nLength of semimajor axis aligned with x-axis.  \nbfloat \n\nLength of semimajor axis aligned with y-axis.  \ncfloat \n\nLength of semimajor axis aligned with z-axis.    Returns \n \nvolfloat \n\nCalculated volume of ellipsoid.  \nsurffloat \n\nCalculated surface area of ellipsoid.     \n line  \nskimage.draw.line(r0, c0, r1, c1) [source]\n \nGenerate line pixel coordinates.  Parameters \n \nr0, c0int \n\nStarting position (row, column).  \nr1, c1int \n\nEnd position (row, column).    Returns \n \nrr, cc(N,) ndarray of int \n\nIndices of pixels that belong to the line. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Notes Anti-aliased line generator is available with line_aa. Examples >>> from skimage.draw import line\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = line(1, 1, 8, 8)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n line_aa  \nskimage.draw.line_aa(r0, c0, r1, c1) [source]\n \nGenerate anti-aliased line pixel coordinates.  Parameters \n \nr0, c0int \n\nStarting position (row, column).  \nr1, c1int \n\nEnd position (row, column).    Returns \n \nrr, cc, val(N,) ndarray (int, int, float) \n\nIndices of pixels (rr, cc) and intensity values (val). img[rr, cc] = val.     References  \n1  \nA Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012 http://members.chello.at/easyfilter/Bresenham.pdf   Examples >>> from skimage.draw import line_aa\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc, val = line_aa(1, 1, 8, 8)\n>>> img[rr, cc] = val * 255\n>>> img\narray([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0, 255,  74,   0,   0,   0,   0,   0,   0,   0],\n       [  0,  74, 255,  74,   0,   0,   0,   0,   0,   0],\n       [  0,   0,  74, 255,  74,   0,   0,   0,   0,   0],\n       [  0,   0,   0,  74, 255,  74,   0,   0,   0,   0],\n       [  0,   0,   0,   0,  74, 255,  74,   0,   0,   0],\n       [  0,   0,   0,   0,   0,  74, 255,  74,   0,   0],\n       [  0,   0,   0,   0,   0,   0,  74, 255,  74,   0],\n       [  0,   0,   0,   0,   0,   0,   0,  74, 255,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=uint8)\n \n line_nd  \nskimage.draw.line_nd(start, stop, *, endpoint=False, integer=True) [source]\n \nDraw a single-pixel thick line in n dimensions. The line produced will be ndim-connected. That is, two subsequent pixels in the line will be either direct or diagonal neighbours in n dimensions.  Parameters \n \nstartarray-like, shape (N,) \n\nThe start coordinates of the line.  \nstoparray-like, shape (N,) \n\nThe end coordinates of the line.  \nendpointbool, optional \n\nWhether to include the endpoint in the returned line. Defaults to False, which allows for easy drawing of multi-point paths.  \nintegerbool, optional \n\nWhether to round the coordinates to integer. If True (default), the returned coordinates can be used to directly index into an array. False could be used for e.g. vector drawing.    Returns \n \ncoordstuple of arrays \n\nThe coordinates of points on the line.     Examples >>> lin = line_nd((1, 1), (5, 2.5), endpoint=False)\n>>> lin\n(array([1, 2, 3, 4]), array([1, 1, 2, 2]))\n>>> im = np.zeros((6, 5), dtype=int)\n>>> im[lin] = 1\n>>> im\narray([[0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0],\n       [0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0]])\n>>> line_nd([2, 1, 1], [5, 5, 2.5], endpoint=True)\n(array([2, 3, 4, 4, 5]), array([1, 2, 3, 4, 5]), array([1, 1, 2, 2, 2]))\n \n polygon  \nskimage.draw.polygon(r, c, shape=None) [source]\n \nGenerate coordinates of pixels within polygon.  Parameters \n \nr(N,) ndarray \n\nRow coordinates of vertices of polygon.  \nc(N,) ndarray \n\nColumn coordinates of vertices of polygon.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for polygons that exceed the image size. If None, the full extent of the polygon is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.    Returns \n \nrr, ccndarray of int \n\nPixel coordinates of polygon. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Examples >>> from skimage.draw import polygon\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> r = np.array([1, 2, 8])\n>>> c = np.array([1, 7, 4])\n>>> rr, cc = polygon(r, c)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n polygon2mask  \nskimage.draw.polygon2mask(image_shape, polygon) [source]\n \nCompute a mask from polygon.  Parameters \n \nimage_shapetuple of size 2. \n\nThe shape of the mask.  \npolygonarray_like. \n\nThe polygon coordinates of shape (N, 2) where N is the number of points.    Returns \n \nmask2-D ndarray of type \u2018bool\u2019. \n\nThe mask that corresponds to the input polygon.     Notes This function does not do any border checking, so that all the vertices need to be within the given shape. Examples >>> image_shape = (128, 128)\n>>> polygon = np.array([[60, 100], [100, 40], [40, 40]])\n>>> mask = polygon2mask(image_shape, polygon)\n>>> mask.shape\n(128, 128)\n \n polygon_perimeter  \nskimage.draw.polygon_perimeter(r, c, shape=None, clip=False) [source]\n \nGenerate polygon perimeter coordinates.  Parameters \n \nr(N,) ndarray \n\nRow coordinates of vertices of polygon.  \nc(N,) ndarray \n\nColumn coordinates of vertices of polygon.  \nshapetuple, optional \n\nImage shape which is used to determine maximum extents of output pixel coordinates. This is useful for polygons that exceed the image size. If None, the full extents of the polygon is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.  \nclipbool, optional \n\nWhether to clip the polygon to the provided shape. If this is set to True, the drawn figure will always be a closed polygon with all edges visible.    Returns \n \nrr, ccndarray of int \n\nPixel coordinates of polygon. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Examples >>> from skimage.draw import polygon_perimeter\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = polygon_perimeter([5, -1, 5, 10],\n...                            [-1, 5, 11, 5],\n...                            shape=img.shape, clip=True)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 1, 1, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 1, 0, 0, 0, 1, 1, 0],\n       [0, 0, 0, 0, 1, 1, 1, 0, 0, 0]], dtype=uint8)\n \n random_shapes  \nskimage.draw.random_shapes(image_shape, max_shapes, min_shapes=1, min_size=2, max_size=None, multichannel=True, num_channels=3, shape=None, intensity_range=None, allow_overlap=False, num_trials=100, random_seed=None) [source]\n \nGenerate an image with random shapes, labeled with bounding boxes. The image is populated with random shapes with random sizes, random locations, and random colors, with or without overlap. Shapes have random (row, col) starting coordinates and random sizes bounded by min_size and max_size. It can occur that a randomly generated shape will not fit the image at all. In that case, the algorithm will try again with new starting coordinates a certain number of times. However, it also means that some shapes may be skipped altogether. In that case, this function will generate fewer shapes than requested.  Parameters \n \nimage_shapetuple \n\nThe number of rows and columns of the image to generate.  \nmax_shapesint \n\nThe maximum number of shapes to (attempt to) fit into the shape.  \nmin_shapesint, optional \n\nThe minimum number of shapes to (attempt to) fit into the shape.  \nmin_sizeint, optional \n\nThe minimum dimension of each shape to fit into the image.  \nmax_sizeint, optional \n\nThe maximum dimension of each shape to fit into the image.  \nmultichannelbool, optional \n\nIf True, the generated image has num_channels color channels, otherwise generates grayscale image.  \nnum_channelsint, optional \n\nNumber of channels in the generated image. If 1, generate monochrome images, else color images with multiple channels. Ignored if multichannel is set to False.  \nshape{rectangle, circle, triangle, ellipse, None} str, optional \n\nThe name of the shape to generate or None to pick random ones.  \nintensity_range{tuple of tuples of uint8, tuple of uint8}, optional \n\nThe range of values to sample pixel values from. For grayscale images the format is (min, max). For multichannel - ((min, max),) if the ranges are equal across the channels, and ((min_0, max_0), \u2026 (min_N, max_N)) if they differ. As the function supports generation of uint8 arrays only, the maximum range is (0, 255). If None, set to (0, 254) for each channel reserving color of intensity = 255 for background.  \nallow_overlapbool, optional \n\nIf True, allow shapes to overlap.  \nnum_trialsint, optional \n\nHow often to attempt to fit a shape into the image before skipping it.  \nrandom_seedint, optional \n\nSeed to initialize the random number generator. If None, a random seed from the operating system is used.    Returns \n \nimageuint8 array \n\nAn image with the fitted shapes.  \nlabelslist \n\nA list of labels, one per shape in the image. Each label is a (category, ((r0, r1), (c0, c1))) tuple specifying the category and bounding box coordinates of the shape.     Examples >>> import skimage.draw\n>>> image, labels = skimage.draw.random_shapes((32, 32), max_shapes=3)\n>>> image \narray([\n   [[255, 255, 255],\n    [255, 255, 255],\n    [255, 255, 255],\n    ...,\n    [255, 255, 255],\n    [255, 255, 255],\n    [255, 255, 255]]], dtype=uint8)\n>>> labels \n[('circle', ((22, 18), (25, 21))),\n ('triangle', ((5, 6), (13, 13)))]\n \n rectangle  \nskimage.draw.rectangle(start, end=None, extent=None, shape=None) [source]\n \nGenerate coordinates of pixels within a rectangle.  Parameters \n \nstarttuple \n\nOrigin point of the rectangle, e.g., ([plane,] row, column).  \nendtuple \n\nEnd point of the rectangle ([plane,] row, column). For a 2D matrix, the slice defined by the rectangle is [start:(end+1)]. Either end or extent must be specified.  \nextenttuple \n\nThe extent (size) of the drawn rectangle. E.g., ([num_planes,] num_rows, num_cols). Either end or extent must be specified. A negative extent is valid, and will result in a rectangle going along the opposite direction. If extent is negative, the start point is not included.  \nshapetuple, optional \n\nImage shape used to determine the maximum bounds of the output coordinates. This is useful for clipping rectangles that exceed the image size. By default, no clipping is done.    Returns \n \ncoordsarray of int, shape (Ndim, Npoints) \n\nThe coordinates of all pixels in the rectangle.     Notes This function can be applied to N-dimensional images, by passing start and end or extent as tuples of length N. Examples >>> import numpy as np\n>>> from skimage.draw import rectangle\n>>> img = np.zeros((5, 5), dtype=np.uint8)\n>>> start = (1, 1)\n>>> extent = (3, 3)\n>>> rr, cc = rectangle(start, extent=extent, shape=img.shape)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n >>> img = np.zeros((5, 5), dtype=np.uint8)\n>>> start = (0, 1)\n>>> end = (3, 3)\n>>> rr, cc = rectangle(start, end=end, shape=img.shape)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n >>> import numpy as np\n>>> from skimage.draw import rectangle\n>>> img = np.zeros((6, 6), dtype=np.uint8)\n>>> start = (3, 3)\n>>>\n>>> rr, cc = rectangle(start, extent=(2, 2))\n>>> img[rr, cc] = 1\n>>> rr, cc = rectangle(start, extent=(-2, 2))\n>>> img[rr, cc] = 2\n>>> rr, cc = rectangle(start, extent=(-2, -2))\n>>> img[rr, cc] = 3\n>>> rr, cc = rectangle(start, extent=(2, -2))\n>>> img[rr, cc] = 4\n>>> print(img)\n[[0 0 0 0 0 0]\n [0 3 3 2 2 0]\n [0 3 3 2 2 0]\n [0 4 4 1 1 0]\n [0 4 4 1 1 0]\n [0 0 0 0 0 0]]\n \n rectangle_perimeter  \nskimage.draw.rectangle_perimeter(start, end=None, extent=None, shape=None, clip=False) [source]\n \nGenerate coordinates of pixels that are exactly around a rectangle.  Parameters \n \nstarttuple \n\nOrigin point of the inner rectangle, e.g., (row, column).  \nendtuple \n\nEnd point of the inner rectangle (row, column). For a 2D matrix, the slice defined by inner the rectangle is [start:(end+1)]. Either end or extent must be specified.  \nextenttuple \n\nThe extent (size) of the inner rectangle. E.g., (num_rows, num_cols). Either end or extent must be specified. Negative extents are permitted. See rectangle to better understand how they behave.  \nshapetuple, optional \n\nImage shape used to determine the maximum bounds of the output coordinates. This is useful for clipping perimeters that exceed the image size. By default, no clipping is done. Must be at least length 2. Only the first two values are used to determine the extent of the input image.  \nclipbool, optional \n\nWhether to clip the perimeter to the provided shape. If this is set to True, the drawn figure will always be a closed polygon with all edges visible.    Returns \n \ncoordsarray of int, shape (2, Npoints) \n\nThe coordinates of all pixels in the rectangle.     Examples >>> import numpy as np\n>>> from skimage.draw import rectangle_perimeter\n>>> img = np.zeros((5, 6), dtype=np.uint8)\n>>> start = (2, 3)\n>>> end = (3, 4)\n>>> rr, cc = rectangle_perimeter(start, end=end, shape=img.shape)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1],\n       [0, 0, 1, 0, 0, 1],\n       [0, 0, 1, 0, 0, 1],\n       [0, 0, 1, 1, 1, 1]], dtype=uint8)\n >>> img = np.zeros((5, 5), dtype=np.uint8)\n>>> r, c = rectangle_perimeter(start, (10, 10), shape=img.shape, clip=True)\n>>> img[r, c] = 1\n>>> img\narray([[0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1],\n       [0, 0, 1, 0, 1],\n       [0, 0, 1, 0, 1],\n       [0, 0, 1, 1, 1]], dtype=uint8)\n \n set_color  \nskimage.draw.set_color(image, coords, color, alpha=1) [source]\n \nSet pixel color in the image at the given coordinates. Note that this function modifies the color of the image in-place. Coordinates that exceed the shape of the image will be ignored.  Parameters \n \nimage(M, N, D) ndarray \n\nImage  \ncoordstuple of ((P,) ndarray, (P,) ndarray) \n\nRow and column coordinates of pixels to be colored.  \ncolor(D,) ndarray \n\nColor to be assigned to coordinates in the image.  \nalphascalar or (N,) ndarray \n\nAlpha values used to blend color with image. 0 is transparent, 1 is opaque.     Examples >>> from skimage.draw import line, set_color\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = line(1, 1, 20, 20)\n>>> set_color(img, (rr, cc), 1)\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], dtype=uint8)\n \n\n"}, {"name": "draw.bezier_curve()", "path": "api/skimage.draw#skimage.draw.bezier_curve", "type": "draw", "text": " \nskimage.draw.bezier_curve(r0, c0, r1, c1, r2, c2, weight, shape=None) [source]\n \nGenerate Bezier curve coordinates.  Parameters \n \nr0, c0int \n\nCoordinates of the first control point.  \nr1, c1int \n\nCoordinates of the middle control point.  \nr2, c2int \n\nCoordinates of the last control point.  \nweightdouble \n\nMiddle control point weight, it describes the line tension.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for curves that exceed the image size. If None, the full extent of the curve is used.    Returns \n \nrr, cc(N,) ndarray of int \n\nIndices of pixels that belong to the Bezier curve. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Notes The algorithm is the rational quadratic algorithm presented in reference [1]. References  \n1  \nA Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012 http://members.chello.at/easyfilter/Bresenham.pdf   Examples >>> import numpy as np\n>>> from skimage.draw import bezier_curve\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = bezier_curve(1, 5, 5, -2, 8, 8, 2)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "draw.circle()", "path": "api/skimage.draw#skimage.draw.circle", "type": "draw", "text": " \nskimage.draw.circle(r, c, radius, shape=None) [source]\n \nGenerate coordinates of pixels within circle.  Parameters \n \nr, cdouble \n\nCenter coordinate of disk.  \nradiusdouble \n\nRadius of disk.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for disks that exceed the image size. If None, the full extent of the disk is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.    Returns \n \nrr, ccndarray of int \n\nPixel coordinates of disk. May be used to directly index into an array, e.g. img[rr, cc] = 1.    Warns \n Deprecated:\n\n New in version 0.17: This function is deprecated and will be removed in scikit-image 0.19. Please use the function named disk instead.      \n"}, {"name": "draw.circle_perimeter()", "path": "api/skimage.draw#skimage.draw.circle_perimeter", "type": "draw", "text": " \nskimage.draw.circle_perimeter(r, c, radius, method='bresenham', shape=None) [source]\n \nGenerate circle perimeter coordinates.  Parameters \n \nr, cint \n\nCentre coordinate of circle.  \nradiusint \n\nRadius of circle.  \nmethod{\u2018bresenham\u2019, \u2018andres\u2019}, optional \n\nbresenham : Bresenham method (default) andres : Andres method  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for circles that exceed the image size. If None, the full extent of the circle is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.    Returns \n \nrr, cc(N,) ndarray of int \n\nBresenham and Andres\u2019 method: Indices of pixels that belong to the circle perimeter. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Notes Andres method presents the advantage that concentric circles create a disc whereas Bresenham can make holes. There is also less distortions when Andres circles are rotated. Bresenham method is also known as midpoint circle algorithm. Anti-aliased circle generator is available with circle_perimeter_aa. References  \n1  \nJ.E. Bresenham, \u201cAlgorithm for computer control of a digital plotter\u201d, IBM Systems journal, 4 (1965) 25-30.  \n2  \nE. Andres, \u201cDiscrete circles, rings and spheres\u201d, Computers & Graphics, 18 (1994) 695-706.   Examples >>> from skimage.draw import circle_perimeter\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = circle_perimeter(4, 4, 3)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "draw.circle_perimeter_aa()", "path": "api/skimage.draw#skimage.draw.circle_perimeter_aa", "type": "draw", "text": " \nskimage.draw.circle_perimeter_aa(r, c, radius, shape=None) [source]\n \nGenerate anti-aliased circle perimeter coordinates.  Parameters \n \nr, cint \n\nCentre coordinate of circle.  \nradiusint \n\nRadius of circle.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for circles that exceed the image size. If None, the full extent of the circle is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.    Returns \n \nrr, cc, val(N,) ndarray (int, int, float) \n\nIndices of pixels (rr, cc) and intensity values (val). img[rr, cc] = val.     Notes Wu\u2019s method draws anti-aliased circle. This implementation doesn\u2019t use lookup table optimization. Use the function draw.set_color to apply circle_perimeter_aa results to color images. References  \n1  \nX. Wu, \u201cAn efficient antialiasing technique\u201d, In ACM SIGGRAPH Computer Graphics, 25 (1991) 143-152.   Examples >>> from skimage.draw import circle_perimeter_aa\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc, val = circle_perimeter_aa(4, 4, 3)\n>>> img[rr, cc] = val * 255\n>>> img\narray([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,  60, 211, 255, 211,  60,   0,   0,   0],\n       [  0,  60, 194,  43,   0,  43, 194,  60,   0,   0],\n       [  0, 211,  43,   0,   0,   0,  43, 211,   0,   0],\n       [  0, 255,   0,   0,   0,   0,   0, 255,   0,   0],\n       [  0, 211,  43,   0,   0,   0,  43, 211,   0,   0],\n       [  0,  60, 194,  43,   0,  43, 194,  60,   0,   0],\n       [  0,   0,  60, 211, 255, 211,  60,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=uint8)\n >>> from skimage import data, draw\n>>> image = data.chelsea()\n>>> rr, cc, val = draw.circle_perimeter_aa(r=100, c=100, radius=75)\n>>> draw.set_color(image, (rr, cc), [1, 0, 0], alpha=val)\n \n"}, {"name": "draw.disk()", "path": "api/skimage.draw#skimage.draw.disk", "type": "draw", "text": " \nskimage.draw.disk(center, radius, *, shape=None) [source]\n \nGenerate coordinates of pixels within circle.  Parameters \n \ncentertuple \n\nCenter coordinate of disk.  \nradiusdouble \n\nRadius of disk.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for disks that exceed the image size. If None, the full extent of the disk is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.    Returns \n \nrr, ccndarray of int \n\nPixel coordinates of disk. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Examples >>> from skimage.draw import disk\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = disk((4, 4), 5)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "draw.ellipse()", "path": "api/skimage.draw#skimage.draw.ellipse", "type": "draw", "text": " \nskimage.draw.ellipse(r, c, r_radius, c_radius, shape=None, rotation=0.0) [source]\n \nGenerate coordinates of pixels within ellipse.  Parameters \n \nr, cdouble \n\nCentre coordinate of ellipse.  \nr_radius, c_radiusdouble \n\nMinor and major semi-axes. (r/r_radius)**2 + (c/c_radius)**2 = 1.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for ellipses which exceed the image size. By default the full extent of the ellipse are used. Must be at least length 2. Only the first two values are used to determine the extent.  \nrotationfloat, optional (default 0.) \n\nSet the ellipse rotation (rotation) in range (-PI, PI) in contra clock wise direction, so PI/2 degree means swap ellipse axis    Returns \n \nrr, ccndarray of int \n\nPixel coordinates of ellipse. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Notes The ellipse equation: ((x * cos(alpha) + y * sin(alpha)) / x_radius) ** 2 +\n((x * sin(alpha) - y * cos(alpha)) / y_radius) ** 2 = 1\n Note that the positions of ellipse without specified shape can have also, negative values, as this is correct on the plane. On the other hand using these ellipse positions for an image afterwards may lead to appearing on the other side of image, because image[-1, -1] = image[end-1, end-1] >>> rr, cc = ellipse(1, 2, 3, 6)\n>>> img = np.zeros((6, 12), dtype=np.uint8)\n>>> img[rr, cc] = 1\n>>> img\narray([[1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1]], dtype=uint8)\n Examples >>> from skimage.draw import ellipse\n>>> img = np.zeros((10, 12), dtype=np.uint8)\n>>> rr, cc = ellipse(5, 6, 3, 5, rotation=np.deg2rad(30))\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "draw.ellipse_perimeter()", "path": "api/skimage.draw#skimage.draw.ellipse_perimeter", "type": "draw", "text": " \nskimage.draw.ellipse_perimeter(r, c, r_radius, c_radius, orientation=0, shape=None) [source]\n \nGenerate ellipse perimeter coordinates.  Parameters \n \nr, cint \n\nCentre coordinate of ellipse.  \nr_radius, c_radiusint \n\nMinor and major semi-axes. (r/r_radius)**2 + (c/c_radius)**2 = 1.  \norientationdouble, optional \n\nMajor axis orientation in clockwise direction as radians.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for ellipses that exceed the image size. If None, the full extent of the ellipse is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.    Returns \n \nrr, cc(N,) ndarray of int \n\nIndices of pixels that belong to the ellipse perimeter. May be used to directly index into an array, e.g. img[rr, cc] = 1.     References  \n1  \nA Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012 http://members.chello.at/easyfilter/Bresenham.pdf   Examples >>> from skimage.draw import ellipse_perimeter\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = ellipse_perimeter(5, 5, 3, 4)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n Note that the positions of ellipse without specified shape can have also, negative values, as this is correct on the plane. On the other hand using these ellipse positions for an image afterwards may lead to appearing on the other side of image, because image[-1, -1] = image[end-1, end-1] >>> rr, cc = ellipse_perimeter(2, 3, 4, 5)\n>>> img = np.zeros((9, 12), dtype=np.uint8)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n       [0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "draw.ellipsoid()", "path": "api/skimage.draw#skimage.draw.ellipsoid", "type": "draw", "text": " \nskimage.draw.ellipsoid(a, b, c, spacing=(1.0, 1.0, 1.0), levelset=False) [source]\n \nGenerates ellipsoid with semimajor axes aligned with grid dimensions on grid with specified spacing.  Parameters \n \nafloat \n\nLength of semimajor axis aligned with x-axis.  \nbfloat \n\nLength of semimajor axis aligned with y-axis.  \ncfloat \n\nLength of semimajor axis aligned with z-axis.  \nspacingtuple of floats, length 3 \n\nSpacing in (x, y, z) spatial dimensions.  \nlevelsetbool \n\nIf True, returns the level set for this ellipsoid (signed level set about zero, with positive denoting interior) as np.float64. False returns a binarized version of said level set.    Returns \n \nellip(N, M, P) array \n\nEllipsoid centered in a correctly sized array for given spacing. Boolean dtype unless levelset=True, in which case a float array is returned with the level set above 0.0 representing the ellipsoid.     \n"}, {"name": "draw.ellipsoid_stats()", "path": "api/skimage.draw#skimage.draw.ellipsoid_stats", "type": "draw", "text": " \nskimage.draw.ellipsoid_stats(a, b, c) [source]\n \nCalculates analytical surface area and volume for ellipsoid with semimajor axes aligned with grid dimensions of specified spacing.  Parameters \n \nafloat \n\nLength of semimajor axis aligned with x-axis.  \nbfloat \n\nLength of semimajor axis aligned with y-axis.  \ncfloat \n\nLength of semimajor axis aligned with z-axis.    Returns \n \nvolfloat \n\nCalculated volume of ellipsoid.  \nsurffloat \n\nCalculated surface area of ellipsoid.     \n"}, {"name": "draw.line()", "path": "api/skimage.draw#skimage.draw.line", "type": "draw", "text": " \nskimage.draw.line(r0, c0, r1, c1) [source]\n \nGenerate line pixel coordinates.  Parameters \n \nr0, c0int \n\nStarting position (row, column).  \nr1, c1int \n\nEnd position (row, column).    Returns \n \nrr, cc(N,) ndarray of int \n\nIndices of pixels that belong to the line. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Notes Anti-aliased line generator is available with line_aa. Examples >>> from skimage.draw import line\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = line(1, 1, 8, 8)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "draw.line_aa()", "path": "api/skimage.draw#skimage.draw.line_aa", "type": "draw", "text": " \nskimage.draw.line_aa(r0, c0, r1, c1) [source]\n \nGenerate anti-aliased line pixel coordinates.  Parameters \n \nr0, c0int \n\nStarting position (row, column).  \nr1, c1int \n\nEnd position (row, column).    Returns \n \nrr, cc, val(N,) ndarray (int, int, float) \n\nIndices of pixels (rr, cc) and intensity values (val). img[rr, cc] = val.     References  \n1  \nA Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012 http://members.chello.at/easyfilter/Bresenham.pdf   Examples >>> from skimage.draw import line_aa\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc, val = line_aa(1, 1, 8, 8)\n>>> img[rr, cc] = val * 255\n>>> img\narray([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0, 255,  74,   0,   0,   0,   0,   0,   0,   0],\n       [  0,  74, 255,  74,   0,   0,   0,   0,   0,   0],\n       [  0,   0,  74, 255,  74,   0,   0,   0,   0,   0],\n       [  0,   0,   0,  74, 255,  74,   0,   0,   0,   0],\n       [  0,   0,   0,   0,  74, 255,  74,   0,   0,   0],\n       [  0,   0,   0,   0,   0,  74, 255,  74,   0,   0],\n       [  0,   0,   0,   0,   0,   0,  74, 255,  74,   0],\n       [  0,   0,   0,   0,   0,   0,   0,  74, 255,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=uint8)\n \n"}, {"name": "draw.line_nd()", "path": "api/skimage.draw#skimage.draw.line_nd", "type": "draw", "text": " \nskimage.draw.line_nd(start, stop, *, endpoint=False, integer=True) [source]\n \nDraw a single-pixel thick line in n dimensions. The line produced will be ndim-connected. That is, two subsequent pixels in the line will be either direct or diagonal neighbours in n dimensions.  Parameters \n \nstartarray-like, shape (N,) \n\nThe start coordinates of the line.  \nstoparray-like, shape (N,) \n\nThe end coordinates of the line.  \nendpointbool, optional \n\nWhether to include the endpoint in the returned line. Defaults to False, which allows for easy drawing of multi-point paths.  \nintegerbool, optional \n\nWhether to round the coordinates to integer. If True (default), the returned coordinates can be used to directly index into an array. False could be used for e.g. vector drawing.    Returns \n \ncoordstuple of arrays \n\nThe coordinates of points on the line.     Examples >>> lin = line_nd((1, 1), (5, 2.5), endpoint=False)\n>>> lin\n(array([1, 2, 3, 4]), array([1, 1, 2, 2]))\n>>> im = np.zeros((6, 5), dtype=int)\n>>> im[lin] = 1\n>>> im\narray([[0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0],\n       [0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0]])\n>>> line_nd([2, 1, 1], [5, 5, 2.5], endpoint=True)\n(array([2, 3, 4, 4, 5]), array([1, 2, 3, 4, 5]), array([1, 1, 2, 2, 2]))\n \n"}, {"name": "draw.polygon()", "path": "api/skimage.draw#skimage.draw.polygon", "type": "draw", "text": " \nskimage.draw.polygon(r, c, shape=None) [source]\n \nGenerate coordinates of pixels within polygon.  Parameters \n \nr(N,) ndarray \n\nRow coordinates of vertices of polygon.  \nc(N,) ndarray \n\nColumn coordinates of vertices of polygon.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for polygons that exceed the image size. If None, the full extent of the polygon is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.    Returns \n \nrr, ccndarray of int \n\nPixel coordinates of polygon. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Examples >>> from skimage.draw import polygon\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> r = np.array([1, 2, 8])\n>>> c = np.array([1, 7, 4])\n>>> rr, cc = polygon(r, c)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "draw.polygon2mask()", "path": "api/skimage.draw#skimage.draw.polygon2mask", "type": "draw", "text": " \nskimage.draw.polygon2mask(image_shape, polygon) [source]\n \nCompute a mask from polygon.  Parameters \n \nimage_shapetuple of size 2. \n\nThe shape of the mask.  \npolygonarray_like. \n\nThe polygon coordinates of shape (N, 2) where N is the number of points.    Returns \n \nmask2-D ndarray of type \u2018bool\u2019. \n\nThe mask that corresponds to the input polygon.     Notes This function does not do any border checking, so that all the vertices need to be within the given shape. Examples >>> image_shape = (128, 128)\n>>> polygon = np.array([[60, 100], [100, 40], [40, 40]])\n>>> mask = polygon2mask(image_shape, polygon)\n>>> mask.shape\n(128, 128)\n \n"}, {"name": "draw.polygon_perimeter()", "path": "api/skimage.draw#skimage.draw.polygon_perimeter", "type": "draw", "text": " \nskimage.draw.polygon_perimeter(r, c, shape=None, clip=False) [source]\n \nGenerate polygon perimeter coordinates.  Parameters \n \nr(N,) ndarray \n\nRow coordinates of vertices of polygon.  \nc(N,) ndarray \n\nColumn coordinates of vertices of polygon.  \nshapetuple, optional \n\nImage shape which is used to determine maximum extents of output pixel coordinates. This is useful for polygons that exceed the image size. If None, the full extents of the polygon is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.  \nclipbool, optional \n\nWhether to clip the polygon to the provided shape. If this is set to True, the drawn figure will always be a closed polygon with all edges visible.    Returns \n \nrr, ccndarray of int \n\nPixel coordinates of polygon. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Examples >>> from skimage.draw import polygon_perimeter\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = polygon_perimeter([5, -1, 5, 10],\n...                            [-1, 5, 11, 5],\n...                            shape=img.shape, clip=True)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 1, 1, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 1, 0, 0, 0, 1, 1, 0],\n       [0, 0, 0, 0, 1, 1, 1, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "draw.random_shapes()", "path": "api/skimage.draw#skimage.draw.random_shapes", "type": "draw", "text": " \nskimage.draw.random_shapes(image_shape, max_shapes, min_shapes=1, min_size=2, max_size=None, multichannel=True, num_channels=3, shape=None, intensity_range=None, allow_overlap=False, num_trials=100, random_seed=None) [source]\n \nGenerate an image with random shapes, labeled with bounding boxes. The image is populated with random shapes with random sizes, random locations, and random colors, with or without overlap. Shapes have random (row, col) starting coordinates and random sizes bounded by min_size and max_size. It can occur that a randomly generated shape will not fit the image at all. In that case, the algorithm will try again with new starting coordinates a certain number of times. However, it also means that some shapes may be skipped altogether. In that case, this function will generate fewer shapes than requested.  Parameters \n \nimage_shapetuple \n\nThe number of rows and columns of the image to generate.  \nmax_shapesint \n\nThe maximum number of shapes to (attempt to) fit into the shape.  \nmin_shapesint, optional \n\nThe minimum number of shapes to (attempt to) fit into the shape.  \nmin_sizeint, optional \n\nThe minimum dimension of each shape to fit into the image.  \nmax_sizeint, optional \n\nThe maximum dimension of each shape to fit into the image.  \nmultichannelbool, optional \n\nIf True, the generated image has num_channels color channels, otherwise generates grayscale image.  \nnum_channelsint, optional \n\nNumber of channels in the generated image. If 1, generate monochrome images, else color images with multiple channels. Ignored if multichannel is set to False.  \nshape{rectangle, circle, triangle, ellipse, None} str, optional \n\nThe name of the shape to generate or None to pick random ones.  \nintensity_range{tuple of tuples of uint8, tuple of uint8}, optional \n\nThe range of values to sample pixel values from. For grayscale images the format is (min, max). For multichannel - ((min, max),) if the ranges are equal across the channels, and ((min_0, max_0), \u2026 (min_N, max_N)) if they differ. As the function supports generation of uint8 arrays only, the maximum range is (0, 255). If None, set to (0, 254) for each channel reserving color of intensity = 255 for background.  \nallow_overlapbool, optional \n\nIf True, allow shapes to overlap.  \nnum_trialsint, optional \n\nHow often to attempt to fit a shape into the image before skipping it.  \nrandom_seedint, optional \n\nSeed to initialize the random number generator. If None, a random seed from the operating system is used.    Returns \n \nimageuint8 array \n\nAn image with the fitted shapes.  \nlabelslist \n\nA list of labels, one per shape in the image. Each label is a (category, ((r0, r1), (c0, c1))) tuple specifying the category and bounding box coordinates of the shape.     Examples >>> import skimage.draw\n>>> image, labels = skimage.draw.random_shapes((32, 32), max_shapes=3)\n>>> image \narray([\n   [[255, 255, 255],\n    [255, 255, 255],\n    [255, 255, 255],\n    ...,\n    [255, 255, 255],\n    [255, 255, 255],\n    [255, 255, 255]]], dtype=uint8)\n>>> labels \n[('circle', ((22, 18), (25, 21))),\n ('triangle', ((5, 6), (13, 13)))]\n \n"}, {"name": "draw.rectangle()", "path": "api/skimage.draw#skimage.draw.rectangle", "type": "draw", "text": " \nskimage.draw.rectangle(start, end=None, extent=None, shape=None) [source]\n \nGenerate coordinates of pixels within a rectangle.  Parameters \n \nstarttuple \n\nOrigin point of the rectangle, e.g., ([plane,] row, column).  \nendtuple \n\nEnd point of the rectangle ([plane,] row, column). For a 2D matrix, the slice defined by the rectangle is [start:(end+1)]. Either end or extent must be specified.  \nextenttuple \n\nThe extent (size) of the drawn rectangle. E.g., ([num_planes,] num_rows, num_cols). Either end or extent must be specified. A negative extent is valid, and will result in a rectangle going along the opposite direction. If extent is negative, the start point is not included.  \nshapetuple, optional \n\nImage shape used to determine the maximum bounds of the output coordinates. This is useful for clipping rectangles that exceed the image size. By default, no clipping is done.    Returns \n \ncoordsarray of int, shape (Ndim, Npoints) \n\nThe coordinates of all pixels in the rectangle.     Notes This function can be applied to N-dimensional images, by passing start and end or extent as tuples of length N. Examples >>> import numpy as np\n>>> from skimage.draw import rectangle\n>>> img = np.zeros((5, 5), dtype=np.uint8)\n>>> start = (1, 1)\n>>> extent = (3, 3)\n>>> rr, cc = rectangle(start, extent=extent, shape=img.shape)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n >>> img = np.zeros((5, 5), dtype=np.uint8)\n>>> start = (0, 1)\n>>> end = (3, 3)\n>>> rr, cc = rectangle(start, end=end, shape=img.shape)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n >>> import numpy as np\n>>> from skimage.draw import rectangle\n>>> img = np.zeros((6, 6), dtype=np.uint8)\n>>> start = (3, 3)\n>>>\n>>> rr, cc = rectangle(start, extent=(2, 2))\n>>> img[rr, cc] = 1\n>>> rr, cc = rectangle(start, extent=(-2, 2))\n>>> img[rr, cc] = 2\n>>> rr, cc = rectangle(start, extent=(-2, -2))\n>>> img[rr, cc] = 3\n>>> rr, cc = rectangle(start, extent=(2, -2))\n>>> img[rr, cc] = 4\n>>> print(img)\n[[0 0 0 0 0 0]\n [0 3 3 2 2 0]\n [0 3 3 2 2 0]\n [0 4 4 1 1 0]\n [0 4 4 1 1 0]\n [0 0 0 0 0 0]]\n \n"}, {"name": "draw.rectangle_perimeter()", "path": "api/skimage.draw#skimage.draw.rectangle_perimeter", "type": "draw", "text": " \nskimage.draw.rectangle_perimeter(start, end=None, extent=None, shape=None, clip=False) [source]\n \nGenerate coordinates of pixels that are exactly around a rectangle.  Parameters \n \nstarttuple \n\nOrigin point of the inner rectangle, e.g., (row, column).  \nendtuple \n\nEnd point of the inner rectangle (row, column). For a 2D matrix, the slice defined by inner the rectangle is [start:(end+1)]. Either end or extent must be specified.  \nextenttuple \n\nThe extent (size) of the inner rectangle. E.g., (num_rows, num_cols). Either end or extent must be specified. Negative extents are permitted. See rectangle to better understand how they behave.  \nshapetuple, optional \n\nImage shape used to determine the maximum bounds of the output coordinates. This is useful for clipping perimeters that exceed the image size. By default, no clipping is done. Must be at least length 2. Only the first two values are used to determine the extent of the input image.  \nclipbool, optional \n\nWhether to clip the perimeter to the provided shape. If this is set to True, the drawn figure will always be a closed polygon with all edges visible.    Returns \n \ncoordsarray of int, shape (2, Npoints) \n\nThe coordinates of all pixels in the rectangle.     Examples >>> import numpy as np\n>>> from skimage.draw import rectangle_perimeter\n>>> img = np.zeros((5, 6), dtype=np.uint8)\n>>> start = (2, 3)\n>>> end = (3, 4)\n>>> rr, cc = rectangle_perimeter(start, end=end, shape=img.shape)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1],\n       [0, 0, 1, 0, 0, 1],\n       [0, 0, 1, 0, 0, 1],\n       [0, 0, 1, 1, 1, 1]], dtype=uint8)\n >>> img = np.zeros((5, 5), dtype=np.uint8)\n>>> r, c = rectangle_perimeter(start, (10, 10), shape=img.shape, clip=True)\n>>> img[r, c] = 1\n>>> img\narray([[0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1],\n       [0, 0, 1, 0, 1],\n       [0, 0, 1, 0, 1],\n       [0, 0, 1, 1, 1]], dtype=uint8)\n \n"}, {"name": "draw.set_color()", "path": "api/skimage.draw#skimage.draw.set_color", "type": "draw", "text": " \nskimage.draw.set_color(image, coords, color, alpha=1) [source]\n \nSet pixel color in the image at the given coordinates. Note that this function modifies the color of the image in-place. Coordinates that exceed the shape of the image will be ignored.  Parameters \n \nimage(M, N, D) ndarray \n\nImage  \ncoordstuple of ((P,) ndarray, (P,) ndarray) \n\nRow and column coordinates of pixels to be colored.  \ncolor(D,) ndarray \n\nColor to be assigned to coordinates in the image.  \nalphascalar or (N,) ndarray \n\nAlpha values used to blend color with image. 0 is transparent, 1 is opaque.     Examples >>> from skimage.draw import line, set_color\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = line(1, 1, 20, 20)\n>>> set_color(img, (rr, cc), 1)\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], dtype=uint8)\n \n"}, {"name": "dtype_limits()", "path": "api/skimage#skimage.dtype_limits", "type": "skimage", "text": " \nskimage.dtype_limits(image, clip_negative=False) [source]\n \nReturn intensity limits, i.e. (min, max) tuple, of the image\u2019s dtype.  Parameters \n \nimagendarray \n\nInput image.  \nclip_negativebool, optional \n\nIf True, clip the negative range (i.e. return 0 for min intensity) even if the image dtype allows negative values.    Returns \n \nimin, imaxtuple \n\nLower and upper intensity limits.     \n"}, {"name": "ensure_python_version()", "path": "api/skimage#skimage.ensure_python_version", "type": "skimage", "text": " \nskimage.ensure_python_version(min_version) [source]\n\n"}, {"name": "exposure", "path": "api/skimage.exposure", "type": "exposure", "text": "Module: exposure  \nskimage.exposure.adjust_gamma(image[, \u2026]) Performs Gamma Correction on the input image.  \nskimage.exposure.adjust_log(image[, gain, inv]) Performs Logarithmic correction on the input image.  \nskimage.exposure.adjust_sigmoid(image[, \u2026]) Performs Sigmoid Correction on the input image.  \nskimage.exposure.cumulative_distribution(image) Return cumulative distribution function (cdf) for the given image.  \nskimage.exposure.equalize_adapthist(image[, \u2026]) Contrast Limited Adaptive Histogram Equalization (CLAHE).  \nskimage.exposure.equalize_hist(image[, \u2026]) Return image after histogram equalization.  \nskimage.exposure.histogram(image[, nbins, \u2026]) Return histogram of image.  \nskimage.exposure.is_low_contrast(image[, \u2026]) Determine if an image is low contrast.  \nskimage.exposure.match_histograms(image, \u2026) Adjust an image so that its cumulative histogram matches that of another.  \nskimage.exposure.rescale_intensity(image[, \u2026]) Return image after stretching or shrinking its intensity levels.   adjust_gamma  \nskimage.exposure.adjust_gamma(image, gamma=1, gain=1) [source]\n \nPerforms Gamma Correction on the input image. Also known as Power Law Transform. This function transforms the input image pixelwise according to the equation O = I**gamma after scaling each pixel to the range 0 to 1.  Parameters \n \nimagendarray \n\nInput image.  \ngammafloat, optional \n\nNon negative real number. Default value is 1.  \ngainfloat, optional \n\nThe constant multiplier. Default value is 1.    Returns \n \noutndarray \n\nGamma corrected output image.      See also  \nadjust_log\n\n  Notes For gamma greater than 1, the histogram will shift towards left and the output image will be darker than the input image. For gamma less than 1, the histogram will shift towards right and the output image will be brighter than the input image. References  \n1  \nhttps://en.wikipedia.org/wiki/Gamma_correction   Examples >>> from skimage import data, exposure, img_as_float\n>>> image = img_as_float(data.moon())\n>>> gamma_corrected = exposure.adjust_gamma(image, 2)\n>>> # Output is darker for gamma > 1\n>>> image.mean() > gamma_corrected.mean()\nTrue\n \n Examples using skimage.exposure.adjust_gamma\n \n  Explore 3D images (of cells)   adjust_log  \nskimage.exposure.adjust_log(image, gain=1, inv=False) [source]\n \nPerforms Logarithmic correction on the input image. This function transforms the input image pixelwise according to the equation O = gain*log(1 + I) after scaling each pixel to the range 0 to 1. For inverse logarithmic correction, the equation is O = gain*(2**I - 1).  Parameters \n \nimagendarray \n\nInput image.  \ngainfloat, optional \n\nThe constant multiplier. Default value is 1.  \ninvfloat, optional \n\nIf True, it performs inverse logarithmic correction, else correction will be logarithmic. Defaults to False.    Returns \n \noutndarray \n\nLogarithm corrected output image.      See also  \nadjust_gamma\n\n  References  \n1  \nhttp://www.ece.ucsb.edu/Faculty/Manjunath/courses/ece178W03/EnhancePart1.pdf   \n adjust_sigmoid  \nskimage.exposure.adjust_sigmoid(image, cutoff=0.5, gain=10, inv=False) [source]\n \nPerforms Sigmoid Correction on the input image. Also known as Contrast Adjustment. This function transforms the input image pixelwise according to the equation O = 1/(1 + exp*(gain*(cutoff - I))) after scaling each pixel to the range 0 to 1.  Parameters \n \nimagendarray \n\nInput image.  \ncutofffloat, optional \n\nCutoff of the sigmoid function that shifts the characteristic curve in horizontal direction. Default value is 0.5.  \ngainfloat, optional \n\nThe constant multiplier in exponential\u2019s power of sigmoid function. Default value is 10.  \ninvbool, optional \n\nIf True, returns the negative sigmoid correction. Defaults to False.    Returns \n \noutndarray \n\nSigmoid corrected output image.      See also  \nadjust_gamma\n\n  References  \n1  \nGustav J. Braun, \u201cImage Lightness Rescaling Using Sigmoidal Contrast Enhancement Functions\u201d, http://www.cis.rit.edu/fairchild/PDFs/PAP07.pdf   \n cumulative_distribution  \nskimage.exposure.cumulative_distribution(image, nbins=256) [source]\n \nReturn cumulative distribution function (cdf) for the given image.  Parameters \n \nimagearray \n\nImage array.  \nnbinsint, optional \n\nNumber of bins for image histogram.    Returns \n \nimg_cdfarray \n\nValues of cumulative distribution function.  \nbin_centersarray \n\nCenters of bins.      See also  \nhistogram\n\n  References  \n1  \nhttps://en.wikipedia.org/wiki/Cumulative_distribution_function   Examples >>> from skimage import data, exposure, img_as_float\n>>> image = img_as_float(data.camera())\n>>> hi = exposure.histogram(image)\n>>> cdf = exposure.cumulative_distribution(image)\n>>> np.alltrue(cdf[0] == np.cumsum(hi[0])/float(image.size))\nTrue\n \n Examples using skimage.exposure.cumulative_distribution\n \n  Local Histogram Equalization  \n\n  Explore 3D images (of cells)   equalize_adapthist  \nskimage.exposure.equalize_adapthist(image, kernel_size=None, clip_limit=0.01, nbins=256) [source]\n \nContrast Limited Adaptive Histogram Equalization (CLAHE). An algorithm for local contrast enhancement, that uses histograms computed over different tile regions of the image. Local details can therefore be enhanced even in regions that are darker or lighter than most of the image.  Parameters \n \nimage(N1, \u2026,NN[, C]) ndarray \n\nInput image.  kernel_size: int or array_like, optional\n\nDefines the shape of contextual regions used in the algorithm. If iterable is passed, it must have the same number of elements as image.ndim (without color channel). If integer, it is broadcasted to each image dimension. By default, kernel_size is 1/8 of image height by 1/8 of its width.  \nclip_limitfloat, optional \n\nClipping limit, normalized between 0 and 1 (higher values give more contrast).  \nnbinsint, optional \n\nNumber of gray bins for histogram (\u201cdata range\u201d).    Returns \n \nout(N1, \u2026,NN[, C]) ndarray \n\nEqualized image with float64 dtype.      See also  \nequalize_hist, rescale_intensity\n\n  Notes  \n For color images, the following steps are performed:\n\n The image is converted to HSV color space The CLAHE algorithm is run on the V (Value) channel The image is converted back to RGB space and returned     For RGBA images, the original alpha channel is removed.   Changed in version 0.17: The values returned by this function are slightly shifted upwards because of an internal change in rounding behavior.  References  \n1  \nhttp://tog.acm.org/resources/GraphicsGems/  \n2  \nhttps://en.wikipedia.org/wiki/CLAHE#CLAHE   \n Examples using skimage.exposure.equalize_adapthist\n \n  3D adaptive histogram equalization   equalize_hist  \nskimage.exposure.equalize_hist(image, nbins=256, mask=None) [source]\n \nReturn image after histogram equalization.  Parameters \n \nimagearray \n\nImage array.  \nnbinsint, optional \n\nNumber of bins for image histogram. Note: this argument is ignored for integer images, for which each integer is its own bin.  mask: ndarray of bools or 0s and 1s, optional\n\nArray of same shape as image. Only points at which mask == True are used for the equalization, which is applied to the whole image.    Returns \n \noutfloat array \n\nImage array after histogram equalization.     Notes This function is adapted from [1] with the author\u2019s permission. References  \n1  \nhttp://www.janeriksolem.net/histogram-equalization-with-python-and.html  \n2  \nhttps://en.wikipedia.org/wiki/Histogram_equalization   \n Examples using skimage.exposure.equalize_hist\n \n  Local Histogram Equalization  \n\n  3D adaptive histogram equalization  \n\n  Explore 3D images (of cells)  \n\n  Rank filters   histogram  \nskimage.exposure.histogram(image, nbins=256, source_range='image', normalize=False) [source]\n \nReturn histogram of image. Unlike numpy.histogram, this function returns the centers of bins and does not rebin integer arrays. For integer arrays, each integer value has its own bin, which improves speed and intensity-resolution. The histogram is computed on the flattened image: for color images, the function should be used separately on each channel to obtain a histogram for each color channel.  Parameters \n \nimagearray \n\nInput image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.  \nsource_rangestring, optional \n\n\u2018image\u2019 (default) determines the range from the input image. \u2018dtype\u2019 determines the range from the expected range of the images of that data type.  \nnormalizebool, optional \n\nIf True, normalize the histogram by the sum of its values.    Returns \n \nhistarray \n\nThe values of the histogram.  \nbin_centersarray \n\nThe values at the center of the bins.      See also  \ncumulative_distribution\n\n  Examples >>> from skimage import data, exposure, img_as_float\n>>> image = img_as_float(data.camera())\n>>> np.histogram(image, bins=2)\n(array([ 93585, 168559]), array([0. , 0.5, 1. ]))\n>>> exposure.histogram(image, nbins=2)\n(array([ 93585, 168559]), array([0.25, 0.75]))\n \n Examples using skimage.exposure.histogram\n \n  Rank filters   is_low_contrast  \nskimage.exposure.is_low_contrast(image, fraction_threshold=0.05, lower_percentile=1, upper_percentile=99, method='linear') [source]\n \nDetermine if an image is low contrast.  Parameters \n \nimagearray-like \n\nThe image under test.  \nfraction_thresholdfloat, optional \n\nThe low contrast fraction threshold. An image is considered low- contrast when its range of brightness spans less than this fraction of its data type\u2019s full range. [1]  \nlower_percentilefloat, optional \n\nDisregard values below this percentile when computing image contrast.  \nupper_percentilefloat, optional \n\nDisregard values above this percentile when computing image contrast.  \nmethodstr, optional \n\nThe contrast determination method. Right now the only available option is \u201clinear\u201d.    Returns \n \noutbool \n\nTrue when the image is determined to be low contrast.     References  \n1  \nhttps://scikit-image.org/docs/dev/user_guide/data_types.html   Examples >>> image = np.linspace(0, 0.04, 100)\n>>> is_low_contrast(image)\nTrue\n>>> image[-1] = 1\n>>> is_low_contrast(image)\nTrue\n>>> is_low_contrast(image, upper_percentile=100)\nFalse\n \n match_histograms  \nskimage.exposure.match_histograms(image, reference, *, multichannel=False) [source]\n \nAdjust an image so that its cumulative histogram matches that of another. The adjustment is applied separately for each channel.  Parameters \n \nimagendarray \n\nInput image. Can be gray-scale or in color.  \nreferencendarray \n\nImage to match histogram of. Must have the same number of channels as image.  \nmultichannelbool, optional \n\nApply the matching separately for each channel.    Returns \n \nmatchedndarray \n\nTransformed input image.    Raises \n ValueError\n\nThrown when the number of channels in the input image and the reference differ.     References  \n1  \nhttp://paulbourke.net/miscellaneous/equalisation/   \n rescale_intensity  \nskimage.exposure.rescale_intensity(image, in_range='image', out_range='dtype') [source]\n \nReturn image after stretching or shrinking its intensity levels. The desired intensity range of the input and output, in_range and out_range respectively, are used to stretch or shrink the intensity range of the input image. See examples below.  Parameters \n \nimagearray \n\nImage array.  \nin_range, out_rangestr or 2-tuple, optional \n\nMin and max intensity values of input and output image. The possible values for this parameter are enumerated below.  \u2018image\u2019\n\nUse image min/max as the intensity range.  \u2018dtype\u2019\n\nUse min/max of the image\u2019s dtype as the intensity range.  dtype-name\n\nUse intensity range based on desired dtype. Must be valid key in DTYPE_RANGE.  2-tuple\n\nUse range_values as explicit min/max intensities.      Returns \n \noutarray \n\nImage array after rescaling its intensity. This image is the same dtype as the input image.      See also  \nequalize_hist\n\n  Notes  Changed in version 0.17: The dtype of the output array has changed to match the output dtype, or float if the output range is specified by a pair of floats.  Examples By default, the min/max intensities of the input image are stretched to the limits allowed by the image\u2019s dtype, since in_range defaults to \u2018image\u2019 and out_range defaults to \u2018dtype\u2019: >>> image = np.array([51, 102, 153], dtype=np.uint8)\n>>> rescale_intensity(image)\narray([  0, 127, 255], dtype=uint8)\n It\u2019s easy to accidentally convert an image dtype from uint8 to float: >>> 1.0 * image\narray([ 51., 102., 153.])\n Use rescale_intensity to rescale to the proper range for float dtypes: >>> image_float = 1.0 * image\n>>> rescale_intensity(image_float)\narray([0. , 0.5, 1. ])\n To maintain the low contrast of the original, use the in_range parameter: >>> rescale_intensity(image_float, in_range=(0, 255))\narray([0.2, 0.4, 0.6])\n If the min/max value of in_range is more/less than the min/max image intensity, then the intensity levels are clipped: >>> rescale_intensity(image_float, in_range=(0, 102))\narray([0.5, 1. , 1. ])\n If you have an image with signed integers but want to rescale the image to just the positive range, use the out_range parameter. In that case, the output dtype will be float: >>> image = np.array([-10, 0, 10], dtype=np.int8)\n>>> rescale_intensity(image, out_range=(0, 127))\narray([  0. ,  63.5, 127. ])\n To get the desired range with a specific dtype, use .astype(): >>> rescale_intensity(image, out_range=(0, 127)).astype(np.int8)\narray([  0,  63, 127], dtype=int8)\n If the input image is constant, the output will be clipped directly to the output range: >>> image = np.array([130, 130, 130], dtype=np.int32) >>> rescale_intensity(image, out_range=(0, 127)).astype(np.int32) array([127, 127, 127], dtype=int32) \n Examples using skimage.exposure.rescale_intensity\n \n  Phase Unwrapping  \n\n  Explore 3D images (of cells)  \n\n  Rank filters  \n"}, {"name": "exposure.adjust_gamma()", "path": "api/skimage.exposure#skimage.exposure.adjust_gamma", "type": "exposure", "text": " \nskimage.exposure.adjust_gamma(image, gamma=1, gain=1) [source]\n \nPerforms Gamma Correction on the input image. Also known as Power Law Transform. This function transforms the input image pixelwise according to the equation O = I**gamma after scaling each pixel to the range 0 to 1.  Parameters \n \nimagendarray \n\nInput image.  \ngammafloat, optional \n\nNon negative real number. Default value is 1.  \ngainfloat, optional \n\nThe constant multiplier. Default value is 1.    Returns \n \noutndarray \n\nGamma corrected output image.      See also  \nadjust_log\n\n  Notes For gamma greater than 1, the histogram will shift towards left and the output image will be darker than the input image. For gamma less than 1, the histogram will shift towards right and the output image will be brighter than the input image. References  \n1  \nhttps://en.wikipedia.org/wiki/Gamma_correction   Examples >>> from skimage import data, exposure, img_as_float\n>>> image = img_as_float(data.moon())\n>>> gamma_corrected = exposure.adjust_gamma(image, 2)\n>>> # Output is darker for gamma > 1\n>>> image.mean() > gamma_corrected.mean()\nTrue\n \n"}, {"name": "exposure.adjust_log()", "path": "api/skimage.exposure#skimage.exposure.adjust_log", "type": "exposure", "text": " \nskimage.exposure.adjust_log(image, gain=1, inv=False) [source]\n \nPerforms Logarithmic correction on the input image. This function transforms the input image pixelwise according to the equation O = gain*log(1 + I) after scaling each pixel to the range 0 to 1. For inverse logarithmic correction, the equation is O = gain*(2**I - 1).  Parameters \n \nimagendarray \n\nInput image.  \ngainfloat, optional \n\nThe constant multiplier. Default value is 1.  \ninvfloat, optional \n\nIf True, it performs inverse logarithmic correction, else correction will be logarithmic. Defaults to False.    Returns \n \noutndarray \n\nLogarithm corrected output image.      See also  \nadjust_gamma\n\n  References  \n1  \nhttp://www.ece.ucsb.edu/Faculty/Manjunath/courses/ece178W03/EnhancePart1.pdf   \n"}, {"name": "exposure.adjust_sigmoid()", "path": "api/skimage.exposure#skimage.exposure.adjust_sigmoid", "type": "exposure", "text": " \nskimage.exposure.adjust_sigmoid(image, cutoff=0.5, gain=10, inv=False) [source]\n \nPerforms Sigmoid Correction on the input image. Also known as Contrast Adjustment. This function transforms the input image pixelwise according to the equation O = 1/(1 + exp*(gain*(cutoff - I))) after scaling each pixel to the range 0 to 1.  Parameters \n \nimagendarray \n\nInput image.  \ncutofffloat, optional \n\nCutoff of the sigmoid function that shifts the characteristic curve in horizontal direction. Default value is 0.5.  \ngainfloat, optional \n\nThe constant multiplier in exponential\u2019s power of sigmoid function. Default value is 10.  \ninvbool, optional \n\nIf True, returns the negative sigmoid correction. Defaults to False.    Returns \n \noutndarray \n\nSigmoid corrected output image.      See also  \nadjust_gamma\n\n  References  \n1  \nGustav J. Braun, \u201cImage Lightness Rescaling Using Sigmoidal Contrast Enhancement Functions\u201d, http://www.cis.rit.edu/fairchild/PDFs/PAP07.pdf   \n"}, {"name": "exposure.cumulative_distribution()", "path": "api/skimage.exposure#skimage.exposure.cumulative_distribution", "type": "exposure", "text": " \nskimage.exposure.cumulative_distribution(image, nbins=256) [source]\n \nReturn cumulative distribution function (cdf) for the given image.  Parameters \n \nimagearray \n\nImage array.  \nnbinsint, optional \n\nNumber of bins for image histogram.    Returns \n \nimg_cdfarray \n\nValues of cumulative distribution function.  \nbin_centersarray \n\nCenters of bins.      See also  \nhistogram\n\n  References  \n1  \nhttps://en.wikipedia.org/wiki/Cumulative_distribution_function   Examples >>> from skimage import data, exposure, img_as_float\n>>> image = img_as_float(data.camera())\n>>> hi = exposure.histogram(image)\n>>> cdf = exposure.cumulative_distribution(image)\n>>> np.alltrue(cdf[0] == np.cumsum(hi[0])/float(image.size))\nTrue\n \n"}, {"name": "exposure.equalize_adapthist()", "path": "api/skimage.exposure#skimage.exposure.equalize_adapthist", "type": "exposure", "text": " \nskimage.exposure.equalize_adapthist(image, kernel_size=None, clip_limit=0.01, nbins=256) [source]\n \nContrast Limited Adaptive Histogram Equalization (CLAHE). An algorithm for local contrast enhancement, that uses histograms computed over different tile regions of the image. Local details can therefore be enhanced even in regions that are darker or lighter than most of the image.  Parameters \n \nimage(N1, \u2026,NN[, C]) ndarray \n\nInput image.  kernel_size: int or array_like, optional\n\nDefines the shape of contextual regions used in the algorithm. If iterable is passed, it must have the same number of elements as image.ndim (without color channel). If integer, it is broadcasted to each image dimension. By default, kernel_size is 1/8 of image height by 1/8 of its width.  \nclip_limitfloat, optional \n\nClipping limit, normalized between 0 and 1 (higher values give more contrast).  \nnbinsint, optional \n\nNumber of gray bins for histogram (\u201cdata range\u201d).    Returns \n \nout(N1, \u2026,NN[, C]) ndarray \n\nEqualized image with float64 dtype.      See also  \nequalize_hist, rescale_intensity\n\n  Notes  \n For color images, the following steps are performed:\n\n The image is converted to HSV color space The CLAHE algorithm is run on the V (Value) channel The image is converted back to RGB space and returned     For RGBA images, the original alpha channel is removed.   Changed in version 0.17: The values returned by this function are slightly shifted upwards because of an internal change in rounding behavior.  References  \n1  \nhttp://tog.acm.org/resources/GraphicsGems/  \n2  \nhttps://en.wikipedia.org/wiki/CLAHE#CLAHE   \n"}, {"name": "exposure.equalize_hist()", "path": "api/skimage.exposure#skimage.exposure.equalize_hist", "type": "exposure", "text": " \nskimage.exposure.equalize_hist(image, nbins=256, mask=None) [source]\n \nReturn image after histogram equalization.  Parameters \n \nimagearray \n\nImage array.  \nnbinsint, optional \n\nNumber of bins for image histogram. Note: this argument is ignored for integer images, for which each integer is its own bin.  mask: ndarray of bools or 0s and 1s, optional\n\nArray of same shape as image. Only points at which mask == True are used for the equalization, which is applied to the whole image.    Returns \n \noutfloat array \n\nImage array after histogram equalization.     Notes This function is adapted from [1] with the author\u2019s permission. References  \n1  \nhttp://www.janeriksolem.net/histogram-equalization-with-python-and.html  \n2  \nhttps://en.wikipedia.org/wiki/Histogram_equalization   \n"}, {"name": "exposure.histogram()", "path": "api/skimage.exposure#skimage.exposure.histogram", "type": "exposure", "text": " \nskimage.exposure.histogram(image, nbins=256, source_range='image', normalize=False) [source]\n \nReturn histogram of image. Unlike numpy.histogram, this function returns the centers of bins and does not rebin integer arrays. For integer arrays, each integer value has its own bin, which improves speed and intensity-resolution. The histogram is computed on the flattened image: for color images, the function should be used separately on each channel to obtain a histogram for each color channel.  Parameters \n \nimagearray \n\nInput image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.  \nsource_rangestring, optional \n\n\u2018image\u2019 (default) determines the range from the input image. \u2018dtype\u2019 determines the range from the expected range of the images of that data type.  \nnormalizebool, optional \n\nIf True, normalize the histogram by the sum of its values.    Returns \n \nhistarray \n\nThe values of the histogram.  \nbin_centersarray \n\nThe values at the center of the bins.      See also  \ncumulative_distribution\n\n  Examples >>> from skimage import data, exposure, img_as_float\n>>> image = img_as_float(data.camera())\n>>> np.histogram(image, bins=2)\n(array([ 93585, 168559]), array([0. , 0.5, 1. ]))\n>>> exposure.histogram(image, nbins=2)\n(array([ 93585, 168559]), array([0.25, 0.75]))\n \n"}, {"name": "exposure.is_low_contrast()", "path": "api/skimage.exposure#skimage.exposure.is_low_contrast", "type": "exposure", "text": " \nskimage.exposure.is_low_contrast(image, fraction_threshold=0.05, lower_percentile=1, upper_percentile=99, method='linear') [source]\n \nDetermine if an image is low contrast.  Parameters \n \nimagearray-like \n\nThe image under test.  \nfraction_thresholdfloat, optional \n\nThe low contrast fraction threshold. An image is considered low- contrast when its range of brightness spans less than this fraction of its data type\u2019s full range. [1]  \nlower_percentilefloat, optional \n\nDisregard values below this percentile when computing image contrast.  \nupper_percentilefloat, optional \n\nDisregard values above this percentile when computing image contrast.  \nmethodstr, optional \n\nThe contrast determination method. Right now the only available option is \u201clinear\u201d.    Returns \n \noutbool \n\nTrue when the image is determined to be low contrast.     References  \n1  \nhttps://scikit-image.org/docs/dev/user_guide/data_types.html   Examples >>> image = np.linspace(0, 0.04, 100)\n>>> is_low_contrast(image)\nTrue\n>>> image[-1] = 1\n>>> is_low_contrast(image)\nTrue\n>>> is_low_contrast(image, upper_percentile=100)\nFalse\n \n"}, {"name": "exposure.match_histograms()", "path": "api/skimage.exposure#skimage.exposure.match_histograms", "type": "exposure", "text": " \nskimage.exposure.match_histograms(image, reference, *, multichannel=False) [source]\n \nAdjust an image so that its cumulative histogram matches that of another. The adjustment is applied separately for each channel.  Parameters \n \nimagendarray \n\nInput image. Can be gray-scale or in color.  \nreferencendarray \n\nImage to match histogram of. Must have the same number of channels as image.  \nmultichannelbool, optional \n\nApply the matching separately for each channel.    Returns \n \nmatchedndarray \n\nTransformed input image.    Raises \n ValueError\n\nThrown when the number of channels in the input image and the reference differ.     References  \n1  \nhttp://paulbourke.net/miscellaneous/equalisation/   \n"}, {"name": "exposure.rescale_intensity()", "path": "api/skimage.exposure#skimage.exposure.rescale_intensity", "type": "exposure", "text": " \nskimage.exposure.rescale_intensity(image, in_range='image', out_range='dtype') [source]\n \nReturn image after stretching or shrinking its intensity levels. The desired intensity range of the input and output, in_range and out_range respectively, are used to stretch or shrink the intensity range of the input image. See examples below.  Parameters \n \nimagearray \n\nImage array.  \nin_range, out_rangestr or 2-tuple, optional \n\nMin and max intensity values of input and output image. The possible values for this parameter are enumerated below.  \u2018image\u2019\n\nUse image min/max as the intensity range.  \u2018dtype\u2019\n\nUse min/max of the image\u2019s dtype as the intensity range.  dtype-name\n\nUse intensity range based on desired dtype. Must be valid key in DTYPE_RANGE.  2-tuple\n\nUse range_values as explicit min/max intensities.      Returns \n \noutarray \n\nImage array after rescaling its intensity. This image is the same dtype as the input image.      See also  \nequalize_hist\n\n  Notes  Changed in version 0.17: The dtype of the output array has changed to match the output dtype, or float if the output range is specified by a pair of floats.  Examples By default, the min/max intensities of the input image are stretched to the limits allowed by the image\u2019s dtype, since in_range defaults to \u2018image\u2019 and out_range defaults to \u2018dtype\u2019: >>> image = np.array([51, 102, 153], dtype=np.uint8)\n>>> rescale_intensity(image)\narray([  0, 127, 255], dtype=uint8)\n It\u2019s easy to accidentally convert an image dtype from uint8 to float: >>> 1.0 * image\narray([ 51., 102., 153.])\n Use rescale_intensity to rescale to the proper range for float dtypes: >>> image_float = 1.0 * image\n>>> rescale_intensity(image_float)\narray([0. , 0.5, 1. ])\n To maintain the low contrast of the original, use the in_range parameter: >>> rescale_intensity(image_float, in_range=(0, 255))\narray([0.2, 0.4, 0.6])\n If the min/max value of in_range is more/less than the min/max image intensity, then the intensity levels are clipped: >>> rescale_intensity(image_float, in_range=(0, 102))\narray([0.5, 1. , 1. ])\n If you have an image with signed integers but want to rescale the image to just the positive range, use the out_range parameter. In that case, the output dtype will be float: >>> image = np.array([-10, 0, 10], dtype=np.int8)\n>>> rescale_intensity(image, out_range=(0, 127))\narray([  0. ,  63.5, 127. ])\n To get the desired range with a specific dtype, use .astype(): >>> rescale_intensity(image, out_range=(0, 127)).astype(np.int8)\narray([  0,  63, 127], dtype=int8)\n If the input image is constant, the output will be clipped directly to the output range: >>> image = np.array([130, 130, 130], dtype=np.int32) >>> rescale_intensity(image, out_range=(0, 127)).astype(np.int32) array([127, 127, 127], dtype=int32) \n"}, {"name": "feature", "path": "api/skimage.feature", "type": "feature", "text": "Module: feature  \nskimage.feature.blob_dog(image[, min_sigma, \u2026]) Finds blobs in the given grayscale image.  \nskimage.feature.blob_doh(image[, min_sigma, \u2026]) Finds blobs in the given grayscale image.  \nskimage.feature.blob_log(image[, min_sigma, \u2026]) Finds blobs in the given grayscale image.  \nskimage.feature.canny(image[, sigma, \u2026]) Edge filter an image using the Canny algorithm.  \nskimage.feature.corner_fast(image[, n, \u2026]) Extract FAST corners for a given image.  \nskimage.feature.corner_foerstner(image[, sigma]) Compute Foerstner corner measure response image.  \nskimage.feature.corner_harris(image[, \u2026]) Compute Harris corner measure response image.  \nskimage.feature.corner_kitchen_rosenfeld(image) Compute Kitchen and Rosenfeld corner measure response image.  \nskimage.feature.corner_moravec(image[, \u2026]) Compute Moravec corner measure response image.  \nskimage.feature.corner_orientations(image, \u2026) Compute the orientation of corners.  \nskimage.feature.corner_peaks(image[, \u2026]) Find peaks in corner measure response image.  \nskimage.feature.corner_shi_tomasi(image[, sigma]) Compute Shi-Tomasi (Kanade-Tomasi) corner measure response image.  \nskimage.feature.corner_subpix(image, corners) Determine subpixel position of corners.  \nskimage.feature.daisy(image[, step, radius, \u2026]) Extract DAISY feature descriptors densely for the given image.  \nskimage.feature.draw_haar_like_feature(\u2026) Visualization of Haar-like features.  \nskimage.feature.draw_multiblock_lbp(image, \u2026) Multi-block local binary pattern visualization.  \nskimage.feature.greycomatrix(image, \u2026[, \u2026]) Calculate the grey-level co-occurrence matrix.  \nskimage.feature.greycoprops(P[, prop]) Calculate texture properties of a GLCM.  \nskimage.feature.haar_like_feature(int_image, \u2026) Compute the Haar-like features for a region of interest (ROI) of an integral image.  \nskimage.feature.haar_like_feature_coord(\u2026) Compute the coordinates of Haar-like features.  \nskimage.feature.hessian_matrix(image[, \u2026]) Compute Hessian matrix.  \nskimage.feature.hessian_matrix_det(image[, \u2026]) Compute the approximate Hessian Determinant over an image.  \nskimage.feature.hessian_matrix_eigvals(H_elems) Compute eigenvalues of Hessian matrix.  \nskimage.feature.hog(image[, orientations, \u2026]) Extract Histogram of Oriented Gradients (HOG) for a given image.  \nskimage.feature.local_binary_pattern(image, P, R) Gray scale and rotation invariant LBP (Local Binary Patterns).  \nskimage.feature.masked_register_translation(\u2026) Deprecated function.  \nskimage.feature.match_descriptors(\u2026[, \u2026]) Brute-force matching of descriptors.  \nskimage.feature.match_template(image, template) Match a template to a 2-D or 3-D image using normalized correlation.  \nskimage.feature.multiblock_lbp(int_image, r, \u2026) Multi-block local binary pattern (MB-LBP).  \nskimage.feature.multiscale_basic_features(image) Local features for a single- or multi-channel nd image.  \nskimage.feature.peak_local_max(image[, \u2026]) Find peaks in an image as coordinate list or boolean mask.  \nskimage.feature.plot_matches(ax, image1, \u2026) Plot matched features.  \nskimage.feature.register_translation(\u2026[, \u2026]) Deprecated function.  \nskimage.feature.shape_index(image[, sigma, \u2026]) Compute the shape index.  \nskimage.feature.structure_tensor(image[, \u2026]) Compute structure tensor using sum of squared differences.  \nskimage.feature.structure_tensor_eigenvalues(A_elems) Compute eigenvalues of structure tensor.  \nskimage.feature.structure_tensor_eigvals(\u2026) Compute eigenvalues of structure tensor.  \nskimage.feature.BRIEF([descriptor_size, \u2026]) BRIEF binary descriptor extractor.  \nskimage.feature.CENSURE([min_scale, \u2026]) CENSURE keypoint detector.  \nskimage.feature.Cascade Class for cascade of classifiers that is used for object detection.  \nskimage.feature.ORB([downscale, n_scales, \u2026]) Oriented FAST and rotated BRIEF feature detector and binary descriptor extractor.   blob_dog  \nskimage.feature.blob_dog(image, min_sigma=1, max_sigma=50, sigma_ratio=1.6, threshold=2.0, overlap=0.5, *, exclude_border=False) [source]\n \nFinds blobs in the given grayscale image. Blobs are found using the Difference of Gaussian (DoG) method [1]. For each blob found, the method returns its coordinates and the standard deviation of the Gaussian kernel that detected the blob.  Parameters \n \nimage2D or 3D ndarray \n\nInput grayscale image, blobs are assumed to be light on dark background (white on black).  \nmin_sigmascalar or sequence of scalars, optional \n\nThe minimum standard deviation for Gaussian kernel. Keep this low to detect smaller blobs. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.  \nmax_sigmascalar or sequence of scalars, optional \n\nThe maximum standard deviation for Gaussian kernel. Keep this high to detect larger blobs. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.  \nsigma_ratiofloat, optional \n\nThe ratio between the standard deviation of Gaussian Kernels used for computing the Difference of Gaussians  \nthresholdfloat, optional. \n\nThe absolute lower bound for scale space maxima. Local maxima smaller than thresh are ignored. Reduce this to detect blobs with less intensities.  \noverlapfloat, optional \n\nA value between 0 and 1. If the area of two blobs overlaps by a fraction greater than threshold, the smaller blob is eliminated.  \nexclude_bordertuple of ints, int, or False, optional \n\nIf tuple of ints, the length of the tuple must match the input array\u2019s dimensionality. Each element of the tuple will exclude peaks from within exclude_border-pixels of the border of the image along that dimension. If nonzero int, exclude_border excludes peaks from within exclude_border-pixels of the border of the image. If zero or False, peaks are identified regardless of their distance from the border.    Returns \n \nA(n, image.ndim + sigma) ndarray \n\nA 2d array with each row representing 2 coordinate values for a 2D image, and 3 coordinate values for a 3D image, plus the sigma(s) used. When a single sigma is passed, outputs are: (r, c, sigma) or (p, r, c, sigma) where (r, c) or (p, r, c) are coordinates of the blob and sigma is the standard deviation of the Gaussian kernel which detected the blob. When an anisotropic gaussian is used (sigmas per dimension), the detected sigma is returned for each dimension.      See also  \nskimage.filters.difference_of_gaussians\n\n  Notes The radius of each blob is approximately \\(\\sqrt{2}\\sigma\\) for a 2-D image and \\(\\sqrt{3}\\sigma\\) for a 3-D image. References  \n1  \nhttps://en.wikipedia.org/wiki/Blob_detection#The_difference_of_Gaussians_approach   Examples >>> from skimage import data, feature\n>>> feature.blob_dog(data.coins(), threshold=.5, max_sigma=40)\narray([[120.      , 272.      ,  16.777216],\n       [193.      , 213.      ,  16.777216],\n       [263.      , 245.      ,  16.777216],\n       [185.      , 347.      ,  16.777216],\n       [128.      , 154.      ,  10.48576 ],\n       [198.      , 155.      ,  10.48576 ],\n       [124.      , 337.      ,  10.48576 ],\n       [ 45.      , 336.      ,  16.777216],\n       [195.      , 102.      ,  16.777216],\n       [125.      ,  45.      ,  16.777216],\n       [261.      , 173.      ,  16.777216],\n       [194.      , 277.      ,  16.777216],\n       [127.      , 102.      ,  10.48576 ],\n       [125.      , 208.      ,  10.48576 ],\n       [267.      , 115.      ,  10.48576 ],\n       [263.      , 302.      ,  16.777216],\n       [196.      ,  43.      ,  10.48576 ],\n       [260.      ,  46.      ,  16.777216],\n       [267.      , 359.      ,  16.777216],\n       [ 54.      , 276.      ,  10.48576 ],\n       [ 58.      , 100.      ,  10.48576 ],\n       [ 52.      , 155.      ,  16.777216],\n       [ 52.      , 216.      ,  16.777216],\n       [ 54.      ,  42.      ,  16.777216]])\n \n blob_doh  \nskimage.feature.blob_doh(image, min_sigma=1, max_sigma=30, num_sigma=10, threshold=0.01, overlap=0.5, log_scale=False) [source]\n \nFinds blobs in the given grayscale image. Blobs are found using the Determinant of Hessian method [1]. For each blob found, the method returns its coordinates and the standard deviation of the Gaussian Kernel used for the Hessian matrix whose determinant detected the blob. Determinant of Hessians is approximated using [2].  Parameters \n \nimage2D ndarray \n\nInput grayscale image.Blobs can either be light on dark or vice versa.  \nmin_sigmafloat, optional \n\nThe minimum standard deviation for Gaussian Kernel used to compute Hessian matrix. Keep this low to detect smaller blobs.  \nmax_sigmafloat, optional \n\nThe maximum standard deviation for Gaussian Kernel used to compute Hessian matrix. Keep this high to detect larger blobs.  \nnum_sigmaint, optional \n\nThe number of intermediate values of standard deviations to consider between min_sigma and max_sigma.  \nthresholdfloat, optional. \n\nThe absolute lower bound for scale space maxima. Local maxima smaller than thresh are ignored. Reduce this to detect less prominent blobs.  \noverlapfloat, optional \n\nA value between 0 and 1. If the area of two blobs overlaps by a fraction greater than threshold, the smaller blob is eliminated.  \nlog_scalebool, optional \n\nIf set intermediate values of standard deviations are interpolated using a logarithmic scale to the base 10. If not, linear interpolation is used.    Returns \n \nA(n, 3) ndarray \n\nA 2d array with each row representing 3 values, (y,x,sigma) where (y,x) are coordinates of the blob and sigma is the standard deviation of the Gaussian kernel of the Hessian Matrix whose determinant detected the blob.     Notes The radius of each blob is approximately sigma. Computation of Determinant of Hessians is independent of the standard deviation. Therefore detecting larger blobs won\u2019t take more time. In methods line blob_dog() and blob_log() the computation of Gaussians for larger sigma takes more time. The downside is that this method can\u2019t be used for detecting blobs of radius less than 3px due to the box filters used in the approximation of Hessian Determinant. References  \n1  \nhttps://en.wikipedia.org/wiki/Blob_detection#The_determinant_of_the_Hessian  \n2  \nHerbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool, \u201cSURF: Speeded Up Robust Features\u201d ftp://ftp.vision.ee.ethz.ch/publications/articles/eth_biwi_00517.pdf   Examples >>> from skimage import data, feature\n>>> img = data.coins()\n>>> feature.blob_doh(img)\narray([[197.        , 153.        ,  20.33333333],\n       [124.        , 336.        ,  20.33333333],\n       [126.        , 153.        ,  20.33333333],\n       [195.        , 100.        ,  23.55555556],\n       [192.        , 212.        ,  23.55555556],\n       [121.        , 271.        ,  30.        ],\n       [126.        , 101.        ,  20.33333333],\n       [193.        , 275.        ,  23.55555556],\n       [123.        , 205.        ,  20.33333333],\n       [270.        , 363.        ,  30.        ],\n       [265.        , 113.        ,  23.55555556],\n       [262.        , 243.        ,  23.55555556],\n       [185.        , 348.        ,  30.        ],\n       [156.        , 302.        ,  30.        ],\n       [123.        ,  44.        ,  23.55555556],\n       [260.        , 173.        ,  30.        ],\n       [197.        ,  44.        ,  20.33333333]])\n \n blob_log  \nskimage.feature.blob_log(image, min_sigma=1, max_sigma=50, num_sigma=10, threshold=0.2, overlap=0.5, log_scale=False, *, exclude_border=False) [source]\n \nFinds blobs in the given grayscale image. Blobs are found using the Laplacian of Gaussian (LoG) method [1]. For each blob found, the method returns its coordinates and the standard deviation of the Gaussian kernel that detected the blob.  Parameters \n \nimage2D or 3D ndarray \n\nInput grayscale image, blobs are assumed to be light on dark background (white on black).  \nmin_sigmascalar or sequence of scalars, optional \n\nthe minimum standard deviation for Gaussian kernel. Keep this low to detect smaller blobs. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.  \nmax_sigmascalar or sequence of scalars, optional \n\nThe maximum standard deviation for Gaussian kernel. Keep this high to detect larger blobs. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.  \nnum_sigmaint, optional \n\nThe number of intermediate values of standard deviations to consider between min_sigma and max_sigma.  \nthresholdfloat, optional. \n\nThe absolute lower bound for scale space maxima. Local maxima smaller than thresh are ignored. Reduce this to detect blobs with less intensities.  \noverlapfloat, optional \n\nA value between 0 and 1. If the area of two blobs overlaps by a fraction greater than threshold, the smaller blob is eliminated.  \nlog_scalebool, optional \n\nIf set intermediate values of standard deviations are interpolated using a logarithmic scale to the base 10. If not, linear interpolation is used.  \nexclude_bordertuple of ints, int, or False, optional \n\nIf tuple of ints, the length of the tuple must match the input array\u2019s dimensionality. Each element of the tuple will exclude peaks from within exclude_border-pixels of the border of the image along that dimension. If nonzero int, exclude_border excludes peaks from within exclude_border-pixels of the border of the image. If zero or False, peaks are identified regardless of their distance from the border.    Returns \n \nA(n, image.ndim + sigma) ndarray \n\nA 2d array with each row representing 2 coordinate values for a 2D image, and 3 coordinate values for a 3D image, plus the sigma(s) used. When a single sigma is passed, outputs are: (r, c, sigma) or (p, r, c, sigma) where (r, c) or (p, r, c) are coordinates of the blob and sigma is the standard deviation of the Gaussian kernel which detected the blob. When an anisotropic gaussian is used (sigmas per dimension), the detected sigma is returned for each dimension.     Notes The radius of each blob is approximately \\(\\sqrt{2}\\sigma\\) for a 2-D image and \\(\\sqrt{3}\\sigma\\) for a 3-D image. References  \n1  \nhttps://en.wikipedia.org/wiki/Blob_detection#The_Laplacian_of_Gaussian   Examples >>> from skimage import data, feature, exposure\n>>> img = data.coins()\n>>> img = exposure.equalize_hist(img)  # improves detection\n>>> feature.blob_log(img, threshold = .3)\narray([[124.        , 336.        ,  11.88888889],\n       [198.        , 155.        ,  11.88888889],\n       [194.        , 213.        ,  17.33333333],\n       [121.        , 272.        ,  17.33333333],\n       [263.        , 244.        ,  17.33333333],\n       [194.        , 276.        ,  17.33333333],\n       [266.        , 115.        ,  11.88888889],\n       [128.        , 154.        ,  11.88888889],\n       [260.        , 174.        ,  17.33333333],\n       [198.        , 103.        ,  11.88888889],\n       [126.        , 208.        ,  11.88888889],\n       [127.        , 102.        ,  11.88888889],\n       [263.        , 302.        ,  17.33333333],\n       [197.        ,  44.        ,  11.88888889],\n       [185.        , 344.        ,  17.33333333],\n       [126.        ,  46.        ,  11.88888889],\n       [113.        , 323.        ,   1.        ]])\n \n canny  \nskimage.feature.canny(image, sigma=1.0, low_threshold=None, high_threshold=None, mask=None, use_quantiles=False) [source]\n \nEdge filter an image using the Canny algorithm.  Parameters \n \nimage2D array \n\nGrayscale input image to detect edges on; can be of any dtype.  \nsigmafloat, optional \n\nStandard deviation of the Gaussian filter.  \nlow_thresholdfloat, optional \n\nLower bound for hysteresis thresholding (linking edges). If None, low_threshold is set to 10% of dtype\u2019s max.  \nhigh_thresholdfloat, optional \n\nUpper bound for hysteresis thresholding (linking edges). If None, high_threshold is set to 20% of dtype\u2019s max.  \nmaskarray, dtype=bool, optional \n\nMask to limit the application of Canny to a certain area.  \nuse_quantilesbool, optional \n\nIf True then treat low_threshold and high_threshold as quantiles of the edge magnitude image, rather than absolute edge magnitude values. If True then the thresholds must be in the range [0, 1].    Returns \n \noutput2D array (image) \n\nThe binary edge map.      See also  \nskimage.sobel \n  Notes The steps of the algorithm are as follows:  Smooth the image using a Gaussian with sigma width. Apply the horizontal and vertical Sobel operators to get the gradients within the image. The edge strength is the norm of the gradient. Thin potential edges to 1-pixel wide curves. First, find the normal to the edge at each point. This is done by looking at the signs and the relative magnitude of the X-Sobel and Y-Sobel to sort the points into 4 categories: horizontal, vertical, diagonal and antidiagonal. Then look in the normal and reverse directions to see if the values in either of those directions are greater than the point in question. Use interpolation to get a mix of points instead of picking the one that\u2019s the closest to the normal. Perform a hysteresis thresholding: first label all points above the high threshold as edges. Then recursively label any point above the low threshold that is 8-connected to a labeled point as an edge.  References  \n1  \nCanny, J., A Computational Approach To Edge Detection, IEEE Trans. Pattern Analysis and Machine Intelligence, 8:679-714, 1986 DOI:10.1109/TPAMI.1986.4767851  \n2  \nWilliam Green\u2019s Canny tutorial https://en.wikipedia.org/wiki/Canny_edge_detector   Examples >>> from skimage import feature\n>>> # Generate noisy image of a square\n>>> im = np.zeros((256, 256))\n>>> im[64:-64, 64:-64] = 1\n>>> im += 0.2 * np.random.rand(*im.shape)\n>>> # First trial with the Canny filter, with the default smoothing\n>>> edges1 = feature.canny(im)\n>>> # Increase the smoothing for better results\n>>> edges2 = feature.canny(im, sigma=3)\n \n corner_fast  \nskimage.feature.corner_fast(image, n=12, threshold=0.15) [source]\n \nExtract FAST corners for a given image.  Parameters \n \nimage2D ndarray \n\nInput image.  \nnint, optional \n\nMinimum number of consecutive pixels out of 16 pixels on the circle that should all be either brighter or darker w.r.t testpixel. A point c on the circle is darker w.r.t test pixel p if Ic < Ip - threshold and brighter if Ic > Ip + threshold. Also stands for the n in FAST-n corner detector.  \nthresholdfloat, optional \n\nThreshold used in deciding whether the pixels on the circle are brighter, darker or similar w.r.t. the test pixel. Decrease the threshold when more corners are desired and vice-versa.    Returns \n \nresponsendarray \n\nFAST corner response image.     References  \n1  \nRosten, E., & Drummond, T. (2006, May). Machine learning for high-speed corner detection. In European conference on computer vision (pp. 430-443). Springer, Berlin, Heidelberg. DOI:10.1007/11744023_34 http://www.edwardrosten.com/work/rosten_2006_machine.pdf  \n2  \nWikipedia, \u201cFeatures from accelerated segment test\u201d, https://en.wikipedia.org/wiki/Features_from_accelerated_segment_test   Examples >>> from skimage.feature import corner_fast, corner_peaks\n>>> square = np.zeros((12, 12))\n>>> square[3:9, 3:9] = 1\n>>> square.astype(int)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> corner_peaks(corner_fast(square, 9), min_distance=1)\narray([[3, 3],\n       [3, 8],\n       [8, 3],\n       [8, 8]])\n \n corner_foerstner  \nskimage.feature.corner_foerstner(image, sigma=1) [source]\n \nCompute Foerstner corner measure response image. This corner detector uses information from the auto-correlation matrix A: A = [(imx**2)   (imx*imy)] = [Axx Axy]\n    [(imx*imy)   (imy**2)]   [Axy Ayy]\n Where imx and imy are first derivatives, averaged with a gaussian filter. The corner measure is then defined as: w = det(A) / trace(A)           (size of error ellipse)\nq = 4 * det(A) / trace(A)**2    (roundness of error ellipse)\n  Parameters \n \nimagendarray \n\nInput image.  \nsigmafloat, optional \n\nStandard deviation used for the Gaussian kernel, which is used as weighting function for the auto-correlation matrix.    Returns \n \nwndarray \n\nError ellipse sizes.  \nqndarray \n\nRoundness of error ellipse.     References  \n1  \nF\u00f6rstner, W., & G\u00fclch, E. (1987, June). A fast operator for detection and precise location of distinct points, corners and centres of circular features. In Proc. ISPRS intercommission conference on fast processing of photogrammetric data (pp. 281-305). https://cseweb.ucsd.edu/classes/sp02/cse252/foerstner/foerstner.pdf  \n2  \nhttps://en.wikipedia.org/wiki/Corner_detection   Examples >>> from skimage.feature import corner_foerstner, corner_peaks\n>>> square = np.zeros([10, 10])\n>>> square[2:8, 2:8] = 1\n>>> square.astype(int)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> w, q = corner_foerstner(square)\n>>> accuracy_thresh = 0.5\n>>> roundness_thresh = 0.3\n>>> foerstner = (q > roundness_thresh) * (w > accuracy_thresh) * w\n>>> corner_peaks(foerstner, min_distance=1)\narray([[2, 2],\n       [2, 7],\n       [7, 2],\n       [7, 7]])\n \n corner_harris  \nskimage.feature.corner_harris(image, method='k', k=0.05, eps=1e-06, sigma=1) [source]\n \nCompute Harris corner measure response image. This corner detector uses information from the auto-correlation matrix A: A = [(imx**2)   (imx*imy)] = [Axx Axy]\n    [(imx*imy)   (imy**2)]   [Axy Ayy]\n Where imx and imy are first derivatives, averaged with a gaussian filter. The corner measure is then defined as: det(A) - k * trace(A)**2\n or: 2 * det(A) / (trace(A) + eps)\n  Parameters \n \nimagendarray \n\nInput image.  \nmethod{\u2018k\u2019, \u2018eps\u2019}, optional \n\nMethod to compute the response image from the auto-correlation matrix.  \nkfloat, optional \n\nSensitivity factor to separate corners from edges, typically in range [0, 0.2]. Small values of k result in detection of sharp corners.  \nepsfloat, optional \n\nNormalisation factor (Noble\u2019s corner measure).  \nsigmafloat, optional \n\nStandard deviation used for the Gaussian kernel, which is used as weighting function for the auto-correlation matrix.    Returns \n \nresponsendarray \n\nHarris response image.     References  \n1  \nhttps://en.wikipedia.org/wiki/Corner_detection   Examples >>> from skimage.feature import corner_harris, corner_peaks\n>>> square = np.zeros([10, 10])\n>>> square[2:8, 2:8] = 1\n>>> square.astype(int)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> corner_peaks(corner_harris(square), min_distance=1)\narray([[2, 2],\n       [2, 7],\n       [7, 2],\n       [7, 7]])\n \n corner_kitchen_rosenfeld  \nskimage.feature.corner_kitchen_rosenfeld(image, mode='constant', cval=0) [source]\n \nCompute Kitchen and Rosenfeld corner measure response image. The corner measure is calculated as follows: (imxx * imy**2 + imyy * imx**2 - 2 * imxy * imx * imy)\n    / (imx**2 + imy**2)\n Where imx and imy are the first and imxx, imxy, imyy the second derivatives.  Parameters \n \nimagendarray \n\nInput image.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.    Returns \n \nresponsendarray \n\nKitchen and Rosenfeld response image.     References  \n1  \nKitchen, L., & Rosenfeld, A. (1982). Gray-level corner detection. Pattern recognition letters, 1(2), 95-102. DOI:10.1016/0167-8655(82)90020-4   \n corner_moravec  \nskimage.feature.corner_moravec(image, window_size=1) [source]\n \nCompute Moravec corner measure response image. This is one of the simplest corner detectors and is comparatively fast but has several limitations (e.g. not rotation invariant).  Parameters \n \nimagendarray \n\nInput image.  \nwindow_sizeint, optional \n\nWindow size.    Returns \n \nresponsendarray \n\nMoravec response image.     References  \n1  \nhttps://en.wikipedia.org/wiki/Corner_detection   Examples >>> from skimage.feature import corner_moravec\n>>> square = np.zeros([7, 7])\n>>> square[3, 3] = 1\n>>> square.astype(int)\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0]])\n>>> corner_moravec(square).astype(int)\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 2, 1, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0]])\n \n corner_orientations  \nskimage.feature.corner_orientations(image, corners, mask) [source]\n \nCompute the orientation of corners. The orientation of corners is computed using the first order central moment i.e. the center of mass approach. The corner orientation is the angle of the vector from the corner coordinate to the intensity centroid in the local neighborhood around the corner calculated using first order central moment.  Parameters \n \nimage2D array \n\nInput grayscale image.  \ncorners(N, 2) array \n\nCorner coordinates as (row, col).  \nmask2D array \n\nMask defining the local neighborhood of the corner used for the calculation of the central moment.    Returns \n \norientations(N, 1) array \n\nOrientations of corners in the range [-pi, pi].     References  \n1  \nEthan Rublee, Vincent Rabaud, Kurt Konolige and Gary Bradski \u201cORB : An efficient alternative to SIFT and SURF\u201d http://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf  \n2  \nPaul L. Rosin, \u201cMeasuring Corner Properties\u201d http://users.cs.cf.ac.uk/Paul.Rosin/corner2.pdf   Examples >>> from skimage.morphology import octagon\n>>> from skimage.feature import (corner_fast, corner_peaks,\n...                              corner_orientations)\n>>> square = np.zeros((12, 12))\n>>> square[3:9, 3:9] = 1\n>>> square.astype(int)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> corners = corner_peaks(corner_fast(square, 9), min_distance=1)\n>>> corners\narray([[3, 3],\n       [3, 8],\n       [8, 3],\n       [8, 8]])\n>>> orientations = corner_orientations(square, corners, octagon(3, 2))\n>>> np.rad2deg(orientations)\narray([  45.,  135.,  -45., -135.])\n \n corner_peaks  \nskimage.feature.corner_peaks(image, min_distance=1, threshold_abs=None, threshold_rel=None, exclude_border=True, indices=True, num_peaks=inf, footprint=None, labels=None, *, num_peaks_per_label=inf, p_norm=inf) [source]\n \nFind peaks in corner measure response image. This differs from skimage.feature.peak_local_max in that it suppresses multiple connected peaks with the same accumulator value.  Parameters \n \nimagendarray \n\nInput image.  \nmin_distanceint, optional \n\nThe minimal allowed distance separating peaks.  \n** \n\nSee skimage.feature.peak_local_max().  \np_normfloat \n\nWhich Minkowski p-norm to use. Should be in the range [1, inf]. A finite large p may cause a ValueError if overflow can occur. inf corresponds to the Chebyshev distance and 2 to the Euclidean distance.    Returns \n \noutputndarray or ndarray of bools \n\n If indices = True : (row, column, \u2026) coordinates of peaks. If indices = False : Boolean array shaped like image, with peaks represented by True values.       See also  \nskimage.feature.peak_local_max\n\n  Notes  Changed in version 0.18: The default value of threshold_rel has changed to None, which corresponds to letting skimage.feature.peak_local_max decide on the default. This is equivalent to threshold_rel=0.  The num_peaks limit is applied before suppression of connected peaks. To limit the number of peaks after suppression, set num_peaks=np.inf and post-process the output of this function. Examples >>> from skimage.feature import peak_local_max\n>>> response = np.zeros((5, 5))\n>>> response[2:4, 2:4] = 1\n>>> response\narray([[0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 1., 1., 0.],\n       [0., 0., 1., 1., 0.],\n       [0., 0., 0., 0., 0.]])\n>>> peak_local_max(response)\narray([[2, 2],\n       [2, 3],\n       [3, 2],\n       [3, 3]])\n>>> corner_peaks(response)\narray([[2, 2]])\n \n corner_shi_tomasi  \nskimage.feature.corner_shi_tomasi(image, sigma=1) [source]\n \nCompute Shi-Tomasi (Kanade-Tomasi) corner measure response image. This corner detector uses information from the auto-correlation matrix A: A = [(imx**2)   (imx*imy)] = [Axx Axy]\n    [(imx*imy)   (imy**2)]   [Axy Ayy]\n Where imx and imy are first derivatives, averaged with a gaussian filter. The corner measure is then defined as the smaller eigenvalue of A: ((Axx + Ayy) - sqrt((Axx - Ayy)**2 + 4 * Axy**2)) / 2\n  Parameters \n \nimagendarray \n\nInput image.  \nsigmafloat, optional \n\nStandard deviation used for the Gaussian kernel, which is used as weighting function for the auto-correlation matrix.    Returns \n \nresponsendarray \n\nShi-Tomasi response image.     References  \n1  \nhttps://en.wikipedia.org/wiki/Corner_detection   Examples >>> from skimage.feature import corner_shi_tomasi, corner_peaks\n>>> square = np.zeros([10, 10])\n>>> square[2:8, 2:8] = 1\n>>> square.astype(int)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> corner_peaks(corner_shi_tomasi(square), min_distance=1)\narray([[2, 2],\n       [2, 7],\n       [7, 2],\n       [7, 7]])\n \n corner_subpix  \nskimage.feature.corner_subpix(image, corners, window_size=11, alpha=0.99) [source]\n \nDetermine subpixel position of corners. A statistical test decides whether the corner is defined as the intersection of two edges or a single peak. Depending on the classification result, the subpixel corner location is determined based on the local covariance of the grey-values. If the significance level for either statistical test is not sufficient, the corner cannot be classified, and the output subpixel position is set to NaN.  Parameters \n \nimagendarray \n\nInput image.  \ncorners(N, 2) ndarray \n\nCorner coordinates (row, col).  \nwindow_sizeint, optional \n\nSearch window size for subpixel estimation.  \nalphafloat, optional \n\nSignificance level for corner classification.    Returns \n \npositions(N, 2) ndarray \n\nSubpixel corner positions. NaN for \u201cnot classified\u201d corners.     References  \n1  \nF\u00f6rstner, W., & G\u00fclch, E. (1987, June). A fast operator for detection and precise location of distinct points, corners and centres of circular features. In Proc. ISPRS intercommission conference on fast processing of photogrammetric data (pp. 281-305). https://cseweb.ucsd.edu/classes/sp02/cse252/foerstner/foerstner.pdf  \n2  \nhttps://en.wikipedia.org/wiki/Corner_detection   Examples >>> from skimage.feature import corner_harris, corner_peaks, corner_subpix\n>>> img = np.zeros((10, 10))\n>>> img[:5, :5] = 1\n>>> img[5:, 5:] = 1\n>>> img.astype(int)\narray([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]])\n>>> coords = corner_peaks(corner_harris(img), min_distance=2)\n>>> coords_subpix = corner_subpix(img, coords, window_size=7)\n>>> coords_subpix\narray([[4.5, 4.5]])\n \n daisy  \nskimage.feature.daisy(image, step=4, radius=15, rings=3, histograms=8, orientations=8, normalization='l1', sigmas=None, ring_radii=None, visualize=False) [source]\n \nExtract DAISY feature descriptors densely for the given image. DAISY is a feature descriptor similar to SIFT formulated in a way that allows for fast dense extraction. Typically, this is practical for bag-of-features image representations. The implementation follows Tola et al. [1] but deviate on the following points:  Histogram bin contribution are smoothed with a circular Gaussian window over the tonal range (the angular range). The sigma values of the spatial Gaussian smoothing in this code do not match the sigma values in the original code by Tola et al. [2]. In their code, spatial smoothing is applied to both the input image and the center histogram. However, this smoothing is not documented in [1] and, therefore, it is omitted.   Parameters \n \nimage(M, N) array \n\nInput image (grayscale).  \nstepint, optional \n\nDistance between descriptor sampling points.  \nradiusint, optional \n\nRadius (in pixels) of the outermost ring.  \nringsint, optional \n\nNumber of rings.  \nhistogramsint, optional \n\nNumber of histograms sampled per ring.  \norientationsint, optional \n\nNumber of orientations (bins) per histogram.  \nnormalization[ \u2018l1\u2019 | \u2018l2\u2019 | \u2018daisy\u2019 | \u2018off\u2019 ], optional \n\nHow to normalize the descriptors  \u2018l1\u2019: L1-normalization of each descriptor. \u2018l2\u2019: L2-normalization of each descriptor. \u2018daisy\u2019: L2-normalization of individual histograms. \u2018off\u2019: Disable normalization.   \nsigmas1D array of float, optional \n\nStandard deviation of spatial Gaussian smoothing for the center histogram and for each ring of histograms. The array of sigmas should be sorted from the center and out. I.e. the first sigma value defines the spatial smoothing of the center histogram and the last sigma value defines the spatial smoothing of the outermost ring. Specifying sigmas overrides the following parameter. rings = len(sigmas) - 1  \nring_radii1D array of int, optional \n\nRadius (in pixels) for each ring. Specifying ring_radii overrides the following two parameters. rings = len(ring_radii) radius = ring_radii[-1] If both sigmas and ring_radii are given, they must satisfy the following predicate since no radius is needed for the center histogram. len(ring_radii) == len(sigmas) + 1  \nvisualizebool, optional \n\nGenerate a visualization of the DAISY descriptors    Returns \n \ndescsarray \n\nGrid of DAISY descriptors for the given image as an array dimensionality (P, Q, R) where P = ceil((M - radius*2) / step) Q = ceil((N - radius*2) / step) R = (rings * histograms + 1) * orientations  \ndescs_img(M, N, 3) array (only if visualize==True) \n\nVisualization of the DAISY descriptors.     References  \n1(1,2)  \nTola et al. \u201cDaisy: An efficient dense descriptor applied to wide- baseline stereo.\u201d Pattern Analysis and Machine Intelligence, IEEE Transactions on 32.5 (2010): 815-830.  \n2  \nhttp://cvlab.epfl.ch/software/daisy   \n draw_haar_like_feature  \nskimage.feature.draw_haar_like_feature(image, r, c, width, height, feature_coord, color_positive_block=(1.0, 0.0, 0.0), color_negative_block=(0.0, 1.0, 0.0), alpha=0.5, max_n_features=None, random_state=None) [source]\n \nVisualization of Haar-like features.  Parameters \n \nimage(M, N) ndarray \n\nThe region of an integral image for which the features need to be computed.  \nrint \n\nRow-coordinate of top left corner of the detection window.  \ncint \n\nColumn-coordinate of top left corner of the detection window.  \nwidthint \n\nWidth of the detection window.  \nheightint \n\nHeight of the detection window.  \nfeature_coordndarray of list of tuples or None, optional \n\nThe array of coordinates to be extracted. This is useful when you want to recompute only a subset of features. In this case feature_type needs to be an array containing the type of each feature, as returned by haar_like_feature_coord(). By default, all coordinates are computed.  \ncolor_positive_rectangletuple of 3 floats \n\nFloats specifying the color for the positive block. Corresponding values define (R, G, B) values. Default value is red (1, 0, 0).  \ncolor_negative_blocktuple of 3 floats \n\nFloats specifying the color for the negative block Corresponding values define (R, G, B) values. Default value is blue (0, 1, 0).  \nalphafloat \n\nValue in the range [0, 1] that specifies opacity of visualization. 1 - fully transparent, 0 - opaque.  \nmax_n_featuresint, default=None \n\nThe maximum number of features to be returned. By default, all features are returned.  \nrandom_stateint, RandomState instance or None, optional \n\nIf int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. The random state is used when generating a set of features smaller than the total number of available features.    Returns \n \nfeatures(M, N), ndarray \n\nAn image in which the different features will be added.     Examples >>> import numpy as np\n>>> from skimage.feature import haar_like_feature_coord\n>>> from skimage.feature import draw_haar_like_feature\n>>> feature_coord, _ = haar_like_feature_coord(2, 2, 'type-4')\n>>> image = draw_haar_like_feature(np.zeros((2, 2)),\n...                                0, 0, 2, 2,\n...                                feature_coord,\n...                                max_n_features=1)\n>>> image\narray([[[0. , 0.5, 0. ],\n        [0.5, 0. , 0. ]],\n\n       [[0.5, 0. , 0. ],\n        [0. , 0.5, 0. ]]])\n \n draw_multiblock_lbp  \nskimage.feature.draw_multiblock_lbp(image, r, c, width, height, lbp_code=0, color_greater_block=(1, 1, 1), color_less_block=(0, 0.69, 0.96), alpha=0.5) [source]\n \nMulti-block local binary pattern visualization. Blocks with higher sums are colored with alpha-blended white rectangles, whereas blocks with lower sums are colored alpha-blended cyan. Colors and the alpha parameter can be changed.  Parameters \n \nimagendarray of float or uint \n\nImage on which to visualize the pattern.  \nrint \n\nRow-coordinate of top left corner of a rectangle containing feature.  \ncint \n\nColumn-coordinate of top left corner of a rectangle containing feature.  \nwidthint \n\nWidth of one of 9 equal rectangles that will be used to compute a feature.  \nheightint \n\nHeight of one of 9 equal rectangles that will be used to compute a feature.  \nlbp_codeint \n\nThe descriptor of feature to visualize. If not provided, the descriptor with 0 value will be used.  \ncolor_greater_blocktuple of 3 floats \n\nFloats specifying the color for the block that has greater intensity value. They should be in the range [0, 1]. Corresponding values define (R, G, B) values. Default value is white (1, 1, 1).  \ncolor_greater_blocktuple of 3 floats \n\nFloats specifying the color for the block that has greater intensity value. They should be in the range [0, 1]. Corresponding values define (R, G, B) values. Default value is cyan (0, 0.69, 0.96).  \nalphafloat \n\nValue in the range [0, 1] that specifies opacity of visualization. 1 - fully transparent, 0 - opaque.    Returns \n \noutputndarray of float \n\nImage with MB-LBP visualization.     References  \n1  \nFace Detection Based on Multi-Block LBP Representation. Lun Zhang, Rufeng Chu, Shiming Xiang, Shengcai Liao, Stan Z. Li http://www.cbsr.ia.ac.cn/users/scliao/papers/Zhang-ICB07-MBLBP.pdf   \n greycomatrix  \nskimage.feature.greycomatrix(image, distances, angles, levels=None, symmetric=False, normed=False) [source]\n \nCalculate the grey-level co-occurrence matrix. A grey level co-occurrence matrix is a histogram of co-occurring greyscale values at a given offset over an image.  Parameters \n \nimagearray_like \n\nInteger typed input image. Only positive valued images are supported. If type is other than uint8, the argument levels needs to be set.  \ndistancesarray_like \n\nList of pixel pair distance offsets.  \nanglesarray_like \n\nList of pixel pair angles in radians.  \nlevelsint, optional \n\nThe input image should contain integers in [0, levels-1], where levels indicate the number of grey-levels counted (typically 256 for an 8-bit image). This argument is required for 16-bit images or higher and is typically the maximum of the image. As the output matrix is at least levels x levels, it might be preferable to use binning of the input image rather than large values for levels.  \nsymmetricbool, optional \n\nIf True, the output matrix P[:, :, d, theta] is symmetric. This is accomplished by ignoring the order of value pairs, so both (i, j) and (j, i) are accumulated when (i, j) is encountered for a given offset. The default is False.  \nnormedbool, optional \n\nIf True, normalize each matrix P[:, :, d, theta] by dividing by the total number of accumulated co-occurrences for the given offset. The elements of the resulting matrix sum to 1. The default is False.    Returns \n \nP4-D ndarray \n\nThe grey-level co-occurrence histogram. The value P[i,j,d,theta] is the number of times that grey-level j occurs at a distance d and at an angle theta from grey-level i. If normed is False, the output is of type uint32, otherwise it is float64. The dimensions are: levels x levels x number of distances x number of angles.     References  \n1  \nThe GLCM Tutorial Home Page, http://www.fp.ucalgary.ca/mhallbey/tutorial.htm  \n2  \nHaralick, RM.; Shanmugam, K., \u201cTextural features for image classification\u201d IEEE Transactions on systems, man, and cybernetics 6 (1973): 610-621. DOI:10.1109/TSMC.1973.4309314  \n3  \nPattern Recognition Engineering, Morton Nadler & Eric P. Smith  \n4  \nWikipedia, https://en.wikipedia.org/wiki/Co-occurrence_matrix   Examples Compute 2 GLCMs: One for a 1-pixel offset to the right, and one for a 1-pixel offset upwards. >>> image = np.array([[0, 0, 1, 1],\n...                   [0, 0, 1, 1],\n...                   [0, 2, 2, 2],\n...                   [2, 2, 3, 3]], dtype=np.uint8)\n>>> result = greycomatrix(image, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4],\n...                       levels=4)\n>>> result[:, :, 0, 0]\narray([[2, 2, 1, 0],\n       [0, 2, 0, 0],\n       [0, 0, 3, 1],\n       [0, 0, 0, 1]], dtype=uint32)\n>>> result[:, :, 0, 1]\narray([[1, 1, 3, 0],\n       [0, 1, 1, 0],\n       [0, 0, 0, 2],\n       [0, 0, 0, 0]], dtype=uint32)\n>>> result[:, :, 0, 2]\narray([[3, 0, 2, 0],\n       [0, 2, 2, 0],\n       [0, 0, 1, 2],\n       [0, 0, 0, 0]], dtype=uint32)\n>>> result[:, :, 0, 3]\narray([[2, 0, 0, 0],\n       [1, 1, 2, 0],\n       [0, 0, 2, 1],\n       [0, 0, 0, 0]], dtype=uint32)\n \n Examples using skimage.feature.greycomatrix\n \n  GLCM Texture Features   greycoprops  \nskimage.feature.greycoprops(P, prop='contrast') [source]\n \nCalculate texture properties of a GLCM. Compute a feature of a grey level co-occurrence matrix to serve as a compact summary of the matrix. The properties are computed as follows:  \u2018contrast\u2019: \\(\\sum_{i,j=0}^{levels-1} P_{i,j}(i-j)^2\\)\n \u2018dissimilarity\u2019: \\(\\sum_{i,j=0}^{levels-1}P_{i,j}|i-j|\\)\n \u2018homogeneity\u2019: \\(\\sum_{i,j=0}^{levels-1}\\frac{P_{i,j}}{1+(i-j)^2}\\)\n \u2018ASM\u2019: \\(\\sum_{i,j=0}^{levels-1} P_{i,j}^2\\)\n \u2018energy\u2019: \\(\\sqrt{ASM}\\)\n \n \u2018correlation\u2019:\n\n \\[\\sum_{i,j=0}^{levels-1} P_{i,j}\\left[\\frac{(i-\\mu_i) \\ (j-\\mu_j)}{\\sqrt{(\\sigma_i^2)(\\sigma_j^2)}}\\right]\\]     Each GLCM is normalized to have a sum of 1 before the computation of texture properties.  Parameters \n \nPndarray \n\nInput array. P is the grey-level co-occurrence histogram for which to compute the specified property. The value P[i,j,d,theta] is the number of times that grey-level j occurs at a distance d and at an angle theta from grey-level i.  \nprop{\u2018contrast\u2019, \u2018dissimilarity\u2019, \u2018homogeneity\u2019, \u2018energy\u2019, \u2018correlation\u2019, \u2018ASM\u2019}, optional \n\nThe property of the GLCM to compute. The default is \u2018contrast\u2019.    Returns \n \nresults2-D ndarray \n\n2-dimensional array. results[d, a] is the property \u2018prop\u2019 for the d\u2019th distance and the a\u2019th angle.     References  \n1  \nThe GLCM Tutorial Home Page, http://www.fp.ucalgary.ca/mhallbey/tutorial.htm   Examples Compute the contrast for GLCMs with distances [1, 2] and angles [0 degrees, 90 degrees] >>> image = np.array([[0, 0, 1, 1],\n...                   [0, 0, 1, 1],\n...                   [0, 2, 2, 2],\n...                   [2, 2, 3, 3]], dtype=np.uint8)\n>>> g = greycomatrix(image, [1, 2], [0, np.pi/2], levels=4,\n...                  normed=True, symmetric=True)\n>>> contrast = greycoprops(g, 'contrast')\n>>> contrast\narray([[0.58333333, 1.        ],\n       [1.25      , 2.75      ]])\n \n Examples using skimage.feature.greycoprops\n \n  GLCM Texture Features   haar_like_feature  \nskimage.feature.haar_like_feature(int_image, r, c, width, height, feature_type=None, feature_coord=None) [source]\n \nCompute the Haar-like features for a region of interest (ROI) of an integral image. Haar-like features have been successfully used for image classification and object detection [1]. It has been used for real-time face detection algorithm proposed in [2].  Parameters \n \nint_image(M, N) ndarray \n\nIntegral image for which the features need to be computed.  \nrint \n\nRow-coordinate of top left corner of the detection window.  \ncint \n\nColumn-coordinate of top left corner of the detection window.  \nwidthint \n\nWidth of the detection window.  \nheightint \n\nHeight of the detection window.  \nfeature_typestr or list of str or None, optional \n\nThe type of feature to consider:  \u2018type-2-x\u2019: 2 rectangles varying along the x axis; \u2018type-2-y\u2019: 2 rectangles varying along the y axis; \u2018type-3-x\u2019: 3 rectangles varying along the x axis; \u2018type-3-y\u2019: 3 rectangles varying along the y axis; \u2018type-4\u2019: 4 rectangles varying along x and y axis.  By default all features are extracted. If using with feature_coord, it should correspond to the feature type of each associated coordinate feature.  \nfeature_coordndarray of list of tuples or None, optional \n\nThe array of coordinates to be extracted. This is useful when you want to recompute only a subset of features. In this case feature_type needs to be an array containing the type of each feature, as returned by haar_like_feature_coord(). By default, all coordinates are computed.    Returns \n \nhaar_features(n_features,) ndarray of int or float \n\nResulting Haar-like features. Each value is equal to the subtraction of sums of the positive and negative rectangles. The data type depends of the data type of int_image: int when the data type of int_image is uint or int and float when the data type of int_image is float.     Notes When extracting those features in parallel, be aware that the choice of the backend (i.e. multiprocessing vs threading) will have an impact on the performance. The rule of thumb is as follows: use multiprocessing when extracting features for all possible ROI in an image; use threading when extracting the feature at specific location for a limited number of ROIs. Refer to the example Face classification using Haar-like feature descriptor for more insights. References  \n1  \nhttps://en.wikipedia.org/wiki/Haar-like_feature  \n2  \nOren, M., Papageorgiou, C., Sinha, P., Osuna, E., & Poggio, T. (1997, June). Pedestrian detection using wavelet templates. In Computer Vision and Pattern Recognition, 1997. Proceedings., 1997 IEEE Computer Society Conference on (pp. 193-199). IEEE. http://tinyurl.com/y6ulxfta DOI:10.1109/CVPR.1997.609319  \n3  \nViola, Paul, and Michael J. Jones. \u201cRobust real-time face detection.\u201d International journal of computer vision 57.2 (2004): 137-154. https://www.merl.com/publications/docs/TR2004-043.pdf DOI:10.1109/CVPR.2001.990517   Examples >>> import numpy as np\n>>> from skimage.transform import integral_image\n>>> from skimage.feature import haar_like_feature\n>>> img = np.ones((5, 5), dtype=np.uint8)\n>>> img_ii = integral_image(img)\n>>> feature = haar_like_feature(img_ii, 0, 0, 5, 5, 'type-3-x')\n>>> feature\narray([-1, -2, -3, -4, -1, -2, -3, -4, -1, -2, -3, -4, -1, -2, -3, -4, -1,\n       -2, -3, -4, -1, -2, -3, -4, -1, -2, -3, -1, -2, -3, -1, -2, -3, -1,\n       -2, -1, -2, -1, -2, -1, -1, -1])\n You can compute the feature for some pre-computed coordinates. >>> from skimage.feature import haar_like_feature_coord\n>>> feature_coord, feature_type = zip(\n...     *[haar_like_feature_coord(5, 5, feat_t)\n...       for feat_t in ('type-2-x', 'type-3-x')])\n>>> # only select one feature over two\n>>> feature_coord = np.concatenate([x[::2] for x in feature_coord])\n>>> feature_type = np.concatenate([x[::2] for x in feature_type])\n>>> feature = haar_like_feature(img_ii, 0, 0, 5, 5,\n...                             feature_type=feature_type,\n...                             feature_coord=feature_coord)\n>>> feature\narray([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0, -1, -3, -1, -3, -1, -3, -1, -3, -1,\n       -3, -1, -3, -1, -3, -2, -1, -3, -2, -2, -2, -1])\n \n haar_like_feature_coord  \nskimage.feature.haar_like_feature_coord(width, height, feature_type=None) [source]\n \nCompute the coordinates of Haar-like features.  Parameters \n \nwidthint \n\nWidth of the detection window.  \nheightint \n\nHeight of the detection window.  \nfeature_typestr or list of str or None, optional \n\nThe type of feature to consider:  \u2018type-2-x\u2019: 2 rectangles varying along the x axis; \u2018type-2-y\u2019: 2 rectangles varying along the y axis; \u2018type-3-x\u2019: 3 rectangles varying along the x axis; \u2018type-3-y\u2019: 3 rectangles varying along the y axis; \u2018type-4\u2019: 4 rectangles varying along x and y axis.  By default all features are extracted.    Returns \n \nfeature_coord(n_features, n_rectangles, 2, 2), ndarray of list of tuple coord \n\nCoordinates of the rectangles for each feature.  \nfeature_type(n_features,), ndarray of str \n\nThe corresponding type for each feature.     Examples >>> import numpy as np\n>>> from skimage.transform import integral_image\n>>> from skimage.feature import haar_like_feature_coord\n>>> feat_coord, feat_type = haar_like_feature_coord(2, 2, 'type-4')\n>>> feat_coord \narray([ list([[(0, 0), (0, 0)], [(0, 1), (0, 1)],\n              [(1, 1), (1, 1)], [(1, 0), (1, 0)]])], dtype=object)\n>>> feat_type\narray(['type-4'], dtype=object)\n \n hessian_matrix  \nskimage.feature.hessian_matrix(image, sigma=1, mode='constant', cval=0, order='rc') [source]\n \nCompute Hessian matrix. The Hessian matrix is defined as: H = [Hrr Hrc]\n    [Hrc Hcc]\n which is computed by convolving the image with the second derivatives of the Gaussian kernel in the respective r- and c-directions.  Parameters \n \nimagendarray \n\nInput image.  \nsigmafloat \n\nStandard deviation used for the Gaussian kernel, which is used as weighting function for the auto-correlation matrix.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.  \norder{\u2018rc\u2019, \u2018xy\u2019}, optional \n\nThis parameter allows for the use of reverse or forward order of the image axes in gradient computation. \u2018rc\u2019 indicates the use of the first axis initially (Hrr, Hrc, Hcc), whilst \u2018xy\u2019 indicates the usage of the last axis initially (Hxx, Hxy, Hyy)    Returns \n \nHrrndarray \n\nElement of the Hessian matrix for each pixel in the input image.  \nHrcndarray \n\nElement of the Hessian matrix for each pixel in the input image.  \nHccndarray \n\nElement of the Hessian matrix for each pixel in the input image.     Examples >>> from skimage.feature import hessian_matrix\n>>> square = np.zeros((5, 5))\n>>> square[2, 2] = 4\n>>> Hrr, Hrc, Hcc = hessian_matrix(square, sigma=0.1, order='rc')\n>>> Hrc\narray([[ 0.,  0.,  0.,  0.,  0.],\n       [ 0.,  1.,  0., -1.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.],\n       [ 0., -1.,  0.,  1.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.]])\n \n hessian_matrix_det  \nskimage.feature.hessian_matrix_det(image, sigma=1, approximate=True) [source]\n \nCompute the approximate Hessian Determinant over an image. The 2D approximate method uses box filters over integral images to compute the approximate Hessian Determinant, as described in [1].  Parameters \n \nimagearray \n\nThe image over which to compute Hessian Determinant.  \nsigmafloat, optional \n\nStandard deviation used for the Gaussian kernel, used for the Hessian matrix.  \napproximatebool, optional \n\nIf True and the image is 2D, use a much faster approximate computation. This argument has no effect on 3D and higher images.    Returns \n \noutarray \n\nThe array of the Determinant of Hessians.     Notes For 2D images when approximate=True, the running time of this method only depends on size of the image. It is independent of sigma as one would expect. The downside is that the result for sigma less than 3 is not accurate, i.e., not similar to the result obtained if someone computed the Hessian and took its determinant. References  \n1  \nHerbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool, \u201cSURF: Speeded Up Robust Features\u201d ftp://ftp.vision.ee.ethz.ch/publications/articles/eth_biwi_00517.pdf   \n hessian_matrix_eigvals  \nskimage.feature.hessian_matrix_eigvals(H_elems) [source]\n \nCompute eigenvalues of Hessian matrix.  Parameters \n \nH_elemslist of ndarray \n\nThe upper-diagonal elements of the Hessian matrix, as returned by hessian_matrix.    Returns \n \neigsndarray \n\nThe eigenvalues of the Hessian matrix, in decreasing order. The eigenvalues are the leading dimension. That is, eigs[i, j, k] contains the ith-largest eigenvalue at position (j, k).     Examples >>> from skimage.feature import hessian_matrix, hessian_matrix_eigvals\n>>> square = np.zeros((5, 5))\n>>> square[2, 2] = 4\n>>> H_elems = hessian_matrix(square, sigma=0.1, order='rc')\n>>> hessian_matrix_eigvals(H_elems)[0]\narray([[ 0.,  0.,  2.,  0.,  0.],\n       [ 0.,  1.,  0.,  1.,  0.],\n       [ 2.,  0., -2.,  0.,  2.],\n       [ 0.,  1.,  0.,  1.,  0.],\n       [ 0.,  0.,  2.,  0.,  0.]])\n \n hog  \nskimage.feature.hog(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(3, 3), block_norm='L2-Hys', visualize=False, transform_sqrt=False, feature_vector=True, multichannel=None) [source]\n \nExtract Histogram of Oriented Gradients (HOG) for a given image. Compute a Histogram of Oriented Gradients (HOG) by  (optional) global image normalization computing the gradient image in row and col\n computing gradient histograms normalizing across blocks flattening into a feature vector   Parameters \n \nimage(M, N[, C]) ndarray \n\nInput image.  \norientationsint, optional \n\nNumber of orientation bins.  \npixels_per_cell2-tuple (int, int), optional \n\nSize (in pixels) of a cell.  \ncells_per_block2-tuple (int, int), optional \n\nNumber of cells in each block.  \nblock_normstr {\u2018L1\u2019, \u2018L1-sqrt\u2019, \u2018L2\u2019, \u2018L2-Hys\u2019}, optional \n\nBlock normalization method:  \nL1 \n\nNormalization using L1-norm.  \nL1-sqrt \n\nNormalization using L1-norm, followed by square root.  \nL2 \n\nNormalization using L2-norm.  \nL2-Hys \n\nNormalization using L2-norm, followed by limiting the maximum values to 0.2 (Hys stands for hysteresis) and renormalization using L2-norm. (default) For details, see [3], [4].    \nvisualizebool, optional \n\nAlso return an image of the HOG. For each cell and orientation bin, the image contains a line segment that is centered at the cell center, is perpendicular to the midpoint of the range of angles spanned by the orientation bin, and has intensity proportional to the corresponding histogram value.  \ntransform_sqrtbool, optional \n\nApply power law compression to normalize the image before processing. DO NOT use this if the image contains negative values. Also see notes section below.  \nfeature_vectorbool, optional \n\nReturn the data as a feature vector by calling .ravel() on the result just before returning.  \nmultichannelboolean, optional \n\nIf True, the last image dimension is considered as a color channel, otherwise as spatial.    Returns \n \nout(n_blocks_row, n_blocks_col, n_cells_row, n_cells_col, n_orient) ndarray \n\nHOG descriptor for the image. If feature_vector is True, a 1D (flattened) array is returned.  \nhog_image(M, N) ndarray, optional \n\nA visualisation of the HOG image. Only provided if visualize is True.     Notes The presented code implements the HOG extraction method from [2] with the following changes: (I) blocks of (3, 3) cells are used ((2, 2) in the paper); (II) no smoothing within cells (Gaussian spatial window with sigma=8pix in the paper); (III) L1 block normalization is used (L2-Hys in the paper). Power law compression, also known as Gamma correction, is used to reduce the effects of shadowing and illumination variations. The compression makes the dark regions lighter. When the kwarg transform_sqrt is set to True, the function computes the square root of each color channel and then applies the hog algorithm to the image. References  \n1  \nhttps://en.wikipedia.org/wiki/Histogram_of_oriented_gradients  \n2  \nDalal, N and Triggs, B, Histograms of Oriented Gradients for Human Detection, IEEE Computer Society Conference on Computer Vision and Pattern Recognition 2005 San Diego, CA, USA, https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf, DOI:10.1109/CVPR.2005.177  \n3  \nLowe, D.G., Distinctive image features from scale-invatiant keypoints, International Journal of Computer Vision (2004) 60: 91, http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf, DOI:10.1023/B:VISI.0000029664.99615.94  \n4  \nDalal, N, Finding People in Images and Videos, Human-Computer Interaction [cs.HC], Institut National Polytechnique de Grenoble - INPG, 2006, https://tel.archives-ouvertes.fr/tel-00390303/file/NavneetDalalThesis.pdf   \n local_binary_pattern  \nskimage.feature.local_binary_pattern(image, P, R, method='default') [source]\n \nGray scale and rotation invariant LBP (Local Binary Patterns). LBP is an invariant descriptor that can be used for texture classification.  Parameters \n \nimage(N, M) array \n\nGraylevel image.  \nPint \n\nNumber of circularly symmetric neighbour set points (quantization of the angular space).  \nRfloat \n\nRadius of circle (spatial resolution of the operator).  \nmethod{\u2018default\u2019, \u2018ror\u2019, \u2018uniform\u2019, \u2018var\u2019} \n\nMethod to determine the pattern.  \n \u2018default\u2019: original local binary pattern which is gray scale but not\n\nrotation invariant.    \n \u2018ror\u2019: extension of default implementation which is gray scale and\n\nrotation invariant.    \n \u2018uniform\u2019: improved rotation invariance with uniform patterns and\n\nfiner quantization of the angular space which is gray scale and rotation invariant.    \n \u2018nri_uniform\u2019: non rotation-invariant uniform patterns variant\n\nwhich is only gray scale invariant [2].    \n \u2018var\u2019: rotation invariant variance measures of the contrast of local\n\nimage texture which is rotation but not gray scale invariant.        Returns \n \noutput(N, M) array \n\nLBP image.     References  \n1  \nMultiresolution Gray-Scale and Rotation Invariant Texture Classification with Local Binary Patterns. Timo Ojala, Matti Pietikainen, Topi Maenpaa. http://www.ee.oulu.fi/research/mvmp/mvg/files/pdf/pdf_94.pdf, 2002.  \n2  \nFace recognition with local binary patterns. Timo Ahonen, Abdenour Hadid, Matti Pietikainen, http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.214.6851, 2004.   \n masked_register_translation  \nskimage.feature.masked_register_translation(src_image, target_image, src_mask, target_mask=None, overlap_ratio=0.3) [source]\n \nDeprecated function. Use skimage.registration.phase_cross_correlation instead. \n match_descriptors  \nskimage.feature.match_descriptors(descriptors1, descriptors2, metric=None, p=2, max_distance=inf, cross_check=True, max_ratio=1.0) [source]\n \nBrute-force matching of descriptors. For each descriptor in the first set this matcher finds the closest descriptor in the second set (and vice-versa in the case of enabled cross-checking).  Parameters \n \ndescriptors1(M, P) array \n\nDescriptors of size P about M keypoints in the first image.  \ndescriptors2(N, P) array \n\nDescriptors of size P about N keypoints in the second image.  \nmetric{\u2018euclidean\u2019, \u2018cityblock\u2019, \u2018minkowski\u2019, \u2018hamming\u2019, \u2026} , optional \n\nThe metric to compute the distance between two descriptors. See scipy.spatial.distance.cdist for all possible types. The hamming distance should be used for binary descriptors. By default the L2-norm is used for all descriptors of dtype float or double and the Hamming distance is used for binary descriptors automatically.  \npint, optional \n\nThe p-norm to apply for metric='minkowski'.  \nmax_distancefloat, optional \n\nMaximum allowed distance between descriptors of two keypoints in separate images to be regarded as a match.  \ncross_checkbool, optional \n\nIf True, the matched keypoints are returned after cross checking i.e. a matched pair (keypoint1, keypoint2) is returned if keypoint2 is the best match for keypoint1 in second image and keypoint1 is the best match for keypoint2 in first image.  \nmax_ratiofloat, optional \n\nMaximum ratio of distances between first and second closest descriptor in the second set of descriptors. This threshold is useful to filter ambiguous matches between the two descriptor sets. The choice of this value depends on the statistics of the chosen descriptor, e.g., for SIFT descriptors a value of 0.8 is usually chosen, see D.G. Lowe, \u201cDistinctive Image Features from Scale-Invariant Keypoints\u201d, International Journal of Computer Vision, 2004.    Returns \n \nmatches(Q, 2) array \n\nIndices of corresponding matches in first and second set of descriptors, where matches[:, 0] denote the indices in the first and matches[:, 1] the indices in the second set of descriptors.     \n match_template  \nskimage.feature.match_template(image, template, pad_input=False, mode='constant', constant_values=0) [source]\n \nMatch a template to a 2-D or 3-D image using normalized correlation. The output is an array with values between -1.0 and 1.0. The value at a given position corresponds to the correlation coefficient between the image and the template. For pad_input=True matches correspond to the center and otherwise to the top-left corner of the template. To find the best match you must search for peaks in the response (output) image.  Parameters \n \nimage(M, N[, D]) array \n\n2-D or 3-D input image.  \ntemplate(m, n[, d]) array \n\nTemplate to locate. It must be (m <= M, n <= N[, d <= D]).  \npad_inputbool \n\nIf True, pad image so that output is the same size as the image, and output values correspond to the template center. Otherwise, the output is an array with shape (M - m + 1, N - n + 1) for an (M, N) image and an (m, n) template, and matches correspond to origin (top-left corner) of the template.  \nmodesee numpy.pad, optional \n\nPadding mode.  \nconstant_valuessee numpy.pad, optional \n\nConstant values used in conjunction with mode='constant'.    Returns \n \noutputarray \n\nResponse image with correlation coefficients.     Notes Details on the cross-correlation are presented in [1]. This implementation uses FFT convolutions of the image and the template. Reference [2] presents similar derivations but the approximation presented in this reference is not used in our implementation. References  \n1  \nJ. P. Lewis, \u201cFast Normalized Cross-Correlation\u201d, Industrial Light and Magic.  \n2  \nBriechle and Hanebeck, \u201cTemplate Matching using Fast Normalized Cross Correlation\u201d, Proceedings of the SPIE (2001). DOI:10.1117/12.421129   Examples >>> template = np.zeros((3, 3))\n>>> template[1, 1] = 1\n>>> template\narray([[0., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 0.]])\n>>> image = np.zeros((6, 6))\n>>> image[1, 1] = 1\n>>> image[4, 4] = -1\n>>> image\narray([[ 0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0., -1.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.]])\n>>> result = match_template(image, template)\n>>> np.round(result, 3)\narray([[ 1.   , -0.125,  0.   ,  0.   ],\n       [-0.125, -0.125,  0.   ,  0.   ],\n       [ 0.   ,  0.   ,  0.125,  0.125],\n       [ 0.   ,  0.   ,  0.125, -1.   ]])\n>>> result = match_template(image, template, pad_input=True)\n>>> np.round(result, 3)\narray([[-0.125, -0.125, -0.125,  0.   ,  0.   ,  0.   ],\n       [-0.125,  1.   , -0.125,  0.   ,  0.   ,  0.   ],\n       [-0.125, -0.125, -0.125,  0.   ,  0.   ,  0.   ],\n       [ 0.   ,  0.   ,  0.   ,  0.125,  0.125,  0.125],\n       [ 0.   ,  0.   ,  0.   ,  0.125, -1.   ,  0.125],\n       [ 0.   ,  0.   ,  0.   ,  0.125,  0.125,  0.125]])\n \n multiblock_lbp  \nskimage.feature.multiblock_lbp(int_image, r, c, width, height) [source]\n \nMulti-block local binary pattern (MB-LBP). The features are calculated similarly to local binary patterns (LBPs), (See local_binary_pattern()) except that summed blocks are used instead of individual pixel values. MB-LBP is an extension of LBP that can be computed on multiple scales in constant time using the integral image. Nine equally-sized rectangles are used to compute a feature. For each rectangle, the sum of the pixel intensities is computed. Comparisons of these sums to that of the central rectangle determine the feature, similarly to LBP.  Parameters \n \nint_image(N, M) array \n\nIntegral image.  \nrint \n\nRow-coordinate of top left corner of a rectangle containing feature.  \ncint \n\nColumn-coordinate of top left corner of a rectangle containing feature.  \nwidthint \n\nWidth of one of the 9 equal rectangles that will be used to compute a feature.  \nheightint \n\nHeight of one of the 9 equal rectangles that will be used to compute a feature.    Returns \n \noutputint \n\n8-bit MB-LBP feature descriptor.     References  \n1  \nFace Detection Based on Multi-Block LBP Representation. Lun Zhang, Rufeng Chu, Shiming Xiang, Shengcai Liao, Stan Z. Li http://www.cbsr.ia.ac.cn/users/scliao/papers/Zhang-ICB07-MBLBP.pdf   \n multiscale_basic_features  \nskimage.feature.multiscale_basic_features(image, multichannel=False, intensity=True, edges=True, texture=True, sigma_min=0.5, sigma_max=16, num_sigma=None, num_workers=None) [source]\n \nLocal features for a single- or multi-channel nd image. Intensity, gradient intensity and local structure are computed at different scales thanks to Gaussian blurring.  Parameters \n \nimagendarray \n\nInput image, which can be grayscale or multichannel.  \nmultichannelbool, default False \n\nTrue if the last dimension corresponds to color channels.  \nintensitybool, default True \n\nIf True, pixel intensities averaged over the different scales are added to the feature set.  \nedgesbool, default True \n\nIf True, intensities of local gradients averaged over the different scales are added to the feature set.  \ntexturebool, default True \n\nIf True, eigenvalues of the Hessian matrix after Gaussian blurring at different scales are added to the feature set.  \nsigma_minfloat, optional \n\nSmallest value of the Gaussian kernel used to average local neighbourhoods before extracting features.  \nsigma_maxfloat, optional \n\nLargest value of the Gaussian kernel used to average local neighbourhoods before extracting features.  \nnum_sigmaint, optional \n\nNumber of values of the Gaussian kernel between sigma_min and sigma_max. If None, sigma_min multiplied by powers of 2 are used.  \nnum_workersint or None, optional \n\nThe number of parallel threads to use. If set to None, the full set of available cores are used.    Returns \n \nfeaturesnp.ndarray \n\nArray of shape image.shape + (n_features,)     \n Examples using skimage.feature.multiscale_basic_features\n \n  Trainable segmentation using local features and random forests   peak_local_max  \nskimage.feature.peak_local_max(image, min_distance=1, threshold_abs=None, threshold_rel=None, exclude_border=True, indices=True, num_peaks=inf, footprint=None, labels=None, num_peaks_per_label=inf, p_norm=inf) [source]\n \nFind peaks in an image as coordinate list or boolean mask. Peaks are the local maxima in a region of 2 * min_distance + 1 (i.e. peaks are separated by at least min_distance). If both threshold_abs and threshold_rel are provided, the maximum of the two is chosen as the minimum intensity threshold of peaks.  Changed in version 0.18: Prior to version 0.18, peaks of the same height within a radius of min_distance were all returned, but this could cause unexpected behaviour. From 0.18 onwards, an arbitrary peak within the region is returned. See issue gh-2592.   Parameters \n \nimagendarray \n\nInput image.  \nmin_distanceint, optional \n\nThe minimal allowed distance separating peaks. To find the maximum number of peaks, use min_distance=1.  \nthreshold_absfloat, optional \n\nMinimum intensity of peaks. By default, the absolute threshold is the minimum intensity of the image.  \nthreshold_relfloat, optional \n\nMinimum intensity of peaks, calculated as max(image) * threshold_rel.  \nexclude_borderint, tuple of ints, or bool, optional \n\nIf positive integer, exclude_border excludes peaks from within exclude_border-pixels of the border of the image. If tuple of non-negative ints, the length of the tuple must match the input array\u2019s dimensionality. Each element of the tuple will exclude peaks from within exclude_border-pixels of the border of the image along that dimension. If True, takes the min_distance parameter as value. If zero or False, peaks are identified regardless of their distance from the border.  \nindicesbool, optional \n\nIf True, the output will be an array representing peak coordinates. The coordinates are sorted according to peaks values (Larger first). If False, the output will be a boolean array shaped as image.shape with peaks present at True elements. indices is deprecated and will be removed in version 0.20. Default behavior will be to always return peak coordinates. You can obtain a mask as shown in the example below.  \nnum_peaksint, optional \n\nMaximum number of peaks. When the number of peaks exceeds num_peaks, return num_peaks peaks based on highest peak intensity.  \nfootprintndarray of bools, optional \n\nIf provided, footprint == 1 represents the local region within which to search for peaks at every point in image.  \nlabelsndarray of ints, optional \n\nIf provided, each unique region labels == value represents a unique region to search for peaks. Zero is reserved for background.  \nnum_peaks_per_labelint, optional \n\nMaximum number of peaks for each label.  \np_normfloat \n\nWhich Minkowski p-norm to use. Should be in the range [1, inf]. A finite large p may cause a ValueError if overflow can occur. inf corresponds to the Chebyshev distance and 2 to the Euclidean distance.    Returns \n \noutputndarray or ndarray of bools \n\n If indices = True : (row, column, \u2026) coordinates of peaks. If indices = False : Boolean array shaped like image, with peaks represented by True values.       See also  \nskimage.feature.corner_peaks\n\n  Notes The peak local maximum function returns the coordinates of local peaks (maxima) in an image. Internally, a maximum filter is used for finding local maxima. This operation dilates the original image. After comparison of the dilated and original image, this function returns the coordinates or a mask of the peaks where the dilated image equals the original image. Examples >>> img1 = np.zeros((7, 7))\n>>> img1[3, 4] = 1\n>>> img1[3, 2] = 1.5\n>>> img1\narray([[0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n       [0. , 0. , 1.5, 0. , 1. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 0. , 0. ]])\n >>> peak_local_max(img1, min_distance=1)\narray([[3, 2],\n       [3, 4]])\n >>> peak_local_max(img1, min_distance=2)\narray([[3, 2]])\n >>> img2 = np.zeros((20, 20, 20))\n>>> img2[10, 10, 10] = 1\n>>> img2[15, 15, 15] = 1\n>>> peak_idx = peak_local_max(img2, exclude_border=0)\n>>> peak_idx\narray([[10, 10, 10],\n       [15, 15, 15]])\n >>> peak_mask = np.zeros_like(img2, dtype=bool)\n>>> peak_mask[tuple(peak_idx.T)] = True\n>>> np.argwhere(peak_mask)\narray([[10, 10, 10],\n       [15, 15, 15]])\n \n Examples using skimage.feature.peak_local_max\n \n  Finding local maxima  \n\n  Watershed segmentation  \n\n  Segment human cells (in mitosis)   plot_matches  \nskimage.feature.plot_matches(ax, image1, image2, keypoints1, keypoints2, matches, keypoints_color='k', matches_color=None, only_matches=False, alignment='horizontal') [source]\n \nPlot matched features.  Parameters \n \naxmatplotlib.axes.Axes \n\nMatches and image are drawn in this ax.  \nimage1(N, M [, 3]) array \n\nFirst grayscale or color image.  \nimage2(N, M [, 3]) array \n\nSecond grayscale or color image.  \nkeypoints1(K1, 2) array \n\nFirst keypoint coordinates as (row, col).  \nkeypoints2(K2, 2) array \n\nSecond keypoint coordinates as (row, col).  \nmatches(Q, 2) array \n\nIndices of corresponding matches in first and second set of descriptors, where matches[:, 0] denote the indices in the first and matches[:, 1] the indices in the second set of descriptors.  \nkeypoints_colormatplotlib color, optional \n\nColor for keypoint locations.  \nmatches_colormatplotlib color, optional \n\nColor for lines which connect keypoint matches. By default the color is chosen randomly.  \nonly_matchesbool, optional \n\nWhether to only plot matches and not plot the keypoint locations.  \nalignment{\u2018horizontal\u2019, \u2018vertical\u2019}, optional \n\nWhether to show images side by side, 'horizontal', or one above the other, 'vertical'.     \n register_translation  \nskimage.feature.register_translation(src_image, target_image, upsample_factor=1, space='real', return_error=True) [source]\n \nDeprecated function. Use skimage.registration.phase_cross_correlation instead. \n shape_index  \nskimage.feature.shape_index(image, sigma=1, mode='constant', cval=0) [source]\n \nCompute the shape index. The shape index, as defined by Koenderink & van Doorn [1], is a single valued measure of local curvature, assuming the image as a 3D plane with intensities representing heights. It is derived from the eigen values of the Hessian, and its value ranges from -1 to 1 (and is undefined (=NaN) in flat regions), with following ranges representing following shapes:  Ranges of the shape index and corresponding shapes.  \nInterval (s in \u2026) Shape   \n[ -1, -7/8) Spherical cup  \n[-7/8, -5/8) Through  \n[-5/8, -3/8) Rut  \n[-3/8, -1/8) Saddle rut  \n[-1/8, +1/8) Saddle  \n[+1/8, +3/8) Saddle ridge  \n[+3/8, +5/8) Ridge  \n[+5/8, +7/8) Dome  \n[+7/8, +1] Spherical cap    Parameters \n \nimagendarray \n\nInput image.  \nsigmafloat, optional \n\nStandard deviation used for the Gaussian kernel, which is used for smoothing the input data before Hessian eigen value calculation.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.    Returns \n \nsndarray \n\nShape index     References  \n1  \nKoenderink, J. J. & van Doorn, A. J., \u201cSurface shape and curvature scales\u201d, Image and Vision Computing, 1992, 10, 557-564. DOI:10.1016/0262-8856(92)90076-F   Examples >>> from skimage.feature import shape_index\n>>> square = np.zeros((5, 5))\n>>> square[2, 2] = 4\n>>> s = shape_index(square, sigma=0.1)\n>>> s\narray([[ nan,  nan, -0.5,  nan,  nan],\n       [ nan, -0. ,  nan, -0. ,  nan],\n       [-0.5,  nan, -1. ,  nan, -0.5],\n       [ nan, -0. ,  nan, -0. ,  nan],\n       [ nan,  nan, -0.5,  nan,  nan]])\n \n structure_tensor  \nskimage.feature.structure_tensor(image, sigma=1, mode='constant', cval=0, order=None) [source]\n \nCompute structure tensor using sum of squared differences. The (2-dimensional) structure tensor A is defined as: A = [Arr Arc]\n    [Arc Acc]\n which is approximated by the weighted sum of squared differences in a local window around each pixel in the image. This formula can be extended to a larger number of dimensions (see [1]).  Parameters \n \nimagendarray \n\nInput image.  \nsigmafloat, optional \n\nStandard deviation used for the Gaussian kernel, which is used as a weighting function for the local summation of squared differences.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.  \norder{\u2018rc\u2019, \u2018xy\u2019}, optional \n\nNOTE: Only applies in 2D. Higher dimensions must always use \u2018rc\u2019 order. This parameter allows for the use of reverse or forward order of the image axes in gradient computation. \u2018rc\u2019 indicates the use of the first axis initially (Arr, Arc, Acc), whilst \u2018xy\u2019 indicates the usage of the last axis initially (Axx, Axy, Ayy).    Returns \n \nA_elemslist of ndarray \n\nUpper-diagonal elements of the structure tensor for each pixel in the input image.      See also  \nstructure_tensor_eigenvalues\n\n  References  \n1  \nhttps://en.wikipedia.org/wiki/Structure_tensor   Examples >>> from skimage.feature import structure_tensor\n>>> square = np.zeros((5, 5))\n>>> square[2, 2] = 1\n>>> Arr, Arc, Acc = structure_tensor(square, sigma=0.1, order='rc')\n>>> Acc\narray([[0., 0., 0., 0., 0.],\n       [0., 1., 0., 1., 0.],\n       [0., 4., 0., 4., 0.],\n       [0., 1., 0., 1., 0.],\n       [0., 0., 0., 0., 0.]])\n \n structure_tensor_eigenvalues  \nskimage.feature.structure_tensor_eigenvalues(A_elems) [source]\n \nCompute eigenvalues of structure tensor.  Parameters \n \nA_elemslist of ndarray \n\nThe upper-diagonal elements of the structure tensor, as returned by structure_tensor.    Returns \n ndarray\n\nThe eigenvalues of the structure tensor, in decreasing order. The eigenvalues are the leading dimension. That is, the coordinate [i, j, k] corresponds to the ith-largest eigenvalue at position (j, k).      See also  \nstructure_tensor\n\n  Examples >>> from skimage.feature import structure_tensor\n>>> from skimage.feature import structure_tensor_eigenvalues\n>>> square = np.zeros((5, 5))\n>>> square[2, 2] = 1\n>>> A_elems = structure_tensor(square, sigma=0.1, order='rc')\n>>> structure_tensor_eigenvalues(A_elems)[0]\narray([[0., 0., 0., 0., 0.],\n       [0., 2., 4., 2., 0.],\n       [0., 4., 0., 4., 0.],\n       [0., 2., 4., 2., 0.],\n       [0., 0., 0., 0., 0.]])\n \n structure_tensor_eigvals  \nskimage.feature.structure_tensor_eigvals(Axx, Axy, Ayy) [source]\n \nCompute eigenvalues of structure tensor.  Parameters \n \nAxxndarray \n\nElement of the structure tensor for each pixel in the input image.  \nAxyndarray \n\nElement of the structure tensor for each pixel in the input image.  \nAyyndarray \n\nElement of the structure tensor for each pixel in the input image.    Returns \n \nl1ndarray \n\nLarger eigen value for each input matrix.  \nl2ndarray \n\nSmaller eigen value for each input matrix.     Examples >>> from skimage.feature import structure_tensor, structure_tensor_eigvals\n>>> square = np.zeros((5, 5))\n>>> square[2, 2] = 1\n>>> Arr, Arc, Acc = structure_tensor(square, sigma=0.1, order='rc')\n>>> structure_tensor_eigvals(Acc, Arc, Arr)[0]\narray([[0., 0., 0., 0., 0.],\n       [0., 2., 4., 2., 0.],\n       [0., 4., 0., 4., 0.],\n       [0., 2., 4., 2., 0.],\n       [0., 0., 0., 0., 0.]])\n \n BRIEF  \nclass skimage.feature.BRIEF(descriptor_size=256, patch_size=49, mode='normal', sigma=1, sample_seed=1) [source]\n \nBases: skimage.feature.util.DescriptorExtractor BRIEF binary descriptor extractor. BRIEF (Binary Robust Independent Elementary Features) is an efficient feature point descriptor. It is highly discriminative even when using relatively few bits and is computed using simple intensity difference tests. For each keypoint, intensity comparisons are carried out for a specifically distributed number N of pixel-pairs resulting in a binary descriptor of length N. For binary descriptors the Hamming distance can be used for feature matching, which leads to lower computational cost in comparison to the L2 norm.  Parameters \n \ndescriptor_sizeint, optional \n\nSize of BRIEF descriptor for each keypoint. Sizes 128, 256 and 512 recommended by the authors. Default is 256.  \npatch_sizeint, optional \n\nLength of the two dimensional square patch sampling region around the keypoints. Default is 49.  \nmode{\u2018normal\u2019, \u2018uniform\u2019}, optional \n\nProbability distribution for sampling location of decision pixel-pairs around keypoints.  \nsample_seedint, optional \n\nSeed for the random sampling of the decision pixel-pairs. From a square window with length patch_size, pixel pairs are sampled using the mode parameter to build the descriptors using intensity comparison. The value of sample_seed must be the same for the images to be matched while building the descriptors.  \nsigmafloat, optional \n\nStandard deviation of the Gaussian low-pass filter applied to the image to alleviate noise sensitivity, which is strongly recommended to obtain discriminative and good descriptors.     Examples >>> from skimage.feature import (corner_harris, corner_peaks, BRIEF,\n...                              match_descriptors)\n>>> import numpy as np\n>>> square1 = np.zeros((8, 8), dtype=np.int32)\n>>> square1[2:6, 2:6] = 1\n>>> square1\narray([[0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)\n>>> square2 = np.zeros((9, 9), dtype=np.int32)\n>>> square2[2:7, 2:7] = 1\n>>> square2\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)\n>>> keypoints1 = corner_peaks(corner_harris(square1), min_distance=1)\n>>> keypoints2 = corner_peaks(corner_harris(square2), min_distance=1)\n>>> extractor = BRIEF(patch_size=5)\n>>> extractor.extract(square1, keypoints1)\n>>> descriptors1 = extractor.descriptors\n>>> extractor.extract(square2, keypoints2)\n>>> descriptors2 = extractor.descriptors\n>>> matches = match_descriptors(descriptors1, descriptors2)\n>>> matches\narray([[0, 0],\n       [1, 1],\n       [2, 2],\n       [3, 3]])\n>>> keypoints1[matches[:, 0]]\narray([[2, 2],\n       [2, 5],\n       [5, 2],\n       [5, 5]])\n>>> keypoints2[matches[:, 1]]\narray([[2, 2],\n       [2, 6],\n       [6, 2],\n       [6, 6]])\n  Attributes \n \ndescriptors(Q, descriptor_size) array of dtype bool \n\n2D ndarray of binary descriptors of size descriptor_size for Q keypoints after filtering out border keypoints with value at an index (i, j) either being True or False representing the outcome of the intensity comparison for i-th keypoint on j-th decision pixel-pair. It is Q == np.sum(mask).  \nmask(N, ) array of dtype bool \n\nMask indicating whether a keypoint has been filtered out (False) or is described in the descriptors array (True).      \n__init__(descriptor_size=256, patch_size=49, mode='normal', sigma=1, sample_seed=1) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nextract(image, keypoints) [source]\n \nExtract BRIEF binary descriptors for given keypoints in image.  Parameters \n \nimage2D array \n\nInput image.  \nkeypoints(N, 2) array \n\nKeypoint coordinates as (row, col).     \n \n CENSURE  \nclass skimage.feature.CENSURE(min_scale=1, max_scale=7, mode='DoB', non_max_threshold=0.15, line_threshold=10) [source]\n \nBases: skimage.feature.util.FeatureDetector CENSURE keypoint detector.  \nmin_scaleint, optional \n\nMinimum scale to extract keypoints from.  \nmax_scaleint, optional \n\nMaximum scale to extract keypoints from. The keypoints will be extracted from all the scales except the first and the last i.e. from the scales in the range [min_scale + 1, max_scale - 1]. The filter sizes for different scales is such that the two adjacent scales comprise of an octave.  \nmode{\u2018DoB\u2019, \u2018Octagon\u2019, \u2018STAR\u2019}, optional \n\nType of bi-level filter used to get the scales of the input image. Possible values are \u2018DoB\u2019, \u2018Octagon\u2019 and \u2018STAR\u2019. The three modes represent the shape of the bi-level filters i.e. box(square), octagon and star respectively. For instance, a bi-level octagon filter consists of a smaller inner octagon and a larger outer octagon with the filter weights being uniformly negative in both the inner octagon while uniformly positive in the difference region. Use STAR and Octagon for better features and DoB for better performance.  \nnon_max_thresholdfloat, optional \n\nThreshold value used to suppress maximas and minimas with a weak magnitude response obtained after Non-Maximal Suppression.  \nline_thresholdfloat, optional \n\nThreshold for rejecting interest points which have ratio of principal curvatures greater than this value.   References  \n1  \nMotilal Agrawal, Kurt Konolige and Morten Rufus Blas \u201cCENSURE: Center Surround Extremas for Realtime Feature Detection and Matching\u201d, https://link.springer.com/chapter/10.1007/978-3-540-88693-8_8 DOI:10.1007/978-3-540-88693-8_8  \n2  \nAdam Schmidt, Marek Kraft, Michal Fularz and Zuzanna Domagala \u201cComparative Assessment of Point Feature Detectors and Descriptors in the Context of Robot Navigation\u201d http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.baztech-268aaf28-0faf-4872-a4df-7e2e61cb364c/c/Schmidt_comparative.pdf DOI:10.1.1.465.1117   Examples >>> from skimage.data import astronaut\n>>> from skimage.color import rgb2gray\n>>> from skimage.feature import CENSURE\n>>> img = rgb2gray(astronaut()[100:300, 100:300])\n>>> censure = CENSURE()\n>>> censure.detect(img)\n>>> censure.keypoints\narray([[  4, 148],\n       [ 12,  73],\n       [ 21, 176],\n       [ 91,  22],\n       [ 93,  56],\n       [ 94,  22],\n       [ 95,  54],\n       [100,  51],\n       [103,  51],\n       [106,  67],\n       [108,  15],\n       [117,  20],\n       [122,  60],\n       [125,  37],\n       [129,  37],\n       [133,  76],\n       [145,  44],\n       [146,  94],\n       [150, 114],\n       [153,  33],\n       [154, 156],\n       [155, 151],\n       [184,  63]])\n>>> censure.scales\narray([2, 6, 6, 2, 4, 3, 2, 3, 2, 6, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 4, 2,\n       2])\n  Attributes \n \nkeypoints(N, 2) array \n\nKeypoint coordinates as (row, col).  \nscales(N, ) array \n\nCorresponding scales.      \n__init__(min_scale=1, max_scale=7, mode='DoB', non_max_threshold=0.15, line_threshold=10) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \ndetect(image) [source]\n \nDetect CENSURE keypoints along with the corresponding scale.  Parameters \n \nimage2D ndarray \n\nInput image.     \n \n Cascade  \nclass skimage.feature.Cascade  \nBases: object Class for cascade of classifiers that is used for object detection. The main idea behind cascade of classifiers is to create classifiers of medium accuracy and ensemble them into one strong classifier instead of just creating a strong one. The second advantage of cascade classifier is that easy examples can be classified only by evaluating some of the classifiers in the cascade, making the process much faster than the process of evaluating a one strong classifier.  Attributes \n \nepscnp.float32_t \n\nAccuracy parameter. Increasing it, makes the classifier detect less false positives but at the same time the false negative score increases.  \nstages_numberPy_ssize_t \n\nAmount of stages in a cascade. Each cascade consists of stumps i.e. trained features.  \nstumps_numberPy_ssize_t \n\nThe overall amount of stumps in all the stages of cascade.  \nfeatures_numberPy_ssize_t \n\nThe overall amount of different features used by cascade. Two stumps can use the same features but has different trained values.  \nwindow_widthPy_ssize_t \n\nThe width of a detection window that is used. Objects smaller than this window can\u2019t be detected.  \nwindow_heightPy_ssize_t \n\nThe height of a detection window.  \nstagesStage* \n\nA link to the c array that stores stages information using Stage struct.  \nfeaturesMBLBP* \n\nLink to the c array that stores MBLBP features using MBLBP struct.  \nLUTscnp.uint32_t* \n\nThe ling to the array with look-up tables that are used by trained MBLBP features (MBLBPStumps) to evaluate a particular region.      \n__init__()  \nInitialize cascade classifier.  Parameters \n \nxml_filefile\u2019s path or file\u2019s object \n\nA file in a OpenCv format from which all the cascade classifier\u2019s parameters are loaded.  \nepscnp.float32_t \n\nAccuracy parameter. Increasing it, makes the classifier detect less false positives but at the same time the false negative score increases.     \n  \ndetect_multi_scale()  \nSearch for the object on multiple scales of input image. The function takes the input image, the scale factor by which the searching window is multiplied on each step, minimum window size and maximum window size that specify the interval for the search windows that are applied to the input image to detect objects.  Parameters \n \nimg2-D or 3-D ndarray \n\nNdarray that represents the input image.  \nscale_factorcnp.float32_t \n\nThe scale by which searching window is multiplied on each step.  \nstep_ratiocnp.float32_t \n\nThe ratio by which the search step in multiplied on each scale of the image. 1 represents the exaustive search and usually is slow. By setting this parameter to higher values the results will be worse but the computation will be much faster. Usually, values in the interval [1, 1.5] give good results.  \nmin_sizetyple (int, int) \n\nMinimum size of the search window.  \nmax_sizetyple (int, int) \n\nMaximum size of the search window.  \nmin_neighbour_numberint \n\nMinimum amount of intersecting detections in order for detection to be approved by the function.  \nintersection_score_thresholdcnp.float32_t \n\nThe minimum value of value of ratio (intersection area) / (small rectangle ratio) in order to merge two detections into one.    Returns \n \noutputlist of dicts \n\nDict have form {\u2018r\u2019: int, \u2018c\u2019: int, \u2018width\u2019: int, \u2018height\u2019: int}, where \u2018r\u2019 represents row position of top left corner of detected window, \u2018c\u2019 - col position, \u2018width\u2019 - width of detected window, \u2018height\u2019 - height of detected window.     \n  \neps \n  \nfeatures_number \n  \nstages_number \n  \nstumps_number \n  \nwindow_height \n  \nwindow_width \n \n ORB  \nclass skimage.feature.ORB(downscale=1.2, n_scales=8, n_keypoints=500, fast_n=9, fast_threshold=0.08, harris_k=0.04) [source]\n \nBases: skimage.feature.util.FeatureDetector, skimage.feature.util.DescriptorExtractor Oriented FAST and rotated BRIEF feature detector and binary descriptor extractor.  Parameters \n \nn_keypointsint, optional \n\nNumber of keypoints to be returned. The function will return the best n_keypoints according to the Harris corner response if more than n_keypoints are detected. If not, then all the detected keypoints are returned.  \nfast_nint, optional \n\nThe n parameter in skimage.feature.corner_fast. Minimum number of consecutive pixels out of 16 pixels on the circle that should all be either brighter or darker w.r.t test-pixel. A point c on the circle is darker w.r.t test pixel p if Ic < Ip - threshold and brighter if Ic > Ip + threshold. Also stands for the n in FAST-n corner detector.  \nfast_thresholdfloat, optional \n\nThe threshold parameter in feature.corner_fast. Threshold used to decide whether the pixels on the circle are brighter, darker or similar w.r.t. the test pixel. Decrease the threshold when more corners are desired and vice-versa.  \nharris_kfloat, optional \n\nThe k parameter in skimage.feature.corner_harris. Sensitivity factor to separate corners from edges, typically in range [0, 0.2]. Small values of k result in detection of sharp corners.  \ndownscalefloat, optional \n\nDownscale factor for the image pyramid. Default value 1.2 is chosen so that there are more dense scales which enable robust scale invariance for a subsequent feature description.  \nn_scalesint, optional \n\nMaximum number of scales from the bottom of the image pyramid to extract the features from.     References  \n1  \nEthan Rublee, Vincent Rabaud, Kurt Konolige and Gary Bradski \u201cORB: An efficient alternative to SIFT and SURF\u201d http://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf   Examples >>> from skimage.feature import ORB, match_descriptors\n>>> img1 = np.zeros((100, 100))\n>>> img2 = np.zeros_like(img1)\n>>> np.random.seed(1)\n>>> square = np.random.rand(20, 20)\n>>> img1[40:60, 40:60] = square\n>>> img2[53:73, 53:73] = square\n>>> detector_extractor1 = ORB(n_keypoints=5)\n>>> detector_extractor2 = ORB(n_keypoints=5)\n>>> detector_extractor1.detect_and_extract(img1)\n>>> detector_extractor2.detect_and_extract(img2)\n>>> matches = match_descriptors(detector_extractor1.descriptors,\n...                             detector_extractor2.descriptors)\n>>> matches\narray([[0, 0],\n       [1, 1],\n       [2, 2],\n       [3, 3],\n       [4, 4]])\n>>> detector_extractor1.keypoints[matches[:, 0]]\narray([[42., 40.],\n       [47., 58.],\n       [44., 40.],\n       [59., 42.],\n       [45., 44.]])\n>>> detector_extractor2.keypoints[matches[:, 1]]\narray([[55., 53.],\n       [60., 71.],\n       [57., 53.],\n       [72., 55.],\n       [58., 57.]])\n  Attributes \n \nkeypoints(N, 2) array \n\nKeypoint coordinates as (row, col).  \nscales(N, ) array \n\nCorresponding scales.  \norientations(N, ) array \n\nCorresponding orientations in radians.  \nresponses(N, ) array \n\nCorresponding Harris corner responses.  \ndescriptors(Q, descriptor_size) array of dtype bool \n\n2D array of binary descriptors of size descriptor_size for Q keypoints after filtering out border keypoints with value at an index (i, j) either being True or False representing the outcome of the intensity comparison for i-th keypoint on j-th decision pixel-pair. It is Q == np.sum(mask).      \n__init__(downscale=1.2, n_scales=8, n_keypoints=500, fast_n=9, fast_threshold=0.08, harris_k=0.04) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \ndetect(image) [source]\n \nDetect oriented FAST keypoints along with the corresponding scale.  Parameters \n \nimage2D array \n\nInput image.     \n  \ndetect_and_extract(image) [source]\n \nDetect oriented FAST keypoints and extract rBRIEF descriptors. Note that this is faster than first calling detect and then extract.  Parameters \n \nimage2D array \n\nInput image.     \n  \nextract(image, keypoints, scales, orientations) [source]\n \nExtract rBRIEF binary descriptors for given keypoints in image. Note that the keypoints must be extracted using the same downscale and n_scales parameters. Additionally, if you want to extract both keypoints and descriptors you should use the faster detect_and_extract.  Parameters \n \nimage2D array \n\nInput image.  \nkeypoints(N, 2) array \n\nKeypoint coordinates as (row, col).  \nscales(N, ) array \n\nCorresponding scales.  \norientations(N, ) array \n\nCorresponding orientations in radians.     \n \n\n"}, {"name": "feature.blob_dog()", "path": "api/skimage.feature#skimage.feature.blob_dog", "type": "feature", "text": " \nskimage.feature.blob_dog(image, min_sigma=1, max_sigma=50, sigma_ratio=1.6, threshold=2.0, overlap=0.5, *, exclude_border=False) [source]\n \nFinds blobs in the given grayscale image. Blobs are found using the Difference of Gaussian (DoG) method [1]. For each blob found, the method returns its coordinates and the standard deviation of the Gaussian kernel that detected the blob.  Parameters \n \nimage2D or 3D ndarray \n\nInput grayscale image, blobs are assumed to be light on dark background (white on black).  \nmin_sigmascalar or sequence of scalars, optional \n\nThe minimum standard deviation for Gaussian kernel. Keep this low to detect smaller blobs. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.  \nmax_sigmascalar or sequence of scalars, optional \n\nThe maximum standard deviation for Gaussian kernel. Keep this high to detect larger blobs. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.  \nsigma_ratiofloat, optional \n\nThe ratio between the standard deviation of Gaussian Kernels used for computing the Difference of Gaussians  \nthresholdfloat, optional. \n\nThe absolute lower bound for scale space maxima. Local maxima smaller than thresh are ignored. Reduce this to detect blobs with less intensities.  \noverlapfloat, optional \n\nA value between 0 and 1. If the area of two blobs overlaps by a fraction greater than threshold, the smaller blob is eliminated.  \nexclude_bordertuple of ints, int, or False, optional \n\nIf tuple of ints, the length of the tuple must match the input array\u2019s dimensionality. Each element of the tuple will exclude peaks from within exclude_border-pixels of the border of the image along that dimension. If nonzero int, exclude_border excludes peaks from within exclude_border-pixels of the border of the image. If zero or False, peaks are identified regardless of their distance from the border.    Returns \n \nA(n, image.ndim + sigma) ndarray \n\nA 2d array with each row representing 2 coordinate values for a 2D image, and 3 coordinate values for a 3D image, plus the sigma(s) used. When a single sigma is passed, outputs are: (r, c, sigma) or (p, r, c, sigma) where (r, c) or (p, r, c) are coordinates of the blob and sigma is the standard deviation of the Gaussian kernel which detected the blob. When an anisotropic gaussian is used (sigmas per dimension), the detected sigma is returned for each dimension.      See also  \nskimage.filters.difference_of_gaussians\n\n  Notes The radius of each blob is approximately \\(\\sqrt{2}\\sigma\\) for a 2-D image and \\(\\sqrt{3}\\sigma\\) for a 3-D image. References  \n1  \nhttps://en.wikipedia.org/wiki/Blob_detection#The_difference_of_Gaussians_approach   Examples >>> from skimage import data, feature\n>>> feature.blob_dog(data.coins(), threshold=.5, max_sigma=40)\narray([[120.      , 272.      ,  16.777216],\n       [193.      , 213.      ,  16.777216],\n       [263.      , 245.      ,  16.777216],\n       [185.      , 347.      ,  16.777216],\n       [128.      , 154.      ,  10.48576 ],\n       [198.      , 155.      ,  10.48576 ],\n       [124.      , 337.      ,  10.48576 ],\n       [ 45.      , 336.      ,  16.777216],\n       [195.      , 102.      ,  16.777216],\n       [125.      ,  45.      ,  16.777216],\n       [261.      , 173.      ,  16.777216],\n       [194.      , 277.      ,  16.777216],\n       [127.      , 102.      ,  10.48576 ],\n       [125.      , 208.      ,  10.48576 ],\n       [267.      , 115.      ,  10.48576 ],\n       [263.      , 302.      ,  16.777216],\n       [196.      ,  43.      ,  10.48576 ],\n       [260.      ,  46.      ,  16.777216],\n       [267.      , 359.      ,  16.777216],\n       [ 54.      , 276.      ,  10.48576 ],\n       [ 58.      , 100.      ,  10.48576 ],\n       [ 52.      , 155.      ,  16.777216],\n       [ 52.      , 216.      ,  16.777216],\n       [ 54.      ,  42.      ,  16.777216]])\n \n"}, {"name": "feature.blob_doh()", "path": "api/skimage.feature#skimage.feature.blob_doh", "type": "feature", "text": " \nskimage.feature.blob_doh(image, min_sigma=1, max_sigma=30, num_sigma=10, threshold=0.01, overlap=0.5, log_scale=False) [source]\n \nFinds blobs in the given grayscale image. Blobs are found using the Determinant of Hessian method [1]. For each blob found, the method returns its coordinates and the standard deviation of the Gaussian Kernel used for the Hessian matrix whose determinant detected the blob. Determinant of Hessians is approximated using [2].  Parameters \n \nimage2D ndarray \n\nInput grayscale image.Blobs can either be light on dark or vice versa.  \nmin_sigmafloat, optional \n\nThe minimum standard deviation for Gaussian Kernel used to compute Hessian matrix. Keep this low to detect smaller blobs.  \nmax_sigmafloat, optional \n\nThe maximum standard deviation for Gaussian Kernel used to compute Hessian matrix. Keep this high to detect larger blobs.  \nnum_sigmaint, optional \n\nThe number of intermediate values of standard deviations to consider between min_sigma and max_sigma.  \nthresholdfloat, optional. \n\nThe absolute lower bound for scale space maxima. Local maxima smaller than thresh are ignored. Reduce this to detect less prominent blobs.  \noverlapfloat, optional \n\nA value between 0 and 1. If the area of two blobs overlaps by a fraction greater than threshold, the smaller blob is eliminated.  \nlog_scalebool, optional \n\nIf set intermediate values of standard deviations are interpolated using a logarithmic scale to the base 10. If not, linear interpolation is used.    Returns \n \nA(n, 3) ndarray \n\nA 2d array with each row representing 3 values, (y,x,sigma) where (y,x) are coordinates of the blob and sigma is the standard deviation of the Gaussian kernel of the Hessian Matrix whose determinant detected the blob.     Notes The radius of each blob is approximately sigma. Computation of Determinant of Hessians is independent of the standard deviation. Therefore detecting larger blobs won\u2019t take more time. In methods line blob_dog() and blob_log() the computation of Gaussians for larger sigma takes more time. The downside is that this method can\u2019t be used for detecting blobs of radius less than 3px due to the box filters used in the approximation of Hessian Determinant. References  \n1  \nhttps://en.wikipedia.org/wiki/Blob_detection#The_determinant_of_the_Hessian  \n2  \nHerbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool, \u201cSURF: Speeded Up Robust Features\u201d ftp://ftp.vision.ee.ethz.ch/publications/articles/eth_biwi_00517.pdf   Examples >>> from skimage import data, feature\n>>> img = data.coins()\n>>> feature.blob_doh(img)\narray([[197.        , 153.        ,  20.33333333],\n       [124.        , 336.        ,  20.33333333],\n       [126.        , 153.        ,  20.33333333],\n       [195.        , 100.        ,  23.55555556],\n       [192.        , 212.        ,  23.55555556],\n       [121.        , 271.        ,  30.        ],\n       [126.        , 101.        ,  20.33333333],\n       [193.        , 275.        ,  23.55555556],\n       [123.        , 205.        ,  20.33333333],\n       [270.        , 363.        ,  30.        ],\n       [265.        , 113.        ,  23.55555556],\n       [262.        , 243.        ,  23.55555556],\n       [185.        , 348.        ,  30.        ],\n       [156.        , 302.        ,  30.        ],\n       [123.        ,  44.        ,  23.55555556],\n       [260.        , 173.        ,  30.        ],\n       [197.        ,  44.        ,  20.33333333]])\n \n"}, {"name": "feature.blob_log()", "path": "api/skimage.feature#skimage.feature.blob_log", "type": "feature", "text": " \nskimage.feature.blob_log(image, min_sigma=1, max_sigma=50, num_sigma=10, threshold=0.2, overlap=0.5, log_scale=False, *, exclude_border=False) [source]\n \nFinds blobs in the given grayscale image. Blobs are found using the Laplacian of Gaussian (LoG) method [1]. For each blob found, the method returns its coordinates and the standard deviation of the Gaussian kernel that detected the blob.  Parameters \n \nimage2D or 3D ndarray \n\nInput grayscale image, blobs are assumed to be light on dark background (white on black).  \nmin_sigmascalar or sequence of scalars, optional \n\nthe minimum standard deviation for Gaussian kernel. Keep this low to detect smaller blobs. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.  \nmax_sigmascalar or sequence of scalars, optional \n\nThe maximum standard deviation for Gaussian kernel. Keep this high to detect larger blobs. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.  \nnum_sigmaint, optional \n\nThe number of intermediate values of standard deviations to consider between min_sigma and max_sigma.  \nthresholdfloat, optional. \n\nThe absolute lower bound for scale space maxima. Local maxima smaller than thresh are ignored. Reduce this to detect blobs with less intensities.  \noverlapfloat, optional \n\nA value between 0 and 1. If the area of two blobs overlaps by a fraction greater than threshold, the smaller blob is eliminated.  \nlog_scalebool, optional \n\nIf set intermediate values of standard deviations are interpolated using a logarithmic scale to the base 10. If not, linear interpolation is used.  \nexclude_bordertuple of ints, int, or False, optional \n\nIf tuple of ints, the length of the tuple must match the input array\u2019s dimensionality. Each element of the tuple will exclude peaks from within exclude_border-pixels of the border of the image along that dimension. If nonzero int, exclude_border excludes peaks from within exclude_border-pixels of the border of the image. If zero or False, peaks are identified regardless of their distance from the border.    Returns \n \nA(n, image.ndim + sigma) ndarray \n\nA 2d array with each row representing 2 coordinate values for a 2D image, and 3 coordinate values for a 3D image, plus the sigma(s) used. When a single sigma is passed, outputs are: (r, c, sigma) or (p, r, c, sigma) where (r, c) or (p, r, c) are coordinates of the blob and sigma is the standard deviation of the Gaussian kernel which detected the blob. When an anisotropic gaussian is used (sigmas per dimension), the detected sigma is returned for each dimension.     Notes The radius of each blob is approximately \\(\\sqrt{2}\\sigma\\) for a 2-D image and \\(\\sqrt{3}\\sigma\\) for a 3-D image. References  \n1  \nhttps://en.wikipedia.org/wiki/Blob_detection#The_Laplacian_of_Gaussian   Examples >>> from skimage import data, feature, exposure\n>>> img = data.coins()\n>>> img = exposure.equalize_hist(img)  # improves detection\n>>> feature.blob_log(img, threshold = .3)\narray([[124.        , 336.        ,  11.88888889],\n       [198.        , 155.        ,  11.88888889],\n       [194.        , 213.        ,  17.33333333],\n       [121.        , 272.        ,  17.33333333],\n       [263.        , 244.        ,  17.33333333],\n       [194.        , 276.        ,  17.33333333],\n       [266.        , 115.        ,  11.88888889],\n       [128.        , 154.        ,  11.88888889],\n       [260.        , 174.        ,  17.33333333],\n       [198.        , 103.        ,  11.88888889],\n       [126.        , 208.        ,  11.88888889],\n       [127.        , 102.        ,  11.88888889],\n       [263.        , 302.        ,  17.33333333],\n       [197.        ,  44.        ,  11.88888889],\n       [185.        , 344.        ,  17.33333333],\n       [126.        ,  46.        ,  11.88888889],\n       [113.        , 323.        ,   1.        ]])\n \n"}, {"name": "feature.BRIEF", "path": "api/skimage.feature#skimage.feature.BRIEF", "type": "feature", "text": " \nclass skimage.feature.BRIEF(descriptor_size=256, patch_size=49, mode='normal', sigma=1, sample_seed=1) [source]\n \nBases: skimage.feature.util.DescriptorExtractor BRIEF binary descriptor extractor. BRIEF (Binary Robust Independent Elementary Features) is an efficient feature point descriptor. It is highly discriminative even when using relatively few bits and is computed using simple intensity difference tests. For each keypoint, intensity comparisons are carried out for a specifically distributed number N of pixel-pairs resulting in a binary descriptor of length N. For binary descriptors the Hamming distance can be used for feature matching, which leads to lower computational cost in comparison to the L2 norm.  Parameters \n \ndescriptor_sizeint, optional \n\nSize of BRIEF descriptor for each keypoint. Sizes 128, 256 and 512 recommended by the authors. Default is 256.  \npatch_sizeint, optional \n\nLength of the two dimensional square patch sampling region around the keypoints. Default is 49.  \nmode{\u2018normal\u2019, \u2018uniform\u2019}, optional \n\nProbability distribution for sampling location of decision pixel-pairs around keypoints.  \nsample_seedint, optional \n\nSeed for the random sampling of the decision pixel-pairs. From a square window with length patch_size, pixel pairs are sampled using the mode parameter to build the descriptors using intensity comparison. The value of sample_seed must be the same for the images to be matched while building the descriptors.  \nsigmafloat, optional \n\nStandard deviation of the Gaussian low-pass filter applied to the image to alleviate noise sensitivity, which is strongly recommended to obtain discriminative and good descriptors.     Examples >>> from skimage.feature import (corner_harris, corner_peaks, BRIEF,\n...                              match_descriptors)\n>>> import numpy as np\n>>> square1 = np.zeros((8, 8), dtype=np.int32)\n>>> square1[2:6, 2:6] = 1\n>>> square1\narray([[0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)\n>>> square2 = np.zeros((9, 9), dtype=np.int32)\n>>> square2[2:7, 2:7] = 1\n>>> square2\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)\n>>> keypoints1 = corner_peaks(corner_harris(square1), min_distance=1)\n>>> keypoints2 = corner_peaks(corner_harris(square2), min_distance=1)\n>>> extractor = BRIEF(patch_size=5)\n>>> extractor.extract(square1, keypoints1)\n>>> descriptors1 = extractor.descriptors\n>>> extractor.extract(square2, keypoints2)\n>>> descriptors2 = extractor.descriptors\n>>> matches = match_descriptors(descriptors1, descriptors2)\n>>> matches\narray([[0, 0],\n       [1, 1],\n       [2, 2],\n       [3, 3]])\n>>> keypoints1[matches[:, 0]]\narray([[2, 2],\n       [2, 5],\n       [5, 2],\n       [5, 5]])\n>>> keypoints2[matches[:, 1]]\narray([[2, 2],\n       [2, 6],\n       [6, 2],\n       [6, 6]])\n  Attributes \n \ndescriptors(Q, descriptor_size) array of dtype bool \n\n2D ndarray of binary descriptors of size descriptor_size for Q keypoints after filtering out border keypoints with value at an index (i, j) either being True or False representing the outcome of the intensity comparison for i-th keypoint on j-th decision pixel-pair. It is Q == np.sum(mask).  \nmask(N, ) array of dtype bool \n\nMask indicating whether a keypoint has been filtered out (False) or is described in the descriptors array (True).      \n__init__(descriptor_size=256, patch_size=49, mode='normal', sigma=1, sample_seed=1) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nextract(image, keypoints) [source]\n \nExtract BRIEF binary descriptors for given keypoints in image.  Parameters \n \nimage2D array \n\nInput image.  \nkeypoints(N, 2) array \n\nKeypoint coordinates as (row, col).     \n \n"}, {"name": "feature.BRIEF.extract()", "path": "api/skimage.feature#skimage.feature.BRIEF.extract", "type": "feature", "text": " \nextract(image, keypoints) [source]\n \nExtract BRIEF binary descriptors for given keypoints in image.  Parameters \n \nimage2D array \n\nInput image.  \nkeypoints(N, 2) array \n\nKeypoint coordinates as (row, col).     \n"}, {"name": "feature.BRIEF.__init__()", "path": "api/skimage.feature#skimage.feature.BRIEF.__init__", "type": "feature", "text": " \n__init__(descriptor_size=256, patch_size=49, mode='normal', sigma=1, sample_seed=1) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "feature.canny()", "path": "api/skimage.feature#skimage.feature.canny", "type": "feature", "text": " \nskimage.feature.canny(image, sigma=1.0, low_threshold=None, high_threshold=None, mask=None, use_quantiles=False) [source]\n \nEdge filter an image using the Canny algorithm.  Parameters \n \nimage2D array \n\nGrayscale input image to detect edges on; can be of any dtype.  \nsigmafloat, optional \n\nStandard deviation of the Gaussian filter.  \nlow_thresholdfloat, optional \n\nLower bound for hysteresis thresholding (linking edges). If None, low_threshold is set to 10% of dtype\u2019s max.  \nhigh_thresholdfloat, optional \n\nUpper bound for hysteresis thresholding (linking edges). If None, high_threshold is set to 20% of dtype\u2019s max.  \nmaskarray, dtype=bool, optional \n\nMask to limit the application of Canny to a certain area.  \nuse_quantilesbool, optional \n\nIf True then treat low_threshold and high_threshold as quantiles of the edge magnitude image, rather than absolute edge magnitude values. If True then the thresholds must be in the range [0, 1].    Returns \n \noutput2D array (image) \n\nThe binary edge map.      See also  \nskimage.sobel \n  Notes The steps of the algorithm are as follows:  Smooth the image using a Gaussian with sigma width. Apply the horizontal and vertical Sobel operators to get the gradients within the image. The edge strength is the norm of the gradient. Thin potential edges to 1-pixel wide curves. First, find the normal to the edge at each point. This is done by looking at the signs and the relative magnitude of the X-Sobel and Y-Sobel to sort the points into 4 categories: horizontal, vertical, diagonal and antidiagonal. Then look in the normal and reverse directions to see if the values in either of those directions are greater than the point in question. Use interpolation to get a mix of points instead of picking the one that\u2019s the closest to the normal. Perform a hysteresis thresholding: first label all points above the high threshold as edges. Then recursively label any point above the low threshold that is 8-connected to a labeled point as an edge.  References  \n1  \nCanny, J., A Computational Approach To Edge Detection, IEEE Trans. Pattern Analysis and Machine Intelligence, 8:679-714, 1986 DOI:10.1109/TPAMI.1986.4767851  \n2  \nWilliam Green\u2019s Canny tutorial https://en.wikipedia.org/wiki/Canny_edge_detector   Examples >>> from skimage import feature\n>>> # Generate noisy image of a square\n>>> im = np.zeros((256, 256))\n>>> im[64:-64, 64:-64] = 1\n>>> im += 0.2 * np.random.rand(*im.shape)\n>>> # First trial with the Canny filter, with the default smoothing\n>>> edges1 = feature.canny(im)\n>>> # Increase the smoothing for better results\n>>> edges2 = feature.canny(im, sigma=3)\n \n"}, {"name": "feature.Cascade", "path": "api/skimage.feature#skimage.feature.Cascade", "type": "feature", "text": " \nclass skimage.feature.Cascade  \nBases: object Class for cascade of classifiers that is used for object detection. The main idea behind cascade of classifiers is to create classifiers of medium accuracy and ensemble them into one strong classifier instead of just creating a strong one. The second advantage of cascade classifier is that easy examples can be classified only by evaluating some of the classifiers in the cascade, making the process much faster than the process of evaluating a one strong classifier.  Attributes \n \nepscnp.float32_t \n\nAccuracy parameter. Increasing it, makes the classifier detect less false positives but at the same time the false negative score increases.  \nstages_numberPy_ssize_t \n\nAmount of stages in a cascade. Each cascade consists of stumps i.e. trained features.  \nstumps_numberPy_ssize_t \n\nThe overall amount of stumps in all the stages of cascade.  \nfeatures_numberPy_ssize_t \n\nThe overall amount of different features used by cascade. Two stumps can use the same features but has different trained values.  \nwindow_widthPy_ssize_t \n\nThe width of a detection window that is used. Objects smaller than this window can\u2019t be detected.  \nwindow_heightPy_ssize_t \n\nThe height of a detection window.  \nstagesStage* \n\nA link to the c array that stores stages information using Stage struct.  \nfeaturesMBLBP* \n\nLink to the c array that stores MBLBP features using MBLBP struct.  \nLUTscnp.uint32_t* \n\nThe ling to the array with look-up tables that are used by trained MBLBP features (MBLBPStumps) to evaluate a particular region.      \n__init__()  \nInitialize cascade classifier.  Parameters \n \nxml_filefile\u2019s path or file\u2019s object \n\nA file in a OpenCv format from which all the cascade classifier\u2019s parameters are loaded.  \nepscnp.float32_t \n\nAccuracy parameter. Increasing it, makes the classifier detect less false positives but at the same time the false negative score increases.     \n  \ndetect_multi_scale()  \nSearch for the object on multiple scales of input image. The function takes the input image, the scale factor by which the searching window is multiplied on each step, minimum window size and maximum window size that specify the interval for the search windows that are applied to the input image to detect objects.  Parameters \n \nimg2-D or 3-D ndarray \n\nNdarray that represents the input image.  \nscale_factorcnp.float32_t \n\nThe scale by which searching window is multiplied on each step.  \nstep_ratiocnp.float32_t \n\nThe ratio by which the search step in multiplied on each scale of the image. 1 represents the exaustive search and usually is slow. By setting this parameter to higher values the results will be worse but the computation will be much faster. Usually, values in the interval [1, 1.5] give good results.  \nmin_sizetyple (int, int) \n\nMinimum size of the search window.  \nmax_sizetyple (int, int) \n\nMaximum size of the search window.  \nmin_neighbour_numberint \n\nMinimum amount of intersecting detections in order for detection to be approved by the function.  \nintersection_score_thresholdcnp.float32_t \n\nThe minimum value of value of ratio (intersection area) / (small rectangle ratio) in order to merge two detections into one.    Returns \n \noutputlist of dicts \n\nDict have form {\u2018r\u2019: int, \u2018c\u2019: int, \u2018width\u2019: int, \u2018height\u2019: int}, where \u2018r\u2019 represents row position of top left corner of detected window, \u2018c\u2019 - col position, \u2018width\u2019 - width of detected window, \u2018height\u2019 - height of detected window.     \n  \neps \n  \nfeatures_number \n  \nstages_number \n  \nstumps_number \n  \nwindow_height \n  \nwindow_width \n \n"}, {"name": "feature.Cascade.detect_multi_scale()", "path": "api/skimage.feature#skimage.feature.Cascade.detect_multi_scale", "type": "feature", "text": " \ndetect_multi_scale()  \nSearch for the object on multiple scales of input image. The function takes the input image, the scale factor by which the searching window is multiplied on each step, minimum window size and maximum window size that specify the interval for the search windows that are applied to the input image to detect objects.  Parameters \n \nimg2-D or 3-D ndarray \n\nNdarray that represents the input image.  \nscale_factorcnp.float32_t \n\nThe scale by which searching window is multiplied on each step.  \nstep_ratiocnp.float32_t \n\nThe ratio by which the search step in multiplied on each scale of the image. 1 represents the exaustive search and usually is slow. By setting this parameter to higher values the results will be worse but the computation will be much faster. Usually, values in the interval [1, 1.5] give good results.  \nmin_sizetyple (int, int) \n\nMinimum size of the search window.  \nmax_sizetyple (int, int) \n\nMaximum size of the search window.  \nmin_neighbour_numberint \n\nMinimum amount of intersecting detections in order for detection to be approved by the function.  \nintersection_score_thresholdcnp.float32_t \n\nThe minimum value of value of ratio (intersection area) / (small rectangle ratio) in order to merge two detections into one.    Returns \n \noutputlist of dicts \n\nDict have form {\u2018r\u2019: int, \u2018c\u2019: int, \u2018width\u2019: int, \u2018height\u2019: int}, where \u2018r\u2019 represents row position of top left corner of detected window, \u2018c\u2019 - col position, \u2018width\u2019 - width of detected window, \u2018height\u2019 - height of detected window.     \n"}, {"name": "feature.Cascade.eps", "path": "api/skimage.feature#skimage.feature.Cascade.eps", "type": "feature", "text": " \neps \n"}, {"name": "feature.Cascade.features_number", "path": "api/skimage.feature#skimage.feature.Cascade.features_number", "type": "feature", "text": " \nfeatures_number \n"}, {"name": "feature.Cascade.stages_number", "path": "api/skimage.feature#skimage.feature.Cascade.stages_number", "type": "feature", "text": " \nstages_number \n"}, {"name": "feature.Cascade.stumps_number", "path": "api/skimage.feature#skimage.feature.Cascade.stumps_number", "type": "feature", "text": " \nstumps_number \n"}, {"name": "feature.Cascade.window_height", "path": "api/skimage.feature#skimage.feature.Cascade.window_height", "type": "feature", "text": " \nwindow_height \n"}, {"name": "feature.Cascade.window_width", "path": "api/skimage.feature#skimage.feature.Cascade.window_width", "type": "feature", "text": " \nwindow_width \n"}, {"name": "feature.Cascade.__init__()", "path": "api/skimage.feature#skimage.feature.Cascade.__init__", "type": "feature", "text": " \n__init__()  \nInitialize cascade classifier.  Parameters \n \nxml_filefile\u2019s path or file\u2019s object \n\nA file in a OpenCv format from which all the cascade classifier\u2019s parameters are loaded.  \nepscnp.float32_t \n\nAccuracy parameter. Increasing it, makes the classifier detect less false positives but at the same time the false negative score increases.     \n"}, {"name": "feature.CENSURE", "path": "api/skimage.feature#skimage.feature.CENSURE", "type": "feature", "text": " \nclass skimage.feature.CENSURE(min_scale=1, max_scale=7, mode='DoB', non_max_threshold=0.15, line_threshold=10) [source]\n \nBases: skimage.feature.util.FeatureDetector CENSURE keypoint detector.  \nmin_scaleint, optional \n\nMinimum scale to extract keypoints from.  \nmax_scaleint, optional \n\nMaximum scale to extract keypoints from. The keypoints will be extracted from all the scales except the first and the last i.e. from the scales in the range [min_scale + 1, max_scale - 1]. The filter sizes for different scales is such that the two adjacent scales comprise of an octave.  \nmode{\u2018DoB\u2019, \u2018Octagon\u2019, \u2018STAR\u2019}, optional \n\nType of bi-level filter used to get the scales of the input image. Possible values are \u2018DoB\u2019, \u2018Octagon\u2019 and \u2018STAR\u2019. The three modes represent the shape of the bi-level filters i.e. box(square), octagon and star respectively. For instance, a bi-level octagon filter consists of a smaller inner octagon and a larger outer octagon with the filter weights being uniformly negative in both the inner octagon while uniformly positive in the difference region. Use STAR and Octagon for better features and DoB for better performance.  \nnon_max_thresholdfloat, optional \n\nThreshold value used to suppress maximas and minimas with a weak magnitude response obtained after Non-Maximal Suppression.  \nline_thresholdfloat, optional \n\nThreshold for rejecting interest points which have ratio of principal curvatures greater than this value.   References  \n1  \nMotilal Agrawal, Kurt Konolige and Morten Rufus Blas \u201cCENSURE: Center Surround Extremas for Realtime Feature Detection and Matching\u201d, https://link.springer.com/chapter/10.1007/978-3-540-88693-8_8 DOI:10.1007/978-3-540-88693-8_8  \n2  \nAdam Schmidt, Marek Kraft, Michal Fularz and Zuzanna Domagala \u201cComparative Assessment of Point Feature Detectors and Descriptors in the Context of Robot Navigation\u201d http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.baztech-268aaf28-0faf-4872-a4df-7e2e61cb364c/c/Schmidt_comparative.pdf DOI:10.1.1.465.1117   Examples >>> from skimage.data import astronaut\n>>> from skimage.color import rgb2gray\n>>> from skimage.feature import CENSURE\n>>> img = rgb2gray(astronaut()[100:300, 100:300])\n>>> censure = CENSURE()\n>>> censure.detect(img)\n>>> censure.keypoints\narray([[  4, 148],\n       [ 12,  73],\n       [ 21, 176],\n       [ 91,  22],\n       [ 93,  56],\n       [ 94,  22],\n       [ 95,  54],\n       [100,  51],\n       [103,  51],\n       [106,  67],\n       [108,  15],\n       [117,  20],\n       [122,  60],\n       [125,  37],\n       [129,  37],\n       [133,  76],\n       [145,  44],\n       [146,  94],\n       [150, 114],\n       [153,  33],\n       [154, 156],\n       [155, 151],\n       [184,  63]])\n>>> censure.scales\narray([2, 6, 6, 2, 4, 3, 2, 3, 2, 6, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 4, 2,\n       2])\n  Attributes \n \nkeypoints(N, 2) array \n\nKeypoint coordinates as (row, col).  \nscales(N, ) array \n\nCorresponding scales.      \n__init__(min_scale=1, max_scale=7, mode='DoB', non_max_threshold=0.15, line_threshold=10) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \ndetect(image) [source]\n \nDetect CENSURE keypoints along with the corresponding scale.  Parameters \n \nimage2D ndarray \n\nInput image.     \n \n"}, {"name": "feature.CENSURE.detect()", "path": "api/skimage.feature#skimage.feature.CENSURE.detect", "type": "feature", "text": " \ndetect(image) [source]\n \nDetect CENSURE keypoints along with the corresponding scale.  Parameters \n \nimage2D ndarray \n\nInput image.     \n"}, {"name": "feature.CENSURE.__init__()", "path": "api/skimage.feature#skimage.feature.CENSURE.__init__", "type": "feature", "text": " \n__init__(min_scale=1, max_scale=7, mode='DoB', non_max_threshold=0.15, line_threshold=10) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "feature.corner_fast()", "path": "api/skimage.feature#skimage.feature.corner_fast", "type": "feature", "text": " \nskimage.feature.corner_fast(image, n=12, threshold=0.15) [source]\n \nExtract FAST corners for a given image.  Parameters \n \nimage2D ndarray \n\nInput image.  \nnint, optional \n\nMinimum number of consecutive pixels out of 16 pixels on the circle that should all be either brighter or darker w.r.t testpixel. A point c on the circle is darker w.r.t test pixel p if Ic < Ip - threshold and brighter if Ic > Ip + threshold. Also stands for the n in FAST-n corner detector.  \nthresholdfloat, optional \n\nThreshold used in deciding whether the pixels on the circle are brighter, darker or similar w.r.t. the test pixel. Decrease the threshold when more corners are desired and vice-versa.    Returns \n \nresponsendarray \n\nFAST corner response image.     References  \n1  \nRosten, E., & Drummond, T. (2006, May). Machine learning for high-speed corner detection. In European conference on computer vision (pp. 430-443). Springer, Berlin, Heidelberg. DOI:10.1007/11744023_34 http://www.edwardrosten.com/work/rosten_2006_machine.pdf  \n2  \nWikipedia, \u201cFeatures from accelerated segment test\u201d, https://en.wikipedia.org/wiki/Features_from_accelerated_segment_test   Examples >>> from skimage.feature import corner_fast, corner_peaks\n>>> square = np.zeros((12, 12))\n>>> square[3:9, 3:9] = 1\n>>> square.astype(int)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> corner_peaks(corner_fast(square, 9), min_distance=1)\narray([[3, 3],\n       [3, 8],\n       [8, 3],\n       [8, 8]])\n \n"}, {"name": "feature.corner_foerstner()", "path": "api/skimage.feature#skimage.feature.corner_foerstner", "type": "feature", "text": " \nskimage.feature.corner_foerstner(image, sigma=1) [source]\n \nCompute Foerstner corner measure response image. This corner detector uses information from the auto-correlation matrix A: A = [(imx**2)   (imx*imy)] = [Axx Axy]\n    [(imx*imy)   (imy**2)]   [Axy Ayy]\n Where imx and imy are first derivatives, averaged with a gaussian filter. The corner measure is then defined as: w = det(A) / trace(A)           (size of error ellipse)\nq = 4 * det(A) / trace(A)**2    (roundness of error ellipse)\n  Parameters \n \nimagendarray \n\nInput image.  \nsigmafloat, optional \n\nStandard deviation used for the Gaussian kernel, which is used as weighting function for the auto-correlation matrix.    Returns \n \nwndarray \n\nError ellipse sizes.  \nqndarray \n\nRoundness of error ellipse.     References  \n1  \nF\u00f6rstner, W., & G\u00fclch, E. (1987, June). A fast operator for detection and precise location of distinct points, corners and centres of circular features. In Proc. ISPRS intercommission conference on fast processing of photogrammetric data (pp. 281-305). https://cseweb.ucsd.edu/classes/sp02/cse252/foerstner/foerstner.pdf  \n2  \nhttps://en.wikipedia.org/wiki/Corner_detection   Examples >>> from skimage.feature import corner_foerstner, corner_peaks\n>>> square = np.zeros([10, 10])\n>>> square[2:8, 2:8] = 1\n>>> square.astype(int)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> w, q = corner_foerstner(square)\n>>> accuracy_thresh = 0.5\n>>> roundness_thresh = 0.3\n>>> foerstner = (q > roundness_thresh) * (w > accuracy_thresh) * w\n>>> corner_peaks(foerstner, min_distance=1)\narray([[2, 2],\n       [2, 7],\n       [7, 2],\n       [7, 7]])\n \n"}, {"name": "feature.corner_harris()", "path": "api/skimage.feature#skimage.feature.corner_harris", "type": "feature", "text": " \nskimage.feature.corner_harris(image, method='k', k=0.05, eps=1e-06, sigma=1) [source]\n \nCompute Harris corner measure response image. This corner detector uses information from the auto-correlation matrix A: A = [(imx**2)   (imx*imy)] = [Axx Axy]\n    [(imx*imy)   (imy**2)]   [Axy Ayy]\n Where imx and imy are first derivatives, averaged with a gaussian filter. The corner measure is then defined as: det(A) - k * trace(A)**2\n or: 2 * det(A) / (trace(A) + eps)\n  Parameters \n \nimagendarray \n\nInput image.  \nmethod{\u2018k\u2019, \u2018eps\u2019}, optional \n\nMethod to compute the response image from the auto-correlation matrix.  \nkfloat, optional \n\nSensitivity factor to separate corners from edges, typically in range [0, 0.2]. Small values of k result in detection of sharp corners.  \nepsfloat, optional \n\nNormalisation factor (Noble\u2019s corner measure).  \nsigmafloat, optional \n\nStandard deviation used for the Gaussian kernel, which is used as weighting function for the auto-correlation matrix.    Returns \n \nresponsendarray \n\nHarris response image.     References  \n1  \nhttps://en.wikipedia.org/wiki/Corner_detection   Examples >>> from skimage.feature import corner_harris, corner_peaks\n>>> square = np.zeros([10, 10])\n>>> square[2:8, 2:8] = 1\n>>> square.astype(int)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> corner_peaks(corner_harris(square), min_distance=1)\narray([[2, 2],\n       [2, 7],\n       [7, 2],\n       [7, 7]])\n \n"}, {"name": "feature.corner_kitchen_rosenfeld()", "path": "api/skimage.feature#skimage.feature.corner_kitchen_rosenfeld", "type": "feature", "text": " \nskimage.feature.corner_kitchen_rosenfeld(image, mode='constant', cval=0) [source]\n \nCompute Kitchen and Rosenfeld corner measure response image. The corner measure is calculated as follows: (imxx * imy**2 + imyy * imx**2 - 2 * imxy * imx * imy)\n    / (imx**2 + imy**2)\n Where imx and imy are the first and imxx, imxy, imyy the second derivatives.  Parameters \n \nimagendarray \n\nInput image.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.    Returns \n \nresponsendarray \n\nKitchen and Rosenfeld response image.     References  \n1  \nKitchen, L., & Rosenfeld, A. (1982). Gray-level corner detection. Pattern recognition letters, 1(2), 95-102. DOI:10.1016/0167-8655(82)90020-4   \n"}, {"name": "feature.corner_moravec()", "path": "api/skimage.feature#skimage.feature.corner_moravec", "type": "feature", "text": " \nskimage.feature.corner_moravec(image, window_size=1) [source]\n \nCompute Moravec corner measure response image. This is one of the simplest corner detectors and is comparatively fast but has several limitations (e.g. not rotation invariant).  Parameters \n \nimagendarray \n\nInput image.  \nwindow_sizeint, optional \n\nWindow size.    Returns \n \nresponsendarray \n\nMoravec response image.     References  \n1  \nhttps://en.wikipedia.org/wiki/Corner_detection   Examples >>> from skimage.feature import corner_moravec\n>>> square = np.zeros([7, 7])\n>>> square[3, 3] = 1\n>>> square.astype(int)\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0]])\n>>> corner_moravec(square).astype(int)\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 2, 1, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0]])\n \n"}, {"name": "feature.corner_orientations()", "path": "api/skimage.feature#skimage.feature.corner_orientations", "type": "feature", "text": " \nskimage.feature.corner_orientations(image, corners, mask) [source]\n \nCompute the orientation of corners. The orientation of corners is computed using the first order central moment i.e. the center of mass approach. The corner orientation is the angle of the vector from the corner coordinate to the intensity centroid in the local neighborhood around the corner calculated using first order central moment.  Parameters \n \nimage2D array \n\nInput grayscale image.  \ncorners(N, 2) array \n\nCorner coordinates as (row, col).  \nmask2D array \n\nMask defining the local neighborhood of the corner used for the calculation of the central moment.    Returns \n \norientations(N, 1) array \n\nOrientations of corners in the range [-pi, pi].     References  \n1  \nEthan Rublee, Vincent Rabaud, Kurt Konolige and Gary Bradski \u201cORB : An efficient alternative to SIFT and SURF\u201d http://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf  \n2  \nPaul L. Rosin, \u201cMeasuring Corner Properties\u201d http://users.cs.cf.ac.uk/Paul.Rosin/corner2.pdf   Examples >>> from skimage.morphology import octagon\n>>> from skimage.feature import (corner_fast, corner_peaks,\n...                              corner_orientations)\n>>> square = np.zeros((12, 12))\n>>> square[3:9, 3:9] = 1\n>>> square.astype(int)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> corners = corner_peaks(corner_fast(square, 9), min_distance=1)\n>>> corners\narray([[3, 3],\n       [3, 8],\n       [8, 3],\n       [8, 8]])\n>>> orientations = corner_orientations(square, corners, octagon(3, 2))\n>>> np.rad2deg(orientations)\narray([  45.,  135.,  -45., -135.])\n \n"}, {"name": "feature.corner_peaks()", "path": "api/skimage.feature#skimage.feature.corner_peaks", "type": "feature", "text": " \nskimage.feature.corner_peaks(image, min_distance=1, threshold_abs=None, threshold_rel=None, exclude_border=True, indices=True, num_peaks=inf, footprint=None, labels=None, *, num_peaks_per_label=inf, p_norm=inf) [source]\n \nFind peaks in corner measure response image. This differs from skimage.feature.peak_local_max in that it suppresses multiple connected peaks with the same accumulator value.  Parameters \n \nimagendarray \n\nInput image.  \nmin_distanceint, optional \n\nThe minimal allowed distance separating peaks.  \n** \n\nSee skimage.feature.peak_local_max().  \np_normfloat \n\nWhich Minkowski p-norm to use. Should be in the range [1, inf]. A finite large p may cause a ValueError if overflow can occur. inf corresponds to the Chebyshev distance and 2 to the Euclidean distance.    Returns \n \noutputndarray or ndarray of bools \n\n If indices = True : (row, column, \u2026) coordinates of peaks. If indices = False : Boolean array shaped like image, with peaks represented by True values.       See also  \nskimage.feature.peak_local_max\n\n  Notes  Changed in version 0.18: The default value of threshold_rel has changed to None, which corresponds to letting skimage.feature.peak_local_max decide on the default. This is equivalent to threshold_rel=0.  The num_peaks limit is applied before suppression of connected peaks. To limit the number of peaks after suppression, set num_peaks=np.inf and post-process the output of this function. Examples >>> from skimage.feature import peak_local_max\n>>> response = np.zeros((5, 5))\n>>> response[2:4, 2:4] = 1\n>>> response\narray([[0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 1., 1., 0.],\n       [0., 0., 1., 1., 0.],\n       [0., 0., 0., 0., 0.]])\n>>> peak_local_max(response)\narray([[2, 2],\n       [2, 3],\n       [3, 2],\n       [3, 3]])\n>>> corner_peaks(response)\narray([[2, 2]])\n \n"}, {"name": "feature.corner_shi_tomasi()", "path": "api/skimage.feature#skimage.feature.corner_shi_tomasi", "type": "feature", "text": " \nskimage.feature.corner_shi_tomasi(image, sigma=1) [source]\n \nCompute Shi-Tomasi (Kanade-Tomasi) corner measure response image. This corner detector uses information from the auto-correlation matrix A: A = [(imx**2)   (imx*imy)] = [Axx Axy]\n    [(imx*imy)   (imy**2)]   [Axy Ayy]\n Where imx and imy are first derivatives, averaged with a gaussian filter. The corner measure is then defined as the smaller eigenvalue of A: ((Axx + Ayy) - sqrt((Axx - Ayy)**2 + 4 * Axy**2)) / 2\n  Parameters \n \nimagendarray \n\nInput image.  \nsigmafloat, optional \n\nStandard deviation used for the Gaussian kernel, which is used as weighting function for the auto-correlation matrix.    Returns \n \nresponsendarray \n\nShi-Tomasi response image.     References  \n1  \nhttps://en.wikipedia.org/wiki/Corner_detection   Examples >>> from skimage.feature import corner_shi_tomasi, corner_peaks\n>>> square = np.zeros([10, 10])\n>>> square[2:8, 2:8] = 1\n>>> square.astype(int)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> corner_peaks(corner_shi_tomasi(square), min_distance=1)\narray([[2, 2],\n       [2, 7],\n       [7, 2],\n       [7, 7]])\n \n"}, {"name": "feature.corner_subpix()", "path": "api/skimage.feature#skimage.feature.corner_subpix", "type": "feature", "text": " \nskimage.feature.corner_subpix(image, corners, window_size=11, alpha=0.99) [source]\n \nDetermine subpixel position of corners. A statistical test decides whether the corner is defined as the intersection of two edges or a single peak. Depending on the classification result, the subpixel corner location is determined based on the local covariance of the grey-values. If the significance level for either statistical test is not sufficient, the corner cannot be classified, and the output subpixel position is set to NaN.  Parameters \n \nimagendarray \n\nInput image.  \ncorners(N, 2) ndarray \n\nCorner coordinates (row, col).  \nwindow_sizeint, optional \n\nSearch window size for subpixel estimation.  \nalphafloat, optional \n\nSignificance level for corner classification.    Returns \n \npositions(N, 2) ndarray \n\nSubpixel corner positions. NaN for \u201cnot classified\u201d corners.     References  \n1  \nF\u00f6rstner, W., & G\u00fclch, E. (1987, June). A fast operator for detection and precise location of distinct points, corners and centres of circular features. In Proc. ISPRS intercommission conference on fast processing of photogrammetric data (pp. 281-305). https://cseweb.ucsd.edu/classes/sp02/cse252/foerstner/foerstner.pdf  \n2  \nhttps://en.wikipedia.org/wiki/Corner_detection   Examples >>> from skimage.feature import corner_harris, corner_peaks, corner_subpix\n>>> img = np.zeros((10, 10))\n>>> img[:5, :5] = 1\n>>> img[5:, 5:] = 1\n>>> img.astype(int)\narray([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]])\n>>> coords = corner_peaks(corner_harris(img), min_distance=2)\n>>> coords_subpix = corner_subpix(img, coords, window_size=7)\n>>> coords_subpix\narray([[4.5, 4.5]])\n \n"}, {"name": "feature.daisy()", "path": "api/skimage.feature#skimage.feature.daisy", "type": "feature", "text": " \nskimage.feature.daisy(image, step=4, radius=15, rings=3, histograms=8, orientations=8, normalization='l1', sigmas=None, ring_radii=None, visualize=False) [source]\n \nExtract DAISY feature descriptors densely for the given image. DAISY is a feature descriptor similar to SIFT formulated in a way that allows for fast dense extraction. Typically, this is practical for bag-of-features image representations. The implementation follows Tola et al. [1] but deviate on the following points:  Histogram bin contribution are smoothed with a circular Gaussian window over the tonal range (the angular range). The sigma values of the spatial Gaussian smoothing in this code do not match the sigma values in the original code by Tola et al. [2]. In their code, spatial smoothing is applied to both the input image and the center histogram. However, this smoothing is not documented in [1] and, therefore, it is omitted.   Parameters \n \nimage(M, N) array \n\nInput image (grayscale).  \nstepint, optional \n\nDistance between descriptor sampling points.  \nradiusint, optional \n\nRadius (in pixels) of the outermost ring.  \nringsint, optional \n\nNumber of rings.  \nhistogramsint, optional \n\nNumber of histograms sampled per ring.  \norientationsint, optional \n\nNumber of orientations (bins) per histogram.  \nnormalization[ \u2018l1\u2019 | \u2018l2\u2019 | \u2018daisy\u2019 | \u2018off\u2019 ], optional \n\nHow to normalize the descriptors  \u2018l1\u2019: L1-normalization of each descriptor. \u2018l2\u2019: L2-normalization of each descriptor. \u2018daisy\u2019: L2-normalization of individual histograms. \u2018off\u2019: Disable normalization.   \nsigmas1D array of float, optional \n\nStandard deviation of spatial Gaussian smoothing for the center histogram and for each ring of histograms. The array of sigmas should be sorted from the center and out. I.e. the first sigma value defines the spatial smoothing of the center histogram and the last sigma value defines the spatial smoothing of the outermost ring. Specifying sigmas overrides the following parameter. rings = len(sigmas) - 1  \nring_radii1D array of int, optional \n\nRadius (in pixels) for each ring. Specifying ring_radii overrides the following two parameters. rings = len(ring_radii) radius = ring_radii[-1] If both sigmas and ring_radii are given, they must satisfy the following predicate since no radius is needed for the center histogram. len(ring_radii) == len(sigmas) + 1  \nvisualizebool, optional \n\nGenerate a visualization of the DAISY descriptors    Returns \n \ndescsarray \n\nGrid of DAISY descriptors for the given image as an array dimensionality (P, Q, R) where P = ceil((M - radius*2) / step) Q = ceil((N - radius*2) / step) R = (rings * histograms + 1) * orientations  \ndescs_img(M, N, 3) array (only if visualize==True) \n\nVisualization of the DAISY descriptors.     References  \n1(1,2)  \nTola et al. \u201cDaisy: An efficient dense descriptor applied to wide- baseline stereo.\u201d Pattern Analysis and Machine Intelligence, IEEE Transactions on 32.5 (2010): 815-830.  \n2  \nhttp://cvlab.epfl.ch/software/daisy   \n"}, {"name": "feature.draw_haar_like_feature()", "path": "api/skimage.feature#skimage.feature.draw_haar_like_feature", "type": "feature", "text": " \nskimage.feature.draw_haar_like_feature(image, r, c, width, height, feature_coord, color_positive_block=(1.0, 0.0, 0.0), color_negative_block=(0.0, 1.0, 0.0), alpha=0.5, max_n_features=None, random_state=None) [source]\n \nVisualization of Haar-like features.  Parameters \n \nimage(M, N) ndarray \n\nThe region of an integral image for which the features need to be computed.  \nrint \n\nRow-coordinate of top left corner of the detection window.  \ncint \n\nColumn-coordinate of top left corner of the detection window.  \nwidthint \n\nWidth of the detection window.  \nheightint \n\nHeight of the detection window.  \nfeature_coordndarray of list of tuples or None, optional \n\nThe array of coordinates to be extracted. This is useful when you want to recompute only a subset of features. In this case feature_type needs to be an array containing the type of each feature, as returned by haar_like_feature_coord(). By default, all coordinates are computed.  \ncolor_positive_rectangletuple of 3 floats \n\nFloats specifying the color for the positive block. Corresponding values define (R, G, B) values. Default value is red (1, 0, 0).  \ncolor_negative_blocktuple of 3 floats \n\nFloats specifying the color for the negative block Corresponding values define (R, G, B) values. Default value is blue (0, 1, 0).  \nalphafloat \n\nValue in the range [0, 1] that specifies opacity of visualization. 1 - fully transparent, 0 - opaque.  \nmax_n_featuresint, default=None \n\nThe maximum number of features to be returned. By default, all features are returned.  \nrandom_stateint, RandomState instance or None, optional \n\nIf int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. The random state is used when generating a set of features smaller than the total number of available features.    Returns \n \nfeatures(M, N), ndarray \n\nAn image in which the different features will be added.     Examples >>> import numpy as np\n>>> from skimage.feature import haar_like_feature_coord\n>>> from skimage.feature import draw_haar_like_feature\n>>> feature_coord, _ = haar_like_feature_coord(2, 2, 'type-4')\n>>> image = draw_haar_like_feature(np.zeros((2, 2)),\n...                                0, 0, 2, 2,\n...                                feature_coord,\n...                                max_n_features=1)\n>>> image\narray([[[0. , 0.5, 0. ],\n        [0.5, 0. , 0. ]],\n\n       [[0.5, 0. , 0. ],\n        [0. , 0.5, 0. ]]])\n \n"}, {"name": "feature.draw_multiblock_lbp()", "path": "api/skimage.feature#skimage.feature.draw_multiblock_lbp", "type": "feature", "text": " \nskimage.feature.draw_multiblock_lbp(image, r, c, width, height, lbp_code=0, color_greater_block=(1, 1, 1), color_less_block=(0, 0.69, 0.96), alpha=0.5) [source]\n \nMulti-block local binary pattern visualization. Blocks with higher sums are colored with alpha-blended white rectangles, whereas blocks with lower sums are colored alpha-blended cyan. Colors and the alpha parameter can be changed.  Parameters \n \nimagendarray of float or uint \n\nImage on which to visualize the pattern.  \nrint \n\nRow-coordinate of top left corner of a rectangle containing feature.  \ncint \n\nColumn-coordinate of top left corner of a rectangle containing feature.  \nwidthint \n\nWidth of one of 9 equal rectangles that will be used to compute a feature.  \nheightint \n\nHeight of one of 9 equal rectangles that will be used to compute a feature.  \nlbp_codeint \n\nThe descriptor of feature to visualize. If not provided, the descriptor with 0 value will be used.  \ncolor_greater_blocktuple of 3 floats \n\nFloats specifying the color for the block that has greater intensity value. They should be in the range [0, 1]. Corresponding values define (R, G, B) values. Default value is white (1, 1, 1).  \ncolor_greater_blocktuple of 3 floats \n\nFloats specifying the color for the block that has greater intensity value. They should be in the range [0, 1]. Corresponding values define (R, G, B) values. Default value is cyan (0, 0.69, 0.96).  \nalphafloat \n\nValue in the range [0, 1] that specifies opacity of visualization. 1 - fully transparent, 0 - opaque.    Returns \n \noutputndarray of float \n\nImage with MB-LBP visualization.     References  \n1  \nFace Detection Based on Multi-Block LBP Representation. Lun Zhang, Rufeng Chu, Shiming Xiang, Shengcai Liao, Stan Z. Li http://www.cbsr.ia.ac.cn/users/scliao/papers/Zhang-ICB07-MBLBP.pdf   \n"}, {"name": "feature.greycomatrix()", "path": "api/skimage.feature#skimage.feature.greycomatrix", "type": "feature", "text": " \nskimage.feature.greycomatrix(image, distances, angles, levels=None, symmetric=False, normed=False) [source]\n \nCalculate the grey-level co-occurrence matrix. A grey level co-occurrence matrix is a histogram of co-occurring greyscale values at a given offset over an image.  Parameters \n \nimagearray_like \n\nInteger typed input image. Only positive valued images are supported. If type is other than uint8, the argument levels needs to be set.  \ndistancesarray_like \n\nList of pixel pair distance offsets.  \nanglesarray_like \n\nList of pixel pair angles in radians.  \nlevelsint, optional \n\nThe input image should contain integers in [0, levels-1], where levels indicate the number of grey-levels counted (typically 256 for an 8-bit image). This argument is required for 16-bit images or higher and is typically the maximum of the image. As the output matrix is at least levels x levels, it might be preferable to use binning of the input image rather than large values for levels.  \nsymmetricbool, optional \n\nIf True, the output matrix P[:, :, d, theta] is symmetric. This is accomplished by ignoring the order of value pairs, so both (i, j) and (j, i) are accumulated when (i, j) is encountered for a given offset. The default is False.  \nnormedbool, optional \n\nIf True, normalize each matrix P[:, :, d, theta] by dividing by the total number of accumulated co-occurrences for the given offset. The elements of the resulting matrix sum to 1. The default is False.    Returns \n \nP4-D ndarray \n\nThe grey-level co-occurrence histogram. The value P[i,j,d,theta] is the number of times that grey-level j occurs at a distance d and at an angle theta from grey-level i. If normed is False, the output is of type uint32, otherwise it is float64. The dimensions are: levels x levels x number of distances x number of angles.     References  \n1  \nThe GLCM Tutorial Home Page, http://www.fp.ucalgary.ca/mhallbey/tutorial.htm  \n2  \nHaralick, RM.; Shanmugam, K., \u201cTextural features for image classification\u201d IEEE Transactions on systems, man, and cybernetics 6 (1973): 610-621. DOI:10.1109/TSMC.1973.4309314  \n3  \nPattern Recognition Engineering, Morton Nadler & Eric P. Smith  \n4  \nWikipedia, https://en.wikipedia.org/wiki/Co-occurrence_matrix   Examples Compute 2 GLCMs: One for a 1-pixel offset to the right, and one for a 1-pixel offset upwards. >>> image = np.array([[0, 0, 1, 1],\n...                   [0, 0, 1, 1],\n...                   [0, 2, 2, 2],\n...                   [2, 2, 3, 3]], dtype=np.uint8)\n>>> result = greycomatrix(image, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4],\n...                       levels=4)\n>>> result[:, :, 0, 0]\narray([[2, 2, 1, 0],\n       [0, 2, 0, 0],\n       [0, 0, 3, 1],\n       [0, 0, 0, 1]], dtype=uint32)\n>>> result[:, :, 0, 1]\narray([[1, 1, 3, 0],\n       [0, 1, 1, 0],\n       [0, 0, 0, 2],\n       [0, 0, 0, 0]], dtype=uint32)\n>>> result[:, :, 0, 2]\narray([[3, 0, 2, 0],\n       [0, 2, 2, 0],\n       [0, 0, 1, 2],\n       [0, 0, 0, 0]], dtype=uint32)\n>>> result[:, :, 0, 3]\narray([[2, 0, 0, 0],\n       [1, 1, 2, 0],\n       [0, 0, 2, 1],\n       [0, 0, 0, 0]], dtype=uint32)\n \n"}, {"name": "feature.greycoprops()", "path": "api/skimage.feature#skimage.feature.greycoprops", "type": "feature", "text": " \nskimage.feature.greycoprops(P, prop='contrast') [source]\n \nCalculate texture properties of a GLCM. Compute a feature of a grey level co-occurrence matrix to serve as a compact summary of the matrix. The properties are computed as follows:  \u2018contrast\u2019: \\(\\sum_{i,j=0}^{levels-1} P_{i,j}(i-j)^2\\)\n \u2018dissimilarity\u2019: \\(\\sum_{i,j=0}^{levels-1}P_{i,j}|i-j|\\)\n \u2018homogeneity\u2019: \\(\\sum_{i,j=0}^{levels-1}\\frac{P_{i,j}}{1+(i-j)^2}\\)\n \u2018ASM\u2019: \\(\\sum_{i,j=0}^{levels-1} P_{i,j}^2\\)\n \u2018energy\u2019: \\(\\sqrt{ASM}\\)\n \n \u2018correlation\u2019:\n\n \\[\\sum_{i,j=0}^{levels-1} P_{i,j}\\left[\\frac{(i-\\mu_i) \\ (j-\\mu_j)}{\\sqrt{(\\sigma_i^2)(\\sigma_j^2)}}\\right]\\]     Each GLCM is normalized to have a sum of 1 before the computation of texture properties.  Parameters \n \nPndarray \n\nInput array. P is the grey-level co-occurrence histogram for which to compute the specified property. The value P[i,j,d,theta] is the number of times that grey-level j occurs at a distance d and at an angle theta from grey-level i.  \nprop{\u2018contrast\u2019, \u2018dissimilarity\u2019, \u2018homogeneity\u2019, \u2018energy\u2019, \u2018correlation\u2019, \u2018ASM\u2019}, optional \n\nThe property of the GLCM to compute. The default is \u2018contrast\u2019.    Returns \n \nresults2-D ndarray \n\n2-dimensional array. results[d, a] is the property \u2018prop\u2019 for the d\u2019th distance and the a\u2019th angle.     References  \n1  \nThe GLCM Tutorial Home Page, http://www.fp.ucalgary.ca/mhallbey/tutorial.htm   Examples Compute the contrast for GLCMs with distances [1, 2] and angles [0 degrees, 90 degrees] >>> image = np.array([[0, 0, 1, 1],\n...                   [0, 0, 1, 1],\n...                   [0, 2, 2, 2],\n...                   [2, 2, 3, 3]], dtype=np.uint8)\n>>> g = greycomatrix(image, [1, 2], [0, np.pi/2], levels=4,\n...                  normed=True, symmetric=True)\n>>> contrast = greycoprops(g, 'contrast')\n>>> contrast\narray([[0.58333333, 1.        ],\n       [1.25      , 2.75      ]])\n \n"}, {"name": "feature.haar_like_feature()", "path": "api/skimage.feature#skimage.feature.haar_like_feature", "type": "feature", "text": " \nskimage.feature.haar_like_feature(int_image, r, c, width, height, feature_type=None, feature_coord=None) [source]\n \nCompute the Haar-like features for a region of interest (ROI) of an integral image. Haar-like features have been successfully used for image classification and object detection [1]. It has been used for real-time face detection algorithm proposed in [2].  Parameters \n \nint_image(M, N) ndarray \n\nIntegral image for which the features need to be computed.  \nrint \n\nRow-coordinate of top left corner of the detection window.  \ncint \n\nColumn-coordinate of top left corner of the detection window.  \nwidthint \n\nWidth of the detection window.  \nheightint \n\nHeight of the detection window.  \nfeature_typestr or list of str or None, optional \n\nThe type of feature to consider:  \u2018type-2-x\u2019: 2 rectangles varying along the x axis; \u2018type-2-y\u2019: 2 rectangles varying along the y axis; \u2018type-3-x\u2019: 3 rectangles varying along the x axis; \u2018type-3-y\u2019: 3 rectangles varying along the y axis; \u2018type-4\u2019: 4 rectangles varying along x and y axis.  By default all features are extracted. If using with feature_coord, it should correspond to the feature type of each associated coordinate feature.  \nfeature_coordndarray of list of tuples or None, optional \n\nThe array of coordinates to be extracted. This is useful when you want to recompute only a subset of features. In this case feature_type needs to be an array containing the type of each feature, as returned by haar_like_feature_coord(). By default, all coordinates are computed.    Returns \n \nhaar_features(n_features,) ndarray of int or float \n\nResulting Haar-like features. Each value is equal to the subtraction of sums of the positive and negative rectangles. The data type depends of the data type of int_image: int when the data type of int_image is uint or int and float when the data type of int_image is float.     Notes When extracting those features in parallel, be aware that the choice of the backend (i.e. multiprocessing vs threading) will have an impact on the performance. The rule of thumb is as follows: use multiprocessing when extracting features for all possible ROI in an image; use threading when extracting the feature at specific location for a limited number of ROIs. Refer to the example Face classification using Haar-like feature descriptor for more insights. References  \n1  \nhttps://en.wikipedia.org/wiki/Haar-like_feature  \n2  \nOren, M., Papageorgiou, C., Sinha, P., Osuna, E., & Poggio, T. (1997, June). Pedestrian detection using wavelet templates. In Computer Vision and Pattern Recognition, 1997. Proceedings., 1997 IEEE Computer Society Conference on (pp. 193-199). IEEE. http://tinyurl.com/y6ulxfta DOI:10.1109/CVPR.1997.609319  \n3  \nViola, Paul, and Michael J. Jones. \u201cRobust real-time face detection.\u201d International journal of computer vision 57.2 (2004): 137-154. https://www.merl.com/publications/docs/TR2004-043.pdf DOI:10.1109/CVPR.2001.990517   Examples >>> import numpy as np\n>>> from skimage.transform import integral_image\n>>> from skimage.feature import haar_like_feature\n>>> img = np.ones((5, 5), dtype=np.uint8)\n>>> img_ii = integral_image(img)\n>>> feature = haar_like_feature(img_ii, 0, 0, 5, 5, 'type-3-x')\n>>> feature\narray([-1, -2, -3, -4, -1, -2, -3, -4, -1, -2, -3, -4, -1, -2, -3, -4, -1,\n       -2, -3, -4, -1, -2, -3, -4, -1, -2, -3, -1, -2, -3, -1, -2, -3, -1,\n       -2, -1, -2, -1, -2, -1, -1, -1])\n You can compute the feature for some pre-computed coordinates. >>> from skimage.feature import haar_like_feature_coord\n>>> feature_coord, feature_type = zip(\n...     *[haar_like_feature_coord(5, 5, feat_t)\n...       for feat_t in ('type-2-x', 'type-3-x')])\n>>> # only select one feature over two\n>>> feature_coord = np.concatenate([x[::2] for x in feature_coord])\n>>> feature_type = np.concatenate([x[::2] for x in feature_type])\n>>> feature = haar_like_feature(img_ii, 0, 0, 5, 5,\n...                             feature_type=feature_type,\n...                             feature_coord=feature_coord)\n>>> feature\narray([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0, -1, -3, -1, -3, -1, -3, -1, -3, -1,\n       -3, -1, -3, -1, -3, -2, -1, -3, -2, -2, -2, -1])\n \n"}, {"name": "feature.haar_like_feature_coord()", "path": "api/skimage.feature#skimage.feature.haar_like_feature_coord", "type": "feature", "text": " \nskimage.feature.haar_like_feature_coord(width, height, feature_type=None) [source]\n \nCompute the coordinates of Haar-like features.  Parameters \n \nwidthint \n\nWidth of the detection window.  \nheightint \n\nHeight of the detection window.  \nfeature_typestr or list of str or None, optional \n\nThe type of feature to consider:  \u2018type-2-x\u2019: 2 rectangles varying along the x axis; \u2018type-2-y\u2019: 2 rectangles varying along the y axis; \u2018type-3-x\u2019: 3 rectangles varying along the x axis; \u2018type-3-y\u2019: 3 rectangles varying along the y axis; \u2018type-4\u2019: 4 rectangles varying along x and y axis.  By default all features are extracted.    Returns \n \nfeature_coord(n_features, n_rectangles, 2, 2), ndarray of list of tuple coord \n\nCoordinates of the rectangles for each feature.  \nfeature_type(n_features,), ndarray of str \n\nThe corresponding type for each feature.     Examples >>> import numpy as np\n>>> from skimage.transform import integral_image\n>>> from skimage.feature import haar_like_feature_coord\n>>> feat_coord, feat_type = haar_like_feature_coord(2, 2, 'type-4')\n>>> feat_coord \narray([ list([[(0, 0), (0, 0)], [(0, 1), (0, 1)],\n              [(1, 1), (1, 1)], [(1, 0), (1, 0)]])], dtype=object)\n>>> feat_type\narray(['type-4'], dtype=object)\n \n"}, {"name": "feature.hessian_matrix()", "path": "api/skimage.feature#skimage.feature.hessian_matrix", "type": "feature", "text": " \nskimage.feature.hessian_matrix(image, sigma=1, mode='constant', cval=0, order='rc') [source]\n \nCompute Hessian matrix. The Hessian matrix is defined as: H = [Hrr Hrc]\n    [Hrc Hcc]\n which is computed by convolving the image with the second derivatives of the Gaussian kernel in the respective r- and c-directions.  Parameters \n \nimagendarray \n\nInput image.  \nsigmafloat \n\nStandard deviation used for the Gaussian kernel, which is used as weighting function for the auto-correlation matrix.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.  \norder{\u2018rc\u2019, \u2018xy\u2019}, optional \n\nThis parameter allows for the use of reverse or forward order of the image axes in gradient computation. \u2018rc\u2019 indicates the use of the first axis initially (Hrr, Hrc, Hcc), whilst \u2018xy\u2019 indicates the usage of the last axis initially (Hxx, Hxy, Hyy)    Returns \n \nHrrndarray \n\nElement of the Hessian matrix for each pixel in the input image.  \nHrcndarray \n\nElement of the Hessian matrix for each pixel in the input image.  \nHccndarray \n\nElement of the Hessian matrix for each pixel in the input image.     Examples >>> from skimage.feature import hessian_matrix\n>>> square = np.zeros((5, 5))\n>>> square[2, 2] = 4\n>>> Hrr, Hrc, Hcc = hessian_matrix(square, sigma=0.1, order='rc')\n>>> Hrc\narray([[ 0.,  0.,  0.,  0.,  0.],\n       [ 0.,  1.,  0., -1.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.],\n       [ 0., -1.,  0.,  1.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.]])\n \n"}, {"name": "feature.hessian_matrix_det()", "path": "api/skimage.feature#skimage.feature.hessian_matrix_det", "type": "feature", "text": " \nskimage.feature.hessian_matrix_det(image, sigma=1, approximate=True) [source]\n \nCompute the approximate Hessian Determinant over an image. The 2D approximate method uses box filters over integral images to compute the approximate Hessian Determinant, as described in [1].  Parameters \n \nimagearray \n\nThe image over which to compute Hessian Determinant.  \nsigmafloat, optional \n\nStandard deviation used for the Gaussian kernel, used for the Hessian matrix.  \napproximatebool, optional \n\nIf True and the image is 2D, use a much faster approximate computation. This argument has no effect on 3D and higher images.    Returns \n \noutarray \n\nThe array of the Determinant of Hessians.     Notes For 2D images when approximate=True, the running time of this method only depends on size of the image. It is independent of sigma as one would expect. The downside is that the result for sigma less than 3 is not accurate, i.e., not similar to the result obtained if someone computed the Hessian and took its determinant. References  \n1  \nHerbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool, \u201cSURF: Speeded Up Robust Features\u201d ftp://ftp.vision.ee.ethz.ch/publications/articles/eth_biwi_00517.pdf   \n"}, {"name": "feature.hessian_matrix_eigvals()", "path": "api/skimage.feature#skimage.feature.hessian_matrix_eigvals", "type": "feature", "text": " \nskimage.feature.hessian_matrix_eigvals(H_elems) [source]\n \nCompute eigenvalues of Hessian matrix.  Parameters \n \nH_elemslist of ndarray \n\nThe upper-diagonal elements of the Hessian matrix, as returned by hessian_matrix.    Returns \n \neigsndarray \n\nThe eigenvalues of the Hessian matrix, in decreasing order. The eigenvalues are the leading dimension. That is, eigs[i, j, k] contains the ith-largest eigenvalue at position (j, k).     Examples >>> from skimage.feature import hessian_matrix, hessian_matrix_eigvals\n>>> square = np.zeros((5, 5))\n>>> square[2, 2] = 4\n>>> H_elems = hessian_matrix(square, sigma=0.1, order='rc')\n>>> hessian_matrix_eigvals(H_elems)[0]\narray([[ 0.,  0.,  2.,  0.,  0.],\n       [ 0.,  1.,  0.,  1.,  0.],\n       [ 2.,  0., -2.,  0.,  2.],\n       [ 0.,  1.,  0.,  1.,  0.],\n       [ 0.,  0.,  2.,  0.,  0.]])\n \n"}, {"name": "feature.hog()", "path": "api/skimage.feature#skimage.feature.hog", "type": "feature", "text": " \nskimage.feature.hog(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(3, 3), block_norm='L2-Hys', visualize=False, transform_sqrt=False, feature_vector=True, multichannel=None) [source]\n \nExtract Histogram of Oriented Gradients (HOG) for a given image. Compute a Histogram of Oriented Gradients (HOG) by  (optional) global image normalization computing the gradient image in row and col\n computing gradient histograms normalizing across blocks flattening into a feature vector   Parameters \n \nimage(M, N[, C]) ndarray \n\nInput image.  \norientationsint, optional \n\nNumber of orientation bins.  \npixels_per_cell2-tuple (int, int), optional \n\nSize (in pixels) of a cell.  \ncells_per_block2-tuple (int, int), optional \n\nNumber of cells in each block.  \nblock_normstr {\u2018L1\u2019, \u2018L1-sqrt\u2019, \u2018L2\u2019, \u2018L2-Hys\u2019}, optional \n\nBlock normalization method:  \nL1 \n\nNormalization using L1-norm.  \nL1-sqrt \n\nNormalization using L1-norm, followed by square root.  \nL2 \n\nNormalization using L2-norm.  \nL2-Hys \n\nNormalization using L2-norm, followed by limiting the maximum values to 0.2 (Hys stands for hysteresis) and renormalization using L2-norm. (default) For details, see [3], [4].    \nvisualizebool, optional \n\nAlso return an image of the HOG. For each cell and orientation bin, the image contains a line segment that is centered at the cell center, is perpendicular to the midpoint of the range of angles spanned by the orientation bin, and has intensity proportional to the corresponding histogram value.  \ntransform_sqrtbool, optional \n\nApply power law compression to normalize the image before processing. DO NOT use this if the image contains negative values. Also see notes section below.  \nfeature_vectorbool, optional \n\nReturn the data as a feature vector by calling .ravel() on the result just before returning.  \nmultichannelboolean, optional \n\nIf True, the last image dimension is considered as a color channel, otherwise as spatial.    Returns \n \nout(n_blocks_row, n_blocks_col, n_cells_row, n_cells_col, n_orient) ndarray \n\nHOG descriptor for the image. If feature_vector is True, a 1D (flattened) array is returned.  \nhog_image(M, N) ndarray, optional \n\nA visualisation of the HOG image. Only provided if visualize is True.     Notes The presented code implements the HOG extraction method from [2] with the following changes: (I) blocks of (3, 3) cells are used ((2, 2) in the paper); (II) no smoothing within cells (Gaussian spatial window with sigma=8pix in the paper); (III) L1 block normalization is used (L2-Hys in the paper). Power law compression, also known as Gamma correction, is used to reduce the effects of shadowing and illumination variations. The compression makes the dark regions lighter. When the kwarg transform_sqrt is set to True, the function computes the square root of each color channel and then applies the hog algorithm to the image. References  \n1  \nhttps://en.wikipedia.org/wiki/Histogram_of_oriented_gradients  \n2  \nDalal, N and Triggs, B, Histograms of Oriented Gradients for Human Detection, IEEE Computer Society Conference on Computer Vision and Pattern Recognition 2005 San Diego, CA, USA, https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf, DOI:10.1109/CVPR.2005.177  \n3  \nLowe, D.G., Distinctive image features from scale-invatiant keypoints, International Journal of Computer Vision (2004) 60: 91, http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf, DOI:10.1023/B:VISI.0000029664.99615.94  \n4  \nDalal, N, Finding People in Images and Videos, Human-Computer Interaction [cs.HC], Institut National Polytechnique de Grenoble - INPG, 2006, https://tel.archives-ouvertes.fr/tel-00390303/file/NavneetDalalThesis.pdf   \n"}, {"name": "feature.local_binary_pattern()", "path": "api/skimage.feature#skimage.feature.local_binary_pattern", "type": "feature", "text": " \nskimage.feature.local_binary_pattern(image, P, R, method='default') [source]\n \nGray scale and rotation invariant LBP (Local Binary Patterns). LBP is an invariant descriptor that can be used for texture classification.  Parameters \n \nimage(N, M) array \n\nGraylevel image.  \nPint \n\nNumber of circularly symmetric neighbour set points (quantization of the angular space).  \nRfloat \n\nRadius of circle (spatial resolution of the operator).  \nmethod{\u2018default\u2019, \u2018ror\u2019, \u2018uniform\u2019, \u2018var\u2019} \n\nMethod to determine the pattern.  \n \u2018default\u2019: original local binary pattern which is gray scale but not\n\nrotation invariant.    \n \u2018ror\u2019: extension of default implementation which is gray scale and\n\nrotation invariant.    \n \u2018uniform\u2019: improved rotation invariance with uniform patterns and\n\nfiner quantization of the angular space which is gray scale and rotation invariant.    \n \u2018nri_uniform\u2019: non rotation-invariant uniform patterns variant\n\nwhich is only gray scale invariant [2].    \n \u2018var\u2019: rotation invariant variance measures of the contrast of local\n\nimage texture which is rotation but not gray scale invariant.        Returns \n \noutput(N, M) array \n\nLBP image.     References  \n1  \nMultiresolution Gray-Scale and Rotation Invariant Texture Classification with Local Binary Patterns. Timo Ojala, Matti Pietikainen, Topi Maenpaa. http://www.ee.oulu.fi/research/mvmp/mvg/files/pdf/pdf_94.pdf, 2002.  \n2  \nFace recognition with local binary patterns. Timo Ahonen, Abdenour Hadid, Matti Pietikainen, http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.214.6851, 2004.   \n"}, {"name": "feature.masked_register_translation()", "path": "api/skimage.feature#skimage.feature.masked_register_translation", "type": "feature", "text": " \nskimage.feature.masked_register_translation(src_image, target_image, src_mask, target_mask=None, overlap_ratio=0.3) [source]\n \nDeprecated function. Use skimage.registration.phase_cross_correlation instead. \n"}, {"name": "feature.match_descriptors()", "path": "api/skimage.feature#skimage.feature.match_descriptors", "type": "feature", "text": " \nskimage.feature.match_descriptors(descriptors1, descriptors2, metric=None, p=2, max_distance=inf, cross_check=True, max_ratio=1.0) [source]\n \nBrute-force matching of descriptors. For each descriptor in the first set this matcher finds the closest descriptor in the second set (and vice-versa in the case of enabled cross-checking).  Parameters \n \ndescriptors1(M, P) array \n\nDescriptors of size P about M keypoints in the first image.  \ndescriptors2(N, P) array \n\nDescriptors of size P about N keypoints in the second image.  \nmetric{\u2018euclidean\u2019, \u2018cityblock\u2019, \u2018minkowski\u2019, \u2018hamming\u2019, \u2026} , optional \n\nThe metric to compute the distance between two descriptors. See scipy.spatial.distance.cdist for all possible types. The hamming distance should be used for binary descriptors. By default the L2-norm is used for all descriptors of dtype float or double and the Hamming distance is used for binary descriptors automatically.  \npint, optional \n\nThe p-norm to apply for metric='minkowski'.  \nmax_distancefloat, optional \n\nMaximum allowed distance between descriptors of two keypoints in separate images to be regarded as a match.  \ncross_checkbool, optional \n\nIf True, the matched keypoints are returned after cross checking i.e. a matched pair (keypoint1, keypoint2) is returned if keypoint2 is the best match for keypoint1 in second image and keypoint1 is the best match for keypoint2 in first image.  \nmax_ratiofloat, optional \n\nMaximum ratio of distances between first and second closest descriptor in the second set of descriptors. This threshold is useful to filter ambiguous matches between the two descriptor sets. The choice of this value depends on the statistics of the chosen descriptor, e.g., for SIFT descriptors a value of 0.8 is usually chosen, see D.G. Lowe, \u201cDistinctive Image Features from Scale-Invariant Keypoints\u201d, International Journal of Computer Vision, 2004.    Returns \n \nmatches(Q, 2) array \n\nIndices of corresponding matches in first and second set of descriptors, where matches[:, 0] denote the indices in the first and matches[:, 1] the indices in the second set of descriptors.     \n"}, {"name": "feature.match_template()", "path": "api/skimage.feature#skimage.feature.match_template", "type": "feature", "text": " \nskimage.feature.match_template(image, template, pad_input=False, mode='constant', constant_values=0) [source]\n \nMatch a template to a 2-D or 3-D image using normalized correlation. The output is an array with values between -1.0 and 1.0. The value at a given position corresponds to the correlation coefficient between the image and the template. For pad_input=True matches correspond to the center and otherwise to the top-left corner of the template. To find the best match you must search for peaks in the response (output) image.  Parameters \n \nimage(M, N[, D]) array \n\n2-D or 3-D input image.  \ntemplate(m, n[, d]) array \n\nTemplate to locate. It must be (m <= M, n <= N[, d <= D]).  \npad_inputbool \n\nIf True, pad image so that output is the same size as the image, and output values correspond to the template center. Otherwise, the output is an array with shape (M - m + 1, N - n + 1) for an (M, N) image and an (m, n) template, and matches correspond to origin (top-left corner) of the template.  \nmodesee numpy.pad, optional \n\nPadding mode.  \nconstant_valuessee numpy.pad, optional \n\nConstant values used in conjunction with mode='constant'.    Returns \n \noutputarray \n\nResponse image with correlation coefficients.     Notes Details on the cross-correlation are presented in [1]. This implementation uses FFT convolutions of the image and the template. Reference [2] presents similar derivations but the approximation presented in this reference is not used in our implementation. References  \n1  \nJ. P. Lewis, \u201cFast Normalized Cross-Correlation\u201d, Industrial Light and Magic.  \n2  \nBriechle and Hanebeck, \u201cTemplate Matching using Fast Normalized Cross Correlation\u201d, Proceedings of the SPIE (2001). DOI:10.1117/12.421129   Examples >>> template = np.zeros((3, 3))\n>>> template[1, 1] = 1\n>>> template\narray([[0., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 0.]])\n>>> image = np.zeros((6, 6))\n>>> image[1, 1] = 1\n>>> image[4, 4] = -1\n>>> image\narray([[ 0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0., -1.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.]])\n>>> result = match_template(image, template)\n>>> np.round(result, 3)\narray([[ 1.   , -0.125,  0.   ,  0.   ],\n       [-0.125, -0.125,  0.   ,  0.   ],\n       [ 0.   ,  0.   ,  0.125,  0.125],\n       [ 0.   ,  0.   ,  0.125, -1.   ]])\n>>> result = match_template(image, template, pad_input=True)\n>>> np.round(result, 3)\narray([[-0.125, -0.125, -0.125,  0.   ,  0.   ,  0.   ],\n       [-0.125,  1.   , -0.125,  0.   ,  0.   ,  0.   ],\n       [-0.125, -0.125, -0.125,  0.   ,  0.   ,  0.   ],\n       [ 0.   ,  0.   ,  0.   ,  0.125,  0.125,  0.125],\n       [ 0.   ,  0.   ,  0.   ,  0.125, -1.   ,  0.125],\n       [ 0.   ,  0.   ,  0.   ,  0.125,  0.125,  0.125]])\n \n"}, {"name": "feature.multiblock_lbp()", "path": "api/skimage.feature#skimage.feature.multiblock_lbp", "type": "feature", "text": " \nskimage.feature.multiblock_lbp(int_image, r, c, width, height) [source]\n \nMulti-block local binary pattern (MB-LBP). The features are calculated similarly to local binary patterns (LBPs), (See local_binary_pattern()) except that summed blocks are used instead of individual pixel values. MB-LBP is an extension of LBP that can be computed on multiple scales in constant time using the integral image. Nine equally-sized rectangles are used to compute a feature. For each rectangle, the sum of the pixel intensities is computed. Comparisons of these sums to that of the central rectangle determine the feature, similarly to LBP.  Parameters \n \nint_image(N, M) array \n\nIntegral image.  \nrint \n\nRow-coordinate of top left corner of a rectangle containing feature.  \ncint \n\nColumn-coordinate of top left corner of a rectangle containing feature.  \nwidthint \n\nWidth of one of the 9 equal rectangles that will be used to compute a feature.  \nheightint \n\nHeight of one of the 9 equal rectangles that will be used to compute a feature.    Returns \n \noutputint \n\n8-bit MB-LBP feature descriptor.     References  \n1  \nFace Detection Based on Multi-Block LBP Representation. Lun Zhang, Rufeng Chu, Shiming Xiang, Shengcai Liao, Stan Z. Li http://www.cbsr.ia.ac.cn/users/scliao/papers/Zhang-ICB07-MBLBP.pdf   \n"}, {"name": "feature.multiscale_basic_features()", "path": "api/skimage.feature#skimage.feature.multiscale_basic_features", "type": "feature", "text": " \nskimage.feature.multiscale_basic_features(image, multichannel=False, intensity=True, edges=True, texture=True, sigma_min=0.5, sigma_max=16, num_sigma=None, num_workers=None) [source]\n \nLocal features for a single- or multi-channel nd image. Intensity, gradient intensity and local structure are computed at different scales thanks to Gaussian blurring.  Parameters \n \nimagendarray \n\nInput image, which can be grayscale or multichannel.  \nmultichannelbool, default False \n\nTrue if the last dimension corresponds to color channels.  \nintensitybool, default True \n\nIf True, pixel intensities averaged over the different scales are added to the feature set.  \nedgesbool, default True \n\nIf True, intensities of local gradients averaged over the different scales are added to the feature set.  \ntexturebool, default True \n\nIf True, eigenvalues of the Hessian matrix after Gaussian blurring at different scales are added to the feature set.  \nsigma_minfloat, optional \n\nSmallest value of the Gaussian kernel used to average local neighbourhoods before extracting features.  \nsigma_maxfloat, optional \n\nLargest value of the Gaussian kernel used to average local neighbourhoods before extracting features.  \nnum_sigmaint, optional \n\nNumber of values of the Gaussian kernel between sigma_min and sigma_max. If None, sigma_min multiplied by powers of 2 are used.  \nnum_workersint or None, optional \n\nThe number of parallel threads to use. If set to None, the full set of available cores are used.    Returns \n \nfeaturesnp.ndarray \n\nArray of shape image.shape + (n_features,)     \n"}, {"name": "feature.ORB", "path": "api/skimage.feature#skimage.feature.ORB", "type": "feature", "text": " \nclass skimage.feature.ORB(downscale=1.2, n_scales=8, n_keypoints=500, fast_n=9, fast_threshold=0.08, harris_k=0.04) [source]\n \nBases: skimage.feature.util.FeatureDetector, skimage.feature.util.DescriptorExtractor Oriented FAST and rotated BRIEF feature detector and binary descriptor extractor.  Parameters \n \nn_keypointsint, optional \n\nNumber of keypoints to be returned. The function will return the best n_keypoints according to the Harris corner response if more than n_keypoints are detected. If not, then all the detected keypoints are returned.  \nfast_nint, optional \n\nThe n parameter in skimage.feature.corner_fast. Minimum number of consecutive pixels out of 16 pixels on the circle that should all be either brighter or darker w.r.t test-pixel. A point c on the circle is darker w.r.t test pixel p if Ic < Ip - threshold and brighter if Ic > Ip + threshold. Also stands for the n in FAST-n corner detector.  \nfast_thresholdfloat, optional \n\nThe threshold parameter in feature.corner_fast. Threshold used to decide whether the pixels on the circle are brighter, darker or similar w.r.t. the test pixel. Decrease the threshold when more corners are desired and vice-versa.  \nharris_kfloat, optional \n\nThe k parameter in skimage.feature.corner_harris. Sensitivity factor to separate corners from edges, typically in range [0, 0.2]. Small values of k result in detection of sharp corners.  \ndownscalefloat, optional \n\nDownscale factor for the image pyramid. Default value 1.2 is chosen so that there are more dense scales which enable robust scale invariance for a subsequent feature description.  \nn_scalesint, optional \n\nMaximum number of scales from the bottom of the image pyramid to extract the features from.     References  \n1  \nEthan Rublee, Vincent Rabaud, Kurt Konolige and Gary Bradski \u201cORB: An efficient alternative to SIFT and SURF\u201d http://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf   Examples >>> from skimage.feature import ORB, match_descriptors\n>>> img1 = np.zeros((100, 100))\n>>> img2 = np.zeros_like(img1)\n>>> np.random.seed(1)\n>>> square = np.random.rand(20, 20)\n>>> img1[40:60, 40:60] = square\n>>> img2[53:73, 53:73] = square\n>>> detector_extractor1 = ORB(n_keypoints=5)\n>>> detector_extractor2 = ORB(n_keypoints=5)\n>>> detector_extractor1.detect_and_extract(img1)\n>>> detector_extractor2.detect_and_extract(img2)\n>>> matches = match_descriptors(detector_extractor1.descriptors,\n...                             detector_extractor2.descriptors)\n>>> matches\narray([[0, 0],\n       [1, 1],\n       [2, 2],\n       [3, 3],\n       [4, 4]])\n>>> detector_extractor1.keypoints[matches[:, 0]]\narray([[42., 40.],\n       [47., 58.],\n       [44., 40.],\n       [59., 42.],\n       [45., 44.]])\n>>> detector_extractor2.keypoints[matches[:, 1]]\narray([[55., 53.],\n       [60., 71.],\n       [57., 53.],\n       [72., 55.],\n       [58., 57.]])\n  Attributes \n \nkeypoints(N, 2) array \n\nKeypoint coordinates as (row, col).  \nscales(N, ) array \n\nCorresponding scales.  \norientations(N, ) array \n\nCorresponding orientations in radians.  \nresponses(N, ) array \n\nCorresponding Harris corner responses.  \ndescriptors(Q, descriptor_size) array of dtype bool \n\n2D array of binary descriptors of size descriptor_size for Q keypoints after filtering out border keypoints with value at an index (i, j) either being True or False representing the outcome of the intensity comparison for i-th keypoint on j-th decision pixel-pair. It is Q == np.sum(mask).      \n__init__(downscale=1.2, n_scales=8, n_keypoints=500, fast_n=9, fast_threshold=0.08, harris_k=0.04) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \ndetect(image) [source]\n \nDetect oriented FAST keypoints along with the corresponding scale.  Parameters \n \nimage2D array \n\nInput image.     \n  \ndetect_and_extract(image) [source]\n \nDetect oriented FAST keypoints and extract rBRIEF descriptors. Note that this is faster than first calling detect and then extract.  Parameters \n \nimage2D array \n\nInput image.     \n  \nextract(image, keypoints, scales, orientations) [source]\n \nExtract rBRIEF binary descriptors for given keypoints in image. Note that the keypoints must be extracted using the same downscale and n_scales parameters. Additionally, if you want to extract both keypoints and descriptors you should use the faster detect_and_extract.  Parameters \n \nimage2D array \n\nInput image.  \nkeypoints(N, 2) array \n\nKeypoint coordinates as (row, col).  \nscales(N, ) array \n\nCorresponding scales.  \norientations(N, ) array \n\nCorresponding orientations in radians.     \n \n"}, {"name": "feature.ORB.detect()", "path": "api/skimage.feature#skimage.feature.ORB.detect", "type": "feature", "text": " \ndetect(image) [source]\n \nDetect oriented FAST keypoints along with the corresponding scale.  Parameters \n \nimage2D array \n\nInput image.     \n"}, {"name": "feature.ORB.detect_and_extract()", "path": "api/skimage.feature#skimage.feature.ORB.detect_and_extract", "type": "feature", "text": " \ndetect_and_extract(image) [source]\n \nDetect oriented FAST keypoints and extract rBRIEF descriptors. Note that this is faster than first calling detect and then extract.  Parameters \n \nimage2D array \n\nInput image.     \n"}, {"name": "feature.ORB.extract()", "path": "api/skimage.feature#skimage.feature.ORB.extract", "type": "feature", "text": " \nextract(image, keypoints, scales, orientations) [source]\n \nExtract rBRIEF binary descriptors for given keypoints in image. Note that the keypoints must be extracted using the same downscale and n_scales parameters. Additionally, if you want to extract both keypoints and descriptors you should use the faster detect_and_extract.  Parameters \n \nimage2D array \n\nInput image.  \nkeypoints(N, 2) array \n\nKeypoint coordinates as (row, col).  \nscales(N, ) array \n\nCorresponding scales.  \norientations(N, ) array \n\nCorresponding orientations in radians.     \n"}, {"name": "feature.ORB.__init__()", "path": "api/skimage.feature#skimage.feature.ORB.__init__", "type": "feature", "text": " \n__init__(downscale=1.2, n_scales=8, n_keypoints=500, fast_n=9, fast_threshold=0.08, harris_k=0.04) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "feature.peak_local_max()", "path": "api/skimage.feature#skimage.feature.peak_local_max", "type": "feature", "text": " \nskimage.feature.peak_local_max(image, min_distance=1, threshold_abs=None, threshold_rel=None, exclude_border=True, indices=True, num_peaks=inf, footprint=None, labels=None, num_peaks_per_label=inf, p_norm=inf) [source]\n \nFind peaks in an image as coordinate list or boolean mask. Peaks are the local maxima in a region of 2 * min_distance + 1 (i.e. peaks are separated by at least min_distance). If both threshold_abs and threshold_rel are provided, the maximum of the two is chosen as the minimum intensity threshold of peaks.  Changed in version 0.18: Prior to version 0.18, peaks of the same height within a radius of min_distance were all returned, but this could cause unexpected behaviour. From 0.18 onwards, an arbitrary peak within the region is returned. See issue gh-2592.   Parameters \n \nimagendarray \n\nInput image.  \nmin_distanceint, optional \n\nThe minimal allowed distance separating peaks. To find the maximum number of peaks, use min_distance=1.  \nthreshold_absfloat, optional \n\nMinimum intensity of peaks. By default, the absolute threshold is the minimum intensity of the image.  \nthreshold_relfloat, optional \n\nMinimum intensity of peaks, calculated as max(image) * threshold_rel.  \nexclude_borderint, tuple of ints, or bool, optional \n\nIf positive integer, exclude_border excludes peaks from within exclude_border-pixels of the border of the image. If tuple of non-negative ints, the length of the tuple must match the input array\u2019s dimensionality. Each element of the tuple will exclude peaks from within exclude_border-pixels of the border of the image along that dimension. If True, takes the min_distance parameter as value. If zero or False, peaks are identified regardless of their distance from the border.  \nindicesbool, optional \n\nIf True, the output will be an array representing peak coordinates. The coordinates are sorted according to peaks values (Larger first). If False, the output will be a boolean array shaped as image.shape with peaks present at True elements. indices is deprecated and will be removed in version 0.20. Default behavior will be to always return peak coordinates. You can obtain a mask as shown in the example below.  \nnum_peaksint, optional \n\nMaximum number of peaks. When the number of peaks exceeds num_peaks, return num_peaks peaks based on highest peak intensity.  \nfootprintndarray of bools, optional \n\nIf provided, footprint == 1 represents the local region within which to search for peaks at every point in image.  \nlabelsndarray of ints, optional \n\nIf provided, each unique region labels == value represents a unique region to search for peaks. Zero is reserved for background.  \nnum_peaks_per_labelint, optional \n\nMaximum number of peaks for each label.  \np_normfloat \n\nWhich Minkowski p-norm to use. Should be in the range [1, inf]. A finite large p may cause a ValueError if overflow can occur. inf corresponds to the Chebyshev distance and 2 to the Euclidean distance.    Returns \n \noutputndarray or ndarray of bools \n\n If indices = True : (row, column, \u2026) coordinates of peaks. If indices = False : Boolean array shaped like image, with peaks represented by True values.       See also  \nskimage.feature.corner_peaks\n\n  Notes The peak local maximum function returns the coordinates of local peaks (maxima) in an image. Internally, a maximum filter is used for finding local maxima. This operation dilates the original image. After comparison of the dilated and original image, this function returns the coordinates or a mask of the peaks where the dilated image equals the original image. Examples >>> img1 = np.zeros((7, 7))\n>>> img1[3, 4] = 1\n>>> img1[3, 2] = 1.5\n>>> img1\narray([[0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n       [0. , 0. , 1.5, 0. , 1. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 0. , 0. ]])\n >>> peak_local_max(img1, min_distance=1)\narray([[3, 2],\n       [3, 4]])\n >>> peak_local_max(img1, min_distance=2)\narray([[3, 2]])\n >>> img2 = np.zeros((20, 20, 20))\n>>> img2[10, 10, 10] = 1\n>>> img2[15, 15, 15] = 1\n>>> peak_idx = peak_local_max(img2, exclude_border=0)\n>>> peak_idx\narray([[10, 10, 10],\n       [15, 15, 15]])\n >>> peak_mask = np.zeros_like(img2, dtype=bool)\n>>> peak_mask[tuple(peak_idx.T)] = True\n>>> np.argwhere(peak_mask)\narray([[10, 10, 10],\n       [15, 15, 15]])\n \n"}, {"name": "feature.plot_matches()", "path": "api/skimage.feature#skimage.feature.plot_matches", "type": "feature", "text": " \nskimage.feature.plot_matches(ax, image1, image2, keypoints1, keypoints2, matches, keypoints_color='k', matches_color=None, only_matches=False, alignment='horizontal') [source]\n \nPlot matched features.  Parameters \n \naxmatplotlib.axes.Axes \n\nMatches and image are drawn in this ax.  \nimage1(N, M [, 3]) array \n\nFirst grayscale or color image.  \nimage2(N, M [, 3]) array \n\nSecond grayscale or color image.  \nkeypoints1(K1, 2) array \n\nFirst keypoint coordinates as (row, col).  \nkeypoints2(K2, 2) array \n\nSecond keypoint coordinates as (row, col).  \nmatches(Q, 2) array \n\nIndices of corresponding matches in first and second set of descriptors, where matches[:, 0] denote the indices in the first and matches[:, 1] the indices in the second set of descriptors.  \nkeypoints_colormatplotlib color, optional \n\nColor for keypoint locations.  \nmatches_colormatplotlib color, optional \n\nColor for lines which connect keypoint matches. By default the color is chosen randomly.  \nonly_matchesbool, optional \n\nWhether to only plot matches and not plot the keypoint locations.  \nalignment{\u2018horizontal\u2019, \u2018vertical\u2019}, optional \n\nWhether to show images side by side, 'horizontal', or one above the other, 'vertical'.     \n"}, {"name": "feature.register_translation()", "path": "api/skimage.feature#skimage.feature.register_translation", "type": "feature", "text": " \nskimage.feature.register_translation(src_image, target_image, upsample_factor=1, space='real', return_error=True) [source]\n \nDeprecated function. Use skimage.registration.phase_cross_correlation instead. \n"}, {"name": "feature.shape_index()", "path": "api/skimage.feature#skimage.feature.shape_index", "type": "feature", "text": " \nskimage.feature.shape_index(image, sigma=1, mode='constant', cval=0) [source]\n \nCompute the shape index. The shape index, as defined by Koenderink & van Doorn [1], is a single valued measure of local curvature, assuming the image as a 3D plane with intensities representing heights. It is derived from the eigen values of the Hessian, and its value ranges from -1 to 1 (and is undefined (=NaN) in flat regions), with following ranges representing following shapes:  Ranges of the shape index and corresponding shapes.  \nInterval (s in \u2026) Shape   \n[ -1, -7/8) Spherical cup  \n[-7/8, -5/8) Through  \n[-5/8, -3/8) Rut  \n[-3/8, -1/8) Saddle rut  \n[-1/8, +1/8) Saddle  \n[+1/8, +3/8) Saddle ridge  \n[+3/8, +5/8) Ridge  \n[+5/8, +7/8) Dome  \n[+7/8, +1] Spherical cap    Parameters \n \nimagendarray \n\nInput image.  \nsigmafloat, optional \n\nStandard deviation used for the Gaussian kernel, which is used for smoothing the input data before Hessian eigen value calculation.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.    Returns \n \nsndarray \n\nShape index     References  \n1  \nKoenderink, J. J. & van Doorn, A. J., \u201cSurface shape and curvature scales\u201d, Image and Vision Computing, 1992, 10, 557-564. DOI:10.1016/0262-8856(92)90076-F   Examples >>> from skimage.feature import shape_index\n>>> square = np.zeros((5, 5))\n>>> square[2, 2] = 4\n>>> s = shape_index(square, sigma=0.1)\n>>> s\narray([[ nan,  nan, -0.5,  nan,  nan],\n       [ nan, -0. ,  nan, -0. ,  nan],\n       [-0.5,  nan, -1. ,  nan, -0.5],\n       [ nan, -0. ,  nan, -0. ,  nan],\n       [ nan,  nan, -0.5,  nan,  nan]])\n \n"}, {"name": "feature.structure_tensor()", "path": "api/skimage.feature#skimage.feature.structure_tensor", "type": "feature", "text": " \nskimage.feature.structure_tensor(image, sigma=1, mode='constant', cval=0, order=None) [source]\n \nCompute structure tensor using sum of squared differences. The (2-dimensional) structure tensor A is defined as: A = [Arr Arc]\n    [Arc Acc]\n which is approximated by the weighted sum of squared differences in a local window around each pixel in the image. This formula can be extended to a larger number of dimensions (see [1]).  Parameters \n \nimagendarray \n\nInput image.  \nsigmafloat, optional \n\nStandard deviation used for the Gaussian kernel, which is used as a weighting function for the local summation of squared differences.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.  \norder{\u2018rc\u2019, \u2018xy\u2019}, optional \n\nNOTE: Only applies in 2D. Higher dimensions must always use \u2018rc\u2019 order. This parameter allows for the use of reverse or forward order of the image axes in gradient computation. \u2018rc\u2019 indicates the use of the first axis initially (Arr, Arc, Acc), whilst \u2018xy\u2019 indicates the usage of the last axis initially (Axx, Axy, Ayy).    Returns \n \nA_elemslist of ndarray \n\nUpper-diagonal elements of the structure tensor for each pixel in the input image.      See also  \nstructure_tensor_eigenvalues\n\n  References  \n1  \nhttps://en.wikipedia.org/wiki/Structure_tensor   Examples >>> from skimage.feature import structure_tensor\n>>> square = np.zeros((5, 5))\n>>> square[2, 2] = 1\n>>> Arr, Arc, Acc = structure_tensor(square, sigma=0.1, order='rc')\n>>> Acc\narray([[0., 0., 0., 0., 0.],\n       [0., 1., 0., 1., 0.],\n       [0., 4., 0., 4., 0.],\n       [0., 1., 0., 1., 0.],\n       [0., 0., 0., 0., 0.]])\n \n"}, {"name": "feature.structure_tensor_eigenvalues()", "path": "api/skimage.feature#skimage.feature.structure_tensor_eigenvalues", "type": "feature", "text": " \nskimage.feature.structure_tensor_eigenvalues(A_elems) [source]\n \nCompute eigenvalues of structure tensor.  Parameters \n \nA_elemslist of ndarray \n\nThe upper-diagonal elements of the structure tensor, as returned by structure_tensor.    Returns \n ndarray\n\nThe eigenvalues of the structure tensor, in decreasing order. The eigenvalues are the leading dimension. That is, the coordinate [i, j, k] corresponds to the ith-largest eigenvalue at position (j, k).      See also  \nstructure_tensor\n\n  Examples >>> from skimage.feature import structure_tensor\n>>> from skimage.feature import structure_tensor_eigenvalues\n>>> square = np.zeros((5, 5))\n>>> square[2, 2] = 1\n>>> A_elems = structure_tensor(square, sigma=0.1, order='rc')\n>>> structure_tensor_eigenvalues(A_elems)[0]\narray([[0., 0., 0., 0., 0.],\n       [0., 2., 4., 2., 0.],\n       [0., 4., 0., 4., 0.],\n       [0., 2., 4., 2., 0.],\n       [0., 0., 0., 0., 0.]])\n \n"}, {"name": "feature.structure_tensor_eigvals()", "path": "api/skimage.feature#skimage.feature.structure_tensor_eigvals", "type": "feature", "text": " \nskimage.feature.structure_tensor_eigvals(Axx, Axy, Ayy) [source]\n \nCompute eigenvalues of structure tensor.  Parameters \n \nAxxndarray \n\nElement of the structure tensor for each pixel in the input image.  \nAxyndarray \n\nElement of the structure tensor for each pixel in the input image.  \nAyyndarray \n\nElement of the structure tensor for each pixel in the input image.    Returns \n \nl1ndarray \n\nLarger eigen value for each input matrix.  \nl2ndarray \n\nSmaller eigen value for each input matrix.     Examples >>> from skimage.feature import structure_tensor, structure_tensor_eigvals\n>>> square = np.zeros((5, 5))\n>>> square[2, 2] = 1\n>>> Arr, Arc, Acc = structure_tensor(square, sigma=0.1, order='rc')\n>>> structure_tensor_eigvals(Acc, Arc, Arr)[0]\narray([[0., 0., 0., 0., 0.],\n       [0., 2., 4., 2., 0.],\n       [0., 4., 0., 4., 0.],\n       [0., 2., 4., 2., 0.],\n       [0., 0., 0., 0., 0.]])\n \n"}, {"name": "filters", "path": "api/skimage.filters", "type": "filters", "text": "Module: filters  \nskimage.filters.apply_hysteresis_threshold(\u2026) Apply hysteresis thresholding to image.  \nskimage.filters.correlate_sparse(image, kernel) Compute valid cross-correlation of padded_array and kernel.  \nskimage.filters.difference_of_gaussians(\u2026) Find features between low_sigma and high_sigma in size.  \nskimage.filters.farid(image, *[, mask]) Find the edge magnitude using the Farid transform.  \nskimage.filters.farid_h(image, *[, mask]) Find the horizontal edges of an image using the Farid transform.  \nskimage.filters.farid_v(image, *[, mask]) Find the vertical edges of an image using the Farid transform.  \nskimage.filters.frangi(image[, sigmas, \u2026]) Filter an image with the Frangi vesselness filter.  \nskimage.filters.gabor(image, frequency[, \u2026]) Return real and imaginary responses to Gabor filter.  \nskimage.filters.gabor_kernel(frequency[, \u2026]) Return complex 2D Gabor filter kernel.  \nskimage.filters.gaussian(image[, sigma, \u2026]) Multi-dimensional Gaussian filter.  \nskimage.filters.hessian(image[, sigmas, \u2026]) Filter an image with the Hybrid Hessian filter.  \nskimage.filters.inverse(data[, \u2026]) Apply the filter in reverse to the given data.  \nskimage.filters.laplace(image[, ksize, mask]) Find the edges of an image using the Laplace operator.  \nskimage.filters.median(image[, selem, out, \u2026]) Return local median of an image.  \nskimage.filters.meijering(image[, sigmas, \u2026]) Filter an image with the Meijering neuriteness filter.  \nskimage.filters.prewitt(image[, mask, axis, \u2026]) Find the edge magnitude using the Prewitt transform.  \nskimage.filters.prewitt_h(image[, mask]) Find the horizontal edges of an image using the Prewitt transform.  \nskimage.filters.prewitt_v(image[, mask]) Find the vertical edges of an image using the Prewitt transform.  \nskimage.filters.rank_order(image) Return an image of the same shape where each pixel is the index of the pixel value in the ascending order of the unique values of image, aka the rank-order value.  \nskimage.filters.roberts(image[, mask]) Find the edge magnitude using Roberts\u2019 cross operator.  \nskimage.filters.roberts_neg_diag(image[, mask]) Find the cross edges of an image using the Roberts\u2019 Cross operator.  \nskimage.filters.roberts_pos_diag(image[, mask]) Find the cross edges of an image using Roberts\u2019 cross operator.  \nskimage.filters.sato(image[, sigmas, \u2026]) Filter an image with the Sato tubeness filter.  \nskimage.filters.scharr(image[, mask, axis, \u2026]) Find the edge magnitude using the Scharr transform.  \nskimage.filters.scharr_h(image[, mask]) Find the horizontal edges of an image using the Scharr transform.  \nskimage.filters.scharr_v(image[, mask]) Find the vertical edges of an image using the Scharr transform.  \nskimage.filters.sobel(image[, mask, axis, \u2026]) Find edges in an image using the Sobel filter.  \nskimage.filters.sobel_h(image[, mask]) Find the horizontal edges of an image using the Sobel transform.  \nskimage.filters.sobel_v(image[, mask]) Find the vertical edges of an image using the Sobel transform.  \nskimage.filters.threshold_isodata([image, \u2026]) Return threshold value(s) based on ISODATA method.  \nskimage.filters.threshold_li(image, *[, \u2026]) Compute threshold value by Li\u2019s iterative Minimum Cross Entropy method.  \nskimage.filters.threshold_local(image, \u2026) Compute a threshold mask image based on local pixel neighborhood.  \nskimage.filters.threshold_mean(image) Return threshold value based on the mean of grayscale values.  \nskimage.filters.threshold_minimum([image, \u2026]) Return threshold value based on minimum method.  \nskimage.filters.threshold_multiotsu(image[, \u2026]) Generate classes-1 threshold values to divide gray levels in image.  \nskimage.filters.threshold_niblack(image[, \u2026]) Applies Niblack local threshold to an array.  \nskimage.filters.threshold_otsu([image, \u2026]) Return threshold value based on Otsu\u2019s method.  \nskimage.filters.threshold_sauvola(image[, \u2026]) Applies Sauvola local threshold to an array.  \nskimage.filters.threshold_triangle(image[, \u2026]) Return threshold value based on the triangle algorithm.  \nskimage.filters.threshold_yen([image, \u2026]) Return threshold value based on Yen\u2019s method.  \nskimage.filters.try_all_threshold(image[, \u2026]) Returns a figure comparing the outputs of different thresholding methods.  \nskimage.filters.unsharp_mask(image[, \u2026]) Unsharp masking filter.  \nskimage.filters.wiener(data[, \u2026]) Minimum Mean Square Error (Wiener) inverse filter.  \nskimage.filters.window(window_type, shape[, \u2026]) Return an n-dimensional window of a given size and dimensionality.  \nskimage.filters.LPIFilter2D(\u2026) Linear Position-Invariant Filter (2-dimensional)  \nskimage.filters.rank    apply_hysteresis_threshold  \nskimage.filters.apply_hysteresis_threshold(image, low, high) [source]\n \nApply hysteresis thresholding to image. This algorithm finds regions where image is greater than high OR image is greater than low and that region is connected to a region greater than high.  Parameters \n \nimagearray, shape (M,[ N, \u2026, P]) \n\nGrayscale input image.  \nlowfloat, or array of same shape as image \n\nLower threshold.  \nhighfloat, or array of same shape as image \n\nHigher threshold.    Returns \n \nthresholdedarray of bool, same shape as image \n\nArray in which True indicates the locations where image was above the hysteresis threshold.     References  \n1  \nJ. Canny. A computational approach to edge detection. IEEE Transactions on Pattern Analysis and Machine Intelligence. 1986; vol. 8, pp.679-698. DOI:10.1109/TPAMI.1986.4767851   Examples >>> image = np.array([1, 2, 3, 2, 1, 2, 1, 3, 2])\n>>> apply_hysteresis_threshold(image, 1.5, 2.5).astype(int)\narray([0, 1, 1, 1, 0, 0, 0, 1, 1])\n \n correlate_sparse  \nskimage.filters.correlate_sparse(image, kernel, mode='reflect') [source]\n \nCompute valid cross-correlation of padded_array and kernel. This function is fast when kernel is large with many zeros. See scipy.ndimage.correlate for a description of cross-correlation.  Parameters \n \nimagendarray, dtype float, shape (M, N,[ \u2026,] P) \n\nThe input array. If mode is \u2018valid\u2019, this array should already be padded, as a margin of the same shape as kernel will be stripped off.  \nkernelndarray, dtype float shape (Q, R,[ \u2026,] S) \n\nThe kernel to be correlated. Must have the same number of dimensions as padded_array. For high performance, it should be sparse (few nonzero entries).  \nmodestring, optional \n\nSee scipy.ndimage.correlate for valid modes. Additionally, mode \u2018valid\u2019 is accepted, in which case no padding is applied and the result is the result for the smaller image for which the kernel is entirely inside the original data.    Returns \n \nresultarray of float, shape (M, N,[ \u2026,] P) \n\nThe result of cross-correlating image with kernel. If mode \u2018valid\u2019 is used, the resulting shape is (M-Q+1, N-R+1,[ \u2026,] P-S+1).     \n difference_of_gaussians  \nskimage.filters.difference_of_gaussians(image, low_sigma, high_sigma=None, *, mode='nearest', cval=0, multichannel=False, truncate=4.0) [source]\n \nFind features between low_sigma and high_sigma in size. This function uses the Difference of Gaussians method for applying band-pass filters to multi-dimensional arrays. The input array is blurred with two Gaussian kernels of differing sigmas to produce two intermediate, filtered images. The more-blurred image is then subtracted from the less-blurred image. The final output image will therefore have had high-frequency components attenuated by the smaller-sigma Gaussian, and low frequency components will have been removed due to their presence in the more-blurred intermediate.  Parameters \n \nimagendarray \n\nInput array to filter.  \nlow_sigmascalar or sequence of scalars \n\nStandard deviation(s) for the Gaussian kernel with the smaller sigmas across all axes. The standard deviations are given for each axis as a sequence, or as a single number, in which case the single number is used as the standard deviation value for all axes.  \nhigh_sigmascalar or sequence of scalars, optional (default is None) \n\nStandard deviation(s) for the Gaussian kernel with the larger sigmas across all axes. The standard deviations are given for each axis as a sequence, or as a single number, in which case the single number is used as the standard deviation value for all axes. If None is given (default), sigmas for all axes are calculated as 1.6 * low_sigma.  \nmode{\u2018reflect\u2019, \u2018constant\u2019, \u2018nearest\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional \n\nThe mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019. Default is \u2018nearest\u2019.  \ncvalscalar, optional \n\nValue to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0  \nmultichannelbool, optional (default: False) \n\nWhether the last axis of the image is to be interpreted as multiple channels. If True, each channel is filtered separately (channels are not mixed together).  \ntruncatefloat, optional (default is 4.0) \n\nTruncate the filter at this many standard deviations.    Returns \n \nfiltered_imagendarray \n\nthe filtered array.      See also  \nskimage.feature.blog_dog \n  Notes This function will subtract an array filtered with a Gaussian kernel with sigmas given by high_sigma from an array filtered with a Gaussian kernel with sigmas provided by low_sigma. The values for high_sigma must always be greater than or equal to the corresponding values in low_sigma, or a ValueError will be raised. When high_sigma is none, the values for high_sigma will be calculated as 1.6x the corresponding values in low_sigma. This ratio was originally proposed by Marr and Hildreth (1980) [1] and is commonly used when approximating the inverted Laplacian of Gaussian, which is used in edge and blob detection. Input image is converted according to the conventions of img_as_float. Except for sigma values, all parameters are used for both filters. References  \n1  \nMarr, D. and Hildreth, E. Theory of Edge Detection. Proc. R. Soc. Lond. Series B 207, 187-217 (1980). https://doi.org/10.1098/rspb.1980.0020   Examples Apply a simple Difference of Gaussians filter to a color image: >>> from skimage.data import astronaut\n>>> from skimage.filters import difference_of_gaussians\n>>> filtered_image = difference_of_gaussians(astronaut(), 2, 10,\n...                                          multichannel=True)\n Apply a Laplacian of Gaussian filter as approximated by the Difference of Gaussians filter: >>> filtered_image = difference_of_gaussians(astronaut(), 2,\n...                                          multichannel=True)\n Apply a Difference of Gaussians filter to a grayscale image using different sigma values for each axis: >>> from skimage.data import camera\n>>> filtered_image = difference_of_gaussians(camera(), (2,5), (3,20))\n \n farid  \nskimage.filters.farid(image, *, mask=None) [source]\n \nFind the edge magnitude using the Farid transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Farid edge map.      See also  \nsobel, prewitt, canny \n  Notes Take the square root of the sum of the squares of the horizontal and vertical derivatives to get a magnitude that is somewhat insensitive to direction. Similar to the Scharr operator, this operator is designed with a rotation invariance constraint. References  \n1  \nFarid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional signals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004. DOI:10.1109/TIP.2004.823819  \n2  \nWikipedia, \u201cFarid and Simoncelli Derivatives.\u201d Available at: <https://en.wikipedia.org/wiki/Image_derivatives#Farid_and_Simoncelli_Derivatives>   Examples >>> from skimage import data\n>>> camera = data.camera()\n>>> from skimage import filters\n>>> edges = filters.farid(camera)\n \n farid_h  \nskimage.filters.farid_h(image, *, mask=None) [source]\n \nFind the horizontal edges of an image using the Farid transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Farid edge map.     Notes The kernel was constructed using the 5-tap weights from [1]. References  \n1  \nFarid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional signals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004. DOI:10.1109/TIP.2004.823819  \n2  \nFarid, H. and Simoncelli, E. P. \u201cOptimally rotation-equivariant directional derivative kernels\u201d, In: 7th International Conference on Computer Analysis of Images and Patterns, Kiel, Germany. Sep, 1997.   \n farid_v  \nskimage.filters.farid_v(image, *, mask=None) [source]\n \nFind the vertical edges of an image using the Farid transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Farid edge map.     Notes The kernel was constructed using the 5-tap weights from [1]. References  \n1  \nFarid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional signals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004. DOI:10.1109/TIP.2004.823819   \n frangi  \nskimage.filters.frangi(image, sigmas=range(1, 10, 2), scale_range=None, scale_step=None, alpha=0.5, beta=0.5, gamma=15, black_ridges=True, mode='reflect', cval=0) [source]\n \nFilter an image with the Frangi vesselness filter. This filter can be used to detect continuous ridges, e.g. vessels, wrinkles, rivers. It can be used to calculate the fraction of the whole image containing such objects. Defined only for 2-D and 3-D images. Calculates the eigenvectors of the Hessian to compute the similarity of an image region to vessels, according to the method described in [1].  Parameters \n \nimage(N, M[, P]) ndarray \n\nArray with input image data.  \nsigmasiterable of floats, optional \n\nSigmas used as scales of filter, i.e., np.arange(scale_range[0], scale_range[1], scale_step)  \nscale_range2-tuple of floats, optional \n\nThe range of sigmas used.  \nscale_stepfloat, optional \n\nStep size between sigmas.  \nalphafloat, optional \n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to deviation from a plate-like structure.  \nbetafloat, optional \n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to deviation from a blob-like structure.  \ngammafloat, optional \n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to areas of high variance/texture/structure.  \nblack_ridgesboolean, optional \n\nWhen True (the default), the filter detects black ridges; when False, it detects white ridges.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.    Returns \n \nout(N, M[, P]) ndarray \n\nFiltered image (maximum of pixels across all scales).      See also  \nmeijering\n\n\nsato\n\n\nhessian\n\n  Notes Written by Marc Schrijver, November 2001 Re-Written by D. J. Kroon, University of Twente, May 2009, [2] Adoption of 3D version from D. G. Ellis, Januar 20017, [3] References  \n1  \nFrangi, A. F., Niessen, W. J., Vincken, K. L., & Viergever, M. A. (1998,). Multiscale vessel enhancement filtering. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 130-137). Springer Berlin Heidelberg. DOI:10.1007/BFb0056195  \n2  \nKroon, D. J.: Hessian based Frangi vesselness filter.  \n3  \nEllis, D. G.: https://github.com/ellisdg/frangi3d/tree/master/frangi   \n gabor  \nskimage.filters.gabor(image, frequency, theta=0, bandwidth=1, sigma_x=None, sigma_y=None, n_stds=3, offset=0, mode='reflect', cval=0) [source]\n \nReturn real and imaginary responses to Gabor filter. The real and imaginary parts of the Gabor filter kernel are applied to the image and the response is returned as a pair of arrays. Gabor filter is a linear filter with a Gaussian kernel which is modulated by a sinusoidal plane wave. Frequency and orientation representations of the Gabor filter are similar to those of the human visual system. Gabor filter banks are commonly used in computer vision and image processing. They are especially suitable for edge detection and texture classification.  Parameters \n \nimage2-D array \n\nInput image.  \nfrequencyfloat \n\nSpatial frequency of the harmonic function. Specified in pixels.  \nthetafloat, optional \n\nOrientation in radians. If 0, the harmonic is in the x-direction.  \nbandwidthfloat, optional \n\nThe bandwidth captured by the filter. For fixed bandwidth, sigma_x and sigma_y will decrease with increasing frequency. This value is ignored if sigma_x and sigma_y are set by the user.  \nsigma_x, sigma_yfloat, optional \n\nStandard deviation in x- and y-directions. These directions apply to the kernel before rotation. If theta = pi/2, then the kernel is rotated 90 degrees so that sigma_x controls the vertical direction.  \nn_stdsscalar, optional \n\nThe linear size of the kernel is n_stds (3 by default) standard deviations.  \noffsetfloat, optional \n\nPhase offset of harmonic function in radians.  \nmode{\u2018constant\u2019, \u2018nearest\u2019, \u2018reflect\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional \n\nMode used to convolve image with a kernel, passed to ndi.convolve  \ncvalscalar, optional \n\nValue to fill past edges of input if mode of convolution is \u2018constant\u2019. The parameter is passed to ndi.convolve.    Returns \n \nreal, imagarrays \n\nFiltered images using the real and imaginary parts of the Gabor filter kernel. Images are of the same dimensions as the input one.     References  \n1  \nhttps://en.wikipedia.org/wiki/Gabor_filter  \n2  \nhttps://web.archive.org/web/20180127125930/http://mplab.ucsd.edu/tutorials/gabor.pdf   Examples >>> from skimage.filters import gabor\n>>> from skimage import data, io\n>>> from matplotlib import pyplot as plt  \n >>> image = data.coins()\n>>> # detecting edges in a coin image\n>>> filt_real, filt_imag = gabor(image, frequency=0.6)\n>>> plt.figure()            \n>>> io.imshow(filt_real)    \n>>> io.show()               \n >>> # less sensitivity to finer details with the lower frequency kernel\n>>> filt_real, filt_imag = gabor(image, frequency=0.1)\n>>> plt.figure()            \n>>> io.imshow(filt_real)    \n>>> io.show()               \n \n gabor_kernel  \nskimage.filters.gabor_kernel(frequency, theta=0, bandwidth=1, sigma_x=None, sigma_y=None, n_stds=3, offset=0) [source]\n \nReturn complex 2D Gabor filter kernel. Gabor kernel is a Gaussian kernel modulated by a complex harmonic function. Harmonic function consists of an imaginary sine function and a real cosine function. Spatial frequency is inversely proportional to the wavelength of the harmonic and to the standard deviation of a Gaussian kernel. The bandwidth is also inversely proportional to the standard deviation.  Parameters \n \nfrequencyfloat \n\nSpatial frequency of the harmonic function. Specified in pixels.  \nthetafloat, optional \n\nOrientation in radians. If 0, the harmonic is in the x-direction.  \nbandwidthfloat, optional \n\nThe bandwidth captured by the filter. For fixed bandwidth, sigma_x and sigma_y will decrease with increasing frequency. This value is ignored if sigma_x and sigma_y are set by the user.  \nsigma_x, sigma_yfloat, optional \n\nStandard deviation in x- and y-directions. These directions apply to the kernel before rotation. If theta = pi/2, then the kernel is rotated 90 degrees so that sigma_x controls the vertical direction.  \nn_stdsscalar, optional \n\nThe linear size of the kernel is n_stds (3 by default) standard deviations  \noffsetfloat, optional \n\nPhase offset of harmonic function in radians.    Returns \n \ngcomplex array \n\nComplex filter kernel.     References  \n1  \nhttps://en.wikipedia.org/wiki/Gabor_filter  \n2  \nhttps://web.archive.org/web/20180127125930/http://mplab.ucsd.edu/tutorials/gabor.pdf   Examples >>> from skimage.filters import gabor_kernel\n>>> from skimage import io\n>>> from matplotlib import pyplot as plt  \n >>> gk = gabor_kernel(frequency=0.2)\n>>> plt.figure()        \n>>> io.imshow(gk.real)  \n>>> io.show()           \n >>> # more ripples (equivalent to increasing the size of the\n>>> # Gaussian spread)\n>>> gk = gabor_kernel(frequency=0.2, bandwidth=0.1)\n>>> plt.figure()        \n>>> io.imshow(gk.real)  \n>>> io.show()           \n \n gaussian  \nskimage.filters.gaussian(image, sigma=1, output=None, mode='nearest', cval=0, multichannel=None, preserve_range=False, truncate=4.0) [source]\n \nMulti-dimensional Gaussian filter.  Parameters \n \nimagearray-like \n\nInput image (grayscale or color) to filter.  \nsigmascalar or sequence of scalars, optional \n\nStandard deviation for Gaussian kernel. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.  \noutputarray, optional \n\nThe output parameter passes an array in which to store the filter output.  \nmode{\u2018reflect\u2019, \u2018constant\u2019, \u2018nearest\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional \n\nThe mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019. Default is \u2018nearest\u2019.  \ncvalscalar, optional \n\nValue to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0  \nmultichannelbool, optional (default: None) \n\nWhether the last axis of the image is to be interpreted as multiple channels. If True, each channel is filtered separately (channels are not mixed together). Only 3 channels are supported. If None, the function will attempt to guess this, and raise a warning if ambiguous, when the array has shape (M, N, 3).  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html  \ntruncatefloat, optional \n\nTruncate the filter at this many standard deviations.    Returns \n \nfiltered_imagendarray \n\nthe filtered array     Notes This function is a wrapper around scipy.ndi.gaussian_filter(). Integer arrays are converted to float. The output should be floating point data type since gaussian converts to float provided image. If output is not provided, another array will be allocated and returned as the result. The multi-dimensional filter is implemented as a sequence of one-dimensional convolution filters. The intermediate arrays are stored in the same data type as the output. Therefore, for output types with a limited precision, the results may be imprecise because intermediate results may be stored with insufficient precision. Examples >>> a = np.zeros((3, 3))\n>>> a[1, 1] = 1\n>>> a\narray([[0., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 0.]])\n>>> gaussian(a, sigma=0.4)  # mild smoothing\narray([[0.00163116, 0.03712502, 0.00163116],\n       [0.03712502, 0.84496158, 0.03712502],\n       [0.00163116, 0.03712502, 0.00163116]])\n>>> gaussian(a, sigma=1)  # more smoothing\narray([[0.05855018, 0.09653293, 0.05855018],\n       [0.09653293, 0.15915589, 0.09653293],\n       [0.05855018, 0.09653293, 0.05855018]])\n>>> # Several modes are possible for handling boundaries\n>>> gaussian(a, sigma=1, mode='reflect')\narray([[0.08767308, 0.12075024, 0.08767308],\n       [0.12075024, 0.16630671, 0.12075024],\n       [0.08767308, 0.12075024, 0.08767308]])\n>>> # For RGB images, each is filtered separately\n>>> from skimage.data import astronaut\n>>> image = astronaut()\n>>> filtered_img = gaussian(image, sigma=1, multichannel=True)\n \n hessian  \nskimage.filters.hessian(image, sigmas=range(1, 10, 2), scale_range=None, scale_step=None, alpha=0.5, beta=0.5, gamma=15, black_ridges=True, mode=None, cval=0) [source]\n \nFilter an image with the Hybrid Hessian filter. This filter can be used to detect continuous edges, e.g. vessels, wrinkles, rivers. It can be used to calculate the fraction of the whole image containing such objects. Defined only for 2-D and 3-D images. Almost equal to Frangi filter, but uses alternative method of smoothing. Refer to [1] to find the differences between Frangi and Hessian filters.  Parameters \n \nimage(N, M[, P]) ndarray \n\nArray with input image data.  \nsigmasiterable of floats, optional \n\nSigmas used as scales of filter, i.e., np.arange(scale_range[0], scale_range[1], scale_step)  \nscale_range2-tuple of floats, optional \n\nThe range of sigmas used.  \nscale_stepfloat, optional \n\nStep size between sigmas.  \nbetafloat, optional \n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to deviation from a blob-like structure.  \ngammafloat, optional \n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to areas of high variance/texture/structure.  \nblack_ridgesboolean, optional \n\nWhen True (the default), the filter detects black ridges; when False, it detects white ridges.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.    Returns \n \nout(N, M[, P]) ndarray \n\nFiltered image (maximum of pixels across all scales).      See also  \nmeijering\n\n\nsato\n\n\nfrangi\n\n  Notes Written by Marc Schrijver (November 2001) Re-Written by D. J. Kroon University of Twente (May 2009) [2] References  \n1  \nNg, C. C., Yap, M. H., Costen, N., & Li, B. (2014,). Automatic wrinkle detection using hybrid Hessian filter. In Asian Conference on Computer Vision (pp. 609-622). Springer International Publishing. DOI:10.1007/978-3-319-16811-1_40  \n2  \nKroon, D. J.: Hessian based Frangi vesselness filter.   \n inverse  \nskimage.filters.inverse(data, impulse_response=None, filter_params={}, max_gain=2, predefined_filter=None) [source]\n \nApply the filter in reverse to the given data.  Parameters \n \ndata(M,N) ndarray \n\nInput data.  \nimpulse_responsecallable f(r, c, **filter_params) \n\nImpulse response of the filter. See LPIFilter2D.__init__.  \nfilter_paramsdict \n\nAdditional keyword parameters to the impulse_response function.  \nmax_gainfloat \n\nLimit the filter gain. Often, the filter contains zeros, which would cause the inverse filter to have infinite gain. High gain causes amplification of artefacts, so a conservative limit is recommended.    Other Parameters \n \npredefined_filterLPIFilter2D \n\nIf you need to apply the same filter multiple times over different images, construct the LPIFilter2D and specify it here.     \n laplace  \nskimage.filters.laplace(image, ksize=3, mask=None) [source]\n \nFind the edges of an image using the Laplace operator.  Parameters \n \nimagendarray \n\nImage to process.  \nksizeint, optional \n\nDefine the size of the discrete Laplacian operator such that it will have a size of (ksize,) * image.ndim.  \nmaskndarray, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutputndarray \n\nThe Laplace edge map.     Notes The Laplacian operator is generated using the function skimage.restoration.uft.laplacian(). \n median  \nskimage.filters.median(image, selem=None, out=None, mode='nearest', cval=0.0, behavior='ndimage') [source]\n \nReturn local median of an image.  Parameters \n \nimagearray-like \n\nInput image.  \nselemndarray, optional \n\nIf behavior=='rank', selem is a 2-D array of 1\u2019s and 0\u2019s. If behavior=='ndimage', selem is a N-D array of 1\u2019s and 0\u2019s with the same number of dimension than image. If None, selem will be a N-D array with 3 elements for each dimension (e.g., vector, square, cube, etc.)  \noutndarray, (same dtype as image), optional \n\nIf None, a new array is allocated.  \nmode{\u2018reflect\u2019, \u2018constant\u2019, \u2018nearest\u2019, \u2018mirror\u2019,\u2019\u2018wrap\u2019}, optional \n\nThe mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019. Default is \u2018nearest\u2019.  New in version 0.15: mode is used when behavior='ndimage'.   \ncvalscalar, optional \n\nValue to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0  New in version 0.15: cval was added in 0.15 is used when behavior='ndimage'.   \nbehavior{\u2018ndimage\u2019, \u2018rank\u2019}, optional \n\nEither to use the old behavior (i.e., < 0.15) or the new behavior. The old behavior will call the skimage.filters.rank.median(). The new behavior will call the scipy.ndimage.median_filter(). Default is \u2018ndimage\u2019.  New in version 0.15: behavior is introduced in 0.15   Changed in version 0.16: Default behavior has been changed from \u2018rank\u2019 to \u2018ndimage\u2019     Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.      See also  \nskimage.filters.rank.median\n\n\nRank-based implementation of the median filtering offering more flexibility with additional parameters but dedicated for unsigned integer images.    Examples >>> from skimage import data\n>>> from skimage.morphology import disk\n>>> from skimage.filters import median\n>>> img = data.camera()\n>>> med = median(img, disk(5))\n \n meijering  \nskimage.filters.meijering(image, sigmas=range(1, 10, 2), alpha=None, black_ridges=True, mode='reflect', cval=0) [source]\n \nFilter an image with the Meijering neuriteness filter. This filter can be used to detect continuous ridges, e.g. neurites, wrinkles, rivers. It can be used to calculate the fraction of the whole image containing such objects. Calculates the eigenvectors of the Hessian to compute the similarity of an image region to neurites, according to the method described in [1].  Parameters \n \nimage(N, M[, \u2026, P]) ndarray \n\nArray with input image data.  \nsigmasiterable of floats, optional \n\nSigmas used as scales of filter  \nalphafloat, optional \n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to deviation from a plate-like structure.  \nblack_ridgesboolean, optional \n\nWhen True (the default), the filter detects black ridges; when False, it detects white ridges.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.    Returns \n \nout(N, M[, \u2026, P]) ndarray \n\nFiltered image (maximum of pixels across all scales).      See also  \nsato\n\n\nfrangi\n\n\nhessian\n\n  References  \n1  \nMeijering, E., Jacob, M., Sarria, J. C., Steiner, P., Hirling, H., Unser, M. (2004). Design and validation of a tool for neurite tracing and analysis in fluorescence microscopy images. Cytometry Part A, 58(2), 167-176. DOI:10.1002/cyto.a.20022   \n prewitt  \nskimage.filters.prewitt(image, mask=None, *, axis=None, mode='reflect', cval=0.0) [source]\n \nFind the edge magnitude using the Prewitt transform.  Parameters \n \nimagearray \n\nThe input image.  \nmaskarray of bool, optional \n\nClip the output image to this mask. (Values where mask=0 will be set to 0.)  \naxisint or sequence of int, optional \n\nCompute the edge filter along this axis. If not provided, the edge magnitude is computed. This is defined as: prw_mag = np.sqrt(sum([prewitt(image, axis=i)**2\n                       for i in range(image.ndim)]) / image.ndim)\n The magnitude is also computed if axis is a sequence.  \nmodestr or sequence of str, optional \n\nThe boundary mode for the convolution. See scipy.ndimage.convolve for a description of the modes. This can be either a single boundary mode or one boundary mode per axis.  \ncvalfloat, optional \n\nWhen mode is 'constant', this is the constant used in values outside the boundary of the image data.    Returns \n \noutputarray of float \n\nThe Prewitt edge map.      See also  \nsobel, scharr\n\n  Notes The edge magnitude depends slightly on edge directions, since the approximation of the gradient operator by the Prewitt operator is not completely rotation invariant. For a better rotation invariance, the Scharr operator should be used. The Sobel operator has a better rotation invariance than the Prewitt operator, but a worse rotation invariance than the Scharr operator. Examples >>> from skimage import data\n>>> from skimage import filters\n>>> camera = data.camera()\n>>> edges = filters.prewitt(camera)\n \n prewitt_h  \nskimage.filters.prewitt_h(image, mask=None) [source]\n \nFind the horizontal edges of an image using the Prewitt transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Prewitt edge map.     Notes We use the following kernel:  1/3   1/3   1/3\n  0     0     0\n-1/3  -1/3  -1/3\n \n prewitt_v  \nskimage.filters.prewitt_v(image, mask=None) [source]\n \nFind the vertical edges of an image using the Prewitt transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Prewitt edge map.     Notes We use the following kernel: 1/3   0  -1/3\n1/3   0  -1/3\n1/3   0  -1/3\n \n rank_order  \nskimage.filters.rank_order(image) [source]\n \nReturn an image of the same shape where each pixel is the index of the pixel value in the ascending order of the unique values of image, aka the rank-order value.  Parameters \n \nimagendarray \n  Returns \n \nlabelsndarray of type np.uint32, of shape image.shape \n\nNew array where each pixel has the rank-order value of the corresponding pixel in image. Pixel values are between 0 and n - 1, where n is the number of distinct unique values in image.  \noriginal_values1-D ndarray \n\nUnique original values of image     Examples >>> a = np.array([[1, 4, 5], [4, 4, 1], [5, 1, 1]])\n>>> a\narray([[1, 4, 5],\n       [4, 4, 1],\n       [5, 1, 1]])\n>>> rank_order(a)\n(array([[0, 1, 2],\n       [1, 1, 0],\n       [2, 0, 0]], dtype=uint32), array([1, 4, 5]))\n>>> b = np.array([-1., 2.5, 3.1, 2.5])\n>>> rank_order(b)\n(array([0, 1, 2, 1], dtype=uint32), array([-1. ,  2.5,  3.1]))\n \n roberts  \nskimage.filters.roberts(image, mask=None) [source]\n \nFind the edge magnitude using Roberts\u2019 cross operator.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Roberts\u2019 Cross edge map.      See also  \nsobel, scharr, prewitt, feature.canny \n  Examples >>> from skimage import data\n>>> camera = data.camera()\n>>> from skimage import filters\n>>> edges = filters.roberts(camera)\n \n roberts_neg_diag  \nskimage.filters.roberts_neg_diag(image, mask=None) [source]\n \nFind the cross edges of an image using the Roberts\u2019 Cross operator. The kernel is applied to the input image to produce separate measurements of the gradient component one orientation.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Robert\u2019s edge map.     Notes We use the following kernel:  0   1\n-1   0\n \n roberts_pos_diag  \nskimage.filters.roberts_pos_diag(image, mask=None) [source]\n \nFind the cross edges of an image using Roberts\u2019 cross operator. The kernel is applied to the input image to produce separate measurements of the gradient component one orientation.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Robert\u2019s edge map.     Notes We use the following kernel: 1   0\n0  -1\n \n sato  \nskimage.filters.sato(image, sigmas=range(1, 10, 2), black_ridges=True, mode=None, cval=0) [source]\n \nFilter an image with the Sato tubeness filter. This filter can be used to detect continuous ridges, e.g. tubes, wrinkles, rivers. It can be used to calculate the fraction of the whole image containing such objects. Defined only for 2-D and 3-D images. Calculates the eigenvectors of the Hessian to compute the similarity of an image region to tubes, according to the method described in [1].  Parameters \n \nimage(N, M[, P]) ndarray \n\nArray with input image data.  \nsigmasiterable of floats, optional \n\nSigmas used as scales of filter.  \nblack_ridgesboolean, optional \n\nWhen True (the default), the filter detects black ridges; when False, it detects white ridges.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.    Returns \n \nout(N, M[, P]) ndarray \n\nFiltered image (maximum of pixels across all scales).      See also  \nmeijering\n\n\nfrangi\n\n\nhessian\n\n  References  \n1  \nSato, Y., Nakajima, S., Shiraga, N., Atsumi, H., Yoshida, S., Koller, T., \u2026, Kikinis, R. (1998). Three-dimensional multi-scale line filter for segmentation and visualization of curvilinear structures in medical images. Medical image analysis, 2(2), 143-168. DOI:10.1016/S1361-8415(98)80009-1   \n scharr  \nskimage.filters.scharr(image, mask=None, *, axis=None, mode='reflect', cval=0.0) [source]\n \nFind the edge magnitude using the Scharr transform.  Parameters \n \nimagearray \n\nThe input image.  \nmaskarray of bool, optional \n\nClip the output image to this mask. (Values where mask=0 will be set to 0.)  \naxisint or sequence of int, optional \n\nCompute the edge filter along this axis. If not provided, the edge magnitude is computed. This is defined as: sch_mag = np.sqrt(sum([scharr(image, axis=i)**2\n                       for i in range(image.ndim)]) / image.ndim)\n The magnitude is also computed if axis is a sequence.  \nmodestr or sequence of str, optional \n\nThe boundary mode for the convolution. See scipy.ndimage.convolve for a description of the modes. This can be either a single boundary mode or one boundary mode per axis.  \ncvalfloat, optional \n\nWhen mode is 'constant', this is the constant used in values outside the boundary of the image data.    Returns \n \noutputarray of float \n\nThe Scharr edge map.      See also  \nsobel, prewitt, canny \n  Notes The Scharr operator has a better rotation invariance than other edge filters such as the Sobel or the Prewitt operators. References  \n1  \nD. Kroon, 2009, Short Paper University Twente, Numerical Optimization of Kernel Based Image Derivatives.  \n2  \nhttps://en.wikipedia.org/wiki/Sobel_operator#Alternative_operators   Examples >>> from skimage import data\n>>> from skimage import filters\n>>> camera = data.camera()\n>>> edges = filters.scharr(camera)\n \n scharr_h  \nskimage.filters.scharr_h(image, mask=None) [source]\n \nFind the horizontal edges of an image using the Scharr transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Scharr edge map.     Notes We use the following kernel:  3   10   3\n 0    0   0\n-3  -10  -3\n References  \n1  \nD. Kroon, 2009, Short Paper University Twente, Numerical Optimization of Kernel Based Image Derivatives.   \n scharr_v  \nskimage.filters.scharr_v(image, mask=None) [source]\n \nFind the vertical edges of an image using the Scharr transform.  Parameters \n \nimage2-D array \n\nImage to process  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Scharr edge map.     Notes We use the following kernel:  3   0   -3\n10   0  -10\n 3   0   -3\n References  \n1  \nD. Kroon, 2009, Short Paper University Twente, Numerical Optimization of Kernel Based Image Derivatives.   \n sobel  \nskimage.filters.sobel(image, mask=None, *, axis=None, mode='reflect', cval=0.0) [source]\n \nFind edges in an image using the Sobel filter.  Parameters \n \nimagearray \n\nThe input image.  \nmaskarray of bool, optional \n\nClip the output image to this mask. (Values where mask=0 will be set to 0.)  \naxisint or sequence of int, optional \n\nCompute the edge filter along this axis. If not provided, the edge magnitude is computed. This is defined as: sobel_mag = np.sqrt(sum([sobel(image, axis=i)**2\n                         for i in range(image.ndim)]) / image.ndim)\n The magnitude is also computed if axis is a sequence.  \nmodestr or sequence of str, optional \n\nThe boundary mode for the convolution. See scipy.ndimage.convolve for a description of the modes. This can be either a single boundary mode or one boundary mode per axis.  \ncvalfloat, optional \n\nWhen mode is 'constant', this is the constant used in values outside the boundary of the image data.    Returns \n \noutputarray of float \n\nThe Sobel edge map.      See also  \nscharr, prewitt, canny \n  References  \n1  \nD. Kroon, 2009, Short Paper University Twente, Numerical Optimization of Kernel Based Image Derivatives.  \n2  \nhttps://en.wikipedia.org/wiki/Sobel_operator   Examples >>> from skimage import data\n>>> from skimage import filters\n>>> camera = data.camera()\n>>> edges = filters.sobel(camera)\n \n Examples using skimage.filters.sobel\n \n  Flood Fill   sobel_h  \nskimage.filters.sobel_h(image, mask=None) [source]\n \nFind the horizontal edges of an image using the Sobel transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Sobel edge map.     Notes We use the following kernel:  1   2   1\n 0   0   0\n-1  -2  -1\n \n sobel_v  \nskimage.filters.sobel_v(image, mask=None) [source]\n \nFind the vertical edges of an image using the Sobel transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Sobel edge map.     Notes We use the following kernel: 1   0  -1\n2   0  -2\n1   0  -1\n \n threshold_isodata  \nskimage.filters.threshold_isodata(image=None, nbins=256, return_all=False, *, hist=None) [source]\n \nReturn threshold value(s) based on ISODATA method. Histogram-based threshold, known as Ridler-Calvard method or inter-means. Threshold values returned satisfy the following equality: threshold = (image[image <= threshold].mean() +\n             image[image > threshold].mean()) / 2.0\n That is, returned thresholds are intensities that separate the image into two groups of pixels, where the threshold intensity is midway between the mean intensities of these groups. For integer images, the above equality holds to within one; for floating- point images, the equality holds to within the histogram bin-width. Either image or hist must be provided. In case hist is given, the actual histogram of the image is ignored.  Parameters \n \nimage(N, M) ndarray, optional \n\nInput image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.  \nreturn_allbool, optional \n\nIf False (default), return only the lowest threshold that satisfies the above equality. If True, return all valid thresholds.  \nhistarray, or 2-tuple of arrays, optional \n\nHistogram to determine the threshold from and a corresponding array of bin center intensities. Alternatively, only the histogram can be passed.    Returns \n \nthresholdfloat or int or array \n\nThreshold value(s).     References  \n1  \nRidler, TW & Calvard, S (1978), \u201cPicture thresholding using an iterative selection method\u201d IEEE Transactions on Systems, Man and Cybernetics 8: 630-632, DOI:10.1109/TSMC.1978.4310039  \n2  \nSezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and Quantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1): 146-165, http://www.busim.ee.boun.edu.tr/~sankur/SankurFolder/Threshold_survey.pdf DOI:10.1117/1.1631315  \n3  \nImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold   Examples >>> from skimage.data import coins\n>>> image = coins()\n>>> thresh = threshold_isodata(image)\n>>> binary = image > thresh\n \n threshold_li  \nskimage.filters.threshold_li(image, *, tolerance=None, initial_guess=None, iter_callback=None) [source]\n \nCompute threshold value by Li\u2019s iterative Minimum Cross Entropy method.  Parameters \n \nimagendarray \n\nInput image.  \ntolerancefloat, optional \n\nFinish the computation when the change in the threshold in an iteration is less than this value. By default, this is half the smallest difference between intensity values in image.  \ninitial_guessfloat or Callable[[array[float]], float], optional \n\nLi\u2019s iterative method uses gradient descent to find the optimal threshold. If the image intensity histogram contains more than two modes (peaks), the gradient descent could get stuck in a local optimum. An initial guess for the iteration can help the algorithm find the globally-optimal threshold. A float value defines a specific start point, while a callable should take in an array of image intensities and return a float value. Example valid callables include numpy.mean (default), lambda arr: numpy.quantile(arr, 0.95), or even skimage.filters.threshold_otsu().  \niter_callbackCallable[[float], Any], optional \n\nA function that will be called on the threshold at every iteration of the algorithm.    Returns \n \nthresholdfloat \n\nUpper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.     References  \n1  \nLi C.H. and Lee C.K. (1993) \u201cMinimum Cross Entropy Thresholding\u201d Pattern Recognition, 26(4): 617-625 DOI:10.1016/0031-3203(93)90115-D  \n2  \nLi C.H. and Tam P.K.S. (1998) \u201cAn Iterative Algorithm for Minimum Cross Entropy Thresholding\u201d Pattern Recognition Letters, 18(8): 771-776 DOI:10.1016/S0167-8655(98)00057-9  \n3  \nSezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and Quantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1): 146-165 DOI:10.1117/1.1631315  \n4  \nImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold   Examples >>> from skimage.data import camera\n>>> image = camera()\n>>> thresh = threshold_li(image)\n>>> binary = image > thresh\n \n threshold_local  \nskimage.filters.threshold_local(image, block_size, method='gaussian', offset=0, mode='reflect', param=None, cval=0) [source]\n \nCompute a threshold mask image based on local pixel neighborhood. Also known as adaptive or dynamic thresholding. The threshold value is the weighted mean for the local neighborhood of a pixel subtracted by a constant. Alternatively the threshold can be determined dynamically by a given function, using the \u2018generic\u2019 method.  Parameters \n \nimage(N, M) ndarray \n\nInput image.  \nblock_sizeint \n\nOdd size of pixel neighborhood which is used to calculate the threshold value (e.g. 3, 5, 7, \u2026, 21, \u2026).  \nmethod{\u2018generic\u2019, \u2018gaussian\u2019, \u2018mean\u2019, \u2018median\u2019}, optional \n\nMethod used to determine adaptive threshold for local neighbourhood in weighted mean image.  \u2018generic\u2019: use custom function (see param parameter) \u2018gaussian\u2019: apply gaussian filter (see param parameter for custom sigma value) \u2018mean\u2019: apply arithmetic mean filter \u2018median\u2019: apply median rank filter  By default the \u2018gaussian\u2019 method is used.  \noffsetfloat, optional \n\nConstant subtracted from weighted mean of neighborhood to calculate the local threshold value. Default offset is 0.  \nmode{\u2018reflect\u2019, \u2018constant\u2019, \u2018nearest\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional \n\nThe mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019. Default is \u2018reflect\u2019.  \nparam{int, function}, optional \n\nEither specify sigma for \u2018gaussian\u2019 method or function object for \u2018generic\u2019 method. This functions takes the flat array of local neighbourhood as a single argument and returns the calculated threshold for the centre pixel.  \ncvalfloat, optional \n\nValue to fill past edges of input if mode is \u2018constant\u2019.    Returns \n \nthreshold(N, M) ndarray \n\nThreshold image. All pixels in the input image higher than the corresponding pixel in the threshold image are considered foreground.     References  \n1  \nhttps://docs.opencv.org/modules/imgproc/doc/miscellaneous_transformations.html?highlight=threshold#adaptivethreshold   Examples >>> from skimage.data import camera\n>>> image = camera()[:50, :50]\n>>> binary_image1 = image > threshold_local(image, 15, 'mean')\n>>> func = lambda arr: arr.mean()\n>>> binary_image2 = image > threshold_local(image, 15, 'generic',\n...                                         param=func)\n \n threshold_mean  \nskimage.filters.threshold_mean(image) [source]\n \nReturn threshold value based on the mean of grayscale values.  Parameters \n \nimage(N, M[, \u2026, P]) ndarray \n\nGrayscale input image.    Returns \n \nthresholdfloat \n\nUpper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.     References  \n1  \nC. A. Glasbey, \u201cAn analysis of histogram-based thresholding algorithms,\u201d CVGIP: Graphical Models and Image Processing, vol. 55, pp. 532-537, 1993. DOI:10.1006/cgip.1993.1040   Examples >>> from skimage.data import camera\n>>> image = camera()\n>>> thresh = threshold_mean(image)\n>>> binary = image > thresh\n \n threshold_minimum  \nskimage.filters.threshold_minimum(image=None, nbins=256, max_iter=10000, *, hist=None) [source]\n \nReturn threshold value based on minimum method. The histogram of the input image is computed if not provided and smoothed until there are only two maxima. Then the minimum in between is the threshold value. Either image or hist must be provided. In case hist is given, the actual histogram of the image is ignored.  Parameters \n \nimage(M, N) ndarray, optional \n\nInput image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.  \nmax_iterint, optional \n\nMaximum number of iterations to smooth the histogram.  \nhistarray, or 2-tuple of arrays, optional \n\nHistogram to determine the threshold from and a corresponding array of bin center intensities. Alternatively, only the histogram can be passed.    Returns \n \nthresholdfloat \n\nUpper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.    Raises \n RuntimeError\n\nIf unable to find two local maxima in the histogram or if the smoothing takes more than 1e4 iterations.     References  \n1  \nC. A. Glasbey, \u201cAn analysis of histogram-based thresholding algorithms,\u201d CVGIP: Graphical Models and Image Processing, vol. 55, pp. 532-537, 1993.  \n2  \nPrewitt, JMS & Mendelsohn, ML (1966), \u201cThe analysis of cell images\u201d, Annals of the New York Academy of Sciences 128: 1035-1053 DOI:10.1111/j.1749-6632.1965.tb11715.x   Examples >>> from skimage.data import camera\n>>> image = camera()\n>>> thresh = threshold_minimum(image)\n>>> binary = image > thresh\n \n threshold_multiotsu  \nskimage.filters.threshold_multiotsu(image, classes=3, nbins=256) [source]\n \nGenerate classes-1 threshold values to divide gray levels in image. The threshold values are chosen to maximize the total sum of pairwise variances between the thresholded graylevel classes. See Notes and [1] for more details.  Parameters \n \nimage(N, M) ndarray \n\nGrayscale input image.  \nclassesint, optional \n\nNumber of classes to be thresholded, i.e. the number of resulting regions.  \nnbinsint, optional \n\nNumber of bins used to calculate the histogram. This value is ignored for integer arrays.    Returns \n \nthresharray \n\nArray containing the threshold values for the desired classes.    Raises \n ValueError\n\nIf image contains less grayscale value then the desired number of classes.     Notes This implementation relies on a Cython function whose complexity is \\(O\\left(\\frac{Ch^{C-1}}{(C-1)!}\\right)\\), where \\(h\\) is the number of histogram bins and \\(C\\) is the number of classes desired. The input image must be grayscale. References  \n1  \nLiao, P-S., Chen, T-S. and Chung, P-C., \u201cA fast algorithm for multilevel thresholding\u201d, Journal of Information Science and Engineering 17 (5): 713-727, 2001. Available at: <https://ftp.iis.sinica.edu.tw/JISE/2001/200109_01.pdf> DOI:10.6688/JISE.2001.17.5.1  \n2  \nTosa, Y., \u201cMulti-Otsu Threshold\u201d, a java plugin for ImageJ. Available at: <http://imagej.net/plugins/download/Multi_OtsuThreshold.java>   Examples >>> from skimage.color import label2rgb\n>>> from skimage import data\n>>> image = data.camera()\n>>> thresholds = threshold_multiotsu(image)\n>>> regions = np.digitize(image, bins=thresholds)\n>>> regions_colorized = label2rgb(regions)\n \n Examples using skimage.filters.threshold_multiotsu\n \n  Multi-Otsu Thresholding  \n\n  Segment human cells (in mitosis)   threshold_niblack  \nskimage.filters.threshold_niblack(image, window_size=15, k=0.2) [source]\n \nApplies Niblack local threshold to an array. A threshold T is calculated for every pixel in the image using the following formula: T = m(x,y) - k * s(x,y)\n where m(x,y) and s(x,y) are the mean and standard deviation of pixel (x,y) neighborhood defined by a rectangular window with size w times w centered around the pixel. k is a configurable parameter that weights the effect of standard deviation.  Parameters \n \nimagendarray \n\nInput image.  \nwindow_sizeint, or iterable of int, optional \n\nWindow size specified as a single odd integer (3, 5, 7, \u2026), or an iterable of length image.ndim containing only odd integers (e.g. (1, 5, 5)).  \nkfloat, optional \n\nValue of parameter k in threshold formula.    Returns \n \nthreshold(N, M) ndarray \n\nThreshold mask. All pixels with an intensity higher than this value are assumed to be foreground.     Notes This algorithm is originally designed for text recognition. The Bradley threshold is a particular case of the Niblack one, being equivalent to >>> from skimage import data\n>>> image = data.page()\n>>> q = 1\n>>> threshold_image = threshold_niblack(image, k=0) * q\n for some value q. By default, Bradley and Roth use q=1. References  \n1  \nW. Niblack, An introduction to Digital Image Processing, Prentice-Hall, 1986.  \n2  \nD. Bradley and G. Roth, \u201cAdaptive thresholding using Integral Image\u201d, Journal of Graphics Tools 12(2), pp. 13-21, 2007. DOI:10.1080/2151237X.2007.10129236   Examples >>> from skimage import data\n>>> image = data.page()\n>>> threshold_image = threshold_niblack(image, window_size=7, k=0.1)\n \n threshold_otsu  \nskimage.filters.threshold_otsu(image=None, nbins=256, *, hist=None) [source]\n \nReturn threshold value based on Otsu\u2019s method. Either image or hist must be provided. If hist is provided, the actual histogram of the image is ignored.  Parameters \n \nimage(N, M) ndarray, optional \n\nGrayscale input image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.  \nhistarray, or 2-tuple of arrays, optional \n\nHistogram from which to determine the threshold, and optionally a corresponding array of bin center intensities. An alternative use of this function is to pass it only hist.    Returns \n \nthresholdfloat \n\nUpper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.     Notes The input image must be grayscale. References  \n1  \nWikipedia, https://en.wikipedia.org/wiki/Otsu\u2019s_Method   Examples >>> from skimage.data import camera\n>>> image = camera()\n>>> thresh = threshold_otsu(image)\n>>> binary = image <= thresh\n \n Examples using skimage.filters.threshold_otsu\n \n  Measure region properties  \n\n  Rank filters   threshold_sauvola  \nskimage.filters.threshold_sauvola(image, window_size=15, k=0.2, r=None) [source]\n \nApplies Sauvola local threshold to an array. Sauvola is a modification of Niblack technique. In the original method a threshold T is calculated for every pixel in the image using the following formula: T = m(x,y) * (1 + k * ((s(x,y) / R) - 1))\n where m(x,y) and s(x,y) are the mean and standard deviation of pixel (x,y) neighborhood defined by a rectangular window with size w times w centered around the pixel. k is a configurable parameter that weights the effect of standard deviation. R is the maximum standard deviation of a greyscale image.  Parameters \n \nimagendarray \n\nInput image.  \nwindow_sizeint, or iterable of int, optional \n\nWindow size specified as a single odd integer (3, 5, 7, \u2026), or an iterable of length image.ndim containing only odd integers (e.g. (1, 5, 5)).  \nkfloat, optional \n\nValue of the positive parameter k.  \nrfloat, optional \n\nValue of R, the dynamic range of standard deviation. If None, set to the half of the image dtype range.    Returns \n \nthreshold(N, M) ndarray \n\nThreshold mask. All pixels with an intensity higher than this value are assumed to be foreground.     Notes This algorithm is originally designed for text recognition. References  \n1  \nJ. Sauvola and M. Pietikainen, \u201cAdaptive document image binarization,\u201d Pattern Recognition 33(2), pp. 225-236, 2000. DOI:10.1016/S0031-3203(99)00055-2   Examples >>> from skimage import data\n>>> image = data.page()\n>>> t_sauvola = threshold_sauvola(image, window_size=15, k=0.2)\n>>> binary_image = image > t_sauvola\n \n threshold_triangle  \nskimage.filters.threshold_triangle(image, nbins=256) [source]\n \nReturn threshold value based on the triangle algorithm.  Parameters \n \nimage(N, M[, \u2026, P]) ndarray \n\nGrayscale input image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.    Returns \n \nthresholdfloat \n\nUpper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.     References  \n1  \nZack, G. W., Rogers, W. E. and Latt, S. A., 1977, Automatic Measurement of Sister Chromatid Exchange Frequency, Journal of Histochemistry and Cytochemistry 25 (7), pp. 741-753 DOI:10.1177/25.7.70454  \n2  \nImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold   Examples >>> from skimage.data import camera\n>>> image = camera()\n>>> thresh = threshold_triangle(image)\n>>> binary = image > thresh\n \n threshold_yen  \nskimage.filters.threshold_yen(image=None, nbins=256, *, hist=None) [source]\n \nReturn threshold value based on Yen\u2019s method. Either image or hist must be provided. In case hist is given, the actual histogram of the image is ignored.  Parameters \n \nimage(N, M) ndarray, optional \n\nInput image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.  \nhistarray, or 2-tuple of arrays, optional \n\nHistogram from which to determine the threshold, and optionally a corresponding array of bin center intensities. An alternative use of this function is to pass it only hist.    Returns \n \nthresholdfloat \n\nUpper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.     References  \n1  \nYen J.C., Chang F.J., and Chang S. (1995) \u201cA New Criterion for Automatic Multilevel Thresholding\u201d IEEE Trans. on Image Processing, 4(3): 370-378. DOI:10.1109/83.366472  \n2  \nSezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and Quantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1): 146-165, DOI:10.1117/1.1631315 http://www.busim.ee.boun.edu.tr/~sankur/SankurFolder/Threshold_survey.pdf  \n3  \nImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold   Examples >>> from skimage.data import camera\n>>> image = camera()\n>>> thresh = threshold_yen(image)\n>>> binary = image <= thresh\n \n try_all_threshold  \nskimage.filters.try_all_threshold(image, figsize=(8, 5), verbose=True) [source]\n \nReturns a figure comparing the outputs of different thresholding methods.  Parameters \n \nimage(N, M) ndarray \n\nInput image.  \nfigsizetuple, optional \n\nFigure size (in inches).  \nverbosebool, optional \n\nPrint function name for each method.    Returns \n \nfig, axtuple \n\nMatplotlib figure and axes.     Notes The following algorithms are used:  isodata li mean minimum otsu triangle yen  Examples >>> from skimage.data import text\n>>> fig, ax = try_all_threshold(text(), figsize=(10, 6), verbose=False)\n \n unsharp_mask  \nskimage.filters.unsharp_mask(image, radius=1.0, amount=1.0, multichannel=False, preserve_range=False) [source]\n \nUnsharp masking filter. The sharp details are identified as the difference between the original image and its blurred version. These details are then scaled, and added back to the original image.  Parameters \n \nimage[P, \u2026, ]M[, N][, C] ndarray \n\nInput image.  \nradiusscalar or sequence of scalars, optional \n\nIf a scalar is given, then its value is used for all dimensions. If sequence is given, then there must be exactly one radius for each dimension except the last dimension for multichannel images. Note that 0 radius means no blurring, and negative values are not allowed.  \namountscalar, optional \n\nThe details will be amplified with this factor. The factor could be 0 or negative. Typically, it is a small positive number, e.g. 1.0.  \nmultichannelbool, optional \n\nIf True, the last image dimension is considered as a color channel, otherwise as spatial. Color channels are processed individually.  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html    Returns \n \noutput[P, \u2026, ]M[, N][, C] ndarray of float \n\nImage with unsharp mask applied.     Notes Unsharp masking is an image sharpening technique. It is a linear image operation, and numerically stable, unlike deconvolution which is an ill-posed problem. Because of this stability, it is often preferred over deconvolution. The main idea is as follows: sharp details are identified as the difference between the original image and its blurred version. These details are added back to the original image after a scaling step: enhanced image = original + amount * (original - blurred) When applying this filter to several color layers independently, color bleeding may occur. More visually pleasing result can be achieved by processing only the brightness/lightness/intensity channel in a suitable color space such as HSV, HSL, YUV, or YCbCr. Unsharp masking is described in most introductory digital image processing books. This implementation is based on [1]. References  \n1  \nMaria Petrou, Costas Petrou \u201cImage Processing: The Fundamentals\u201d, (2010), ed ii., page 357, ISBN 13: 9781119994398 DOI:10.1002/9781119994398  \n2  \nWikipedia. Unsharp masking https://en.wikipedia.org/wiki/Unsharp_masking   Examples >>> array = np.ones(shape=(5,5), dtype=np.uint8)*100\n>>> array[2,2] = 120\n>>> array\narray([[100, 100, 100, 100, 100],\n       [100, 100, 100, 100, 100],\n       [100, 100, 120, 100, 100],\n       [100, 100, 100, 100, 100],\n       [100, 100, 100, 100, 100]], dtype=uint8)\n>>> np.around(unsharp_mask(array, radius=0.5, amount=2),2)\narray([[0.39, 0.39, 0.39, 0.39, 0.39],\n       [0.39, 0.39, 0.38, 0.39, 0.39],\n       [0.39, 0.38, 0.53, 0.38, 0.39],\n       [0.39, 0.39, 0.38, 0.39, 0.39],\n       [0.39, 0.39, 0.39, 0.39, 0.39]])\n >>> array = np.ones(shape=(5,5), dtype=np.int8)*100\n>>> array[2,2] = 127\n>>> np.around(unsharp_mask(array, radius=0.5, amount=2),2)\narray([[0.79, 0.79, 0.79, 0.79, 0.79],\n       [0.79, 0.78, 0.75, 0.78, 0.79],\n       [0.79, 0.75, 1.  , 0.75, 0.79],\n       [0.79, 0.78, 0.75, 0.78, 0.79],\n       [0.79, 0.79, 0.79, 0.79, 0.79]])\n >>> np.around(unsharp_mask(array, radius=0.5, amount=2, preserve_range=True), 2)\narray([[100.  , 100.  ,  99.99, 100.  , 100.  ],\n       [100.  ,  99.39,  95.48,  99.39, 100.  ],\n       [ 99.99,  95.48, 147.59,  95.48,  99.99],\n       [100.  ,  99.39,  95.48,  99.39, 100.  ],\n       [100.  , 100.  ,  99.99, 100.  , 100.  ]])\n \n wiener  \nskimage.filters.wiener(data, impulse_response=None, filter_params={}, K=0.25, predefined_filter=None) [source]\n \nMinimum Mean Square Error (Wiener) inverse filter.  Parameters \n \ndata(M,N) ndarray \n\nInput data.  \nKfloat or (M,N) ndarray \n\nRatio between power spectrum of noise and undegraded image.  \nimpulse_responsecallable f(r, c, **filter_params) \n\nImpulse response of the filter. See LPIFilter2D.__init__.  \nfilter_paramsdict \n\nAdditional keyword parameters to the impulse_response function.    Other Parameters \n \npredefined_filterLPIFilter2D \n\nIf you need to apply the same filter multiple times over different images, construct the LPIFilter2D and specify it here.     \n window  \nskimage.filters.window(window_type, shape, warp_kwargs=None) [source]\n \nReturn an n-dimensional window of a given size and dimensionality.  Parameters \n \nwindow_typestring, float, or tuple \n\nThe type of window to be created. Any window type supported by scipy.signal.get_window is allowed here. See notes below for a current list, or the SciPy documentation for the version of SciPy on your machine.  \nshapetuple of int or int \n\nThe shape of the window along each axis. If an integer is provided, a 1D window is generated.  \nwarp_kwargsdict \n\nKeyword arguments passed to skimage.transform.warp (e.g., warp_kwargs={'order':3} to change interpolation method).    Returns \n \nnd_windowndarray \n\nA window of the specified shape. dtype is np.double.     Notes This function is based on scipy.signal.get_window and thus can access all of the window types available to that function (e.g., \"hann\", \"boxcar\"). Note that certain window types require parameters that have to be supplied with the window name as a tuple (e.g., (\"tukey\", 0.8)). If only a float is supplied, it is interpreted as the beta parameter of the Kaiser window. See https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.windows.get_window.html for more details. Note that this function generates a double precision array of the specified shape and can thus generate very large arrays that consume a large amount of available memory. The approach taken here to create nD windows is to first calculate the Euclidean distance from the center of the intended nD window to each position in the array. That distance is used to sample, with interpolation, from a 1D window returned from scipy.signal.get_window. The method of interpolation can be changed with the order keyword argument passed to skimage.transform.warp. Some coordinates in the output window will be outside of the original signal; these will be filled in with zeros. Window types: - boxcar - triang - blackman - hamming - hann - bartlett - flattop - parzen - bohman - blackmanharris - nuttall - barthann - kaiser (needs beta) - gaussian (needs standard deviation) - general_gaussian (needs power, width) - slepian (needs width) - dpss (needs normalized half-bandwidth) - chebwin (needs attenuation) - exponential (needs decay scale) - tukey (needs taper fraction) References  \n1  \nTwo-dimensional window design, Wikipedia, https://en.wikipedia.org/wiki/Two_dimensional_window_design   Examples Return a Hann window with shape (512, 512): >>> from skimage.filters import window\n>>> w = window('hann', (512, 512))\n Return a Kaiser window with beta parameter of 16 and shape (256, 256, 35): >>> w = window(16, (256, 256, 35))\n Return a Tukey window with an alpha parameter of 0.8 and shape (100, 300): >>> w = window(('tukey', 0.8), (100, 300))\n \n LPIFilter2D  \nclass skimage.filters.LPIFilter2D(impulse_response, **filter_params) [source]\n \nBases: object Linear Position-Invariant Filter (2-dimensional)  \n__init__(impulse_response, **filter_params) [source]\n \n Parameters \n \nimpulse_responsecallable f(r, c, **filter_params) \n\nFunction that yields the impulse response. r and c are 1-dimensional vectors that represent row and column positions, in other words coordinates are (r[0],c[0]),(r[0],c[1]) etc. **filter_params are passed through. In other words, impulse_response would be called like this: >>> def impulse_response(r, c, **filter_params):\n...     pass\n>>>\n>>> r = [0,0,0,1,1,1,2,2,2]\n>>> c = [0,1,2,0,1,2,0,1,2]\n>>> filter_params = {'kw1': 1, 'kw2': 2, 'kw3': 3}\n>>> impulse_response(r, c, **filter_params)\n     Examples Gaussian filter: Use a 1-D gaussian in each direction without normalization coefficients. >>> def filt_func(r, c, sigma = 1):\n...     return np.exp(-np.hypot(r, c)/sigma)\n>>> filter = LPIFilter2D(filt_func)\n \n \n\n"}, {"name": "filters.apply_hysteresis_threshold()", "path": "api/skimage.filters#skimage.filters.apply_hysteresis_threshold", "type": "filters", "text": " \nskimage.filters.apply_hysteresis_threshold(image, low, high) [source]\n \nApply hysteresis thresholding to image. This algorithm finds regions where image is greater than high OR image is greater than low and that region is connected to a region greater than high.  Parameters \n \nimagearray, shape (M,[ N, \u2026, P]) \n\nGrayscale input image.  \nlowfloat, or array of same shape as image \n\nLower threshold.  \nhighfloat, or array of same shape as image \n\nHigher threshold.    Returns \n \nthresholdedarray of bool, same shape as image \n\nArray in which True indicates the locations where image was above the hysteresis threshold.     References  \n1  \nJ. Canny. A computational approach to edge detection. IEEE Transactions on Pattern Analysis and Machine Intelligence. 1986; vol. 8, pp.679-698. DOI:10.1109/TPAMI.1986.4767851   Examples >>> image = np.array([1, 2, 3, 2, 1, 2, 1, 3, 2])\n>>> apply_hysteresis_threshold(image, 1.5, 2.5).astype(int)\narray([0, 1, 1, 1, 0, 0, 0, 1, 1])\n \n"}, {"name": "filters.correlate_sparse()", "path": "api/skimage.filters#skimage.filters.correlate_sparse", "type": "filters", "text": " \nskimage.filters.correlate_sparse(image, kernel, mode='reflect') [source]\n \nCompute valid cross-correlation of padded_array and kernel. This function is fast when kernel is large with many zeros. See scipy.ndimage.correlate for a description of cross-correlation.  Parameters \n \nimagendarray, dtype float, shape (M, N,[ \u2026,] P) \n\nThe input array. If mode is \u2018valid\u2019, this array should already be padded, as a margin of the same shape as kernel will be stripped off.  \nkernelndarray, dtype float shape (Q, R,[ \u2026,] S) \n\nThe kernel to be correlated. Must have the same number of dimensions as padded_array. For high performance, it should be sparse (few nonzero entries).  \nmodestring, optional \n\nSee scipy.ndimage.correlate for valid modes. Additionally, mode \u2018valid\u2019 is accepted, in which case no padding is applied and the result is the result for the smaller image for which the kernel is entirely inside the original data.    Returns \n \nresultarray of float, shape (M, N,[ \u2026,] P) \n\nThe result of cross-correlating image with kernel. If mode \u2018valid\u2019 is used, the resulting shape is (M-Q+1, N-R+1,[ \u2026,] P-S+1).     \n"}, {"name": "filters.difference_of_gaussians()", "path": "api/skimage.filters#skimage.filters.difference_of_gaussians", "type": "filters", "text": " \nskimage.filters.difference_of_gaussians(image, low_sigma, high_sigma=None, *, mode='nearest', cval=0, multichannel=False, truncate=4.0) [source]\n \nFind features between low_sigma and high_sigma in size. This function uses the Difference of Gaussians method for applying band-pass filters to multi-dimensional arrays. The input array is blurred with two Gaussian kernels of differing sigmas to produce two intermediate, filtered images. The more-blurred image is then subtracted from the less-blurred image. The final output image will therefore have had high-frequency components attenuated by the smaller-sigma Gaussian, and low frequency components will have been removed due to their presence in the more-blurred intermediate.  Parameters \n \nimagendarray \n\nInput array to filter.  \nlow_sigmascalar or sequence of scalars \n\nStandard deviation(s) for the Gaussian kernel with the smaller sigmas across all axes. The standard deviations are given for each axis as a sequence, or as a single number, in which case the single number is used as the standard deviation value for all axes.  \nhigh_sigmascalar or sequence of scalars, optional (default is None) \n\nStandard deviation(s) for the Gaussian kernel with the larger sigmas across all axes. The standard deviations are given for each axis as a sequence, or as a single number, in which case the single number is used as the standard deviation value for all axes. If None is given (default), sigmas for all axes are calculated as 1.6 * low_sigma.  \nmode{\u2018reflect\u2019, \u2018constant\u2019, \u2018nearest\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional \n\nThe mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019. Default is \u2018nearest\u2019.  \ncvalscalar, optional \n\nValue to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0  \nmultichannelbool, optional (default: False) \n\nWhether the last axis of the image is to be interpreted as multiple channels. If True, each channel is filtered separately (channels are not mixed together).  \ntruncatefloat, optional (default is 4.0) \n\nTruncate the filter at this many standard deviations.    Returns \n \nfiltered_imagendarray \n\nthe filtered array.      See also  \nskimage.feature.blog_dog \n  Notes This function will subtract an array filtered with a Gaussian kernel with sigmas given by high_sigma from an array filtered with a Gaussian kernel with sigmas provided by low_sigma. The values for high_sigma must always be greater than or equal to the corresponding values in low_sigma, or a ValueError will be raised. When high_sigma is none, the values for high_sigma will be calculated as 1.6x the corresponding values in low_sigma. This ratio was originally proposed by Marr and Hildreth (1980) [1] and is commonly used when approximating the inverted Laplacian of Gaussian, which is used in edge and blob detection. Input image is converted according to the conventions of img_as_float. Except for sigma values, all parameters are used for both filters. References  \n1  \nMarr, D. and Hildreth, E. Theory of Edge Detection. Proc. R. Soc. Lond. Series B 207, 187-217 (1980). https://doi.org/10.1098/rspb.1980.0020   Examples Apply a simple Difference of Gaussians filter to a color image: >>> from skimage.data import astronaut\n>>> from skimage.filters import difference_of_gaussians\n>>> filtered_image = difference_of_gaussians(astronaut(), 2, 10,\n...                                          multichannel=True)\n Apply a Laplacian of Gaussian filter as approximated by the Difference of Gaussians filter: >>> filtered_image = difference_of_gaussians(astronaut(), 2,\n...                                          multichannel=True)\n Apply a Difference of Gaussians filter to a grayscale image using different sigma values for each axis: >>> from skimage.data import camera\n>>> filtered_image = difference_of_gaussians(camera(), (2,5), (3,20))\n \n"}, {"name": "filters.farid()", "path": "api/skimage.filters#skimage.filters.farid", "type": "filters", "text": " \nskimage.filters.farid(image, *, mask=None) [source]\n \nFind the edge magnitude using the Farid transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Farid edge map.      See also  \nsobel, prewitt, canny \n  Notes Take the square root of the sum of the squares of the horizontal and vertical derivatives to get a magnitude that is somewhat insensitive to direction. Similar to the Scharr operator, this operator is designed with a rotation invariance constraint. References  \n1  \nFarid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional signals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004. DOI:10.1109/TIP.2004.823819  \n2  \nWikipedia, \u201cFarid and Simoncelli Derivatives.\u201d Available at: <https://en.wikipedia.org/wiki/Image_derivatives#Farid_and_Simoncelli_Derivatives>   Examples >>> from skimage import data\n>>> camera = data.camera()\n>>> from skimage import filters\n>>> edges = filters.farid(camera)\n \n"}, {"name": "filters.farid_h()", "path": "api/skimage.filters#skimage.filters.farid_h", "type": "filters", "text": " \nskimage.filters.farid_h(image, *, mask=None) [source]\n \nFind the horizontal edges of an image using the Farid transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Farid edge map.     Notes The kernel was constructed using the 5-tap weights from [1]. References  \n1  \nFarid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional signals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004. DOI:10.1109/TIP.2004.823819  \n2  \nFarid, H. and Simoncelli, E. P. \u201cOptimally rotation-equivariant directional derivative kernels\u201d, In: 7th International Conference on Computer Analysis of Images and Patterns, Kiel, Germany. Sep, 1997.   \n"}, {"name": "filters.farid_v()", "path": "api/skimage.filters#skimage.filters.farid_v", "type": "filters", "text": " \nskimage.filters.farid_v(image, *, mask=None) [source]\n \nFind the vertical edges of an image using the Farid transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Farid edge map.     Notes The kernel was constructed using the 5-tap weights from [1]. References  \n1  \nFarid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional signals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004. DOI:10.1109/TIP.2004.823819   \n"}, {"name": "filters.frangi()", "path": "api/skimage.filters#skimage.filters.frangi", "type": "filters", "text": " \nskimage.filters.frangi(image, sigmas=range(1, 10, 2), scale_range=None, scale_step=None, alpha=0.5, beta=0.5, gamma=15, black_ridges=True, mode='reflect', cval=0) [source]\n \nFilter an image with the Frangi vesselness filter. This filter can be used to detect continuous ridges, e.g. vessels, wrinkles, rivers. It can be used to calculate the fraction of the whole image containing such objects. Defined only for 2-D and 3-D images. Calculates the eigenvectors of the Hessian to compute the similarity of an image region to vessels, according to the method described in [1].  Parameters \n \nimage(N, M[, P]) ndarray \n\nArray with input image data.  \nsigmasiterable of floats, optional \n\nSigmas used as scales of filter, i.e., np.arange(scale_range[0], scale_range[1], scale_step)  \nscale_range2-tuple of floats, optional \n\nThe range of sigmas used.  \nscale_stepfloat, optional \n\nStep size between sigmas.  \nalphafloat, optional \n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to deviation from a plate-like structure.  \nbetafloat, optional \n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to deviation from a blob-like structure.  \ngammafloat, optional \n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to areas of high variance/texture/structure.  \nblack_ridgesboolean, optional \n\nWhen True (the default), the filter detects black ridges; when False, it detects white ridges.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.    Returns \n \nout(N, M[, P]) ndarray \n\nFiltered image (maximum of pixels across all scales).      See also  \nmeijering\n\n\nsato\n\n\nhessian\n\n  Notes Written by Marc Schrijver, November 2001 Re-Written by D. J. Kroon, University of Twente, May 2009, [2] Adoption of 3D version from D. G. Ellis, Januar 20017, [3] References  \n1  \nFrangi, A. F., Niessen, W. J., Vincken, K. L., & Viergever, M. A. (1998,). Multiscale vessel enhancement filtering. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 130-137). Springer Berlin Heidelberg. DOI:10.1007/BFb0056195  \n2  \nKroon, D. J.: Hessian based Frangi vesselness filter.  \n3  \nEllis, D. G.: https://github.com/ellisdg/frangi3d/tree/master/frangi   \n"}, {"name": "filters.gabor()", "path": "api/skimage.filters#skimage.filters.gabor", "type": "filters", "text": " \nskimage.filters.gabor(image, frequency, theta=0, bandwidth=1, sigma_x=None, sigma_y=None, n_stds=3, offset=0, mode='reflect', cval=0) [source]\n \nReturn real and imaginary responses to Gabor filter. The real and imaginary parts of the Gabor filter kernel are applied to the image and the response is returned as a pair of arrays. Gabor filter is a linear filter with a Gaussian kernel which is modulated by a sinusoidal plane wave. Frequency and orientation representations of the Gabor filter are similar to those of the human visual system. Gabor filter banks are commonly used in computer vision and image processing. They are especially suitable for edge detection and texture classification.  Parameters \n \nimage2-D array \n\nInput image.  \nfrequencyfloat \n\nSpatial frequency of the harmonic function. Specified in pixels.  \nthetafloat, optional \n\nOrientation in radians. If 0, the harmonic is in the x-direction.  \nbandwidthfloat, optional \n\nThe bandwidth captured by the filter. For fixed bandwidth, sigma_x and sigma_y will decrease with increasing frequency. This value is ignored if sigma_x and sigma_y are set by the user.  \nsigma_x, sigma_yfloat, optional \n\nStandard deviation in x- and y-directions. These directions apply to the kernel before rotation. If theta = pi/2, then the kernel is rotated 90 degrees so that sigma_x controls the vertical direction.  \nn_stdsscalar, optional \n\nThe linear size of the kernel is n_stds (3 by default) standard deviations.  \noffsetfloat, optional \n\nPhase offset of harmonic function in radians.  \nmode{\u2018constant\u2019, \u2018nearest\u2019, \u2018reflect\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional \n\nMode used to convolve image with a kernel, passed to ndi.convolve  \ncvalscalar, optional \n\nValue to fill past edges of input if mode of convolution is \u2018constant\u2019. The parameter is passed to ndi.convolve.    Returns \n \nreal, imagarrays \n\nFiltered images using the real and imaginary parts of the Gabor filter kernel. Images are of the same dimensions as the input one.     References  \n1  \nhttps://en.wikipedia.org/wiki/Gabor_filter  \n2  \nhttps://web.archive.org/web/20180127125930/http://mplab.ucsd.edu/tutorials/gabor.pdf   Examples >>> from skimage.filters import gabor\n>>> from skimage import data, io\n>>> from matplotlib import pyplot as plt  \n >>> image = data.coins()\n>>> # detecting edges in a coin image\n>>> filt_real, filt_imag = gabor(image, frequency=0.6)\n>>> plt.figure()            \n>>> io.imshow(filt_real)    \n>>> io.show()               \n >>> # less sensitivity to finer details with the lower frequency kernel\n>>> filt_real, filt_imag = gabor(image, frequency=0.1)\n>>> plt.figure()            \n>>> io.imshow(filt_real)    \n>>> io.show()               \n \n"}, {"name": "filters.gabor_kernel()", "path": "api/skimage.filters#skimage.filters.gabor_kernel", "type": "filters", "text": " \nskimage.filters.gabor_kernel(frequency, theta=0, bandwidth=1, sigma_x=None, sigma_y=None, n_stds=3, offset=0) [source]\n \nReturn complex 2D Gabor filter kernel. Gabor kernel is a Gaussian kernel modulated by a complex harmonic function. Harmonic function consists of an imaginary sine function and a real cosine function. Spatial frequency is inversely proportional to the wavelength of the harmonic and to the standard deviation of a Gaussian kernel. The bandwidth is also inversely proportional to the standard deviation.  Parameters \n \nfrequencyfloat \n\nSpatial frequency of the harmonic function. Specified in pixels.  \nthetafloat, optional \n\nOrientation in radians. If 0, the harmonic is in the x-direction.  \nbandwidthfloat, optional \n\nThe bandwidth captured by the filter. For fixed bandwidth, sigma_x and sigma_y will decrease with increasing frequency. This value is ignored if sigma_x and sigma_y are set by the user.  \nsigma_x, sigma_yfloat, optional \n\nStandard deviation in x- and y-directions. These directions apply to the kernel before rotation. If theta = pi/2, then the kernel is rotated 90 degrees so that sigma_x controls the vertical direction.  \nn_stdsscalar, optional \n\nThe linear size of the kernel is n_stds (3 by default) standard deviations  \noffsetfloat, optional \n\nPhase offset of harmonic function in radians.    Returns \n \ngcomplex array \n\nComplex filter kernel.     References  \n1  \nhttps://en.wikipedia.org/wiki/Gabor_filter  \n2  \nhttps://web.archive.org/web/20180127125930/http://mplab.ucsd.edu/tutorials/gabor.pdf   Examples >>> from skimage.filters import gabor_kernel\n>>> from skimage import io\n>>> from matplotlib import pyplot as plt  \n >>> gk = gabor_kernel(frequency=0.2)\n>>> plt.figure()        \n>>> io.imshow(gk.real)  \n>>> io.show()           \n >>> # more ripples (equivalent to increasing the size of the\n>>> # Gaussian spread)\n>>> gk = gabor_kernel(frequency=0.2, bandwidth=0.1)\n>>> plt.figure()        \n>>> io.imshow(gk.real)  \n>>> io.show()           \n \n"}, {"name": "filters.gaussian()", "path": "api/skimage.filters#skimage.filters.gaussian", "type": "filters", "text": " \nskimage.filters.gaussian(image, sigma=1, output=None, mode='nearest', cval=0, multichannel=None, preserve_range=False, truncate=4.0) [source]\n \nMulti-dimensional Gaussian filter.  Parameters \n \nimagearray-like \n\nInput image (grayscale or color) to filter.  \nsigmascalar or sequence of scalars, optional \n\nStandard deviation for Gaussian kernel. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.  \noutputarray, optional \n\nThe output parameter passes an array in which to store the filter output.  \nmode{\u2018reflect\u2019, \u2018constant\u2019, \u2018nearest\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional \n\nThe mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019. Default is \u2018nearest\u2019.  \ncvalscalar, optional \n\nValue to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0  \nmultichannelbool, optional (default: None) \n\nWhether the last axis of the image is to be interpreted as multiple channels. If True, each channel is filtered separately (channels are not mixed together). Only 3 channels are supported. If None, the function will attempt to guess this, and raise a warning if ambiguous, when the array has shape (M, N, 3).  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html  \ntruncatefloat, optional \n\nTruncate the filter at this many standard deviations.    Returns \n \nfiltered_imagendarray \n\nthe filtered array     Notes This function is a wrapper around scipy.ndi.gaussian_filter(). Integer arrays are converted to float. The output should be floating point data type since gaussian converts to float provided image. If output is not provided, another array will be allocated and returned as the result. The multi-dimensional filter is implemented as a sequence of one-dimensional convolution filters. The intermediate arrays are stored in the same data type as the output. Therefore, for output types with a limited precision, the results may be imprecise because intermediate results may be stored with insufficient precision. Examples >>> a = np.zeros((3, 3))\n>>> a[1, 1] = 1\n>>> a\narray([[0., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 0.]])\n>>> gaussian(a, sigma=0.4)  # mild smoothing\narray([[0.00163116, 0.03712502, 0.00163116],\n       [0.03712502, 0.84496158, 0.03712502],\n       [0.00163116, 0.03712502, 0.00163116]])\n>>> gaussian(a, sigma=1)  # more smoothing\narray([[0.05855018, 0.09653293, 0.05855018],\n       [0.09653293, 0.15915589, 0.09653293],\n       [0.05855018, 0.09653293, 0.05855018]])\n>>> # Several modes are possible for handling boundaries\n>>> gaussian(a, sigma=1, mode='reflect')\narray([[0.08767308, 0.12075024, 0.08767308],\n       [0.12075024, 0.16630671, 0.12075024],\n       [0.08767308, 0.12075024, 0.08767308]])\n>>> # For RGB images, each is filtered separately\n>>> from skimage.data import astronaut\n>>> image = astronaut()\n>>> filtered_img = gaussian(image, sigma=1, multichannel=True)\n \n"}, {"name": "filters.hessian()", "path": "api/skimage.filters#skimage.filters.hessian", "type": "filters", "text": " \nskimage.filters.hessian(image, sigmas=range(1, 10, 2), scale_range=None, scale_step=None, alpha=0.5, beta=0.5, gamma=15, black_ridges=True, mode=None, cval=0) [source]\n \nFilter an image with the Hybrid Hessian filter. This filter can be used to detect continuous edges, e.g. vessels, wrinkles, rivers. It can be used to calculate the fraction of the whole image containing such objects. Defined only for 2-D and 3-D images. Almost equal to Frangi filter, but uses alternative method of smoothing. Refer to [1] to find the differences between Frangi and Hessian filters.  Parameters \n \nimage(N, M[, P]) ndarray \n\nArray with input image data.  \nsigmasiterable of floats, optional \n\nSigmas used as scales of filter, i.e., np.arange(scale_range[0], scale_range[1], scale_step)  \nscale_range2-tuple of floats, optional \n\nThe range of sigmas used.  \nscale_stepfloat, optional \n\nStep size between sigmas.  \nbetafloat, optional \n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to deviation from a blob-like structure.  \ngammafloat, optional \n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to areas of high variance/texture/structure.  \nblack_ridgesboolean, optional \n\nWhen True (the default), the filter detects black ridges; when False, it detects white ridges.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.    Returns \n \nout(N, M[, P]) ndarray \n\nFiltered image (maximum of pixels across all scales).      See also  \nmeijering\n\n\nsato\n\n\nfrangi\n\n  Notes Written by Marc Schrijver (November 2001) Re-Written by D. J. Kroon University of Twente (May 2009) [2] References  \n1  \nNg, C. C., Yap, M. H., Costen, N., & Li, B. (2014,). Automatic wrinkle detection using hybrid Hessian filter. In Asian Conference on Computer Vision (pp. 609-622). Springer International Publishing. DOI:10.1007/978-3-319-16811-1_40  \n2  \nKroon, D. J.: Hessian based Frangi vesselness filter.   \n"}, {"name": "filters.inverse()", "path": "api/skimage.filters#skimage.filters.inverse", "type": "filters", "text": " \nskimage.filters.inverse(data, impulse_response=None, filter_params={}, max_gain=2, predefined_filter=None) [source]\n \nApply the filter in reverse to the given data.  Parameters \n \ndata(M,N) ndarray \n\nInput data.  \nimpulse_responsecallable f(r, c, **filter_params) \n\nImpulse response of the filter. See LPIFilter2D.__init__.  \nfilter_paramsdict \n\nAdditional keyword parameters to the impulse_response function.  \nmax_gainfloat \n\nLimit the filter gain. Often, the filter contains zeros, which would cause the inverse filter to have infinite gain. High gain causes amplification of artefacts, so a conservative limit is recommended.    Other Parameters \n \npredefined_filterLPIFilter2D \n\nIf you need to apply the same filter multiple times over different images, construct the LPIFilter2D and specify it here.     \n"}, {"name": "filters.laplace()", "path": "api/skimage.filters#skimage.filters.laplace", "type": "filters", "text": " \nskimage.filters.laplace(image, ksize=3, mask=None) [source]\n \nFind the edges of an image using the Laplace operator.  Parameters \n \nimagendarray \n\nImage to process.  \nksizeint, optional \n\nDefine the size of the discrete Laplacian operator such that it will have a size of (ksize,) * image.ndim.  \nmaskndarray, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutputndarray \n\nThe Laplace edge map.     Notes The Laplacian operator is generated using the function skimage.restoration.uft.laplacian(). \n"}, {"name": "filters.LPIFilter2D", "path": "api/skimage.filters#skimage.filters.LPIFilter2D", "type": "filters", "text": " \nclass skimage.filters.LPIFilter2D(impulse_response, **filter_params) [source]\n \nBases: object Linear Position-Invariant Filter (2-dimensional)  \n__init__(impulse_response, **filter_params) [source]\n \n Parameters \n \nimpulse_responsecallable f(r, c, **filter_params) \n\nFunction that yields the impulse response. r and c are 1-dimensional vectors that represent row and column positions, in other words coordinates are (r[0],c[0]),(r[0],c[1]) etc. **filter_params are passed through. In other words, impulse_response would be called like this: >>> def impulse_response(r, c, **filter_params):\n...     pass\n>>>\n>>> r = [0,0,0,1,1,1,2,2,2]\n>>> c = [0,1,2,0,1,2,0,1,2]\n>>> filter_params = {'kw1': 1, 'kw2': 2, 'kw3': 3}\n>>> impulse_response(r, c, **filter_params)\n     Examples Gaussian filter: Use a 1-D gaussian in each direction without normalization coefficients. >>> def filt_func(r, c, sigma = 1):\n...     return np.exp(-np.hypot(r, c)/sigma)\n>>> filter = LPIFilter2D(filt_func)\n \n \n"}, {"name": "filters.LPIFilter2D.__init__()", "path": "api/skimage.filters#skimage.filters.LPIFilter2D.__init__", "type": "filters", "text": " \n__init__(impulse_response, **filter_params) [source]\n \n Parameters \n \nimpulse_responsecallable f(r, c, **filter_params) \n\nFunction that yields the impulse response. r and c are 1-dimensional vectors that represent row and column positions, in other words coordinates are (r[0],c[0]),(r[0],c[1]) etc. **filter_params are passed through. In other words, impulse_response would be called like this: >>> def impulse_response(r, c, **filter_params):\n...     pass\n>>>\n>>> r = [0,0,0,1,1,1,2,2,2]\n>>> c = [0,1,2,0,1,2,0,1,2]\n>>> filter_params = {'kw1': 1, 'kw2': 2, 'kw3': 3}\n>>> impulse_response(r, c, **filter_params)\n     Examples Gaussian filter: Use a 1-D gaussian in each direction without normalization coefficients. >>> def filt_func(r, c, sigma = 1):\n...     return np.exp(-np.hypot(r, c)/sigma)\n>>> filter = LPIFilter2D(filt_func)\n \n"}, {"name": "filters.median()", "path": "api/skimage.filters#skimage.filters.median", "type": "filters", "text": " \nskimage.filters.median(image, selem=None, out=None, mode='nearest', cval=0.0, behavior='ndimage') [source]\n \nReturn local median of an image.  Parameters \n \nimagearray-like \n\nInput image.  \nselemndarray, optional \n\nIf behavior=='rank', selem is a 2-D array of 1\u2019s and 0\u2019s. If behavior=='ndimage', selem is a N-D array of 1\u2019s and 0\u2019s with the same number of dimension than image. If None, selem will be a N-D array with 3 elements for each dimension (e.g., vector, square, cube, etc.)  \noutndarray, (same dtype as image), optional \n\nIf None, a new array is allocated.  \nmode{\u2018reflect\u2019, \u2018constant\u2019, \u2018nearest\u2019, \u2018mirror\u2019,\u2019\u2018wrap\u2019}, optional \n\nThe mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019. Default is \u2018nearest\u2019.  New in version 0.15: mode is used when behavior='ndimage'.   \ncvalscalar, optional \n\nValue to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0  New in version 0.15: cval was added in 0.15 is used when behavior='ndimage'.   \nbehavior{\u2018ndimage\u2019, \u2018rank\u2019}, optional \n\nEither to use the old behavior (i.e., < 0.15) or the new behavior. The old behavior will call the skimage.filters.rank.median(). The new behavior will call the scipy.ndimage.median_filter(). Default is \u2018ndimage\u2019.  New in version 0.15: behavior is introduced in 0.15   Changed in version 0.16: Default behavior has been changed from \u2018rank\u2019 to \u2018ndimage\u2019     Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.      See also  \nskimage.filters.rank.median\n\n\nRank-based implementation of the median filtering offering more flexibility with additional parameters but dedicated for unsigned integer images.    Examples >>> from skimage import data\n>>> from skimage.morphology import disk\n>>> from skimage.filters import median\n>>> img = data.camera()\n>>> med = median(img, disk(5))\n \n"}, {"name": "filters.meijering()", "path": "api/skimage.filters#skimage.filters.meijering", "type": "filters", "text": " \nskimage.filters.meijering(image, sigmas=range(1, 10, 2), alpha=None, black_ridges=True, mode='reflect', cval=0) [source]\n \nFilter an image with the Meijering neuriteness filter. This filter can be used to detect continuous ridges, e.g. neurites, wrinkles, rivers. It can be used to calculate the fraction of the whole image containing such objects. Calculates the eigenvectors of the Hessian to compute the similarity of an image region to neurites, according to the method described in [1].  Parameters \n \nimage(N, M[, \u2026, P]) ndarray \n\nArray with input image data.  \nsigmasiterable of floats, optional \n\nSigmas used as scales of filter  \nalphafloat, optional \n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to deviation from a plate-like structure.  \nblack_ridgesboolean, optional \n\nWhen True (the default), the filter detects black ridges; when False, it detects white ridges.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.    Returns \n \nout(N, M[, \u2026, P]) ndarray \n\nFiltered image (maximum of pixels across all scales).      See also  \nsato\n\n\nfrangi\n\n\nhessian\n\n  References  \n1  \nMeijering, E., Jacob, M., Sarria, J. C., Steiner, P., Hirling, H., Unser, M. (2004). Design and validation of a tool for neurite tracing and analysis in fluorescence microscopy images. Cytometry Part A, 58(2), 167-176. DOI:10.1002/cyto.a.20022   \n"}, {"name": "filters.prewitt()", "path": "api/skimage.filters#skimage.filters.prewitt", "type": "filters", "text": " \nskimage.filters.prewitt(image, mask=None, *, axis=None, mode='reflect', cval=0.0) [source]\n \nFind the edge magnitude using the Prewitt transform.  Parameters \n \nimagearray \n\nThe input image.  \nmaskarray of bool, optional \n\nClip the output image to this mask. (Values where mask=0 will be set to 0.)  \naxisint or sequence of int, optional \n\nCompute the edge filter along this axis. If not provided, the edge magnitude is computed. This is defined as: prw_mag = np.sqrt(sum([prewitt(image, axis=i)**2\n                       for i in range(image.ndim)]) / image.ndim)\n The magnitude is also computed if axis is a sequence.  \nmodestr or sequence of str, optional \n\nThe boundary mode for the convolution. See scipy.ndimage.convolve for a description of the modes. This can be either a single boundary mode or one boundary mode per axis.  \ncvalfloat, optional \n\nWhen mode is 'constant', this is the constant used in values outside the boundary of the image data.    Returns \n \noutputarray of float \n\nThe Prewitt edge map.      See also  \nsobel, scharr\n\n  Notes The edge magnitude depends slightly on edge directions, since the approximation of the gradient operator by the Prewitt operator is not completely rotation invariant. For a better rotation invariance, the Scharr operator should be used. The Sobel operator has a better rotation invariance than the Prewitt operator, but a worse rotation invariance than the Scharr operator. Examples >>> from skimage import data\n>>> from skimage import filters\n>>> camera = data.camera()\n>>> edges = filters.prewitt(camera)\n \n"}, {"name": "filters.prewitt_h()", "path": "api/skimage.filters#skimage.filters.prewitt_h", "type": "filters", "text": " \nskimage.filters.prewitt_h(image, mask=None) [source]\n \nFind the horizontal edges of an image using the Prewitt transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Prewitt edge map.     Notes We use the following kernel:  1/3   1/3   1/3\n  0     0     0\n-1/3  -1/3  -1/3\n \n"}, {"name": "filters.prewitt_v()", "path": "api/skimage.filters#skimage.filters.prewitt_v", "type": "filters", "text": " \nskimage.filters.prewitt_v(image, mask=None) [source]\n \nFind the vertical edges of an image using the Prewitt transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Prewitt edge map.     Notes We use the following kernel: 1/3   0  -1/3\n1/3   0  -1/3\n1/3   0  -1/3\n \n"}, {"name": "filters.rank", "path": "api/skimage.filters.rank", "type": "filters", "text": "Module: filters.rank  \nskimage.filters.rank.autolevel(image, selem) Auto-level image using local histogram.  \nskimage.filters.rank.autolevel_percentile(\u2026) Return greyscale local autolevel of an image.  \nskimage.filters.rank.bottomhat(image, selem) Local bottom-hat of an image.  \nskimage.filters.rank.enhance_contrast(image, \u2026) Enhance contrast of an image.  \nskimage.filters.rank.enhance_contrast_percentile(\u2026) Enhance contrast of an image.  \nskimage.filters.rank.entropy(image, selem[, \u2026]) Local entropy.  \nskimage.filters.rank.equalize(image, selem) Equalize image using local histogram.  \nskimage.filters.rank.geometric_mean(image, selem) Return local geometric mean of an image.  \nskimage.filters.rank.gradient(image, selem) Return local gradient of an image (i.e.  \nskimage.filters.rank.gradient_percentile(\u2026) Return local gradient of an image (i.e.  \nskimage.filters.rank.majority(image, selem, *) Majority filter assign to each pixel the most occuring value within its neighborhood.  \nskimage.filters.rank.maximum(image, selem[, \u2026]) Return local maximum of an image.  \nskimage.filters.rank.mean(image, selem[, \u2026]) Return local mean of an image.  \nskimage.filters.rank.mean_bilateral(image, selem) Apply a flat kernel bilateral filter.  \nskimage.filters.rank.mean_percentile(image, \u2026) Return local mean of an image.  \nskimage.filters.rank.median(image[, selem, \u2026]) Return local median of an image.  \nskimage.filters.rank.minimum(image, selem[, \u2026]) Return local minimum of an image.  \nskimage.filters.rank.modal(image, selem[, \u2026]) Return local mode of an image.  \nskimage.filters.rank.noise_filter(image, selem) Noise feature.  \nskimage.filters.rank.otsu(image, selem[, \u2026]) Local Otsu\u2019s threshold value for each pixel.  \nskimage.filters.rank.percentile(image, selem) Return local percentile of an image.  \nskimage.filters.rank.pop(image, selem[, \u2026]) Return the local number (population) of pixels.  \nskimage.filters.rank.pop_bilateral(image, selem) Return the local number (population) of pixels.  \nskimage.filters.rank.pop_percentile(image, selem) Return the local number (population) of pixels.  \nskimage.filters.rank.subtract_mean(image, selem) Return image subtracted from its local mean.  \nskimage.filters.rank.subtract_mean_percentile(\u2026) Return image subtracted from its local mean.  \nskimage.filters.rank.sum(image, selem[, \u2026]) Return the local sum of pixels.  \nskimage.filters.rank.sum_bilateral(image, selem) Apply a flat kernel bilateral filter.  \nskimage.filters.rank.sum_percentile(image, selem) Return the local sum of pixels.  \nskimage.filters.rank.threshold(image, selem) Local threshold of an image.  \nskimage.filters.rank.threshold_percentile(\u2026) Local threshold of an image.  \nskimage.filters.rank.tophat(image, selem[, \u2026]) Local top-hat of an image.  \nskimage.filters.rank.windowed_histogram(\u2026) Normalized sliding window histogram   autolevel  \nskimage.filters.rank.autolevel(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nAuto-level image using local histogram. This filter locally stretches the histogram of gray values to cover the entire range of values from \u201cwhite\u201d to \u201cblack\u201d.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import autolevel\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> auto = autolevel(img, disk(5))\n>>> auto_vol = autolevel(volume, ball(5))\n \n Examples using skimage.filters.rank.autolevel\n \n  Rank filters   autolevel_percentile  \nskimage.filters.rank.autolevel_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nReturn greyscale local autolevel of an image. This filter locally stretches the histogram of greyvalues to cover the entire range of values from \u201cwhite\u201d to \u201cblack\u201d. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n Examples using skimage.filters.rank.autolevel_percentile\n \n  Rank filters   bottomhat  \nskimage.filters.rank.bottomhat(image, selem, out=None, mask=None, shift_x=False, shift_y=False) [source]\n \nLocal bottom-hat of an image. This filter computes the morphological closing of the image and then subtracts the result from the original image.  Parameters \n \nimage2-D array (integer or float) \n\nInput image.  \nselem2-D array (integer or float) \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (integer or float), optional \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint, optional \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.    Warns \n Deprecated:\n\n New in version 0.17.  This function is deprecated and will be removed in scikit-image 0.19. This filter was misnamed and we believe that the usefulness is narrow.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk\n>>> from skimage.filters.rank import bottomhat\n>>> img = data.camera()\n>>> out = bottomhat(img, disk(5))  \n \n enhance_contrast  \nskimage.filters.rank.enhance_contrast(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nEnhance contrast of an image. This replaces each pixel by the local maximum if the pixel gray value is closer to the local maximum than the local minimum. Otherwise it is replaced by the local minimum.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image     Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import enhance_contrast\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = enhance_contrast(img, disk(5))\n>>> out_vol = enhance_contrast(volume, ball(5))\n \n Examples using skimage.filters.rank.enhance_contrast\n \n  Rank filters   enhance_contrast_percentile  \nskimage.filters.rank.enhance_contrast_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nEnhance contrast of an image. This replaces each pixel by the local maximum if the pixel greyvalue is closer to the local maximum than the local minimum. Otherwise it is replaced by the local minimum. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n Examples using skimage.filters.rank.enhance_contrast_percentile\n \n  Rank filters   entropy  \nskimage.filters.rank.entropy(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nLocal entropy. The entropy is computed using base 2 logarithm i.e. the filter returns the minimum number of bits needed to encode the local gray level distribution.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (float) \n\nOutput image.     References  \n1  \nhttps://en.wikipedia.org/wiki/Entropy_(information_theory)   Examples >>> from skimage import data\n>>> from skimage.filters.rank import entropy\n>>> from skimage.morphology import disk, ball\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> ent = entropy(img, disk(5))\n>>> ent_vol = entropy(volume, ball(5))\n \n Examples using skimage.filters.rank.entropy\n \n  Tinting gray-scale images  \n\n  Entropy  \n\n  Rank filters   equalize  \nskimage.filters.rank.equalize(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nEqualize image using local histogram.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import equalize\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> equ = equalize(img, disk(5))\n>>> equ_vol = equalize(volume, ball(5))\n \n Examples using skimage.filters.rank.equalize\n \n  Local Histogram Equalization  \n\n  Rank filters   geometric_mean  \nskimage.filters.rank.geometric_mean(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local geometric mean of an image.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     References  \n1  \nGonzalez, R. C. and Wood, R. E. \u201cDigital Image Processing (3rd Edition).\u201d Prentice-Hall Inc, 2006.   Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import mean\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> avg = geometric_mean(img, disk(5))\n>>> avg_vol = geometric_mean(volume, ball(5))\n \n gradient  \nskimage.filters.rank.gradient(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local gradient of an image (i.e. local maximum - local minimum).  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import gradient\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = gradient(img, disk(5))\n>>> out_vol = gradient(volume, ball(5))\n \n Examples using skimage.filters.rank.gradient\n \n  Markers for watershed transform  \n\n  Rank filters   gradient_percentile  \nskimage.filters.rank.gradient_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nReturn local gradient of an image (i.e. local maximum - local minimum). Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n majority  \nskimage.filters.rank.majority(image, selem, *, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nMajority filter assign to each pixel the most occuring value within its neighborhood.  Parameters \n \nimagendarray \n\nImage array (uint8, uint16 array).  \nselem2-D array (integer or float) \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \noutndarray (integer or float), optional \n\nIf None, a new array will be allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint, optional \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     Examples >>> from skimage import data\n>>> from skimage.filters.rank import majority\n>>> from skimage.morphology import disk, ball\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> maj_img = majority(img, disk(5))\n>>> maj_img_vol = majority(volume, ball(5))\n \n maximum  \nskimage.filters.rank.maximum(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local maximum of an image.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.      See also  \nskimage.morphology.dilation\n\n  Notes The lower algorithm complexity makes skimage.filters.rank.maximum more efficient for larger images and structuring elements. Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import maximum\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = maximum(img, disk(5))\n>>> out_vol = maximum(volume, ball(5))\n \n Examples using skimage.filters.rank.maximum\n \n  Rank filters   mean  \nskimage.filters.rank.mean(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local mean of an image.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import mean\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> avg = mean(img, disk(5))\n>>> avg_vol = mean(volume, ball(5))\n \n Examples using skimage.filters.rank.mean\n \n  Segment human cells (in mitosis)  \n\n  Rank filters   mean_bilateral  \nskimage.filters.rank.mean_bilateral(image, selem, out=None, mask=None, shift_x=False, shift_y=False, s0=10, s1=10) [source]\n \nApply a flat kernel bilateral filter. This is an edge-preserving and noise reducing denoising filter. It averages pixels based on their spatial closeness and radiometric similarity. Spatial closeness is measured by considering only the local pixel neighborhood given by a structuring element. Radiometric similarity is defined by the greylevel interval [g-s0, g+s1] where g is the current pixel greylevel. Only pixels belonging to the structuring element and having a greylevel inside this interval are averaged.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \ns0, s1int \n\nDefine the [s0, s1] interval around the greyvalue of the center pixel to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.      See also  \ndenoise_bilateral \n  Examples >>> from skimage import data\n>>> from skimage.morphology import disk\n>>> from skimage.filters.rank import mean_bilateral\n>>> img = data.camera().astype(np.uint16)\n>>> bilat_img = mean_bilateral(img, disk(20), s0=10,s1=10)\n \n Examples using skimage.filters.rank.mean_bilateral\n \n  Rank filters   mean_percentile  \nskimage.filters.rank.mean_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nReturn local mean of an image. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n median  \nskimage.filters.rank.median(image, selem=None, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local median of an image.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s. If None, a full square of size 3 is used.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.      See also  \nskimage.filters.median\n\n\nImplementation of a median filtering which handles images with floating precision.    Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import median\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> med = median(img, disk(5))\n>>> med_vol = median(volume, ball(5))\n \n Examples using skimage.filters.rank.median\n \n  Markers for watershed transform  \n\n  Rank filters   minimum  \nskimage.filters.rank.minimum(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local minimum of an image.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.      See also  \nskimage.morphology.erosion\n\n  Notes The lower algorithm complexity makes skimage.filters.rank.minimum more efficient for larger images and structuring elements. Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import minimum\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = minimum(img, disk(5))\n>>> out_vol = minimum(volume, ball(5))\n \n Examples using skimage.filters.rank.minimum\n \n  Rank filters   modal  \nskimage.filters.rank.modal(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local mode of an image. The mode is the value that appears most often in the local histogram.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import modal\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = modal(img, disk(5))\n>>> out_vol = modal(volume, ball(5))\n \n noise_filter  \nskimage.filters.rank.noise_filter(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nNoise feature.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     References  \n1  \nN. Hashimoto et al. Referenceless image quality evaluation for whole slide imaging. J Pathol Inform 2012;3:9.   Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import noise_filter\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = noise_filter(img, disk(5))\n>>> out_vol = noise_filter(volume, ball(5))\n \n otsu  \nskimage.filters.rank.otsu(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nLocal Otsu\u2019s threshold value for each pixel.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     References  \n1  \nhttps://en.wikipedia.org/wiki/Otsu\u2019s_method   Examples >>> from skimage import data\n>>> from skimage.filters.rank import otsu\n>>> from skimage.morphology import disk, ball\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> local_otsu = otsu(img, disk(5))\n>>> thresh_image = img >= local_otsu\n>>> local_otsu_vol = otsu(volume, ball(5))\n>>> thresh_image_vol = volume >= local_otsu_vol\n \n Examples using skimage.filters.rank.otsu\n \n  Rank filters   percentile  \nskimage.filters.rank.percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0) [source]\n \nReturn local percentile of an image. Returns the value of the p0 lower percentile of the local greyvalue distribution. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0float in [0, \u2026, 1] \n\nSet the percentile value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n pop  \nskimage.filters.rank.pop(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn the local number (population) of pixels. The number of pixels is defined as the number of pixels which are included in the structuring element and the mask.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage.morphology import square, cube # Need to add 3D example\n>>> import skimage.filters.rank as rank\n>>> img = 255 * np.array([[0, 0, 0, 0, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 0, 0, 0, 0]], dtype=np.uint8)\n>>> rank.pop(img, square(3))\narray([[4, 6, 6, 6, 4],\n       [6, 9, 9, 9, 6],\n       [6, 9, 9, 9, 6],\n       [6, 9, 9, 9, 6],\n       [4, 6, 6, 6, 4]], dtype=uint8)\n \n pop_bilateral  \nskimage.filters.rank.pop_bilateral(image, selem, out=None, mask=None, shift_x=False, shift_y=False, s0=10, s1=10) [source]\n \nReturn the local number (population) of pixels. The number of pixels is defined as the number of pixels which are included in the structuring element and the mask. Additionally pixels must have a greylevel inside the interval [g-s0, g+s1] where g is the greyvalue of the center pixel.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \ns0, s1int \n\nDefine the [s0, s1] interval around the greyvalue of the center pixel to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     Examples >>> from skimage.morphology import square\n>>> import skimage.filters.rank as rank\n>>> img = 255 * np.array([[0, 0, 0, 0, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 0, 0, 0, 0]], dtype=np.uint16)\n>>> rank.pop_bilateral(img, square(3), s0=10, s1=10)\narray([[3, 4, 3, 4, 3],\n       [4, 4, 6, 4, 4],\n       [3, 6, 9, 6, 3],\n       [4, 4, 6, 4, 4],\n       [3, 4, 3, 4, 3]], dtype=uint16)\n \n pop_percentile  \nskimage.filters.rank.pop_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nReturn the local number (population) of pixels. The number of pixels is defined as the number of pixels which are included in the structuring element and the mask. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n subtract_mean  \nskimage.filters.rank.subtract_mean(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn image subtracted from its local mean.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Notes Subtracting the mean value may introduce underflow. To compensate this potential underflow, the obtained difference is downscaled by a factor of 2 and shifted by n_bins / 2 - 1, the median value of the local histogram (n_bins = max(3, image.max()) +1 for 16-bits images and 256 otherwise). Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import subtract_mean\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = subtract_mean(img, disk(5))\n>>> out_vol = subtract_mean(volume, ball(5))\n \n subtract_mean_percentile  \nskimage.filters.rank.subtract_mean_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nReturn image subtracted from its local mean. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n sum  \nskimage.filters.rank.sum(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn the local sum of pixels. Note that the sum may overflow depending on the data type of the input array.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage.morphology import square, cube # Need to add 3D example\n>>> import skimage.filters.rank as rank         # Cube seems to fail but\n>>> img = np.array([[0, 0, 0, 0, 0],            # Ball can pass\n...                 [0, 1, 1, 1, 0],\n...                 [0, 1, 1, 1, 0],\n...                 [0, 1, 1, 1, 0],\n...                 [0, 0, 0, 0, 0]], dtype=np.uint8)\n>>> rank.sum(img, square(3))\narray([[1, 2, 3, 2, 1],\n       [2, 4, 6, 4, 2],\n       [3, 6, 9, 6, 3],\n       [2, 4, 6, 4, 2],\n       [1, 2, 3, 2, 1]], dtype=uint8)\n \n sum_bilateral  \nskimage.filters.rank.sum_bilateral(image, selem, out=None, mask=None, shift_x=False, shift_y=False, s0=10, s1=10) [source]\n \nApply a flat kernel bilateral filter. This is an edge-preserving and noise reducing denoising filter. It averages pixels based on their spatial closeness and radiometric similarity. Spatial closeness is measured by considering only the local pixel neighborhood given by a structuring element (selem). Radiometric similarity is defined by the greylevel interval [g-s0, g+s1] where g is the current pixel greylevel. Only pixels belonging to the structuring element AND having a greylevel inside this interval are summed. Note that the sum may overflow depending on the data type of the input array.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \ns0, s1int \n\nDefine the [s0, s1] interval around the greyvalue of the center pixel to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.      See also  \ndenoise_bilateral \n  Examples >>> from skimage import data\n>>> from skimage.morphology import disk\n>>> from skimage.filters.rank import sum_bilateral\n>>> img = data.camera().astype(np.uint16)\n>>> bilat_img = sum_bilateral(img, disk(10), s0=10, s1=10)\n \n sum_percentile  \nskimage.filters.rank.sum_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nReturn the local sum of pixels. Only greyvalues between percentiles [p0, p1] are considered in the filter. Note that the sum may overflow depending on the data type of the input array.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n threshold  \nskimage.filters.rank.threshold(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nLocal threshold of an image. The resulting binary mask is True if the gray value of the center pixel is greater than the local mean.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage.morphology import square, cube # Need to add 3D example\n>>> from skimage.filters.rank import threshold\n>>> img = 255 * np.array([[0, 0, 0, 0, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 0, 0, 0, 0]], dtype=np.uint8)\n>>> threshold(img, square(3))\narray([[0, 0, 0, 0, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 0, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n \n threshold_percentile  \nskimage.filters.rank.threshold_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0) [source]\n \nLocal threshold of an image. The resulting binary mask is True if the greyvalue of the center pixel is greater than the local mean. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0float in [0, \u2026, 1] \n\nSet the percentile value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n tophat  \nskimage.filters.rank.tophat(image, selem, out=None, mask=None, shift_x=False, shift_y=False) [source]\n \nLocal top-hat of an image. This filter computes the morphological opening of the image and then subtracts the result from the original image.  Parameters \n \nimage2-D array (integer or float) \n\nInput image.  \nselem2-D array (integer or float) \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (integer or float), optional \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint, optional \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.    Warns \n Deprecated:\n\n New in version 0.17.  This function is deprecated and will be removed in scikit-image 0.19. This filter was misnamed and we believe that the usefulness is narrow.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk\n>>> from skimage.filters.rank import tophat\n>>> img = data.camera()\n>>> out = tophat(img, disk(5))  \n \n windowed_histogram  \nskimage.filters.rank.windowed_histogram(image, selem, out=None, mask=None, shift_x=False, shift_y=False, n_bins=None) [source]\n \nNormalized sliding window histogram  Parameters \n \nimage2-D array (integer or float) \n\nInput image.  \nselem2-D array (integer or float) \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (integer or float), optional \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint, optional \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \nn_binsint or None \n\nThe number of histogram bins. Will default to image.max() + 1 if None is passed.    Returns \n \nout3-D array (float) \n\nArray of dimensions (H,W,N), where (H,W) are the dimensions of the input image and N is n_bins or image.max() + 1 if no value is provided as a parameter. Effectively, each pixel is a N-D feature vector that is the histogram. The sum of the elements in the feature vector will be 1, unless no pixels in the window were covered by both selem and mask, in which case all elements will be 0.     Examples >>> from skimage import data\n>>> from skimage.filters.rank import windowed_histogram\n>>> from skimage.morphology import disk, ball\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> hist_img = windowed_histogram(img, disk(5))\n \n\n"}, {"name": "filters.rank.autolevel()", "path": "api/skimage.filters.rank#skimage.filters.rank.autolevel", "type": "filters", "text": " \nskimage.filters.rank.autolevel(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nAuto-level image using local histogram. This filter locally stretches the histogram of gray values to cover the entire range of values from \u201cwhite\u201d to \u201cblack\u201d.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import autolevel\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> auto = autolevel(img, disk(5))\n>>> auto_vol = autolevel(volume, ball(5))\n \n"}, {"name": "filters.rank.autolevel_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.autolevel_percentile", "type": "filters", "text": " \nskimage.filters.rank.autolevel_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nReturn greyscale local autolevel of an image. This filter locally stretches the histogram of greyvalues to cover the entire range of values from \u201cwhite\u201d to \u201cblack\u201d. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n"}, {"name": "filters.rank.bottomhat()", "path": "api/skimage.filters.rank#skimage.filters.rank.bottomhat", "type": "filters", "text": " \nskimage.filters.rank.bottomhat(image, selem, out=None, mask=None, shift_x=False, shift_y=False) [source]\n \nLocal bottom-hat of an image. This filter computes the morphological closing of the image and then subtracts the result from the original image.  Parameters \n \nimage2-D array (integer or float) \n\nInput image.  \nselem2-D array (integer or float) \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (integer or float), optional \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint, optional \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.    Warns \n Deprecated:\n\n New in version 0.17.  This function is deprecated and will be removed in scikit-image 0.19. This filter was misnamed and we believe that the usefulness is narrow.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk\n>>> from skimage.filters.rank import bottomhat\n>>> img = data.camera()\n>>> out = bottomhat(img, disk(5))  \n \n"}, {"name": "filters.rank.enhance_contrast()", "path": "api/skimage.filters.rank#skimage.filters.rank.enhance_contrast", "type": "filters", "text": " \nskimage.filters.rank.enhance_contrast(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nEnhance contrast of an image. This replaces each pixel by the local maximum if the pixel gray value is closer to the local maximum than the local minimum. Otherwise it is replaced by the local minimum.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image     Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import enhance_contrast\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = enhance_contrast(img, disk(5))\n>>> out_vol = enhance_contrast(volume, ball(5))\n \n"}, {"name": "filters.rank.enhance_contrast_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.enhance_contrast_percentile", "type": "filters", "text": " \nskimage.filters.rank.enhance_contrast_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nEnhance contrast of an image. This replaces each pixel by the local maximum if the pixel greyvalue is closer to the local maximum than the local minimum. Otherwise it is replaced by the local minimum. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n"}, {"name": "filters.rank.entropy()", "path": "api/skimage.filters.rank#skimage.filters.rank.entropy", "type": "filters", "text": " \nskimage.filters.rank.entropy(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nLocal entropy. The entropy is computed using base 2 logarithm i.e. the filter returns the minimum number of bits needed to encode the local gray level distribution.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (float) \n\nOutput image.     References  \n1  \nhttps://en.wikipedia.org/wiki/Entropy_(information_theory)   Examples >>> from skimage import data\n>>> from skimage.filters.rank import entropy\n>>> from skimage.morphology import disk, ball\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> ent = entropy(img, disk(5))\n>>> ent_vol = entropy(volume, ball(5))\n \n"}, {"name": "filters.rank.equalize()", "path": "api/skimage.filters.rank#skimage.filters.rank.equalize", "type": "filters", "text": " \nskimage.filters.rank.equalize(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nEqualize image using local histogram.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import equalize\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> equ = equalize(img, disk(5))\n>>> equ_vol = equalize(volume, ball(5))\n \n"}, {"name": "filters.rank.geometric_mean()", "path": "api/skimage.filters.rank#skimage.filters.rank.geometric_mean", "type": "filters", "text": " \nskimage.filters.rank.geometric_mean(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local geometric mean of an image.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     References  \n1  \nGonzalez, R. C. and Wood, R. E. \u201cDigital Image Processing (3rd Edition).\u201d Prentice-Hall Inc, 2006.   Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import mean\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> avg = geometric_mean(img, disk(5))\n>>> avg_vol = geometric_mean(volume, ball(5))\n \n"}, {"name": "filters.rank.gradient()", "path": "api/skimage.filters.rank#skimage.filters.rank.gradient", "type": "filters", "text": " \nskimage.filters.rank.gradient(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local gradient of an image (i.e. local maximum - local minimum).  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import gradient\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = gradient(img, disk(5))\n>>> out_vol = gradient(volume, ball(5))\n \n"}, {"name": "filters.rank.gradient_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.gradient_percentile", "type": "filters", "text": " \nskimage.filters.rank.gradient_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nReturn local gradient of an image (i.e. local maximum - local minimum). Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n"}, {"name": "filters.rank.majority()", "path": "api/skimage.filters.rank#skimage.filters.rank.majority", "type": "filters", "text": " \nskimage.filters.rank.majority(image, selem, *, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nMajority filter assign to each pixel the most occuring value within its neighborhood.  Parameters \n \nimagendarray \n\nImage array (uint8, uint16 array).  \nselem2-D array (integer or float) \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \noutndarray (integer or float), optional \n\nIf None, a new array will be allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint, optional \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     Examples >>> from skimage import data\n>>> from skimage.filters.rank import majority\n>>> from skimage.morphology import disk, ball\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> maj_img = majority(img, disk(5))\n>>> maj_img_vol = majority(volume, ball(5))\n \n"}, {"name": "filters.rank.maximum()", "path": "api/skimage.filters.rank#skimage.filters.rank.maximum", "type": "filters", "text": " \nskimage.filters.rank.maximum(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local maximum of an image.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.      See also  \nskimage.morphology.dilation\n\n  Notes The lower algorithm complexity makes skimage.filters.rank.maximum more efficient for larger images and structuring elements. Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import maximum\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = maximum(img, disk(5))\n>>> out_vol = maximum(volume, ball(5))\n \n"}, {"name": "filters.rank.mean()", "path": "api/skimage.filters.rank#skimage.filters.rank.mean", "type": "filters", "text": " \nskimage.filters.rank.mean(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local mean of an image.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import mean\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> avg = mean(img, disk(5))\n>>> avg_vol = mean(volume, ball(5))\n \n"}, {"name": "filters.rank.mean_bilateral()", "path": "api/skimage.filters.rank#skimage.filters.rank.mean_bilateral", "type": "filters", "text": " \nskimage.filters.rank.mean_bilateral(image, selem, out=None, mask=None, shift_x=False, shift_y=False, s0=10, s1=10) [source]\n \nApply a flat kernel bilateral filter. This is an edge-preserving and noise reducing denoising filter. It averages pixels based on their spatial closeness and radiometric similarity. Spatial closeness is measured by considering only the local pixel neighborhood given by a structuring element. Radiometric similarity is defined by the greylevel interval [g-s0, g+s1] where g is the current pixel greylevel. Only pixels belonging to the structuring element and having a greylevel inside this interval are averaged.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \ns0, s1int \n\nDefine the [s0, s1] interval around the greyvalue of the center pixel to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.      See also  \ndenoise_bilateral \n  Examples >>> from skimage import data\n>>> from skimage.morphology import disk\n>>> from skimage.filters.rank import mean_bilateral\n>>> img = data.camera().astype(np.uint16)\n>>> bilat_img = mean_bilateral(img, disk(20), s0=10,s1=10)\n \n"}, {"name": "filters.rank.mean_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.mean_percentile", "type": "filters", "text": " \nskimage.filters.rank.mean_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nReturn local mean of an image. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n"}, {"name": "filters.rank.median()", "path": "api/skimage.filters.rank#skimage.filters.rank.median", "type": "filters", "text": " \nskimage.filters.rank.median(image, selem=None, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local median of an image.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s. If None, a full square of size 3 is used.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.      See also  \nskimage.filters.median\n\n\nImplementation of a median filtering which handles images with floating precision.    Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import median\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> med = median(img, disk(5))\n>>> med_vol = median(volume, ball(5))\n \n"}, {"name": "filters.rank.minimum()", "path": "api/skimage.filters.rank#skimage.filters.rank.minimum", "type": "filters", "text": " \nskimage.filters.rank.minimum(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local minimum of an image.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.      See also  \nskimage.morphology.erosion\n\n  Notes The lower algorithm complexity makes skimage.filters.rank.minimum more efficient for larger images and structuring elements. Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import minimum\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = minimum(img, disk(5))\n>>> out_vol = minimum(volume, ball(5))\n \n"}, {"name": "filters.rank.modal()", "path": "api/skimage.filters.rank#skimage.filters.rank.modal", "type": "filters", "text": " \nskimage.filters.rank.modal(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local mode of an image. The mode is the value that appears most often in the local histogram.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import modal\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = modal(img, disk(5))\n>>> out_vol = modal(volume, ball(5))\n \n"}, {"name": "filters.rank.noise_filter()", "path": "api/skimage.filters.rank#skimage.filters.rank.noise_filter", "type": "filters", "text": " \nskimage.filters.rank.noise_filter(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nNoise feature.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     References  \n1  \nN. Hashimoto et al. Referenceless image quality evaluation for whole slide imaging. J Pathol Inform 2012;3:9.   Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import noise_filter\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = noise_filter(img, disk(5))\n>>> out_vol = noise_filter(volume, ball(5))\n \n"}, {"name": "filters.rank.otsu()", "path": "api/skimage.filters.rank#skimage.filters.rank.otsu", "type": "filters", "text": " \nskimage.filters.rank.otsu(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nLocal Otsu\u2019s threshold value for each pixel.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     References  \n1  \nhttps://en.wikipedia.org/wiki/Otsu\u2019s_method   Examples >>> from skimage import data\n>>> from skimage.filters.rank import otsu\n>>> from skimage.morphology import disk, ball\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> local_otsu = otsu(img, disk(5))\n>>> thresh_image = img >= local_otsu\n>>> local_otsu_vol = otsu(volume, ball(5))\n>>> thresh_image_vol = volume >= local_otsu_vol\n \n"}, {"name": "filters.rank.percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.percentile", "type": "filters", "text": " \nskimage.filters.rank.percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0) [source]\n \nReturn local percentile of an image. Returns the value of the p0 lower percentile of the local greyvalue distribution. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0float in [0, \u2026, 1] \n\nSet the percentile value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n"}, {"name": "filters.rank.pop()", "path": "api/skimage.filters.rank#skimage.filters.rank.pop", "type": "filters", "text": " \nskimage.filters.rank.pop(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn the local number (population) of pixels. The number of pixels is defined as the number of pixels which are included in the structuring element and the mask.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage.morphology import square, cube # Need to add 3D example\n>>> import skimage.filters.rank as rank\n>>> img = 255 * np.array([[0, 0, 0, 0, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 0, 0, 0, 0]], dtype=np.uint8)\n>>> rank.pop(img, square(3))\narray([[4, 6, 6, 6, 4],\n       [6, 9, 9, 9, 6],\n       [6, 9, 9, 9, 6],\n       [6, 9, 9, 9, 6],\n       [4, 6, 6, 6, 4]], dtype=uint8)\n \n"}, {"name": "filters.rank.pop_bilateral()", "path": "api/skimage.filters.rank#skimage.filters.rank.pop_bilateral", "type": "filters", "text": " \nskimage.filters.rank.pop_bilateral(image, selem, out=None, mask=None, shift_x=False, shift_y=False, s0=10, s1=10) [source]\n \nReturn the local number (population) of pixels. The number of pixels is defined as the number of pixels which are included in the structuring element and the mask. Additionally pixels must have a greylevel inside the interval [g-s0, g+s1] where g is the greyvalue of the center pixel.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \ns0, s1int \n\nDefine the [s0, s1] interval around the greyvalue of the center pixel to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     Examples >>> from skimage.morphology import square\n>>> import skimage.filters.rank as rank\n>>> img = 255 * np.array([[0, 0, 0, 0, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 0, 0, 0, 0]], dtype=np.uint16)\n>>> rank.pop_bilateral(img, square(3), s0=10, s1=10)\narray([[3, 4, 3, 4, 3],\n       [4, 4, 6, 4, 4],\n       [3, 6, 9, 6, 3],\n       [4, 4, 6, 4, 4],\n       [3, 4, 3, 4, 3]], dtype=uint16)\n \n"}, {"name": "filters.rank.pop_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.pop_percentile", "type": "filters", "text": " \nskimage.filters.rank.pop_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nReturn the local number (population) of pixels. The number of pixels is defined as the number of pixels which are included in the structuring element and the mask. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n"}, {"name": "filters.rank.subtract_mean()", "path": "api/skimage.filters.rank#skimage.filters.rank.subtract_mean", "type": "filters", "text": " \nskimage.filters.rank.subtract_mean(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn image subtracted from its local mean.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Notes Subtracting the mean value may introduce underflow. To compensate this potential underflow, the obtained difference is downscaled by a factor of 2 and shifted by n_bins / 2 - 1, the median value of the local histogram (n_bins = max(3, image.max()) +1 for 16-bits images and 256 otherwise). Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import subtract_mean\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = subtract_mean(img, disk(5))\n>>> out_vol = subtract_mean(volume, ball(5))\n \n"}, {"name": "filters.rank.subtract_mean_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.subtract_mean_percentile", "type": "filters", "text": " \nskimage.filters.rank.subtract_mean_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nReturn image subtracted from its local mean. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n"}, {"name": "filters.rank.sum()", "path": "api/skimage.filters.rank#skimage.filters.rank.sum", "type": "filters", "text": " \nskimage.filters.rank.sum(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn the local sum of pixels. Note that the sum may overflow depending on the data type of the input array.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage.morphology import square, cube # Need to add 3D example\n>>> import skimage.filters.rank as rank         # Cube seems to fail but\n>>> img = np.array([[0, 0, 0, 0, 0],            # Ball can pass\n...                 [0, 1, 1, 1, 0],\n...                 [0, 1, 1, 1, 0],\n...                 [0, 1, 1, 1, 0],\n...                 [0, 0, 0, 0, 0]], dtype=np.uint8)\n>>> rank.sum(img, square(3))\narray([[1, 2, 3, 2, 1],\n       [2, 4, 6, 4, 2],\n       [3, 6, 9, 6, 3],\n       [2, 4, 6, 4, 2],\n       [1, 2, 3, 2, 1]], dtype=uint8)\n \n"}, {"name": "filters.rank.sum_bilateral()", "path": "api/skimage.filters.rank#skimage.filters.rank.sum_bilateral", "type": "filters", "text": " \nskimage.filters.rank.sum_bilateral(image, selem, out=None, mask=None, shift_x=False, shift_y=False, s0=10, s1=10) [source]\n \nApply a flat kernel bilateral filter. This is an edge-preserving and noise reducing denoising filter. It averages pixels based on their spatial closeness and radiometric similarity. Spatial closeness is measured by considering only the local pixel neighborhood given by a structuring element (selem). Radiometric similarity is defined by the greylevel interval [g-s0, g+s1] where g is the current pixel greylevel. Only pixels belonging to the structuring element AND having a greylevel inside this interval are summed. Note that the sum may overflow depending on the data type of the input array.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \ns0, s1int \n\nDefine the [s0, s1] interval around the greyvalue of the center pixel to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.      See also  \ndenoise_bilateral \n  Examples >>> from skimage import data\n>>> from skimage.morphology import disk\n>>> from skimage.filters.rank import sum_bilateral\n>>> img = data.camera().astype(np.uint16)\n>>> bilat_img = sum_bilateral(img, disk(10), s0=10, s1=10)\n \n"}, {"name": "filters.rank.sum_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.sum_percentile", "type": "filters", "text": " \nskimage.filters.rank.sum_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nReturn the local sum of pixels. Only greyvalues between percentiles [p0, p1] are considered in the filter. Note that the sum may overflow depending on the data type of the input array.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n"}, {"name": "filters.rank.threshold()", "path": "api/skimage.filters.rank#skimage.filters.rank.threshold", "type": "filters", "text": " \nskimage.filters.rank.threshold(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nLocal threshold of an image. The resulting binary mask is True if the gray value of the center pixel is greater than the local mean.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage.morphology import square, cube # Need to add 3D example\n>>> from skimage.filters.rank import threshold\n>>> img = 255 * np.array([[0, 0, 0, 0, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 0, 0, 0, 0]], dtype=np.uint8)\n>>> threshold(img, square(3))\narray([[0, 0, 0, 0, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 0, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "filters.rank.threshold_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.threshold_percentile", "type": "filters", "text": " \nskimage.filters.rank.threshold_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0) [source]\n \nLocal threshold of an image. The resulting binary mask is True if the greyvalue of the center pixel is greater than the local mean. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0float in [0, \u2026, 1] \n\nSet the percentile value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n"}, {"name": "filters.rank.tophat()", "path": "api/skimage.filters.rank#skimage.filters.rank.tophat", "type": "filters", "text": " \nskimage.filters.rank.tophat(image, selem, out=None, mask=None, shift_x=False, shift_y=False) [source]\n \nLocal top-hat of an image. This filter computes the morphological opening of the image and then subtracts the result from the original image.  Parameters \n \nimage2-D array (integer or float) \n\nInput image.  \nselem2-D array (integer or float) \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (integer or float), optional \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint, optional \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.    Warns \n Deprecated:\n\n New in version 0.17.  This function is deprecated and will be removed in scikit-image 0.19. This filter was misnamed and we believe that the usefulness is narrow.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk\n>>> from skimage.filters.rank import tophat\n>>> img = data.camera()\n>>> out = tophat(img, disk(5))  \n \n"}, {"name": "filters.rank.windowed_histogram()", "path": "api/skimage.filters.rank#skimage.filters.rank.windowed_histogram", "type": "filters", "text": " \nskimage.filters.rank.windowed_histogram(image, selem, out=None, mask=None, shift_x=False, shift_y=False, n_bins=None) [source]\n \nNormalized sliding window histogram  Parameters \n \nimage2-D array (integer or float) \n\nInput image.  \nselem2-D array (integer or float) \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (integer or float), optional \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint, optional \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \nn_binsint or None \n\nThe number of histogram bins. Will default to image.max() + 1 if None is passed.    Returns \n \nout3-D array (float) \n\nArray of dimensions (H,W,N), where (H,W) are the dimensions of the input image and N is n_bins or image.max() + 1 if no value is provided as a parameter. Effectively, each pixel is a N-D feature vector that is the histogram. The sum of the elements in the feature vector will be 1, unless no pixels in the window were covered by both selem and mask, in which case all elements will be 0.     Examples >>> from skimage import data\n>>> from skimage.filters.rank import windowed_histogram\n>>> from skimage.morphology import disk, ball\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> hist_img = windowed_histogram(img, disk(5))\n \n"}, {"name": "filters.rank_order()", "path": "api/skimage.filters#skimage.filters.rank_order", "type": "filters", "text": " \nskimage.filters.rank_order(image) [source]\n \nReturn an image of the same shape where each pixel is the index of the pixel value in the ascending order of the unique values of image, aka the rank-order value.  Parameters \n \nimagendarray \n  Returns \n \nlabelsndarray of type np.uint32, of shape image.shape \n\nNew array where each pixel has the rank-order value of the corresponding pixel in image. Pixel values are between 0 and n - 1, where n is the number of distinct unique values in image.  \noriginal_values1-D ndarray \n\nUnique original values of image     Examples >>> a = np.array([[1, 4, 5], [4, 4, 1], [5, 1, 1]])\n>>> a\narray([[1, 4, 5],\n       [4, 4, 1],\n       [5, 1, 1]])\n>>> rank_order(a)\n(array([[0, 1, 2],\n       [1, 1, 0],\n       [2, 0, 0]], dtype=uint32), array([1, 4, 5]))\n>>> b = np.array([-1., 2.5, 3.1, 2.5])\n>>> rank_order(b)\n(array([0, 1, 2, 1], dtype=uint32), array([-1. ,  2.5,  3.1]))\n \n"}, {"name": "filters.roberts()", "path": "api/skimage.filters#skimage.filters.roberts", "type": "filters", "text": " \nskimage.filters.roberts(image, mask=None) [source]\n \nFind the edge magnitude using Roberts\u2019 cross operator.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Roberts\u2019 Cross edge map.      See also  \nsobel, scharr, prewitt, feature.canny \n  Examples >>> from skimage import data\n>>> camera = data.camera()\n>>> from skimage import filters\n>>> edges = filters.roberts(camera)\n \n"}, {"name": "filters.roberts_neg_diag()", "path": "api/skimage.filters#skimage.filters.roberts_neg_diag", "type": "filters", "text": " \nskimage.filters.roberts_neg_diag(image, mask=None) [source]\n \nFind the cross edges of an image using the Roberts\u2019 Cross operator. The kernel is applied to the input image to produce separate measurements of the gradient component one orientation.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Robert\u2019s edge map.     Notes We use the following kernel:  0   1\n-1   0\n \n"}, {"name": "filters.roberts_pos_diag()", "path": "api/skimage.filters#skimage.filters.roberts_pos_diag", "type": "filters", "text": " \nskimage.filters.roberts_pos_diag(image, mask=None) [source]\n \nFind the cross edges of an image using Roberts\u2019 cross operator. The kernel is applied to the input image to produce separate measurements of the gradient component one orientation.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Robert\u2019s edge map.     Notes We use the following kernel: 1   0\n0  -1\n \n"}, {"name": "filters.sato()", "path": "api/skimage.filters#skimage.filters.sato", "type": "filters", "text": " \nskimage.filters.sato(image, sigmas=range(1, 10, 2), black_ridges=True, mode=None, cval=0) [source]\n \nFilter an image with the Sato tubeness filter. This filter can be used to detect continuous ridges, e.g. tubes, wrinkles, rivers. It can be used to calculate the fraction of the whole image containing such objects. Defined only for 2-D and 3-D images. Calculates the eigenvectors of the Hessian to compute the similarity of an image region to tubes, according to the method described in [1].  Parameters \n \nimage(N, M[, P]) ndarray \n\nArray with input image data.  \nsigmasiterable of floats, optional \n\nSigmas used as scales of filter.  \nblack_ridgesboolean, optional \n\nWhen True (the default), the filter detects black ridges; when False, it detects white ridges.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.    Returns \n \nout(N, M[, P]) ndarray \n\nFiltered image (maximum of pixels across all scales).      See also  \nmeijering\n\n\nfrangi\n\n\nhessian\n\n  References  \n1  \nSato, Y., Nakajima, S., Shiraga, N., Atsumi, H., Yoshida, S., Koller, T., \u2026, Kikinis, R. (1998). Three-dimensional multi-scale line filter for segmentation and visualization of curvilinear structures in medical images. Medical image analysis, 2(2), 143-168. DOI:10.1016/S1361-8415(98)80009-1   \n"}, {"name": "filters.scharr()", "path": "api/skimage.filters#skimage.filters.scharr", "type": "filters", "text": " \nskimage.filters.scharr(image, mask=None, *, axis=None, mode='reflect', cval=0.0) [source]\n \nFind the edge magnitude using the Scharr transform.  Parameters \n \nimagearray \n\nThe input image.  \nmaskarray of bool, optional \n\nClip the output image to this mask. (Values where mask=0 will be set to 0.)  \naxisint or sequence of int, optional \n\nCompute the edge filter along this axis. If not provided, the edge magnitude is computed. This is defined as: sch_mag = np.sqrt(sum([scharr(image, axis=i)**2\n                       for i in range(image.ndim)]) / image.ndim)\n The magnitude is also computed if axis is a sequence.  \nmodestr or sequence of str, optional \n\nThe boundary mode for the convolution. See scipy.ndimage.convolve for a description of the modes. This can be either a single boundary mode or one boundary mode per axis.  \ncvalfloat, optional \n\nWhen mode is 'constant', this is the constant used in values outside the boundary of the image data.    Returns \n \noutputarray of float \n\nThe Scharr edge map.      See also  \nsobel, prewitt, canny \n  Notes The Scharr operator has a better rotation invariance than other edge filters such as the Sobel or the Prewitt operators. References  \n1  \nD. Kroon, 2009, Short Paper University Twente, Numerical Optimization of Kernel Based Image Derivatives.  \n2  \nhttps://en.wikipedia.org/wiki/Sobel_operator#Alternative_operators   Examples >>> from skimage import data\n>>> from skimage import filters\n>>> camera = data.camera()\n>>> edges = filters.scharr(camera)\n \n"}, {"name": "filters.scharr_h()", "path": "api/skimage.filters#skimage.filters.scharr_h", "type": "filters", "text": " \nskimage.filters.scharr_h(image, mask=None) [source]\n \nFind the horizontal edges of an image using the Scharr transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Scharr edge map.     Notes We use the following kernel:  3   10   3\n 0    0   0\n-3  -10  -3\n References  \n1  \nD. Kroon, 2009, Short Paper University Twente, Numerical Optimization of Kernel Based Image Derivatives.   \n"}, {"name": "filters.scharr_v()", "path": "api/skimage.filters#skimage.filters.scharr_v", "type": "filters", "text": " \nskimage.filters.scharr_v(image, mask=None) [source]\n \nFind the vertical edges of an image using the Scharr transform.  Parameters \n \nimage2-D array \n\nImage to process  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Scharr edge map.     Notes We use the following kernel:  3   0   -3\n10   0  -10\n 3   0   -3\n References  \n1  \nD. Kroon, 2009, Short Paper University Twente, Numerical Optimization of Kernel Based Image Derivatives.   \n"}, {"name": "filters.sobel()", "path": "api/skimage.filters#skimage.filters.sobel", "type": "filters", "text": " \nskimage.filters.sobel(image, mask=None, *, axis=None, mode='reflect', cval=0.0) [source]\n \nFind edges in an image using the Sobel filter.  Parameters \n \nimagearray \n\nThe input image.  \nmaskarray of bool, optional \n\nClip the output image to this mask. (Values where mask=0 will be set to 0.)  \naxisint or sequence of int, optional \n\nCompute the edge filter along this axis. If not provided, the edge magnitude is computed. This is defined as: sobel_mag = np.sqrt(sum([sobel(image, axis=i)**2\n                         for i in range(image.ndim)]) / image.ndim)\n The magnitude is also computed if axis is a sequence.  \nmodestr or sequence of str, optional \n\nThe boundary mode for the convolution. See scipy.ndimage.convolve for a description of the modes. This can be either a single boundary mode or one boundary mode per axis.  \ncvalfloat, optional \n\nWhen mode is 'constant', this is the constant used in values outside the boundary of the image data.    Returns \n \noutputarray of float \n\nThe Sobel edge map.      See also  \nscharr, prewitt, canny \n  References  \n1  \nD. Kroon, 2009, Short Paper University Twente, Numerical Optimization of Kernel Based Image Derivatives.  \n2  \nhttps://en.wikipedia.org/wiki/Sobel_operator   Examples >>> from skimage import data\n>>> from skimage import filters\n>>> camera = data.camera()\n>>> edges = filters.sobel(camera)\n \n"}, {"name": "filters.sobel_h()", "path": "api/skimage.filters#skimage.filters.sobel_h", "type": "filters", "text": " \nskimage.filters.sobel_h(image, mask=None) [source]\n \nFind the horizontal edges of an image using the Sobel transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Sobel edge map.     Notes We use the following kernel:  1   2   1\n 0   0   0\n-1  -2  -1\n \n"}, {"name": "filters.sobel_v()", "path": "api/skimage.filters#skimage.filters.sobel_v", "type": "filters", "text": " \nskimage.filters.sobel_v(image, mask=None) [source]\n \nFind the vertical edges of an image using the Sobel transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Sobel edge map.     Notes We use the following kernel: 1   0  -1\n2   0  -2\n1   0  -1\n \n"}, {"name": "filters.threshold_isodata()", "path": "api/skimage.filters#skimage.filters.threshold_isodata", "type": "filters", "text": " \nskimage.filters.threshold_isodata(image=None, nbins=256, return_all=False, *, hist=None) [source]\n \nReturn threshold value(s) based on ISODATA method. Histogram-based threshold, known as Ridler-Calvard method or inter-means. Threshold values returned satisfy the following equality: threshold = (image[image <= threshold].mean() +\n             image[image > threshold].mean()) / 2.0\n That is, returned thresholds are intensities that separate the image into two groups of pixels, where the threshold intensity is midway between the mean intensities of these groups. For integer images, the above equality holds to within one; for floating- point images, the equality holds to within the histogram bin-width. Either image or hist must be provided. In case hist is given, the actual histogram of the image is ignored.  Parameters \n \nimage(N, M) ndarray, optional \n\nInput image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.  \nreturn_allbool, optional \n\nIf False (default), return only the lowest threshold that satisfies the above equality. If True, return all valid thresholds.  \nhistarray, or 2-tuple of arrays, optional \n\nHistogram to determine the threshold from and a corresponding array of bin center intensities. Alternatively, only the histogram can be passed.    Returns \n \nthresholdfloat or int or array \n\nThreshold value(s).     References  \n1  \nRidler, TW & Calvard, S (1978), \u201cPicture thresholding using an iterative selection method\u201d IEEE Transactions on Systems, Man and Cybernetics 8: 630-632, DOI:10.1109/TSMC.1978.4310039  \n2  \nSezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and Quantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1): 146-165, http://www.busim.ee.boun.edu.tr/~sankur/SankurFolder/Threshold_survey.pdf DOI:10.1117/1.1631315  \n3  \nImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold   Examples >>> from skimage.data import coins\n>>> image = coins()\n>>> thresh = threshold_isodata(image)\n>>> binary = image > thresh\n \n"}, {"name": "filters.threshold_li()", "path": "api/skimage.filters#skimage.filters.threshold_li", "type": "filters", "text": " \nskimage.filters.threshold_li(image, *, tolerance=None, initial_guess=None, iter_callback=None) [source]\n \nCompute threshold value by Li\u2019s iterative Minimum Cross Entropy method.  Parameters \n \nimagendarray \n\nInput image.  \ntolerancefloat, optional \n\nFinish the computation when the change in the threshold in an iteration is less than this value. By default, this is half the smallest difference between intensity values in image.  \ninitial_guessfloat or Callable[[array[float]], float], optional \n\nLi\u2019s iterative method uses gradient descent to find the optimal threshold. If the image intensity histogram contains more than two modes (peaks), the gradient descent could get stuck in a local optimum. An initial guess for the iteration can help the algorithm find the globally-optimal threshold. A float value defines a specific start point, while a callable should take in an array of image intensities and return a float value. Example valid callables include numpy.mean (default), lambda arr: numpy.quantile(arr, 0.95), or even skimage.filters.threshold_otsu().  \niter_callbackCallable[[float], Any], optional \n\nA function that will be called on the threshold at every iteration of the algorithm.    Returns \n \nthresholdfloat \n\nUpper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.     References  \n1  \nLi C.H. and Lee C.K. (1993) \u201cMinimum Cross Entropy Thresholding\u201d Pattern Recognition, 26(4): 617-625 DOI:10.1016/0031-3203(93)90115-D  \n2  \nLi C.H. and Tam P.K.S. (1998) \u201cAn Iterative Algorithm for Minimum Cross Entropy Thresholding\u201d Pattern Recognition Letters, 18(8): 771-776 DOI:10.1016/S0167-8655(98)00057-9  \n3  \nSezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and Quantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1): 146-165 DOI:10.1117/1.1631315  \n4  \nImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold   Examples >>> from skimage.data import camera\n>>> image = camera()\n>>> thresh = threshold_li(image)\n>>> binary = image > thresh\n \n"}, {"name": "filters.threshold_local()", "path": "api/skimage.filters#skimage.filters.threshold_local", "type": "filters", "text": " \nskimage.filters.threshold_local(image, block_size, method='gaussian', offset=0, mode='reflect', param=None, cval=0) [source]\n \nCompute a threshold mask image based on local pixel neighborhood. Also known as adaptive or dynamic thresholding. The threshold value is the weighted mean for the local neighborhood of a pixel subtracted by a constant. Alternatively the threshold can be determined dynamically by a given function, using the \u2018generic\u2019 method.  Parameters \n \nimage(N, M) ndarray \n\nInput image.  \nblock_sizeint \n\nOdd size of pixel neighborhood which is used to calculate the threshold value (e.g. 3, 5, 7, \u2026, 21, \u2026).  \nmethod{\u2018generic\u2019, \u2018gaussian\u2019, \u2018mean\u2019, \u2018median\u2019}, optional \n\nMethod used to determine adaptive threshold for local neighbourhood in weighted mean image.  \u2018generic\u2019: use custom function (see param parameter) \u2018gaussian\u2019: apply gaussian filter (see param parameter for custom sigma value) \u2018mean\u2019: apply arithmetic mean filter \u2018median\u2019: apply median rank filter  By default the \u2018gaussian\u2019 method is used.  \noffsetfloat, optional \n\nConstant subtracted from weighted mean of neighborhood to calculate the local threshold value. Default offset is 0.  \nmode{\u2018reflect\u2019, \u2018constant\u2019, \u2018nearest\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional \n\nThe mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019. Default is \u2018reflect\u2019.  \nparam{int, function}, optional \n\nEither specify sigma for \u2018gaussian\u2019 method or function object for \u2018generic\u2019 method. This functions takes the flat array of local neighbourhood as a single argument and returns the calculated threshold for the centre pixel.  \ncvalfloat, optional \n\nValue to fill past edges of input if mode is \u2018constant\u2019.    Returns \n \nthreshold(N, M) ndarray \n\nThreshold image. All pixels in the input image higher than the corresponding pixel in the threshold image are considered foreground.     References  \n1  \nhttps://docs.opencv.org/modules/imgproc/doc/miscellaneous_transformations.html?highlight=threshold#adaptivethreshold   Examples >>> from skimage.data import camera\n>>> image = camera()[:50, :50]\n>>> binary_image1 = image > threshold_local(image, 15, 'mean')\n>>> func = lambda arr: arr.mean()\n>>> binary_image2 = image > threshold_local(image, 15, 'generic',\n...                                         param=func)\n \n"}, {"name": "filters.threshold_mean()", "path": "api/skimage.filters#skimage.filters.threshold_mean", "type": "filters", "text": " \nskimage.filters.threshold_mean(image) [source]\n \nReturn threshold value based on the mean of grayscale values.  Parameters \n \nimage(N, M[, \u2026, P]) ndarray \n\nGrayscale input image.    Returns \n \nthresholdfloat \n\nUpper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.     References  \n1  \nC. A. Glasbey, \u201cAn analysis of histogram-based thresholding algorithms,\u201d CVGIP: Graphical Models and Image Processing, vol. 55, pp. 532-537, 1993. DOI:10.1006/cgip.1993.1040   Examples >>> from skimage.data import camera\n>>> image = camera()\n>>> thresh = threshold_mean(image)\n>>> binary = image > thresh\n \n"}, {"name": "filters.threshold_minimum()", "path": "api/skimage.filters#skimage.filters.threshold_minimum", "type": "filters", "text": " \nskimage.filters.threshold_minimum(image=None, nbins=256, max_iter=10000, *, hist=None) [source]\n \nReturn threshold value based on minimum method. The histogram of the input image is computed if not provided and smoothed until there are only two maxima. Then the minimum in between is the threshold value. Either image or hist must be provided. In case hist is given, the actual histogram of the image is ignored.  Parameters \n \nimage(M, N) ndarray, optional \n\nInput image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.  \nmax_iterint, optional \n\nMaximum number of iterations to smooth the histogram.  \nhistarray, or 2-tuple of arrays, optional \n\nHistogram to determine the threshold from and a corresponding array of bin center intensities. Alternatively, only the histogram can be passed.    Returns \n \nthresholdfloat \n\nUpper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.    Raises \n RuntimeError\n\nIf unable to find two local maxima in the histogram or if the smoothing takes more than 1e4 iterations.     References  \n1  \nC. A. Glasbey, \u201cAn analysis of histogram-based thresholding algorithms,\u201d CVGIP: Graphical Models and Image Processing, vol. 55, pp. 532-537, 1993.  \n2  \nPrewitt, JMS & Mendelsohn, ML (1966), \u201cThe analysis of cell images\u201d, Annals of the New York Academy of Sciences 128: 1035-1053 DOI:10.1111/j.1749-6632.1965.tb11715.x   Examples >>> from skimage.data import camera\n>>> image = camera()\n>>> thresh = threshold_minimum(image)\n>>> binary = image > thresh\n \n"}, {"name": "filters.threshold_multiotsu()", "path": "api/skimage.filters#skimage.filters.threshold_multiotsu", "type": "filters", "text": " \nskimage.filters.threshold_multiotsu(image, classes=3, nbins=256) [source]\n \nGenerate classes-1 threshold values to divide gray levels in image. The threshold values are chosen to maximize the total sum of pairwise variances between the thresholded graylevel classes. See Notes and [1] for more details.  Parameters \n \nimage(N, M) ndarray \n\nGrayscale input image.  \nclassesint, optional \n\nNumber of classes to be thresholded, i.e. the number of resulting regions.  \nnbinsint, optional \n\nNumber of bins used to calculate the histogram. This value is ignored for integer arrays.    Returns \n \nthresharray \n\nArray containing the threshold values for the desired classes.    Raises \n ValueError\n\nIf image contains less grayscale value then the desired number of classes.     Notes This implementation relies on a Cython function whose complexity is \\(O\\left(\\frac{Ch^{C-1}}{(C-1)!}\\right)\\), where \\(h\\) is the number of histogram bins and \\(C\\) is the number of classes desired. The input image must be grayscale. References  \n1  \nLiao, P-S., Chen, T-S. and Chung, P-C., \u201cA fast algorithm for multilevel thresholding\u201d, Journal of Information Science and Engineering 17 (5): 713-727, 2001. Available at: <https://ftp.iis.sinica.edu.tw/JISE/2001/200109_01.pdf> DOI:10.6688/JISE.2001.17.5.1  \n2  \nTosa, Y., \u201cMulti-Otsu Threshold\u201d, a java plugin for ImageJ. Available at: <http://imagej.net/plugins/download/Multi_OtsuThreshold.java>   Examples >>> from skimage.color import label2rgb\n>>> from skimage import data\n>>> image = data.camera()\n>>> thresholds = threshold_multiotsu(image)\n>>> regions = np.digitize(image, bins=thresholds)\n>>> regions_colorized = label2rgb(regions)\n \n"}, {"name": "filters.threshold_niblack()", "path": "api/skimage.filters#skimage.filters.threshold_niblack", "type": "filters", "text": " \nskimage.filters.threshold_niblack(image, window_size=15, k=0.2) [source]\n \nApplies Niblack local threshold to an array. A threshold T is calculated for every pixel in the image using the following formula: T = m(x,y) - k * s(x,y)\n where m(x,y) and s(x,y) are the mean and standard deviation of pixel (x,y) neighborhood defined by a rectangular window with size w times w centered around the pixel. k is a configurable parameter that weights the effect of standard deviation.  Parameters \n \nimagendarray \n\nInput image.  \nwindow_sizeint, or iterable of int, optional \n\nWindow size specified as a single odd integer (3, 5, 7, \u2026), or an iterable of length image.ndim containing only odd integers (e.g. (1, 5, 5)).  \nkfloat, optional \n\nValue of parameter k in threshold formula.    Returns \n \nthreshold(N, M) ndarray \n\nThreshold mask. All pixels with an intensity higher than this value are assumed to be foreground.     Notes This algorithm is originally designed for text recognition. The Bradley threshold is a particular case of the Niblack one, being equivalent to >>> from skimage import data\n>>> image = data.page()\n>>> q = 1\n>>> threshold_image = threshold_niblack(image, k=0) * q\n for some value q. By default, Bradley and Roth use q=1. References  \n1  \nW. Niblack, An introduction to Digital Image Processing, Prentice-Hall, 1986.  \n2  \nD. Bradley and G. Roth, \u201cAdaptive thresholding using Integral Image\u201d, Journal of Graphics Tools 12(2), pp. 13-21, 2007. DOI:10.1080/2151237X.2007.10129236   Examples >>> from skimage import data\n>>> image = data.page()\n>>> threshold_image = threshold_niblack(image, window_size=7, k=0.1)\n \n"}, {"name": "filters.threshold_otsu()", "path": "api/skimage.filters#skimage.filters.threshold_otsu", "type": "filters", "text": " \nskimage.filters.threshold_otsu(image=None, nbins=256, *, hist=None) [source]\n \nReturn threshold value based on Otsu\u2019s method. Either image or hist must be provided. If hist is provided, the actual histogram of the image is ignored.  Parameters \n \nimage(N, M) ndarray, optional \n\nGrayscale input image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.  \nhistarray, or 2-tuple of arrays, optional \n\nHistogram from which to determine the threshold, and optionally a corresponding array of bin center intensities. An alternative use of this function is to pass it only hist.    Returns \n \nthresholdfloat \n\nUpper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.     Notes The input image must be grayscale. References  \n1  \nWikipedia, https://en.wikipedia.org/wiki/Otsu\u2019s_Method   Examples >>> from skimage.data import camera\n>>> image = camera()\n>>> thresh = threshold_otsu(image)\n>>> binary = image <= thresh\n \n"}, {"name": "filters.threshold_sauvola()", "path": "api/skimage.filters#skimage.filters.threshold_sauvola", "type": "filters", "text": " \nskimage.filters.threshold_sauvola(image, window_size=15, k=0.2, r=None) [source]\n \nApplies Sauvola local threshold to an array. Sauvola is a modification of Niblack technique. In the original method a threshold T is calculated for every pixel in the image using the following formula: T = m(x,y) * (1 + k * ((s(x,y) / R) - 1))\n where m(x,y) and s(x,y) are the mean and standard deviation of pixel (x,y) neighborhood defined by a rectangular window with size w times w centered around the pixel. k is a configurable parameter that weights the effect of standard deviation. R is the maximum standard deviation of a greyscale image.  Parameters \n \nimagendarray \n\nInput image.  \nwindow_sizeint, or iterable of int, optional \n\nWindow size specified as a single odd integer (3, 5, 7, \u2026), or an iterable of length image.ndim containing only odd integers (e.g. (1, 5, 5)).  \nkfloat, optional \n\nValue of the positive parameter k.  \nrfloat, optional \n\nValue of R, the dynamic range of standard deviation. If None, set to the half of the image dtype range.    Returns \n \nthreshold(N, M) ndarray \n\nThreshold mask. All pixels with an intensity higher than this value are assumed to be foreground.     Notes This algorithm is originally designed for text recognition. References  \n1  \nJ. Sauvola and M. Pietikainen, \u201cAdaptive document image binarization,\u201d Pattern Recognition 33(2), pp. 225-236, 2000. DOI:10.1016/S0031-3203(99)00055-2   Examples >>> from skimage import data\n>>> image = data.page()\n>>> t_sauvola = threshold_sauvola(image, window_size=15, k=0.2)\n>>> binary_image = image > t_sauvola\n \n"}, {"name": "filters.threshold_triangle()", "path": "api/skimage.filters#skimage.filters.threshold_triangle", "type": "filters", "text": " \nskimage.filters.threshold_triangle(image, nbins=256) [source]\n \nReturn threshold value based on the triangle algorithm.  Parameters \n \nimage(N, M[, \u2026, P]) ndarray \n\nGrayscale input image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.    Returns \n \nthresholdfloat \n\nUpper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.     References  \n1  \nZack, G. W., Rogers, W. E. and Latt, S. A., 1977, Automatic Measurement of Sister Chromatid Exchange Frequency, Journal of Histochemistry and Cytochemistry 25 (7), pp. 741-753 DOI:10.1177/25.7.70454  \n2  \nImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold   Examples >>> from skimage.data import camera\n>>> image = camera()\n>>> thresh = threshold_triangle(image)\n>>> binary = image > thresh\n \n"}, {"name": "filters.threshold_yen()", "path": "api/skimage.filters#skimage.filters.threshold_yen", "type": "filters", "text": " \nskimage.filters.threshold_yen(image=None, nbins=256, *, hist=None) [source]\n \nReturn threshold value based on Yen\u2019s method. Either image or hist must be provided. In case hist is given, the actual histogram of the image is ignored.  Parameters \n \nimage(N, M) ndarray, optional \n\nInput image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.  \nhistarray, or 2-tuple of arrays, optional \n\nHistogram from which to determine the threshold, and optionally a corresponding array of bin center intensities. An alternative use of this function is to pass it only hist.    Returns \n \nthresholdfloat \n\nUpper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.     References  \n1  \nYen J.C., Chang F.J., and Chang S. (1995) \u201cA New Criterion for Automatic Multilevel Thresholding\u201d IEEE Trans. on Image Processing, 4(3): 370-378. DOI:10.1109/83.366472  \n2  \nSezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and Quantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1): 146-165, DOI:10.1117/1.1631315 http://www.busim.ee.boun.edu.tr/~sankur/SankurFolder/Threshold_survey.pdf  \n3  \nImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold   Examples >>> from skimage.data import camera\n>>> image = camera()\n>>> thresh = threshold_yen(image)\n>>> binary = image <= thresh\n \n"}, {"name": "filters.try_all_threshold()", "path": "api/skimage.filters#skimage.filters.try_all_threshold", "type": "filters", "text": " \nskimage.filters.try_all_threshold(image, figsize=(8, 5), verbose=True) [source]\n \nReturns a figure comparing the outputs of different thresholding methods.  Parameters \n \nimage(N, M) ndarray \n\nInput image.  \nfigsizetuple, optional \n\nFigure size (in inches).  \nverbosebool, optional \n\nPrint function name for each method.    Returns \n \nfig, axtuple \n\nMatplotlib figure and axes.     Notes The following algorithms are used:  isodata li mean minimum otsu triangle yen  Examples >>> from skimage.data import text\n>>> fig, ax = try_all_threshold(text(), figsize=(10, 6), verbose=False)\n \n"}, {"name": "filters.unsharp_mask()", "path": "api/skimage.filters#skimage.filters.unsharp_mask", "type": "filters", "text": " \nskimage.filters.unsharp_mask(image, radius=1.0, amount=1.0, multichannel=False, preserve_range=False) [source]\n \nUnsharp masking filter. The sharp details are identified as the difference between the original image and its blurred version. These details are then scaled, and added back to the original image.  Parameters \n \nimage[P, \u2026, ]M[, N][, C] ndarray \n\nInput image.  \nradiusscalar or sequence of scalars, optional \n\nIf a scalar is given, then its value is used for all dimensions. If sequence is given, then there must be exactly one radius for each dimension except the last dimension for multichannel images. Note that 0 radius means no blurring, and negative values are not allowed.  \namountscalar, optional \n\nThe details will be amplified with this factor. The factor could be 0 or negative. Typically, it is a small positive number, e.g. 1.0.  \nmultichannelbool, optional \n\nIf True, the last image dimension is considered as a color channel, otherwise as spatial. Color channels are processed individually.  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html    Returns \n \noutput[P, \u2026, ]M[, N][, C] ndarray of float \n\nImage with unsharp mask applied.     Notes Unsharp masking is an image sharpening technique. It is a linear image operation, and numerically stable, unlike deconvolution which is an ill-posed problem. Because of this stability, it is often preferred over deconvolution. The main idea is as follows: sharp details are identified as the difference between the original image and its blurred version. These details are added back to the original image after a scaling step: enhanced image = original + amount * (original - blurred) When applying this filter to several color layers independently, color bleeding may occur. More visually pleasing result can be achieved by processing only the brightness/lightness/intensity channel in a suitable color space such as HSV, HSL, YUV, or YCbCr. Unsharp masking is described in most introductory digital image processing books. This implementation is based on [1]. References  \n1  \nMaria Petrou, Costas Petrou \u201cImage Processing: The Fundamentals\u201d, (2010), ed ii., page 357, ISBN 13: 9781119994398 DOI:10.1002/9781119994398  \n2  \nWikipedia. Unsharp masking https://en.wikipedia.org/wiki/Unsharp_masking   Examples >>> array = np.ones(shape=(5,5), dtype=np.uint8)*100\n>>> array[2,2] = 120\n>>> array\narray([[100, 100, 100, 100, 100],\n       [100, 100, 100, 100, 100],\n       [100, 100, 120, 100, 100],\n       [100, 100, 100, 100, 100],\n       [100, 100, 100, 100, 100]], dtype=uint8)\n>>> np.around(unsharp_mask(array, radius=0.5, amount=2),2)\narray([[0.39, 0.39, 0.39, 0.39, 0.39],\n       [0.39, 0.39, 0.38, 0.39, 0.39],\n       [0.39, 0.38, 0.53, 0.38, 0.39],\n       [0.39, 0.39, 0.38, 0.39, 0.39],\n       [0.39, 0.39, 0.39, 0.39, 0.39]])\n >>> array = np.ones(shape=(5,5), dtype=np.int8)*100\n>>> array[2,2] = 127\n>>> np.around(unsharp_mask(array, radius=0.5, amount=2),2)\narray([[0.79, 0.79, 0.79, 0.79, 0.79],\n       [0.79, 0.78, 0.75, 0.78, 0.79],\n       [0.79, 0.75, 1.  , 0.75, 0.79],\n       [0.79, 0.78, 0.75, 0.78, 0.79],\n       [0.79, 0.79, 0.79, 0.79, 0.79]])\n >>> np.around(unsharp_mask(array, radius=0.5, amount=2, preserve_range=True), 2)\narray([[100.  , 100.  ,  99.99, 100.  , 100.  ],\n       [100.  ,  99.39,  95.48,  99.39, 100.  ],\n       [ 99.99,  95.48, 147.59,  95.48,  99.99],\n       [100.  ,  99.39,  95.48,  99.39, 100.  ],\n       [100.  , 100.  ,  99.99, 100.  , 100.  ]])\n \n"}, {"name": "filters.wiener()", "path": "api/skimage.filters#skimage.filters.wiener", "type": "filters", "text": " \nskimage.filters.wiener(data, impulse_response=None, filter_params={}, K=0.25, predefined_filter=None) [source]\n \nMinimum Mean Square Error (Wiener) inverse filter.  Parameters \n \ndata(M,N) ndarray \n\nInput data.  \nKfloat or (M,N) ndarray \n\nRatio between power spectrum of noise and undegraded image.  \nimpulse_responsecallable f(r, c, **filter_params) \n\nImpulse response of the filter. See LPIFilter2D.__init__.  \nfilter_paramsdict \n\nAdditional keyword parameters to the impulse_response function.    Other Parameters \n \npredefined_filterLPIFilter2D \n\nIf you need to apply the same filter multiple times over different images, construct the LPIFilter2D and specify it here.     \n"}, {"name": "filters.window()", "path": "api/skimage.filters#skimage.filters.window", "type": "filters", "text": " \nskimage.filters.window(window_type, shape, warp_kwargs=None) [source]\n \nReturn an n-dimensional window of a given size and dimensionality.  Parameters \n \nwindow_typestring, float, or tuple \n\nThe type of window to be created. Any window type supported by scipy.signal.get_window is allowed here. See notes below for a current list, or the SciPy documentation for the version of SciPy on your machine.  \nshapetuple of int or int \n\nThe shape of the window along each axis. If an integer is provided, a 1D window is generated.  \nwarp_kwargsdict \n\nKeyword arguments passed to skimage.transform.warp (e.g., warp_kwargs={'order':3} to change interpolation method).    Returns \n \nnd_windowndarray \n\nA window of the specified shape. dtype is np.double.     Notes This function is based on scipy.signal.get_window and thus can access all of the window types available to that function (e.g., \"hann\", \"boxcar\"). Note that certain window types require parameters that have to be supplied with the window name as a tuple (e.g., (\"tukey\", 0.8)). If only a float is supplied, it is interpreted as the beta parameter of the Kaiser window. See https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.windows.get_window.html for more details. Note that this function generates a double precision array of the specified shape and can thus generate very large arrays that consume a large amount of available memory. The approach taken here to create nD windows is to first calculate the Euclidean distance from the center of the intended nD window to each position in the array. That distance is used to sample, with interpolation, from a 1D window returned from scipy.signal.get_window. The method of interpolation can be changed with the order keyword argument passed to skimage.transform.warp. Some coordinates in the output window will be outside of the original signal; these will be filled in with zeros. Window types: - boxcar - triang - blackman - hamming - hann - bartlett - flattop - parzen - bohman - blackmanharris - nuttall - barthann - kaiser (needs beta) - gaussian (needs standard deviation) - general_gaussian (needs power, width) - slepian (needs width) - dpss (needs normalized half-bandwidth) - chebwin (needs attenuation) - exponential (needs decay scale) - tukey (needs taper fraction) References  \n1  \nTwo-dimensional window design, Wikipedia, https://en.wikipedia.org/wiki/Two_dimensional_window_design   Examples Return a Hann window with shape (512, 512): >>> from skimage.filters import window\n>>> w = window('hann', (512, 512))\n Return a Kaiser window with beta parameter of 16 and shape (256, 256, 35): >>> w = window(16, (256, 256, 35))\n Return a Tukey window with an alpha parameter of 0.8 and shape (100, 300): >>> w = window(('tukey', 0.8), (100, 300))\n \n"}, {"name": "future", "path": "api/skimage.future", "type": "future", "text": "Module: future Functionality with an experimental API. Although you can count on the functions in this package being around in the future, the API may change with any version update and will not follow the skimage two-version deprecation path. Therefore, use the functions herein with care, and do not use them in production code that will depend on updated skimage versions.  \nskimage.future.fit_segmenter(labels, \u2026) Segmentation using labeled parts of the image and a classifier.  \nskimage.future.manual_lasso_segmentation(image) Return a label image based on freeform selections made with the mouse.  \nskimage.future.manual_polygon_segmentation(image) Return a label image based on polygon selections made with the mouse.  \nskimage.future.predict_segmenter(features, clf) Segmentation of images using a pretrained classifier.  \nskimage.future.TrainableSegmenter([clf, \u2026]) Estimator for classifying pixels.  \nskimage.future.graph    fit_segmenter  \nskimage.future.fit_segmenter(labels, features, clf) [source]\n \nSegmentation using labeled parts of the image and a classifier.  Parameters \n \nlabelsndarray of ints \n\nImage of labels. Labels >= 1 correspond to the training set and label 0 to unlabeled pixels to be segmented.  \nfeaturesndarray \n\nArray of features, with the first dimension corresponding to the number of features, and the other dimensions correspond to labels.shape.  \nclfclassifier object \n\nclassifier object, exposing a fit and a predict method as in scikit-learn\u2019s API, for example an instance of RandomForestClassifier or LogisticRegression classifier.    Returns \n \nclfclassifier object \n\nclassifier trained on labels    Raises \n \nNotFittedError if self.clf has not been fitted yet (use self.fit). \n   \n Examples using skimage.future.fit_segmenter\n \n  Trainable segmentation using local features and random forests   manual_lasso_segmentation  \nskimage.future.manual_lasso_segmentation(image, alpha=0.4, return_all=False) [source]\n \nReturn a label image based on freeform selections made with the mouse.  Parameters \n \nimage(M, N[, 3]) array \n\nGrayscale or RGB image.  \nalphafloat, optional \n\nTransparency value for polygons drawn over the image.  \nreturn_allbool, optional \n\nIf True, an array containing each separate polygon drawn is returned. (The polygons may overlap.) If False (default), latter polygons \u201coverwrite\u201d earlier ones where they overlap.    Returns \n \nlabelsarray of int, shape ([Q, ]M, N) \n\nThe segmented regions. If mode is \u2018separate\u2019, the leading dimension of the array corresponds to the number of regions that the user drew.     Notes Press and hold the left mouse button to draw around each object. Examples >>> from skimage import data, future, io\n>>> camera = data.camera()\n>>> mask = future.manual_lasso_segmentation(camera)  \n>>> io.imshow(mask)  \n>>> io.show()  \n \n manual_polygon_segmentation  \nskimage.future.manual_polygon_segmentation(image, alpha=0.4, return_all=False) [source]\n \nReturn a label image based on polygon selections made with the mouse.  Parameters \n \nimage(M, N[, 3]) array \n\nGrayscale or RGB image.  \nalphafloat, optional \n\nTransparency value for polygons drawn over the image.  \nreturn_allbool, optional \n\nIf True, an array containing each separate polygon drawn is returned. (The polygons may overlap.) If False (default), latter polygons \u201coverwrite\u201d earlier ones where they overlap.    Returns \n \nlabelsarray of int, shape ([Q, ]M, N) \n\nThe segmented regions. If mode is \u2018separate\u2019, the leading dimension of the array corresponds to the number of regions that the user drew.     Notes Use left click to select the vertices of the polygon and right click to confirm the selection once all vertices are selected. Examples >>> from skimage import data, future, io\n>>> camera = data.camera()\n>>> mask = future.manual_polygon_segmentation(camera)  \n>>> io.imshow(mask)  \n>>> io.show()  \n \n predict_segmenter  \nskimage.future.predict_segmenter(features, clf) [source]\n \nSegmentation of images using a pretrained classifier.  Parameters \n \nfeaturesndarray \n\nArray of features, with the last dimension corresponding to the number of features, and the other dimensions are compatible with the shape of the image to segment, or a flattened image.  \nclfclassifier object \n\ntrained classifier object, exposing a predict method as in scikit-learn\u2019s API, for example an instance of RandomForestClassifier or LogisticRegression classifier. The classifier must be already trained, for example with skimage.segmentation.fit_segmenter().    Returns \n \noutputndarray \n\nLabeled array, built from the prediction of the classifier.     \n Examples using skimage.future.predict_segmenter\n \n  Trainable segmentation using local features and random forests   TrainableSegmenter  \nclass skimage.future.TrainableSegmenter(clf=None, features_func=None) [source]\n \nBases: object Estimator for classifying pixels.  Parameters \n \nclfclassifier object, optional \n\nclassifier object, exposing a fit and a predict method as in scikit-learn\u2019s API, for example an instance of RandomForestClassifier or LogisticRegression classifier.  \nfeatures_funcfunction, optional \n\nfunction computing features on all pixels of the image, to be passed to the classifier. The output should be of shape (m_features, *labels.shape). If None, skimage.segmentation.multiscale_basic_features() is used.     Methods  \nfit(image, labels) Train classifier using partially labeled (annotated) image.  \npredict(image) Segment new image using trained internal classifier.    \ncompute_features     \n__init__(clf=None, features_func=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \ncompute_features(image) [source]\n\n  \nfit(image, labels) [source]\n \nTrain classifier using partially labeled (annotated) image.  Parameters \n \nimagendarray \n\nInput image, which can be grayscale or multichannel, and must have a number of dimensions compatible with self.features_func.  \nlabelsndarray of ints \n\nLabeled array of shape compatible with image (same shape for a single-channel image). Labels >= 1 correspond to the training set and label 0 to unlabeled pixels to be segmented.     \n  \npredict(image) [source]\n \nSegment new image using trained internal classifier.  Parameters \n \nimagendarray \n\nInput image, which can be grayscale or multichannel, and must have a number of dimensions compatible with self.features_func.    Raises \n \nNotFittedError if self.clf has not been fitted yet (use self.fit). \n   \n \n\n"}, {"name": "future.fit_segmenter()", "path": "api/skimage.future#skimage.future.fit_segmenter", "type": "future", "text": " \nskimage.future.fit_segmenter(labels, features, clf) [source]\n \nSegmentation using labeled parts of the image and a classifier.  Parameters \n \nlabelsndarray of ints \n\nImage of labels. Labels >= 1 correspond to the training set and label 0 to unlabeled pixels to be segmented.  \nfeaturesndarray \n\nArray of features, with the first dimension corresponding to the number of features, and the other dimensions correspond to labels.shape.  \nclfclassifier object \n\nclassifier object, exposing a fit and a predict method as in scikit-learn\u2019s API, for example an instance of RandomForestClassifier or LogisticRegression classifier.    Returns \n \nclfclassifier object \n\nclassifier trained on labels    Raises \n \nNotFittedError if self.clf has not been fitted yet (use self.fit). \n   \n"}, {"name": "future.graph", "path": "api/skimage.future.graph", "type": "future", "text": "Module: future.graph  \nskimage.future.graph.cut_normalized(labels, rag) Perform Normalized Graph cut on the Region Adjacency Graph.  \nskimage.future.graph.cut_threshold(labels, \u2026) Combine regions separated by weight less than threshold.  \nskimage.future.graph.merge_hierarchical(\u2026) Perform hierarchical merging of a RAG.  \nskimage.future.graph.ncut(labels, rag[, \u2026]) Perform Normalized Graph cut on the Region Adjacency Graph.  \nskimage.future.graph.rag_boundary(labels, \u2026) Comouter RAG based on region boundaries  \nskimage.future.graph.rag_mean_color(image, \u2026) Compute the Region Adjacency Graph using mean colors.  \nskimage.future.graph.show_rag(labels, rag, image) Show a Region Adjacency Graph on an image.  \nskimage.future.graph.RAG([label_image, \u2026]) The Region Adjacency Graph (RAG) of an image, subclasses networx.Graph   cut_normalized  \nskimage.future.graph.cut_normalized(labels, rag, thresh=0.001, num_cuts=10, in_place=True, max_edge=1.0, *, random_state=None) [source]\n \nPerform Normalized Graph cut on the Region Adjacency Graph. Given an image\u2019s labels and its similarity RAG, recursively perform a 2-way normalized cut on it. All nodes belonging to a subgraph that cannot be cut further are assigned a unique label in the output.  Parameters \n \nlabelsndarray \n\nThe array of labels.  \nragRAG \n\nThe region adjacency graph.  \nthreshfloat \n\nThe threshold. A subgraph won\u2019t be further subdivided if the value of the N-cut exceeds thresh.  \nnum_cutsint \n\nThe number or N-cuts to perform before determining the optimal one.  \nin_placebool \n\nIf set, modifies rag in place. For each node n the function will set a new attribute rag.nodes[n]['ncut label'].  \nmax_edgefloat, optional \n\nThe maximum possible value of an edge in the RAG. This corresponds to an edge between identical regions. This is used to put self edges in the RAG.  \nrandom_stateint, RandomState instance or None, optional \n\nIf int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. The random state is used for the starting point of scipy.sparse.linalg.eigsh.    Returns \n \noutndarray \n\nThe new labeled array.     References  \n1  \nShi, J.; Malik, J., \u201cNormalized cuts and image segmentation\u201d, Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 888-905, August 2000.   Examples >>> from skimage import data, segmentation\n>>> from skimage.future import graph\n>>> img = data.astronaut()\n>>> labels = segmentation.slic(img)\n>>> rag = graph.rag_mean_color(img, labels, mode='similarity')\n>>> new_labels = graph.cut_normalized(labels, rag)\n \n cut_threshold  \nskimage.future.graph.cut_threshold(labels, rag, thresh, in_place=True) [source]\n \nCombine regions separated by weight less than threshold. Given an image\u2019s labels and its RAG, output new labels by combining regions whose nodes are separated by a weight less than the given threshold.  Parameters \n \nlabelsndarray \n\nThe array of labels.  \nragRAG \n\nThe region adjacency graph.  \nthreshfloat \n\nThe threshold. Regions connected by edges with smaller weights are combined.  \nin_placebool \n\nIf set, modifies rag in place. The function will remove the edges with weights less that thresh. If set to False the function makes a copy of rag before proceeding.    Returns \n \noutndarray \n\nThe new labelled array.     References  \n1  \nAlain Tremeau and Philippe Colantoni \u201cRegions Adjacency Graph Applied To Color Image Segmentation\u201d DOI:10.1109/83.841950   Examples >>> from skimage import data, segmentation\n>>> from skimage.future import graph\n>>> img = data.astronaut()\n>>> labels = segmentation.slic(img)\n>>> rag = graph.rag_mean_color(img, labels)\n>>> new_labels = graph.cut_threshold(labels, rag, 10)\n \n merge_hierarchical  \nskimage.future.graph.merge_hierarchical(labels, rag, thresh, rag_copy, in_place_merge, merge_func, weight_func) [source]\n \nPerform hierarchical merging of a RAG. Greedily merges the most similar pair of nodes until no edges lower than thresh remain.  Parameters \n \nlabelsndarray \n\nThe array of labels.  \nragRAG \n\nThe Region Adjacency Graph.  \nthreshfloat \n\nRegions connected by an edge with weight smaller than thresh are merged.  \nrag_copybool \n\nIf set, the RAG copied before modifying.  \nin_place_mergebool \n\nIf set, the nodes are merged in place. Otherwise, a new node is created for each merge..  \nmerge_funccallable \n\nThis function is called before merging two nodes. For the RAG graph while merging src and dst, it is called as follows merge_func(graph, src, dst).  \nweight_funccallable \n\nThe function to compute the new weights of the nodes adjacent to the merged node. This is directly supplied as the argument weight_func to merge_nodes.    Returns \n \noutndarray \n\nThe new labeled array.     \n ncut  \nskimage.future.graph.ncut(labels, rag, thresh=0.001, num_cuts=10, in_place=True, max_edge=1.0, *, random_state=None) [source]\n \nPerform Normalized Graph cut on the Region Adjacency Graph. Given an image\u2019s labels and its similarity RAG, recursively perform a 2-way normalized cut on it. All nodes belonging to a subgraph that cannot be cut further are assigned a unique label in the output.  Parameters \n \nlabelsndarray \n\nThe array of labels.  \nragRAG \n\nThe region adjacency graph.  \nthreshfloat \n\nThe threshold. A subgraph won\u2019t be further subdivided if the value of the N-cut exceeds thresh.  \nnum_cutsint \n\nThe number or N-cuts to perform before determining the optimal one.  \nin_placebool \n\nIf set, modifies rag in place. For each node n the function will set a new attribute rag.nodes[n]['ncut label'].  \nmax_edgefloat, optional \n\nThe maximum possible value of an edge in the RAG. This corresponds to an edge between identical regions. This is used to put self edges in the RAG.  \nrandom_stateint, RandomState instance or None, optional \n\nIf int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. The random state is used for the starting point of scipy.sparse.linalg.eigsh.    Returns \n \noutndarray \n\nThe new labeled array.     References  \n1  \nShi, J.; Malik, J., \u201cNormalized cuts and image segmentation\u201d, Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 888-905, August 2000.   Examples >>> from skimage import data, segmentation\n>>> from skimage.future import graph\n>>> img = data.astronaut()\n>>> labels = segmentation.slic(img)\n>>> rag = graph.rag_mean_color(img, labels, mode='similarity')\n>>> new_labels = graph.cut_normalized(labels, rag)\n \n rag_boundary  \nskimage.future.graph.rag_boundary(labels, edge_map, connectivity=2) [source]\n \nComouter RAG based on region boundaries Given an image\u2019s initial segmentation and its edge map this method constructs the corresponding Region Adjacency Graph (RAG). Each node in the RAG represents a set of pixels within the image with the same label in labels. The weight between two adjacent regions is the average value in edge_map along their boundary.  \nlabelsndarray \n\nThe labelled image.  \nedge_mapndarray \n\nThis should have the same shape as that of labels. For all pixels along the boundary between 2 adjacent regions, the average value of the corresponding pixels in edge_map is the edge weight between them.  \nconnectivityint, optional \n\nPixels with a squared distance less than connectivity from each other are considered adjacent. It can range from 1 to labels.ndim. Its behavior is the same as connectivity parameter in scipy.ndimage.filters.generate_binary_structure.   Examples >>> from skimage import data, segmentation, filters, color\n>>> from skimage.future import graph\n>>> img = data.chelsea()\n>>> labels = segmentation.slic(img)\n>>> edge_map = filters.sobel(color.rgb2gray(img))\n>>> rag = graph.rag_boundary(labels, edge_map)\n \n rag_mean_color  \nskimage.future.graph.rag_mean_color(image, labels, connectivity=2, mode='distance', sigma=255.0) [source]\n \nCompute the Region Adjacency Graph using mean colors. Given an image and its initial segmentation, this method constructs the corresponding Region Adjacency Graph (RAG). Each node in the RAG represents a set of pixels within image with the same label in labels. The weight between two adjacent regions represents how similar or dissimilar two regions are depending on the mode parameter.  Parameters \n \nimagendarray, shape(M, N, [\u2026, P,] 3) \n\nInput image.  \nlabelsndarray, shape(M, N, [\u2026, P]) \n\nThe labelled image. This should have one dimension less than image. If image has dimensions (M, N, 3) labels should have dimensions (M, N).  \nconnectivityint, optional \n\nPixels with a squared distance less than connectivity from each other are considered adjacent. It can range from 1 to labels.ndim. Its behavior is the same as connectivity parameter in scipy.ndimage.generate_binary_structure.  \nmode{\u2018distance\u2019, \u2018similarity\u2019}, optional \n\nThe strategy to assign edge weights. \u2018distance\u2019 : The weight between two adjacent regions is the \\(|c_1 - c_2|\\), where \\(c_1\\) and \\(c_2\\) are the mean colors of the two regions. It represents the Euclidean distance in their average color. \u2018similarity\u2019 : The weight between two adjacent is \\(e^{-d^2/sigma}\\) where \\(d=|c_1 - c_2|\\), where \\(c_1\\) and \\(c_2\\) are the mean colors of the two regions. It represents how similar two regions are.  \nsigmafloat, optional \n\nUsed for computation when mode is \u201csimilarity\u201d. It governs how close to each other two colors should be, for their corresponding edge weight to be significant. A very large value of sigma could make any two colors behave as though they were similar.    Returns \n \noutRAG \n\nThe region adjacency graph.     References  \n1  \nAlain Tremeau and Philippe Colantoni \u201cRegions Adjacency Graph Applied To Color Image Segmentation\u201d DOI:10.1109/83.841950   Examples >>> from skimage import data, segmentation\n>>> from skimage.future import graph\n>>> img = data.astronaut()\n>>> labels = segmentation.slic(img)\n>>> rag = graph.rag_mean_color(img, labels)\n \n show_rag  \nskimage.future.graph.show_rag(labels, rag, image, border_color='black', edge_width=1.5, edge_cmap='magma', img_cmap='bone', in_place=True, ax=None) [source]\n \nShow a Region Adjacency Graph on an image. Given a labelled image and its corresponding RAG, show the nodes and edges of the RAG on the image with the specified colors. Edges are displayed between the centroid of the 2 adjacent regions in the image.  Parameters \n \nlabelsndarray, shape (M, N) \n\nThe labelled image.  \nragRAG \n\nThe Region Adjacency Graph.  \nimagendarray, shape (M, N[, 3]) \n\nInput image. If colormap is None, the image should be in RGB format.  \nborder_colorcolor spec, optional \n\nColor with which the borders between regions are drawn.  \nedge_widthfloat, optional \n\nThe thickness with which the RAG edges are drawn.  \nedge_cmapmatplotlib.colors.Colormap, optional \n\nAny matplotlib colormap with which the edges are drawn.  \nimg_cmapmatplotlib.colors.Colormap, optional \n\nAny matplotlib colormap with which the image is draw. If set to None the image is drawn as it is.  \nin_placebool, optional \n\nIf set, the RAG is modified in place. For each node n the function will set a new attribute rag.nodes[n]['centroid'].  \naxmatplotlib.axes.Axes, optional \n\nThe axes to draw on. If not specified, new axes are created and drawn on.    Returns \n \nlcmatplotlib.collections.LineCollection \n\nA colection of lines that represent the edges of the graph. It can be passed to the matplotlib.figure.Figure.colorbar() function.     Examples >>> from skimage import data, segmentation\n>>> from skimage.future import graph\n>>> import matplotlib.pyplot as plt\n>>>\n>>> img = data.coffee()\n>>> labels = segmentation.slic(img)\n>>> g =  graph.rag_mean_color(img, labels)\n>>> lc = graph.show_rag(labels, g, img)\n>>> cbar = plt.colorbar(lc)\n \n RAG  \nclass skimage.future.graph.RAG(label_image=None, connectivity=1, data=None, **attr) [source]\n \nBases: networkx.classes.graph.Graph The Region Adjacency Graph (RAG) of an image, subclasses networx.Graph  Parameters \n \nlabel_imagearray of int \n\nAn initial segmentation, with each region labeled as a different integer. Every unique value in label_image will correspond to a node in the graph.  \nconnectivityint in {1, \u2026, label_image.ndim}, optional \n\nThe connectivity between pixels in label_image. For a 2D image, a connectivity of 1 corresponds to immediate neighbors up, down, left, and right, while a connectivity of 2 also includes diagonal neighbors. See scipy.ndimage.generate_binary_structure.  \ndatanetworkx Graph specification, optional \n\nInitial or additional edges to pass to the NetworkX Graph constructor. See networkx.Graph. Valid edge specifications include edge list (list of tuples), NumPy arrays, and SciPy sparse matrices.  \n**attrkeyword arguments, optional \n\nAdditional attributes to add to the graph.      \n__init__(label_image=None, connectivity=1, data=None, **attr) [source]\n \nInitialize a graph with edges, name, or graph attributes.  Parameters \n \nincoming_graph_datainput graph (optional, default: None) \n\nData to initialize graph. If None (default) an empty graph is created. The data can be an edge list, or any NetworkX graph object. If the corresponding optional Python packages are installed the data can also be a NumPy matrix or 2d ndarray, a SciPy sparse matrix, or a PyGraphviz graph.  \nattrkeyword arguments, optional (default= no attributes) \n\nAttributes to add to graph as key=value pairs.      See also  \nconvert \n  Examples >>> G = nx.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc\n>>> G = nx.Graph(name=\"my graph\")\n>>> e = [(1, 2), (2, 3), (3, 4)]  # list of edges\n>>> G = nx.Graph(e)\n Arbitrary graph attribute pairs (key=value) may be assigned >>> G = nx.Graph(e, day=\"Friday\")\n>>> G.graph\n{'day': 'Friday'}\n \n  \nadd_edge(u, v, attr_dict=None, **attr) [source]\n \nAdd an edge between u and v while updating max node id.  See also networkx.Graph.add_edge().  \n  \nadd_node(n, attr_dict=None, **attr) [source]\n \nAdd node n while updating the maximum node id.  See also networkx.Graph.add_node().  \n  \ncopy() [source]\n \nCopy the graph with its max node id.  See also networkx.Graph.copy().  \n  \nfresh_copy() [source]\n \nReturn a fresh copy graph with the same data structure. A fresh copy has no nodes, edges or graph attributes. It is the same data structure as the current graph. This method is typically used to create an empty version of the graph. This is required when subclassing Graph with networkx v2 and does not cause problems for v1. Here is more detail from the network migrating from 1.x to 2.x document: With the new GraphViews (SubGraph, ReversedGraph, etc)\nyou can't assume that ``G.__class__()`` will create a new\ninstance of the same graph type as ``G``. In fact, the\ncall signature for ``__class__`` differs depending on\nwhether ``G`` is a view or a base class. For v2.x you\nshould use ``G.fresh_copy()`` to create a null graph of\nthe correct type---ready to fill with nodes and edges.\n \n  \nmerge_nodes(src, dst, weight_func=<function min_weight>, in_place=True, extra_arguments=[], extra_keywords={}) [source]\n \nMerge node src and dst. The new combined node is adjacent to all the neighbors of src and dst. weight_func is called to decide the weight of edges incident on the new node.  Parameters \n \nsrc, dstint \n\nNodes to be merged.  \nweight_funccallable, optional \n\nFunction to decide the attributes of edges incident on the new node. For each neighbor n for src and `dst, weight_func will be called as follows: weight_func(src, dst, n, *extra_arguments, **extra_keywords). src, dst and n are IDs of vertices in the RAG object which is in turn a subclass of networkx.Graph. It is expected to return a dict of attributes of the resulting edge.  \nin_placebool, optional \n\nIf set to True, the merged node has the id dst, else merged node has a new id which is returned.  \nextra_argumentssequence, optional \n\nThe sequence of extra positional arguments passed to weight_func.  \nextra_keywordsdictionary, optional \n\nThe dict of keyword arguments passed to the weight_func.    Returns \n \nidint \n\nThe id of the new node.     Notes If in_place is False the resulting node has a new id, rather than dst. \n  \nnext_id() [source]\n \nReturns the id for the new node to be inserted. The current implementation returns one more than the maximum id.  Returns \n \nidint \n\nThe id of the new node to be inserted.     \n \n\n"}, {"name": "future.graph.cut_normalized()", "path": "api/skimage.future.graph#skimage.future.graph.cut_normalized", "type": "future", "text": " \nskimage.future.graph.cut_normalized(labels, rag, thresh=0.001, num_cuts=10, in_place=True, max_edge=1.0, *, random_state=None) [source]\n \nPerform Normalized Graph cut on the Region Adjacency Graph. Given an image\u2019s labels and its similarity RAG, recursively perform a 2-way normalized cut on it. All nodes belonging to a subgraph that cannot be cut further are assigned a unique label in the output.  Parameters \n \nlabelsndarray \n\nThe array of labels.  \nragRAG \n\nThe region adjacency graph.  \nthreshfloat \n\nThe threshold. A subgraph won\u2019t be further subdivided if the value of the N-cut exceeds thresh.  \nnum_cutsint \n\nThe number or N-cuts to perform before determining the optimal one.  \nin_placebool \n\nIf set, modifies rag in place. For each node n the function will set a new attribute rag.nodes[n]['ncut label'].  \nmax_edgefloat, optional \n\nThe maximum possible value of an edge in the RAG. This corresponds to an edge between identical regions. This is used to put self edges in the RAG.  \nrandom_stateint, RandomState instance or None, optional \n\nIf int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. The random state is used for the starting point of scipy.sparse.linalg.eigsh.    Returns \n \noutndarray \n\nThe new labeled array.     References  \n1  \nShi, J.; Malik, J., \u201cNormalized cuts and image segmentation\u201d, Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 888-905, August 2000.   Examples >>> from skimage import data, segmentation\n>>> from skimage.future import graph\n>>> img = data.astronaut()\n>>> labels = segmentation.slic(img)\n>>> rag = graph.rag_mean_color(img, labels, mode='similarity')\n>>> new_labels = graph.cut_normalized(labels, rag)\n \n"}, {"name": "future.graph.cut_threshold()", "path": "api/skimage.future.graph#skimage.future.graph.cut_threshold", "type": "future", "text": " \nskimage.future.graph.cut_threshold(labels, rag, thresh, in_place=True) [source]\n \nCombine regions separated by weight less than threshold. Given an image\u2019s labels and its RAG, output new labels by combining regions whose nodes are separated by a weight less than the given threshold.  Parameters \n \nlabelsndarray \n\nThe array of labels.  \nragRAG \n\nThe region adjacency graph.  \nthreshfloat \n\nThe threshold. Regions connected by edges with smaller weights are combined.  \nin_placebool \n\nIf set, modifies rag in place. The function will remove the edges with weights less that thresh. If set to False the function makes a copy of rag before proceeding.    Returns \n \noutndarray \n\nThe new labelled array.     References  \n1  \nAlain Tremeau and Philippe Colantoni \u201cRegions Adjacency Graph Applied To Color Image Segmentation\u201d DOI:10.1109/83.841950   Examples >>> from skimage import data, segmentation\n>>> from skimage.future import graph\n>>> img = data.astronaut()\n>>> labels = segmentation.slic(img)\n>>> rag = graph.rag_mean_color(img, labels)\n>>> new_labels = graph.cut_threshold(labels, rag, 10)\n \n"}, {"name": "future.graph.merge_hierarchical()", "path": "api/skimage.future.graph#skimage.future.graph.merge_hierarchical", "type": "future", "text": " \nskimage.future.graph.merge_hierarchical(labels, rag, thresh, rag_copy, in_place_merge, merge_func, weight_func) [source]\n \nPerform hierarchical merging of a RAG. Greedily merges the most similar pair of nodes until no edges lower than thresh remain.  Parameters \n \nlabelsndarray \n\nThe array of labels.  \nragRAG \n\nThe Region Adjacency Graph.  \nthreshfloat \n\nRegions connected by an edge with weight smaller than thresh are merged.  \nrag_copybool \n\nIf set, the RAG copied before modifying.  \nin_place_mergebool \n\nIf set, the nodes are merged in place. Otherwise, a new node is created for each merge..  \nmerge_funccallable \n\nThis function is called before merging two nodes. For the RAG graph while merging src and dst, it is called as follows merge_func(graph, src, dst).  \nweight_funccallable \n\nThe function to compute the new weights of the nodes adjacent to the merged node. This is directly supplied as the argument weight_func to merge_nodes.    Returns \n \noutndarray \n\nThe new labeled array.     \n"}, {"name": "future.graph.ncut()", "path": "api/skimage.future.graph#skimage.future.graph.ncut", "type": "future", "text": " \nskimage.future.graph.ncut(labels, rag, thresh=0.001, num_cuts=10, in_place=True, max_edge=1.0, *, random_state=None) [source]\n \nPerform Normalized Graph cut on the Region Adjacency Graph. Given an image\u2019s labels and its similarity RAG, recursively perform a 2-way normalized cut on it. All nodes belonging to a subgraph that cannot be cut further are assigned a unique label in the output.  Parameters \n \nlabelsndarray \n\nThe array of labels.  \nragRAG \n\nThe region adjacency graph.  \nthreshfloat \n\nThe threshold. A subgraph won\u2019t be further subdivided if the value of the N-cut exceeds thresh.  \nnum_cutsint \n\nThe number or N-cuts to perform before determining the optimal one.  \nin_placebool \n\nIf set, modifies rag in place. For each node n the function will set a new attribute rag.nodes[n]['ncut label'].  \nmax_edgefloat, optional \n\nThe maximum possible value of an edge in the RAG. This corresponds to an edge between identical regions. This is used to put self edges in the RAG.  \nrandom_stateint, RandomState instance or None, optional \n\nIf int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. The random state is used for the starting point of scipy.sparse.linalg.eigsh.    Returns \n \noutndarray \n\nThe new labeled array.     References  \n1  \nShi, J.; Malik, J., \u201cNormalized cuts and image segmentation\u201d, Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 888-905, August 2000.   Examples >>> from skimage import data, segmentation\n>>> from skimage.future import graph\n>>> img = data.astronaut()\n>>> labels = segmentation.slic(img)\n>>> rag = graph.rag_mean_color(img, labels, mode='similarity')\n>>> new_labels = graph.cut_normalized(labels, rag)\n \n"}, {"name": "future.graph.RAG", "path": "api/skimage.future.graph#skimage.future.graph.RAG", "type": "future", "text": " \nclass skimage.future.graph.RAG(label_image=None, connectivity=1, data=None, **attr) [source]\n \nBases: networkx.classes.graph.Graph The Region Adjacency Graph (RAG) of an image, subclasses networx.Graph  Parameters \n \nlabel_imagearray of int \n\nAn initial segmentation, with each region labeled as a different integer. Every unique value in label_image will correspond to a node in the graph.  \nconnectivityint in {1, \u2026, label_image.ndim}, optional \n\nThe connectivity between pixels in label_image. For a 2D image, a connectivity of 1 corresponds to immediate neighbors up, down, left, and right, while a connectivity of 2 also includes diagonal neighbors. See scipy.ndimage.generate_binary_structure.  \ndatanetworkx Graph specification, optional \n\nInitial or additional edges to pass to the NetworkX Graph constructor. See networkx.Graph. Valid edge specifications include edge list (list of tuples), NumPy arrays, and SciPy sparse matrices.  \n**attrkeyword arguments, optional \n\nAdditional attributes to add to the graph.      \n__init__(label_image=None, connectivity=1, data=None, **attr) [source]\n \nInitialize a graph with edges, name, or graph attributes.  Parameters \n \nincoming_graph_datainput graph (optional, default: None) \n\nData to initialize graph. If None (default) an empty graph is created. The data can be an edge list, or any NetworkX graph object. If the corresponding optional Python packages are installed the data can also be a NumPy matrix or 2d ndarray, a SciPy sparse matrix, or a PyGraphviz graph.  \nattrkeyword arguments, optional (default= no attributes) \n\nAttributes to add to graph as key=value pairs.      See also  \nconvert \n  Examples >>> G = nx.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc\n>>> G = nx.Graph(name=\"my graph\")\n>>> e = [(1, 2), (2, 3), (3, 4)]  # list of edges\n>>> G = nx.Graph(e)\n Arbitrary graph attribute pairs (key=value) may be assigned >>> G = nx.Graph(e, day=\"Friday\")\n>>> G.graph\n{'day': 'Friday'}\n \n  \nadd_edge(u, v, attr_dict=None, **attr) [source]\n \nAdd an edge between u and v while updating max node id.  See also networkx.Graph.add_edge().  \n  \nadd_node(n, attr_dict=None, **attr) [source]\n \nAdd node n while updating the maximum node id.  See also networkx.Graph.add_node().  \n  \ncopy() [source]\n \nCopy the graph with its max node id.  See also networkx.Graph.copy().  \n  \nfresh_copy() [source]\n \nReturn a fresh copy graph with the same data structure. A fresh copy has no nodes, edges or graph attributes. It is the same data structure as the current graph. This method is typically used to create an empty version of the graph. This is required when subclassing Graph with networkx v2 and does not cause problems for v1. Here is more detail from the network migrating from 1.x to 2.x document: With the new GraphViews (SubGraph, ReversedGraph, etc)\nyou can't assume that ``G.__class__()`` will create a new\ninstance of the same graph type as ``G``. In fact, the\ncall signature for ``__class__`` differs depending on\nwhether ``G`` is a view or a base class. For v2.x you\nshould use ``G.fresh_copy()`` to create a null graph of\nthe correct type---ready to fill with nodes and edges.\n \n  \nmerge_nodes(src, dst, weight_func=<function min_weight>, in_place=True, extra_arguments=[], extra_keywords={}) [source]\n \nMerge node src and dst. The new combined node is adjacent to all the neighbors of src and dst. weight_func is called to decide the weight of edges incident on the new node.  Parameters \n \nsrc, dstint \n\nNodes to be merged.  \nweight_funccallable, optional \n\nFunction to decide the attributes of edges incident on the new node. For each neighbor n for src and `dst, weight_func will be called as follows: weight_func(src, dst, n, *extra_arguments, **extra_keywords). src, dst and n are IDs of vertices in the RAG object which is in turn a subclass of networkx.Graph. It is expected to return a dict of attributes of the resulting edge.  \nin_placebool, optional \n\nIf set to True, the merged node has the id dst, else merged node has a new id which is returned.  \nextra_argumentssequence, optional \n\nThe sequence of extra positional arguments passed to weight_func.  \nextra_keywordsdictionary, optional \n\nThe dict of keyword arguments passed to the weight_func.    Returns \n \nidint \n\nThe id of the new node.     Notes If in_place is False the resulting node has a new id, rather than dst. \n  \nnext_id() [source]\n \nReturns the id for the new node to be inserted. The current implementation returns one more than the maximum id.  Returns \n \nidint \n\nThe id of the new node to be inserted.     \n \n"}, {"name": "future.graph.RAG.add_edge()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.add_edge", "type": "future", "text": " \nadd_edge(u, v, attr_dict=None, **attr) [source]\n \nAdd an edge between u and v while updating max node id.  See also networkx.Graph.add_edge().  \n"}, {"name": "future.graph.RAG.add_node()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.add_node", "type": "future", "text": " \nadd_node(n, attr_dict=None, **attr) [source]\n \nAdd node n while updating the maximum node id.  See also networkx.Graph.add_node().  \n"}, {"name": "future.graph.RAG.copy()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.copy", "type": "future", "text": " \ncopy() [source]\n \nCopy the graph with its max node id.  See also networkx.Graph.copy().  \n"}, {"name": "future.graph.RAG.fresh_copy()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.fresh_copy", "type": "future", "text": " \nfresh_copy() [source]\n \nReturn a fresh copy graph with the same data structure. A fresh copy has no nodes, edges or graph attributes. It is the same data structure as the current graph. This method is typically used to create an empty version of the graph. This is required when subclassing Graph with networkx v2 and does not cause problems for v1. Here is more detail from the network migrating from 1.x to 2.x document: With the new GraphViews (SubGraph, ReversedGraph, etc)\nyou can't assume that ``G.__class__()`` will create a new\ninstance of the same graph type as ``G``. In fact, the\ncall signature for ``__class__`` differs depending on\nwhether ``G`` is a view or a base class. For v2.x you\nshould use ``G.fresh_copy()`` to create a null graph of\nthe correct type---ready to fill with nodes and edges.\n \n"}, {"name": "future.graph.RAG.merge_nodes()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.merge_nodes", "type": "future", "text": " \nmerge_nodes(src, dst, weight_func=<function min_weight>, in_place=True, extra_arguments=[], extra_keywords={}) [source]\n \nMerge node src and dst. The new combined node is adjacent to all the neighbors of src and dst. weight_func is called to decide the weight of edges incident on the new node.  Parameters \n \nsrc, dstint \n\nNodes to be merged.  \nweight_funccallable, optional \n\nFunction to decide the attributes of edges incident on the new node. For each neighbor n for src and `dst, weight_func will be called as follows: weight_func(src, dst, n, *extra_arguments, **extra_keywords). src, dst and n are IDs of vertices in the RAG object which is in turn a subclass of networkx.Graph. It is expected to return a dict of attributes of the resulting edge.  \nin_placebool, optional \n\nIf set to True, the merged node has the id dst, else merged node has a new id which is returned.  \nextra_argumentssequence, optional \n\nThe sequence of extra positional arguments passed to weight_func.  \nextra_keywordsdictionary, optional \n\nThe dict of keyword arguments passed to the weight_func.    Returns \n \nidint \n\nThe id of the new node.     Notes If in_place is False the resulting node has a new id, rather than dst. \n"}, {"name": "future.graph.RAG.next_id()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.next_id", "type": "future", "text": " \nnext_id() [source]\n \nReturns the id for the new node to be inserted. The current implementation returns one more than the maximum id.  Returns \n \nidint \n\nThe id of the new node to be inserted.     \n"}, {"name": "future.graph.RAG.__init__()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.__init__", "type": "future", "text": " \n__init__(label_image=None, connectivity=1, data=None, **attr) [source]\n \nInitialize a graph with edges, name, or graph attributes.  Parameters \n \nincoming_graph_datainput graph (optional, default: None) \n\nData to initialize graph. If None (default) an empty graph is created. The data can be an edge list, or any NetworkX graph object. If the corresponding optional Python packages are installed the data can also be a NumPy matrix or 2d ndarray, a SciPy sparse matrix, or a PyGraphviz graph.  \nattrkeyword arguments, optional (default= no attributes) \n\nAttributes to add to graph as key=value pairs.      See also  \nconvert \n  Examples >>> G = nx.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc\n>>> G = nx.Graph(name=\"my graph\")\n>>> e = [(1, 2), (2, 3), (3, 4)]  # list of edges\n>>> G = nx.Graph(e)\n Arbitrary graph attribute pairs (key=value) may be assigned >>> G = nx.Graph(e, day=\"Friday\")\n>>> G.graph\n{'day': 'Friday'}\n \n"}, {"name": "future.graph.rag_boundary()", "path": "api/skimage.future.graph#skimage.future.graph.rag_boundary", "type": "future", "text": " \nskimage.future.graph.rag_boundary(labels, edge_map, connectivity=2) [source]\n \nComouter RAG based on region boundaries Given an image\u2019s initial segmentation and its edge map this method constructs the corresponding Region Adjacency Graph (RAG). Each node in the RAG represents a set of pixels within the image with the same label in labels. The weight between two adjacent regions is the average value in edge_map along their boundary.  \nlabelsndarray \n\nThe labelled image.  \nedge_mapndarray \n\nThis should have the same shape as that of labels. For all pixels along the boundary between 2 adjacent regions, the average value of the corresponding pixels in edge_map is the edge weight between them.  \nconnectivityint, optional \n\nPixels with a squared distance less than connectivity from each other are considered adjacent. It can range from 1 to labels.ndim. Its behavior is the same as connectivity parameter in scipy.ndimage.filters.generate_binary_structure.   Examples >>> from skimage import data, segmentation, filters, color\n>>> from skimage.future import graph\n>>> img = data.chelsea()\n>>> labels = segmentation.slic(img)\n>>> edge_map = filters.sobel(color.rgb2gray(img))\n>>> rag = graph.rag_boundary(labels, edge_map)\n \n"}, {"name": "future.graph.rag_mean_color()", "path": "api/skimage.future.graph#skimage.future.graph.rag_mean_color", "type": "future", "text": " \nskimage.future.graph.rag_mean_color(image, labels, connectivity=2, mode='distance', sigma=255.0) [source]\n \nCompute the Region Adjacency Graph using mean colors. Given an image and its initial segmentation, this method constructs the corresponding Region Adjacency Graph (RAG). Each node in the RAG represents a set of pixels within image with the same label in labels. The weight between two adjacent regions represents how similar or dissimilar two regions are depending on the mode parameter.  Parameters \n \nimagendarray, shape(M, N, [\u2026, P,] 3) \n\nInput image.  \nlabelsndarray, shape(M, N, [\u2026, P]) \n\nThe labelled image. This should have one dimension less than image. If image has dimensions (M, N, 3) labels should have dimensions (M, N).  \nconnectivityint, optional \n\nPixels with a squared distance less than connectivity from each other are considered adjacent. It can range from 1 to labels.ndim. Its behavior is the same as connectivity parameter in scipy.ndimage.generate_binary_structure.  \nmode{\u2018distance\u2019, \u2018similarity\u2019}, optional \n\nThe strategy to assign edge weights. \u2018distance\u2019 : The weight between two adjacent regions is the \\(|c_1 - c_2|\\), where \\(c_1\\) and \\(c_2\\) are the mean colors of the two regions. It represents the Euclidean distance in their average color. \u2018similarity\u2019 : The weight between two adjacent is \\(e^{-d^2/sigma}\\) where \\(d=|c_1 - c_2|\\), where \\(c_1\\) and \\(c_2\\) are the mean colors of the two regions. It represents how similar two regions are.  \nsigmafloat, optional \n\nUsed for computation when mode is \u201csimilarity\u201d. It governs how close to each other two colors should be, for their corresponding edge weight to be significant. A very large value of sigma could make any two colors behave as though they were similar.    Returns \n \noutRAG \n\nThe region adjacency graph.     References  \n1  \nAlain Tremeau and Philippe Colantoni \u201cRegions Adjacency Graph Applied To Color Image Segmentation\u201d DOI:10.1109/83.841950   Examples >>> from skimage import data, segmentation\n>>> from skimage.future import graph\n>>> img = data.astronaut()\n>>> labels = segmentation.slic(img)\n>>> rag = graph.rag_mean_color(img, labels)\n \n"}, {"name": "future.graph.show_rag()", "path": "api/skimage.future.graph#skimage.future.graph.show_rag", "type": "future", "text": " \nskimage.future.graph.show_rag(labels, rag, image, border_color='black', edge_width=1.5, edge_cmap='magma', img_cmap='bone', in_place=True, ax=None) [source]\n \nShow a Region Adjacency Graph on an image. Given a labelled image and its corresponding RAG, show the nodes and edges of the RAG on the image with the specified colors. Edges are displayed between the centroid of the 2 adjacent regions in the image.  Parameters \n \nlabelsndarray, shape (M, N) \n\nThe labelled image.  \nragRAG \n\nThe Region Adjacency Graph.  \nimagendarray, shape (M, N[, 3]) \n\nInput image. If colormap is None, the image should be in RGB format.  \nborder_colorcolor spec, optional \n\nColor with which the borders between regions are drawn.  \nedge_widthfloat, optional \n\nThe thickness with which the RAG edges are drawn.  \nedge_cmapmatplotlib.colors.Colormap, optional \n\nAny matplotlib colormap with which the edges are drawn.  \nimg_cmapmatplotlib.colors.Colormap, optional \n\nAny matplotlib colormap with which the image is draw. If set to None the image is drawn as it is.  \nin_placebool, optional \n\nIf set, the RAG is modified in place. For each node n the function will set a new attribute rag.nodes[n]['centroid'].  \naxmatplotlib.axes.Axes, optional \n\nThe axes to draw on. If not specified, new axes are created and drawn on.    Returns \n \nlcmatplotlib.collections.LineCollection \n\nA colection of lines that represent the edges of the graph. It can be passed to the matplotlib.figure.Figure.colorbar() function.     Examples >>> from skimage import data, segmentation\n>>> from skimage.future import graph\n>>> import matplotlib.pyplot as plt\n>>>\n>>> img = data.coffee()\n>>> labels = segmentation.slic(img)\n>>> g =  graph.rag_mean_color(img, labels)\n>>> lc = graph.show_rag(labels, g, img)\n>>> cbar = plt.colorbar(lc)\n \n"}, {"name": "future.manual_lasso_segmentation()", "path": "api/skimage.future#skimage.future.manual_lasso_segmentation", "type": "future", "text": " \nskimage.future.manual_lasso_segmentation(image, alpha=0.4, return_all=False) [source]\n \nReturn a label image based on freeform selections made with the mouse.  Parameters \n \nimage(M, N[, 3]) array \n\nGrayscale or RGB image.  \nalphafloat, optional \n\nTransparency value for polygons drawn over the image.  \nreturn_allbool, optional \n\nIf True, an array containing each separate polygon drawn is returned. (The polygons may overlap.) If False (default), latter polygons \u201coverwrite\u201d earlier ones where they overlap.    Returns \n \nlabelsarray of int, shape ([Q, ]M, N) \n\nThe segmented regions. If mode is \u2018separate\u2019, the leading dimension of the array corresponds to the number of regions that the user drew.     Notes Press and hold the left mouse button to draw around each object. Examples >>> from skimage import data, future, io\n>>> camera = data.camera()\n>>> mask = future.manual_lasso_segmentation(camera)  \n>>> io.imshow(mask)  \n>>> io.show()  \n \n"}, {"name": "future.manual_polygon_segmentation()", "path": "api/skimage.future#skimage.future.manual_polygon_segmentation", "type": "future", "text": " \nskimage.future.manual_polygon_segmentation(image, alpha=0.4, return_all=False) [source]\n \nReturn a label image based on polygon selections made with the mouse.  Parameters \n \nimage(M, N[, 3]) array \n\nGrayscale or RGB image.  \nalphafloat, optional \n\nTransparency value for polygons drawn over the image.  \nreturn_allbool, optional \n\nIf True, an array containing each separate polygon drawn is returned. (The polygons may overlap.) If False (default), latter polygons \u201coverwrite\u201d earlier ones where they overlap.    Returns \n \nlabelsarray of int, shape ([Q, ]M, N) \n\nThe segmented regions. If mode is \u2018separate\u2019, the leading dimension of the array corresponds to the number of regions that the user drew.     Notes Use left click to select the vertices of the polygon and right click to confirm the selection once all vertices are selected. Examples >>> from skimage import data, future, io\n>>> camera = data.camera()\n>>> mask = future.manual_polygon_segmentation(camera)  \n>>> io.imshow(mask)  \n>>> io.show()  \n \n"}, {"name": "future.predict_segmenter()", "path": "api/skimage.future#skimage.future.predict_segmenter", "type": "future", "text": " \nskimage.future.predict_segmenter(features, clf) [source]\n \nSegmentation of images using a pretrained classifier.  Parameters \n \nfeaturesndarray \n\nArray of features, with the last dimension corresponding to the number of features, and the other dimensions are compatible with the shape of the image to segment, or a flattened image.  \nclfclassifier object \n\ntrained classifier object, exposing a predict method as in scikit-learn\u2019s API, for example an instance of RandomForestClassifier or LogisticRegression classifier. The classifier must be already trained, for example with skimage.segmentation.fit_segmenter().    Returns \n \noutputndarray \n\nLabeled array, built from the prediction of the classifier.     \n"}, {"name": "future.TrainableSegmenter", "path": "api/skimage.future#skimage.future.TrainableSegmenter", "type": "future", "text": " \nclass skimage.future.TrainableSegmenter(clf=None, features_func=None) [source]\n \nBases: object Estimator for classifying pixels.  Parameters \n \nclfclassifier object, optional \n\nclassifier object, exposing a fit and a predict method as in scikit-learn\u2019s API, for example an instance of RandomForestClassifier or LogisticRegression classifier.  \nfeatures_funcfunction, optional \n\nfunction computing features on all pixels of the image, to be passed to the classifier. The output should be of shape (m_features, *labels.shape). If None, skimage.segmentation.multiscale_basic_features() is used.     Methods  \nfit(image, labels) Train classifier using partially labeled (annotated) image.  \npredict(image) Segment new image using trained internal classifier.    \ncompute_features     \n__init__(clf=None, features_func=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \ncompute_features(image) [source]\n\n  \nfit(image, labels) [source]\n \nTrain classifier using partially labeled (annotated) image.  Parameters \n \nimagendarray \n\nInput image, which can be grayscale or multichannel, and must have a number of dimensions compatible with self.features_func.  \nlabelsndarray of ints \n\nLabeled array of shape compatible with image (same shape for a single-channel image). Labels >= 1 correspond to the training set and label 0 to unlabeled pixels to be segmented.     \n  \npredict(image) [source]\n \nSegment new image using trained internal classifier.  Parameters \n \nimagendarray \n\nInput image, which can be grayscale or multichannel, and must have a number of dimensions compatible with self.features_func.    Raises \n \nNotFittedError if self.clf has not been fitted yet (use self.fit). \n   \n \n"}, {"name": "future.TrainableSegmenter.compute_features()", "path": "api/skimage.future#skimage.future.TrainableSegmenter.compute_features", "type": "future", "text": " \ncompute_features(image) [source]\n\n"}, {"name": "future.TrainableSegmenter.fit()", "path": "api/skimage.future#skimage.future.TrainableSegmenter.fit", "type": "future", "text": " \nfit(image, labels) [source]\n \nTrain classifier using partially labeled (annotated) image.  Parameters \n \nimagendarray \n\nInput image, which can be grayscale or multichannel, and must have a number of dimensions compatible with self.features_func.  \nlabelsndarray of ints \n\nLabeled array of shape compatible with image (same shape for a single-channel image). Labels >= 1 correspond to the training set and label 0 to unlabeled pixels to be segmented.     \n"}, {"name": "future.TrainableSegmenter.predict()", "path": "api/skimage.future#skimage.future.TrainableSegmenter.predict", "type": "future", "text": " \npredict(image) [source]\n \nSegment new image using trained internal classifier.  Parameters \n \nimagendarray \n\nInput image, which can be grayscale or multichannel, and must have a number of dimensions compatible with self.features_func.    Raises \n \nNotFittedError if self.clf has not been fitted yet (use self.fit). \n   \n"}, {"name": "future.TrainableSegmenter.__init__()", "path": "api/skimage.future#skimage.future.TrainableSegmenter.__init__", "type": "future", "text": " \n__init__(clf=None, features_func=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "Geometrical transformations of images", "path": "user_guide/geometrical_transform", "type": "Guide", "text": "Geometrical transformations of images Cropping, resizing and rescaling images Images being NumPy arrays (as described in the A crash course on NumPy for images section), cropping an image can be done with simple slicing operations. Below we crop a 100x100 square corresponding to the top-left corner of the astronaut image. Note that this operation is done for all color channels (the color dimension is the last, third dimension): >>> from skimage import data\n>>> img = data.astronaut()\n>>> top_left = img[:100, :100]\n In order to change the shape of the image, skimage.color provides several functions described in Rescale, resize, and downscale . \nfrom skimage import data, color\nfrom skimage.transform import rescale, resize, downscale_local_mean\n\nimage = color.rgb2gray(data.astronaut())\n\nimage_rescaled = rescale(image, 0.25, anti_aliasing=False)\nimage_resized = resize(image, (image.shape[0] // 4, image.shape[1] // 4),\n                       anti_aliasing=True)\nimage_downscaled = downscale_local_mean(image, (4, 3))\n\n  Projective transforms (homographies) Homographies are transformations of a Euclidean space that preserve the alignment of points. Specific cases of homographies correspond to the conservation of more properties, such as parallelism (affine transformation), shape (similar transformation) or distances (Euclidean transformation). The different types of homographies available in scikit-image are presented in Types of homographies. Projective transformations can either be created using the explicit parameters (e.g. scale, shear, rotation and translation): from skimage import data\nfrom skimage import transform\nfrom skimage import img_as_float\n\ntform = transform.EuclideanTransform(\n   rotation=np.pi / 12.,\n   translation = (100, -20)\n   )\n or the full transformation matrix: from skimage import data\nfrom skimage import transform\nfrom skimage import img_as_float\n\nmatrix = np.array([[np.cos(np.pi/12), -np.sin(np.pi/12), 100],\n                   [np.sin(np.pi/12), np.cos(np.pi/12), -20],\n                   [0, 0, 1]])\ntform = transform.EuclideanTransform(matrix)\n The transformation matrix of a transform is available as its tform.params attribute. Transformations can be composed by multiplying matrices with the @ matrix multiplication operator. Transformation matrices use Homogeneous coordinates, which are the extension of Cartesian coordinates used in Euclidean geometry to the more general projective geometry. In particular, points at infinity can be represented with finite coordinates. Transformations can be applied to images using skimage.transform.warp(): img = img_as_float(data.chelsea())\ntf_img = transform.warp(img, tform.inverse)\n  The different transformations in skimage.transform have a estimate method in order to estimate the parameters of the transformation from two sets of points (the source and the destination), as explained in the Using geometric transformations tutorial: text = data.text()\n\nsrc = np.array([[0, 0], [0, 50], [300, 50], [300, 0]])\ndst = np.array([[155, 15], [65, 40], [260, 130], [360, 95]])\n\ntform3 = transform.ProjectiveTransform()\ntform3.estimate(src, dst)\nwarped = transform.warp(text, tform3, output_shape=(50, 300))\n  The estimate method uses least-squares optimization to minimize the distance between source and optimization. Source and destination points can be determined manually, or using the different methods for feature detection available in skimage.feature, such as  \nCorner detection, \nORB feature detector and binary descriptor, \nBRIEF binary descriptor, etc.  and matching points using skimage.feature.match_descriptors() before estimating transformation parameters. However, spurious matches are often made, and it is advisable to use the RANSAC algorithm (instead of simple least-squares optimization) to improve the robustness to outliers, as explained in Robust matching using RANSAC.  Examples showing applications of transformation estimation are  stereo matching Fundamental matrix estimation and image rectification Using geometric transformations\n  The estimate method is point-based, that is, it uses only a set of points from the source and destination images. For estimating translations (shifts), it is also possible to use a full-field method using all pixels, based on Fourier-space cross-correlation. This method is implemented by skimage.registration.register_translation() and explained in the Image Registration tutorial.  The Using Polar and Log-Polar Transformations for Registration tutorial explains a variant of this full-field method for estimating a rotation, by using first a log-polar transformation.\n"}, {"name": "Getting help on using skimage", "path": "user_guide/getting_help", "type": "Guide", "text": "Getting help on using skimage Besides the user guide, there exist other opportunities to get help on using skimage. Examples gallery The General examples gallery provides graphical examples of typical image processing tasks. By a quick glance at the different thumbnails, the user may find an example close to a typical use case of interest. Each graphical example page displays an introductory paragraph, a figure, and the source code that generated the figure. Downloading the Python source code enables one to modify quickly the example into a case closer to one\u2019s image processing applications. Users are warmly encouraged to report on their use of skimage on the Mailing-list, in order to propose more examples in the future. Contributing examples to the gallery can be done on github (see How to contribute to scikit-image). Search field The quick search field located in the navigation bar of the html documentation can be used to search for specific keywords (segmentation, rescaling, denoising, etc.). API Discovery NumPy provides a lookfor function to search API functions. By default lookfor will search the NumPy API. NumPy lookfor example: `np.lookfor('eigenvector') ` But it can be used to search in modules, by passing in the module name as a string: ` np.lookfor('boundaries', 'skimage') ` or the module itself. `\n> import skimage\n> np.lookfor('boundaries', skimage)\n` Docstrings Docstrings of skimage functions are formatted using Numpy\u2019s documentation standard, starting with a Parameters section for the arguments and a Returns section for the objects returned by the function. Also, most functions include one or more examples. Mailing-list The scikit-image mailing-list is scikit-image@python.org (users should join before posting). This mailing-list is shared by users and developers, and it is the right place to ask any question about skimage, or in general, image processing using Python. Posting snippets of code with minimal examples ensures to get more relevant and focused answers. We would love to hear from how you use skimage for your work on the mailing-list!\n"}, {"name": "Getting started", "path": "user_guide/getting_started", "type": "Guide", "text": "Getting started scikit-image is an image processing Python package that works with numpy arrays. The package is imported as skimage: >>> import skimage\n Most functions of skimage are found within submodules: >>> from skimage import data\n>>> camera = data.camera()\n A list of submodules and functions is found on the API reference webpage. Within scikit-image, images are represented as NumPy arrays, for example 2-D arrays for grayscale 2-D images >>> type(camera)\n<type 'numpy.ndarray'>\n>>> # An image with 512 rows and 512 columns\n>>> camera.shape\n(512, 512)\n The skimage.data submodule provides a set of functions returning example images, that can be used to get started quickly on using scikit-image\u2019s functions: >>> coins = data.coins()\n>>> from skimage import filters\n>>> threshold_value = filters.threshold_otsu(coins)\n>>> threshold_value\n107\n Of course, it is also possible to load your own images as NumPy arrays from image files, using skimage.io.imread(): >>> import os\n>>> filename = os.path.join(skimage.data_dir, 'moon.png')\n>>> from skimage import io\n>>> moon = io.imread(filename)\n Use natsort to load multiple images >>> import os\n>>> from natsort import natsorted, ns\n>>> from skimage import io\n>>> list_files = os.listdir('.')\n>>> list_files\n['01.png', '010.png', '0101.png', '0190.png', '02.png']\n>>> list_files = natsorted(list_files)\n>>> list_files\n['01.png', '02.png', '010.png', '0101.png', '0190.png']\n>>> image_list = []\n>>> for filename in list_files:\n...   image_list.append(io.imread(filename))\n\n"}, {"name": "graph", "path": "api/skimage.graph", "type": "graph", "text": "Module: graph  \nskimage.graph.route_through_array(array, \u2026) Simple example of how to use the MCP and MCP_Geometric classes.  \nskimage.graph.shortest_path(arr[, reach, \u2026]) Find the shortest path through an n-d array from one side to another.  \nskimage.graph.MCP(costs[, offsets, \u2026]) A class for finding the minimum cost path through a given n-d costs array.  \nskimage.graph.MCP_Connect(costs[, offsets, \u2026]) Connect source points using the distance-weighted minimum cost function.  \nskimage.graph.MCP_Flexible(costs[, offsets, \u2026]) Find minimum cost paths through an N-d costs array.  \nskimage.graph.MCP_Geometric(costs[, \u2026]) Find distance-weighted minimum cost paths through an n-d costs array.   route_through_array  \nskimage.graph.route_through_array(array, start, end, fully_connected=True, geometric=True) [source]\n \nSimple example of how to use the MCP and MCP_Geometric classes. See the MCP and MCP_Geometric class documentation for explanation of the path-finding algorithm.  Parameters \n \narrayndarray \n\nArray of costs.  \nstartiterable \n\nn-d index into array defining the starting point  \nenditerable \n\nn-d index into array defining the end point  \nfully_connectedbool (optional) \n\nIf True, diagonal moves are permitted, if False, only axial moves.  \ngeometricbool (optional) \n\nIf True, the MCP_Geometric class is used to calculate costs, if False, the MCP base class is used. See the class documentation for an explanation of the differences between MCP and MCP_Geometric.    Returns \n \npathlist \n\nList of n-d index tuples defining the path from start to end.  \ncostfloat \n\nCost of the path. If geometric is False, the cost of the path is the sum of the values of array along the path. If geometric is True, a finer computation is made (see the documentation of the MCP_Geometric class).      See also  \nMCP, MCP_Geometric\n\n  Examples >>> import numpy as np\n>>> from skimage.graph import route_through_array\n>>>\n>>> image = np.array([[1, 3], [10, 12]])\n>>> image\narray([[ 1,  3],\n       [10, 12]])\n>>> # Forbid diagonal steps\n>>> route_through_array(image, [0, 0], [1, 1], fully_connected=False)\n([(0, 0), (0, 1), (1, 1)], 9.5)\n>>> # Now allow diagonal steps: the path goes directly from start to end\n>>> route_through_array(image, [0, 0], [1, 1])\n([(0, 0), (1, 1)], 9.19238815542512)\n>>> # Cost is the sum of array values along the path (16 = 1 + 3 + 12)\n>>> route_through_array(image, [0, 0], [1, 1], fully_connected=False,\n... geometric=False)\n([(0, 0), (0, 1), (1, 1)], 16.0)\n>>> # Larger array where we display the path that is selected\n>>> image = np.arange((36)).reshape((6, 6))\n>>> image\narray([[ 0,  1,  2,  3,  4,  5],\n       [ 6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17],\n       [18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29],\n       [30, 31, 32, 33, 34, 35]])\n>>> # Find the path with lowest cost\n>>> indices, weight = route_through_array(image, (0, 0), (5, 5))\n>>> indices = np.stack(indices, axis=-1)\n>>> path = np.zeros_like(image)\n>>> path[indices[0], indices[1]] = 1\n>>> path\narray([[1, 1, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1]])\n \n shortest_path  \nskimage.graph.shortest_path(arr, reach=1, axis=-1, output_indexlist=False) [source]\n \nFind the shortest path through an n-d array from one side to another.  Parameters \n \narrndarray of float64 \n\nreachint, optional \n\nBy default (reach = 1), the shortest path can only move one row up or down for every step it moves forward (i.e., the path gradient is limited to 1). reach defines the number of elements that can be skipped along each non-axis dimension at each step.  \naxisint, optional \n\nThe axis along which the path must always move forward (default -1)  \noutput_indexlistbool, optional \n\nSee return value p for explanation.    Returns \n \npiterable of int \n\nFor each step along axis, the coordinate of the shortest path. If output_indexlist is True, then the path is returned as a list of n-d tuples that index into arr. If False, then the path is returned as an array listing the coordinates of the path along the non-axis dimensions for each step along the axis dimension. That is, p.shape == (arr.shape[axis], arr.ndim-1) except that p is squeezed before returning so if arr.ndim == 2, then p.shape == (arr.shape[axis],)  \ncostfloat \n\nCost of path. This is the absolute sum of all the differences along the path.     \n MCP  \nclass skimage.graph.MCP(costs, offsets=None, fully_connected=True, sampling=None)  \nBases: object A class for finding the minimum cost path through a given n-d costs array. Given an n-d costs array, this class can be used to find the minimum-cost path through that array from any set of points to any other set of points. Basic usage is to initialize the class and call find_costs() with a one or more starting indices (and an optional list of end indices). After that, call traceback() one or more times to find the path from any given end-position to the closest starting index. New paths through the same costs array can be found by calling find_costs() repeatedly. The cost of a path is calculated simply as the sum of the values of the costs array at each point on the path. The class MCP_Geometric, on the other hand, accounts for the fact that diagonal vs. axial moves are of different lengths, and weights the path cost accordingly. Array elements with infinite or negative costs will simply be ignored, as will paths whose cumulative cost overflows to infinite.  Parameters \n \ncostsndarray \n\noffsetsiterable, optional \n\nA list of offset tuples: each offset specifies a valid move from a given n-d position. If not provided, offsets corresponding to a singly- or fully-connected n-d neighborhood will be constructed with make_offsets(), using the fully_connected parameter value.  \nfully_connectedbool, optional \n\nIf no offsets are provided, this determines the connectivity of the generated neighborhood. If true, the path may go along diagonals between elements of the costs array; otherwise only axial moves are permitted.  \nsamplingtuple, optional \n\nFor each dimension, specifies the distance between two cells/voxels. If not given or None, the distance is assumed unit.    Attributes \n \noffsetsndarray \n\nEquivalent to the offsets provided to the constructor, or if none were so provided, the offsets created for the requested n-d neighborhood. These are useful for interpreting the traceback array returned by the find_costs() method.      \n__init__(costs, offsets=None, fully_connected=True, sampling=None)  \nSee class documentation. \n  \nfind_costs()  \nFind the minimum-cost path from the given starting points. This method finds the minimum-cost path to the specified ending indices from any one of the specified starting indices. If no end positions are given, then the minimum-cost path to every position in the costs array will be found.  Parameters \n \nstartsiterable \n\nA list of n-d starting indices (where n is the dimension of the costs array). The minimum cost path to the closest/cheapest starting point will be found.  \nendsiterable, optional \n\nA list of n-d ending indices.  \nfind_all_endsbool, optional \n\nIf \u2018True\u2019 (default), the minimum-cost-path to every specified end-position will be found; otherwise the algorithm will stop when a a path is found to any end-position. (If no ends were specified, then this parameter has no effect.)    Returns \n \ncumulative_costsndarray \n\nSame shape as the costs array; this array records the minimum cost path from the nearest/cheapest starting index to each index considered. (If ends were specified, not all elements in the array will necessarily be considered: positions not evaluated will have a cumulative cost of inf. If find_all_ends is \u2018False\u2019, only one of the specified end-positions will have a finite cumulative cost.)  \ntracebackndarray \n\nSame shape as the costs array; this array contains the offset to any given index from its predecessor index. The offset indices index into the offsets attribute, which is a array of n-d offsets. In the 2-d case, if offsets[traceback[x, y]] is (-1, -1), that means that the predecessor of [x, y] in the minimum cost path to some start position is [x+1, y+1]. Note that if the offset_index is -1, then the given index was not considered.     \n  \ngoal_reached()  \nint goal_reached(int index, float cumcost) This method is called each iteration after popping an index from the heap, before examining the neighbours. This method can be overloaded to modify the behavior of the MCP algorithm. An example might be to stop the algorithm when a certain cumulative cost is reached, or when the front is a certain distance away from the seed point. This method should return 1 if the algorithm should not check the current point\u2019s neighbours and 2 if the algorithm is now done. \n  \ntraceback(end)  \nTrace a minimum cost path through the pre-calculated traceback array. This convenience function reconstructs the the minimum cost path to a given end position from one of the starting indices provided to find_costs(), which must have been called previously. This function can be called as many times as desired after find_costs() has been run.  Parameters \n \nenditerable \n\nAn n-d index into the costs array.    Returns \n \ntracebacklist of n-d tuples \n\nA list of indices into the costs array, starting with one of the start positions passed to find_costs(), and ending with the given end index. These indices specify the minimum-cost path from any given start index to the end index. (The total cost of that path can be read out from the cumulative_costs array returned by find_costs().)     \n \n MCP_Connect  \nclass skimage.graph.MCP_Connect(costs, offsets=None, fully_connected=True)  \nBases: skimage.graph._mcp.MCP Connect source points using the distance-weighted minimum cost function. A front is grown from each seed point simultaneously, while the origin of the front is tracked as well. When two fronts meet, create_connection() is called. This method must be overloaded to deal with the found edges in a way that is appropriate for the application.  \n__init__(*args, **kwargs)  \nInitialize self. See help(type(self)) for accurate signature. \n  \ncreate_connection()  \ncreate_connection id1, id2, pos1, pos2, cost1, cost2) Overload this method to keep track of the connections that are found during MCP processing. Note that a connection with the same ids can be found multiple times (but with different positions and costs). At the time that this method is called, both points are \u201cfrozen\u201d and will not be visited again by the MCP algorithm.  Parameters \n \nid1int \n\nThe seed point id where the first neighbor originated from.  \nid2int \n\nThe seed point id where the second neighbor originated from.  \npos1tuple \n\nThe index of of the first neighbour in the connection.  \npos2tuple \n\nThe index of of the second neighbour in the connection.  \ncost1float \n\nThe cumulative cost at pos1.  \ncost2float \n\nThe cumulative costs at pos2.     \n \n MCP_Flexible  \nclass skimage.graph.MCP_Flexible(costs, offsets=None, fully_connected=True)  \nBases: skimage.graph._mcp.MCP Find minimum cost paths through an N-d costs array. See the documentation for MCP for full details. This class differs from MCP in that several methods can be overloaded (from pure Python) to modify the behavior of the algorithm and/or create custom algorithms based on MCP. Note that goal_reached can also be overloaded in the MCP class.  \n__init__(costs, offsets=None, fully_connected=True, sampling=None)  \nSee class documentation. \n  \nexamine_neighbor(index, new_index, offset_length)  \nThis method is called once for every pair of neighboring nodes, as soon as both nodes are frozen. This method can be overloaded to obtain information about neightboring nodes, and/or to modify the behavior of the MCP algorithm. One example is the MCP_Connect class, which checks for meeting fronts using this hook. \n  \ntravel_cost(old_cost, new_cost, offset_length)  \nThis method calculates the travel cost for going from the current node to the next. The default implementation returns new_cost. Overload this method to adapt the behaviour of the algorithm. \n  \nupdate_node(index, new_index, offset_length)  \nThis method is called when a node is updated, right after new_index is pushed onto the heap and the traceback map is updated. This method can be overloaded to keep track of other arrays that are used by a specific implementation of the algorithm. For instance the MCP_Connect class uses it to update an id map. \n \n MCP_Geometric  \nclass skimage.graph.MCP_Geometric(costs, offsets=None, fully_connected=True)  \nBases: skimage.graph._mcp.MCP Find distance-weighted minimum cost paths through an n-d costs array. See the documentation for MCP for full details. This class differs from MCP in that the cost of a path is not simply the sum of the costs along that path. This class instead assumes that the costs array contains at each position the \u201ccost\u201d of a unit distance of travel through that position. For example, a move (in 2-d) from (1, 1) to (1, 2) is assumed to originate in the center of the pixel (1, 1) and terminate in the center of (1, 2). The entire move is of distance 1, half through (1, 1) and half through (1, 2); thus the cost of that move is (1/2)*costs[1,1] + (1/2)*costs[1,2]. On the other hand, a move from (1, 1) to (2, 2) is along the diagonal and is sqrt(2) in length. Half of this move is within the pixel (1, 1) and the other half in (2, 2), so the cost of this move is calculated as (sqrt(2)/2)*costs[1,1] + (sqrt(2)/2)*costs[2,2]. These calculations don\u2019t make a lot of sense with offsets of magnitude greater than 1. Use the sampling argument in order to deal with anisotropic data.  \n__init__(costs, offsets=None, fully_connected=True, sampling=None)  \nSee class documentation. \n \n\n"}, {"name": "graph.MCP", "path": "api/skimage.graph#skimage.graph.MCP", "type": "graph", "text": " \nclass skimage.graph.MCP(costs, offsets=None, fully_connected=True, sampling=None)  \nBases: object A class for finding the minimum cost path through a given n-d costs array. Given an n-d costs array, this class can be used to find the minimum-cost path through that array from any set of points to any other set of points. Basic usage is to initialize the class and call find_costs() with a one or more starting indices (and an optional list of end indices). After that, call traceback() one or more times to find the path from any given end-position to the closest starting index. New paths through the same costs array can be found by calling find_costs() repeatedly. The cost of a path is calculated simply as the sum of the values of the costs array at each point on the path. The class MCP_Geometric, on the other hand, accounts for the fact that diagonal vs. axial moves are of different lengths, and weights the path cost accordingly. Array elements with infinite or negative costs will simply be ignored, as will paths whose cumulative cost overflows to infinite.  Parameters \n \ncostsndarray \n\noffsetsiterable, optional \n\nA list of offset tuples: each offset specifies a valid move from a given n-d position. If not provided, offsets corresponding to a singly- or fully-connected n-d neighborhood will be constructed with make_offsets(), using the fully_connected parameter value.  \nfully_connectedbool, optional \n\nIf no offsets are provided, this determines the connectivity of the generated neighborhood. If true, the path may go along diagonals between elements of the costs array; otherwise only axial moves are permitted.  \nsamplingtuple, optional \n\nFor each dimension, specifies the distance between two cells/voxels. If not given or None, the distance is assumed unit.    Attributes \n \noffsetsndarray \n\nEquivalent to the offsets provided to the constructor, or if none were so provided, the offsets created for the requested n-d neighborhood. These are useful for interpreting the traceback array returned by the find_costs() method.      \n__init__(costs, offsets=None, fully_connected=True, sampling=None)  \nSee class documentation. \n  \nfind_costs()  \nFind the minimum-cost path from the given starting points. This method finds the minimum-cost path to the specified ending indices from any one of the specified starting indices. If no end positions are given, then the minimum-cost path to every position in the costs array will be found.  Parameters \n \nstartsiterable \n\nA list of n-d starting indices (where n is the dimension of the costs array). The minimum cost path to the closest/cheapest starting point will be found.  \nendsiterable, optional \n\nA list of n-d ending indices.  \nfind_all_endsbool, optional \n\nIf \u2018True\u2019 (default), the minimum-cost-path to every specified end-position will be found; otherwise the algorithm will stop when a a path is found to any end-position. (If no ends were specified, then this parameter has no effect.)    Returns \n \ncumulative_costsndarray \n\nSame shape as the costs array; this array records the minimum cost path from the nearest/cheapest starting index to each index considered. (If ends were specified, not all elements in the array will necessarily be considered: positions not evaluated will have a cumulative cost of inf. If find_all_ends is \u2018False\u2019, only one of the specified end-positions will have a finite cumulative cost.)  \ntracebackndarray \n\nSame shape as the costs array; this array contains the offset to any given index from its predecessor index. The offset indices index into the offsets attribute, which is a array of n-d offsets. In the 2-d case, if offsets[traceback[x, y]] is (-1, -1), that means that the predecessor of [x, y] in the minimum cost path to some start position is [x+1, y+1]. Note that if the offset_index is -1, then the given index was not considered.     \n  \ngoal_reached()  \nint goal_reached(int index, float cumcost) This method is called each iteration after popping an index from the heap, before examining the neighbours. This method can be overloaded to modify the behavior of the MCP algorithm. An example might be to stop the algorithm when a certain cumulative cost is reached, or when the front is a certain distance away from the seed point. This method should return 1 if the algorithm should not check the current point\u2019s neighbours and 2 if the algorithm is now done. \n  \ntraceback(end)  \nTrace a minimum cost path through the pre-calculated traceback array. This convenience function reconstructs the the minimum cost path to a given end position from one of the starting indices provided to find_costs(), which must have been called previously. This function can be called as many times as desired after find_costs() has been run.  Parameters \n \nenditerable \n\nAn n-d index into the costs array.    Returns \n \ntracebacklist of n-d tuples \n\nA list of indices into the costs array, starting with one of the start positions passed to find_costs(), and ending with the given end index. These indices specify the minimum-cost path from any given start index to the end index. (The total cost of that path can be read out from the cumulative_costs array returned by find_costs().)     \n \n"}, {"name": "graph.MCP.find_costs()", "path": "api/skimage.graph#skimage.graph.MCP.find_costs", "type": "graph", "text": " \nfind_costs()  \nFind the minimum-cost path from the given starting points. This method finds the minimum-cost path to the specified ending indices from any one of the specified starting indices. If no end positions are given, then the minimum-cost path to every position in the costs array will be found.  Parameters \n \nstartsiterable \n\nA list of n-d starting indices (where n is the dimension of the costs array). The minimum cost path to the closest/cheapest starting point will be found.  \nendsiterable, optional \n\nA list of n-d ending indices.  \nfind_all_endsbool, optional \n\nIf \u2018True\u2019 (default), the minimum-cost-path to every specified end-position will be found; otherwise the algorithm will stop when a a path is found to any end-position. (If no ends were specified, then this parameter has no effect.)    Returns \n \ncumulative_costsndarray \n\nSame shape as the costs array; this array records the minimum cost path from the nearest/cheapest starting index to each index considered. (If ends were specified, not all elements in the array will necessarily be considered: positions not evaluated will have a cumulative cost of inf. If find_all_ends is \u2018False\u2019, only one of the specified end-positions will have a finite cumulative cost.)  \ntracebackndarray \n\nSame shape as the costs array; this array contains the offset to any given index from its predecessor index. The offset indices index into the offsets attribute, which is a array of n-d offsets. In the 2-d case, if offsets[traceback[x, y]] is (-1, -1), that means that the predecessor of [x, y] in the minimum cost path to some start position is [x+1, y+1]. Note that if the offset_index is -1, then the given index was not considered.     \n"}, {"name": "graph.MCP.goal_reached()", "path": "api/skimage.graph#skimage.graph.MCP.goal_reached", "type": "graph", "text": " \ngoal_reached()  \nint goal_reached(int index, float cumcost) This method is called each iteration after popping an index from the heap, before examining the neighbours. This method can be overloaded to modify the behavior of the MCP algorithm. An example might be to stop the algorithm when a certain cumulative cost is reached, or when the front is a certain distance away from the seed point. This method should return 1 if the algorithm should not check the current point\u2019s neighbours and 2 if the algorithm is now done. \n"}, {"name": "graph.MCP.traceback()", "path": "api/skimage.graph#skimage.graph.MCP.traceback", "type": "graph", "text": " \ntraceback(end)  \nTrace a minimum cost path through the pre-calculated traceback array. This convenience function reconstructs the the minimum cost path to a given end position from one of the starting indices provided to find_costs(), which must have been called previously. This function can be called as many times as desired after find_costs() has been run.  Parameters \n \nenditerable \n\nAn n-d index into the costs array.    Returns \n \ntracebacklist of n-d tuples \n\nA list of indices into the costs array, starting with one of the start positions passed to find_costs(), and ending with the given end index. These indices specify the minimum-cost path from any given start index to the end index. (The total cost of that path can be read out from the cumulative_costs array returned by find_costs().)     \n"}, {"name": "graph.MCP.__init__()", "path": "api/skimage.graph#skimage.graph.MCP.__init__", "type": "graph", "text": " \n__init__(costs, offsets=None, fully_connected=True, sampling=None)  \nSee class documentation. \n"}, {"name": "graph.MCP_Connect", "path": "api/skimage.graph#skimage.graph.MCP_Connect", "type": "graph", "text": " \nclass skimage.graph.MCP_Connect(costs, offsets=None, fully_connected=True)  \nBases: skimage.graph._mcp.MCP Connect source points using the distance-weighted minimum cost function. A front is grown from each seed point simultaneously, while the origin of the front is tracked as well. When two fronts meet, create_connection() is called. This method must be overloaded to deal with the found edges in a way that is appropriate for the application.  \n__init__(*args, **kwargs)  \nInitialize self. See help(type(self)) for accurate signature. \n  \ncreate_connection()  \ncreate_connection id1, id2, pos1, pos2, cost1, cost2) Overload this method to keep track of the connections that are found during MCP processing. Note that a connection with the same ids can be found multiple times (but with different positions and costs). At the time that this method is called, both points are \u201cfrozen\u201d and will not be visited again by the MCP algorithm.  Parameters \n \nid1int \n\nThe seed point id where the first neighbor originated from.  \nid2int \n\nThe seed point id where the second neighbor originated from.  \npos1tuple \n\nThe index of of the first neighbour in the connection.  \npos2tuple \n\nThe index of of the second neighbour in the connection.  \ncost1float \n\nThe cumulative cost at pos1.  \ncost2float \n\nThe cumulative costs at pos2.     \n \n"}, {"name": "graph.MCP_Connect.create_connection()", "path": "api/skimage.graph#skimage.graph.MCP_Connect.create_connection", "type": "graph", "text": " \ncreate_connection()  \ncreate_connection id1, id2, pos1, pos2, cost1, cost2) Overload this method to keep track of the connections that are found during MCP processing. Note that a connection with the same ids can be found multiple times (but with different positions and costs). At the time that this method is called, both points are \u201cfrozen\u201d and will not be visited again by the MCP algorithm.  Parameters \n \nid1int \n\nThe seed point id where the first neighbor originated from.  \nid2int \n\nThe seed point id where the second neighbor originated from.  \npos1tuple \n\nThe index of of the first neighbour in the connection.  \npos2tuple \n\nThe index of of the second neighbour in the connection.  \ncost1float \n\nThe cumulative cost at pos1.  \ncost2float \n\nThe cumulative costs at pos2.     \n"}, {"name": "graph.MCP_Connect.__init__()", "path": "api/skimage.graph#skimage.graph.MCP_Connect.__init__", "type": "graph", "text": " \n__init__(*args, **kwargs)  \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "graph.MCP_Flexible", "path": "api/skimage.graph#skimage.graph.MCP_Flexible", "type": "graph", "text": " \nclass skimage.graph.MCP_Flexible(costs, offsets=None, fully_connected=True)  \nBases: skimage.graph._mcp.MCP Find minimum cost paths through an N-d costs array. See the documentation for MCP for full details. This class differs from MCP in that several methods can be overloaded (from pure Python) to modify the behavior of the algorithm and/or create custom algorithms based on MCP. Note that goal_reached can also be overloaded in the MCP class.  \n__init__(costs, offsets=None, fully_connected=True, sampling=None)  \nSee class documentation. \n  \nexamine_neighbor(index, new_index, offset_length)  \nThis method is called once for every pair of neighboring nodes, as soon as both nodes are frozen. This method can be overloaded to obtain information about neightboring nodes, and/or to modify the behavior of the MCP algorithm. One example is the MCP_Connect class, which checks for meeting fronts using this hook. \n  \ntravel_cost(old_cost, new_cost, offset_length)  \nThis method calculates the travel cost for going from the current node to the next. The default implementation returns new_cost. Overload this method to adapt the behaviour of the algorithm. \n  \nupdate_node(index, new_index, offset_length)  \nThis method is called when a node is updated, right after new_index is pushed onto the heap and the traceback map is updated. This method can be overloaded to keep track of other arrays that are used by a specific implementation of the algorithm. For instance the MCP_Connect class uses it to update an id map. \n \n"}, {"name": "graph.MCP_Flexible.examine_neighbor()", "path": "api/skimage.graph#skimage.graph.MCP_Flexible.examine_neighbor", "type": "graph", "text": " \nexamine_neighbor(index, new_index, offset_length)  \nThis method is called once for every pair of neighboring nodes, as soon as both nodes are frozen. This method can be overloaded to obtain information about neightboring nodes, and/or to modify the behavior of the MCP algorithm. One example is the MCP_Connect class, which checks for meeting fronts using this hook. \n"}, {"name": "graph.MCP_Flexible.travel_cost()", "path": "api/skimage.graph#skimage.graph.MCP_Flexible.travel_cost", "type": "graph", "text": " \ntravel_cost(old_cost, new_cost, offset_length)  \nThis method calculates the travel cost for going from the current node to the next. The default implementation returns new_cost. Overload this method to adapt the behaviour of the algorithm. \n"}, {"name": "graph.MCP_Flexible.update_node()", "path": "api/skimage.graph#skimage.graph.MCP_Flexible.update_node", "type": "graph", "text": " \nupdate_node(index, new_index, offset_length)  \nThis method is called when a node is updated, right after new_index is pushed onto the heap and the traceback map is updated. This method can be overloaded to keep track of other arrays that are used by a specific implementation of the algorithm. For instance the MCP_Connect class uses it to update an id map. \n"}, {"name": "graph.MCP_Flexible.__init__()", "path": "api/skimage.graph#skimage.graph.MCP_Flexible.__init__", "type": "graph", "text": " \n__init__(costs, offsets=None, fully_connected=True, sampling=None)  \nSee class documentation. \n"}, {"name": "graph.MCP_Geometric", "path": "api/skimage.graph#skimage.graph.MCP_Geometric", "type": "graph", "text": " \nclass skimage.graph.MCP_Geometric(costs, offsets=None, fully_connected=True)  \nBases: skimage.graph._mcp.MCP Find distance-weighted minimum cost paths through an n-d costs array. See the documentation for MCP for full details. This class differs from MCP in that the cost of a path is not simply the sum of the costs along that path. This class instead assumes that the costs array contains at each position the \u201ccost\u201d of a unit distance of travel through that position. For example, a move (in 2-d) from (1, 1) to (1, 2) is assumed to originate in the center of the pixel (1, 1) and terminate in the center of (1, 2). The entire move is of distance 1, half through (1, 1) and half through (1, 2); thus the cost of that move is (1/2)*costs[1,1] + (1/2)*costs[1,2]. On the other hand, a move from (1, 1) to (2, 2) is along the diagonal and is sqrt(2) in length. Half of this move is within the pixel (1, 1) and the other half in (2, 2), so the cost of this move is calculated as (sqrt(2)/2)*costs[1,1] + (sqrt(2)/2)*costs[2,2]. These calculations don\u2019t make a lot of sense with offsets of magnitude greater than 1. Use the sampling argument in order to deal with anisotropic data.  \n__init__(costs, offsets=None, fully_connected=True, sampling=None)  \nSee class documentation. \n \n"}, {"name": "graph.MCP_Geometric.__init__()", "path": "api/skimage.graph#skimage.graph.MCP_Geometric.__init__", "type": "graph", "text": " \n__init__(costs, offsets=None, fully_connected=True, sampling=None)  \nSee class documentation. \n"}, {"name": "graph.route_through_array()", "path": "api/skimage.graph#skimage.graph.route_through_array", "type": "graph", "text": " \nskimage.graph.route_through_array(array, start, end, fully_connected=True, geometric=True) [source]\n \nSimple example of how to use the MCP and MCP_Geometric classes. See the MCP and MCP_Geometric class documentation for explanation of the path-finding algorithm.  Parameters \n \narrayndarray \n\nArray of costs.  \nstartiterable \n\nn-d index into array defining the starting point  \nenditerable \n\nn-d index into array defining the end point  \nfully_connectedbool (optional) \n\nIf True, diagonal moves are permitted, if False, only axial moves.  \ngeometricbool (optional) \n\nIf True, the MCP_Geometric class is used to calculate costs, if False, the MCP base class is used. See the class documentation for an explanation of the differences between MCP and MCP_Geometric.    Returns \n \npathlist \n\nList of n-d index tuples defining the path from start to end.  \ncostfloat \n\nCost of the path. If geometric is False, the cost of the path is the sum of the values of array along the path. If geometric is True, a finer computation is made (see the documentation of the MCP_Geometric class).      See also  \nMCP, MCP_Geometric\n\n  Examples >>> import numpy as np\n>>> from skimage.graph import route_through_array\n>>>\n>>> image = np.array([[1, 3], [10, 12]])\n>>> image\narray([[ 1,  3],\n       [10, 12]])\n>>> # Forbid diagonal steps\n>>> route_through_array(image, [0, 0], [1, 1], fully_connected=False)\n([(0, 0), (0, 1), (1, 1)], 9.5)\n>>> # Now allow diagonal steps: the path goes directly from start to end\n>>> route_through_array(image, [0, 0], [1, 1])\n([(0, 0), (1, 1)], 9.19238815542512)\n>>> # Cost is the sum of array values along the path (16 = 1 + 3 + 12)\n>>> route_through_array(image, [0, 0], [1, 1], fully_connected=False,\n... geometric=False)\n([(0, 0), (0, 1), (1, 1)], 16.0)\n>>> # Larger array where we display the path that is selected\n>>> image = np.arange((36)).reshape((6, 6))\n>>> image\narray([[ 0,  1,  2,  3,  4,  5],\n       [ 6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17],\n       [18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29],\n       [30, 31, 32, 33, 34, 35]])\n>>> # Find the path with lowest cost\n>>> indices, weight = route_through_array(image, (0, 0), (5, 5))\n>>> indices = np.stack(indices, axis=-1)\n>>> path = np.zeros_like(image)\n>>> path[indices[0], indices[1]] = 1\n>>> path\narray([[1, 1, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1]])\n \n"}, {"name": "graph.shortest_path()", "path": "api/skimage.graph#skimage.graph.shortest_path", "type": "graph", "text": " \nskimage.graph.shortest_path(arr, reach=1, axis=-1, output_indexlist=False) [source]\n \nFind the shortest path through an n-d array from one side to another.  Parameters \n \narrndarray of float64 \n\nreachint, optional \n\nBy default (reach = 1), the shortest path can only move one row up or down for every step it moves forward (i.e., the path gradient is limited to 1). reach defines the number of elements that can be skipped along each non-axis dimension at each step.  \naxisint, optional \n\nThe axis along which the path must always move forward (default -1)  \noutput_indexlistbool, optional \n\nSee return value p for explanation.    Returns \n \npiterable of int \n\nFor each step along axis, the coordinate of the shortest path. If output_indexlist is True, then the path is returned as a list of n-d tuples that index into arr. If False, then the path is returned as an array listing the coordinates of the path along the non-axis dimensions for each step along the axis dimension. That is, p.shape == (arr.shape[axis], arr.ndim-1) except that p is squeezed before returning so if arr.ndim == 2, then p.shape == (arr.shape[axis],)  \ncostfloat \n\nCost of path. This is the absolute sum of all the differences along the path.     \n"}, {"name": "Handling Video Files", "path": "user_guide/video", "type": "Guide", "text": "Handling Video Files Sometimes it is necessary to read a sequence of images from a standard video file, such as .avi and .mov files. In a scientific context, it is usually better to avoid these formats in favor of a simple directory of images or a multi-dimensional TIF. Video formats are more difficult to read piecemeal, typically do not support random frame access or research-minded meta data, and use lossy compression if not carefully configured. But video files are in widespread use, and they are easy to share, so it is convenient to be equipped to read and write them when necessary. Tools for reading video files vary in their ease of installation and use, their disk and memory usage, and their cross-platform compatibility. This is a practical guide. A Workaround: Convert the Video to an Image Sequence For a one-off solution, the simplest, surest route is to convert the video to a collection of sequentially-numbered image files, often called an image sequence. Then the images files can be read into an ImageCollection by skimage.io.imread_collection. Converting the video to frames can be done easily in ImageJ, a cross-platform, GUI-based program from the bio-imaging community, or FFmpeg, a powerful command-line utility for manipulating video files. In FFmpeg, the following command generates an image file from each frame in a video. The files are numbered with five digits, padded on the left with zeros. ffmpeg -i \"video.mov\" -f image2 \"video-frame%05d.png\"\n More information is available in an FFmpeg tutorial on image sequences. Generating an image sequence has disadvantages: they can be large and unwieldy, and generating them can take some time. It is generally preferable to work directly with the original video file. For a more direct solution, we need to execute FFmpeg or LibAV from Python to read frames from the video. FFmpeg and LibAV are two large open-source projects that decode video from the sprawling variety of formats used in the wild. There are several ways to use them from Python. Each, unfortunately, has some disadvantages. PyAV PyAV uses FFmpeg\u2019s (or LibAV\u2019s) libraries to read image data directly from the video file. It invokes them using Cython bindings, so it is very fast. import av\nv = av.open('path/to/video.mov')\n PyAV\u2019s API reflects the way frames are stored in a video file. for packet in container.demux():\n    for frame in packet.decode():\n        if frame.type == 'video':\n            img = frame.to_image()  # PIL/Pillow image\n            arr = np.asarray(img)  # numpy array\n            # Do something!\n Adding Random Access to PyAV The Video class in PIMS invokes PyAV and adds additional functionality to solve a common problem in scientific applications, accessing a video by frame number. Video file formats are designed to be searched in an approximate way, by time, and they do not support an efficient means of seeking a specific frame number. PIMS adds this missing functionality by decoding (but not reading) the entire video at and producing an internal table of contents that supports indexing by frame. import pims\nv = pims.Video('path/to/video.mov')\nv[-1]  # a 2D numpy array representing the last frame\n MoviePy Moviepy invokes FFmpeg through a subprocess, pipes the decoded video from FFmpeg into RAM, and reads it out. This approach is straightforward, but it can be brittle, and it\u2019s not workable for large videos that exceed available RAM. It works on all platforms if FFmpeg is installed. Since it does not link to FFmpeg\u2019s underlying libraries, it is easier to install but about half as fast. from moviepy.editor import VideoFileClip\nmyclip = VideoFileClip(\"some_video.avi\")\n Imageio Imageio takes the same approach as MoviePy. It supports a wide range of other image file formats as well. import imageio\nfilename = '/tmp/file.mp4'\nvid = imageio.get_reader(filename,  'ffmpeg')\n\nfor num, image in vid.iter_data():\n    print(image.mean())\n\nmetadata = vid.get_meta_data()\n OpenCV Finally, another solution is the VideoReader class in OpenCV, which has bindings to FFmpeg. If you need OpenCV for other reasons, then this may be the best approach.\n"}, {"name": "How to parallelize loops", "path": "user_guide/tutorial_parallelization", "type": "Guide", "text": "How to parallelize loops In image processing, we frequently apply the same algorithm on a large batch of images. In this paragraph, we propose to use joblib to parallelize loops. Here is an example of such repetitive tasks: from skimage import data, color, util\nfrom skimage.restoration import denoise_tv_chambolle\nfrom skimage.feature import hog\n\ndef task(image):\n    \"\"\"\n    Apply some functions and return an image.\n    \"\"\"\n    image = denoise_tv_chambolle(image[0][0], weight=0.1, multichannel=True)\n    fd, hog_image = hog(color.rgb2gray(image), orientations=8,\n                        pixels_per_cell=(16, 16), cells_per_block=(1, 1),\n                        visualize=True)\n    return hog_image\n\n\n# Prepare images\nhubble = data.hubble_deep_field()\nwidth = 10\npics = util.view_as_windows(hubble, (width, hubble.shape[1], hubble.shape[2]), step=width)\n To call the function task on each element of the list pics, it is usual to write a for loop. To measure the execution time of this loop, you can use ipython and measure the execution time with %timeit. def classic_loop():\n    for image in pics:\n        task(image)\n\n\n%timeit classic_loop()\n Another equivalent way to code this loop is to use a comprehension list which has the same efficiency. def comprehension_loop():\n    [task(image) for image in pics]\n\n%timeit comprehension_loop()\n joblib is a library providing an easy way to parallelize for loops once we have a comprehension list. The number of jobs can be specified. from joblib import Parallel, delayed\ndef joblib_loop():\n    Parallel(n_jobs=4)(delayed(task)(i) for i in pics)\n\n%timeit joblib_loop()\n\n"}, {"name": "I/O Plugin Infrastructure", "path": "user_guide/plugins", "type": "Guide", "text": "I/O Plugin Infrastructure A plugin consists of two files, the source and the descriptor .ini. Let\u2019s say we\u2019d like to provide a plugin for imshow using matplotlib. We\u2019ll call our plugin mpl: skimage/io/_plugins/mpl.py\nskimage/io/_plugins/mpl.ini\n The name of the .py and .ini files must correspond. Inside the .ini file, we give the plugin meta-data: [mpl] <-- name of the plugin, may be anything\ndescription = Matplotlib image I/O plugin\nprovides = imshow <-- a comma-separated list, one or more of\n                      imshow, imsave, imread, _app_show\n The \u201cprovides\u201d-line lists all the functions provided by the plugin. Since our plugin provides imshow, we have to define it inside mpl.py: # This is mpl.py\n\nimport matplotlib.pyplot as plt\n\ndef imshow(img):\n    plt.imshow(img)\n Note that, by default, imshow is non-blocking, so a special function _app_show must be provided to block the GUI. We can modify our plugin to provide it as follows: [mpl]\nprovides = imshow, _app_show\n # This is mpl.py\n\nimport matplotlib.pyplot as plt\n\ndef imshow(img):\n    plt.imshow(img)\n\ndef _app_show():\n    plt.show()\n Any plugin in the _plugins directory is automatically examined by skimage.io upon import. You may list all the plugins on your system: >>> import skimage.io as io\n>>> io.find_available_plugins()\n{'gtk': ['imshow'],\n 'matplotlib': ['imshow', 'imread', 'imread_collection'],\n 'pil': ['imread', 'imsave', 'imread_collection'],\n 'qt': ['imshow', 'imsave', 'imread', 'imread_collection'],\n 'test': ['imsave', 'imshow', 'imread', 'imread_collection'],}\n or only those already loaded: >>> io.find_available_plugins(loaded=True)\n{'matplotlib': ['imshow', 'imread', 'imread_collection'],\n 'pil': ['imread', 'imsave', 'imread_collection']}\n A plugin is loaded using the use_plugin command: >>> import skimage.io as io\n>>> io.use_plugin('pil') # Use all capabilities provided by PIL\n or >>> io.use_plugin('pil', 'imread') # Use only the imread capability of PIL\n Note that, if more than one plugin provides certain functionality, the last plugin loaded is used. To query a plugin\u2019s capabilities, use plugin_info: >>> io.plugin_info('pil')\n>>>\n{'description': 'Image reading via the Python Imaging Library',\n 'provides': 'imread, imsave'}\n\n"}, {"name": "Image adjustment: transforming image content", "path": "user_guide/transforming_image_data", "type": "Guide", "text": "Image adjustment: transforming image content Color manipulation Most functions for manipulating color channels are found in the submodule skimage.color. Conversion between color models Color images can be represented using different color spaces. One of the most common color spaces is the RGB space, where an image has red, green and blue channels. However, other color models are widely used, such as the HSV color model, where hue, saturation and value are independent channels, or the CMYK model used for printing. skimage.color provides utility functions to convert images to and from different color spaces. Integer-type arrays can be transformed to floating-point type by the conversion operation: >>> # bright saturated red\n>>> red_pixel_rgb = np.array([[[255, 0, 0]]], dtype=np.uint8)\n>>> color.rgb2hsv(red_pixel_rgb)\narray([[[ 0.,  1.,  1.]]])\n>>> #\u00a0darker saturated blue\n>>> dark_blue_pixel_rgb = np.array([[[0, 0, 100]]], dtype=np.uint8)\n>>> color.rgb2hsv(dark_blue_pixel_rgb)\narray([[[ 0.66666667,  1.        ,  0.39215686]]])\n>>> # less saturated pink\n>>> pink_pixel_rgb = np.array([[[255, 100, 255]]], dtype=np.uint8)\n>>> color.rgb2hsv(pink_pixel_rgb)\narray([[[ 0.83333333,  0.60784314,  1.        ]]])\n Conversion from RGBA to RGB - Removing alpha channel through alpha blending Converting an RGBA image to an RGB image by alpha blending it with a background is realized with rgba2rgb() >>> from skimage.color import rgba2rgb\n>>> from skimage import data\n>>> img_rgba = data.logo()\n>>> img_rgb = rgba2rgb(img_rgba)\n Conversion between color and gray values Converting an RGB image to a grayscale image is realized with rgb2gray() >>> from skimage.color import rgb2gray\n>>> from skimage import data\n>>> img = data.astronaut()\n>>> img_gray = rgb2gray(img)\n rgb2gray() uses a non-uniform weighting of color channels, because of the different sensitivity of the human eye to different colors. Therefore, such a weighting ensures luminance preservation from RGB to grayscale: >>> red_pixel = np.array([[[255, 0, 0]]], dtype=np.uint8)\n>>> color.rgb2gray(red_pixel)\narray([[ 0.2125]])\n>>> green_pixel = np.array([[[0, 255, 0]]], dtype=np.uint8)\n>>> color.rgb2gray(green_pixel)\narray([[ 0.7154]])\n Converting a grayscale image to RGB with gray2rgb() simply duplicates the gray values over the three color channels. Image inversion An inverted image is also called complementary image. For binary images, True values become False and conversely. For grayscale images, pixel values are replaced by the difference of the maximum value of the data type and the actual value. For RGB images, the same operation is done for each channel. This operation can be achieved with skimage.util.invert(): >>> from skimage import util\n>>> img = data.camera()\n>>> inverted_img = util.invert(img)\n Painting images with labels label2rgb() can be used to superimpose colors on a grayscale image using an array of labels to encode the regions to be represented with the same color.   Examples:  Tinting gray-scale images Find the intersection of two segmentations RAG Thresholding   Contrast and exposure Image pixels can take values determined by the dtype of the image (see Image data types and what they mean), such as 0 to 255 for uint8 images or [0,\n1] for floating-point images. However, most images either have a narrower range of values (because of poor contrast), or have most pixel values concentrated in a subrange of the accessible values. skimage.exposure provides functions that spread the intensity values over a larger range. A first class of methods compute a nonlinear function of the intensity, that is independent of the pixel values of a specific image. Such methods are often used for correcting a known non-linearity of sensors, or receptors such as the human eye. A well-known example is Gamma correction, implemented in adjust_gamma(). Other methods re-distribute pixel values according to the histogram of the image. The histogram of pixel values is computed with skimage.exposure.histogram(): >>> image = np.array([[1, 3], [1, 1]])\n>>> exposure.histogram(image)\n(array([3, 0, 1]), array([1, 2, 3]))\n histogram() returns the number of pixels for each value bin, and the centers of the bins. The behavior of histogram() is therefore slightly different from the one of numpy.histogram(), which returns the boundaries of the bins. The simplest contrast enhancement rescale_intensity() consists in stretching pixel values to the whole allowed range, using a linear transformation: >>> from skimage import exposure\n>>> text = data.text()\n>>> text.min(), text.max()\n(10, 197)\n>>> better_contrast = exposure.rescale_intensity(text)\n>>> better_contrast.min(), better_contrast.max()\n(0, 255)\n Even if an image uses the whole value range, sometimes there is very little weight at the ends of the value range. In such a case, clipping pixel values using percentiles of the image improves the contrast (at the expense of some loss of information, because some pixels are saturated by this operation): >>> moon = data.moon()\n>>> v_min, v_max = np.percentile(moon, (0.2, 99.8))\n>>> v_min, v_max\n(10.0, 186.0)\n>>> better_contrast = exposure.rescale_intensity(\n...                                     moon, in_range=(v_min, v_max))\n The function equalize_hist() maps the cumulative distribution function (cdf) of pixel values onto a linear cdf, ensuring that all parts of the value range are equally represented in the image. As a result, details are enhanced in large regions with poor contrast. As a further refinement, histogram equalization can be performed in subregions of the image with equalize_adapthist(), in order to correct for exposure gradients across the image. See the example Histogram Equalization.   Examples:  Histogram Equalization  \n"}, {"name": "Image data types and what they mean", "path": "user_guide/data_types", "type": "Guide", "text": "Image data types and what they mean In skimage, images are simply numpy arrays, which support a variety of data types 1, i.e. \u201cdtypes\u201d. To avoid distorting image intensities (see Rescaling intensity values), we assume that images use the following dtype ranges:   \nData type Range   \nuint8 0 to 255  \nuint16 0 to 65535  \nuint32 0 to 232 - 1  \nfloat -1 to 1 or 0 to 1  \nint8 -128 to 127  \nint16 -32768 to 32767  \nint32 -231 to 231 - 1   Note that float images should be restricted to the range -1 to 1 even though the data type itself can exceed this range; all integer dtypes, on the other hand, have pixel intensities that can span the entire data type range. With a few exceptions, 64-bit (u)int images are not supported. Functions in skimage are designed so that they accept any of these dtypes, but, for efficiency, may return an image of a different dtype (see Output types). If you need a particular dtype, skimage provides utility functions that convert dtypes and properly rescale image intensities (see Input types). You should never use astype on an image, because it violates these assumptions about the dtype range: >>> from skimage.util import img_as_float\n>>> image = np.arange(0, 50, 10, dtype=np.uint8)\n>>> print(image.astype(float)) # These float values are out of range.\n[  0.  10.  20.  30.  40.]\n>>> print(img_as_float(image))\n[ 0.          0.03921569  0.07843137  0.11764706  0.15686275]\n Input types Although we aim to preserve the data range and type of input images, functions may support only a subset of these data-types. In such a case, the input will be converted to the required type (if possible), and a warning message printed to the log if a memory copy is needed. Type requirements should be noted in the docstrings. The following utility functions in the main package are available to developers and users:   \nFunction name Description   \nimg_as_float Convert to 64-bit floating point.  \nimg_as_ubyte Convert to 8-bit uint.  \nimg_as_uint Convert to 16-bit uint.  \nimg_as_int Convert to 16-bit int.   These functions convert images to the desired dtype and properly rescale their values: >>> from skimage.util import img_as_ubyte\n>>> image = np.array([0, 0.5, 1], dtype=float)\n>>> img_as_ubyte(image)\narray([  0, 128, 255], dtype=uint8)\n Be careful! These conversions can result in a loss of precision, since 8 bits cannot hold the same amount of information as 64 bits: >>> image = np.array([0, 0.5, 0.503, 1], dtype=float)\n>>> image_as_ubyte(image)\narray([  0, 128, 128, 255], dtype=uint8)\n Additionally, some functions take a preserve_range argument where a range conversion is convenient but not necessary. For example, interpolation in transform.warp requires an image of type float, which should have a range in [0, 1]. So, by default, input images will be rescaled to this range. However, in some cases, the image values represent physical measurements, such as temperature or rainfall values, that the user does not want rescaled. With preserve_range=True, the original range of the data will be preserved, even though the output is a float image. Users must then ensure this non-standard image is properly processed by downstream functions, which may expect an image in [0, 1]. >>> from skimage import data\n>>> from skimage.transform import rescale\n>>> image = data.coins()\n>>> image.dtype, image.min(), image.max(), image.shape\n(dtype('uint8'), 1, 252, (303, 384))\n>>> rescaled = rescale(image, 0.5)\n>>> (rescaled.dtype, np.round(rescaled.min(), 4),\n...  np.round(rescaled.max(), 4), rescaled.shape)\n(dtype('float64'), 0.0147, 0.9456, (152, 192))\n>>> rescaled = rescale(image, 0.5, preserve_range=True)\n>>> (rescaled.dtype, np.round(rescaled.min()),\n...  np.round(rescaled.max()), rescaled.shape\n(dtype('float64'), 4.0, 241.0, (152, 192))\n Output types The output type of a function is determined by the function author and is documented for the benefit of the user. While this requires the user to explicitly convert the output to whichever format is needed, it ensures that no unnecessary data copies take place. A user that requires a specific type of output (e.g., for display purposes), may write: >>> from skimage.util import img_as_uint\n>>> out = img_as_uint(sobel(image))\n>>> plt.imshow(out)\n Working with OpenCV It is possible that you may need to use an image created using skimage with OpenCV or vice versa. OpenCV image data can be accessed (without copying) in NumPy (and, thus, in scikit-image). OpenCV uses BGR (instead of scikit-image\u2019s RGB) for color images, and its dtype is uint8 by default (See Image data types and what they mean). BGR stands for Blue Green Red. Converting BGR to RGB or vice versa The color images in skimage and OpenCV have 3 dimensions: width, height and color. RGB and BGR use the same color space, except the order of colors is reversed. Note that in scikit-image we usually refer to rows and columns instead of width and height (see Coordinate conventions). The following instruction effectively reverses the order of the colors, leaving the rows and columns unaffected. >>> image = image[:, :, ::-1]\n Using an image from OpenCV with skimage\n If cv_image is an array of unsigned bytes, skimage will understand it by default. If you prefer working with floating point images, img_as_float() can be used to convert the image: >>> from skimage.util import img_as_float\n>>> image = img_as_float(any_opencv_image)\n Using an image from skimage with OpenCV The reverse can be achieved with img_as_ubyte(): >>> from skimage.util import img_as_ubyte\n>>> cv_image = img_as_ubyte(any_skimage_image)\n Image processing pipeline This dtype behavior allows you to string together any skimage function without worrying about the image dtype. On the other hand, if you want to use a custom function that requires a particular dtype, you should call one of the dtype conversion functions (here, func1 and func2 are skimage functions): >>> from skimage.util import img_as_float\n>>> image = img_as_float(func1(func2(image)))\n>>> processed_image = custom_func(image)\n Better yet, you can convert the image internally and use a simplified processing pipeline: >>> def custom_func(image):\n...     image = img_as_float(image)\n...     # do something\n...\n>>> processed_image = custom_func(func1(func2(image)))\n Rescaling intensity values When possible, functions should avoid blindly stretching image intensities (e.g. rescaling a float image so that the min and max intensities are 0 and 1), since this can heavily distort an image. For example, if you\u2019re looking for bright markers in dark images, there may be an image where no markers are present; stretching its input intensity to span the full range would make background noise look like markers. Sometimes, however, you have images that should span the entire intensity range but do not. For example, some cameras store images with 10-, 12-, or 14-bit depth per pixel. If these images are stored in an array with dtype uint16, then the image won\u2019t extend over the full intensity range, and thus, would appear dimmer than it should. To correct for this, you can use the rescale_intensity function to rescale the image so that it uses the full dtype range: >>> from skimage import exposure\n>>> image = exposure.rescale_intensity(img10bit, in_range=(0, 2**10 - 1))\n Here, the in_range argument is set to the maximum range for a 10-bit image. By default, rescale_intensity stretches the values of in_range to match the range of the dtype. rescale_intensity also accepts strings as inputs to in_range and out_range, so the example above could also be written as: >>> image = exposure.rescale_intensity(img10bit, in_range='uint10')\n Note about negative values People very often represent images in signed dtypes, even though they only manipulate the positive values of the image (e.g., using only 0-127 in an int8 image). For this reason, conversion functions only spread the positive values of a signed dtype over the entire range of an unsigned dtype. In other words, negative values are clipped to 0 when converting from signed to unsigned dtypes. (Negative values are preserved when converting between signed dtypes.) To prevent this clipping behavior, you should rescale your image beforehand: >>> image = exposure.rescale_intensity(img_int32, out_range=(0, 2**31 - 1))\n>>> img_uint8 = img_as_ubyte(image)\n This behavior is symmetric: The values in an unsigned dtype are spread over just the positive range of a signed dtype. References  \n1  \nhttps://docs.scipy.org/doc/numpy/user/basics.types.html  \n"}, {"name": "Image Segmentation", "path": "user_guide/tutorial_segmentation", "type": "Guide", "text": "Image Segmentation Image segmentation is the task of labeling the pixels of objects of interest in an image. In this tutorial, we will see how to segment objects from a background. We use the coins image from skimage.data. This image shows several coins outlined against a darker background. The segmentation of the coins cannot be done directly from the histogram of grey values, because the background shares enough grey levels with the coins that a thresholding segmentation is not sufficient.  >>> from skimage import data\n>>> from skimage.exposure import histogram\n>>> coins = data.coins()\n>>> hist, hist_centers = histogram(coins)\n Simply thresholding the image leads either to missing significant parts of the coins, or to merging parts of the background with the coins. This is due to the inhomogeneous lighting of the image.  A first idea is to take advantage of the local contrast, that is, to use the gradients rather than the grey values. Edge-based segmentation Let us first try to detect edges that enclose the coins. For edge detection, we use the Canny detector of skimage.feature.canny >>> from skimage.feature import canny\n>>> edges = canny(coins/255.)\n As the background is very smooth, almost all edges are found at the boundary of the coins, or inside the coins. >>> from scipy import ndimage as ndi\n>>> fill_coins = ndi.binary_fill_holes(edges)\n  Now that we have contours that delineate the outer boundary of the coins, we fill the inner part of the coins using the ndi.binary_fill_holes function, which uses mathematical morphology to fill the holes.  Most coins are well segmented out of the background. Small objects from the background can be easily removed using the ndi.label function to remove objects smaller than a small threshold. >>> label_objects, nb_labels = ndi.label(fill_coins)\n>>> sizes = np.bincount(label_objects.ravel())\n>>> mask_sizes = sizes > 20\n>>> mask_sizes[0] = 0\n>>> coins_cleaned = mask_sizes[label_objects]\n However, the segmentation is not very satisfying, since one of the coins has not been segmented correctly at all. The reason is that the contour that we got from the Canny detector was not completely closed, therefore the filling function did not fill the inner part of the coin.  Therefore, this segmentation method is not very robust: if we miss a single pixel of the contour of the object, we will not be able to fill it. Of course, we could try to dilate the contours in order to close them. However, it is preferable to try a more robust method. Region-based segmentation Let us first determine markers of the coins and the background. These markers are pixels that we can label unambiguously as either object or background. Here, the markers are found at the two extreme parts of the histogram of grey values: >>> markers = np.zeros_like(coins)\n>>> markers[coins < 30] = 1\n>>> markers[coins > 150] = 2\n We will use these markers in a watershed segmentation. The name watershed comes from an analogy with hydrology. The watershed transform floods an image of elevation starting from markers, in order to determine the catchment basins of these markers. Watershed lines separate these catchment basins, and correspond to the desired segmentation. The choice of the elevation map is critical for good segmentation. Here, the amplitude of the gradient provides a good elevation map. We use the Sobel operator for computing the amplitude of the gradient: >>> from skimage.filters import sobel\n>>> elevation_map = sobel(coins)\n From the 3-D surface plot shown below, we see that high barriers effectively separate the coins from the background.  and here is the corresponding 2-D plot:  The next step is to find markers of the background and the coins based on the extreme parts of the histogram of grey values: >>> markers = np.zeros_like(coins)\n>>> markers[coins < 30] = 1\n>>> markers[coins > 150] = 2\n  Let us now compute the watershed transform: >>> from skimage.segmentation import watershed\n>>> segmentation = watershed(elevation_map, markers)\n  With this method, the result is satisfying for all coins. Even if the markers for the background were not well distributed, the barriers in the elevation map were high enough for these markers to flood the entire background. We remove a few small holes with mathematical morphology: >>> segmentation = ndi.binary_fill_holes(segmentation - 1)\n We can now label all the coins one by one using ndi.label: >>> labeled_coins, _ = ndi.label(segmentation)\n \n"}, {"name": "Image Viewer", "path": "user_guide/viewer", "type": "Guide", "text": "Image Viewer  Warning The scikit-image viewer is deprecated since 0.18 and will be removed in 0.20. Please, refer to the visualization software page for alternatives.  Quick Start skimage.viewer provides a matplotlib-based canvas for displaying images and a Qt-based GUI-toolkit, with the goal of making it easy to create interactive image editors. You can simply use it to display an image: from skimage import data\nfrom skimage.viewer import ImageViewer\n\nimage = data.coins()\nviewer = ImageViewer(image)\nviewer.show()\n Of course, you could just as easily use imshow from matplotlib (or alternatively, skimage.io.imshow which adds support for multiple io-plugins) to display images. The advantage of ImageViewer is that you can easily add plugins for manipulating images. Currently, only a few plugins are implemented, but it is easy to write your own. Before going into the details, let\u2019s see an example of how a pre-defined plugin is added to the viewer: from skimage.viewer.plugins.lineprofile import LineProfile\n\nviewer = ImageViewer(image)\nviewer += LineProfile(viewer)\noverlay, data = viewer.show()[0]\n The viewer\u2019s show() method returns a list of tuples, one for each attached plugin. Each tuple contains two elements: an overlay of the same shape as the input image, and a data field (which may be None). A plugin class documents its return value in its output method. In this example, only one plugin is attached, so the list returned by show will have length 1. We extract the single tuple and bind its overlay and data elements to individual variables. Here, overlay contains an image of the line drawn on the viewer, and data contains the 1-dimensional intensity profile along that line. At the moment, there are not many plugins pre-defined, but there is a really simple interface for creating your own plugin. First, let us create a plugin to call the total-variation denoising function, denoise_tv_bregman: from skimage.filters import denoise_tv_bregman\nfrom skimage.viewer.plugins.base import Plugin\n\ndenoise_plugin = Plugin(image_filter=denoise_tv_bregman)\n  Note The Plugin assumes the first argument given to the image filter is the image from the image viewer. In the future, this should be changed so you can pass the image to a different argument of the filter function.  To actually interact with the filter, you have to add widgets that adjust the parameters of the function. Typically, that means adding a slider widget and connecting it to the filter parameter and the minimum and maximum values of the slider: from skimage.viewer.widgets import Slider\nfrom skimage.viewer.widgets.history import SaveButtons\n\ndenoise_plugin += Slider('weight', 0.01, 0.5, update_on='release')\ndenoise_plugin += SaveButtons()\n Here, we connect a slider widget to the filter\u2019s \u2018weight\u2019 argument. We also added some buttons for saving the image to file or to the scikit-image image stack (see skimage.io.push and skimage.io.pop). All that\u2019s left is to create an image viewer and add the plugin to that viewer. viewer = ImageViewer(image)\nviewer += denoise_plugin\ndenoised = viewer.show()[0][0]\n Here, we access only the overlay returned by the plugin, which contains the filtered image for the last used setting of weight.  \n"}, {"name": "img_as_bool()", "path": "api/skimage#skimage.img_as_bool", "type": "skimage", "text": " \nskimage.img_as_bool(image, force_copy=False) [source]\n \nConvert an image to boolean format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of bool (bool_) \n\nOutput image.     Notes The upper half of the input dtype\u2019s positive range is True, and the lower half is False. All negative values (if present) are False. \n"}, {"name": "img_as_float()", "path": "api/skimage#skimage.img_as_float", "type": "skimage", "text": " \nskimage.img_as_float(image, force_copy=False) [source]\n \nConvert an image to floating point format. This function is similar to img_as_float64, but will not convert lower-precision floating point arrays to float64.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of float \n\nOutput image.     Notes The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0]. \n"}, {"name": "img_as_float32()", "path": "api/skimage#skimage.img_as_float32", "type": "skimage", "text": " \nskimage.img_as_float32(image, force_copy=False) [source]\n \nConvert an image to single-precision (32-bit) floating point format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of float32 \n\nOutput image.     Notes The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0]. \n"}, {"name": "img_as_float64()", "path": "api/skimage#skimage.img_as_float64", "type": "skimage", "text": " \nskimage.img_as_float64(image, force_copy=False) [source]\n \nConvert an image to double-precision (64-bit) floating point format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of float64 \n\nOutput image.     Notes The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0]. \n"}, {"name": "img_as_int()", "path": "api/skimage#skimage.img_as_int", "type": "skimage", "text": " \nskimage.img_as_int(image, force_copy=False) [source]\n \nConvert an image to 16-bit signed integer format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of int16 \n\nOutput image.     Notes The values are scaled between -32768 and 32767. If the input data-type is positive-only (e.g., uint8), then the output image will still only have positive values. \n"}, {"name": "img_as_ubyte()", "path": "api/skimage#skimage.img_as_ubyte", "type": "skimage", "text": " \nskimage.img_as_ubyte(image, force_copy=False) [source]\n \nConvert an image to 8-bit unsigned integer format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of ubyte (uint8) \n\nOutput image.     Notes Negative input values will be clipped. Positive values are scaled between 0 and 255. \n"}, {"name": "img_as_uint()", "path": "api/skimage#skimage.img_as_uint", "type": "skimage", "text": " \nskimage.img_as_uint(image, force_copy=False) [source]\n \nConvert an image to 16-bit unsigned integer format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of uint16 \n\nOutput image.     Notes Negative input values will be clipped. Positive values are scaled between 0 and 65535. \n"}, {"name": "io", "path": "api/skimage.io", "type": "io", "text": "Module: io Utilities to read and write images in various formats. The following plug-ins are available:  \nPlugin Description  \nqt Fast image display using the Qt library. Deprecated since 0.18. Will be removed in 0.20.  \nimread Image reading and writing via imread  \ngdal Image reading via the GDAL Library (www.gdal.org)  \nsimpleitk Image reading and writing via SimpleITK  \ngtk Fast image display using the GTK library  \npil Image reading via the Python Imaging Library  \nfits FITS image reading via PyFITS  \nmatplotlib Display or save images using Matplotlib  \ntifffile Load and save TIFF and TIFF-based images using tifffile.py  \nimageio Image reading via the ImageIO Library    \nskimage.io.call_plugin(kind, *args, **kwargs) Find the appropriate plugin of \u2018kind\u2019 and execute it.  \nskimage.io.concatenate_images(ic) Concatenate all images in the image collection into an array.  \nskimage.io.find_available_plugins([loaded]) List available plugins.  \nskimage.io.imread(fname[, as_gray, plugin]) Load an image from file.  \nskimage.io.imread_collection(load_pattern[, \u2026]) Load a collection of images.  \nskimage.io.imread_collection_wrapper(imread)   \nskimage.io.imsave(fname, arr[, plugin, \u2026]) Save an image to file.  \nskimage.io.imshow(arr[, plugin]) Display an image.  \nskimage.io.imshow_collection(ic[, plugin]) Display a collection of images.  \nskimage.io.load_sift(f) Read SIFT or SURF features from externally generated file.  \nskimage.io.load_surf(f) Read SIFT or SURF features from externally generated file.  \nskimage.io.plugin_info(plugin) Return plugin meta-data.  \nskimage.io.plugin_order() Return the currently preferred plugin order.  \nskimage.io.pop() Pop an image from the shared image stack.  \nskimage.io.push(img) Push an image onto the shared image stack.  \nskimage.io.reset_plugins()   \nskimage.io.show() Display pending images.  \nskimage.io.use_plugin(name[, kind]) Set the default plugin for a specified operation.  \nskimage.io.ImageCollection(load_pattern[, \u2026]) Load and manage a collection of image files.  \nskimage.io.MultiImage(filename[, \u2026]) A class containing all frames from multi-frame images.  \nskimage.io.collection Data structures to hold collections of images, with optional caching.  \nskimage.io.manage_plugins Handle image reading, writing and plotting plugins.  \nskimage.io.sift   \nskimage.io.util    call_plugin  \nskimage.io.call_plugin(kind, *args, **kwargs) [source]\n \nFind the appropriate plugin of \u2018kind\u2019 and execute it.  Parameters \n \nkind{\u2018imshow\u2019, \u2018imsave\u2019, \u2018imread\u2019, \u2018imread_collection\u2019} \n\nFunction to look up.  \npluginstr, optional \n\nPlugin to load. Defaults to None, in which case the first matching plugin is used.  \n*args, **kwargsarguments and keyword arguments \n\nPassed to the plugin function.     \n concatenate_images  \nskimage.io.concatenate_images(ic) [source]\n \nConcatenate all images in the image collection into an array.  Parameters \n \nican iterable of images \n\nThe images to be concatenated.    Returns \n \narray_catndarray \n\nAn array having one more dimension than the images in ic.    Raises \n ValueError\n\nIf images in ic don\u2019t have identical shapes.      See also  \nImageCollection.concatenate, MultiImage.concatenate \n  Notes concatenate_images receives any iterable object containing images, including ImageCollection and MultiImage, and returns a NumPy array. \n find_available_plugins  \nskimage.io.find_available_plugins(loaded=False) [source]\n \nList available plugins.  Parameters \n \nloadedbool \n\nIf True, show only those plugins currently loaded. By default, all plugins are shown.    Returns \n \npdict \n\nDictionary with plugin names as keys and exposed functions as values.     \n imread  \nskimage.io.imread(fname, as_gray=False, plugin=None, **plugin_args) [source]\n \nLoad an image from file.  Parameters \n \nfnamestring \n\nImage file name, e.g. test.jpg or URL.  \nas_graybool, optional \n\nIf True, convert color images to gray-scale (64-bit floats). Images that are already in gray-scale format are not converted.  \npluginstr, optional \n\nName of plugin to use. By default, the different plugins are tried (starting with imageio) until a suitable candidate is found. If not given and fname is a tiff file, the tifffile plugin will be used.    Returns \n \nimg_arrayndarray \n\nThe different color bands/channels are stored in the third dimension, such that a gray-image is MxN, an RGB-image MxNx3 and an RGBA-image MxNx4.    Other Parameters \n \nplugin_argskeywords \n\nPassed to the given plugin.     \n imread_collection  \nskimage.io.imread_collection(load_pattern, conserve_memory=True, plugin=None, **plugin_args) [source]\n \nLoad a collection of images.  Parameters \n \nload_patternstr or list \n\nList of objects to load. These are usually filenames, but may vary depending on the currently active plugin. See the docstring for ImageCollection for the default behaviour of this parameter.  \nconserve_memorybool, optional \n\nIf True, never keep more than one in memory at a specific time. Otherwise, images will be cached once they are loaded.    Returns \n \nicImageCollection \n\nCollection of images.    Other Parameters \n \nplugin_argskeywords \n\nPassed to the given plugin.     \n imread_collection_wrapper  \nskimage.io.imread_collection_wrapper(imread) [source]\n\n imsave  \nskimage.io.imsave(fname, arr, plugin=None, check_contrast=True, **plugin_args) [source]\n \nSave an image to file.  Parameters \n \nfnamestr \n\nTarget filename.  \narrndarray of shape (M,N) or (M,N,3) or (M,N,4) \n\nImage data.  \npluginstr, optional \n\nName of plugin to use. By default, the different plugins are tried (starting with imageio) until a suitable candidate is found. If not given and fname is a tiff file, the tifffile plugin will be used.  \ncheck_contrastbool, optional \n\nCheck for low contrast and print warning (default: True).    Other Parameters \n \nplugin_argskeywords \n\nPassed to the given plugin.     Notes When saving a JPEG, the compression ratio may be controlled using the quality keyword argument which is an integer with values in [1, 100] where 1 is worst quality and smallest file size, and 100 is best quality and largest file size (default 75). This is only available when using the PIL and imageio plugins. \n imshow  \nskimage.io.imshow(arr, plugin=None, **plugin_args) [source]\n \nDisplay an image.  Parameters \n \narrndarray or str \n\nImage data or name of image file.  \npluginstr \n\nName of plugin to use. By default, the different plugins are tried (starting with imageio) until a suitable candidate is found.    Other Parameters \n \nplugin_argskeywords \n\nPassed to the given plugin.     \n Examples using skimage.io.imshow\n \n  Explore 3D images (of cells)   imshow_collection  \nskimage.io.imshow_collection(ic, plugin=None, **plugin_args) [source]\n \nDisplay a collection of images.  Parameters \n \nicImageCollection \n\nCollection to display.  \npluginstr \n\nName of plugin to use. By default, the different plugins are tried until a suitable candidate is found.    Other Parameters \n \nplugin_argskeywords \n\nPassed to the given plugin.     \n load_sift  \nskimage.io.load_sift(f) [source]\n \nRead SIFT or SURF features from externally generated file. This routine reads SIFT or SURF files generated by binary utilities from http://people.cs.ubc.ca/~lowe/keypoints/ and http://www.vision.ee.ethz.ch/~surf/. This routine does not generate SIFT/SURF features from an image. These algorithms are patent encumbered. Please use skimage.feature.CENSURE instead.  Parameters \n \nfilelikestring or open file \n\nInput file generated by the feature detectors from http://people.cs.ubc.ca/~lowe/keypoints/ or http://www.vision.ee.ethz.ch/~surf/ .  \nmode{\u2018SIFT\u2019, \u2018SURF\u2019}, optional \n\nKind of descriptor used to generate filelike.    Returns \n \ndatarecord array with fields \n\n \n row: int\n\nrow position of feature    \n column: int\n\ncolumn position of feature    \n scale: float\n\nfeature scale    \n orientation: float\n\nfeature orientation    \n data: array\n\nfeature values         \n load_surf  \nskimage.io.load_surf(f) [source]\n \nRead SIFT or SURF features from externally generated file. This routine reads SIFT or SURF files generated by binary utilities from http://people.cs.ubc.ca/~lowe/keypoints/ and http://www.vision.ee.ethz.ch/~surf/. This routine does not generate SIFT/SURF features from an image. These algorithms are patent encumbered. Please use skimage.feature.CENSURE instead.  Parameters \n \nfilelikestring or open file \n\nInput file generated by the feature detectors from http://people.cs.ubc.ca/~lowe/keypoints/ or http://www.vision.ee.ethz.ch/~surf/ .  \nmode{\u2018SIFT\u2019, \u2018SURF\u2019}, optional \n\nKind of descriptor used to generate filelike.    Returns \n \ndatarecord array with fields \n\n \n row: int\n\nrow position of feature    \n column: int\n\ncolumn position of feature    \n scale: float\n\nfeature scale    \n orientation: float\n\nfeature orientation    \n data: array\n\nfeature values         \n plugin_info  \nskimage.io.plugin_info(plugin) [source]\n \nReturn plugin meta-data.  Parameters \n \npluginstr \n\nName of plugin.    Returns \n \nmdict \n\nMeta data as specified in plugin .ini.     \n plugin_order  \nskimage.io.plugin_order() [source]\n \nReturn the currently preferred plugin order.  Returns \n \npdict \n\nDictionary of preferred plugin order, with function name as key and plugins (in order of preference) as value.     \n pop  \nskimage.io.pop() [source]\n \nPop an image from the shared image stack.  Returns \n \nimgndarray \n\nImage popped from the stack.     \n push  \nskimage.io.push(img) [source]\n \nPush an image onto the shared image stack.  Parameters \n \nimgndarray \n\nImage to push.     \n reset_plugins  \nskimage.io.reset_plugins() [source]\n\n show  \nskimage.io.show() [source]\n \nDisplay pending images. Launch the event loop of the current gui plugin, and display all pending images, queued via imshow. This is required when using imshow from non-interactive scripts. A call to show will block execution of code until all windows have been closed. Examples >>> import skimage.io as io\n >>> for i in range(4):\n...     ax_im = io.imshow(np.random.rand(50, 50))\n>>> io.show() \n \n use_plugin  \nskimage.io.use_plugin(name, kind=None) [source]\n \nSet the default plugin for a specified operation. The plugin will be loaded if it hasn\u2019t been already.  Parameters \n \nnamestr \n\nName of plugin.  \nkind{\u2018imsave\u2019, \u2018imread\u2019, \u2018imshow\u2019, \u2018imread_collection\u2019, \u2018imshow_collection\u2019}, optional \n\nSet the plugin for this function. By default, the plugin is set for all functions.      See also  \navailable_plugins \n\nList of available plugins    Examples To use Matplotlib as the default image reader, you would write: >>> from skimage import io\n>>> io.use_plugin('matplotlib', 'imread')\n To see a list of available plugins run io.available_plugins. Note that this lists plugins that are defined, but the full list may not be usable if your system does not have the required libraries installed. \n ImageCollection  \nclass skimage.io.ImageCollection(load_pattern, conserve_memory=True, load_func=None, **load_func_kwargs) [source]\n \nBases: object Load and manage a collection of image files.  Parameters \n \nload_patternstr or list of str \n\nPattern string or list of strings to load. The filename path can be absolute or relative.  \nconserve_memorybool, optional \n\nIf True, ImageCollection does not keep more than one in memory at a specific time. Otherwise, images will be cached once they are loaded.    Other Parameters \n \nload_funccallable \n\nimread by default. See notes below.     Notes Note that files are always returned in alphanumerical order. Also note that slicing returns a new ImageCollection, not a view into the data. ImageCollection can be modified to load images from an arbitrary source by specifying a combination of load_pattern and load_func. For an ImageCollection ic, ic[5] uses load_func(load_pattern[5]) to load the image. Imagine, for example, an ImageCollection that loads every third frame from a video file: video_file = 'no_time_for_that_tiny.gif'\n\ndef vidread_step(f, step):\n    vid = imageio.get_reader(f)\n    seq = [v for v in vid.iter_data()]\n    return seq[::step]\n\nic = ImageCollection(video_file, load_func=vidread_step, step=3)\n\nic  # is an ImageCollection object of length 1 because there is 1 file\n\nx = ic[0]  # calls vidread_step(video_file, step=3)\nx[5]  # is the sixth element of a list of length 8 (24 / 3)\n Another use of load_func would be to convert all images to uint8: def imread_convert(f):\n    return imread(f).astype(np.uint8)\n\nic = ImageCollection('/tmp/*.png', load_func=imread_convert)\n Examples >>> import skimage.io as io\n>>> from skimage import data_dir\n >>> coll = io.ImageCollection(data_dir + '/chess*.png')\n>>> len(coll)\n2\n>>> coll[0].shape\n(200, 200)\n >>> ic = io.ImageCollection(['/tmp/work/*.png', '/tmp/other/*.jpg'])\n  Attributes \n \nfileslist of str \n\nIf a pattern string is given for load_pattern, this attribute stores the expanded file list. Otherwise, this is equal to load_pattern.      \n__init__(load_pattern, conserve_memory=True, load_func=None, **load_func_kwargs) [source]\n \nLoad and manage a collection of images. \n  \nconcatenate() [source]\n \nConcatenate all images in the collection into an array.  Returns \n \narnp.ndarray \n\nAn array having one more dimension than the images in self.    Raises \n ValueError\n\nIf images in the ImageCollection don\u2019t have identical shapes.      See also  \nconcatenate_images\n\n  \n  \nproperty conserve_memory \n  \nproperty files \n  \nreload(n=None) [source]\n \nClear the image cache.  Parameters \n \nnNone or int \n\nClear the cache for this image only. By default, the entire cache is erased.     \n \n MultiImage  \nclass skimage.io.MultiImage(filename, conserve_memory=True, dtype=None, **imread_kwargs) [source]\n \nBases: skimage.io.collection.ImageCollection A class containing all frames from multi-frame images.  Parameters \n \nload_patternstr or list of str \n\nPattern glob or filenames to load. The path can be absolute or relative.  \nconserve_memorybool, optional \n\nWhether to conserve memory by only caching a single frame. Default is True.    Other Parameters \n \nload_funccallable \n\nimread by default. See notes below.     Notes If conserve_memory=True the memory footprint can be reduced, however the performance can be affected because frames have to be read from file more often. The last accessed frame is cached, all other frames will have to be read from file. The current implementation makes use of tifffile for Tiff files and PIL otherwise. Examples >>> from skimage import data_dir\n >>> img = MultiImage(data_dir + '/multipage.tif') \n>>> len(img) \n2\n>>> for frame in img: \n...     print(frame.shape) \n(15, 10)\n(15, 10)\n  \n__init__(filename, conserve_memory=True, dtype=None, **imread_kwargs) [source]\n \nLoad a multi-img. \n  \nproperty filename \n \n\n"}, {"name": "io.call_plugin()", "path": "api/skimage.io#skimage.io.call_plugin", "type": "io", "text": " \nskimage.io.call_plugin(kind, *args, **kwargs) [source]\n \nFind the appropriate plugin of \u2018kind\u2019 and execute it.  Parameters \n \nkind{\u2018imshow\u2019, \u2018imsave\u2019, \u2018imread\u2019, \u2018imread_collection\u2019} \n\nFunction to look up.  \npluginstr, optional \n\nPlugin to load. Defaults to None, in which case the first matching plugin is used.  \n*args, **kwargsarguments and keyword arguments \n\nPassed to the plugin function.     \n"}, {"name": "io.concatenate_images()", "path": "api/skimage.io#skimage.io.concatenate_images", "type": "io", "text": " \nskimage.io.concatenate_images(ic) [source]\n \nConcatenate all images in the image collection into an array.  Parameters \n \nican iterable of images \n\nThe images to be concatenated.    Returns \n \narray_catndarray \n\nAn array having one more dimension than the images in ic.    Raises \n ValueError\n\nIf images in ic don\u2019t have identical shapes.      See also  \nImageCollection.concatenate, MultiImage.concatenate \n  Notes concatenate_images receives any iterable object containing images, including ImageCollection and MultiImage, and returns a NumPy array. \n"}, {"name": "io.find_available_plugins()", "path": "api/skimage.io#skimage.io.find_available_plugins", "type": "io", "text": " \nskimage.io.find_available_plugins(loaded=False) [source]\n \nList available plugins.  Parameters \n \nloadedbool \n\nIf True, show only those plugins currently loaded. By default, all plugins are shown.    Returns \n \npdict \n\nDictionary with plugin names as keys and exposed functions as values.     \n"}, {"name": "io.ImageCollection", "path": "api/skimage.io#skimage.io.ImageCollection", "type": "io", "text": " \nclass skimage.io.ImageCollection(load_pattern, conserve_memory=True, load_func=None, **load_func_kwargs) [source]\n \nBases: object Load and manage a collection of image files.  Parameters \n \nload_patternstr or list of str \n\nPattern string or list of strings to load. The filename path can be absolute or relative.  \nconserve_memorybool, optional \n\nIf True, ImageCollection does not keep more than one in memory at a specific time. Otherwise, images will be cached once they are loaded.    Other Parameters \n \nload_funccallable \n\nimread by default. See notes below.     Notes Note that files are always returned in alphanumerical order. Also note that slicing returns a new ImageCollection, not a view into the data. ImageCollection can be modified to load images from an arbitrary source by specifying a combination of load_pattern and load_func. For an ImageCollection ic, ic[5] uses load_func(load_pattern[5]) to load the image. Imagine, for example, an ImageCollection that loads every third frame from a video file: video_file = 'no_time_for_that_tiny.gif'\n\ndef vidread_step(f, step):\n    vid = imageio.get_reader(f)\n    seq = [v for v in vid.iter_data()]\n    return seq[::step]\n\nic = ImageCollection(video_file, load_func=vidread_step, step=3)\n\nic  # is an ImageCollection object of length 1 because there is 1 file\n\nx = ic[0]  # calls vidread_step(video_file, step=3)\nx[5]  # is the sixth element of a list of length 8 (24 / 3)\n Another use of load_func would be to convert all images to uint8: def imread_convert(f):\n    return imread(f).astype(np.uint8)\n\nic = ImageCollection('/tmp/*.png', load_func=imread_convert)\n Examples >>> import skimage.io as io\n>>> from skimage import data_dir\n >>> coll = io.ImageCollection(data_dir + '/chess*.png')\n>>> len(coll)\n2\n>>> coll[0].shape\n(200, 200)\n >>> ic = io.ImageCollection(['/tmp/work/*.png', '/tmp/other/*.jpg'])\n  Attributes \n \nfileslist of str \n\nIf a pattern string is given for load_pattern, this attribute stores the expanded file list. Otherwise, this is equal to load_pattern.      \n__init__(load_pattern, conserve_memory=True, load_func=None, **load_func_kwargs) [source]\n \nLoad and manage a collection of images. \n  \nconcatenate() [source]\n \nConcatenate all images in the collection into an array.  Returns \n \narnp.ndarray \n\nAn array having one more dimension than the images in self.    Raises \n ValueError\n\nIf images in the ImageCollection don\u2019t have identical shapes.      See also  \nconcatenate_images\n\n  \n  \nproperty conserve_memory \n  \nproperty files \n  \nreload(n=None) [source]\n \nClear the image cache.  Parameters \n \nnNone or int \n\nClear the cache for this image only. By default, the entire cache is erased.     \n \n"}, {"name": "io.ImageCollection.concatenate()", "path": "api/skimage.io#skimage.io.ImageCollection.concatenate", "type": "io", "text": " \nconcatenate() [source]\n \nConcatenate all images in the collection into an array.  Returns \n \narnp.ndarray \n\nAn array having one more dimension than the images in self.    Raises \n ValueError\n\nIf images in the ImageCollection don\u2019t have identical shapes.      See also  \nconcatenate_images\n\n  \n"}, {"name": "io.ImageCollection.conserve_memory()", "path": "api/skimage.io#skimage.io.ImageCollection.conserve_memory", "type": "io", "text": " \nproperty conserve_memory \n"}, {"name": "io.ImageCollection.files()", "path": "api/skimage.io#skimage.io.ImageCollection.files", "type": "io", "text": " \nproperty files \n"}, {"name": "io.ImageCollection.reload()", "path": "api/skimage.io#skimage.io.ImageCollection.reload", "type": "io", "text": " \nreload(n=None) [source]\n \nClear the image cache.  Parameters \n \nnNone or int \n\nClear the cache for this image only. By default, the entire cache is erased.     \n"}, {"name": "io.ImageCollection.__init__()", "path": "api/skimage.io#skimage.io.ImageCollection.__init__", "type": "io", "text": " \n__init__(load_pattern, conserve_memory=True, load_func=None, **load_func_kwargs) [source]\n \nLoad and manage a collection of images. \n"}, {"name": "io.imread()", "path": "api/skimage.io#skimage.io.imread", "type": "io", "text": " \nskimage.io.imread(fname, as_gray=False, plugin=None, **plugin_args) [source]\n \nLoad an image from file.  Parameters \n \nfnamestring \n\nImage file name, e.g. test.jpg or URL.  \nas_graybool, optional \n\nIf True, convert color images to gray-scale (64-bit floats). Images that are already in gray-scale format are not converted.  \npluginstr, optional \n\nName of plugin to use. By default, the different plugins are tried (starting with imageio) until a suitable candidate is found. If not given and fname is a tiff file, the tifffile plugin will be used.    Returns \n \nimg_arrayndarray \n\nThe different color bands/channels are stored in the third dimension, such that a gray-image is MxN, an RGB-image MxNx3 and an RGBA-image MxNx4.    Other Parameters \n \nplugin_argskeywords \n\nPassed to the given plugin.     \n"}, {"name": "io.imread_collection()", "path": "api/skimage.io#skimage.io.imread_collection", "type": "io", "text": " \nskimage.io.imread_collection(load_pattern, conserve_memory=True, plugin=None, **plugin_args) [source]\n \nLoad a collection of images.  Parameters \n \nload_patternstr or list \n\nList of objects to load. These are usually filenames, but may vary depending on the currently active plugin. See the docstring for ImageCollection for the default behaviour of this parameter.  \nconserve_memorybool, optional \n\nIf True, never keep more than one in memory at a specific time. Otherwise, images will be cached once they are loaded.    Returns \n \nicImageCollection \n\nCollection of images.    Other Parameters \n \nplugin_argskeywords \n\nPassed to the given plugin.     \n"}, {"name": "io.imread_collection_wrapper()", "path": "api/skimage.io#skimage.io.imread_collection_wrapper", "type": "io", "text": " \nskimage.io.imread_collection_wrapper(imread) [source]\n\n"}, {"name": "io.imsave()", "path": "api/skimage.io#skimage.io.imsave", "type": "io", "text": " \nskimage.io.imsave(fname, arr, plugin=None, check_contrast=True, **plugin_args) [source]\n \nSave an image to file.  Parameters \n \nfnamestr \n\nTarget filename.  \narrndarray of shape (M,N) or (M,N,3) or (M,N,4) \n\nImage data.  \npluginstr, optional \n\nName of plugin to use. By default, the different plugins are tried (starting with imageio) until a suitable candidate is found. If not given and fname is a tiff file, the tifffile plugin will be used.  \ncheck_contrastbool, optional \n\nCheck for low contrast and print warning (default: True).    Other Parameters \n \nplugin_argskeywords \n\nPassed to the given plugin.     Notes When saving a JPEG, the compression ratio may be controlled using the quality keyword argument which is an integer with values in [1, 100] where 1 is worst quality and smallest file size, and 100 is best quality and largest file size (default 75). This is only available when using the PIL and imageio plugins. \n"}, {"name": "io.imshow()", "path": "api/skimage.io#skimage.io.imshow", "type": "io", "text": " \nskimage.io.imshow(arr, plugin=None, **plugin_args) [source]\n \nDisplay an image.  Parameters \n \narrndarray or str \n\nImage data or name of image file.  \npluginstr \n\nName of plugin to use. By default, the different plugins are tried (starting with imageio) until a suitable candidate is found.    Other Parameters \n \nplugin_argskeywords \n\nPassed to the given plugin.     \n"}, {"name": "io.imshow_collection()", "path": "api/skimage.io#skimage.io.imshow_collection", "type": "io", "text": " \nskimage.io.imshow_collection(ic, plugin=None, **plugin_args) [source]\n \nDisplay a collection of images.  Parameters \n \nicImageCollection \n\nCollection to display.  \npluginstr \n\nName of plugin to use. By default, the different plugins are tried until a suitable candidate is found.    Other Parameters \n \nplugin_argskeywords \n\nPassed to the given plugin.     \n"}, {"name": "io.load_sift()", "path": "api/skimage.io#skimage.io.load_sift", "type": "io", "text": " \nskimage.io.load_sift(f) [source]\n \nRead SIFT or SURF features from externally generated file. This routine reads SIFT or SURF files generated by binary utilities from http://people.cs.ubc.ca/~lowe/keypoints/ and http://www.vision.ee.ethz.ch/~surf/. This routine does not generate SIFT/SURF features from an image. These algorithms are patent encumbered. Please use skimage.feature.CENSURE instead.  Parameters \n \nfilelikestring or open file \n\nInput file generated by the feature detectors from http://people.cs.ubc.ca/~lowe/keypoints/ or http://www.vision.ee.ethz.ch/~surf/ .  \nmode{\u2018SIFT\u2019, \u2018SURF\u2019}, optional \n\nKind of descriptor used to generate filelike.    Returns \n \ndatarecord array with fields \n\n \n row: int\n\nrow position of feature    \n column: int\n\ncolumn position of feature    \n scale: float\n\nfeature scale    \n orientation: float\n\nfeature orientation    \n data: array\n\nfeature values         \n"}, {"name": "io.load_surf()", "path": "api/skimage.io#skimage.io.load_surf", "type": "io", "text": " \nskimage.io.load_surf(f) [source]\n \nRead SIFT or SURF features from externally generated file. This routine reads SIFT or SURF files generated by binary utilities from http://people.cs.ubc.ca/~lowe/keypoints/ and http://www.vision.ee.ethz.ch/~surf/. This routine does not generate SIFT/SURF features from an image. These algorithms are patent encumbered. Please use skimage.feature.CENSURE instead.  Parameters \n \nfilelikestring or open file \n\nInput file generated by the feature detectors from http://people.cs.ubc.ca/~lowe/keypoints/ or http://www.vision.ee.ethz.ch/~surf/ .  \nmode{\u2018SIFT\u2019, \u2018SURF\u2019}, optional \n\nKind of descriptor used to generate filelike.    Returns \n \ndatarecord array with fields \n\n \n row: int\n\nrow position of feature    \n column: int\n\ncolumn position of feature    \n scale: float\n\nfeature scale    \n orientation: float\n\nfeature orientation    \n data: array\n\nfeature values         \n"}, {"name": "io.MultiImage", "path": "api/skimage.io#skimage.io.MultiImage", "type": "io", "text": " \nclass skimage.io.MultiImage(filename, conserve_memory=True, dtype=None, **imread_kwargs) [source]\n \nBases: skimage.io.collection.ImageCollection A class containing all frames from multi-frame images.  Parameters \n \nload_patternstr or list of str \n\nPattern glob or filenames to load. The path can be absolute or relative.  \nconserve_memorybool, optional \n\nWhether to conserve memory by only caching a single frame. Default is True.    Other Parameters \n \nload_funccallable \n\nimread by default. See notes below.     Notes If conserve_memory=True the memory footprint can be reduced, however the performance can be affected because frames have to be read from file more often. The last accessed frame is cached, all other frames will have to be read from file. The current implementation makes use of tifffile for Tiff files and PIL otherwise. Examples >>> from skimage import data_dir\n >>> img = MultiImage(data_dir + '/multipage.tif') \n>>> len(img) \n2\n>>> for frame in img: \n...     print(frame.shape) \n(15, 10)\n(15, 10)\n  \n__init__(filename, conserve_memory=True, dtype=None, **imread_kwargs) [source]\n \nLoad a multi-img. \n  \nproperty filename \n \n"}, {"name": "io.MultiImage.filename()", "path": "api/skimage.io#skimage.io.MultiImage.filename", "type": "io", "text": " \nproperty filename \n"}, {"name": "io.MultiImage.__init__()", "path": "api/skimage.io#skimage.io.MultiImage.__init__", "type": "io", "text": " \n__init__(filename, conserve_memory=True, dtype=None, **imread_kwargs) [source]\n \nLoad a multi-img. \n"}, {"name": "io.plugin_info()", "path": "api/skimage.io#skimage.io.plugin_info", "type": "io", "text": " \nskimage.io.plugin_info(plugin) [source]\n \nReturn plugin meta-data.  Parameters \n \npluginstr \n\nName of plugin.    Returns \n \nmdict \n\nMeta data as specified in plugin .ini.     \n"}, {"name": "io.plugin_order()", "path": "api/skimage.io#skimage.io.plugin_order", "type": "io", "text": " \nskimage.io.plugin_order() [source]\n \nReturn the currently preferred plugin order.  Returns \n \npdict \n\nDictionary of preferred plugin order, with function name as key and plugins (in order of preference) as value.     \n"}, {"name": "io.pop()", "path": "api/skimage.io#skimage.io.pop", "type": "io", "text": " \nskimage.io.pop() [source]\n \nPop an image from the shared image stack.  Returns \n \nimgndarray \n\nImage popped from the stack.     \n"}, {"name": "io.push()", "path": "api/skimage.io#skimage.io.push", "type": "io", "text": " \nskimage.io.push(img) [source]\n \nPush an image onto the shared image stack.  Parameters \n \nimgndarray \n\nImage to push.     \n"}, {"name": "io.reset_plugins()", "path": "api/skimage.io#skimage.io.reset_plugins", "type": "io", "text": " \nskimage.io.reset_plugins() [source]\n\n"}, {"name": "io.show()", "path": "api/skimage.io#skimage.io.show", "type": "io", "text": " \nskimage.io.show() [source]\n \nDisplay pending images. Launch the event loop of the current gui plugin, and display all pending images, queued via imshow. This is required when using imshow from non-interactive scripts. A call to show will block execution of code until all windows have been closed. Examples >>> import skimage.io as io\n >>> for i in range(4):\n...     ax_im = io.imshow(np.random.rand(50, 50))\n>>> io.show() \n \n"}, {"name": "io.use_plugin()", "path": "api/skimage.io#skimage.io.use_plugin", "type": "io", "text": " \nskimage.io.use_plugin(name, kind=None) [source]\n \nSet the default plugin for a specified operation. The plugin will be loaded if it hasn\u2019t been already.  Parameters \n \nnamestr \n\nName of plugin.  \nkind{\u2018imsave\u2019, \u2018imread\u2019, \u2018imshow\u2019, \u2018imread_collection\u2019, \u2018imshow_collection\u2019}, optional \n\nSet the plugin for this function. By default, the plugin is set for all functions.      See also  \navailable_plugins \n\nList of available plugins    Examples To use Matplotlib as the default image reader, you would write: >>> from skimage import io\n>>> io.use_plugin('matplotlib', 'imread')\n To see a list of available plugins run io.available_plugins. Note that this lists plugins that are defined, but the full list may not be usable if your system does not have the required libraries installed. \n"}, {"name": "lookfor()", "path": "api/skimage#skimage.lookfor", "type": "skimage", "text": " \nskimage.lookfor(what) [source]\n \nDo a keyword search on scikit-image docstrings.  Parameters \n \nwhatstr \n\nWords to look for.     Examples >>> import skimage\n>>> skimage.lookfor('regular_grid')\nSearch results for 'regular_grid'\n---------------------------------\nskimage.lookfor\n    Do a keyword search on scikit-image docstrings.\nskimage.util.regular_grid\n    Find `n_points` regularly spaced along `ar_shape`.\n \n"}, {"name": "measure", "path": "api/skimage.measure", "type": "measure", "text": "Module: measure  \nskimage.measure.approximate_polygon(coords, \u2026) Approximate a polygonal chain with the specified tolerance.  \nskimage.measure.block_reduce(image, block_size) Downsample image by applying function func to local blocks.  \nskimage.measure.euler_number(image[, \u2026]) Calculate the Euler characteristic in binary image.  \nskimage.measure.find_contours(image[, \u2026]) Find iso-valued contours in a 2D array for a given level value.  \nskimage.measure.grid_points_in_poly(shape, verts) Test whether points on a specified grid are inside a polygon.  \nskimage.measure.inertia_tensor(image[, mu]) Compute the inertia tensor of the input image.  \nskimage.measure.inertia_tensor_eigvals(image) Compute the eigenvalues of the inertia tensor of the image.  \nskimage.measure.label(input[, background, \u2026]) Label connected regions of an integer array.  \nskimage.measure.marching_cubes(volume[, \u2026]) Marching cubes algorithm to find surfaces in 3d volumetric data.  \nskimage.measure.marching_cubes_classic(volume) Classic marching cubes algorithm to find surfaces in 3d volumetric data.  \nskimage.measure.marching_cubes_lewiner(volume) Lewiner marching cubes algorithm to find surfaces in 3d volumetric data.  \nskimage.measure.mesh_surface_area(verts, faces) Compute surface area, given vertices & triangular faces  \nskimage.measure.moments(image[, order]) Calculate all raw image moments up to a certain order.  \nskimage.measure.moments_central(image[, \u2026]) Calculate all central image moments up to a certain order.  \nskimage.measure.moments_coords(coords[, order]) Calculate all raw image moments up to a certain order.  \nskimage.measure.moments_coords_central(coords) Calculate all central image moments up to a certain order.  \nskimage.measure.moments_hu(nu) Calculate Hu\u2019s set of image moments (2D-only).  \nskimage.measure.moments_normalized(mu[, order]) Calculate all normalized central image moments up to a certain order.  \nskimage.measure.perimeter(image[, neighbourhood]) Calculate total perimeter of all objects in binary image.  \nskimage.measure.perimeter_crofton(image[, \u2026]) Calculate total Crofton perimeter of all objects in binary image.  \nskimage.measure.points_in_poly(points, verts) Test whether points lie inside a polygon.  \nskimage.measure.profile_line(image, src, dst) Return the intensity profile of an image measured along a scan line.  \nskimage.measure.ransac(data, model_class, \u2026) Fit a model to data with the RANSAC (random sample consensus) algorithm.  \nskimage.measure.regionprops(label_image[, \u2026]) Measure properties of labeled image regions.  \nskimage.measure.regionprops_table(label_image) Compute image properties and return them as a pandas-compatible table.  \nskimage.measure.shannon_entropy(image[, base]) Calculate the Shannon entropy of an image.  \nskimage.measure.subdivide_polygon(coords[, \u2026]) Subdivision of polygonal curves using B-Splines.  \nskimage.measure.CircleModel() Total least squares estimator for 2D circles.  \nskimage.measure.EllipseModel() Total least squares estimator for 2D ellipses.  \nskimage.measure.LineModelND() Total least squares estimator for N-dimensional lines.   approximate_polygon  \nskimage.measure.approximate_polygon(coords, tolerance) [source]\n \nApproximate a polygonal chain with the specified tolerance. It is based on the Douglas-Peucker algorithm. Note that the approximated polygon is always within the convex hull of the original polygon.  Parameters \n \ncoords(N, 2) array \n\nCoordinate array.  \ntolerancefloat \n\nMaximum distance from original points of polygon to approximated polygonal chain. If tolerance is 0, the original coordinate array is returned.    Returns \n \ncoords(M, 2) array \n\nApproximated polygonal chain where M <= N.     References  \n1  \nhttps://en.wikipedia.org/wiki/Ramer-Douglas-Peucker_algorithm   \n block_reduce  \nskimage.measure.block_reduce(image, block_size, func=<function sum>, cval=0, func_kwargs=None) [source]\n \nDownsample image by applying function func to local blocks. This function is useful for max and mean pooling, for example.  Parameters \n \nimagendarray \n\nN-dimensional input image.  \nblock_sizearray_like \n\nArray containing down-sampling integer factor along each axis.  \nfunccallable \n\nFunction object which is used to calculate the return value for each local block. This function must implement an axis parameter. Primary functions are numpy.sum, numpy.min, numpy.max, numpy.mean and numpy.median. See also func_kwargs.  \ncvalfloat \n\nConstant padding value if image is not perfectly divisible by the block size.  \nfunc_kwargsdict \n\nKeyword arguments passed to func. Notably useful for passing dtype argument to np.mean. Takes dictionary of inputs, e.g.: func_kwargs={'dtype': np.float16}).    Returns \n \nimagendarray \n\nDown-sampled image with same number of dimensions as input image.     Examples >>> from skimage.measure import block_reduce\n>>> image = np.arange(3*3*4).reshape(3, 3, 4)\n>>> image \narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]],\n       [[24, 25, 26, 27],\n        [28, 29, 30, 31],\n        [32, 33, 34, 35]]])\n>>> block_reduce(image, block_size=(3, 3, 1), func=np.mean)\narray([[[16., 17., 18., 19.]]])\n>>> image_max1 = block_reduce(image, block_size=(1, 3, 4), func=np.max)\n>>> image_max1 \narray([[[11]],\n       [[23]],\n       [[35]]])\n>>> image_max2 = block_reduce(image, block_size=(3, 1, 4), func=np.max)\n>>> image_max2 \narray([[[27],\n        [31],\n        [35]]])\n \n euler_number  \nskimage.measure.euler_number(image, connectivity=None) [source]\n \nCalculate the Euler characteristic in binary image. For 2D objects, the Euler number is the number of objects minus the number of holes. For 3D objects, the Euler number is obtained as the number of objects plus the number of holes, minus the number of tunnels, or loops.  Parameters \n image: (N, M) ndarray or (N, M, D) ndarray.\n\n2D or 3D images. If image is not binary, all values strictly greater than zero are considered as the object.  \nconnectivityint, optional \n\nMaximum number of orthogonal hops to consider a pixel/voxel as a neighbor. Accepted values are ranging from 1 to input.ndim. If None, a full connectivity of input.ndim is used. 4 or 8 neighborhoods are defined for 2D images (connectivity 1 and 2, respectively). 6 or 26 neighborhoods are defined for 3D images, (connectivity 1 and 3, respectively). Connectivity 2 is not defined.    Returns \n \neuler_numberint \n\nEuler characteristic of the set of all objects in the image.     Notes The Euler characteristic is an integer number that describes the topology of the set of all objects in the input image. If object is 4-connected, then background is 8-connected, and conversely. The computation of the Euler characteristic is based on an integral geometry formula in discretized space. In practice, a neighbourhood configuration is constructed, and a LUT is applied for each configuration. The coefficients used are the ones of Ohser et al. It can be useful to compute the Euler characteristic for several connectivities. A large relative difference between results for different connectivities suggests that the image resolution (with respect to the size of objects and holes) is too low. References  \n1  \nS. Rivollier. Analyse d\u2019image geometrique et morphometrique par diagrammes de forme et voisinages adaptatifs generaux. PhD thesis, 2010. Ecole Nationale Superieure des Mines de Saint-Etienne. https://tel.archives-ouvertes.fr/tel-00560838  \n2  \nOhser J., Nagel W., Schladitz K. (2002) The Euler Number of Discretized Sets - On the Choice of Adjacency in Homogeneous Lattices. In: Mecke K., Stoyan D. (eds) Morphology of Condensed Matter. Lecture Notes in Physics, vol 600. Springer, Berlin, Heidelberg.   Examples >>> import numpy as np\n>>> SAMPLE = np.zeros((100,100,100));\n>>> SAMPLE[40:60, 40:60, 40:60]=1\n>>> euler_number(SAMPLE) \n1...\n>>> SAMPLE[45:55,45:55,45:55] = 0;\n>>> euler_number(SAMPLE) \n2...\n>>> SAMPLE = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n...                    [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n...                    [1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0],\n...                    [0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1],\n...                    [0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]])\n>>> euler_number(SAMPLE)  # doctest:\n0\n>>> euler_number(SAMPLE, connectivity=1)  # doctest:\n2\n \n Examples using skimage.measure.euler_number\n \n  Euler number   find_contours  \nskimage.measure.find_contours(image, level=None, fully_connected='low', positive_orientation='low', *, mask=None) [source]\n \nFind iso-valued contours in a 2D array for a given level value. Uses the \u201cmarching squares\u201d method to compute a the iso-valued contours of the input 2D array for a particular level value. Array values are linearly interpolated to provide better precision for the output contours.  Parameters \n \nimage2D ndarray of double \n\nInput image in which to find contours.  \nlevelfloat, optional \n\nValue along which to find contours in the array. By default, the level is set to (max(image) + min(image)) / 2  Changed in version 0.18: This parameter is now optional.   \nfully_connectedstr, {\u2018low\u2019, \u2018high\u2019} \n\nIndicates whether array elements below the given level value are to be considered fully-connected (and hence elements above the value will only be face connected), or vice-versa. (See notes below for details.)  \npositive_orientationstr, {\u2018low\u2019, \u2018high\u2019} \n\nIndicates whether the output contours will produce positively-oriented polygons around islands of low- or high-valued elements. If \u2018low\u2019 then contours will wind counter- clockwise around elements below the iso-value. Alternately, this means that low-valued elements are always on the left of the contour. (See below for details.)  \nmask2D ndarray of bool, or None \n\nA boolean mask, True where we want to draw contours. Note that NaN values are always excluded from the considered region (mask is set to False wherever array is NaN).    Returns \n \ncontourslist of (n,2)-ndarrays \n\nEach contour is an ndarray of shape (n, 2), consisting of n (row, column) coordinates along the contour.      See also  \nskimage.measure.marching_cubes\n\n  Notes The marching squares algorithm is a special case of the marching cubes algorithm [1]. A simple explanation is available here: http://users.polytech.unice.fr/~lingrand/MarchingCubes/algo.html There is a single ambiguous case in the marching squares algorithm: when a given 2 x 2-element square has two high-valued and two low-valued elements, each pair diagonally adjacent. (Where high- and low-valued is with respect to the contour value sought.) In this case, either the high-valued elements can be \u2018connected together\u2019 via a thin isthmus that separates the low-valued elements, or vice-versa. When elements are connected together across a diagonal, they are considered \u2018fully connected\u2019 (also known as \u2018face+vertex-connected\u2019 or \u20188-connected\u2019). Only high-valued or low-valued elements can be fully-connected, the other set will be considered as \u2018face-connected\u2019 or \u20184-connected\u2019. By default, low-valued elements are considered fully-connected; this can be altered with the \u2018fully_connected\u2019 parameter. Output contours are not guaranteed to be closed: contours which intersect the array edge or a masked-off region (either where mask is False or where array is NaN) will be left open. All other contours will be closed. (The closed-ness of a contours can be tested by checking whether the beginning point is the same as the end point.) Contours are oriented. By default, array values lower than the contour value are to the left of the contour and values greater than the contour value are to the right. This means that contours will wind counter-clockwise (i.e. in \u2018positive orientation\u2019) around islands of low-valued pixels. This behavior can be altered with the \u2018positive_orientation\u2019 parameter. The order of the contours in the output list is determined by the position of the smallest x,y (in lexicographical order) coordinate in the contour. This is a side-effect of how the input array is traversed, but can be relied upon.  Warning Array coordinates/values are assumed to refer to the center of the array element. Take a simple example input: [0, 1]. The interpolated position of 0.5 in this array is midway between the 0-element (at x=0) and the 1-element (at x=1), and thus would fall at x=0.5.  This means that to find reasonable contours, it is best to find contours midway between the expected \u201clight\u201d and \u201cdark\u201d values. In particular, given a binarized array, do not choose to find contours at the low or high value of the array. This will often yield degenerate contours, especially around structures that are a single array element wide. Instead choose a middle value, as above. References  \n1  \nLorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings) 21(4) July 1987, p. 163-170). DOI:10.1145/37401.37422   Examples >>> a = np.zeros((3, 3))\n>>> a[0, 0] = 1\n>>> a\narray([[1., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n>>> find_contours(a, 0.5)\n[array([[0. , 0.5],\n       [0.5, 0. ]])]\n \n Examples using skimage.measure.find_contours\n \n  Contour finding  \n\n  Measure region properties   grid_points_in_poly  \nskimage.measure.grid_points_in_poly(shape, verts) [source]\n \nTest whether points on a specified grid are inside a polygon. For each (r, c) coordinate on a grid, i.e. (0, 0), (0, 1) etc., test whether that point lies inside a polygon.  Parameters \n \nshapetuple (M, N) \n\nShape of the grid.  \nverts(V, 2) array \n\nSpecify the V vertices of the polygon, sorted either clockwise or anti-clockwise. The first point may (but does not need to be) duplicated.    Returns \n \nmask(M, N) ndarray of bool \n\nTrue where the grid falls inside the polygon.      See also  \npoints_in_poly\n\n  \n inertia_tensor  \nskimage.measure.inertia_tensor(image, mu=None) [source]\n \nCompute the inertia tensor of the input image.  Parameters \n \nimagearray \n\nThe input image.  \nmuarray, optional \n\nThe pre-computed central moments of image. The inertia tensor computation requires the central moments of the image. If an application requires both the central moments and the inertia tensor (for example, skimage.measure.regionprops), then it is more efficient to pre-compute them and pass them to the inertia tensor call.    Returns \n \nTarray, shape (image.ndim, image.ndim) \n\nThe inertia tensor of the input image. \\(T_{i, j}\\) contains the covariance of image intensity along axes \\(i\\) and \\(j\\).     References  \n1  \nhttps://en.wikipedia.org/wiki/Moment_of_inertia#Inertia_tensor  \n2  \nBernd J\u00e4hne. Spatio-Temporal Image Processing: Theory and Scientific Applications. (Chapter 8: Tensor Methods) Springer, 1993.   \n inertia_tensor_eigvals  \nskimage.measure.inertia_tensor_eigvals(image, mu=None, T=None) [source]\n \nCompute the eigenvalues of the inertia tensor of the image. The inertia tensor measures covariance of the image intensity along the image axes. (See inertia_tensor.) The relative magnitude of the eigenvalues of the tensor is thus a measure of the elongation of a (bright) object in the image.  Parameters \n \nimagearray \n\nThe input image.  \nmuarray, optional \n\nThe pre-computed central moments of image.  \nTarray, shape (image.ndim, image.ndim) \n\nThe pre-computed inertia tensor. If T is given, mu and image are ignored.    Returns \n \neigvalslist of float, length image.ndim \n\nThe eigenvalues of the inertia tensor of image, in descending order.     Notes Computing the eigenvalues requires the inertia tensor of the input image. This is much faster if the central moments (mu) are provided, or, alternatively, one can provide the inertia tensor (T) directly. \n label  \nskimage.measure.label(input, background=None, return_num=False, connectivity=None) [source]\n \nLabel connected regions of an integer array. Two pixels are connected when they are neighbors and have the same value. In 2D, they can be neighbors either in a 1- or 2-connected sense. The value refers to the maximum number of orthogonal hops to consider a pixel/voxel a neighbor: 1-connectivity     2-connectivity     diagonal connection close-up\n\n     [ ]           [ ]  [ ]  [ ]             [ ]\n      |               \\  |  /                 |  <- hop 2\n[ ]--[x]--[ ]      [ ]--[x]--[ ]        [x]--[ ]\n      |               /  |  \\             hop 1\n     [ ]           [ ]  [ ]  [ ]\n  Parameters \n \ninputndarray of dtype int \n\nImage to label.  \nbackgroundint, optional \n\nConsider all pixels with this value as background pixels, and label them as 0. By default, 0-valued pixels are considered as background pixels.  \nreturn_numbool, optional \n\nWhether to return the number of assigned labels.  \nconnectivityint, optional \n\nMaximum number of orthogonal hops to consider a pixel/voxel as a neighbor. Accepted values are ranging from 1 to input.ndim. If None, a full connectivity of input.ndim is used.    Returns \n \nlabelsndarray of dtype int \n\nLabeled array, where all connected regions are assigned the same integer value.  \nnumint, optional \n\nNumber of labels, which equals the maximum label index and is only returned if return_num is True.      See also  \nregionprops\n\n\nregionprops_table\n\n  References  \n1  \nChristophe Fiorio and Jens Gustedt, \u201cTwo linear time Union-Find strategies for image processing\u201d, Theoretical Computer Science 154 (1996), pp. 165-181.  \n2  \nKensheng Wu, Ekow Otoo and Arie Shoshani, \u201cOptimizing connected component labeling algorithms\u201d, Paper LBNL-56864, 2005, Lawrence Berkeley National Laboratory (University of California), http://repositories.cdlib.org/lbnl/LBNL-56864   Examples >>> import numpy as np\n>>> x = np.eye(3).astype(int)\n>>> print(x)\n[[1 0 0]\n [0 1 0]\n [0 0 1]]\n>>> print(label(x, connectivity=1))\n[[1 0 0]\n [0 2 0]\n [0 0 3]]\n>>> print(label(x, connectivity=2))\n[[1 0 0]\n [0 1 0]\n [0 0 1]]\n>>> print(label(x, background=-1))\n[[1 2 2]\n [2 1 2]\n [2 2 1]]\n>>> x = np.array([[1, 0, 0],\n...               [1, 1, 5],\n...               [0, 0, 0]])\n>>> print(label(x))\n[[1 0 0]\n [1 1 2]\n [0 0 0]]\n \n Examples using skimage.measure.label\n \n  Measure region properties  \n\n  Euler number  \n\n  Segment human cells (in mitosis)   marching_cubes  \nskimage.measure.marching_cubes(volume, level=None, *, spacing=(1.0, 1.0, 1.0), gradient_direction='descent', step_size=1, allow_degenerate=True, method='lewiner', mask=None) [source]\n \nMarching cubes algorithm to find surfaces in 3d volumetric data. In contrast with Lorensen et al. approach [2], Lewiner et al. algorithm is faster, resolves ambiguities, and guarantees topologically correct results. Therefore, this algorithm generally a better choice.  Parameters \n \nvolume(M, N, P) array \n\nInput data volume to find isosurfaces. Will internally be converted to float32 if necessary.  \nlevelfloat, optional \n\nContour value to search for isosurfaces in volume. If not given or None, the average of the min and max of vol is used.  \nspacinglength-3 tuple of floats, optional \n\nVoxel spacing in spatial dimensions corresponding to numpy array indexing dimensions (M, N, P) as in volume.  \ngradient_directionstring, optional \n\nControls if the mesh was generated from an isosurface with gradient descent toward objects of interest (the default), or the opposite, considering the left-hand rule. The two options are: * descent : Object was greater than exterior * ascent : Exterior was greater than object  \nstep_sizeint, optional \n\nStep size in voxels. Default 1. Larger steps yield faster but coarser results. The result will always be topologically correct though.  \nallow_degeneratebool, optional \n\nWhether to allow degenerate (i.e. zero-area) triangles in the end-result. Default True. If False, degenerate triangles are removed, at the cost of making the algorithm slower.  method: str, optional\n\nOne of \u2018lewiner\u2019, \u2018lorensen\u2019 or \u2018_lorensen\u2019. Specify witch of Lewiner et al. or Lorensen et al. method will be used. The \u2018_lorensen\u2019 flag correspond to an old implementation that will be deprecated in version 0.19.  \nmask(M, N, P) array, optional \n\nBoolean array. The marching cube algorithm will be computed only on True elements. This will save computational time when interfaces are located within certain region of the volume M, N, P-e.g. the top half of the cube-and also allow to compute finite surfaces-i.e. open surfaces that do not end at the border of the cube.    Returns \n \nverts(V, 3) array \n\nSpatial coordinates for V unique mesh vertices. Coordinate order matches input volume (M, N, P). If allow_degenerate is set to True, then the presence of degenerate triangles in the mesh can make this array have duplicate vertices.  \nfaces(F, 3) array \n\nDefine triangular faces via referencing vertex indices from verts. This algorithm specifically outputs triangles, so each face has exactly three indices.  \nnormals(V, 3) array \n\nThe normal direction at each vertex, as calculated from the data.  \nvalues(V, ) array \n\nGives a measure for the maximum value of the data in the local region near each vertex. This can be used by visualization tools to apply a colormap to the mesh.      See also  \nskimage.measure.mesh_surface_area\n\n\nskimage.measure.find_contours\n\n  Notes The algorithm [1] is an improved version of Chernyaev\u2019s Marching Cubes 33 algorithm. It is an efficient algorithm that relies on heavy use of lookup tables to handle the many different cases, keeping the algorithm relatively easy. This implementation is written in Cython, ported from Lewiner\u2019s C++ implementation. To quantify the area of an isosurface generated by this algorithm, pass verts and faces to skimage.measure.mesh_surface_area. Regarding visualization of algorithm output, to contour a volume named myvolume about the level 0.0, using the mayavi package: >>>\n>> from mayavi import mlab\n>> verts, faces, _, _ = marching_cubes(myvolume, 0.0)\n>> mlab.triangular_mesh([vert[0] for vert in verts],\n                        [vert[1] for vert in verts],\n                        [vert[2] for vert in verts],\n                        faces)\n>> mlab.show()\n Similarly using the visvis package: >>>\n>> import visvis as vv\n>> verts, faces, normals, values = marching_cubes(myvolume, 0.0)\n>> vv.mesh(np.fliplr(verts), faces, normals, values)\n>> vv.use().Run()\n To reduce the number of triangles in the mesh for better performance, see this example using the mayavi package. References  \n1  \nThomas Lewiner, Helio Lopes, Antonio Wilson Vieira and Geovan Tavares. Efficient implementation of Marching Cubes\u2019 cases with topological guarantees. Journal of Graphics Tools 8(2) pp. 1-15 (december 2003). DOI:10.1080/10867651.2003.10487582  \n2  \nLorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings) 21(4) July 1987, p. 163-170). DOI:10.1145/37401.37422   \n marching_cubes_classic  \nskimage.measure.marching_cubes_classic(volume, level=None, spacing=(1.0, 1.0, 1.0), gradient_direction='descent') [source]\n \nClassic marching cubes algorithm to find surfaces in 3d volumetric data. Note that the marching_cubes() algorithm is recommended over this algorithm, because it\u2019s faster and produces better results.  Parameters \n \nvolume(M, N, P) array of doubles \n\nInput data volume to find isosurfaces. Will be cast to np.float64.  \nlevelfloat \n\nContour value to search for isosurfaces in volume. If not given or None, the average of the min and max of vol is used.  \nspacinglength-3 tuple of floats \n\nVoxel spacing in spatial dimensions corresponding to numpy array indexing dimensions (M, N, P) as in volume.  \ngradient_directionstring \n\nControls if the mesh was generated from an isosurface with gradient descent toward objects of interest (the default), or the opposite. The two options are: * descent : Object was greater than exterior * ascent : Exterior was greater than object    Returns \n \nverts(V, 3) array \n\nSpatial coordinates for V unique mesh vertices. Coordinate order matches input volume (M, N, P). If allow_degenerate is set to True, then the presence of degenerate triangles in the mesh can make this array have duplicate vertices.  \nfaces(F, 3) array \n\nDefine triangular faces via referencing vertex indices from verts. This algorithm specifically outputs triangles, so each face has exactly three indices.      See also  \nskimage.measure.marching_cubes\n\n\nskimage.measure.mesh_surface_area\n\n  Notes The marching cubes algorithm is implemented as described in [1]. A simple explanation is available here: http://users.polytech.unice.fr/~lingrand/MarchingCubes/algo.html\n There are several known ambiguous cases in the marching cubes algorithm. Using point labeling as in [1], Figure 4, as shown:     v8 ------ v7\n   / |       / |        y\n  /  |      /  |        ^  z\nv4 ------ v3   |        | /\n |  v5 ----|- v6        |/          (note: NOT right handed!)\n |  /      |  /          ----> x\n | /       | /\nv1 ------ v2\n Most notably, if v4, v8, v2, and v6 are all >= level (or any generalization of this case) two parallel planes are generated by this algorithm, separating v4 and v8 from v2 and v6. An equally valid interpretation would be a single connected thin surface enclosing all four points. This is the best known ambiguity, though there are others. This algorithm does not attempt to resolve such ambiguities; it is a naive implementation of marching cubes as in [1], but may be a good beginning for work with more recent techniques (Dual Marching Cubes, Extended Marching Cubes, Cubic Marching Squares, etc.). Because of interactions between neighboring cubes, the isosurface(s) generated by this algorithm are NOT guaranteed to be closed, particularly for complicated contours. Furthermore, this algorithm does not guarantee a single contour will be returned. Indeed, ALL isosurfaces which cross level will be found, regardless of connectivity. The output is a triangular mesh consisting of a set of unique vertices and connecting triangles. The order of these vertices and triangles in the output list is determined by the position of the smallest x,y,z (in lexicographical order) coordinate in the contour. This is a side-effect of how the input array is traversed, but can be relied upon. The generated mesh guarantees coherent orientation as of version 0.12. To quantify the area of an isosurface generated by this algorithm, pass outputs directly into skimage.measure.mesh_surface_area. References  \n1(1,2,3)  \nLorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings) 21(4) July 1987, p. 163-170). DOI:10.1145/37401.37422   \n marching_cubes_lewiner  \nskimage.measure.marching_cubes_lewiner(volume, level=None, spacing=(1.0, 1.0, 1.0), gradient_direction='descent', step_size=1, allow_degenerate=True, use_classic=False, mask=None) [source]\n \nLewiner marching cubes algorithm to find surfaces in 3d volumetric data. In contrast to marching_cubes_classic(), this algorithm is faster, resolves ambiguities, and guarantees topologically correct results. Therefore, this algorithm generally a better choice, unless there is a specific need for the classic algorithm.  Parameters \n \nvolume(M, N, P) array \n\nInput data volume to find isosurfaces. Will internally be converted to float32 if necessary.  \nlevelfloat \n\nContour value to search for isosurfaces in volume. If not given or None, the average of the min and max of vol is used.  \nspacinglength-3 tuple of floats \n\nVoxel spacing in spatial dimensions corresponding to numpy array indexing dimensions (M, N, P) as in volume.  \ngradient_directionstring \n\nControls if the mesh was generated from an isosurface with gradient descent toward objects of interest (the default), or the opposite, considering the left-hand rule. The two options are: * descent : Object was greater than exterior * ascent : Exterior was greater than object  \nstep_sizeint \n\nStep size in voxels. Default 1. Larger steps yield faster but coarser results. The result will always be topologically correct though.  \nallow_degeneratebool \n\nWhether to allow degenerate (i.e. zero-area) triangles in the end-result. Default True. If False, degenerate triangles are removed, at the cost of making the algorithm slower.  \nuse_classicbool \n\nIf given and True, the classic marching cubes by Lorensen (1987) is used. This option is included for reference purposes. Note that this algorithm has ambiguities and is not guaranteed to produce a topologically correct result. The results with using this option are not generally the same as the marching_cubes_classic() function.  \nmask(M, N, P) array \n\nBoolean array. The marching cube algorithm will be computed only on True elements. This will save computational time when interfaces are located within certain region of the volume M, N, P-e.g. the top half of the cube-and also allow to compute finite surfaces-i.e. open surfaces that do not end at the border of the cube.    Returns \n \nverts(V, 3) array \n\nSpatial coordinates for V unique mesh vertices. Coordinate order matches input volume (M, N, P). If allow_degenerate is set to True, then the presence of degenerate triangles in the mesh can make this array have duplicate vertices.  \nfaces(F, 3) array \n\nDefine triangular faces via referencing vertex indices from verts. This algorithm specifically outputs triangles, so each face has exactly three indices.  \nnormals(V, 3) array \n\nThe normal direction at each vertex, as calculated from the data.  \nvalues(V, ) array \n\nGives a measure for the maximum value of the data in the local region near each vertex. This can be used by visualization tools to apply a colormap to the mesh.      See also  \nskimage.measure.marching_cubes\n\n\nskimage.measure.mesh_surface_area\n\n  Notes The algorithm [1] is an improved version of Chernyaev\u2019s Marching Cubes 33 algorithm. It is an efficient algorithm that relies on heavy use of lookup tables to handle the many different cases, keeping the algorithm relatively easy. This implementation is written in Cython, ported from Lewiner\u2019s C++ implementation. To quantify the area of an isosurface generated by this algorithm, pass verts and faces to skimage.measure.mesh_surface_area. Regarding visualization of algorithm output, to contour a volume named myvolume about the level 0.0, using the mayavi package: >>> from mayavi import mlab \n>>> verts, faces, normals, values = marching_cubes_lewiner(myvolume, 0.0) \n>>> mlab.triangular_mesh([vert[0] for vert in verts],\n...                      [vert[1] for vert in verts],\n...                      [vert[2] for vert in verts],\n...                      faces) \n>>> mlab.show() \n Similarly using the visvis package: >>> import visvis as vv \n>>> verts, faces, normals, values = marching_cubes_lewiner(myvolume, 0.0) \n>>> vv.mesh(np.fliplr(verts), faces, normals, values) \n>>> vv.use().Run() \n References  \n1  \nThomas Lewiner, Helio Lopes, Antonio Wilson Vieira and Geovan Tavares. Efficient implementation of Marching Cubes\u2019 cases with topological guarantees. Journal of Graphics Tools 8(2) pp. 1-15 (december 2003). DOI:10.1080/10867651.2003.10487582   \n mesh_surface_area  \nskimage.measure.mesh_surface_area(verts, faces) [source]\n \nCompute surface area, given vertices & triangular faces  Parameters \n \nverts(V, 3) array of floats \n\nArray containing (x, y, z) coordinates for V unique mesh vertices.  \nfaces(F, 3) array of ints \n\nList of length-3 lists of integers, referencing vertex coordinates as provided in verts    Returns \n \nareafloat \n\nSurface area of mesh. Units now [coordinate units] ** 2.      See also  \nskimage.measure.marching_cubes\n\n\nskimage.measure.marching_cubes_classic\n\n  Notes The arguments expected by this function are the first two outputs from skimage.measure.marching_cubes. For unit correct output, ensure correct spacing was passed to skimage.measure.marching_cubes. This algorithm works properly only if the faces provided are all triangles. \n moments  \nskimage.measure.moments(image, order=3) [source]\n \nCalculate all raw image moments up to a certain order.  The following properties can be calculated from raw image moments:\n\n Area as: M[0, 0]. Centroid as: {M[1, 0] / M[0, 0], M[0, 1] / M[0, 0]}.    Note that raw moments are neither translation, scale nor rotation invariant.  Parameters \n \nimagenD double or uint8 array \n\nRasterized shape as image.  \norderint, optional \n\nMaximum order of moments. Default is 3.    Returns \n \nm(order + 1, order + 1) array \n\nRaw image moments.     References  \n1  \nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.  \n2  \nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.  \n3  \nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.  \n4  \nhttps://en.wikipedia.org/wiki/Image_moment   Examples >>> image = np.zeros((20, 20), dtype=np.double)\n>>> image[13:17, 13:17] = 1\n>>> M = moments(image)\n>>> centroid = (M[1, 0] / M[0, 0], M[0, 1] / M[0, 0])\n>>> centroid\n(14.5, 14.5)\n \n moments_central  \nskimage.measure.moments_central(image, center=None, order=3, **kwargs) [source]\n \nCalculate all central image moments up to a certain order. The center coordinates (cr, cc) can be calculated from the raw moments as: {M[1, 0] / M[0, 0], M[0, 1] / M[0, 0]}. Note that central moments are translation invariant but not scale and rotation invariant.  Parameters \n \nimagenD double or uint8 array \n\nRasterized shape as image.  \ncentertuple of float, optional \n\nCoordinates of the image centroid. This will be computed if it is not provided.  \norderint, optional \n\nThe maximum order of moments computed.    Returns \n \nmu(order + 1, order + 1) array \n\nCentral image moments.     References  \n1  \nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.  \n2  \nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.  \n3  \nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.  \n4  \nhttps://en.wikipedia.org/wiki/Image_moment   Examples >>> image = np.zeros((20, 20), dtype=np.double)\n>>> image[13:17, 13:17] = 1\n>>> M = moments(image)\n>>> centroid = (M[1, 0] / M[0, 0], M[0, 1] / M[0, 0])\n>>> moments_central(image, centroid)\narray([[16.,  0., 20.,  0.],\n       [ 0.,  0.,  0.,  0.],\n       [20.,  0., 25.,  0.],\n       [ 0.,  0.,  0.,  0.]])\n \n moments_coords  \nskimage.measure.moments_coords(coords, order=3) [source]\n \nCalculate all raw image moments up to a certain order.  The following properties can be calculated from raw image moments:\n\n Area as: M[0, 0]. Centroid as: {M[1, 0] / M[0, 0], M[0, 1] / M[0, 0]}.    Note that raw moments are neither translation, scale nor rotation invariant.  Parameters \n \ncoords(N, D) double or uint8 array \n\nArray of N points that describe an image of D dimensionality in Cartesian space.  \norderint, optional \n\nMaximum order of moments. Default is 3.    Returns \n \nM(order + 1, order + 1, \u2026) array \n\nRaw image moments. (D dimensions)     References  \n1  \nJohannes Kilian. Simple Image Analysis By Moments. Durham University, version 0.2, Durham, 2001.   Examples >>> coords = np.array([[row, col]\n...                    for row in range(13, 17)\n...                    for col in range(14, 18)], dtype=np.double)\n>>> M = moments_coords(coords)\n>>> centroid = (M[1, 0] / M[0, 0], M[0, 1] / M[0, 0])\n>>> centroid\n(14.5, 15.5)\n \n moments_coords_central  \nskimage.measure.moments_coords_central(coords, center=None, order=3) [source]\n \nCalculate all central image moments up to a certain order.  The following properties can be calculated from raw image moments:\n\n Area as: M[0, 0]. Centroid as: {M[1, 0] / M[0, 0], M[0, 1] / M[0, 0]}.    Note that raw moments are neither translation, scale nor rotation invariant.  Parameters \n \ncoords(N, D) double or uint8 array \n\nArray of N points that describe an image of D dimensionality in Cartesian space. A tuple of coordinates as returned by np.nonzero is also accepted as input.  \ncentertuple of float, optional \n\nCoordinates of the image centroid. This will be computed if it is not provided.  \norderint, optional \n\nMaximum order of moments. Default is 3.    Returns \n \nMc(order + 1, order + 1, \u2026) array \n\nCentral image moments. (D dimensions)     References  \n1  \nJohannes Kilian. Simple Image Analysis By Moments. Durham University, version 0.2, Durham, 2001.   Examples >>> coords = np.array([[row, col]\n...                    for row in range(13, 17)\n...                    for col in range(14, 18)])\n>>> moments_coords_central(coords)\narray([[16.,  0., 20.,  0.],\n       [ 0.,  0.,  0.,  0.],\n       [20.,  0., 25.,  0.],\n       [ 0.,  0.,  0.,  0.]])\n As seen above, for symmetric objects, odd-order moments (columns 1 and 3, rows 1 and 3) are zero when centered on the centroid, or center of mass, of the object (the default). If we break the symmetry by adding a new point, this no longer holds: >>> coords2 = np.concatenate((coords, [[17, 17]]), axis=0)\n>>> np.round(moments_coords_central(coords2),\n...          decimals=2)  \narray([[17.  ,  0.  , 22.12, -2.49],\n       [ 0.  ,  3.53,  1.73,  7.4 ],\n       [25.88,  6.02, 36.63,  8.83],\n       [ 4.15, 19.17, 14.8 , 39.6 ]])\n Image moments and central image moments are equivalent (by definition) when the center is (0, 0): >>> np.allclose(moments_coords(coords),\n...             moments_coords_central(coords, (0, 0)))\nTrue\n \n moments_hu  \nskimage.measure.moments_hu(nu) [source]\n \nCalculate Hu\u2019s set of image moments (2D-only). Note that this set of moments is proofed to be translation, scale and rotation invariant.  Parameters \n \nnu(M, M) array \n\nNormalized central image moments, where M must be >= 4.    Returns \n \nnu(7,) array \n\nHu\u2019s set of image moments.     References  \n1  \nM. K. Hu, \u201cVisual Pattern Recognition by Moment Invariants\u201d, IRE Trans. Info. Theory, vol. IT-8, pp. 179-187, 1962  \n2  \nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.  \n3  \nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.  \n4  \nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.  \n5  \nhttps://en.wikipedia.org/wiki/Image_moment   Examples >>> image = np.zeros((20, 20), dtype=np.double)\n>>> image[13:17, 13:17] = 0.5\n>>> image[10:12, 10:12] = 1\n>>> mu = moments_central(image)\n>>> nu = moments_normalized(mu)\n>>> moments_hu(nu)\narray([7.45370370e-01, 3.51165981e-01, 1.04049179e-01, 4.06442107e-02,\n       2.64312299e-03, 2.40854582e-02, 4.33680869e-19])\n \n moments_normalized  \nskimage.measure.moments_normalized(mu, order=3) [source]\n \nCalculate all normalized central image moments up to a certain order. Note that normalized central moments are translation and scale invariant but not rotation invariant.  Parameters \n \nmu(M,[ \u2026,] M) array \n\nCentral image moments, where M must be greater than or equal to order.  \norderint, optional \n\nMaximum order of moments. Default is 3.    Returns \n \nnu(order + 1,[ \u2026,] order + 1) array \n\nNormalized central image moments.     References  \n1  \nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.  \n2  \nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.  \n3  \nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.  \n4  \nhttps://en.wikipedia.org/wiki/Image_moment   Examples >>> image = np.zeros((20, 20), dtype=np.double)\n>>> image[13:17, 13:17] = 1\n>>> m = moments(image)\n>>> centroid = (m[0, 1] / m[0, 0], m[1, 0] / m[0, 0])\n>>> mu = moments_central(image, centroid)\n>>> moments_normalized(mu)\narray([[       nan,        nan, 0.078125  , 0.        ],\n       [       nan, 0.        , 0.        , 0.        ],\n       [0.078125  , 0.        , 0.00610352, 0.        ],\n       [0.        , 0.        , 0.        , 0.        ]])\n \n perimeter  \nskimage.measure.perimeter(image, neighbourhood=4) [source]\n \nCalculate total perimeter of all objects in binary image.  Parameters \n \nimage(N, M) ndarray \n\n2D binary image.  \nneighbourhood4 or 8, optional \n\nNeighborhood connectivity for border pixel determination. It is used to compute the contour. A higher neighbourhood widens the border on which the perimeter is computed.    Returns \n \nperimeterfloat \n\nTotal perimeter of all objects in binary image.     References  \n1  \nK. Benkrid, D. Crookes. Design and FPGA Implementation of a Perimeter Estimator. The Queen\u2019s University of Belfast. http://www.cs.qub.ac.uk/~d.crookes/webpubs/papers/perimeter.doc   Examples >>> from skimage import data, util\n>>> from skimage.measure import label\n>>> # coins image (binary)\n>>> img_coins = data.coins() > 110\n>>> # total perimeter of all objects in the image\n>>> perimeter(img_coins, neighbourhood=4)  \n7796.867...\n>>> perimeter(img_coins, neighbourhood=8)  \n8806.268...\n \n Examples using skimage.measure.perimeter\n \n  Different perimeters   perimeter_crofton  \nskimage.measure.perimeter_crofton(image, directions=4) [source]\n \nCalculate total Crofton perimeter of all objects in binary image.  Parameters \n \nimage(N, M) ndarray \n\n2D image. If image is not binary, all values strictly greater than zero are considered as the object.  \ndirections2 or 4, optional \n\nNumber of directions used to approximate the Crofton perimeter. By default, 4 is used: it should be more accurate than 2. Computation time is the same in both cases.    Returns \n \nperimeterfloat \n\nTotal perimeter of all objects in binary image.     Notes This measure is based on Crofton formula [1], which is a measure from integral geometry. It is defined for general curve length evaluation via a double integral along all directions. In a discrete space, 2 or 4 directions give a quite good approximation, 4 being more accurate than 2 for more complex shapes. Similar to perimeter(), this function returns an approximation of the perimeter in continuous space. References  \n1  \nhttps://en.wikipedia.org/wiki/Crofton_formula  \n2  \nS. Rivollier. Analyse d\u2019image geometrique et morphometrique par diagrammes de forme et voisinages adaptatifs generaux. PhD thesis, 2010. Ecole Nationale Superieure des Mines de Saint-Etienne. https://tel.archives-ouvertes.fr/tel-00560838   Examples >>> from skimage import data, util\n>>> from skimage.measure import label\n>>> # coins image (binary)\n>>> img_coins = data.coins() > 110\n>>> # total perimeter of all objects in the image\n>>> perimeter_crofton(img_coins, directions=2)  \n8144.578...\n>>> perimeter_crofton(img_coins, directions=4)  \n7837.077...\n \n Examples using skimage.measure.perimeter_crofton\n \n  Different perimeters   points_in_poly  \nskimage.measure.points_in_poly(points, verts) [source]\n \nTest whether points lie inside a polygon.  Parameters \n \npoints(N, 2) array \n\nInput points, (x, y).  \nverts(M, 2) array \n\nVertices of the polygon, sorted either clockwise or anti-clockwise. The first point may (but does not need to be) duplicated.    Returns \n \nmask(N,) array of bool \n\nTrue if corresponding point is inside the polygon.      See also  \ngrid_points_in_poly\n\n  \n profile_line  \nskimage.measure.profile_line(image, src, dst, linewidth=1, order=None, mode=None, cval=0.0, *, reduce_func=<function mean>) [source]\n \nReturn the intensity profile of an image measured along a scan line.  Parameters \n \nimagendarray, shape (M, N[, C]) \n\nThe image, either grayscale (2D array) or multichannel (3D array, where the final axis contains the channel information).  \nsrcarray_like, shape (2, ) \n\nThe coordinates of the start point of the scan line.  \ndstarray_like, shape (2, ) \n\nThe coordinates of the end point of the scan line. The destination point is included in the profile, in contrast to standard numpy indexing.  \nlinewidthint, optional \n\nWidth of the scan, perpendicular to the line  \norderint in {0, 1, 2, 3, 4, 5}, optional \n\nThe order of the spline interpolation, default is 0 if image.dtype is bool and 1 otherwise. The order has to be in the range 0-5. See skimage.transform.warp for detail.  \nmode{\u2018constant\u2019, \u2018nearest\u2019, \u2018reflect\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional \n\nHow to compute any values falling outside of the image.  \ncvalfloat, optional \n\nIf mode is \u2018constant\u2019, what constant value to use outside the image.  \nreduce_funccallable, optional \n\nFunction used to calculate the aggregation of pixel values perpendicular to the profile_line direction when linewidth > 1. If set to None the unreduced array will be returned.    Returns \n \nreturn_valuearray \n\nThe intensity profile along the scan line. The length of the profile is the ceil of the computed length of the scan line.     Examples >>> x = np.array([[1, 1, 1, 2, 2, 2]])\n>>> img = np.vstack([np.zeros_like(x), x, x, x, np.zeros_like(x)])\n>>> img\narray([[0, 0, 0, 0, 0, 0],\n       [1, 1, 1, 2, 2, 2],\n       [1, 1, 1, 2, 2, 2],\n       [1, 1, 1, 2, 2, 2],\n       [0, 0, 0, 0, 0, 0]])\n>>> profile_line(img, (2, 1), (2, 4))\narray([1., 1., 2., 2.])\n>>> profile_line(img, (1, 0), (1, 6), cval=4)\narray([1., 1., 1., 2., 2., 2., 4.])\n The destination point is included in the profile, in contrast to standard numpy indexing. For example: >>> profile_line(img, (1, 0), (1, 6))  # The final point is out of bounds\narray([1., 1., 1., 2., 2., 2., 0.])\n>>> profile_line(img, (1, 0), (1, 5))  # This accesses the full first row\narray([1., 1., 1., 2., 2., 2.])\n For different reduce_func inputs: >>> profile_line(img, (1, 0), (1, 3), linewidth=3, reduce_func=np.mean)\narray([0.66666667, 0.66666667, 0.66666667, 1.33333333])\n>>> profile_line(img, (1, 0), (1, 3), linewidth=3, reduce_func=np.max)\narray([1, 1, 1, 2])\n>>> profile_line(img, (1, 0), (1, 3), linewidth=3, reduce_func=np.sum)\narray([2, 2, 2, 4])\n The unreduced array will be returned when reduce_func is None or when reduce_func acts on each pixel value individually. >>> profile_line(img, (1, 2), (4, 2), linewidth=3, order=0,\n...     reduce_func=None)\narray([[1, 1, 2],\n       [1, 1, 2],\n       [1, 1, 2],\n       [0, 0, 0]])\n>>> profile_line(img, (1, 0), (1, 3), linewidth=3, reduce_func=np.sqrt)\narray([[1.        , 1.        , 0.        ],\n       [1.        , 1.        , 0.        ],\n       [1.        , 1.        , 0.        ],\n       [1.41421356, 1.41421356, 0.        ]])\n \n ransac  \nskimage.measure.ransac(data, model_class, min_samples, residual_threshold, is_data_valid=None, is_model_valid=None, max_trials=100, stop_sample_num=inf, stop_residuals_sum=0, stop_probability=1, random_state=None, initial_inliers=None) [source]\n \nFit a model to data with the RANSAC (random sample consensus) algorithm. RANSAC is an iterative algorithm for the robust estimation of parameters from a subset of inliers from the complete data set. Each iteration performs the following tasks:  Select min_samples random samples from the original data and check whether the set of data is valid (see is_data_valid). Estimate a model to the random subset (model_cls.estimate(*data[random_subset]) and check whether the estimated model is valid (see is_model_valid). Classify all data as inliers or outliers by calculating the residuals to the estimated model (model_cls.residuals(*data)) - all data samples with residuals smaller than the residual_threshold are considered as inliers. Save estimated model as best model if number of inlier samples is maximal. In case the current estimated model has the same number of inliers, it is only considered as the best model if it has less sum of residuals.  These steps are performed either a maximum number of times or until one of the special stop criteria are met. The final model is estimated using all inlier samples of the previously determined best model.  Parameters \n \ndata[list, tuple of] (N, \u2026) array \n\nData set to which the model is fitted, where N is the number of data points and the remaining dimension are depending on model requirements. If the model class requires multiple input data arrays (e.g. source and destination coordinates of skimage.transform.AffineTransform), they can be optionally passed as tuple or list. Note, that in this case the functions estimate(*data), residuals(*data), is_model_valid(model, *random_data) and is_data_valid(*random_data) must all take each data array as separate arguments.  \nmodel_classobject \n\nObject with the following object methods:  success = estimate(*data) residuals(*data)  where success indicates whether the model estimation succeeded (True or None for success, False for failure).  \nmin_samplesint in range (0, N) \n\nThe minimum number of data points to fit a model to.  \nresidual_thresholdfloat larger than 0 \n\nMaximum distance for a data point to be classified as an inlier.  \nis_data_validfunction, optional \n\nThis function is called with the randomly selected data before the model is fitted to it: is_data_valid(*random_data).  \nis_model_validfunction, optional \n\nThis function is called with the estimated model and the randomly selected data: is_model_valid(model, *random_data), .  \nmax_trialsint, optional \n\nMaximum number of iterations for random sample selection.  \nstop_sample_numint, optional \n\nStop iteration if at least this number of inliers are found.  \nstop_residuals_sumfloat, optional \n\nStop iteration if sum of residuals is less than or equal to this threshold.  \nstop_probabilityfloat in range [0, 1], optional \n\nRANSAC iteration stops if at least one outlier-free set of the training data is sampled with probability >= stop_probability, depending on the current best model\u2019s inlier ratio and the number of trials. This requires to generate at least N samples (trials): N >= log(1 - probability) / log(1 - e**m) where the probability (confidence) is typically set to a high value such as 0.99, e is the current fraction of inliers w.r.t. the total number of samples, and m is the min_samples value.  \nrandom_stateint, RandomState instance or None, optional \n\nIf int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.  \ninitial_inliersarray-like of bool, shape (N,), optional \n\nInitial samples selection for model estimation    Returns \n \nmodelobject \n\nBest model with largest consensus set.  \ninliers(N, ) array \n\nBoolean mask of inliers classified as True.     References  \n1  \n\u201cRANSAC\u201d, Wikipedia, https://en.wikipedia.org/wiki/RANSAC   Examples Generate ellipse data without tilt and add noise: >>> t = np.linspace(0, 2 * np.pi, 50)\n>>> xc, yc = 20, 30\n>>> a, b = 5, 10\n>>> x = xc + a * np.cos(t)\n>>> y = yc + b * np.sin(t)\n>>> data = np.column_stack([x, y])\n>>> np.random.seed(seed=1234)\n>>> data += np.random.normal(size=data.shape)\n Add some faulty data: >>> data[0] = (100, 100)\n>>> data[1] = (110, 120)\n>>> data[2] = (120, 130)\n>>> data[3] = (140, 130)\n Estimate ellipse model using all available data: >>> model = EllipseModel()\n>>> model.estimate(data)\nTrue\n>>> np.round(model.params)  \narray([ 72.,  75.,  77.,  14.,   1.])\n Estimate ellipse model using RANSAC: >>> ransac_model, inliers = ransac(data, EllipseModel, 20, 3, max_trials=50)\n>>> abs(np.round(ransac_model.params))\narray([20., 30.,  5., 10.,  0.])\n>>> inliers \narray([False, False, False, False,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True], dtype=bool)\n>>> sum(inliers) > 40\nTrue\n RANSAC can be used to robustly estimate a geometric transformation. In this section, we also show how to use a proportion of the total samples, rather than an absolute number. >>> from skimage.transform import SimilarityTransform\n>>> np.random.seed(0)\n>>> src = 100 * np.random.rand(50, 2)\n>>> model0 = SimilarityTransform(scale=0.5, rotation=1, translation=(10, 20))\n>>> dst = model0(src)\n>>> dst[0] = (10000, 10000)\n>>> dst[1] = (-100, 100)\n>>> dst[2] = (50, 50)\n>>> ratio = 0.5  # use half of the samples\n>>> min_samples = int(ratio * len(src))\n>>> model, inliers = ransac((src, dst), SimilarityTransform, min_samples, 10,\n...                         initial_inliers=np.ones(len(src), dtype=bool))\n>>> inliers\narray([False, False, False,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True])\n \n regionprops  \nskimage.measure.regionprops(label_image, intensity_image=None, cache=True, coordinates=None, *, extra_properties=None) [source]\n \nMeasure properties of labeled image regions.  Parameters \n \nlabel_image(M, N[, P]) ndarray \n\nLabeled input image. Labels with value 0 are ignored.  Changed in version 0.14.1: Previously, label_image was processed by numpy.squeeze and so any number of singleton dimensions was allowed. This resulted in inconsistent handling of images with singleton dimensions. To recover the old behaviour, use regionprops(np.squeeze(label_image), ...).   \nintensity_image(M, N[, P][, C]) ndarray, optional \n\nIntensity (i.e., input) image with same size as labeled image, plus optionally an extra dimension for multichannel data. Default is None.  Changed in version 0.18.0: The ability to provide an extra dimension for channels was added.   \ncachebool, optional \n\nDetermine whether to cache calculated properties. The computation is much faster for cached properties, whereas the memory consumption increases.  \ncoordinatesDEPRECATED \n\nThis argument is deprecated and will be removed in a future version of scikit-image. See Coordinate conventions for more details.  Deprecated since version 0.16.0: Use \u201crc\u201d coordinates everywhere. It may be sufficient to call numpy.transpose on your label image to get the same values as 0.15 and earlier. However, for some properties, the transformation will be less trivial. For example, the new orientation is \\(\\frac{\\pi}{2}\\) plus the old orientation.   \nextra_propertiesIterable of callables \n\nAdd extra property computation functions that are not included with skimage. The name of the property is derived from the function name, the dtype is inferred by calling the function on a small sample. If the name of an extra property clashes with the name of an existing property the extra property wil not be visible and a UserWarning is issued. A property computation function must take a region mask as its first argument. If the property requires an intensity image, it must accept the intensity image as the second argument.    Returns \n \npropertieslist of RegionProperties \n\nEach item describes one labeled region, and can be accessed using the attributes listed below.      See also  \nlabel\n\n  Notes The following properties can be accessed as attributes or keys:  \nareaint \n\nNumber of pixels of the region.  \nbboxtuple \n\nBounding box (min_row, min_col, max_row, max_col). Pixels belonging to the bounding box are in the half-open interval [min_row; max_row) and [min_col; max_col).  \nbbox_areaint \n\nNumber of pixels of bounding box.  \ncentroidarray \n\nCentroid coordinate tuple (row, col).  \nconvex_areaint \n\nNumber of pixels of convex hull image, which is the smallest convex polygon that encloses the region.  \nconvex_image(H, J) ndarray \n\nBinary convex hull image which has the same size as bounding box.  \ncoords(N, 2) ndarray \n\nCoordinate list (row, col) of the region.  \neccentricityfloat \n\nEccentricity of the ellipse that has the same second-moments as the region. The eccentricity is the ratio of the focal distance (distance between focal points) over the major axis length. The value is in the interval [0, 1). When it is 0, the ellipse becomes a circle.  \nequivalent_diameterfloat \n\nThe diameter of a circle with the same area as the region.  \neuler_numberint \n\nEuler characteristic of the set of non-zero pixels. Computed as number of connected components subtracted by number of holes (input.ndim connectivity). In 3D, number of connected components plus number of holes subtracted by number of tunnels.  \nextentfloat \n\nRatio of pixels in the region to pixels in the total bounding box. Computed as area / (rows * cols)  \nferet_diameter_maxfloat \n\nMaximum Feret\u2019s diameter computed as the longest distance between points around a region\u2019s convex hull contour as determined by find_contours. [5]  \nfilled_areaint \n\nNumber of pixels of the region will all the holes filled in. Describes the area of the filled_image.  \nfilled_image(H, J) ndarray \n\nBinary region image with filled holes which has the same size as bounding box.  \nimage(H, J) ndarray \n\nSliced binary region image which has the same size as bounding box.  \ninertia_tensorndarray \n\nInertia tensor of the region for the rotation around its mass.  \ninertia_tensor_eigvalstuple \n\nThe eigenvalues of the inertia tensor in decreasing order.  \nintensity_imagendarray \n\nImage inside region bounding box.  \nlabelint \n\nThe label in the labeled input image.  \nlocal_centroidarray \n\nCentroid coordinate tuple (row, col), relative to region bounding box.  \nmajor_axis_lengthfloat \n\nThe length of the major axis of the ellipse that has the same normalized second central moments as the region.  \nmax_intensityfloat \n\nValue with the greatest intensity in the region.  \nmean_intensityfloat \n\nValue with the mean intensity in the region.  \nmin_intensityfloat \n\nValue with the least intensity in the region.  \nminor_axis_lengthfloat \n\nThe length of the minor axis of the ellipse that has the same normalized second central moments as the region.  \nmoments(3, 3) ndarray \n\nSpatial moments up to 3rd order: m_ij = sum{ array(row, col) * row^i * col^j }\n where the sum is over the row, col coordinates of the region.  \nmoments_central(3, 3) ndarray \n\nCentral moments (translation invariant) up to 3rd order: mu_ij = sum{ array(row, col) * (row - row_c)^i * (col - col_c)^j }\n where the sum is over the row, col coordinates of the region, and row_c and col_c are the coordinates of the region\u2019s centroid.  \nmoments_hutuple \n\nHu moments (translation, scale and rotation invariant).  \nmoments_normalized(3, 3) ndarray \n\nNormalized moments (translation and scale invariant) up to 3rd order: nu_ij = mu_ij / m_00^[(i+j)/2 + 1]\n where m_00 is the zeroth spatial moment.  \norientationfloat \n\nAngle between the 0th axis (rows) and the major axis of the ellipse that has the same second moments as the region, ranging from -pi/2 to pi/2 counter-clockwise.  \nperimeterfloat \n\nPerimeter of object which approximates the contour as a line through the centers of border pixels using a 4-connectivity.  \nperimeter_croftonfloat \n\nPerimeter of object approximated by the Crofton formula in 4 directions.  \nslicetuple of slices \n\nA slice to extract the object from the source image.  \nsolidityfloat \n\nRatio of pixels in the region to pixels of the convex hull image.  \nweighted_centroidarray \n\nCentroid coordinate tuple (row, col) weighted with intensity image.  \nweighted_local_centroidarray \n\nCentroid coordinate tuple (row, col), relative to region bounding box, weighted with intensity image.  \nweighted_moments(3, 3) ndarray \n\nSpatial moments of intensity image up to 3rd order: wm_ij = sum{ array(row, col) * row^i * col^j }\n where the sum is over the row, col coordinates of the region.  \nweighted_moments_central(3, 3) ndarray \n\nCentral moments (translation invariant) of intensity image up to 3rd order: wmu_ij = sum{ array(row, col) * (row - row_c)^i * (col - col_c)^j }\n where the sum is over the row, col coordinates of the region, and row_c and col_c are the coordinates of the region\u2019s weighted centroid.  \nweighted_moments_hutuple \n\nHu moments (translation, scale and rotation invariant) of intensity image.  \nweighted_moments_normalized(3, 3) ndarray \n\nNormalized moments (translation and scale invariant) of intensity image up to 3rd order: wnu_ij = wmu_ij / wm_00^[(i+j)/2 + 1]\n where wm_00 is the zeroth spatial moment (intensity-weighted area).   Each region also supports iteration, so that you can do: for prop in region:\n    print(prop, region[prop])\n References  \n1  \nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.  \n2  \nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.  \n3  \nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.  \n4  \nhttps://en.wikipedia.org/wiki/Image_moment  \n5  \nW. Pabst, E. Gregorov\u00e1. Characterization of particles and particle systems, pp. 27-28. ICT Prague, 2007. https://old.vscht.cz/sil/keramika/Characterization_of_particles/CPPS%20_English%20version_.pdf   Examples >>> from skimage import data, util\n>>> from skimage.measure import label, regionprops\n>>> img = util.img_as_ubyte(data.coins()) > 110\n>>> label_img = label(img, connectivity=img.ndim)\n>>> props = regionprops(label_img)\n>>> # centroid of first labeled object\n>>> props[0].centroid\n(22.72987986048314, 81.91228523446583)\n>>> # centroid of first labeled object\n>>> props[0]['centroid']\n(22.72987986048314, 81.91228523446583)\n Add custom measurements by passing functions as extra_properties >>> from skimage import data, util\n>>> from skimage.measure import label, regionprops\n>>> import numpy as np\n>>> img = util.img_as_ubyte(data.coins()) > 110\n>>> label_img = label(img, connectivity=img.ndim)\n>>> def pixelcount(regionmask):\n...     return np.sum(regionmask)\n>>> props = regionprops(label_img, extra_properties=(pixelcount,))\n>>> props[0].pixelcount\n7741\n>>> props[1]['pixelcount']\n42\n \n Examples using skimage.measure.regionprops\n \n  Measure region properties   regionprops_table  \nskimage.measure.regionprops_table(label_image, intensity_image=None, properties=('label', 'bbox'), *, cache=True, separator='-', extra_properties=None) [source]\n \nCompute image properties and return them as a pandas-compatible table. The table is a dictionary mapping column names to value arrays. See Notes section below for details.  New in version 0.16.   Parameters \n \nlabel_image(N, M[, P]) ndarray \n\nLabeled input image. Labels with value 0 are ignored.  \nintensity_image(M, N[, P][, C]) ndarray, optional \n\nIntensity (i.e., input) image with same size as labeled image, plus optionally an extra dimension for multichannel data. Default is None.  Changed in version 0.18.0: The ability to provide an extra dimension for channels was added.   \npropertiestuple or list of str, optional \n\nProperties that will be included in the resulting dictionary For a list of available properties, please see regionprops(). Users should remember to add \u201clabel\u201d to keep track of region identities.  \ncachebool, optional \n\nDetermine whether to cache calculated properties. The computation is much faster for cached properties, whereas the memory consumption increases.  \nseparatorstr, optional \n\nFor non-scalar properties not listed in OBJECT_COLUMNS, each element will appear in its own column, with the index of that element separated from the property name by this separator. For example, the inertia tensor of a 2D region will appear in four columns: inertia_tensor-0-0, inertia_tensor-0-1, inertia_tensor-1-0, and inertia_tensor-1-1 (where the separator is -). Object columns are those that cannot be split in this way because the number of columns would change depending on the object. For example, image and coords.  \nextra_propertiesIterable of callables \n\nAdd extra property computation functions that are not included with skimage. The name of the property is derived from the function name, the dtype is inferred by calling the function on a small sample. If the name of an extra property clashes with the name of an existing property the extra property wil not be visible and a UserWarning is issued. A property computation function must take a region mask as its first argument. If the property requires an intensity image, it must accept the intensity image as the second argument.    Returns \n \nout_dictdict \n\nDictionary mapping property names to an array of values of that property, one value per region. This dictionary can be used as input to pandas DataFrame to map property names to columns in the frame and regions to rows. If the image has no regions, the arrays will have length 0, but the correct type.     Notes Each column contains either a scalar property, an object property, or an element in a multidimensional array. Properties with scalar values for each region, such as \u201ceccentricity\u201d, will appear as a float or int array with that property name as key. Multidimensional properties of fixed size for a given image dimension, such as \u201ccentroid\u201d (every centroid will have three elements in a 3D image, no matter the region size), will be split into that many columns, with the name {property_name}{separator}{element_num} (for 1D properties), {property_name}{separator}{elem_num0}{separator}{elem_num1} (for 2D properties), and so on. For multidimensional properties that don\u2019t have a fixed size, such as \u201cimage\u201d (the image of a region varies in size depending on the region size), an object array will be used, with the corresponding property name as the key. Examples >>> from skimage import data, util, measure\n>>> image = data.coins()\n>>> label_image = measure.label(image > 110, connectivity=image.ndim)\n>>> props = measure.regionprops_table(label_image, image,\n...                           properties=['label', 'inertia_tensor',\n...                                       'inertia_tensor_eigvals'])\n>>> props  \n{'label': array([ 1,  2, ...]), ...\n 'inertia_tensor-0-0': array([  4.012...e+03,   8.51..., ...]), ...\n ...,\n 'inertia_tensor_eigvals-1': array([  2.67...e+02,   2.83..., ...])}\n The resulting dictionary can be directly passed to pandas, if installed, to obtain a clean DataFrame: >>> import pandas as pd  \n>>> data = pd.DataFrame(props)  \n>>> data.head()  \n   label  inertia_tensor-0-0  ...  inertia_tensor_eigvals-1\n0      1         4012.909888  ...                267.065503\n1      2            8.514739  ...                  2.834806\n2      3            0.666667  ...                  0.000000\n3      4            0.000000  ...                  0.000000\n4      5            0.222222  ...                  0.111111\n [5 rows x 7 columns] If we want to measure a feature that does not come as a built-in property, we can define custom functions and pass them as extra_properties. For example, we can create a custom function that measures the intensity quartiles in a region: >>> from skimage import data, util, measure\n>>> import numpy as np\n>>> def quartiles(regionmask, intensity):\n...     return np.percentile(intensity[regionmask], q=(25, 50, 75))\n>>>\n>>> image = data.coins()\n>>> label_image = measure.label(image > 110, connectivity=image.ndim)\n>>> props = measure.regionprops_table(label_image, intensity_image=image,\n...                                   properties=('label',),\n...                                   extra_properties=(quartiles,))\n>>> import pandas as pd \n>>> pd.DataFrame(props).head() \n       label  quartiles-0  quartiles-1  quartiles-2\n0      1       117.00        123.0        130.0\n1      2       111.25        112.0        114.0\n2      3       111.00        111.0        111.0\n3      4       111.00        111.5        112.5\n4      5       112.50        113.0        114.0\n \n Examples using skimage.measure.regionprops_table\n \n  Measure region properties   shannon_entropy  \nskimage.measure.shannon_entropy(image, base=2) [source]\n \nCalculate the Shannon entropy of an image. The Shannon entropy is defined as S = -sum(pk * log(pk)), where pk are frequency/probability of pixels of value k.  Parameters \n \nimage(N, M) ndarray \n\nGrayscale input image.  \nbasefloat, optional \n\nThe logarithmic base to use.    Returns \n \nentropyfloat \n   Notes The returned value is measured in bits or shannon (Sh) for base=2, natural unit (nat) for base=np.e and hartley (Hart) for base=10. References  \n1  \nhttps://en.wikipedia.org/wiki/Entropy_(information_theory)  \n2  \nhttps://en.wiktionary.org/wiki/Shannon_entropy   Examples >>> from skimage import data\n>>> from skimage.measure import shannon_entropy\n>>> shannon_entropy(data.camera())\n7.231695011055706\n \n subdivide_polygon  \nskimage.measure.subdivide_polygon(coords, degree=2, preserve_ends=False) [source]\n \nSubdivision of polygonal curves using B-Splines. Note that the resulting curve is always within the convex hull of the original polygon. Circular polygons stay closed after subdivision.  Parameters \n \ncoords(N, 2) array \n\nCoordinate array.  \ndegree{1, 2, 3, 4, 5, 6, 7}, optional \n\nDegree of B-Spline. Default is 2.  \npreserve_endsbool, optional \n\nPreserve first and last coordinate of non-circular polygon. Default is False.    Returns \n \ncoords(M, 2) array \n\nSubdivided coordinate array.     References  \n1  \nhttp://mrl.nyu.edu/publications/subdiv-course2000/coursenotes00.pdf   \n CircleModel  \nclass skimage.measure.CircleModel [source]\n \nBases: skimage.measure.fit.BaseModel Total least squares estimator for 2D circles. The functional model of the circle is: r**2 = (x - xc)**2 + (y - yc)**2\n This estimator minimizes the squared distances from all points to the circle: min{ sum((r - sqrt((x_i - xc)**2 + (y_i - yc)**2))**2) }\n A minimum number of 3 points is required to solve for the parameters. Examples >>> t = np.linspace(0, 2 * np.pi, 25)\n>>> xy = CircleModel().predict_xy(t, params=(2, 3, 4))\n>>> model = CircleModel()\n>>> model.estimate(xy)\nTrue\n>>> tuple(np.round(model.params, 5))\n(2.0, 3.0, 4.0)\n>>> res = model.residuals(xy)\n>>> np.abs(np.round(res, 9))\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0.])\n  Attributes \n \nparamstuple \n\nCircle model parameters in the following order xc, yc, r.      \n__init__() [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(data) [source]\n \nEstimate circle model from data using total least squares.  Parameters \n \ndata(N, 2) array \n\nN points with (x, y) coordinates, respectively.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n  \npredict_xy(t, params=None) [source]\n \nPredict x- and y-coordinates using the estimated model.  Parameters \n \ntarray \n\nAngles in circle in radians. Angles start to count from positive x-axis to positive y-axis in a right-handed system.  \nparams(3, ) array, optional \n\nOptional custom parameter set.    Returns \n \nxy(\u2026, 2) array \n\nPredicted x- and y-coordinates.     \n  \nresiduals(data) [source]\n \nDetermine residuals of data to model. For each point the shortest distance to the circle is returned.  Parameters \n \ndata(N, 2) array \n\nN points with (x, y) coordinates, respectively.    Returns \n \nresiduals(N, ) array \n\nResidual for each data point.     \n \n EllipseModel  \nclass skimage.measure.EllipseModel [source]\n \nBases: skimage.measure.fit.BaseModel Total least squares estimator for 2D ellipses. The functional model of the ellipse is: xt = xc + a*cos(theta)*cos(t) - b*sin(theta)*sin(t)\nyt = yc + a*sin(theta)*cos(t) + b*cos(theta)*sin(t)\nd = sqrt((x - xt)**2 + (y - yt)**2)\n where (xt, yt) is the closest point on the ellipse to (x, y). Thus d is the shortest distance from the point to the ellipse. The estimator is based on a least squares minimization. The optimal solution is computed directly, no iterations are required. This leads to a simple, stable and robust fitting method. The params attribute contains the parameters in the following order: xc, yc, a, b, theta\n Examples >>> xy = EllipseModel().predict_xy(np.linspace(0, 2 * np.pi, 25),\n...                                params=(10, 15, 4, 8, np.deg2rad(30)))\n>>> ellipse = EllipseModel()\n>>> ellipse.estimate(xy)\nTrue\n>>> np.round(ellipse.params, 2)\narray([10.  , 15.  ,  4.  ,  8.  ,  0.52])\n>>> np.round(abs(ellipse.residuals(xy)), 5)\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0.])\n  Attributes \n \nparamstuple \n\nEllipse model parameters in the following order xc, yc, a, b, theta.      \n__init__() [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(data) [source]\n \nEstimate circle model from data using total least squares.  Parameters \n \ndata(N, 2) array \n\nN points with (x, y) coordinates, respectively.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     References  \n1  \nHalir, R.; Flusser, J. \u201cNumerically stable direct least squares fitting of ellipses\u201d. In Proc. 6th International Conference in Central Europe on Computer Graphics and Visualization. WSCG (Vol. 98, pp. 125-132).   \n  \npredict_xy(t, params=None) [source]\n \nPredict x- and y-coordinates using the estimated model.  Parameters \n \ntarray \n\nAngles in circle in radians. Angles start to count from positive x-axis to positive y-axis in a right-handed system.  \nparams(5, ) array, optional \n\nOptional custom parameter set.    Returns \n \nxy(\u2026, 2) array \n\nPredicted x- and y-coordinates.     \n  \nresiduals(data) [source]\n \nDetermine residuals of data to model. For each point the shortest distance to the ellipse is returned.  Parameters \n \ndata(N, 2) array \n\nN points with (x, y) coordinates, respectively.    Returns \n \nresiduals(N, ) array \n\nResidual for each data point.     \n \n LineModelND  \nclass skimage.measure.LineModelND [source]\n \nBases: skimage.measure.fit.BaseModel Total least squares estimator for N-dimensional lines. In contrast to ordinary least squares line estimation, this estimator minimizes the orthogonal distances of points to the estimated line. Lines are defined by a point (origin) and a unit vector (direction) according to the following vector equation: X = origin + lambda * direction\n Examples >>> x = np.linspace(1, 2, 25)\n>>> y = 1.5 * x + 3\n>>> lm = LineModelND()\n>>> lm.estimate(np.stack([x, y], axis=-1))\nTrue\n>>> tuple(np.round(lm.params, 5))\n(array([1.5 , 5.25]), array([0.5547 , 0.83205]))\n>>> res = lm.residuals(np.stack([x, y], axis=-1))\n>>> np.abs(np.round(res, 9))\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0.])\n>>> np.round(lm.predict_y(x[:5]), 3)\narray([4.5  , 4.562, 4.625, 4.688, 4.75 ])\n>>> np.round(lm.predict_x(y[:5]), 3)\narray([1.   , 1.042, 1.083, 1.125, 1.167])\n  Attributes \n \nparamstuple \n\nLine model parameters in the following order origin, direction.      \n__init__() [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(data) [source]\n \nEstimate line model from data. This minimizes the sum of shortest (orthogonal) distances from the given data points to the estimated line.  Parameters \n \ndata(N, dim) array \n\nN points in a space of dimensionality dim >= 2.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n  \npredict(x, axis=0, params=None) [source]\n \nPredict intersection of the estimated line model with a hyperplane orthogonal to a given axis.  Parameters \n \nx(n, 1) array \n\nCoordinates along an axis.  \naxisint \n\nAxis orthogonal to the hyperplane intersecting the line.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \ndata(n, m) array \n\nPredicted coordinates.    Raises \n ValueError\n\nIf the line is parallel to the given axis.     \n  \npredict_x(y, params=None) [source]\n \nPredict x-coordinates for 2D lines using the estimated model. Alias for: predict(y, axis=1)[:, 0]\n  Parameters \n \nyarray \n\ny-coordinates.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \nxarray \n\nPredicted x-coordinates.     \n  \npredict_y(x, params=None) [source]\n \nPredict y-coordinates for 2D lines using the estimated model. Alias for: predict(x, axis=0)[:, 1]\n  Parameters \n \nxarray \n\nx-coordinates.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \nyarray \n\nPredicted y-coordinates.     \n  \nresiduals(data, params=None) [source]\n \nDetermine residuals of data to model. For each point, the shortest (orthogonal) distance to the line is returned. It is obtained by projecting the data onto the line.  Parameters \n \ndata(N, dim) array \n\nN points in a space of dimension dim.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \nresiduals(N, ) array \n\nResidual for each data point.     \n \n\n"}, {"name": "measure.approximate_polygon()", "path": "api/skimage.measure#skimage.measure.approximate_polygon", "type": "measure", "text": " \nskimage.measure.approximate_polygon(coords, tolerance) [source]\n \nApproximate a polygonal chain with the specified tolerance. It is based on the Douglas-Peucker algorithm. Note that the approximated polygon is always within the convex hull of the original polygon.  Parameters \n \ncoords(N, 2) array \n\nCoordinate array.  \ntolerancefloat \n\nMaximum distance from original points of polygon to approximated polygonal chain. If tolerance is 0, the original coordinate array is returned.    Returns \n \ncoords(M, 2) array \n\nApproximated polygonal chain where M <= N.     References  \n1  \nhttps://en.wikipedia.org/wiki/Ramer-Douglas-Peucker_algorithm   \n"}, {"name": "measure.block_reduce()", "path": "api/skimage.measure#skimage.measure.block_reduce", "type": "measure", "text": " \nskimage.measure.block_reduce(image, block_size, func=<function sum>, cval=0, func_kwargs=None) [source]\n \nDownsample image by applying function func to local blocks. This function is useful for max and mean pooling, for example.  Parameters \n \nimagendarray \n\nN-dimensional input image.  \nblock_sizearray_like \n\nArray containing down-sampling integer factor along each axis.  \nfunccallable \n\nFunction object which is used to calculate the return value for each local block. This function must implement an axis parameter. Primary functions are numpy.sum, numpy.min, numpy.max, numpy.mean and numpy.median. See also func_kwargs.  \ncvalfloat \n\nConstant padding value if image is not perfectly divisible by the block size.  \nfunc_kwargsdict \n\nKeyword arguments passed to func. Notably useful for passing dtype argument to np.mean. Takes dictionary of inputs, e.g.: func_kwargs={'dtype': np.float16}).    Returns \n \nimagendarray \n\nDown-sampled image with same number of dimensions as input image.     Examples >>> from skimage.measure import block_reduce\n>>> image = np.arange(3*3*4).reshape(3, 3, 4)\n>>> image \narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]],\n       [[24, 25, 26, 27],\n        [28, 29, 30, 31],\n        [32, 33, 34, 35]]])\n>>> block_reduce(image, block_size=(3, 3, 1), func=np.mean)\narray([[[16., 17., 18., 19.]]])\n>>> image_max1 = block_reduce(image, block_size=(1, 3, 4), func=np.max)\n>>> image_max1 \narray([[[11]],\n       [[23]],\n       [[35]]])\n>>> image_max2 = block_reduce(image, block_size=(3, 1, 4), func=np.max)\n>>> image_max2 \narray([[[27],\n        [31],\n        [35]]])\n \n"}, {"name": "measure.CircleModel", "path": "api/skimage.measure#skimage.measure.CircleModel", "type": "measure", "text": " \nclass skimage.measure.CircleModel [source]\n \nBases: skimage.measure.fit.BaseModel Total least squares estimator for 2D circles. The functional model of the circle is: r**2 = (x - xc)**2 + (y - yc)**2\n This estimator minimizes the squared distances from all points to the circle: min{ sum((r - sqrt((x_i - xc)**2 + (y_i - yc)**2))**2) }\n A minimum number of 3 points is required to solve for the parameters. Examples >>> t = np.linspace(0, 2 * np.pi, 25)\n>>> xy = CircleModel().predict_xy(t, params=(2, 3, 4))\n>>> model = CircleModel()\n>>> model.estimate(xy)\nTrue\n>>> tuple(np.round(model.params, 5))\n(2.0, 3.0, 4.0)\n>>> res = model.residuals(xy)\n>>> np.abs(np.round(res, 9))\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0.])\n  Attributes \n \nparamstuple \n\nCircle model parameters in the following order xc, yc, r.      \n__init__() [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(data) [source]\n \nEstimate circle model from data using total least squares.  Parameters \n \ndata(N, 2) array \n\nN points with (x, y) coordinates, respectively.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n  \npredict_xy(t, params=None) [source]\n \nPredict x- and y-coordinates using the estimated model.  Parameters \n \ntarray \n\nAngles in circle in radians. Angles start to count from positive x-axis to positive y-axis in a right-handed system.  \nparams(3, ) array, optional \n\nOptional custom parameter set.    Returns \n \nxy(\u2026, 2) array \n\nPredicted x- and y-coordinates.     \n  \nresiduals(data) [source]\n \nDetermine residuals of data to model. For each point the shortest distance to the circle is returned.  Parameters \n \ndata(N, 2) array \n\nN points with (x, y) coordinates, respectively.    Returns \n \nresiduals(N, ) array \n\nResidual for each data point.     \n \n"}, {"name": "measure.CircleModel.estimate()", "path": "api/skimage.measure#skimage.measure.CircleModel.estimate", "type": "measure", "text": " \nestimate(data) [source]\n \nEstimate circle model from data using total least squares.  Parameters \n \ndata(N, 2) array \n\nN points with (x, y) coordinates, respectively.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n"}, {"name": "measure.CircleModel.predict_xy()", "path": "api/skimage.measure#skimage.measure.CircleModel.predict_xy", "type": "measure", "text": " \npredict_xy(t, params=None) [source]\n \nPredict x- and y-coordinates using the estimated model.  Parameters \n \ntarray \n\nAngles in circle in radians. Angles start to count from positive x-axis to positive y-axis in a right-handed system.  \nparams(3, ) array, optional \n\nOptional custom parameter set.    Returns \n \nxy(\u2026, 2) array \n\nPredicted x- and y-coordinates.     \n"}, {"name": "measure.CircleModel.residuals()", "path": "api/skimage.measure#skimage.measure.CircleModel.residuals", "type": "measure", "text": " \nresiduals(data) [source]\n \nDetermine residuals of data to model. For each point the shortest distance to the circle is returned.  Parameters \n \ndata(N, 2) array \n\nN points with (x, y) coordinates, respectively.    Returns \n \nresiduals(N, ) array \n\nResidual for each data point.     \n"}, {"name": "measure.CircleModel.__init__()", "path": "api/skimage.measure#skimage.measure.CircleModel.__init__", "type": "measure", "text": " \n__init__() [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "measure.EllipseModel", "path": "api/skimage.measure#skimage.measure.EllipseModel", "type": "measure", "text": " \nclass skimage.measure.EllipseModel [source]\n \nBases: skimage.measure.fit.BaseModel Total least squares estimator for 2D ellipses. The functional model of the ellipse is: xt = xc + a*cos(theta)*cos(t) - b*sin(theta)*sin(t)\nyt = yc + a*sin(theta)*cos(t) + b*cos(theta)*sin(t)\nd = sqrt((x - xt)**2 + (y - yt)**2)\n where (xt, yt) is the closest point on the ellipse to (x, y). Thus d is the shortest distance from the point to the ellipse. The estimator is based on a least squares minimization. The optimal solution is computed directly, no iterations are required. This leads to a simple, stable and robust fitting method. The params attribute contains the parameters in the following order: xc, yc, a, b, theta\n Examples >>> xy = EllipseModel().predict_xy(np.linspace(0, 2 * np.pi, 25),\n...                                params=(10, 15, 4, 8, np.deg2rad(30)))\n>>> ellipse = EllipseModel()\n>>> ellipse.estimate(xy)\nTrue\n>>> np.round(ellipse.params, 2)\narray([10.  , 15.  ,  4.  ,  8.  ,  0.52])\n>>> np.round(abs(ellipse.residuals(xy)), 5)\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0.])\n  Attributes \n \nparamstuple \n\nEllipse model parameters in the following order xc, yc, a, b, theta.      \n__init__() [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(data) [source]\n \nEstimate circle model from data using total least squares.  Parameters \n \ndata(N, 2) array \n\nN points with (x, y) coordinates, respectively.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     References  \n1  \nHalir, R.; Flusser, J. \u201cNumerically stable direct least squares fitting of ellipses\u201d. In Proc. 6th International Conference in Central Europe on Computer Graphics and Visualization. WSCG (Vol. 98, pp. 125-132).   \n  \npredict_xy(t, params=None) [source]\n \nPredict x- and y-coordinates using the estimated model.  Parameters \n \ntarray \n\nAngles in circle in radians. Angles start to count from positive x-axis to positive y-axis in a right-handed system.  \nparams(5, ) array, optional \n\nOptional custom parameter set.    Returns \n \nxy(\u2026, 2) array \n\nPredicted x- and y-coordinates.     \n  \nresiduals(data) [source]\n \nDetermine residuals of data to model. For each point the shortest distance to the ellipse is returned.  Parameters \n \ndata(N, 2) array \n\nN points with (x, y) coordinates, respectively.    Returns \n \nresiduals(N, ) array \n\nResidual for each data point.     \n \n"}, {"name": "measure.EllipseModel.estimate()", "path": "api/skimage.measure#skimage.measure.EllipseModel.estimate", "type": "measure", "text": " \nestimate(data) [source]\n \nEstimate circle model from data using total least squares.  Parameters \n \ndata(N, 2) array \n\nN points with (x, y) coordinates, respectively.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     References  \n1  \nHalir, R.; Flusser, J. \u201cNumerically stable direct least squares fitting of ellipses\u201d. In Proc. 6th International Conference in Central Europe on Computer Graphics and Visualization. WSCG (Vol. 98, pp. 125-132).   \n"}, {"name": "measure.EllipseModel.predict_xy()", "path": "api/skimage.measure#skimage.measure.EllipseModel.predict_xy", "type": "measure", "text": " \npredict_xy(t, params=None) [source]\n \nPredict x- and y-coordinates using the estimated model.  Parameters \n \ntarray \n\nAngles in circle in radians. Angles start to count from positive x-axis to positive y-axis in a right-handed system.  \nparams(5, ) array, optional \n\nOptional custom parameter set.    Returns \n \nxy(\u2026, 2) array \n\nPredicted x- and y-coordinates.     \n"}, {"name": "measure.EllipseModel.residuals()", "path": "api/skimage.measure#skimage.measure.EllipseModel.residuals", "type": "measure", "text": " \nresiduals(data) [source]\n \nDetermine residuals of data to model. For each point the shortest distance to the ellipse is returned.  Parameters \n \ndata(N, 2) array \n\nN points with (x, y) coordinates, respectively.    Returns \n \nresiduals(N, ) array \n\nResidual for each data point.     \n"}, {"name": "measure.EllipseModel.__init__()", "path": "api/skimage.measure#skimage.measure.EllipseModel.__init__", "type": "measure", "text": " \n__init__() [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "measure.euler_number()", "path": "api/skimage.measure#skimage.measure.euler_number", "type": "measure", "text": " \nskimage.measure.euler_number(image, connectivity=None) [source]\n \nCalculate the Euler characteristic in binary image. For 2D objects, the Euler number is the number of objects minus the number of holes. For 3D objects, the Euler number is obtained as the number of objects plus the number of holes, minus the number of tunnels, or loops.  Parameters \n image: (N, M) ndarray or (N, M, D) ndarray.\n\n2D or 3D images. If image is not binary, all values strictly greater than zero are considered as the object.  \nconnectivityint, optional \n\nMaximum number of orthogonal hops to consider a pixel/voxel as a neighbor. Accepted values are ranging from 1 to input.ndim. If None, a full connectivity of input.ndim is used. 4 or 8 neighborhoods are defined for 2D images (connectivity 1 and 2, respectively). 6 or 26 neighborhoods are defined for 3D images, (connectivity 1 and 3, respectively). Connectivity 2 is not defined.    Returns \n \neuler_numberint \n\nEuler characteristic of the set of all objects in the image.     Notes The Euler characteristic is an integer number that describes the topology of the set of all objects in the input image. If object is 4-connected, then background is 8-connected, and conversely. The computation of the Euler characteristic is based on an integral geometry formula in discretized space. In practice, a neighbourhood configuration is constructed, and a LUT is applied for each configuration. The coefficients used are the ones of Ohser et al. It can be useful to compute the Euler characteristic for several connectivities. A large relative difference between results for different connectivities suggests that the image resolution (with respect to the size of objects and holes) is too low. References  \n1  \nS. Rivollier. Analyse d\u2019image geometrique et morphometrique par diagrammes de forme et voisinages adaptatifs generaux. PhD thesis, 2010. Ecole Nationale Superieure des Mines de Saint-Etienne. https://tel.archives-ouvertes.fr/tel-00560838  \n2  \nOhser J., Nagel W., Schladitz K. (2002) The Euler Number of Discretized Sets - On the Choice of Adjacency in Homogeneous Lattices. In: Mecke K., Stoyan D. (eds) Morphology of Condensed Matter. Lecture Notes in Physics, vol 600. Springer, Berlin, Heidelberg.   Examples >>> import numpy as np\n>>> SAMPLE = np.zeros((100,100,100));\n>>> SAMPLE[40:60, 40:60, 40:60]=1\n>>> euler_number(SAMPLE) \n1...\n>>> SAMPLE[45:55,45:55,45:55] = 0;\n>>> euler_number(SAMPLE) \n2...\n>>> SAMPLE = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n...                    [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n...                    [1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0],\n...                    [0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1],\n...                    [0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]])\n>>> euler_number(SAMPLE)  # doctest:\n0\n>>> euler_number(SAMPLE, connectivity=1)  # doctest:\n2\n \n"}, {"name": "measure.find_contours()", "path": "api/skimage.measure#skimage.measure.find_contours", "type": "measure", "text": " \nskimage.measure.find_contours(image, level=None, fully_connected='low', positive_orientation='low', *, mask=None) [source]\n \nFind iso-valued contours in a 2D array for a given level value. Uses the \u201cmarching squares\u201d method to compute a the iso-valued contours of the input 2D array for a particular level value. Array values are linearly interpolated to provide better precision for the output contours.  Parameters \n \nimage2D ndarray of double \n\nInput image in which to find contours.  \nlevelfloat, optional \n\nValue along which to find contours in the array. By default, the level is set to (max(image) + min(image)) / 2  Changed in version 0.18: This parameter is now optional.   \nfully_connectedstr, {\u2018low\u2019, \u2018high\u2019} \n\nIndicates whether array elements below the given level value are to be considered fully-connected (and hence elements above the value will only be face connected), or vice-versa. (See notes below for details.)  \npositive_orientationstr, {\u2018low\u2019, \u2018high\u2019} \n\nIndicates whether the output contours will produce positively-oriented polygons around islands of low- or high-valued elements. If \u2018low\u2019 then contours will wind counter- clockwise around elements below the iso-value. Alternately, this means that low-valued elements are always on the left of the contour. (See below for details.)  \nmask2D ndarray of bool, or None \n\nA boolean mask, True where we want to draw contours. Note that NaN values are always excluded from the considered region (mask is set to False wherever array is NaN).    Returns \n \ncontourslist of (n,2)-ndarrays \n\nEach contour is an ndarray of shape (n, 2), consisting of n (row, column) coordinates along the contour.      See also  \nskimage.measure.marching_cubes\n\n  Notes The marching squares algorithm is a special case of the marching cubes algorithm [1]. A simple explanation is available here: http://users.polytech.unice.fr/~lingrand/MarchingCubes/algo.html There is a single ambiguous case in the marching squares algorithm: when a given 2 x 2-element square has two high-valued and two low-valued elements, each pair diagonally adjacent. (Where high- and low-valued is with respect to the contour value sought.) In this case, either the high-valued elements can be \u2018connected together\u2019 via a thin isthmus that separates the low-valued elements, or vice-versa. When elements are connected together across a diagonal, they are considered \u2018fully connected\u2019 (also known as \u2018face+vertex-connected\u2019 or \u20188-connected\u2019). Only high-valued or low-valued elements can be fully-connected, the other set will be considered as \u2018face-connected\u2019 or \u20184-connected\u2019. By default, low-valued elements are considered fully-connected; this can be altered with the \u2018fully_connected\u2019 parameter. Output contours are not guaranteed to be closed: contours which intersect the array edge or a masked-off region (either where mask is False or where array is NaN) will be left open. All other contours will be closed. (The closed-ness of a contours can be tested by checking whether the beginning point is the same as the end point.) Contours are oriented. By default, array values lower than the contour value are to the left of the contour and values greater than the contour value are to the right. This means that contours will wind counter-clockwise (i.e. in \u2018positive orientation\u2019) around islands of low-valued pixels. This behavior can be altered with the \u2018positive_orientation\u2019 parameter. The order of the contours in the output list is determined by the position of the smallest x,y (in lexicographical order) coordinate in the contour. This is a side-effect of how the input array is traversed, but can be relied upon.  Warning Array coordinates/values are assumed to refer to the center of the array element. Take a simple example input: [0, 1]. The interpolated position of 0.5 in this array is midway between the 0-element (at x=0) and the 1-element (at x=1), and thus would fall at x=0.5.  This means that to find reasonable contours, it is best to find contours midway between the expected \u201clight\u201d and \u201cdark\u201d values. In particular, given a binarized array, do not choose to find contours at the low or high value of the array. This will often yield degenerate contours, especially around structures that are a single array element wide. Instead choose a middle value, as above. References  \n1  \nLorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings) 21(4) July 1987, p. 163-170). DOI:10.1145/37401.37422   Examples >>> a = np.zeros((3, 3))\n>>> a[0, 0] = 1\n>>> a\narray([[1., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n>>> find_contours(a, 0.5)\n[array([[0. , 0.5],\n       [0.5, 0. ]])]\n \n"}, {"name": "measure.grid_points_in_poly()", "path": "api/skimage.measure#skimage.measure.grid_points_in_poly", "type": "measure", "text": " \nskimage.measure.grid_points_in_poly(shape, verts) [source]\n \nTest whether points on a specified grid are inside a polygon. For each (r, c) coordinate on a grid, i.e. (0, 0), (0, 1) etc., test whether that point lies inside a polygon.  Parameters \n \nshapetuple (M, N) \n\nShape of the grid.  \nverts(V, 2) array \n\nSpecify the V vertices of the polygon, sorted either clockwise or anti-clockwise. The first point may (but does not need to be) duplicated.    Returns \n \nmask(M, N) ndarray of bool \n\nTrue where the grid falls inside the polygon.      See also  \npoints_in_poly\n\n  \n"}, {"name": "measure.inertia_tensor()", "path": "api/skimage.measure#skimage.measure.inertia_tensor", "type": "measure", "text": " \nskimage.measure.inertia_tensor(image, mu=None) [source]\n \nCompute the inertia tensor of the input image.  Parameters \n \nimagearray \n\nThe input image.  \nmuarray, optional \n\nThe pre-computed central moments of image. The inertia tensor computation requires the central moments of the image. If an application requires both the central moments and the inertia tensor (for example, skimage.measure.regionprops), then it is more efficient to pre-compute them and pass them to the inertia tensor call.    Returns \n \nTarray, shape (image.ndim, image.ndim) \n\nThe inertia tensor of the input image. \\(T_{i, j}\\) contains the covariance of image intensity along axes \\(i\\) and \\(j\\).     References  \n1  \nhttps://en.wikipedia.org/wiki/Moment_of_inertia#Inertia_tensor  \n2  \nBernd J\u00e4hne. Spatio-Temporal Image Processing: Theory and Scientific Applications. (Chapter 8: Tensor Methods) Springer, 1993.   \n"}, {"name": "measure.inertia_tensor_eigvals()", "path": "api/skimage.measure#skimage.measure.inertia_tensor_eigvals", "type": "measure", "text": " \nskimage.measure.inertia_tensor_eigvals(image, mu=None, T=None) [source]\n \nCompute the eigenvalues of the inertia tensor of the image. The inertia tensor measures covariance of the image intensity along the image axes. (See inertia_tensor.) The relative magnitude of the eigenvalues of the tensor is thus a measure of the elongation of a (bright) object in the image.  Parameters \n \nimagearray \n\nThe input image.  \nmuarray, optional \n\nThe pre-computed central moments of image.  \nTarray, shape (image.ndim, image.ndim) \n\nThe pre-computed inertia tensor. If T is given, mu and image are ignored.    Returns \n \neigvalslist of float, length image.ndim \n\nThe eigenvalues of the inertia tensor of image, in descending order.     Notes Computing the eigenvalues requires the inertia tensor of the input image. This is much faster if the central moments (mu) are provided, or, alternatively, one can provide the inertia tensor (T) directly. \n"}, {"name": "measure.label()", "path": "api/skimage.measure#skimage.measure.label", "type": "measure", "text": " \nskimage.measure.label(input, background=None, return_num=False, connectivity=None) [source]\n \nLabel connected regions of an integer array. Two pixels are connected when they are neighbors and have the same value. In 2D, they can be neighbors either in a 1- or 2-connected sense. The value refers to the maximum number of orthogonal hops to consider a pixel/voxel a neighbor: 1-connectivity     2-connectivity     diagonal connection close-up\n\n     [ ]           [ ]  [ ]  [ ]             [ ]\n      |               \\  |  /                 |  <- hop 2\n[ ]--[x]--[ ]      [ ]--[x]--[ ]        [x]--[ ]\n      |               /  |  \\             hop 1\n     [ ]           [ ]  [ ]  [ ]\n  Parameters \n \ninputndarray of dtype int \n\nImage to label.  \nbackgroundint, optional \n\nConsider all pixels with this value as background pixels, and label them as 0. By default, 0-valued pixels are considered as background pixels.  \nreturn_numbool, optional \n\nWhether to return the number of assigned labels.  \nconnectivityint, optional \n\nMaximum number of orthogonal hops to consider a pixel/voxel as a neighbor. Accepted values are ranging from 1 to input.ndim. If None, a full connectivity of input.ndim is used.    Returns \n \nlabelsndarray of dtype int \n\nLabeled array, where all connected regions are assigned the same integer value.  \nnumint, optional \n\nNumber of labels, which equals the maximum label index and is only returned if return_num is True.      See also  \nregionprops\n\n\nregionprops_table\n\n  References  \n1  \nChristophe Fiorio and Jens Gustedt, \u201cTwo linear time Union-Find strategies for image processing\u201d, Theoretical Computer Science 154 (1996), pp. 165-181.  \n2  \nKensheng Wu, Ekow Otoo and Arie Shoshani, \u201cOptimizing connected component labeling algorithms\u201d, Paper LBNL-56864, 2005, Lawrence Berkeley National Laboratory (University of California), http://repositories.cdlib.org/lbnl/LBNL-56864   Examples >>> import numpy as np\n>>> x = np.eye(3).astype(int)\n>>> print(x)\n[[1 0 0]\n [0 1 0]\n [0 0 1]]\n>>> print(label(x, connectivity=1))\n[[1 0 0]\n [0 2 0]\n [0 0 3]]\n>>> print(label(x, connectivity=2))\n[[1 0 0]\n [0 1 0]\n [0 0 1]]\n>>> print(label(x, background=-1))\n[[1 2 2]\n [2 1 2]\n [2 2 1]]\n>>> x = np.array([[1, 0, 0],\n...               [1, 1, 5],\n...               [0, 0, 0]])\n>>> print(label(x))\n[[1 0 0]\n [1 1 2]\n [0 0 0]]\n \n"}, {"name": "measure.LineModelND", "path": "api/skimage.measure#skimage.measure.LineModelND", "type": "measure", "text": " \nclass skimage.measure.LineModelND [source]\n \nBases: skimage.measure.fit.BaseModel Total least squares estimator for N-dimensional lines. In contrast to ordinary least squares line estimation, this estimator minimizes the orthogonal distances of points to the estimated line. Lines are defined by a point (origin) and a unit vector (direction) according to the following vector equation: X = origin + lambda * direction\n Examples >>> x = np.linspace(1, 2, 25)\n>>> y = 1.5 * x + 3\n>>> lm = LineModelND()\n>>> lm.estimate(np.stack([x, y], axis=-1))\nTrue\n>>> tuple(np.round(lm.params, 5))\n(array([1.5 , 5.25]), array([0.5547 , 0.83205]))\n>>> res = lm.residuals(np.stack([x, y], axis=-1))\n>>> np.abs(np.round(res, 9))\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0.])\n>>> np.round(lm.predict_y(x[:5]), 3)\narray([4.5  , 4.562, 4.625, 4.688, 4.75 ])\n>>> np.round(lm.predict_x(y[:5]), 3)\narray([1.   , 1.042, 1.083, 1.125, 1.167])\n  Attributes \n \nparamstuple \n\nLine model parameters in the following order origin, direction.      \n__init__() [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(data) [source]\n \nEstimate line model from data. This minimizes the sum of shortest (orthogonal) distances from the given data points to the estimated line.  Parameters \n \ndata(N, dim) array \n\nN points in a space of dimensionality dim >= 2.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n  \npredict(x, axis=0, params=None) [source]\n \nPredict intersection of the estimated line model with a hyperplane orthogonal to a given axis.  Parameters \n \nx(n, 1) array \n\nCoordinates along an axis.  \naxisint \n\nAxis orthogonal to the hyperplane intersecting the line.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \ndata(n, m) array \n\nPredicted coordinates.    Raises \n ValueError\n\nIf the line is parallel to the given axis.     \n  \npredict_x(y, params=None) [source]\n \nPredict x-coordinates for 2D lines using the estimated model. Alias for: predict(y, axis=1)[:, 0]\n  Parameters \n \nyarray \n\ny-coordinates.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \nxarray \n\nPredicted x-coordinates.     \n  \npredict_y(x, params=None) [source]\n \nPredict y-coordinates for 2D lines using the estimated model. Alias for: predict(x, axis=0)[:, 1]\n  Parameters \n \nxarray \n\nx-coordinates.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \nyarray \n\nPredicted y-coordinates.     \n  \nresiduals(data, params=None) [source]\n \nDetermine residuals of data to model. For each point, the shortest (orthogonal) distance to the line is returned. It is obtained by projecting the data onto the line.  Parameters \n \ndata(N, dim) array \n\nN points in a space of dimension dim.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \nresiduals(N, ) array \n\nResidual for each data point.     \n \n"}, {"name": "measure.LineModelND.estimate()", "path": "api/skimage.measure#skimage.measure.LineModelND.estimate", "type": "measure", "text": " \nestimate(data) [source]\n \nEstimate line model from data. This minimizes the sum of shortest (orthogonal) distances from the given data points to the estimated line.  Parameters \n \ndata(N, dim) array \n\nN points in a space of dimensionality dim >= 2.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n"}, {"name": "measure.LineModelND.predict()", "path": "api/skimage.measure#skimage.measure.LineModelND.predict", "type": "measure", "text": " \npredict(x, axis=0, params=None) [source]\n \nPredict intersection of the estimated line model with a hyperplane orthogonal to a given axis.  Parameters \n \nx(n, 1) array \n\nCoordinates along an axis.  \naxisint \n\nAxis orthogonal to the hyperplane intersecting the line.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \ndata(n, m) array \n\nPredicted coordinates.    Raises \n ValueError\n\nIf the line is parallel to the given axis.     \n"}, {"name": "measure.LineModelND.predict_x()", "path": "api/skimage.measure#skimage.measure.LineModelND.predict_x", "type": "measure", "text": " \npredict_x(y, params=None) [source]\n \nPredict x-coordinates for 2D lines using the estimated model. Alias for: predict(y, axis=1)[:, 0]\n  Parameters \n \nyarray \n\ny-coordinates.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \nxarray \n\nPredicted x-coordinates.     \n"}, {"name": "measure.LineModelND.predict_y()", "path": "api/skimage.measure#skimage.measure.LineModelND.predict_y", "type": "measure", "text": " \npredict_y(x, params=None) [source]\n \nPredict y-coordinates for 2D lines using the estimated model. Alias for: predict(x, axis=0)[:, 1]\n  Parameters \n \nxarray \n\nx-coordinates.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \nyarray \n\nPredicted y-coordinates.     \n"}, {"name": "measure.LineModelND.residuals()", "path": "api/skimage.measure#skimage.measure.LineModelND.residuals", "type": "measure", "text": " \nresiduals(data, params=None) [source]\n \nDetermine residuals of data to model. For each point, the shortest (orthogonal) distance to the line is returned. It is obtained by projecting the data onto the line.  Parameters \n \ndata(N, dim) array \n\nN points in a space of dimension dim.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \nresiduals(N, ) array \n\nResidual for each data point.     \n"}, {"name": "measure.LineModelND.__init__()", "path": "api/skimage.measure#skimage.measure.LineModelND.__init__", "type": "measure", "text": " \n__init__() [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "measure.marching_cubes()", "path": "api/skimage.measure#skimage.measure.marching_cubes", "type": "measure", "text": " \nskimage.measure.marching_cubes(volume, level=None, *, spacing=(1.0, 1.0, 1.0), gradient_direction='descent', step_size=1, allow_degenerate=True, method='lewiner', mask=None) [source]\n \nMarching cubes algorithm to find surfaces in 3d volumetric data. In contrast with Lorensen et al. approach [2], Lewiner et al. algorithm is faster, resolves ambiguities, and guarantees topologically correct results. Therefore, this algorithm generally a better choice.  Parameters \n \nvolume(M, N, P) array \n\nInput data volume to find isosurfaces. Will internally be converted to float32 if necessary.  \nlevelfloat, optional \n\nContour value to search for isosurfaces in volume. If not given or None, the average of the min and max of vol is used.  \nspacinglength-3 tuple of floats, optional \n\nVoxel spacing in spatial dimensions corresponding to numpy array indexing dimensions (M, N, P) as in volume.  \ngradient_directionstring, optional \n\nControls if the mesh was generated from an isosurface with gradient descent toward objects of interest (the default), or the opposite, considering the left-hand rule. The two options are: * descent : Object was greater than exterior * ascent : Exterior was greater than object  \nstep_sizeint, optional \n\nStep size in voxels. Default 1. Larger steps yield faster but coarser results. The result will always be topologically correct though.  \nallow_degeneratebool, optional \n\nWhether to allow degenerate (i.e. zero-area) triangles in the end-result. Default True. If False, degenerate triangles are removed, at the cost of making the algorithm slower.  method: str, optional\n\nOne of \u2018lewiner\u2019, \u2018lorensen\u2019 or \u2018_lorensen\u2019. Specify witch of Lewiner et al. or Lorensen et al. method will be used. The \u2018_lorensen\u2019 flag correspond to an old implementation that will be deprecated in version 0.19.  \nmask(M, N, P) array, optional \n\nBoolean array. The marching cube algorithm will be computed only on True elements. This will save computational time when interfaces are located within certain region of the volume M, N, P-e.g. the top half of the cube-and also allow to compute finite surfaces-i.e. open surfaces that do not end at the border of the cube.    Returns \n \nverts(V, 3) array \n\nSpatial coordinates for V unique mesh vertices. Coordinate order matches input volume (M, N, P). If allow_degenerate is set to True, then the presence of degenerate triangles in the mesh can make this array have duplicate vertices.  \nfaces(F, 3) array \n\nDefine triangular faces via referencing vertex indices from verts. This algorithm specifically outputs triangles, so each face has exactly three indices.  \nnormals(V, 3) array \n\nThe normal direction at each vertex, as calculated from the data.  \nvalues(V, ) array \n\nGives a measure for the maximum value of the data in the local region near each vertex. This can be used by visualization tools to apply a colormap to the mesh.      See also  \nskimage.measure.mesh_surface_area\n\n\nskimage.measure.find_contours\n\n  Notes The algorithm [1] is an improved version of Chernyaev\u2019s Marching Cubes 33 algorithm. It is an efficient algorithm that relies on heavy use of lookup tables to handle the many different cases, keeping the algorithm relatively easy. This implementation is written in Cython, ported from Lewiner\u2019s C++ implementation. To quantify the area of an isosurface generated by this algorithm, pass verts and faces to skimage.measure.mesh_surface_area. Regarding visualization of algorithm output, to contour a volume named myvolume about the level 0.0, using the mayavi package: >>>\n>> from mayavi import mlab\n>> verts, faces, _, _ = marching_cubes(myvolume, 0.0)\n>> mlab.triangular_mesh([vert[0] for vert in verts],\n                        [vert[1] for vert in verts],\n                        [vert[2] for vert in verts],\n                        faces)\n>> mlab.show()\n Similarly using the visvis package: >>>\n>> import visvis as vv\n>> verts, faces, normals, values = marching_cubes(myvolume, 0.0)\n>> vv.mesh(np.fliplr(verts), faces, normals, values)\n>> vv.use().Run()\n To reduce the number of triangles in the mesh for better performance, see this example using the mayavi package. References  \n1  \nThomas Lewiner, Helio Lopes, Antonio Wilson Vieira and Geovan Tavares. Efficient implementation of Marching Cubes\u2019 cases with topological guarantees. Journal of Graphics Tools 8(2) pp. 1-15 (december 2003). DOI:10.1080/10867651.2003.10487582  \n2  \nLorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings) 21(4) July 1987, p. 163-170). DOI:10.1145/37401.37422   \n"}, {"name": "measure.marching_cubes_classic()", "path": "api/skimage.measure#skimage.measure.marching_cubes_classic", "type": "measure", "text": " \nskimage.measure.marching_cubes_classic(volume, level=None, spacing=(1.0, 1.0, 1.0), gradient_direction='descent') [source]\n \nClassic marching cubes algorithm to find surfaces in 3d volumetric data. Note that the marching_cubes() algorithm is recommended over this algorithm, because it\u2019s faster and produces better results.  Parameters \n \nvolume(M, N, P) array of doubles \n\nInput data volume to find isosurfaces. Will be cast to np.float64.  \nlevelfloat \n\nContour value to search for isosurfaces in volume. If not given or None, the average of the min and max of vol is used.  \nspacinglength-3 tuple of floats \n\nVoxel spacing in spatial dimensions corresponding to numpy array indexing dimensions (M, N, P) as in volume.  \ngradient_directionstring \n\nControls if the mesh was generated from an isosurface with gradient descent toward objects of interest (the default), or the opposite. The two options are: * descent : Object was greater than exterior * ascent : Exterior was greater than object    Returns \n \nverts(V, 3) array \n\nSpatial coordinates for V unique mesh vertices. Coordinate order matches input volume (M, N, P). If allow_degenerate is set to True, then the presence of degenerate triangles in the mesh can make this array have duplicate vertices.  \nfaces(F, 3) array \n\nDefine triangular faces via referencing vertex indices from verts. This algorithm specifically outputs triangles, so each face has exactly three indices.      See also  \nskimage.measure.marching_cubes\n\n\nskimage.measure.mesh_surface_area\n\n  Notes The marching cubes algorithm is implemented as described in [1]. A simple explanation is available here: http://users.polytech.unice.fr/~lingrand/MarchingCubes/algo.html\n There are several known ambiguous cases in the marching cubes algorithm. Using point labeling as in [1], Figure 4, as shown:     v8 ------ v7\n   / |       / |        y\n  /  |      /  |        ^  z\nv4 ------ v3   |        | /\n |  v5 ----|- v6        |/          (note: NOT right handed!)\n |  /      |  /          ----> x\n | /       | /\nv1 ------ v2\n Most notably, if v4, v8, v2, and v6 are all >= level (or any generalization of this case) two parallel planes are generated by this algorithm, separating v4 and v8 from v2 and v6. An equally valid interpretation would be a single connected thin surface enclosing all four points. This is the best known ambiguity, though there are others. This algorithm does not attempt to resolve such ambiguities; it is a naive implementation of marching cubes as in [1], but may be a good beginning for work with more recent techniques (Dual Marching Cubes, Extended Marching Cubes, Cubic Marching Squares, etc.). Because of interactions between neighboring cubes, the isosurface(s) generated by this algorithm are NOT guaranteed to be closed, particularly for complicated contours. Furthermore, this algorithm does not guarantee a single contour will be returned. Indeed, ALL isosurfaces which cross level will be found, regardless of connectivity. The output is a triangular mesh consisting of a set of unique vertices and connecting triangles. The order of these vertices and triangles in the output list is determined by the position of the smallest x,y,z (in lexicographical order) coordinate in the contour. This is a side-effect of how the input array is traversed, but can be relied upon. The generated mesh guarantees coherent orientation as of version 0.12. To quantify the area of an isosurface generated by this algorithm, pass outputs directly into skimage.measure.mesh_surface_area. References  \n1(1,2,3)  \nLorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings) 21(4) July 1987, p. 163-170). DOI:10.1145/37401.37422   \n"}, {"name": "measure.marching_cubes_lewiner()", "path": "api/skimage.measure#skimage.measure.marching_cubes_lewiner", "type": "measure", "text": " \nskimage.measure.marching_cubes_lewiner(volume, level=None, spacing=(1.0, 1.0, 1.0), gradient_direction='descent', step_size=1, allow_degenerate=True, use_classic=False, mask=None) [source]\n \nLewiner marching cubes algorithm to find surfaces in 3d volumetric data. In contrast to marching_cubes_classic(), this algorithm is faster, resolves ambiguities, and guarantees topologically correct results. Therefore, this algorithm generally a better choice, unless there is a specific need for the classic algorithm.  Parameters \n \nvolume(M, N, P) array \n\nInput data volume to find isosurfaces. Will internally be converted to float32 if necessary.  \nlevelfloat \n\nContour value to search for isosurfaces in volume. If not given or None, the average of the min and max of vol is used.  \nspacinglength-3 tuple of floats \n\nVoxel spacing in spatial dimensions corresponding to numpy array indexing dimensions (M, N, P) as in volume.  \ngradient_directionstring \n\nControls if the mesh was generated from an isosurface with gradient descent toward objects of interest (the default), or the opposite, considering the left-hand rule. The two options are: * descent : Object was greater than exterior * ascent : Exterior was greater than object  \nstep_sizeint \n\nStep size in voxels. Default 1. Larger steps yield faster but coarser results. The result will always be topologically correct though.  \nallow_degeneratebool \n\nWhether to allow degenerate (i.e. zero-area) triangles in the end-result. Default True. If False, degenerate triangles are removed, at the cost of making the algorithm slower.  \nuse_classicbool \n\nIf given and True, the classic marching cubes by Lorensen (1987) is used. This option is included for reference purposes. Note that this algorithm has ambiguities and is not guaranteed to produce a topologically correct result. The results with using this option are not generally the same as the marching_cubes_classic() function.  \nmask(M, N, P) array \n\nBoolean array. The marching cube algorithm will be computed only on True elements. This will save computational time when interfaces are located within certain region of the volume M, N, P-e.g. the top half of the cube-and also allow to compute finite surfaces-i.e. open surfaces that do not end at the border of the cube.    Returns \n \nverts(V, 3) array \n\nSpatial coordinates for V unique mesh vertices. Coordinate order matches input volume (M, N, P). If allow_degenerate is set to True, then the presence of degenerate triangles in the mesh can make this array have duplicate vertices.  \nfaces(F, 3) array \n\nDefine triangular faces via referencing vertex indices from verts. This algorithm specifically outputs triangles, so each face has exactly three indices.  \nnormals(V, 3) array \n\nThe normal direction at each vertex, as calculated from the data.  \nvalues(V, ) array \n\nGives a measure for the maximum value of the data in the local region near each vertex. This can be used by visualization tools to apply a colormap to the mesh.      See also  \nskimage.measure.marching_cubes\n\n\nskimage.measure.mesh_surface_area\n\n  Notes The algorithm [1] is an improved version of Chernyaev\u2019s Marching Cubes 33 algorithm. It is an efficient algorithm that relies on heavy use of lookup tables to handle the many different cases, keeping the algorithm relatively easy. This implementation is written in Cython, ported from Lewiner\u2019s C++ implementation. To quantify the area of an isosurface generated by this algorithm, pass verts and faces to skimage.measure.mesh_surface_area. Regarding visualization of algorithm output, to contour a volume named myvolume about the level 0.0, using the mayavi package: >>> from mayavi import mlab \n>>> verts, faces, normals, values = marching_cubes_lewiner(myvolume, 0.0) \n>>> mlab.triangular_mesh([vert[0] for vert in verts],\n...                      [vert[1] for vert in verts],\n...                      [vert[2] for vert in verts],\n...                      faces) \n>>> mlab.show() \n Similarly using the visvis package: >>> import visvis as vv \n>>> verts, faces, normals, values = marching_cubes_lewiner(myvolume, 0.0) \n>>> vv.mesh(np.fliplr(verts), faces, normals, values) \n>>> vv.use().Run() \n References  \n1  \nThomas Lewiner, Helio Lopes, Antonio Wilson Vieira and Geovan Tavares. Efficient implementation of Marching Cubes\u2019 cases with topological guarantees. Journal of Graphics Tools 8(2) pp. 1-15 (december 2003). DOI:10.1080/10867651.2003.10487582   \n"}, {"name": "measure.mesh_surface_area()", "path": "api/skimage.measure#skimage.measure.mesh_surface_area", "type": "measure", "text": " \nskimage.measure.mesh_surface_area(verts, faces) [source]\n \nCompute surface area, given vertices & triangular faces  Parameters \n \nverts(V, 3) array of floats \n\nArray containing (x, y, z) coordinates for V unique mesh vertices.  \nfaces(F, 3) array of ints \n\nList of length-3 lists of integers, referencing vertex coordinates as provided in verts    Returns \n \nareafloat \n\nSurface area of mesh. Units now [coordinate units] ** 2.      See also  \nskimage.measure.marching_cubes\n\n\nskimage.measure.marching_cubes_classic\n\n  Notes The arguments expected by this function are the first two outputs from skimage.measure.marching_cubes. For unit correct output, ensure correct spacing was passed to skimage.measure.marching_cubes. This algorithm works properly only if the faces provided are all triangles. \n"}, {"name": "measure.moments()", "path": "api/skimage.measure#skimage.measure.moments", "type": "measure", "text": " \nskimage.measure.moments(image, order=3) [source]\n \nCalculate all raw image moments up to a certain order.  The following properties can be calculated from raw image moments:\n\n Area as: M[0, 0]. Centroid as: {M[1, 0] / M[0, 0], M[0, 1] / M[0, 0]}.    Note that raw moments are neither translation, scale nor rotation invariant.  Parameters \n \nimagenD double or uint8 array \n\nRasterized shape as image.  \norderint, optional \n\nMaximum order of moments. Default is 3.    Returns \n \nm(order + 1, order + 1) array \n\nRaw image moments.     References  \n1  \nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.  \n2  \nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.  \n3  \nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.  \n4  \nhttps://en.wikipedia.org/wiki/Image_moment   Examples >>> image = np.zeros((20, 20), dtype=np.double)\n>>> image[13:17, 13:17] = 1\n>>> M = moments(image)\n>>> centroid = (M[1, 0] / M[0, 0], M[0, 1] / M[0, 0])\n>>> centroid\n(14.5, 14.5)\n \n"}, {"name": "measure.moments_central()", "path": "api/skimage.measure#skimage.measure.moments_central", "type": "measure", "text": " \nskimage.measure.moments_central(image, center=None, order=3, **kwargs) [source]\n \nCalculate all central image moments up to a certain order. The center coordinates (cr, cc) can be calculated from the raw moments as: {M[1, 0] / M[0, 0], M[0, 1] / M[0, 0]}. Note that central moments are translation invariant but not scale and rotation invariant.  Parameters \n \nimagenD double or uint8 array \n\nRasterized shape as image.  \ncentertuple of float, optional \n\nCoordinates of the image centroid. This will be computed if it is not provided.  \norderint, optional \n\nThe maximum order of moments computed.    Returns \n \nmu(order + 1, order + 1) array \n\nCentral image moments.     References  \n1  \nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.  \n2  \nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.  \n3  \nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.  \n4  \nhttps://en.wikipedia.org/wiki/Image_moment   Examples >>> image = np.zeros((20, 20), dtype=np.double)\n>>> image[13:17, 13:17] = 1\n>>> M = moments(image)\n>>> centroid = (M[1, 0] / M[0, 0], M[0, 1] / M[0, 0])\n>>> moments_central(image, centroid)\narray([[16.,  0., 20.,  0.],\n       [ 0.,  0.,  0.,  0.],\n       [20.,  0., 25.,  0.],\n       [ 0.,  0.,  0.,  0.]])\n \n"}, {"name": "measure.moments_coords()", "path": "api/skimage.measure#skimage.measure.moments_coords", "type": "measure", "text": " \nskimage.measure.moments_coords(coords, order=3) [source]\n \nCalculate all raw image moments up to a certain order.  The following properties can be calculated from raw image moments:\n\n Area as: M[0, 0]. Centroid as: {M[1, 0] / M[0, 0], M[0, 1] / M[0, 0]}.    Note that raw moments are neither translation, scale nor rotation invariant.  Parameters \n \ncoords(N, D) double or uint8 array \n\nArray of N points that describe an image of D dimensionality in Cartesian space.  \norderint, optional \n\nMaximum order of moments. Default is 3.    Returns \n \nM(order + 1, order + 1, \u2026) array \n\nRaw image moments. (D dimensions)     References  \n1  \nJohannes Kilian. Simple Image Analysis By Moments. Durham University, version 0.2, Durham, 2001.   Examples >>> coords = np.array([[row, col]\n...                    for row in range(13, 17)\n...                    for col in range(14, 18)], dtype=np.double)\n>>> M = moments_coords(coords)\n>>> centroid = (M[1, 0] / M[0, 0], M[0, 1] / M[0, 0])\n>>> centroid\n(14.5, 15.5)\n \n"}, {"name": "measure.moments_coords_central()", "path": "api/skimage.measure#skimage.measure.moments_coords_central", "type": "measure", "text": " \nskimage.measure.moments_coords_central(coords, center=None, order=3) [source]\n \nCalculate all central image moments up to a certain order.  The following properties can be calculated from raw image moments:\n\n Area as: M[0, 0]. Centroid as: {M[1, 0] / M[0, 0], M[0, 1] / M[0, 0]}.    Note that raw moments are neither translation, scale nor rotation invariant.  Parameters \n \ncoords(N, D) double or uint8 array \n\nArray of N points that describe an image of D dimensionality in Cartesian space. A tuple of coordinates as returned by np.nonzero is also accepted as input.  \ncentertuple of float, optional \n\nCoordinates of the image centroid. This will be computed if it is not provided.  \norderint, optional \n\nMaximum order of moments. Default is 3.    Returns \n \nMc(order + 1, order + 1, \u2026) array \n\nCentral image moments. (D dimensions)     References  \n1  \nJohannes Kilian. Simple Image Analysis By Moments. Durham University, version 0.2, Durham, 2001.   Examples >>> coords = np.array([[row, col]\n...                    for row in range(13, 17)\n...                    for col in range(14, 18)])\n>>> moments_coords_central(coords)\narray([[16.,  0., 20.,  0.],\n       [ 0.,  0.,  0.,  0.],\n       [20.,  0., 25.,  0.],\n       [ 0.,  0.,  0.,  0.]])\n As seen above, for symmetric objects, odd-order moments (columns 1 and 3, rows 1 and 3) are zero when centered on the centroid, or center of mass, of the object (the default). If we break the symmetry by adding a new point, this no longer holds: >>> coords2 = np.concatenate((coords, [[17, 17]]), axis=0)\n>>> np.round(moments_coords_central(coords2),\n...          decimals=2)  \narray([[17.  ,  0.  , 22.12, -2.49],\n       [ 0.  ,  3.53,  1.73,  7.4 ],\n       [25.88,  6.02, 36.63,  8.83],\n       [ 4.15, 19.17, 14.8 , 39.6 ]])\n Image moments and central image moments are equivalent (by definition) when the center is (0, 0): >>> np.allclose(moments_coords(coords),\n...             moments_coords_central(coords, (0, 0)))\nTrue\n \n"}, {"name": "measure.moments_hu()", "path": "api/skimage.measure#skimage.measure.moments_hu", "type": "measure", "text": " \nskimage.measure.moments_hu(nu) [source]\n \nCalculate Hu\u2019s set of image moments (2D-only). Note that this set of moments is proofed to be translation, scale and rotation invariant.  Parameters \n \nnu(M, M) array \n\nNormalized central image moments, where M must be >= 4.    Returns \n \nnu(7,) array \n\nHu\u2019s set of image moments.     References  \n1  \nM. K. Hu, \u201cVisual Pattern Recognition by Moment Invariants\u201d, IRE Trans. Info. Theory, vol. IT-8, pp. 179-187, 1962  \n2  \nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.  \n3  \nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.  \n4  \nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.  \n5  \nhttps://en.wikipedia.org/wiki/Image_moment   Examples >>> image = np.zeros((20, 20), dtype=np.double)\n>>> image[13:17, 13:17] = 0.5\n>>> image[10:12, 10:12] = 1\n>>> mu = moments_central(image)\n>>> nu = moments_normalized(mu)\n>>> moments_hu(nu)\narray([7.45370370e-01, 3.51165981e-01, 1.04049179e-01, 4.06442107e-02,\n       2.64312299e-03, 2.40854582e-02, 4.33680869e-19])\n \n"}, {"name": "measure.moments_normalized()", "path": "api/skimage.measure#skimage.measure.moments_normalized", "type": "measure", "text": " \nskimage.measure.moments_normalized(mu, order=3) [source]\n \nCalculate all normalized central image moments up to a certain order. Note that normalized central moments are translation and scale invariant but not rotation invariant.  Parameters \n \nmu(M,[ \u2026,] M) array \n\nCentral image moments, where M must be greater than or equal to order.  \norderint, optional \n\nMaximum order of moments. Default is 3.    Returns \n \nnu(order + 1,[ \u2026,] order + 1) array \n\nNormalized central image moments.     References  \n1  \nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.  \n2  \nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.  \n3  \nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.  \n4  \nhttps://en.wikipedia.org/wiki/Image_moment   Examples >>> image = np.zeros((20, 20), dtype=np.double)\n>>> image[13:17, 13:17] = 1\n>>> m = moments(image)\n>>> centroid = (m[0, 1] / m[0, 0], m[1, 0] / m[0, 0])\n>>> mu = moments_central(image, centroid)\n>>> moments_normalized(mu)\narray([[       nan,        nan, 0.078125  , 0.        ],\n       [       nan, 0.        , 0.        , 0.        ],\n       [0.078125  , 0.        , 0.00610352, 0.        ],\n       [0.        , 0.        , 0.        , 0.        ]])\n \n"}, {"name": "measure.perimeter()", "path": "api/skimage.measure#skimage.measure.perimeter", "type": "measure", "text": " \nskimage.measure.perimeter(image, neighbourhood=4) [source]\n \nCalculate total perimeter of all objects in binary image.  Parameters \n \nimage(N, M) ndarray \n\n2D binary image.  \nneighbourhood4 or 8, optional \n\nNeighborhood connectivity for border pixel determination. It is used to compute the contour. A higher neighbourhood widens the border on which the perimeter is computed.    Returns \n \nperimeterfloat \n\nTotal perimeter of all objects in binary image.     References  \n1  \nK. Benkrid, D. Crookes. Design and FPGA Implementation of a Perimeter Estimator. The Queen\u2019s University of Belfast. http://www.cs.qub.ac.uk/~d.crookes/webpubs/papers/perimeter.doc   Examples >>> from skimage import data, util\n>>> from skimage.measure import label\n>>> # coins image (binary)\n>>> img_coins = data.coins() > 110\n>>> # total perimeter of all objects in the image\n>>> perimeter(img_coins, neighbourhood=4)  \n7796.867...\n>>> perimeter(img_coins, neighbourhood=8)  \n8806.268...\n \n"}, {"name": "measure.perimeter_crofton()", "path": "api/skimage.measure#skimage.measure.perimeter_crofton", "type": "measure", "text": " \nskimage.measure.perimeter_crofton(image, directions=4) [source]\n \nCalculate total Crofton perimeter of all objects in binary image.  Parameters \n \nimage(N, M) ndarray \n\n2D image. If image is not binary, all values strictly greater than zero are considered as the object.  \ndirections2 or 4, optional \n\nNumber of directions used to approximate the Crofton perimeter. By default, 4 is used: it should be more accurate than 2. Computation time is the same in both cases.    Returns \n \nperimeterfloat \n\nTotal perimeter of all objects in binary image.     Notes This measure is based on Crofton formula [1], which is a measure from integral geometry. It is defined for general curve length evaluation via a double integral along all directions. In a discrete space, 2 or 4 directions give a quite good approximation, 4 being more accurate than 2 for more complex shapes. Similar to perimeter(), this function returns an approximation of the perimeter in continuous space. References  \n1  \nhttps://en.wikipedia.org/wiki/Crofton_formula  \n2  \nS. Rivollier. Analyse d\u2019image geometrique et morphometrique par diagrammes de forme et voisinages adaptatifs generaux. PhD thesis, 2010. Ecole Nationale Superieure des Mines de Saint-Etienne. https://tel.archives-ouvertes.fr/tel-00560838   Examples >>> from skimage import data, util\n>>> from skimage.measure import label\n>>> # coins image (binary)\n>>> img_coins = data.coins() > 110\n>>> # total perimeter of all objects in the image\n>>> perimeter_crofton(img_coins, directions=2)  \n8144.578...\n>>> perimeter_crofton(img_coins, directions=4)  \n7837.077...\n \n"}, {"name": "measure.points_in_poly()", "path": "api/skimage.measure#skimage.measure.points_in_poly", "type": "measure", "text": " \nskimage.measure.points_in_poly(points, verts) [source]\n \nTest whether points lie inside a polygon.  Parameters \n \npoints(N, 2) array \n\nInput points, (x, y).  \nverts(M, 2) array \n\nVertices of the polygon, sorted either clockwise or anti-clockwise. The first point may (but does not need to be) duplicated.    Returns \n \nmask(N,) array of bool \n\nTrue if corresponding point is inside the polygon.      See also  \ngrid_points_in_poly\n\n  \n"}, {"name": "measure.profile_line()", "path": "api/skimage.measure#skimage.measure.profile_line", "type": "measure", "text": " \nskimage.measure.profile_line(image, src, dst, linewidth=1, order=None, mode=None, cval=0.0, *, reduce_func=<function mean>) [source]\n \nReturn the intensity profile of an image measured along a scan line.  Parameters \n \nimagendarray, shape (M, N[, C]) \n\nThe image, either grayscale (2D array) or multichannel (3D array, where the final axis contains the channel information).  \nsrcarray_like, shape (2, ) \n\nThe coordinates of the start point of the scan line.  \ndstarray_like, shape (2, ) \n\nThe coordinates of the end point of the scan line. The destination point is included in the profile, in contrast to standard numpy indexing.  \nlinewidthint, optional \n\nWidth of the scan, perpendicular to the line  \norderint in {0, 1, 2, 3, 4, 5}, optional \n\nThe order of the spline interpolation, default is 0 if image.dtype is bool and 1 otherwise. The order has to be in the range 0-5. See skimage.transform.warp for detail.  \nmode{\u2018constant\u2019, \u2018nearest\u2019, \u2018reflect\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional \n\nHow to compute any values falling outside of the image.  \ncvalfloat, optional \n\nIf mode is \u2018constant\u2019, what constant value to use outside the image.  \nreduce_funccallable, optional \n\nFunction used to calculate the aggregation of pixel values perpendicular to the profile_line direction when linewidth > 1. If set to None the unreduced array will be returned.    Returns \n \nreturn_valuearray \n\nThe intensity profile along the scan line. The length of the profile is the ceil of the computed length of the scan line.     Examples >>> x = np.array([[1, 1, 1, 2, 2, 2]])\n>>> img = np.vstack([np.zeros_like(x), x, x, x, np.zeros_like(x)])\n>>> img\narray([[0, 0, 0, 0, 0, 0],\n       [1, 1, 1, 2, 2, 2],\n       [1, 1, 1, 2, 2, 2],\n       [1, 1, 1, 2, 2, 2],\n       [0, 0, 0, 0, 0, 0]])\n>>> profile_line(img, (2, 1), (2, 4))\narray([1., 1., 2., 2.])\n>>> profile_line(img, (1, 0), (1, 6), cval=4)\narray([1., 1., 1., 2., 2., 2., 4.])\n The destination point is included in the profile, in contrast to standard numpy indexing. For example: >>> profile_line(img, (1, 0), (1, 6))  # The final point is out of bounds\narray([1., 1., 1., 2., 2., 2., 0.])\n>>> profile_line(img, (1, 0), (1, 5))  # This accesses the full first row\narray([1., 1., 1., 2., 2., 2.])\n For different reduce_func inputs: >>> profile_line(img, (1, 0), (1, 3), linewidth=3, reduce_func=np.mean)\narray([0.66666667, 0.66666667, 0.66666667, 1.33333333])\n>>> profile_line(img, (1, 0), (1, 3), linewidth=3, reduce_func=np.max)\narray([1, 1, 1, 2])\n>>> profile_line(img, (1, 0), (1, 3), linewidth=3, reduce_func=np.sum)\narray([2, 2, 2, 4])\n The unreduced array will be returned when reduce_func is None or when reduce_func acts on each pixel value individually. >>> profile_line(img, (1, 2), (4, 2), linewidth=3, order=0,\n...     reduce_func=None)\narray([[1, 1, 2],\n       [1, 1, 2],\n       [1, 1, 2],\n       [0, 0, 0]])\n>>> profile_line(img, (1, 0), (1, 3), linewidth=3, reduce_func=np.sqrt)\narray([[1.        , 1.        , 0.        ],\n       [1.        , 1.        , 0.        ],\n       [1.        , 1.        , 0.        ],\n       [1.41421356, 1.41421356, 0.        ]])\n \n"}, {"name": "measure.ransac()", "path": "api/skimage.measure#skimage.measure.ransac", "type": "measure", "text": " \nskimage.measure.ransac(data, model_class, min_samples, residual_threshold, is_data_valid=None, is_model_valid=None, max_trials=100, stop_sample_num=inf, stop_residuals_sum=0, stop_probability=1, random_state=None, initial_inliers=None) [source]\n \nFit a model to data with the RANSAC (random sample consensus) algorithm. RANSAC is an iterative algorithm for the robust estimation of parameters from a subset of inliers from the complete data set. Each iteration performs the following tasks:  Select min_samples random samples from the original data and check whether the set of data is valid (see is_data_valid). Estimate a model to the random subset (model_cls.estimate(*data[random_subset]) and check whether the estimated model is valid (see is_model_valid). Classify all data as inliers or outliers by calculating the residuals to the estimated model (model_cls.residuals(*data)) - all data samples with residuals smaller than the residual_threshold are considered as inliers. Save estimated model as best model if number of inlier samples is maximal. In case the current estimated model has the same number of inliers, it is only considered as the best model if it has less sum of residuals.  These steps are performed either a maximum number of times or until one of the special stop criteria are met. The final model is estimated using all inlier samples of the previously determined best model.  Parameters \n \ndata[list, tuple of] (N, \u2026) array \n\nData set to which the model is fitted, where N is the number of data points and the remaining dimension are depending on model requirements. If the model class requires multiple input data arrays (e.g. source and destination coordinates of skimage.transform.AffineTransform), they can be optionally passed as tuple or list. Note, that in this case the functions estimate(*data), residuals(*data), is_model_valid(model, *random_data) and is_data_valid(*random_data) must all take each data array as separate arguments.  \nmodel_classobject \n\nObject with the following object methods:  success = estimate(*data) residuals(*data)  where success indicates whether the model estimation succeeded (True or None for success, False for failure).  \nmin_samplesint in range (0, N) \n\nThe minimum number of data points to fit a model to.  \nresidual_thresholdfloat larger than 0 \n\nMaximum distance for a data point to be classified as an inlier.  \nis_data_validfunction, optional \n\nThis function is called with the randomly selected data before the model is fitted to it: is_data_valid(*random_data).  \nis_model_validfunction, optional \n\nThis function is called with the estimated model and the randomly selected data: is_model_valid(model, *random_data), .  \nmax_trialsint, optional \n\nMaximum number of iterations for random sample selection.  \nstop_sample_numint, optional \n\nStop iteration if at least this number of inliers are found.  \nstop_residuals_sumfloat, optional \n\nStop iteration if sum of residuals is less than or equal to this threshold.  \nstop_probabilityfloat in range [0, 1], optional \n\nRANSAC iteration stops if at least one outlier-free set of the training data is sampled with probability >= stop_probability, depending on the current best model\u2019s inlier ratio and the number of trials. This requires to generate at least N samples (trials): N >= log(1 - probability) / log(1 - e**m) where the probability (confidence) is typically set to a high value such as 0.99, e is the current fraction of inliers w.r.t. the total number of samples, and m is the min_samples value.  \nrandom_stateint, RandomState instance or None, optional \n\nIf int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.  \ninitial_inliersarray-like of bool, shape (N,), optional \n\nInitial samples selection for model estimation    Returns \n \nmodelobject \n\nBest model with largest consensus set.  \ninliers(N, ) array \n\nBoolean mask of inliers classified as True.     References  \n1  \n\u201cRANSAC\u201d, Wikipedia, https://en.wikipedia.org/wiki/RANSAC   Examples Generate ellipse data without tilt and add noise: >>> t = np.linspace(0, 2 * np.pi, 50)\n>>> xc, yc = 20, 30\n>>> a, b = 5, 10\n>>> x = xc + a * np.cos(t)\n>>> y = yc + b * np.sin(t)\n>>> data = np.column_stack([x, y])\n>>> np.random.seed(seed=1234)\n>>> data += np.random.normal(size=data.shape)\n Add some faulty data: >>> data[0] = (100, 100)\n>>> data[1] = (110, 120)\n>>> data[2] = (120, 130)\n>>> data[3] = (140, 130)\n Estimate ellipse model using all available data: >>> model = EllipseModel()\n>>> model.estimate(data)\nTrue\n>>> np.round(model.params)  \narray([ 72.,  75.,  77.,  14.,   1.])\n Estimate ellipse model using RANSAC: >>> ransac_model, inliers = ransac(data, EllipseModel, 20, 3, max_trials=50)\n>>> abs(np.round(ransac_model.params))\narray([20., 30.,  5., 10.,  0.])\n>>> inliers \narray([False, False, False, False,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True], dtype=bool)\n>>> sum(inliers) > 40\nTrue\n RANSAC can be used to robustly estimate a geometric transformation. In this section, we also show how to use a proportion of the total samples, rather than an absolute number. >>> from skimage.transform import SimilarityTransform\n>>> np.random.seed(0)\n>>> src = 100 * np.random.rand(50, 2)\n>>> model0 = SimilarityTransform(scale=0.5, rotation=1, translation=(10, 20))\n>>> dst = model0(src)\n>>> dst[0] = (10000, 10000)\n>>> dst[1] = (-100, 100)\n>>> dst[2] = (50, 50)\n>>> ratio = 0.5  # use half of the samples\n>>> min_samples = int(ratio * len(src))\n>>> model, inliers = ransac((src, dst), SimilarityTransform, min_samples, 10,\n...                         initial_inliers=np.ones(len(src), dtype=bool))\n>>> inliers\narray([False, False, False,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True])\n \n"}, {"name": "measure.regionprops()", "path": "api/skimage.measure#skimage.measure.regionprops", "type": "measure", "text": " \nskimage.measure.regionprops(label_image, intensity_image=None, cache=True, coordinates=None, *, extra_properties=None) [source]\n \nMeasure properties of labeled image regions.  Parameters \n \nlabel_image(M, N[, P]) ndarray \n\nLabeled input image. Labels with value 0 are ignored.  Changed in version 0.14.1: Previously, label_image was processed by numpy.squeeze and so any number of singleton dimensions was allowed. This resulted in inconsistent handling of images with singleton dimensions. To recover the old behaviour, use regionprops(np.squeeze(label_image), ...).   \nintensity_image(M, N[, P][, C]) ndarray, optional \n\nIntensity (i.e., input) image with same size as labeled image, plus optionally an extra dimension for multichannel data. Default is None.  Changed in version 0.18.0: The ability to provide an extra dimension for channels was added.   \ncachebool, optional \n\nDetermine whether to cache calculated properties. The computation is much faster for cached properties, whereas the memory consumption increases.  \ncoordinatesDEPRECATED \n\nThis argument is deprecated and will be removed in a future version of scikit-image. See Coordinate conventions for more details.  Deprecated since version 0.16.0: Use \u201crc\u201d coordinates everywhere. It may be sufficient to call numpy.transpose on your label image to get the same values as 0.15 and earlier. However, for some properties, the transformation will be less trivial. For example, the new orientation is \\(\\frac{\\pi}{2}\\) plus the old orientation.   \nextra_propertiesIterable of callables \n\nAdd extra property computation functions that are not included with skimage. The name of the property is derived from the function name, the dtype is inferred by calling the function on a small sample. If the name of an extra property clashes with the name of an existing property the extra property wil not be visible and a UserWarning is issued. A property computation function must take a region mask as its first argument. If the property requires an intensity image, it must accept the intensity image as the second argument.    Returns \n \npropertieslist of RegionProperties \n\nEach item describes one labeled region, and can be accessed using the attributes listed below.      See also  \nlabel\n\n  Notes The following properties can be accessed as attributes or keys:  \nareaint \n\nNumber of pixels of the region.  \nbboxtuple \n\nBounding box (min_row, min_col, max_row, max_col). Pixels belonging to the bounding box are in the half-open interval [min_row; max_row) and [min_col; max_col).  \nbbox_areaint \n\nNumber of pixels of bounding box.  \ncentroidarray \n\nCentroid coordinate tuple (row, col).  \nconvex_areaint \n\nNumber of pixels of convex hull image, which is the smallest convex polygon that encloses the region.  \nconvex_image(H, J) ndarray \n\nBinary convex hull image which has the same size as bounding box.  \ncoords(N, 2) ndarray \n\nCoordinate list (row, col) of the region.  \neccentricityfloat \n\nEccentricity of the ellipse that has the same second-moments as the region. The eccentricity is the ratio of the focal distance (distance between focal points) over the major axis length. The value is in the interval [0, 1). When it is 0, the ellipse becomes a circle.  \nequivalent_diameterfloat \n\nThe diameter of a circle with the same area as the region.  \neuler_numberint \n\nEuler characteristic of the set of non-zero pixels. Computed as number of connected components subtracted by number of holes (input.ndim connectivity). In 3D, number of connected components plus number of holes subtracted by number of tunnels.  \nextentfloat \n\nRatio of pixels in the region to pixels in the total bounding box. Computed as area / (rows * cols)  \nferet_diameter_maxfloat \n\nMaximum Feret\u2019s diameter computed as the longest distance between points around a region\u2019s convex hull contour as determined by find_contours. [5]  \nfilled_areaint \n\nNumber of pixels of the region will all the holes filled in. Describes the area of the filled_image.  \nfilled_image(H, J) ndarray \n\nBinary region image with filled holes which has the same size as bounding box.  \nimage(H, J) ndarray \n\nSliced binary region image which has the same size as bounding box.  \ninertia_tensorndarray \n\nInertia tensor of the region for the rotation around its mass.  \ninertia_tensor_eigvalstuple \n\nThe eigenvalues of the inertia tensor in decreasing order.  \nintensity_imagendarray \n\nImage inside region bounding box.  \nlabelint \n\nThe label in the labeled input image.  \nlocal_centroidarray \n\nCentroid coordinate tuple (row, col), relative to region bounding box.  \nmajor_axis_lengthfloat \n\nThe length of the major axis of the ellipse that has the same normalized second central moments as the region.  \nmax_intensityfloat \n\nValue with the greatest intensity in the region.  \nmean_intensityfloat \n\nValue with the mean intensity in the region.  \nmin_intensityfloat \n\nValue with the least intensity in the region.  \nminor_axis_lengthfloat \n\nThe length of the minor axis of the ellipse that has the same normalized second central moments as the region.  \nmoments(3, 3) ndarray \n\nSpatial moments up to 3rd order: m_ij = sum{ array(row, col) * row^i * col^j }\n where the sum is over the row, col coordinates of the region.  \nmoments_central(3, 3) ndarray \n\nCentral moments (translation invariant) up to 3rd order: mu_ij = sum{ array(row, col) * (row - row_c)^i * (col - col_c)^j }\n where the sum is over the row, col coordinates of the region, and row_c and col_c are the coordinates of the region\u2019s centroid.  \nmoments_hutuple \n\nHu moments (translation, scale and rotation invariant).  \nmoments_normalized(3, 3) ndarray \n\nNormalized moments (translation and scale invariant) up to 3rd order: nu_ij = mu_ij / m_00^[(i+j)/2 + 1]\n where m_00 is the zeroth spatial moment.  \norientationfloat \n\nAngle between the 0th axis (rows) and the major axis of the ellipse that has the same second moments as the region, ranging from -pi/2 to pi/2 counter-clockwise.  \nperimeterfloat \n\nPerimeter of object which approximates the contour as a line through the centers of border pixels using a 4-connectivity.  \nperimeter_croftonfloat \n\nPerimeter of object approximated by the Crofton formula in 4 directions.  \nslicetuple of slices \n\nA slice to extract the object from the source image.  \nsolidityfloat \n\nRatio of pixels in the region to pixels of the convex hull image.  \nweighted_centroidarray \n\nCentroid coordinate tuple (row, col) weighted with intensity image.  \nweighted_local_centroidarray \n\nCentroid coordinate tuple (row, col), relative to region bounding box, weighted with intensity image.  \nweighted_moments(3, 3) ndarray \n\nSpatial moments of intensity image up to 3rd order: wm_ij = sum{ array(row, col) * row^i * col^j }\n where the sum is over the row, col coordinates of the region.  \nweighted_moments_central(3, 3) ndarray \n\nCentral moments (translation invariant) of intensity image up to 3rd order: wmu_ij = sum{ array(row, col) * (row - row_c)^i * (col - col_c)^j }\n where the sum is over the row, col coordinates of the region, and row_c and col_c are the coordinates of the region\u2019s weighted centroid.  \nweighted_moments_hutuple \n\nHu moments (translation, scale and rotation invariant) of intensity image.  \nweighted_moments_normalized(3, 3) ndarray \n\nNormalized moments (translation and scale invariant) of intensity image up to 3rd order: wnu_ij = wmu_ij / wm_00^[(i+j)/2 + 1]\n where wm_00 is the zeroth spatial moment (intensity-weighted area).   Each region also supports iteration, so that you can do: for prop in region:\n    print(prop, region[prop])\n References  \n1  \nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.  \n2  \nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.  \n3  \nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.  \n4  \nhttps://en.wikipedia.org/wiki/Image_moment  \n5  \nW. Pabst, E. Gregorov\u00e1. Characterization of particles and particle systems, pp. 27-28. ICT Prague, 2007. https://old.vscht.cz/sil/keramika/Characterization_of_particles/CPPS%20_English%20version_.pdf   Examples >>> from skimage import data, util\n>>> from skimage.measure import label, regionprops\n>>> img = util.img_as_ubyte(data.coins()) > 110\n>>> label_img = label(img, connectivity=img.ndim)\n>>> props = regionprops(label_img)\n>>> # centroid of first labeled object\n>>> props[0].centroid\n(22.72987986048314, 81.91228523446583)\n>>> # centroid of first labeled object\n>>> props[0]['centroid']\n(22.72987986048314, 81.91228523446583)\n Add custom measurements by passing functions as extra_properties >>> from skimage import data, util\n>>> from skimage.measure import label, regionprops\n>>> import numpy as np\n>>> img = util.img_as_ubyte(data.coins()) > 110\n>>> label_img = label(img, connectivity=img.ndim)\n>>> def pixelcount(regionmask):\n...     return np.sum(regionmask)\n>>> props = regionprops(label_img, extra_properties=(pixelcount,))\n>>> props[0].pixelcount\n7741\n>>> props[1]['pixelcount']\n42\n \n"}, {"name": "measure.regionprops_table()", "path": "api/skimage.measure#skimage.measure.regionprops_table", "type": "measure", "text": " \nskimage.measure.regionprops_table(label_image, intensity_image=None, properties=('label', 'bbox'), *, cache=True, separator='-', extra_properties=None) [source]\n \nCompute image properties and return them as a pandas-compatible table. The table is a dictionary mapping column names to value arrays. See Notes section below for details.  New in version 0.16.   Parameters \n \nlabel_image(N, M[, P]) ndarray \n\nLabeled input image. Labels with value 0 are ignored.  \nintensity_image(M, N[, P][, C]) ndarray, optional \n\nIntensity (i.e., input) image with same size as labeled image, plus optionally an extra dimension for multichannel data. Default is None.  Changed in version 0.18.0: The ability to provide an extra dimension for channels was added.   \npropertiestuple or list of str, optional \n\nProperties that will be included in the resulting dictionary For a list of available properties, please see regionprops(). Users should remember to add \u201clabel\u201d to keep track of region identities.  \ncachebool, optional \n\nDetermine whether to cache calculated properties. The computation is much faster for cached properties, whereas the memory consumption increases.  \nseparatorstr, optional \n\nFor non-scalar properties not listed in OBJECT_COLUMNS, each element will appear in its own column, with the index of that element separated from the property name by this separator. For example, the inertia tensor of a 2D region will appear in four columns: inertia_tensor-0-0, inertia_tensor-0-1, inertia_tensor-1-0, and inertia_tensor-1-1 (where the separator is -). Object columns are those that cannot be split in this way because the number of columns would change depending on the object. For example, image and coords.  \nextra_propertiesIterable of callables \n\nAdd extra property computation functions that are not included with skimage. The name of the property is derived from the function name, the dtype is inferred by calling the function on a small sample. If the name of an extra property clashes with the name of an existing property the extra property wil not be visible and a UserWarning is issued. A property computation function must take a region mask as its first argument. If the property requires an intensity image, it must accept the intensity image as the second argument.    Returns \n \nout_dictdict \n\nDictionary mapping property names to an array of values of that property, one value per region. This dictionary can be used as input to pandas DataFrame to map property names to columns in the frame and regions to rows. If the image has no regions, the arrays will have length 0, but the correct type.     Notes Each column contains either a scalar property, an object property, or an element in a multidimensional array. Properties with scalar values for each region, such as \u201ceccentricity\u201d, will appear as a float or int array with that property name as key. Multidimensional properties of fixed size for a given image dimension, such as \u201ccentroid\u201d (every centroid will have three elements in a 3D image, no matter the region size), will be split into that many columns, with the name {property_name}{separator}{element_num} (for 1D properties), {property_name}{separator}{elem_num0}{separator}{elem_num1} (for 2D properties), and so on. For multidimensional properties that don\u2019t have a fixed size, such as \u201cimage\u201d (the image of a region varies in size depending on the region size), an object array will be used, with the corresponding property name as the key. Examples >>> from skimage import data, util, measure\n>>> image = data.coins()\n>>> label_image = measure.label(image > 110, connectivity=image.ndim)\n>>> props = measure.regionprops_table(label_image, image,\n...                           properties=['label', 'inertia_tensor',\n...                                       'inertia_tensor_eigvals'])\n>>> props  \n{'label': array([ 1,  2, ...]), ...\n 'inertia_tensor-0-0': array([  4.012...e+03,   8.51..., ...]), ...\n ...,\n 'inertia_tensor_eigvals-1': array([  2.67...e+02,   2.83..., ...])}\n The resulting dictionary can be directly passed to pandas, if installed, to obtain a clean DataFrame: >>> import pandas as pd  \n>>> data = pd.DataFrame(props)  \n>>> data.head()  \n   label  inertia_tensor-0-0  ...  inertia_tensor_eigvals-1\n0      1         4012.909888  ...                267.065503\n1      2            8.514739  ...                  2.834806\n2      3            0.666667  ...                  0.000000\n3      4            0.000000  ...                  0.000000\n4      5            0.222222  ...                  0.111111\n [5 rows x 7 columns] If we want to measure a feature that does not come as a built-in property, we can define custom functions and pass them as extra_properties. For example, we can create a custom function that measures the intensity quartiles in a region: >>> from skimage import data, util, measure\n>>> import numpy as np\n>>> def quartiles(regionmask, intensity):\n...     return np.percentile(intensity[regionmask], q=(25, 50, 75))\n>>>\n>>> image = data.coins()\n>>> label_image = measure.label(image > 110, connectivity=image.ndim)\n>>> props = measure.regionprops_table(label_image, intensity_image=image,\n...                                   properties=('label',),\n...                                   extra_properties=(quartiles,))\n>>> import pandas as pd \n>>> pd.DataFrame(props).head() \n       label  quartiles-0  quartiles-1  quartiles-2\n0      1       117.00        123.0        130.0\n1      2       111.25        112.0        114.0\n2      3       111.00        111.0        111.0\n3      4       111.00        111.5        112.5\n4      5       112.50        113.0        114.0\n \n"}, {"name": "measure.shannon_entropy()", "path": "api/skimage.measure#skimage.measure.shannon_entropy", "type": "measure", "text": " \nskimage.measure.shannon_entropy(image, base=2) [source]\n \nCalculate the Shannon entropy of an image. The Shannon entropy is defined as S = -sum(pk * log(pk)), where pk are frequency/probability of pixels of value k.  Parameters \n \nimage(N, M) ndarray \n\nGrayscale input image.  \nbasefloat, optional \n\nThe logarithmic base to use.    Returns \n \nentropyfloat \n   Notes The returned value is measured in bits or shannon (Sh) for base=2, natural unit (nat) for base=np.e and hartley (Hart) for base=10. References  \n1  \nhttps://en.wikipedia.org/wiki/Entropy_(information_theory)  \n2  \nhttps://en.wiktionary.org/wiki/Shannon_entropy   Examples >>> from skimage import data\n>>> from skimage.measure import shannon_entropy\n>>> shannon_entropy(data.camera())\n7.231695011055706\n \n"}, {"name": "measure.subdivide_polygon()", "path": "api/skimage.measure#skimage.measure.subdivide_polygon", "type": "measure", "text": " \nskimage.measure.subdivide_polygon(coords, degree=2, preserve_ends=False) [source]\n \nSubdivision of polygonal curves using B-Splines. Note that the resulting curve is always within the convex hull of the original polygon. Circular polygons stay closed after subdivision.  Parameters \n \ncoords(N, 2) array \n\nCoordinate array.  \ndegree{1, 2, 3, 4, 5, 6, 7}, optional \n\nDegree of B-Spline. Default is 2.  \npreserve_endsbool, optional \n\nPreserve first and last coordinate of non-circular polygon. Default is False.    Returns \n \ncoords(M, 2) array \n\nSubdivided coordinate array.     References  \n1  \nhttp://mrl.nyu.edu/publications/subdiv-course2000/coursenotes00.pdf   \n"}, {"name": "metrics", "path": "api/skimage.metrics", "type": "metrics", "text": "Module: metrics  \nskimage.metrics.adapted_rand_error([\u2026]) Compute Adapted Rand error as defined by the SNEMI3D contest.  \nskimage.metrics.contingency_table(im_true, \u2026) Return the contingency table for all regions in matched segmentations.  \nskimage.metrics.hausdorff_distance(image0, \u2026) Calculate the Hausdorff distance between nonzero elements of given images.  \nskimage.metrics.mean_squared_error(image0, \u2026) Compute the mean-squared error between two images.  \nskimage.metrics.normalized_root_mse(\u2026[, \u2026]) Compute the normalized root mean-squared error (NRMSE) between two images.  \nskimage.metrics.peak_signal_noise_ratio(\u2026) Compute the peak signal to noise ratio (PSNR) for an image.  \nskimage.metrics.structural_similarity(im1, \u2026) Compute the mean structural similarity index between two images.  \nskimage.metrics.variation_of_information([\u2026]) Return symmetric conditional entropies associated with the VI.   adapted_rand_error  \nskimage.metrics.adapted_rand_error(image_true=None, image_test=None, *, table=None, ignore_labels=(0, )) [source]\n \nCompute Adapted Rand error as defined by the SNEMI3D contest. [1]  Parameters \n \nimage_truendarray of int \n\nGround-truth label image, same shape as im_test.  \nimage_testndarray of int \n\nTest image.  \ntablescipy.sparse array in crs format, optional \n\nA contingency table built with skimage.evaluate.contingency_table. If None, it will be computed on the fly.  \nignore_labelssequence of int, optional \n\nLabels to ignore. Any part of the true image labeled with any of these values will not be counted in the score.    Returns \n \narefloat \n\nThe adapted Rand error; equal to \\(1 - \\frac{2pr}{p + r}\\), where p and r are the precision and recall described below.  \nprecfloat \n\nThe adapted Rand precision: this is the number of pairs of pixels that have the same label in the test label image and in the true image, divided by the number in the test image.  \nrecfloat \n\nThe adapted Rand recall: this is the number of pairs of pixels that have the same label in the test label image and in the true image, divided by the number in the true image.     Notes Pixels with label 0 in the true segmentation are ignored in the score. References  \n1  \nArganda-Carreras I, Turaga SC, Berger DR, et al. (2015) Crowdsourcing the creation of image segmentation algorithms for connectomics. Front. Neuroanat. 9:142. DOI:10.3389/fnana.2015.00142   \n contingency_table  \nskimage.metrics.contingency_table(im_true, im_test, *, ignore_labels=None, normalize=False) [source]\n \nReturn the contingency table for all regions in matched segmentations.  Parameters \n \nim_truendarray of int \n\nGround-truth label image, same shape as im_test.  \nim_testndarray of int \n\nTest image.  \nignore_labelssequence of int, optional \n\nLabels to ignore. Any part of the true image labeled with any of these values will not be counted in the score.  \nnormalizebool \n\nDetermines if the contingency table is normalized by pixel count.    Returns \n \ncontscipy.sparse.csr_matrix \n\nA contingency table. cont[i, j] will equal the number of voxels labeled i in im_true and j in im_test.     \n hausdorff_distance  \nskimage.metrics.hausdorff_distance(image0, image1) [source]\n \nCalculate the Hausdorff distance between nonzero elements of given images. The Hausdorff distance [1] is the maximum distance between any point on image0 and its nearest point on image1, and vice-versa.  Parameters \n \nimage0, image1ndarray \n\nArrays where True represents a point that is included in a set of points. Both arrays must have the same shape.    Returns \n \ndistancefloat \n\nThe Hausdorff distance between coordinates of nonzero pixels in image0 and image1, using the Euclidian distance.     References  \n1  \nhttp://en.wikipedia.org/wiki/Hausdorff_distance   Examples >>> points_a = (3, 0)\n>>> points_b = (6, 0)\n>>> shape = (7, 1)\n>>> image_a = np.zeros(shape, dtype=bool)\n>>> image_b = np.zeros(shape, dtype=bool)\n>>> image_a[points_a] = True\n>>> image_b[points_b] = True\n>>> hausdorff_distance(image_a, image_b)\n3.0\n \n Examples using skimage.metrics.hausdorff_distance\n \n  Hausdorff Distance   mean_squared_error  \nskimage.metrics.mean_squared_error(image0, image1) [source]\n \nCompute the mean-squared error between two images.  Parameters \n \nimage0, image1ndarray \n\nImages. Any dimensionality, must have same shape.    Returns \n \nmsefloat \n\nThe mean-squared error (MSE) metric.     Notes  Changed in version 0.16: This function was renamed from skimage.measure.compare_mse to skimage.metrics.mean_squared_error.  \n normalized_root_mse  \nskimage.metrics.normalized_root_mse(image_true, image_test, *, normalization='euclidean') [source]\n \nCompute the normalized root mean-squared error (NRMSE) between two images.  Parameters \n \nimage_truendarray \n\nGround-truth image, same shape as im_test.  \nimage_testndarray \n\nTest image.  \nnormalization{\u2018euclidean\u2019, \u2018min-max\u2019, \u2018mean\u2019}, optional \n\nControls the normalization method to use in the denominator of the NRMSE. There is no standard method of normalization across the literature [1]. The methods available here are as follows:  \n\u2018euclidean\u2019 : normalize by the averaged Euclidean norm of im_true: NRMSE = RMSE * sqrt(N) / || im_true ||\n where || . || denotes the Frobenius norm and N = im_true.size. This result is equivalent to: NRMSE = || im_true - im_test || / || im_true ||.\n  \u2018min-max\u2019 : normalize by the intensity range of im_true. \u2018mean\u2019 : normalize by the mean of im_true\n     Returns \n \nnrmsefloat \n\nThe NRMSE metric.     Notes  Changed in version 0.16: This function was renamed from skimage.measure.compare_nrmse to skimage.metrics.normalized_root_mse.  References  \n1  \nhttps://en.wikipedia.org/wiki/Root-mean-square_deviation   \n peak_signal_noise_ratio  \nskimage.metrics.peak_signal_noise_ratio(image_true, image_test, *, data_range=None) [source]\n \nCompute the peak signal to noise ratio (PSNR) for an image.  Parameters \n \nimage_truendarray \n\nGround-truth image, same shape as im_test.  \nimage_testndarray \n\nTest image.  \ndata_rangeint, optional \n\nThe data range of the input image (distance between minimum and maximum possible values). By default, this is estimated from the image data-type.    Returns \n \npsnrfloat \n\nThe PSNR metric.     Notes  Changed in version 0.16: This function was renamed from skimage.measure.compare_psnr to skimage.metrics.peak_signal_noise_ratio.  References  \n1  \nhttps://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio   \n structural_similarity  \nskimage.metrics.structural_similarity(im1, im2, *, win_size=None, gradient=False, data_range=None, multichannel=False, gaussian_weights=False, full=False, **kwargs) [source]\n \nCompute the mean structural similarity index between two images.  Parameters \n \nim1, im2ndarray \n\nImages. Any dimensionality with same shape.  \nwin_sizeint or None, optional \n\nThe side-length of the sliding window used in comparison. Must be an odd value. If gaussian_weights is True, this is ignored and the window size will depend on sigma.  \ngradientbool, optional \n\nIf True, also return the gradient with respect to im2.  \ndata_rangefloat, optional \n\nThe data range of the input image (distance between minimum and maximum possible values). By default, this is estimated from the image data-type.  \nmultichannelbool, optional \n\nIf True, treat the last dimension of the array as channels. Similarity calculations are done independently for each channel then averaged.  \ngaussian_weightsbool, optional \n\nIf True, each patch has its mean and variance spatially weighted by a normalized Gaussian kernel of width sigma=1.5.  \nfullbool, optional \n\nIf True, also return the full structural similarity image.    Returns \n \nmssimfloat \n\nThe mean structural similarity index over the image.  \ngradndarray \n\nThe gradient of the structural similarity between im1 and im2 [2]. This is only returned if gradient is set to True.  \nSndarray \n\nThe full SSIM image. This is only returned if full is set to True.    Other Parameters \n \nuse_sample_covariancebool \n\nIf True, normalize covariances by N-1 rather than, N where N is the number of pixels within the sliding window.  \nK1float \n\nAlgorithm parameter, K1 (small constant, see [1]).  \nK2float \n\nAlgorithm parameter, K2 (small constant, see [1]).  \nsigmafloat \n\nStandard deviation for the Gaussian when gaussian_weights is True.     Notes To match the implementation of Wang et. al. [1], set gaussian_weights to True, sigma to 1.5, and use_sample_covariance to False.  Changed in version 0.16: This function was renamed from skimage.measure.compare_ssim to skimage.metrics.structural_similarity.  References  \n1(1,2,3)  \nWang, Z., Bovik, A. C., Sheikh, H. R., & Simoncelli, E. P. (2004). Image quality assessment: From error visibility to structural similarity. IEEE Transactions on Image Processing, 13, 600-612. https://ece.uwaterloo.ca/~z70wang/publications/ssim.pdf, DOI:10.1109/TIP.2003.819861  \n2  \nAvanaki, A. N. (2009). Exact global histogram specification optimized for structural similarity. Optical Review, 16, 613-621. arXiv:0901.0065 DOI:10.1007/s10043-009-0119-z   \n variation_of_information  \nskimage.metrics.variation_of_information(image0=None, image1=None, *, table=None, ignore_labels=()) [source]\n \nReturn symmetric conditional entropies associated with the VI. [1] The variation of information is defined as VI(X,Y) = H(X|Y) + H(Y|X). If X is the ground-truth segmentation, then H(X|Y) can be interpreted as the amount of under-segmentation and H(X|Y) as the amount of over-segmentation. In other words, a perfect over-segmentation will have H(X|Y)=0 and a perfect under-segmentation will have H(Y|X)=0.  Parameters \n \nimage0, image1ndarray of int \n\nLabel images / segmentations, must have same shape.  \ntablescipy.sparse array in csr format, optional \n\nA contingency table built with skimage.evaluate.contingency_table. If None, it will be computed with skimage.evaluate.contingency_table. If given, the entropies will be computed from this table and any images will be ignored.  \nignore_labelssequence of int, optional \n\nLabels to ignore. Any part of the true image labeled with any of these values will not be counted in the score.    Returns \n \nvindarray of float, shape (2,) \n\nThe conditional entropies of image1|image0 and image0|image1.     References  \n1  \nMarina Meil\u0103 (2007), Comparing clusterings\u2014an information based distance, Journal of Multivariate Analysis, Volume 98, Issue 5, Pages 873-895, ISSN 0047-259X, DOI:10.1016/j.jmva.2006.11.013.   \n\n"}, {"name": "metrics.adapted_rand_error()", "path": "api/skimage.metrics#skimage.metrics.adapted_rand_error", "type": "metrics", "text": " \nskimage.metrics.adapted_rand_error(image_true=None, image_test=None, *, table=None, ignore_labels=(0, )) [source]\n \nCompute Adapted Rand error as defined by the SNEMI3D contest. [1]  Parameters \n \nimage_truendarray of int \n\nGround-truth label image, same shape as im_test.  \nimage_testndarray of int \n\nTest image.  \ntablescipy.sparse array in crs format, optional \n\nA contingency table built with skimage.evaluate.contingency_table. If None, it will be computed on the fly.  \nignore_labelssequence of int, optional \n\nLabels to ignore. Any part of the true image labeled with any of these values will not be counted in the score.    Returns \n \narefloat \n\nThe adapted Rand error; equal to \\(1 - \\frac{2pr}{p + r}\\), where p and r are the precision and recall described below.  \nprecfloat \n\nThe adapted Rand precision: this is the number of pairs of pixels that have the same label in the test label image and in the true image, divided by the number in the test image.  \nrecfloat \n\nThe adapted Rand recall: this is the number of pairs of pixels that have the same label in the test label image and in the true image, divided by the number in the true image.     Notes Pixels with label 0 in the true segmentation are ignored in the score. References  \n1  \nArganda-Carreras I, Turaga SC, Berger DR, et al. (2015) Crowdsourcing the creation of image segmentation algorithms for connectomics. Front. Neuroanat. 9:142. DOI:10.3389/fnana.2015.00142   \n"}, {"name": "metrics.contingency_table()", "path": "api/skimage.metrics#skimage.metrics.contingency_table", "type": "metrics", "text": " \nskimage.metrics.contingency_table(im_true, im_test, *, ignore_labels=None, normalize=False) [source]\n \nReturn the contingency table for all regions in matched segmentations.  Parameters \n \nim_truendarray of int \n\nGround-truth label image, same shape as im_test.  \nim_testndarray of int \n\nTest image.  \nignore_labelssequence of int, optional \n\nLabels to ignore. Any part of the true image labeled with any of these values will not be counted in the score.  \nnormalizebool \n\nDetermines if the contingency table is normalized by pixel count.    Returns \n \ncontscipy.sparse.csr_matrix \n\nA contingency table. cont[i, j] will equal the number of voxels labeled i in im_true and j in im_test.     \n"}, {"name": "metrics.hausdorff_distance()", "path": "api/skimage.metrics#skimage.metrics.hausdorff_distance", "type": "metrics", "text": " \nskimage.metrics.hausdorff_distance(image0, image1) [source]\n \nCalculate the Hausdorff distance between nonzero elements of given images. The Hausdorff distance [1] is the maximum distance between any point on image0 and its nearest point on image1, and vice-versa.  Parameters \n \nimage0, image1ndarray \n\nArrays where True represents a point that is included in a set of points. Both arrays must have the same shape.    Returns \n \ndistancefloat \n\nThe Hausdorff distance between coordinates of nonzero pixels in image0 and image1, using the Euclidian distance.     References  \n1  \nhttp://en.wikipedia.org/wiki/Hausdorff_distance   Examples >>> points_a = (3, 0)\n>>> points_b = (6, 0)\n>>> shape = (7, 1)\n>>> image_a = np.zeros(shape, dtype=bool)\n>>> image_b = np.zeros(shape, dtype=bool)\n>>> image_a[points_a] = True\n>>> image_b[points_b] = True\n>>> hausdorff_distance(image_a, image_b)\n3.0\n \n"}, {"name": "metrics.mean_squared_error()", "path": "api/skimage.metrics#skimage.metrics.mean_squared_error", "type": "metrics", "text": " \nskimage.metrics.mean_squared_error(image0, image1) [source]\n \nCompute the mean-squared error between two images.  Parameters \n \nimage0, image1ndarray \n\nImages. Any dimensionality, must have same shape.    Returns \n \nmsefloat \n\nThe mean-squared error (MSE) metric.     Notes  Changed in version 0.16: This function was renamed from skimage.measure.compare_mse to skimage.metrics.mean_squared_error.  \n"}, {"name": "metrics.normalized_root_mse()", "path": "api/skimage.metrics#skimage.metrics.normalized_root_mse", "type": "metrics", "text": " \nskimage.metrics.normalized_root_mse(image_true, image_test, *, normalization='euclidean') [source]\n \nCompute the normalized root mean-squared error (NRMSE) between two images.  Parameters \n \nimage_truendarray \n\nGround-truth image, same shape as im_test.  \nimage_testndarray \n\nTest image.  \nnormalization{\u2018euclidean\u2019, \u2018min-max\u2019, \u2018mean\u2019}, optional \n\nControls the normalization method to use in the denominator of the NRMSE. There is no standard method of normalization across the literature [1]. The methods available here are as follows:  \n\u2018euclidean\u2019 : normalize by the averaged Euclidean norm of im_true: NRMSE = RMSE * sqrt(N) / || im_true ||\n where || . || denotes the Frobenius norm and N = im_true.size. This result is equivalent to: NRMSE = || im_true - im_test || / || im_true ||.\n  \u2018min-max\u2019 : normalize by the intensity range of im_true. \u2018mean\u2019 : normalize by the mean of im_true\n     Returns \n \nnrmsefloat \n\nThe NRMSE metric.     Notes  Changed in version 0.16: This function was renamed from skimage.measure.compare_nrmse to skimage.metrics.normalized_root_mse.  References  \n1  \nhttps://en.wikipedia.org/wiki/Root-mean-square_deviation   \n"}, {"name": "metrics.peak_signal_noise_ratio()", "path": "api/skimage.metrics#skimage.metrics.peak_signal_noise_ratio", "type": "metrics", "text": " \nskimage.metrics.peak_signal_noise_ratio(image_true, image_test, *, data_range=None) [source]\n \nCompute the peak signal to noise ratio (PSNR) for an image.  Parameters \n \nimage_truendarray \n\nGround-truth image, same shape as im_test.  \nimage_testndarray \n\nTest image.  \ndata_rangeint, optional \n\nThe data range of the input image (distance between minimum and maximum possible values). By default, this is estimated from the image data-type.    Returns \n \npsnrfloat \n\nThe PSNR metric.     Notes  Changed in version 0.16: This function was renamed from skimage.measure.compare_psnr to skimage.metrics.peak_signal_noise_ratio.  References  \n1  \nhttps://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio   \n"}, {"name": "metrics.structural_similarity()", "path": "api/skimage.metrics#skimage.metrics.structural_similarity", "type": "metrics", "text": " \nskimage.metrics.structural_similarity(im1, im2, *, win_size=None, gradient=False, data_range=None, multichannel=False, gaussian_weights=False, full=False, **kwargs) [source]\n \nCompute the mean structural similarity index between two images.  Parameters \n \nim1, im2ndarray \n\nImages. Any dimensionality with same shape.  \nwin_sizeint or None, optional \n\nThe side-length of the sliding window used in comparison. Must be an odd value. If gaussian_weights is True, this is ignored and the window size will depend on sigma.  \ngradientbool, optional \n\nIf True, also return the gradient with respect to im2.  \ndata_rangefloat, optional \n\nThe data range of the input image (distance between minimum and maximum possible values). By default, this is estimated from the image data-type.  \nmultichannelbool, optional \n\nIf True, treat the last dimension of the array as channels. Similarity calculations are done independently for each channel then averaged.  \ngaussian_weightsbool, optional \n\nIf True, each patch has its mean and variance spatially weighted by a normalized Gaussian kernel of width sigma=1.5.  \nfullbool, optional \n\nIf True, also return the full structural similarity image.    Returns \n \nmssimfloat \n\nThe mean structural similarity index over the image.  \ngradndarray \n\nThe gradient of the structural similarity between im1 and im2 [2]. This is only returned if gradient is set to True.  \nSndarray \n\nThe full SSIM image. This is only returned if full is set to True.    Other Parameters \n \nuse_sample_covariancebool \n\nIf True, normalize covariances by N-1 rather than, N where N is the number of pixels within the sliding window.  \nK1float \n\nAlgorithm parameter, K1 (small constant, see [1]).  \nK2float \n\nAlgorithm parameter, K2 (small constant, see [1]).  \nsigmafloat \n\nStandard deviation for the Gaussian when gaussian_weights is True.     Notes To match the implementation of Wang et. al. [1], set gaussian_weights to True, sigma to 1.5, and use_sample_covariance to False.  Changed in version 0.16: This function was renamed from skimage.measure.compare_ssim to skimage.metrics.structural_similarity.  References  \n1(1,2,3)  \nWang, Z., Bovik, A. C., Sheikh, H. R., & Simoncelli, E. P. (2004). Image quality assessment: From error visibility to structural similarity. IEEE Transactions on Image Processing, 13, 600-612. https://ece.uwaterloo.ca/~z70wang/publications/ssim.pdf, DOI:10.1109/TIP.2003.819861  \n2  \nAvanaki, A. N. (2009). Exact global histogram specification optimized for structural similarity. Optical Review, 16, 613-621. arXiv:0901.0065 DOI:10.1007/s10043-009-0119-z   \n"}, {"name": "metrics.variation_of_information()", "path": "api/skimage.metrics#skimage.metrics.variation_of_information", "type": "metrics", "text": " \nskimage.metrics.variation_of_information(image0=None, image1=None, *, table=None, ignore_labels=()) [source]\n \nReturn symmetric conditional entropies associated with the VI. [1] The variation of information is defined as VI(X,Y) = H(X|Y) + H(Y|X). If X is the ground-truth segmentation, then H(X|Y) can be interpreted as the amount of under-segmentation and H(X|Y) as the amount of over-segmentation. In other words, a perfect over-segmentation will have H(X|Y)=0 and a perfect under-segmentation will have H(Y|X)=0.  Parameters \n \nimage0, image1ndarray of int \n\nLabel images / segmentations, must have same shape.  \ntablescipy.sparse array in csr format, optional \n\nA contingency table built with skimage.evaluate.contingency_table. If None, it will be computed with skimage.evaluate.contingency_table. If given, the entropies will be computed from this table and any images will be ignored.  \nignore_labelssequence of int, optional \n\nLabels to ignore. Any part of the true image labeled with any of these values will not be counted in the score.    Returns \n \nvindarray of float, shape (2,) \n\nThe conditional entropies of image1|image0 and image0|image1.     References  \n1  \nMarina Meil\u0103 (2007), Comparing clusterings\u2014an information based distance, Journal of Multivariate Analysis, Volume 98, Issue 5, Pages 873-895, ISSN 0047-259X, DOI:10.1016/j.jmva.2006.11.013.   \n"}, {"name": "morphology", "path": "api/skimage.morphology", "type": "morphology", "text": "Module: morphology  \nskimage.morphology.area_closing(image[, \u2026]) Perform an area closing of the image.  \nskimage.morphology.area_opening(image[, \u2026]) Perform an area opening of the image.  \nskimage.morphology.ball(radius[, dtype]) Generates a ball-shaped structuring element.  \nskimage.morphology.binary_closing(image[, \u2026]) Return fast binary morphological closing of an image.  \nskimage.morphology.binary_dilation(image[, \u2026]) Return fast binary morphological dilation of an image.  \nskimage.morphology.binary_erosion(image[, \u2026]) Return fast binary morphological erosion of an image.  \nskimage.morphology.binary_opening(image[, \u2026]) Return fast binary morphological opening of an image.  \nskimage.morphology.black_tophat(image[, \u2026]) Return black top hat of an image.  \nskimage.morphology.closing(image[, selem, out]) Return greyscale morphological closing of an image.  \nskimage.morphology.convex_hull_image(image) Compute the convex hull image of a binary image.  \nskimage.morphology.convex_hull_object(image, *) Compute the convex hull image of individual objects in a binary image.  \nskimage.morphology.cube(width[, dtype]) Generates a cube-shaped structuring element.  \nskimage.morphology.diameter_closing(image[, \u2026]) Perform a diameter closing of the image.  \nskimage.morphology.diameter_opening(image[, \u2026]) Perform a diameter opening of the image.  \nskimage.morphology.diamond(radius[, dtype]) Generates a flat, diamond-shaped structuring element.  \nskimage.morphology.dilation(image[, selem, \u2026]) Return greyscale morphological dilation of an image.  \nskimage.morphology.disk(radius[, dtype]) Generates a flat, disk-shaped structuring element.  \nskimage.morphology.erosion(image[, selem, \u2026]) Return greyscale morphological erosion of an image.  \nskimage.morphology.flood(image, seed_point, *) Mask corresponding to a flood fill.  \nskimage.morphology.flood_fill(image, \u2026[, \u2026]) Perform flood filling on an image.  \nskimage.morphology.h_maxima(image, h[, selem]) Determine all maxima of the image with height >= h.  \nskimage.morphology.h_minima(image, h[, selem]) Determine all minima of the image with depth >= h.  \nskimage.morphology.label(input[, \u2026]) Label connected regions of an integer array.  \nskimage.morphology.local_maxima(image[, \u2026]) Find local maxima of n-dimensional array.  \nskimage.morphology.local_minima(image[, \u2026]) Find local minima of n-dimensional array.  \nskimage.morphology.max_tree(image[, \u2026]) Build the max tree from an image.  \nskimage.morphology.max_tree_local_maxima(image) Determine all local maxima of the image.  \nskimage.morphology.medial_axis(image[, \u2026]) Compute the medial axis transform of a binary image  \nskimage.morphology.octagon(m, n[, dtype]) Generates an octagon shaped structuring element.  \nskimage.morphology.octahedron(radius[, dtype]) Generates a octahedron-shaped structuring element.  \nskimage.morphology.opening(image[, selem, out]) Return greyscale morphological opening of an image.  \nskimage.morphology.reconstruction(seed, mask) Perform a morphological reconstruction of an image.  \nskimage.morphology.rectangle(nrows, ncols[, \u2026]) Generates a flat, rectangular-shaped structuring element.  \nskimage.morphology.remove_small_holes(ar[, \u2026]) Remove contiguous holes smaller than the specified size.  \nskimage.morphology.remove_small_objects(ar) Remove objects smaller than the specified size.  \nskimage.morphology.skeletonize(image, *[, \u2026]) Compute the skeleton of a binary image.  \nskimage.morphology.skeletonize_3d(image) Compute the skeleton of a binary image.  \nskimage.morphology.square(width[, dtype]) Generates a flat, square-shaped structuring element.  \nskimage.morphology.star(a[, dtype]) Generates a star shaped structuring element.  \nskimage.morphology.thin(image[, max_iter]) Perform morphological thinning of a binary image.  \nskimage.morphology.watershed(image[, \u2026]) Deprecated function.  \nskimage.morphology.white_tophat(image[, \u2026]) Return white top hat of an image.   area_closing  \nskimage.morphology.area_closing(image, area_threshold=64, connectivity=1, parent=None, tree_traverser=None) [source]\n \nPerform an area closing of the image. Area closing removes all dark structures of an image with a surface smaller than area_threshold. The output image is larger than or equal to the input image for every pixel and all local minima have at least a surface of area_threshold pixels. Area closings are similar to morphological closings, but they do not use a fixed structuring element, but rather a deformable one, with surface = area_threshold. In the binary case, area closings are equivalent to remove_small_holes; this operator is thus extended to gray-level images. Technically, this operator is based on the max-tree representation of the image.  Parameters \n \nimagendarray \n\nThe input image for which the area_closing is to be calculated. This image can be of any type.  \narea_thresholdunsigned int \n\nThe size parameter (number of pixels). The default value is arbitrarily chosen to be 64.  \nconnectivityunsigned int, optional \n\nThe neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.  \nparentndarray, int64, optional \n\nParent image representing the max tree of the inverted image. The value of each pixel is the index of its parent in the ravelled array. See Note for further details.  \ntree_traverser1D array, int64, optional \n\nThe ordered pixel indices (referring to the ravelled array). The pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent).    Returns \n \noutputndarray \n\nOutput image of the same shape and type as input image.      See also  \nskimage.morphology.area_opening\n\n\nskimage.morphology.diameter_opening\n\n\nskimage.morphology.diameter_closing\n\n\nskimage.morphology.max_tree\n\n\nskimage.morphology.remove_small_objects\n\n\nskimage.morphology.remove_small_holes\n\n  Notes If a max-tree representation (parent and tree_traverser) are given to the function, they must be calculated from the inverted image for this function, i.e.: >>> P, S = max_tree(invert(f)) >>> closed = diameter_closing(f, 3, parent=P, tree_traverser=S) References  \n1  \nVincent L., Proc. \u201cGrayscale area openings and closings, their efficient implementation and applications\u201d, EURASIP Workshop on Mathematical Morphology and its Applications to Signal Processing, Barcelona, Spain, pp.22-27, May 1993.  \n2  \nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d (Chapter 6), 2nd edition (2003), ISBN 3540429883. DOI:10.1007/978-3-662-05088-0  \n3  \nSalembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected Operators for Image and Sequence Processing. IEEE Transactions on Image Processing, 7(4), 555-570. DOI:10.1109/83.663500  \n4  \nNajman, L., & Couprie, M. (2006). Building the component tree in quasi-linear time. IEEE Transactions on Image Processing, 15(11), 3531-3539. DOI:10.1109/TIP.2006.877518  \n5  \nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. DOI:10.1109/TIP.2014.2336551   Examples We create an image (quadratic function with a minimum in the center and 4 additional local minima. >>> w = 12\n>>> x, y = np.mgrid[0:w,0:w]\n>>> f = 180 + 0.2*((x - w/2)**2 + (y-w/2)**2)\n>>> f[2:3,1:5] = 160; f[2:4,9:11] = 140; f[9:11,2:4] = 120\n>>> f[9:10,9:11] = 100; f[10,10] = 100\n>>> f = f.astype(int)\n We can calculate the area closing: >>> closed = area_closing(f, 8, connectivity=1)\n All small minima are removed, and the remaining minima have at least a size of 8. \n area_opening  \nskimage.morphology.area_opening(image, area_threshold=64, connectivity=1, parent=None, tree_traverser=None) [source]\n \nPerform an area opening of the image. Area opening removes all bright structures of an image with a surface smaller than area_threshold. The output image is thus the largest image smaller than the input for which all local maxima have at least a surface of area_threshold pixels. Area openings are similar to morphological openings, but they do not use a fixed structuring element, but rather a deformable one, with surface = area_threshold. Consequently, the area_opening with area_threshold=1 is the identity. In the binary case, area openings are equivalent to remove_small_objects; this operator is thus extended to gray-level images. Technically, this operator is based on the max-tree representation of the image.  Parameters \n \nimagendarray \n\nThe input image for which the area_opening is to be calculated. This image can be of any type.  \narea_thresholdunsigned int \n\nThe size parameter (number of pixels). The default value is arbitrarily chosen to be 64.  \nconnectivityunsigned int, optional \n\nThe neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.  \nparentndarray, int64, optional \n\nParent image representing the max tree of the image. The value of each pixel is the index of its parent in the ravelled array.  \ntree_traverser1D array, int64, optional \n\nThe ordered pixel indices (referring to the ravelled array). The pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent).    Returns \n \noutputndarray \n\nOutput image of the same shape and type as the input image.      See also  \nskimage.morphology.area_closing\n\n\nskimage.morphology.diameter_opening\n\n\nskimage.morphology.diameter_closing\n\n\nskimage.morphology.max_tree\n\n\nskimage.morphology.remove_small_objects\n\n\nskimage.morphology.remove_small_holes\n\n  References  \n1  \nVincent L., Proc. \u201cGrayscale area openings and closings, their efficient implementation and applications\u201d, EURASIP Workshop on Mathematical Morphology and its Applications to Signal Processing, Barcelona, Spain, pp.22-27, May 1993.  \n2  \nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d (Chapter 6), 2nd edition (2003), ISBN 3540429883. :DOI:10.1007/978-3-662-05088-0  \n3  \nSalembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected Operators for Image and Sequence Processing. IEEE Transactions on Image Processing, 7(4), 555-570. :DOI:10.1109/83.663500  \n4  \nNajman, L., & Couprie, M. (2006). Building the component tree in quasi-linear time. IEEE Transactions on Image Processing, 15(11), 3531-3539. :DOI:10.1109/TIP.2006.877518  \n5  \nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. :DOI:10.1109/TIP.2014.2336551   Examples We create an image (quadratic function with a maximum in the center and 4 additional local maxima. >>> w = 12\n>>> x, y = np.mgrid[0:w,0:w]\n>>> f = 20 - 0.2*((x - w/2)**2 + (y-w/2)**2)\n>>> f[2:3,1:5] = 40; f[2:4,9:11] = 60; f[9:11,2:4] = 80\n>>> f[9:10,9:11] = 100; f[10,10] = 100\n>>> f = f.astype(int)\n We can calculate the area opening: >>> open = area_opening(f, 8, connectivity=1)\n The peaks with a surface smaller than 8 are removed. \n ball  \nskimage.morphology.ball(radius, dtype=<class 'numpy.uint8'>) [source]\n \nGenerates a ball-shaped structuring element. This is the 3D equivalent of a disk. A pixel is within the neighborhood if the Euclidean distance between it and the origin is no greater than radius.  Parameters \n \nradiusint \n\nThe radius of the ball-shaped structuring element.    Returns \n \nselemndarray \n\nThe structuring element where elements of the neighborhood are 1 and 0 otherwise.    Other Parameters \n \ndtypedata-type \n\nThe data type of the structuring element.     \n Examples using skimage.morphology.ball\n \n  Local Histogram Equalization  \n\n  Rank filters   binary_closing  \nskimage.morphology.binary_closing(image, selem=None, out=None) [source]\n \nReturn fast binary morphological closing of an image. This function returns the same result as greyscale closing but performs faster for binary images. The morphological closing on an image is defined as a dilation followed by an erosion. Closing can remove small dark spots (i.e. \u201cpepper\u201d) and connect small bright cracks. This tends to \u201cclose\u201d up (dark) gaps between (bright) features.  Parameters \n \nimagendarray \n\nBinary input image.  \nselemndarray, optional \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a cross-shaped structuring element (connectivity=1).  \noutndarray of bool, optional \n\nThe array to store the result of the morphology. If None, is passed, a new array will be allocated.    Returns \n \nclosingndarray of bool \n\nThe result of the morphological closing.     \n Examples using skimage.morphology.binary_closing\n \n  Flood Fill   binary_dilation  \nskimage.morphology.binary_dilation(image, selem=None, out=None) [source]\n \nReturn fast binary morphological dilation of an image. This function returns the same result as greyscale dilation but performs faster for binary images. Morphological dilation sets a pixel at (i,j) to the maximum over all pixels in the neighborhood centered at (i,j). Dilation enlarges bright regions and shrinks dark regions.  Parameters \n \nimagendarray \n\nBinary input image.  \nselemndarray, optional \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a cross-shaped structuring element (connectivity=1).  \noutndarray of bool, optional \n\nThe array to store the result of the morphology. If None is passed, a new array will be allocated.    Returns \n \ndilatedndarray of bool or uint \n\nThe result of the morphological dilation with values in [False, True].     \n binary_erosion  \nskimage.morphology.binary_erosion(image, selem=None, out=None) [source]\n \nReturn fast binary morphological erosion of an image. This function returns the same result as greyscale erosion but performs faster for binary images. Morphological erosion sets a pixel at (i,j) to the minimum over all pixels in the neighborhood centered at (i,j). Erosion shrinks bright regions and enlarges dark regions.  Parameters \n \nimagendarray \n\nBinary input image.  \nselemndarray, optional \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a cross-shaped structuring element (connectivity=1).  \noutndarray of bool, optional \n\nThe array to store the result of the morphology. If None is passed, a new array will be allocated.    Returns \n \nerodedndarray of bool or uint \n\nThe result of the morphological erosion taking values in [False, True].     \n binary_opening  \nskimage.morphology.binary_opening(image, selem=None, out=None) [source]\n \nReturn fast binary morphological opening of an image. This function returns the same result as greyscale opening but performs faster for binary images. The morphological opening on an image is defined as an erosion followed by a dilation. Opening can remove small bright spots (i.e. \u201csalt\u201d) and connect small dark cracks. This tends to \u201copen\u201d up (dark) gaps between (bright) features.  Parameters \n \nimagendarray \n\nBinary input image.  \nselemndarray, optional \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a cross-shaped structuring element (connectivity=1).  \noutndarray of bool, optional \n\nThe array to store the result of the morphology. If None is passed, a new array will be allocated.    Returns \n \nopeningndarray of bool \n\nThe result of the morphological opening.     \n Examples using skimage.morphology.binary_opening\n \n  Flood Fill   black_tophat  \nskimage.morphology.black_tophat(image, selem=None, out=None) [source]\n \nReturn black top hat of an image. The black top hat of an image is defined as its morphological closing minus the original image. This operation returns the dark spots of the image that are smaller than the structuring element. Note that dark spots in the original image are bright spots after the black top hat.  Parameters \n \nimagendarray \n\nImage array.  \nselemndarray, optional \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).  \noutndarray, optional \n\nThe array to store the result of the morphology. If None is passed, a new array will be allocated.    Returns \n \noutarray, same shape and type as image \n\nThe result of the morphological black top hat.      See also  \nwhite_tophat\n\n  References  \n1  \nhttps://en.wikipedia.org/wiki/Top-hat_transform   Examples >>> # Change dark peak to bright peak and subtract background\n>>> import numpy as np\n>>> from skimage.morphology import square\n>>> dark_on_grey = np.array([[7, 6, 6, 6, 7],\n...                          [6, 5, 4, 5, 6],\n...                          [6, 4, 0, 4, 6],\n...                          [6, 5, 4, 5, 6],\n...                          [7, 6, 6, 6, 7]], dtype=np.uint8)\n>>> black_tophat(dark_on_grey, square(3))\narray([[0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 1, 5, 1, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n \n closing  \nskimage.morphology.closing(image, selem=None, out=None) [source]\n \nReturn greyscale morphological closing of an image. The morphological closing on an image is defined as a dilation followed by an erosion. Closing can remove small dark spots (i.e. \u201cpepper\u201d) and connect small bright cracks. This tends to \u201cclose\u201d up (dark) gaps between (bright) features.  Parameters \n \nimagendarray \n\nImage array.  \nselemndarray, optional \n\nThe neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).  \noutndarray, optional \n\nThe array to store the result of the morphology. If None, is passed, a new array will be allocated.    Returns \n \nclosingarray, same shape and type as image \n\nThe result of the morphological closing.     Examples >>> # Close a gap between two bright lines\n>>> import numpy as np\n>>> from skimage.morphology import square\n>>> broken_line = np.array([[0, 0, 0, 0, 0],\n...                         [0, 0, 0, 0, 0],\n...                         [1, 1, 0, 1, 1],\n...                         [0, 0, 0, 0, 0],\n...                         [0, 0, 0, 0, 0]], dtype=np.uint8)\n>>> closing(broken_line, square(3))\narray([[0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1],\n       [0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n \n convex_hull_image  \nskimage.morphology.convex_hull_image(image, offset_coordinates=True, tolerance=1e-10) [source]\n \nCompute the convex hull image of a binary image. The convex hull is the set of pixels included in the smallest convex polygon that surround all white pixels in the input image.  Parameters \n \nimagearray \n\nBinary input image. This array is cast to bool before processing.  \noffset_coordinatesbool, optional \n\nIf True, a pixel at coordinate, e.g., (4, 7) will be represented by coordinates (3.5, 7), (4.5, 7), (4, 6.5), and (4, 7.5). This adds some \u201cextent\u201d to a pixel when computing the hull.  \ntolerancefloat, optional \n\nTolerance when determining whether a point is inside the hull. Due to numerical floating point errors, a tolerance of 0 can result in some points erroneously being classified as being outside the hull.    Returns \n \nhull(M, N) array of bool \n\nBinary image with pixels in convex hull set to True.     References  \n1  \nhttps://blogs.mathworks.com/steve/2011/10/04/binary-image-convex-hull-algorithm-notes/   \n convex_hull_object  \nskimage.morphology.convex_hull_object(image, *, connectivity=2) [source]\n \nCompute the convex hull image of individual objects in a binary image. The convex hull is the set of pixels included in the smallest convex polygon that surround all white pixels in the input image.  Parameters \n \nimage(M, N) ndarray \n\nBinary input image.  \nconnectivity{1, 2}, int, optional \n\nDetermines the neighbors of each pixel. Adjacent elements within a squared distance of connectivity from pixel center are considered neighbors.: 1-connectivity      2-connectivity\n      [ ]           [ ]  [ ]  [ ]\n       |               \\  |  /\n [ ]--[x]--[ ]      [ ]--[x]--[ ]\n       |               /  |  \\\n      [ ]           [ ]  [ ]  [ ]\n    Returns \n \nhullndarray of bool \n\nBinary image with pixels inside convex hull set to True.     Notes This function uses skimage.morphology.label to define unique objects, finds the convex hull of each using convex_hull_image, and combines these regions with logical OR. Be aware the convex hulls of unconnected objects may overlap in the result. If this is suspected, consider using convex_hull_image separately on each object or adjust connectivity. \n cube  \nskimage.morphology.cube(width, dtype=<class 'numpy.uint8'>) [source]\n \nGenerates a cube-shaped structuring element. This is the 3D equivalent of a square. Every pixel along the perimeter has a chessboard distance no greater than radius (radius=floor(width/2)) pixels.  Parameters \n \nwidthint \n\nThe width, height and depth of the cube.    Returns \n \nselemndarray \n\nA structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.    Other Parameters \n \ndtypedata-type \n\nThe data type of the structuring element.     \n diameter_closing  \nskimage.morphology.diameter_closing(image, diameter_threshold=8, connectivity=1, parent=None, tree_traverser=None) [source]\n \nPerform a diameter closing of the image. Diameter closing removes all dark structures of an image with maximal extension smaller than diameter_threshold. The maximal extension is defined as the maximal extension of the bounding box. The operator is also called Bounding Box Closing. In practice, the result is similar to a morphological closing, but long and thin structures are not removed. Technically, this operator is based on the max-tree representation of the image.  Parameters \n \nimagendarray \n\nThe input image for which the diameter_closing is to be calculated. This image can be of any type.  \ndiameter_thresholdunsigned int \n\nThe maximal extension parameter (number of pixels). The default value is 8.  \nconnectivityunsigned int, optional \n\nThe neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.  \nparentndarray, int64, optional \n\nPrecomputed parent image representing the max tree of the inverted image. This function is fast, if precomputed parent and tree_traverser are provided. See Note for further details.  \ntree_traverser1D array, int64, optional \n\nPrecomputed traverser, where the pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent). This function is fast, if precomputed parent and tree_traverser are provided. See Note for further details.    Returns \n \noutputndarray \n\nOutput image of the same shape and type as input image.      See also  \nskimage.morphology.area_opening\n\n\nskimage.morphology.area_closing\n\n\nskimage.morphology.diameter_opening\n\n\nskimage.morphology.max_tree\n\n  Notes If a max-tree representation (parent and tree_traverser) are given to the function, they must be calculated from the inverted image for this function, i.e.: >>> P, S = max_tree(invert(f)) >>> closed = diameter_closing(f, 3, parent=P, tree_traverser=S) References  \n1  \nWalter, T., & Klein, J.-C. (2002). Automatic Detection of Microaneurysms in Color Fundus Images of the Human Retina by Means of the Bounding Box Closing. In A. Colosimo, P. Sirabella, A. Giuliani (Eds.), Medical Data Analysis. Lecture Notes in Computer Science, vol 2526, pp. 210-220. Springer Berlin Heidelberg. DOI:10.1007/3-540-36104-9_23  \n2  \nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. DOI:10.1109/TIP.2014.2336551   Examples We create an image (quadratic function with a minimum in the center and 4 additional local minima. >>> w = 12\n>>> x, y = np.mgrid[0:w,0:w]\n>>> f = 180 + 0.2*((x - w/2)**2 + (y-w/2)**2)\n>>> f[2:3,1:5] = 160; f[2:4,9:11] = 140; f[9:11,2:4] = 120\n>>> f[9:10,9:11] = 100; f[10,10] = 100\n>>> f = f.astype(int)\n We can calculate the diameter closing: >>> closed = diameter_closing(f, 3, connectivity=1)\n All small minima with a maximal extension of 2 or less are removed. The remaining minima have all a maximal extension of at least 3. \n diameter_opening  \nskimage.morphology.diameter_opening(image, diameter_threshold=8, connectivity=1, parent=None, tree_traverser=None) [source]\n \nPerform a diameter opening of the image. Diameter opening removes all bright structures of an image with maximal extension smaller than diameter_threshold. The maximal extension is defined as the maximal extension of the bounding box. The operator is also called Bounding Box Opening. In practice, the result is similar to a morphological opening, but long and thin structures are not removed. Technically, this operator is based on the max-tree representation of the image.  Parameters \n \nimagendarray \n\nThe input image for which the area_opening is to be calculated. This image can be of any type.  \ndiameter_thresholdunsigned int \n\nThe maximal extension parameter (number of pixels). The default value is 8.  \nconnectivityunsigned int, optional \n\nThe neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.  \nparentndarray, int64, optional \n\nParent image representing the max tree of the image. The value of each pixel is the index of its parent in the ravelled array.  \ntree_traverser1D array, int64, optional \n\nThe ordered pixel indices (referring to the ravelled array). The pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent).    Returns \n \noutputndarray \n\nOutput image of the same shape and type as the input image.      See also  \nskimage.morphology.area_opening\n\n\nskimage.morphology.area_closing\n\n\nskimage.morphology.diameter_closing\n\n\nskimage.morphology.max_tree\n\n  References  \n1  \nWalter, T., & Klein, J.-C. (2002). Automatic Detection of Microaneurysms in Color Fundus Images of the Human Retina by Means of the Bounding Box Closing. In A. Colosimo, P. Sirabella, A. Giuliani (Eds.), Medical Data Analysis. Lecture Notes in Computer Science, vol 2526, pp. 210-220. Springer Berlin Heidelberg. DOI:10.1007/3-540-36104-9_23  \n2  \nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. DOI:10.1109/TIP.2014.2336551   Examples We create an image (quadratic function with a maximum in the center and 4 additional local maxima. >>> w = 12\n>>> x, y = np.mgrid[0:w,0:w]\n>>> f = 20 - 0.2*((x - w/2)**2 + (y-w/2)**2)\n>>> f[2:3,1:5] = 40; f[2:4,9:11] = 60; f[9:11,2:4] = 80\n>>> f[9:10,9:11] = 100; f[10,10] = 100\n>>> f = f.astype(int)\n We can calculate the diameter opening: >>> open = diameter_opening(f, 3, connectivity=1)\n The peaks with a maximal extension of 2 or less are removed. The remaining peaks have all a maximal extension of at least 3. \n diamond  \nskimage.morphology.diamond(radius, dtype=<class 'numpy.uint8'>) [source]\n \nGenerates a flat, diamond-shaped structuring element. A pixel is part of the neighborhood (i.e. labeled 1) if the city block/Manhattan distance between it and the center of the neighborhood is no greater than radius.  Parameters \n \nradiusint \n\nThe radius of the diamond-shaped structuring element.    Returns \n \nselemndarray \n\nThe structuring element where elements of the neighborhood are 1 and 0 otherwise.    Other Parameters \n \ndtypedata-type \n\nThe data type of the structuring element.     \n dilation  \nskimage.morphology.dilation(image, selem=None, out=None, shift_x=False, shift_y=False) [source]\n \nReturn greyscale morphological dilation of an image. Morphological dilation sets a pixel at (i,j) to the maximum over all pixels in the neighborhood centered at (i,j). Dilation enlarges bright regions and shrinks dark regions.  Parameters \n \nimagendarray \n\nImage array.  \nselemndarray, optional \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).  \noutndarray, optional \n\nThe array to store the result of the morphology. If None, is passed, a new array will be allocated.  \nshift_x, shift_ybool, optional \n\nshift structuring element about center point. This only affects eccentric structuring elements (i.e. selem with even numbered sides).    Returns \n \ndilateduint8 array, same shape and type as image \n\nThe result of the morphological dilation.     Notes For uint8 (and uint16 up to a certain bit-depth) data, the lower algorithm complexity makes the skimage.filters.rank.maximum function more efficient for larger images and structuring elements. Examples >>> # Dilation enlarges bright regions\n>>> import numpy as np\n>>> from skimage.morphology import square\n>>> bright_pixel = np.array([[0, 0, 0, 0, 0],\n...                          [0, 0, 0, 0, 0],\n...                          [0, 0, 1, 0, 0],\n...                          [0, 0, 0, 0, 0],\n...                          [0, 0, 0, 0, 0]], dtype=np.uint8)\n>>> dilation(bright_pixel, square(3))\narray([[0, 0, 0, 0, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n \n Examples using skimage.morphology.dilation\n \n  Rank filters   disk  \nskimage.morphology.disk(radius, dtype=<class 'numpy.uint8'>) [source]\n \nGenerates a flat, disk-shaped structuring element. A pixel is within the neighborhood if the Euclidean distance between it and the origin is no greater than radius.  Parameters \n \nradiusint \n\nThe radius of the disk-shaped structuring element.    Returns \n \nselemndarray \n\nThe structuring element where elements of the neighborhood are 1 and 0 otherwise.    Other Parameters \n \ndtypedata-type \n\nThe data type of the structuring element.     \n Examples using skimage.morphology.disk\n \n  Local Histogram Equalization  \n\n  Entropy  \n\n  Markers for watershed transform  \n\n  Flood Fill  \n\n  Segment human cells (in mitosis)  \n\n  Rank filters   erosion  \nskimage.morphology.erosion(image, selem=None, out=None, shift_x=False, shift_y=False) [source]\n \nReturn greyscale morphological erosion of an image. Morphological erosion sets a pixel at (i,j) to the minimum over all pixels in the neighborhood centered at (i,j). Erosion shrinks bright regions and enlarges dark regions.  Parameters \n \nimagendarray \n\nImage array.  \nselemndarray, optional \n\nThe neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).  \noutndarrays, optional \n\nThe array to store the result of the morphology. If None is passed, a new array will be allocated.  \nshift_x, shift_ybool, optional \n\nshift structuring element about center point. This only affects eccentric structuring elements (i.e. selem with even numbered sides).    Returns \n \nerodedarray, same shape as image \n\nThe result of the morphological erosion.     Notes For uint8 (and uint16 up to a certain bit-depth) data, the lower algorithm complexity makes the skimage.filters.rank.minimum function more efficient for larger images and structuring elements. Examples >>> # Erosion shrinks bright regions\n>>> import numpy as np\n>>> from skimage.morphology import square\n>>> bright_square = np.array([[0, 0, 0, 0, 0],\n...                           [0, 1, 1, 1, 0],\n...                           [0, 1, 1, 1, 0],\n...                           [0, 1, 1, 1, 0],\n...                           [0, 0, 0, 0, 0]], dtype=np.uint8)\